<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Subnetwork nodes on Zichen Wang</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/</link>
        <description>Recent content in Subnetwork nodes on Zichen Wang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 15 Mar 2023 20:13:00 -0500</lastBuildDate><atom:link href="https://zichen34.github.io/writenotes/model/subnetwork/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>read: Wi-HSNN for dimension reduction</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-batch-train/</link>
        <pubDate>Wed, 15 Mar 2023 20:13:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-batch-train/</guid>
        <description>&lt;p&gt;Authors: Wandong Zhang, Jonathan Wu, Yimin Yang &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0925231220311255&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neurocomputing (2020-07-18)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Wide hierarchical subnetwork-based neural network (Wi-HSNN)&lt;/li&gt;
&lt;li&gt;Iterative training by adding subnetnetwork nodes into modle one-by-one&lt;/li&gt;
&lt;li&gt;batch-by-batch training instead of processing the entire dataset (one-batch)
&lt;ul&gt;
&lt;li&gt;Place365 has 1.8 million samples&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h2&gt;
&lt;h2 id=&#34;3-proposed-wi-hsnn&#34;&gt;3 Proposed Wi-HSNN&lt;/h2&gt;
&lt;p&gt;Loss function of SLFN is MSE: min J= Â½ â€–ğ“ - g(ğ—, ğ°â‚, ğ›)â‹…ğ°â‚‚â€–Â²&lt;/p&gt;
&lt;p&gt;The output weights can be initialized from the Target data (letting ğ‡ denote g(ğ—, ğ°â‚, ğ›)):&lt;/p&gt;
&lt;p&gt;ğ°â‚‚ = ( ğ‡áµ€ğ‡ + I/C )â»Â¹ ğ‡áµ€ ğ“&lt;/p&gt;
&lt;p&gt;In this paper, the input data ğ— is first transformed to ğ…â‚‘â‚™,
which then passes a SNN (SLFN, subnetwork node) and becomes g(ğ…â‚‘â‚™â‹…ğšâ‚‘â‚“ + bâ‚‘â‚“).
Next, the input ğ— is fed into multiple SNN sequentially.
And their outputs are accumulated with the weight ğšâ‚œ.&lt;/p&gt;
&lt;p&gt;Therefore, if there are L SNNs, the loss function for this problem is:&lt;/p&gt;
&lt;p&gt;min J = Â½ â€–ğ“ - âˆ‘áµ¢â‚Œâ‚á´¸ g(ğ…â‚‘â‚™â±â‹…ğšâ‚‘â‚“â± + bâ‚‘â‚“â±) â‹… ğšâ‚œá´¸â€–Â²&lt;/p&gt;
&lt;h3 id=&#34;32-training-the-wi-hsnn&#34;&gt;3.2 Training the Wi-HSNN&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Feedforward with randomly initialized (ğšâ‚‘â‚™Â¹, bâ‚‘â‚™Â¹) and (ğšâ‚‘â‚“Â¹, bâ‚‘â‚“Â¹)
and calculate the optimal output weights based on pseudo-inverse:&lt;/p&gt;
&lt;p&gt;ğ…â‚‘â‚™Â¹ = g(ğ—â‹…ğšâ‚‘â‚™Â¹ + bâ‚‘â‚™Â¹) &lt;br&gt;
ğ…â‚œÂ¹ = ğ…â‚‘â‚“Â¹ = g(ğ…â‚‘â‚™Â¹ â‹…ğšâ‚‘â‚“Â¹ + bâ‚‘â‚“Â¹)    &lt;br&gt;
ğšâ‚œÂ¹ = (ğ…â‚œáµ€ğ…â‚œ + I/C)â»Â¹ ğ…â‚œáµ€ ğ“&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Obtain the feedback error (&amp;ldquo;feature H&amp;rdquo;) matrix ğâ‚‘â‚“Â¹ and ğâ‚‘â‚™Â¹
for solving the weights ğšâ‚‘â‚“ (=ğ°â‚‚), ğšâ‚‘â‚™ (=ğ°â‚) of next iteration.&lt;/p&gt;
&lt;p&gt;ğâ‚‘â‚“Â¹ = gâ»Â¹ (ğÂ¹ â‹… (I/C + (ğšâ‚œÂ¹)áµ€â‹…ğšâ‚œÂ¹)â»Â¹ â‹… (ğšâ‚œÂ¹)áµ€ ) &lt;br&gt;
ğâ‚‘â‚™Â¹ = gâ»Â¹ (ğâ‚‘â‚“Â¹ â‹… (I/C + (ğšâ‚‘â‚“Â¹)áµ€ â‹… ğšâ‚‘â‚“Â¹ )â»Â¹ â‹… (ğšâ‚‘â‚“Â¹)áµ€ )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the ğšâ‚‘â‚“, ğšâ‚‘â‚™ for next SNN:&lt;/p&gt;
&lt;p&gt;ğšâ‚‘â‚™â± = (ğ—áµ€ğ— + I/C )â»Â¹ ğ—áµ€ ğâ‚‘â‚“â±â»Â¹  (Entrance layer weights) &lt;br&gt;
ğšâ‚‘â‚“â± = ( (ğ…â‚‘â‚™â±)áµ€ğ…â‚‘â‚™â± + I/C )â»Â¹ (ğ…â‚‘â‚™â±)áµ€ ğâ‚‘â‚™â±â»Â¹    (Exit layer weights)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Summarize outputs ğ…â‚œâ± from all exisiting SNN, and update output weight ğšâ‚œâ±.&lt;/p&gt;
&lt;p&gt;ğ…â‚œâ± = âˆ‘â‚–â‚Œâ‚â± ğ…â‚‘â‚“áµ     &lt;br&gt;
ğšâ‚œâ± = ( (ğ…â‚œâ±)áµ€ğ…â‚œâ± + I/C )â»Â¹ (ğ…â‚œâ±)áµ€ ğ“&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Obtain the feedback error ğâ‚‘â‚“â± and ğâ‚‘â‚™â±:&lt;/p&gt;
&lt;p&gt;ğâ‚‘â‚“â± = gâ»Â¹ (ğâ± â‹… (I/C + (ğšâ‚œâ±)áµ€â‹…ğšâ‚œâ±)â»Â¹ â‹… (ğšâ‚œâ±)áµ€ ) &lt;br&gt;
ğâ‚‘â‚™â± = gâ»Â¹ (ğâ‚‘â‚“â± â‹… (I/C + (ğšâ‚‘â‚“â±)áµ€ â‹… ğšâ‚‘â‚“â± )â»Â¹ â‹… (ğšâ‚‘â‚“â±)áµ€ )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat step 3 to step 5 L-2 times.
ğ…â‚œá´¸ is the final encoding. And ğ˜ = ğ…â‚œá´¸ ğšâ‚œá´¸ is the final classification prediction.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;34-batch-by-batch-scheme-with-parallelism-strategy&#34;&gt;3.4 Batch-by-batch scheme with parallelism strategy&lt;/h3&gt;
&lt;p&gt;The entire feature set ğ…á´ºá•½áµˆ (i.e., ğ‡) and the target set ğ“ are split into p subsets:&lt;/p&gt;
&lt;p&gt;ğ‡ = $[^{ğ‡(ğ±â‚)}_{^{&amp;hellip;}_{ğ‡(ğ±â‚š)}}]$, ğ“ = $[^{ğ“(ğ±â‚)}_{^{&amp;hellip;}_{ğ“(ğ±â‚š)}}]$&lt;/p&gt;
&lt;p&gt;The desired weights matrix ğ°â‚‚ (i.e., &amp;ldquo;ğ›ƒ&amp;rdquo;) represented weith pseudo-inverse matrix becomes&lt;/p&gt;
&lt;p&gt;ğ°â‚‚ = (ğ‡áµ€ğ‡ + I/C)â»Â¹ğ…áµ€â‹…ğ“&lt;br&gt;
ğ°â‚‚ = ([ğ‡â‚áµ€,&amp;hellip;, ğ‡â‚šáµ€] $[^{_{ğ‡(ğ±â‚)}} _{^{&amp;hellip;}_{ğ‡(ğ±â‚š)}}] + ^I_{^-_C}$)â»Â¹
[ğ‡â‚áµ€,&amp;hellip;, ğ‡â‚šáµ€]
$[ ^{_{ğ“(ğ±â‚)}}_{^{&amp;hellip;}_{ğ“(ğ±â‚š)}} ]$&lt;br&gt;
ğ°â‚‚ = ([ğ‡(ğ±â‚)áµ€ğ‡(ğ±â‚) + &amp;hellip; + ğ‡(ğ±â‚š)áµ€ğ‡(ğ±â‚š)] + I/C)â»Â¹ â‹… [ğ‡(ğ±â‚)áµ€ğ“(ğ±â‚) + &amp;hellip; + ğ‡(ğ±â‚š)áµ€ğ“(ğ±â‚š) ] &lt;br&gt;
ğ°â‚‚ = (âˆ‘áµ¢â‚Œâ‚áµ– ğ‡(ğ±áµ¢)áµ€ğ‡(ğ±áµ¢) + I/C)â»Â¹ â‹… âˆ‘áµ¢â‚Œâ‚áµ– ğ‡(ğ±áµ¢)áµ€ğ“(ğ±áµ¢)&lt;/p&gt;
&lt;p&gt;First, calculate (âˆ‘áµ¢â‚Œâ‚áµ– ğ‡(ğ±áµ¢)áµ€ğ‡(ğ±áµ¢) + I/C)â»Â¹.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The matrices are accumulated batch by batch in the code.
After the 1st iteration, K=(ğ‡â‚áµ€ğ‡â‚ + I/C)â»Â¹ has obtained and returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When next batch ğ‡â‚‚ is retrieved:&lt;/p&gt;
&lt;p&gt;K_new = (ğ‡â‚‚áµ€ğ‡â‚‚ + ğ‡â‚áµ€ğ‡â‚ + I/C)â»Â¹ &lt;br&gt;
= (ğ‡â‚‚áµ€ğ‡â‚‚ + Kâ»Â¹)â»Â¹  # analogy to (UBV + A)â»Â¹, where U=ğ‡â‚‚áµ€, V=ğ‡â‚‚, B=I, A=Kâ»Â¹ &lt;br&gt;
Woodbury: &amp;ldquo;Aâ»Â¹ - Aâ»Â¹â‹…Uâ‹…(I+BVâ‹…Aâ»Â¹â‹…U)â»Â¹ BV Aâ»Â¹ &amp;quot; &lt;a class=&#34;link&#34; href=&#34;#%e5%91%a8%e5%bf%97%e6%88%90&#34; &gt;1&lt;/a&gt;   &lt;br&gt;
= K - Kâ‹…ğ‡â‚‚áµ€â‹…(I+ğ‡â‚‚â‹…Kâ‹…ğ‡â‚‚áµ€)â»Â¹ ğ‡â‚‚ K  &lt;br&gt;
= (I - Kâ‹…ğ‡â‚‚áµ€â‹…(I+ğ‡â‚‚â‹…Kâ‹…ğ‡â‚‚áµ€)â»Â¹ ğ‡â‚‚ ) â‹… K &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let Kâ‚š = (I - Kâ‹…ğ‡â‚‚áµ€â‹…(I+ğ‡â‚‚â‹…Kâ‹…ğ‡â‚‚áµ€)â»Â¹ ğ‡â‚‚ ).
So K_new = Kâ‚š â‹… K&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, for the second item âˆ‘áµ¢â‚Œâ‚áµ– ğ‡(ğ±áµ¢)áµ€ğ“(ğ±áµ¢),&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In the first batch, ğ›ƒ=Kâ‹…ğ‡â‚áµ€â‹…ğ“â‚ is obtained and returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When the second batch coming,
there should be (ğ‡â‚áµ€â‹…ğ“â‚ + ğ‡â‚‚áµ€â‹…ğ“â‚‚)&lt;/p&gt;
&lt;p&gt;ğ›ƒ_new = K_new â‹… (ğ‡â‚áµ€â‹…ğ“â‚ + ğ‡â‚‚áµ€â‹…ğ“â‚‚) &lt;br&gt;
= K_new â‹… (ğ‡â‚áµ€â‹…ğ“â‚ + ğ‡â‚‚áµ€â‹…ğ“â‚‚)     &lt;br&gt;
= Kâ‚š â‹… K â‹… ğ‡â‚áµ€â‹…ğ“â‚ + K_new â‹… ğ‡â‚‚áµ€â‹…ğ“â‚‚  &lt;br&gt;
= Kâ‚š â‹… ğ›ƒ + K_new â‹… ğ‡â‚‚áµ€â‹…ğ“â‚‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;çŸ©é˜µä¹‹å’Œçš„é€†&#34;&gt;çŸ©é˜µä¹‹å’Œçš„é€†&lt;/h3&gt;
&lt;p&gt;(DDG search: &amp;ldquo;çŸ©é˜µä¹‹å’Œçš„é€†&amp;rdquo;)
&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_33866593/article/details/103035289&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ä¸¤ä¸ªçŸ©é˜µç›¸åŠ åæ±‚é€† - ~æµ·æ£ ä¾æ—§~ - CSDN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.docin.com/p-713940771.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;å…³äºä¸¤ä¸ªçŸ©é˜µä¹‹å’Œé€†é˜µçš„è®¨è®º - docin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Google search: &amp;ldquo;ä¸¤çŸ©é˜µå’Œçš„é€†&amp;rdquo;)
&lt;a class=&#34;link&#34; href=&#34;https://ccjou.wordpress.com/2009/03/17/%E4%BA%8C%E7%9F%A9%E9%99%A3%E5%92%8C%E7%9A%84%E9%80%86%E7%9F%A9%E9%99%A3/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;å…©çŸ©é™£å’Œçš„é€†çŸ©é™£ - ç·šä»£å•Ÿç¤ºéŒ„ - å‘¨å¿—æˆ(é˜³æ˜äº¤å¤§)&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;çŸ©é˜µä¹‹å’Œçš„é€†-ä¸ç­‰äº-é€†çŸ©é˜µçš„å’Œ&#34;&gt;çŸ©é˜µä¹‹å’Œçš„é€† ä¸ç­‰äº é€†çŸ©é˜µçš„å’Œ&lt;/h3&gt;
&lt;p&gt;(A+B)â»Â¹ â‰  Aâ»Â¹ + Bâ»Â¹&lt;/p&gt;
&lt;p&gt;âˆµ (A+B)(Aâ»Â¹ + Bâ»Â¹) = E + BAâ»Â¹ + ABâ»Â¹ + E â‰  E&lt;/p&gt;
&lt;p&gt;æœ€ç®€å•çš„ä¾‹å­ï¼šå– A=B=Eï¼Œ(A+B)(Aâ»Â¹ + Bâ»Â¹) = E + BAâ»Â¹ + ABâ»Â¹ + E = 4E â‰  E
&lt;a class=&#34;link&#34; href=&#34;http://zhidao.baidu.com/question/541608921/answer/1368194722&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;çŸ©é˜µå’Œçš„é€†çŸ©é˜µ é€†çŸ©é˜µçš„å’Œ ç›¸ç­‰å— -ç™¾åº¦çŸ¥é“&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ref&#34;&gt;Ref&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;div id=&#34;å‘¨å¿—æˆ&#34;&gt;&lt;a href=&#34;https://ccjou.wordpress.com/2009/03/17/%E4%BA%8C%E7%9F%A9%E9%99%A3%E5%92%8C%E7%9A%84%E9%80%86%E7%9F%A9%E9%99%A3/&#34;&gt;å…©çŸ©é™£å’Œçš„é€†çŸ©é™£ - ç·šä»£å•Ÿç¤ºéŒ„ - å‘¨å¿—æˆ(é˜³æ˜äº¤å¤§)&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>read: Width-Growth Model with Subnetwork Nodes</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-refine-weights/</link>
        <pubDate>Wed, 15 Feb 2023 12:40:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-refine-weights/</guid>
        <description>&lt;p&gt;Authors: Wandong Zhang et. al. Publish date: 2020-03-30 (Finished in 2019)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/9050859&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IEEE Trans. Industrial Informatics&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/u/0/d/1buXSzjPvb-d56kPDDqKaccDxzkBBw87R/view&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;G.Drive&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=O0LaYK8AAAAJ&amp;amp;citation_for_view=O0LaYK8AAAAJ:u5HHmVD_uO8C&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;G.Scholar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Try to summary (2023-02-26):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Different features are concatenated and then fed into a &amp;ldquo;I-ELM with subnetwork nodes&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;What is optimized it the combination weights, but the feature vectors themselves are not changed.&lt;/li&gt;
&lt;li&gt;It is the weights (IW, ğ›ƒ) are refined.&lt;/li&gt;
&lt;li&gt;Sepecifically, the new R-SNN node is improved by adding a part of unlearned wights accquired from the residual error of the last node.
That is the weights are accumulated on the newest node. So the ultimate R-SNN node contains all the previous training outcomes.
What we kept is only the last R-SNN node, i.e., a SLFN.&lt;/li&gt;
&lt;li&gt;Does that require the performance of the final R-SNN is the best among the former nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(In code) The update process of a SLFN is as follows:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
subgraph node1[SLFN1]
h1((h1)) &amp; h2((h2)) &amp; he((he)) &amp; hd((hd))
end

In[&#34;X\n data\n matrix&#34;] --&gt; IW1 --&gt; h1 &amp; h2 &amp; he &amp; hd 
node1 --- beta1(&#34;ğ›ƒ1&#34;) --&gt; Yout1 --&gt; Error1
Target --&gt;|&#34;pinv&#34;| beta1
beta1 &amp; Error1 --&gt;|&#34;inv: ğ›ƒâ‹…P=ğâ‚&#34;| P[&#34;P\n ( The H\n yielding\n Error1) &#34;]
P &amp; In --&gt; IW&#39;[&#34;residual\n IW&#34;]
IW&#39; &amp; IW1 --&gt;sum((+)) --&gt; IW2

subgraph node2[SLFN2]
h21((h1)) &amp; h22((h2)) &amp; h2e((he)) &amp; h2d((hd))
end

IW2 --&gt; h21 &amp; h22 &amp; h2e &amp; h2d 
node2 --- beta2(&#34;ğ›ƒ2&#34;)
Target --&gt; |&#34;pinv&#34;| beta2
beta2 --&gt; Yout2
&lt;/div&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A supervised multi-layer subnetwork-based feature refinement and classification model for representation learning.&lt;/li&gt;
&lt;li&gt;Expand the width for a generalized hidden layer rather than stack more layers to go deeper&lt;/li&gt;
&lt;li&gt;One-shot solution for finding the meaningful latent space to recognize the objects rather than searching separate spaces to find a generalized feature space.&lt;/li&gt;
&lt;li&gt;Multimodal fusion fusing various feature sources into a superstate encoding instead of a unimodal feature coding in the traditional feature representation methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-introduction&#34;&gt;â… . Introduction&lt;/h2&gt;
&lt;p&gt;(Task &amp;amp; Application &amp;amp; List of ralated research field &amp;amp; Problem &amp;amp; Existing solutions brif)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Task: high-dimensional data processing and learning&lt;/li&gt;
&lt;li&gt;Problem definition: selecting the optimal feature descriptors&lt;/li&gt;
&lt;li&gt;2 branch of solutions: hand-crafted descriptors and deep-learning-based features.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Criticize the former feature extraction solutions and introduce proposed method:)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Features derived from approaches of those 2 categories are too inflexible to contribute a robust model.&lt;/li&gt;
&lt;li&gt;This method &amp;ldquo;encodes and refines these? raw features from multiple sources to improve the classification performance&amp;rdquo;. &lt;br&gt;
For example: 4 extracted features (from AlexNet, ResNet, HMP, and SPF) are concatenated into 1 vector taken as the input to a &amp;ldquo;3-layer&amp;rdquo; model,
where only a single &amp;ldquo;genearlized&amp;rdquo; hidden layer (latent space) bridges the raw feature space (transformation ax+b) and the final target space (residual error).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Recap deep learning models and mention the theory base of this work)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep networks often get &amp;ldquo;trapped in local minimum and are sensitive to the learning rate&amp;rdquo; because their training fundation is BP.&lt;/li&gt;
&lt;li&gt;Regression-based feature learning. Least-squares representation learning methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Problems to be solved) &lt;br&gt;
Drawbacks of regression-based approaches:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;block&amp;rdquo; models? don&amp;rsquo;t perform one-shot training philosophy based on the relation between raw data and the target.&lt;/li&gt;
&lt;li&gt;A model trained by some &amp;ldquo;designed&amp;rdquo; process has a inferious generalizatio n capacity than the model derived from one-shot training strategy (least-squares).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Drawbacks of multilayer neural networks &amp;amp; solution &lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deeper layer-stacked models suffer from overfitting with limited training samples.&lt;/li&gt;
&lt;li&gt;Network-in-network structure enhances the network&amp;rsquo;s generalization capacity for learning feature. &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/&#34; &gt;ELM with subnetwork nodes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Contributions:
&lt;ul&gt;
&lt;li&gt;Subnetwork neural nodes (SNN) realized multilayer representation learning. Unlike the ensembled network, the SNN is trained based on the error term.&lt;/li&gt;
&lt;li&gt;Feature space transformation and the classification are solved together by searching iteratively the optimal encoding space (hidden layer).&lt;/li&gt;
&lt;li&gt;Concatenation of multiple features result more discriminative representations for samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;-literature-review&#34;&gt;â…¡. Literature review&lt;/h2&gt;
&lt;h3 id=&#34;a-conventional-feature-coding&#34;&gt;A. Conventional Feature Coding&lt;/h3&gt;
&lt;p&gt;&amp;quot; Supervised method of learning representaiton evaluates the importance of a specific feature through the correlation between features and categories.&amp;quot;&lt;/p&gt;
&lt;p&gt;Conventional feature coding of images depends on prior knowledge of the problem. Thus, the features are not complete representations.&lt;/p&gt;
&lt;p&gt;This paper enhances the feature by fusing (discriminative) hand-crafted features and (class-specific) CNN-based features.&lt;/p&gt;
&lt;h3 id=&#34;b-least-squares-encoding-methods&#34;&gt;B. Least-Squares Encoding Methods&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The least-squares approximation methods, such as random forest and &lt;strong&gt;alternating minimization&lt;/strong&gt;, have been exhaustively investigated in single-layer neural networks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related works: Moore-Penrose inverse; Universal approximation capacity of I-ELM, ELM autoe-ncoder&lt;a class=&#34;link&#34; href=&#34;#ELM-AE&#34; &gt;14&lt;/a&gt;, Features combined with subnetwork nodes &lt;a class=&#34;link&#34; href=&#34;#2lyrSubnet&#34; &gt;18&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Each SNN is applied as a &lt;strong&gt;local feature descriptor&lt;/strong&gt;.
Hence, the subspace features can be extracted? from the original data independently,
and the useful features are generated via the &lt;strong&gt;combination&lt;/strong&gt; of these features.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;-proposed-method&#34;&gt;â…¢. Proposed Method&lt;/h2&gt;
&lt;h3 id=&#34;a-algorithmic-summary&#34;&gt;A. Algorithmic Summary&lt;/h3&gt;
&lt;p&gt;Two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;: concatenate various feature vectors into a single &amp;ldquo;supervector&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Train&lt;/strong&gt; the width-growth model: &lt;br&gt;
Terminology:
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;layer&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;th&gt;marker&lt;/th&gt;
&lt;th&gt;params&lt;/th&gt;
&lt;th&gt;in&lt;/th&gt;
&lt;th&gt;out&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;Entrance (feature) layer&lt;/td&gt;
&lt;td&gt;ğ‘“&lt;/td&gt;
&lt;td&gt;ğ–áµ¢á¶ , ğ›áµ¢á¶  random&lt;/td&gt;
&lt;td&gt;vct&lt;/td&gt;
&lt;td&gt;linear combination ğ‡&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hidden&lt;/td&gt;
&lt;td&gt;Refinement layer/subspace&lt;/td&gt;
&lt;td&gt;ğ‘Ÿ&lt;/td&gt;
&lt;td&gt;ğ–áµ¢Ê³, ğ›áµ¢Ê³ (ğš,b)&lt;/td&gt;
&lt;td&gt;ğ‡&lt;/td&gt;
&lt;td&gt;partial feature Î¨&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;Least square learning layr&lt;/td&gt;
&lt;td&gt;ğ‘£&lt;/td&gt;
&lt;td&gt;ğ–áµ¢áµ› (ğ›ƒ)&lt;/td&gt;
&lt;td&gt;Î¨&lt;/td&gt;
&lt;td&gt;sum up all partial features: ğšª &lt;br&gt; residual error ğ&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(An entrance layer and a refinement layer both are &amp;ldquo;SNN&amp;rdquo;, and their combination is a &amp;ldquo;R-SNN&amp;rdquo;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Initialization: For the 1st R-SNN, ğ–â‚á¶ , ğ–â‚Ê³ are random generating a false feature Î¨. &lt;br&gt;
Then the first least-square method (pseudoinverse) is performed to calculate ğ–â‚áµ› based on target ğ˜ and Î¨.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iteratively add the R-SNN (2â‰¤ iâ‰¤ L) (refinement subspace) into the hidden layer (optimal feature space)&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart TB
subgraph In[input feature]
x1((1)) &amp; x2((2)) &amp; xe((&#34;â‹®&#34;)) &amp; xn((n))
end

EnW(&#34;Entrance layer\n ğ–áµ¢á¶ , ğ›áµ¢á¶ \n random&#34;)

subgraph H[&#34;entrance feature ğ‡&#34;]
h1((1)) &amp; h2((2)) &amp; he((&#34;â‹®&#34;)) &amp; hD((D))
end

RefineW(&#34;Refinement layer\n ğ–áµ¢Ê³, ğ›áµ¢Ê³&#34;)

subgraph Psi[partial feature Î¨]
Î¨1((1)) &amp; Î¨2((2)) &amp; Î¨e((&#34;â‹®&#34;)) &amp; Î¨d((d))
end

OW(&#34;Output layer\n ğ–áµ¢áµ›&#34;)

subgraph Out[&#34;Output vector&#34;]
o1((1)) &amp; o2((2)) &amp; oe((&#34;â‹®&#34;)) &amp; om((m))
end

x1 &amp; x2 &amp; xe &amp; xn --&gt; EnW --&gt; h1 &amp; h2 &amp; he &amp; hD 
    --&gt; RefineW --&gt; Î¨1 &amp; Î¨2 &amp; Î¨e &amp; Î¨d 
    --&gt; OW --&gt; o1 &amp; o2 &amp; oe &amp; om

Out --&gt;|&#34;- ğáµ¢â‚‹â‚&#34;| erri[&#34;ğáµ¢&#34;]

erri &amp; OW -.-&gt;|pinv| newÎ¨(&#34;ğ \n yielding\n ğáµ¢&#34;)

subgraph H1[&#34;entrace feature ğ‡áµ¢â‚Šâ‚&#34;]
h11((1)) &amp; h12((2)) &amp; h1e((&#34;â‹®&#34;)) &amp; h1D((D))
end

In --&gt; EnW1(&#34;Entrance layer\n ğ–áµ¢â‚Šâ‚á¶ , ğ›áµ¢â‚Šâ‚á¶ \n random&#34;)
   --&gt; h11 &amp; h12 &amp; h1e &amp; h1D

H1 --&gt; RefineW1(&#34;Refinement layer\n ğ–áµ¢â‚Šâ‚Ê³, ğ›áµ¢â‚Šâ‚Ê³&#34;)
   %%-.-|solved by P| newÎ¨ 
newÎ¨ -.-&gt; RefineW1

subgraph Psi1[partial feature Î¨]
Î¨11((1)) &amp; Î¨12((2)) &amp; Î¨1e((&#34;â‹®&#34;)) &amp; Î¨1d((d))
end

RefineW1 --&gt; Î¨11 &amp; Î¨12 &amp; Î¨1e &amp; Î¨1d --&gt; OW1(&#34;Output layer\n ğ–áµ¢â‚Šâ‚áµ›&#34;)

%%OW1 -.-|solved by| erri
erri -.-&gt; OW1

subgraph Out1[&#34;Output vector&#34;]
o11((1)) &amp; o12((2)) &amp; o1e((&#34;â‹®&#34;)) &amp; o1m((m))
end

OW1 --&gt; o11 &amp; o12 &amp; o1e &amp; o1m 
Out1 --&gt;|&#34;- ğáµ¢&#34;| erri+1[&#34;ğáµ¢â‚Šâ‚&#34;] --&gt; newP
&lt;/div&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;b-model-definition&#34;&gt;B. Model Definition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SLFN solves the regression problem can be expressed as:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MLNN has nested transformation:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Proposed method is a generlized SLFN:&lt;/p&gt;
&lt;p&gt;minimize J = Â½ â€–ğ˜-f(ğ‡áµ¢á¶ , ğ–áµ¢Ê³, ğ›áµ¢Ê³)â‹…ğ–_Láµ›â€–Â²,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;f(ğ‡áµ¢á¶ , ğ–áµ¢Ê³, ğ›áµ¢Ê³) = âˆ‘áµ¢â‚Œâ‚á´¸ g(ğ‡áµ¢á¶  â‹… ğ–áµ¢Ê³ + ğ›áµ¢Ê³): sum all R-SNN&lt;/li&gt;
&lt;li&gt;ğ‡áµ¢á¶  = g(ğ–áµ¢á¶ , ğ›áµ¢á¶ , ğ—)&lt;/li&gt;
&lt;li&gt;ğ˜ âˆˆ â„á´ºá•½áµ: expected output, target feature&lt;/li&gt;
&lt;li&gt;ğ— âˆˆ â„á´ºá•½â¿: input matrix&lt;/li&gt;
&lt;li&gt;L : number of R-SNN node&lt;/li&gt;
&lt;li&gt;g : activateion function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3 differences from other least-squares-based MLNNs&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;SNN combines each dimension of the feature vector serving as local feature descriptor.
While the R-SNN is the basic unit to refine feature vectors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimal feature is the aggregation of R-SNN added one by one.
R-SNN is densly connected to input vector and output layer containing twice linear projection.
Different R-SNNs are independent because they learn from different error.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The latent space is the aggregation of all R-SNN nodes subspace.
So the parameters training has no block-wise communication between different spaces.
That means the feature refinement and classification are doen together.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c-proposed-width-growth-model&#34;&gt;C. Proposed Width-Growth Model&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Input weights and bias ğ–áµ¢á¶ , ğ›áµ¢á¶ : randomly initialized; &lt;br&gt;
Entrance feature: ğ‡áµ¢á¶  = g(ğ—ğ–áµ¢á¶ + ğ›áµ¢á¶ ); &lt;br&gt;
Refined partial feature: Î¨áµ¢=g(ğ‡áµ¢á¶ ğ–áµ¢Ê³+ ğ›áµ¢Ê³), where ğ›áµ¢Ê³ is random; &lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output weights: ğ–áµ¢áµ›=(ğˆ/C + ğšªáµ€ğšª)â»Â¹ğšªáµ€â‹…ğ˜, &lt;br&gt;
where C is hyperparameter for regularization,
and (ğˆ/C + ğšªáµ€ğšª)â»Â¹ğšªáµ€ is the pseudoinverse of output vector ğšª (label?) &lt;br&gt;
Error: ğáµ¢ = ğ˜ - ğ–áµ¢áµ›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ is the desired matrix generating ğáµ¢ by: ğáµ¢â‹…ğ–áµ¢áµ›=ğáµ¢, so &lt;br&gt;
ğáµ¢ = ğáµ¢â‹…(I/C + (ğ–áµ¢áµ›)áµ€ğ–áµ¢áµ›)â»Â¹(ğ–áµ¢áµ›)áµ€&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Refinement layer weights of next R-SNN:  &lt;br&gt;
ğ–áµ¢â‚Šâ‚Ê³ = (I/C +ğ‡áµ¢áµ€ğ‡áµ¢)â»Â¹ğ‡áµ¢áµ€ â‹… gâ»Â¹(ğáµ¢),     &lt;br&gt;
because g(ğ‡áµ¢â‚Šâ‚â‹…ğ–áµ¢â‚Šâ‚Ê³+ ğ›áµ¢â‚Šâ‚Ê³) = ğáµ¢. &lt;br&gt;
Next partial feature: Î¨áµ¢â‚Šâ‚ = g(ğ‡áµ¢â‚Šâ‚â‹…ğ–áµ¢â‚Šâ‚Ê³+ ğ›áµ¢â‚Šâ‚Ê³)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accumulate the partial feature to the optimal feature:
ğšªáµ¢â‚Šâ‚ = ğšªáµ¢ + Î¨áµ¢â‚Šâ‚  &lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update error: ğáµ¢â‚Šâ‚ = ğáµ¢-ğ–áµ¢áµ›ğšªáµ¢&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Repeat steps 4-6 L-2 times, and the final feature ğšª$_L$ is the generalized feature correponding to the best output parameter ğ– $_Láµ›$ for classification.&lt;/p&gt;
&lt;h2 id=&#34;ref&#34;&gt;Ref&lt;/h2&gt;
&lt;ol start=&#34;14&#34;&gt;
&lt;li&gt;
&lt;div id=&#34;ELM-AE&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/6733226&#34;&gt;L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, â€œRepresentational learning with extreme learning machine for big data,â€ IEEE Intell. Syst.,Dec. 2013.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;2lyrSubnet&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8627989&#34;&gt; Y. Yang and Q. J. Wu, â€œFeatures combined from hundreds of midlayers: Hierarchical networks with subnetwork nodes,â€ IEEE Trans. Neural Netw. Learn. Syst., Nov. 2019.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;ELM-MLP&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7103337&#34;&gt;J. Tang, C. Deng, and G.-B. Huang, â€œExtreme learning machine for multilayer perceptron,â€ IEEE Trans. Neural Netw. Learn. Syst., Apr. 2016.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>read: B-ELM</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-bidirec/</link>
        <pubDate>Fri, 10 Feb 2023 19:29:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-bidirec/</guid>
        <description>&lt;p&gt;Authors: Yimin Yang; Yaonan Wang; Xiaofang Yuan&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.yiminyang.com/code.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/6222007&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;TNNLS (2012-06-20)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Try to summary: &lt;br&gt;
(2023-02-15) B-ELM is a variant of I-ELM by dividing the hidden nodes into 2 types: odd and even,
which differ in wehther the input parameters (ğš,b) is &lt;strong&gt;randomly&lt;/strong&gt; generated or &lt;strong&gt;calculated&lt;/strong&gt; by twice inverse operations.&lt;/p&gt;
&lt;p&gt;Specifically, the first node is solved from the target ğ­. Then for subsequent nodes, the even node is solved based on the residual error of its previous odd node.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The output weights of the odd nodes are calculated as: ğ‡â‚‚â‚™â‚‹â‚Ê³ + eâ‚‚â‚™â‚‹â‚‚ (or ğ­) â” ğ›ƒâ‚‚â‚™â‚‹â‚ (â”eâ‚‚â‚™â‚‹â‚)&lt;/li&gt;
&lt;li&gt;The input weights ğš and bias b of even nodes are solved based on the residual error: eâ‚‚â‚™â‚‹â‚ â” ğ‡â‚‚â‚™áµ‰ â” ğš,b (â” ^ğ‡â‚‚â‚™áµ‰ â” ğ›ƒâ‚‚â‚™ â” eâ‚‚â‚™)&lt;/li&gt;
&lt;li&gt;Note: the superscript r and e stand for &amp;ldquo;random&amp;rdquo; and &amp;ldquo;error&amp;rdquo; marking the source of H.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This algorithm tends to reduce network output error to 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;by solving the input weights ğš and bias b based on the network residual error.
(In other words, the residual error is represented by a, b of subsequent nodes.
Or the error is absorbed by others parameters besides ğ›ƒ.)&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/B-ELM_goal.png width=&gt;
  
  


&lt;h2 id=&#34;-introduction&#34;&gt;â… . Introduction&lt;/h2&gt;
&lt;p&gt;For ELM with a fixed structure, the best number of hidden nodes need to ffound by trial-and-error,
because the residual error is not always decreasing when there are more hidden nodes in an ELM.&lt;/p&gt;
&lt;p&gt;For incremental ELM, the hidden node is added one by one, so the residual error keeps decreasing.
But the model training has to do multiple iterations, i.e., calculating inverse matrix is needed after adding each node.&lt;/p&gt;
&lt;p&gt;Compared with other incremental ELM, this method is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Faster and with fewer hidden nodes&lt;/li&gt;
&lt;li&gt;showing the relationship between the network output residual error ğ and output weights ğ›ƒ, which is named &amp;ldquo;error-output weights ellipse&amp;rdquo;&lt;/li&gt;
&lt;li&gt;The hidden layer (input weights) with determined parameters instead of random numbers would make the error reduce, or improve the accuracy.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;-preliminaries-and-notation&#34;&gt;â…¡. Preliminaries and Notation&lt;/h2&gt;
&lt;h3 id=&#34;a-notations-and-definitions&#34;&gt;A. Notations and Definitions&lt;/h3&gt;
&lt;p&gt;âŸ¨u,vâŸ© = âˆ«â‚“u(ğ±)vâ€¾(ğ±)dğ± is the &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Frobenius_inner_product&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Frobenius inner product&lt;/a&gt; of two matrices u,v,
where the overline denotes the complex conjugate.&lt;/p&gt;
&lt;h3 id=&#34;b-i-elm&#34;&gt;B. I-ELM&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;(proved by Huang&lt;a class=&#34;link&#34; href=&#34;#univrsl&#34; &gt;â¸&lt;/a&gt;): indicated that (For the incremental ELM,) the target function can be approximated with more nodes added into the network by reducing the residual error to 0,
as the ğ›ƒ of each new node is calculated based on the error of the network last status eâ‚™â‚‹â‚ as:&lt;/p&gt;
&lt;p&gt;ğ›ƒ = $\frac{âŸ¨eâ‚™â‚‹â‚, ğ‡â‚™Ê³âŸ©}{â€–ğ‡â‚™Ê³â€–Â²}$&lt;/p&gt;
&lt;p&gt;The numerator is the inner product measuring the distance from the nth (random) hidden node ğ‡â‚™Ê³ to be added and the error of the network with n-1 hidden nodes.&lt;/p&gt;
&lt;p&gt;So the output of the newly added hidden nodes ğ‡â‚™Ê³ are getting smaller and smaller, because they are learning something from the residual error.&lt;/p&gt;
&lt;h2 id=&#34;-proposed-bidirectional-elm-method&#34;&gt;â…¢. Proposed Bidirectional ELM Method&lt;/h2&gt;
&lt;figure&gt;&lt;img src=&#34;https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-2-source-large.gif&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;a-structure-of-the-proposed-bidirectional-elm-method&#34;&gt;A. Structure of the Proposed Bidirectional ELM Method&lt;/h2&gt;
&lt;p&gt;Two types of node, the node with &lt;strong&gt;odd index&lt;/strong&gt; {2n-1} has random ğš,b,
while the ğš,b of the node with &lt;strong&gt;even index&lt;/strong&gt; {2n} are calculated based on the residual error of the network with an odd number of nodes at the last moment.&lt;/p&gt;
&lt;p&gt;Their output weights both are calculated based on Lemma 1. The ğ‡ of the even node is from residual error, not from the random a,b.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;del&gt;The odd node aims to approximate the target through ğ‡â‚‚â‚™â‚‹â‚Ê³Î²â‚‚â‚™â‚‹â‚, where ğ‡â‚‚â‚™â‚‹â‚ is yield based on random a,b;&lt;/del&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The odd node 2n-1 approximates the previous residual error with random generated a,b;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But the even node approximates the residual error ğâ‚‚â‚™â‚‹â‚ through ğ‡â‚‚â‚™áµ‰Î²â‚‚â‚™ with calculated a,b,
where ğ‡â‚‚â‚™áµ‰ is yield with the weights a,b solved based on the residual error ğâ‚‚â‚™â‚‹â‚ from the target (the job hasn&amp;rsquo;t done by all the previous nodes)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;del&gt;&lt;strong&gt;Bi-direction&lt;/strong&gt; means the approximation is learned from both the target and the error,
where the odd node (Î²â‚‚â‚™â‚‹â‚) solved by the target, while the even node (Î²â‚‚â‚™) calculated by the error.
So a pair of odd node and even node is a complete step toward to the target.&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bi-directional&lt;/strong&gt; means Hâ‚‚â‚™áµ‰ is calculated first from eq.(6); Then it is calculated again using the ^a,^b, which are solved based on Hâ‚‚â‚™áµ‰, to get the updated ^Hâ‚‚â‚™áµ‰, which is used to calculate the output weight for the next random odd node based on the Lemma 1.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
subgraph in[inputs]
x1 &amp; xe[&#34;â‹®&#34;] &amp; xn
end

rand((&#34;random\n ğš,b&#34;)) 
calculated((&#34;solved\n ğš,b&#34;))

subgraph hid[hidden]
H1 &amp; he[&#34;â‹®&#34;] &amp; h2n-1 &amp; h2n
end

x1 &amp; xe &amp; xn --&gt; rand --&gt; h2n-1[&#34;ğ‡â‚‚â‚™â‚‹â‚Ê³&#34;] ---|Lemma 1| Î²2n-1((&#34;Î²â‚‚â‚™â‚‹â‚&#34;))--&gt; e2n-1[&#34;eâ‚‚â‚™â‚‹â‚&#34;]
x1 &amp; xe &amp; xn --&gt; calculated --&gt; h2n[&#34;ğ‡â‚‚â‚™áµ‰&#34;] ---|Lemma 1| Î²2n((&#34;Î²â‚‚â‚™&#34;))--&gt; e2n[&#34;eâ‚‚â‚™&#34;]

h2n -.- |&#34;â¬… inv of\n Î²â‚‚â‚™â‚‹â‚&#34;| e2n-1

calculated -.-|&#34;â¬… inv of\n ğ±&#34;| h2n
&lt;/div&gt;

&lt;p&gt;The dot lines represent the inverse calculation. &lt;br&gt;
Î²â‚‚â‚™â‚‹â‚ is derived from the network residual error of the last status.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Block diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart TB
init[Initialization: \n Given training set,\n expect accracy Î· and \n let #hidden nodes L=0] --&gt;
incre[L = L+1] --&gt; OdEv{&#34;Is L \n odd or even?&#34;}
OdEv --&gt;|L=2n+1| I-ELM --&gt; calcE[Calculate \n residual error E]
OdEv --&gt;|L=2n| Theorem2 --&gt; update[&#34;Update ^H_L = ^ğš_L ğ± + ^b)\n and calculate the output weight \n Î²_L based on eq.(7)&#34;] --&gt; calcE

subgraph Theorem2
direction TB
calcH[&#34;Calculate output matrix \n H_L basd on eq.(6)&#34;] 
--&gt; calcab[&#34;Calculate hidden-node parameters \n (^ğš_L,^b_L) based on eq.(14)&#34;]
end

subgraph I-ELM
direction TB
rand[&#34;Randomly assign hidden-node\n parameters (ğš_L,b_L) \n and obtain\n output matrix H_L&#34;] 
--&gt; |Lemma 1| outW[&#34;Calculate the output weight \n Î²L according to eq.(8)&#34;]
end

calcE --&gt; thres{&#34;â€–Eâ€–&lt;Î·&#34;} --&gt; |Yes| END
%%thres ----&gt;|No| incre  %% mess up the chart
incre o---o|No| thres
&lt;/div&gt;

&lt;h3 id=&#34;b-bidirectional-elm-method&#34;&gt;B. Bidirectional ELM method&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt; states the target function ğ‘“ is approximated by the existing network ğ‘“â‚‚â‚™â‚‹â‚‚ plus the last two nodes: ğ‡â‚‚â‚™â‚‹â‚Ê³Î²â‚‚â‚™â‚‹â‚ and ğ‡â‚‚â‚™áµ‰Î²â‚‚â‚™, when nâ†’âˆ. On the other hand, the residual error the network is 0:&lt;/p&gt;
&lt;p&gt;$lim_{nâ†’âˆ}â€–ğ‘“-(ğ‘“â‚‚â‚™â‚‹â‚‚ + ğ‡â‚‚â‚™â‚‹â‚Ê³Î²â‚‚â‚™â‚‹â‚ + ğ‡â‚‚â‚™áµ‰Î²â‚‚â‚™)â€– = 0$&lt;/p&gt;
&lt;p&gt;where the sequence of the ğ‡â‚‚â‚™áµ‰ (the output of the even node calculated from the feedback error) is determined by the inverse of last output weight Î²â‚‚â‚™â‚‹â‚:&lt;/p&gt;
&lt;p&gt;$ğ‡â‚‚â‚™áµ‰ = eâ‚‚â‚™â‚‹â‚ â‹…(Î²â‚‚â‚™â‚‹â‚)â»Â¹$  â€ƒ (6)&lt;/p&gt;
&lt;p&gt;That means $ğ‡â‚‚â‚™áµ‰ â‹… Î²â‚‚â‚™â‚‹â‚ = eâ‚‚â‚™â‚‹â‚$, this even node is approaching the last residual error based on the known output weight Î²â‚‚â‚™â‚‹â‚.&lt;/p&gt;
&lt;p&gt;Then its output weight can be calculated based on the Lemma 1 as:&lt;/p&gt;
&lt;p&gt;$Î²â‚‚â‚™ = \frac{âŸ¨eâ‚‚â‚™â‚‹â‚, ğ‡â‚‚â‚™áµ‰âŸ©}{â€–ğ‡â‚‚â‚™áµ‰â€–Â²}$ â€ƒ (7)&lt;/p&gt;
&lt;p&gt;Once this even node is determined, the corresponding residual error eâ‚‚â‚™can be generated,
and also the output weight of the next odd node (with ğ‡â‚‚â‚™â‚Šâ‚Ê³ from random weights) can be calculated based on Lemma 1:&lt;/p&gt;
&lt;p&gt;$Î²â‚‚â‚™â‚Šâ‚ = \frac{âŸ¨eâ‚‚â‚™, ğ‡â‚‚â‚™â‚Šâ‚Ê³âŸ©}{â€–ğ‡â‚‚â‚™â‚Šâ‚Ê³â€–Â²}$ â€ƒ (8)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt; further states the updated ^ğ‡â‚‚â‚™áµ‰ calculated with the optimal ^a, ^b solved based on ğ‡â‚‚â‚™áµ‰ by the least-square (pseudo inverse) can also let the residual error converge to 0.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark 1&lt;/strong&gt; clarifies the differences between this method and I-ELM, that is the input weights and bias of even nodes are calculated, not randomly generated. And the output weights are set similarly based on Lemma 1.&lt;/p&gt;
&lt;p&gt;Based on the eq. 6, the Î” = â€–eâ‚‚â‚™â‚‹â‚â€–Â² + â€–eâ‚‚â‚™â€–Â² can be reformalized to an ellipse curve.&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/Raspberrycai1/e3b4fd08c08c2c4820fa11d3cce04fef.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Training needs to store parameters for each node. And testing needs to query each node sequentially.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;
&lt;div id=&#34;univrsl&#34;&gt;&lt;a href=&#34;http://extreme-learning-machines.org/pdf/I-ELM.pdf&#34;&gt;G. B. Huang, L. Chen, and C. K. Siew, â€œUniversal approximation using incremental constructive feedforward networks with random hidden nodes,â€ IEEE Trans. Neural Netw., Jul.2006.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>sum: ELM</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/c-sum-elm/</link>
        <pubDate>Tue, 31 Jan 2023 15:49:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/c-sum-elm/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Extreme_learning_machine&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Wikipedia-ELM&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Controversy: RBF (1980s) raised the similar idea of ELM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dispute about the originality of ELM: &lt;a class=&#34;link&#34; href=&#34;https://elmorigin.wixsite.com/originofelm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Origins of ELM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.extreme-learning-machines.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Portal of ELM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/7140733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;python toolbox: hpelm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;facts&#34;&gt;Facts&lt;/h2&gt;
&lt;p&gt;ELM is &lt;a class=&#34;link&#34; href=&#34;#intro_csdn&#34; &gt;â½Â¹â¾&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a type of single hidden layer feedforward neural network (SLFN).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The parameters (ğ°,b) between input layer and hidden layer are set randomly. &lt;br&gt;
Thus, for N input n-dimensional samples and L hidden nodes, the output of the hidden layer is $ğ‡ = ğ—_{NÃ—n} ğ–_{nÃ—L}+ğ›_{nÃ—L}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Only the number of hidden nodes needs to be predefined manually without other hyper-parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The output weights are initialized randomly and solved based on the pseudo inverse matrix in one-shot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For a n-dimensional sample ğ±â±¼ and its target ğ­â±¼=[táµ¢â‚, táµ¢â‚‚, &amp;hellip;, táµ¢â‚˜]áµ€âˆˆ â„áµ, &lt;br&gt;
the output of ELM with L hidden nodes is ğ¨â±¼ = âˆ‘áµ¢â‚Œâ‚á´¸ ğ›ƒáµ¢ g(ğ°áµ¢áµ€â‹…ğ±â±¼ + báµ¢), where &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;g(â‹…) is activation function;&lt;/li&gt;
&lt;li&gt;ğ›ƒáµ¢ is the weights of the ith ouput unit: ğ›ƒáµ¢=[Î²áµ¢â‚, Î²áµ¢â‚‚, &amp;hellip;, Î²áµ¢â‚™]áµ€;&lt;/li&gt;
&lt;li&gt;ğ°áµ¢ is input weight: ğ°áµ¢=[wáµ¢â‚, wáµ¢â‚‚, &amp;hellip;, wáµ¢â‚™]áµ€;&lt;/li&gt;
&lt;li&gt;ğ±â±¼ is a n-dimensional input: ğ±â±¼=[xáµ¢â‚, xáµ¢â‚‚, &amp;hellip;, xáµ¢â‚™]áµ€âˆˆ â„â¿;&lt;/li&gt;
&lt;li&gt;báµ¢ is the bias of the ith hidden unit;&lt;/li&gt;
&lt;li&gt;ğ¨â±¼ is a m-dimensional vector: ğ¨â±¼=[oáµ¢â‚, oáµ¢â‚‚, &amp;hellip;, oáµ¢â‚˜]áµ€âˆˆ â„áµ;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ideal parameters (ğ°,b,ğ›ƒ) should satisfy: &lt;br&gt;
âˆ‘áµ¢â‚Œâ‚á´¸ ğ›ƒáµ¢ g(ğ°áµ¢áµ€â‹…ğ±â±¼ + báµ¢) = ğ­â±¼  &lt;br&gt;
For total N samples, this mapping can be reforomalized with matrices: &lt;br&gt;
$ğ‡_{NÃ—L} \pmb\beta_{LÃ—m} = ğ“_{NÃ—m}$, where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ğ‡ is the output of the hidden layer for N samples: &lt;br&gt;
$$ğ‡(ğ°â‚,&amp;hellip;,ğ°_L, bâ‚,&amp;hellip;,b_L, ğ±â‚,&amp;hellip;ğ±_L) = \\
\begin{bmatrix} g(ğ°â‚â‹…ğ±â‚+bâ‚) &amp;amp; \dots &amp;amp; g(ğ°_Lâ‹…ğ±â‚+b_L)\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
g(ğ°â‚â‹…ğ±_N+bâ‚) &amp;amp; \dots &amp;amp; g(ğ°_Lâ‹…ğ±_N+b_L) \end{bmatrix}_{NÃ—L}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ›ƒ is the output weights matrix: &lt;br&gt;
[ ğ›ƒâ‚áµ€ ; &amp;hellip; ; ğ›ƒ$_Láµ€ ]_{LÃ—m}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Target data: ğ“ = $\begin{bmatrix}ğ“â‚áµ€\\ \vdots \\ğ“_Náµ€\end{bmatrix}_{NÃ—m}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generally, $ğ‡_{NÃ—m}$ is not a square matrix (not invertible). Hence, ğ›ƒ=ğ‡â»Â¹ğ“ cannot be applied.
However, the optimal ğ›ƒ can be approached by minimizing the traning error iteratively:
âˆ‘â±¼â‚Œâ‚á´ºâ€–ğ¨â±¼-ğ­â±¼â€–.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Best estimation: $\^ğ°áµ¢, \^báµ¢$, ^ğ›ƒáµ¢ satisfy: &lt;br&gt;
â€–ğ‡(^ğ°áµ¢, ^báµ¢)â‹…^ğ›ƒáµ¢- ğ“â€– = min_{ğ°áµ¢, báµ¢, ğ›ƒáµ¢} â€–ğ‡(ğ°áµ¢, báµ¢)â‹…ğ›ƒáµ¢- ğ“â€–, where i=1,&amp;hellip;,L&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Loss function: J = âˆ‘â±¼â‚Œâ‚á´º (âˆ‘áµ¢â‚Œâ‚á´¸ ğ›ƒáµ¢â‹…g(ğ°áµ¢â‹…ğ±â±¼ + báµ¢) - ğ­â±¼)Â²&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solve ğ›ƒ based on the âˆ‚J/âˆ‚ğ›ƒ=0, such that the optimal parameter is: &lt;br&gt;
^ğ›ƒ = $ğ‡^â€  ğ“$ = (ğ‡áµ€ğ‡)â»Â¹ğ‡áµ€ ğ“, &lt;br&gt;
where $ğ‡^â€ $ is the Moore-Penrose inverse (Pseudo-inverse) of ğ‡. &lt;br&gt;
It can be proved that the norm of ^ğ›ƒ is the smallest and unique solution (for a set of random (ğ°áµ¢, báµ¢)).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;moore-penrose-inverse&#34;&gt;Moore-Penrose inverse&lt;/h2&gt;
&lt;p&gt;Also called pseudoinverse or generalized inverse &lt;a class=&#34;link&#34; href=&#34;#wiki_MPinv&#34; &gt;â½Â²â¾&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(bilibili search: &amp;ldquo;ä¼ªé€†çŸ©é˜µ&amp;rdquo;)
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1364y1678r/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;æ·±åº¦å­¦ä¹ -å•ƒèŠ±ä¹¦0103ä¼ªé€†çŸ©é˜µæœ€å°äºŒä¹˜&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(DDG search: &amp;ldquo;ä¼ªé€†çŸ©é˜µ&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.zhihu.com/question/47688307&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ä¼ªé€†çŸ©é˜µçš„æ„ä¹‰åŠæ±‚æ³•ï¼Ÿ - çŸ¥ä¹&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;numpylinalgpinv&#34;&gt;numpy.linalg.pinv()&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;pinv(ğ—) = (ğ—áµ€ ğ—)â»Â¹ ğ—áµ€&lt;/li&gt;
&lt;li&gt;pinv(ğ—) ğ— = ğˆ
&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/shiyuzuxiaqianli/article/details/105558488&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;pythonä¹‹numpyä¹‹ä¼ªé€†numpy.linalg.pinv - åƒè¡Œç™¾è¡Œ - CSDN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-code&#34;&gt;Example Code&lt;/h2&gt;
&lt;p&gt;This matlab code &lt;a class=&#34;link&#34; href=&#34;#intro_csdn&#34; &gt;â½Â¹â¾&lt;/a&gt; trains and tests a ELM on the NIR spectra dataset (regression) and the Iris dataset (classification).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note that each &lt;strong&gt;column&lt;/strong&gt; is a sample, and each row is an attribute/feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q: number of samples&lt;/li&gt;
&lt;li&gt;R: input features&lt;/li&gt;
&lt;li&gt;S: output features&lt;/li&gt;
&lt;li&gt;$P_{RÃ—Q}$: input pattern matrix&lt;/li&gt;
&lt;li&gt;$T_{SÃ—Q}$: target data matrix&lt;/li&gt;
&lt;li&gt;N: number of hidden nodes&lt;/li&gt;
&lt;li&gt;TF: transfer function&lt;/li&gt;
&lt;li&gt;$IW_{NÃ—R}$: input weights matrix&lt;/li&gt;
&lt;li&gt;$B_{NÃ—Q}$: bias matrix&lt;/li&gt;
&lt;li&gt;$LW_{NÃ—S}$: transposed output weights matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Train (calculate the LW):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$tempH_{NÃ—Q} = IW_{NÃ—R}â‹…P_{RÃ—Q} + B_{NÃ—Q}$&lt;/li&gt;
&lt;li&gt;$H_{NÃ—Q} = TF(tempH)$&lt;/li&gt;
&lt;li&gt;$LW_{SÃ—N} = T_{SÃ—Q}$â‹… pinv(H), based on: ğ›ƒ$_{SÃ—N} ğ‡_{NÃ—Q} = ğ“_{SÃ—Q}$&lt;/li&gt;
&lt;/ul&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/Raspberrycai1/c305edcf289ecddb5b8caa7d2d5558fa.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$tempH_{NÃ—Q} = IW_{NÃ—R}â‹…P_{RÃ—Q} + B_{NÃ—Q}$&lt;/li&gt;
&lt;li&gt;$H_{NÃ—Q} = TF(tempH)$&lt;/li&gt;
&lt;li&gt;$Y_{SÃ—Q} = LW_{SÃ—N}â‹…H_{NÃ—Q}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-code-py&#34;&gt;Example code (py)&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/build-an-extreme-learning-machine-in-python-91d1e8958599&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Build an Extreme Learning Machine in Python | by Glenn Paul Gara &amp;hellip;&lt;/a&gt;
searched by DDG: &amp;ldquo;incremental elm python&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;i-elm&#34;&gt;I-ELM&lt;/h2&gt;
&lt;p&gt;incremental just means adding neurons?&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/mlearning-ai/incremental-extreme-machine-learning-i-elm-11d5f26b220a&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;github&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;os-elm&#34;&gt;OS-ELM&lt;/h2&gt;
&lt;p&gt;On-line elm&lt;/p&gt;
&lt;h2 id=&#34;deep-incremental-rvfl&#34;&gt;Deep incremental RVFL&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1568494623004283&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Deep incremental random vector functional-link network: A non-iterative constructive sketch via greedy feature learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;div id=&#34;intro_csdn&#34;&gt;&lt;a href=&#34;https://blog.csdn.net/lyxleft/article/details/82892383&#34;&gt;æé™å­¦ä¹ æœº(Extreme Learning Machine, ELM)åŸç†è¯¦è§£å’ŒMATLABå®ç° - å¥”è·‘çš„Yancy - CSDN&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;wiki_MPinv&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&#34;&gt;Moore-Penorse - Wikipedia&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(&lt;a class=&#34;link&#34; href=&#34;#facts&#34; &gt;Back to Top&lt;/a&gt;)&lt;/p&gt;
</description>
        </item>
        <item>
        <title>watch: SNN - æ¨æ˜“æ—» | WeChat Live</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/</link>
        <pubDate>Fri, 27 Jan 2023 10:19:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/</guid>
        <description>&lt;p&gt;ä¸‹ä¸€ä»£å›½é™…åäººé’å¹´å­¦å­é¢å¯¹é¢ ç¬¬6æœŸ 2022å¹´10æœˆ20æ—¥ å‘¨å››&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Research work&lt;/li&gt;
&lt;li&gt;Suggestions for graduate student (1st year)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Research Contribution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-iterative learning strategy for training neural networks&lt;/strong&gt;
including single-layer networks, multi-layer networks, autoencoders, hierarchical networks, and deep networks.
All the related publications in this category are my first-author papers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The proposed methods for pattern recognition related applications&lt;/strong&gt;:
Image Recognition, Video Recognition, Hybrid System Approximation, Robotics System Identification, EEG-brain Signal Processing.
Most the related publications in this category are co-author papers with my HQPs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;my-research-works&#34;&gt;My research works&lt;/h3&gt;
&lt;p&gt;In the past 10 years, the works about Artificial neural networksï¼š&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Theoretical Contributions to ANN&lt;/th&gt;
&lt;th&gt;Machine Learning based Applications&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Single-layer network with non-iterative learning&lt;/td&gt;
&lt;td&gt;Data Analysis and Robotics System Identification (Ph.D.)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hierarchical NN with Subnetwork Neurons&lt;/td&gt;
&lt;td&gt;Image Recognitions (Post Doctoral Fellow)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deep Networks without iterative learning&lt;/td&gt;
&lt;td&gt;Pattern Recognition (since 2018)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;-single-layer-network-with-non-iterative-learning&#34;&gt;â… . Single-layer network with non-iterative learning&lt;/h2&gt;
&lt;h3 id=&#34;starting-with-a-small-idea&#34;&gt;Starting with a small idea&lt;/h3&gt;
&lt;p&gt;The labtorary mainly studies robots, control, mechanics.
After 2008 Chinese Winter Storms, they got funding for creating Powerline De-icing robots.&lt;/p&gt;
&lt;p&gt;The supervisor (Yaonan Wang): &amp;ldquo;Can you find a Neural Network for Identifying Robotics Dynamic Systems?&amp;rdquo; (in 2009 winter)&lt;/p&gt;
&lt;p&gt;Later, I found the following paper:
&amp;ldquo;Universal approximation using incremental constructive feedforward networks with random hidden nodes&amp;rdquo;, By Huang, Guang-Bin et.al (Cannot be found on IEEE)
&lt;a class=&#34;link&#34; href=&#34;http://extreme-learning-machines.org/pdf/I-ELM.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;version on elm portal&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-is-the-neural-network&#34;&gt;What is the Neural Network?&lt;/h3&gt;
&lt;p&gt;Single hidden layer feedforward NN:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart BT
subgraph in[Input Layer]
x1((1)) &amp; xe((&#34;...&#34;)) &amp; xn((n))
end
subgraph hid[Hidden Layer]
h1((&#34;ğ›‚â‚,â™­â‚,ğ›ƒâ‚&#34;)) &amp; h2((&#34;ğ›‚â‚‚,â™­â‚‚,ğ›ƒâ‚‚&#34;)) &amp; he((&#34;......&#34;)) &amp; hL((&#34;ğ›‚L,â™­L,ğ›ƒL&#34;))
end
subgraph out[Output Layer]
y1((1)) &amp; ye((&#34;...&#34;)) &amp; ym((m))
end
x1 &amp; xn --&gt; h1 &amp; h2 &amp; hL --&gt; y1 &amp; ym
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Output of additive hidden neurons:
G(ğšáµ¢, báµ¢, ğ±) = g(ğšáµ¢â‹…ğ±+báµ¢)&lt;/li&gt;
&lt;li&gt;Output of RBF hidden nodes:
G(ğšáµ¢, báµ¢, ğ±) = gâ€–ğ±-ğšáµ¢â€–&lt;/li&gt;
&lt;li&gt;The output function of SLFNs is:
fâ‚—(ğ±) = âˆ‘â‚—â‚Œâ‚á´¸ ğ›ƒáµ¢â‹…G(ğšáµ¢, báµ¢, ğ±)&lt;/li&gt;
&lt;/ul&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_SLFN.jpg width=&gt;
  
  


&lt;h3 id=&#34;network-training&#34;&gt;Network training&lt;/h3&gt;
&lt;p&gt;Advantage: Approximate unknown system through learnable parameters.&lt;/p&gt;
&lt;p&gt;Mathematical Model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Approximation capability: Any continuous target function f(x) can be approximated by Single-layer feedforward network with appropriate parameters (ğ›‚,â™­,ğ›ƒ).
In other words, given any small positive value e, for SLFN with enough number of hidden nodes, we have: &lt;br&gt;
â€–f(ğ±)-fâ‚—(ğ±)â€– &amp;lt; e&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In real applications, target function f(ğ±) is usually unknown. One wishes that unknown f could be approximated by the trained network fâ‚—(ğ±).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-is-extreme-learning-machine&#34;&gt;What is Extreme Learning Machine?&lt;/h3&gt;
&lt;p&gt;Feed forward random network without using BP to train, such that it has a good real-time performance. And it fits the real-time robot task exactly.&lt;/p&gt;
&lt;h3 id=&#34;b-elm&#34;&gt;B-ELM&lt;/h3&gt;
&lt;p&gt;(2011) &amp;ldquo;Bidirectional ELM for regression problem and its learning effectiveness&amp;rdquo;, IEEE Trans. NNLS., 2012
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/6222007&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(23-02-10) This the inception of his subnetwork series work, and I was supposed to read this brief firt.
&lt;a class=&#34;link&#34; href=&#34;../ReadLit/B-note-B-ELM.md&#34; &gt;paperNote&lt;/a&gt;&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_B-ELM.jpeg width=&gt;
  
  


&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Original ELM has 3 kinds of parameters: ğš is called the &amp;ldquo;input weights&amp;rdquo;, b is the bias, ğ›ƒ is the &amp;ldquo;output weights&amp;rdquo;,
which are consistent with earlier feedfoward network, though current single-layer feedfoward network has removed the ğ›ƒ.&lt;/p&gt;
&lt;p&gt;The 1st layer in ELM is generated randomly and the 2nd layer is constructed based on Moore-Penrose inverse without iteration.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
in((X)) --&gt; lyr1[&#34;Layer1:\n ğšâ¿, b\n Random\n Generated&#34;]
--&gt; weightSum1((&#34;Xâ‹…ğšâ¿ + b&#34;)) --&gt; act[Activation\n Function\n g]
--&gt; z((&#34;g(Xâ‹…ğšâ¿+b)&#34;)) --&gt; lyr2[&#34;Layer2:\n ğ›ƒ&#34;] 
--&gt; weightSum2((&#34;O =\n ğ›ƒâ‹…g(Xâ‹…ğšâ¿+b)\n =T&#34;))
-.-&gt;|&#34;ğ›ƒ =\n g(Xâ‹…ğšâ¿+b)â»Â¹T&#34;| lyr2
classDef lyrs fill:#ff9
class lyr1,act,lyr2 lyrs;
&lt;/div&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bidirectional ELM&lt;/p&gt;
&lt;p&gt;In original ELM, the ğšâ¿,b are random numbers, but they can be yield if pulling the error back further, that is doing twice more inverse computation.
Therefore, in order to calculate the ğšâ¿,b, there are 3 times inverse computation: for output weights ğ›ƒ, activation function g(â‹…) and activation z (Xâ‹…ğšâ¿+b) respectively.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;bump&#39; } } }%%
flowchart LR
in((X)) --&gt; lyr1[&#34;Layer1:\n ğšâ¿, b\n Random\n Generated&#34;]
--&gt; weightSum1((&#34;Xâ‹…ğšâ¿ + b&#34;)) --&gt; act[Activation\n Function\n g]
--&gt; z((&#34;g(Xâ‹…ğšâ¿+b)&#34;)) --&gt; lyr2[&#34;Layer2:\n ğ›ƒ&#34;] --&gt; out((&#34;O =\n ğ›ƒâ‹…g(Xâ‹…ğšâ¿+b)\n =T&#34;))
-.-&gt;|&#34;ğ›ƒ =\n g(Xâ‹…ğšâ¿+b)â»Â¹T&#34;| lyr2
classDef lyrs fill:#ff9
class lyr1,act,lyr2 lyrs;

out -.-&gt; |&#34;Xâ‹…ğšâ¿+b =\n gâ»Â¹ T ğ›ƒâ»Â¹&#34;|weightSum1
out -.-&gt; |&#34;ğšâ¿ =\n Xâ»Â¹ (gâ»Â¹ T ğ›ƒâ»Â¹ -b ),\n b = mse(O-T)&#34;|lyr1 
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;ğ›ƒ = [g(Xâ‹…ğšâ¿+b)]â»Â¹T&lt;/li&gt;
&lt;li&gt;Xâ‹…ğšâ¿+b = gâ»Â¹ T ğ›ƒâ»Â¹&lt;/li&gt;
&lt;li&gt;ğšâ¿ = Xâ»Â¹ (gâ»Â¹ T ğ›ƒâ»Â¹ -b ), and b = mse(O-T)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The error is surprisingly small even with few hidden nodes.
Compared with the original ELM, the required neurons in this method are reduced by 100-400 times, and the testing error reduced 1%-3%, and also the training time reduced 26-250 times over 10 datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Major differences&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Randomized Networks&lt;/th&gt;
&lt;th&gt;Bidirectional ELM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Classifier&lt;/td&gt;
&lt;td&gt;ELM; Echo State Netowrk;&lt;br&gt; Random Forest;&lt;br&gt; Vector Functional-link Network&lt;/td&gt;
&lt;td&gt;Only works for regression task&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performance&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Similar performance;&lt;br&gt; Faster speed;&lt;br&gt; Less required neurons&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Learning strategy&lt;/td&gt;
&lt;td&gt;(Semi-)Randomized input weights;&lt;br&gt; Non-iterative training;&lt;br&gt; Single-layer network&lt;/td&gt;
&lt;td&gt;Non-iterative training;&lt;br&gt; Single-layer network;&lt;br&gt; Calculated weights in a network&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;-hierarchical-nn-with-subnetwork-neurons&#34;&gt;â…¡. Hierarchical NN with Subnetwork Neurons&lt;/h2&gt;
&lt;h3 id=&#34;single-layer-network-with-subnetwork-neurons&#34;&gt;Single-Layer Network with Subnetwork Neurons&lt;/h3&gt;
&lt;p&gt;In 2014, deep learning is becoming popular. How to extend the B-ELM as a multi-layer network? &lt;br&gt;
&amp;ldquo;Extreme Learning Machine With Subnetwork Hidden Nodes for Regression and Classification&amp;rdquo;.
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8627989&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;; &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/&#34; &gt;paperNote&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart TB
base[Bidirectional ELM] --&gt; theory[Theoretical Contributions on ANN] &amp; app[Industrial Applications]
theory --&gt; multilyr[1. Two-layer Neural Networks?\n 2. Hierarchical Neural Networks?]
app --&gt; tasks[1. Feature Extraction\n 2. Dimension Reduction\n 3. Image Recognition]
&lt;/div&gt;

&lt;p&gt;Pull the residual error back to multiple B-ELMs sequentially:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
in((X)) --&gt; lyr11 &amp; lyr21 &amp; lyrN1

subgraph net1[B-ELM 1]
lyr11[&#34;ğšÂ¹,bÂ¹&#34;] ==&gt;|&#34;Xâ‹…ğšÂ¹+bÂ¹&#34;| act1[g] ==&gt;|&#34;g(Xâ‹…ğšÂ¹+bÂ¹)&#34;|lyr12[&#34;ğ›ƒÂ¹&#34;] ==&gt;|&#34;ğ›ƒÂ¹â‹…g(Xâ‹…ğšÂ¹+bÂ¹)&#34;| out((&#34;O â” T\n E=T&#34;))
-.-&gt; lyr12 -.-&gt; act1 -.-&gt; lyr11
lyr11 --&gt; act1 --&gt; lyr12 
end
lyr12 --&gt; out1((&#34;OÂ¹â”T\n E=T-OÂ¹&#34;)) -.-&gt; lyr22

subgraph net2[B-ELM 2]
lyr22[&#34;ğ›ƒÂ²&#34;] -.-&gt; act2[g] -.-&gt; lyr21[&#34;ğšÂ²,bÂ²&#34;]
lyr21 --&gt;|&#34;Xâ‹…ğšÂ² + bÂ²&#34;|act2 --&gt;|&#34;g(Xâ‹…ğšÂ²+bÂ²)&#34;|lyr22 
end
lyr22--&gt; out2((&#34;OÂ²+OÂ¹â”T\n E=T-(OÂ²+OÂ¹)&#34;)) 

out2 -.-&gt;|&#34;Solve\n multiple\n B-ELMs&#34;| outn-1((&#34;E=T-âˆ‘áµ¢â‚Œâ‚á´ºâ»Â¹ Oâ±&#34;))-.-&gt;lyrN2
%%out2 -.-&gt; lyrn2
%%subgraph nets[multiple B-ELMs]
%%lyrn2[&#34;ğ›ƒâ¿&#34;] -.-&gt; actn[g] -.-&gt; lyrn1[&#34;ğšâ¿,bâ¿&#34;]
%%lyrn1 --&gt;|&#34;Xâ‹…ğšâ¿ + bâ¿&#34;|actn --&gt;|&#34;g(Xâ‹…ğšâ¿+bâ¿)&#34;|lyrn2
%%end
%%lyrn2 --&gt; outn-1((&#34;E=T-âˆ‘áµ¢â‚Œâ‚á´ºâ»Â¹ Oâ±&#34;))-.-&gt;lyrN2

subgraph netN[&#34;B-ELM N&#34;]
lyrN2[&#34;ğ›ƒá´º&#34;] -.-&gt; actN[g] -.-&gt; lyrN1[&#34;ğšá´º,bá´º&#34;]
lyrN1 --&gt;|&#34;Xâ‹…ğšá´º + bá´º&#34;|actN --&gt;|&#34;g(Xâ‹…ğšá´º+bá´º)&#34;|lyrN2 
end
lyrN2--&gt; outN((&#34;âˆ‘áµ¢â‚Œâ‚á´º Oâ±â”T&#34;))

classDef lyrs fill:#ff9
class lyr11,act1,lyr12,lyr21,act2,lyr22,lyrN1,actN,lyrN2 lyrs;
linkStyle 9,10,11,15,16,17,22,23,24 stroke:#0af,stroke-width:3px
%%classDef node font-size:20px;
&lt;/div&gt;

&lt;p&gt;Dotted links are computation with inverse.
&lt;font color=&#34;#0bf&#34;&gt;Cyan links&lt;/font&gt; is the second feedforward using the updated parameters to give a trustworthy result OÂ¹.
The objective is to approach the target T, so there is a residual error E=T-OÂ¹.
Then another B-ELM (with same structure) is used to reduce the error continuously.
And this time, the prediction is OÂ²+OÂ¹, which is the approximation of T.
Here, the residual error is E=T-(OÂ²+OÂ¹)&lt;/p&gt;
&lt;p&gt;Repeatedly pulling the residual error to a new B-ELM N times is equivalent to N SLFNs. But B-ELM is fast without iteration and less computation with a few hidden nodes in each SLFN.&lt;/p&gt;
&lt;p&gt;Based on original SLFN structure, each node contains a SLFN.&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Subnetwork_Nodes.jpeg width=&gt;
  
  


&lt;h3 id=&#34;two-layer-network-with-subnetwork-neurons&#34;&gt;Two-Layer Network with Subnetwork Neurons&lt;/h3&gt;
&lt;p&gt;(2015) How to extend the Single-layer network with subnetwork nodes system to a two-layer network system?&lt;/p&gt;
&lt;p&gt;A general two-layer system was built in paper: &amp;ldquo;Multilayer Extreme Learning Machine with Subnetwork Hidden Neurons for Representation Learning&amp;rdquo;
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/7295596&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;; &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/&#34; &gt;paperNote&lt;/a&gt;&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Two-layers_ELM.jpeg width=&gt;
  
  


&lt;p&gt;Though it only contains two &amp;ldquo;general&amp;rdquo; layers, this system includes hundreds of networks, and it&amp;rsquo;s fast due to the modest quantity and no iteration.&lt;/p&gt;
&lt;p&gt;Compared with ELM and B-ELM, it got better performance over 35 datasets:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Classification (vs ELM)&lt;/th&gt;
&lt;th&gt;Regression (vs B-ELM)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Required Neurons&lt;/td&gt;
&lt;td&gt;Reduced 2-20 times&lt;/td&gt;
&lt;td&gt;Reduced 13-20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Metrics&lt;/td&gt;
&lt;td&gt;Accuracy increase 1-17%&lt;/td&gt;
&lt;td&gt;Testing error reduced 1-8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Speed&lt;/td&gt;
&lt;td&gt;faster 25-200times&lt;/td&gt;
&lt;td&gt;faster 5-20%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Two-layer network system can perform image compression or reconstruciton, etc.
This method is better than Deep Belief Network on small datasets.
But it&amp;rsquo;s inferior than deep learning with transfer learning technics on huge datasets.&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-network-with-subnetwork-neurons&#34;&gt;Hierarchical Network with Subnetwork Neurons&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Features combined from hundreds of mid-layers: Hierarchical networks with subnetwork nodes&amp;rdquo; IEEE Trans. NNLS, 2019.
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8627989&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Hierarchical_Network.jpeg width=&gt;
  
  


&lt;p&gt;From a single-layer network with subnetwork neurons to the multi-layer network, and then to a neural network system, these 3 papers cost 5 years or so.&lt;/p&gt;
&lt;p&gt;Compared with deep learning network, it&amp;rsquo;s extremely fast and performs well on small datasets, like Scene15, Caltech101, Caltech256.
But for large datasets, deep learning is the winner.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Somewhat regretfully, I turned to deep learning a bit late. But been hesitant to do research along this approach.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Major differences between ours and Deep Networks&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;SGD based methods in DCNN&lt;/th&gt;
&lt;th&gt;Moore-Penrose inverse matrix based methods&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Hyper params&lt;/td&gt;
&lt;td&gt;lr; momentum; bs; L2 regulariation; epochs&lt;/td&gt;
&lt;td&gt;L2 regularization (non-sensitive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performance&lt;/td&gt;
&lt;td&gt;higher performance in Computer Vision tasks (with huge datasets);&lt;br&gt; GPU-based computation resource;&lt;br&gt; More parameters;&lt;br&gt; More required training time&lt;/td&gt;
&lt;td&gt;Faster learning speed/less tuning;&lt;br&gt; Promising performance in Tabular Datasets;&lt;br&gt; Less over-fitting problem.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;-deep-networks-without-iterative-learning&#34;&gt;â…¢. Deep Networks without iterative learning&lt;/h2&gt;
&lt;p&gt;Since 2018: How to combine the non-iterative method (M-P inverse matrix) with deep convolutional network to gain advantages? This took 2-3 years.&lt;/p&gt;
&lt;p&gt;This is the age of Deep Learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Interesting 20 years of cycles&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rosenblatt&amp;rsquo;s Perceptron proposed in mid of 1950s, sent to &amp;ldquo;winter&amp;rdquo; in 1970s.&lt;/li&gt;
&lt;li&gt;Back-Propagation and Hopfield Network Proposed in 1970s, reaching research peak in mid of 1990s.&lt;/li&gt;
&lt;li&gt;Support vector machines proposed in 1995, reaching research peak early this century.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are exceptional cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most basic deep learning algorithms proposed in 1960s-1980s, becoming popular only since 2012 (for example, LeNet proposed in 1989).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ImageNet pushed deep learning, because only when the huge network structure of deep learning meets the matched huge dataset, it can achive good performance.&lt;/p&gt;
&lt;p&gt;The success of deep learning enlist three factors: 1. NN structure and algorithm; 2. Big data; 3. GPU availability.&lt;/p&gt;
&lt;p&gt;Hundreds of layers result in tedious training time. &amp;ldquo;The study intensity is infinitely small and the study duration is infinitely large.&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The improvement space of deep neural network is limited. So can we introduce non-iterative learning strategies for training deep networks&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Training speed is more important for scientific research than accuracy.
And also it&amp;rsquo;s necessary to reduce the dependence on GPUs and the involvement of undeterministic hyper parameters (lr,bs,&amp;hellip;)&lt;/p&gt;
&lt;h3 id=&#34;retraining-dcnn-with-the-non-iterative-strategy&#34;&gt;Retraining DCNN with the non-iterative strategy&lt;/h3&gt;
&lt;p&gt;(2019): &amp;ldquo;Recomputation of the dense layers for the performance improvement of DCNN.&amp;rdquo; IEEE TPAMI, 2020.
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8718406&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;graph LR
base[&#34;1. Two-layer Neural Networks\n 2. Hierarchical Neural Networks&#34;]
--&gt; explore[&#34;1. Deep learning withe non-iterative method&#34;]
&lt;/div&gt;






  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_NonIterative&amp;#43;CNN.jpeg width=&gt;
  
  


&lt;p&gt;In a DCNN, the first few layers are convolutional layers, maxpooling, then there&amp;rsquo;re 3 or 1 dense layer.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If I cannot train all of the hundreds layers in my non-iterative method, can I train only certain layers that are easy trained with my method, rather the SGD?&lt;/p&gt;
&lt;/blockquote&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Retrain_FC_CNN.jpeg width=&gt;
  
  


&lt;p&gt;Only the fully-connected layers are trained by non-iterative method (inverse matrix), and the rest of layers are trained by gradient descent (SGD, SGDM, Adam).&lt;/p&gt;
&lt;p&gt;On some medium-size datasets(CIFAR10, CIFAR100, SUN397), this approach brought a moderate improvement because there are only 3 dense layer out of a 50/100-layer network (most of layers are trained with SGD), but speeds up the training.&lt;/p&gt;
&lt;p&gt;One layer can be trained within 1-2 seconds.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>read: ELM with Subnetwork Nodes</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/</link>
        <pubDate>Fri, 20 Jan 2023 11:46:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/7314912&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IEEE Cybernetics(2015-11-02)&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/d/13FD2IifYUndeTXeV6NrlS_ka7-2JFUTY/view&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Google Drive&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=fFP4b9kAAAAJ&amp;amp;citation_for_view=fFP4b9kAAAAJ:rHJHxKgnXwkC&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;G.Scholar&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the first paper of ELM with subnetwork nodes by &lt;a class=&#34;link&#34; href=&#34;http://www.yiminyang.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Yimin Yang&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;The second paper in the series is &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/&#34; &gt;MltLyr ELM with subnetwork nodes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The outline of Yang&amp;rsquo;s research works: &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/&#34; &gt;Yang-WeChatLive-20221020&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learning effectiveness and speed of SLFN are bottleneck.&lt;/li&gt;
&lt;li&gt;ELM is fast.&lt;/li&gt;
&lt;li&gt;Grow subnetwork nodes by pulling back residual network error to the hidden layer.&lt;/li&gt;
&lt;li&gt;Better generalization performance with fewr hidden nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-introduction&#34;&gt;â… . Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bring out the subject: &lt;br&gt;
FNN (universial approximator) â” SLFN&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is an SLFN? &lt;br&gt;
Input layer + hidden  layer + output layer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Math description: &lt;br&gt;
For N arbitary distinct samples {(ğ±áµ¢,ğ­áµ¢)}áµ¢â‚Œâ‚á´º, where ğ±áµ¢âˆˆ ğ‘â¿ and ğ­áµ¢âˆˆ ğ‘áµ, the network output is:&lt;/p&gt;
&lt;p&gt;$ğŸ_L(ğ±)$ = âˆ‘áµ¢â‚Œâ‚á´¸ ğ›ƒáµ¢h(ğšáµ¢â‹…ğ±â±¼ + báµ¢) = âˆ‘áµ¢â‚Œâ‚á´¸ ğ‡áµ¢â‹…ğ›ƒáµ¢, j=1,&amp;hellip;,N   â€ƒ    (1)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SLFN output is the weighted sum of ğ¿ hidden nodes (perceptrons) with the factor ğ›ƒ.&lt;/li&gt;
&lt;li&gt;The ith perceptron receives the weighted sum of ğ‘ inputs through its parameters (ğšáµ¢, báµ¢), ğšáµ¢âˆˆ ğ‘â¿, bâˆˆ ğ‘, and performs activation function h.
Its contribution ratio to the all output nodes is ğ›ƒáµ¢.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELM traits: &lt;br&gt;
NN (all params are adjustable) â” partial random networks â” ELM is a full-random learning method,
where the input weights and bias (ğš, b) are generated randomly and independent of training data.
(Will the Glorot normalization has no effect?)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELM advantages: &lt;br&gt;
An unification of FNNs and SVM/LS-SVM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELM application: &lt;br&gt;
CV, da,&amp;hellip;, online learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The choice of the regularization parameter C which affects the generalization performance of ELM mainly relies on trial-and-error method.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How many neurons should be used in ELM. Although Huang suggested to use more than 1000 hidden nodes, whether the number of hidden nodes can be further reduced without affecting learning effectiveness for large-size/high dimension data set. &lt;br&gt;
Several improved ELM methods, like &lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/6222007&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;B-ELM&lt;/a&gt; pulls the network residual error back to the hidden layer but it only works for regression task,
and other methods bring a higher computation complexity when compared to standard ELM.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Growing subnetwork hidden nodes to the exisiting network by pulling back the network residual error to hidden layers. A hidden node itself can be formed by several hidden nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Faster than BP, SVM and other ELMs and compatible to regreesion and classification problems.&lt;/li&gt;
&lt;li&gt;The regularized parameter C do not affect the generalization performance of this method.&lt;/li&gt;
&lt;li&gt;This method with m hidden nodes (the desired output dimensionality) can achieve better training accuracy than the original ELM with a large number of hidden nodes.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-definitions-and-basic-elm&#34;&gt;â…¡. Definitions and Basic-ELM&lt;/h2&gt;
&lt;h3 id=&#34;a-notations-and-definitions&#34;&gt;A. Notations and Definitions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ğ‘ : set of real numbers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;{(ğ±áµ¢,ğ­áµ¢)}áµ¢â‚Œâ‚á´º : N arbitrary distinct samples,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ±áµ¢ = [xáµ¢â‚,xáµ¢â‚‚,&amp;hellip;,xáµ¢â‚™]áµ€ : n-dim input data, ğ±áµ¢âˆˆ ğ‘â¿ is a column vector;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ­áµ¢ : m-dim desired output data, ğ­áµ¢âˆˆ ğ‘áµ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ± : input data matrix, ğ±=[ğ±â‚,&amp;hellip;ğ±_N], ğ±áµ¢âˆˆ ğ‘â¿á•á´º&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ­ : desired output data matrix, ğ­=[ğ­â‚,&amp;hellip;ğ­_N], ğ­âˆˆ ğ‘áµá•á´º&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(^ğšâ‚™,^bâ‚™) : parameters of the ğ‘›th subnetwork hidden node, ^ğšâ‚™âˆˆ ğ‘â¿á•áµ, ^bâ‚™âˆˆ ğ‘ (suppose the number of hidden nodes equals to the output dimension m, thus the mapping is from n to m.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;^ğšâ‚™ = [ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜], nÃ—m weights matrix (for a n-dimension sample ğ±áµ¢), ğšâ‚™â‚˜âˆˆ ğ‘â¿&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğâ‚™ : residual error of current network output ğ‘“â‚™ with  ğ‘› hidden nodes (for N samples),
i.e., ğâ‚™=ğ­-ğ‘“â‚™, ğâ‚™âˆˆ ğ‘áµá•á´º.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ‡ : output matrix of the hidden layer (of SLFN) for tarining set {(ğ±áµ¢,ğ­áµ¢)}áµ¢â‚Œâ‚á´º,
ğ‡ = [h(ğ±â‚),&amp;hellip;,h(ğ±_N)]áµ€, ğ‡âˆˆ ğ‘á´ºá•áµ, ğ‡ = g(ğ±áµ€ğš+ğ›)???&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;h(ğ±) : activation function. &lt;del&gt;ELM feature mapping (or Huang&amp;rsquo;s transform)&lt;/del&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğ‡áµ¢ : the ğ‘–th hidden node output w.r.t. inputs, i.e., the ğ‘–th column of ğ‡&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ğˆ : unit matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sum(ğ) : the sum of all elements of the matrix ğ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;b-basic-elm&#34;&gt;B. Basic-ELM&lt;/h3&gt;
&lt;p&gt;ELM is proposed for single-hidden-layer feedforward networks (SLFNs).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The output function of ELM with L hidden nodes for SLFNs is:&lt;/p&gt;
&lt;p&gt;$ğ‘“_L(ğ±)$ = âˆ‘áµ¢â‚Œâ‚á´¸ Î²áµ¢â‹…h(ğšáµ¢â‹…ğ±â±¼ + báµ¢) = âˆ‘áµ¢â‚Œâ‚á´¸ ğ‡áµ¢â‹…ğ›ƒáµ¢, j=1,&amp;hellip;,N.&lt;/p&gt;
&lt;p&gt;where h(â‹…) denotes an activation function, (ğšáµ¢, báµ¢), ğšáµ¢âˆˆ ğ‘â¿, báµ¢âˆˆ ğ‘, denotes the ith hidden node parameters, and ğ›ƒáµ¢ is the ith output weight between the ith hidden node and the output nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on Bartlett&amp;rsquo;s theory, ELM theory aims to reach not only the smallest training error, but also the smallest norm of output weights
(Least square-least norm solution, where the regularization &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/calc/pseudo-inverse/&#34; &gt;makes an invertible matrix&lt;/a&gt;, such that a special solution can be determined.):&lt;/p&gt;
&lt;p&gt;Minimize: â€–ğ›ƒâ€–Â² + Câ‹…â€–ğ‡ğ›ƒ - ğ­â€–Â²&lt;/p&gt;
&lt;p&gt;&amp;ldquo;then the generalization performance depends on the size of weights rather than the number of nodes.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt; (proved by Huang):&lt;br&gt;
Given an SLFN with nonconstant piecewise continuous hidden nodes ğ‡(ğ±, ğš, b),
then for any continuous target function ğ‘“ and any function sequence ğ‡â‚™Ê³(ğ±) = ğ‡(ğ±, ğšâ‚™, bâ‚™) randomly generated based on any continuous sampling distribution,&lt;/p&gt;
&lt;p&gt;lim$_{nââˆ}$ â€–ğ‘“ - (ğ‘“â‚™â‚‹â‚ + ğ‡â‚™Ê³â‹…ğ›ƒâ‚™)â€– = 0&lt;/p&gt;
&lt;p&gt;holds with probabitliy 1 if: &lt;br&gt;
ğ›ƒâ‚™ = âŸ¨ğâ‚™â‚‹â‚, ğ‡â‚™Ê³âŸ© / â€–ğ‡â‚™Ê³â€–Â² (the weight of the ğ‘›th node)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;âŸ¨ , âŸ©&amp;rdquo; stands for &amp;ldquo;dot product&amp;rdquo; (&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Frobenius_inner_product&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Frobenius inner product&lt;/a&gt;) of two matrices and is a scalar.&lt;/li&gt;
&lt;li&gt;n is the number of hidden nodes in the hidden layer.&lt;/li&gt;
&lt;li&gt;ğâ‚™â‚‹â‚ is the residual error of the last iteration, i.e., when there were n-1 hidden nodes.&lt;/li&gt;
&lt;li&gt;ğ‡â‚™Ê³ is the output matrix of the current hidden layer (activated but havn&amp;rsquo;t scaled by ğ›ƒ).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intuitively, as the residual error reduces, the weight of the newer node gets smaller.&lt;/p&gt;
&lt;h2 id=&#34;-proposed-elm-method-with-subnetwork-hidden-nodes&#34;&gt;â…¢. Proposed ELM Method With Subnetwork Hidden Nodes&lt;/h2&gt;
&lt;h3 id=&#34;a-structure-of-the-proposed-method&#34;&gt;A. Structure of the Proposed Method&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Motivations&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Selecting an appropriate number of neurons can resort to optimization algorithms.&lt;/li&gt;
&lt;li&gt;The generalization performance depends on the size of the weights rather than the number of weights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Inspiration&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;A hidden node itself can be a subnetwork formed by several nodes.
And these &lt;strong&gt;subnetwork hidden nodes&lt;/strong&gt; and output weights itself should be the smallest norm, and also aim to reach the smallest training error.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objectives&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Given N training samples {(ğ±áµ¢,ğ­áµ¢)}áµ¢â‚Œâ‚á´º, ğ±áµ¢âˆˆ ğ‘â¿, ğ­áµ¢âˆˆ ğ‘áµ, generated from the same continuous system,
if activation function h is invertible, the objectives are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;â€–ğâ‚™â‚‹â‚â€– â‰¥ â€–ğâ‚™â‚‹â‚ - h(^ğšâ‚™,ğ±)â€– â‰¥ â€–ğâ‚™â‚‹â‚ - ğ‡â‚™â€– â‰¥ â€–ğâ‚™â‚‹â‚ - ğ‡â‚™â‹…ğ›ƒâ‚™â€– (residual error is decreasing.)&lt;/li&gt;
&lt;li&gt;â€–h(^ğšâ‚™,ğ±) - ğâ‚™â‚‹â‚â€– = min_{ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜} â€–h(ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜) - ğâ‚™â‚‹â‚â€–  (minimize weights inside nodes)&lt;/li&gt;
&lt;li&gt;â€–ğ‡â‚™â‹…^ğ›ƒâ‚™ - ğâ‚™â‚‹â‚â€– = min_{ğ›ƒ} â€–ğ‡â‚™â‹…ğ›ƒ - ğâ‚™â‚‹â‚â€–   (minimize the weights outside nodes)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^ğšâ‚™ and ^ğ›ƒâ‚™ are the optimal (the ultimate status) parameters with the smallest norm among all the least squares solutions.&lt;/li&gt;
&lt;li&gt;ğ‡â‚™ = h(^ğšâ‚™, ^bâ‚™, ğ±) is the output of the nth hidden node with the optimal parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If activation function h is invertible, subnetwork hidden nodes in SLFN can be calculated by pulling back network residual error to hidden layers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For example, with sine function as the activation function, training a subnetwork hidden node (^ğš) is equivalent to finding a least-square solution ^ğšâ‚™ (letting the derivative of MSE=0) with the least norm for the linear system:&lt;/p&gt;
&lt;p&gt;[ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜]â‹…ğ± = arcsin(ğâ‚™â‚‹â‚), ğâ‚™â‚‹â‚âˆˆ (0,1],&lt;/p&gt;
&lt;p&gt;such that the optimal ^ğšâ‚™ satifies:&lt;/p&gt;
&lt;p&gt;â€–sin(^ğšâ‚™, ğ±) - ğâ‚™â‚‹â‚â€– = min_{ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜} â€–sin(ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜, ğ±) - ğâ‚™â‚‹â‚â€–,&lt;/p&gt;
&lt;p&gt;That means the output of the nth &amp;ldquo;subnetwork hidden node&amp;rdquo; ^ğšâ‚™ is approaching the residual error ğâ‚™â‚‹â‚ of the last status.&lt;/p&gt;
&lt;p&gt;The input weights ^ğšâ‚™ of a node for this model is a matrix (instead of a vector), beacuse each &amp;ldquo;subnetwork (general) hidden node&amp;rdquo; &lt;strong&gt;contains a standard SLFN&lt;/strong&gt; (several hidden nodes) internally.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Differences with standard ELM&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;ELM with subnetwork&lt;/th&gt;
&lt;th&gt;standard ELM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hidden node&lt;/td&gt;
&lt;td&gt;m neurons: $ğš_fâˆˆ ğ‘â¿á•áµ, ğ›_fâˆˆ ğ‘áµ$&lt;/td&gt;
&lt;td&gt;single neuron:&lt;br&gt; ğšâˆˆ ğ‘â¿, bâˆˆ ğ‘&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;construct&lt;/td&gt;
&lt;td&gt;calculated&lt;/td&gt;
&lt;td&gt;generated randomly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;# hidden nodes&lt;/td&gt;
&lt;td&gt;L x m (m âŸ‚ L, m = #output dim)&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;b-proposed-method&#34;&gt;B. Proposed Method&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;: &lt;br&gt;
Given a bounded nonconstant piecewise continuous activation function h, there is: &lt;br&gt;
lim$_{(ğš,b)â†’(ğšâ‚€,bâ‚€)}$ â€–h(ğšâ‹…ğ±+b) - h(ğšâ‚€â‹…ğ±+bâ‚€)â€– = 0 (è¿ç»­æ€§)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: &lt;br&gt;
Given N arbitrary distinct samples {(ğ±áµ¢,ğ­áµ¢)}áµ¢â‚Œâ‚á´º, ğ±áµ¢âˆˆ ğ‘â¿, ğ­áµ¢âˆˆ ğ‘áµ, a sigmoid or sine activation function h, and then for any continuous desired outputs ğ­, the limit of error converges to 0:&lt;/p&gt;
&lt;p&gt;lim$_{nââˆ}$ â€– ğ­-{ uâ»Â¹(h(^ğšâ‹…ğ±+b))} â€– = 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Prove the sequence â€–ğâ‚™â€– is decreasing with 0 as the lower bound and it converges.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For the ğ‘›th subnetwork hidden node containing m hidden nodes, the linear mapping is:&lt;/p&gt;
&lt;p&gt;ğ›Œâ‚™ = [ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜]â‹…ğ±, ğ›Œâ‚™âˆˆ ğ‘áµ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then ğ›Œâ‚™ passes through the activation function. Because the target is error, which should become 0 at the end, the error at present is the output of activation function:&lt;/p&gt;
&lt;p&gt;ğâ‚™â‚‹â‚ = h(ğ›Œâ‚™) âˆˆ ğ‘áµ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The inverse function of h is hâ»Â¹, and its input value should range from (0,1].
Therefore, if trying to solve ğ›Œâ‚™ from ğâ‚™â‚‹â‚, every element in ğâ‚™â‚‹â‚ should be scaled to the range of (0,1] by the normalized function u(â‹…). Then, ğ›Œâ‚™ can be calculated through:&lt;/p&gt;
&lt;p&gt;ğ›Œâ‚™ = hâ»Â¹(u(ğâ‚™â‚‹â‚))&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Further, the input weights of this subnetwork hidden node can be solved:&lt;/p&gt;
&lt;p&gt;$$\^ğšâ‚™ = [ğšâ‚™â‚,&amp;hellip;,ğšâ‚™â‚˜] = hâ»Â¹(u(ğâ‚™â‚‹â‚))â‹…ğ±â»Â¹$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For different activation functions, there will be:&lt;/p&gt;
&lt;p&gt;$$\rm \{^{\^ğšâ‚™ = arcsin(u(ğâ‚™â‚‹â‚))â‹…ğ±â»Â¹,\quad sine}
_{\^ğšâ‚™ = -log( (1/u(ğâ‚™â‚‹â‚)) - 1)â‹…ğ±â»Â¹,\quad sigmoid}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(This work is a continuation on ELM, that is once the ğ›ƒ calculated based on target ğ­ and ğ‡â»Â¹, the residual error is also fixed, so it serves as the target for ğš,b.
Still, applying least-square, the optimial a can be calculated based on &amp;ldquo;target&amp;rdquo; e and ğ±â»Â¹.)&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;
&lt;p&gt;^b is the mean of hidden nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The error can be reduced by adding the bias&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do feedforward using the calculated ^ğšâ‚™ and ^bâ‚™, so this time the output of the hidden layer is:&lt;/p&gt;
&lt;p&gt;$$\^ğ‡â‚™áµ‰ = uâ»Â¹(h(\^ğšâ‚™â‹…ğ±+\^bâ‚™))$$&lt;/p&gt;
&lt;p&gt;Because ğâ‚™â‚‹â‚ is the last output of the activation fuction, ğâ‚™â‚‹â‚ and $\^ğ‡â‚™áµ‰$ are the same things.
So they can subtract from each other. Then the residual error for this time is:&lt;/p&gt;
&lt;p&gt;Î” = â€–ğâ‚™â‚‹â‚â€–Â² - â€–ğâ‚™â‚‹â‚ - ^ğ‡â‚™áµ‰â€–Â² &lt;br&gt;
= â€–ğâ‚™â‚‹â‚â€–Â² - (â€–ğâ‚™â‚‹â‚â€–Â² - 2â€–ğâ‚™â‚‹â‚â€–â€–^ğ‡â‚™áµ‰â€– + â€–^ğ‡â‚™áµ‰â€–Â²) &lt;br&gt;
= 2â€–ğâ‚™â‚‹â‚â€–â€–^ğ‡â‚™áµ‰â€– - â€–^ğ‡â‚™áµ‰â€–Â² &lt;br&gt;
= 2âŸ¨ğâ‚™â‚‹â‚, ^ğ‡â‚™áµ‰âŸ© - â€–^ğ‡â‚™áµ‰â€–Â² &lt;br&gt;
= â€–^ğ‡â‚™áµ‰â€–Â² ( 2âŸ¨ğâ‚™â‚‹â‚, ^ğ‡â‚™áµ‰âŸ©/â€–^ğ‡â‚™áµ‰â€–Â² - 1 )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Î” is â‰¥ 0&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prove the limit converges to 0 when n tends to infinity.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The target value is approximated while the error is decreased.&lt;/p&gt;
&lt;p&gt;The final estimation is the summation of the ouput of d subnetwork hidden nodes&lt;/p&gt;
&lt;p&gt;The VC dimension is lower than standard ELM, i.e., the dimension of feature space mâ‰ª L, so the generalization ability of this method is better.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;#abstract&#34; &gt;(Back to top)&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>read: Multilayer Subnetwork Nodes</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/</link>
        <pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/</guid>
        <description>&lt;p&gt;Authors: &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=fFP4b9kAAAAJ&amp;amp;citation_for_view=fFP4b9kAAAAJ:-mN3Mh-tlDkC&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Yimin Yang&lt;/a&gt;, and Q. M. Jonathan Wu &lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/7295596&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IEEE Cybernetics&lt;/a&gt;; Publish Date: 2015-10-09.&lt;/p&gt;
&lt;p&gt;This is the 2nd paper in his series, and the first paper is &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/&#34; &gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(æ„Ÿè§‰Introå†™å¾—ä¸é”™ï¼Œé€»è¾‘æ€§å¼ºï¼Œä¿¡æ¯é‡å¤§ï¼›ä½†åé¢methodéƒ¨åˆ†å¥½å¤štypo)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Representation learning of multilayer ELM with subnetwork nodes outperform conventional feature learning methods.&lt;/p&gt;
&lt;h2 id=&#34;i-introduction&#34;&gt;I. Introduction&lt;/h2&gt;
&lt;p&gt;model performance â” data representaiton/features â” processing pipelines design and data transformations â” data representation â” effective learning&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Feature reduction and extraction techniques can be conducted in a supervised, unsupervised or semi-supervised manner.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELMs learn representations of data to extract useful information when building classifiers or predictors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELMs provide a unified learning framework for &amp;ldquo;generalized&amp;rdquo; single-hidden layer feedforward NNs (SLFNs).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In ELM methods, the hidden layer parameters of NN need not be tuned during training, but generated randomly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ML-ELM is adding (multiple) general hidden nodes (subnetwork nodes) to existing single-hidden-layer ELM networks.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A versatile platform with faster speed and better generalization performance on feature extraction.&lt;/li&gt;
&lt;li&gt;Its generalization performance is not sensitive to the parameters of the networks in the learning process.&lt;/li&gt;
&lt;li&gt;ML-ELM has universal approximation capability and representation learning.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ii-preliminaries-and-basic-elm&#34;&gt;II. Preliminaries and Basic-ELM&lt;/h2&gt;
&lt;h3 id=&#34;a-notations&#34;&gt;A. Notations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ğ‘ : set of real numbers&lt;/li&gt;
&lt;li&gt;{(ğ±áµ¢,ğ²áµ¢)áµ¢â‚Œâ‚á´¹} (ğ±áµ¢âˆˆ ğ‘â¿,ğ²áµ¢âˆˆ ğ‘áµ) : M arbitrary distinct samples,&lt;/li&gt;
&lt;li&gt;ğ± : an input data matrix ğ±âˆˆ ğ‘â¿á•á´¹&lt;/li&gt;
&lt;li&gt;ğ² : desired output data matrix ğ²âˆˆ ğ‘áµá•á´¹&lt;/li&gt;
&lt;li&gt;ğ›‚áµ¢ : weight vector connecting the ğ‘–th hidden nodes and the input nodes&lt;/li&gt;
&lt;li&gt;â™­áµ¢ : bias of the ğ‘–th hidden nodes&lt;/li&gt;
&lt;li&gt;Î²áµ¢ : output weight between the ğ‘–th hidden node and the output node&lt;/li&gt;
&lt;li&gt;ğ : residual error of current network output, i.e., ğ=ğ²-ğŸ&lt;/li&gt;
&lt;li&gt;ğˆ : unit matrix&lt;/li&gt;
&lt;li&gt;sum(ğ) : the sum of all elements of the matrix ğ&lt;/li&gt;
&lt;li&gt;g : sigmoid or sine activation function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(TABLE 1)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(ğ›‚,â™­) : a hidden node (in basic ELM)&lt;/li&gt;
&lt;li&gt;(ğš,ğ‘) : a general hidden node (or subnetwork node)&lt;/li&gt;
&lt;li&gt;^ğšÊ²_f : input weight of the jth general hidden node in feature mapping layer. ^ğšÊ²_fâˆˆ ğ‘áµˆá•â¿&lt;/li&gt;
&lt;li&gt;^bÊ²_f : bias of the ğ‘—th general hidden node in feature mapping layer ^bÊ²_fâˆˆ ğ‘&lt;/li&gt;
&lt;li&gt;(ğ›‚áµ¢Ê²_f,â™­Ê²_f) : the ğ‘–th general hidden node in the ğ‘—th general hidden node.&lt;/li&gt;
&lt;li&gt;(^ğšâ‚•,^ğ‘â‚•) : hidden nodes in ELM learning layer and ^ğšâ‚•âˆˆ ğ‘ áµá•áµˆ&lt;/li&gt;
&lt;li&gt;uâ±¼ : normalized function in the ğ‘—th general node, uâ±¼(â‹…):ğ‘ â” (0,1], uâ±¼â»Â¹ represent its reverse function&lt;/li&gt;
&lt;li&gt;ğ‡Ê²_f : feature data generated by ğ‘—general nodes in a feature mapping layer,
i.e., ğ‡Ê²_f = âˆ‘áµ¢â‚Œâ‚Ê² uáµ¢â»Â¹ â‹… g(ğ±, ^ğšâ±_f, ^bâ±_f), ğ‡Ê²_fâˆˆ ğ‘áµˆá•á´¹&lt;/li&gt;
&lt;li&gt;ğ‡Ê²â±_f : feature data generated by the ğ‘–th feature mapping layer&lt;/li&gt;
&lt;li&gt;M : number of training samples&lt;/li&gt;
&lt;li&gt;n : input data dimension&lt;/li&gt;
&lt;li&gt;m : output data dimension&lt;/li&gt;
&lt;li&gt;d : feature data dimension&lt;/li&gt;
&lt;li&gt;ğ_L : the residual error of current two-layer network (L general nodes in the first layer and (ğšâ‚•,ğ‘â‚•) in the second layer)&lt;/li&gt;
&lt;li&gt;ğÊ²_L : the residual error of current two-layer network (L general nodes in the first layer and ğ‘—general nodes in the second layer)&lt;/li&gt;
&lt;li&gt;L : the numbers of general hidden nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;b-basic-elm&#34;&gt;B. Basic-ELM&lt;/h3&gt;
&lt;p&gt;The output function of ELM for SLFNs fed with input matrix ğ± is: &lt;br&gt;
fâ‚™(ğ±)=âˆ‘áµ¢â‚Œâ‚â¿ Î²áµ¢ g(ğ±, ğ›‚áµ¢, â™­áµ¢).&lt;/p&gt;
&lt;p&gt;&amp;ldquo;ELM theory aims to reach the smallest training error but also the smallest norm of output weights&amp;rdquo; (regularization term?), so the objective is to minimize: &lt;br&gt;
â€–Î²áµ¢â€–â‚šá¶£Â¹ + Câ‹…â€–âˆ‘áµ¢â‚Œâ‚â¿ Î²áµ¢ g(ğ±, ğ›‚áµ¢, â™­áµ¢) - ğ²â€–á¶£Â²_q, i=1,&amp;hellip;,n. â€ƒ (á¶£ signifies Î¼)&lt;/p&gt;
&lt;p&gt;where Î¼â‚&amp;gt;0, Î¼â‚‚&amp;gt;0, p,q = 0, Â½, 1, 2, &amp;hellip;, +âˆ, C is a positive value, g(ğ±, ğ›‚, â™­) is referred to as ELM feature mapping (linear projection+activation) or Huang&amp;rsquo;s transform.&lt;/p&gt;
&lt;p&gt;(Convergence proved by Huang et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;: Given M aribitrary distinct samples {(ğ±, ğ²)}, ğ±âˆˆ ğ‘â¿á•á´¹, ğ²âˆˆ ğ‘áµá•á´¹ sampled from a continuous system, an activation function g,
then for any continous target function ğ² and any function sequence g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³) randomly generated based on any continuous sampling distribution,
lim_{nââˆ} â€–ğ²-ï¼ˆfâ‚™â‚‹â‚ + g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³)ï¼‰â€–=0 holds with probabiltiy one if &lt;br&gt;
Î²â‚™ = âŸ¨ğâ‚™â‚‹â‚, g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³)âŸ© / â€–g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³)â€–Â²,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where (ğ›‚â‚™Ê³, â™­â‚™Ê³) represesnts the ğ‘›th random hidden node, and ğâ‚™â‚‹â‚ = ğ²-fâ‚™â‚‹â‚&lt;/p&gt;
&lt;h2 id=&#34;iii-proposed-method&#34;&gt;III. Proposed Method&lt;/h2&gt;
&lt;h3 id=&#34;a-elm-with-subnetwork-nodes&#34;&gt;A. ELM With Subnetwork Nodes&lt;/h3&gt;
&lt;p&gt;A hidden node can be a subnetwork formed by several hidden nodes. Hence, a single mapping layer can contain multiple networks.&lt;/p&gt;
&lt;p&gt;Comparision of the feature mapping layer:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
subgraph A[basic ELM]
direction BT
x1[&#34;xâ‚&#34;]--&gt; h1((&#34;ğ›‚â‚,â™­â‚, Î²â‚&#34;)) &amp; he1((...)) &amp; hL((&#34;ğ›‚L,â™­L, Î²L&#34;)) 
xe[x...]--&gt; h1((&#34;ğ›‚â‚,â™­â‚, Î²â‚&#34;)) &amp; he1((...)) &amp; hL((&#34;ğ›‚L,â™­L, Î²L&#34;)) 
xn[&#34;xâ‚™&#34;]--&gt; h1((&#34;ğ›‚â‚,â™­â‚, Î²â‚&#34;)) &amp; he1((...)) &amp; hL((&#34;ğ›‚L,â™­L, Î²L&#34;)) 

h1 --&gt; y1[&#34;yâ‚&#34;] &amp; ye[...] &amp; ym[&#34;yâ‚˜&#34;]
he1--&gt; y1[&#34;yâ‚&#34;] &amp; ye[...] &amp; ym[&#34;yâ‚˜&#34;]
hL --&gt; y1[&#34;yâ‚&#34;] &amp; ye[...] &amp; ym[&#34;yâ‚˜&#34;]

subgraph A1[&#34;ELM feature mapping layer&#34;]
h1 &amp; he1 &amp; hL
end
end

subgraph A1[&#34;ELM feature mapping layer&#34;]
h1 &amp; he1 &amp; hL
end

subgraph B[ELM with subnetwork nodes]
direction BT
x1_[&#34;xâ‚&#34;] --&gt; ghn1 &amp; ghne((...)) &amp; ghnL
xe_[x...] --&gt; ghn1 &amp; ghne((...)) &amp; ghnL
xn_[&#34;xâ‚™&#34;] --&gt; ghn1 &amp; ghne((...)) &amp; ghnL

ghn1--&gt; y1_[&#34;yâ‚&#34;] &amp; ye_[...] &amp; ym_[&#34;yâ‚˜&#34;]
ghne--&gt; y1_[&#34;yâ‚&#34;] &amp; ye_[...] &amp; ym_[&#34;yâ‚˜&#34;]
ghnL--&gt; y1_[&#34;yâ‚&#34;] &amp; ye_[...] &amp; ym_[&#34;yâ‚˜&#34;]
end

subgraph ghn1[&#34;^ğ›‚Â¹_f, ^â™­Â¹_f, with weight uâ‚â»Â¹&#34;]
direction TB
n11((&#34;ğ›‚Â¹_f1,\n â™­Â¹_f1&#34;)) &amp; n1e((...)) &amp; n1m((&#34;ğ›‚Â¹_fm,\n â™­Â¹_fm&#34;))
end

subgraph ghne[&#34;general hidden nodes&#34;]
direction TB
ne1((1)) &amp; nee((...)) &amp; nem((m))
end

subgraph ghnL[&#34;^ğ›‚á´¸_f, ^â™­_f, with weight u\_Lâ»Â¹&#34;]
direction TB
nL1((&#34;ğ›‚á´¸_f1,\n â™­á´¸_f1&#34;)) &amp; nLe((...)) &amp; nLm((&#34;ğ›‚á´¸_fm,\n â™­á´¸_fm&#34;))
end
&lt;/div&gt;

&lt;p&gt;Three differences between ELM feature mapping layer and this feature mapping layer.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Difference&lt;/th&gt;
&lt;th&gt;Standard ELM&lt;/th&gt;
&lt;th&gt;ELM with subnetwork nodes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hidden node&lt;/td&gt;
&lt;td&gt;single hidden node generated&lt;br&gt; one by one&lt;/td&gt;
&lt;td&gt;general hidden node having subnetwork&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;# hidden node&lt;/td&gt;
&lt;td&gt;Independent to the output dim ğ‘š&lt;/td&gt;
&lt;td&gt;In a subnetwork, it equals to the output dim&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;relation&lt;/td&gt;
&lt;td&gt;A special case of the subnetwork case&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;b-proposed-method-for-representation-learning&#34;&gt;B. Proposed Method for Representation Learning&lt;/h3&gt;
&lt;h4 id=&#34;1-optimal-projecting-parameters-and-optimal-feature-data&#34;&gt;1) Optimal Projecting Parameters and Optimal Feature Data&lt;/h4&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
x1[&#34;xâ‚&#34;]--&gt; h1((&#34;^ğ›‚Â¹_f,^â™­Â¹_f, Î²Â¹&#34;)) &amp; h2((&#34;^ğ›‚Â²_f,^â™­Â²_f, Î²Â²&#34;)) &amp; he1((&#34;â‹®&#34;)) &amp; hL((&#34;^ğ›‚á´¸_f,^â™­á´¸_f, ^Î²á´¸&#34;)) 
xe[x...]--&gt; h1((&#34;^ğ›‚Â¹_f,^â™­Â¹_f, Î²Â¹&#34;)) &amp; h2((&#34;^ğ›‚Â²_f,^â™­Â²_f, Î²Â²&#34;)) &amp; he1((&#34;â‹®&#34;)) &amp; hL((&#34;^ğ›‚á´¸_f,^â™­á´¸_f, ^Î²á´¸&#34;)) 
xn[&#34;xâ‚™&#34;]--&gt; h1((&#34;^ğ›‚Â¹_f,^â™­Â¹_f, Î²Â¹&#34;)) &amp; h2((&#34;^ğ›‚Â²_f,^â™­Â²_f, Î²Â²&#34;)) &amp; he1((&#34;â‹®&#34;)) &amp; hL((&#34;^ğ›‚á´¸_f,^â™­á´¸_f, ^Î²á´¸&#34;)) 

h1 &amp; h2 &amp; he1 &amp; hL --&gt; feat[&#34;d-dimension\n Feature data&#34;]
feat --&gt; n1 &amp; n2 &amp; ne &amp; nm --&gt; y1_[&#34;yâ‚&#34;] &amp; ye_[&#34;â‹®&#34;] &amp; ym_[&#34;yâ‚˜&#34;]

subgraph A1[&#34;ELM feature mapping layer&#34;]
h1 &amp; h2 &amp; he1 &amp; hL
end

subgraph elm[&#34;ELM-learning layer&#34;]
n1((&#34;ğ›‚â‚•â‚,^â™­â‚•&#34;)) &amp; n2((&#34;ğ›‚â‚•â‚‚,^â™­â‚•&#34;)) &amp; ne((&#34;â‹®&#34;)) &amp; nm((&#34;ğ›‚â‚•â‚˜,^â™­â‚•&#34;))
end
&lt;/div&gt;

&lt;p&gt;Objective of representation learning: Represent the input features meaningfully in several different representations as follows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Represen-&lt;br&gt;tation&lt;/th&gt;
&lt;th&gt;feat dim (ğ‘‘) vs&lt;br&gt; in-dim (ğ‘›)&lt;/th&gt;
&lt;th&gt;feature&lt;/th&gt;
&lt;th&gt;target output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Dimension Reduction&lt;/td&gt;
&lt;td&gt;ğ‘‘ &amp;lt; ğ‘›&lt;/td&gt;
&lt;td&gt;H_f âˆˆ ğ‘áµˆá•á´¹&lt;/td&gt;
&lt;td&gt;ğ²=label (Supervise)&lt;br&gt;or ğ²=ğ± (Unsp~)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Expanded Dimension&lt;/td&gt;
&lt;td&gt;ğ‘‘ &amp;gt; ğ‘›&lt;/td&gt;
&lt;td&gt;H_f âˆˆ ğ‘áµˆá•á´¹&lt;/td&gt;
&lt;td&gt;ğ²=label (Supervise)&lt;br&gt;or ğ²=ğ± (Unsp~)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The feature data is ğ‡_f(ğ±â‚–, ^ğš_f, ^b_f), where the weights of feature mapping layer ^ğšÊ²_f, j=1,&amp;hellip;,L belongs to ğ‘áµˆá•â¿.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;: Given a nonlinear piecewise continous activation function g, we call &lt;br&gt;
{(^ğšÊ²_f, ^bÊ²_f)â±¼â‚Œâ‚á´¸} (^ğšÊ²_f âˆˆ ğ‘áµˆá•â¿) the ğ¿ optimal general hidden nodes &lt;br&gt;
and ğ‡âƒ° âƒ°_f= âˆ‘áµ¢â‚Œâ‚á´¸ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) the optimal feature data if it satisfies: &lt;br&gt;
â€–ğ_Lâ€– â‰¤ min_{ğ‡âƒ°á´¸_fâˆˆ ğ‘áµˆá•á´¹} ( min_{ğšâ‚•âˆˆ ğ‘ áµá•áµˆ} â€–ğ²-uâ‚•â»Â¹ g(ğ‡âƒ°á´¸_f, ğšâ‚•, bâ‚•)â€– )  â€‚ (4)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where ğ_L = â€–ğ²-uâ‚•â»Â¹ g(ğ‡âƒ°á´¸ âƒ°_f, ^ğšâ‚•, ^bâ‚•)â€– and sequence â€–ğ_Lâ€– is decreasing and bounded below by zero.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remark 1&lt;/strong&gt;: If the optimal projecting parameters are obtained in the feature mapping layer
{(^ğšÊ²_f, ^bÊ²_f)â±¼â‚Œâ‚á´¸} (where ^ğš_f âˆˆ ğ‘áµˆá•â¿), &lt;br&gt;
the original n-dimension data points ğ± will be converted to d-dimension data points:
ğ‡âƒ°_f= âˆ‘â±¼â‚Œâ‚á´¸ g(ğ±â‚–, ^ğšÊ²_f, ^bÊ²_f), which satisfy the inequality (4).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thus the purpose is to find optimal projecting parameters that make the inequality (4) true for all data points.&lt;/p&gt;
&lt;h4 id=&#34;2-learning-steps&#34;&gt;2) Learning Steps&lt;/h4&gt;
&lt;p&gt;Based on the inverse of the activation function.&lt;/p&gt;
&lt;p&gt;Given M arbitrary distinct training samples {(ğ±â‚–,ğ²â‚–)â‚–â‚Œâ‚á´¹}, ğ±â‚–âˆˆ ğ‘â¿, ğ²â‚–âˆˆ ğ‘áµ, which are sampled from a continuous system.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set j=1 to initialize a &lt;strong&gt;general node&lt;/strong&gt; of the feature mapping layer randomly as: &lt;br&gt;
ğ‡âƒ°Ê²_f = g(^ğšÊ²_fâ‹…ğ± + ^bÊ²_f), (^ğšÊ²_f)áµ€â‹…^ğšÊ²_f=ğˆ, (^bÊ²_f)áµ€â‹…^bÊ²_f=1,&lt;/p&gt;
&lt;p&gt;where ^ğšÊ²âˆˆ ğ‘áµˆá•â¿, ^bÊ²_fâˆˆ ğ‘ is the orthogonal random weight and bias of feature mapping layer. ğ‡âƒ°Ê²_f is current feature data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given a sigmoid or sine activation function g, for any continous desired outputs ğ², the parameters in the (general) ELM learning layer are obtained as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^ğšâ‚• = gâ»Â¹(uâ‚™(ğ²)) â‹… (ğ‡âƒ°Ê²_f)â»Â¹, ^ğšÊ²â‚•âˆˆ ğ‘áµˆá•áµ,&lt;/li&gt;
&lt;li&gt;^bâ‚• = âˆšmse(^ğšâ‚•Ê² â‹… ğ‡âƒ°Ê²_f - gâ»Â¹(uâ‚™(ğ²)) ), ^bÊ²â‚™âˆˆ ğ‘,&lt;/li&gt;
&lt;li&gt;$gâ»Â¹(â‹…) = \{^{arcsin(â‹…) \quad if\ g(â‹…)=sin(â‹…)}_{-log(1/(â‹…)-1) \quad if\ g(â‹…) = 1/(1+eâ»â½Ë™â¾)}$, _&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where ğ‡âƒ°â»Â¹ = ğ‡âƒ°áµ€( (C/ğˆ) + ğ‡âƒ° ğ‡âƒ°áµ€)â»Â¹; C is a positive value; uâ‚™ is a normalized function
uâ‚™(ğ²): ğ‘â”(0,1]; gâ»Â¹ and uâ‚™â»Â¹ represent their reverse function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the output error ğâ±¼ as &lt;br&gt;
ğâ±¼ = ğ² - uâ‚™â»Â¹ g(ğ‡âƒ°Ê²_f, ^ğšâ‚•, ^bâ‚•)  &lt;br&gt;
So the error feedback data is ğâ±¼ = gâ»Â¹(uâ‚™(ğâ±¼))â‹…(^ğšâ‚•)â»Â¹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set j=j+1, add a new general node (^ğšÊ²_f, ^bÊ²_f) in the feature mapping layer by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^ğšÊ²_f = gâ»Â¹( uâ±¼(ğâ±¼â‚‹â‚) ) â‹… ğ±â»Â¹, ^ğšÊ²_fâˆˆ ğ‘â¿á•áµˆ&lt;/li&gt;
&lt;li&gt;^bÊ²_f = âˆšmse(^ğšÊ²_f â‹… ğ± - ğâ±¼â‚‹â‚), ^bÊ²âˆˆ ğ‘&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and update the feature data ğ‡âƒ°Ê²_f = âˆ‘áµ¢â‚Œâ‚Ê² uâ‚—â»Â¹ g(ğ±, ^ğšË¡_f, ^bË¡_f)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat step 2-4 ğ¿-1 times. (Finally, ğ¿ nodes are added into feature mapping layer.)
The set of parameters {^ğšÊ²_f,^bÊ²_f}â±¼â‚Œâ‚á´¸ are the optimal projecting parameters
and the feature data ğ‡âƒ°á´¸_f = âˆ‘â±¼â‚Œâ‚á´¸ uâ±¼â»Â¹ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) = ğ‡âƒ° âƒ°_f are the optimal feature data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;c-proof-of-the-proposed-method&#34;&gt;C. Proof of the Proposed Method&lt;/h3&gt;
&lt;p&gt;(Proof of Convergence)&lt;/p&gt;
&lt;p&gt;Given M arbitrary distinct samples {(ğ±â‚–,ğ²â‚–)}â‚–â‚Œâ‚á´¹ (ğ±â‚–âˆˆ ğ‘â¿, ğ²â‚–âˆˆ ğ‘áµ)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;: Given a bounded nonconstant piecewise continuous activation function g, we have &lt;br&gt;
lim_{(ğ›‚,â™­)â†’(^ğ›‚,^â™­)} â€–g(ğ±,ğ›‚,â™­) - g(ğ±,^ğ›‚,^â™­)â€– = 0
where the (^ğ›‚,^â™­) is one of the least-squares solutions of a general linear system ğ›‚â‹…ğ±+â™­.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Remark 2:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Lemma 2 shows that SLFN training problem can be considered as finding optimal hidden parameters which satisfy:
g(^ğ›‚â‚,^â™­â‚) + &amp;hellip; + g(^ğ›‚_L,^â™­_L) â†’ ğ². â€ƒ ğ›‚ (alpha) stands for basic ELM hidden node.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thus training an SLFN is equivalent to finding a least-square general input weight ^ğšâ‚• of the (linear+activation) system g(^ğšâ‚•â‹…ğ±) = ğ².&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If activation function g is invertible, the input weights matrix can be obtained by pulling back the residual error to the hidden layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, if g is a sine function,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The output of the hidden layer matrix is ğ²=sin(ğšâ‚• â‹… ğ±).&lt;/li&gt;
&lt;li&gt;Thus, ğšâ‚•â‹…ğ± = arcsin(ğ²), ğ²âˆˆ (0,1].&lt;/li&gt;
&lt;li&gt;The smallest norm least-squares solution of the linear system sin(ğšâ‚•â‹…ğ±)=ğ² is: &lt;br&gt;
^ğšâ‚• = arcsin(ğ²)â‹…ğ±â»Â¹,
where ğ±â»Â¹ is the Moore-Penrose generalized inverse of matrix ğ±. ğ±â»Â¹ = ğ±áµ€( (C/ğˆ) + ğ±ğ±áµ€)â»Â¹&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: Given M arbitrary distinct samples {(ğ±áµ¢,ğ²áµ¢)áµ¢â‚Œâ‚á´¹}, (ğ±áµ¢âˆˆ ğ‘â¿, ğ²áµ¢âˆˆ ğ‘áµ) and a sigmoid or sine activation function g, for any continuous desired outputs ğ², we have:&lt;br&gt;
the optimal weights ^ğšâ‚• = argmin_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹(g(ğ±,ğšâ‚•)) - ğ²â€– &lt;br&gt;
least square error â€–g(ğ±,^ğšâ‚•,^bâ‚•) - ğ²â€– â‰¤ min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹(g(ğšâ‚•â‹…ğ±)) - ğ²â€–  &lt;br&gt;
if the parameters are obtained by (similar to Algorithm step-2):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^ğšâ‚• = gâ»Â¹( u(ğ²))â‹…ğ±â»Â¹, ^ğšâ‚• âˆˆ ğ‘áµá•â¿&lt;/li&gt;
&lt;li&gt;^bâ‚• = âˆšmse(^ğšâ‚•â‹…ğ± - gâ»Â¹(u(ğ²))), ^bâ‚•âˆˆ ğ‘&lt;/li&gt;
&lt;li&gt;$gâ»Â¹(â‹…) = \{^{arcsin(â‹…) \quad if\ g(â‹…)=sin(â‹…)}_{-log(1/(â‹…)-1) \quad if\ g(â‹…) = 1/(1+eâ»â½Ë™â¾)}$, _&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Let ğ›Œ=ğšâ‚•â‹…ğ±, and ğ›Œ satisfy g(ğ›Œ) = ğ². Normalizing ğ² to (0,1] by u(ğ²) to let ğ›Œâˆˆ ğ‘. &lt;br&gt;
Thus, for a sine hidden node, ğ›Œ = gâ»Â¹(u(ğ²)) = arcsin(u(ğ²)).
While for a sigmoid hidden node, ğ›Œ = gâ»Â¹(u(ğ²)) = -log(1/u(ğ²) - 1).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;^ğšâ‚• is the solution for the linear system (g(ğšâ‚•â‹…ğ±)=ğ²). &lt;br&gt;
For sine activation: ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹ = arcsin(u(ğ²))â‹…ğ±â»Â¹.
For sigmoid activation: ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹ = -log(1/u(ğ²) - 1)â‹…ğ±â»Â¹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the least-squares solutions of a general linear system ğšâ‚•â‹…ğ±=ğ›Œ is ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹, which means the smallest error can be reached by this solution:
â€–^ğšâ‚•â‹…ğ± -ğ›Œâ‚™â€– = min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–ğšâ‚•â‹…ğ± - gâ»Â¹( u(ğ²) )â€–   â€ƒ (18)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The special solution ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹ has the smallest norm among all the least-squares solutions of ğšâ‚•â‹…ğ± = ğ›Œ.
The error can be further reduced by adding bias bâ‚™:
^bâ‚• = âˆšmse(^ğšâ‚•â‹…ğ± - hâ»Â¹( u(ğ²) ))&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on eq. (18) and Lemma2, optimization by minimizing the L2-loss can be reformulated as:
min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹( g(ğšâ‚•â‹…ğ±) ) - uâ»Â¹( g(ğ›Œ))â€–
= â€–uâ»Â¹( g(^ğšâ‚•â‹…ğ±) ) - uâ»Â¹( g(ğ›Œ))â€–
â‰¥ â€–uâ»Â¹( g(^ğšâ‚•â‹…ğ± + ^bâ‚•) ) - ğ²â€–    â€ƒ (20)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on eq. (18) and eq. (20), the optimal weights is proved as:
^aâ‚• = arg min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–g(ğ±,ğšâ‚•) - ğ²â€– &lt;br&gt;
And it satisfy: â€–g(ğ±,^ğšâ‚•,^bâ‚•) - ğ²â€– â‰¤ min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹( g(^ğšâ‚•â‹…ğ±) ) - ğ² â€–&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Based on Lemma 2 and Theorem 1, Theorem 2 is given:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt;: Given M arbitrary distinct samples (ğ±, ğ²), ğ±âˆˆ ğ‘â¿á•á´¹, ğ²âˆˆ ğ‘áµá•á´¹, a sigmoid or sine activation function g,
and the initial orthogonal random weights ^ğšÂ¹_f and bias ^bÂ¹_f.
For any continuous desired output ğ², the optimal feature data is: &lt;br&gt;
ğ‡âƒ°á´¸âƒ° _f(ğ±, (^ğšÂ¹_f, &amp;hellip;, ^ğšá´¸_f), (^bÂ¹_f,&amp;hellip;,^bá´¸_f))
= âˆ‘â±¼â‚Œâ‚á´¸ uâ±¼â»Â¹ g(^ğšÊ²_f â‹… ğ± + ^bÊ²_f)
which satisfy: &lt;br&gt;
â€–ğ_Lâ€– â‰¤ min_{^ğšÊ²_fâˆˆ ğ‘â¿á•áµˆ} ( min_{ğšâ‚•âˆˆ ğ‘ áµá•áµˆ} â€–ğ²-uâ‚™â»Â¹ g(ğ‡âƒ°á´¸_f, ğšâ‚•, bâ‚•)â€–) â€‚  (21)&lt;/p&gt;
&lt;p&gt;and â€–ğ_Lâ€– is decreasing and bounded below by zero if these parameters are obtained by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğ‡âƒ°Ê²_f = âˆ‘áµ¢â‚Œâ‚Ê² uáµ¢â»Â¹ g(ğ±, ^ğšâ±_f, ^bâ±_f),&lt;/li&gt;
&lt;li&gt;^ğšâ‚• = gâ»Â¹(uâ‚™(ğ²)) â‹… (ğ‡âƒ°Ê²_f)â»Â¹, ^ğšâ‚•âˆˆ ğ‘ áµá•áµˆ,&lt;/li&gt;
&lt;li&gt;^bâ‚• = âˆšmse(^ğšâ‚•â‹…ğ‡âƒ°Ê²_f - gâ»Â¹( u(ğ²) )), ^bâ‚•âˆˆ ğ‘&lt;/li&gt;
&lt;li&gt;gâ»Â¹(â‹…) = \{^{arcsin(â‹…) \quad if\ g(â‹…)=sin(â‹…)}_{-log(1/(â‹…)-1) \quad if\ g(â‹…) = 1/(1+eâ»â½Ë™â¾)}$, _&lt;/li&gt;
&lt;li&gt;ğâ±¼ = ğ² - uâ‚™â»Â¹( g(ğ‡âƒ°Ê²_f, ^ğšâ‚•, ^bâ‚•), ğâ±¼ = gâ»Â¹(uâ‚™(ğâ±¼))â‹…(^ğšâ‚•)â»Â¹ ),&lt;/li&gt;
&lt;li&gt;^ğšÊ²_f = gâ»Â¹(uâ±¼(ğâ±¼â‚‹â‚)) â‹… ğ±â»Â¹, ^ğšÊ²âˆˆ ğ‘â¿á•áµˆ&lt;/li&gt;
&lt;li&gt;^bÊ²_f = âˆšmse(^ğšÊ²_f â‹… ğ± - ğâ±¼â‚‹â‚), ^bÊ²_fâˆˆ ğ‘&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;Base on Theorem 1, the validity of (21) is obvious. So here, we just prove that the error â€–ğ_Lâ€– is decreasing and bounded below by zero.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Let Î” = â€–eâ±¼â‚‹â‚â€–Â² - â€–ğ² - uâ‚™â»Â¹g(ğ‡âƒ°Ê²_f, ^ğšâ‚•, ^bâ‚•)â€–Â² (last error-current output), and take the newest item apart: &lt;br&gt;
= â€–eâ±¼â‚‹â‚â€–Â² - â€–ğ² - uâ‚™â»Â¹g( (âˆ‘áµ¢â‚Œâ‚Ê²â»Â¹ uáµ¢â»Â¹ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) + uáµ¢â»Â¹g(ğ±, ^ğšÊ²_f, ^bÊ²_f) ), ^ğšâ‚•, ^bâ‚•) â€–Â²  â€ƒ   (24)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let ^TÊ² = uâ‚™â»Â¹g(uâ±¼â»Â¹g(ğ±, ^ğšÊ²_f, ^bÊ²_f), ^ğšâ‚•, ^bâ‚•). Because activation function is sigmoid or sine function, eq. (24) can be simplified as: &lt;br&gt;
Î” â‰¥ â€–ğâ±¼â‚‹â‚â€–Â² - â€–ğ² - uâ‚™â»Â¹g( (âˆ‘áµ¢â‚Œâ‚Ê²â»Â¹ uáµ¢â»Â¹ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) ), ^ğšâ‚•, ^bâ‚•) - ^TÊ²â€–Â² &lt;br&gt;
= â€–ğâ±¼â‚‹â‚â€–Â² - â€–ğâ±¼â‚‹â‚ - ^TÊ²â€–Â²  â€ƒ (unfold)   &lt;br&gt;
= â€–ğâ±¼â‚‹â‚â€–Â² - (â€–ğâ±¼â‚‹â‚â€–Â² - 2&amp;lt;eâ±¼â‚‹â‚, â€–^TÊ²â€–&amp;gt; + â€–^TÊ²â€–Â²) Â  (&amp;quot;&amp;lt;&amp;gt;&amp;quot; is dot product of 2 matrices: Frobenius inner product)&lt;br&gt;
= 2&amp;lt;ğâ±¼â‚‹â‚, â€–^TÊ²â€–&amp;gt; - â€–^TÊ²â€–Â²   &lt;br&gt;
= â€–^TÊ²â€–Â² ( 2&amp;lt;ğâ±¼â‚‹â‚, â€–^TÊ²â€–&amp;gt; / â€–^TÊ²â€–Â² - 1 )   â€ƒ (25)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We set ^TÊ² = uâ‚™â»Â¹g(uâ±¼â»Â¹g( ğ±, ^ğšÊ²_f, ^bÊ²_f) ), ^ğšâ‚•, ^bâ‚• ) = ğâ±¼â‚‹â‚ Â± Ïƒ. (Ïƒ is variance, and ğâ±¼â‚‹â‚ is the expectation) &lt;br&gt;
So ğâ±¼â‚‹â‚ = ^TÊ² Â± Ïƒ. &lt;br&gt;
Then &amp;lt;ğâ±¼â‚‹â‚, â€–^TÊ²â€–&amp;gt; = &amp;lt;^TÊ²Â± Ïƒ, â€–^TÊ²â€–&amp;gt; = &amp;lt;â€–^TÊ²â€–Â² Â± &amp;lt;â€–^TÊ²â€–,Ïƒ&amp;gt; &amp;gt;&lt;/p&gt;
&lt;p&gt;Hence, eq. (25) can be reformulated: &lt;br&gt;
Î” â‰¥ â€–^TÊ²â€–Â² ( 2&amp;lt; â€–^TÊ²â€–Â² Â± &amp;lt;â€–^TÊ²â€–,Ïƒ&amp;gt; &amp;gt; / â€–^TÊ²â€–Â² - 1 ) &lt;br&gt;
= â€–^TÊ²â€–Â² ( 1 Â± 2â€–Ïƒâ‹…(^TÊ²)áµ€â€–/â€–^TÊ²â€–Â²)  (Wandong thinks there should be a 2.) &lt;br&gt;
â‰¥&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, based on Theorem 1 and eq. (7), there will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â€–^TÊ² - ğâ±¼â‚‹â‚â€– â‰¤ min_{^ğšÊ²_fâˆˆ ğ‘áµˆá•â¿} â€–uâ‚™â»Â¹g(uâ±¼â»Â¹g( ğ±, ^ğšÊ²_f, ^bÊ²_f), ^ğšâ‚•, ^bâ‚•) -ğâ±¼â‚‹â‚â€–&lt;/li&gt;
&lt;li&gt;â€–Ïƒâ€– â‰¤ â€–^TÊ²â€–&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus Î” â‰¥ 0 can be proved as:
Î” â‰¥ â€–^TÊ²â€–Â² (1 Â± â€–Ïƒâ€– / â€–^TÊ²â€–) â‰¥ 0  â€ƒ (28)&lt;/p&gt;
&lt;p&gt;Eq. (28) means â€–ğâ±¼â‚‹â‚â€– â‰¥ â€–ğâ±¼â€– and â€–ğâ€– is decreasing and bounded below by zero.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Based on Theorem 2, Theorem 3 is given:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 3&lt;/strong&gt;: Given M arbitrary distinct samples (ğ±, ğ²), ğ±âˆˆ ğ‘â¿á•á´¹, ğ²âˆˆ ğ‘áµá•á´¹, a sigmoid or sine activation function g,
and optimal feature data ğ‡âƒ°á´¸_f obtained by Algorithm 1, &lt;br&gt;
then lim_{jâ+âˆ} â€–ğ² - Î²â‚â‹…uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚) - &amp;hellip; - Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼, ğ‘â±¼)â€– = 0
holds with probability one if :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğšâ±¼ = gâ»Â¹( u(ğ²) ) â‹… (ğ‡âƒ°á´¸_f)â»Â¹, ^ğšâ±¼âˆˆ ğ‘áµá•â¿&lt;/li&gt;
&lt;li&gt;bâ±¼ = âˆšmse(^ğšâ±¼â‹…(ğ‡âƒ°á´¸_f) - gâ»Â¹(u(ğ²))), ^bâ±¼âˆˆ ğ‘&lt;/li&gt;
&lt;li&gt;Î²â±¼ = âŸ¨ğâ±¼â‚‹â‚, g(ğ‡âƒ°á´¸_f, ğšâ±¼, bâ±¼)âŸ© / â€–g(ğ‡âƒ°á´¸_f, ğšâ±¼, bâ±¼)â€–Â², Î²â±¼âˆˆ ğ‘&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;First prove that the sequence â€–ğâ±¼á´¸â€– is decreasing and bounded below by zero.
Then prove that the lim_{jâ+âˆ} â€–ğâ±¼á´¸â€– = 0&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Based on Theorem 1 and Lemma 1, the network output error satisfies:
â€–ğâ±¼á´¸â€– = â€–ğ² - Î²â‚â‹…uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚) - &amp;hellip; - Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼, ğ‘â±¼)â€–  &lt;br&gt;
â‰¤ â€–ğ² -uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚)â€–  &lt;br&gt;
= â€–ğâ‚á´¸â€–&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on Theorem 2, there will be: &lt;br&gt;
â€–ğâ±¼á´¸â€– â‰¤ â€–ğâ±¼á´¸â»Â¹â€– â‰¤ &amp;hellip; â‰¤ â€–ğâ±¼Â¹â€–&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thus, â€–ğâ±¼á´¸â€– â‰¤ â€–ğâ±¼á´¸â»Â¹â€– â‰¤ &amp;hellip; â‰¤ â€–ğâ±¼Â¹â€– â‰¤ &amp;hellip; â‰¤ â€–ğâ‚Â¹â€– and â€–ğâ±¼á´¸â€– is decreasing and bounded below by 0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on Lemma 1, when all hidden nodes randomly generated based on any continuous sampling distribution,
lim_{nââˆ} â€–f - (fâ‚™â‚‹â‚ + Î²â‚™â‹…g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³) )â€– = 0 holds with probability one if
Î²â‚™ = âŸ¨ğâ‚™â‚‹â‚, g(ğ‡âƒ°á´¸â±¼, ğ›‚â‚™Ê³, â™­â‚™Ê³)âŸ© / â€–g(ğ‡âƒ°á´¸â±¼, ğ›‚â‚™Ê³, â™­â‚™Ê³)â€–Â².&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, ELM theories have shown that almost any nonlinear piecewise continuous random hidden node can be use in ELM, and the resultant networks have universal approximation capbilities. &lt;br&gt;
According to the definition of general hidden neurons, (a general hidden node contains m (basic) hidden node),
a general hidden node (ğš,b) = (ğ›‚Ê³â‚, &amp;hellip;, ğ›‚Ê³â‚˜, bÊ³â‚, &amp;hellip;, bÊ³â‚˜), . &lt;br&gt;
Thus its output is g(ğ‡âƒ°á´¸_f, ğšâ±¼Ê³, â™­â±¼Ê³) â‰¡ âˆ‘áµ¢â‚Œâ‚áµ g(ğ‡âƒ°á´¸_f, ğ›‚Ê³áµ¢, bÊ³áµ¢).&lt;/p&gt;
&lt;p&gt;Therefore, lim_{jââˆ} â€– ğ² - Î²â‚â‹…uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚) - &amp;hellip; - Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼, ğ‘â±¼)â€–&lt;br&gt;
= lim_{nââˆ} â€–f- (fâ‚™â‚‹â‚ + Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼Ê³, ğ‘â±¼Ê³)) â€– = 0&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;d-proposed-method-with-multinetwork-structures&#34;&gt;D. Proposed Method With Multinetwork Structures&lt;/h3&gt;
&lt;div class=&#34;mermaid&#34;&gt;%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;basis&#39; } } }%%
flowchart LR
subgraph in[&#34;input feature&#34;]
x1((x1)) &amp; xe((&#34;â‹®&#34;)) &amp; xn((xn))
end
subgraph net1[&#34;Layer 1&#34;]
l11((a,b)) &amp; l1e((&#34;â‹®&#34;)) &amp; l1L((a,b))
end
subgraph net2[&#34;Layer 2&#34;]
l21((a,b)) &amp; l2e((&#34;â‹®&#34;)) &amp; l2L((a,b))
end
subgraph netC[&#34;Layer C&#34;]
lC1((a,b)) &amp; lCe((&#34;â‹®&#34;)) &amp; lCL((a,b))
end
x1 &amp; xn --&gt; l11 &amp; l1L
l11 &amp; l1L --&gt; l21 &amp; l2L
l21 &amp; l2L -.-&gt; lC1 &amp; lCL

subgraph out[&#34;output y&#34;]
direction LR
y1((1)) &amp; ye((&#34;â‹®&#34;)) &amp; ym((m))
end
subgraph belm1[&#34;Basic ELM 1&#34;]
direction LR
b11((&#34;aâ‚•,bâ‚•&#34;)) &amp; b1e((&#34;â‹®&#34;)) &amp; b1m((aâ‚•,bâ‚•))
end
subgraph belm2[&#34;Basic ELM 2&#34;]
direction LR
b21((aâ‚•,bâ‚•)) &amp; b2e((&#34;â‹®&#34;)) &amp; b2m((aâ‚•,bâ‚•))
end

net1 --&gt; feat1[&#34;Feature\n data\n ğ‡Â¹&lt;sub&gt;f&lt;/sub&gt;&#34;] --&gt; belm1 --&gt; out
feat1 --&gt; net2 --&gt; feat2[&#34;Feature\n data\n ğ‡Â²&lt;sub&gt;f&lt;/sub&gt;&#34;] --&gt; belm2 --&gt; out
netC --&gt; featC[&#34;Feature\n data\n ğ‡á¶œ&lt;sub&gt;f&lt;/sub&gt;&#34;]

subgraph MultiLayer ELM
in &amp; net1 &amp; net2 &amp; netC
end
%% inverse for initialization weights
linkStyle 12,13,14,16,17,18 stroke:#f0f
&lt;/div&gt;

&lt;p&gt;&lt;font color=&#34;#f0f&#34;&gt;Pink links&lt;/font&gt; will do inverse to calculate the weights for corresponding layers.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>images</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/img/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/img/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
