<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Subnetwork nodes on Zichen Wang</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/</link>
        <description>Recent content in Subnetwork nodes on Zichen Wang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 15 Mar 2023 20:13:00 -0500</lastBuildDate><atom:link href="https://zichen34.github.io/writenotes/model/subnetwork/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>read: Wi-HSNN for dimension reduction</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-batch-train/</link>
        <pubDate>Wed, 15 Mar 2023 20:13:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-batch-train/</guid>
        <description>&lt;p&gt;Authors: Wandong Zhang, Jonathan Wu, Yimin Yang &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0925231220311255&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neurocomputing (2020-07-18)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Wide hierarchical subnetwork-based neural network (Wi-HSNN)&lt;/li&gt;
&lt;li&gt;Iterative training by adding subnetnetwork nodes into modle one-by-one&lt;/li&gt;
&lt;li&gt;batch-by-batch training instead of processing the entire dataset (one-batch)
&lt;ul&gt;
&lt;li&gt;Place365 has 1.8 million samples&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h2&gt;
&lt;h2 id=&#34;3-proposed-wi-hsnn&#34;&gt;3 Proposed Wi-HSNN&lt;/h2&gt;
&lt;p&gt;Loss function of SLFN is MSE: min J= ½ ‖𝐓 - g(𝐗, 𝐰₁, 𝐛)⋅𝐰₂‖²&lt;/p&gt;
&lt;p&gt;The output weights can be initialized from the Target data (letting 𝐇 denote g(𝐗, 𝐰₁, 𝐛)):&lt;/p&gt;
&lt;p&gt;𝐰₂ = ( 𝐇ᵀ𝐇 + I/C )⁻¹ 𝐇ᵀ 𝐓&lt;/p&gt;
&lt;p&gt;In this paper, the input data 𝐗 is first transformed to 𝐅ₑₙ,
which then passes a SNN (SLFN, subnetwork node) and becomes g(𝐅ₑₙ⋅𝐚ₑₓ + bₑₓ).
Next, the input 𝐗 is fed into multiple SNN sequentially.
And their outputs are accumulated with the weight 𝐚ₜ.&lt;/p&gt;
&lt;p&gt;Therefore, if there are L SNNs, the loss function for this problem is:&lt;/p&gt;
&lt;p&gt;min J = ½ ‖𝐓 - ∑ᵢ₌₁ᴸ g(𝐅ₑₙⁱ⋅𝐚ₑₓⁱ + bₑₓⁱ) ⋅ 𝐚ₜᴸ‖²&lt;/p&gt;
&lt;h3 id=&#34;32-training-the-wi-hsnn&#34;&gt;3.2 Training the Wi-HSNN&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Feedforward with randomly initialized (𝐚ₑₙ¹, bₑₙ¹) and (𝐚ₑₓ¹, bₑₓ¹)
and calculate the optimal output weights based on pseudo-inverse:&lt;/p&gt;
&lt;p&gt;𝐅ₑₙ¹ = g(𝐗⋅𝐚ₑₙ¹ + bₑₙ¹) &lt;br&gt;
𝐅ₜ¹ = 𝐅ₑₓ¹ = g(𝐅ₑₙ¹ ⋅𝐚ₑₓ¹ + bₑₓ¹)    &lt;br&gt;
𝐚ₜ¹ = (𝐅ₜᵀ𝐅ₜ + I/C)⁻¹ 𝐅ₜᵀ 𝐓&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Obtain the feedback error (&amp;ldquo;feature H&amp;rdquo;) matrix 𝐏ₑₓ¹ and 𝐏ₑₙ¹
for solving the weights 𝐚ₑₓ (=𝐰₂), 𝐚ₑₙ (=𝐰₁) of next iteration.&lt;/p&gt;
&lt;p&gt;𝐏ₑₓ¹ = g⁻¹ (𝐞¹ ⋅ (I/C + (𝐚ₜ¹)ᵀ⋅𝐚ₜ¹)⁻¹ ⋅ (𝐚ₜ¹)ᵀ ) &lt;br&gt;
𝐏ₑₙ¹ = g⁻¹ (𝐏ₑₓ¹ ⋅ (I/C + (𝐚ₑₓ¹)ᵀ ⋅ 𝐚ₑₓ¹ )⁻¹ ⋅ (𝐚ₑₓ¹)ᵀ )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the 𝐚ₑₓ, 𝐚ₑₙ for next SNN:&lt;/p&gt;
&lt;p&gt;𝐚ₑₙⁱ = (𝐗ᵀ𝐗 + I/C )⁻¹ 𝐗ᵀ 𝐏ₑₓⁱ⁻¹  (Entrance layer weights) &lt;br&gt;
𝐚ₑₓⁱ = ( (𝐅ₑₙⁱ)ᵀ𝐅ₑₙⁱ + I/C )⁻¹ (𝐅ₑₙⁱ)ᵀ 𝐏ₑₙⁱ⁻¹    (Exit layer weights)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Summarize outputs 𝐅ₜⁱ from all exisiting SNN, and update output weight 𝐚ₜⁱ.&lt;/p&gt;
&lt;p&gt;𝐅ₜⁱ = ∑ₖ₌₁ⁱ 𝐅ₑₓᵏ     &lt;br&gt;
𝐚ₜⁱ = ( (𝐅ₜⁱ)ᵀ𝐅ₜⁱ + I/C )⁻¹ (𝐅ₜⁱ)ᵀ 𝐓&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Obtain the feedback error 𝐏ₑₓⁱ and 𝐏ₑₙⁱ:&lt;/p&gt;
&lt;p&gt;𝐏ₑₓⁱ = g⁻¹ (𝐞ⁱ ⋅ (I/C + (𝐚ₜⁱ)ᵀ⋅𝐚ₜⁱ)⁻¹ ⋅ (𝐚ₜⁱ)ᵀ ) &lt;br&gt;
𝐏ₑₙⁱ = g⁻¹ (𝐏ₑₓⁱ ⋅ (I/C + (𝐚ₑₓⁱ)ᵀ ⋅ 𝐚ₑₓⁱ )⁻¹ ⋅ (𝐚ₑₓⁱ)ᵀ )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat step 3 to step 5 L-2 times.
𝐅ₜᴸ is the final encoding. And 𝐘 = 𝐅ₜᴸ 𝐚ₜᴸ is the final classification prediction.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;34-batch-by-batch-scheme-with-parallelism-strategy&#34;&gt;3.4 Batch-by-batch scheme with parallelism strategy&lt;/h3&gt;
&lt;p&gt;The entire feature set 𝐅ᴺᕽᵈ (i.e., 𝐇) and the target set 𝐓 are split into p subsets:&lt;/p&gt;
&lt;p&gt;𝐇 = $[^{𝐇(𝐱₁)}_{^{&amp;hellip;}_{𝐇(𝐱ₚ)}}]$, 𝐓 = $[^{𝐓(𝐱₁)}_{^{&amp;hellip;}_{𝐓(𝐱ₚ)}}]$&lt;/p&gt;
&lt;p&gt;The desired weights matrix 𝐰₂ (i.e., &amp;ldquo;𝛃&amp;rdquo;) represented weith pseudo-inverse matrix becomes&lt;/p&gt;
&lt;p&gt;𝐰₂ = (𝐇ᵀ𝐇 + I/C)⁻¹𝐅ᵀ⋅𝐓&lt;br&gt;
𝐰₂ = ([𝐇₁ᵀ,&amp;hellip;, 𝐇ₚᵀ] $[^{_{𝐇(𝐱₁)}} _{^{&amp;hellip;}_{𝐇(𝐱ₚ)}}] + ^I_{^-_C}$)⁻¹
[𝐇₁ᵀ,&amp;hellip;, 𝐇ₚᵀ]
$[ ^{_{𝐓(𝐱₁)}}_{^{&amp;hellip;}_{𝐓(𝐱ₚ)}} ]$&lt;br&gt;
𝐰₂ = ([𝐇(𝐱₁)ᵀ𝐇(𝐱₁) + &amp;hellip; + 𝐇(𝐱ₚ)ᵀ𝐇(𝐱ₚ)] + I/C)⁻¹ ⋅ [𝐇(𝐱₁)ᵀ𝐓(𝐱₁) + &amp;hellip; + 𝐇(𝐱ₚ)ᵀ𝐓(𝐱ₚ) ] &lt;br&gt;
𝐰₂ = (∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐇(𝐱ᵢ) + I/C)⁻¹ ⋅ ∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐓(𝐱ᵢ)&lt;/p&gt;
&lt;p&gt;First, calculate (∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐇(𝐱ᵢ) + I/C)⁻¹.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The matrices are accumulated batch by batch in the code.
After the 1st iteration, K=(𝐇₁ᵀ𝐇₁ + I/C)⁻¹ has obtained and returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When next batch 𝐇₂ is retrieved:&lt;/p&gt;
&lt;p&gt;K_new = (𝐇₂ᵀ𝐇₂ + 𝐇₁ᵀ𝐇₁ + I/C)⁻¹ &lt;br&gt;
= (𝐇₂ᵀ𝐇₂ + K⁻¹)⁻¹  # analogy to (UBV + A)⁻¹, where U=𝐇₂ᵀ, V=𝐇₂, B=I, A=K⁻¹ &lt;br&gt;
Woodbury: &amp;ldquo;A⁻¹ - A⁻¹⋅U⋅(I+BV⋅A⁻¹⋅U)⁻¹ BV A⁻¹ &amp;quot; &lt;a class=&#34;link&#34; href=&#34;#%e5%91%a8%e5%bf%97%e6%88%90&#34; &gt;1&lt;/a&gt;   &lt;br&gt;
= K - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ K  &lt;br&gt;
= (I - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ ) ⋅ K &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let Kₚ = (I - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ ).
So K_new = Kₚ ⋅ K&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then, for the second item ∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐓(𝐱ᵢ),&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In the first batch, 𝛃=K⋅𝐇₁ᵀ⋅𝐓₁ is obtained and returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When the second batch coming,
there should be (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂)&lt;/p&gt;
&lt;p&gt;𝛃_new = K_new ⋅ (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂) &lt;br&gt;
= K_new ⋅ (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂)     &lt;br&gt;
= Kₚ ⋅ K ⋅ 𝐇₁ᵀ⋅𝐓₁ + K_new ⋅ 𝐇₂ᵀ⋅𝐓₂  &lt;br&gt;
= Kₚ ⋅ 𝛃 + K_new ⋅ 𝐇₂ᵀ⋅𝐓₂&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;矩阵之和的逆&#34;&gt;矩阵之和的逆&lt;/h3&gt;
&lt;p&gt;(DDG search: &amp;ldquo;矩阵之和的逆&amp;rdquo;)
&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_33866593/article/details/103035289&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;两个矩阵相加后求逆 - ~海棠依旧~ - CSDN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.docin.com/p-713940771.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;关于两个矩阵之和逆阵的讨论 - docin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Google search: &amp;ldquo;两矩阵和的逆&amp;rdquo;)
&lt;a class=&#34;link&#34; href=&#34;https://ccjou.wordpress.com/2009/03/17/%E4%BA%8C%E7%9F%A9%E9%99%A3%E5%92%8C%E7%9A%84%E9%80%86%E7%9F%A9%E9%99%A3/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;兩矩陣和的逆矩陣 - 線代啟示錄 - 周志成(阳明交大)&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;矩阵之和的逆-不等于-逆矩阵的和&#34;&gt;矩阵之和的逆 不等于 逆矩阵的和&lt;/h3&gt;
&lt;p&gt;(A+B)⁻¹ ≠ A⁻¹ + B⁻¹&lt;/p&gt;
&lt;p&gt;∵ (A+B)(A⁻¹ + B⁻¹) = E + BA⁻¹ + AB⁻¹ + E ≠ E&lt;/p&gt;
&lt;p&gt;最简单的例子：取 A=B=E，(A+B)(A⁻¹ + B⁻¹) = E + BA⁻¹ + AB⁻¹ + E = 4E ≠ E
&lt;a class=&#34;link&#34; href=&#34;http://zhidao.baidu.com/question/541608921/answer/1368194722&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;矩阵和的逆矩阵 逆矩阵的和 相等吗 -百度知道&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ref&#34;&gt;Ref&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;div id=&#34;周志成&#34;&gt;&lt;a href=&#34;https://ccjou.wordpress.com/2009/03/17/%E4%BA%8C%E7%9F%A9%E9%99%A3%E5%92%8C%E7%9A%84%E9%80%86%E7%9F%A9%E9%99%A3/&#34;&gt;兩矩陣和的逆矩陣 - 線代啟示錄 - 周志成(阳明交大)&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>read: Width-Growth Model with Subnetwork Nodes</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-refine-weights/</link>
        <pubDate>Wed, 15 Feb 2023 12:40:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-refine-weights/</guid>
        <description>&lt;p&gt;Authors: Wandong Zhang et. al. Publish date: 2020-03-30 (Finished in 2019)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/9050859&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IEEE Trans. Industrial Informatics&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/u/0/d/1buXSzjPvb-d56kPDDqKaccDxzkBBw87R/view&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;G.Drive&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=O0LaYK8AAAAJ&amp;amp;citation_for_view=O0LaYK8AAAAJ:u5HHmVD_uO8C&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;G.Scholar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Try to summary (2023-02-26):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Different features are concatenated and then fed into a &amp;ldquo;I-ELM with subnetwork nodes&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;What is optimized it the combination weights, but the feature vectors themselves are not changed.&lt;/li&gt;
&lt;li&gt;It is the weights (IW, 𝛃) are refined.&lt;/li&gt;
&lt;li&gt;Sepecifically, the new R-SNN node is improved by adding a part of unlearned wights accquired from the residual error of the last node.
That is the weights are accumulated on the newest node. So the ultimate R-SNN node contains all the previous training outcomes.
What we kept is only the last R-SNN node, i.e., a SLFN.&lt;/li&gt;
&lt;li&gt;Does that require the performance of the final R-SNN is the best among the former nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(In code) The update process of a SLFN is as follows:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
subgraph node1[SLFN1]
h1((h1)) &amp; h2((h2)) &amp; he((he)) &amp; hd((hd))
end

In[&#34;X\n data\n matrix&#34;] --&gt; IW1 --&gt; h1 &amp; h2 &amp; he &amp; hd 
node1 --- beta1(&#34;𝛃1&#34;) --&gt; Yout1 --&gt; Error1
Target --&gt;|&#34;pinv&#34;| beta1
beta1 &amp; Error1 --&gt;|&#34;inv: 𝛃⋅P=𝐞₁&#34;| P[&#34;P\n ( The H\n yielding\n Error1) &#34;]
P &amp; In --&gt; IW&#39;[&#34;residual\n IW&#34;]
IW&#39; &amp; IW1 --&gt;sum((+)) --&gt; IW2

subgraph node2[SLFN2]
h21((h1)) &amp; h22((h2)) &amp; h2e((he)) &amp; h2d((hd))
end

IW2 --&gt; h21 &amp; h22 &amp; h2e &amp; h2d 
node2 --- beta2(&#34;𝛃2&#34;)
Target --&gt; |&#34;pinv&#34;| beta2
beta2 --&gt; Yout2
&lt;/div&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A supervised multi-layer subnetwork-based feature refinement and classification model for representation learning.&lt;/li&gt;
&lt;li&gt;Expand the width for a generalized hidden layer rather than stack more layers to go deeper&lt;/li&gt;
&lt;li&gt;One-shot solution for finding the meaningful latent space to recognize the objects rather than searching separate spaces to find a generalized feature space.&lt;/li&gt;
&lt;li&gt;Multimodal fusion fusing various feature sources into a superstate encoding instead of a unimodal feature coding in the traditional feature representation methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-introduction&#34;&gt;Ⅰ. Introduction&lt;/h2&gt;
&lt;p&gt;(Task &amp;amp; Application &amp;amp; List of ralated research field &amp;amp; Problem &amp;amp; Existing solutions brif)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Task: high-dimensional data processing and learning&lt;/li&gt;
&lt;li&gt;Problem definition: selecting the optimal feature descriptors&lt;/li&gt;
&lt;li&gt;2 branch of solutions: hand-crafted descriptors and deep-learning-based features.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Criticize the former feature extraction solutions and introduce proposed method:)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Features derived from approaches of those 2 categories are too inflexible to contribute a robust model.&lt;/li&gt;
&lt;li&gt;This method &amp;ldquo;encodes and refines these? raw features from multiple sources to improve the classification performance&amp;rdquo;. &lt;br&gt;
For example: 4 extracted features (from AlexNet, ResNet, HMP, and SPF) are concatenated into 1 vector taken as the input to a &amp;ldquo;3-layer&amp;rdquo; model,
where only a single &amp;ldquo;genearlized&amp;rdquo; hidden layer (latent space) bridges the raw feature space (transformation ax+b) and the final target space (residual error).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Recap deep learning models and mention the theory base of this work)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep networks often get &amp;ldquo;trapped in local minimum and are sensitive to the learning rate&amp;rdquo; because their training fundation is BP.&lt;/li&gt;
&lt;li&gt;Regression-based feature learning. Least-squares representation learning methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Problems to be solved) &lt;br&gt;
Drawbacks of regression-based approaches:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;block&amp;rdquo; models? don&amp;rsquo;t perform one-shot training philosophy based on the relation between raw data and the target.&lt;/li&gt;
&lt;li&gt;A model trained by some &amp;ldquo;designed&amp;rdquo; process has a inferious generalizatio n capacity than the model derived from one-shot training strategy (least-squares).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Drawbacks of multilayer neural networks &amp;amp; solution &lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deeper layer-stacked models suffer from overfitting with limited training samples.&lt;/li&gt;
&lt;li&gt;Network-in-network structure enhances the network&amp;rsquo;s generalization capacity for learning feature. &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/&#34; &gt;ELM with subnetwork nodes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Contributions:
&lt;ul&gt;
&lt;li&gt;Subnetwork neural nodes (SNN) realized multilayer representation learning. Unlike the ensembled network, the SNN is trained based on the error term.&lt;/li&gt;
&lt;li&gt;Feature space transformation and the classification are solved together by searching iteratively the optimal encoding space (hidden layer).&lt;/li&gt;
&lt;li&gt;Concatenation of multiple features result more discriminative representations for samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;-literature-review&#34;&gt;Ⅱ. Literature review&lt;/h2&gt;
&lt;h3 id=&#34;a-conventional-feature-coding&#34;&gt;A. Conventional Feature Coding&lt;/h3&gt;
&lt;p&gt;&amp;quot; Supervised method of learning representaiton evaluates the importance of a specific feature through the correlation between features and categories.&amp;quot;&lt;/p&gt;
&lt;p&gt;Conventional feature coding of images depends on prior knowledge of the problem. Thus, the features are not complete representations.&lt;/p&gt;
&lt;p&gt;This paper enhances the feature by fusing (discriminative) hand-crafted features and (class-specific) CNN-based features.&lt;/p&gt;
&lt;h3 id=&#34;b-least-squares-encoding-methods&#34;&gt;B. Least-Squares Encoding Methods&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The least-squares approximation methods, such as random forest and &lt;strong&gt;alternating minimization&lt;/strong&gt;, have been exhaustively investigated in single-layer neural networks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related works: Moore-Penrose inverse; Universal approximation capacity of I-ELM, ELM autoe-ncoder&lt;a class=&#34;link&#34; href=&#34;#ELM-AE&#34; &gt;14&lt;/a&gt;, Features combined with subnetwork nodes &lt;a class=&#34;link&#34; href=&#34;#2lyrSubnet&#34; &gt;18&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Each SNN is applied as a &lt;strong&gt;local feature descriptor&lt;/strong&gt;.
Hence, the subspace features can be extracted? from the original data independently,
and the useful features are generated via the &lt;strong&gt;combination&lt;/strong&gt; of these features.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;-proposed-method&#34;&gt;Ⅲ. Proposed Method&lt;/h2&gt;
&lt;h3 id=&#34;a-algorithmic-summary&#34;&gt;A. Algorithmic Summary&lt;/h3&gt;
&lt;p&gt;Two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;: concatenate various feature vectors into a single &amp;ldquo;supervector&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Train&lt;/strong&gt; the width-growth model: &lt;br&gt;
Terminology:
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;layer&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;th&gt;marker&lt;/th&gt;
&lt;th&gt;params&lt;/th&gt;
&lt;th&gt;in&lt;/th&gt;
&lt;th&gt;out&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;input&lt;/td&gt;
&lt;td&gt;Entrance (feature) layer&lt;/td&gt;
&lt;td&gt;𝑓&lt;/td&gt;
&lt;td&gt;𝐖ᵢᶠ, 𝐛ᵢᶠ random&lt;/td&gt;
&lt;td&gt;vct&lt;/td&gt;
&lt;td&gt;linear combination 𝐇&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hidden&lt;/td&gt;
&lt;td&gt;Refinement layer/subspace&lt;/td&gt;
&lt;td&gt;𝑟&lt;/td&gt;
&lt;td&gt;𝐖ᵢʳ, 𝐛ᵢʳ (𝐚,b)&lt;/td&gt;
&lt;td&gt;𝐇&lt;/td&gt;
&lt;td&gt;partial feature Ψ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;Least square learning layr&lt;/td&gt;
&lt;td&gt;𝑣&lt;/td&gt;
&lt;td&gt;𝐖ᵢᵛ (𝛃)&lt;/td&gt;
&lt;td&gt;Ψ&lt;/td&gt;
&lt;td&gt;sum up all partial features: 𝚪 &lt;br&gt; residual error 𝐞&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(An entrance layer and a refinement layer both are &amp;ldquo;SNN&amp;rdquo;, and their combination is a &amp;ldquo;R-SNN&amp;rdquo;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Initialization: For the 1st R-SNN, 𝐖₁ᶠ, 𝐖₁ʳ are random generating a false feature Ψ. &lt;br&gt;
Then the first least-square method (pseudoinverse) is performed to calculate 𝐖₁ᵛ based on target 𝐘 and Ψ.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iteratively add the R-SNN (2≤ i≤ L) (refinement subspace) into the hidden layer (optimal feature space)&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart TB
subgraph In[input feature]
x1((1)) &amp; x2((2)) &amp; xe((&#34;⋮&#34;)) &amp; xn((n))
end

EnW(&#34;Entrance layer\n 𝐖ᵢᶠ, 𝐛ᵢᶠ\n random&#34;)

subgraph H[&#34;entrance feature 𝐇&#34;]
h1((1)) &amp; h2((2)) &amp; he((&#34;⋮&#34;)) &amp; hD((D))
end

RefineW(&#34;Refinement layer\n 𝐖ᵢʳ, 𝐛ᵢʳ&#34;)

subgraph Psi[partial feature Ψ]
Ψ1((1)) &amp; Ψ2((2)) &amp; Ψe((&#34;⋮&#34;)) &amp; Ψd((d))
end

OW(&#34;Output layer\n 𝐖ᵢᵛ&#34;)

subgraph Out[&#34;Output vector&#34;]
o1((1)) &amp; o2((2)) &amp; oe((&#34;⋮&#34;)) &amp; om((m))
end

x1 &amp; x2 &amp; xe &amp; xn --&gt; EnW --&gt; h1 &amp; h2 &amp; he &amp; hD 
    --&gt; RefineW --&gt; Ψ1 &amp; Ψ2 &amp; Ψe &amp; Ψd 
    --&gt; OW --&gt; o1 &amp; o2 &amp; oe &amp; om

Out --&gt;|&#34;- 𝐞ᵢ₋₁&#34;| erri[&#34;𝐞ᵢ&#34;]

erri &amp; OW -.-&gt;|pinv| newΨ(&#34;𝐏 \n yielding\n 𝐞ᵢ&#34;)

subgraph H1[&#34;entrace feature 𝐇ᵢ₊₁&#34;]
h11((1)) &amp; h12((2)) &amp; h1e((&#34;⋮&#34;)) &amp; h1D((D))
end

In --&gt; EnW1(&#34;Entrance layer\n 𝐖ᵢ₊₁ᶠ, 𝐛ᵢ₊₁ᶠ\n random&#34;)
   --&gt; h11 &amp; h12 &amp; h1e &amp; h1D

H1 --&gt; RefineW1(&#34;Refinement layer\n 𝐖ᵢ₊₁ʳ, 𝐛ᵢ₊₁ʳ&#34;)
   %%-.-|solved by P| newΨ 
newΨ -.-&gt; RefineW1

subgraph Psi1[partial feature Ψ]
Ψ11((1)) &amp; Ψ12((2)) &amp; Ψ1e((&#34;⋮&#34;)) &amp; Ψ1d((d))
end

RefineW1 --&gt; Ψ11 &amp; Ψ12 &amp; Ψ1e &amp; Ψ1d --&gt; OW1(&#34;Output layer\n 𝐖ᵢ₊₁ᵛ&#34;)

%%OW1 -.-|solved by| erri
erri -.-&gt; OW1

subgraph Out1[&#34;Output vector&#34;]
o11((1)) &amp; o12((2)) &amp; o1e((&#34;⋮&#34;)) &amp; o1m((m))
end

OW1 --&gt; o11 &amp; o12 &amp; o1e &amp; o1m 
Out1 --&gt;|&#34;- 𝐞ᵢ&#34;| erri+1[&#34;𝐞ᵢ₊₁&#34;] --&gt; newP
&lt;/div&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;b-model-definition&#34;&gt;B. Model Definition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SLFN solves the regression problem can be expressed as:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MLNN has nested transformation:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Proposed method is a generlized SLFN:&lt;/p&gt;
&lt;p&gt;minimize J = ½ ‖𝐘-f(𝐇ᵢᶠ, 𝐖ᵢʳ, 𝐛ᵢʳ)⋅𝐖_Lᵛ‖²,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;f(𝐇ᵢᶠ, 𝐖ᵢʳ, 𝐛ᵢʳ) = ∑ᵢ₌₁ᴸ g(𝐇ᵢᶠ ⋅ 𝐖ᵢʳ + 𝐛ᵢʳ): sum all R-SNN&lt;/li&gt;
&lt;li&gt;𝐇ᵢᶠ = g(𝐖ᵢᶠ, 𝐛ᵢᶠ, 𝐗)&lt;/li&gt;
&lt;li&gt;𝐘 ∈ ℝᴺᕽᵐ: expected output, target feature&lt;/li&gt;
&lt;li&gt;𝐗 ∈ ℝᴺᕽⁿ: input matrix&lt;/li&gt;
&lt;li&gt;L : number of R-SNN node&lt;/li&gt;
&lt;li&gt;g : activateion function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3 differences from other least-squares-based MLNNs&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;SNN combines each dimension of the feature vector serving as local feature descriptor.
While the R-SNN is the basic unit to refine feature vectors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimal feature is the aggregation of R-SNN added one by one.
R-SNN is densly connected to input vector and output layer containing twice linear projection.
Different R-SNNs are independent because they learn from different error.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The latent space is the aggregation of all R-SNN nodes subspace.
So the parameters training has no block-wise communication between different spaces.
That means the feature refinement and classification are doen together.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c-proposed-width-growth-model&#34;&gt;C. Proposed Width-Growth Model&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Input weights and bias 𝐖ᵢᶠ, 𝐛ᵢᶠ: randomly initialized; &lt;br&gt;
Entrance feature: 𝐇ᵢᶠ = g(𝐗𝐖ᵢᶠ+ 𝐛ᵢᶠ); &lt;br&gt;
Refined partial feature: Ψᵢ=g(𝐇ᵢᶠ𝐖ᵢʳ+ 𝐛ᵢʳ), where 𝐛ᵢʳ is random; &lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output weights: 𝐖ᵢᵛ=(𝐈/C + 𝚪ᵀ𝚪)⁻¹𝚪ᵀ⋅𝐘, &lt;br&gt;
where C is hyperparameter for regularization,
and (𝐈/C + 𝚪ᵀ𝚪)⁻¹𝚪ᵀ is the pseudoinverse of output vector 𝚪 (label?) &lt;br&gt;
Error: 𝐞ᵢ = 𝐘 - 𝐖ᵢᵛ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐏 is the desired matrix generating 𝐞ᵢ by: 𝐏ᵢ⋅𝐖ᵢᵛ=𝐞ᵢ, so &lt;br&gt;
𝐏ᵢ = 𝐞ᵢ⋅(I/C + (𝐖ᵢᵛ)ᵀ𝐖ᵢᵛ)⁻¹(𝐖ᵢᵛ)ᵀ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Refinement layer weights of next R-SNN:  &lt;br&gt;
𝐖ᵢ₊₁ʳ = (I/C +𝐇ᵢᵀ𝐇ᵢ)⁻¹𝐇ᵢᵀ ⋅ g⁻¹(𝐏ᵢ),     &lt;br&gt;
because g(𝐇ᵢ₊₁⋅𝐖ᵢ₊₁ʳ+ 𝐛ᵢ₊₁ʳ) = 𝐏ᵢ. &lt;br&gt;
Next partial feature: Ψᵢ₊₁ = g(𝐇ᵢ₊₁⋅𝐖ᵢ₊₁ʳ+ 𝐛ᵢ₊₁ʳ)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accumulate the partial feature to the optimal feature:
𝚪ᵢ₊₁ = 𝚪ᵢ + Ψᵢ₊₁  &lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update error: 𝐞ᵢ₊₁ = 𝐞ᵢ-𝐖ᵢᵛ𝚪ᵢ&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Repeat steps 4-6 L-2 times, and the final feature 𝚪$_L$ is the generalized feature correponding to the best output parameter 𝐖 $_Lᵛ$ for classification.&lt;/p&gt;
&lt;h2 id=&#34;ref&#34;&gt;Ref&lt;/h2&gt;
&lt;ol start=&#34;14&#34;&gt;
&lt;li&gt;
&lt;div id=&#34;ELM-AE&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/6733226&#34;&gt;L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, “Representational learning with extreme learning machine for big data,” IEEE Intell. Syst.,Dec. 2013.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;2lyrSubnet&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8627989&#34;&gt; Y. Yang and Q. J. Wu, “Features combined from hundreds of midlayers: Hierarchical networks with subnetwork nodes,” IEEE Trans. Neural Netw. Learn. Syst., Nov. 2019.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;ELM-MLP&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7103337&#34;&gt;J. Tang, C. Deng, and G.-B. Huang, “Extreme learning machine for multilayer perceptron,” IEEE Trans. Neural Netw. Learn. Syst., Apr. 2016.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>read: B-ELM</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-bidirec/</link>
        <pubDate>Fri, 10 Feb 2023 19:29:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-bidirec/</guid>
        <description>&lt;p&gt;Authors: Yimin Yang; Yaonan Wang; Xiaofang Yuan&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.yiminyang.com/code.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/6222007&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;TNNLS (2012-06-20)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Try to summary: &lt;br&gt;
(2023-02-15) B-ELM is a variant of I-ELM by dividing the hidden nodes into 2 types: odd and even,
which differ in wehther the input parameters (𝐚,b) is &lt;strong&gt;randomly&lt;/strong&gt; generated or &lt;strong&gt;calculated&lt;/strong&gt; by twice inverse operations.&lt;/p&gt;
&lt;p&gt;Specifically, the first node is solved from the target 𝐭. Then for subsequent nodes, the even node is solved based on the residual error of its previous odd node.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The output weights of the odd nodes are calculated as: 𝐇₂ₙ₋₁ʳ + e₂ₙ₋₂ (or 𝐭) ➔ 𝛃₂ₙ₋₁ (➔e₂ₙ₋₁)&lt;/li&gt;
&lt;li&gt;The input weights 𝐚 and bias b of even nodes are solved based on the residual error: e₂ₙ₋₁ ➔ 𝐇₂ₙᵉ ➔ 𝐚,b (➔ ^𝐇₂ₙᵉ ➔ 𝛃₂ₙ ➔ e₂ₙ)&lt;/li&gt;
&lt;li&gt;Note: the superscript r and e stand for &amp;ldquo;random&amp;rdquo; and &amp;ldquo;error&amp;rdquo; marking the source of H.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This algorithm tends to reduce network output error to 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;by solving the input weights 𝐚 and bias b based on the network residual error.
(In other words, the residual error is represented by a, b of subsequent nodes.
Or the error is absorbed by others parameters besides 𝛃.)&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/B-ELM_goal.png width=&gt;
  
  


&lt;h2 id=&#34;-introduction&#34;&gt;Ⅰ. Introduction&lt;/h2&gt;
&lt;p&gt;For ELM with a fixed structure, the best number of hidden nodes need to ffound by trial-and-error,
because the residual error is not always decreasing when there are more hidden nodes in an ELM.&lt;/p&gt;
&lt;p&gt;For incremental ELM, the hidden node is added one by one, so the residual error keeps decreasing.
But the model training has to do multiple iterations, i.e., calculating inverse matrix is needed after adding each node.&lt;/p&gt;
&lt;p&gt;Compared with other incremental ELM, this method is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Faster and with fewer hidden nodes&lt;/li&gt;
&lt;li&gt;showing the relationship between the network output residual error 𝐞 and output weights 𝛃, which is named &amp;ldquo;error-output weights ellipse&amp;rdquo;&lt;/li&gt;
&lt;li&gt;The hidden layer (input weights) with determined parameters instead of random numbers would make the error reduce, or improve the accuracy.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;-preliminaries-and-notation&#34;&gt;Ⅱ. Preliminaries and Notation&lt;/h2&gt;
&lt;h3 id=&#34;a-notations-and-definitions&#34;&gt;A. Notations and Definitions&lt;/h3&gt;
&lt;p&gt;⟨u,v⟩ = ∫ₓu(𝐱)v‾(𝐱)d𝐱 is the &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Frobenius_inner_product&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Frobenius inner product&lt;/a&gt; of two matrices u,v,
where the overline denotes the complex conjugate.&lt;/p&gt;
&lt;h3 id=&#34;b-i-elm&#34;&gt;B. I-ELM&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;(proved by Huang&lt;a class=&#34;link&#34; href=&#34;#univrsl&#34; &gt;⁸&lt;/a&gt;): indicated that (For the incremental ELM,) the target function can be approximated with more nodes added into the network by reducing the residual error to 0,
as the 𝛃 of each new node is calculated based on the error of the network last status eₙ₋₁ as:&lt;/p&gt;
&lt;p&gt;𝛃 = $\frac{⟨eₙ₋₁, 𝐇ₙʳ⟩}{‖𝐇ₙʳ‖²}$&lt;/p&gt;
&lt;p&gt;The numerator is the inner product measuring the distance from the nth (random) hidden node 𝐇ₙʳ to be added and the error of the network with n-1 hidden nodes.&lt;/p&gt;
&lt;p&gt;So the output of the newly added hidden nodes 𝐇ₙʳ are getting smaller and smaller, because they are learning something from the residual error.&lt;/p&gt;
&lt;h2 id=&#34;-proposed-bidirectional-elm-method&#34;&gt;Ⅲ. Proposed Bidirectional ELM Method&lt;/h2&gt;
&lt;figure&gt;&lt;img src=&#34;https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/5962385/6256723/6222007/6222007-fig-2-source-large.gif&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;a-structure-of-the-proposed-bidirectional-elm-method&#34;&gt;A. Structure of the Proposed Bidirectional ELM Method&lt;/h2&gt;
&lt;p&gt;Two types of node, the node with &lt;strong&gt;odd index&lt;/strong&gt; {2n-1} has random 𝐚,b,
while the 𝐚,b of the node with &lt;strong&gt;even index&lt;/strong&gt; {2n} are calculated based on the residual error of the network with an odd number of nodes at the last moment.&lt;/p&gt;
&lt;p&gt;Their output weights both are calculated based on Lemma 1. The 𝐇 of the even node is from residual error, not from the random a,b.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;del&gt;The odd node aims to approximate the target through 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁, where 𝐇₂ₙ₋₁ is yield based on random a,b;&lt;/del&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The odd node 2n-1 approximates the previous residual error with random generated a,b;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But the even node approximates the residual error 𝐞₂ₙ₋₁ through 𝐇₂ₙᵉβ₂ₙ with calculated a,b,
where 𝐇₂ₙᵉ is yield with the weights a,b solved based on the residual error 𝐞₂ₙ₋₁ from the target (the job hasn&amp;rsquo;t done by all the previous nodes)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;del&gt;&lt;strong&gt;Bi-direction&lt;/strong&gt; means the approximation is learned from both the target and the error,
where the odd node (β₂ₙ₋₁) solved by the target, while the even node (β₂ₙ) calculated by the error.
So a pair of odd node and even node is a complete step toward to the target.&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bi-directional&lt;/strong&gt; means H₂ₙᵉ is calculated first from eq.(6); Then it is calculated again using the ^a,^b, which are solved based on H₂ₙᵉ, to get the updated ^H₂ₙᵉ, which is used to calculate the output weight for the next random odd node based on the Lemma 1.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
subgraph in[inputs]
x1 &amp; xe[&#34;⋮&#34;] &amp; xn
end

rand((&#34;random\n 𝐚,b&#34;)) 
calculated((&#34;solved\n 𝐚,b&#34;))

subgraph hid[hidden]
H1 &amp; he[&#34;⋮&#34;] &amp; h2n-1 &amp; h2n
end

x1 &amp; xe &amp; xn --&gt; rand --&gt; h2n-1[&#34;𝐇₂ₙ₋₁ʳ&#34;] ---|Lemma 1| β2n-1((&#34;β₂ₙ₋₁&#34;))--&gt; e2n-1[&#34;e₂ₙ₋₁&#34;]
x1 &amp; xe &amp; xn --&gt; calculated --&gt; h2n[&#34;𝐇₂ₙᵉ&#34;] ---|Lemma 1| β2n((&#34;β₂ₙ&#34;))--&gt; e2n[&#34;e₂ₙ&#34;]

h2n -.- |&#34;⬅ inv of\n β₂ₙ₋₁&#34;| e2n-1

calculated -.-|&#34;⬅ inv of\n 𝐱&#34;| h2n
&lt;/div&gt;

&lt;p&gt;The dot lines represent the inverse calculation. &lt;br&gt;
β₂ₙ₋₁ is derived from the network residual error of the last status.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Block diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart TB
init[Initialization: \n Given training set,\n expect accracy η and \n let #hidden nodes L=0] --&gt;
incre[L = L+1] --&gt; OdEv{&#34;Is L \n odd or even?&#34;}
OdEv --&gt;|L=2n+1| I-ELM --&gt; calcE[Calculate \n residual error E]
OdEv --&gt;|L=2n| Theorem2 --&gt; update[&#34;Update ^H_L = ^𝐚_L 𝐱 + ^b)\n and calculate the output weight \n β_L based on eq.(7)&#34;] --&gt; calcE

subgraph Theorem2
direction TB
calcH[&#34;Calculate output matrix \n H_L basd on eq.(6)&#34;] 
--&gt; calcab[&#34;Calculate hidden-node parameters \n (^𝐚_L,^b_L) based on eq.(14)&#34;]
end

subgraph I-ELM
direction TB
rand[&#34;Randomly assign hidden-node\n parameters (𝐚_L,b_L) \n and obtain\n output matrix H_L&#34;] 
--&gt; |Lemma 1| outW[&#34;Calculate the output weight \n βL according to eq.(8)&#34;]
end

calcE --&gt; thres{&#34;‖E‖&lt;η&#34;} --&gt; |Yes| END
%%thres ----&gt;|No| incre  %% mess up the chart
incre o---o|No| thres
&lt;/div&gt;

&lt;h3 id=&#34;b-bidirectional-elm-method&#34;&gt;B. Bidirectional ELM method&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt; states the target function 𝑓 is approximated by the existing network 𝑓₂ₙ₋₂ plus the last two nodes: 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁ and 𝐇₂ₙᵉβ₂ₙ, when n→∞. On the other hand, the residual error the network is 0:&lt;/p&gt;
&lt;p&gt;$lim_{n→∞}‖𝑓-(𝑓₂ₙ₋₂ + 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁ + 𝐇₂ₙᵉβ₂ₙ)‖ = 0$&lt;/p&gt;
&lt;p&gt;where the sequence of the 𝐇₂ₙᵉ (the output of the even node calculated from the feedback error) is determined by the inverse of last output weight β₂ₙ₋₁:&lt;/p&gt;
&lt;p&gt;$𝐇₂ₙᵉ = e₂ₙ₋₁ ⋅(β₂ₙ₋₁)⁻¹$    (6)&lt;/p&gt;
&lt;p&gt;That means $𝐇₂ₙᵉ ⋅ β₂ₙ₋₁ = e₂ₙ₋₁$, this even node is approaching the last residual error based on the known output weight β₂ₙ₋₁.&lt;/p&gt;
&lt;p&gt;Then its output weight can be calculated based on the Lemma 1 as:&lt;/p&gt;
&lt;p&gt;$β₂ₙ = \frac{⟨e₂ₙ₋₁, 𝐇₂ₙᵉ⟩}{‖𝐇₂ₙᵉ‖²}$   (7)&lt;/p&gt;
&lt;p&gt;Once this even node is determined, the corresponding residual error e₂ₙcan be generated,
and also the output weight of the next odd node (with 𝐇₂ₙ₊₁ʳ from random weights) can be calculated based on Lemma 1:&lt;/p&gt;
&lt;p&gt;$β₂ₙ₊₁ = \frac{⟨e₂ₙ, 𝐇₂ₙ₊₁ʳ⟩}{‖𝐇₂ₙ₊₁ʳ‖²}$   (8)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt; further states the updated ^𝐇₂ₙᵉ calculated with the optimal ^a, ^b solved based on 𝐇₂ₙᵉ by the least-square (pseudo inverse) can also let the residual error converge to 0.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark 1&lt;/strong&gt; clarifies the differences between this method and I-ELM, that is the input weights and bias of even nodes are calculated, not randomly generated. And the output weights are set similarly based on Lemma 1.&lt;/p&gt;
&lt;p&gt;Based on the eq. 6, the Δ = ‖e₂ₙ₋₁‖² + ‖e₂ₙ‖² can be reformalized to an ellipse curve.&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/Raspberrycai1/e3b4fd08c08c2c4820fa11d3cce04fef.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Training needs to store parameters for each node. And testing needs to query each node sequentially.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;
&lt;div id=&#34;univrsl&#34;&gt;&lt;a href=&#34;http://extreme-learning-machines.org/pdf/I-ELM.pdf&#34;&gt;G. B. Huang, L. Chen, and C. K. Siew, “Universal approximation using incremental constructive feedforward networks with random hidden nodes,” IEEE Trans. Neural Netw., Jul.2006.&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>sum: ELM</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/c-sum-elm/</link>
        <pubDate>Tue, 31 Jan 2023 15:49:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/c-sum-elm/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Extreme_learning_machine&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Wikipedia-ELM&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Controversy: RBF (1980s) raised the similar idea of ELM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dispute about the originality of ELM: &lt;a class=&#34;link&#34; href=&#34;https://elmorigin.wixsite.com/originofelm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Origins of ELM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.extreme-learning-machines.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Portal of ELM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/7140733&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;python toolbox: hpelm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;facts&#34;&gt;Facts&lt;/h2&gt;
&lt;p&gt;ELM is &lt;a class=&#34;link&#34; href=&#34;#intro_csdn&#34; &gt;⁽¹⁾&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a type of single hidden layer feedforward neural network (SLFN).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The parameters (𝐰,b) between input layer and hidden layer are set randomly. &lt;br&gt;
Thus, for N input n-dimensional samples and L hidden nodes, the output of the hidden layer is $𝐇 = 𝐗_{N×n} 𝐖_{n×L}+𝐛_{n×L}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Only the number of hidden nodes needs to be predefined manually without other hyper-parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The output weights are initialized randomly and solved based on the pseudo inverse matrix in one-shot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For a n-dimensional sample 𝐱ⱼ and its target 𝐭ⱼ=[tᵢ₁, tᵢ₂, &amp;hellip;, tᵢₘ]ᵀ∈ ℝᵐ, &lt;br&gt;
the output of ELM with L hidden nodes is 𝐨ⱼ = ∑ᵢ₌₁ᴸ 𝛃ᵢ g(𝐰ᵢᵀ⋅𝐱ⱼ + bᵢ), where &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;g(⋅) is activation function;&lt;/li&gt;
&lt;li&gt;𝛃ᵢ is the weights of the ith ouput unit: 𝛃ᵢ=[βᵢ₁, βᵢ₂, &amp;hellip;, βᵢₙ]ᵀ;&lt;/li&gt;
&lt;li&gt;𝐰ᵢ is input weight: 𝐰ᵢ=[wᵢ₁, wᵢ₂, &amp;hellip;, wᵢₙ]ᵀ;&lt;/li&gt;
&lt;li&gt;𝐱ⱼ is a n-dimensional input: 𝐱ⱼ=[xᵢ₁, xᵢ₂, &amp;hellip;, xᵢₙ]ᵀ∈ ℝⁿ;&lt;/li&gt;
&lt;li&gt;bᵢ is the bias of the ith hidden unit;&lt;/li&gt;
&lt;li&gt;𝐨ⱼ is a m-dimensional vector: 𝐨ⱼ=[oᵢ₁, oᵢ₂, &amp;hellip;, oᵢₘ]ᵀ∈ ℝᵐ;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ideal parameters (𝐰,b,𝛃) should satisfy: &lt;br&gt;
∑ᵢ₌₁ᴸ 𝛃ᵢ g(𝐰ᵢᵀ⋅𝐱ⱼ + bᵢ) = 𝐭ⱼ  &lt;br&gt;
For total N samples, this mapping can be reforomalized with matrices: &lt;br&gt;
$𝐇_{N×L} \pmb\beta_{L×m} = 𝐓_{N×m}$, where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;𝐇 is the output of the hidden layer for N samples: &lt;br&gt;
$$𝐇(𝐰₁,&amp;hellip;,𝐰_L, b₁,&amp;hellip;,b_L, 𝐱₁,&amp;hellip;𝐱_L) = \\
\begin{bmatrix} g(𝐰₁⋅𝐱₁+b₁) &amp;amp; \dots &amp;amp; g(𝐰_L⋅𝐱₁+b_L)\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
g(𝐰₁⋅𝐱_N+b₁) &amp;amp; \dots &amp;amp; g(𝐰_L⋅𝐱_N+b_L) \end{bmatrix}_{N×L}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝛃 is the output weights matrix: &lt;br&gt;
[ 𝛃₁ᵀ ; &amp;hellip; ; 𝛃$_Lᵀ ]_{L×m}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Target data: 𝐓 = $\begin{bmatrix}𝐓₁ᵀ\\ \vdots \\𝐓_Nᵀ\end{bmatrix}_{N×m}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generally, $𝐇_{N×m}$ is not a square matrix (not invertible). Hence, 𝛃=𝐇⁻¹𝐓 cannot be applied.
However, the optimal 𝛃 can be approached by minimizing the traning error iteratively:
∑ⱼ₌₁ᴺ‖𝐨ⱼ-𝐭ⱼ‖.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Best estimation: $\^𝐰ᵢ, \^bᵢ$, ^𝛃ᵢ satisfy: &lt;br&gt;
‖𝐇(^𝐰ᵢ, ^bᵢ)⋅^𝛃ᵢ- 𝐓‖ = min_{𝐰ᵢ, bᵢ, 𝛃ᵢ} ‖𝐇(𝐰ᵢ, bᵢ)⋅𝛃ᵢ- 𝐓‖, where i=1,&amp;hellip;,L&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Loss function: J = ∑ⱼ₌₁ᴺ (∑ᵢ₌₁ᴸ 𝛃ᵢ⋅g(𝐰ᵢ⋅𝐱ⱼ + bᵢ) - 𝐭ⱼ)²&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solve 𝛃 based on the ∂J/∂𝛃=0, such that the optimal parameter is: &lt;br&gt;
^𝛃 = $𝐇^† 𝐓$ = (𝐇ᵀ𝐇)⁻¹𝐇ᵀ 𝐓, &lt;br&gt;
where $𝐇^†$ is the Moore-Penrose inverse (Pseudo-inverse) of 𝐇. &lt;br&gt;
It can be proved that the norm of ^𝛃 is the smallest and unique solution (for a set of random (𝐰ᵢ, bᵢ)).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;moore-penrose-inverse&#34;&gt;Moore-Penrose inverse&lt;/h2&gt;
&lt;p&gt;Also called pseudoinverse or generalized inverse &lt;a class=&#34;link&#34; href=&#34;#wiki_MPinv&#34; &gt;⁽²⁾&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(bilibili search: &amp;ldquo;伪逆矩阵&amp;rdquo;)
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1364y1678r/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深度学习-啃花书0103伪逆矩阵最小二乘&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(DDG search: &amp;ldquo;伪逆矩阵&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.zhihu.com/question/47688307&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;伪逆矩阵的意义及求法？ - 知乎&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;numpylinalgpinv&#34;&gt;numpy.linalg.pinv()&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;pinv(𝐗) = (𝐗ᵀ 𝐗)⁻¹ 𝐗ᵀ&lt;/li&gt;
&lt;li&gt;pinv(𝐗) 𝐗 = 𝐈
&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/shiyuzuxiaqianli/article/details/105558488&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;python之numpy之伪逆numpy.linalg.pinv - 千行百行 - CSDN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-code&#34;&gt;Example Code&lt;/h2&gt;
&lt;p&gt;This matlab code &lt;a class=&#34;link&#34; href=&#34;#intro_csdn&#34; &gt;⁽¹⁾&lt;/a&gt; trains and tests a ELM on the NIR spectra dataset (regression) and the Iris dataset (classification).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note that each &lt;strong&gt;column&lt;/strong&gt; is a sample, and each row is an attribute/feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q: number of samples&lt;/li&gt;
&lt;li&gt;R: input features&lt;/li&gt;
&lt;li&gt;S: output features&lt;/li&gt;
&lt;li&gt;$P_{R×Q}$: input pattern matrix&lt;/li&gt;
&lt;li&gt;$T_{S×Q}$: target data matrix&lt;/li&gt;
&lt;li&gt;N: number of hidden nodes&lt;/li&gt;
&lt;li&gt;TF: transfer function&lt;/li&gt;
&lt;li&gt;$IW_{N×R}$: input weights matrix&lt;/li&gt;
&lt;li&gt;$B_{N×Q}$: bias matrix&lt;/li&gt;
&lt;li&gt;$LW_{N×S}$: transposed output weights matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Train (calculate the LW):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$tempH_{N×Q} = IW_{N×R}⋅P_{R×Q} + B_{N×Q}$&lt;/li&gt;
&lt;li&gt;$H_{N×Q} = TF(tempH)$&lt;/li&gt;
&lt;li&gt;$LW_{S×N} = T_{S×Q}$⋅ pinv(H), based on: 𝛃$_{S×N} 𝐇_{N×Q} = 𝐓_{S×Q}$&lt;/li&gt;
&lt;/ul&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/Raspberrycai1/c305edcf289ecddb5b8caa7d2d5558fa.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$tempH_{N×Q} = IW_{N×R}⋅P_{R×Q} + B_{N×Q}$&lt;/li&gt;
&lt;li&gt;$H_{N×Q} = TF(tempH)$&lt;/li&gt;
&lt;li&gt;$Y_{S×Q} = LW_{S×N}⋅H_{N×Q}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-code-py&#34;&gt;Example code (py)&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/build-an-extreme-learning-machine-in-python-91d1e8958599&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Build an Extreme Learning Machine in Python | by Glenn Paul Gara &amp;hellip;&lt;/a&gt;
searched by DDG: &amp;ldquo;incremental elm python&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;i-elm&#34;&gt;I-ELM&lt;/h2&gt;
&lt;p&gt;incremental just means adding neurons?&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/mlearning-ai/incremental-extreme-machine-learning-i-elm-11d5f26b220a&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;github&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;os-elm&#34;&gt;OS-ELM&lt;/h2&gt;
&lt;p&gt;On-line elm&lt;/p&gt;
&lt;h2 id=&#34;deep-incremental-rvfl&#34;&gt;Deep incremental RVFL&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1568494623004283&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Deep incremental random vector functional-link network: A non-iterative constructive sketch via greedy feature learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;div id=&#34;intro_csdn&#34;&gt;&lt;a href=&#34;https://blog.csdn.net/lyxleft/article/details/82892383&#34;&gt;极限学习机(Extreme Learning Machine, ELM)原理详解和MATLAB实现 - 奔跑的Yancy - CSDN&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;wiki_MPinv&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&#34;&gt;Moore-Penorse - Wikipedia&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(&lt;a class=&#34;link&#34; href=&#34;#facts&#34; &gt;Back to Top&lt;/a&gt;)&lt;/p&gt;
</description>
        </item>
        <item>
        <title>watch: SNN - 杨易旻 | WeChat Live</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/</link>
        <pubDate>Fri, 27 Jan 2023 10:19:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/</guid>
        <description>&lt;p&gt;下一代国际华人青年学子面对面 第6期 2022年10月20日 周四&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Research work&lt;/li&gt;
&lt;li&gt;Suggestions for graduate student (1st year)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Research Contribution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-iterative learning strategy for training neural networks&lt;/strong&gt;
including single-layer networks, multi-layer networks, autoencoders, hierarchical networks, and deep networks.
All the related publications in this category are my first-author papers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The proposed methods for pattern recognition related applications&lt;/strong&gt;:
Image Recognition, Video Recognition, Hybrid System Approximation, Robotics System Identification, EEG-brain Signal Processing.
Most the related publications in this category are co-author papers with my HQPs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;my-research-works&#34;&gt;My research works&lt;/h3&gt;
&lt;p&gt;In the past 10 years, the works about Artificial neural networks：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Theoretical Contributions to ANN&lt;/th&gt;
&lt;th&gt;Machine Learning based Applications&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Single-layer network with non-iterative learning&lt;/td&gt;
&lt;td&gt;Data Analysis and Robotics System Identification (Ph.D.)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hierarchical NN with Subnetwork Neurons&lt;/td&gt;
&lt;td&gt;Image Recognitions (Post Doctoral Fellow)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deep Networks without iterative learning&lt;/td&gt;
&lt;td&gt;Pattern Recognition (since 2018)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;-single-layer-network-with-non-iterative-learning&#34;&gt;Ⅰ. Single-layer network with non-iterative learning&lt;/h2&gt;
&lt;h3 id=&#34;starting-with-a-small-idea&#34;&gt;Starting with a small idea&lt;/h3&gt;
&lt;p&gt;The labtorary mainly studies robots, control, mechanics.
After 2008 Chinese Winter Storms, they got funding for creating Powerline De-icing robots.&lt;/p&gt;
&lt;p&gt;The supervisor (Yaonan Wang): &amp;ldquo;Can you find a Neural Network for Identifying Robotics Dynamic Systems?&amp;rdquo; (in 2009 winter)&lt;/p&gt;
&lt;p&gt;Later, I found the following paper:
&amp;ldquo;Universal approximation using incremental constructive feedforward networks with random hidden nodes&amp;rdquo;, By Huang, Guang-Bin et.al (Cannot be found on IEEE)
&lt;a class=&#34;link&#34; href=&#34;http://extreme-learning-machines.org/pdf/I-ELM.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;version on elm portal&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-is-the-neural-network&#34;&gt;What is the Neural Network?&lt;/h3&gt;
&lt;p&gt;Single hidden layer feedforward NN:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart BT
subgraph in[Input Layer]
x1((1)) &amp; xe((&#34;...&#34;)) &amp; xn((n))
end
subgraph hid[Hidden Layer]
h1((&#34;𝛂₁,♭₁,𝛃₁&#34;)) &amp; h2((&#34;𝛂₂,♭₂,𝛃₂&#34;)) &amp; he((&#34;......&#34;)) &amp; hL((&#34;𝛂L,♭L,𝛃L&#34;))
end
subgraph out[Output Layer]
y1((1)) &amp; ye((&#34;...&#34;)) &amp; ym((m))
end
x1 &amp; xn --&gt; h1 &amp; h2 &amp; hL --&gt; y1 &amp; ym
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Output of additive hidden neurons:
G(𝐚ᵢ, bᵢ, 𝐱) = g(𝐚ᵢ⋅𝐱+bᵢ)&lt;/li&gt;
&lt;li&gt;Output of RBF hidden nodes:
G(𝐚ᵢ, bᵢ, 𝐱) = g‖𝐱-𝐚ᵢ‖&lt;/li&gt;
&lt;li&gt;The output function of SLFNs is:
fₗ(𝐱) = ∑ₗ₌₁ᴸ 𝛃ᵢ⋅G(𝐚ᵢ, bᵢ, 𝐱)&lt;/li&gt;
&lt;/ul&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_SLFN.jpg width=&gt;
  
  


&lt;h3 id=&#34;network-training&#34;&gt;Network training&lt;/h3&gt;
&lt;p&gt;Advantage: Approximate unknown system through learnable parameters.&lt;/p&gt;
&lt;p&gt;Mathematical Model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Approximation capability: Any continuous target function f(x) can be approximated by Single-layer feedforward network with appropriate parameters (𝛂,♭,𝛃).
In other words, given any small positive value e, for SLFN with enough number of hidden nodes, we have: &lt;br&gt;
‖f(𝐱)-fₗ(𝐱)‖ &amp;lt; e&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In real applications, target function f(𝐱) is usually unknown. One wishes that unknown f could be approximated by the trained network fₗ(𝐱).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-is-extreme-learning-machine&#34;&gt;What is Extreme Learning Machine?&lt;/h3&gt;
&lt;p&gt;Feed forward random network without using BP to train, such that it has a good real-time performance. And it fits the real-time robot task exactly.&lt;/p&gt;
&lt;h3 id=&#34;b-elm&#34;&gt;B-ELM&lt;/h3&gt;
&lt;p&gt;(2011) &amp;ldquo;Bidirectional ELM for regression problem and its learning effectiveness&amp;rdquo;, IEEE Trans. NNLS., 2012
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/6222007&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(23-02-10) This the inception of his subnetwork series work, and I was supposed to read this brief firt.
&lt;a class=&#34;link&#34; href=&#34;../ReadLit/B-note-B-ELM.md&#34; &gt;paperNote&lt;/a&gt;&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_B-ELM.jpeg width=&gt;
  
  


&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Original ELM has 3 kinds of parameters: 𝐚 is called the &amp;ldquo;input weights&amp;rdquo;, b is the bias, 𝛃 is the &amp;ldquo;output weights&amp;rdquo;,
which are consistent with earlier feedfoward network, though current single-layer feedfoward network has removed the 𝛃.&lt;/p&gt;
&lt;p&gt;The 1st layer in ELM is generated randomly and the 2nd layer is constructed based on Moore-Penrose inverse without iteration.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
in((X)) --&gt; lyr1[&#34;Layer1:\n 𝐚ⁿ, b\n Random\n Generated&#34;]
--&gt; weightSum1((&#34;X⋅𝐚ⁿ + b&#34;)) --&gt; act[Activation\n Function\n g]
--&gt; z((&#34;g(X⋅𝐚ⁿ+b)&#34;)) --&gt; lyr2[&#34;Layer2:\n 𝛃&#34;] 
--&gt; weightSum2((&#34;O =\n 𝛃⋅g(X⋅𝐚ⁿ+b)\n =T&#34;))
-.-&gt;|&#34;𝛃 =\n g(X⋅𝐚ⁿ+b)⁻¹T&#34;| lyr2
classDef lyrs fill:#ff9
class lyr1,act,lyr2 lyrs;
&lt;/div&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bidirectional ELM&lt;/p&gt;
&lt;p&gt;In original ELM, the 𝐚ⁿ,b are random numbers, but they can be yield if pulling the error back further, that is doing twice more inverse computation.
Therefore, in order to calculate the 𝐚ⁿ,b, there are 3 times inverse computation: for output weights 𝛃, activation function g(⋅) and activation z (X⋅𝐚ⁿ+b) respectively.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;bump&#39; } } }%%
flowchart LR
in((X)) --&gt; lyr1[&#34;Layer1:\n 𝐚ⁿ, b\n Random\n Generated&#34;]
--&gt; weightSum1((&#34;X⋅𝐚ⁿ + b&#34;)) --&gt; act[Activation\n Function\n g]
--&gt; z((&#34;g(X⋅𝐚ⁿ+b)&#34;)) --&gt; lyr2[&#34;Layer2:\n 𝛃&#34;] --&gt; out((&#34;O =\n 𝛃⋅g(X⋅𝐚ⁿ+b)\n =T&#34;))
-.-&gt;|&#34;𝛃 =\n g(X⋅𝐚ⁿ+b)⁻¹T&#34;| lyr2
classDef lyrs fill:#ff9
class lyr1,act,lyr2 lyrs;

out -.-&gt; |&#34;X⋅𝐚ⁿ+b =\n g⁻¹ T 𝛃⁻¹&#34;|weightSum1
out -.-&gt; |&#34;𝐚ⁿ =\n X⁻¹ (g⁻¹ T 𝛃⁻¹ -b ),\n b = mse(O-T)&#34;|lyr1 
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;𝛃 = [g(X⋅𝐚ⁿ+b)]⁻¹T&lt;/li&gt;
&lt;li&gt;X⋅𝐚ⁿ+b = g⁻¹ T 𝛃⁻¹&lt;/li&gt;
&lt;li&gt;𝐚ⁿ = X⁻¹ (g⁻¹ T 𝛃⁻¹ -b ), and b = mse(O-T)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The error is surprisingly small even with few hidden nodes.
Compared with the original ELM, the required neurons in this method are reduced by 100-400 times, and the testing error reduced 1%-3%, and also the training time reduced 26-250 times over 10 datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Major differences&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Randomized Networks&lt;/th&gt;
&lt;th&gt;Bidirectional ELM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Classifier&lt;/td&gt;
&lt;td&gt;ELM; Echo State Netowrk;&lt;br&gt; Random Forest;&lt;br&gt; Vector Functional-link Network&lt;/td&gt;
&lt;td&gt;Only works for regression task&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performance&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Similar performance;&lt;br&gt; Faster speed;&lt;br&gt; Less required neurons&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Learning strategy&lt;/td&gt;
&lt;td&gt;(Semi-)Randomized input weights;&lt;br&gt; Non-iterative training;&lt;br&gt; Single-layer network&lt;/td&gt;
&lt;td&gt;Non-iterative training;&lt;br&gt; Single-layer network;&lt;br&gt; Calculated weights in a network&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;-hierarchical-nn-with-subnetwork-neurons&#34;&gt;Ⅱ. Hierarchical NN with Subnetwork Neurons&lt;/h2&gt;
&lt;h3 id=&#34;single-layer-network-with-subnetwork-neurons&#34;&gt;Single-Layer Network with Subnetwork Neurons&lt;/h3&gt;
&lt;p&gt;In 2014, deep learning is becoming popular. How to extend the B-ELM as a multi-layer network? &lt;br&gt;
&amp;ldquo;Extreme Learning Machine With Subnetwork Hidden Nodes for Regression and Classification&amp;rdquo;.
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8627989&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;; &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/&#34; &gt;paperNote&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart TB
base[Bidirectional ELM] --&gt; theory[Theoretical Contributions on ANN] &amp; app[Industrial Applications]
theory --&gt; multilyr[1. Two-layer Neural Networks?\n 2. Hierarchical Neural Networks?]
app --&gt; tasks[1. Feature Extraction\n 2. Dimension Reduction\n 3. Image Recognition]
&lt;/div&gt;

&lt;p&gt;Pull the residual error back to multiple B-ELMs sequentially:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
in((X)) --&gt; lyr11 &amp; lyr21 &amp; lyrN1

subgraph net1[B-ELM 1]
lyr11[&#34;𝐚¹,b¹&#34;] ==&gt;|&#34;X⋅𝐚¹+b¹&#34;| act1[g] ==&gt;|&#34;g(X⋅𝐚¹+b¹)&#34;|lyr12[&#34;𝛃¹&#34;] ==&gt;|&#34;𝛃¹⋅g(X⋅𝐚¹+b¹)&#34;| out((&#34;O ➔ T\n E=T&#34;))
-.-&gt; lyr12 -.-&gt; act1 -.-&gt; lyr11
lyr11 --&gt; act1 --&gt; lyr12 
end
lyr12 --&gt; out1((&#34;O¹➔T\n E=T-O¹&#34;)) -.-&gt; lyr22

subgraph net2[B-ELM 2]
lyr22[&#34;𝛃²&#34;] -.-&gt; act2[g] -.-&gt; lyr21[&#34;𝐚²,b²&#34;]
lyr21 --&gt;|&#34;X⋅𝐚² + b²&#34;|act2 --&gt;|&#34;g(X⋅𝐚²+b²)&#34;|lyr22 
end
lyr22--&gt; out2((&#34;O²+O¹➔T\n E=T-(O²+O¹)&#34;)) 

out2 -.-&gt;|&#34;Solve\n multiple\n B-ELMs&#34;| outn-1((&#34;E=T-∑ᵢ₌₁ᴺ⁻¹ Oⁱ&#34;))-.-&gt;lyrN2
%%out2 -.-&gt; lyrn2
%%subgraph nets[multiple B-ELMs]
%%lyrn2[&#34;𝛃ⁿ&#34;] -.-&gt; actn[g] -.-&gt; lyrn1[&#34;𝐚ⁿ,bⁿ&#34;]
%%lyrn1 --&gt;|&#34;X⋅𝐚ⁿ + bⁿ&#34;|actn --&gt;|&#34;g(X⋅𝐚ⁿ+bⁿ)&#34;|lyrn2
%%end
%%lyrn2 --&gt; outn-1((&#34;E=T-∑ᵢ₌₁ᴺ⁻¹ Oⁱ&#34;))-.-&gt;lyrN2

subgraph netN[&#34;B-ELM N&#34;]
lyrN2[&#34;𝛃ᴺ&#34;] -.-&gt; actN[g] -.-&gt; lyrN1[&#34;𝐚ᴺ,bᴺ&#34;]
lyrN1 --&gt;|&#34;X⋅𝐚ᴺ + bᴺ&#34;|actN --&gt;|&#34;g(X⋅𝐚ᴺ+bᴺ)&#34;|lyrN2 
end
lyrN2--&gt; outN((&#34;∑ᵢ₌₁ᴺ Oⁱ➔T&#34;))

classDef lyrs fill:#ff9
class lyr11,act1,lyr12,lyr21,act2,lyr22,lyrN1,actN,lyrN2 lyrs;
linkStyle 9,10,11,15,16,17,22,23,24 stroke:#0af,stroke-width:3px
%%classDef node font-size:20px;
&lt;/div&gt;

&lt;p&gt;Dotted links are computation with inverse.
&lt;font color=&#34;#0bf&#34;&gt;Cyan links&lt;/font&gt; is the second feedforward using the updated parameters to give a trustworthy result O¹.
The objective is to approach the target T, so there is a residual error E=T-O¹.
Then another B-ELM (with same structure) is used to reduce the error continuously.
And this time, the prediction is O²+O¹, which is the approximation of T.
Here, the residual error is E=T-(O²+O¹)&lt;/p&gt;
&lt;p&gt;Repeatedly pulling the residual error to a new B-ELM N times is equivalent to N SLFNs. But B-ELM is fast without iteration and less computation with a few hidden nodes in each SLFN.&lt;/p&gt;
&lt;p&gt;Based on original SLFN structure, each node contains a SLFN.&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Subnetwork_Nodes.jpeg width=&gt;
  
  


&lt;h3 id=&#34;two-layer-network-with-subnetwork-neurons&#34;&gt;Two-Layer Network with Subnetwork Neurons&lt;/h3&gt;
&lt;p&gt;(2015) How to extend the Single-layer network with subnetwork nodes system to a two-layer network system?&lt;/p&gt;
&lt;p&gt;A general two-layer system was built in paper: &amp;ldquo;Multilayer Extreme Learning Machine with Subnetwork Hidden Neurons for Representation Learning&amp;rdquo;
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/7295596&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;; &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/&#34; &gt;paperNote&lt;/a&gt;&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Two-layers_ELM.jpeg width=&gt;
  
  


&lt;p&gt;Though it only contains two &amp;ldquo;general&amp;rdquo; layers, this system includes hundreds of networks, and it&amp;rsquo;s fast due to the modest quantity and no iteration.&lt;/p&gt;
&lt;p&gt;Compared with ELM and B-ELM, it got better performance over 35 datasets:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Classification (vs ELM)&lt;/th&gt;
&lt;th&gt;Regression (vs B-ELM)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Required Neurons&lt;/td&gt;
&lt;td&gt;Reduced 2-20 times&lt;/td&gt;
&lt;td&gt;Reduced 13-20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Metrics&lt;/td&gt;
&lt;td&gt;Accuracy increase 1-17%&lt;/td&gt;
&lt;td&gt;Testing error reduced 1-8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Speed&lt;/td&gt;
&lt;td&gt;faster 25-200times&lt;/td&gt;
&lt;td&gt;faster 5-20%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Two-layer network system can perform image compression or reconstruciton, etc.
This method is better than Deep Belief Network on small datasets.
But it&amp;rsquo;s inferior than deep learning with transfer learning technics on huge datasets.&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-network-with-subnetwork-neurons&#34;&gt;Hierarchical Network with Subnetwork Neurons&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Features combined from hundreds of mid-layers: Hierarchical networks with subnetwork nodes&amp;rdquo; IEEE Trans. NNLS, 2019.
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8627989&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Hierarchical_Network.jpeg width=&gt;
  
  


&lt;p&gt;From a single-layer network with subnetwork neurons to the multi-layer network, and then to a neural network system, these 3 papers cost 5 years or so.&lt;/p&gt;
&lt;p&gt;Compared with deep learning network, it&amp;rsquo;s extremely fast and performs well on small datasets, like Scene15, Caltech101, Caltech256.
But for large datasets, deep learning is the winner.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Somewhat regretfully, I turned to deep learning a bit late. But been hesitant to do research along this approach.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Major differences between ours and Deep Networks&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;SGD based methods in DCNN&lt;/th&gt;
&lt;th&gt;Moore-Penrose inverse matrix based methods&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Hyper params&lt;/td&gt;
&lt;td&gt;lr; momentum; bs; L2 regulariation; epochs&lt;/td&gt;
&lt;td&gt;L2 regularization (non-sensitive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performance&lt;/td&gt;
&lt;td&gt;higher performance in Computer Vision tasks (with huge datasets);&lt;br&gt; GPU-based computation resource;&lt;br&gt; More parameters;&lt;br&gt; More required training time&lt;/td&gt;
&lt;td&gt;Faster learning speed/less tuning;&lt;br&gt; Promising performance in Tabular Datasets;&lt;br&gt; Less over-fitting problem.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;-deep-networks-without-iterative-learning&#34;&gt;Ⅲ. Deep Networks without iterative learning&lt;/h2&gt;
&lt;p&gt;Since 2018: How to combine the non-iterative method (M-P inverse matrix) with deep convolutional network to gain advantages? This took 2-3 years.&lt;/p&gt;
&lt;p&gt;This is the age of Deep Learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Interesting 20 years of cycles&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rosenblatt&amp;rsquo;s Perceptron proposed in mid of 1950s, sent to &amp;ldquo;winter&amp;rdquo; in 1970s.&lt;/li&gt;
&lt;li&gt;Back-Propagation and Hopfield Network Proposed in 1970s, reaching research peak in mid of 1990s.&lt;/li&gt;
&lt;li&gt;Support vector machines proposed in 1995, reaching research peak early this century.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are exceptional cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most basic deep learning algorithms proposed in 1960s-1980s, becoming popular only since 2012 (for example, LeNet proposed in 1989).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ImageNet pushed deep learning, because only when the huge network structure of deep learning meets the matched huge dataset, it can achive good performance.&lt;/p&gt;
&lt;p&gt;The success of deep learning enlist three factors: 1. NN structure and algorithm; 2. Big data; 3. GPU availability.&lt;/p&gt;
&lt;p&gt;Hundreds of layers result in tedious training time. &amp;ldquo;The study intensity is infinitely small and the study duration is infinitely large.&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The improvement space of deep neural network is limited. So can we introduce non-iterative learning strategies for training deep networks&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Training speed is more important for scientific research than accuracy.
And also it&amp;rsquo;s necessary to reduce the dependence on GPUs and the involvement of undeterministic hyper parameters (lr,bs,&amp;hellip;)&lt;/p&gt;
&lt;h3 id=&#34;retraining-dcnn-with-the-non-iterative-strategy&#34;&gt;Retraining DCNN with the non-iterative strategy&lt;/h3&gt;
&lt;p&gt;(2019): &amp;ldquo;Recomputation of the dense layers for the performance improvement of DCNN.&amp;rdquo; IEEE TPAMI, 2020.
&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/8718406&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;graph LR
base[&#34;1. Two-layer Neural Networks\n 2. Hierarchical Neural Networks&#34;]
--&gt; explore[&#34;1. Deep learning withe non-iterative method&#34;]
&lt;/div&gt;






  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_NonIterative&amp;#43;CNN.jpeg width=&gt;
  
  


&lt;p&gt;In a DCNN, the first few layers are convolutional layers, maxpooling, then there&amp;rsquo;re 3 or 1 dense layer.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If I cannot train all of the hundreds layers in my non-iterative method, can I train only certain layers that are easy trained with my method, rather the SGD?&lt;/p&gt;
&lt;/blockquote&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/subnetwork/img/Yang_Retrain_FC_CNN.jpeg width=&gt;
  
  


&lt;p&gt;Only the fully-connected layers are trained by non-iterative method (inverse matrix), and the rest of layers are trained by gradient descent (SGD, SGDM, Adam).&lt;/p&gt;
&lt;p&gt;On some medium-size datasets(CIFAR10, CIFAR100, SUN397), this approach brought a moderate improvement because there are only 3 dense layer out of a 50/100-layer network (most of layers are trained with SGD), but speeds up the training.&lt;/p&gt;
&lt;p&gt;One layer can be trained within 1-2 seconds.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>read: ELM with Subnetwork Nodes</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/</link>
        <pubDate>Fri, 20 Jan 2023 11:46:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/7314912&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IEEE Cybernetics(2015-11-02)&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/d/13FD2IifYUndeTXeV6NrlS_ka7-2JFUTY/view&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Google Drive&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=fFP4b9kAAAAJ&amp;amp;citation_for_view=fFP4b9kAAAAJ:rHJHxKgnXwkC&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;G.Scholar&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the first paper of ELM with subnetwork nodes by &lt;a class=&#34;link&#34; href=&#34;http://www.yiminyang.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Yimin Yang&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;The second paper in the series is &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/&#34; &gt;MltLyr ELM with subnetwork nodes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The outline of Yang&amp;rsquo;s research works: &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/&#34; &gt;Yang-WeChatLive-20221020&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learning effectiveness and speed of SLFN are bottleneck.&lt;/li&gt;
&lt;li&gt;ELM is fast.&lt;/li&gt;
&lt;li&gt;Grow subnetwork nodes by pulling back residual network error to the hidden layer.&lt;/li&gt;
&lt;li&gt;Better generalization performance with fewr hidden nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-introduction&#34;&gt;Ⅰ. Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bring out the subject: &lt;br&gt;
FNN (universial approximator) ➔ SLFN&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is an SLFN? &lt;br&gt;
Input layer + hidden  layer + output layer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Math description: &lt;br&gt;
For N arbitary distinct samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, where 𝐱ᵢ∈ 𝐑ⁿ and 𝐭ᵢ∈ 𝐑ᵐ, the network output is:&lt;/p&gt;
&lt;p&gt;$𝐟_L(𝐱)$ = ∑ᵢ₌₁ᴸ 𝛃ᵢh(𝐚ᵢ⋅𝐱ⱼ + bᵢ) = ∑ᵢ₌₁ᴸ 𝐇ᵢ⋅𝛃ᵢ, j=1,&amp;hellip;,N        (1)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SLFN output is the weighted sum of 𝐿 hidden nodes (perceptrons) with the factor 𝛃.&lt;/li&gt;
&lt;li&gt;The ith perceptron receives the weighted sum of 𝑁 inputs through its parameters (𝐚ᵢ, bᵢ), 𝐚ᵢ∈ 𝐑ⁿ, b∈ 𝐑, and performs activation function h.
Its contribution ratio to the all output nodes is 𝛃ᵢ.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELM traits: &lt;br&gt;
NN (all params are adjustable) ➔ partial random networks ➔ ELM is a full-random learning method,
where the input weights and bias (𝐚, b) are generated randomly and independent of training data.
(Will the Glorot normalization has no effect?)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELM advantages: &lt;br&gt;
An unification of FNNs and SVM/LS-SVM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELM application: &lt;br&gt;
CV, da,&amp;hellip;, online learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The choice of the regularization parameter C which affects the generalization performance of ELM mainly relies on trial-and-error method.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How many neurons should be used in ELM. Although Huang suggested to use more than 1000 hidden nodes, whether the number of hidden nodes can be further reduced without affecting learning effectiveness for large-size/high dimension data set. &lt;br&gt;
Several improved ELM methods, like &lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/6222007&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;B-ELM&lt;/a&gt; pulls the network residual error back to the hidden layer but it only works for regression task,
and other methods bring a higher computation complexity when compared to standard ELM.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Growing subnetwork hidden nodes to the exisiting network by pulling back the network residual error to hidden layers. A hidden node itself can be formed by several hidden nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Faster than BP, SVM and other ELMs and compatible to regreesion and classification problems.&lt;/li&gt;
&lt;li&gt;The regularized parameter C do not affect the generalization performance of this method.&lt;/li&gt;
&lt;li&gt;This method with m hidden nodes (the desired output dimensionality) can achieve better training accuracy than the original ELM with a large number of hidden nodes.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-definitions-and-basic-elm&#34;&gt;Ⅱ. Definitions and Basic-ELM&lt;/h2&gt;
&lt;h3 id=&#34;a-notations-and-definitions&#34;&gt;A. Notations and Definitions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;𝐑 : set of real numbers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;{(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ : N arbitrary distinct samples,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐱ᵢ = [xᵢ₁,xᵢ₂,&amp;hellip;,xᵢₙ]ᵀ : n-dim input data, 𝐱ᵢ∈ 𝐑ⁿ is a column vector;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐭ᵢ : m-dim desired output data, 𝐭ᵢ∈ 𝐑ᵐ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐱 : input data matrix, 𝐱=[𝐱₁,&amp;hellip;𝐱_N], 𝐱ᵢ∈ 𝐑ⁿᕁᴺ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐭 : desired output data matrix, 𝐭=[𝐭₁,&amp;hellip;𝐭_N], 𝐭∈ 𝐑ᵐᕁᴺ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(^𝐚ₙ,^bₙ) : parameters of the 𝑛th subnetwork hidden node, ^𝐚ₙ∈ 𝐑ⁿᕁᵐ, ^bₙ∈ 𝐑 (suppose the number of hidden nodes equals to the output dimension m, thus the mapping is from n to m.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;^𝐚ₙ = [𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ], n×m weights matrix (for a n-dimension sample 𝐱ᵢ), 𝐚ₙₘ∈ 𝐑ⁿ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐞ₙ : residual error of current network output 𝑓ₙ with  𝑛 hidden nodes (for N samples),
i.e., 𝐞ₙ=𝐭-𝑓ₙ, 𝐞ₙ∈ 𝐑ᵐᕁᴺ.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐇 : output matrix of the hidden layer (of SLFN) for tarining set {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ,
𝐇 = [h(𝐱₁),&amp;hellip;,h(𝐱_N)]ᵀ, 𝐇∈ 𝐑ᴺᕁᵐ, 𝐇 = g(𝐱ᵀ𝐚+𝐛)???&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;h(𝐱) : activation function. &lt;del&gt;ELM feature mapping (or Huang&amp;rsquo;s transform)&lt;/del&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐇ᵢ : the 𝑖th hidden node output w.r.t. inputs, i.e., the 𝑖th column of 𝐇&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝐈 : unit matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sum(𝐞) : the sum of all elements of the matrix 𝐞&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;b-basic-elm&#34;&gt;B. Basic-ELM&lt;/h3&gt;
&lt;p&gt;ELM is proposed for single-hidden-layer feedforward networks (SLFNs).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The output function of ELM with L hidden nodes for SLFNs is:&lt;/p&gt;
&lt;p&gt;$𝑓_L(𝐱)$ = ∑ᵢ₌₁ᴸ βᵢ⋅h(𝐚ᵢ⋅𝐱ⱼ + bᵢ) = ∑ᵢ₌₁ᴸ 𝐇ᵢ⋅𝛃ᵢ, j=1,&amp;hellip;,N.&lt;/p&gt;
&lt;p&gt;where h(⋅) denotes an activation function, (𝐚ᵢ, bᵢ), 𝐚ᵢ∈ 𝐑ⁿ, bᵢ∈ 𝐑, denotes the ith hidden node parameters, and 𝛃ᵢ is the ith output weight between the ith hidden node and the output nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on Bartlett&amp;rsquo;s theory, ELM theory aims to reach not only the smallest training error, but also the smallest norm of output weights
(Least square-least norm solution, where the regularization &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/calc/pseudo-inverse/&#34; &gt;makes an invertible matrix&lt;/a&gt;, such that a special solution can be determined.):&lt;/p&gt;
&lt;p&gt;Minimize: ‖𝛃‖² + C⋅‖𝐇𝛃 - 𝐭‖²&lt;/p&gt;
&lt;p&gt;&amp;ldquo;then the generalization performance depends on the size of weights rather than the number of nodes.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt; (proved by Huang):&lt;br&gt;
Given an SLFN with nonconstant piecewise continuous hidden nodes 𝐇(𝐱, 𝐚, b),
then for any continuous target function 𝑓 and any function sequence 𝐇ₙʳ(𝐱) = 𝐇(𝐱, 𝐚ₙ, bₙ) randomly generated based on any continuous sampling distribution,&lt;/p&gt;
&lt;p&gt;lim$_{n➝∞}$ ‖𝑓 - (𝑓ₙ₋₁ + 𝐇ₙʳ⋅𝛃ₙ)‖ = 0&lt;/p&gt;
&lt;p&gt;holds with probabitliy 1 if: &lt;br&gt;
𝛃ₙ = ⟨𝐞ₙ₋₁, 𝐇ₙʳ⟩ / ‖𝐇ₙʳ‖² (the weight of the 𝑛th node)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;⟨ , ⟩&amp;rdquo; stands for &amp;ldquo;dot product&amp;rdquo; (&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Frobenius_inner_product&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Frobenius inner product&lt;/a&gt;) of two matrices and is a scalar.&lt;/li&gt;
&lt;li&gt;n is the number of hidden nodes in the hidden layer.&lt;/li&gt;
&lt;li&gt;𝐞ₙ₋₁ is the residual error of the last iteration, i.e., when there were n-1 hidden nodes.&lt;/li&gt;
&lt;li&gt;𝐇ₙʳ is the output matrix of the current hidden layer (activated but havn&amp;rsquo;t scaled by 𝛃).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intuitively, as the residual error reduces, the weight of the newer node gets smaller.&lt;/p&gt;
&lt;h2 id=&#34;-proposed-elm-method-with-subnetwork-hidden-nodes&#34;&gt;Ⅲ. Proposed ELM Method With Subnetwork Hidden Nodes&lt;/h2&gt;
&lt;h3 id=&#34;a-structure-of-the-proposed-method&#34;&gt;A. Structure of the Proposed Method&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Motivations&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Selecting an appropriate number of neurons can resort to optimization algorithms.&lt;/li&gt;
&lt;li&gt;The generalization performance depends on the size of the weights rather than the number of weights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Inspiration&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;A hidden node itself can be a subnetwork formed by several nodes.
And these &lt;strong&gt;subnetwork hidden nodes&lt;/strong&gt; and output weights itself should be the smallest norm, and also aim to reach the smallest training error.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objectives&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Given N training samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐱ᵢ∈ 𝐑ⁿ, 𝐭ᵢ∈ 𝐑ᵐ, generated from the same continuous system,
if activation function h is invertible, the objectives are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;‖𝐞ₙ₋₁‖ ≥ ‖𝐞ₙ₋₁ - h(^𝐚ₙ,𝐱)‖ ≥ ‖𝐞ₙ₋₁ - 𝐇ₙ‖ ≥ ‖𝐞ₙ₋₁ - 𝐇ₙ⋅𝛃ₙ‖ (residual error is decreasing.)&lt;/li&gt;
&lt;li&gt;‖h(^𝐚ₙ,𝐱) - 𝐞ₙ₋₁‖ = min_{𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ} ‖h(𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ) - 𝐞ₙ₋₁‖  (minimize weights inside nodes)&lt;/li&gt;
&lt;li&gt;‖𝐇ₙ⋅^𝛃ₙ - 𝐞ₙ₋₁‖ = min_{𝛃} ‖𝐇ₙ⋅𝛃 - 𝐞ₙ₋₁‖   (minimize the weights outside nodes)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^𝐚ₙ and ^𝛃ₙ are the optimal (the ultimate status) parameters with the smallest norm among all the least squares solutions.&lt;/li&gt;
&lt;li&gt;𝐇ₙ = h(^𝐚ₙ, ^bₙ, 𝐱) is the output of the nth hidden node with the optimal parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If activation function h is invertible, subnetwork hidden nodes in SLFN can be calculated by pulling back network residual error to hidden layers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For example, with sine function as the activation function, training a subnetwork hidden node (^𝐚) is equivalent to finding a least-square solution ^𝐚ₙ (letting the derivative of MSE=0) with the least norm for the linear system:&lt;/p&gt;
&lt;p&gt;[𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ]⋅𝐱 = arcsin(𝐞ₙ₋₁), 𝐞ₙ₋₁∈ (0,1],&lt;/p&gt;
&lt;p&gt;such that the optimal ^𝐚ₙ satifies:&lt;/p&gt;
&lt;p&gt;‖sin(^𝐚ₙ, 𝐱) - 𝐞ₙ₋₁‖ = min_{𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ} ‖sin(𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ, 𝐱) - 𝐞ₙ₋₁‖,&lt;/p&gt;
&lt;p&gt;That means the output of the nth &amp;ldquo;subnetwork hidden node&amp;rdquo; ^𝐚ₙ is approaching the residual error 𝐞ₙ₋₁ of the last status.&lt;/p&gt;
&lt;p&gt;The input weights ^𝐚ₙ of a node for this model is a matrix (instead of a vector), beacuse each &amp;ldquo;subnetwork (general) hidden node&amp;rdquo; &lt;strong&gt;contains a standard SLFN&lt;/strong&gt; (several hidden nodes) internally.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Differences with standard ELM&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;ELM with subnetwork&lt;/th&gt;
&lt;th&gt;standard ELM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hidden node&lt;/td&gt;
&lt;td&gt;m neurons: $𝐚_f∈ 𝐑ⁿᕁᵐ, 𝐛_f∈ 𝐑ᵐ$&lt;/td&gt;
&lt;td&gt;single neuron:&lt;br&gt; 𝐚∈ 𝐑ⁿ, b∈ 𝐑&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;construct&lt;/td&gt;
&lt;td&gt;calculated&lt;/td&gt;
&lt;td&gt;generated randomly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;# hidden nodes&lt;/td&gt;
&lt;td&gt;L x m (m ⟂ L, m = #output dim)&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;b-proposed-method&#34;&gt;B. Proposed Method&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;: &lt;br&gt;
Given a bounded nonconstant piecewise continuous activation function h, there is: &lt;br&gt;
lim$_{(𝐚,b)→(𝐚₀,b₀)}$ ‖h(𝐚⋅𝐱+b) - h(𝐚₀⋅𝐱+b₀)‖ = 0 (连续性)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: &lt;br&gt;
Given N arbitrary distinct samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐱ᵢ∈ 𝐑ⁿ, 𝐭ᵢ∈ 𝐑ᵐ, a sigmoid or sine activation function h, and then for any continuous desired outputs 𝐭, the limit of error converges to 0:&lt;/p&gt;
&lt;p&gt;lim$_{n➝∞}$ ‖ 𝐭-{ u⁻¹(h(^𝐚⋅𝐱+b))} ‖ = 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Prove the sequence ‖𝐞ₙ‖ is decreasing with 0 as the lower bound and it converges.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For the 𝑛th subnetwork hidden node containing m hidden nodes, the linear mapping is:&lt;/p&gt;
&lt;p&gt;𝛌ₙ = [𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ]⋅𝐱, 𝛌ₙ∈ 𝐑ᵐ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then 𝛌ₙ passes through the activation function. Because the target is error, which should become 0 at the end, the error at present is the output of activation function:&lt;/p&gt;
&lt;p&gt;𝐞ₙ₋₁ = h(𝛌ₙ) ∈ 𝐑ᵐ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The inverse function of h is h⁻¹, and its input value should range from (0,1].
Therefore, if trying to solve 𝛌ₙ from 𝐞ₙ₋₁, every element in 𝐞ₙ₋₁ should be scaled to the range of (0,1] by the normalized function u(⋅). Then, 𝛌ₙ can be calculated through:&lt;/p&gt;
&lt;p&gt;𝛌ₙ = h⁻¹(u(𝐞ₙ₋₁))&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Further, the input weights of this subnetwork hidden node can be solved:&lt;/p&gt;
&lt;p&gt;$$\^𝐚ₙ = [𝐚ₙ₁,&amp;hellip;,𝐚ₙₘ] = h⁻¹(u(𝐞ₙ₋₁))⋅𝐱⁻¹$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For different activation functions, there will be:&lt;/p&gt;
&lt;p&gt;$$\rm \{^{\^𝐚ₙ = arcsin(u(𝐞ₙ₋₁))⋅𝐱⁻¹,\quad sine}
_{\^𝐚ₙ = -log( (1/u(𝐞ₙ₋₁)) - 1)⋅𝐱⁻¹,\quad sigmoid}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(This work is a continuation on ELM, that is once the 𝛃 calculated based on target 𝐭 and 𝐇⁻¹, the residual error is also fixed, so it serves as the target for 𝐚,b.
Still, applying least-square, the optimial a can be calculated based on &amp;ldquo;target&amp;rdquo; e and 𝐱⁻¹.)&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;
&lt;p&gt;^b is the mean of hidden nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The error can be reduced by adding the bias&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do feedforward using the calculated ^𝐚ₙ and ^bₙ, so this time the output of the hidden layer is:&lt;/p&gt;
&lt;p&gt;$$\^𝐇ₙᵉ = u⁻¹(h(\^𝐚ₙ⋅𝐱+\^bₙ))$$&lt;/p&gt;
&lt;p&gt;Because 𝐞ₙ₋₁ is the last output of the activation fuction, 𝐞ₙ₋₁ and $\^𝐇ₙᵉ$ are the same things.
So they can subtract from each other. Then the residual error for this time is:&lt;/p&gt;
&lt;p&gt;Δ = ‖𝐞ₙ₋₁‖² - ‖𝐞ₙ₋₁ - ^𝐇ₙᵉ‖² &lt;br&gt;
= ‖𝐞ₙ₋₁‖² - (‖𝐞ₙ₋₁‖² - 2‖𝐞ₙ₋₁‖‖^𝐇ₙᵉ‖ + ‖^𝐇ₙᵉ‖²) &lt;br&gt;
= 2‖𝐞ₙ₋₁‖‖^𝐇ₙᵉ‖ - ‖^𝐇ₙᵉ‖² &lt;br&gt;
= 2⟨𝐞ₙ₋₁, ^𝐇ₙᵉ⟩ - ‖^𝐇ₙᵉ‖² &lt;br&gt;
= ‖^𝐇ₙᵉ‖² ( 2⟨𝐞ₙ₋₁, ^𝐇ₙᵉ⟩/‖^𝐇ₙᵉ‖² - 1 )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Δ is ≥ 0&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prove the limit converges to 0 when n tends to infinity.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The target value is approximated while the error is decreased.&lt;/p&gt;
&lt;p&gt;The final estimation is the summation of the ouput of d subnetwork hidden nodes&lt;/p&gt;
&lt;p&gt;The VC dimension is lower than standard ELM, i.e., the dimension of feature space m≪ L, so the generalization ability of this method is better.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;#abstract&#34; &gt;(Back to top)&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>read: Multilayer Subnetwork Nodes</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/</link>
        <pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/</guid>
        <description>&lt;p&gt;Authors: &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=fFP4b9kAAAAJ&amp;amp;citation_for_view=fFP4b9kAAAAJ:-mN3Mh-tlDkC&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Yimin Yang&lt;/a&gt;, and Q. M. Jonathan Wu &lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/abstract/document/7295596&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IEEE Cybernetics&lt;/a&gt;; Publish Date: 2015-10-09.&lt;/p&gt;
&lt;p&gt;This is the 2nd paper in his series, and the first paper is &lt;a class=&#34;link&#34; href=&#34;https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/&#34; &gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(感觉Intro写得不错，逻辑性强，信息量大；但后面method部分好多typo)&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Representation learning of multilayer ELM with subnetwork nodes outperform conventional feature learning methods.&lt;/p&gt;
&lt;h2 id=&#34;i-introduction&#34;&gt;I. Introduction&lt;/h2&gt;
&lt;p&gt;model performance ➔ data representaiton/features ➔ processing pipelines design and data transformations ➔ data representation ➔ effective learning&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Feature reduction and extraction techniques can be conducted in a supervised, unsupervised or semi-supervised manner.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELMs learn representations of data to extract useful information when building classifiers or predictors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ELMs provide a unified learning framework for &amp;ldquo;generalized&amp;rdquo; single-hidden layer feedforward NNs (SLFNs).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In ELM methods, the hidden layer parameters of NN need not be tuned during training, but generated randomly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ML-ELM is adding (multiple) general hidden nodes (subnetwork nodes) to existing single-hidden-layer ELM networks.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A versatile platform with faster speed and better generalization performance on feature extraction.&lt;/li&gt;
&lt;li&gt;Its generalization performance is not sensitive to the parameters of the networks in the learning process.&lt;/li&gt;
&lt;li&gt;ML-ELM has universal approximation capability and representation learning.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ii-preliminaries-and-basic-elm&#34;&gt;II. Preliminaries and Basic-ELM&lt;/h2&gt;
&lt;h3 id=&#34;a-notations&#34;&gt;A. Notations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;𝐑 : set of real numbers&lt;/li&gt;
&lt;li&gt;{(𝐱ᵢ,𝐲ᵢ)ᵢ₌₁ᴹ} (𝐱ᵢ∈ 𝐑ⁿ,𝐲ᵢ∈ 𝐑ᵐ) : M arbitrary distinct samples,&lt;/li&gt;
&lt;li&gt;𝐱 : an input data matrix 𝐱∈ 𝐑ⁿᕁᴹ&lt;/li&gt;
&lt;li&gt;𝐲 : desired output data matrix 𝐲∈ 𝐑ᵐᕁᴹ&lt;/li&gt;
&lt;li&gt;𝛂ᵢ : weight vector connecting the 𝑖th hidden nodes and the input nodes&lt;/li&gt;
&lt;li&gt;♭ᵢ : bias of the 𝑖th hidden nodes&lt;/li&gt;
&lt;li&gt;βᵢ : output weight between the 𝑖th hidden node and the output node&lt;/li&gt;
&lt;li&gt;𝐞 : residual error of current network output, i.e., 𝐞=𝐲-𝐟&lt;/li&gt;
&lt;li&gt;𝐈 : unit matrix&lt;/li&gt;
&lt;li&gt;sum(𝐞) : the sum of all elements of the matrix 𝐞&lt;/li&gt;
&lt;li&gt;g : sigmoid or sine activation function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(TABLE 1)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(𝛂,♭) : a hidden node (in basic ELM)&lt;/li&gt;
&lt;li&gt;(𝐚,𝑏) : a general hidden node (or subnetwork node)&lt;/li&gt;
&lt;li&gt;^𝐚ʲ_f : input weight of the jth general hidden node in feature mapping layer. ^𝐚ʲ_f∈ 𝐑ᵈᕁⁿ&lt;/li&gt;
&lt;li&gt;^bʲ_f : bias of the 𝑗th general hidden node in feature mapping layer ^bʲ_f∈ 𝐑&lt;/li&gt;
&lt;li&gt;(𝛂ᵢʲ_f,♭ʲ_f) : the 𝑖th general hidden node in the 𝑗th general hidden node.&lt;/li&gt;
&lt;li&gt;(^𝐚ₕ,^𝑏ₕ) : hidden nodes in ELM learning layer and ^𝐚ₕ∈ 𝐑 ᵐᕁᵈ&lt;/li&gt;
&lt;li&gt;uⱼ : normalized function in the 𝑗th general node, uⱼ(⋅):𝐑 ➔ (0,1], uⱼ⁻¹ represent its reverse function&lt;/li&gt;
&lt;li&gt;𝐇ʲ_f : feature data generated by 𝑗general nodes in a feature mapping layer,
i.e., 𝐇ʲ_f = ∑ᵢ₌₁ʲ uᵢ⁻¹ ⋅ g(𝐱, ^𝐚ⁱ_f, ^bⁱ_f), 𝐇ʲ_f∈ 𝐑ᵈᕁᴹ&lt;/li&gt;
&lt;li&gt;𝐇ʲⁱ_f : feature data generated by the 𝑖th feature mapping layer&lt;/li&gt;
&lt;li&gt;M : number of training samples&lt;/li&gt;
&lt;li&gt;n : input data dimension&lt;/li&gt;
&lt;li&gt;m : output data dimension&lt;/li&gt;
&lt;li&gt;d : feature data dimension&lt;/li&gt;
&lt;li&gt;𝐞_L : the residual error of current two-layer network (L general nodes in the first layer and (𝐚ₕ,𝑏ₕ) in the second layer)&lt;/li&gt;
&lt;li&gt;𝐞ʲ_L : the residual error of current two-layer network (L general nodes in the first layer and 𝑗general nodes in the second layer)&lt;/li&gt;
&lt;li&gt;L : the numbers of general hidden nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;b-basic-elm&#34;&gt;B. Basic-ELM&lt;/h3&gt;
&lt;p&gt;The output function of ELM for SLFNs fed with input matrix 𝐱 is: &lt;br&gt;
fₙ(𝐱)=∑ᵢ₌₁ⁿ βᵢ g(𝐱, 𝛂ᵢ, ♭ᵢ).&lt;/p&gt;
&lt;p&gt;&amp;ldquo;ELM theory aims to reach the smallest training error but also the smallest norm of output weights&amp;rdquo; (regularization term?), so the objective is to minimize: &lt;br&gt;
‖βᵢ‖ₚᶣ¹ + C⋅‖∑ᵢ₌₁ⁿ βᵢ g(𝐱, 𝛂ᵢ, ♭ᵢ) - 𝐲‖ᶣ²_q, i=1,&amp;hellip;,n.   (ᶣ signifies μ)&lt;/p&gt;
&lt;p&gt;where μ₁&amp;gt;0, μ₂&amp;gt;0, p,q = 0, ½, 1, 2, &amp;hellip;, +∞, C is a positive value, g(𝐱, 𝛂, ♭) is referred to as ELM feature mapping (linear projection+activation) or Huang&amp;rsquo;s transform.&lt;/p&gt;
&lt;p&gt;(Convergence proved by Huang et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;: Given M aribitrary distinct samples {(𝐱, 𝐲)}, 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ sampled from a continuous system, an activation function g,
then for any continous target function 𝐲 and any function sequence g(𝐱, 𝛂ₙʳ, ♭ₙʳ) randomly generated based on any continuous sampling distribution,
lim_{n➝∞} ‖𝐲-（fₙ₋₁ + g(𝐱, 𝛂ₙʳ, ♭ₙʳ)）‖=0 holds with probabiltiy one if &lt;br&gt;
βₙ = ⟨𝐞ₙ₋₁, g(𝐱, 𝛂ₙʳ, ♭ₙʳ)⟩ / ‖g(𝐱, 𝛂ₙʳ, ♭ₙʳ)‖²,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where (𝛂ₙʳ, ♭ₙʳ) represesnts the 𝑛th random hidden node, and 𝐞ₙ₋₁ = 𝐲-fₙ₋₁&lt;/p&gt;
&lt;h2 id=&#34;iii-proposed-method&#34;&gt;III. Proposed Method&lt;/h2&gt;
&lt;h3 id=&#34;a-elm-with-subnetwork-nodes&#34;&gt;A. ELM With Subnetwork Nodes&lt;/h3&gt;
&lt;p&gt;A hidden node can be a subnetwork formed by several hidden nodes. Hence, a single mapping layer can contain multiple networks.&lt;/p&gt;
&lt;p&gt;Comparision of the feature mapping layer:&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
subgraph A[basic ELM]
direction BT
x1[&#34;x₁&#34;]--&gt; h1((&#34;𝛂₁,♭₁, β₁&#34;)) &amp; he1((...)) &amp; hL((&#34;𝛂L,♭L, βL&#34;)) 
xe[x...]--&gt; h1((&#34;𝛂₁,♭₁, β₁&#34;)) &amp; he1((...)) &amp; hL((&#34;𝛂L,♭L, βL&#34;)) 
xn[&#34;xₙ&#34;]--&gt; h1((&#34;𝛂₁,♭₁, β₁&#34;)) &amp; he1((...)) &amp; hL((&#34;𝛂L,♭L, βL&#34;)) 

h1 --&gt; y1[&#34;y₁&#34;] &amp; ye[...] &amp; ym[&#34;yₘ&#34;]
he1--&gt; y1[&#34;y₁&#34;] &amp; ye[...] &amp; ym[&#34;yₘ&#34;]
hL --&gt; y1[&#34;y₁&#34;] &amp; ye[...] &amp; ym[&#34;yₘ&#34;]

subgraph A1[&#34;ELM feature mapping layer&#34;]
h1 &amp; he1 &amp; hL
end
end

subgraph A1[&#34;ELM feature mapping layer&#34;]
h1 &amp; he1 &amp; hL
end

subgraph B[ELM with subnetwork nodes]
direction BT
x1_[&#34;x₁&#34;] --&gt; ghn1 &amp; ghne((...)) &amp; ghnL
xe_[x...] --&gt; ghn1 &amp; ghne((...)) &amp; ghnL
xn_[&#34;xₙ&#34;] --&gt; ghn1 &amp; ghne((...)) &amp; ghnL

ghn1--&gt; y1_[&#34;y₁&#34;] &amp; ye_[...] &amp; ym_[&#34;yₘ&#34;]
ghne--&gt; y1_[&#34;y₁&#34;] &amp; ye_[...] &amp; ym_[&#34;yₘ&#34;]
ghnL--&gt; y1_[&#34;y₁&#34;] &amp; ye_[...] &amp; ym_[&#34;yₘ&#34;]
end

subgraph ghn1[&#34;^𝛂¹_f, ^♭¹_f, with weight u₁⁻¹&#34;]
direction TB
n11((&#34;𝛂¹_f1,\n ♭¹_f1&#34;)) &amp; n1e((...)) &amp; n1m((&#34;𝛂¹_fm,\n ♭¹_fm&#34;))
end

subgraph ghne[&#34;general hidden nodes&#34;]
direction TB
ne1((1)) &amp; nee((...)) &amp; nem((m))
end

subgraph ghnL[&#34;^𝛂ᴸ_f, ^♭_f, with weight u\_L⁻¹&#34;]
direction TB
nL1((&#34;𝛂ᴸ_f1,\n ♭ᴸ_f1&#34;)) &amp; nLe((...)) &amp; nLm((&#34;𝛂ᴸ_fm,\n ♭ᴸ_fm&#34;))
end
&lt;/div&gt;

&lt;p&gt;Three differences between ELM feature mapping layer and this feature mapping layer.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Difference&lt;/th&gt;
&lt;th&gt;Standard ELM&lt;/th&gt;
&lt;th&gt;ELM with subnetwork nodes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hidden node&lt;/td&gt;
&lt;td&gt;single hidden node generated&lt;br&gt; one by one&lt;/td&gt;
&lt;td&gt;general hidden node having subnetwork&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;# hidden node&lt;/td&gt;
&lt;td&gt;Independent to the output dim 𝑚&lt;/td&gt;
&lt;td&gt;In a subnetwork, it equals to the output dim&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;relation&lt;/td&gt;
&lt;td&gt;A special case of the subnetwork case&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;b-proposed-method-for-representation-learning&#34;&gt;B. Proposed Method for Representation Learning&lt;/h3&gt;
&lt;h4 id=&#34;1-optimal-projecting-parameters-and-optimal-feature-data&#34;&gt;1) Optimal Projecting Parameters and Optimal Feature Data&lt;/h4&gt;
&lt;div class=&#34;mermaid&#34;&gt;flowchart LR
x1[&#34;x₁&#34;]--&gt; h1((&#34;^𝛂¹_f,^♭¹_f, β¹&#34;)) &amp; h2((&#34;^𝛂²_f,^♭²_f, β²&#34;)) &amp; he1((&#34;⋮&#34;)) &amp; hL((&#34;^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ&#34;)) 
xe[x...]--&gt; h1((&#34;^𝛂¹_f,^♭¹_f, β¹&#34;)) &amp; h2((&#34;^𝛂²_f,^♭²_f, β²&#34;)) &amp; he1((&#34;⋮&#34;)) &amp; hL((&#34;^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ&#34;)) 
xn[&#34;xₙ&#34;]--&gt; h1((&#34;^𝛂¹_f,^♭¹_f, β¹&#34;)) &amp; h2((&#34;^𝛂²_f,^♭²_f, β²&#34;)) &amp; he1((&#34;⋮&#34;)) &amp; hL((&#34;^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ&#34;)) 

h1 &amp; h2 &amp; he1 &amp; hL --&gt; feat[&#34;d-dimension\n Feature data&#34;]
feat --&gt; n1 &amp; n2 &amp; ne &amp; nm --&gt; y1_[&#34;y₁&#34;] &amp; ye_[&#34;⋮&#34;] &amp; ym_[&#34;yₘ&#34;]

subgraph A1[&#34;ELM feature mapping layer&#34;]
h1 &amp; h2 &amp; he1 &amp; hL
end

subgraph elm[&#34;ELM-learning layer&#34;]
n1((&#34;𝛂ₕ₁,^♭ₕ&#34;)) &amp; n2((&#34;𝛂ₕ₂,^♭ₕ&#34;)) &amp; ne((&#34;⋮&#34;)) &amp; nm((&#34;𝛂ₕₘ,^♭ₕ&#34;))
end
&lt;/div&gt;

&lt;p&gt;Objective of representation learning: Represent the input features meaningfully in several different representations as follows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Represen-&lt;br&gt;tation&lt;/th&gt;
&lt;th&gt;feat dim (𝑑) vs&lt;br&gt; in-dim (𝑛)&lt;/th&gt;
&lt;th&gt;feature&lt;/th&gt;
&lt;th&gt;target output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Dimension Reduction&lt;/td&gt;
&lt;td&gt;𝑑 &amp;lt; 𝑛&lt;/td&gt;
&lt;td&gt;H_f ∈ 𝐑ᵈᕁᴹ&lt;/td&gt;
&lt;td&gt;𝐲=label (Supervise)&lt;br&gt;or 𝐲=𝐱 (Unsp~)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Expanded Dimension&lt;/td&gt;
&lt;td&gt;𝑑 &amp;gt; 𝑛&lt;/td&gt;
&lt;td&gt;H_f ∈ 𝐑ᵈᕁᴹ&lt;/td&gt;
&lt;td&gt;𝐲=label (Supervise)&lt;br&gt;or 𝐲=𝐱 (Unsp~)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The feature data is 𝐇_f(𝐱ₖ, ^𝐚_f, ^b_f), where the weights of feature mapping layer ^𝐚ʲ_f, j=1,&amp;hellip;,L belongs to 𝐑ᵈᕁⁿ.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;: Given a nonlinear piecewise continous activation function g, we call &lt;br&gt;
{(^𝐚ʲ_f, ^bʲ_f)ⱼ₌₁ᴸ} (^𝐚ʲ_f ∈ 𝐑ᵈᕁⁿ) the 𝐿 optimal general hidden nodes &lt;br&gt;
and 𝐇⃰ ⃰_f= ∑ᵢ₌₁ᴸ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) the optimal feature data if it satisfies: &lt;br&gt;
‖𝐞_L‖ ≤ min_{𝐇⃰ᴸ_f∈ 𝐑ᵈᕁᴹ} ( min_{𝐚ₕ∈ 𝐑 ᵐᕁᵈ} ‖𝐲-uₕ⁻¹ g(𝐇⃰ᴸ_f, 𝐚ₕ, bₕ)‖ )    (4)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where 𝐞_L = ‖𝐲-uₕ⁻¹ g(𝐇⃰ᴸ ⃰_f, ^𝐚ₕ, ^bₕ)‖ and sequence ‖𝐞_L‖ is decreasing and bounded below by zero.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remark 1&lt;/strong&gt;: If the optimal projecting parameters are obtained in the feature mapping layer
{(^𝐚ʲ_f, ^bʲ_f)ⱼ₌₁ᴸ} (where ^𝐚_f ∈ 𝐑ᵈᕁⁿ), &lt;br&gt;
the original n-dimension data points 𝐱 will be converted to d-dimension data points:
𝐇⃰_f= ∑ⱼ₌₁ᴸ g(𝐱ₖ, ^𝐚ʲ_f, ^bʲ_f), which satisfy the inequality (4).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thus the purpose is to find optimal projecting parameters that make the inequality (4) true for all data points.&lt;/p&gt;
&lt;h4 id=&#34;2-learning-steps&#34;&gt;2) Learning Steps&lt;/h4&gt;
&lt;p&gt;Based on the inverse of the activation function.&lt;/p&gt;
&lt;p&gt;Given M arbitrary distinct training samples {(𝐱ₖ,𝐲ₖ)ₖ₌₁ᴹ}, 𝐱ₖ∈ 𝐑ⁿ, 𝐲ₖ∈ 𝐑ᵐ, which are sampled from a continuous system.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Set j=1 to initialize a &lt;strong&gt;general node&lt;/strong&gt; of the feature mapping layer randomly as: &lt;br&gt;
𝐇⃰ʲ_f = g(^𝐚ʲ_f⋅𝐱 + ^bʲ_f), (^𝐚ʲ_f)ᵀ⋅^𝐚ʲ_f=𝐈, (^bʲ_f)ᵀ⋅^bʲ_f=1,&lt;/p&gt;
&lt;p&gt;where ^𝐚ʲ∈ 𝐑ᵈᕁⁿ, ^bʲ_f∈ 𝐑 is the orthogonal random weight and bias of feature mapping layer. 𝐇⃰ʲ_f is current feature data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given a sigmoid or sine activation function g, for any continous desired outputs 𝐲, the parameters in the (general) ELM learning layer are obtained as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^𝐚ₕ = g⁻¹(uₙ(𝐲)) ⋅ (𝐇⃰ʲ_f)⁻¹, ^𝐚ʲₕ∈ 𝐑ᵈᕁᵐ,&lt;/li&gt;
&lt;li&gt;^bₕ = √mse(^𝐚ₕʲ ⋅ 𝐇⃰ʲ_f - g⁻¹(uₙ(𝐲)) ), ^bʲₙ∈ 𝐑,&lt;/li&gt;
&lt;li&gt;$g⁻¹(⋅) = \{^{arcsin(⋅) \quad if\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \quad if\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where 𝐇⃰⁻¹ = 𝐇⃰ᵀ( (C/𝐈) + 𝐇⃰ 𝐇⃰ᵀ)⁻¹; C is a positive value; uₙ is a normalized function
uₙ(𝐲): 𝐑➔(0,1]; g⁻¹ and uₙ⁻¹ represent their reverse function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the output error 𝐞ⱼ as &lt;br&gt;
𝐞ⱼ = 𝐲 - uₙ⁻¹ g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ)  &lt;br&gt;
So the error feedback data is 𝐏ⱼ = g⁻¹(uₙ(𝐞ⱼ))⋅(^𝐚ₕ)⁻¹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set j=j+1, add a new general node (^𝐚ʲ_f, ^bʲ_f) in the feature mapping layer by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^𝐚ʲ_f = g⁻¹( uⱼ(𝐏ⱼ₋₁) ) ⋅ 𝐱⁻¹, ^𝐚ʲ_f∈ 𝐑ⁿᕁᵈ&lt;/li&gt;
&lt;li&gt;^bʲ_f = √mse(^𝐚ʲ_f ⋅ 𝐱 - 𝐏ⱼ₋₁), ^bʲ∈ 𝐑&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and update the feature data 𝐇⃰ʲ_f = ∑ᵢ₌₁ʲ uₗ⁻¹ g(𝐱, ^𝐚ˡ_f, ^bˡ_f)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeat step 2-4 𝐿-1 times. (Finally, 𝐿 nodes are added into feature mapping layer.)
The set of parameters {^𝐚ʲ_f,^bʲ_f}ⱼ₌₁ᴸ are the optimal projecting parameters
and the feature data 𝐇⃰ᴸ_f = ∑ⱼ₌₁ᴸ uⱼ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) = 𝐇⃰ ⃰_f are the optimal feature data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;c-proof-of-the-proposed-method&#34;&gt;C. Proof of the Proposed Method&lt;/h3&gt;
&lt;p&gt;(Proof of Convergence)&lt;/p&gt;
&lt;p&gt;Given M arbitrary distinct samples {(𝐱ₖ,𝐲ₖ)}ₖ₌₁ᴹ (𝐱ₖ∈ 𝐑ⁿ, 𝐲ₖ∈ 𝐑ᵐ)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;: Given a bounded nonconstant piecewise continuous activation function g, we have &lt;br&gt;
lim_{(𝛂,♭)→(^𝛂,^♭)} ‖g(𝐱,𝛂,♭) - g(𝐱,^𝛂,^♭)‖ = 0
where the (^𝛂,^♭) is one of the least-squares solutions of a general linear system 𝛂⋅𝐱+♭.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Remark 2:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Lemma 2 shows that SLFN training problem can be considered as finding optimal hidden parameters which satisfy:
g(^𝛂₁,^♭₁) + &amp;hellip; + g(^𝛂_L,^♭_L) → 𝐲.   𝛂 (alpha) stands for basic ELM hidden node.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thus training an SLFN is equivalent to finding a least-square general input weight ^𝐚ₕ of the (linear+activation) system g(^𝐚ₕ⋅𝐱) = 𝐲.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If activation function g is invertible, the input weights matrix can be obtained by pulling back the residual error to the hidden layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, if g is a sine function,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The output of the hidden layer matrix is 𝐲=sin(𝐚ₕ ⋅ 𝐱).&lt;/li&gt;
&lt;li&gt;Thus, 𝐚ₕ⋅𝐱 = arcsin(𝐲), 𝐲∈ (0,1].&lt;/li&gt;
&lt;li&gt;The smallest norm least-squares solution of the linear system sin(𝐚ₕ⋅𝐱)=𝐲 is: &lt;br&gt;
^𝐚ₕ = arcsin(𝐲)⋅𝐱⁻¹,
where 𝐱⁻¹ is the Moore-Penrose generalized inverse of matrix 𝐱. 𝐱⁻¹ = 𝐱ᵀ( (C/𝐈) + 𝐱𝐱ᵀ)⁻¹&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: Given M arbitrary distinct samples {(𝐱ᵢ,𝐲ᵢ)ᵢ₌₁ᴹ}, (𝐱ᵢ∈ 𝐑ⁿ, 𝐲ᵢ∈ 𝐑ᵐ) and a sigmoid or sine activation function g, for any continuous desired outputs 𝐲, we have:&lt;br&gt;
the optimal weights ^𝐚ₕ = argmin_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹(g(𝐱,𝐚ₕ)) - 𝐲‖ &lt;br&gt;
least square error ‖g(𝐱,^𝐚ₕ,^bₕ) - 𝐲‖ ≤ min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹(g(𝐚ₕ⋅𝐱)) - 𝐲‖  &lt;br&gt;
if the parameters are obtained by (similar to Algorithm step-2):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;^𝐚ₕ = g⁻¹( u(𝐲))⋅𝐱⁻¹, ^𝐚ₕ ∈ 𝐑ᵐᕁⁿ&lt;/li&gt;
&lt;li&gt;^bₕ = √mse(^𝐚ₕ⋅𝐱 - g⁻¹(u(𝐲))), ^bₕ∈ 𝐑&lt;/li&gt;
&lt;li&gt;$g⁻¹(⋅) = \{^{arcsin(⋅) \quad if\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \quad if\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Let 𝛌=𝐚ₕ⋅𝐱, and 𝛌 satisfy g(𝛌) = 𝐲. Normalizing 𝐲 to (0,1] by u(𝐲) to let 𝛌∈ 𝐑. &lt;br&gt;
Thus, for a sine hidden node, 𝛌 = g⁻¹(u(𝐲)) = arcsin(u(𝐲)).
While for a sigmoid hidden node, 𝛌 = g⁻¹(u(𝐲)) = -log(1/u(𝐲) - 1).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;^𝐚ₕ is the solution for the linear system (g(𝐚ₕ⋅𝐱)=𝐲). &lt;br&gt;
For sine activation: ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ = arcsin(u(𝐲))⋅𝐱⁻¹.
For sigmoid activation: ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ = -log(1/u(𝐲) - 1)⋅𝐱⁻¹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the least-squares solutions of a general linear system 𝐚ₕ⋅𝐱=𝛌 is ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹, which means the smallest error can be reached by this solution:
‖^𝐚ₕ⋅𝐱 -𝛌ₙ‖ = min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖𝐚ₕ⋅𝐱 - g⁻¹( u(𝐲) )‖     (18)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The special solution ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ has the smallest norm among all the least-squares solutions of 𝐚ₕ⋅𝐱 = 𝛌.
The error can be further reduced by adding bias bₙ:
^bₕ = √mse(^𝐚ₕ⋅𝐱 - h⁻¹( u(𝐲) ))&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on eq. (18) and Lemma2, optimization by minimizing the L2-loss can be reformulated as:
min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹( g(𝐚ₕ⋅𝐱) ) - u⁻¹( g(𝛌))‖
= ‖u⁻¹( g(^𝐚ₕ⋅𝐱) ) - u⁻¹( g(𝛌))‖
≥ ‖u⁻¹( g(^𝐚ₕ⋅𝐱 + ^bₕ) ) - 𝐲‖      (20)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on eq. (18) and eq. (20), the optimal weights is proved as:
^aₕ = arg min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖g(𝐱,𝐚ₕ) - 𝐲‖ &lt;br&gt;
And it satisfy: ‖g(𝐱,^𝐚ₕ,^bₕ) - 𝐲‖ ≤ min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹( g(^𝐚ₕ⋅𝐱) ) - 𝐲 ‖&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Based on Lemma 2 and Theorem 1, Theorem 2 is given:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt;: Given M arbitrary distinct samples (𝐱, 𝐲), 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ, a sigmoid or sine activation function g,
and the initial orthogonal random weights ^𝐚¹_f and bias ^b¹_f.
For any continuous desired output 𝐲, the optimal feature data is: &lt;br&gt;
𝐇⃰ᴸ⃰ _f(𝐱, (^𝐚¹_f, &amp;hellip;, ^𝐚ᴸ_f), (^b¹_f,&amp;hellip;,^bᴸ_f))
= ∑ⱼ₌₁ᴸ uⱼ⁻¹ g(^𝐚ʲ_f ⋅ 𝐱 + ^bʲ_f)
which satisfy: &lt;br&gt;
‖𝐞_L‖ ≤ min_{^𝐚ʲ_f∈ 𝐑ⁿᕁᵈ} ( min_{𝐚ₕ∈ 𝐑 ᵐᕁᵈ} ‖𝐲-uₙ⁻¹ g(𝐇⃰ᴸ_f, 𝐚ₕ, bₕ)‖)    (21)&lt;/p&gt;
&lt;p&gt;and ‖𝐞_L‖ is decreasing and bounded below by zero if these parameters are obtained by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;𝐇⃰ʲ_f = ∑ᵢ₌₁ʲ uᵢ⁻¹ g(𝐱, ^𝐚ⁱ_f, ^bⁱ_f),&lt;/li&gt;
&lt;li&gt;^𝐚ₕ = g⁻¹(uₙ(𝐲)) ⋅ (𝐇⃰ʲ_f)⁻¹, ^𝐚ₕ∈ 𝐑 ᵐᕁᵈ,&lt;/li&gt;
&lt;li&gt;^bₕ = √mse(^𝐚ₕ⋅𝐇⃰ʲ_f - g⁻¹( u(𝐲) )), ^bₕ∈ 𝐑&lt;/li&gt;
&lt;li&gt;g⁻¹(⋅) = \{^{arcsin(⋅) \quad if\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \quad if\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _&lt;/li&gt;
&lt;li&gt;𝐞ⱼ = 𝐲 - uₙ⁻¹( g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ), 𝐏ⱼ = g⁻¹(uₙ(𝐞ⱼ))⋅(^𝐚ₕ)⁻¹ ),&lt;/li&gt;
&lt;li&gt;^𝐚ʲ_f = g⁻¹(uⱼ(𝐏ⱼ₋₁)) ⋅ 𝐱⁻¹, ^𝐚ʲ∈ 𝐑ⁿᕁᵈ&lt;/li&gt;
&lt;li&gt;^bʲ_f = √mse(^𝐚ʲ_f ⋅ 𝐱 - 𝐏ⱼ₋₁), ^bʲ_f∈ 𝐑&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;Base on Theorem 1, the validity of (21) is obvious. So here, we just prove that the error ‖𝐞_L‖ is decreasing and bounded below by zero.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Let Δ = ‖eⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ)‖² (last error-current output), and take the newest item apart: &lt;br&gt;
= ‖eⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g( (∑ᵢ₌₁ʲ⁻¹ uᵢ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) + uᵢ⁻¹g(𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ) ‖²      (24)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let ^Tʲ = uₙ⁻¹g(uⱼ⁻¹g(𝐱, ^𝐚ʲ_f, ^bʲ_f), ^𝐚ₕ, ^bₕ). Because activation function is sigmoid or sine function, eq. (24) can be simplified as: &lt;br&gt;
Δ ≥ ‖𝐞ⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g( (∑ᵢ₌₁ʲ⁻¹ uᵢ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ) - ^Tʲ‖² &lt;br&gt;
= ‖𝐞ⱼ₋₁‖² - ‖𝐞ⱼ₋₁ - ^Tʲ‖²    (unfold)   &lt;br&gt;
= ‖𝐞ⱼ₋₁‖² - (‖𝐞ⱼ₋₁‖² - 2&amp;lt;eⱼ₋₁, ‖^Tʲ‖&amp;gt; + ‖^Tʲ‖²)   (&amp;quot;&amp;lt;&amp;gt;&amp;quot; is dot product of 2 matrices: Frobenius inner product)&lt;br&gt;
= 2&amp;lt;𝐞ⱼ₋₁, ‖^Tʲ‖&amp;gt; - ‖^Tʲ‖²   &lt;br&gt;
= ‖^Tʲ‖² ( 2&amp;lt;𝐞ⱼ₋₁, ‖^Tʲ‖&amp;gt; / ‖^Tʲ‖² - 1 )     (25)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We set ^Tʲ = uₙ⁻¹g(uⱼ⁻¹g( 𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ ) = 𝐞ⱼ₋₁ ± σ. (σ is variance, and 𝐞ⱼ₋₁ is the expectation) &lt;br&gt;
So 𝐞ⱼ₋₁ = ^Tʲ ± σ. &lt;br&gt;
Then &amp;lt;𝐞ⱼ₋₁, ‖^Tʲ‖&amp;gt; = &amp;lt;^Tʲ± σ, ‖^Tʲ‖&amp;gt; = &amp;lt;‖^Tʲ‖² ± &amp;lt;‖^Tʲ‖,σ&amp;gt; &amp;gt;&lt;/p&gt;
&lt;p&gt;Hence, eq. (25) can be reformulated: &lt;br&gt;
Δ ≥ ‖^Tʲ‖² ( 2&amp;lt; ‖^Tʲ‖² ± &amp;lt;‖^Tʲ‖,σ&amp;gt; &amp;gt; / ‖^Tʲ‖² - 1 ) &lt;br&gt;
= ‖^Tʲ‖² ( 1 ± 2‖σ⋅(^Tʲ)ᵀ‖/‖^Tʲ‖²)  (Wandong thinks there should be a 2.) &lt;br&gt;
≥&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, based on Theorem 1 and eq. (7), there will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‖^Tʲ - 𝐞ⱼ₋₁‖ ≤ min_{^𝐚ʲ_f∈ 𝐑ᵈᕁⁿ} ‖uₙ⁻¹g(uⱼ⁻¹g( 𝐱, ^𝐚ʲ_f, ^bʲ_f), ^𝐚ₕ, ^bₕ) -𝐞ⱼ₋₁‖&lt;/li&gt;
&lt;li&gt;‖σ‖ ≤ ‖^Tʲ‖&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus Δ ≥ 0 can be proved as:
Δ ≥ ‖^Tʲ‖² (1 ± ‖σ‖ / ‖^Tʲ‖) ≥ 0    (28)&lt;/p&gt;
&lt;p&gt;Eq. (28) means ‖𝐞ⱼ₋₁‖ ≥ ‖𝐞ⱼ‖ and ‖𝐞‖ is decreasing and bounded below by zero.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Based on Theorem 2, Theorem 3 is given:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 3&lt;/strong&gt;: Given M arbitrary distinct samples (𝐱, 𝐲), 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ, a sigmoid or sine activation function g,
and optimal feature data 𝐇⃰ᴸ_f obtained by Algorithm 1, &lt;br&gt;
then lim_{j➝+∞} ‖𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - &amp;hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖ = 0
holds with probability one if :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;𝐚ⱼ = g⁻¹( u(𝐲) ) ⋅ (𝐇⃰ᴸ_f)⁻¹, ^𝐚ⱼ∈ 𝐑ᵐᕁⁿ&lt;/li&gt;
&lt;li&gt;bⱼ = √mse(^𝐚ⱼ⋅(𝐇⃰ᴸ_f) - g⁻¹(u(𝐲))), ^bⱼ∈ 𝐑&lt;/li&gt;
&lt;li&gt;βⱼ = ⟨𝐞ⱼ₋₁, g(𝐇⃰ᴸ_f, 𝐚ⱼ, bⱼ)⟩ / ‖g(𝐇⃰ᴸ_f, 𝐚ⱼ, bⱼ)‖², βⱼ∈ 𝐑&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proof:&lt;/p&gt;
&lt;p&gt;First prove that the sequence ‖𝐞ⱼᴸ‖ is decreasing and bounded below by zero.
Then prove that the lim_{j➝+∞} ‖𝐞ⱼᴸ‖ = 0&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Based on Theorem 1 and Lemma 1, the network output error satisfies:
‖𝐞ⱼᴸ‖ = ‖𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - &amp;hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖  &lt;br&gt;
≤ ‖𝐲 -u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁)‖  &lt;br&gt;
= ‖𝐞₁ᴸ‖&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on Theorem 2, there will be: &lt;br&gt;
‖𝐞ⱼᴸ‖ ≤ ‖𝐞ⱼᴸ⁻¹‖ ≤ &amp;hellip; ≤ ‖𝐞ⱼ¹‖&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thus, ‖𝐞ⱼᴸ‖ ≤ ‖𝐞ⱼᴸ⁻¹‖ ≤ &amp;hellip; ≤ ‖𝐞ⱼ¹‖ ≤ &amp;hellip; ≤ ‖𝐞₁¹‖ and ‖𝐞ⱼᴸ‖ is decreasing and bounded below by 0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on Lemma 1, when all hidden nodes randomly generated based on any continuous sampling distribution,
lim_{n➝∞} ‖f - (fₙ₋₁ + βₙ⋅g(𝐱, 𝛂ₙʳ, ♭ₙʳ) )‖ = 0 holds with probability one if
βₙ = ⟨𝐞ₙ₋₁, g(𝐇⃰ᴸⱼ, 𝛂ₙʳ, ♭ₙʳ)⟩ / ‖g(𝐇⃰ᴸⱼ, 𝛂ₙʳ, ♭ₙʳ)‖².&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, ELM theories have shown that almost any nonlinear piecewise continuous random hidden node can be use in ELM, and the resultant networks have universal approximation capbilities. &lt;br&gt;
According to the definition of general hidden neurons, (a general hidden node contains m (basic) hidden node),
a general hidden node (𝐚,b) = (𝛂ʳ₁, &amp;hellip;, 𝛂ʳₘ, bʳ₁, &amp;hellip;, bʳₘ), . &lt;br&gt;
Thus its output is g(𝐇⃰ᴸ_f, 𝐚ⱼʳ, ♭ⱼʳ) ≡ ∑ᵢ₌₁ᵐ g(𝐇⃰ᴸ_f, 𝛂ʳᵢ, bʳᵢ).&lt;/p&gt;
&lt;p&gt;Therefore, lim_{j➝∞} ‖ 𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - &amp;hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖&lt;br&gt;
= lim_{n➝∞} ‖f- (fₙ₋₁ + βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼʳ, 𝑏ⱼʳ)) ‖ = 0&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;d-proposed-method-with-multinetwork-structures&#34;&gt;D. Proposed Method With Multinetwork Structures&lt;/h3&gt;
&lt;div class=&#34;mermaid&#34;&gt;%%{ init: { &#39;flowchart&#39;: { &#39;curve&#39;: &#39;basis&#39; } } }%%
flowchart LR
subgraph in[&#34;input feature&#34;]
x1((x1)) &amp; xe((&#34;⋮&#34;)) &amp; xn((xn))
end
subgraph net1[&#34;Layer 1&#34;]
l11((a,b)) &amp; l1e((&#34;⋮&#34;)) &amp; l1L((a,b))
end
subgraph net2[&#34;Layer 2&#34;]
l21((a,b)) &amp; l2e((&#34;⋮&#34;)) &amp; l2L((a,b))
end
subgraph netC[&#34;Layer C&#34;]
lC1((a,b)) &amp; lCe((&#34;⋮&#34;)) &amp; lCL((a,b))
end
x1 &amp; xn --&gt; l11 &amp; l1L
l11 &amp; l1L --&gt; l21 &amp; l2L
l21 &amp; l2L -.-&gt; lC1 &amp; lCL

subgraph out[&#34;output y&#34;]
direction LR
y1((1)) &amp; ye((&#34;⋮&#34;)) &amp; ym((m))
end
subgraph belm1[&#34;Basic ELM 1&#34;]
direction LR
b11((&#34;aₕ,bₕ&#34;)) &amp; b1e((&#34;⋮&#34;)) &amp; b1m((aₕ,bₕ))
end
subgraph belm2[&#34;Basic ELM 2&#34;]
direction LR
b21((aₕ,bₕ)) &amp; b2e((&#34;⋮&#34;)) &amp; b2m((aₕ,bₕ))
end

net1 --&gt; feat1[&#34;Feature\n data\n 𝐇¹&lt;sub&gt;f&lt;/sub&gt;&#34;] --&gt; belm1 --&gt; out
feat1 --&gt; net2 --&gt; feat2[&#34;Feature\n data\n 𝐇²&lt;sub&gt;f&lt;/sub&gt;&#34;] --&gt; belm2 --&gt; out
netC --&gt; featC[&#34;Feature\n data\n 𝐇ᶜ&lt;sub&gt;f&lt;/sub&gt;&#34;]

subgraph MultiLayer ELM
in &amp; net1 &amp; net2 &amp; netC
end
%% inverse for initialization weights
linkStyle 12,13,14,16,17,18 stroke:#f0f
&lt;/div&gt;

&lt;p&gt;&lt;font color=&#34;#f0f&#34;&gt;Pink links&lt;/font&gt; will do inverse to calculate the weights for corresponding layers.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>images</title>
        <link>https://zichen34.github.io/writenotes/model/subnetwork/img/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/subnetwork/img/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
