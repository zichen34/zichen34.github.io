<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>models on Zichen Wang</title>
        <link>https://zichen34.github.io/writenotes/model/</link>
        <description>Recent content in models on Zichen Wang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 02 Jun 2024 10:47:00 +0000</lastBuildDate><atom:link href="https://zichen34.github.io/writenotes/model/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>read: NVS with Pose-conditioned Diffusion Models</title>
        <link>https://zichen34.github.io/writenotes/model/nvs/b-note-nvs-dm-posecond/</link>
        <pubDate>Sat, 12 Aug 2023 09:40:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nvs/b-note-nvs-dm-posecond/</guid>
        <description>&lt;img src="https://ar5iv.labs.arxiv.org/html/2210.04628/assets/figures/training.png" alt="Featured image of post read: NVS with Pose-conditioned Diffusion Models" /&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/a6o/3d-diffusion-pytorch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code-pytroch&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.04628&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=HtoA0oT30jC&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OpenReview&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qa&#34;&gt;Q&amp;amp;A&lt;/h2&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Condition image vs target image?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;img2img&lt;/strong&gt; diffusion model is conditioned with &lt;strong&gt;pose&lt;/strong&gt; and a &lt;strong&gt;single&lt;/strong&gt; source view to generate multiviews.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stochastic conditioning: Randomly select a view from &lt;strong&gt;avaliable views&lt;/strong&gt; as condition image at each &lt;strong&gt;denoising step&lt;/strong&gt; during &lt;strong&gt;sampling&lt;/strong&gt;?,
rather than using only the given view.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reconstruct a NeRF to &lt;strong&gt;measure&lt;/strong&gt; 3D consistency of multi-views.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NeRF is not their ultimate objective.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Regressive&lt;/strong&gt; methods for NVS from sparse views based on NeRF are still not &lt;strong&gt;generalizable&lt;/strong&gt; enough or able to produce high-quality completion for the &lt;strong&gt;occluded&lt;/strong&gt; parts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Regularized NeRF (RegNeRF) suffer from artifacts when only few views are given &lt;del&gt;because they&lt;/del&gt; and didn&amp;rsquo;t apply the features of commen prior of multiple scenes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regressing a NeRF from image feataures (pixel-NeRF) tend to get blurred images.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Geometry-free&lt;/strong&gt; methods for NVS obtain colors that aren&amp;rsquo;t directly derived from volume rendering.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Light field network&lt;/li&gt;
&lt;li&gt;Scene Representation Transformer&lt;/li&gt;
&lt;li&gt;EG3D combines StyleGAN and volume rendering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3D diffusion model is a &lt;strong&gt;generative and geometry-free&lt;/strong&gt; method.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;strong&gt;pairs&lt;/strong&gt; of images of the &lt;strong&gt;same&lt;/strong&gt; scene to train a diffusion model.&lt;/li&gt;
&lt;li&gt;During training, one of them serves as the original, and the other is the condition image.&lt;/li&gt;
&lt;li&gt;The trained model can produce a multi-view set of a scene given one condition image.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;They consider multiple views from a scene are &lt;strong&gt;not independent&lt;/strong&gt;, but follow the distribution of the &lt;strong&gt;training views&lt;/strong&gt;, to enhance multi-view consistency.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The distributions of different views, given a scene with a total observation set 𝐒, $p(𝐱|S)$ are conditionally independent (different).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NeRF solves NVS under an even strict condition: each ray in the scen is conditionally independent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;However, with this nature, the diffusion model cannot guarantee the samplings (generated images), conditioned with different source view, follow a common distribution,
i.e., the diffusion model needs a unique distribution to learn.&lt;/p&gt;
&lt;p&gt;Ideally, the common distribution should be p(S), but it&amp;rsquo;s difficult to approximate the entire scene based on sparse views. (Not sure, my guess.)&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why they reused the generated views previously for later condition.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pose-conditioned&#34;&gt;Pose-conditioned&lt;/h3&gt;
&lt;p&gt;Given the data distribution p(𝐱₁, 𝐱₂), diffusion model learns the distribution of one of the two images conditioned on the other image and both poses.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Noise schedule involving signal-to-noise ratio λ.&lt;/li&gt;
&lt;li&gt;Loss function of DDPM&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;stochastic-condition&#34;&gt;Stochastic condition&lt;/h3&gt;
&lt;figure&gt;&lt;img src=&#34;https://ar5iv.labs.arxiv.org/html/2210.04628/assets/figures/stochastic_cond_shoe.png&#34;
         alt=&#34;Figure 3: Stochastic conditioning sampler&#34;/&gt;&lt;figcaption&gt;
            &lt;p&gt;Figure 3: Stochastic conditioning sampler&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Markovian model&lt;/strong&gt; didn&amp;rsquo;t perform well, where the next image is conditioned on (k) previously generated views.
Thus, a scene can be represented as $p(𝐗) = ∏ᵢp(𝐱ᵢ|𝐱_{&amp;lt;i})$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Using all previous sampled views is imfesible due to the &lt;strong&gt;limited memory&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;They found &lt;strong&gt;k=2&lt;/strong&gt; can achieve 3D consistency, but more previous states impair the sampling quality.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And instead of conditioning on the last few samplings as in the Markovian model,
2 views are stochastically selected as condition images at &lt;strong&gt;each denoising step&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generating a new view $𝐱ₖ₊₁$ needs 256 denoising steps,
where each time the condition image 𝐱ᵢ is &lt;strong&gt;randomly&lt;/strong&gt; chosen from the current views set 𝜲 = {𝐱₁,&amp;hellip;,𝐱ₖ}.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In a denoising step, noise in the intermediate image $\hat 𝐱ₖ₊₁$ will be subtracted from $𝐳ₖ₊₁^{(λₜ)}$,
which follows a &lt;strong&gt;forward&lt;/strong&gt; noising distribution 𝒒,
given the noisy image $𝐳ₖ₊₁^{(λₜ₋₁)}$ of last step and the denoised image $\hat 𝐱ₖ₊₁$.&lt;/p&gt;
&lt;p&gt;$$
\hat 𝐱ₖ₊₁ = \frac{1}{\sqrt{σ(λₜ)}}
\left( 𝐳ₖ₊₁^{(λₜ)} - \sqrt{σ(-λₜ)}\ ε_θ(𝐳ₖ₊₁^{(λₜ)}, 𝐱ᵢ) \right)  \\
\ \\
𝐳ₖ₊₁^{(λₜ₋₁)} \sim q(𝐳ₖ₊₁^{λₜ₋₁};\ 𝐳ₖ₊₁^{(λₜ)}, \hat 𝐱ₖ₊₁ )
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;( I guess 𝒒 gets &lt;strong&gt;&amp;ldquo;reversed&amp;rdquo;&lt;/strong&gt; after applying Bayes rule)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first noisy image $𝐳ₖ₊₁^{(λ_T)}$ is Gaussian N(0,𝐈).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After 256 steps finished, &lt;strong&gt;add&lt;/strong&gt; the result image 𝐱ₖ₊₁ to set 𝜲.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;256 can be larger to cover all the existing views.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This scheme approximate the true autoregressive sampling.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Autoregressive model always use &lt;strong&gt;all&lt;/strong&gt; previous states to predict the next state,
unlike Markov chain only considering &lt;strong&gt;limited recent&lt;/strong&gt; outputs,&lt;/p&gt;
&lt;p&gt;Therefore, to train an autoregressive model, a &lt;strong&gt;sequence&lt;/strong&gt;, i.e. multi-view training data here, is needed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;True autoregressive sampling needs a score model of the form
$log\ q(𝐳ₖ₊₁^{(λ)} | 𝐱₁,&amp;hellip;,𝐱ₖ)$ and multi-view training data.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But they&amp;rsquo;re not interesting in multiple source views here.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;x-unet&#34;&gt;X-UNet&lt;/h3&gt;
&lt;figure&gt;&lt;img src=&#34;https://ar5iv.labs.arxiv.org/html/2210.04628/assets/figures/unet.png&#34;
         alt=&#34;Figure 4: X-UNet Architecture&#34;/&gt;&lt;figcaption&gt;
            &lt;p&gt;Figure 4: X-UNet Architecture&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;UNet with only self-attention fails to generate images with multi-view consistency,
given limited training images.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Different frame has a different noise-level&lt;/li&gt;
&lt;li&gt;Positional encoding of pose is the same size of feature maps&lt;/li&gt;
&lt;li&gt;Use cross-attention to make two images attend to each other.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Inputs:&lt;/p&gt;
&lt;p&gt;Concat two images?
such that the weights of Conv2d and self-attention layers are shared for the noisy image and condition image&lt;/p&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments&lt;/h3&gt;
&lt;p&gt;Dataset: SRN ShapeNet (synthetic cars and chairs) &lt;a class=&#34;link&#34; href=&#34;&#34; &gt;github&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;file&lt;/th&gt;
&lt;th&gt;size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cars_train.zip&lt;/td&gt;
&lt;td&gt;3.26GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;chairs_train.zip&lt;/td&gt;
&lt;td&gt;60.3GB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Use instant-NGP without view dependent modules&lt;/p&gt;
&lt;div id=&#34;flow_0&#34;&gt;
&lt;/div&gt;

</description>
        </item>
        <item>
        <title>read: MVDiffusion generates multi-view images</title>
        <link>https://zichen34.github.io/writenotes/model/nvs/b-note-mvdiffusion-scene/</link>
        <pubDate>Thu, 10 Aug 2023 20:40:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nvs/b-note-mvdiffusion-scene/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/Tangshitao/MVDiffusion&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2307.01097&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv (2307)&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://mvdiffusion.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ProjPage&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abs--intro&#34;&gt;Abs &amp;amp; Intro&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fine-tune&lt;/strong&gt; the pre-trained &lt;strong&gt;text&lt;/strong&gt;-to-image diffusion model (SD)&lt;/li&gt;
&lt;li&gt;Insert &lt;strong&gt;cross-attention&lt;/strong&gt; blocks between UNet blocks;&lt;/li&gt;
&lt;li&gt;Generate multiple views &lt;strong&gt;in parallel&lt;/strong&gt; using a SD,
and &lt;strong&gt;fuse&lt;/strong&gt; multi views by attention;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Freeze&lt;/strong&gt; pre-trained weights while training the attention blocks&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Solving problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generating panorama&lt;/li&gt;
&lt;li&gt;Extrapolate one perspective image to a full 360-degree view&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;preliminary&#34;&gt;Preliminary&lt;/h2&gt;
&lt;p&gt;MVDiffusion derives from &lt;strong&gt;LDM&lt;/strong&gt; (Latent Diffusion Model&lt;a class=&#34;link&#34; href=&#34;#1&#34; &gt;¹&lt;/a&gt;), which contains 3 modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;VAE&lt;/strong&gt; for transfering the generation process to a latent space,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;denoising model&lt;/strong&gt; (UNet) for sampling from the distribution of the inputs&amp;rsquo; latent codes,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;condition encoder&lt;/strong&gt; for providing descriptors.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Loss function is similar to original diffusion model: the MSE between original noise and predicted noise, which conditioned on noisy latents, timestep, and featues.&lt;/p&gt;
&lt;p&gt;$$L_{LDM} = ∑$$&lt;/p&gt;
&lt;p&gt;Convolution layers are insert in each UNet block:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature maps at each level will be added into UNet blocks.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://pbs.twimg.com/media/F0Q2fmwWwAEjOYo?format=jpg&amp;amp;name=large&#34;
         alt=&#34;Figure 2&#34;/&gt;&lt;figcaption&gt;
            &lt;p&gt;Figure 2&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;p&gt;pixel-to-pixel correspondences enable the &lt;strong&gt;multi-view consistent&lt;/strong&gt; generation of panorama and depth2img,
because they have homography matrix and projection matrix to determine the matched pixel pairs in two images.&lt;/p&gt;
&lt;h3 id=&#34;panorama&#34;&gt;Panorama&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Text-conditioned model&lt;/strong&gt;: generate 8 &lt;strong&gt;target&lt;/strong&gt; images from noise conditioned by &lt;strong&gt;per-view&lt;/strong&gt; text prompts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The final linear layer in CAA blocks are initialized to &lt;strong&gt;zero&lt;/strong&gt; to avoid disrupt the SD&amp;rsquo;s original capacity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple latents will be predict by UNet from noise images and then restored to images by the pre-trained VAE&amp;rsquo;s decoder.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Image&amp;amp;Text-conditioned model&lt;/strong&gt;: generate 7 target images based on 1 &lt;strong&gt;condition&lt;/strong&gt; image and respective text prompts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Based on SD&amp;rsquo;s &lt;strong&gt;impainting&lt;/strong&gt; modle as it &lt;strong&gt;takes 1 condition image&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Don&amp;rsquo;t impaint the condition image by concatenating the noise input with a all-one mask (4 channels in total)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compeletly regenerate the input image by concatenating the noise input with a all-zero mask (4 channels in total)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Concatenating all-one and all-zero channel during training make the model learn to apply different processes to condition image and target image.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;depth2img&#34;&gt;depth2img&lt;/h3&gt;



&lt;div class=&#34;goat svg-container &#34;&gt;
  
    &lt;svg
      xmlns=&#34;http://www.w3.org/2000/svg&#34;
      font-family=&#34;Menlo,Lucida Console,monospace&#34;
      
        viewBox=&#34;0 0 376 281&#34;
      &gt;
      &lt;g transform=&#39;translate(8,16)&#39;&gt;
&lt;path d=&#39;M 96,16 L 112,16&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 96,64 L 112,64&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 264,64 L 280,64&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 272,128 L 304,128&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 48,192 L 88,192&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 32,80 L 32,176&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 168,32 L 168,48&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 168,160 L 168,176&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 168,224 L 168,240&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 320,96 L 320,112&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;polygon points=&#39;96.000000,192.000000 84.000000,186.399994 84.000000,197.600006&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(0.000000, 88.000000, 192.000000)&#39;&gt;&lt;/polygon&gt;
&lt;polygon points=&#39;120.000000,16.000000 108.000000,10.400000 108.000000,21.600000&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(0.000000, 112.000000, 16.000000)&#39;&gt;&lt;/polygon&gt;
&lt;polygon points=&#39;120.000000,64.000000 108.000000,58.400002 108.000000,69.599998&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(0.000000, 112.000000, 64.000000)&#39;&gt;&lt;/polygon&gt;
&lt;polygon points=&#39;176.000000,48.000000 164.000000,42.400002 164.000000,53.599998&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(90.000000, 168.000000, 48.000000)&#39;&gt;&lt;/polygon&gt;
&lt;polygon points=&#39;176.000000,176.000000 164.000000,170.399994 164.000000,181.600006&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(90.000000, 168.000000, 176.000000)&#39;&gt;&lt;/polygon&gt;
&lt;polygon points=&#39;176.000000,240.000000 164.000000,234.399994 164.000000,245.600006&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(90.000000, 168.000000, 240.000000)&#39;&gt;&lt;/polygon&gt;
&lt;polygon points=&#39;280.000000,128.000000 268.000000,122.400002 268.000000,133.600006&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(180.000000, 272.000000, 128.000000)&#39;&gt;&lt;/polygon&gt;
&lt;polygon points=&#39;288.000000,64.000000 276.000000,58.400002 276.000000,69.599998&#39; fill=&#39;currentColor&#39; transform=&#39;rotate(0.000000, 280.000000, 64.000000)&#39;&gt;&lt;/polygon&gt;
&lt;path d=&#39;M 320,112 A 16,16 0 0,1 304,128&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;path d=&#39;M 32,176 A 16,16 0 0,0 48,192&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39;&gt;&lt;/path&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;0&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;S&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;0&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;0&#39; y=&#39;36&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;&amp;amp;&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;0&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;T&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;8&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;8&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;8&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;16&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;q&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;16&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;p&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;16&#39; y=&#39;36&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;p&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;16&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;x&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;24&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;u&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;24&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;24&#39; y=&#39;36&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;24&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;32&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;32&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;h&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;32&#39; y=&#39;36&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;40&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;n&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;40&#39; y=&#39;36&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;40&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;p&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;48&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;c&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;48&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;48&#39; y=&#39;36&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;48&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;r&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;56&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;56&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;a&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;56&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;64&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;p&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;64&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;72&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;72&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;72&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;p&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;80&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;f&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;80&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;104&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;I&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;112&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;A&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;112&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;120&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;n&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;120&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;a&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;128&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;T&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;128&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;y&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;128&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;g&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;128&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;M&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;136&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;S&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;136&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;k&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;136&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;136&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;136&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;144&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;u&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;144&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;144&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;x&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;144&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;144&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;&amp;amp;&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;144&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;b&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;y&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;w&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;148&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;T&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;212&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;152&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;-&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;148&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;212&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;160&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;l&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;f&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;c&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;148&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;a&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;x&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;212&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;168&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;r&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;c&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;148&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;g&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;176&#39; y=&#39;212&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;a&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;n&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;l&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;148&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;-&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;212&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;l&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;184&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;192&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;192&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;192&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;192&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;n&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;192&#39; y=&#39;148&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;192&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;c&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;192&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;200&#39; y=&#39;4&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;f&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;200&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;200&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;200&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;200&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;200&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;a&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;208&#39; y=&#39;20&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;208&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;208&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;208&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;n&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;208&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;g&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;216&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;216&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;c&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;216&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;216&#39; y=&#39;260&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;224&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;224&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;u&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;224&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;232&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;n&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;232&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;232&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;t&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;240&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;240&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;240&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;248&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;248&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;v&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;248&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;o&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;256&#39; y=&#39;132&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;256&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;n&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;264&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;272&#39; y=&#39;196&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;d&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;296&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;k&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;304&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;304&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;i&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;312&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;y&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;312&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;320&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;-&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;320&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;a&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;328&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;f&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;328&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;g&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;336&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;r&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;336&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;344&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;a&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;344&#39; y=&#39;84&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;s&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;352&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;m&lt;/text&gt;
&lt;text text-anchor=&#39;middle&#39; x=&#39;360&#39; y=&#39;68&#39; fill=&#39;currentColor&#39; style=&#39;font-size:1em&#39;&gt;e&lt;/text&gt;
&lt;/g&gt;

    &lt;/svg&gt;
  
&lt;/div&gt;
&lt;p&gt;This two-stage design is because SD&amp;rsquo;s impainting modle &lt;strong&gt;doesn&amp;rsquo;t&lt;/strong&gt; support depth map condition.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So the Text-conditioned model is reused to generate condition images.&lt;/li&gt;
&lt;li&gt;Then the Image&amp;amp;Text-conditioned model interpolate the two condition images.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correspondence-aware-attention&#34;&gt;Correspondence-aware attention&lt;/h3&gt;
&lt;p&gt;Aggregate the features of KxK neighbor pixels on every target feature maps to each pixel their own feature vector.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The source pixel $s$ perform positional encoding γ(0)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The neighbor pixel $t_*^l$ around the corresponding pixel $t^l$ on the target image $l$ perform position encoding $γ(s_*^l-s)$,
which means the neighbor pixel $t_*^l$ need to be warpped back to source feature map to find the distance from $s_*^l$ to the source pixel $s$.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://pbs.twimg.com/media/F0Q2-IMWcAA5TC0?format=png&amp;amp;name=small&#34;
           alt=&#34;Figure 3&#34;/&gt;&lt;figcaption&gt;
              &lt;p&gt;Figure 3&lt;/p&gt;
          &lt;/figcaption&gt;
  &lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ref&#34;&gt;Ref&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;div id=&#34;1&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;High-Resolution Image Synthesis with Latent Diffusion Models - Robin Rombach&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>read: DiffuStereo reconstruct 3D human</title>
        <link>https://zichen34.github.io/writenotes/model/nvs/b-note-diffustereo-human/</link>
        <pubDate>Thu, 10 Aug 2023 18:40:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nvs/b-note-diffustereo-human/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2207.08000&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Embed diffusion model into stereo matching network&lt;/li&gt;
&lt;li&gt;Adopt multi-level network for high-resolution input&lt;/li&gt;
&lt;li&gt;Fuse generated depth map to reconstruct 3D human model.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Sparse-view methods, which predict geometry based on appearance,
cannot produce detailed human model because of lacking sufficient multiview stereo &lt;strong&gt;matching&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Continuous models are basically obtained from traditional stereo methods based on a continuous varitional formulation, which can solved by diffusion model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pipeline:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reconstruct &lt;strong&gt;coarse field&lt;/strong&gt; first by using DoubleField;&lt;/li&gt;
&lt;li&gt;Render &lt;strong&gt;depth maps&lt;/strong&gt; from multiple viewpoints&lt;/li&gt;
&lt;li&gt;Compute &lt;strong&gt;disparity&lt;/strong&gt; flow masks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refine&lt;/strong&gt; disparity flow with diffusion model
&lt;ul&gt;
&lt;li&gt;Level 1: Use CNN to extract &lt;strong&gt;feature maps&lt;/strong&gt; of disparity flow masks&lt;/li&gt;
&lt;li&gt;Level 2: &lt;strong&gt;Condition&lt;/strong&gt; diffusion model with feature maps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fuse 3D points through interpolation.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;&lt;img src=&#34;http://www.liuyebin.com/diffustereo/assets/pipeline_v3.jpg&#34;/&gt;
   &lt;/figure&gt;

&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
