<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Shapes on Zichen Wang</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/</link>
        <description>Recent content in Shapes on Zichen Wang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 06 Mar 2024 17:25:00 +0000</lastBuildDate><atom:link href="https://zichen34.github.io/writenotes/model/shapes/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>sympo: Point Cloud Processing</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/c-symp-pc_pro/</link>
        <pubDate>Wed, 06 Mar 2024 17:25:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/c-symp-pc_pro/</guid>
        <description>&lt;h2 id=&#34;neural-depth&#34;&gt;Neural Depth&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GPU-Accelerated 3D Point Cloud Processing with Hierarchical GMM&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/somanshu25/GPU-Accelerated-Point-Cloud-Registration-Using-Hierarchical-GMM&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Video: &lt;a class=&#34;link&#34; href=&#34;https://developer.nvidia.com/gtc/2019/video/s9623&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GTC Silicon Valley-2019: GPU-Accelerated 3D Point Cloud Processing with Hierarchical Gaussian Mixtures&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Found by DDG (image) with searching the paper: &amp;ldquo;GPU-Accelerated LOD Generation for Point Clouds&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visualization&#34;&gt;Visualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GPU-Accelerated LOD Generation for Point Clouds&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/m-schuetz/CudaLOD&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://snosixtyboo.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Bernhard Kerbl (3DGS)&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://papers.cool/arxiv/2302.14801&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CoolPapers&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CudaLOD&lt;/li&gt;
&lt;li&gt;Multi-level details for point cloud&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;refinement&#34;&gt;Refinement&lt;/h2&gt;
&lt;p&gt;(2024-03-15)&lt;/p&gt;
&lt;p&gt;Journey when searching: &amp;ldquo;point cloud refinement research&amp;rdquo; &lt;a class=&#34;link&#34; href=&#34;https://duckduckgo.com/?q=point&amp;#43;cloud&amp;#43;refinement&amp;#43;research&amp;amp;ia=web&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DDG&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Real-Time Point Cloud Refinement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.researchgate.net/publication/237009007_Real-Time_Point_Cloud_Refinement&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RSGate&lt;/a&gt;
(2024-03-16)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upsampiling point cloud by inserting new point set&lt;/li&gt;
&lt;li&gt;Splatting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A voxel-based multiview point cloud refinement method via factor graph optimization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/WuHao-WHU/EVPA?tab=readme-ov-file&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;No Code&lt;/a&gt;
| Paper not accessible&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s compared with ICP, ICP-Normal, GICP, LUM, BALM&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;Search the title of above paper 1 &lt;a class=&#34;link&#34; href=&#34;https://duckduckgo.com/?q=Real-Time&amp;#43;Point&amp;#43;Cloud&amp;#43;Refinement&amp;amp;ia=web&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DDG&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PR-GCN: A Deep Graph Convolutional Network With Point Refinement for 6D Pose Estimation&lt;/strong&gt; (ICCV 2021)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.semanticscholar.org/paper/PR-GCN%3A-A-Deep-Graph-Convolutional-Network-with-for-Zhou-Wang/a442e264eaa8be82b74b902f10fb83c73cd1a482&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Semantic&lt;/a&gt;
&lt;figure&gt;&lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2108.09916&#34;&gt;&lt;img src=&#34;https://ar5iv.labs.arxiv.org/html/2108.09916/assets/x2.png&#34;/&gt;&lt;/a&gt;
   &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;(2024-03-16)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KNN and Graph Convolution Network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;edge-aware&#34;&gt;Edge-aware&lt;/h3&gt;
&lt;p&gt;The paper-1 is cited by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Ec-net: an edge-aware point set consolidation network&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/yulequan/EC-Net&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>sympo: Complete Point Clouds</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/c-symp-cpltpc/</link>
        <pubDate>Sat, 24 Feb 2024 20:25:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/c-symp-cpltpc/</guid>
        <description>&lt;h2 id=&#34;explore&#34;&gt;Explore&lt;/h2&gt;
&lt;p&gt;(2024-02-24)&lt;/p&gt;
&lt;p&gt;DDG search: &amp;ldquo;point cloud fusion&amp;rdquo;, found the wikipedia page of &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Point_cloud&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&amp;ldquo;Point cloud&amp;rdquo;&lt;/a&gt;,
where the caption of the illustration 3 mentioned:
&amp;ldquo;reconstructing 3D shapes from multi-view &lt;strong&gt;depth map&lt;/strong&gt;&amp;rdquo;.
That picture is from:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes with Deep Generative Networks&lt;/strong&gt;
~ CVPR 2017&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/Amir-Arsalan/Synthesize3DviaDepthOrSil&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| Amir A. Soltani, &lt;a class=&#34;link&#34; href=&#34;https://jiajunwu.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jiajun Wu&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;Then I searched the paper&amp;rsquo;s title in &lt;a class=&#34;link&#34; href=&#34;https://duckduckgo.com/?q=Synthesizing&amp;#43;3D&amp;#43;Shapes&amp;#43;via&amp;#43;Modeling&amp;#43;Multi-View&amp;#43;Depth&amp;#43;Maps&amp;#43;and&amp;#43;Silhouettes&amp;#43;with&amp;#43;Deep&amp;#43;Generative&amp;#43;Networks&amp;amp;ia=web&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DDG&lt;/a&gt;
, and I found:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Render4Completion: Synthesizing Multi-View Depth Maps for 3D Shape Completion&lt;/strong&gt;
~ ICCV 2019 workshop&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.08366&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://papers.cool/arxiv/1904.08366&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CoolPapers&lt;/a&gt;
| Tao Hu, &lt;a class=&#34;link&#34; href=&#34;https://h312h.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Zhizhong Han&lt;/a&gt;, Matthias Zwicker&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data: Render multi-view depth maps for incomplete object&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train: 8-view incomplete depth map -&amp;gt; NN -&amp;gt; 8-view completed depth map&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3D shape: Unproject 8 depth maps&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Implicit Fields for Generative Shape Modeling&lt;/strong&gt;
~ CVPR 2019&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1812.02822&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://czq142857.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Zhiqin Chen&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;https://www.cs.sfu.ca/~haoz/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hao Zhang&lt;/a&gt;, SFU&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Improved Modeling of 3D Shapes with Multi-view Depth Maps&lt;/strong&gt;
~ 3DV 2020&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kampta/multiview-shapes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://papers.cool/arxiv/2009.03298&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CoolPapers&lt;/a&gt;
| Kamal Gupta, Matthias Zwicker&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;single view&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>sympo: Point Cloud from Depth Maps</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/c-symp-dmapest/</link>
        <pubDate>Wed, 21 Feb 2024 12:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/c-symp-dmapest/</guid>
        <description>&lt;hr&gt;
&lt;h2 id=&#34;depth-completion&#34;&gt;Depth Completion&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://paperswithcode.com/task/depth-completion&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Depth Completion - Paper with code&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Guided Convolutional Network for Depth Completion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1908.01238&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://github.com/kakaxi314/GuideNet&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://papers.cool/arxiv/1908.01238&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CoolPapers&lt;/a&gt;
| Jie Tang, Ping Tan&lt;/p&gt;
&lt;p&gt;(2024-02-21)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic convolution kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;depth-super-resolution&#34;&gt;Depth Super-Resolution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/zhyever/PatchFusion&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://zhyever.github.io/patchfusion/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ProjPage&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://zhyever.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Zhenyu Li&lt;/a&gt;, Peter Wonka&lt;/p&gt;
&lt;p&gt;(2024-02-23)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Joint Implicit Image Function for Guided Depth Super-Resolution&lt;/strong&gt;
~ ACM MM 2021&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ashawkey/jiif&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2107.08717&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://me.kiui.moe/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jiaxiang Tang&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(2024-02-22)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>watch: Jun Gao | ML for 3D content generation</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/%E9%AB%98%E8%B4%A8%E9%87%8F3d%E5%86%85%E5%AE%B9%E5%88%9B%E9%80%A0-%E9%AB%98%E4%BF%8A/</link>
        <pubDate>Thu, 13 Jul 2023 18:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/%E9%AB%98%E8%B4%A8%E9%87%8F3d%E5%86%85%E5%AE%B9%E5%88%9B%E9%80%A0-%E9%AB%98%E4%BF%8A/</guid>
        <description>&lt;p&gt;Source video: &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1mo4y1A7it&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;英伟达高俊: AI高质量三维内容生成（内容生成系列【一】）&lt;/a&gt;
北京智源大会2023 视觉与多模态大模型&lt;/p&gt;
&lt;h2 id=&#34;the-representation-of-3d-objects&#34;&gt;The representation of 3D objects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implicit field is in favor of neural network, where it can be optimized by gradient.&lt;/li&gt;
&lt;li&gt;mesh can achieve real-time rendering and is handy for downstream creation, and good topology.&lt;/li&gt;
&lt;li&gt;Marching cube is not fully differentiable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DMTet: A differentiable iso-surfacing is an implict field, and also a mesh.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An field where only the location at surface has value?&lt;/li&gt;
&lt;li&gt;a field only has one mesh?&lt;/li&gt;
&lt;li&gt;Diff-render&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2d-images-supervise-3d-generation&#34;&gt;2D images supervise 3D generation&lt;/h2&gt;
&lt;p&gt;2D GAN advantages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;various discriminator architecture&lt;/li&gt;
&lt;li&gt;powerful generator&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;GAN3D&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latent codes of geometry and texture are sampled from 3D gaussian as prior&lt;/li&gt;
&lt;li&gt;3D generator: Tri-plane consistute the implicit field.&lt;/li&gt;
&lt;li&gt;Get a mesh by DMTet from the generated geometry and texture, then render it to 2D image&lt;/li&gt;
&lt;li&gt;Use GAN to discriminate if the render is real and backward the gradient of loss&lt;/li&gt;
&lt;li&gt;Limitation: class label conditioned. One model can only can generate 1 category of objects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;text-prompts-generate-3d-objects&#34;&gt;Text prompts generate 3D objects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2D diffusion used &lt;strong&gt;socre function&lt;/strong&gt; to encourage high-fadality images&lt;/li&gt;
&lt;li&gt;score function needs a full image, but NeRF are trained batch-by-batch of rays, not a full image.&lt;/li&gt;
&lt;li&gt;Dream fusion can only render 64x64 images, so its geometry is low-quality.&lt;/li&gt;
&lt;li&gt;Coarse to fine: Use instant-ngp generate a rough geometry based on low-resolution diffusion model， then use DMTet convert the geometry to mesh;
So that a highe-resolution image can be rendered, which can offer a &lt;strong&gt;strong&lt;/strong&gt; gradient for fine geometry&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future work&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;a universal model can generate any category of objects.&lt;/li&gt;
&lt;li&gt;composite objects to form a scene&lt;/li&gt;
&lt;li&gt;dynamic objects&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>watch: 215-智能三维内容生成</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/215-%E8%A7%86%E8%A7%89%E4%B8%93%E9%A2%98%E6%99%BA%E8%83%BD%E4%B8%89%E7%BB%B4%E5%86%85%E5%AE%B9%E7%94%9F%E6%88%90--%E9%87%8D%E5%BB%BA%E4%B8%8E%E5%88%9B%E9%80%A0-kangxue-yin/</link>
        <pubDate>Fri, 07 Jan 2022 22:22:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/215-%E8%A7%86%E8%A7%89%E4%B8%93%E9%A2%98%E6%99%BA%E8%83%BD%E4%B8%89%E7%BB%B4%E5%86%85%E5%AE%B9%E7%94%9F%E6%88%90--%E9%87%8D%E5%BB%BA%E4%B8%8E%E5%88%9B%E9%80%A0-kangxue-yin/</guid>
        <description>&lt;p&gt;title: &amp;ldquo;3D Content Creation and Stylization with AI&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Source link: &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1mZ4y1U7ns/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GAMES Webinar 215-视觉专题：智能三维内容生成&amp;ndash;重建与创造-Kangxue Yin&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;3d内容创作&#34;&gt;3D内容创作：&lt;/h2&gt;
&lt;p&gt;游戏，电影特效，动画，VR/AR，专业化程度高（建模软件）&lt;/p&gt;
&lt;p&gt;动画电影创作流程:&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/shapes/img/215-3D%E5%8A%A8%E7%94%BB%E7%94%B5%E5%BD%B1%E5%88%9B%E4%BD%9C%E6%B5%81%E7%A8%8B_4_55.png width=&gt;
  
  


&lt;ol&gt;
&lt;li&gt;剧本&lt;/li&gt;
&lt;li&gt;原画&lt;/li&gt;
&lt;li&gt;Layout: 物体相机摆放，位置关系&lt;/li&gt;
&lt;li&gt;特效技术研发&lt;/li&gt;
&lt;li&gt;2D 建模为3D&lt;/li&gt;
&lt;li&gt;纹理&lt;/li&gt;
&lt;li&gt;绑定骨骼，皮肤，相对运动关系&lt;/li&gt;
&lt;li&gt;人物运动（运动捕捉）&lt;/li&gt;
&lt;li&gt;VFX 特效模拟（爆炸）&lt;/li&gt;
&lt;li&gt;打光&lt;/li&gt;
&lt;li&gt;渲染(物理仿真)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;梦工厂pipeline：https://www.youtube.com/watch?v=ru0tQRJ4qKs&lt;/p&gt;
&lt;p&gt;3D 内容成本高：2009 阿凡达（3D） $ 2.37亿 ，而 2010年的2D电影 让子弹飞 只有 $ 0.18亿&lt;/p&gt;
&lt;p&gt;元宇宙需要大量3D 内容&lt;/p&gt;
&lt;p&gt;降低3D内容制作成本：让普通玩家参与创作（房子，汽车&amp;hellip;）NFT，未经训练，细节不足，风格不够多样，用人工智能辅助。&lt;/p&gt;
&lt;h2 id=&#34;research-works&#34;&gt;Research works&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape&lt;/p&gt;
&lt;p&gt;输入 voxel 模型，网络合成细节。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;3D represention 选择&lt;/p&gt;
&lt;p&gt;Discrete Repre&lt;/p&gt;

    
    
    
    
      
      
      
      
       
      
      
       
      
      &lt;img src= /writenotes/model/shapes/img/215-%E7%A6%BB%E6%95%A3%E8%A1%A8%E7%A4%BA%E6%9C%89%E7%BC%BA%E7%82%B9_11-31.png width=&gt;
      
      
    

&lt;p&gt;Implicit Fields:&lt;/p&gt;

    
    
    
    
      
      
      
      
       
      
      
       
      
      &lt;img src= /writenotes/model/shapes/img/215-%E9%9A%90%E5%90%AB%E5%9C%BA%E7%9A%84%E7%BC%BA%E7%82%B9_13-18.png width=&gt;
      
      
    

&lt;p&gt;生成网络细节不足，转换为mesh有问题&lt;/p&gt;

    
    
    
    
      
      
      
      
       
      
      
       
      
      &lt;img src= /writenotes/model/shapes/img/215-%E6%8A%8A%E9%9A%90%E5%BC%8F%E6%96%B9%E7%A8%8B%E8%BD%AC%E5%8C%96%E4%B8%BA%E6%98%BE%E5%BC%8F%E8%A1%A8%E9%9D%A2_14-06.png width=&gt;
      
      
    

&lt;p&gt;differentiable iso-surfacing 把隐式方程转化为一个mesh，然后用 mesh 和 ground truth 之间的差（loss）来优化网络，降低 iso-surfacing 离散化带来的误差，其中使用的不是 Marching cube 而是 Marching Tetrahedra&lt;/p&gt;

    
    
    
    
      
      
      
      
       
      
      
       
      
      &lt;img src= /writenotes/model/shapes/img/215-Marching_Tetra_15-33.png width=&gt;
      
      
    

&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3DStyleNet: Creating&lt;/p&gt;
&lt;p&gt;3D 物体的风格迁移&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://zichen34.github.io/writenotes/model/shapes/img/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/img/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
