<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>王木头学科学 on Zichen Wang</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/</link>
        <description>Recent content in 王木头学科学 on Zichen Wang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 08 Jan 2023 16:09:00 +0000</lastBuildDate><atom:link href="https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>watch: DL - 王木头 22 | Overview of Probability theory, Statistics, Information theory</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/22_%E6%A6%82%E7%8E%87%E8%AE%BA1-%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%81%94/</link>
        <pubDate>Sun, 08 Jan 2023 16:09:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/22_%E6%A6%82%E7%8E%87%E8%AE%BA1-%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%81%94/</guid>
        <description>&lt;p&gt;视频封面：一个统一视角下的概率论+统计学+信息论&lt;/p&gt;
&lt;p&gt;原视频：&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1vv4y1B714/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1. 从头开始，把概率、统计、信息论中零散的知识统一起来-王木头学科学&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;概率论最基础的问题：用数学的方式描述&amp;quot;不确定性&amp;quot;（或&amp;quot;可能性&amp;quot;）&lt;/p&gt;
&lt;p&gt;把所有事件及其发生的可能性写到一个表 f 里，每次通过查表就能知道可能性是多少: f(S) = K。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;idx&lt;/th&gt;
&lt;th&gt;S&lt;/th&gt;
&lt;th&gt;K&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;事件a&lt;/td&gt;
&lt;td&gt;数值1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;事件b&lt;/td&gt;
&lt;td&gt;数值2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;事件c&lt;/td&gt;
&lt;td&gt;数值3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;..&lt;/td&gt;
&lt;td&gt;&amp;hellip;..&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这里的“可能性” 满足一些限制：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数值要满足事件可能性的相对关系。比如，如果事件 a 的可能性 &amp;gt; 事件 b 的可能性，则 数值1 &amp;gt; 数值2&lt;/li&gt;
&lt;li&gt;数值需要满足事件的包含关系。比如，事件 c = {事件a，事件b}（a,b中任意一个发生），则 数值3 = 数值1 + 数值2&lt;/li&gt;
&lt;li&gt;注意，这里并未要求所有数值归一化，所以还不是概率值&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种方式只是把“可能性”做了数学符号化，并不是“数学化”。除了要保证定义出来的这个体系自洽之外，还要尽可能简约。
比如上面的事件 c 并不需要单独定义，它的数值可以从事件 a 和 b 推导出来。
所以表格中并不需要列举所有的事件，只需要包含不可再分的原子事件及其可能性。&lt;/p&gt;
&lt;p&gt;V 2.0&lt;/p&gt;
&lt;p&gt;表1：&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;idx&lt;/th&gt;
&lt;th&gt;S&lt;/th&gt;
&lt;th&gt;K&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;原子事件a&lt;/td&gt;
&lt;td&gt;数值1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;原子事件b&lt;/td&gt;
&lt;td&gt;数值2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;原子事件c&lt;/td&gt;
&lt;td&gt;数值3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;原子事件d&lt;/td&gt;
&lt;td&gt;数值4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;..&lt;/td&gt;
&lt;td&gt;&amp;hellip;..&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;它可以定义出：f(S) = K。只留下原子事件后，就可以确定“可能性”的最大值为所有原子事件“合”在一起组成的事件发生的可能性（只要有1个原子事件发生，这件事就算发生）：
max(∑K) = ∑_{s∈all} f(s)&lt;/p&gt;
&lt;p&gt;有了最大可能性之后，可以定义归一化的数值 K = K/(∑_{s∈all} f(s)) ∈ [0,1]&lt;/p&gt;
&lt;p&gt;从上面的原子事件可推导出以下表2:&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;P(S)&lt;/th&gt;
&lt;th&gt;∑K&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;{1,2}&lt;/td&gt;
&lt;td&gt;数值1+数值2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{1,2,3}&lt;/td&gt;
&lt;td&gt;数值1+数值2+数值3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{3,4}&lt;/td&gt;
&lt;td&gt;数值3+数值4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{2,4}&lt;/td&gt;
&lt;td&gt;数值2+数值4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&amp;hellip;.&lt;/td&gt;
&lt;td&gt;&amp;hellip;..&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;P(S) 表示集合 S 的幂集。&lt;/p&gt;
&lt;p&gt;但是原子事件对于不同的问题，不好确定。对于离散问题（掷骰子），原子事件就是点数。
但对于连续的变量（温度），原子事件可以取一个小区间。因为可以无限细分，当区间趋近于无穷小时，它发生的可能性就趋于0。
如果所有原子事件的可能性都是0，就无法从表 1 推导出表 2。&lt;/p&gt;
&lt;p&gt;可以从表 2 建立数学体系。&lt;/p&gt;
&lt;p&gt;V 3.0&lt;/p&gt;
&lt;p&gt;对于连续的情况，原子事件就是一个个点，如表 3，它们对应的数值并不代表发生的可能性，而是有其他意义，因为我们要用表 4 建立定义，所以表 3 的数值是由表 4 推导出来的。&lt;/p&gt;
&lt;p&gt;表 3：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ℝ&lt;/th&gt;
&lt;th&gt;S&lt;/th&gt;
&lt;th&gt;K&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;点a&lt;/td&gt;
&lt;td&gt;数值1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;点b&lt;/td&gt;
&lt;td&gt;数值2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;点c&lt;/td&gt;
&lt;td&gt;数值3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;点d&lt;/td&gt;
&lt;td&gt;数值4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;.&lt;/td&gt;
&lt;td&gt;&amp;hellip;..&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;表 4：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;P(ℝ)&lt;/th&gt;
&lt;th&gt;K&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;[1,2]&lt;/td&gt;
&lt;td&gt;f[数值1,数值2]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;[1,3]&lt;/td&gt;
&lt;td&gt;f[数值1,数值3]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;[3,4]&lt;/td&gt;
&lt;td&gt;f[数值3,数值4]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;[1,2]∪[3,4]&lt;/td&gt;
&lt;td&gt;f[数值1,数值2]+f[数值3,数值4]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;.&lt;/td&gt;
&lt;td&gt;&amp;hellip;..&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 13 | L1L2 Reg (3), Bayesian Probability</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/13_l1l2%E6%AD%A3%E5%88%99%E5%8C%963-%E8%B4%9D%E5%8F%B6%E6%96%AF/</link>
        <pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/13_l1l2%E6%AD%A3%E5%88%99%E5%8C%963-%E8%B4%9D%E5%8F%B6%E6%96%AF/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1fR4y177jP&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;贝叶斯解释“L1和L2正则化”，本质上是最大后验估计。如何深入理解贝叶斯公式？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(贝叶斯公式中的概率可以是随机事件的概率，也可以是概率分布&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem#:~:text=and%20Y%20are-,continuous,-%2C&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;wiki&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;似然函数 L(θ|X)：X发生之后，关于θ的函数。
似然值：参数 θ 发生后，X 发生的概率，即 L(θ|X) = P(X|θ)。&lt;/p&gt;
&lt;p&gt;频率派认为训练数据是来自真实分布的采样，似然值（X发生概率）最大的 θ 就是最接近真实的，但并不能说这组参数就是最有可能的分布。
贝叶斯派想直接求参数 θ 的概率，认为真实θ分布发生的概率应该最大。&lt;/p&gt;
&lt;p&gt;MLE 在两个地方做了近似：寻找使 X 发生概率最大的 θ 并不等于 θ 发生的概率最大；θ 是一个分布，找到的是它的众数。
MAP 只有一个近似：寻找概率最大的 θ 分布是严谨的，只剩&amp;quot;θ 是一个分布，找到的是它的众数&amp;quot;。&lt;br&gt;
(221216): 概率模型的参数θ出现的概率是概率密度函数，但在概率密度函数上，任一个点（一个θ）的概率是0，因为它对应的面积为0，所以只有谈论&amp;quot;一段区间&amp;quot;出现的概率才有意义，比如θ落在0.2~0.3之间的概率为60%。&lt;/p&gt;
&lt;h2 id=&#34;map-等于-mle-加上正则化&#34;&gt;MAP 等于 MLE 加上正则化&lt;/h2&gt;
&lt;p&gt;θ 在训练集 X 上发生的概率 P(θ|X) 可用贝叶斯公式展开：&lt;/p&gt;
&lt;p&gt;P(θ|X) = (P(X|θ)/P(X)) ⋅ P(θ)&lt;/p&gt;
&lt;p&gt;模型参数 θ 的概率 = X 在参数为 θ 时发生的概率 与 X 在所有可能的 θ 下发生的概率之和 的比值 × θ 的起始概率。
其中 P(X) =∫ P(X|θ)P(θ) dθ 。&lt;/p&gt;
&lt;p&gt;P(θ|X) 和 P(θ) 都是 θ 的概率，不过前者是有条件的（是X发生之后的，是更新后的），所以叫做“后验分布”（X是试验，即“试验之后”）；后者是在X发生之前，（不看X）心中已有的知识，是先验分布。&lt;/p&gt;
&lt;p&gt;P(X|θ) 和 P(X) 都是 X 的概率，不过前者是在参数为 θ 时，X 发生的概率，后者是各种参数得到 X 的概率之和，它们的比值是先验分布 P(θ) 的置信度。&lt;/p&gt;
&lt;p&gt;从先验分布P(θ)出发，算出实验结果 X 在参数为 θ 时发生的概率，然后除以 X 在各种θ时发生的概率之和，做为系数，对先验分布做修正。&lt;/p&gt;
&lt;p&gt;P(X) 是对所有可能的 θ 做积分（也称边缘概率），比如对于抛硬币实验，就要对正-反概率：从 (正:0, 反:1) 到 (正:1, 反:0) 之间的所有情况，求出X发生的概率再积分（是个常数）。无法直接求，可以用蒙特卡洛方法近似。
不过，想求概率最大时的 θ，并不必求出概率最大是多少。因为 P(X) 与 θ 无关，所以 P(θ|X) ∝ P(X|θ) P(θ)，只需求似然度乘以先验分布乘积最大时，对应的 θ 就行。&lt;/p&gt;
&lt;p&gt;MLE 认定：θ = arg max_θ P(X|θ)；&lt;br&gt;
MAP 认定：θ = arg max_θ (P(X|θ)⋅P(θ))&lt;/p&gt;
&lt;p&gt;(MLE和MAP的求解推导 &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1GZ4y1m7gv/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;极大似然估计/最大后验估计—通过抛硬币例子理解&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;代入神经网络中的符号：&lt;/p&gt;
&lt;p&gt;MLE: &lt;code&gt;W = arg max_W P(X,Y|W) ∝ arg max_W log P(X,Y|W)&lt;/code&gt;，这就是最大似然估计损失函数（“是猫”的概率）&lt;/p&gt;
&lt;p&gt;MAP: &lt;code&gt;W = arg max_W P(W|X,Y) ∝ arg max_W (log P(X,Y|W) + log P(W))&lt;/code&gt;，损失函数相比 MLE 多了先验分布 P(W)&lt;/p&gt;
&lt;p&gt;理论上先验分布可以任意选择，只要使用大量数据迭代无限次后，后验分布的最大概率对应的就是真实θ，但如果数据量有限，不同的先验分布会收敛到不同的θ。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;如果采用正态分布 wᵢ~N(0,σ²) 作为先验分布 P(W)，则其对应 L2 正则化项：λ||𝐖||₂+C：&lt;/p&gt;
&lt;p&gt;$$
log P(W) = log ∏_i \frac{1}{σ\sqrt{2π}} e^{-\frac{(w_i-0)²}{2σ²}} = -\frac{1}{2σ²}∑_i w_i² + C
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果采用拉普拉斯分布 wᵢ~Laplace(0,b) 作为先验分布 P(W)，则其对应 L1 正则化项：λ||𝐖||₁+C：&lt;/p&gt;
&lt;p&gt;$$
log P(W) = log ∏_i \frac{1}{2b} e^{-\frac{|w_i-0|}{b}} = -\frac{1}{b}∑_i |w_i| + C
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;正态分布和拉普拉斯分布都是限定了高维空间中的向量 𝐖 的位置，L2正则化约束了向量模长（坐标平方和再开方）服从正态分布，L1正则化约束了两向量距离（坐标之差）服从拉普拉斯分布&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果先验分布 P(W) 是平均分布，概率是常数，与W无关，MAP 就退化成了 MLE&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MAP 比 MLE 多了一个先验分布，先验分布就是正则化项。&lt;/p&gt;
&lt;p&gt;贝叶斯公式描述的是：用新的实验结果对先验分布做修正，先验分布就是优化的起点，不同的先验分布，对应的优化起点不同.&lt;/p&gt;
&lt;p&gt;最大后验估计像是一个损失函数的集合，选择不同的先验分布，相当于选择了不同的损失函数，包括MLE, L1正则化，L2正则化。&lt;/p&gt;
&lt;h2 id=&#34;map-比-mle-更正确&#34;&gt;MAP 比 MLE 更正确&lt;/h2&gt;
&lt;p&gt;有一个人，女性，27岁，985硕士毕业，单身，在上海生活，平时喜欢表达，幽默，有些理想主义，关心少数人群，经常在网上发表犀利言论，问这个人更有可能是一个脱口秀演员呢？ 还是更有可能是一个女性主义的脱口秀演员？&lt;/p&gt;
&lt;p&gt;直觉选择后者，但其实前者概率更大，因为“脱口秀演员”包含的范围更大。&lt;/p&gt;
&lt;p&gt;人的直觉运用了最大似然估计，选择“女性主义的脱口秀演员” 更可能出现描述的那些特质。
如果运用最大后验估计，按照概率乘法即可计算出哪个情况概率较大。&lt;/p&gt;
&lt;p&gt;给定特征集合 D={女性，27岁，985硕士毕业，&amp;hellip;, 发表犀利言论}，
结论集合 T = {t₁=p₁, t₂=p₁^p₂}，其中 p₁ 是脱口秀演员，p₂ 是女性主义脱口秀演员&lt;/p&gt;
&lt;p&gt;最大似然估计：
T = arg max_T P(D|T)&lt;/p&gt;
&lt;p&gt;当 T = t₂ 时：P(D|T=t₂) = P(D|p₁^p₂) = P(p₁^p₂|D) P(D) / P(p₁^p₂) = P(p₁,p₂,D)/P(p₁,p₂)
= P(p₂|p₁,D) P(D|p₁) P(p₁) / (P(p₁|p₂)P(p₁)) = P(p₂|p₁,D) P(D|p₁) / P(p₁|p₂)
= (P(p₂|p₁,D)/P(p₁|p₂)) P(D|T=t₁)&lt;/p&gt;
&lt;p&gt;P(D|T=t₂) 与 P(D|T=t₁) 之间相差一个大于1的系数，所以当 T=t₂ 时似然值更大&lt;/p&gt;
&lt;p&gt;最大后验估计：
T = arg max_T P(T|D)&lt;/p&gt;
&lt;p&gt;当 T = t₂ 时：P(T=t₂|D) = P(p₁^p₂|D) = P(D|p₁^p₂) P(p₁^p₂) / P(D) = P(p₁,p₂,D) / P(D)
= P(p₂|p₁,D) P(p₁|D) P(D)/ P(D) = P(p₂|p₁,D) P(p₁|D)
= P(p₂|p₁,D) P(T=t₁|D)&lt;/p&gt;
&lt;p&gt;P(T=t₂|D) 与 P(T=t₁|D) 之间相差一个小于1的系数，所以当 T=t₁ 时后验概率更大&lt;/p&gt;
&lt;p&gt;所以用贝叶斯方式思考更理性&lt;/p&gt;
&lt;h2 id=&#34;用贝叶斯理解概率&#34;&gt;用贝叶斯理解概率&lt;/h2&gt;
&lt;p&gt;概率的两种理解：频率派和贝叶斯派&lt;/p&gt;
&lt;p&gt;频率派认为概率是某件事多次发生的频率，抛硬币可以多次重复，但不适合解释神经网络，(多分类)神经网络蕴含的模型与人脑中的模型的差异是损失函数，被看做为一个概率（“是猫的概率”）。这个概率是是softmax 对各类别上的 activation z 做归一化后得到的，是样本之间的差异，不好用频率解释。&lt;/p&gt;
&lt;p&gt;贝叶斯公式中用的都是概率密度函数，先验概率 P(θ) 理解成在没有数据可供参考时，对神经网络参数的相信程度，相信程度可以是正态分布；后验概率是已知数据X之后，经过修正的对网络参数的相信程度。θ分布修正后，似然度就发生变化，在配分函数中的占比就变化，从而继续修正分布，当某个分布的似然度在配分函数中占比最大时，它就是最优的 θ 分布，其后验概率最大，它发生的可能性最大，对它的相信程度最高。&lt;/p&gt;
&lt;p&gt;分母 P(X) 是边缘概率，也叫配分函数，将其按条件概率展开：&lt;/p&gt;
&lt;p&gt;$$
P(θ_i | X) = \frac{P(X|θ_i) P(θ_i)}{∑ P(X|θ) P(θ)} = \frac{P(X|θ_i) P(θ_i)}{\int_θ P(X|θ) P(θ) dθ}&lt;/p&gt;
&lt;p&gt;= \frac{P(X|θ_i) P(θ_i)}{P(X|θ_i) P(θ_i) + ∑_{k≠i} P(X|θ_k) P(θₖ) dθ}
$$&lt;/p&gt;
&lt;p&gt;分子是具体的一个 θ，分母是把所有可能的 θ 全部取一遍并相加，如果 θ 是连续的，就是积分的过程。&lt;/p&gt;
&lt;p&gt;置信度是 [0,1] 之间的数，但不好求，似然值用梯度下降求，配分函数用蒙特卡洛方法求&lt;/p&gt;
&lt;h2 id=&#34;用贝叶斯理解梯度下降&#34;&gt;用贝叶斯理解梯度下降&lt;/h2&gt;
&lt;p&gt;找最优参数的过程拆解成一个序列：
P(θ|X) = P(s₁ s₂ s₃ &amp;hellip; sₙ|X)；
给定数据集 X， 先得到 s₁，下降一次得到 s₂，再得到 s₃，下降n次到达 sₙ&lt;/p&gt;
&lt;p&gt;用贝叶斯公式展开：&lt;/p&gt;
&lt;p&gt;P(θ|X) = P(s₁ s₂ s₃ &amp;hellip; sₙ|X) = P(X|s₁ s₂ s₃ &amp;hellip; sₙ) P(s₁ s₂ s₃ &amp;hellip; sₙ) / P(X) = P(s₁ s₂ s₃ &amp;hellip; sₙ, X)/P(X)
= P(sₙ | s₁ s₂ s₃ &amp;hellip; sₙ₋₁, X) P(s₁ s₂ s₃ &amp;hellip; sₙ₋₁, X)/P(X)&lt;/p&gt;
&lt;p&gt;= P(sₙ | s₁ s₂ s₃ &amp;hellip; sₙ₋₁, X) P(sₙ₋₁| s₁ s₂ s₃ &amp;hellip; sₙ₋₂, X) &amp;hellip;. P(s₃|s₁,s₂,X) P(s₂|s₁,X) P(s₁|X) P(X)/P(X)&lt;/p&gt;
&lt;p&gt;= P(sₙ | s₁ s₂ s₃ &amp;hellip; sₙ₋₁, X) P(sₙ₋₁| s₁ s₂ s₃ &amp;hellip; sₙ₋₂, X) &amp;hellip;. P(s₃|s₁,s₂,X) P(s₂|s₁,X) P(s₁|X)&lt;/p&gt;
&lt;p&gt;从右向左求。&lt;/p&gt;
&lt;p&gt;假设中间的每一步并不依赖前面的所有结果，只依赖于它前面一步的结果:&lt;/p&gt;
&lt;p&gt;P(θ|X) = P(sₙ | sₙ₋₁, X) P(sₙ₋₁| sₙ₋₂, X) &amp;hellip;. P(s₃|s₂,X) P(s₂|s₁,X) P(s₁|X)&lt;/p&gt;
&lt;p&gt;要求 P(θ|X) 的最大值，计算量还是太大，不是考虑整体乘积达到最大，只考虑局部每一小节，下一步等于上一步的最大值:&lt;/p&gt;
&lt;p&gt;$$s_{i+1} = arg max_{s_{i+1}} P(s_{i+1} | s_i, X)$$&lt;/p&gt;
&lt;p&gt;从第一项到最后一项依次都取最大，最后的结果不一定是整体最大值，但也能得到一个近似的结果。上式就是最大后验估计，
损失函数是：P(sᵢ₊₁ | sᵢ, X)，
则梯度下降为： sᵢ₊₁ = sᵢ + η ⋅ ∇ P(sᵢ₊₁ | sᵢ, X)&lt;/p&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 11 | L1L2 Reg (1), Larange Multiplier</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/11_l1l2%E6%AD%A3%E5%88%99%E5%8C%961-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/</link>
        <pubDate>Mon, 07 Nov 2022 11:52:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/11_l1l2%E6%AD%A3%E5%88%99%E5%8C%961-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/</guid>
        <description>&lt;p&gt;原视频：&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Z44y147xA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;“L1和L2正则化”直观理解(之一)，从拉格朗日乘数法角度进行理解 - 王木头学科学 -bilibili&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(2023-01-30) 加上正则化项，就变为求拉格朗日函数的最小值，即求&amp;quot;损失函数+权重模长&amp;quot;整体的最小值。
λ 是个比值，λ不同对应的W的位置不同，即λ确定了W的位置。然后 𝐖 自己不断调整，使 J(𝐖) 与 λ‖𝐖‖ 二者的梯度等大反向(抵消)，从而使 $∇_𝐖 L(𝐖,λ)=0$。λ不同，则最优W就不同。_&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;机器学习的两个核心议题：优化与正则化。优化找到最优参数；正则化减少模型的过拟合：对模型权重𝐖（不处理b）进行L1和L2正则化&lt;/p&gt;
&lt;p&gt;从3个角度理解 L1 和 L2 正则化：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;拉格朗日乘数法&lt;/li&gt;
&lt;li&gt;权重衰减&lt;/li&gt;
&lt;li&gt;贝叶斯概率&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;正则化&#34;&gt;正则化&lt;/h2&gt;
&lt;p&gt;与“正则表达式”没有关系，正则表达式是编程中用来处理字符串的技术。
正则化常常指模型权重的L1和L2范数。Dropout 也是一种正则化，在训练时，随机的让一些神经元失效。&lt;/p&gt;
&lt;p&gt;花书：凡是可以减少泛化误差（过拟合），而不是减少训练误差的方法，都可以称作正则化方法。&lt;/p&gt;
&lt;p&gt;L1, L2 正则化项是加在损失函数上，约束模型参数𝐖的 L1, L2 范数的项。L1范数使W变得稀疏，L2范数使W变得小。&lt;/p&gt;
&lt;p&gt;拟合时一般希望保留更多特征，高次的函数表达能力更强，所以模型倾向于学出高次项的系数，使训练误差更小，但不能很好地预测新数据，没有找到基础规律，出现过拟合 &lt;a class=&#34;link&#34; href=&#34;#3-%e8%8e%ab%e7%83%a6&#34; &gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;如果特征（各属性）已知，可以把 polynomials 的高次项系数置为0；否则，需采用与&amp;quot;特征的定义&amp;quot;无关的约束，可以选择模长最小的，误差(mse)也最小的参数。
Problem is now well-posed for any degree。Even very high polynomials, simple function tend to be learned with regularization&lt;a class=&#34;link&#34; href=&#34;#2-UCI&#34; &gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
所以对细节特征的系数（θ₃²+θ₄²）增大惩罚力度，bias them not to be large （min ∑ᵢᵐ(y^(xᵢ)-yᵢ)² + 1000θ₃²+1000θ₄²）&lt;a class=&#34;link&#34; href=&#34;#1-%e5%87%a9%e5%ad%90%e7%99%bd&#34; &gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;范数：向量长度。网络权重 𝐖 是高维空间中的一个向量（也可认为W是高维空间一个点，范数即为这个点到原点的距离）。
若使用欧几里得距离（坐标差求平方和再开根号）度量它的模长，就是 L2 范数: ‖𝐖‖₂=√(|w₁|²+|w₂|²+&amp;hellip;+|wᵢ|²)；
若用曼哈顿距离计算（坐标差的绝对值求和），就是 L1 范数: ‖𝐖‖₁=|w₁|+|w₂|+&amp;hellip;+|wᵢ|。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 Lp 范数的 p 取值大于等于 1 时，有相同距离的点构成的集合是一个凸集；&lt;/li&gt;
&lt;li&gt;当 p 取 0-1 之间，是非凸集。&lt;/li&gt;
&lt;li&gt;当p小于等于1时，会带来稀疏性，所以 the L1 norm is the only norm which both induces some sparsity in the solution and remains convex for easy optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果问题对应的函数是一个凸函数，它的取值范围（可行域）是一个凸集，这就是一个凸优化问题，容易求解。L1和L2正则化在某种程度上，就是在利用 L1 和 L2 范数的凸集特性。&lt;/p&gt;
&lt;h2 id=&#34;出现过拟合的原因之一w太大会把噪声放大&#34;&gt;出现过拟合的原因之一：W太大会把噪声放大&lt;/h2&gt;
&lt;p&gt;神经网络的输出层做了三件事：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;接收上一层的激活值 a⁽ˡ⁻¹⁾，乘以权重 Wˡᵀ，加上偏移bᵀ，得到线性叠加后的 zˡ： zˡ= Wˡᵀ⋅a⁽ˡ⁻¹⁾+bᵀ。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再做（激活引入非线性/限制取值0-1，然后）softmax 把线性结果 zˡ 变成概率分布: aˡ = softmax (zˡ)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输出层的损失函数：J(aˡ) = MLE (aˡ)。&lt;br&gt;
极大似然 MLE 与 CrossEntropy 等价，前者最大化重复采样训练样本的概率，后者最小化网络给出的类别概率乘以真实概率对应的信息量。
（softmax 搭配MLE，等价于最大熵。）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;调整 𝐖,𝐛 使损失函数J(𝐖,𝐛)最小，但当损失值最小时，对应的 𝐖,𝐛 不是唯一的。比如:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;把前面所有隐藏层的权重和偏置都放大2倍（用ReLu激活），但同时输出层的权重和偏置缩小 1/2⁽ˡ⁻¹⁾倍，得到的线性结果不变，则损失值不变，所以最终优化得到的参数与初始值相关。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果输入放大 2 倍的同时输出层权重缩小 2 倍，线性结果不变，损失值也不变。所以最终优化得到的 𝐖,𝐛 与输入数据相关。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/L1L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%80_9-28.png width=&gt;
  
  


&lt;p&gt;训练神经网络时的目的是找到损失函数 J(W,b) 的最小值，但相同的损失值可能对应多组绝对值（模长）不同的(W,b)。&lt;/p&gt;
&lt;p&gt;绝对值不同的 𝐖,𝐛 可能在训练集上的损失值相同，但在测试集、unseen输入、&amp;ldquo;带噪声的输入&amp;quot;上的表现有差别，不同参数的泛化能力不同。
绝对值大的参数会把噪声放大，从而影响预测结果。&lt;/p&gt;
&lt;p&gt;正则化就是人为的设定参数𝐖 的取值范围（可行域），让W不能超出该范围，从而&lt;strong&gt;在可行域范围内，求损失函数的最小值&lt;/strong&gt;。带条件的优化问题可以用拉格朗日乘数法求解。
我们只需要规定 𝐖 的范围（多项式各未知数的系数），因为 W 直接决定了模型曲线是什么样子，而 b 与最终拟合的曲线的形状（过拟合与否）无关，b只影响平移，因此只要𝐖 被约束好了，b在训练时会自动调整好，所以不需额外做约束。&lt;/p&gt;
&lt;h2 id=&#34;w-的可行域范围&#34;&gt;w 的可行域范围&lt;/h2&gt;
&lt;p&gt;在求损失函数最小值时，权重 𝐖 的模长不要超过C：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;若采用曼哈顿距离度量模长，即计算向量的L1范数，则优化问题为：&lt;br&gt;
min J(𝐖 , 𝐛, 𝐗), s.t. ‖𝐖‖₁-C≤0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若采用欧几里得距离计算模长（向量的L2范数），则优化问题为：&lt;br&gt;
min J(𝐖 , 𝐛, 𝐗), s.t. ‖𝐖‖₂-C≤0&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;把优化问题写成&lt;strong&gt;拉格朗日函数&lt;/strong&gt; = 目标函数 + 约束条件乘以拉格朗日乘子 λ： &lt;br&gt;
L(𝐖, λ) = J(𝐖) + λ(‖𝐖‖₁ - C), &lt;br&gt;
然后求解：$\rm min_𝐖 \ max_λ L(𝐖,λ), s.t. λ≥0$ （不考虑b,X）&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/L1L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%80_14-14.png width=&gt;
  
  


&lt;p&gt;上图坐标轴是 𝐖 的各个维度，椭圆是损失函数J(𝐖)的等高线，中心是损失函数的最小值对应的 𝐖，绿色框代表可行域范围（𝐖 的模长限制），蓝色点是在满足约束条件时，损失函数能取到的最小值，对应的𝐖 ；或者说在可行域范围内找损失函数梯度+𝐖可行域梯度=0的点。&lt;/p&gt;
&lt;p&gt;“当目标函数是一个凸函数或是一个凹函数，并且对应的约束条件是一个凸集，那么整个问题（目标函数+λ⋅约束条件）就是一个凸优化问题”。
约束条件采用L1或L2范数时，可行域是一个凸集。对应于凸集的约束条件并不会改变原来问题的性质，
即如果原来的问题本身是一个凸问题，加上这个约束条件后，仍然是凸问题，如果原来的问题是非凸问题，加上这个约束条件也不会让这个问题变得更糟糕。&lt;/p&gt;
&lt;p&gt;(2022-11-07) 损失函数与正则化项都画在二维平面上，是因为研究的只是“两个对象”：输入X和输出Y之间的关系？&lt;br&gt;
不，因为是要限制w的模长，二维平面只表示了二维w。加了正则化项，相当于修改了初始时的w，缩短它到那条虚线的距离，但不考虑它对应的损失值变大还是变小了，虚线总是沿着损失函数的梯度方向，而且连接着损失值最小的那个w，不过那条虚线上的w是等比的，等比例缩放的 w 能收敛到的最值是相同的&lt;/p&gt;
&lt;p&gt;(2023-01-30) 正则化项 λ‖𝐖‖ 的作用是持久的。在 λ‖𝐖‖ 的基础上，𝐖 做调整以使损失值最小，也就是使 J(𝐖) 与 λ(‖𝐖‖₁ - C) 两项的梯度抵消（等大反向）。&amp;ldquo;在原始最小二乘的结果上做了缩放&amp;rdquo;&lt;a class=&#34;link&#34; href=&#34;#book%e7%8e%8b%e5%a4%a9%e4%b8%80&#34; &gt;4&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;l1-l2-正则化项&#34;&gt;L1, L2 正则化项&lt;/h2&gt;
&lt;p&gt;把拉格朗日函数展开：&lt;/p&gt;
&lt;p&gt;L(𝐖,λ)  = J(𝐖) + λ(‖𝐖‖₂-C) = J(𝐖) + λ‖𝐖‖₂ - λC&lt;/p&gt;
&lt;p&gt;但是常见的 “损失函数+ L2正则化项” 的表达式是 拉格朗日函数+λC：&lt;/p&gt;
&lt;p&gt;L&amp;rsquo;(𝐖,λ) = L(𝐖,λ) + λC = J(𝐖) + λ‖𝐖‖₂&lt;/p&gt;
&lt;p&gt;在求L&amp;rsquo; 和 L 这两个拉格朗日函数最小值时，虽然最小值可能不一样，但对应的 𝐖 是一样的
（因为只要求解 $∇_𝐖 L&amp;rsquo;(𝐖,λ) =∇_𝐖 L(𝐖,λ)=0$）：&lt;/p&gt;
&lt;p&gt;$arg_𝐖 (min_𝐖\ max_λ L&amp;rsquo;(𝐖,λ), s.t.\ λ≥0) = arg_𝐖 (min_𝐖\ max_λ L(𝐖,λ), s.t.\ λ≥0)$&lt;/p&gt;
&lt;p&gt;如何直观理解 L&amp;rsquo; 呢？
或者说：为什么&amp;quot;常用的L2正则化表达式&amp;quot;比&amp;quot;损失函数加上 𝐖 约束条件写成的拉格朗日函数&amp;rdquo;，少一个 λC 呢？&lt;/p&gt;
&lt;p&gt;C 决定了 𝐖 的模长，不指定 C 是因为 λ 可以控制模长的范围。拉格朗日乘子λ的作用是调节&amp;quot;约束条件λ‖𝐖‖的梯度&amp;quot;的大小，使之与&amp;quot;目标函数J(𝐖)的梯度&amp;quot;等大反向抵消（求导=0就是求梯度=0的点）。
当λ=0时，相当于没有约束；而当λ=inf时，W会趋于0，失去拟合能力了。λ 可以用cross validation 来选择&lt;/p&gt;
&lt;p&gt;




  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/L1L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%80_20-06.png width=&gt;
  
  

&lt;center style=&#34;font-size 12px&#34;&gt;红色是损失函数的等高线，绿色是约束条件的等高线，箭头表示梯度&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;λ 的绝对值等于&amp;quot;损失函数J(𝐖)的梯度大小&amp;quot;除以&amp;quot;约束条件对应函数λ‖𝐖‖的梯度大小&amp;quot;，所以空间中各点处的λ就能算出来，反过来不同的 λ 对应的点的位置不同。只有在恰当的位置J(W)与λ‖𝐖‖的梯度之和才等于0。
~~不同的 λ 对应的梯度=0的点的位置不同，损失函数J(𝐖)最小值不同，但对应的（拉格朗日函数的最优）W是相同的??? ~~&lt;/p&gt;
&lt;p&gt;λ和C之中只有一个超参数：在拉格朗日函数中，C是超参数，人为指定模长范围C后，λ 也就跟着确定了；而在 L&amp;rsquo; 中，λ 是超参数，人为指定后，就能唯一确定梯度=0的点的位置。&lt;/p&gt;
&lt;p&gt;L2正则化（给定λ后）确定的最值点（最优𝐖）基本不会落在坐标轴上（可行域是圆形），
而L1正则化找到的满足约束条件与目标函数的梯度之和=0的点（可行域范围与损失函数等高线相切的点）容易取在坐标轴上（可行域有尖角且落在坐标轴上，容易与损失函数相切），
当最优 𝐖 落在某一坐标轴上时，只有那一维不是0，其他维度的坐标都是0。&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/L1L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%80_24-56.png width=&gt;
  
  


&lt;p&gt;比如依据两个特征做判断时，最优 𝐖 不落在坐标轴上，则两个特征都有一定的决定作用，
而通过调整 λ 使最优W落在坐标轴上时，则只会关注该轴特征的有无，所以使用 L1 可以使决策变得简单，使 W 向量变得稀疏（只有某个“重要”维度(对结果贡献大)有值，其他维都是0），把特征之间的关系去耦合了，把模型复杂度降低了，从而减少过拟合。
但是L1 的解不太稳定，训练时各批次数据的损失函数不同，椭圆会变化，切点可能从一个轴换到另一个轴上，对应的W变化大。&lt;a class=&#34;link&#34; href=&#34;3-%e8%8e%ab%e7%83%a6&#34; &gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;L1 和 L2 都是对 W 进行约束，但效果不同，可以把二者结合起来一起用。L2正则化只是限制模长，L1 正则化还带来了稀疏性&lt;/p&gt;
&lt;h2 id=&#34;正则化带来的损失值误差不重要啊&#34;&gt;正则化带来的损失值误差？不重要啊&lt;/h2&gt;
&lt;p&gt;&lt;del&gt;评论：上图绘制的椭圆对应的损失函数J(𝐖)的接收的输入是整个实数域，而在神经网络中，损失函数接收的是网络的输出值，网络不同的权重可能对应相同的输出值，损失值也就相同&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;因为不同的初始参数 𝐖 最终收敛到的损失函数最小值可能是相等的，比如等比缩放参数时，min J(𝐖 ,𝐛) = min J(a𝐖 ,a𝐛)， (𝐖 ,𝐛) 与 (a𝐖 ,a𝐛) 共线（虚线），&lt;/p&gt;
&lt;p&gt;加约束条件λ‖𝐖‖与不加约束条件(λ=0)，通过最小化拉格朗日函数L(𝐖,λ)收敛到的 W 不同（W由λ决定），但是如果两个向量W是共线的，&lt;del&gt;它们对应的 L(W,λ) 能够收敛到的最小值（where L(W,λ)的梯度=0）是相同的，只不过λ取的不好，还没走到最小值，还没平衡好 J(W) 和 ||W||之间的权重比例。&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;同一虚线上的 W 最终能够收敛到的拉格朗日函数值是相同的。虚线上的点是本可能收敛到的最值&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/L1L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%80_25-17.png width=&gt;
  
  


&lt;p&gt;如果不加约束条件，初始的较大的 W 本可以收敛到J(W)椭圆中心（损失最小值），但是因为指定了约束条件λ，模长被限制了，被拉向原点了，找到的最优W与椭圆中心离得很远，损失值J(W)变大了，看起来就带来了误差。
但是（最小化拉格朗日函数过程中的）误差并不是到椭圆中心的距离，而是到“虚线”的距离，虚线上的 W 已经调整到能使 J(W) 与 ||W||的梯度共线，因为 λ 是人为设置的，所以W还要继续调整，最终虚线上的W使 L(W,λ)的梯度=0。&lt;/p&gt;
&lt;p&gt;在虚线上的不同 W 对应的拉格朗日函数值L(W,λ)的梯度是相同的（在虚线上的点，方向共线已满足，只是λ不同），
虽然可行域越小，对损失函数J(W)的最小值偏离的误差就越大，因为W初始值决定了最终能收敛到哪个J(W)，但关注的不是它J(W)。&lt;/p&gt;
&lt;p&gt;W调整到最后：J(W)的梯度方向与||W||梯度方向共线，并且对于超参数缩放因子λ，有L(W,λ)的梯度=0。&lt;/p&gt;
&lt;p&gt;所以真正关注的到最优解的偏差，是到虚线的距离，超参数λ选的好与坏，带来的偏差不大，所以能用正则化就用。&lt;/p&gt;
&lt;h2 id=&#34;-&#34;&gt;-&lt;/h2&gt;
&lt;p&gt;例子：只有两个分量的W 与对应的损失值 绘制的图像为：&lt;/p&gt;
&lt;p&gt;同心椭圆是损失函数 J 的等高线，即每一圈椭圆上，损失值是相同的，中间的灰色图形的最外沿的一圈代表 W 的范数。正方形代表 L1 范数的图像，圆形代表 L2 范数的图像&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;看下这篇：&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/jianxinzhou/p/4083921.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;机器学习之正则化（Regularization）- Acjx -博客园&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;DDG搜索：entropy regularization 熵正则化&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;(2023-01-30) &lt;br&gt;
DDG search: &amp;ldquo;正则化项&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;正则化的作用&#34;&gt;正则化的作用&lt;/h2&gt;
&lt;p&gt;原文链接：&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/gshgsh1228/article/details/52199870&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;正则化及正则化项的理解 - guuuuu - CSDN&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;防止过拟合：𝐖 越小，拟合的函数曲线越简单光滑，越不容易过拟合；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;正则化项代表先验信息：试验之前的对‖𝐖‖的认知，λ 是对先验信息的相信程度。&lt;br&gt;
频率派直接对参数𝐖进行分析（而贝叶斯派是对参数出现的概率P(𝐖)进行分析）；
参数 𝐖 直接出现在损失函数中，所以频率派对损失函数做修正：加上了先验部分知识，即正则化项。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于模型：y = θ₀ + θ₁x₁ + &amp;hellip; + θⱼxⱼ + &amp;hellip; + θₙxₙ，解最优化问题：&lt;br&gt;
$arg\ min_{θ₀,θ₁,&amp;hellip;,θₙ}\ J(θ) = 1/2m ⋅(∑ᵢ₌₁ᵐ(h_θ(xⁱ)-yⁱ)² + λ∑ⱼ₌₁ⁿ(θⱼ-\^θⱼ)²)$，
其中 ^θⱼ 为先验解。 &lt;br&gt;
λ 不同大小的选择，体现了这个先验解 ^θⱼ 的可信程度。如果 λ 是一个很小的整数，那正则化项将不起什么作用，说明给的先验解有很大的不确定性，在一定程度上是不可信的；&lt;br&gt;
如果 λ 很大，则正则化项占支配地位，最后的解将靠近于 ^θⱼ。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;频率派&lt;/th&gt;
&lt;th&gt;贝叶斯派&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;找最优𝐖&lt;/td&gt;
&lt;td&gt;直接对参数 𝐖 本身下手&lt;/td&gt;
&lt;td&gt;对参数出现的概率 P(𝐖) 下手&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;思路&lt;/td&gt;
&lt;td&gt;最优 𝐖 使训练误差最小&lt;/td&gt;
&lt;td&gt;最优 𝐖 出现的后验概率 P(𝐖&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;优化公式&lt;/td&gt;
&lt;td&gt;损失函数&lt;/td&gt;
&lt;td&gt;贝叶斯公式&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;先验&lt;/td&gt;
&lt;td&gt;正则化项（对𝐖 的先验认知）&lt;/td&gt;
&lt;td&gt;自带先验概率（P(𝐖)）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;修正&lt;/td&gt;
&lt;td&gt;加上一部分先验信息&lt;/td&gt;
&lt;td&gt;对似然值做 &amp;ldquo;P(𝐖)/配分函数&amp;rdquo; 的缩放&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;超参数&lt;/td&gt;
&lt;td&gt;λ; L1 or L2范数&lt;/td&gt;
&lt;td&gt;先验概率的分布: Laplace, Gaussian&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;有助于处理&lt;strong&gt;条件数&lt;/strong&gt;（condition number）不好的情况下，矩阵求逆困难的问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;概念：如果方阵 A 是非奇异的（A的行列式不等于0，正定矩阵一定是非奇异的），那么 A 的 condition number 定义为：𝜅(A) = ‖A‖ ‖A⁻¹‖&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;可以看出：如果 A 是奇异的，那么 A 的条件数为无穷大。条件数越小，所获得的解越可靠，模型鲁棒性越好，抗干扰能力越强。
例如对于模型 AX=b，A 的条件数越小（A的行列式远不接近于0），那么 A，b 的稍微的变化对解 X 的影响越小，对 X 的求解对样本集（A，b）中引入的干扰的抵抗能力越强，即所求解 X 越可靠。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;DDG search: &amp;ldquo;正则化项 pytorch&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/147579819&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PyTorch 12.正则化-科技猛兽-知乎&lt;/a&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# optimizer w and w/o regularization&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;optim_normal&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SGD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;net_normal&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lr_init&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;momentum&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;optim_wdecay&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SGD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;net_weight_decay&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lr_init&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;momentum&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;weight_decay&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;DDG search: &amp;ldquo;pytorch optimizer weight decay&amp;rdquo;
&lt;a class=&#34;link&#34; href=&#34;https://pytorch.org/docs/stable/generated/torch.optim.SGD.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SGD-pytorch docs&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;(2023-02-26)
DDG serach: &amp;ldquo;正则化项 可以完全避免过拟合吗&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.zhihu.com/question/389848505&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;在机器学习中，L2正则化为什么能够缓过拟合？ - 知乎&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ref&#34;&gt;Ref&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;div id=&#34;1-凩子白&#34;&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Kq4y1K7hZ&#34;&gt;[知识梳理-03] Regularization 正则化-凩子白-bilibili&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;2-UCI&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sO4ZirJh9ds&#34;&gt;Linear regression (6): Regularization (UCI cs273a) - Alexander Ihler&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;3-莫烦&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TmzzQoO8mr4&#34;&gt;什么是 L1 L2 正规化 正则化 Regularization (深度学习 deep learning)&lt;/a&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div id=&#34;book王天一&#34;&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 08 | Advanced Gradient Descent</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/08_%E6%94%B9%E8%BF%9B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
        <pubDate>Sun, 02 Oct 2022 12:07:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/08_%E6%94%B9%E8%BF%9B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</guid>
        <description>&lt;p&gt;Source video: &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1r64y1s7fU&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;“随机梯度下降、牛顿法、动量法、Nesterov、AdaGrad、RMSprop、Adam”，打包理解对梯度下降法的优化&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;梯度下降法在实际应用中的优化：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减小每次梯度下降的计算量：随机（分批次）梯度下降&lt;/li&gt;
&lt;li&gt;减少迭代次数，即优化下降路径&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;随机梯度下降&#34;&gt;随机梯度下降&lt;/h2&gt;
&lt;p&gt;对于一个凸问题，时间复杂度与误差量级的关系：&lt;/p&gt;
&lt;p&gt;对于一个强凸问题，能收敛得更快&lt;/p&gt;
&lt;p&gt;正常情况下，标准梯度下降法（1个batch）应该比随机梯度下降法快，但可以证明，不会快过 1/k&lt;/p&gt;
&lt;p&gt;一个 batch 上的损失函数，可能与整个数据集上的损失函数不同，各处对应的梯度也不同，
所以每次迭代时的梯度方向不一定是“全体数据的误差函数”上的最优，每一步的行进可能会偏离下降最快的最优路径，从而导致需要更多次的迭代，才能到达极值点。&lt;/p&gt;
&lt;p&gt;如下图，从 A 点 到 A&amp;rsquo; 点的最优路径是橙色线，如果分两步，先走到 B ，B 再沿着它的梯度方向走，就走偏了。&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D_%E5%88%86%E6%89%B9.png width=&gt;
  
  


&lt;p&gt;减小步长，可以让下降路径更贴近最优下降路径，但是计算量太大。&lt;/p&gt;
&lt;h2 id=&#34;牛顿法&#34;&gt;牛顿法&lt;/h2&gt;
&lt;p&gt;牛顿法是用来拟合曲线的，在梯度下降中，就是拟合损失函数表面上的最优下降路径对应的曲线。&lt;/p&gt;
&lt;p&gt;对于一个只有一维变量的问题，纵轴是各变量取值对应的误差，蓝色曲线即是损失函数。
要到达损失函数的最小值处，根据梯度下降法，先求出损失函数在当前点的梯度（各个方向分量，按向量加法相加），这里只有一个变量（一个方向），就是求损失函数的切线。
然后变量沿着梯度（切线）方向移动一点，看看此时的误差值。&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D_%E4%B8%80%E7%BB%B4.png width=&gt;
  
  


&lt;ol&gt;
&lt;li&gt;抛物线比直线更贴近损失函数，从而使下降路径与损失函数更贴合，而不是折线。&lt;/li&gt;
&lt;li&gt;因为整个数据集上的损失函数未知，每下降一步，就在当前点的邻域范围内做泰勒展开，用一段高次函数对损失函数做近似代替；
又因为是找下降的方向，所以要保留到二次项，这样就能 &lt;strong&gt;拟合出在损失函数（表面）上的下降路径&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;牛顿法的每一步是确定的：抛物线的顶点对应的横坐标就是这一步要走到的位置，所以牛顿法里没有步长。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下降的方向就不是梯度的方向了?&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D_%E7%89%9B%E9%A1%BF%E6%B3%95.png width=&gt;
  
  


&lt;p&gt;如上图，灰色的直线是到极值点的最优路径，但是未知。
牛顿法希望每一步都走在损失函数上，即拟合出损失函数（表面）上的最优下降路径。&lt;/p&gt;
&lt;p&gt;泰勒展开保留二阶导，用二次函数近似表达损失函数上的每一点：&lt;/p&gt;
&lt;p&gt;f(x) = J(a₀) + J&amp;rsquo;(a₀)(x-a₀) + 1/2 J&amp;quot;(a₀)(x-a₀)²&lt;/p&gt;
&lt;p&gt;求 f(x) 的极值，就是求顶点所在位置，令 f&amp;rsquo;(x) = 0:&lt;/p&gt;
&lt;p&gt;$$
f&amp;rsquo;(x) = 0 + J&amp;rsquo;(a₀) + J&amp;quot;(a₀)(x-a₀) = 0 \\
x = a₀ - J&amp;rsquo;(a₀) / J&amp;quot;(a₀)
$$&lt;/p&gt;
&lt;p&gt;然后让变量走到顶点的位置，对应于权重更新：W = W - J&amp;rsquo;(a₀) / J&amp;quot;(a₀)，没有学习率η。
用一阶导数与二阶导数的比值对（一元）变量W做更新。&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1SE411E72Y/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;path-int&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;直观理解：按固定步长做梯度下降法是橙色线，步长无穷小时，下降路径是灰色线，牛顿法的下降路径是绿色线，比梯度下降法更贴近最优路径的灰色线。
所以牛顿法是在拟合最优路径对应的曲线。&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D_%E8%B7%AF%E5%BE%84%E6%8B%9F%E5%90%88.png width=&gt;
  
  


&lt;p&gt;对高维函数求二阶偏导，需要算 Hessian 矩阵。对于高维的损失函数 J(W)，参数更新公式为：
𝐖 = 𝐖 - 𝛁𝐉²(𝐖)⁻¹ ⋅ 𝛁𝐉(𝐖)，其中 𝛁𝐉²(𝐖) 就是 Hessian 方阵 𝐇(𝐖)&lt;/p&gt;
&lt;p&gt;$$
𝐇(𝐖) =
\begin{bmatrix}
\frac{∂}{∂W₁}\frac{∂f}{∂W₁} &amp;amp; \frac{∂}{∂W₂}\frac{∂f}{∂W₁} &amp;amp; &amp;hellip; &amp;amp; \frac{∂}{∂Wₙ}\frac{∂f}{∂W₁}\\
\frac{∂}{∂W₁}\frac{∂f}{∂W₂} &amp;amp; \frac{∂}{∂W₂}\frac{∂f}{∂W₂} &amp;amp; &amp;hellip; &amp;amp; \frac{∂}{∂Wₙ}\frac{∂f}{∂W₂}\\
&amp;hellip; \\
\frac{∂}{∂W₁}\frac{∂f}{∂Wₙ} &amp;amp; \frac{∂}{∂W₂}\frac{∂f}{∂Wₙ} &amp;amp; &amp;hellip; &amp;amp; \frac{∂}{∂Wₙ}\frac{∂f}{∂Wₙ}\\
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;列向量更新公式：𝐖ₙₓ₁ = 𝐖ₙₓ₁ - 𝐇(𝐖)⁻¹ₙₓₙ ⋅ 𝛁𝐉(𝐖)ₙₓ₁&lt;/p&gt;
&lt;p&gt;虽然迭代次数比梯度下降法少，但并不实用，缺点:&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1SE411E72Y/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;path-int&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算量太大，每一步都要计算n维向量的一阶差分 O(n)，Hessian矩阵 O(n²)，及其逆&lt;/li&gt;
&lt;li&gt;不能保证目标函数值在迭代过程中一直下降，可能会先升高，再下降；&lt;/li&gt;
&lt;li&gt;不能保证收敛：因为牛顿法使用二阶泰勒展开近似，需要初始点在极小点附近，每一步都满足近似条件（Hessian矩阵是正定的），效果才会比较好。
如果离得很远，获得的结果可能非常奇怪。一般应用其他方法先搜索到极小点附近，再用牛顿法（或拟牛顿法）来继续更高精度的搜索。&lt;/li&gt;
&lt;li&gt;如果目标函数 f(x) 只是一阶可微，二阶不可微（Hessian矩阵不存在），牛顿法就不适用了。
如果二阶可微，理论上牛顿法收敛速度比梯度法要快。牛顿法收敛阶数至少是2，梯度法收敛阶数最差情况下是1。&lt;/li&gt;
&lt;li&gt;如果是一个多元凸函数，但是不是处处可导，Taylor近似展开不能适应，牛顿法不可应用。&lt;/li&gt;
&lt;li&gt;若 H 不可逆，需要用 Levenberg-Marquadt 修正：加常量阵 λ𝐈，即给对角线加上足够大的值，使所有的 eigenvalue 都大于0，意味着在任何方向上的一阶导数都大于0.
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1zE41177WB&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;path-int&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;动量法&#34;&gt;动量法&lt;/h2&gt;
&lt;p&gt;牛顿法同时考虑损失函数的所有维度，找出最优下降路径。&lt;/p&gt;
&lt;p&gt;梯度是一个多维的向量，可以把各个维度拆开（向量的分解），单独分析每个方向上的变化&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D_%E5%8A%A8%E9%87%8F%E6%B3%95%E5%88%86%E8%A7%A3.png width=&gt;
  
  


&lt;p&gt;加权是要把维度分开考虑&lt;/p&gt;
&lt;p&gt;考虑下降两步，抵消相反方向的梯度
惯性 mv，力作用的效果？把速度抵消，减少震荡&lt;/p&gt;
&lt;p&gt;前几步梯度大，直接加的话，占主导，而且不准，用一个系数控制前后两个部分的权重就用 beta 和 1-beta （exponentially weighted moving average），越是先前发生的状态，乘以的（1-beta）次数越多，占比越来越小，对当前的值影响越小&lt;/p&gt;
&lt;h2 id=&#34;nesterov&#34;&gt;Nesterov&lt;/h2&gt;
&lt;p&gt;等高线坐标系下的点是权重W，Nesterov 把上一时刻的权重按照上一时刻的梯度方向走了一步，没有与当前时刻的梯度加权，把权重点移动到的新位置，这个位置是比如果采用加权时的权重超调一些的，把新位置的梯度带回去，也就是考虑了下一步位置的情况&lt;/p&gt;
&lt;p&gt;当前梯度方向d1，历史梯度方向d2，未来梯度方向d3，共同决定向哪个方向走。未来就是先按d2走一步梯度下降，走到一个点，计算那个点的梯度。不是按d1和d2的加权方向走，那样就是提前做了决策，还把决策的结果拿过来用了，应该用历史数据来决策&lt;/p&gt;
&lt;h2 id=&#34;adagrad&#34;&gt;AdaGrad&lt;/h2&gt;
&lt;p&gt;当前梯度除以累计历史梯度内积再开根号，梯度的各方向分量数量级不一致，所以除以累计历史梯度模长，梯度值大的那个方向，除以一个数（可能与该方向数值在同一数量级）就没那么大了，不至于在那个方向上走太多，而且分母越加越大，梯度越往后越小。
稀疏数据集：只要关心维度/特征的有无就能把类别分开，而不需关心特征是否明显（猴子和人）；非稀疏数据：需要关注同一特征上的差别，在同一特征上的数值，才能分类（长尾猫和短尾猫）
稀疏数据不同样本没有共同的特征，不同特征的绝对值差异大，数值相减后差值大，梯度大&lt;/p&gt;
&lt;p&gt;维度越多，越不需要区分在同一特征上的数值差异，只需关心特征的有无，单位球内的点分散到各个维度上了，不再挤在同一维度上了，各个轴上的长度就不用那么长了，所以体积就变小了&lt;/p&gt;
&lt;p&gt;王木头解释成学习率的衰减不太合适&lt;/p&gt;
&lt;p&gt;梯度按泰勒公式展开，牛顿法用了二次项，动量法修正0次项基础值，AdaGrad修正1次项（梯度变化量）&lt;/p&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 10 | Method of Lagrange Multipliers</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/10_%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/</link>
        <pubDate>Sat, 01 Oct 2022 14:14:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/10_%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1HP4y1Y79e&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;10“拉格朗日对偶问题”如何直观理解？“KKT条件” “Slater条件” “凸优化”打包理解&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;101-拉格朗日乘数法&#34;&gt;10.1 拉格朗日乘数法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一种寻找多元函数在其变量受到一个或多个&lt;strong&gt;约束条件&lt;/strong&gt;时的极值的方法（自变量的取值范围有限制）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将一个有n个变量与k个约束条件的最优化问题转换为一个解有 n+k 个变量的方程组的解的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对每个约束条件用拉格朗日乘子(待定系数) λᵢ 加权，加到目标函数后面，就是拉格朗日函数。
拉格朗日函数与目标函数的最值是一样的，所以求目标函数的最值就转化为求拉格朗日函数的最值。&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
目标函数求最小值：&amp;amp; min\ f_0(𝐱),\quad 𝐱 ∈ ℝⁿ \\
m个约束条件：&amp;amp; s.t. \quad f_i(𝐱) ≤ 0,\ 其中 i=1,2,3&amp;hellip;m \\
拉格朗日函数：&amp;amp; L(𝐱,\pmb{λ}) = f_0(𝐱) + \sum λ_i f_i(𝐱)
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;目标函数被加上了约束条件，变量只能在规定的范围内取值，求导得到的极值点可能不在规定范围内，而在规定范围内可能也没有极值点。拉格朗日乘数法把带约束问题转化为无约束问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个约束条件用拉格朗日乘子加权&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;102-用梯度理解lagrange-multiplier&#34;&gt;10.2 用梯度理解Lagrange Multiplier&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个约束条件的情况：&lt;/p&gt;
&lt;p&gt;求 $f(x,y)$ 的最小值，并且有一个约束条件 $y=g(x)$。&lt;/p&gt;
&lt;p&gt;该问题的拉格朗日函数为：$L(x,y) = f(x,y) + λ(y-g(x))$；[约束条件=0 表示一条线]&lt;/p&gt;
&lt;p&gt;对拉格朗日函数求梯度，梯度等于0的点就是极值对应的点；&lt;/p&gt;
&lt;p&gt;把 $∇ L(x,y) = 0$，沿x,y两个方向展开，调整λ使两个方向上的偏导都为0，也就是梯度在两个方向上的分量都为0：&lt;/p&gt;
&lt;p&gt;$$
\begin{cases}
\frac{∂f(x,y)}{∂x} + λ\frac{∂(y-g(x))}{∂x} = 0 \\
\frac{∂f(x,y)}{∂y} + λ\frac{∂(y-g(x))}{∂y} = 0
\end{cases}
$$&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
  &lt;td&gt;




  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E7%8E%8B%E6%9C%A8%E5%A4%B4%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95.png width=&gt;
  
  

1&lt;/td&gt;
  &lt;td&gt;




  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E7%8E%8B%E6%9C%A8%E5%A4%B4%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95_%E6%A2%AF%E5%BA%A6.png width=&gt;
  
  

2&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;在图1中，坐标系是x-y，同心圆是目标函数 $f(x,y)$ 的等高线，圆心点对应的函数值最小，越往外值越大。红线是(x,y)约束条件。&lt;/p&gt;
&lt;p&gt;图2显示了两个函数的梯度，只有在相切的位置，目标函数的梯度方向与约束条件的梯度方向才是&lt;strong&gt;共线&lt;/strong&gt;的，再通过拉格朗日乘子 λ 调整向量长短，使两个梯度相加才可能为零。除了相切点位置，都无法实现两梯度之和为零。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多个约束条件：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
目标函数：&amp;amp; min \ f(𝐱), \quad 𝐱∈ ℝⁿ &amp;amp; \text{(𝐱 是个n维向量)}\\
m个约束条件：&amp;amp; s.t. \quad g_i(𝐱) = \pmb{a_i^T} ⋅ 𝐱 + b_i ≤ 0, &amp;amp;\text{(m个超平面围成的区域)}\\
&amp;amp; 其中i=1,2,3&amp;hellip;m, \pmb{a_i} ∈ ℝⁿ, \ b_i ∈ ℝⁿ \\
拉格朗日函数：&amp;amp; L(𝐱, \pmb λ) = f(𝐱) + \sum λ_i g_i(𝐱)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;假设有5个一维的约束条件，就是5条直线，并且假设它们围成了一个五边形，还要假设五边形的内部是5个约束条件同时满足的区域，如下图1。&lt;/p&gt;
&lt;p&gt;(对于二维平面上的一条直线:$y=ax+b$，想知道 $y≤0$ 表示的是哪块区域，可以根据(x,0)这个点判断。
$y=ax+b ≤0 \Rightarrow x≤\frac{-b}{a}$，
所以在$\frac{-b}{a}$左侧就是规定的区域。
如果a是负数，符号就会改变，从而多条直线能围出一个封闭的限制区域。
)&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
  &lt;td&gt;




  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E7%8E%8B%E6%9C%A8%E5%A4%B4_%E5%A4%9A%E4%B8%AA%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6.png width=&gt;
  
  

1&lt;/td&gt;
  &lt;td&gt;




  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E7%8E%8B%E6%9C%A8%E5%A4%B4_%E5%A4%9A%E4%B8%AA%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6_2.png width=&gt;
  
  

2&lt;/td&gt;
&lt;/table&gt;
&lt;p&gt;对拉格朗日函数求梯度，令其等于零：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\pmb ∇ L(𝐱,\pmb λ) = 0
\Downarrow
-\pmb ∇ f(\pmb λ) = ∑ λ_i ⋅ \pmb∇ g_i(𝐱)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;目标函数梯度的反方向应等于所有约束条件的梯度加权和。&lt;/p&gt;
&lt;p&gt;由图2可知，真正起贡献的只有两个约束条件在，只需求两个约束条件的梯度和，五边形内部是 ≤ 0，则外部是&amp;gt;0，所以梯度方向指向外面，两个梯度通过 λ 调节成与目标函数梯度等大反向.&lt;/p&gt;
&lt;p&gt;两约束条件的交点 x&amp;rsquo; 既满足这两个约束条件，在这点的梯度=0：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; g_α(x&amp;rsquo;) = g_β(x&amp;rsquo;) = 0 \
&amp;amp; λ_α ∇ g_α(x&amp;rsquo;) + λ_β ∇ g_β(x&amp;rsquo;) = -∇ f(x) ⇒ λ_α, λ_β≠0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;而该点 x&amp;rsquo; 也满足其他约束条件 gᵢ(x&amp;rsquo;)&amp;lt;0，但它们的 λᵢ 都等于0，不起作用: λᵢ∇ gᵢ(x)=0 ⇒ λᵢ=0 (i≠α,β)
λ 不能小于 0，否则会把梯度方向取反，会参与梯度叠加。&lt;/p&gt;
&lt;p&gt;所有的 λᵢ 都≥0，如果λᵢ=0, 它对应的约束条件 gᵢ(x) 是松弛的；如果λᵢ&amp;gt;0, 它对应的约束条件 gᵢ(x) 是紧致的。约束条件像皮筋，把最优解从最值点拽到了极值点。
当目标函数的最小值落在可行域范围内，则所有的约束条件都是松弛的。&lt;/p&gt;
&lt;p&gt;拉格朗日乘数法本质还是求导=0，梯度等于0的点是极值点，但不一定是最值点&lt;/p&gt;
&lt;p&gt;KKT 条件中的互补松弛&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 05 | Info Quantity &amp; Cross Entropy</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/05_%E4%BA%A4%E5%8F%89%E7%86%B5/</link>
        <pubDate>Fri, 30 Sep 2022 13:30:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/05_%E4%BA%A4%E5%8F%89%E7%86%B5/</guid>
        <description>&lt;p&gt;Source video: &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV15V411W7VB?spm_id_from=333.999.0.0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;5-“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;信息量&#34;&gt;信息量&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;事件发生 &lt;strong&gt;概率&lt;/strong&gt; 的负对数 -log₂ p&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若$f(p)$ 被定义为信息量，要让体系自洽则需满足：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
f(p) &amp;amp; \coloneqq 信息量 \\
f(p_1 ⋅ p_2) &amp;amp; = f(p_1) + f(p_2)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;对于阿根廷从八强打到冠军这件事，可以拆成两件事：阿根廷进入决赛+阿根廷赢了决赛。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;这两种描述的信息量是一样的：$f$(阿根廷夺冠) = $f$(阿根廷进入决赛) + $f$(阿根廷赢了决赛) $\Rightarrow f(\frac{1}{8}) = f(\frac{1}{4}) + f(\frac{1}{2})$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;另外还要满足事件间的概率关系：P(阿根廷夺冠) = P(阿根廷进入决赛) ⋅ P(阿根廷赢了决赛)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以选择 log 函数，可以满足自洽；从直观来看，发生概率越小，所含信息量越大，
而 log 函数是递增的，所以系数取 -1；而底数可以选 e，也可选 2。&lt;/p&gt;
&lt;p&gt;$$
f(x) \coloneqq -log_2 x
$$&lt;/p&gt;
&lt;p&gt;底数选2，相当于用抛硬币事件来衡量信息量。某事件的发生概率是 1/8，相当于抛3个硬币全部朝上的概率。并且以 2 为底计算出的信息量的单位是比特。类似地，输入 16 比特的数据就是把 16 个 0/1 确定下来&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;熵&#34;&gt;熵&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;系统中各事件信息量的 &lt;strong&gt;期望&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;H(P) \coloneqq E(P_f) \\
&amp;amp;= ∑_{i=1}^m p_i ⋅ f(p_i) = ∑_{i=1}^m p_i(-log_2p_i) \\
&amp;amp;= -∑_{i=1}^m p_i ⋅ log_2 p_i
\end{aligned}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;衡量整个系统中的所有事件的不确定性&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kl散度-相对熵&#34;&gt;KL散度 (相对熵)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;系统Q相对于系统P差多少：事件 i 在两系统中的 &lt;strong&gt;信息量之差&lt;/strong&gt;，按照 i 在系统 P 中的概率加权求和。&lt;/p&gt;
&lt;p&gt;$$
D_{KL}(P\|Q) = ∑_{i=1}^m p_i ⋅ (-log_2 q_i) -
∑_{i=1}^m p_i ⋅ (-log_2 p_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;无法直接对比两个不同种类模型之间的差异（无法公度），而且人脑中的概率模型不清楚，无法求熵，需要 &lt;strong&gt;相对熵&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/KL%E6%95%A3%E5%BA%A6%E5%AE%9A%E4%B9%89.png width=&gt;
    
    
  
  &lt;center&gt;Q系统P系统的概率分布&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;对于某事件 $i$ 在系统 Q 中的信息量 $f_Q(q_i)$ 减去它对应到在系统 P 中的信息量 $f_P(p_i)$，再按照在系统 P 中的概率 $p$ 求期望：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
D_{KL} (P \| Q)
&amp;amp; \coloneqq ∑_{i=1}^m p_i ⋅ \left(f_Q(q_i) - f_P(p_i)\right)  \\
&amp;amp;= ∑_{i=1}^m p_i ⋅ \left((-log_2 q_i) - (-log_2 p_i)\right) \\
&amp;amp;= \underbrace{∑_{i=1}^m p_i ⋅ (-log_2 q_i)}_{交叉熵H(P,Q)}-
\underbrace{∑ᵢ₌₁ᵐ pᵢ⋅ (-log₂pᵢ)}_{P的熵}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;如果事件 i 在两系统中的信息量相等，差值为0，说明两个系统完全相等。&lt;/p&gt;
&lt;p&gt;根据吉不斯不等式：&lt;/p&gt;
&lt;p&gt;如果有两个概率系统，$∑_{i=1}^n p_i = ∑_{i=1}^n q_i = 1$，且 $p_i, q_i \in (0,1]$，则有：&lt;/p&gt;
&lt;p&gt;$$
- ∑_{i=1}^n p_i log_{p_i} ≤ -∑_{i=1}^n p_i log_{q_i}
$$&lt;/p&gt;
&lt;p&gt;当且仅当 $p_i = q_i\ ∀ i$ 时，等号成立。&lt;/p&gt;
&lt;p&gt;也就是说 KL 散度恒大于等于0（距离）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;交叉熵&#34;&gt;交叉熵&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$$
H(P,Q) = ∑_{i=1}^m p_i ⋅ (- \operatorname{log}_2 q_i)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;事件 i 在概率系统 Q 中的信息量，按照 i 在系统 P 中的概率加权求和&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了使 KL 散度最小，即概率系统 Q 最接近系统P，就需要交叉熵最小（最接近系统P的熵）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于一张图片，人脑系统（目标系统 P）只有 2 个事件：是猫和不是猫
(&amp;ldquo;是猫&amp;quot;事件发生的概率为 x，则&amp;quot;非猫&amp;quot;事件发生的概率为 1-x)，
神经网络（系统Q）的结果（y）是像猫的概率，所以交叉熵为：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
H(P,Q) &amp;amp;= ∑_{i=1}^2 p_i ⋅ (-log_2 q_i) \\
&amp;amp;= x ⋅ log(y) + (1-x) ⋅ log(1-y) \\
&amp;amp;= 1 ⋅ log(y) + 0 ⋅ log(1-y)
\end{aligned}
$$&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E4%BA%BA%E8%84%91%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BA%A4%E5%8F%89%E7%86%B5.png width=&gt;
    
    
  

&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 04 | Loss Functions</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/04_%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
        <pubDate>Fri, 30 Sep 2022 13:26:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/04_%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Y64y1Q7hi?spm_id_from=333.999.0.0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;4-“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;损失函数&#34;&gt;损失函数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;定量衡量两个概率模型的差异&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;三种方法：
&lt;ol&gt;
&lt;li&gt;最小二乘法&lt;/li&gt;
&lt;li&gt;极大似然&lt;/li&gt;
&lt;li&gt;交叉熵&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;最小二乘法&#34;&gt;最小二乘法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;直接比较判断结果。&lt;/p&gt;
&lt;p&gt;min $\sum_{i=1}^n (x_i - y_i)^2$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;人脑中的概率模型无法准确说出，而神经网络的概率模型蕴藏在参数里面，没有统一的表达？只能从结果入手。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将每次人的判断结果 $x_i$ (1/0) 与 神经网络的判断结果 $y_i$ (45%:1, 55%:0) 的误差 $|x_i -y_i|$ 求和取最小，从而保证在结果上看是最接近的。&lt;/p&gt;
&lt;p&gt;$$
min \sum_{i=1}^n |x_i - y_i|
$$&lt;/p&gt;
&lt;p&gt;因绝对值在定义域上不是全程可导的，求平方不影响 x 和 y 的大小关系，而且全程可导，加1/2是为了求导方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用最小二乘法作为损失函数，使用梯度下降法很麻烦&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;极大似然&#34;&gt;极大似然&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于已发生的现实事件，有很多概率模型都能导致这个情况发生，取似然值最大的那个概率模型作为最“真实的”概率模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由于噪声的存在，现实世界中的概率模型偏移了理想世界中的模型，由于理想世界与现实世界之间有次元壁，无法直接知道理想世界中真实的概率模型，所以只能从现实世界反推，估计出一个概率模型，它使该现实事件发生的可能性最大。&lt;/p&gt;
&lt;p&gt;比如掷10次硬币的结果是7次正面，3次反面。有3种概率模型：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;概率统计模型 θ&lt;/th&gt;
&lt;th&gt;正面&lt;/th&gt;
&lt;th&gt;反面&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.1  &lt;/td&gt;
&lt;td&gt;0.9  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.7  &lt;/td&gt;
&lt;td&gt;0.3  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.8  &lt;/td&gt;
&lt;td&gt;0.2  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这三种概率模型都可以掷出7次正面，3次反面。不过第2种概率模型掷出7正3反的概率（似然值）最大，所以认为第2种是最接近“真实的”概率模型。&lt;/p&gt;
&lt;p&gt;$$
P(C₁, C₂, C₃, &amp;hellip;, C_10 | \theta) = \prod_{i=1}^{10} P(C_i | \theta)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;似然值&lt;/strong&gt;：用可能导致现实事件发生的概率模型，计算出来的这种情况发生的概率值.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用已经标注好的 n 张图片（硬币正反）去训练神经网络，神经网络的概率模型 (𝐖, 𝐛) 产生出这n张图片的概率就是模型的似然值：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;P(x_1, x_2, x_3, &amp;hellip;, x_n | \mathbf {W,b}) \\
&amp;amp;= \prod_{i=1}^n P(x_i | \mathbf{W,b}) &amp;amp; \text{$x_i \in {0,1}$,表示&amp;quot;是猫&amp;quot;,&amp;ldquo;不是猫&amp;rdquo;}\\
&amp;amp;= \prod_{i=1}^n P(x_i | y_i) &amp;amp; \text{𝐖,𝐛决定了模型（给出的&amp;quot;是猫的概率&amp;quot;$y_i$}） \\
&amp;amp;= \prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-{x_i}} &amp;amp;\text{采样服从0-1分布: $P=\begin{cases} y_i, &amp;amp; x=1是猫 \\ 1-y_i, &amp;amp; x=0不是猫\end{cases}$}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;比如有7张是猫，3张不是猫，并假设神经网络在 𝐖,𝐛 的参数下，判断是猫的概率是 45%，不是猫的概率是 55%。那么此模型的似然值=$(0.45)^7 (0.55)^3$&lt;/p&gt;
&lt;p&gt;连乘变连加：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;log \left( \prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-x_i} \right) \\
&amp;amp;= \sum_{i=1}^n log(y_i^{x_i} (1-y_i)^{1-x_i}) \\
&amp;amp;= \sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i))
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;当似然值达到最大的时候，就是最接近人脑的模型。&lt;/p&gt;
&lt;p&gt;极大似然估计：$max \sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i))$&lt;/p&gt;
&lt;p&gt;习惯求极小：$min - \sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i))$&lt;/p&gt;
&lt;p&gt;x 是训练数据（从真实分布中采样得到的），有自己的分布（目标分布），希望模型输出的标签与输入的标签分布一致，所以 &amp;ldquo;让学习到的分布产生输入数据的概率最大 &lt;code&gt;∑log P(输入数据 | learned分布)&lt;/code&gt;&amp;rdquo; 与 &amp;ldquo;最小化两分布差异 &lt;code&gt;∑ P(目标分布) log(P(learned分布))&lt;/code&gt;” 等价，也就是极大似然与交叉熵等价。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 06 | Gradient Descent</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/06_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
        <pubDate>Tue, 04 Jan 2022 23:29:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/06_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Zg411T71b?spm_id_from=333.999.0.0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;6-如何理解“梯度下降法”？什么是“反向传播”？通过一个视频，一步一步全部搞明白&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;正向传播&#34;&gt;正向传播&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;输入数据沿着神经网络正向传递&lt;/li&gt;
&lt;li&gt;输入数据与各个感知机的w和b点乘，将结果代入激活函数，给出判断结果&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;反向传播&#34;&gt;反向传播&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;把判断结果的偏差传递给各个w和b，根据参数对偏差贡献的大小，成比例的调整&lt;/li&gt;
&lt;li&gt;未训练好的神经网络的$\mathbf w,b$不准确，导致判断结果有偏差。如果某 $\mathbf w/b$ 对最终的判断结果有重大影响，则该参数对于偏差也是有重大影响的。所以在减小误差的过程中，应优先调整d对偏差有重大影响的参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;脑补过程：&lt;/p&gt;
&lt;p&gt;神经网络输出层的输出值是 $a^{[3]}$:&lt;/p&gt;
&lt;p&gt;$$
\mathbf a^{[3]} = \sigma(\mathbf w^{[3]} \cdot \mathbf a^{[2]} + b^{[3]})
$$&lt;/p&gt;
&lt;p&gt;其中 $\sigma$ 是激活函数，$\mathbf w^{[3]}$输出层权重，$\mathbf a^{[2]}$上一层的输出值，$b^{[3]}$是输出层偏置系数。&lt;/p&gt;
&lt;p&gt;可以计算损失函数（交叉熵），得到偏差J：&lt;/p&gt;
&lt;p&gt;$$
J = \frac{1}{n} \left( -∑_{i=1}^n (y^{(i)}) ⋅ log_2 a^{[3](i)} + (1-y^{(i)} ⋅ log_2(1-a^{[3](i)}) )\right)
$$&lt;/p&gt;
&lt;p&gt;其中 $y_i$ 是理想系统中输出y的概率；$log₂ a^{[3](i)}$是判断结果的信息量。&lt;/p&gt;
&lt;p&gt;偏差直接来自输出层的感知机:&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E5%81%8F%E5%B7%AE%204-43.png width=&gt;
  
  


&lt;p&gt;偏差来自三部分：当前层的 w 和 b，以及上一层的输出 $a^{[2]}$。其中 w 和 b 可以根据占比直接调整，而 $a^{[2]}$ 的偏差来自于上一层，按照贡献大小分配偏差：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E4%B8%8A%E4%B8%80%E5%B1%82%E7%9A%84%E5%81%8F%E5%B7%AE%E5%88%86%E9%85%8D%206-14.png width=&gt;
  
  


&lt;p&gt;然后前一层的感知机占有的偏差又可分成3部分，调整 $w, b$ 和 上一层的输出.&lt;/p&gt;
&lt;p&gt;第一隐藏层的感知机的偏差与第二隐藏层所有感知机相关：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E7%AC%AC%E4%B8%80%E9%9A%90%E8%97%8F%E5%B1%82%206-48.png width=&gt;
  
  


&lt;p&gt;由此，整个神经网络中的每个 $w$ 和 b 都能分配到偏差占比。&lt;/p&gt;
&lt;p&gt;以上利用的是数值的加法分配偏差，还可使用向量的加法来分配偏差，不过首先要确定向量的方向。&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E5%90%91%E9%87%8F%E5%8A%A0%E6%B3%95%208-33.png width=&gt;
  
  


&lt;p&gt;梯度的反方向就是要找的向量方向：数值减小最快的方向。
梯度可以在i方向和j方向上分解，对于点(x,y)沿变化率最大的方向变化就是在i和j方向上同步变化。&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%2014-02.png width=&gt;
  
  







  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E8%BF%AD%E4%BB%A3%2017-26.png width=&gt;
  
  


&lt;p&gt;对(输出层)损失函数 J 求梯度：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;\nabla J (\mathbf w^{[3]}, a^{[2]}, b^{[3]}) \\
&amp;amp; = (\alpha, \beta, \gamma) &amp;amp; \text{三个分量的系数，简略表示} \\
&amp;amp; = \alpha \cdot \mathbf i + \beta \cdot \mathbf j + \gamma \cdot \mathbf k
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;输出层的输出沿梯度方向变化$\eta$步长，向目标值靠近：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbf w^{[3]}&lt;em&gt;{(\rm target)} &amp;amp;= \mathbf w^{[3]} - \eta \cdot \alpha \
b^{[3]}&lt;/em&gt;{(\rm target)} &amp;amp;= b^{[3]} - \eta \cdot \gamma \
\
a^{[2]}&lt;em&gt;{(\rm target)} &amp;amp;= a^{[2]} - \eta \cdot \beta &amp;amp; 需要反向传递分配到第2隐藏层上 \
\cancel{\eta \cdot}\ \beta &amp;amp;= a^{[2]}&lt;/em&gt;{(\rm target)} - a^{[2]}_{(\rm now)}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;感知机输出值a的中间传递不考虑 $\eta$，偏差最终分配在输入层的w和b上，其中含有$\eta$。从而可以看出隐藏层与输出层类似，也是目标与输出之间的差值。因此对于第2隐藏层的&amp;quot;损失函数&amp;quot;：
$J^{[2]} = a^{[2]}&lt;em&gt;{(\rm target)} - a^{[2]}&lt;/em&gt;{(\rm now)}$，$J^{[2]} (\mathbf w^{[3]}, a^{[2]}, b^{[3]})$ 开启下一轮：求梯度,求损失函数&lt;/p&gt;
&lt;p&gt;各层的运算形式相同（损失函数），就可以迭代分配偏差。J沿梯度方向变化就可以最快的减小偏差，因此使用了向量的加法做偏差分配。&lt;/p&gt;
&lt;h2 id=&#34;梯度&#34;&gt;梯度&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;求偏导就是求（固定另一个维度）曲线的切线，两个偏导组合就是两条切线组合，两条切线确定了切面。所以对曲面求偏导，就是在求切面&lt;/li&gt;
&lt;li&gt;把两个切线合成一个向量，就是梯度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\alpha, \beta,\gamma$ 的具体表示（求偏导）：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\alpha &amp;amp;= \frac{\partial J}{\partial \mathbf w^{[3]}} \
\gamma &amp;amp;= \frac{\partial J}{\partial b^{[3]}} \
\beta &amp;amp;= \frac{\partial J}{\partial \mathbf a^{[2]}}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;分配偏差：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbf w^{[3]} &amp;amp;= \mathbf w^{[3]} - \eta \cdot \frac{\partial J}{\partial \mathbf w^{[3]}} \
b^{[3]} &amp;amp;= b^{[3]} - \eta \cdot \frac{\partial J}{\partial b^{[3]}} \
\frac{\partial J}{\partial \mathbf a^{[2]}} &amp;amp;= a^{[2]}&lt;em&gt;{(\rm target)} - a^{[2]}&lt;/em&gt;{(\rm now)} &amp;amp;
\text{“新损失函数”}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;令$J^{[2]}=a^{[2]}&lt;em&gt;{(\rm target)} - a^{[2]}&lt;/em&gt;{(\rm now)}$ 作为下一轮的损失函数，对损失函数$J^{[2]}(\mathbf w^{[2]}, \mathbf a^{[1]}, b^{[2]}$)分配：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbf w^{[2]} &amp;amp;= \mathbf w^{[2]} - \eta \cdot \frac{\partial J^{[2]}}{\partial \mathbf w^{[2]}} \
b^{[2]} &amp;amp;= b^{[2]} - \eta \cdot \frac{\partial J^{[2]}}{\partial b^{[2]}} \&lt;/p&gt;
&lt;p&gt;\frac{1}{n} \sum_{i=1}{n} \frac{\partial J^{[2]}}{\partial \mathbf a^{[1]}} &amp;amp;= a^{[1]}&lt;em&gt;{(\rm target)} - a^{[1]}&lt;/em&gt;{(\rm now)} &amp;amp;
\text{“$a^{[1]}$的偏差是4个$a^{[2]}$的偏差之平均”}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;令 𝑱⁽¹⁾=a₍ₜₐᵣᵨₑₜ₎⁽¹⁾ - a₍ₙₒᵥ₎⁽¹⁾ 作为下一轮的损失函数，对损失函数$J^{[1]}(\mathbf w^{[1]}, \mathbf a^{[0]}, b^{[1]}$)分配，其中 $a^{[0]}$ 是输入，无法修改，只调整 w 和 b。&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E7%AC%AC%E4%BA%8C%E8%BD%AE%2027-04.png width=&gt;
  
  


&lt;p&gt;对于第 $l$ 层的第 $i$ 个感知机，接受上一层所有神经元的输出 $a^{[l-1]}$，与它的 $\mathbf w_i^{[l]}$ 和 $b_i^{[l]}$ 做线性运算得到 $z_{i}^{[l]}$，把 z 送入激活函数得到这个感知机的输出：$\sigma(z_i^{[l]})=a_i^{[l]}$&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E7%AC%A6%E5%8F%B7%E7%90%86%E8%A7%A3%2030-32.png width=&gt;
  
  


&lt;p&gt;$$
\begin{aligned}
z_i^{[l]} &amp;amp;= \mathbf w_i^{[l]} \cdot \mathbf a^{[l-1]} + b_i^{[l]} \
z_i^{[l]} &amp;amp;=
\begin{bmatrix}
\mathbf w_{i,1}^{[l]} \ \vdots \\mathbf w_{i,n}^{[l]}
\end{bmatrix}^T
\cdot
\begin{bmatrix}
a_{1}^{[l-1]} \ \vdots \ a_{n}^{[l-1]}
\end{bmatrix}
+ b_i^{[l]}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;对一层感知机进行表述:&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E5%90%84%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%BF%90%E7%AE%97%2032-02.png width=&gt;
  
  


&lt;p&gt;没有下标 $i$ 表示整层，第$l$层各感知机的输出：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbf z^{l} &amp;amp;= \mathbf w^{l} \cdot \mathbf a^{l-1} + b^{[l]} \
\mathbf z^{l} &amp;amp;=
\begin{bmatrix}
z_1^{[l]} \ \vdots \z_i^{[l]}
\end{bmatrix}
=
\begin{bmatrix}
\mathbf w_{1,1}^{[l]} &amp;amp; \cdot &amp;amp; \mathbf w_{1,j}^{[l]} \
\vdots &amp;amp; \ddots &amp;amp; \cdots \
\mathbf w_{i,1}^{[l]} &amp;amp; \cdot &amp;amp; \mathbf w_{i,j}^{[l]} \
\end{bmatrix}
\cdot
\begin{bmatrix}
a_1^{[l-1]} \ \vdots \ a_j^{[l-1]}
\end{bmatrix}
+
\begin{bmatrix}
b_1^{[l]} \ \vdots \ b_i^{[l]}
\end{bmatrix} \&lt;/p&gt;
&lt;p&gt;a^{[l]} &amp;amp;=\sigma\left(z^{[l]}\right)=\sigma\left(\left[\begin{array}{c}
z_{1}^{[l]} \
\vdots \
z_{i}^{[l]}
\end{array}\right]\right)=\left[\begin{array}{c}
\sigma\left(z_{1}^{[l]}\right) \
\vdots \
\sigma\left(z_{i}^{[l]}\right)
\end{array}\right]=\left[\begin{array}{c}
a_{1}^{[l]} \
\vdots \
a_{i}^{[l]}
\end{array}\right]&lt;/p&gt;
&lt;p&gt;\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;输出层的损失函数：$J(y,a^{[l]})$，y是label，(k)是第几个样本：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E8%BE%93%E5%87%BA%E5%B1%82%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2034-05.png width=&gt;
  
  


&lt;p&gt;每个样本$\mathbf x$都有 j 个属性，对应第0层的输出 $a^{[0]}$：&lt;/p&gt;
&lt;p&gt;$$
a^{[l]}=\sigma\left(z^{[l]}\right)=\sigma\left(\left[\begin{array}{c}
z_{1}^{[l]} \
\vdots \
z_{i}^{[l]}
\end{array}\right]\right)=\left[\begin{array}{c}
\sigma\left(z_{1}^{[l]}\right) \
\vdots \
\sigma\left(z_{i}^{[l]}\right)
\end{array}\right]=\left[\begin{array}{c}
a_{1}^{[l]} \
\vdots \
a_{i}^{[l]}
\end{array}\right]
$$&lt;/p&gt;
&lt;p&gt;多分类问题，有多个输出 $aᵢ^{&lt;a class=&#34;link&#34; href=&#34;k&#34; &gt;l&lt;/a&gt;}$，此时的损失函数是把所有输出节点都考虑进来。不考虑常量: 样本x和标签y，损失函数的输出层感知机的函数：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E5%A4%9A%E5%88%86%E7%B1%BB%E7%AE%80%E5%8C%96%2036-03.png width=&gt;
  
  


&lt;p&gt;反向传播：对J求梯度，给各变量分配偏差(偏导)，走$eta$步长&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E5%A4%9A%E5%88%86%E7%B1%BB%E6%B1%82%E6%A2%AF%E5%BA%A6%2036-18.png width=&gt;
  
  


&lt;p&gt;把对输出层各感知机的偏导看作是第 $l+1$ 层，第 l+1 层只对 第l层的一个感知机有作用，$J_1^{[l+1]}$ 是 $\mathbf w_1^{[l]},\ a^{l-1},\ b_1^{l}$ 的函数：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E5%A4%9A%E5%88%86%E7%B1%BB%E6%B1%82%E6%A2%AF%E5%BA%A62%2037-58.png width=&gt;
  
  


&lt;p&gt;最后$\nabla J$的各项都从输出层 $a$ 开始求导（链式求导）：&lt;/p&gt;
&lt;p&gt;$$
\nabla J_{1}^{[l+1]} =
\left(
\frac{\partial J}{\partial a_{1}^{[l]}}
\frac{\partial a_{1}^{[l]}}{\partial \sigma}
\frac{\partial \sigma}{\partial z_{1}^{[l]}}
\frac{\partial z_{1}^{[l]}}{\partial W_{1}^{[l]}},
\quad
\frac{\partial J}{\partial a_{1}^{[l]}}
\frac{\partial a_{1}^{[l]}}{\partial \sigma}
\frac{\partial \sigma}{\partial z_{1}^{[l]}}
\frac{\partial z_{1}^{[l]}}{\partial a^{[l-1]}},
\quad
\sqrt{\frac{\partial J}{\partial a_{1}^{[l]}}
\frac{\partial a_{1}^{[l]}}{\partial \sigma}}
\frac{\partial \sigma}{\partial z_{1}^{[l]}}
\frac{\partial z_{1}^{[l]}}{\partial b_{1}^{[l]}}
\right)
$$&lt;/p&gt;
&lt;p&gt;其中 $\mathbf w$ 是向量，对其求偏导要对它的每个分量求偏导：&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{l}
\frac{\partial J_{1}^{[l+1]}}{\partial W_{1}^{[l]}}=\left(\frac{\partial J_{1}^{[l+1]}}{\partial W_{1,1}^{[l]}}, \frac{\partial J_{1}^{[l+1]}}{\partial W_{1,2}^{[l]}}, \ldots, \frac{\partial J_{1}^{[l+1]}}{\partial W_{1, j}^{[l]}}\right) \
\frac{\partial J_{1}^{[l+1]}}{\partial a^{[l-1]}}=\left(\frac{\partial J_{1}^{[l+1]}}{\partial a_{1}^{[l-1]}}, \frac{\partial J_{1}^{[l+1]}}{\partial a_{2}^{[l-1]}}, \ldots, \frac{\partial J_{1}^{[l+1]}}{\partial a_{j}^{[l-1]}}\right)
\end{array}
$$&lt;/p&gt;
&lt;p&gt;每层的 $\mathbf w$ 和 $b$ 求偏导之后可直接修改：&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{c}
W_{1}^{[l]}=W_{1}^{[l]}-\eta \cdot \frac{\partial J_{1}^{[l+1]}}{\partial W_{1}^{[l]}} \quad b_{1}^{[l]}=b_{1}^{[l]}-\eta \cdot \frac{\partial J_{1}^{[l+1]}}{\partial b_{1}^{[l]}} \
\vdots \
W_{i}^{[l]}=W_{i}^{[l]}-\eta \cdot \frac{\partial J_{i}^{[l+1]}}{\partial W_{i}^{[l]}} \quad b_{i}^{[l]}=b_{1}^{[l]}-\eta \cdot \frac{\partial J_{i}^{[l+1]}}{\partial b_{i}^{[l]}}
\end{array}
$$&lt;/p&gt;
&lt;p&gt;对 $a^{l-1}$ 求偏导得到的是第 $a^{[l-1]}$ 层的变化量，作为损失函数：&lt;/p&gt;
&lt;p&gt;$$
J_{1}^{[l]}=\frac{\partial J_{1}^{[l+1]}}{\partial a^{[l-1]}} \quad \cdots \quad J_{i}^{[l]}=\frac{\partial Jᵢ^{[l+1]}}{\partial a^{[l-1]}}
$$&lt;/p&gt;
&lt;p&gt;3个窗口反向移动，做下一轮:&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E4%B8%8B%E4%B8%80%E8%BD%AE%2040-41.png width=&gt;
  
  


&lt;p&gt;以第2隐藏层为中心，进行分析：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E7%AA%97%E5%8F%A3%E7%A7%BB%E5%8A%A8%2041-30.png width=&gt;
  
  


&lt;p&gt;把 a 展开（对a的各分量求偏导）：&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{c}
\nabla J_{1}^{[l+1]}=\left(\frac{\partial J_{1}^{[l+1]}}{\partial W_{1}^{[l]}}, \frac{\partial J_{1}^{[l+1]}}{\partial a^{[l-1]}}, \frac{\partial J_{1}^{[l+1]}}{\partial b_{1}^{[l]}}, \ldots, \frac{\left.\partial J_{1}^{[l+1]}\right]}{\partial W_{i}^{[l]}}, \frac{\partial J_{1}^{[l+1]}}{\partial a^{[l-1]}}, \frac{\partial J_{1}^{[l+1]}}{\partial b_{i}^{[l]}}\right) \
\vdots \
\nabla J_{k}^{[l+1]}=\left(\frac{\partial J_{k}^{[l+1]}}{\partial W_{1}^{[l]}}, \frac{\partial J_{k}^{[l+1]}}{\partial a^{[l-1]}}, \frac{\partial J_{k}^{[l+1]}}{\partial b_{1}^{[l]}}, \ldots, \frac{\partial J_{k}^{[l+1]}}{\partial W_{i}^{[l]}}, \frac{\partial J_{k}^{[l+1]}}{\partial a^{[l-1]}}, \frac{\partial J_{k}^{[l+1]}}{\partial b_{i}^{[l]}}\right)
\end{array}
$$&lt;/p&gt;
&lt;p&gt;第l层的每个感知机的偏差由第l+1 层的所有偏差值赋予：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E7%94%B1%E4%B8%8A%E4%B8%80%E5%B1%82%E6%89%80%E6%9C%89%E5%81%8F%E5%B7%AE%E8%B5%8B%E4%BA%88%2042-00.png width=&gt;
  
  


&lt;p&gt;然后第l层各参数的变化量：&lt;/p&gt;
&lt;p&gt;$$
\left(\Delta W_{1}^{[l]}, \Delta a^{[l-1]}, \Delta b_{1}^{[l]}, \ldots, \Delta W_{i}^{[l]}, \Delta a^{[l-1]}, \Delta b_{i}^{[l]}\right)
$$&lt;/p&gt;
&lt;p&gt;$\delta a$ 作为下一轮的损失函数：&lt;/p&gt;
&lt;p&gt;$$
\left(\Delta W_{1}^{[l]}, J_{1}^{[l]}, \Delta b_{1}^{[l]}, \ldots, \Delta W_{i}^{[l]}, J_{i}^{[l]} \quad, \Delta b_{i}^{[l]}\right)
$$&lt;/p&gt;
&lt;p&gt;第2次迭代：&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD_%E7%AC%AC2%E6%AC%A1%E8%BF%AD%E4%BB%A3%2042-43.png width=&gt;
  
  


&lt;p&gt;$a^{[0]}$ 是常量输入，求导为零，此时的$J^{l-1}$只与 $\mathbf w$ 和 $b$ 有关:&lt;/p&gt;
&lt;p&gt;$$
\begin{array}{c}
\nabla J_{1}^{[l+1]}=
\left(
\frac{\partial J_{1}^{[l+1]}}{\partial W_{1}^{[l]}},
\frac{\partial J_{1}^{[l+1]}}{\partial b_{1}^{[l]}},\
\ldots,
\frac{\partial J_{1}^{[l+1]}}{\partial W_{i}^{[l]}},
\frac{\partial J_{1}^{[l+1]}}{\partial b_{i}^{[l]}}\right) \
\vdots \&lt;/p&gt;
&lt;p&gt;\nabla J_{k}^{[l+1]}=
\left(
\frac{\partial J_{k}^{[l+1]}}{\partial W_{1}^{[l]}},
\frac{\partial J_{k}^{[l+1]}}{\partial b_{1}^{[l]}},
\ldots,
\frac{\partial J_{k}^{[l+1]}}{\partial W_{i}^{[l]}},
\frac{\partial J_{k}^{[l+1]}}{\partial b_{i}^{[l]}}\right) \
\
\left(
\Delta W_{1}^{[l]}, \Delta b_{1}^{[l]}, \ldots, \Delta W_{i}^{[l]}, \Delta b_{i}^{[l]}\right)
\end{array}
$$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 01 | Convolution</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/01_%E5%8D%B7%E7%A7%AF/</link>
        <pubDate>Thu, 23 Dec 2021 07:46:00 -0500</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/01_%E5%8D%B7%E7%A7%AF/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1VV411478E/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;卷积&#34;&gt;卷积&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\int_{-\infin}^{+\infin} f(\tau) g(x-\tau) d\tau$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个人一边进食一边消化，进食函数 f 显示了各时刻的进食量：&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8D%B7%E7%A7%AF_%E8%BF%9B%E9%A3%9F%E5%87%BD%E6%95%B0%203-00.png width=&gt;
    
    
  

&lt;p&gt;消化函数 g 表示肚子里剩余食物的比例随时间的变化，它与吃多少无关：&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8D%B7%E7%A7%AF_%E6%B6%88%E5%8C%96%E5%87%BD%E6%95%B0%203-29.png width=&gt;
    
    
  

&lt;p&gt;比如要求一个人在下午2点，肚子里还剩多少食物，就是之前吃的每一顿饭经过了“独立”的消化后剩余的部分再求和：&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8D%B7%E7%A7%AF_%E4%B8%8B%E5%8D%882%E7%82%B9%E5%89%A9%E4%BD%99%E9%A3%9F%E7%89%A9%206-43.png width=&gt;
    
    
  

&lt;p&gt;一般情况，求t时刻的胃中还剩多少食物：x时刻吃的食物f(x)，经过了(t-x)时间，还剩下的比例是$g(t-x)$，乘以总量就是x时刻吃的食物到t时刻还剩多少$f(x)\cdot g(t-x)$，对t之前每一时刻吃的东西到了t时刻还剩多少，加起来：$\int_0^t f(x) g(t-x) dx$&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8D%B7%E7%A7%AF_t%E6%97%B6%E5%88%BB%E5%89%A9%E4%BD%99%E9%A3%9F%E7%89%A9%207-32.png width=&gt;
    
    
  

&lt;p&gt;其中两函数的自变量相加后，x就被消掉了，这是卷积的一个标志。&lt;/p&gt;
&lt;p&gt;x 与 (t-x) 在图像上的对应：&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8D%B7%E7%A7%AF%E5%9B%BE%E5%83%8F%208-18.png width=&gt;
    
    
  

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个系统，输入(f)不稳定，输出(g)稳定，就可以用卷积来求这个系统的存量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;另一种理解：之前发生的事件对当前事件的影响，每件事的影响力随时间变化是g，所以导致 t 时刻事件的发生是之前各时刻事件的影响在t时刻的和。&lt;/p&gt;
&lt;p&gt;如果影响是随距离变化的函数，那么在某位置的事件就是之前各位置在此位置产生的影响之和。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;始皇既没，余威震于殊俗。 &amp;ndash; 贾谊《过秦论》&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;图像的卷积操作&#34;&gt;图像的卷积操作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;周围像素点对当前像素点的影响&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3x3卷积核规定周围一圈像素对当前像素点的影响，5x5就是用了周围2圈。平滑卷积操作就是把当前像素值替换为与周围像素的平均值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;卷积核与图片的数学运算：$f(x,y)\star g(m,n) = \sum f(x,y) \cdot g(m-x, n-y)$。（x与m-x相加只剩m，y与n-y相加只剩n，两个维度上都是卷积）&lt;/p&gt;
&lt;p&gt;如果只考虑点(x,y) 周围相邻1个像素对它的影响，并且每个相邻像素对当前像素的影响由卷积核g规定。比如要求像素f(x-1,y-1)对f(x,y)的影响，就是它本身乘以对应的比例g。
类比吃饭的例子，f(x,y)是t时刻，f(x-1,y-1)是x时刻，x时刻吃的食物到了t时刻还剩下百分之$g(t-x)$，所以 $f(x-1,y-1)$ 对应的比例为 $g(x-(x-1), y-(y-1)) = g(1,1)$，图片像素位置f与卷积核中g的位置并不一一对应，而是要旋转180度。&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8D%B7%E7%A7%AF_%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF%2019-39.png width=&gt;
    
    
  

&lt;p&gt;g函数不等于卷积核：&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E5%8D%B7%E7%A7%AF_g%E5%87%BD%E6%95%B0%E4%B8%8D%E7%AD%89%E4%BA%8E%E5%8D%B7%E7%A7%AF%E6%A0%B8%2020-35.png width=&gt;
    
    
  

&lt;p&gt;图像的卷积操作省略了g函数的旋转，直接与卷积核相乘再相加&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;卷积层&#34;&gt;卷积层&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;卷积核是局部特征的模板&lt;/li&gt;
&lt;li&gt;不考虑某位置就将其卷积核对应位置设置成零，而要重点考虑某位置，就设置得高一些&lt;/li&gt;
&lt;li&gt;保留局部特征，得到feature map&lt;/li&gt;
&lt;li&gt;垂直边界滤波器和水平边界滤波器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://poloclub.github.io/cnn-explainer/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;卷积神经网络可视化-github&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>watch: DL - 王木头 02 | Perceptron</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/02-3/</link>
        <pubDate>Wed, 22 Dec 2021 14:39:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/02-3/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1uB4y1M7Ju/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;什么是“感知机”，它的缺陷为什么让“神经网络”陷入低潮&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;感知机&#34;&gt;感知机&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;分类工具&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;只有两个输出&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为线性二分类问题(定义域元素无穷多，值域只有两个值)提供了模板答案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个线性函数再加一个激活函数&lt;/p&gt;

  
  
  
  
    
    
    
    
     
    
    
     
    
    &lt;img src= /writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/%E7%8E%8B%E6%9C%A8%E5%A4%B4%E6%84%9F%E7%9F%A5%E6%9C%BA.png width=&gt;
    
    
  

&lt;p&gt;线性函数是对标准模型的描述，激活函数是判断输入数据是否符合标准模型&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;感知机缺陷&#34;&gt;感知机缺陷&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;异或运算无法实现&lt;/li&gt;
&lt;li&gt;异或没办法线性可分：无法只用一条线把0和1区分开，必须要画一个圈才能将0和1区分开&lt;/li&gt;
&lt;li&gt;增加感知机数量：通过中间层，合并两个相同的状态&lt;/li&gt;
&lt;li&gt;核方法升维&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Yo4y1k7yU?spm_id_from=333.999.0.0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;3-“神经网络”是什么？如何直观理解它的能力极限？它是如何无限逼近真理的？&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;神经网络&#34;&gt;神经网络&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;多层感知机&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输入层，隐藏层，输出层&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;全连接网络：每个节点都和下一层节点全部相连&lt;/li&gt;
&lt;li&gt;前馈神经网络：数据的传递方向是单向向前传播&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;普遍逼近定理：只要神经网络有隐藏层，它就可以任意逼近一个连续函数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不像感知机那样，用两侧的数据（是否）把分界线夹逼出来&lt;/p&gt;
&lt;p&gt;对于多个类别，因为无法明确的定义每一个类别(猫的定义是什么？)，所以不同类别之间的界限不是黑白分明，没有明确的是非，只能说最有可能是哪一类。激活函数采用sigmoid函数，而不是感知机的0-1阶跃函数，把是非问题转化为好坏问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;人和神经网络都用自己的认知(标准)去判断是非，神经网络把自己的结果与人的结果进行比对，调整自己的模型，使两个模型之间的差异(损失函数)最小&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>images</title>
        <link>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
