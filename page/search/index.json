[{"content":"Output An Image Use Template Supports:\nRecursive clone\n1 git clone https://github.com/zichen34/RayTracing-Cherno.git Install development system dependencies: r1-GPT5\n1 2 3 4 5 6 sudo apt update sudo apt install -y build-essential git cmake \\ libx11-dev libxcursor-dev libxi-dev libxrandr-dev libxinerama-dev \\ libgl1-mesa-dev libvulkan-dev vulkan-tools \\ libwayland-dev libxkbcommon-dev \\ libglm-dev ::: aside\nReferences {{{ GPT5 - Run app on Ubuntu }}} ::: Build Premake5 From Source Problems:\nThe lastest released binary premake-5.0.0-beta7 requires GLIBC_2.38\nCommands of installing prebuilt bin {{{ ```shell # Quick option: download a premake5 binary wget https://github.com/premake/premake-core/releases/download/v5.0.0-beta7/premake-5.0.0-beta7-linux.tar.gz tar -xzf premake-5.0.0-beta7-linux.tar.gz sudo mv premake5 /usr/local/bin/ premake5 --version ``` }}} The binary premake5 is not usable due a GLIBC mismatch:\n1 2 3 zichen@zichen-X570-AORUS-PRO-WIFI:~/Downloads$ premake5 --version premake5: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38\u0026#39; not found (required by premake5) premake5: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38\u0026#39; not found (required by premake5) Supports:\nCheck GLIBC version:\n1 2 ldd --version # glibc version lsb_release -ds # Ubuntu version Output {{{ ```shell zichen@zichen-X570-AORUS-PRO-WIFI:~/Downloads$ ldd --version ldd (Ubuntu GLIBC 2.35-0ubuntu3.11) 2.35 Copyright (C) 2022 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. zichen@zichen-X570-AORUS-PRO-WIFI:~/Downloads$ lsb_release -ds Ubuntu 22.04.5 LTS\n1 2 }}} \u0026lt;/details\u0026gt; Build premake5 from downloaded source code package r3-Github\n1 2 3 unzip premake-5.0.0-beta7-src.zip -d premak5-src/ $ cd build/gmake.unix $ make config=release ::: aside\nReferences {{{ 1. [GPT5 - Run app on Ubuntu](https://chatgpt.com/c/69128f99-ff3c-832d-88ab-fe1517ca84db) 2. [Download Premake | Premake](https://premake.github.io/download) 3. [premake-core/BUILD.txt](https://github.com/premake/premake-core/blob/master/BUILD.txt) }}} ::: Resize Image Replace Image in the Original Structure Problems:\nMaintain the same memory address pointer to the image, while substituting an new image with updated width and height. Supports:\nResize r1-\n::: aside\nReferences {{{ 1. [Rendering a Sphere Using Ray Tracing! // Ray Tracing Series - The Cherno](https://youtu.be/v9vndyfk2U8?list=PLlrATfBNZ98edc5GshdBtREv5asFW3yXl\u0026t=332) }}} ::: Render A Ball Problems:\nRay hits a ball, and return color value\nDetermine whether a Line intersects a Circle Problems:\nSupports:\nParametric equations:\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | | Ray | Circle | +======================+==============================+===============================+ | Function | $y = x$ | $(x-a)^2 + (y-b)^2 = r^2$ | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | Parametric | (Origin, Direction, Len) | | | equations in | | | | Vector form | | | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | Parametric | $x = o_x + d_x t$ | | | equations in | $y = o_y + d_y t$ | | | component form | | | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\n","date":"2025-11-08T13:06:24-05:00","permalink":"http://blog.zichen.uk/post/writenotes/vis/b-note-ray_tracing-cherno/","title":"Ray Tracing Cherno"},{"content":"Build Install yarn on Ubu22 Problems:\nBuilding browser extension requires yarn Actions:\ncorepack is included in Node.js\n1 2 zichen@zichen-X570-AORUS-PRO-WIFI:~$ node --version v22.16.0 Enable corepack\n1 corepack enable Check yarn\n1 2 3 4 5 zichen@zichen-X570-AORUS-PRO-WIFI:~$ yarn --version ! Corepack is about to download https://registry.yarnpkg.com/yarn/-/yarn-1.22.22.tgz ? Do you want to continue? [Y/n] Y 1.22.22 ::: aside\nReferences: Gemini 2.5P - Yarn Not Found: Installation Guide ::: ","date":"2025-10-10T23:36:48-04:00","permalink":"http://blog.zichen.uk/post/writenotes/lang/java/","title":"Java Misc"},{"content":"NexusGS Environment ➀ Replace environment.yml Problmes:\nNexusGS borrowed FSGS\u0026rsquo;s environment.yml for conda\nVersion of Python, PyTorch mismatched\nReplace environment.yml after git clone during building the Docker image\nSupports:\nCorrect name, python version in environment.yml\n1 2 3 4 5 6 7 8 name: nexus dependencies: - python=3.10 - pip: - torch==2.0.0 --index-url https://download.pytorch.org/whl/cu118 - torchvision==0.15.1 --index-url https://download.pytorch.org/whl/cu118 - torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118 - numpy\u0026lt;2 Copy local environment.yml to the Docker image\n1 COPY environment.yml . ::: {.notes}\nThe local environment.yml is stored with Dockerfile together. ::: ➁ Create Requirements.txt Problmes:\nConvert the environment.yml to a requirements.txt for pip.\nThen, a uv environment can be created on the host machine for debugging.\nSupports:\nrequirements.txt doesn\u0026rsquo;t include python version\nrequirements.txt doesn\u0026rsquo;t include cudatoolkit, because Pip does not manage CUDA installtion. r1-Gemini\nIn other words, pip requires CUDA 11.8 to be installed on the host system for debugging.\n::: aside\nReferences: Gemini 2.5P - Conda to Pip Requirements Conversion ::: Actions:\nI\u0026rsquo;ll figure out how to debug inside the Docker container. ➂ Dockerfile Builds Image Problmes:\nCreate a running environment for NexusGS Supports:\nCUDA version limitation\nThe system can only have one active CUDA installation at a time. Currently, CUDA 11.3 is installed, but it doesn\u0026rsquo;t support PyTorch 2.0, which is used by NexusGS.\nI don\u0026rsquo;t want to install another CUDA as it\u0026rsquo;s time consuming.\nContainerized build\nBuild a docker container that includes the specific CUDA version.\nDriver-CUDA relationship\nThe nvidia-driver on the host machine determines the highest CUDA version supported, CUDA-enabled Docker images must be compatible with this driver version.\n::: aside\nReferences: {{{ USMizuki/NexusGS }}} ::: Actions:\nCreate a Dockerfile r1-Gemini\nTODO: Migrate the source code to GitLab\nDownload source code to HDD for reading\n1 2 cd /mnt/Seagate4T/04-Projects git clone https://github.com/USMizuki/NexusGS.git Build image\n1 docker build -t nexusgs:latest /home/zichen/Projects/NexusGS Run container\n1 2 3 4 docker run -it --rm --gpus all \\ -v /path/to/your/datasets:/workspace/datasets \\ -v /mnt/Seagate4T/04-Projects/NexusGS:/workspace/outputs \\ nexusgs:latest ::: aside\nReferences: Gemini 2.5P - NexusGS: Sparse View Synthesis Project ::: ➃ Pip Build Fail Problmes:\nPip failed to build submodules/diff-gaussian-rasterization-confidence\nTraceback {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 2.889 File \u0026#34;/opt/conda/envs/nexus/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\u0026#34;, line 510, in _build_extensions_serial 2.889 self.build_extension(ext) 2.889 File \u0026#34;/opt/conda/envs/nexus/lib/python3.10/site-packages/setuptools/command/build_ext.py\u0026#34;, line 264, in build_extension 2.889 _build_ext.build_extension(self, ext) 2.889 File \u0026#34;/opt/conda/envs/nexus/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\u0026#34;, line 565, in build_extension 2.889 objects = self.compiler.compile( 2.889 File \u0026#34;/opt/conda/envs/nexus/lib/python3.10/site-packages/setuptools/_distutils/compilers/C/base.py\u0026#34;, line 655, in compile 2.889 self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts) 2.889 File \u0026#34;/opt/conda/envs/nexus/lib/python3.10/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 581, in unix_wrap_single_compile 2.889 cflags = unix_cuda_flags(cflags) 2.889 File \u0026#34;/opt/conda/envs/nexus/lib/python3.10/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 548, in unix_cuda_flags 2.889 cflags + _get_cuda_arch_flags(cflags)) 2.889 File \u0026#34;/opt/conda/envs/nexus/lib/python3.10/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 1773, in _get_cuda_arch_flags 2.889 arch_list[-1] += \u0026#39;+PTX\u0026#39; 2.889 IndexError: list index out of range 2.889 [end of output] 2.889 2.889 note: This error originates from a subprocess, and is likely not a problem with pip. 2.889 ERROR: Failed building wheel for diff_gaussian_rasterization 2.889 Running setup.py clean for diff_gaussian_rasterization 4.088 Failed to build diff_gaussian_rasterization 4.226 error: failed-wheel-build-for-install 4.226 4.226 × Failed to build installable wheels for some pyproject.toml based projects 4.226 ╰─\u0026gt; diff_gaussian_rasterization -------------------- ERROR: failed to build: failed to solve: process \u0026#34;/bin/sh -c pip install submodules/diff-gaussian-rasterization-confidence\u0026#34; did not complete successfully: exit code: 1 }}}\nSupports:\nDocker builds images using CPU, with no CUDA devices visible. Therefore, PyTorch cannot detect the GPU\u0026rsquo;s compute capability. r1-Gemini ::: aside\nReferences: Gemini 2.5P - Docker Build Error: CUDA Architecture Fix ::: Actions:\nSpecify target CUDA compute capability\nSet the TORCH_CUDA_ARCH_LIST environment variable to tell the compiler which CUDA architecture to build for.\n1 2 RUN TORCH_CUDA_ARCH_LIST=\u0026#34;7.5 8.0 8.6 9.0\u0026#34; pip install submodules/diff-gaussian-rasterization-confidence RUN TORCH_CUDA_ARCH_LIST=\u0026#34;7.5 8.0 8.6 9.0\u0026#34; pip install submodules/simple-knn ➄ Copy Dataset Failed Problmes:\nI don’t want to use the original dataset stored on my hard drive directly in the program, because I’m concerned it might be modified.\nTherefore, I prefer to copy the dataset into the Docker container instead.\nTL;DR: Copy data is unnecessary. Supports:\nUse COPY docker command\n1 2 # Copy the local \u0026#39;datasets\u0026#39; folder into the container\u0026#39;s workspace COPY ./datasets /workspace/datasets The workspace in container 1 2 3 4 5 NexusGS/ ├── Dockerfile ├── datasets/ \u0026lt;-- Your datasets go here (e.g., LLFF folder) ├── scripts/ └── ... (other project files) COPY will result in a bigger image. Symbolic link is required\nData outside of the build context, i.e. the current folder (.), is not accessible for the Docker daemon.\nCreate a symbolic link that points to actual data\n1 ln -s /mnt/Seagate4T/05-DataBank/nerf_llff_data ./LLFF ::: aside\nReferences: {{{ Gemini 2.5P - NexusGS: Sparse View Synthesis Project }}} ::: Actions:\nModify the Dockerfile\n1 COPY ./LLFF/ /workspace/datasets/LLFF Note: COPY ./LLFF /workspace/datasets/LLFF is different.\nDocker will process the symbolic link itself, instead of the content inside the LLFF folder.\nRebuild the image\n1 docker build -t nexusgs:latest . Run the container without mounting dataset\n1 2 3 docker run -it --rm --gpus all \\ -v /mnt/Seagate4T/04-Projects/NexusGS/outputs:/workspace/outputs \\ nexusgs:latest Results:\nCOPY data from a symbolic link is not allowed\n/LLFF is not found. It\u0026rsquo;s not included in the .dockerignore. Error message {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 =\u0026gt; ERROR [ 8/10] COPY ./LLFF/ /workspace/datasets/LLFF 0.0s ------ \u0026gt; [ 8/10] COPY ./LLFF/ /workspace/datasets/LLFF: ------ Dockerfile:43 -------------------- 41 | 42 | # Copy the host \u0026#39;nerf_llff_data\u0026#39; folder into the container\u0026#39;s workspace 43 | \u0026gt;\u0026gt;\u0026gt; COPY ./LLFF/ /workspace/datasets/LLFF 44 | 45 | # Install the custom submodules -------------------- ERROR: failed to build: failed to solve: failed to compute cache key: failed to calculate checksum of ref da83f08b-6e43-4168-8960-34c6cc4c07ee::i715mm83err1nsdosaei0t2tl: \u0026#34;/LLFF\u0026#34;: not found (base) zichen@zichen-X570-AORUS-PRO-WIFI:~/Projects/NexusGS$ ls -al total 16 drwxrwxr-x 3 zichen zichen 4096 Oct 2 21:31 . drwxrwxr-x 6 zichen zichen 4096 Oct 1 21:38 .. -rw-rw-r-- 1 zichen zichen 1782 Oct 2 21:31 Dockerfile drwxrwxr-x 8 zichen zichen 4096 Oct 2 13:35 .git lrwxrwxrwx 1 zichen zichen 41 Oct 2 21:27 LLFF -\u0026gt; /mnt/Seagate4T/05-DataBank/nerf_llff_data }}}\n➅ Dataset Read-Only Problmes:\nSet the volume to read-only to prevent it from being modified r1-Gemini ::: aside\nReferences: {{{ Gemini 2.5P - NexusGS: Sparse View Synthesis Project }}} ::: Supports:\nAppend :ro to the end of the volume definition, to make the mounted directory read-only inside the container\n1 -v ./datasets:/workspace/datasets:ro Actions:\nRemove the COPY command from the Dockerfile\nRebuild the image\n1 docker build -t nexusgs:latest . Run container\n1 2 3 4 docker run -it --rm --gpus all \\ -v /mnt/Seagate4T/05-DataBank/nerf_llff_data:/workspace/datasets/LLFF:ro \\ -v /mnt/Seagate4T/04-Projects/NexusGS/output:/workspace/output \\ nexusgs:latest ➆ Run LLFF fern Problmes:\nRun the example case of LLFF fern Supports:\nNexusGS requires Optical flow data: llff_flow\n1 2 3 4 5 6 7 8 ├── dataset ├── nerf_llff_data ├── fern ├── sparse ├── images ├── images_8 ├── 3_views \u0026lt;-- Copy from llff_flow ├── flow Actions:\nMount flow data for each scene\nRun container:\n1 2 3 4 5 docker run -it --rm --gpus all -v /mnt/Seagate4T/05-DataBank/nerf_llff_data:/workspace/dataset/nerf_llff_data:ro \\ -v /mnt/Seagate4T/05-DataBank/llff_flow/fern/3_views:/workspace/dataset/nerf_llff_data/fern/3_views \\ -v /mnt/Seagate4T/04-Projects/NexusGS/output:/workspace/output \\ nexusgs:latest Execute shell script\nNon-HuggingFace script: Run train.py, render.py, and metrics.py\n1 sh scripts/run_llff.sh 0 Results:\nOutput\nLog {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 root@d8ffb5435336:/workspace# sh scripts/run_llff.sh 0 [5000, 10000, 30000] Optimizing output/llff/fern/3_views Output folder: output/llff/fern/3_views [03/10 17:59:57] Reading camera 20/20 [03/10 17:59:58] 2.8834194898605348 cameras_extent [03/10 17:59:58] Loading Training Cameras [03/10 17:59:58] 3it [00:00, 5.73it/s] Loading Test Cameras [03/10 17:59:58] 3it [00:00, 198.50it/s] Loading Eval Cameras [03/10 17:59:58] 14it [00:00, 159.13it/s] /opt/conda/envs/nexus/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be requ ired to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] Number of points at initialisation : 538504 [03/10 17:59:58] Number of points at initialisation : 538504 [03/10 17:59:58] Training progress: 17%|████████▋ | 5000/30000 [01:12\u0026lt;06:02, 68.88it/s, Loss=0.0012369, Points=525819] Downloading: \u0026#34;https://download.pytorch.org/models/vgg16-397923af.pth\u0026#34; to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth 100%|███████████████████████████████████████████████████████████████████████| 528M/528M [05:42\u0026lt;00:00, 1.62MB/s] Downloading: \u0026#34;https://raw.githubusercontent.com/richzhang/PerceptualSimilarity/master/lpips/weights/v0.1/vgg.pth\u0026#34; to /root/.cache/torch/hub/checkpoints/vgg.pth 100%|███████████████████████████████████████████████████████████████████████| 7.12k/7.12k [00:00\u0026lt;00:00, 15.2MB/s] 0%| | 0.00/7.12k [00:00\u0026lt;?, ?B/s] [ITER 5000] Evaluating test: L1 0.05085379630327225 PSNR 21.436749140421547 SSIM 0.701229194800059 LPIPS 0.20546899239222208 [03/10 18:06:57] [ITER 5000] Evaluating train: L1 0.0012755183658252158 PSNR 52.29058202107747 SSIM 0.9994754989941914 LPIPS 0.0005458221421577036 [03/10 18:07:01] Training progress: 33%|█████████████████ | 10000/30000 [08:20\u0026lt;05:07, 64.95it/s, Loss=0.0008756, Points=501851] [ITER 10000] Evaluating test: L1 0.048601570228735604 PSNR 21.6707280476888 SSIM 0.7072837154070536 LPIPS 0.2020971179008484 [03/10 18:08:22] [ITER 10000] Evaluating train: L1 0.0009691654122434556 PSNR 55.08801142374674 SSIM 0.9997365872065226 LPIPS 0.00024261641374323517 [03/10 18:08:25] Training progress: 100%|███████████████████████████████████████████████████| 30000/30000 [14:04\u0026lt;00:00, 35.51it/s, Loss=0.0007041, Points=447917] [ITER 30000] Evaluating test: L1 0.04780491938193639 PSNR 21.859390894571938 SSIM 0.7095310091972351 LPIPS 0.201074277361234 [03/10 18:14:07] [ITER 30000] Evaluating train: L1 0.000792427861597389 PSNR 56.9059575398763 SSIM 0.9998162388801575 LPIPS 0.00017058776090076813 [03/10 18:14:10] [ITER 30000] Saving Gaussians [03/10 18:14:10] Training complete. [03/10 18:14:12] Looking for config file in output/llff/fern/3_views/cfg_args Config file found: output/llff/fern/3_views/cfg_args Rendering output/llff/fern/3_views Loading trained model at iteration 30000 [03/10 18:14:15] Reading camera 20/20 [03/10 18:14:15] 2.8834194898605348 cameras_extent [03/10 18:14:15] Loading Training Cameras [03/10 18:14:15] 3it [00:01, 2.91it/s] Loading Test Cameras [03/10 18:14:16] 3it [00:00, 181.28it/s] Loading Eval Cameras [03/10 18:14:16] 14it [00:00, 221.72it/s] Rendering progress: 100%|█████████████████████████████████| 3/3 [00:00\u0026lt;00:00, 7.33it/s] Rendering progress: 100%|█████████████████████████████████| 3/3 [00:00\u0026lt;00:00, 7.43it/s] Scene: output/llff/fern/3_views Method: ours_30000 Metric evaluation progress: 100%|█████████████████████████| 3/3 [00:03\u0026lt;00:00, 1.22s/it] SSIM : 0.7092038 PSNR : 21.8406734 LPIPS: 0.2013377 }}}\nCode Understand ➀ DeepWiki Problmes:\nI study code through step-by-step debugging previously. But writing a VSCode debug config file still needs some time.\nI recently noticed DeepWiki can give an detail explanation for a repo.\nSupports:\nDeepWiki 在 \u0026ldquo;Environmental Setups\u0026rdquo;r1-DW 部分解读错误， 但是分析代码文件之间的逻辑关联还是有参考价值的，可以快速了解项目结构。 ::: aside\nReferences: {{{ DeepWiki - USMizuki/NexusGS }}} ::: ➁ NotebookLM Problmes:\n我看陌生的英文文档会犯困，所以想听音频/看视频。\n我知道 NotebookLM 可以生成音频，辅助学习\nSupports:\nInsert a URL as source, only one webpage is included, not all content on the site\nIt generates Flashcards for questioning.\n::: aside\nReferences: {{{ NotebookLM }}} ::: ➂ Read Aloud Problmes:\n我阅读网页会睡着，我需要工具为我朗读网页 Supports:\nAI Text Reader: Read Long Text Aloud Online, No Sign-Up - notegpt.io Searched by webpage ai read aloud at DDG Edge browser has a built-in Read Aloud function.\nIt can jump to where I clicked. ➃ Debug Step-by-Step Problmes:\nUse VSCode to debug the code with the llff fern dataset Supports:\nHyperparameters in run_llff.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 python train.py --source_path dataset/nerf_llff_data/fern \\ --model_path output/llff/fern/3_views \\ --eval --n_views 3 \\ --save_iterations 30000 \\ --iterations 30000 \\ --densify_until_iter 30000 \\ --position_lr_max_steps 30000 \\ --dataset_type llff \\ --images images_8 \\ --split_num 4 \\ --valid_dis_threshold 1.0 \\ --drop_rate 1.0 \\ --near_n 2 \\ Actions:\nCreate a launch.json file for debugging\nPython Debugger \u0026ndash;\u0026gt; Python File with Arguments\n➄ Debug Inside Container Problmes:\nI don\u0026rsquo;t want to install CUDA on the host machine.\nHow to debugg the python program within a docker container?\nPDB? GDB? or VSCode headless?\nSupports:\nUse debugpy ::: aside\nReferences: Gemini 2.5P - Debugging Python in Docker Containers ::: Eval on DTU ➀ Ask DeepWiki Problmes:\nHow do I prepare the dataset as a DTU data_type? ::: aside\nReferences: {{{ Related Snippets Extract by DeepWiki }}} ::: ➁ Export Point Cloud Problmes:\nDataflow\n┌ │ └ ─ D ─ ─ T ─ ─ U ─ ┐ ├ ┘ ─ ▷ ┌ │ └ ─ 3 ─ ─ ─ ─ i ─ ─ m ─ ─ a ─ ─ g ─ ─ e ─ ─ s ─ ┐ ├ ┘ ─ ▷ ┌ │ └ ─ C ─ ─ o ─ ─ l ─ ─ m ─ ─ a ─ ─ p ─ ┐ ├ ┘ ─ ▷ ┌ │ └ ─ P ─ ─ o ─ ─ i ─ ─ n ─ ─ t ─ ─ s ─ ┐ ├ ┘ ─ ▷ ┌ │ └ ─ A ─ ─ d ─ ─ d ─ ─ ─ ─ t ─ ─ o ─ ─ ─ ─ O ─ ─ p ─ ─ t ─ ─ i ─ ─ m ─ ─ i ─ ─ z ─ ─ e ─ ─ r ─ ┐ ├ ┘ ─ ▷ ┌ │ └ ─ 3 ─ ─ d ─ ─ g ─ ─ s ─ ┐ ├ ┘ ─ ▷ ┌ │ └ ─ R ─ ─ e ─ ─ n ─ ─ d ─ ─ e ─ ─ r ─ ┐ ├ ┘ ─ ▷ ┌ │ └ ─ E ─ ─ v ─ ─ a ─ ─ l ─ ─ u ─ ─ a ─ ─ t ─ ─ e ─ ─ ─ ─ C ─ ─ o ─ ─ m ─ ─ p ─ ─ . ─ ─ ─ ─ \u0026amp; ─ ─ ─ ─ A ─ ─ c ─ ─ c ─ ─ . ─ ┐ │ ┘ Supports:\nColmap dataset type is determined by the existence of sparse directory\nSources: scene/__init__.py, line #53\n","date":"2025-10-01T19:41:52-04:00","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/d-test-depth_gs/","title":"Test NexusGS"},{"content":"Beamer Compare Beamer and Quarto\nSupports:\n(2025-09-12T12:02)\nBeamer 不能插入多媒体？\nBeamer 要写 Latex，源文本不易阅读， 而 Quarto 是 markdown，易于在终端内阅读\nText box\nProblems:\n(2025-09-12T13:08)\n我想给单词 加外框以强调，我不知道 Quarto 能不能做到。\n我猜 Quarto 的 Markdown 语法做不到。 Beamer 应该可以\n我还想用箭头指向外框，箭头\nReferences: {{{\nBeamer简易教学 | 4 文本盒子 - bilibili - 易木木响叮当 Searched by beamer at bilibili }}} Supports:\n","date":"2025-09-12T11:59:41-04:00","permalink":"http://blog.zichen.uk/post/writenotes/lang/latex_secs/beamer/","title":"Memo: Lang - Latex | Beamer"},{"content":"Dataframe Reference Read data from file\nReferences:\nHow to Read Text Files with Pandas? - GeeksforGeeks Searched by python dataframe read txt file in DDG Supports:\nRead a table, where columns are separated by |\n1 df = pd.read_csv(\u0026#34;Analysis_Chart_Data_1.txt\u0026#34;, sep=\u0026#34;|\u0026#34;) Index a certain column\nReferences:\npandas.DataFrame.columns - pandas 2.3.1 documentation Remove a certain row\nReferences:\nHow to Drop First Row in Pandas? - GeeksforGeeks Searched by python dataframe remove the first row in DDG Supports:\n(2025-08-11T15:38)\nUse drop() r1-GfG\n1 df.drop(index=df.index[0], axis=0, inplace=True) ","date":"2025-08-11T13:19:33-04:00","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/data_visual/","title":"Memo: Lang - Python | Data Visualize"},{"content":"Data Types Problems:\nThe composition of a data type References:\nTypedef declaration - cppreference.com Searched by typedef in c lang in DDG Notes:\n(2025-05-16T15:09)\nSupports:\nTypedef declaration -\u0026gt; Type -\u0026gt; Declarations -\u0026gt; Identifier -\u0026gt; Enumerations\nMake A Cases Set Problems:\nA lumbar has 3 bladders, how can the active infating bladder be referenced? Issues:\nEnum Use enum for case label\nReferences:\nEnumerations - cppreference.com 元宝 - C语言枚举知识点总结 Supports:\nEach enumerator is integer constant (2025-05-21T13:26)\nEnum can restrict the valid values.\nThis reminds me in Python, using a class to confine the available properties.\n好像是高天讲过\nTypedef Typedef Enum Typedef Struct 两种创建结构体的方式 Problems:\nHow to creat a struct in C language Supports:\nstruct\n1 2 3 4 5 6 7 // app.h // Defining and create a structure static struct lin_frame_02 /*Normal LIN Transmit Commands*/ { uint8_t Commands_Pump_Speed; uint8_t Commands_Lumbar_Command; } lin_frame_02; If adding static, each file included app.h will have its own struct 1 2 3 4 5 6 7 // app.c #include \u0026#34;app.h\u0026#34; struct lin_frame_02 watch_lin_frame_02; // lin_frame_02\u0026#39;s values get updated... watch_lin_frame_02 = lin_frame_02; app.c will have another lin_frame_02 in different memory area. typedef struct r2-GPT\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // app.h // Define the struct type typedef struct { uint8_t Commands_Pump_Speed; uint8_t Commands_Lumbar_Command; ... uint8_t Haptic_Mode; uint8_t Haptic_Cushion_Left; uint8_t Haptic_Cushion_Right; uint8_t Haptic_Back_Left; uint8_t Haptic_Back_Right; uint8_t Cushion_Massage; } lin_frame_02_t; // Declare the global instance (no static!) extern lin_frame_02_t lin_frame_02; 1 2 3 4 // app.c #include \u0026#34;app.h\u0026#34; lin_frame_02_t lin_frame_02; // single global definition ::: aside\nReferences {{{ 1. [C Structures - GeeksforGeeks](https://www.geeksforgeeks.org/c/structures-c/) Searched by `c language struct define` at [DDG](https://duckduckgo.com/?q=c+language+struct+define\u0026ia=web) 2. [GPT5 - Accessing lin_frame_02 issue](https://chatgpt.com/c/69177e7f-660c-8327-852e-6deae882cf07) }}} ::: Stack Memory 压栈\nSupports:\n(2025-07-02T15:26)\nRL78 的内存划分\nstatic 有固定的地址，它们不会在程序运行时被销毁。 局部变量占用的内存在栈区，程序执行完毕后，它们在栈中的空间会被清除（出栈）， 所以局部变量在函数每次执行时，都会在栈中重新分配地址，所以地址不一样。\n当要调用 func1 的时候，需要把它所需使用的资源放入 stack\n压栈顺序：\n返回地址（即 func1 的下一个函数的地址），\n传入参数（如果是 primitive 变量直接把值放入栈中， 如果是数组，就把数组第一字节的地址放入栈中）\n返回值（只是留出空间？因为还没算出结果？）\n局部变量只是给它们在栈里留出空间（也有的 compiler 会分配固定地址）\n栈的使用是动态变化的，因为同一时间，只能有一个函数运行 （严格来讲是只有一条指令运行，除非是双核 cpu，那就需要处理资源调度问题） 如果有中断发生，那么 ISR 的变量（返回地址，传入变量，返回值，局部变量）就需要压栈， 如果中断太多，可能就会发生 stack overflow\n被 static 修饰的局部变量被放在和全局变量一起，不在栈中，所以不会被销毁\n8 位处理器返回一个 16 位数据时，需要 2 次操作，这两步操作不能被打断，需要加锁\n栈帧\nReferences:\n2. 编译、连接、栈帧、变量的生存域 - 范懿的线上学堂 Supports:\n(2025-07-03T08:59)\n函数的标志是小括号。 难道栈这种数据结构的出现是因为有数据 Last In First Out 的需求？因为程序是线性执行的 r1-范懿 How to Define Global Var Supports:\nJackie\u0026rsquo;s practice\nDefine in demo.c\n1 uint8_t g_var; Declare in demo.h\n1 extern uint8_t g_var; Initialization in main.c\n1 2 3 #include demo.h g_var = 0; ","date":"2025-05-16T10:55:12-04:00","permalink":"http://blog.zichen.uk/post/writenotes/lang/b-note-clang/","title":"C Lang "},{"content":"Multi-View DTU Issues:\nDirectory Structure SampleSet Read Depth Map Directory Structure Problems:\nThe directory structure of the DTU dataset Supports:\nDownload page: MVS Data Set - 2014 | DTU Robot Image Data Sets\n124 scans - Rectified.zip (123 GB)\njsmind {{{ }}}\nR 1 e 2 c 4 t i s f c i a e n d s ─ ┬ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ├ ├ └ ─ ─ ─ ─ s s s s c c c c a a a a n n n n 1 2 x 1 2 3 8 9 2 i m a g e s ─ ┬ ├ ├ ├ ├ │ ├ ├ ├ ├ │ ├ ├ ├ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ r r r r r r r r r r r r r e e e e e e e e e e e e e c c c c c c c c c c c c c t t t t t t t t t t t t t _ _ _ _ _ _ _ _ _ _ _ _ _ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ? ? ? 4 1 1 1 1 1 2 2 2 2 ? ? ? 9 _ _ _ _ _ _ _ _ _ _ _ _ _ 0 1 ? 6 m 0 ? 6 m 0 ? 6 m _ _ _ _ a _ _ _ a _ _ _ a r r r r x r r r x r r r x 5 5 5 5 . 5 5 5 . 5 5 5 . 0 0 0 0 p 0 0 0 p 0 0 0 p 0 0 0 0 n 0 0 0 n 0 0 0 n 0 0 0 0 g 0 0 0 g 0 0 0 g . . . . . . . . . . p p p p p p p p p p n n n n n n n n n n g g g g g g g g g g SampleSet.zip\nSupports:\nScan1 and scan6 Actions:\nDirectory tree\njsmind {{{ }}}\nS a m p l e S e t ─ ┬ ├ ├ ├ ├ │ │ │ │ │ │ │ │ │ │ └ ─ ─ ─ ─ ─ ─ 1 C D M M R 8 V a a V e 9 P t t S a 5 R a l d . 2 s a D M m 0 e b a e p 1 t t . 4 4 s e a t b v ─ x i i a ┬ ├ ├ ├ │ │ │ │ │ ├ └ t b n l ─ ─ ─ ─ ─ ─ . c u C C O P R S t l a a l b o e u x u t l e s i c r t d i i a M n t f e o b n a t i a d n r e s s f c . a d k ─ i e p c t ┬ ├ ├ │ │ └ e s d o i ─ ─ ─ ─ d f d o c f s t e n a u t o m r l l p u ─ a ┬ ├ └ ─ ─ ─ s s s t t t l l l 0 0 0 0 0 0 1 x 6 _ _ _ t t t o o o t t t a a a l l l Read Depth Map Subsets Issues:\nmvs_training Notes:\nmvs_training\nSupports:\nThe DTU dataset is processed for MVSNet by Yaoyao\nImage size (dimension recognized by identify): 640x512\nThe focal length of the camera matrix inside the train/ folder corresponds to the camera parameters of feature maps. Focal length get scaled alongside only downsampling operations.\nflowchart LR A[\"Rectified image\n(1600x1200)\"] --\u003e B[\"Downsample\n(800x600)\"] --\u003e C[\"Crop\n(640x512)\"] --\u003e D[\"Feature map\n(160x128)\"] The principle point (c_x, c_y) get affected by both downsampling and cropping operations.\nflowchart LR A[\"Princple point\n(823.205, 619.071)\"] --\u003e B[\"Download\n(411.6, 309.54)\"] --\u003e C[\"Crop\n(331.6, 265.54)\"] --\u003e D[\"Feature map\n(82.9, 66.38)\"] Table {{{ SampleSet Decupled train SampleSet/MVS Data/Calibration/cal18$ cat pos_001.txt 2607.429996 -3.844898 1498.178098 -533936.661373 -192.076910 2862.552532 681.798177 23434.686572 -0.241605 -0.030951 0.969881 22.540121 mvs_training/dtu/Cameras$ cat 00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 2892.33 0 823.205 0 2883.18 619.071 0 0 1\n425 2.5 mvs_training/dtu/Cameras/train$ cat 00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0\nintrinsic 361.54125 0.0 82.900625 0.0 360.3975 66.383875 0.0 0.0 1.0\n425.0 2.5 }}} ┌ │ │ │ │ ├ │ │ │ │ │ │ │ │ │ │ │ │ │ ├ │ │ │ │ │ │ │ │ │ │ │ │ │ └ ─ S c ─ m c ─ m c ─ ─ a a ─ v a ─ v a ─ ─ m t ─ s t ─ s t ─ ─ p ─ _ ─ _ ─ ─ l p ─ t 0 ─ t 0 ─ ─ e o ─ r 0 ─ r 0 ─ ─ S s ─ a 0 ─ a 0 ─ ─ e _ ─ i 0 ─ i 0 ─ ─ t 0 ─ n 0 ─ n 0 ─ ─ / 0 ─ i 0 ─ i 0 ─ ─ M 1 ─ n 0 ─ n 0 ─ ─ V . ─ g 0 ─ g 0 ─ ─ S t ─ / _ ─ / _ ─ ─ x ─ d c ─ d c ─ ─ D t ─ t a ─ t a ─ ─ a ─ u m ─ u m ─ ─ t ─ / . ─ / . ─ ─ a ─ C t ─ C t ─ ─ / ─ a x ─ a x ─ ─ C ─ m t ─ m t ─ ─ a ─ e ─ e ─ ─ l ─ r ─ r ─ ─ i ─ a ─ a ─ ─ b ─ s ─ s ─ ─ r ─ $ ─ / ─ ─ a ─ ─ t ─ ─ t ─ ─ r ─ ─ i ─ ─ a ─ ─ o ─ ─ i ─ ─ n ─ ─ n ─ ─ / ─ ─ $ ─ ─ c ─ ─ ─ ─ a ─ ─ ─ ─ l ─ ─ ─ ─ 1 ─ ─ ─ ─ 8 ─ ─ ─ ─ $ ─ ─ ─ ─ ─ ─ ─ ┬ │ │ │ │ ┼ │ │ │ │ │ │ │ │ │ │ │ │ │ ┼ │ │ │ │ │ │ │ │ │ │ │ │ │ ┴ ─ 2 - - ─ e 0 - - 0 i 2 0 0 4 ─ e 0 - - 0 i 3 0 0 4 ─ ─ 6 1 0 ─ x . 0 0 . n 8 2 ─ x . 0 0 . n 6 . . 2 ─ ─ 0 9 . ─ t 9 . . 0 t 9 2 0 5 ─ t 9 . . 0 t 1 0 0 5 ─ ─ 7 2 2 ─ r 7 0 2 r 2 8 ─ r 7 0 2 r . . ─ ─ . . 4 ─ i 0 1 4 0 i . 8 1 2 ─ i 0 1 4 0 i 5 3 0 0 ─ ─ 4 0 1 ─ n 2 4 1 . n 3 3 . ─ n 2 4 1 . n 4 6 . ─ ─ 2 7 6 ─ s 6 7 6 0 s 3 . 5 ─ s 6 7 6 0 s 1 0 0 2 ─ ─ 9 6 0 ─ i 3 4 0 i 1 ─ i 3 4 0 i 2 . . ─ ─ 9 9 5 ─ c 2 5 0 c 0 8 ─ c 2 5 0 c 5 3 1 5 ─ ─ 9 1 ─ 0 9 . ─ 0 9 . 9 . ─ ─ 6 0 - ─ . - 0 8 6 ─ . - 0 0 7 0 ─ ─ 0 ─ 0 0 0 2 1 ─ 0 0 0 . 5 ─ ─ - 2 . ─ 0 . . 1 3 9 ─ 0 . . 1 0 ─ ─ 3 8 0 ─ 7 9 0 . . . ─ 7 9 0 . 6 ─ ─ . 6 3 ─ 4 9 3 0 2 0 ─ 4 9 3 0 8 6 ─ ─ 8 2 0 ─ 7 9 0 0 7 ─ 7 9 0 2 . ─ ─ 4 . 9 ─ 9 4 9 5 1 ─ 9 4 9 . 3 ─ ─ 4 5 5 ─ 8 9 5 ─ 8 9 5 9 8 ─ ─ 8 5 1 ─ 3 3 1 ─ 3 3 1 0 3 ─ ─ 9 2 ─ ─ 0 8 ─ ─ 8 5 0 ─ 0 0 0 ─ 0 0 0 6 7 ─ ─ 3 . ─ . . . ─ . . . 2 5 ─ ─ 1 2 9 ─ 2 0 9 ─ 2 0 9 5 ─ ─ 4 6 ─ 4 2 6 ─ 4 2 6 ─ ─ 9 6 9 ─ 1 8 9 ─ 1 8 9 ─ ─ 8 8 8 ─ 9 2 8 ─ 9 2 8 ─ ─ . 1 8 ─ 3 2 8 ─ 3 2 8 ─ ─ 1 . 1 ─ 9 3 1 ─ 9 3 1 ─ ─ 7 7 ─ 4 ─ 4 ─ ─ 8 9 2 ─ - 2 ─ - 2 ─ ─ 0 8 2 ─ 1 3 2 ─ 1 3 2 ─ ─ 9 1 . ─ 9 . . ─ 9 . . ─ ─ 8 7 5 ─ 1 2 5 ─ 1 2 5 ─ ─ 7 4 ─ . 8 4 ─ . 8 4 ─ ─ - 0 ─ 0 8 0 ─ 0 8 0 ─ ─ 5 2 1 ─ 2 3 1 ─ 2 3 1 ─ ─ 3 3 2 ─ 2 ─ 2 ─ ─ 3 4 1 ─ ─ ─ ─ 9 3 ─ ─ ─ ─ 3 4 ─ ─ ─ ─ 6 . ─ ─ ─ ─ . 6 ─ ─ ─ ─ 6 8 ─ ─ ─ ─ 6 6 ─ ─ ─ ─ 1 5 ─ ─ ─ ─ 3 7 ─ ─ ─ ─ 7 2 ─ ─ ─ ─ 3 ─ ─ ─ ─ ─ ─ ─ ┐ │ │ │ │ ┤ │ │ │ │ │ │ │ │ │ │ │ │ │ ┤ │ │ │ │ │ │ │ │ │ │ │ │ │ ┘ Actions:\nm v s _ t r a i n i n g ─ d t u ─ ┬ │ │ │ │ │ │ │ │ ├ │ │ │ │ │ │ │ │ └ ─ ─ ─ C D R a e e m p c e t t r h i a ─ f s ┬ │ │ │ │ │ │ └ i ─ ─ ─ e ┬ ├ ├ ├ └ s ( s d ─ ─ ─ ─ ─ c 1 c ─ 0 0 0 p t a 6 a ┬ │ │ │ │ │ │ │ │ │ │ │ └ 0 0 0 a r n 0 n ─ ─ 0 0 0 i a 1 1 s 3 ( s 0 0 0 r i _ x 2 c 4 6 c 0 0 0 . n t 8 a 3 4 a 0 0 0 t ─ r 1 _ n 0 n 0 0 6 x ┬ ├ ├ └ a 2 t 1 i 1 0 x 3 t ─ ─ ─ ─ i 8 r _ m x 2 _ _ _ 0 0 0 0 n ) a t a 8 c c c 0 0 0 0 ─ i r g 5 _ a a a 0 0 0 0 ┬ ├ ├ │ ├ ├ └ n a e 1 t m m m 0 0 0 0 ─ ─ ─ ─ ─ ─ i s 2 r . . . 0 0 0 0 d . d d . d n ) a t t t 0 0 0 0 e . e e . e ─ i x x x 0 0 0 4 p . p p . p ┬ ├ ├ ├ │ ├ ├ ├ │ ├ ├ └ n t t t 0 1 x 8 t . t t . t ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ _ _ _ _ h h h h r r r r r r r r r r c c c c _ _ _ _ e e e e e e e e e e a a a a m m v v c c c c c c c c c c m m m m a a i i t t t t t t t t t t . . . . p p s s _ _ _ _ _ _ _ _ _ _ t t t t _ _ u u 0 0 0 0 0 0 0 0 0 0 x x x x 0 0 a a 0 0 0 0 0 0 0 4 4 4 t t t t 0 0 l l 1 1 1 1 2 2 2 9 9 9 0 4 _ _ _ _ _ _ _ _ _ _ _ _ 0 8 0 0 0 1 x 6 0 x 6 0 x 6 . . 0 0 _ _ _ _ _ _ _ _ _ _ p p 0 4 r r r r r r r r r r f f 0 8 5 5 5 5 5 5 5 5 5 5 m m . . 0 0 0 0 0 0 0 0 0 0 p p 0 0 0 0 0 0 0 0 0 0 n n 0 0 0 0 0 0 0 0 0 0 g g . . . . . . . . . . p p p p p p p p p p n n n n n n n n n n g g g g g g g g g g Convert DTU to PCN Problems:\nReorgnize the DTU dataset to the format of the PCN dataset References:\nGemini 2.5P | DTU to PCN Dataset Conversion (May08,25) Compl. \u0026amp; Acc. (Python) Problems:\nThe official evaluation for DTU dataset metrics (completeness and accuracy) is performed using MATLAB scripts. However, I don\u0026rsquo;t have a Matlab license.\nI remember there are alternative methods for measurement, which can be implemented in Python.\nSupports:\nPython Implementation r1-Fast ::: aside\nReferences: {{{ Gwencong/Fast-DTU-Evaluation Searched by DTU dataset completeness accuracy compute at DDG Gemini 2.5P - PyTorch CUDA Version Mismatch Error SO - ImportError: cannot import name \u0026lsquo;packaging\u0026rsquo; from \u0026lsquo;pkg_resources\u0026rsquo; when \u0026hellip; Searched by .venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py\u0026quot;, line 25, in \u0026lt;module\u0026gt; from pkg_resources import packagin at DDG unable to build from source - `cannot import name \u0026lsquo;packaging\u0026rsquo; from \u0026lsquo;pkg \u0026hellip; | GitHub issues ImportError: cannot import name \u0026lsquo;packaging\u0026rsquo; from \u0026lsquo;pkg_resources\u0026rsquo;\u0026rsquo; - CSDN博客 Searched by ImportError: cannot import name 'packaging' from 'pkg_resources' at DDG }}} ::: Actions:\nDownload source code\n1 2 git clone https://github.com/Gwencong/Fast-DTU-Evaluation.git cd Fast-DTU-Evaluation Specify dependencies versions in requirements.txt\n1 2 3 4 5 6 7 --extra-index-url https://download.pytorch.org/whl/cu113 torch==1.12.1+cu113 torchvision==0.13.1+cu113 # torchaudio==0.12.1 # setuptools\u0026gt;=59.6.0,\u0026lt;70.0.0 setuptools==69.5.1 numpy\u0026lt;2 Match torch version with the system CUDA version (11.3) r2-Gemini\ntorch 1.10 doesn\u0026rsquo;t support Python higher than 3.10 r2-Gemini\nsetuptools==69.5.1 avoids error: r3-SO,r4-Issue,r5-CSDN\n1 ImportError: cannot import name \u0026#39;packaging\u0026#39; from \u0026#39;pkg_resources\u0026#39; Traceback {{{ 1 2 3 4 5 6 7 (Fast-DTU-Evaluation) zichen@zichen-X570-AORUS-PRO-WIFI:~/Projects/Fast-DTU-Evaluation$ python chamfer3D/setup.py install Traceback (most recent call last): File \u0026#34;/home/zichen/Projects/Fast-DTU-Evaluation/chamfer3D/setup.py\u0026#34;, line 2, in \u0026lt;module\u0026gt; from torch.utils.cpp_extension import BuildExtension, CUDAExtension File \u0026#34;/home/zichen/Projects/Fast-DTU-Evaluation/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 25, in \u0026lt;module\u0026gt; from pkg_resources import packaging # type: ignore[attr-defined] ImportError: cannot import name \u0026#39;packaging\u0026#39; from \u0026#39;pkg_resources\u0026#39; (/home/zichen/Projects/Fast-DTU-Evaluation/.venv/lib/python3.10/site-packages/pkg_resources/init.py) }}}\nsetuptools \u0026lt; 70.0.0 is not in the \u0026ldquo;PyTorch package index\u0026rdquo;, which is looked through by uv. So adding unsafe-best-match to allow uv find the compatible version from all souce.\nCreate environment\n1 2 3 4 5 6 7 8 9 10 11 12 uv venv -p python3.10 source .venv/bin/activate # Enable PyPI repository for setuptools uv pip install -r requirements.txt --index-strategy unsafe-best-match # Specify compiler versions for CUDA export CC=/usr/bin/gcc-9 export CXX=/usr/bin/g++-9 export CUDAHOSTCXX=/usr/bin/g++-9 cd chamfer3D \u0026amp;\u0026amp; python setup.py install # build and install chamfer3D package CUDA 11.3 requires g++ \u0026lt;= 10.0.0 r6-\nExample error {{{ 1 RuntimeError: The current installed version of x86_64-linux-gnu-g++ (11.4.0) is greater than the maximum required version by CUDA 11.3 (10.0.0). Please make sure to use an adequate version of x86_64-linux-gnu-g++ (\u0026gt;=5.0.0, \u0026lt;=10.0.0). }}}\nupdate-alternative --config g++ alone for specifying g++-9 doesn\u0026rsquo;t work (still detect g++-10 (10.5.0)); set CXX explicitly.\nRun evaluation with directories of predicted and ground-truth point clouds specified:\n1 2 3 CUDA_VISIBLE_DEVICES=0 python eval_dtu.py --method scan \\ --pred_dir \u0026#34;/mnt/Seagate4T/04-Projects/CasMVSNet_pl-comments/output/no_densify_250929_2views_iter30K_640x512_noDepthReg/combined_ply\u0026#34; \\ --gt_dir \u0026#34;/mnt/Seagate4T/05-DataBank/SampleSet/MVS Data\u0026#34; --save --num_workers 1 --method constitutes the .ply filename. Results are saved as result.txt under --pred_dir. Eval Framework Understand Problems:\nThe Matlab code understanding Supports:\nAI explanation r1- ::: aside\nReferences: {{{ DTU Evaluation Framework | caoPhoenix/CasMVSNet | DeepWiki Searched by DTU dataset completeness accuracy compute at DDG DTU Dataset Evaluation | hbb1/2d-gaussian-splatting | DeepWiki }}} ::: Real Estate 10K Tanks and Temples References:\nTanks and Temples Benchmark | Homepage Novel View Synthesis Static Issues:\nNeRF MipNeRF360 Notes:\nNeRF\nSupports:\nMeta info:\n8 Feed-forward scenes Dynamic Issues:\nD-NeRF Notes:\nD-NeRF: Point Cloud ShapeNet PCN Issues:\nDirectory Structure Read As PyTorch Datasets O3D Visualization Point Cloud References:\nGemini Deep Research | PCN 点云数据集介绍 元宝 | Python类结构概览方法 元宝 | 笔记转写 Visualize point cloud | Open3D 0.19.0 documentation Notes:\nDirectory Structure\nSupports:\nPoint Completion Network Actions:\n(2025-04-27T22:37)\nPCN dataset directory structure (~/Downloads/Datasets_Unpack/ShapeNetCompletion)\n1 2 3 4 5 6 7 8 9 10 11 ~/Downloads/Datasets_Unpack/ShapeNetCompletion$ tree -L 2 . ├── test │ ├── complete │ └── partial ├── train │ ├── complete │ └── partial └── val ├── complete └── partial In the partial folder, each \u0026ldquo;complete\u0026rdquo; point cloud is split into 8 portions.\n(2025-04-28T22:30)\nRead the PCN Dataset as a PyTorch Dataset\nSupports:\nSouce code: PointAttN/dataset.py Actions:\nCall graph of the Dataset PCN_pcdr2-元宝, r3-元宝\n(2025-05-10T13:49)\nVisualize Point Clouds using O3D\nSupports:\no3d read .pcd datar4-Docs:\n1 2 3 4 5 6 7 8 9 10 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/zichen/Downloads/Datasets_Unpack/ShapeNetCompletion/train/complete/02691156/10155655850468db78d106ce0a280f87.pcd\u0026#34;) vis = o3d.visualization.Visualizer() vis.create_window(width=400, height=400) ctr = vis.get_view_control() param = o3d.io.read_pinhole_camera_parameters(\u0026#34;PCN_Cam_Parm_save.json\u0026#34;) vis.add_geometry(pcd) ctr.convert_from_pinhole_camera_parameters(param) vis.run() Actions:\n3D Models Objverse ","date":"2025-04-26T13:04:30-04:00","image":"https://roboimagedata.compute.dtu.dk/wp-content/uploads/2014/06/all_images_cropped1.png","permalink":"http://blog.zichen.uk/post/writenotes/vis/b-note-popular_cv_datasets/","title":"Memo: Vis - 3D | Popular Computer Vision Datasets"},{"content":"Upsampling DL-based PU Net Problems:\nUpsampling task: Increase density and Increase details of a point cloud.\n3DGS needs to increase density in some cases.\nIn the application of stream transmission, 3DGS needs to be compressed. That means reducing the number of Gaussians before transmission, and restoring density after receiving the low-resolution 3DGS model. References:\nPU-Net: Point Cloud Upsampling Network 【官方】2025小迈步之使用 AI 求解偏微分方程：探索 PINN 和 NO 的应用 -bilibili - MATLAB中国 【GAMES Webinar-175期:Bijective Projection in a Shell-Zhongshi Jiang-几何处理专题 - GAMES-Webinar This video was played automatically after the newest episode in the list loop mode. Addressing:\n(2025-04-15T01:01:07)\nPatch-wise feature extraction\nSupports:\nContext information Loss function encourages points on underlying surface\nSupports:\nThis paper doesn\u0026rsquo;t explicitly reconstruct surface. In contrast, MLS does compute a surface with polynomials.\nSplats rendering doesn\u0026rsquo;t require a mesh, but the underlying surface is useful for further process. For example, the surface can serve as a reference when the resolution of a set of 3D Gaussians is required to be adaptive to present different LOD.\nTherefore, determining the surface is an important task.\n(2025-04-17T06:34)\nSplats 在应用中可能确实不需要 mesh 参与，但是 mesh 可以作为处理过程中的中间态（类比于“潜空间”）， 比如构建出 mesh 之后，可以通过做 remesh 来实现先从稀疏到稠密，再从稠密到稀疏的映射r3-Jiang（这让我想起了神经网络的维度变化：先升维再降维）， 从而实现上采样。\n不过，这种上采样方式还是依赖 mesh 的质量。\n(2025-04-15T23:51:43)\nPINN embeds physics information into network through loss functionsr2-小迈步\nAnother way is using neural operator Actions:\nRepresentation GaussianPU Problems:\nI suspected the gaps between Gaussians will be revealed when zooming in. References:\nGaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting Refered by DeepSeek on Apr 05,25 Flow Matching EF References:\nEfficient Point Clouds Upsampling via Flow Matching Refered by 超绝发小论文新思路：点云！ - 小小的文章 - 知乎 (Apr 15, 25) Searched by GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting in DDG Registration SDFReg References:\nSDFReg: Learning Signed Distance Functions for Point Cloud Registration Refered by r1-1 Completion DL (2025-04-14T14:32:48)\nThese four papers are for point cloud completion tasks. PointAttN Problems:\n“摆脱对 KNN 的依赖” References:\n秘塔 | 今天学点啥 元宝 | 笔记整理 Notes:\nWhat are their innovations?\nReasons:\nFiguring out their novelty to compare it with mine method. Actions:\nexisting methods use explicit local region partitions like kNNs, which makes them sensitive to density distribution and limits receptive fields.\nI do used kNN. Reviewer 2 also said \u0026ldquo;sensitivity\u0026rdquo; (2025-04-30T01:03)\nHow do they generate new points?\nSupports:\nA point cloud learn itself through a downsampled version r1-秘塔, r2-元宝 (2025-04-30T23:38)\nNew points are Seeds, which are feature vectors Line #206 DMF-Net References:\nDMF-Net Refered by Transformer+点云！新思路结合！ - 梅花三弄的文章 - 知乎 Found in s1 PINN Problems:\n(2025-04-15T19:17:00)\nI believe a point cloud is a physical field, which is supposed to be approximated/regressed by a PINN.\nSpecifically, the observation (image or sampled points) are training data, from which the physics field is integrated into a neural network.\nAddressing:\n(2025-04-16T18:33:00)\nPINN: 把物理先验知识加入神经网络的训练中r2-小迈步\nSupports:\nNERF 也是pinn 吧，其中的物理先验是渲染方程。渲染方程是光传输的近似，所以也可以直接应用光传输或者计算成像的物理方程作为损失函数 Projection EAR Problems:\nEnhance edges in a point cloud References:\nEdge-Aware Point Set Resampling - VCC 秘塔-今天学点啥 | Edge-Aware Point Set Resampling Notes:\n(2025-05-03T23:21)\n利用平滑区域的法向量指导填充边缘的法向量r2-秘塔\nSupports:\nEAR 依据平滑区域的法向量渐进式 Progressively 推测尖锐区域的法向量\nSteps:\nEnsure the confidence of normal vectors of the flat areas: Denoising + 重投影\nCompleting edges: Progressively upsampling by expanding flat arear\nflowchart LR A[\"EAR\"] --\u003e Goal[\"尖锐区域的法向量\"] A --\u003e Base[\"利用平滑区域\"] -- \"逐步填充\n双侧投影(新点)\" --\u003e Goal Denoise[\"双边滤波\n去除噪声\n测试不同 level 噪声\"] Refine[\"各向异性重投影\n异构投影\"] subgraph Prep direction BT Denoise --\u003e Refine end Refine --\u003e Base Usages of normal vectors\nEAR：法向量 -\u0026gt; \u0026ldquo;检测\u0026rdquo;/突出边缘 -\u0026gt; 投影填充，增强边缘\n可微渲染 (3DGS)：像素在多视图中的一致性 -\u0026gt; 点的空间位置 -\u0026gt; 几何 -\u0026gt; 采样 -\u0026gt; 渲染\n辐射度缩放？\n这也许也说明了：辐射度与几何之间存在关联，可以利用辐射度推断几何 Meso-Skeleton D-Points References:\nDeep Points Consolidation - VCC 元宝 | 计算机图形学笔记整理 Notes:\n(2025-05-04T02:07)\n以“中尺度骨架”作为全局信息r2-元宝\nSupports:\nD-point 是把 Meso-skeleton 的点与外层点用 直线段相连，那我是否可以用 多项式曲线 连接一个内点和一个外点呢？\nD-Point 的叙事也是：结合基础层次的 全局信息和外延的局部信息\nD-Point 可以保留 动物的翅膀，这个任务 不常见。 以及对细微物体的重建也是一个值得探索的课题。\nRepulsion 的目的是鼓励形成曲面？\n⋯ ⋯ 对点云做搜索（retrieveal）也很重要啊！\n比如一个场景被扫描成点云之后（点云不包含语义信息），我想搜索：“房间里有几把椅子？”。\n所以这个问题是：如何对点云做物体识别？（PointNet 是做什么的？）\n直接对点云做操作处理，计算量太大，而这种“深点”可以找到点云的拓扑吗？Meso-skeleton 是点云的拓扑吗？\n应该不是，因为输入点云的形状不一定完整，提取出来的拓扑结构不是物体本身的拓扑。\n不过把点云的拓扑结构作为神经网络的训练数据，应该比较高效\nDenoising References:\nNoiseTrans: Point Cloud Denoising with Transformers Refered by 梅花三弄 ","date":"2025-04-15T00:26:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/points/c-symp-ptcld_refinement/","title":"Sympo: Points | Point Cloud Refinement"},{"content":"Pointmap Regress DUSt3R Problems:\nDense Unconstrained Stereo References:\nAddressing:\n(2025-04-15T13:33:17)\nCast pair-wise to pointmap regression Multi Depth Map Comparison Dust3R VS MVSNet Problems:\nCompare the point clouds created by depth-map fusion (of MVSNet) and Dust3R (2025-04-17T00:58:47)\n纯粹基于数据的流体力学仿真被判无法比基于数值的方法更进一步提高精度 r2\n我想点云重建也一样，神经网络生成的点云精度不会很高，所以需要后面跟一个精调。 References:\nGAMES Webinar 前沿探索-3R重塑三维，共话重建未来 - bilibili - GAMES-Webinar 纯基于数据的学习对物理仿真被判无效-bilibili - Scott_CFD Notes:\n(2025-04-11)\nWhat is a Point map\nSupports:\nImage mapping to point mapr1\n基于由 Dust3R 生成的 Point map 的 SLAM，位姿无法被恢复的好\n人工设计的 预设的三维结构（代价体）限制了模型的能力上限\n姚遥：Completeness will be good for sure, but he suspects the percision is inferious than 倾斜摄影/测绘\nFlare: Feed-forward geometryr1-34:03 VGGTr1-41:51\nSupports:\nVGGT: Visual Geometry Grounded Transformer - GitHub ","date":"2025-04-11T07:03:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/points/c-symp-ptcld_creation/","title":"Sympo: Point Cloud Creation"},{"content":"PointAttN Debugging Issues:\nCreate Environment\nTest on PCN Dataset\nTest on C3D Dataset\nReferences:\nPrevious PyTorch Versions CUDA Toolkit 11.3 Downloads - NVIDIA Developer ModuleNotFoundError: No module named \u0026lsquo;mmcv\u0026rsquo; - CSDN博客 Searched by ModuleNotFoundError: No module named 'mmcv' in DDG 【mmcv报错】ModuleNotFoundError: No module named \u0026lsquo;mmcv.runner - CSDN博客 Searched by No module named 'mmcv.runner' in DDG No force_fp32? · Issue #550 · open-mmlab/mmengine - GitHub Searched by No module named 'mmcv.runner' force_fp32 in DDG docs/zh_cn/get_started/api_reference.md Searched by repo:open-mmlab/mmcv runner in Branch 2.x 你好，我想问一下为什么运行 python test_c3d.py -c PointAttN.yaml后，test.log里没有日志生成？ #31 Searched by logging.info( in Issues Size Mismatch for test_c3d.py #7 Searched by c3d in Issues Addressing:\n(2025-03-31)\nCreate a conda environment\nSupports:\nSystem Overview:\n1 2 3 4 5 6 zichen@zichen-X570-AORUS-PRO-WIFI:~$ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 22.04.5 LTS Release: 22.04 Codename: jammy GPU and NVIDIA Driver:\n1 2 3 4 5 6 7 8 9 10 11 12 13 zichen@zichen-X570-AORUS-PRO-WIFI:~$ nvidia-smi Tue Apr 1 01:11:13 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.120 Driver Version: 550.120 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 Ti Off | 00000000:09:00.0 Off | Off | | 0% 25C P8 34W / 450W | 729MiB / 24564MiB | 1% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ CUDA Toolkit is needed as GPU-version PyTorch was compiled by a certain CUDA Toolkit.\nInstall the run filer2-Nv\n1 2 3 4 5 6 (base) zichen@zichen-X570-AORUS-PRO-WIFI:~$ nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Sun_Mar_21_19:15:46_PDT_2021 Cuda compilation tools, release 11.3, V11.3.58 Build cuda_11.3.r11.3/compiler.29745058_0 1 2 3 4 5 # Conda\u0026#39;s pytorch pkg needs CTK runtime # conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=11.3 -c pytorch -c conda-forge # The torch 1.9 compiled with ctk 11.1 requires ctk 11.1 pre-installed on system. pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html Without CUDA Toolkit, solely installing PyTorch 1.9.0 is not enough r1-Docs along with a runtime cudatoolkit\nThere will be an error:\n1 2 3 4 5 6 7 8 9 10 11 12 Exception has occurred: ImportError /mnt/Seagate4T/anaconda3/envs/pointattn/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent File \u0026#34;/home/zichen/Projects/PointAttN/utils/train_utils.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; import torch File \u0026#34;/home/zichen/Projects/PointAttN/test_c3d.py\u0026#34;, line 8, in \u0026lt;module\u0026gt; from utils.train_utils import * ImportError: /mnt/Seagate4T/anaconda3/envs/pointattn/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent # NO CUDA Toolkit installed (pointattn) zichen@zichen-X570-AORUS-PRO-WIFI:~/Projects/PointAttN$ nvcc -V Command \u0026#39;nvcc\u0026#39; not found, but can be installed with: sudo apt install nvidia-cuda-toolkit (2025-04-07)\nAfter the CUDA Toolkit 11.3 was installed, that error persists.\nThe PyTorch installed by conda is a cpu version:\n1 2 3 4 (pointattn) zichen@zichen-X570-AORUS-PRO-WIFI:~/Projects/PointAttN$ conda list pytorch 1.9.0 py3.8_cpu_0 pytorch torchvision 0.10.0 py38_cpu pytorch The pytorch=1.9.0 is a cpu version?\npip’s package name is torch, not pytorch.\n1 2 3 Pip subprocess error: ERROR: Could not find a version that satisfies the requirement pytorch==1.9.0 (from versions: 0.1.2, 1.0.2) ERROR: No matching distribution found for pytorch==1.9.0 Conda’s pytorch and PyPI’s torch are different packages, even if the version numbers match. Conda and pip packages are built differently and may link to incompatible CUDA/cuDNN versions.\n(2025-04-08)\nmmcv is needed by mm3d_pn2\nOtherwise, there will an error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 Exception has occurred: ModuleNotFoundError No module named \u0026#39;mmcv\u0026#39; File \u0026#34;/home/zichen/Projects/PointAttN/utils/mm3d_pn2/ops/__init__.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; from mmcv.ops import (RoIAlign, SigmoidFocalLoss, get_compiler_version, File \u0026#34;/home/zichen/Projects/PointAttN/utils/mm3d_pn2/__init__.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; from .ops import (nms, RoIAlign, roi_align, get_compiler_version, get_compiling_cuda_version, File \u0026#34;/home/zichen/Projects/PointAttN/models/PointAttN.py\u0026#34;, line 10, in \u0026lt;module\u0026gt; from utils.mm3d_pn2 import furthest_point_sample, gather_points File \u0026#34;/home/zichen/Projects/PointAttN/test_c3d.py\u0026#34;, line 34, in test model_module = importlib.import_module(\u0026#39;.%s\u0026#39; % args.model_name, \u0026#39;models\u0026#39;) File \u0026#34;/home/zichen/Projects/PointAttN/test_c3d.py\u0026#34;, line 80, in \u0026lt;module\u0026gt; test() ModuleNotFoundError: No module named \u0026#39;mmcv\u0026#39; pip install mmcv will build mmcv, and install: mmcv-2.2.0 mmengine-0.10.7 opencv-python-4.11.0.86 yapf-0.43.0\npip uninstall mmcv\n(2025-04-09)\nInstall mmcv-1.x.\nSince mmcv-2.x, the mmcv.runner has been removed r4-CSDN,r6-GitHub, reminded by Cursor initially.\n@force_fp32() is used to constrain data type, but it can be replaced by autocast of PyTorch r5-GitHub.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Exception has occurred: ModuleNotFoundError No module named \u0026#39;mmcv.runner\u0026#39; File \u0026#34;/home/zichen/Projects/PointAttN/utils/mm3d_pn2/ops/furthest_point_sample/points_sampler.py\u0026#34;, line 3, in \u0026lt;module\u0026gt; from mmcv.runner import force_fp32 File \u0026#34;/home/zichen/Projects/PointAttN/utils/mm3d_pn2/ops/furthest_point_sample/__init__.py\u0026#34;, line 3, in \u0026lt;module\u0026gt; from .points_sampler import Points_Sampler File \u0026#34;/home/zichen/Projects/PointAttN/utils/mm3d_pn2/ops/__init__.py\u0026#34;, line 6, in \u0026lt;module\u0026gt; from .furthest_point_sample import (Points_Sampler, furthest_point_sample, File \u0026#34;/home/zichen/Projects/PointAttN/utils/mm3d_pn2/__init__.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; from .ops import (nms, RoIAlign, roi_align, get_compiler_version, get_compiling_cuda_version, File \u0026#34;/home/zichen/Projects/PointAttN/models/PointAttN.py\u0026#34;, line 10, in \u0026lt;module\u0026gt; from utils.mm3d_pn2 import furthest_point_sample, gather_points File \u0026#34;/home/zichen/Projects/PointAttN/test_c3d.py\u0026#34;, line 34, in test model_module = importlib.import_module(\u0026#39;.%s\u0026#39; % args.model_name, \u0026#39;models\u0026#39;) File \u0026#34;/home/zichen/Projects/PointAttN/test_c3d.py\u0026#34;, line 80, in \u0026lt;module\u0026gt; test() ModuleNotFoundError: No module named \u0026#39;mmcv.runner\u0026#39; Actions:\nRemove pytorch from requiremnts.txt as it\u0026rsquo;s a cpu version.\nAsk Windsurf to generate an environment.yaml for conda\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 name: pointattn channels: - pytorch - conda-forge - defaults dependencies: - python=3.8 # - pytorch=1.9.0 # - torchvision==0.10.0 # - torchaudio==0.9.0 # - cudatoolkit=11.3 - pip - h5py=3.6.0 - matplotlib=3.4.3 - munch=2.5.0 - pyyaml=5.4.1 - pip: - torch==1.9.0+cu111 - torchvision==0.10.0+cu111 - -f https://download.pytorch.org/whl/torch_stable.html - transforms3d - tensorpack - open3d==0.13.0 - openmim - mmcv-full==1.7.2 # Additional dependencies for building the 3rd-party modules - setuptools - wheel - numpy Create env:\n1 conda env create -f environment.yml Specify gcc for nvcc (CUDA Toolkit 11.3), which supports gcc-10 tops, during compiling modules:\n1 2 3 4 5 6 export MAX_GCC_VERSION=10 sudo apt install gcc-$MAX_GCC_VERSION g++-$MAX_GCC_VERSION sudo ln -s /usr/bin/gcc-$MAX_GCC_VERSION /usr/local/cuda/bin/gcc sudo ln -s /usr/bin/g++-$MAX_GCC_VERSION /usr/local/cuda/bin/g++ The default gcc-11.4 results in an error when nvcc compiles chamfer3D:\n1 /usr/local/cuda-11.3/include/crt/host_config.h:139:2: error: #error -- unsupported GNU version! gcc versions later than 10 are not supported! Build 3rd-party modules:\n1 2 3 4 5 cd utils/ChamferDistancePytorch/chamfer3D python setup.py install cd utils/mm3d_pn2 python setup.py build_ext --inplace (2025-04-02)\nTest on the PCN Dataset\nSupports:\nPCN(processed data) -\u0026gt; ShapeNetCompletion (10.6 GB) - Infinite Gateway (GRNet) -\u0026gt; OneDrive Actions:\nCreate a launch.json\n\u0026ldquo;Debug\u0026rdquo; -\u0026gt; \u0026ldquo;Python Debugger\u0026rdquo; -\u0026gt; \u0026ldquo;Python File with Arguments Debug the currently active Python file with arguments\u0026rdquo;\nEdit the path to dataset in PointAttN.yaml:\nSelect Interpreter with the correct environment: pointattn.\nActions:\n(2025-04-28T09:14)\ntest.log is emptyr7-Issue. Test on the Completion3D Dataset\nSupports:\nBrowser has no response after clicking the URL to 2048K points of Completion3D: Stanford 3D Point Cloud Completion Benchmark\nI guess its size exceed the limit of browser.\nUse wget to download dataset2019.zip (1.5GB):\n1 2 3 4 5 6 7 8 9 10 11 zichen@zichen-X570-AORUS-PRO-WIFI:/mnt/Seagate4T/05-DataBank/PointAttN$ wget http://download.cs.stanford.edu/downloads/completion3d/dataset2019.zip --2025-04-02 23:52:45-- http://download.cs.stanford.edu/downloads/completion3d/dataset2019.zip Resolving download.cs.stanford.edu (download.cs.stanford.edu)... 171.64.64.22 Connecting to download.cs.stanford.edu (download.cs.stanford.edu)|171.64.64.22|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 1585860897 (1.5G) [application/zip] Saving to: ‘dataset2019.zip’ dataset2019.zip 100%[======================================\u0026gt;] 1.48G 5.14MB/s in 4m 59s 2025-04-02 23:57:45 (5.05 MB/s) - ‘dataset2019.zip’ saved [1585860897/1585860897] Actions:\nRun python3 test_c3d.py -c PointAttN_test_c3d.yaml\nErrorr8-Issue:\nCode Read Problems:\nHow does the forward Notes:\n(2025-04-11)\nModel Breakdown\nSupports:\nA neural network model contains 4 parts: Dataset, Network definition, Loss, Optimizer ","date":"2025-03-31T22:12:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/points/d-test-ptcld_completion_methods/","title":"Test: PtCld | Various Approaches for Point Cloud Completion"},{"content":"Env Manager Anaconda3 官方文档\n去官网下载安装包\n赋予执行权限，然后执行脚本\n1 2 chomd +x Anaconda3-2020.11-Linux-x86_64.sh bash Anaconda3-2020.11-Linux-x86_64.sh 指定安装位置\n默认是：/home/jack/anaconda3 我指定到了/home/jack/Programs/anaconda3\n安装完成后会询问是否要运行conda init，输入yes。 这会将命令行工具添加到系统的PATH环境变量中。默认每次启动shell会进入(base)环境，如果不想默认进入环境，设置：\n1 conda config --set auto_activate_base false 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 Preparing transaction: done Executing transaction: done installation finished. Do you wish the installer to initialize Anaconda3 by running conda init? [yes|no] [no] \u0026gt;\u0026gt;\u0026gt; yes no change /home/jack/Programs/anaconda3/condabin/conda no change /home/jack/Programs/anaconda3/bin/conda no change /home/jack/Programs/anaconda3/bin/conda-env no change /home/jack/Programs/anaconda3/bin/activate no change /home/jack/Programs/anaconda3/bin/deactivate no change /home/jack/Programs/anaconda3/etc/profile.d/conda.sh no change /home/jack/Programs/anaconda3/etc/fish/conf.d/conda.fish no change /home/jack/Programs/anaconda3/shell/condabin/Conda.psm1 no change /home/jack/Programs/anaconda3/shell/condabin/conda-hook.ps1 no change /home/jack/Programs/anaconda3/lib/python3.8/site-packages/xontrib/conda.xsh no change /home/jack/Programs/anaconda3/etc/profile.d/conda.csh modified /home/jack/.bashrc ==\u0026gt; For changes to take effect, close and re-open your current shell. \u0026lt;== If you\u0026#39;d prefer that conda\u0026#39;s base environment not be activated on startup, set the auto_activate_base parameter to false: conda config --set auto_activate_base false Thank you for installing Anaconda3! =========================================================================== Working with Python and Jupyter notebooks is a breeze with PyCharm Pro, designed to be used with Anaconda. Download now and have the best data tools at your fingertips. PyCharm Pro for Anaconda is available at: https://www.anaconda.com/pycharm 卸载 Anaconda\n1 rm -rf ~/anaconda3 ~/.condarc ~/.conda ~/continum 打开~/.bashrc , 并且从环境变量中移除 Anaconda 内容\n打开 jupyter notebook\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #进入环境 jupyter notebook \u0026amp; # 然后浏览器显示说：`Access to the file was denied` (base) jack@ThinkPad:~/Downloads$ jupyter notebook \u0026amp; [2] 39989 (base) jack@ThinkPad:~/Downloads$ [I 21:59:09.058 NotebookApp] The port 8888 is already in use, trying another port. [I 21:59:09.058 NotebookApp] The port 8889 is already in use, trying another port. [I 21:59:09.058 NotebookApp] The port 8890 is already in use, trying another port. [I 21:59:09.125 NotebookApp] JupyterLab extension loaded from /home/jack/Programs/anaconda3/lib/python3.8/site-packages/jupyterlab [I 21:59:09.125 NotebookApp] JupyterLab application directory is /home/jack/Programs/anaconda3/share/jupyter/lab [I 21:59:09.128 NotebookApp] Serving notebooks from local directory: /home/jack/Downloads [I 21:59:09.128 NotebookApp] Jupyter Notebook 6.1.4 is running at: [I 21:59:09.128 NotebookApp] http://localhost:8891/?token=a431e81cb5226516384a834ccda51fa12ed131 [I 21:59:09.128 NotebookApp] or http://127.0.0.1:8891/?token=a431e81cb5226516384a834ccda51fa12ed131 [I 21:59:09.128 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 21:59:09.173 NotebookApp] To access the notebook, open this file in a browser: file:///home/jack/.local/share/jupyter/runtime/nbserver-39989-open.html Or copy and paste one of these URLs: http://localhost:8891/?token=a431e81cb5226516384a834ccda51fa12ed131 or http://127.0.0.1:8891/?token=a431e81cb5226516384a834ccda51fa12ed131 然后我复制粘贴了其中一个链接就进去了\n1 [I 21:59:48.623 NotebookApp] 302 GET /?token=a431e81cb5226516384a834ccda51fa12ed131 (127.0.0.1) 0.91ms 查看conda版本: conda --version\n更新conda: conda update conda\n查看condas帮助: conda --help\nThe packages will take massive disk space, so a solution is specifying another location instead of /home/z/anaconda3. Tips\n(2024-04-04)\n1 2 3 conda config --add pkgs_dirs \u0026lt;Drive\u0026gt;/\u0026lt;pkg_path\u0026gt; conda env create --file environment.yml --prefix \u0026lt;Drive\u0026gt;/\u0026lt;env_path\u0026gt;/gaussian_splatting conda activate \u0026lt;Drive\u0026gt;/\u0026lt;env_path\u0026gt;/gaussian_splatting 新建虚拟环境 Docs\n1 conda create --name \u0026lt;env_name\u0026gt; \u0026lt;package_names\u0026gt; (2024-05-15) 环境名（和Git仓库名）还是小写，（之前想起过，但是忘了，好像是）因为敲命令时，大写字母不好打，还得多按 Capslock。 \u0026lt;env_name\u0026gt; 即创建的环境名。建议以英文命名，且不加空格，名称两边不加尖括号\u0026lt;\u0026gt;。\n\u0026lt;package_names\u0026gt; 即安装在环境中的包名。名称两边不加尖括号\u0026lt;\u0026gt;。 如果要安装指定的版本号，则只需要在包名后面以=和版本号的形式执行。如：\n1 conda create --name python2 python=2.7 即创建一个名为“python2”的环境，环境中安装版本为2.7的python。\n如果要在新创建的环境中创建多个包，则直接在 \u0026lt;package_names\u0026gt; 后以空格隔开，添加多个包名即可。如：\n1 conda create -n conda-test python=3.6 numpy pandas 即创建一个名为“conda-test ”的环境，环境中安装版本为3.6的python，同时也安装了numpy和pandas。–name同样可以替换为-n。\n从配置文件创建环境:\n1 conda env create -f environment.yml 切换conda环境\n1 conda activate env_name 退出虚拟环境\n1 conda deactivate 显示安装过的所有虚拟环境\n1 2 3 conda info --envs conda info -e conda env list 复制环境\n1 conda create --name new_env_name --clone copied_env_name 删除环境\n1 conda remove --name env_name --all 包管理\n1 2 3 4 5 6 7 8 conda search --full-name 查找的包名\t#精确查找包 condas search 查找的内容\t#模糊查找包 conda list\t#获取当前环境中已安装的包信息 conda install 要安装的包名\t#在当前环境中安装包 conda install --name 环境名 待安装的包名\t#指定环境安装包 conda install --name test_env django=2.0.6\t#指定在test-env环境中安装django，并指定版本为2.0.6 pip install 包名\t#conda安装不上的包，可用pip安装 安装opencv库\n1 pip install opencv-python 但是不能显示图片：\n1 cv2.imshow(\u0026#34;Input\u0026#34;, img) 会报错：\n1 2 QObject::moveToThread: Current thread (0x5618bc769b70) is not the object\u0026#39;s thread (0x5618bd757380). Cannot move to target thread (0x5618bc769b70) 不安装binary (PypiDoc SOF回答)\n1 pip install --no-binary opencv-python opencv-python 然后，还要再加两句，才能正常显示：\n1 2 cv2.waitKey(0) cv2.destroyAllWindows() Lambda Server:\n在服务器上安装完conda 之后，也把路径~/anaconda3/bin添加到了环境变量.profile，但是无法执行命令：conda activate myenv，报错：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CommandNotFoundError: Your shell has not been properly configured to use \u0026#39;conda activate\u0026#39;. To initialize your shell, run $ conda init \u0026lt;SHELL_NAME\u0026gt; Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See \u0026#39;conda init --help\u0026#39; for more information and options. IMPORTANT: You may need to close and restart your shell after running \u0026#39;conda init\u0026#39;. 并且通过SSH也无法自动补全（Why do some ssh sessions offer autocomplete and some not?）\n原来是我每次登录，bash都没有启动。我要手动启动。\n(2023-10-17) 在 script 中使用 conda 也会遇到这个问题，可以在 script 中执行 conda activate 之前刷新 conda：\n1 2 source $HOME/anaconda3/etc/profile.d/conda.sh conda activate nerf Python - Activate conda env through shell script\nenvironment.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 # run: conda env create -f environment.yml name: AIM channels: - pytorch - conda-forge dependencies: - python==3.7.13 - pytorch==1.10.0 - torchvision==0.11.0 - cudatoolkit=11.3 - pip - pip: - timm==0.5.4 Note that if set cudatoolkit==11.3, the environment cannot be solved: ResolvePackageNotFound: - cudatoolkit==11.3\nRun Jupyter notebook on server: jupyter notebook --no-browser --port=1234, and forward 1234 to local\u0026rsquo;s 1234 via: ssh -L 1234:localhost:1234 zichen@serveripxxx\nRunning Jupyter Notebook on a remote server\nJupyter notebook: The port is already in use, trying another port?\n导出环境, export a Conda environment and list its packages： conda env export \u0026gt; environment.yml Envs Path 更改 packages 安装路径，/home 空间不够了\n(2024-06-25)\n我在 3dgs 的 repo 里看到过关于安装 env 到其他路径，然后搜索： conda config --add pkgs_dirs \u0026lt;Drive\u0026gt;/\u0026lt;pkg_path\u0026gt; in DDG\nRef: Change conda default pkgs_dirs and envs dirs -SO\n移动 pkgs 文件夹到另一个 anaconda 目录下：(base) zichen@lambda-server:~$ mv anaconda3/pkgs/ /data2/zichen/anaconda/\n在 .condarc 中添加搜索路径:\n可以用命令：(base) zichen@lambda-server:~$ conda config --add pkgs_dirs /data2/zichen/anaconda/pkgs/\n或者直接编辑 .condarc：\n1 2 pkgs_dirs: - /data2/zichen/anaconda/pkgs 仍然可以像以前一样进入环境：conda activate gaussian_splatting\n或者： (base) zichen@lambda-server:~$ conda activate /home/zichen/anaconda3/envs/gaussian_splatting/\n创建环境：conda env create --file environment.yml --prefix /data2/zichen/anaconda/envs/gaussian_splatting\n更改 envs 路径：\n添加搜索路径：(base) zichen@lambda-server:~$ conda config --add envs_dirs /data2/zichen/anaconda/envs/， 然后就能识别到安装在 /data2/zichen 中的环境了。 我今天就先不移动了，我担心有的环境变量还得改。我的一个环境4-6个GB，主要以前为了做 SNN，存储的NeRF中间特征向量体积太大。 (2024/09/20)\n动机：想重装系统，把 /home/zichen 下的 anaconda 文件夹迁移到 /data (机械硬盘) 上\n如果移动整个 anaconda3 文件夹到 /data，则 conda 无法找到，报错：\n1 2 zichen@homepc:~$ conda bash: /home/zichen/anaconda3/bin/conda: No such file or directory 只移动 envs 和 pkgs 就好，他俩占主要的体积 步骤： Conda - Docs\n1 2 3 4 5 6 # 1. 只移动 anaconda3 文件夹下的 envs 和 pkgs mv /home/zichen/anaconda3/envs /data/anaconda3/ mv /home/zichen/anaconda3/pkgs /data/anaconda3/ # 2. 在 .condarc 中添加配置 vim ~/.condarc 在 ~/.condarc 中添加配置：\nMiniconda on Drive Problems:\nSince my /home directory gets wiped whenever I reinstall my system, I don’t want to reinstall Conda each time. Instead, I’m considering installing it on a separate hard drive.\nConda packages consume a lot of storage space, so I don\u0026rsquo;t want to store them on my SSD. However, I\u0026rsquo;m unsure whether separating the conda binaries (core installation) and python package (envs) advisable.\n/home can be mounted to a dedicated SSD partition excluded from formatting during reinstallation, preserving all configurations and data. Storing both conda binaries and python packages on an Hard drive disk ensures portability: I can access my personalized environments in another machine by just mounting my HDD.\nIf you install Conda’s core on the SSD but keep packages on the HDD, environments become SSD-dependent. Without the SSD, Conda (and its core paths) would be missing, rendering the HDD-stored packages unusable.\nNotes:\nInstall miniconda in $HOME and edit .condarc to specify paths to envs and pkgs. r1-DS\nReferences: {{{\nDeepSeek DeepSeek conda init and conda activate — conda 25.3.1.dev6 documentation Searched by conda init in DDG Installing Miniconda - Anaconda }}} Supports:\n(2025-04-01)\nConda Command Speed on HDD\nConda commands (e.g., conda install, conda create) and environment activation will be slower on an HDD compared to an SSD due to the HDD’s higher latency and lower read/write speeds.\nHowever, once a Python environment is loaded, runtime performance (e.g., executing scripts) is less affected.\nPartitioning SSD for Persistence\nPartition the SDD into / (root) and /home in the next reinstallation. Such that the /home part can keep persistent without being wiped in reinstallations later on.\nAnd then, no bother to reinstall the conda installed in /home.\nEven with a persistent /home, maintain backups (cloud/external drive). Partitions can still be accidentally formatted, and SSDs can fail.\nPartitioning won\u0026rsquo;t harm SSD lifespan, as the partitions are logical divisions, no mapping to fixed physical locations.\nUsing Conda Environments Across Machines\nIf I want to use my environment (could be stored on hard drive) on others machine, there are two approches: One is use conda pack create a portable archive of the env (.tar.gz), and the other one is reference the env via --prefix (e.g., conda activate --prefix /mnt/hdd/envs/my_env).\nConda installed on an HDD with Linux installation is not compatible with Windows system. In the end, only packages are reuseable. Using Conda on an External Drive\nIf conda is installed in external drive, it should be added in PATH and define a conda shell function. This step can be done with conda init r2-DS, r3-Docs\nInitialization is to provide a conda shell function that allows the Python code to interact with the shell context more intimately. It also allows a cleaner PATH manipulation and snappier responses in some conda commands.\nIf the mount point changes, the PATH need to be updated. (I don\u0026rsquo;t like this) Actions:\nInstall in $HOMEr4-Docs\n1 2 3 4 mkdir -p ~/miniconda3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 rm ~/miniconda3/miniconda.sh If the /bin hasn\u0026rsquo;t been appended to $PATH, conda is not recognized.\nTo execute conda init, first enter to the base env by executing:\n1 source ~/miniconda3/bin/activate conda init --all update $PATH on all currently available shells:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (base) zichen@zichen-X570-AORUS-PRO-WIFI:~$ conda init --all no change /home/zichen/miniconda3/condabin/conda no change /home/zichen/miniconda3/bin/conda no change /home/zichen/miniconda3/bin/conda-env no change /home/zichen/miniconda3/bin/activate no change /home/zichen/miniconda3/bin/deactivate no change /home/zichen/miniconda3/etc/profile.d/conda.sh no change /home/zichen/miniconda3/etc/fish/conf.d/conda.fish no change /home/zichen/miniconda3/shell/condabin/Conda.psm1 no change /home/zichen/miniconda3/shell/condabin/conda-hook.ps1 no change /home/zichen/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh no change /home/zichen/miniconda3/etc/profile.d/conda.csh modified /home/zichen/.bashrc modified /home/zichen/.zshrc modified /home/zichen/.config/fish/config.fish modified /home/zichen/.xonshrc modified /home/zichen/.tcshrc Edit .condarc:\n1 2 3 4 5 6 # ~/.condarc pkgs_dirs: - /mnt/Seagate4T/anaconda3/pkgs # Package cache (downloaded .tar.bz2 files) envs_dirs: - /mnt/Seagate4T/anaconda3/envs Results:\nPackages and environments will be stored in the specified directories. venv Compare venv and conda\nReferences:\n从pip到uv：一口气梳理现代Python项目管理全流程！ - 隔壁的程序员老王 Supports:\n(2025-09-05T23:16)\nvenv 只能管理 python 项目，Conda 可以管理其他语言的项目 Conda 有自己的项目管理逻辑，而 Python 官方现在使用 pyproject.toml 直接使用 pip install/uninstall，间接的依赖包不会被删除，而使用 pyproject.toml 就可以解决这个问题。 Conda 没有硬链接，所以依赖包占用体积很大，UV (pyproject.toml) 会检查重复的包 Basic Usages\nReferences:\nPython 教程：UV - 一个更快的、一体化的包管理器，用于替代 Pip 和 Venv - bilibili - 一摩尔炸鸡翅 Searched by conda 与 venv at Bilibili Supports:\n(2025-09-05T11:37)\nCreate a virtual env\n1 python venv ./venv Packages are stored in the current folder direnv Basic Usages\nReferences:\n使用direnv管理开发环境 - bilibili - 帕特里柯基 · 25-03-02 Supports:\n(2025-09-05T14:10)\n每个项目文件夹有个自己的 .envrc，可以设置进入目录后要执行的命令 Docker For Py Dev Pros Problems:\nCompare Docker and Virtual Env Supports:\n(2025-09-09T21:35)\nDocker 适合后台常驻的 App。 如果在开发阶段，每一个项目的环境不同，所以需要单独创建一个容器。\nDocker 的优点是：\n数据文件夹可以自动挂载，不必操心文件路径\n目录结构简单，运行结果不会与其他项目混淆\n(2025-09-23T12:57)\nDocker 可以解决 cuda 版本的问题。开发不同项目时，不用费心修改特定版本的 cuda binary 的软链接了，以及环境变量，compiler gcc 的版本都可以在 .env 中设定。 或者有时无法切换 host 中的版本，比如没有 sudo 权限。 r2-Disc\n可执行程序 cuda inside /usr/local 是一个软连接，指向特定版本的 cuda:\n/usr/local 1 2 3 4 5 (base) zichen@zichen-X570-AORUS-PRO-WIFI:/usr/local$ tree -L 1 . ├── bin ├── cuda -\u0026gt; /usr/local/cuda-11.3/ ├── cuda-11.3 ::: aside\nReferences {{{ 1. [Gemini 2.5P - Docker 容器存储空间分析与优化](https://g.co/gemini/share/1d271d05d23b) 2. [Discord - MrNeRF \u0026 Brush](https://discord.com/channels/1293599119989805087/1388829600427020318/1419822110053765250) }}} ::: Cons 2. 缺点 * 我每次要进入项目环境，还需要重新构建容器，python 安装包还要重新下载？ * 不同 container 之间不可复用 package，浪费存储空间，而 uv 好像可以复用 (2025-09-14T17:25) 3. 容器的体积占用 * 能否把 host 的 package 挂载到 container 里，从而复用？ Package Manager pip Update package: pip install --upgrade numpy\nList all available versions: pip install versions numpy (SO)\nUV Install UV Supports:\nInstall uv with standalone installer r1-astral ::: aside\nReferences {{{ 1. [GitHub - astral-sh/uv: An extremely fast Python package and project ...]() }}} ::: Python Dev Pipeline Problems:\nUse UV to handle the whole pipeline of python porject development Supports\nFive steps of developing a Python project r2-一切\nPython 安装： Official release, apt/yum, pyenv\n虚拟环境：venv, virtual env\n依赖管理：pip, pipenv, poetry\n工具安装：pipx\n打包发布：pip\n::: aside\nReferences: {{{\nPython 教程：UV - 一个更快的、一体化的包管理器，用于替代 Pip 和 Venv - bilibili - 一摩尔炸鸡翅 Searched by conda 与 venv at Bilibili 让uv管理Python的一切 - 隔壁的程序员老王 }}} ::: UV Manages Environment Problems:\nPython programs require situable environment to run correctly Supports:\nTwo conditions of an Python environment:\nPython version\nDependencies\nUV Install Python Problems:\nInstall a Python using UV Supports:\nList all available version of python:\n1 uv python list Install a specific Python: uv python install \u0026lt;python_version\u0026gt;\n1 uv python install cpython-3.12 UV Install Packages Problems:\nInstall a package into the current project Supports:\nInstalling a dependency creates a .venv environment (directory):\n1 uv add pydantic_ai Project dependencies are recorded in pyproject.toml Indicate a dependency that should not be included in the packaged application\n1 uv add ruff --dev --dev 通常用于限定：与代码实现相关的依赖，比如单元测试用到的 pytest 或者 mock 库 ruff 属于一个“工具”，用于检查规范，与代码实现不相关 Install tool packages into the system directory (~/.local/bin)\n1 uv tool install ruff This package is independently executable out of the current project, because it has its own environment. UV Uninstall Packages Supports:\nRemove package\n1 uv remove ruff --dev UV Run Run Script Supports:\nRun a script using a specific version of Python\n1 uv run -p 3.12 \u0026lt;script_name\u0026gt; Run a specific version of Python\n1 2 uv run -p 3.12 python uv run -p pypy python If the specified version doesn\u0026rsquo;t exist yet, UV will install it automatically: Run a script using the current envrionment\n1 uv run \u0026lt;script_name\u0026gt; [args] UV Init Supports:\nuv init converts an empty folder to a uv project:\nGenerated files {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 zichen@zichen-X570-AORUS-PRO-WIFI:~/Downloads/uv_test_tmp$ uv init Initialized project `uv-test-tmp` zichen@zichen-X570-AORUS-PRO-WIFI:~/Downloads/uv_test_tmp$ tree -L 1 -a . ├── .git ├── .gitignore ├── main.py ├── pyproject.toml ├── .python-version └── README.md 1 directory, 5 files }}}\nCreate a uv project with a specific version of python\n1 uv init -p 3.13 Python version is specified in the file .python-version UV Tree Problems:\nPrint the dependency relationships between the various packages in the current project. Supports:\nDisplay package tree:\n1 uv tree UV Tool Check Installed Tools Problems:\nPackages that installed as tools are installed into system folders, so its active domain is across project-level environments. Supports:\nList all tools\n1 uv tool list UV Packages a Project Problems:\nBuild a project to a single executable package Supports:\nEdit pyproject.toml and execute uv build\nAdd section project.scripts\n1 2 3 [project.scripts] # \u0026lt;custom_package_name\u0026gt; = \u0026#34;\u0026lt;script_without_extension\u0026gt;:\u0026lt;func_name\u0026gt;\u0026#34; ai = \u0026#34;ai:main\u0026#34; uv build packages the project to a .whl file, which can be installed through uv add or uv tool install.\n1 uv tool install dist/test-0.1.0-py3-none-any.whl Example of ai.py Script Problems:\nRun the example script cradiator/ai.py ::: aside\nReferences: {{{ 让uv管理Python的一切 - 隔壁的程序员老王 }}} ::: Supports:\nClone from gist\n1 ~/Projects$ git clone https://gist.github.com/b486a3148be3ab63ae7d0c5376fcf783.git Actions:\nInitialize the folder as a uv project\n1 ~/Projects/script-cradiator_ai$ uv init Switch environment:\n1 ~/Projects/script-cradiator_ai$ source .venv/bin/activate Install dependencies into the current environment\n1 uv add \u0026#34;pydantic-ai-slim[google]\u0026#34; Edit script\nSet the Gemini API key as an environment variable r1-Docs\n1 export GOOGLE_API_KEY=your-api-key Update script with the new version of pydantic-ai (1.1.0) r3-Claude\nRun script\n1 uv run ai_claude.py List files Run failed. The script contains bugs that still need to be fixed. ::: aside\nGoogle - Pydantic AI Gemini 2.5P - Fix Gemini API Key Error Claude ::: Notebook Jupyter Marimo Notes:\n教学工具\nReferences:\n新一代 python 编程notebook: Marimo 初探 - bilibili - 一起玩MicroBlocks Supports:\n(2025-03-22)\n在场景中学习 pythonr1-Bili Jupyter Docker Stacks Problems:\nUse notebook from browser without opening VSCode base-notebook Image Problems:\nJupyter provides various images as a pre-setup of developing specific applications r2-GitHub.\nI want to use notebook to practice the usage of some packages.\nSupports:\nRun base-notebook container r1-Docs ::: aside\nReferences {{{ 1. [Data science with JupyterLab | Docker Docs](https://docs.docker.com/guides/jupyter/) Searched by `jupyterlab docker` at [DDG](https://duckduckgo.com/?q=jupyterlab+docker\u0026ia=web) 2. [Jupyter Docker Stacks - GitHub](https://github.com/jupyter/docker-stacks) }}} ::: ","date":"2025-03-22T12:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/dev_env/","title":"Memo: Lang - Python | Dev Environments"},{"content":"References:\n(2025/02/09)\n圆锥曲线的离心率到底离了什么心？【MaExpo获奖作品】 - 漫士沉思录 DeepSeek-a7e9e Notes:\n(2025-02-22)\n离心率是顶点处，圆锥曲线的焦点偏离瞬时圆周运动的圆心的程度， 是圆锥曲线的焦半径与瞬时圆周运动的半径的比值 r1-漫士。\n(This plotly.js code is generated by DeepSeek)\n","date":"2025-02-22T18:55:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/conic_section/","title":"Memo: Geom | Conic Section"},{"content":"References:\nWaves and Optics | Online Physics Courses | Rice University Searched by wave optics course in DDG Introduction Examples Solving Kinematics with Two Steps:\nFree Body Diagram -\u0026gt; Newton\u0026rsquo;s Second Law -\u0026gt; EOM Solve: Integrating or Guessing -\u0026gt; Initial Conditions Solve Kinematics Objectives:\nReview the steps for solving a calculus-based physics problem before studying simple harmonic motion\n(2025-07-17T23:33)\n求一个运动学问题，本质是对时间积分。 初始条件是什么？初始条件是时间 t=0 的时候。 Problems:\nWhat is the trajectory of an object under a constant force?\nWhat is the position $(x_t,y_t)$ of the object in an arbitrary time?\nWhat is x, as a function of time? $x = f_1(t)$\nWhat is y, as a function of time? $y = f_2(t)$\nNotes:\nExample of firing a cannonball on a cliff\nPhysical situation illustration and Free Body Diagram:\ny ₀ 0 Y ┄ ● ┊ ┊ ┊ ┊ ┊ ┊ x ₀ X v F ₀ m . ᵧ g B . D v . ₀ v ₀ ₓ Steps:\nApply laws of physics: Differential Equations Of Motion (E.O.M.) in the both x-direction and y-direction\n$$ \\begin{array}{c|c} \\begin{aligned} ∑F_y \u0026= m a_y \\\\\\ -mg \u0026= m a_y \\\\\\ -g \u0026= a_y \\quad \\text{Cancel $m$} \\\\\\ -g \u0026= \\frac{d (\\frac{dy}{dt})}{dt} \\quad \\text{Represent $a_y$ with y,t} \\end{aligned} \u0026 \\begin{aligned} ∑F_x \u0026= m a_x \\\\\\ 0 \u0026= m a_x \\\\\\ 0 \u0026= a_x \\\\\\ 0 \u0026= \\frac{d^2 x}{dt^2} \\\\\\ \\end{aligned} \\\\\\ \\end{array} \\\\\\ $$ Newton\u0026rsquo;s second law: The sum of forces in the y-direction is the mass times the acceleration in the y.\nThere is only the gravity pulling it down after it takes off.\nTo find $y$, as a $f(t)$, the equation needs to be transformed to a form, including y and t.\nTo determine y, which is expected to be a function of t, the acceleration $a_y$ should be written as a representation containing both y and t, i.e., the secondary acceleration of y w.r.t. t.\nSolve the differential equations by integrating them\n$$ \\begin{array}{c|c} \\begin{aligned} -g \u0026= \\frac{d^2 y}{dt^2} \\\\\\ -gt + c_1 \u0026= \\frac{dy}{dt} \\\\\\ -\\frac{1}{2} gt^2 + c_1 t + c_2 \u0026= y \\end{aligned} \u0026 \\begin{aligned} 0 \u0026= \\frac{d^2 x}{dt^2} \\\\\\ c_3 \u0026 = \\frac{dx}{dt} \\\\\\ c_3 t + c_4 \u0026= x \\end{aligned} \\end{array} $$ Those constants of the integrations: $c_1,\\ c_2,\\ c_3,\\ c_4$ depend on the initial conditions.\nWhen $t = 0$, position $y = y_0$ and velocity in y $\\frac{dy}{dt} = v_{0y}$. Therefore,\n$$ c_2 = y_0 \\\\\\ c_1 = v_{0y} $$Similarly, in the x-direction, according to the condition at $t=0$, i.e., $x = x_0$ and $\\frac{dx}{dt} = v_{0x}$, there are:\n$$ c_3 = v_{0x} \\\\\\ c_4 = x_0 $$ The velocity in the y-direction is: $\\frac{dy}{dt} = -gt + v_{0y}$, which is accelerating downwards. Whereas, the velocity in the x-direction is constant: $\\frac{dx}{dt} = v_{0x}$.\nThe position of the object in y-direction is a parabola. The position of the object in x-direction is a linear function.\nTest the solution\nSolve Simple Harmonic Problems:\nSolve the equation of motion for simple harmonic motion\nOscillator Equation of Motion\nNotes:\n(2025-02-08)\nTwo properties of the point of stable equilibrium:\nNet force is 0\nThe force is pulling back\nExample of a spring connected to wall and a mass\nPhysical situation and free body diagram:\nN a t u r a l l e n g t h 0 X - x . ┊ ' ┄ ┄ ┄ ┄ 0 ┄ ┄ . ┊ ' x Apply the Two Steps:\nApply Newton\u0026rsquo;s second law to write Equation Of Motion:\n$$ -k \\cdot x(t) = ma $$ Hooke\u0026rsquo;s law: The force exerted by a spring is equal to the spring constant $k$ (in $N/m$) multiplied by the displacement from its natural length. The negative sign indicates that the force acts in the direction opposite to the displacement.\nUse some notations for convenience:\n$$ \\begin{aligned} x \u0026= x(t) \u0026 \\text{A function of time} \\\\\\ \\dot{x} \u0026= \\frac{dx}{dt} \u0026 \\text{First derivative} \\\\\\ \\ddot{x} \u0026= \\frac{d^2 x}{dt^2} \u0026 \\text{Second derivative} \\end{aligned} $$Therefore, the EOM can be written as:\n$$-kx = m \\ddot{x}$$ Solve the differential equation to find out $x$\nCannot integrate this equation w.r.t. time becuase the exact form of $x$ is unknown.\n(2025-03-13)\nComparision: Situation Constant force Varying force EOM -mg=ma -kx=ma Feature Force\u0026rsquo;s 原函数是一次函数 Force varies along with x Solving x Integrate directly w.r.t. $t$ Cannot integrate as $x(t)$\u0026rsquo;s form is unknown Guess $x(t)$\nWhich functions satisfy the condition that their second derivatives are themselves ($\\ddot x = x$)?\n$x = \\boxed{ A sin(Bt + C) }$. $\\dot{x} = B Acos(Bt + C)$, $\\ddot{x} = -B^2 \\boxed{ Asin(Bt + C) }$ (using chain rule) $x = \\boxed{ A cos(Bt + C) }$, $\\dot{x} = -B Asin(Bt + C)$, $\\ddot{x} = -B^2 \\boxed{ Acos(Bt + C) }$ $x = \\boxed{ Ae^{(Bt + C)} }$ (Exponential function) $\\dot{x} = B Ae^{(Bt + C)} $, $\\ddot{x} = B^2 \\boxed{ Ae^{(Bt + C)} }$, $x = 0$ (Constant function) I tried to look for potential functions among \u0026ldquo;elementary functions\u0026rdquo;, however, I found they are not able to cover all functions in the world.\nSince $cos(t) = sin(t+π/2)$, the 2nd derivative of $cos(t)$ equals the 2nd derivative of $sin(t+π/2)$.\n(2025-03-18)\n三角函数: sinx, cosx 的二阶导等于它的相反数， 指数函数的二阶导等于它 自身：$\\frac{d^2(e^x)}{dx^2}=e^x$\nIf x(t) = sin(t), cos(t), or $e^t$ and plug the x(t) in the EOM: $-kx(t) = m \\ddot{x(t)}$, the x(t) and $\\ddot{x(t)}$ will be cancelled, leaving the relationship between k and m.\nFor sin(t) and cos(t), the spring constant k differs from m by a factor of B^2. For e^t, k is -B^2 times m.\nB is the angular frequency = $\\sqrt{k/m}$ or $\\sqrt{-k/m}$.\nSubstitute a possible solution: $x = Asin(Bt + C)$ function into the EOM to find conditions that make the guessed $x$ one of possible solutions.\n$$ \\begin{aligned} -k Asin(Bt + C) \u0026= m \\cdot [ -AB^2 sin(Bt + C) ] \\\\ -k \\cancel{ Asin(Bt + C) } \u0026= m \\cdot [ \\cancel{-A} B^2 \\cancel{ sin(Bt + C) } ] \\\\ k \u0026= m B^2 \\end{aligned} $$ As long as $B$ is a specific value: $\\sqrt{\\frac{k}{m}}$, while A and C can be any values, the $x$ is equal to $A sin( \\sqrt{\\frac{k}{m}}t + C)$ (2025-03-19)\nThe B in $x(t) = Asin(Bt+C)$ needs to be $\\sqrt{\\frac{k}{m}}$ to make the EOM: $-kx = ma$ satisfied.\n$$ k = B^2 m \\\\\\ k = (\\sqrt{\\frac{k}{m}})^2 m $$ Mnemonics\nActions:\n(2025-07-20T13:05)\n因为 x\u0026quot;(t) 的具体形式未知，所以无法通过对 x\u0026quot;(t) 直接积分以求出 x(t)\n解为 A sin(Bt+C) 或 Acos(Bt+C) 时，B 是“背头金”，B = $\\sqrt{k/m}$\n三角函数的二阶导等于它的相反数乘以系数 B^2，而指数函数的二阶导等于它自己乘以系数 B^2\n如果 x(t)=Asin(Bt+C) 或 Acos(Bt+C)，则 x\u0026quot;(t)= -B^2 x(t)，所以 -kx(t) = mx\u0026quot;(t) -\u0026gt; k= mB^2\n如果 x(t)=A e^{Bx+C}，则 x\u0026quot;(t) = B^2 x(t)，所以 -kx(t) = mx\u0026quot;(t) -\u0026gt; -k = mB^2\n三角函数可以和 e 指数函数通过欧拉公式互相转化：e^{ix} = cosx + isinx， 但是其中涉及到 i，这是两种解的 B 相差一个 i 的原因吗？\nQuiz: Is $Asin(Bt+C)+Dcos(Bt+C)$ a possible solution to the simple harmonic oscillator equation of motion?\nAnswer: Yes\nI think there are two criteria need to be satisfied for a possible solution:\nIts secondary derivative is equal to itself multiplied by some factors, becuase this way the $x$ itself can be cancelled.\nThe conditions that derived from plugging the guessed x into EOM can be met.\nFirst, the proposed function\u0026rsquo;s second derivative could be equal to the scaled itself.\nThe first derivative:\n$\\dot{x} = AB cos(Bt + C) - DB sin(Bt + C)$\nThe second derivative:\n$$ \\begin{aligned} \\ddot{x} \u0026= -AB^2 sin(Bt + C) - DB^2 cos(Bt + C) \\\\ \u0026 = -B^2 [A sin(Bt+C) + D cos(Bt+C)] \\\\ \u0026 = -B^2 x \\end{aligned} $$ No need to be exactly the same as x The $x$ cannot become $\\ddot{x}$ by performing add or multiplication operations.\nThe $x$ can be $-\\ddot{x}$ if B=1.\n$$ \\begin{aligned} x \u0026= \\boxed{ Asin(t+C)+Dcos(t+C) } \\\\ \\ddot{x} \u0026= - ( \\boxed{ Asin(t+C)+Dcos(t+C) } ) = -x \\end{aligned} $$ So the first criterion is satisfied.\nSecond, plugging the $x(t)$ in the EOM for simple harmonic motion:\nOld notes $$ \\begin{aligned} -kx \u0026= m \\ddot{x} \\\\\\ -k [Asin(Bt+C) + Dcos(Bt+C)] \u0026= -AB^2 m sin(Bt + C) - DB^2 m cos(Bt + C) \\\\\\ (AB^2m-kA) sin(Bt+C) \u0026= (kD-DB^2m) cos(Bt+C) \\\\\\ \\end{aligned} $$There are two conditions to be satisfied to make the above equation held:\nSame amplitude:\n$$ AB^2m-kA = kD-DB^2m \\\\\\ B = \\sqrt{\\frac{k}{m}} $$ Phase requirement: When $θ = \\frac{π}{4} + nπ$, sin(θ) = cos(θ)\n有没有可能是因为 cos(θ) = sin(θ+pi/2)，就可以把 cos(θ) 合进 sin 里面呢？那样就变成了一个 sin 函数 $$ \\begin{aligned} -kx = m \\ddot x \\\\ -kx = -mB^2x \\\\ k = B^2m \\\\ B = \\sqrt{\\frac{k}{m}} \\end{aligned} $$ Oscillator Solution Four Parameters: A,B,C,T\nReferences:\nModule 1 \u0026gt; Oscillator Solution | Coursera Supports:\nA is Amplitude\nC is Phase lag: The distance between the first t\u0026rsquo; where sin(t\u0026rsquo;)=0 and the zero tick on the time axis\nTo write the EOM on each bead, think of each bead at the zero tick of the time axis.\n取决于时间轴零点的选取。相位滞后，但其实是坐标轴（零点）向前移动了 phi\nPhase lag is caused by the difference in the selection of the origin.\n(2025-07-23T07:21)\nC 决定了 时间轴零点 的选取 C 由时间零点决定，C 是相对于时间零点测量的。\nC = pi/3, 就是说波形的起点在所选取的时间零点的左边 pi/3 处。\n\u0026ldquo;Phase lag has to do with where the sinusoid sits relative to the time origin.\u0026rdquo;\n相位滞后本质上是时间的滞后：波上的同一点需要慢 C 秒才能传到零点。\n一个经过调制的波相对于无相位偏移的正弦波慢了 C 秒，\n\u0026ldquo;Phase lag is the offset in the part with a time in it.\u0026rdquo;\n(2025-07-24T23:47)\n观测到的 一段 波形与波形的起点相关，波形的 表达式（phase lag）取决于时间轴零点的选取。\n画图 3 个波：sin(t), sin(t+pi/3), sin(t+2pi/3)\nT = 2pi/B\nB is angular frequency\n$\\sqrt{k}{m}$ is natural frequency Plot of sinusoid: $Asin(Bt+C)$\nmindmap Oscillator Frequency Amplitude, Phase Lag (2025-06-17T00:47)\nDoes \u0026ldquo;phase\u0026rdquo; mean trend? no only the value at a point, but the rising and falling trend also are a factor of phase. And also the rising/falling speed included?\nt sin(-pi/3) is the original sin(0)\ncos(pi/3) is the original cos(0)\nsin(t) = cos(t+pi/2)\nB is independent on A and C\nSupports:\nFrequency is independent to amplitude and phase lag, but determined by spring constant and mass. summary Kinematics -\u0026gt; Constant force -\u0026gt; Harmonic -\u0026gt; Sin Solution\nSupports:\n(2025-08-06T07:13)\nImportant conclusions\n","date":"2025-02-07T23:57:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/waveoptics-jason-rice/","title":"Memo: Wave Optics"},{"content":"(2025-01-18)\nRelations:\nC i r c l e E c l i p s e M S E u V i l D g t e i n - v v a a R l r o u i t e a a t t a e e n d G + a e u S i s t g s r e i e n a c v n h e c t o r s ","date":"2025-01-18T17:00:00Z","image":"https://avatars.githubusercontent.com/u/105787223?v=4","permalink":"http://blog.zichen.uk/post/writenotes/calc/la-%E7%9F%A9%E9%98%B5%E5%8A%9B%E9%87%8F/","title":"Read: LA - 矩阵力量"},{"content":"Abstract:\nRestrict\nGaussian\nDepict Radioactive radiance and irradiance together\nNot SOTA, but make the impossible rendering complex scene with conherent light possible\n","date":"2025-01-17T22:35:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/invrender/b-note-genl_plt/","title":"Read: Render | Generalized PLT"},{"content":"Author: Eugene Hecht - Adelphi University\nNotes Brief History Times Things Exodus looking-glasses of the women Egypt polished copper bronze, mirror Greek Theory of Pythagoras, Plato, Aristotle Rectlinear propagation: incidence and refraction\n","date":"2025-01-14T06:49:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/waveoptics/optics-eugene/","title":"Read: Optics (5ed)"},{"content":"Pages on Archive.org:\np15 (2024-12-28)\n势：电荷/距离 绪论p15\n量纲: 幂次，体积单位相对于长度单位的量纲是 3\n","date":"2024-12-28T15:14:00Z","image":"https://img1.doubanio.com/lpic/s4617018.jpg","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/tem-maxwell/","title":"Read: TEM - Maxwell | Notes"},{"content":"(Feature image from RTK7F124FPC01000BJ - RL78/F24 (R7F124FPJ) Target Board | Renesas )\nRL78 Family References:\nRL78 Series - Low-Power 8-Bit \u0026amp; 16-Bit Microcontrollers (MCU) | Renesas Searched by RL78 F24 Flash Led in DDG RL78/F23, F24 RENESAS MCU Collected in RL78 Low Power 16-bit MCUs for Automotive | Renesas Searched in s1 Notes:\nBrief comparision among MCUs r1-MCU:\nRL78/F24 (R7F124FPJ 100-pin) block diagram shows the UART0 (RXD0 and TXD0) is inside the SAU0 r2-F24\nUART Hardware References:\nImplementing UART Tx in RL78 F14 - YouTube - SM training academy Searched by UART code demonstration for Renesas RL78/F23 in DDG video RTK7F124FPC01000BJ - RL78/F24 (R7F124FPJ) Target Board | Renesas Searched by renesas R7F124FPJ3A downloader in DDG E2 Emulator Lite RTE0T0002LKCE00000R User\u0026rsquo;s Manual Searched by Renesas E2 lite usage in DDG Searched by in DDG Environment:\nRL78 / F24 Target board 100 pin\nMCU: R7F124FPJ3A, 2335AM456 PCB: RTK7F124FPC01000BJ REV.1.0 E2 Emulator Lite: RTE0T0002LKCE00000R\nNotes:\n(2024-11-25)\nThe TTL needs to be converted to USB voltage level to achieve the communication between a PC and a UART r1-SM.\nHowever, I don\u0026rsquo;t have a convertor.\nI have an Arduino Nano, which can be connected to a USB (COM) port.\nP C ━ ━ ━ M U C i S a n B b i l e ━ ━ A ━ r ( d C u v i t n T R G r o x x N ) D R T R x x L 7 8 / F 2 4 I don\u0026rsquo;t Short circuit pins as RL78/G23 required? r2-Intro in the video r1-Show.\nI just connected the VDD pin (#20 in the connector CN1) to 3.3V, and the GND (#22 in CN1) to ground.\nSmcg Uart References:\nHow to Create a UART Communication Project Using Smart Configurator in e² studio Collected by RL78 Smart Configurator | Renesas Searched by smart configurator for RL78 F23 UART in DDG RL78 Smart Configurator User’s Guide: IAREW Collected by RL78/F24 Guide for Engineer - Renesas Electronics Corporation Searched by UART Communication component on RL78/F24 in DDG Searched by in DDG Notes:\n(2024-11-26)\nCreat a new Smcg project:\nSelect device:\nAdd UART component and specify operation:\nThe resource can be RLIN30 or RLIN31\nThe file structure of code generated by Smcg r2-Docs\nModify Code References:\nSmart Configurator User\u0026rsquo;s Manual: RL78 API Reference Listed in the Documentation: RL78 Smart Configurator | Renesas Notes:\n(2024-11-25)\nThe r_cg_serial.h lists all the functions can be used in the UART communication r1-16:07.\nThat means I may need to be familiar with the usage of each function r1-Manual.\nBasically, 2 functions are most often used:\n1 2 3 void R_UART0_Start(void); MD_STATUS R_UART0_Send(uint8_t* const tx_buf, uint16_t tx_num); A callback function r_uart0_callback_sendend(void) (defined in r_cg_serial_user.c) will be executed after the transmission completed. r1-33:48\nIAREW Build Error References:\nHow to Create a UART Communication Project Using Smart Configurator in e² studio - RenesasPresents Problems:\nI copied the code from video r1-YouTube, then clicked the Build All in IAR. The building failed.\nTwo errors that some sizes (of sections) exceed the block size OPT_BYTE and SECUR_ID\nBuild Error:\nError Messages 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Messages UART-Test-Debug Reading project nodes ... Cleaning ... 14 files. r_bsp_common_iar.asm cstartup.s Config_UARTO.c Config_UARTO_user.c r_cg_systeminit.c r_cg_sau_common.c hdwinit.c r_bsp_init.c main.c r_bsp_common.c Pin.c mcu_clocks.c vecttbl.c UART-Test.out Error[Lp004]: actual size (0x5) exceeds maximum size (0x4) for block \u0026#34;OPT_BYTE\u0026#34; Error[Lp004]: actual size (0x10) exceeds maximum size (0xa) for block \u0026#34;SECUR_ID\u0026#34; Total number of errors: 2 Total number of warnings: 0 Build failed Notes:\nE2 Studio References:\nE2 Emulator Lite RTE0T0002LKCE00000R User\u0026rsquo;s Manual Searched by Renesas E2 lite usage in DDG Notes:\n(2024-11-28)\nHardware Resources\nPins of TXD0 (Pin: P15) and RXD0 (Pin: P16) on the target board: Connector2 - pin#5 (TXD0) and pin#7 (RXD0).\nConfigure The Components\nCreate an E2 Studio project\nConfigure UART Communication: Transmission and Reception\nAdd User Code And Run The Project\nUART_F24.c: Define reception buffer and messages to be sent outside the main function as global variables\n1 2 3 4 5 6 7 8 9 10 11 12 /** Global variables and functions (to be accessed by other files) */ uint8_t g_Uart0RxBuf;\t/* UART0 receive buffer */ uint8_t g_Uart0RxErr; /* UART0 receive error status */ MD_STATUS g_Uart0TxEnd;\t/* UART0 transmission end */ /** Private global variables and functions (to be sent) */ static uint8_t messageOK[4] = {\u0026#34;OK\\r\\n\u0026#34;};\t/* Message for receiving \u0026#34;T\u0026#34; */ static uint8_t messageok[4] = {\u0026#34;ok\\r\\n\u0026#34;};\t/* Message for receiving \u0026#34;t\u0026#34; */ static uint8_t messageUC[4] = {\u0026#34;UC\\r\\n\u0026#34;};\t/* Message for receiving other characters */ static uint8_t messageFE[4] = {\u0026#34;FE\\r\\n\u0026#34;};\t/* Framing error message */ static uint8_t messagePE[4] = {\u0026#34;PE\\r\\n\u0026#34;};\t/* Parity error message */ static uint8_t messageOE[4] = {\u0026#34;OE\\r\\n\u0026#34;};\t/* Overrun error message */ Config_UART0_user.c: Customized callback functions (triggered by respective interruption)\nThe same code can be built with 0 error, but prompting liscene warning:\nAnalysis:\nMaybe IAR\u0026rsquo;s liscene has been expired. So, the program cannot be built correctly.\nToday should be the last of my evaluation liscene, as the prompt shows \u0026ldquo;will be expired in 0 days\u0026rdquo;. (2024-11-27)\nNov 27 becomes red:\nE2 studio cannot debug: Must connect to the debugger\nE2 studio cannot debug: Do not power the board when debugging with a debugger. r1-Manual\nICD Debugger (2025-01-04)\nConnect tag of Microchip ICD (In-Circuit Debugger) to 14-pin cable of E2 Lite:\n1 2 G ▽ N D T C K R V e T s R e E t F G N D R e s e t 1 1 3 4 E2 Lite Programmer 14 pin Tag Connect 6 pin Pin 2 Pin 4 – GND Pin 5 Pin 5 – Tool0 Pin 8 + 9 Pin 2 – VCC Pin 13 + 10 Pin 1 - MCU RST Reference: EE_Team \u0026gt; Hardware \u0026gt; E41 \u0026gt; E41 Programmer to ECU.docx Upload Problems:\nI don\u0026rsquo;t want to test the code by launching \u0026ldquo;Debug\u0026rdquo; every time. I want to embed the program into the MCU and let it run itself after power on.\nThe TX and RX wires from Arduino will power the RL78 target board, and then the debugging cannot be started, as the error: Emulator power is selected, but external power is being applied.\nReferences:\nCan I evaluate and develop the RL78 microcontroller on the Arduino IDE? - Renesas Electronics Searched by Renesas RL78 embed a program into MCU in DDG Serial Terminal Option in E2? - e2studio - Forum - e2studio IDE \u0026hellip; Searched by e2 studio serial monitor in DDG e2 studio Integrated Development Environment User\u0026rsquo;s Manual: Getting \u0026hellip; Found in s2 Notes:\n(2024-11-29)\nArduino IDE can develope RL78 MCU. It supports G-class MCU r1-FAQs. I am not sure how about F24.\nPlug in the TX and RX wires after the debugging process launched r1-Manual.\nE2 has a serial monitor: TM Terminal r2-Forum.\nThere is a \u0026lsquo;Download\u0026rsquo; button in the debugging tools r3-Manual, but mine is gray.\nPin Connection Problems:\nThe RL78/F24 target board are connected to Arduino Nano with TX and RX wires, The Arduino will transmit the character t to the RL78/F24. Then, the RL78/F24 is expected to return ok back.\nThe Arduino Nano code is from r2-Docs. And I have extended the RX0 and TX1 to pins D3 and D2\nReferences:\nHow to read data through UART - Raspberry Pi Forums Searched by raspberry pi uart receive and display on terminal in Google Universal Asynchronous Receiver-Transmitter (UART) - Arduino Docs Searched by Arduino sends and receives character via UART in DDG Practices:\nTry to send t to F24 by using Serial.println():\nResults\nThe Output in Serial Monitor:\n1 2 3 4 t t t t (2024-12-02)\nSerial.write() writes data to the serial port, while the Serial.print() writes data to USB (printing in terminal), reminded by the code of r1-Raspi Serial.print() will send data.\nResults:\nThe Serial Monitor output became:\n1 tttttt... The voltage on TX1 and D2 sometimes keep 4.6V for a long time, and sometimes they become 0.0 V. I didn\u0026rsquo;t understand why is that.\nTest the original TX1, RX0\nReasons:\nTry my luck. Actions:\nAfter I re-plugged the wire: from D2 + D3 to TX1 + RX0, I noticed that the history message in Serial Terminal showed \u0026ldquo;FE\u0026rdquo;.\nF24\u0026rsquo;s pin-5 \u0026gt; D3, and pin-7 \u0026gt; TX1\nResults:\nF24 gives wrong feedback. The Serial Monitor printed:\n1 2 3 4 FE UC FE UC (2024-12-05)\nRectify pin connects by separating the 2 groups of serial ports without crossing them.\nReasons:\nFigured out the correct usage of SoftwareSerial Actions:\nResults:\nGarbled characters appears Analysis:\nThe data received by F24 was problematic, and didn\u0026rsquo;t match its expectations.\nI should confirm what data are sent from the Arduino Nano.\nI can set a breakpoint in the E2 Studio to watch the data that F24 received.\nBlink LED2 Problems:\nHow to download the program to the board?\nHow to use the debugger?\nReferences:\nInterfacing LED with Renesas RL78 Microcontroller - YouTube - SM training academy Played following r1-SM RL78/F24 Target Board RTK7F124FPC01000BJ User\u0026rsquo;s Manual Rev.1.01 Smart Configurator Tutorial - Create a LED Blinking Program Configuring RL78 Timer | Renesas Searched by RL78 blink Led in DDG e² studio Quick Start Guide (3/3) - Build and Debug for RL78 - YouTube - RenesasPresents Searched by RL78 debug skill in DDG video Notes:\n(2024-11-28)\nSchematic:\nWhen the P67 is set to Low, the LED2 will be ON r1-3:38.\nWhen the P67 is pulled down, the LED will light. Only need to connect the debugger to the target board. No power supply to the target board.\n✴ T a B r o g a e r t d 1 4 b - a p n i d n E L 2 i t e ━ ━ M U i S n B i ━ ━ P U C S B Configure The Components:\nBlinking LED2 needs to configure 2 components: Ports and Timer Array Unit r3-Tutorial\nComponents Resources Function Setting Ports PORT6 On/Off LED (P67) P67: Output Interval Timer TAU0_0 Toggle LED2 every 500ms Timer: TAU0_0 Counter clock: CK00 Interrupt interval: 500ms Enable Interrupt (INTTM00) Interrupt level: Level 3 Smart Configurator r1-12:17:\nAdd component \u0026gt; Ports \u0026gt; Config_PORT \u0026gt; PORT6 \u0026gt; P67\nIf directly check the pin P67, it\u0026rsquo;ll NOT be initialized:\nSmart Configurator: Timer Array Unit\nSelect using E2 Lite for debugging\nAlso, if using CS+, don\u0026rsquo;t forget to check: Debug \u0026gt; Using Debug Tool\nClick \u0026ldquo;Generate Code\u0026rdquo;\nAdd User Code and Run Project\nConfig_TAU0_user.c: Include Pin.h and write timer interrupt behavior:\n1 2 3 #include \u0026#34;Pin.h\u0026#34; ... PIN_WRITE(LED2) = ~PIN_READ(LED2); Pin.h: Define macro for LED2\nNote, the video shows \u0026ldquo;Users guide\u0026rdquo;, which doesn\u0026rsquo;t appear in my Pin.h 1 #define LED2 6,7 (2024-12-20)\nSymbolic name can be specified in the Smart Configurator\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // src/smc_gen/r_pincfg/Pin.h /* User\u0026#39;s guide for symbolic name. * The generated symbolic names can be used in the user application as follows: * * Example: Toggle LED1 at Pin P54. * There are 2 ways to toggle LED1 * 1) Using symbolic name macro * Assuming the symbolic name for P54 is \u0026#34;LED1\u0026#34;, the generated macro definition will be: * #define LED1 5,4 * * To use this macro definition to toggle the LED1, call the symbolic name APIs: * PIN_WRITE(LED1) = ~PIN_READ(LED1) * * 2) Not using symbolic name macro * Call the symbolic name APIs directly * PIN_WRITE(5,4) = ~PIN_READ(5,4) */ /* Symbolic name */ #define MCU_RXD1 1,1 #define MCU_LIN_EN 1,0 Blink_LED.c: main function.\n1 2 3 4 5 int main(void){ R_Config_TAU0_0_Start(); EI(); while(1); } (2025-04-17T15:01:15)\nIf do not use a timer interrupt, the main() can be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026#34;r_smc_entry.h\u0026#34; void delay(long d){ while(d--); } int main(void) { while(1) { PIN_WRITE(LED1) = 0;\t// ON delay(500000); PIN_WRITE(LED1) = 1;\t// OFF delay(500000); } return 0; } Build project to generate binary file\nProject \u0026gt; Build Project Right-click on the project name \u0026gt; Debug As \u0026gt; 3 Renesas GDB Hardware Debugging\nDebug configuration (using default settings):\nClick Resume button to continue the program. The program will stop at the beginning of the main funciton.\nClick Resume again to start execution.\n(2024-12-10)\nAdd breadpoint by double-cliking and add a global to watch by dragging it to \u0026ldquo;Expressions\u0026rdquo; view r4-YouTube. SPI Problems:\n(2024-11-29)\nUse RL78/F24 to read values of the pressure sensor Bosch SMP581. References:\nELCL Slave Select Pin Function (4-wire SPI) Tutorial (1/3) - Create project for RL78/G23 Found on the page: RL78 Smart ConfiguratorSolution Toolkit when looking for solution to burn the hex code to MCU, and entering this page by accidentaly clicking. The page is opened when searching (241129) smart configurator for RL78 F23 UART in DDG RL78/G23 Handshake-based SPI Master Transmission/Reception Rev.1.01 | Renesas Searched by RL78 SPI in (241206) Renesas search page Try to search the above keywords after browsing the search results of F24 SPI. Total 6 pages don\u0026rsquo;t include SPI application note. RL78/G10 Serial Array Unit (CSI Master Communication) CC-RL | Renesas Searched by s2 Sample Code for RL78/G23 Handshake-based SPI Master Transmission/Reception Rev.1.01 Searched by R01AN5889JJ0101 in Renesas search e² studio Quick Start Guide - Install e² studio and CC-RL Compiler on Linux | Renesas Searched by e2 studio install CC-RL in DDG Searched by s2 Notes:\nELCL (Logic and Event Link Controller)\nELCL module can simplify the circuit by reducing 4 wires to 3 r1-Tutorial.\nThe video demonstration uses board: RL78 - G23 64 pin (R7F100GLGxFB), which has an option of \u0026ldquo;Download ELCL module\u0026rdquo;. Although F24 doesn\u0026rsquo;t have ELCL, it has SPI.\nELCL enables an MCU to serve as an slave device, by simulating an \u0026ldquo;slave selection\u0026rdquo; signal.\n(2024-12-06)\nSPI is a kind of CSI communication r2-G23\nTransmit/receive is analogous to the SPI.transfer16() in Arduino: Read 1 byte from the bus and at the same time send 1 byte to the bus.\nSample Code (2024-12-09)\nOpen the sample project after downloading it r4-Code\nActions:\nInstall more packages\ninstalling CC-RL may require license manager r5-Linux\nResults:\nThose packages are not needed, at least don\u0026rsquo;t help to open the sample project.\nThose packages do not include the required CC-RL compiler.\nUse IAR, where the company has valid license manager.\nBTW, CS+ can build the project:\nConfig the SPI for F24 based on the sample .smcg file\nReasons:\nThe .smcg file in the sample code can be opened with Smart Configuartor. The LEDs are used as indicators, the code is not controlling LEDs. See ch 1.5 for objectives description.\nG23 Schematic\nThe data_length is 1 byte (8 bits). And the array uint8_t is containing the uint8_t type. So, 1 cell stores 1 data.\nThe function: R_Config_CSI00_Send_Receive() just sends 1 char, and receives 1 char.\n1 2 // Lin #305 R_Config_CSI00_Send_Receive(\u0026amp;g_tx_data, data_length, \u0026amp;g_rx_data); The interrupt: INTCSI00 ?\nThe line #284: g_num = g_rx_data_stored[0] \u0026amp; 0x3F;\nThe cnt-1 in the Line #252: g_rx_data_stored[cnt-1] = g_rx_data\nThe (0x80 \u0026gt; g_rx_data_stored[0]) in line #255\nSMP581 (2024-12-08)\nCreate Project\nM S O P O S 1 0 5 S I 5 0 C M N P 2 5 M P S 8 I 1 I 7 1 S 6 0 O 0 S h P S u S 1 C 9 t C 7 K t K 0 l 0 e b o a r d N P N C C 6 C 3 N S 6 S 0 1 Configure The Components\nActions:\nComponents\nComponents Resources Function Setting SPI CSI00 SCK00, SI00, SO00 Ports PORT6 Chip selection P67: Output If checking the option: Output 1, the LED is off initially r1-Tutorial. Sensor specifics:\nThe sensor is of 16 bits.\nThe SPI_MODE1 of Arduino corresponds to the Type 2 in RL78\nAdd User Code and Run Project\nDefine the \u0026ldquo;Chip Selection\u0026rdquo; pin in src\\smc_gen\\general\\r_cg_userdefine.h:\n1 #define CS1_pin P6_bit.no6 In another way, this pin can be called by assigning a symbolic name to it, and then use PIN_WRITE():\n1 2 3 4 5 // src/smc_gen/r_pincfg/Pin.h #define MCU_Pressure_CS 6,3 // src/hw/hw_smp581.c PIN_WRITE(MCU_Pressure_CS) = 0; No response data received:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 uint16_t g_tx_data; /* Send data buffer */ uint16_t g_rx_data; /* Receive data buffer */ const uint16_t data_length = 16; const uint16_t MODE[3] = /* Command code */ { 0B0011000000000000, /* Read pressure, Keep all errors */ 0B0101000000000000, /* Read temperature, Keep all errors */ 0B1001000000000000, /* Config \u0026amp; Identi */ }; void main(void) { R_Config_CSI00_Start(); /* Enable CSI00 module */ CS1_pin =0; /* set CS1 active */ g_tx_data = MODE[0]; while(1U) { R_Config_CSI00_Send_Receive(\u0026amp;g_tx_data, data_length=16, \u0026amp;g_rx_data); } } No Received Data Problems:\n(2024-12-10)\nA few examples r1-Forum indicated that calling SPI is easy. But why I cannot receive data? References:\nRL78/G14 SPI - Forum - RL78 MCU - Renesas Engineering Community Searched by RL78 F24 SPI experiment in DDG How to perform SPI interface in Renesas? - Forum - RL78 MCU - Renesas Engineering Community Searched by RL78 F24 SPI experiment in DDG RL78/G23 ELCL Slave Select Pin Function (for 4-wire SPI) Application Note Rev.2.00 (R01AN5614EJ0200) Practices:\n(2024-12-10)\nTry to debug.\nThe values passed to the arguments are not displayed\n(2024-12-12)\nTake the Arduino code as an example\nResults:\nThe sensor works fine with Arduino Analysis:\nMISO of the SMP581 should connect to MISO of MCU: SO00 (Serial Output) of RL78/F24.\nStable power supply is necessary for the sensor to return correct values. Both 5V and 3.3V can make SMP581 work correctly.\nOtherwise, unstable voltage (from the sparkfun stick) won\u0026rsquo;t enable the SMP581 return data, only 0.\nUse RL78/F24 to provide the CS signal.\nActions:\nKeep the CS1_pin always low and connect it to SMP581. Results:\nConstant low voltage doesn\u0026rsquo;t let the sensor return data.\nIf there is only digitalWrite(ssPin, LOW); without digitalWrite(ssPin, HIGH);, no returned data:\n1 2 3 4 5 6 7 8 9 10 11 12 Requesting Temperature Value and Diagnosis... Cmd has been transfered Data received: 101000000000000 Diag bits rmvd: 0 Number of LSB: 0 Current temperature (℃): 10530.32 Requesting Pressure Value and Diagnosis... Cmd has been transfered Data received: 101000000000000 Diag bits rmvd: 0 Number of LSB: 0 Current pressure (kPa): 60.00 Analysis:\nDelay is necessary for SMP581 to return, which is also mentioned in r2-Forum Use a LED-blinking INTTM00 of RL78/F24 to control the CS, and the MISO and MOSI are connected to Arduino\nResults:\nData received by Arduino is not correct. Analysis:\nThe CS pin turning on and off should be together with the data transfer function. ✅ Use syntex turning the CS1_pin ON and OFF to surround the data transfer function:\nReasons:\nTo mimic the Arduino code. Shown in the forum post r2-Forum Actions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 void delay(long d){ while(d--); } int main(void) { EI(); R_Config_CSI00_Start(); /* Enable CSI00 module */ while(1U) { CS1_pin = 0; /* Select the sensor by pulling the pin down */ g_tx_data = MODE[0]; result = R_Config_CSI00_Send_Receive(\u0026amp;g_tx_data, data_length, \u0026amp;g_rx_data); delay(1000000); CS1_pin = 1; } return 0; } Results:\nThe g_rx_data can keep changing (21274 or 21270):\nThe converted physics values: Pressure (㎪) and temperature (℃) are similar to Arduino results.\ncommit: 7ba9e25 (C:\\Users\\ZichenWang\\e2_studio\\workspace\\F24-SPI-2024-12-09\\F24_from_G23_Sample_Code)\nAnalysis:\nTODO: Use interrupt when receiving data to read data instead of waiting for a period of time using delay function, as mentioned in r2-Forum and r3-Manual. Do Not Use Delay Problems:\nThe main function is a loop. The delay() will make the main loop wait there, and cannot perform other services.\nTherefore, a \u0026ldquo;waiting state\u0026rdquo; can be set in the state machine for a function.\nNotes:\n(2024-12-23)\nThe function is divided into 3 states by the \u0026ldquo;delay\u0026rdquo; state:\nThe original function:\n1 2 3 4 5 6 7 void bsp_spi_read_adc_smp581(uint16_t* tx_data_addr, uint8_t data_length, uint16_t* rx_data_addr) { CS1_pin = 0;\t/* Chip selection */ R_Config_CSI00_Send_Receive(tx_data_addr, data_length, rx_data_addr);\t/* The returned SPI data will be stored in rx_data*/ delay(1000000); CS1_pin = 1;\t/* Chip deselection*/ } Convert the function to a state machine:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 static uint8_t state = 0; static uint32_t start_time; void bsp_spi_read_adc_smp581(uint16_t* tx_data_addr, uint8_t data_length, uint16_t* rx_data_addr) { switch (state) { case 0: CS1_pin = 0;\t/* Chip selection */ R_Config_CSI00_Send_Receive(tx_data_addr, data_length, rx_data_addr); current_time = service_get_1_ms(); state++; break; case 1: if ((service_get_1_ms()-start_time)\u0026gt;1000) { state++; } break; case 2: CS1_pin = 1;\t/* Chip deselection*/ state = 0; break; } } Single Cmd Caused NC (2024/01/04)\nProblems:\nWhen dubugging the project: F24-SPI-2024-12-09 (8034804d), the value keeps 100.xx unchanged.\nThe sensor works fine when connected to Arduino Nano\nThe previous version before converting to layer-wise code seems to work fine.\nImage-1 shows the first pause at the breakpoint after pressure_kpa has been converted.\nImage-2 shows the second pause. The pressure can be updated promptly without getting stuck.\n(2025-01-08)\nA single main function for F24 can get the updating pressure values, but the layer-wise code get the pressure value unchanged: 100.130348 (21262). (2025-01-09)\nKeep sending the pressure-request command only will cause the sensor not response. The pressure_kpa remains 59.9989 unchanged, which is the same as the value that the Arduino code printed, without connecting to the sensor.\nI found this problem because I initially want to change the \u0026ldquo;workable version\u0026rdquo; little by little utill get to the \u0026ldquo;layer-wise code\u0026rdquo; version.\nBy doing this, I believe I can what step is problematic. The first step I tried is removing the temperature reading part. After doing that, the code appears the same problem: stucks at 59.9989.\nSending the pressure-requesting command twice doesn\u0026rsquo;t work either.\nEven using different variables doesn\u0026rsquo;t fix it.\nReferences:\nAddressing:\n1. Use UART to print values, rather watching values during debugging\n(2024-01-09)\nSending pressure-requesting and temperature-requesting commands together Value Keep Increasing Problems:\nThe pressure value read from sensor only increases and never drops, so the value after the first reading are not correct. Issues:\nCheck with Shuttleboard Correct Timing Test with SPI Interrupt Addressing:\nReproduce error by porting previous project of F24 - FPJ3A (\u0026ldquo;F24-SPI-2024-12-09\u0026rdquo;) testing the Shuttleboard.\nActions:\nSmart configurator only set 2 modules: SPI and Port (1 pin: P63)\nThen copy the main function.\nAdd a breakpoint at the first sentence in the while loop: P6_bit.no3 = 0;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #include \u0026#34;r_smc_entry.h\u0026#34; uint16_t g_tx_data; /* Send data buffer */ uint16_t g_rx_data = 0xFFFFU; /* Receive data buffer */ uint16_t pressure_code_adc; uint16_t temperature_code_adc; float pressure_kpa; float temperature_cel; const uint16_t data_length = 2; /* 2 bytes */ const uint16_t MODE[3] = /* Command code */ { 0B0011000000000000, /* Read pressure, Keep all errors */ 0B0101000000000000, /* Read temperatur, Keep all errors */ 0B1001000000000000, /* Config \u0026amp; Identi */ }; volatile MD_STATUS result = 0; void delay(long d){ while(d--); } int main (void); int main(void) { EI(); R_Config_CSI00_Start(); /* Enable CSI00 module */ while(1U) { /* Read pressure from SMP581 */ P6_bit.no3 = 0; g_tx_data = MODE[0]; result = R_Config_CSI00_Send_Receive(\u0026amp;g_tx_data, data_length, \u0026amp;g_rx_data); delay(1000000); P6_bit.no3 = 1; pressure_code_adc = g_rx_data \u0026amp; 0x0FFE;; pressure_code_adc = pressure_code_adc \u0026gt;\u0026gt; 1; pressure_kpa = (pressure_code_adc + 584.57) / 9.743; /* Read temperature from SMP581 */ P6_bit.no3 = 0; g_tx_data = MODE[1]; result = R_Config_CSI00_Send_Receive(\u0026amp;g_tx_data, data_length, \u0026amp;g_rx_data); delay(1000000); P6_bit.no3 = 1; temperature_code_adc = g_rx_data \u0026amp; 0x0FFE;; temperature_code_adc = temperature_code_adc \u0026gt;\u0026gt; 1; temperature_cel = (temperature_code_adc - 248) / 6.2; NOP(); } return 0; } Results:\nAlthough the pressure returned from the inidividual sensor on the Shuttleboard may be not accurate, the value maintains around 102 kPa. However, the values of sensor on E41 board grows gradually.\nIt maybe only record the history highest value.\nEven redownloding the code to MCU, the variable shown in the Watch windows keeps last value! (Redownloading is because I accidentally pressed F6 beside F5.)\nI also tested: Resetting the value of g_rx_data to 0 after each reading. But the g_rx_data always still keeps grow, instead of oscillating around the normal atm pressure.\nAnalysis:\nI guess this could be hardware problem. The register inside the sensor could never be reset after each reading.\nI think this is not a software issue, as the same code works correctly with the individual SMP581 sensor. I suspect the reason may be that some buffer inside the sensor may not be reset after each reading, because during my debugging, when I redownload the program, the sensor returns the value from the previous debugging session.\n(2025-02-11)\nCorrect Timing\nReasons:\nThe technical support suspected the timing is problematic. Actions:\nDetermine the timing through oscilloscope.\nAdjust the timing\n1 2 3 4 5 6 PIN_WRITE(CS_PIN) = 0; delay(50); g_tx_data = MODE[0]; result = R_Config_CSI00_Send_Receive(\u0026amp;g_tx_data, data_length, \u0026amp;g_rx_data); delay(200); PIN_WRITE(CS_PIN) = 1; I think using interrupt is a better way. (2025-02-12)\nTest the SPI interrupt function\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 // main.c uint8_t g_tx_data[2]; /* Send data buffer */ uint8_t g_rx_data[2] = 0xFFU; /* Receive data buffer */ uint16_t pressure_code_adc; float pressure_kpa; const uint16_t data_length = 2; const uint8_t TX_DATA[] = /* Command code */ { 0B00110000, 0B00000000, /* Read pressure, Keep all errors */ 0B01010000, 0B00000000, /* Read temperatur, Keep all errors */ 0B10010000, 0B00000000, /* Config \u0026amp; Identi */ }; volatile MD_STATUS result = 0; void delay(long d){ while(d--); } int main(void) { R_Config_TAU0_3_Higher8bits_Start(); /* 1ms interval timer start */ R_Config_CSI00_Start(); /* Enable CSI00 module */ while(1U) { /* Wait for 1 ms */ DI(); while (TMIF03H == 0U) { HALT(); } TMIF03H = 0U; /* Issue Chip Select Signal */ P6_bit.no3 = 0; /* Read pressure from SMP581 */ EI();\t// Enable lower interval 16 us? result = R_Config_CSI00_Send_Receive(\u0026amp;TX_DATA, data_length, \u0026amp;g_rx_data); HALT(); delay(200); // I found this delay is necessary P6_bit.no3 = 1; pressure_code_adc = ((g_rx_data[0] \u0026amp; 0x07) \u0026lt;\u0026lt; 7) | ((g_rx_data[1] \u0026gt;\u0026gt; 1) \u0026amp; 0x7F); pressure_kpa = (pressure_code_adc + 584.57) / 9.743; NOP(); } return 0; } Parse Diagnostic Bits Problems:\nImplement the diagnostic function for the pressure sensor SMP581 Addressing:\n(2025/03/31)\nAsk DeepSeek to generate code based on provided information\nReasons:\nI don\u0026rsquo;t use windsurf as I need to remove the words: SMP581 from the source code. The datasheet of SMP581 is not public. Actions:\nFeed the related information in documentation along with the current service\n6.3.2.2 Definition of the Bit Stream Coming from the sensor\nSlave Response Variant1 16 bits: Diagnostic bits (bit 15 – 11): These 5 bits transmit the diagnostic information of the sensor and reflect possible failures of the sensor. Data bits (bit 10 – 1): These 10 data bits contain the value (pressure / temperature) requested by the master. Parity (bit 0): The slave response is safeguarded by an odd parity over bits 15 to 1. Even (including 0) will lead to a 1, odd will lead to a 0.\n6.3.2.3 Diagnostic Bits\nIn 16 bit SPI mode, the sensor transmits diagnostic information via diagnosis bits Diag0\u0026hellip;4 (bits 11\u0026hellip;15) when answering master requests for pressure/diagnosis or temperature/diagnosis. The sensor recognizes errors and transmits diagnostic codes given in the table below. In case of multiple errors, the sensor sends the failure with the highest priority. In case of an error, the sensor continues to send pressure and temperature values.\nDiagnosis code\nFailure Detection Priority (1= highest) D15 D14 D13 D12 D11 Sensor memory error OTP: At start up, RAM: Continuous 1 1 0 0 0 0 Acquisition chain failure At start up 2 0 1 0 0 0 Pressure sensing element failure* At start up/ Continuous 3 0 0 1 0 0 ADC’s upper limit Continuous 4 0 0 0 1 0 ADC’s lower limit Continuous 5 0 0 0 0 1 No error Continuous 6 0 1 0 1 0 Pressure sensing element failure - bond failure of pressure signal monitored only at Startup. Bond failure of sensing element supply and temperature-diode monitored continuously. In addition to the diagnosis codes above, the sensor recognizes communication errors. Communication errors are: • The usage of an invalid command\n• Communication cycles with the number of clock cycles during NCS low (active) being not equal to a multiple (including 0) of 16.\nThe sensor then responds with \u0026ldquo;0b0000 0000 0000 0001\u0026rdquo; in the next communication cycle.\nLIN Blink LED Problems:\nI need to implement LIN communication. Usually, in a LIN network, there is a single Master and multiple slaves.\nHowever, I currently have only one RL78/F24 board. Also, I don\u0026rsquo;t have Babylin or other emulators to simulate the host computer.\nSo, I plan to test the communication between PC and the target board via the UART and LIN protocol base on documents r2-SIS.\nIf it\u0026rsquo;s possible to let the F24 be a master r1-Master, sending frames, which ask the UART to transmit something?\nF24 has 2 LIN channels! which be separately set as a master and a slave.\nBut how to write code for different channels on a single MCU?\nHow to call RLIN api?\n(2024-12-03)\nSimple project: Master channel send a commend to the slave channel to perform the behavior: blinking the LED. Create E2 Studio Project References:\nRL78/F13, F14 Group LIN Slave Mode (RLIN3) Rev.1.02 Shared by Joseph I think he found it by searching F13 RLIN in Renesas Search page RL78/F2x RLIN3 Module Software Integration System Rev.1.00 Searched by F23 RLIN in Renesas Searche page How to Create a LIN Project with Smart Configurator on e² studio - YouTube - RenesasPresents Chapter 5 - How to Build LIN Application | RLIN3 Module Software Integration System Smart Configurator Notes:\n(2024-12-04)\nCreate an e2 studio project r3-YouTube:\nActions:\nFile \u0026gt; New \u0026gt; Renesas C/C++ Project \u0026gt; Renesas RL78\n\u0026ldquo;Renesas CC-RL C/C++ Executable Project\u0026rdquo;\nProject Name: F24_Two_LIN_Channels_Blinking_LED\nTarget board: RL78 - F24 100pin \u0026gt; R7F124FPJ3xFB;\nCreate Hardware Debug Configuration: E2 Lite (RL78)\nUse Smart Configurator\nClocks \u0026gt; Base frequency: 40 MHz \u0026gt; High-speed on-chip oscillator: 40 MHz \u0026gt; Check LIN0 clock and LIN1 clock\nComponents \u0026gt; r_bsp \u0026gt; Enable: StartClock, Set~, Change~,\nAdd Components \u0026gt; rlin3\nr_riln3 \u0026gt; RLIN3 channel0 setting \u0026gt;\nAdd Components \u0026gt; Interval Timer \u0026gt; 16 bit count mode (Resource: TAU0_1) \u0026gt;\nOperation clock: CK00; Clock source: fCLK/2^8; Interval value: 30 ms\nThe master requires schedule implemented based on a timer r2-RLIN3 p39. Add Components \u0026gt; Ports \u0026gt; PORT6\nP66 \u0026gt; Out \u0026gt; Output 1\nAdd Components \u0026gt; Interval Timer \u0026gt; 16 bit count mode \u0026gt; TAU0_0\nOperation clock: CK00; Clock source: fCLK/2^8; Interval value: 400ms Interrupt setting check: End of timer channel 0 count, generate an interrupt (INTTM00)\nA change pops when generating code:\nCopy the Smcg generated lib: r_rlin3_lib to under the src/.\nLIN Configurator Actions:\nUse LIN Configurator for RL78/F23_F24 to setup LDF:\nSet Baud rate for both channels to 19200\nSet an unconditional frame for the Master: channel0\nSet schedule for the Master channel: Transmit these two frames repeatedly\nSet frames for Slave: Channel1, Publishing responses\nBoth frame published by the slave required an Response_Error_Signal\nSave LIN Configurator project beside the src/\nGenerate the driver source code into src/r_riln3_lib/\nCS+ Builds Library Actions:\nUse CS+ to compile the driver source files:\nOpen the generated .mtpj project file. Each channel (Mater \u0026amp; Slave) has their own project:\nBuild the CS+ project separately:\nLibrary file liblin21s_CCRL_0.lib is generated r4-Ch5.\nE2 Studio Compiler Actions:\nGo back the e2 Studio to configure the compiler: Include the LIN library.\nOpen Building properties, encountering an error: No property pages\nMaybe e2 studio should keep open, while I close it before.\nI generated the code again:\nSmcg generates code, and copy r_rlin3_lib to /src; LIN Configurator generates code into src/r_rlin3_lib CS+ builds project. Then, the e2 studio\u0026rsquo;s File \u0026gt; Properties can be opened.\nAdd files: Relocatable files, object files, and library file setting\nExclude the \u0026ldquo;Driver source code\u0026rdquo; generated by the LIN Configurator from building process.\nAdd source files: drv/conf/ to Compiler\nThe File \u0026gt; Properties dosen\u0026rsquo;t appear again.\nAfter I canceled the excluding (un-check the \u0026ldquo;HardwareDebug\u0026rdquo; option), the File \u0026gt; Properties still doesn\u0026rsquo;t show up.\nI re-generate the codes, but this time the properties missed many options:\nAfter clicking the Refresh, the properties are restored, but the Linker disappeared:\nI closed the E2 studio, and reopen it, the File \u0026gt; Properties can show up, although the \u0026ldquo;No property pages\u0026rdquo; pops again at first. I have to select the project name! As I gradually Unfold the project level-by-level, the available properties are loaded gradually.\nAfter I unfold the project folder, those properties disappeared again: It seems that I have to Select the .smcg file to load settings.\n(Note: here, I have excluded those driver source code.)\nAdd Macro Definition: __LIN_CH0_P1__\nMemory Small model\nRe-Build Project:\nUse e2 Studio to configure the compiler for slave (Channel1)\nFold the Project and Make the project name selected. Or just Right click on the project name.\nFile \u0026gt; Properties \u0026gt; C/C++ Build: Settings \u0026gt; Linker \u0026gt; Input \u0026gt; Add liblin21s_CCRL_1.lib (built by CS+)\nAdd source dir: Settings \u0026gt; Compiler \u0026gt; Source \u0026gt; Add src/r_rlin3_lib/r_lin_drv/Channel1/conf/\nAdd Macro definition: __LIN_CH1_P1_ r2-Docs p29\nRight click on the project \u0026gt; Build Project\nAdd User Code Sample Code F13 References:\nRL78 LIN sample code - Forum - RL78 MCU - Renesas Engineering Community Searched by RL78/F13, F14 Group LIN Master Mode (RLIN3) in DDG EK-RA6M4 sample project: [ERROR] Toolchain configured for project is not currently available. - Forum - RA MCU - Renesas Engineering Community Searched by [ERROR] Toolchain configured for project is not currently available. Please add/enable toolchain through Renesas Toolchain Management or select a different toolchain for this project. in DDG Notes:\n(2024-12-18)\nOpen project\nActions:\nDownload the zip files r1-Forum: MASTER_LIN.zip; SLAVE_LIN.zip\nImport existing projects \u0026gt; Select archive file: MASTER_LIN.zip \u0026gt; Finish\nThe sample code for F13 require CC-RL compiler, which however, cannot be installed:\n(2024-12-19)\nDon\u0026rsquo;t find make\n1 2 3 4 5 6 7 8 9 10 Extracting support files... [ERROR] Toolchain configured for project is not currently available. Please add/enable toolchain through Renesas Toolchain Management or select a different toolchain for this project. 08:49:47 **** Incremental Build of configuration HardwareDebug for project MASTER_LIN **** make -r --output-sync -j20 all Cannot run program \u0026#34;make\u0026#34;: Launching failed Error: Program \u0026#34;make\u0026#34; not found in PATH PATH=[C:/Renesas/e2_studio/eclipse//plugins/org.eclipse.justj.openjdk.hotspot.jre.full.win32.x86_64_21.0.3.v20240426-1530/jre/bin/server;C:/Renesas/e2_studio/eclipse//plugins/org.eclipse.justj.openjdk.hotspot.jre.full.win32.x86_64_21.0.3.v20240426-1530/jre/bin;C:\\ProgramData\\GCC for Renesas RL78 4.9.2.202201-GNURL78-ELF\\rl78-elf\\rl78-elf\\bin;C:\\ProgramData\\LLVM for Renesas RL78 17.0.1.202409\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\dotnet\\;C:\\Program Files\\Tailscale\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\WireGuard\\;C:\\ProgramData\\GCC for Renesas RL78 4.9.2.202201-GNURL78-ELF\\rl78-elf\\rl78-elf\\bin;C:\\ProgramData\\LLVM for Renesas RL78 17.0.1.202409\\bin;C:\\Users\\ZichenWang\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ZichenWang\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\ZichenWang\\.dotnet\\tools;C:\\Program Files (x86)\\Nmap;C:\\texlive\\2024\\bin\\windows;C:\\Renesas\\e2_studio\\eclipse] 08:49:47 Build Failed. 1 errors, 0 warnings. (took 47ms) However, my another project can be built successfully\n1 2 3 4 5 6 Extracting support files... 09:01:55 **** Incremental Build of configuration HardwareDebug for project F24_from_G23_Sample_Code **** make -r --output-sync -j20 all Build complete. 09:01:55 Build Finished. 0 errors, 0 warnings. (took 157ms) Actions:\nCheck toolchain in File \u0026gt; Properties r2-Forum\nThe compiler Renesas CC-RL was set to an old version: v1.10.00. However, my compiler version is 1.14.00\nExclude files from building\nActions:\nExclude MASTER_LIN/src/MASTER_LIN.c from build\nOtherwise, there will be an error: Duplicate symbol \u0026quot;_main\u0026quot; in \u0026quot;.\\src\\MASTER_LIN.obj\u0026quot;\nBuilding failure messages 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 Invoking Linker: MASTER_LIN.abs LinkerMASTER_LIN.tmp= -MAKEUD=\u0026#34;C:\\Users\\ZichenWang\\Downloads\\Y-ASK-RL78F13-LIN\\MASTER_LIN\\HardwareDebug/MASTER_LIN_l.ud\u0026#34; -device=\u0026#34;C:/Users/ZichenWang/.eclipse/com.renesas.platform_1435879475/DebugComp/RL78/RL78/Common/DR5F10BMG.DVF\u0026#34; -list -nooptimize -entry=_start -auto_section_layout -security_id=00000000000000000000 -debug_monitor=1FE00-1FFFF -user_opt_byte=FFFFE8 -ocdbg=84 -ocdtr -input=\u0026#34;.\\generate\\cstart.obj\u0026#34; -input=\u0026#34;.\\generate\\stkinit.obj\u0026#34; -input=\u0026#34;.\\src\\MASTER_LIN.obj\u0026#34; -input=\u0026#34;.\\src\\RLIN_driver.obj\u0026#34; -input=\u0026#34;.\\src\\RLIN_driver_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_cgc.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_cgc_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_intc.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_intc_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_port.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_port_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_wdt.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_wdt_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_main.obj\u0026#34; -input=\u0026#34;.\\src\\r_systeminit.obj\u0026#34; -library=\u0026#34;.\\MASTER_LIN.lib\u0026#34; -output=\u0026#34;MASTER_LIN.abs\u0026#34; -debug -nocompress -memory=high -rom=.data=.dataR,.sdata=.sdataR -nomessage -nologo W0561018:The evaluation period of CC-RL V1 is valid for the remaining 37 days. After that, functional limit will be applied. Please consider purchasing the product. E0562300:Duplicate symbol \u0026#34;_main\u0026#34; in \u0026#34;.\\src\\MASTER_LIN.obj\u0026#34; Renesas Optimizing Linker Abort make: *** [makefile:111: MASTER_LIN.abs] Error 1 \u0026#34;make -r --output-sync -j20 all\u0026#34; terminated with exit code 2. Build might be incomplete. 13:04:51 Build Failed. 2 errors, 29 warnings. (took 1s.701ms) Exclude MASTER_LIN/generate/hdwinit.asm from build:\nOtherwise, there is an error: Duplicate symbol \u0026quot;_hdwinit\u0026quot; in \u0026quot;.\\generate\\hdwinit.obj\u0026quot;\nBuilding failure messages 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 Extracting support files... 13:12:23 **** Incremental Build of configuration HardwareDebug for project MASTER_LIN **** make -r --output-sync -j20 all Invoking Library Generator: MASTER_LIN.lib Library Generator Completed LibraryGeneratorMASTER_LIN.tmp= -MAKEUD_LBG=\u0026#34;C:\\Users\\ZichenWang\\Downloads\\Y-ASK-RL78F13-LIN\\MASTER_LIN\\HardwareDebug\\MASTER_LIN_lbg.ud\u0026#34; -cpu=S3 -output=MASTER_LIN.lib -nologo Invoking Linker: MASTER_LIN.abs LinkerMASTER_LIN.tmp= W0561018:The evaluation period of CC-RL V1 is valid for the remaining 37 days. After that, functional limit will be applied. Please consider purchasing the product. -MAKEUD=\u0026#34;C:\\Users\\ZichenWang\\Downloads\\Y-ASK-RL78F13-LIN\\MASTER_LIN\\HardwareDebug/MASTER_LIN_l.ud\u0026#34; E0562300:Duplicate symbol \u0026#34;_hdwinit\u0026#34; in \u0026#34;.\\generate\\hdwinit.obj\u0026#34; -device=\u0026#34;C:/Users/ZichenWang/.eclipse/com.renesas.platform_1435879475/DebugComp/RL78/RL78/Common/DR5F10BMG.DVF\u0026#34; Renesas Optimizing Linker Abort -list -nooptimize -entry=_start make: *** [makefile:111: MASTER_LIN.abs] Error 1 -auto_section_layout -security_id=00000000000000000000 -debug_monitor=1FE00-1FFFF -user_opt_byte=FFFFE8 -ocdbg=84 -ocdtr -input=\u0026#34;.\\generate\\cstart.obj\u0026#34; -input=\u0026#34;.\\generate\\hdwinit.obj\u0026#34; -input=\u0026#34;.\\generate\\stkinit.obj\u0026#34; -input=\u0026#34;.\\src\\RLIN_driver.obj\u0026#34; -input=\u0026#34;.\\src\\RLIN_driver_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_cgc.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_cgc_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_intc.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_intc_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_port.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_port_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_wdt.obj\u0026#34; -input=\u0026#34;.\\src\\r_cg_wdt_user.obj\u0026#34; -input=\u0026#34;.\\src\\r_main.obj\u0026#34; -input=\u0026#34;.\\src\\r_systeminit.obj\u0026#34; -library=\u0026#34;.\\MASTER_LIN.lib\u0026#34; -output=\u0026#34;MASTER_LIN.abs\u0026#34; -debug -nocompress -memory=high -rom=.data=.dataR,.sdata=.sdataR -nomessage -nologo \u0026#34;make -r --output-sync -j20 all\u0026#34; terminated with exit code 2. Build might be incomplete. 13:12:23 Build Failed. 2 errors, 1 warnings. (took 352ms) PWM Principle PWM 信号频率的理论上限是时钟频率的一半\nReferences:\n元宝 - Single PWM Drive Pump Solenoid 元宝 - 时钟与PWM信号关系解析 Gemini 2.5P - PWM 信号与电平跳变 Supports:\n(2025-06-03T14:26)\nPWM 与时钟一样也是方波，由一次上升和一次下降组成。数字电路一般是由时钟的上升沿（或由下降沿）触发，所以生成一个 PWM 周期需要两个时钟周期：\nC P L W K M ↑ ⋮ 1 p e ↑ r i o d ↑ ⋮ ↑ ⋮ 双边沿触发的数字电路设计太复杂\n电路被时钟边沿触发后，还需要经过计数器更新、比较寄存器 CCR，输出缓冲等运算，可能无法在半个时钟周期内（如果在上升沿触发，也就是在下降沿来临前），完成这些动作，实现有效跳变。\n(2025-06-04T12:10)\n不确定这是不是 “synchronization with f_CLK” mentioned in Page 415 of r1-HrdManual f_PWM = f_CLK/2 时，PWM 分辨率为 2（计数器仅有 0，1 两种状态），没有应用价值r1-元宝\nPWM 分辨率是自动重装载值 ARR，严格来讲是 1/(ARR+1)。ARR 是在一个 PWM 周期内的 时钟周期数量\nARR 计数值 = 3，分辨率是 1/4:\nC P L W K M ↑ ⋮ ⋮ ⋮ ↑ ⋮ ⋮ 1 p e ↑ r i o d ↑ ↑ ⋮ ↑ ⋮ ⋮ ↑ ↑ ↑ ⋮ 如果 CLK 频率是 72 MHz，不使用预分频器，ARR=3，则 PWM 频率是 72M/4 = 18M\n脉宽调制：控制一个 pwm 周期内高电平和低电平的比例。 时钟信号的占空比是 50%，高低电平各占一半，而 PWM 生成的方波占空比可以在 0% 和 100% 变化，通过设置计数器的最大值。\n一个 PWM 周期内通常不允许大于两次电平跳变，所以无法实现电平为 010100 的一个 PWM 周期r3-Gemini\nARR = 6\nC P L W K M ↑ ⋮ ⋮ ⋮ ↑ 1 ↑ p e r i ↑ o d ↑ ↑ ↑ ⋮ ↑ ↑ 加预分频器是为了避免超出计数器的最大计数值r2-元宝\n主时钟频率 72MHz，目标 PWM 频率 1kHz，不使用 Prescaler，则 ARR 是 71,999，如果使用 16 位的计数器（最大计数值是 65,535，还未数到 71,999，就复位到0了。 预分频器无法增加占空比分辨率，即缩小占空比的最小步进\n主时钟频率 72MHz，目标 PWM 频率 100 kHz，不使用 Prescaler，则自动重装载值是 720，如果使用 16 位的计数器（最大计数值是 65,535，但有效计数区间只是 720，占空比的分辨率只有 1/720，没有充分利用 16 位计数器的理论分辨率 1/65535.\n如果使用 Prescaler = 2, ARR 会进一步减小到 360，占空比分辨率是 1/360。为了增加分辨率，可以增加主时钟频率。\nPWM 信号的频率不会影响电机转速\nSupports:\nPWM 信号频率影响稳定性和损耗r1-元宝 Renesas Configure Issues:\nConfigure Two Channels Configure Two PWM Renesas Hardware Minimum Code Notes:\nConfigurations for One PWM with Two Channels in Smart Configurator\nSupports:\nmodule PWM Output\nTwo channels have different duty of cycles:\nConfigure Two PWM with Different Frequencies\nSupports:\n(2025-06-03T09:25)\nPump and solenoid requires different-frequency PWM signals. Renesas RL F23 Hardware\nReferences:\nRL78/F23, F24 - User\u0026rsquo;s manual: Hardware Supports:\nTAU Registers (Page 372 in r1)\nTimer Count Operation Mode (Page 417)\nMinimum Code\nReferences:\n4.2.9 PWM Output - Smart Configurator User\u0026rsquo;s Manual: RL78 API Reference Searched by PWM smart configurator rl78 in Renesas search RL78/G13 Timer Array Unit(PWM Output) CC-RL - Docs Searched by Renesas rl78 timer pwm in DDG Supports:\n(2025-05-22T15:11)\n6 functionsr1-Docs:\n1 2 3 4 5 6 R_{Config_TAUm_n}_Create R_{Config_TAUm_n}_Start R_{Config_TAUm_n}_Stop R_{Config_TAUm_n}_Create_Userinit r_{Config_TAUm_n}_channeln_interrupt r_{Config_TAUm_n}_channelp_interrupt Example code of F13r2-Docs\nHigh-level understanding\nDuty-cycle determines the overall voltage. If the duty-cycle is 50% and the power supply is 12V, the equivalent power supply is 6V. F24 LED Problems:\nDifferent duty cycle corresponds to different level of brightness, as essentially the voltage varies alongside the duty cycle. Algorithm Design\nSoftware Design\n(2025-05-29T12:07)\nConfigurations:\nReferences:\nGenerating PWM signals in RL78 microcontroller - YouTube - SM training academy RL78/G23 - Timer Array Unit (PWM output) - Renesas Web Simulator Supports:\nClock, Debugger, Pin 67 out for LED2, Master and Slave\n周期方波信号：由定时器 TAU0 的 channel 0 (Master) 的中断产生\n比较器：由定时器 TAU0 的 channel 0 (Slave) 的中断产生\nActions:\nmain.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026#34;r_smc_entry.h\u0026#34; void main (void); void main(void) { R_Config_TAU0_0_Start(); /* TAU00,TAU01 operation enable */ EI(); while (1U) { HALT(); /* Waiting interrupt */ } } Config_TAU0_0_user.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 static void __near r_Config_TAU0_0_channel0_interrupt(void) { /* Start user code for r_Config_TAU0_0_channel0_interrupt. Do not edit comment generated here */ static uint8_t s_tm00_count = 0U; /* INTTM00 interrupt times counter */ uint16_t temp_duty = 0U; /* Duty factor calculation */ if (250 == (++s_tm00_count)) /* Update the duty cycle per 500ms */ { s_tm00_count = 0U; /* Interrupt counter reset */ P5 ^= 0x04; /* Invert LED2 */ temp_duty = TDR01; /* Read the current duty cycle (duty factor) setting */ if (temp_duty \u0026lt;= (_E100_TAU_TDR01_VALUE / 9)) { /* If current duty cycle is 90% (duty factor is 10%), */ temp_duty = _E100_TAU_TDR01_VALUE; /* Set duty cycle to 10% (duty factor to 90%) */ } else { /* Increase duty cycle by 20% (decrease duty factor by 20%) */ temp_duty -= ((_E100_TAU_TDR01_VALUE / 9) * 2); } TDR01 = temp_duty; /* Update duty cycle (duty factor) */ } else { ; } /* End user code. Do not edit comment generated here */ } Test Case\nTest Report\nADC Problems:\nThe api code generated by \u0026ldquo;Code Generator\u0026rdquo; in CS+ is different from the code generated by \u0026ldquo;Smart Configurator\u0026rdquo;. References:\nADC Configuration - Forum - RL78 MCU - Renesas Engineering Community Searched by renesas rl78 adc in DDG ADC Sample code - Forum - RL78 MCU - Renesas Engineering Community Found in s1 RL78/G13 A/D Conveter(Software Trigger and Sequential Conversion Modes) CC-RL - R01AN2581EJ0200 Rev. 2.00 Searched by renesas rl78 ad conversion in DDG RL78/G23 A/D Converter (Scan mode) - Renesas Found in s2 Web simu Found on MCU Simulator Online Practices:\n(2025-01-10)\nCreate \u0026gt; Start \u0026gt; Get_ValueResult r1-Forum\nThere are only 3 api functions for consideration in the generated source file: Config_S12AD01.c.\n1 2 3 4 5 6 7 8 9 10 11 // R_Config_S12AD01_Create() // R_Config_S12AD01_Start() // R_Config_S12AD01_Stop() // R_Config_S12AD01_Get_ValueResult() ADC needs to be executed inside interruption r2-Forum\n(2025-01-30)\nThis is not necessary. The R_Config_S12AD0_Get_ValueResult(ADCHANNEL0, \u0026amp;g_result_buffer); can just put in the while(1U) loop. RL78/G13 r3-Manual has different code from F24.\nRL78/G23 r4-Manual code is not searched yet.\nOnline simulator r5-Simu includes several AD projects.\nG23 code on web simu used ADCS register, which doesn\u0026rsquo;t exist in F23 project.\n(2025-01-30)\nI don\u0026rsquo;t set up the interrupt function, and only write the main.c\nBut the ~_Start is necessary before ~_Get_ValueResult to enable conversion.\nActions:\nSmcg select: Software trigger\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // main.c #include \u0026#34;r_smc_entry.h\u0026#34; uint16_t g_result_buffer = 0U; void main(void); void main(void) { g_result_buffer = 0U; /* Initialize result buffer */ R_Systeminit(); EI(); while (1U) { R_Config_S12AD0_Start(); R_Config_S12AD0_Get_ValueResult(ADCHANNEL0, \u0026amp;g_result_buffer); } } If the value is 0x3328d, calculting the source voltage according to the: Voltage Divider Calculator\nD i R R G o 1 2 N d D P e V ( ( o s 4 1 w ( 7 8 e 0 K M K r . ) C ) 7 U s V _ u ) B p a p t l V y o l 1 t 5 . 2 V A D C $$ \\begin{aligned} \\frac{x}{4096} \\times 5 = y \\\\\\ y = \\frac{V_S \\times R_2}{R_1 + R_2} \\\\\\ = \\frac{V_S \\times 47}{47 + 18} \\\\\\ \\end{aligned} $$ 12-bit A/D converter: 4096 levels\nReference voltage is VDD = 5V to the MCU\n$R_2$ = 18K, $R_1$ = 47K\n$$ \\begin{aligned} y = \\frac{3328}{4096} \\times 5 = 4.0625 \\\\\\ V_S = 4.0625 \\times 65 / 18 = 14.67 \\end{aligned} $$14.67 + 0.7 = 15.37 V, which is close to Power supply 15.2 V.\nIAR IDE Blinky LED Problems:\nUse IAR IDE to implement a binking LED project for RL78 F23 Issues:\nBuilding Debug References:\nCreate a blinky project from scratch (IAR™) - YouTube - RenesasPresents Searched by renesas IAR blink LED project in DDG RL78 Smart Configurator User\u0026rsquo;s Guide: IAR Seached by renesas R20AN0581 in DDG Mentioned in RL78/G23 - Renesas Electronics Corporation Searched by renesas RL78 IAR blink LED project in DDG Using Smart Configurator with IAR Embedded Workbench for RL78 (2/2) - Creating an EWB Project for Smart Configurator Basic debugging - IAR Searched by IAR debugg settings in DDG C-SPY® Debugging Guide for the Renesas - IAR Searched by IAR debug settings for renesas e2 lite in DDG IAR Embedded Workbench Overview - Part 1 - YouTube - IAR Searched by IAR debugging demostration in DDG videos Practices:\n(2025-04-08)\n✅ Building project with Smart Configurator + IAR\nSupports:\nCreate Smart Configurator project under the IAR project folder ➔ Generate Code (.ipcf and .ewp same dir)r3-Video Actions:\nBuild Errors:\n1 2 Error[Lp004]: actual size (0x5) exceeds maximum size (0x4) for block \u0026#34;OPT_BYTE\u0026#34;\tError[Lp004]: actual size (0x10) exceeds maximum size (0xa) for block \u0026#34;SECUR_ID\u0026#34;\t(2025-04-10)\nSet the Device to matched MCU modelr6-Ytb\nResults:\nBuild succeeded. (2025-04-11)\n✅ Debug with IAR\nSupports:\nSet breakpointr4-Basic, r5-Guide\nSepcify the debugger to E2 Lite, and check the option: \u0026ldquo;Run to main\u0026rdquo; in project options r6-Ytb\nActions:\nUnable to set a breakpoint at main() function. And it never hit any breakpoints:\nConnecting the E2 Lite debugger resolve this error\n(2025-04-21T16:40:09)\n✅ IAR blink LED\nSupports:\nMCU_V_D_IN1 is high side pin.\nSame code can blink the LED during debugging in CS+, however debugging with IAR doesn\u0026rsquo;t result in LED blinking. Check pin definition and ensure its reference is correct.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026#34;r_smc_entry.h\u0026#34; void delay(long d){ while(d--); } int main(void) { while(1) { PIN_WRITE(MCU_V_D_IN1) = 0; delay(500000); PIN_WRITE(MCU_V_D_IN1) = 1; delay(500000); } return 0; } Migrate from CS+ Problems:\nHow to port our previous Renesas CS+ projects to IAR projects References:\nRL78 Software Porting Guide - Renesas Electronics Corporation This guide is for porting between different MCUs. Searched by How to port Renesas CS+ projects to IAR projects in DDG Migration guide: Migrating from the CS+ CA78K0R toolchain for RL78 to IAR Embedded Workbench® for RL78 - IAR Found in s1 Porting the code from CC-RL compiler to IAR Searched by port CS+ to IAR in Renesas Notes:\n(2025-04-16T11:06:03)\nIAR has a tool: Convert to IAR for RL78r2-guide\nSupports:\nThere is only \u0026ldquo;Project type\u0026rdquo; selectable: \u0026ldquo;CubeSuite for RL78\u0026rdquo; Actions:\nUsing default settings leads to a compilation error Build Options Problems:\nCompilation options settings Issues:\nOptimization level Error Notes:\n(2025-04-22)\nOptimization level:\nSupports:\nDo not apply optimization for step-by-step debugging Actions:\nProject -\u0026gt; Options -\u0026gt; C/C++ Compiler -\u0026gt; Level: None (2025-04-24T10:17:31)\nError:\nSupports:\nDebug the RLIN3 example project\nPop-up window:\n1 2 3 4 5 6 7 8 9 10 11 .----------------------------------------. |E2LITE × | | | | Target power mode mismatch | | ? | | Press YES to try again. | | | | Pressing NO will end debug session. | | | | Yes No | \u0026#39;----------------------------------------\u0026#39; As shown in title of \u0026ldquo;E2Lite Hardware Setup (R7F124FPJ)\u0026rdquo;\nThe F23 MCU isn\u0026rsquo;t compatiable with the debugger setting for F24 MCU. Actions:\nBy only connecting the debugger to the F24 development board, the program can be downloaded and debugged without needing to separately power the development board (using the default hardware setup). ","date":"2024-11-25T10:28:00Z","image":"https://www.renesas.com/sites/default/files/RTK7F124FPC01000BJ_RL78F24-Target-Board.png","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-test-rl78_beginner/","title":"Test: EE - Boards | RL78 Beginner"},{"content":"(The feature image credits: Introducing the Smart Configurator Board Support Feature for Quick Project Setup - Renesas Blog. Searched by renesas smart configurator in DDG image and Google lens.)\ne2 Studio Debug References:\ne² studio快速入门指南 (3/3) - 构建和调试RL78项目 - bilibili - 瑞萨电子 Searched by RL78 smart configurator in bilibili Searched by Notes:\nIt\u0026rsquo;s an variant of GDB. r1-Bili Install References:\ne² studioIDE and Coding Tool | Renesas Notes:\nInstall package: e² studio 2024-10 installer for Windows\nInstalled CS+ for CC V8.12.00\nOpen Projects References:\ne2studio - Importing and Exporting an e2studio Project - YouTube - RenesasPresents Searched by How to open an e2 studio project in DDG Importing Projects to e² studio | Renesas Customer Hub Searched by import CS+ project to e2 studio in DDG Smart Configurator , Code Generator - Renesas Electronics Corporation Searched by renesas rcpc file in DDG Porting from the e² studio to CS+ | Renesas Searched by will e2 studio generate an .rcpc file automatically? in DDG Project Conversion between e2 studio and CS+, Notes and Tips Searched by s4 Notes:\n(2024-12-18)\nAdd a project (.zip) into the workspace r1-YouTube\nImport \u0026gt; General \u0026gt; Existing Projects into Workspace \u0026gt; (Next) Select archive file \u0026gt; Finish\n(2025-01-14)\nExample of opening e41:\nImport existing projects\n(2024-12-30)\nImport a CS+ project into e2 studio by selecting the .rcpe file r2-Hub:\n(2024-12-31)\n.rcpc means \u0026ldquo;Renesas Common Project File\u0026rdquo; r3-Note\nAn .rcpc can be generated by Export the project as a Renesas Common Project File, which can be ported to CS+. r4-Port\nInversely, CS+ will generate an .rcpe for converting to an e2 studio project r5-Convert.\nSmart Configurator Symbolic Name References:\nRL78 Smart Configurator - Manuals+ Searched by RL78 Smart Configurator How to configure Pin functions in DDG RL78 Smart Configurator - User\u0026rsquo;s Guide: IAREW - Docs Found in RL78/F23 Guide for Engineer - Renesas Electronics Corporation Searched by RL78/F23, F24 Setup Procedure for LIN Communication in Master Mode (Guidance) in DDG Notes:\n(2024-11-06)\nThe \u0026ldquo;Symbolic Name\u0026rdquo; attribute of pins will show up by clicking the \u0026ldquo;Pin Number\u0026rdquo; page\n(2024-11-13)\nUse the Smart Configurator to generate code for developing in IAR r2-Docs. CS+ for CC (RL78) Workspace Options References\n【General - Text Editor】 category | CS+ V4.01.00 - tool-support.renesas.com Searched byrenesas CS+ editor options in DDG Text Editor (2025/02/21)\nTab width: 8 -\u0026gt; 4 Build Process ROM Problem:\nBuilding failed with this error:\n1 2 E0562320:Section address overflowed out of range : \u0026#34;.monitor2\u0026#34; Renesas Optimizing Linker Abort Reset to Defaults Problem:\nI don\u0026rsquo;t know how to restore default settings after changing some options. References:\nCS+ V8.04.00 Integrated Development Environment User\u0026rsquo;s Manual: Project \u0026hellip; Searched by renesas CS+ reset building options in DDG Practices:\n(2025-01-10)\nRight click on the pane to open a context menu. Generate HEX Problems:\nHow to generate a .hex file like this image r1-Prod, each line has a fixed length.\nBut in the hex file generated by CS+, each line is very long. References:\nUnderstanding Hex Files - Electronic Products Searched by hex file example in Google Images Practices:\n(2025-01-10)\nCC-RL (Build Tool) \u0026gt; Property \u0026gt; Hex Format \u0026gt; Maximum byte count for data record\n(2025-01-14)\nVSCode extension for .hex files:\nIntel HEX format can update the checksum.\nThe meaning of each segment is introduced in r1-Electronic Products.\nReferences\nI am unable to resolve this error E0562320:Section address overflowed out of range : \u0026ldquo;.text\u0026rdquo; - Support Forums Found in the Smart Browser inside the CS+. Smart Browser is opened by clicking Help for Message after selecting the error message. Notes:\nThe Property tab of containing\nWrite Flash Problem:\n(2025-01-10)\n客户要求 hex 文件的开头是代码的描述信息，这个就需要我们在编译的时候，把描述信息也包含进来吧， 请问这个应该怎么做？是不是要设置 linker？\n描述信息是一个如下所示的结构体，客户要求把这个结构体放在 hex 文件中，放在应用代码之前\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct code_descriptor { uint16_t component_id; /* = 79 */ uint8_t pcba_id; /* = 3 */ uint8_t assembly_id; /* = 1 */ uint16_t usage_id; /* = 1 */ uint8_t unused1; /* = 0 */ uint8_t firmware_type; /* = 0 */ uint64_t git_hash; /* any identifier that is unique to a firmware release */ uint16_t build_configuration_id; /* = 0xFFFF */ uint16_t unused2; /* = 0 */ uint16_t bootloader_protocol_version; /* = 5 */ uint16_t security_rev; /* = 0 */ void (*c_int00)(void); /* address where the app will take over execution */ const uint32_t *crc32; /* must be 4-byte aligned and the _last_ word in the app image */ }; Write a struct containing software information into flash memory (ROM), not RAM.\nThe struct should be the first part of the hex file. While memcpy is writing data into the RAM.\n0 0 0 x x x 0 5 5 0 0 0 0 0 1 0 0 0 . . . . b ( o s f c t o P p e l o e o s r t c a n x t t o v i u s s t l r g e o r h t o u r c n i _ ( a c a t _ t s c d t m b y e v o e ) y _ c a d r t i . r e e d . i ) a b l e s ) . h e x References:\nAsk ChatGPT How to make a struct compiled first and being put at first in the hex file? The compiler I am using is: CC-RL. Are the steps the same? CC-RL Compiler User\u0026rsquo;s Manual - Renesas Electronics Corporation Searched by renesas cc-rl manual in DDG Ask ChatGPT Practices:\n(2025-01-12)\nUse the #pragma section directive to place the struct in a named section, and modify the linker script r1-ChatGPT.\nThe linker script should be the .clnk file, as the command in the .map file shows an option: -subcommand=DefaultBuild\\Test_INTTM00.clnk\nCode:\n1 2 3 4 5 6 7 8 9 10 #pragma section data MySection struct MyStruct { int a; char b; }; #pragma section #pragma section data MySection struct MyStruct my_struct = { 0x1234, \u0026#39;A\u0026#39; }; #pragma section Linker script:\n1 2 3 4 5 6 7 8 9 SECTION ROM1 START: 0x000000 END: 0x00FFFF { .MySection .text .data ... } Introductions on Manual r2-Manual:\nChapter: 3.2.5 Link map information (p. 298) Building options can be specified through the property of CC-RL (Build Tool) in \u0026ldquo;CS+ for CC\u0026rdquo; IDE:\nCannot manually specify sections when the option is on: Link Options \u0026gt; Section \u0026gt; Layout sections automatically\nI found this after I tried many time to change the Section start address, but cannot get desired results: .vect section isn\u0026rsquo;t allowed to be modified. MySection is not assigned address.\nAnd once ChatGPT suggested turning on the \u0026ldquo;Layout automatically\u0026rdquo; temporarily\nDebugging Tips\nUse the map file to confirm memory usage and section placements. If issues persist, temporarily enable \u0026ldquo;Layout sections automatically\u0026rdquo; to let the linker determine the layout, then analyze the resulting map file for hints. By turning off the \u0026ldquo;Layout sections automatically\u0026rdquo;, setting it to No, The Section start address will list all sections.\nAlso, the Load address in Hex Output Options \u0026gt; Output File has range limitations, where it cannot be specified as an arbitrary address, such as 5000 (0 is okay).\n1 (E)\tE0562101\tE0562101:Invalid address specified in option \u0026#34;output\u0026#34; : \u0026#34;5000\u0026#34;\tR7F123FGG3AFB_V05.mtpj Files after building:\nThe file QualityReport(R7F123FGG3AFB_V05,DefaultBuild).txt records the building information.\nMapping list exists in R7F123FGG3AFB_V05.map\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 *** Mapping List *** SECTION START END SIZE ALIGN .vect 00000000 0000007f 80 0 .constf 00000080 00000080 0 2 .init_array 00000080 00000080 0 2 .sdata 00000080 00000080 0 2 .data 00000082 00000085 4 2 .option_byte 000000c0 000000c4 5 1 .RLIB 000000c5 000000c5 0 1 .security_id 000000c6 000000d5 10 1 .flash_security_id 000000d6 000000e5 10 1 .text 000000e6 0000029d 1b8 1 .textf 0000029e 00001165 ec8 1 .SLIB 00001166 00001889 724 1 .const 00003000 0000309f a0 2 .bss 000fcf00 000fcf77 78 2 .dataR 000fcf78 000fcf7b 4 2 .sbss 000ffe20 000ffe20 0 2 .sdataR 000ffe20 000ffe20 0 2 Assemble list is shown in the app_main.prn, which is generated by checking the option: Compile Options -\u0026gt; Assemble List -\u0026gt; Output assemble list file -\u0026gt; Yes\nBuilding command is recorded in R7F123FGG3AFB_V05.clnk.\nThe command -lnkcmd is not allowed to be written in the filed: Link Options \u0026gt; Others \u0026gt; Other additional options r3-ChatGPT.\nThe declared section name will be added a suffix _n, which indicates \u0026ldquo;near\u0026rdquo; r4-Manual p.918 Quick Guide.\nThe sequence of different sections in the generated .hex file ususally is (referring to the .map file):\nP a . . . . . . . . . . . . . . . t S v o s f c t R S t c d s i d b s s t E e p e l M o e L L e o a d n a s d b M e C c t c a y n x I I x n t a i t s a s y r T t i u s C s t B B t s a t t a t s C n I o r h o t f t a _ R a o O n i _ n f a R n A N _ t s s r s b y e t r t y _ c _ a _ t i u n y n e d r i t y _ i d S 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 T 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 A 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 R 0 0 0 0 0 0 0 0 0 0 0 0 0 0 f f f f f T 0 0 0 0 5 5 5 5 5 5 6 6 6 6 c c f f f 0 0 0 0 0 0 0 2 2 9 8 8 8 8 f f e e e 0 c c d 0 3 d 8 8 a 7 7 7 7 0 0 2 2 2 0 0 6 6 0 0 0 8 8 c 4 4 8 8 0 4 0 0 0 E 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 N 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 D 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 f f f f f 0 0 0 0 5 5 5 5 5 6 6 6 6 6 c c f f f 0 0 0 0 0 0 2 2 9 8 8 8 8 8 f f e e e 7 c d e 0 c 8 8 a 7 7 7 7 7 0 7 2 2 2 f 4 5 5 3 f 7 8 b 3 4 7 8 8 3 b 0 0 3 S I 1 7 e Z 8 1 1 a b 2 c 7 E 0 5 0 0 4 0 8 0 4 8 0 4 0 0 4 8 0 0 4 A 0 1 1 1 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 L I G N The 4 sections .vect, .option_byte, .security_id, .flash_security_id will always be at the beginning of the .hex file after each time building, even though I manually specify their address in the Section start address.\nThose 4 sections are not initially listed in the option Section start address, so I guess they cannot be changed.\nHowever, I can switch the position of\nThe flash memory left for application code ranges 0x0000 to 0x8000\n(2025-01-23)\nThe code descriptor struct is required to be put at 0x5000. We don\u0026rsquo;t have access to .vect sections Fred Assist (2025-01-14)\nDefine struct Define Section Problem:\nError:\nReferences:\nWhat is the use of \u0026ldquo;#pragma section \u0026rdquo; in C? - SO Searched by pragma section in embedded development in DDG Previously searched for: How to define a section in c program? in DDG RL78 Family One Image Bootloader RL78 One Image Bootloader Example \u0026hellip; Searched by rl78 AppStartAddr in DDG Practices:\n(2025-01-13)\nGCC attribute supports __attribute__ section r1-SO.\nWhen you are confused, just go to debug the code or read the documents.\nI was randomly reading the manual.\nJumping sequence: Quick Guide A.2.1 -\u0026gt; \u0026ldquo;Changing compiler output section name\u0026rdquo; Page 379 in Ch 4.2.4\nI found the important table on Page 380. which indicates that different types of variables will be assigned to different sections. And finally understanded the explanation about the format rule of section names on Page 379.\nI tried to test a simple case, an integer:\n1 2 #pragma section const MyConst const uint16_t __near component_id = 79; And I also tried to use the section name MyConst_n with suffix added as demonstrated in the example, and also shown in the generated .map file. It works.\n(2025-01-15)\nConstant data cannot exist before address 0x3000 r2-Manual. Padding Problems:\nThe customer\u0026rsquo;s SDP requires each line in the hex file to be the same length. But the lines in generated hex file have various length.\nHow to padd each line to the same length?\nNotes:\n(2025-01-14)\nThe hex file needs to be continuous, as the memory needs to be filled seamlessly, instead of only including the sections occupied by code. (2025-01-16)\nEnsure the Start address is the text section (for code). It\u0026rsquo;s better to delete the .hex file before rebuilding it, because the settings of CS+ may not be kept when packaging the project into a .zip file.\n5 5 5 5 0 0 0 0 0 2 D E 0 6 F 0 C J J A o u u p d m m p e p p c d t t o e a a d s b b e c l l r e e s i t p e a t n r o d t r Vect Problem:\nNeed to use the vector table stored in the customer\u0026rsquo;s bootloader program. References:\n-VECT | CS+ V6.00.00 CC-RL Compiler User\u0026rsquo;s Manual, RL78 Family, Rev 1.13 RL78 Bootloader Jump to App Issue Searched by jmp asm code for rl78 in Google RL78 Dual Image Bootloader Example - Application Project Searched by RL78 Bootloader Jump to App Issue in DDG Are there any examples how to define ISR vectors? Searched by renesas rl78 isr handler in DDG Practices:\n(2025-01-14)\nThe -vect options r1-Docs can be added in the compiling command in the Other additional options field.\nAdd -show=vector in command to print \u0026ldquo;Variable Vector Table List\u0026rdquo; in the .map file r2-Manual P.304 (2025-02-19)\nI think the text Boyd sent on Jan 14 was copied out from the linker script of the boot project developed on his side.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 -VECTN=4=5026 -VECTN=6=5029 -VECTN=8=502C -VECTN=A=502F -VECTN=C=5032 -VECTN=E=5035 -VECTN=10=5038 -VECTN=12=503B -VECTN=14=503E -VECTN=16=5041 -VECTN=18=5044 -VECTN=1A=5047 -VECTN=1C=504A -VECTN=1E=504D -VECTN=20=5050 -VECTN=22=5053 -VECTN=24=5056 -VECTN=26=5059 -VECTN=28=505C -VECTN=2A=505F -VECTN=2C=5062 -VECTN=2E=5065 -VECTN=30=5068 -VECTN=32=506B -VECTN=34=506E -VECTN=36=5071 -VECTN=38=5074 -VECTN=3A=5077 -VECTN=3C=507A -VECTN=3E=507D -VECTN=40=5080 -VECTN=42=5083 -VECTN=44=5086 -VECTN=46=5089 -VECTN=48=508C -VECTN=4A=508F -VECTN=4C=5092 -VECTN=4E=5095 -VECTN=50=5098 -VECTN=52=509B -VECTN=54=509E -VECTN=56=50A1 -VECTN=58=50A4 -VECTN=5A=50A7 -VECTN=5C=50AA -VECTN=5E=50AD -VECTN=60=50B0 -VECTN=62=50B3 -VECTN=64=50B6 -VECTN=66=50B9 -VECTN=68=50BC -VECTN=6A=50BF -VECTN=6C=50C2 -VECTN=6E=50C5 -VECTN=70=50C8 -VECTN=72=50CB -VECTN=74=50CE -VECTN=76=50D1 -VECTN=78=50D4 -VECTN=7A=50D7 -VECTN=7C=50DA -VECTN=7E=50DD The \u0026ldquo;vector number\u0026rdquo; has an interval of 2 bytes, whereas each entry in the jump table is of 3 bytes, as they are a BR !_func instruction, whose Op code and Oprand take 3 bytes in total, e.g., ED5766 (2025-01-15)\nThe vector table area range (from 0x00 -\u0026gt; 0x7F) belongs to the vector table of the bootloader program controlled by the customer r3-Forums.\nI cannot use the customer\u0026rsquo;s vector table, so I have to implement a jump table.\nThe jump table is an array of function pointers to the ISR handlers r4-Example, r5-Question.\n(2025-01-20)\n-split_vect will create sections according to \u0026ldquo;vector table address\u0026rdquo; r2-Manual P.201 (Noticed this option when looking through CS+ CC-RL property)\nBy adding this option, the \u0026ldquo;Vector Table Address\u0026rdquo; can be observed in the \u0026ldquo;Mapping List\u0026rdquo; of the .map file:\nSECTION START END SIZE ALIGN .vect0 0x00 0x01 2 0 .vect2 0x02 0x03 2 0 .vect44 0x2c 0x2d 2 0 .data 0x74 0x77 4 2 As shown in the above list, the address of vect44 is 0x2C, which is exactly the INTTM00 interrupt.\n0x2C is the address of vector #20 both in the vector table and the flash memory in a normal project (as shown in the following image).\nUnused area in a vector table can be assigned with address with -vectn, which can be configured in the option: Address setting for specified area of vector table\nHowever, -vectn=2C=5062 cannot change the default address 5080 stored in the vector table, maybe due to the priority:\nBy setting the output hex file format to two bytes per line, it\u0026rsquo;s easier to check the vector table:\nThe 0x5080 is the address of the interrupt function: r_Config_TAU0_0_interrupt, whose corresponding symbol is: \u0026ldquo;the external name of the target function prefixed with an underscore (_)\u0026rdquo; r2-Manual p.199\nTherefore, I need the address 0x5080 (inside the cell whose address in the vector table is 0x2C, i.e., the 44-th byte in the vector table, corresponding to the vector number is #22) to be 0x5062.\nBut, I suspect that I have no access to the function r_Config_TAU0_0_interrupt at all, so I need to jump to my customized function.\nThat is why there should be a jmp command at 0x5062.\nConsidering to add a snippet of assemble code r2-Manual p.389:\n1 #pragma inline_asm I wonder if I can use call instead of jmp. And then I found #pragma callt down below in the Docs.\nCallt References:\nRL78 Family C compiler CC-RL Programming Techniques - Rev.1.10 Searched by rl78 callt usage in DDG CC-RL Compiler User\u0026rsquo;s Manual, RL78 Family, Rev 1.13 (2025-01-20)\n#pragma callt will notify the compiler that the function will be called by callt instruction. It\u0026rsquo;s similar to: #pragma interrupt r_Config_TAU0_0_interrupt(vect=INTTM00), which notifies the compiler that it\u0026rsquo;s a hardware interrupt handler.\nThe callt instruction table is: 0x80 - 0xBF after the vector table (0x00 - 0x7F) r2-Manual p.395.\nBoyd also mentioned callt in his email.\nAnd the call instruction is of 2 bytes, satisfying the requires of 3-byte long command.\n__callt and #pragma callt have similar effect r1-Tech.\n1 2 3 4 5 6 7 8 9 10 11 #pragma callt func1 void func1(void) { system_timer++; } void main (void) { func1(); } The section .callt0 is not configured when not using \u0026ldquo;Layout Automatically\u0026rdquo;, so I need to create it manually. And the memory has to be 0x80-0xBF. Otherwise, without satisfying the above 2 conditions, there will be an error: (E)\tE0562320\tE0562320:Section address overflowed out of range : \u0026quot;.callt0\u0026quot;\nI guess that the function (name) to be called by the callt instruction cannot be relocated, as they\u0026rsquo;re recorded in the \u0026ldquo;callt instruction table\u0026rdquo;, stored in the area: 0x80 - 0xBF\nAccording to the customer\u0026rsquo;s instruction below, I think I should: Put \u0026ldquo;function calling\u0026rdquo; (based on callt instruction) at the specified address. Such that, when an interrupt is triggered, the program will jump to corresponding address. For example, I put func1(); at 0x5026. Then, when the interrupt whose address in the vector table is 0x4 is enabled, the program will jump to 0x5026\nAllocate 0x5026-0x50E0 for a \u0026ldquo;vector table\u0026rdquo;. Interrupts will jump to locations in this table. To find the address (A) of an interrupt (N) use the formula: A = 0x5026 + 0x3 * ((N / 0x2) - 0x2). Or use the table below. The address should contain the instruction \u0026ldquo;CALL isr_function\u0026rdquo; (please verify this instruction is 3 bytes long).\nInterrupt (N) Address (A) 0x4 0x5026 0x6 0x5029 0x8 0x502C 0xA 0x502F 0xC 0x5032 0xE 0x5035 0x10 0x5038 Jump Table Problem:\nAfter definition, how to link the jump table? References:\nRL78 Dual Image Bootloader Example - Application Project RL78/F23, F24 Interrupt Source Determination Searched by f23 interrupt address list in DDG Notes:\n(2025-01-16)\nInitialized vector table \u0026gt; Initialized Interrupt function \u0026gt; Jump table \u0026gt; ISR Handler \u0026gt; ISR r1-Manual p.8.\nThe initial vector table contains 16-bit length address, ranging from 0x0000 ~ 0xFFFF\nJump table is at the beginning of each application.\n\u0026ldquo;Handler\u0026rdquo; means function name\nJump table contains 4-byte address, which are pointers to the function name\n0 0 0 x x x 0 0 0 0 0 C 0 0 0 0 4 0 1 0 0 8 ⋮ | 0 0 I x x S 0 0 R C 0 x 0 0 _ 0 4 v 1 t ( 6 . e I J 0 t ( n u 0 e ) i m x . t p ( t c J v i P o M e a t t ( n P c l a r c s t i b o t 0 z l t d x e e o e 0 d ) 0 I 0 I S 4 S R 1 R 0 H 0 f d 8 u l n r c ) ) ⋮ B A o p o p t An entry of jump table: 0 0 0 x x x 5 5 5 0 0 1 2 8 9 0 0 9 ⋮ ⋮ 0 I I x S S 5 R R 0 8 H f 0 a u n n ( d c P l t t e i r r o n t ( o 0 x I 5 S 1 R 9 9 H ) d l r ) ⋮ ⋮ The initial (the normal situation, generated by Smart Configurator) ISR will need to implement a JMP function.\nThe address of the ISR Handler may change every compiling, the the boot knows can found its address in the cell of the Jump table, as the jump table is put at a fixed position: the beginning of application region.\nSo the initialized Interrupt function: ISRx_vte() should do something write a JMP to go to the cell of the jump table, to get the address of the ISR Handler.\nThe address to the ISR Handler can\nTimer ISR Problem:\n(2025-01-16)\nDuplicate the example code in CS+ to evaluate an interval timer interrupt (2025-01-26)\nThe vector table (0x00 - 0x7F) specifies the address to be jumpped.\na 0 0 0 0 0 0 0 0 d 0 0 0 0 0 0 0 0 d 0 1 2 3 4 5 6 7 r 0 0 0 0 0 0 0 0 V 3 0 0 0 0 0 0 0 e D 0 0 0 0 0 0 0 c 5 0 0 0 0 0 0 0 t 2 0 0 0 0 0 0 0 o r F 0 0 0 0 0 0 0 F 0 0 0 0 0 0 0 t F 0 0 0 0 0 0 0 a F 0 0 0 0 0 0 0 b l 0 0 0 0 0 0 0 0 e 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o r 0 0 0 0 0 0 0 0 d 0 0 0 0 0 0 0 0 e 0 0 0 0 0 0 0 0 r 0 0 0 0 0 0 0 0 o 0 0 0 0 0 0 0 0 f 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 b 0 0 0 0 0 0 0 0 y t 0 0 0 0 0 0 0 0 e 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 i 0 0 0 0 0 0 0 0 n 0 0 F 0 0 0 0 0 a 0 0 B 0 0 0 0 0 n 0 0 5 0 0 0 0 0 0 0 1 0 0 0 0 0 a d 0 0 0 0 0 0 0 0 d 0 0 0 0 0 0 0 0 r 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 i s 0 0 r 7 e F v e r s e d ) The 0x51FB is the address of the interrupt function _r_Config_TAU0_0_interrupt Practices:\n(2025-01-27)\nThe bootloader specifies the interrupts jumping address:\nThe following is customer vector table: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 7 0 0 0 0 0 0 0 0 0 3 5 6 8 9 B C 0 8 0 8 0 8 0 8 0 5 5 5 5 5 5 5 1 0 0 0 0 0 0 0 0 3 5 6 8 9 B C 0 B 3 B 3 B 3 B 0 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 2 3 5 6 8 9 B C 6 E 6 E 6 E 6 E 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 2 4 5 7 8 A B D 9 1 9 1 9 1 9 1 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 2 4 5 7 8 A B D C 4 C 4 C 4 C 4 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 2 4 5 7 8 A B D F 7 F 7 F 7 F 7 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 3 4 6 7 9 A C D 2 A 2 A 2 A 2 A 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 3 4 6 7 9 A C D 5 D 5 D 5 D 5 D 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 0 0 7 F Pointer _ s E y D s 5 6 _ 0 0 t 6 5 0 i 2 0 1 x m x 5 _ 0 2 0 c x C 6 n 5 2 t 1 r 6 0 v t _ R L I N 0 0 E _ x 5 D T 5 0 0 r 5 5 9 0 a 0 3 0 5 x n 9 x 5 5 s 2 0 _ 2 5 i 3 n t e r r u p t ED corresponds the assembly instruction: BR !_vt_RLIN0_Reception_interrupt.\nSimilarly, EC is the instruction with two !: BR !!_vt_RLIN0_Reception_interrupt, corresponding to assembly code: EC095500.\nBR !!_func generates 4 byte assembly code, whereas BR !_func will create 3 byte assembly code.\n(2025-01-28)\nRemove _static keyword to use the original interrupt function (timer r_Config_TAU0_0_interrupt).\nInterrupt functions need RETI, instead of RET. So, it is suggested just using the generated interrupt functions, because they may be wrapped in a context with stack in and RETI.\nRemove _static to call the interrupt function in other files.\nThe jumptable.asm:\n1 2 3 .extern _r_Config_TAU0_0_interrupt .org 0x5062 BR !_r_Config_TAU0_0_interrupt LIN ISR Problem:\nCreate a jump table including the pointer to the handler to LIN interrupt. (2025-01-29)\nThe LIN transmission interrupt function in our application code are not being triggered. I want to test the example LIN code combined with the customer bootloader hex. References:\nRL78/G23 A/D Converter (Software trigger wait mode) Study this doc and sample code to implement ADC today. Notes:\nThe Vector table address of INTLIN0WUP is 4A r2-Docs.\nAnalogy to the example in r1-Manual p.8.\n2 2 V e c 0 t x o 5 r 0 5 t 3 a b l e 5 0 5 3 R J _ M I P n t \u0026amp; e J r u r m u p p t T _ a f b u [ n 1 c ] f f J u u u n n m c c p _ _ 0 1 T a n n b a a m m e e f R u e n a c d _ 1 b u f f e r . . . The address of jump table is fixed. So the address of the first cell \u0026amp;Jump Tab[1] is fixed too. (2025-01-17)\nExample code in Boot project:\n1 6 V e c 0 t x o 6 r 0 0 t 0 a b l e 6 0 0 0 r a _ p u p a _ ( r m f t a u 0 i n _ n c i _ n i d t s e e r f r _ i r u n u a i p r t t t i _ 0 o s _ n e t ) n x d ( ( ) ) 4 4 4 4 1 1 1 1 0 0 0 0 0 0 0 0 0 4 8 C u u a a r r t t 0 0 _ _ i i n n t t e e J r r u r r m u u p p p t t T _ _ a s r b e e n c d e _ i i v s e r _ i s r u R a e r a t d 0 ( _ b f i u u n f n t f c e e r r d r . e u . f p . i t n _ i r t e i c o e n i ) v e _ i s r Jump Tab is at the beginning of the app flash: 0x41000, i.e., ADDR_APP1_FLASH_START.\nSections setting: (in Application project)\nAddress Section 0x041000 .constfAPP_ID_f 0x041004 .constfAPP_VERSIONf 0x041008 .constfISR_JUMP_TABLE_f 0x041200 .textf The Boot project defines the same interrput functions: r_uart0_interrupt_send() and r_uart0_interrupt_receive() as the Application project.\nWhen an interrupt is triggered, the interrupt function in the Boot project will be executed.\nHowever, If I put the redefined interrupt in the Application project, there will be an error:\n1 (E)\tE0562142\tE0562142:Interrupt table address \u0026#34;0x2c\u0026#34; of \u0026#34;.vect\u0026#34; has multiple definition\tBlink_P16.mtpj I think that is because I have two same interrupt in the project.\nFor the example code, I suspected that the Boot project and the Application project are compiled separately, and then the two .hex files are flashed separately as well.\nKaushik said the example code (Dual boot) is different from the situation we are facing.\nIn our project, we do not have access to the Boot project (i.e., the Bootloader program). The customer just tell us once an interrupt is triggered, the program will go to some address, where the interrupt function should be stored.\nFor example, when the interrupt INTTM00 (whose Vector Table Address is 2C) is triggered, according to the table provided by customer, the program will jump to 0x5062.\nSo, I think a section containing a function pointer should be put at 0x5062, because I think a \u0026ldquo;function pointer\u0026rdquo; will make the program to execute a function.\nThe content stored at 0x5062 is calling the interrupt function by \u0026ldquo;de-referencing\u0026rdquo; the function pointer.\nJ u \u0026amp; \u0026amp; m f f p u u n n T c c a _ _ b 1 2 l e This section , referring to Manual of CC-RL Page 380.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026#34;utility.h\u0026#34; #pragma section INT_TIMER void __far Config_TAU0_0_interrupt(void) { /* Get the address of the relevant jump table entry */ uint32_t addr = *(__far uint32_t *)(ADDR_ISR_TIMER); /* Create a function pointer based on the jump table entry */ void (*app_main_isr_timer)(void) = (void (*)())(addr); /* Call the function */ app_main_isr_timer(); } (2025-01-22)\nUse assembly command (Kaushik found this method work after he looked at the asm file in the example project r1-Example)\n1 2 3 4 5 6 7 8 # jumptable.asm .extern _system_timer_counter ; Declare the external function .section DATA, DATA .org 0x5062 ;interrupt 0x002C -VECTN=2C=5062 BR !!_system_timer_counter ; Store the 16-bit address pointer of the interrupt function BR (branch) is used for control flow, jumping to far addresses r1-样例. (2025-01-29)\nTest LIN transmission interrupt function with Hongsheng\u0026rsquo;s example code\nReasons:\nI modified the transmission function in Hongsheng\u0026rsquo;s example code by reading the recepted data explicitly, as the reception buffer doesn\u0026rsquo;t get data if I copy the source files of LIN functions following his instructions.\nI haven\u0026rsquo;t figured out the problem of integrating the LIN functions. Maybe there is extra operation after copying source files.\nActions:\nTest the LIN function with BabyLIN Check Clock to 40 MHz; Add watchdog (Overflow time: 2^14/fWDT to make 1092, around 1000 ms; Cancel INTWDTI) Include the jumptable.asm, where the generated interrupt functions are called, with the _static decorator canclled. Compile the sample project to a .hex file Merge the bootloader and LIN app .hex Results:\nGo to exception: ff\nI included the iodefine.h into the project, but still run into ff.\nThe iodefine.h may be not necessary, and the other 3 .asm (cstart, hwinit, stkinit, generated by CS+ when creating a new project) either because cstart.asm is already in the mcu folder. So, maybe the other 2 are in somewhere else.\n(2025-01-30)\nReplace parts of the customer\u0026rsquo;s bootloader with the own project bootloader.\nReasons:\n我用 Hongsheng 项目编译生成的 bootloader 部分， 替换了客户 bootloader 中的对应部分，然后 LIN 就能工作了。 （E41_Test_Project01_20250109-Wed\\E41_Test_Project01-Wed-Copy_to_Remove_Timer\\DefaultBuild）\n然后我想看到底是哪部分的问题，Hongsheng 项目编译生成的 hex 里面 的 bootloader 部分很简单， 只是 vector table 里面有几个要跳转去的中断函数的地址， 然后在 0x0070 后面有个 Tesla bootloader 没有的 callt table （0x80-0xBF）， 然后还有开头的 4 个字节不一样，我就只把这些替换了，LIN 仍然可以工作。\nActions:\n0x00C0 is the option byte controlling watchdog timer r1-Manual. 0x00C6 - 0x00D5 is the Security ID.\nIt\u0026rsquo;s not necessary to modify the customer\u0026rsquo;s vector table. Only insert the \u0026ldquo;callt\u0026rdquo; table and change the program start address to 0x5122, and the end address to 0xFFFF\nThe program starts executing from 0x5122. 0x50E0 is the .text (code) section.\n(2025-01-31)\nCheck the address in the callt table:\nReasons:\nWhat\u0026rsquo;s the exact difference causing LIN transmission interrupt doesn\u0026rsquo;t work. Actions:\nThe area 0x80 ~ 0xBF is the addresses of functions.\naddr value functions called in app? (ED) 0x0080 0x50E0 _main Y 0x0082 0x50F8 _R_Systeminit N 0x0084 0x50FD _hdwinit N 0x0086 0x510D _bsp_init_system Y 0x0088 0x511F _bsp_init_hardware Y 0x008A 0x519D _R_BSP_GetFclkFreqHz N 0x008C 0x51B8 _start_clock N 0x008E 0x5295 _stop_clock N 0x0090 0x52E8 _set_fclk_clock_source N 0x0092 0x556C _get_fclk_freq_hz N 0x0094 0x5782 _change_clock_setting N 0x0096 0x5A1C _mcu_clock_setup N 0x0098 0x5AA3 _R_Config_PORT_Create N 0x009A 0x5AB5 _R_Config_PORT_UserInit N 0x009C 0x5AC3 _LinSendBreak N 0x009E 0x5ACD _LinSendBreakData N 0x00A0 0x5BCA _RLIN_Slave_Init N 0x00A2 0x5C24 _RLIN_Slave_HeaderReceive N 0x00A4 0x5C30 _RLIN_Slave_Transmit N 0x00A6 0x5C73 _RLIN_Slave_Receive N 0x00A8 0x5C89 _RLIN_Slave_NoResponse N 0x00AA 0x5C8E _Clear_DataBuf N 0x00AC 0x5CB7 _Get_reponse_RxData N 0x00AE 0x5CF4 _ProcessingReceivedData N 0x00B0 0x5D43 _R_Config_WDT_Create N 0x00B2 0x5D4C _R_Config_WDT_Restart N 0x00B4 0x5D50 _R_Config_WDT_UserInit N If a function is called, there should be ED (or EF) + addr\nThese 2 function _bsp_init_system and _bsp_init_hardware follows FC. Results:\nLIN funcstions are declared in the callt table. Analysis:\nThe RLIN module is added by copy \u0026amp; paste source files, instead of generated by smart configurator.\nSo, maybe those LIN functions are not stored together with other interrupt functions. Thus, the callt table is created to declare them.\nRenesas support sent a example project for RLIN3. I don\u0026rsquo;t know if using that can make the LIN functions in normal locations.\nHowever, RLIN3 is also a library, whose code won\u0026rsquo;t be generated by the Smart Configurator.\nTry to don\u0026rsquo;t use the callt table.\nReasons:\nThe 4 LIN interrupt functions can be reached by setting the jumptable.asm. Is the callt table used only to make code shorter? Actions:\nAdd __callt keyword at those functions definition. Results:\nAdding __callt will not change the address of those functions. Example UART References:\n样例程序\u0026ndash;RL78 bootloader via UART for CCRL Shared by Chinese Renesas support Notes:\nDebug can be performed by selecting the Using Debug Tool as RL78 Simulator r1-Example. E2 Lite Debugger Errors Prohibited area Problem:\n(2025-01-23)\nIn CS+, when trying to download program to MCU, there is an error:\nThe E41 project and the LIN project from Hongsheng both encounter this problem: References:\nE1203124 error on CS+ when trying to debug RL78 | Renesas Customer Hub Searched by Direct Error Cause] Writing to the on-chip debug reserved area is prohibited.(address: 0x00000002)(E1203124) in DDG Notes:\n(2025-01-24)\nSet the Linker Options: Devices \u0026gt; Set debug monitor area\nI found the in-use options are the same as the settings of Hongsheng\u0026rsquo;s example LIN project. I guess those settings are inheritated from the LIN function. But I don\u0026rsquo;t know why we can debug his LIN project correctly without modify those settings. (2025-01-26)\nThe debugger setting is set by Smart Configurator, when generating code.\nThe following prompt occurs when I generate code for a F24 project, after configuring Timer and Port modules.\nIncorrect Secu ID Problem:\nAfter I flash the bootloader provided by customer, I cannot download and debug our project due to an error: \u0026ldquo;Incorrect ID Code\u0026rdquo;\nReferences:\nError (C0602202) occurs when I debug a RH850-family MCU. (CS+, E2, E1, E20) - Renesas Electronics Searched by renesas Incorrect ID Code.(C0602202) in DDG CC-RL Compiler User’s Manual - Renesas Electronics Corporation E1/E20/E2 Emulator, E2 Emulator Lite - Additional Document for User’s Manual - (Notes on Connection of RL78) Mentioned in Writing to the on-chip debug reserved area is prohibited? - Forum - RL78 MCU - Renesas Engineering Community Searched by Direct Error Cause] Writing to the on-chip debug reserved area is prohibited.(address: 0x00000002)(E1203124) in DDG Practices:\n(2025-01-24)\n\u0026ldquo;This error message is displayed when the security ID (ID code) written in flash memory does not match the security ID specified by the debug tool.\u0026rdquo; r1-Support\nThe security id is written at addresses 0xC4 to 0xCD r3-Manual, while in the customer\u0026rsquo;s bootloader, security id is at 0xC6 to 0xD5. (2025-01-26)\nSet up Security ID in Smart Configurator and specify Security ID in the Debug Tool options.\n16 bytes (32 digits): 0x00000000000000000000FFFFFFFFFFFF (The frist 10 bytes are 0x00, and the rest 6 bytes are set to 0xFF) The smcg configure: \u0026ldquo;Do not erase flash memory data\u0026rdquo; corresponds to A5 for the \u0026ldquo;Option byte values for OCD\u0026rdquo; in the \u0026ldquo;Link Options\u0026rdquo; of CS+.\n\u0026ldquo;Erase flash ROM\u0026rdquo; can be set to \u0026ldquo;Yes\u0026rdquo; to reset the security ID, when downloading failed.\nRectify the security id in the customer bootloader program.\nThe security ID in CC-RL of old version V1.09 is of 10 bytes:\n1 :0A 00C6 0000000000000000000000 30 However, we are using a newer version CC-RL V1.15, where the length of security id can be various as long as not exceed the maximum size r2-Manual.\n1 :10 00C6 00 00000000000000000000FFFFFFFFFFFF 30 Watch Not Real Time Problems:\nThe variables added into the Watch window are not changing in real time. Notes:\n(2025/02/05)\nUse Smart Configurator to re-generate code again. Then, the variables can be updating in real time with pink color. CLOCK Problems:\nCannot download programs, prompting with an error:\n1 2 3 4 5 Error(E1200416) Download failed. [Direct Error Cause] No response from the emulation CPU. Please confimm the sighal of the CLOCK or RESET,WAIT and so on.(E1200416) Notes:\n(2025-04-09)\nThe debugger or the MCU is broken.\nSupports:\nSajad took apart the housing of the debugger and check the board, not found visible problem.\nHe was trying to update the firmware of debugger, but he found RFP.\nUse RFP to flash program as it can provide more information.\n(2025-04-15T16:37:19)\nThe same error occurs again when MCU is broken while the debug is working. Actions:\nRFP cannot flash either\n1 Error(E4000004): A framing error occurred while receiving data. (BFW: 0354) Results:\n\u0026ldquo;This debugger is completely gone,\u0026rdquo; Sajad said after seeing that error. Renesas Flash Programmer Flash Hex to Micro Problems:\nFlashing the program into the MCU allows the ECU to start automatically upon power-up, eliminating the need to connect a debugger each time to initiate execution. Notes:\nGenerate Hex file during building\nSupports:\nProject Tree -\u0026gt; CC-RL (Build Tool) -\u0026gt; Hex Output Options -\u0026gt; Output File, Hex file format: Intel HEX file Flash Hex file\nSupports:\nOpen Renesas Flash Programmer V3.17.00 -\u0026gt; File -\u0026gt; New Project\nProject Information: Microcontroller: RL78, Project Name, Project Folder.\nCommunication: E2 emulator Lite. Microcontroller type will be detected automatically: R7F123FGG\nSet \u0026ldquo;Program Files\u0026rdquo; to the target hex file.\nRemove all wires connecting with the Debugger to unhold the \u0026ldquo;Reset\u0026rdquo; pin. Even the debugger is not power on, the \u0026ldquo;Reset\u0026rdquo; still is hold.\nConnect BabyLIN and send commands.\nMerge Hex Problem:\nDownloading two hex files: one is the bootloader, the other is the application program, using software: Renesas Flash Programmer (RFP) doesn\u0026rsquo;t result in ECU running correctly.\nSpecifically, the bootloader should handle the INTTM00 interrupt and jump to the ISR in the application program to generate a blinking LED effect. However, by downloading the 2 separate hex files: LMBR_94xxxxxxx5C_P3_A1_U1_bam_boot_combined.hex and app_crc.hex, the LED is not blinking.\nWhereas if downloading the application hex file through the provided SDP tool after flashing the bootloader hex using RFP, the ECU shows up correct result: blinking LED.\nNotes:\n(2025-01-24)\nStitch the bootloader hex file and the application hex file together:\n1 2 3 4 5 6 7 8 :101ED000AAAAAAAA41268DD4FFFFFFFFFFFFFFFF9A :101EE000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF02 :101EF000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF2 :101F0000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFE1 :081F1000FFFFFFFFFFFFFFFFD1 :105000004F0003010100000055AA55AA55AA55AA50 :10501000FFFF000005000000F3500000DC800000EE :105020000000000000000000000000000000000080 Kaushik copied all content in the bootloader hex file: LMBR_94xxxxxxx5C_P3_A1_U1_bam_boot_combined.hex\nand pasted it on the top of app_crc.hex, after deleted the last line of the bootloader:\n1 2 3 4 5 6 7 8 # The original LMBR_94xxxxxxx5C_P3_A1_U1_bam_boot_combined.hex ..... :101ED000AAAAAAAA41268DD4FFFFFFFFFFFFFFFF9A :101EE000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF02 :101EF000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF2 :101F0000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFE1 :081F1000FFFFFFFFFFFFFFFFD1 :00000001FF 只需要把客户提供的 bootloader 的 hex file 和 我生成的 app code 复制粘贴到一起， 就是一份完整的 hex file，不需要使用客户提供的专用的 SDP 工具。Kaushik 把 bootloader 的 最后一行删了\nbootloader hex 和 app hex 不能是分开的两个单独的文件通过 Renesas Flash Programmer 下载， 而应该先合并（删除bootloader 的最后一行）成一个 hex，再下载\n合并之前，需要对 Application project 编译生成 Blink_P16_VT.hex 做处理， 首先需要把开头的 vector table .vect 删了， 然后补全 hex file 中的 gap，用 0 填充， 最后计算 crc，并且放到指定地址，从而得到 app_crc.hex。\n1 2 3 4 5 # app_crc.hex :105000004F0003010100000055AA55AA55AA55AA50 :10501000FFFF000005000000F3500000DC800000EE :105020000000000000000000000000000000000080 ..... Web Simulator References:\nRL78 Web Simulator Found in Software and Tools which is the parent dir of Smart Configurator Videos for RL78 Family Notes:\n(2024-11-08)\nWeb simulator was released on Nov 05. On Ubuntu (2025-05-23T18:05)\nE2 studio on Ubuntu 24.04\nReferences:\nGemini 2.5P - VS Code and Renesas CC-RL\nRenesas VS Code Extension Problems:\nUse VSCode IDE to build and debug a Renesas project Supports:\nRenesas VSC extension can monitor Local, Registers, Watch, and Call stack r1-Present ::: aside\nReferences {{{ 1. [Visual Studio Code: How to Create, Build, and Debug Smart Configurator Project for RL78 - RenesasPresents](https://youtu.be/Y_nR5R029Q4) Searched by `renesas rl78 f23 I2C IICA configuration` at [DDG Videos](https://duckduckgo.com/?origin=funnel_home_bing\u0026t=h_\u0026q=renesas+rl78+f23+I2C+IICA+configuration\u0026ia=videos\u0026iax=videos\u0026iai=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DY_nR5R029Q4) 2. [8. Quick Start for Renesas RA — Renesas VS Code Extensions 1.0.0 ...](https://www2.renesas.eu/_custom/software/ree_eclipse/vscode/docs/quick-start-ra.html) Searched by `renesas smart configurator for ubuntu` at [DDG](https://duckduckgo.com/?q=renesas+smart+configurator+for+ubuntu\u0026ia=web) }}} ::: ","date":"2024-11-06T16:24:00Z","image":"https://www.renesas.com/sites/default/files/styles/blog/public/2021-10/splash.jpg?itok=-Y3ykqCC","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-board-renesas/","title":"Memo: EE - Boards | Renesas SDK"},{"content":"(Feature image from Raspberry Pi Pinout)\nHardware Resources References:\nRaspberry Pi hardware - Raspberry Pi Documentation Searched by s3 Notes:\nOverview\nSPI References:\n树莓派（主）与STM32（从）使用SPI通信 - CSDN博客 Searched by 树莓派 SPI 实验 in DDG 树莓派 wiringPi 用户手册 - 小鹏STEM Searched by 树莓派 3B SPI 实验 in DDG Raspberry Pi Pinout Guide: How to use the Raspberry Pi GPIOs? - RNT Searched by raspberry pi 3B enable gpio in DDG 树莓派3B+ SPI Flash编程器 - 佐须之男的博客 - forgotfun.org Searched by s2 DDG Searched by in DDG Notes:\nSystem info:\nBoard: Raspberry Pi 3B OS: Ubuntu Server 24.10 Use ssh to connect the Raspberry Pi Enable SPI interface (r1-CSDN)\n1 2 sudo apt-get install raspi-config sudo raspi-config Verify SPI is enabled:\n1 2 jack@Rpi:~$ ls /dev/spi* /dev/spidev0.0 /dev/spidev0.1 GPIO pinout (r3-RNT): SPI library on Raspberry Pi: wiringPi (r2-小鹏)\nLib Spidev References:\nPython驱动树莓派SPI接口 - 简书 - 月见樽 Searched by 树莓派 3B SPI 实验 in DDG Controlling an SPI device with the Raspberry Pi - Takaitra.com Refered by r1 doceme/py-spidev - GitHub Searched by `` in DDG Searched by `` in DDG Notes:\n(2024-10-31)\nSPI library for Python: py-spidev\nCompile py-spidev from source code (r2-Takaitra)\nInstalled python3-dev ahead. Use python3 instead of python by setting the env ${PYTHON}. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 jack@Rpi:~$ cd Programs/py-spidev/ jack@Rpi:~/Programs/py-spidev$ make python setup.py build make: python: No such file or directory make: *** [Makefile:4: all] Error 127 jack@Rpi:~/Programs/py-spidev$ ls CHANGELOG.md MANIFEST.in README.md setup.py LICENSE Makefile setup.cfg spidev_module.c jack@Rpi:~/Programs/py-spidev$ cat Makefile PYTHON ?= python all: $(PYTHON) setup.py build install: $(PYTHON) setup.py install clean: $(PYTHON) setup.py clean rm -rf build dist cleandir distclean: clean $(PYTHON) setup.py clean -a jack@Rpi:~/Programs/py-spidev$ echo ${PYTHON} jack@Rpi:~/Programs/py-spidev$ export PYTHON=\u0026#39;python3\u0026#39; jack@Rpi:~/Programs/py-spidev$ echo ${PYTHON} python3 Error: gcc missed 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 jack@Rpi:~/Programs/py-spidev$ make python3 setup.py build /usr/lib/python3/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options !! ******************************************************************************** Usage of dash-separated \u0026#39;description-file\u0026#39; will not be supported in future versions. Please use the underscore name \u0026#39;description_file\u0026#39; instead. This deprecation is overdue, please update your project and remove deprecated calls to avoid build errors in the future. See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details. ******************************************************************************** !! opt = self.warn_dash_deprecation(opt, section) running build running build_ext building \u0026#39;spidev\u0026#39; extension creating build creating build/temp.linux-aarch64-cpython-312 aarch64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -fPIC -I/usr/include/python3.12 -c spidev_module.c -o build/temp.linux-aarch64-cpython-312/spidev_module.o error: command \u0026#39;aarch64-linux-gnu-gcc\u0026#39; failed: No such file or directory make: *** [Makefile:4: all] Error 1 Have use apt-file serach aarch64-linux-gnu-gcc, which lists many packages.\n1 2 3 4 5 jack@Rpi:~/Programs/py-spidev$ apt-file search aarch64-linux-gnu-gcc gcc-11: /usr/bin/aarch64-linux-gnu-gcc-11 gcc-11: /usr/bin/aarch64-linux-gnu-gcc-ar-11 gcc-11: /usr/bin/aarch64-linux-gnu-gcc-nm-11 ... Need to install gcc, the default version is 14\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 jack@Rpi:~/Programs/py-spidev$ dpkg -l gcc dpkg-query: no packages found matching gcc jack@Rpi:~/Programs/py-spidev$ gcc -v Command \u0026#39;gcc\u0026#39; not found, but can be installed with: sudo apt install gcc jack@Rpi:~/Programs/py-spidev$ sudo apt install gcc Installing: gcc Installing dependencies: cpp gcc-14-aarch64-linux-gnu libhwasan0 libtsan2 cpp-14 gcc-aarch64-linux-gnu libisl23 libubsan1 cpp-14-aarch64-linux-gnu libasan8 libitm1 cpp-aarch64-linux-gnu libcc1-0 liblsan0 gcc-14 libgcc-14-dev libmpc3 Suggested packages: cpp-doc gcc-multilib libtool gdb gdb-aarch64-linux-gnu gcc-14-locales autoconf flex gcc-doc cpp-14-doc automake bison gcc-14-doc Summary: Upgrading: 0, Installing: 18, Removing: 0, Not Upgrading: 2 Download size: 45.1 MB Space needed: 148 MB / 57.1 GB available Continue? [Y/n] ... jack@Rpi:~/Programs/py-spidev$ gcc -v Using built-in specs. COLLECT_GCC=gcc COLLECT_LTO_WRAPPER=/usr/libexec/gcc/aarch64-linux-gnu/14/lto-wrapper OFFLOAD_TARGET_NAMES=nvptx-none OFFLOAD_TARGET_DEFAULT=1 Target: aarch64-linux-gnu Configured with: ../src/configure -v --with-pkgversion=\u0026#39;Ubuntu 14.2.0-4ubuntu2\u0026#39; --with-bugurl=file:///usr/share/doc/gcc-14/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2,rust --prefix=/usr --with-gcc-major-version-only --program-suffix=-14 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/libexec --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-libstdcxx-backtrace --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-offload-targets=nvptx-none=/build/gcc-14-T7YiXd/gcc-14-14.2.0/debian/tmp-nvptx/usr --enable-offload-defaulted --without-cuda-driver --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-serialization=2 Thread model: posix Supported LTO compression algorithms: zlib zstd gcc version 14.2.0 (Ubuntu 14.2.0-4ubuntu2) After make succeeded, sudo make install executes setup.py where python appears as well.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jack@Rpi:~/Programs/py-spidev$ sudo make install python setup.py install make: python: No such file or directory make: *** [Makefile:7: install] Error 127 jack@Rpi:~/Programs/py-spidev$ echo ${PYTHON} python3 jack@Rpi:~/Programs/py-spidev$ cat -n Makefile | sed -n 1,7p 1 PYTHON ?= python 2 3 all: 4 $(PYTHON) setup.py build 5 6 install: 7 $(PYTHON) setup.py install I saw the fist line of setup.py is #!/usr/bin/env python, so I suspected the python here also needs to \u0026ldquo;remap\u0026rdquo; to python3.\nCreate an alias python for python3 doesn\u0026rsquo;t work for the ``\n1 2 jack@Rpi:~/Programs/py-spidev$ sudo apt install neovim jack@Rpi:~/Programs/py-spidev$ nvim ~/.bashrc Append a command at the end of the ~/.bashrc:\n1 2 # (2024-10-31) alias python=\u0026#34;python3\u0026#34; Still cannot override python even I have set the environment variable:\n1 2 3 4 5 6 7 8 9 jack@Rpi:~/Programs/py-spidev$ export PYTHON=\u0026#39;python3\u0026#39; jack@Rpi:~/Programs/py-spidev$ sudo make install python setup.py install make: python: No such file or directory make: *** [Makefile:7: install] Error 127 jack@Rpi:~/Programs/py-spidev$ echo ${PYTHON} python3 However, sudo make install PYTHON=python3 works, as suggested by ChatGPT.\n1 2 3 4 5 6 7 8 ... Processing spidev-3.6-py3.12-linux-aarch64.egg creating /usr/local/lib/python3.12/dist-packages/spidev-3.6-py3.12-linux-aarch64.egg Extracting spidev-3.6-py3.12-linux-aarch64.egg to /usr/local/lib/python3.12/dist-packages Adding spidev 3.6 to easy-install.pth file Installed /usr/local/lib/python3.12/dist-packages/spidev-3.6-py3.12-linux-aarch64.egg Processing dependencies for spidev==3.6 Demo Code (2024-11-01)\nCode: \u0026ldquo;spi_test.py\u0026rdquo; (r2-Takaitra)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import spidev import time spi = spidev.SpiDev() spi.open(0, 0) # arguments: (bus, device), i.e., (SPI0, CS0) spi.max_speed_hz = 7629 spi.mode = 0b01 # [CPOL | CPHA] # Split an integer input into a two byte array to send via SPI def write_pot(input): msb = input \u0026gt;\u0026gt; 8 # Shift the higher 8 bits to the lower half lsb = input \u0026amp; 0xFF # Only keep the lower 8 bits spi.xfer([msb, lsb]) # Repeatedly switch a MCP4151 digital pot off then on while True: write_pot(0x1FF) time.sleep(0.5) write_pot(0x00) time.sleep(0.5) Execute this script, the program will keep running forever:\n1 python3 spi_test.py Pinout References:\nRaspberry Pi hardware - Raspberry Pi Documentation Searched by raspberry pi 3B pinout in DDG Raspberry Pi Pinout Found in video: Windows下通过串口连接树莓派 Raspberry Pi (USB to TTL, PuTTY) - bilibili - 老刘玩机 Searched by 树莓派3D 串口 实验 in bilibili Searched by in DDG Notes:\n(2024-11-01)\nCheck pinout with command pinout\n1 2 sudo apt install python3-gpiozero pinout (2024-11-29)\nThe serial (TX and RX) on Model 3B are turned off by default, because they are occupied by the Bluetooth r2-bilibili. UART Comm With Arduino References:\nHow to read data through UART - Raspberry Pi Forums Searched by raspberry pi uart receive and display on terminal in Google 树莓派4 UART 多串口配置通信 | 树莓派实验室 Searched by 树莓派 UART in Google UART配置- 树莓派中文文档 Searched by in s2 Searched by in DDG Notes:\n(2024-11-29)\nArduino: Serial.write(), and Raspberry Pi print the received message in terminal r1-Forum. (2024-12-05)\nThe pin: TX1 and RX0 both are 5V, whereas the UART on Rpi is 3.3V. So they cannot connect directly r3-Hackpi. ","date":"2024-10-31T19:13:00Z","image":"https://raw.githubusercontent.com/pinout-xyz/Pinout.xyz/master/resources/raspberry-pi-pinout.png","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-board-raspi/","title":"Memo: EE - Boards | Raspi As MCU"},{"content":"Chat Assistant References:\nAnsys把大模型GPT和自家的产品结合了，已发布AnsysGPT - bilibili - 曾导SJTU Posted by user: 曾导SJTU Ansys AI：以 AI 的速度改变仿真 - Ansys中国 Directed by video: Fluent GPU求解器:前所未有的速度和规模的CFD研究 | Simulation World - Ansys中国 Notes:\n(2024/10/27)\nAnsysGPT is trianed based on documents. (r1-Bili) ","date":"2024-10-27T14:48:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/simu/c-coll-indsimu/","title":"Coll: MPFS - Ind | AI in Simulation Softwares"},{"content":"Feature image from Arduino - LED - Blink | Arduino Tutorial - Arduino Getting Started (Searched by arduino blink led experiment in DDG )\nBurn LED (2024-10-28)\nI broke an LED because I connected the LED directly with 3.3 V without using a current-limiting resistor.\nA typical current through an LED is Blink Nano Can\u0026rsquo;t Upload References:\n电子教学 - 【教学】Arduino上传失败的24种解决方法 - bilibili - joyspace Searched by Arduino烧录失败 in bilibili avrdude: ser_open (): can\u0026rsquo;t open device \u0026ldquo;\\.\\COM4\u0026rdquo;: acces denied Searched by avrdude: ser_open(): can't open device \u0026quot;\\\\.\\COM4\u0026quot;: Access is denied. in DDG CH340 Windows 10 driver download | arduined.eu Searched by Arduino Nano CH340 in DDG Uploading a simple sketch takes forever - arduino uno - SE Searched by arduino IDE uploading long time in DDG (2024-10-29)\nProblem: Cannot upload firmware to board: Arduino Nano\nHardware connection: Plug USB cable barely between the Nano board and laptop.\nBoard and Channel selection:\n![img](Ardu-Test-Blink-04-USB_COM4-2024-10-29 000412.png)\nMy own sketch cannot be uploaded to board.\nThe \u0026ldquo;Uploading\u0026rdquo; reminder hangs forever, and finally the following errors prompted:\n1 2 3 4 5 6 7 8 9 Sketch uses 924 bytes (3%) of program storage space. Maximum is 30720 bytes. Global variables use 9 bytes (0%) of dynamic memory, leaving 2039 bytes for local variables. Maximum is 2048 bytes. avrdude: stk500_recv(): programmer is not responding avrdude: stk500_getsync() attempt 1 of 10: not in sync: resp=0xaa avrdude: stk500_recv(): programmer is not responding avrdude: stk500_getsync() attempt 2 of 10: not in sync: resp=0xaa ... # repeated messages avrdude: stk500_recv(): programmer is not responding avrdude: stk500_getsync() attempt 10 of 10: not in sync: resp=0xaa The official example (\u0026lsquo;File\u0026rsquo; -\u0026gt; \u0026lsquo;Examples\u0026rsquo; -\u0026gt; \u0026lsquo;01.Basics\u0026rsquo; -\u0026gt; \u0026lsquo;Blink\u0026rsquo;) cannot be uploaded either\n1 avrdude: ser_open(): can\u0026#39;t open device \u0026#34;\\\\.\\COM4\u0026#34;: Access is denied. \u0026lsquo;Serial Plotter\u0026rsquo; and \u0026lsquo;Serial Monitor\u0026rsquo; both don\u0026rsquo;t return anything. Sometimes, it showed \u0026lsquo;Not connected. Select a board and a port to connect automatically.\u0026rsquo;.\nAdressing\nInstall driver for CH340 chip.\nReasons\nSomeone mentioned this driver in his question (r2-Forum).\nVideo mentioned this chip (r1-bili).\nMy Nano board does have this chip. Back Front Kaushik mentioned that maybe I dont have the correct driver installed.\nActions:\nI checked the driver in \u0026lsquo;Device Manager\u0026rsquo; before installation (r3-eu), and found it had been installed:\nResults:\nI didn\u0026rsquo;t install the driver right away, becaues I also noticed NoMachine (remote desktop) also use USB. Analysis: Maybe the NoMachine occupies some resources?\nClose the remote connection of NoMachine.\nResults: After relaunching the Arduino IDE, the \u0026lsquo;Uplong\u0026rsquo; still can\u0026rsquo;t complete. Try to Update the driver, as suggested by (r3-eu)\nActions:\nClick \u0026ldquo;Update\u0026rdquo;\nResults:\nIt\u0026rsquo;s already installed:\nInstall the driver again by using CH341SER\\SETUP.EXE\nResults:\nFailed:\nQuit NoMachine first, then install SETUP.EXE:\nActions:\nQuit NoMachine service: Tray menu Confirm quit Results:\nDriver installation still failed. Uninstall NoMachine and restart the laptop.\nActions:\nCheck the ports. There is only one port: COM3\nResults:\nCannot upload the firmware. Plug the cable into the other USB slot, whose index is COM5:\nActions:\nPlug into another USB\nResults:\nDoesn\u0026rsquo;t work either. Reinstall the driver SETUP.EXE,\nResults:\n1.No luck: \u0026ldquo;Driver install failure!\u0026rdquo; persists.\nSelect another processor: ATmega328P (Old Bootloader) instead of ATmega328P\nReasons:\nGlipsed an answer that mentioned to use the old version (r4-SE)\nThe combination of my previous trials is:\nResults:\nIt works!\nThe driver apperas again automatically, though I didn\u0026rsquo;t install it manually.\nConclusion:\nThe problem is not the CH340 driver, but the processor. LED Chaser References:\nArduino基础课程01——初始Arduino - bilibili - funcodecc Directed by Arduino基础课程09——SPI通信 Arduino Tutorial: LED Sequential Control- Beginner Project - YouTube - Drone How Searched by arduino experiments flowing led in Google video Materials:\nRecite the code after the first watch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 void setup() { pinLed[] = {1,2,3,4}; // Indices of pins to be used numLed = 4 // Configure pins // Specify Mode for pins: INPUT or OUTPUT // Set every pin to OUTPUT to drive an LED for {i=1, i\u0026lt;=numLed, i++} { pinMode(pinLed[i], \u0026#39;OUTPUT\u0026#39;); // Input 2 args: pin id and mode } } void loop() { for{i=0; i\u0026lt;=numLed; i++} { digitalWrite(pinLed[i], HIGH); // Two levels of voltages delay(500); // ms digitalWrite(pinLed[i], LOW); } for{i=numLed, i\u0026gt;0; i--} { digitalWrite(pinLed[i], HIGH); delay(500); digitalWrite(pinLed[i], LOW) } } My Mistakes:\nOUTPUT as a pinMode seems to be a global variable, NOT a string.\nData types are missed when create a variable: int numLed = 4.\nThe pin-1 of UNO cannot be set as OUTPUT? The pins used are started from pin-2: int pinLed[] = [2,3,4,5]\nThe arguments of syntax for are separated by ; and enclosed by parentheses ( ), not curly braces. And the temporary counter i also needs declaration of data type.\ni is the index of the array, ranging from 0-3.\nDon\u0026rsquo;t forget semicolons at the end of each line;\nVariables used both by setup() and loop(), i.e., numLed and pinLed should be declared outside of these functions.\nDon\u0026rsquo;t forget current-limit resistor. There is a warning from the simulation platform (funcode.cc): \u0026ldquo;The maximum current allowed by led-node is: 0.02\u0026rdquo;\nRectified code for Arduino Nano:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 int pinLed[] = {10,11,12,13}; int numLed = 4; void setup(){ // Set the mode of each pin to OUTPUT for(int i=0; i\u0026lt;=numLed; i++){ pinMode(pinLed[i], OUTPUT); } } void loop(){ // From pin-2 to pin-5 for(int i=0; i\u0026lt;=numLed; i++){ digitalWrite(pinLed[i], HIGH); delay(500); digitalWrite(pinLed[i], LOW); } // From pin-5 to pin-2 for(int i=numLed-1; i\u0026gt;=0; i--){ digitalWrite(pinLed[i], HIGH); delay(500); digitalWrite(pinLed[i], LOW); } } Mount components onto breadboard, and upload the firmware to board. Circuit Demostration https://i.ibb.co/PCqxrB6/Ardu-Test-Blink-Nano-Breadboard-2024-10-29-115901.gif Practice:\nUse two pins: #2 and #3 to light two LEDs alternatively. Each LED is on for 0.5 seconds and off for 0.5 seconds.\nCode:\nUse for to configure each pin 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int pinLed[] = {2,3}; int numLeds = 2; void setup() { for(int i=0; i\u0026lt;numLeds; i++){ pinMode(pinLed[i], OUTPUT); } } void loop() { for(int i=0; i\u0026lt;numLeds; i++){ digitalWrite(pinLed[i], HIGH); delay(500); digitalWrite(pinLed[i], LOW); delay(500); } } Simulation platform: funcode.cc\nPiezo Simulation References:\nDetect a Knock - Arduino Docs Found in Home / Programming / Built-in Examples - Arduino Piezo sensor with Arduino UNO - How does work Piezo sensor (Code and Circuit Diagram) - YouTube - uSriTu Hobby Searched by Piezo electric disc experiments with Arduino in DDG video Arduino Workshop-Piezo Knock Sensor - Hackster.io Found in s2 Materials:\n(2024-10-25)\nSimulation platform: SPI LED Array References:\nMajicDesigns/MD_Parola Was looking for examples directly using the generic library \u0026lt;SPI.h\u0026gt; instead of calling sensor-specific library. Although Wokwi supports some devices, most of them uses derived libraries. I found wokwi-max7219-matrix Dot Matrix Reference - Wokwi Docs when browsing the Docs of Supported Hardware -\u0026gt; Diagram Reference. One of its \u0026lsquo;Simulator examples\u0026rsquo; is Dot Matrix Clock, which writes #include \u0026lt;SPI.h\u0026gt; explicitly. Notes:\n(2024-10-31)\nA library controlling LED array using SPI interface. Serial References:\nArduino基础课程07——串行通信 - bilibili - funcodecc Universal Asynchronous Receiver-Transmitter (UART) | Arduino Documentation Searched by Arduino sends and receives character via UART in DDG serial - \u0026lsquo;Serial1\u0026rsquo; was not declared in this scope - Arduino Stack Exchange Searched by Arduino Serial exit status 1 Compilation error: 'Serial1' was not declared in this scope in DDG \u0026lsquo;Serial1\u0026rsquo; was not declared in this scope - Arduino Forum Found in s3 Adding More Serial Ports to your board. - Arduino Docs Refered by r3-SE Arduino项目实战_第四课_串口通信_(3)软串口与双机通信_哔哩哔哩_bilibili - 电子疯狂创客 Searched by Arduino nano 串口实验 site:bilibili.com in DDG Notes:\nJust connect the Nano board with laptop with a USB cable, then upload the code (r1-Bili):\n1 2 3 4 5 6 7 8 void setup() { Serial.begin(9600); } void loop() { Serial.println(\u0026#34;Hello\u0026#34;); delay(1000); } (2024-11-29)\nSerial1 doesn\u0026rsquo;t exist on Nano or Uno. Nano only has one serial port, called Serial.\nI think this Serial is not only connected to pins: TX1 and RX0, but also the USB port.\nNano has NO pins for Serial1 r4-Forum,r2-Docs\nHowever, it can be instantiated with the SoftwareSerial library r3-SE to extend the RX and TX to simulate another serial port on other pins r5-Tutorial:\n1 2 3 4 5 6 7 8 #include \u0026lt;SoftwareSerial.h\u0026gt; SoftwareSerial Serial1(3,2); //Rx to pin D3, Tx to pin D2 void setup() { Serial.begin(9600); // Initialize the Serial monitor for debugging Serial1.begin(9600); // Initialize Serial1 for sending data } (2024-12-05)\nThe SoftwareSerial is an individual serial port, which can do the complete serial function by iteself.\nReasons:\nDo not mix up the pins of the generic serial (TX1 \u0026amp; RX0). The following image shows two boards are connected with their SoftwareSerial r6-bili:\nSerial.print() will send (wirite) data to the bus.\nActions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #include \u0026lt;SoftwareSerial.h\u0026gt; SoftwareSerial Sserial1(3,2); // RX, TX String str; char c; void setup() { Sserial1.begin(9600); Serial.begin(9600); } void loop() { /* Check the buffer of the SoftwareSerial */ while (Sserial1.available()) { c = Sserial1.read(); /* read 1 byte */ str +=c; if (c == \u0026#39;\\n\u0026#39;) { /* Use original serial to sent to PC */ Serial.print(str); str = \u0026#34;\u0026#34;; /* Reset str */ } } /* Use the SoftwareSerial to transmit str */ while (Serial.available()) { c = Serial.read(); // Recv str from PC str += c; if (c == \u0026#39;\\n\u0026#39;) { // Ended with newline /* Use SoftwareSerial to sent to other dev */ Sserial1.print(str); str = \u0026#34;\u0026#34;; } } } Results:\nThe code in Arduino Docs is equivalent besides the additional SoftwareSerial usage. Pinout References:\nPIN number to use with arduino nano - Arduino Forum Searched by arduino nano pin numbers in DDG Searched by in Notes:\nA pin should be referred to in the program by the number in its name.\nAlthough the D1 and A1 have the same \u0026ldquo;number\u0026rdquo; 1, but they are gotten controlled with two totally different functions: digitalWrite() and analogWrite().\nThe name D13 means the digital pin-13. The combination of a number plus a function digitalWrite/analogWrite is unique; there is no confusion.\n1 2 3 int pinLed = 13; pinMode(pinLed, OUTPUT); digitalWrite(pinLed, LOW); Do not get confused by other images labeling \u0026ldquo;pin number\u0026rdquo;, which are only for breadboard (r1-Forum), such as this image for \u0026ldquo;Nano Every\u0026rdquo;. For the SS pin, I should look up for D10 pin on the board and use digitalWrite() to control it:\n1 2 3 int ssPin = 10; pinMode(ssPin, OUTPUT); digitalWrite(ssPin, LOW); ","date":"2024-10-25T11:29:00Z","image":"https://arduinogetstarted.com/images/tutorial/arduino-led-wiring-diagram.jpg","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-test-arduino/","title":"Test: EE - Boards | Arduino Beginners"},{"content":"(Feature image from: Types of Arduino Boards – Quick Comparison on Specification and Features - Circuit Digest Searched by arduino series in DDG image)\nSimulation Platforms ::: aside References:\n如何在线虚拟仿真Arduino？Arduino仿真软件介绍!_哔哩哔哩_bilibili Searched by arduino 仿真实验室 in DDG Arduino 用Proteus仿真基础知识和实例 - CSDN博客 (Found in s1) 欢迎使用Wokwi! | Wokwi Docs UnoArduSim：无需Arduino板即可学习Arduino编程和调试代码的模拟器 - 知乎 【工具推荐】Arduino仿真工具合辑 - CSDN博客 Arduino 仿真 - 小鹏STEM Best Arduino Simulators (Online \u0026amp; Offline): Our 11 Picks Searched by Simulator for Arduino in DDG Fritzing电路仿真 - 萧驭 - 博客园 Searched by fritzing 仿真 in DDG Arduino Workshop-Piezo Knock Sensor - Hackster.io Searched by Piezo electric disc experiments with Arduino in DDG Supported Hardware - Docs - Wokwi Build a circuit - Fritzing Searched by fritzing breadboard in DDG ::: Notes:\n(2024-10-23)\nTinkercad\nCircuits on Tinkercad - Tinkercad (r1-bili) (2024-10-25)\nAble to simulate Piezo element.\nAble to simulate Force Sensor (Prompted by keyword: \u0026ldquo;pressure\u0026rdquo;)\nProteus\nIt can do simulation for UNO. (r2-csdn)\nInput .hex file generated from Arduino IDE. (r5-csdn\nWokwi\nExample of using I2C to read an acceleromotor: wokwi-mpu6050 6-Axis Accel \u0026amp; Gyro Sensor\nDon\u0026rsquo;t know how to simulate the communcation between two Arduino boards, as there is only one file: sketch.ino in the code editor.\nDoesn\u0026rsquo;t have Piezo elements for simulation.\nIt has a piezoelectric buzzer: wokwi-buzzer Reference - Wokwi Docs\nDoesn\u0026rsquo;t have oscilloscope.\nIt has Logic Analyzer for learning I2C protocol. (Found when browsing the Docs)\nMost of the \u0026lsquo;Supported Hardware\u0026rsquo; have associate libraries. (r10-Wokwi)\nUnoArduSim Fritzing (2024-10-24)\nFritzing\nMany hardware breadboard illustions (Piezo-demo) has a designation of this software.\nSome review mentioned its simulation is so-so. (Frit-Cnblogs)\nVersions after 1.0.0 are not free. I downloaded: fritzing.0.9.3b.64.pc.zip from a 3rd party website.\n(2024-12-03)\nThe component: Mystery Part is used for searching a part that has the same properties you set to the \u0026ldquo;Mystery Part\u0026rdquo; r11-Circuit. Import Components to Fritzing Problems:\nHow to add SparkFun components to fritzing? ::: aside\nReferences {{{ 1. [sparkfun/Fritzing_Parts](https://github.com/sparkfun/Fritzing_Parts) }}} ::: Supports:\n\u0026ldquo;Core\u0026rdquo; \u0026ndash;\u0026gt; \u0026ldquo;Import\u0026rdquo; r1-add ::: aside\nReferences {{{ 1. [How to add a new part to Fritzing? - Wiztaqnia](https://www.wiztaqnia.com/how-to-add-a-new-part-to-fritzing/) Searched by `add external part to fritzing` at [DDG](https://duckduckgo.com/?q=add+external+part+to+fritzing\u0026ia=web) }}} ::: Boards Nano References:\nNano - Arduino Docs Searched by arduino nano pins in DDG Master Your Arduino Nano Pins with This Handy Guide! - YouTube - The Last Outpost Workshop Found in s1\nFront\n* Image from [Arduino Nano — Arduino Official Store](https://store.arduino.cc/products/arduino-nano) * Searched by `arduino nano pins` in [DDG image](https://duckduckgo.com/?q=arduino+nano+pins\u0026iax=images\u0026ia=images) Pinout\nNotes:\n(2024-10-28)\nNano\u0026rsquo;s pin-13 is the built-in on-board LED (r1-Docs).\nPins of Nano:\n(2024-10-29)\nA stands for Analog;\nD stands for Digital;\nThe bi-directional pins can be set as INPUT or OUTPUT modes;\nVIN and Micro USB port CANNOT simultaneously plug in, as they both are input source.\nVIN should be input 7 - 12 V voltage. RX and TX can be used as INPUT/OUTPUT pins, as long the Serial library is not called.\n","date":"2024-10-23T22:04:00Z","image":"https://circuitdigest.com/sites/default/files/field/image/TYPES%20OF%20ARDUINO%20BOARDS.jpg","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-board-arduino/","title":"Memo: EE - Boards | Arduino Products Review"},{"content":"Study Tips (2024-10-16)\n嵌入式的学习笔记要 截图 如何做单片机学习笔记【洋桃大百科P005】 - bilibili - 洋桃电子 Simulation Circuit (2024-10-17)\nRef: 推荐一款电路仿真软件 - 李皆宁 (Searched by \u0026ldquo;I2C 电路仿真\u0026rdquo; in bilibili) LTspice (2024-10-17)\nRef:\nLTspice官方教程 | 运放电路仿真实战系列（全16节） - 亚德诺半导体 (Recommended after v1) Proteus crack References:\nProteus Pro 8.18 Crack With License Key [Latest 2024] - GitHub Gist Searched by Proteus crack in DDG 傻瓜式手把手教你怎么安装Proteus8.15以及软件汉化_proteus8.15汉化-CSDN博客 Searched by 下载安装破解 Proteus in DDG Notes:\n(2024-10-27)\nSteps:\nDownload Proteus 8.17 SP5\nFree trial ( Proteus Design Suite 8.17 SP5 (Build 39395)) doesn\u0026rsquo;t ask to input \u0026ldquo;locally installed license key\u0026rdquo;, as shown in crack-CSDN.\nSo, the \u0026lsquo;Proteus 8 Demonstartion\u0026rsquo; may not the full version software: \u0026lsquo;Proteus 8 Professional\u0026rsquo;.\nI selected \u0026ldquo;Typical\u0026rdquo; installation. ","date":"2024-10-17T20:29:00Z","permalink":"http://blog.zichen.uk/post/writenotes/embed/c-cad-cad_tools/","title":"Coll: EE - ECAD | Dev Tools"},{"content":" Forgot which video I watched on bilibili, where a related video is recommended: \u0026ldquo;【嵌入式攻城狮必备知识】韦东山手把手带你如何看懂硬件、看懂原理图\u0026rdquo; Searches:\nSearch: \u0026ldquo;如何看懂电路图\u0026rdquo; in bilibili\n如何看懂电路图？如何画原理图？【极速入门数模电路P02】 - 洋桃电子 ","date":"2024-10-16T11:02:00Z","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-cad-read_schematics/","title":"Memo: EE - ECAD | Read Schematics"},{"content":"Modes (2024-10-15)\nReferences:\nStart: I2C is open-drain I2C Communication Protocol - GfG\nSearch: \u0026ldquo;Open-drain wire\u0026rdquo; in DDG\nOpen Drain : Definition, Configuration and Open Drain GPIO - ElProCus\n上拉电阻的通俗解释，你真正知道吗？ - 爱上半导体\n单片机怎么输出高电平！推挽输出和开漏输出最本质的区别？ - 爱上半导体\n推挽 开漏 高阻 这都是谁想出来的词？？ - 工科男孙老师\nNotes:\nFlexibly leverage the \u0026ldquo;switch\u0026rdquo; characteristic of transistors (MOS)\nWhen the switch is connected, its resistance is pretty small.\nHowever, when the switch is disconnected, it\u0026rsquo;s regarded as an open circuit as its resistance is very high.\nBased on the Ohm\u0026rsquo;s law to resistors in series or parallel, the voltage on GPIO can be adjusted by manipulting the equivalent resistance value around the GPIO.\n(2024-10-30)\n开关是智能控制的关键，二极管不是开关，要想让二极管变成开关，必须要能够自由的控制给二极管通电的方向，所以三级管就被发明了，三极管是两个二极管 Review series and parallel connections of resistors. 10.3: Resistors in Series and Parallel - Physics LibreTexts\nIn a series circuit, the current is the same in each resistor. In a parallel circuit, the potential drop across each resistor is the same. Why is there a potential drop across a resistor? Energy loss? MOSFET References:\n【硬核科普】带你认识CPU第00期——什么是MOSFET - 硬件茶谈 Searched by mosfet in bilibili Notes:\n(2024-11-05)\nA MOSFET = two PN junction + a pair of electrode plates.\nThe 2 PN junctions are side-by-side.\nThe 2 plates hold a electric field.\n⊝ ","date":"2024-10-15T12:03:00Z","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-board-gpio/","title":"Memo: EE - Circuits | GPIO"},{"content":"(Feature image from: YuanBiju/serial-communication-protocols Searched by Serial communication protocol in DDG image)\nI2C Bus Comm Protocol I2C: Inter-Integrated Circuit S S C D L A V B c u c s ▮ ▮ ᴵ M ² C ꟲ U ᴵ L ² C ꟲ D 2 ᴵ 4 ² C ꟲ 0 2 ᴵ G ² P ꟲ S S e ᴵ n ² s ꟲ o r s Master and Slave\nAllow multiple master, however, only 1 master at a time\nThe first device making a falling edge to the bus while the SCL is in high stage (a start signal) will occupy the bus exclusively, until this device making a rising edge when the SCL is high.\nTwo wires: SCL (serial clock) and SDA (serial data)\nThe SCL signal determines when to sample the SDA.\nTherefore, the slave can stall the master by holding the SCL line low longer. (r4-GfG) Default (recessive, idle) state is a high voltage (1) because of the pull-up resistors.\n::: aside\nReferences: {{{ I2C Communication Protocol - GeeksforGeeks Searched by i2c protocol and UML sequence diagram in DDG }}} ::: Serial transmission: the signal on the SDA is transferred byte-by-byte\nThere is a Receiver response after every byte (8 bits).\nFollowing the Start condition, the first byte is a 7-bit device address plus a Read/Write bit, followed by a ACK bit from the receiver. The second byte is an address of a memory in the device, followed by a ACK bit from the receiver. The third byte is a 1-byte data to be transferred, followed by a ACK bit from the receiver.\nHigher bits are transmitted first\nSingle data wire (Half-duplex)：Transmit and receive share a common wire.\nBidirectional, i.e., the data flow direction can flip between master and slave.\nMaster and slave cannot talk at the same time (r2-半双工).\nCommunication means just 2 operations: Write or Read. The master sends out an instruction of writing data to a device, or requesting data from a device.\n::: aside\nReferences {{{ 2. [单工半双工全双工区别，通俗易懂 - bilibili](https://www.bilibili.com/video/BV1pY41137dN/) Searched by `半双工` in [DDG](https://duckduckgo.com/?q=%E5%8D%8A%E5%8F%8C%E5%B7%A5\u0026ia=web) }}} ::: Bus status 4 Phases\nTransferring starts when the bus appears that: the SDA falls down when the SCL is a high voltage.\nTransferring stops when the bus shows that: the SDA raises up when the SCL is a high voltage.\nData transmission: SDA makes change when SDL is low, and keeps consistant during SDL is high (r5-CSDN).\nReceiver Response: Every 8 bits (1 byte), the SDA should be a low voltage, representing ACK, whereas a high voltage (recessive) indicates NACK: the byte just sent was not received successfully.\n::: aside\nReferences {{{ 5. [Proteus仿真之IIC通信(AT24C02) - CSDN博客](https://blog.csdn.net/qq_43213240/article/details/131170778) Found in [I2C-simu-s1](#I2C-simu-s1) (2024-10-28) }}} ::: Frame Structure Start condition: SDA drops turnning to dominant level (0), triggering the SCL to oscillate.\nDevice Address: 7 bits (allowing max 128 devices mounted on the I2C bus)\nThe voltage on SDA needs to keep consistant during the entire high phase of the SCL.\nThe frame will be broadcasted and captured by all slaves, and only the matched device will response.\nRead / Write: 1 bit, 0 for Writing and 1 for Reading.\nWhen the master is sending data to the slave, SDA is 0 (dominant) at this bit;\nWhereas 1 indicates the master is receiving data from the slave.\nS S C D L A ▲ ┊ ┊ ┊ ┊ * ▲ ┊ ┊ ┊ * ┄ ┄ ┄ ┄ ┄ ┄ ┄ ┄ I ┄ ┄ ┄ ┄ d ┄ ┄ ┄ ┄ l ┄ ┄ ┄ ┄ e ┄ ┊ ┊ ┊ ┊ ┊ ┊ ┊ ┊ ┄ ┊ ┊ ┊ ┄ ┄ S r S ┄ ┄ t t ┄ ┄ a ┊ ┊ ┊ ┊ ┊ ┊ ┊ ┊ ┊ ┊ ┊ ┊ 7 1 ┊ ┊ : : 6 0 : : : : 5 1 : : : : 4 0 : : : : 3 0 : : : : 2 1 : : : : 1 0 : : : : 0 1 : : A : : c 0 R s p k : : : : P S t p : : Transferring one byte needs 9 bits. In contrast, the data field of a LIN frame is 10 bits.\nI noticed that the voltage change representing a 1-byte data only occurs when the SCL is low.\nIs that the reason for designing the start and stop conditions as the voltage changes occur when SCL is high?\nIn other words, the occurence of the \u0026lsquo;voltage change\u0026rsquo; on SDA differentiate the two fields: Start/Stop bits and data/ACK field.\nThere is NO repeated Start/Stop bits between bytes transferring, i.e., the next byte will be directly transferred after the ACK bit, as shown in the following wave shape (r3-EEWorld):\nThere is an interrupt after ACK bit r6-SE:\nThe image was also referenced by r5-CSDN ::: aside\nReferences {{{ 1. [树莓派开发I2C介绍 - 石然老师](https://www.bilibili.com/video/BV11F411g7cN/) Searched by `树莓派 i2c 通信` in [Bilibili](https://search.bilibili.com/all?vt=48473987\u0026keyword=%E6%A0%91%E8%8E%93%E6%B4%BE%20i2c%20%E9%80%9A%E4%BF%A1\u0026from_source=webtop_search\u0026spm_id_from=333.1007\u0026search_source=5) 3. [示波器和I2C时序波形图的关系分析-电子工程世界](https://news.eeworld.com.cn/Test_and_measurement/ic486741.html) Searched by `I2C 示波器 波形 电子发烧友` in [DDG Images](https://duckduckgo.com/?q=I2C+%E7%A4%BA%E6%B3%A2%E5%99%A8+%E6%B3%A2%E5%BD%A2+%E7%94%B5%E5%AD%90%E5%8F%91%E7%83%A7%E5%8F%8B\u0026iax=images\u0026ia=images) Referenced by [r1-Bili](#I2C-r1) 6. [What determines I2C interrupt service time after address byte ACK? - SE](https://electronics.stackexchange.com/q/719068) Found by search [image](#I2C-wav-r5) in Google }}} ::: Write-Data Frame References:\n4分钟看懂！I2C通讯协议 最简单的总线通讯！ - 爱上半导体 Searched by I2C in Bilibili Notes:\nExample of writing a 1-byte data into an EEPROM chip.\n┌ │ └ ─ S ─ S t a r t ┬ │ ┴ ─ 1 ─ ┬ │ ┴ ─ 0 ─ o ┬ │ ┴ n ─ 1 ─ S A ┬ │ ┴ l d 0 t ─ 0 ─ a d x h ┬ │ ┴ v r A e ─ 0 ─ e e 5 ┬ │ ┴ s b ─ 1 ─ s u ┬ │ ┴ s ─ 0 ─ ┬ │ ┴ ─ 1 ─ ┬ │ ┴ ─ 0 ─ W r i t e ┬ │ ┴ ─ 0 ─ R s p ┬ │ ┴ ─ 0 ─ ┬ │ ┴ ─ 0 ─ i ┬ │ ┴ n ─ 0 ─ M A ┬ │ ┴ e d 0 t ─ 0 ─ m d x h ┬ │ ┴ o r 0 e ─ 0 ─ r e 1 ┬ │ ┴ y s c ─ 0 ─ s h ┬ │ ┴ i ─ 0 ─ p ┬ │ ┴ ─ 1 ─ ┬ │ ┴ ─ 0 ─ R s p ┬ │ ┴ ─ 0 ─ ┬ │ ┴ ─ 0 ─ ┬ │ ┴ ─ 0 ─ 1 ┬ │ ┴ D 0 ─ 0 ─ a b x ┬ │ ┴ t y 0 ─ 1 ─ a t F ┬ │ ┴ e ─ 1 ─ ┬ │ ┴ ─ 1 ─ ┬ │ ┴ ─ 1 ─ ┬ │ ┴ ─ 0 ─ R s p ┬ │ ┴ ─ P ─ S t p ┐ │ ┘ EEPROM chip: 24C02 has 256 bytes (256 × 8-bit) memory. Datasheet\nThe memory address 0x01 will be written into the register of the EEPROM chip (? Not sure), and the data 0x0F will be stored in the memory.\nRead-Data Frame ❌ A read data frame consists of one write operation followed by one read operation. ┌ │ └ ─ S ─ S t a r t ┬ │ ┴ M t ─ 1 ─ C o ┬ │ ┴ U ─ 0 ─ t ┬ │ ┴ i h ─ 1 ─ S A 0 o s e ┬ │ ┴ l d x n ─ 0 ─ a d A t E ┬ │ ┴ v r 5 t h E ─ 0 ─ e e h e P ┬ │ ┴ s e R ─ 1 ─ s m O ┬ │ ┴ b a M ─ 0 ─ u s ┬ │ ┴ s t ( ─ 1 ─ e s ┬ │ ┴ r l ─ 0 ─ W r i t e a ┬ │ ┴ s v ─ 0 ─ R s p e e ┬ │ ┴ n ) ─ 0 ─ d . ┬ │ ┴ i ─ 0 ─ n ┬ │ ┴ g ─ 0 ─ M A 0 i ┬ │ ┴ e d x n i ─ 0 ─ m d 0 n ┬ │ ┴ o r 1 t s ─ 0 ─ r e h t ┬ │ ┴ y s e r ─ 0 ─ s u ┬ │ ┴ c c ─ 0 ─ h t ┬ │ ┴ i i ─ 1 ─ p o ┬ │ ┴ n ─ 0 ─ R s p ┬ │ ┴ ─ S ─ S t a r t ┬ │ ┴ M d ─ 1 ─ C a ┬ │ ┴ U t ─ 0 ─ a ┬ │ ┴ i ─ 1 ─ S A 0 o s o ┬ │ ┴ l d x n n ─ 0 ─ a d A t ┬ │ ┴ v r 5 t h t ─ 0 ─ e e h e h ┬ │ ┴ s e e ─ 1 ─ s m ┬ │ ┴ b a b ─ 0 ─ u s u ┬ │ ┴ s t s ─ 1 ─ e , ┬ │ ┴ r ─ 1 ─ R e a d , a ┬ │ ┴ n ─ 0 ─ R s p w d ┬ │ ┴ h ─ 0 ─ o t ┬ │ ┴ h ─ 0 ─ a e ┬ │ ┴ r s ─ 0 ─ e b k M ┬ │ ┴ D s y 0 s C ─ 0 ─ a p x U ┬ │ ┴ t o s 0 t ─ 1 ─ a n l F h w ┬ │ ┴ s a e i ─ 1 ─ e v l ┬ │ ┴ d e E l ─ 1 ─ E ┬ │ ┴ P r ─ 1 ─ R e ┬ │ ┴ O s ─ 0 ─ R s p M p ┬ │ ┴ o ─ P ─ S t p t n ┐ │ ┘ o s e p . u b l i s h The address of memory to be read is 0x01, which will be put into a register in the EEPROM chip (? Not sure.)\n(2024-10-26)\nThe \u0026ldquo;empty\u0026rdquo; write command may aim to locate the target memory in the EEPROM. Simu-I2C-CSDN Question: If the slave is MCU, why the \u0026ldquo;Slave address\u0026rdquo; after the second Start bit is still the address of the EEPROM?\nI2C on Arduino ::: aside\nReferences: {{{ 2. Inter-Integrated Circuit (I2C) Protocol - Arduino Docs Searched by I2C on arduino in DDG 3. Wire - Arduino Reference Found in: Docs Home -\u0026gt; \u0026lsquo;Rapid Access\u0026rsquo;: Language Reference }}} ::: Notes:\nThe \u0026lt;Wire.h\u0026gt; library implemented the underlying I2C protocol and is included in all Arduino board packages. So one only need to call its API to transfer data with I2C. (r2-Docs)\nInitializing Wire and Serial for I2C transmission and printing data to console.\nCalling the derived library for a specific device, e.g., \u0026lt;Adafruit_MPU6050.h\u0026gt; to communicate is more convinent than calling the lower-lever Wire.h library.\n\u0026ldquo;Adafruit\u0026rdquo; is the organization that developed the \u0026lsquo;STEMMA QT\u0026rsquo; connector.\nThe device identifier and the number of bytes to be read needs to be specified in the instruction forming function: requestFrom(device_id, n_bytes).\nTo read one byte from the bus: Wire.read(). To check how many bytes left on the bus: Wire.available() (r3-Docs).\nTo write bytes onto the bus: Wire.write()\n\\begin{algorithm} \\caption{Master Reads 6 Bytes} \\begin{algorithmic} \\REQUIRE{Wire.h} \\FUNCTION{setup}{} \\STATE{Wire.begin()} \\STATE{Serial.begin(9600)} \\COMMENT{For serial monitor} \\ENDFUNCTION \\FUNCTION{loop}{} \\STATE{Wire.requestFrom(8, 6)} \\COMMENT{request from device 8 for 6 bytes} \\WHILE{Wire.available()} \\COMMENT{read byte one-by-one until the countdown reaches 0} \\STATE{char c = Wire.read()} \\STATE{Serial.print(c)} \\ENDWHILE \\STATE{delay(500)} \\ENDFUNCTION \\end{algorithmic} \\end{algorithm} The program running on the slave should declaim the identifier for the slave at the begining: Wire.begin(8)\nThe responser sends data hardly when it received the request instruction by executing onRequest function\nThe slave only response once, so the data is only write() once in the setup() function.\nOne character is a byte.\n\\begin{algorithm} \\caption{Device Responses 6 Bytes} \\begin{algorithmic} \\FUNCTION{setup}{()} \\STATE{Wire.begin(8)} \\ENDFUNCTION \\FUNCTION{loop}{()} \\STATE{delay(100)} \\ENDFUNCTION \\FUNCTION{onRequest}{()} \\STATE{Wire.write(\"hello \")} \\ENDFUNCTION \\end{algorithmic} \\end{algorithm} Simu I2C Problems:\nStart: I wondered what does the wave of a read frame look like. Supports:\nThe first bare write command without writing any data is called \u0026ldquo;伪写入\u0026rdquo; Simu-I2C-CSDN\nI2C waveform r3-OfficialTut\nObserving I2C waveform using a logic analyzer on Wokwi r5-Docs ::: aside\n**References**: {{{ 基于proteus仿真的单片机IIC（I2C）通信（超详细） - CSDN Searched by Proteus 仿真 I2C 通信 in DDG I2C总线协议，STM32单片机在proteus仿真实例代码讲解 - bilibili - sfdsl1 Searched by I2C 电路仿真 (2024-10-17) in bilibili Interal tutorial page in the \u0026lsquo;Getting Started\u0026rsquo; section of Proteus 8 Demonstration. Proteus Tutorial : Proteus VSM for Arduino - YouTube - Labcenter Electronics Ltd Searched by Proteus VSM Tutorial in DDG 逻辑分析仪使用指南 | Wokwi Docs Searched by 逻辑分析仪 用法 in DDG }}} ::: Haptic Motor Driver Problems:\nHow to control a haptic motor?\nHaptic motor driver: DRV2605LDGSTr2-Datasheet\nI2C address of Sparkfun Haptic Motor Driver - DRV2605L (ROB-14538): 0x5A r3-Intro\n::: aside\nReferences:{{{ 2. [Datasheet: DRV2605L 2- to 5.2-V Haptic Driver for LRA and ERM with Effect Library and Smart-Loop Architecture (pdf)](https://www.ti.com/lit/ds/symlink/drv2605l.pdf) Searched by `DRV2605LDGST` at [DDG](https://duckduckgo.com/?q=DRV2605LDGST\u0026ia=web) 3. [Breakout Introduction: SparkFun Haptic Motor Driver - DRV2605L - Digi-Key](https://mm.digikey.com/Volume0/opasdata/d220001/medias/docus/778/ROB-14538_Web.pdf) Searched by `Sparkfun Haptic Motor Driver - DRV2605L` at [DDG](https://duckduckgo.com/?q=Sparkfun+Haptic+Motor+Driver+-+DRV2605L\u0026ia=web) 4. [Haptic Motor Driver Hook-Up Guide - SparkFun Learn](https://learn.sparkfun.com/tutorials/haptic-motor-driver-hook-up-guide/all) 5. [SparkFun Haptic Motor Driver - GitHub](https://github.com/sparkfun/Haptic_Motor_Driver) }}} ::: ➀ Connection Arduino and DRV2605L Problems:\nHow to connect an Arduino to the SparkFun Haptic Motor Driver (DRV2605L)? ::: aside\nReferences: {{{ GPT5 - Arduino SparkFun connection guide }}} ::: Supports:\nIN is Multi-mode input. I2C is selectable as PWM, analog, or trigger. If not used, this pin should be connected to GND Datasheet Sec 5.\nEN controls the power state Datasheet Sec 8.4.1, logic high. Datasheet Sec 9.3.1\nSDA and SCL Datasheet Sec 8.5.3\nConnection to application processor Datasheet Sec 9.1.\n::: aside\nReferences {{{ 1. [GPT5 Pro - Arduino SparkFun connection guide](https://chatgpt.com/c/68f67d1d-c4a0-8331-96db-037b33206b56) }}} ::: Actions:\nPins of the driver board\nPin To GND GND VCC Nano 5V Out SDA Nano A4 I2C Data SCL Nano A5 I2C Clock IN GND EN Nano 5V Out Observe voltage between Out pins\nThe driver board preserves a default firmware already.\nThe default firmware for the DRV2605L is set for use with ERM type motors. Tutorial\n➁ Registers Definition of DRV2605L Problems:\nUse Arduino Nano to control an LRA motor to generate haptic motion Supports:\nRegisters settings for Internal Trigger Mode r1-Learn\nMODE bits (Addr: 0x01, lowest 3 bits)Sec 8.4.2: Mode selection register \u0026ndash;\u0026gt; 0 (Internal trigger mode)Table 2\nFeedback control (Addr: 0x1A)Sec 8.6.20 0x00110110 (0x36) \u0026ndash;\u0026gt; 0 (ERM mode); 011 (4x feedback gain ratio between braking gain and driving gain); 01 (Medium loop gain for the feedback control); 10 (For ERM motor, 1.365x analog gain of the back-EMF amplifier.)\nLibrary Selection register (Addr: 0x03, lowest 3 bits)Sec 8.3.5.2: LIBRARY_SEL \u0026ndash;\u0026gt; 2 (Library B for ERM)Table 1\nEight waveform sequencer register (Addr: 0x04 ~ 0x0B) storing \u0026ldquo;waveform identifier\u0026rdquo;, the index of waveform in a ROM library, will be read to play the corresponding waveform.\nGO register (Addr: 0x0C) remains high until the playback of the haptic waveform sequence is complete.\nArduino Code {{{ ```cpp #include //SparkFun Haptic Motor Driver Library #include //I2C library SFE_HMD_DRV2605L HMD; //Create haptic motor driver object\nvoid setup() { HMD.begin(); // Read Status register Serial.begin(9600); HMD.Mode(0); // Internal trigger input mode \u0026ndash; Must use the GO() function to trigger playback. HMD.MotorSelect(0xB6); // ERM motor, 4x Braking, Medium loop gain, 1.365x back EMF gain HMD.Library(6); //1-5 \u0026amp; 7 for ERM motors, 6 for LRA motors\n} void loop() { int seq = 0; //There are 8 sequence registers that can queue up to 8 waveforms for(int wave = 1; wave \u0026lt;=123; wave++) //There are 123 waveform effects { HMD.Waveform(seq, wave); // Write the waveform identifier wave to the register seq HMD.go(); // Start play waveforms delay(600); //give enough time to play effect Serial.print(\u0026ldquo;Waveform Sequence: \u0026ldquo;); Serial.println(seq); Serial.print(\u0026ldquo;Effect No.: \u0026ldquo;); Serial.println(wave);\nif (wave%8==0) //Each Waveform register can queue 8 effects { seq=seq+1; } if (wave%64==0) // After the last register is used start over { seq=0; } } }\n1 }}} ::: aside\nReferences {{{ 1. [Haptic Motor Driver Hook-Up Guide - SparkFun Learn](https://learn.sparkfun.com/tutorials/haptic-motor-driver-hook-up-guide#:~:text=example%20in%20detail.-,Internal%20Trigger%20Mode,-The%20internal%20trigger) }}} ::: Actions:\nConnect a motor (got from John)\nResults:\nIterate 123 effects in the interal-ROM library Datasheet Sec 8.3.5.2:\nI2C on Renesas ➀ SmartConfig Gen I2C For RL78-F23 Problems:\nUse Smart Configurator to generate I2C driver code Supports:\nI2C component configuration on the Sample Code of Multi-slave r1-Sample\nAccording to the circuit schematic, pins p11 and p12 are used for I2C, corresponding to the simplified I2C function, rather than \u0026ldquo;IICA\u0026rdquo; ::: aside\nReferences {{{ 1. [RL78/G23 I2C Supporting Multiple Slave Address (Master) Rev.1.00 - Sample Code - R01AN5825XX0100](https://www.renesas.com/en/search?keywords=rl78%20i2c\u0026type=Documents#:~:text=RL78/G23%20I2C%20Supporting%20Multiple%20Slave%20Address%20(Master)%20Rev.1.00%20%2D%20Sample%20Code) }}} ::: Actinos:\nIf enabling the SER00 merely, the default pins are not p11 and p12.\nI manually set the p11 and p12 to be used as serial.\nAPIs of IICA Problems:\nFunctions of starting I2C transmission from the sample code for G23 r1-Code ::: aside\nReferences {{{ 1. [RL78/G23 I2C Supporting Multiple Slave Address (Master) Rev.1.00 - Sample Code](https://www.renesas.com/en/document/scd/rl78g23-i2c-supporting-multiple-slave-address-master-sample-code?queryID=c2b867e0601d8a2a2274a2a639faa841) }}} ::: ➀ Read Data to 7-bit Addr Problems:\nRead 256 bytes starting at address 0x00:\n1 status = R_IICA0_Rx(sladr, 0x00, g_Rx_buff, 256); Supports:\nDefinition:\n1 2 // src\\smc_gen\\Config_IICA0\\Config_IICA0_user.c uint8_t R_IICA0_Rx(uint8_t sladr7, uint8_t adr, uint8_t * const rx_buf, uint16_t rx_num) Slave address (7 bit address): sladr\nRead starting address: 0x00\nReceived data pointer: g_Rx_buff\nReceive data number: 256\n➁ Write Data to 7-bit Addr Problems:\nWrite 16 bytes data to slave\n1 2 3 4 5 for ( i = 0 ; i \u0026lt;16 ; ++i ) { /* transmit 16 bytes data to slave */ txbuf = (uint8_t *)\u0026amp;INC_DATA[i][0]; status = R_IICA0_Tx(sladr, i*16, (uint8_t *)txbuf, 16); } Supports:\nDefinition\n1 uint8_t R_IICA0_Tx(uint8_t sladr7, uint8_t adr, uint8_t * const tx_buf, uint16_t tx_num); Slave Address: sladr\nWriting address: i*16\nData to be sent: txbuf\nNumber of bytes to be sent: 16\n➂ Read/Write to 8-bit Addr Problems:\nRead data\n1 status = R_IICA0_Master_Receive((sladr \u0026lt;\u0026lt; 1 ), \u0026amp;g_Rx_buff[(i*16)], 16 ); Write data:\n1 status = R_IICA0_Master_Send((sladr \u0026lt;\u0026lt; 1 ), g_Tx_buff, 257 ); Supports:\nDefinition\n1 uint8_t R_IICA0_Master_Receive(uint8_t sladr8, uint8_t * const rx_buf, uint16_t rx_num) Slave address 8-bit: sladr Data to be read: rx_buf Number of bytes: rx_num Difference from R_IICA0_Rx\nR_IICA0_Rx requires data pointer adr (in the slave memory), while R_IICA0_Master_Receive doesn\u0026rsquo;t, so it should be used when the adr, i.e. (interal) address register is set r1-GPT ::: aside\nReferences {{{ 1. [GPT5 - Explain code functionality](https://chatgpt.com/c/68fa4028-666c-8329-bf86-fa8c930608db) }}} ::: RL78-F23 Controls Haptic Problems:\nUse RL78/F23 haptic ECU to contol the haptic motor driver board Supports:\nI2C Frame of Writing the Mode Selection Register 0x01\n┌ │ └ │ │ │ │ │ ─ S ─ S t a r t ┌ ┬ │ ┴ │ │ │ │ │ ─ ─ 0 ─ - t ┬ │ ┴ - a ─ 1 ─ - o r ┬ │ ┴ n g ─ 0 ─ S A e ┬ │ ┴ l d 0 t t ─ 1 ─ a d x h ┬ │ ┴ v r 5 e a ─ 1 ─ e e A d ┬ │ ┴ s b d ─ 0 ─ s u r ┬ │ ┴ s e ─ 1 ─ s ┬ │ ┴ s ─ 0 ─ - ┐ ┬ │ ┴ │ │ │ │ │ ─ 0 ─ W r i t e ┬ │ ┴ │ │ │ │ │ ─ 0 ─ R s p ┌ ┬ │ ┴ │ │ │ │ │ ─ 0 ─ - M ┬ ┴ - o ─ 0 ─ - i d ┬ ┴ n e ─ 0 ─ R A ┬ ┴ e d 0 D r ─ 0 ─ g d x R e ┬ ┴ i r 0 V g ─ 0 ─ s e 1 2 i ┬ ┴ t s 6 s ─ 0 ─ e s 0 t ┬ ┴ r 5 e ─ 0 ─ L r ┬ ┴ ─ 1 ─ - ┐ ┬ │ ┴ │ │ │ │ │ ─ 0 ─ R s p ┌ ┬ │ ┴ │ │ │ │ │ ─ 0 ─ - R ┬ ┴ - e ─ 0 ─ - g ┬ ┴ i ─ 0 ─ V 1 s ┬ ┴ a 0 t ─ 0 ─ l b x e ┬ ┴ u y 0 r ─ 0 ─ e t 0 ┬ ┴ e v ─ 0 ─ a ┬ ┴ l ─ 0 ─ u ┬ ┴ e ─ 0 ─ - ┐ ┬ │ ┴ │ │ │ │ │ ─ 0 ─ R s p ┬ │ ┴ │ │ │ │ │ ─ P ─ S t o p ┐ │ ┘ │ │ │ │ │ ➁ I2C Multiplexer NCA9548A Problems:\nIdentical parts have the same I2C address on the bus, so the MCU cannot individually control each component\nTherefore, a \u0026ldquo;switch\u0026rdquo; is used to select one of the components.\n::: aside\nReferences {{{ 1. [Gemini 2.5P - I2C Switch for Identical Haptic Motors](https://gemini.google.com/share/67102feee696) 2. [NCA9548APW (8-channel I2C-bus switch with reset) | Nexperia](https://www.nexperia.com/product/NCA9548APW) }}} ::: Supports:\nSDA for selecting channel 0\nWrite the control register:\nfirst byte: 0x11100000 (0x70 + 0) second byte: 0x01 ┌ │ └ s ─ S ─ │ t ┌ ┬ │ ┴ a ─ ─ 1 ─ r t ┬ │ ┴ t a ─ 1 ─ r ┬ │ ┴ g ─ 1 ─ e ┬ │ ┴ t ─ 0 ─ ┬ │ ┴ a ─ A ─ d ─ 2 ─ d ┬ │ ┴ r ─ A ─ e ─ 1 ─ s ┬ │ ┴ s ─ A ─ ─ ─ 0 ─ ┐ ┬ │ ┴ R ─ 0 ─ │ W ┬ │ ┴ ⁻ ─ A ─ ┌ ┬ │ ┴ ─ ─ B ─ ─ 7 ─ ─ ─ c ─ B ─ o ─ 6 ─ n ─ ─ t ─ B ─ r ─ 5 ─ o ─ ─ l ─ B ─ ─ 4 ─ r ─ ─ e ─ B ─ g ─ 3 ─ i ─ ─ s ─ B ─ t ─ 2 ─ e ─ ─ r ─ B ─ ─ 1 ─ ─ ─ ─ B ─ ─ ─ 0 ─ ┐ ┬ │ ┴ ─ A ─ ┬ │ ┴ ─ P ─ │ s ┐ │ ┘ t o p Actions:\nWrite 1 byte to the address\nWrite-Read Register of NCA9548A Problems:\nSet flag variables for transmission completion\nRL78-F23 Read DRV2605L Remove StopCondition From Callback Problems:\nThe variable that monitors a target register does not change after a single write-read sequence.\n1 2 3 4 5 6 7 8 9 uint8_t watch_mod=1; void main(void) { // Write 0 to register 0x01 drv2605l_write_reg(0x01, 0x00); // Read the value in the register 0x01 drv2605l_read_reg(0x01, \u0026amp;watch_mod); } The value of watch_mod doesn\u0026rsquo;t change, maintaining its initial value. As a result, it\u0026rsquo;s unclear whether the write or the sebsequent read is failing.\nTL;DR Answer: The callback only set the communication completion flag. Supports:\nIn a simplified IIC reading flow, singals comparison of issuing a StopCondition() immediately after the write v.s. issuing a repeated Start are illustrated below. r1-Forum\n{{{\nRead without stop condition Read with stop condition R_IIC00_Master_Send(0XC0, data_buf_tx, 1); while(IIC00_flg_end == 0) { NOP(); } IIC00_flg_end = 0; // R_IIC00_StopCondition(); \u003c----- //IIC00_flg_end is set to 1 when interrupt is generated R_IIC00_Master_Receive(0XC0, data_buf_tx2, 2); while(IIC00_flg_end == 0) { NOP(); } IIC00_flg_end = 0; R_IIC00_StopCondition(); R_IIC00_Master_Send(0XC0, data_buf_tx, 1); while(IIC00_flg_end == 0) { NOP(); } IIC00_flg_end = 0; R_IIC00_StopCondition(); //IIC00_flg_end is set to 1 when interrupt is generated R_IIC00_Master_Receive(0XC0, data_buf_tx2, 2); while(IIC00_flg_end == 0) { NOP(); } IIC00_flg_end = 0; R_IIC00_StopCondition(); }}} Based on the DRV2605L datasheetSec 8.5.3.5, a single-byte read uses a repeated start condition, between the address/write phase (for register pointer) and the address/read phase, there is not a stop condition in between. Instead, a start condition (belonging to the address/read phase) follows.\nThe bus should do: StartCondition \u0026ndash;\u0026gt; SlaveAddr+W \u0026mdash;\u0026gt; RegAddr \u0026mdash;\u0026gt; StartCondition \u0026ndash;\u0026gt; SlaveAddr+R \u0026ndash;\u0026gt; DataBuf I realized a \u0026ldquo;Repeat start condition\u0026rdquo; doesn\u0026rsquo;t include a stop condition while I was explaining my code to Jackie, when I noticed there is not a dropping edge, as a stop condition appears.\nThere must not be a stop condition after the initial write phase that sets the register pointer; there must be the start condition of the read phase.\nIf a StopCondition is inserted, the device won\u0026rsquo;t treat the preceding write as the prerequisite pointer write for the following read.\nThe paragraph above the timing diagram already describes the correct bus signals, but I initially overlooked it.\nJackie inspired me that the StartCondition and StopCondition are practical elements consituting the protocal implementation, during she reviewed my code.\n::: aside\nReferences {{{ 1. [RL78/G13 - Not able to generate repeated start using simplified I2C(SDA00) - RL78 MCU Forum | Renesas Engineering Community](https://community.renesas.com/mcu/rl78/f/rl78-forum/28849/rl78-g13---not-able-to-generate-repeated-start-using-simplified-i2c-sda00?queryID=bf2e67bb2347628dfa3326f8038c49c2) Searched by `rl78 simplified i2c sample code` at [Search | Renesas](https://www.renesas.com/en/search?keywords=rl78%20simplified%20i2c%20sample%20code) }}} ::: Actions:\nR_IIC00_StopCondition() should not be called inside the ISR callback, because the Stop condition is not always the correct bus action at the end of a write phase. some transactions require an immediate Start condition to read depending on specific devices.\nThe stop R_IIC00_StopCondition() should only be issued at the point where the entire IIC communication concludes.\nThe callback function only set the flag that signal the current data phase is done.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Config_IIC00_user.c #define DRV2605L_ADDR7 0x5A static volatile uint8_t i2c_done = 0; static volatile uint8_t i2c_err = 0; static void r_Config_IIC00_callback_master_sendend(void) { // R_Config_IIC00_StopCondition(); /* STOP after write */ i2c_done = 1; } static void r_Config_IIC00_callback_master_receiveend(void) { i2c_done = 1; } static void r_Config_IIC00_callback_master_error(MD_STATUS flag) { R_Config_IIC00_StopCondition(); i2c_err = (uint8_t)flag; i2c_done = 1; } These functions are called in the ISR r_Config_IIC00_interrupt() Write register\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Write 1 register (reg, val) in a single I2C transaction MD_STATUS drv2605l_write_reg(uint8_t reg, uint8_t val) { uint8_t tx[2] = { reg, val }; i2c_done = i2c_err = 0; R_Config_IIC00_Master_Send((uint8_t)(DRV2605L_ADDR7\u0026lt;\u0026lt;1), tx, 2); while(i2c_done == 0) { /* wait */ NOP(); } R_Config_IIC00_StopCondition();\t// Followed by a StopCondition after writing done return i2c_err ? MD_ERROR1 : MD_OK; } Read register\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Read 1 register (does a pointer write, then a read) MD_STATUS drv2605l_read_reg(uint8_t register_addr, uint8_t *val) { i2c_done = i2c_err = 0; R_Config_IIC00_Master_Send((uint8_t)(DRV2605L_ADDR7\u0026lt;\u0026lt;1), \u0026amp;reg, 1); while(i2c_done == 0) { /* wait */ NOP(); } // Don\u0026#39;t append StopCondition, since another StartCondition is required. i2c_done = i2c_err = 0; R_Config_IIC00_Master_Receive((uint8_t)(DRV2605L_ADDR7\u0026lt;\u0026lt;1), val, 1); while(i2c_done == 0) { /* wait */ NOP(); }\t// Followed by a StopCondition after reading done R_Config_IIC00_StopCondition(); return i2c_err ? MD_ERROR1 : MD_OK; } Write and read the 0x01 register\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // main.c #include \u0026#34;r_smc_entry.h\u0026#34; #include \u0026#34;src/multiplexer.h\u0026#34; #include \u0026#34;src/drv2605l.h\u0026#34; void main(void); uint8_t watch_mod=1; void main(void) { R_Config_IIC00_Create(); // init registers \u0026amp; pins EI(); // enable global maskable interrupts uint8_t mux = 0x70; // match your board pins uint8_t val; /* 1) Reset state should be 0x00 */ nca9548a_disable_all(mux); nca9548a_read_selected(mux, \u0026amp;val); // expect 0x00 /* 2) Enable CH3 and verify */ nca9548a_select_one(mux, 3); nca9548a_read_selected(mux, \u0026amp;val); // expect 0x08 /* 3) Probe a known device behind CH3 */ // Test write the mode register drv2605l_write_reg(0x01, 0x00); drv2605l_read_reg(0x01, \u0026amp;watch_mod); /* 4) Deselect all and confirm device disappears */ // nca9548a_disable_all(mux); while(1) { } } Need StopCondition After Write RegAddr Problems:\nAfter writing a register of DRV2605L, then reading the register results in 0xff all the time. Supports:\nI accidently noticed the code of SparkFun library, and its reading function has a intermediate stop:\n","date":"2024-10-12T21:29:00Z","image":"https://camo.githubusercontent.com/11db1977635115805436cb0521d6730576947e4949bbc3734dada1cbdfe44a0d/68747470733a2f2f7777772e656c656374726f6e6963736875622e6f72672f77702d636f6e74656e742f75706c6f6164732f323031372f30362f53657269616c2d436f6d6d756e69636174696f6e2e6a7067","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-comm-i2c_spi/","title":"Memo: EE - Comm | I2C and SPI Bus"},{"content":"(Feature image from: File type plantuml - Files \u0026amp; Folders Icons Searched by plant umlicon in DDG)\nFirst Impressions PlantUML (plantuml.jar) is a java application. So, it can be executed by using java -jar plantuml.jar aDiag.txt Docs\nHeard from this video: 优秀软件推荐之PlantUML - Karl1864, where the presenter calls PlantUML a \u0026ldquo;java 程序\u0026rdquo; (Searched by \u0026ldquo;plantUML\u0026rdquo; in Bilibili ) PlantUML can plot Timing diagram and Network diagram:\nTiming nwdiag Different from Mermaid rendering is in browser, PlantUML needs a server. However, uploading private data to a free public server is dangerous with the risk being caught. PlantUML FAQs; Docsy PlantUML support\nInstall \u0026amp; Config VSCode Ext References:\nConfiguring and Running PlantUML with VS Code - Medium · Sadaf Siddiqu Searched by \u0026ldquo;vscode for plantuml\u0026rdquo; in DDG Notes:\n(2024-10-12)\nA server or a local Java (serving as the plantform for plantuml.jar) is required to render diagrams qjebbs\nSo, when there is no server provided, pressing Alt + d won\u0026rsquo;t display the preview of the diagram.\nIf using the official server https://www.plantuml.com/plantuml, the data will be shared with the server. How long do the images generated by PlantUML Server live for?\nThe Graphviz is not necessary since the extension already contains. Reopen the VSCode, the preview can be rendered correctly.\nThe preview will be updated real-time.\n(2025/03/14)\nPreviewing diagram successes without specifying plantuml.jar location or setting Plantuml: Render to PlantUMLServer Windows References:\nA Guide to setup and use Plantuml · GitHub Searched by plantuml jar in DDG Local Installation Procedure - Quick Start Guide to PlantUML Notes:\n(2025/03/14)\nCommand line using plantuml-lgpl-1.2025.2.jar r2-Start\nActions:\nCommand r1-Gist\n1 java -jar %plant_uml% diagram.puml -o output -progress -tpdf Ubuntu References:\nQuick Start Guide to PlantUML Frequently Asked Questions about Installation Notes:\nDownload the plantuml-gplv2-1.2024.8.jar (GPL v2) r1-Start.\nTest this page r2-Faq:\n1 java -jar ~/Programs/plantuml-gplv2-1.2024.8.jar -verbose ./index.md Syntax Remove Foot Boxes Don\u0026rsquo;t show objects at bottom\n(2024/10/12)\nUse skinparam command\nRef: Using PlantUML in VSCode - Christopher Fuhrman (Searched by \u0026ldquo;plantuml preview in vscode doesn\u0026rsquo;t work\u0026rdquo; in DDG)\n1 2 3 4 5 6 7 8 9 10 11 @startuml Hello title skinparam style strictuml skinparam SequenceMessageAlignment center \u0026#39; Sequence diagram A -\u0026gt; B : Hello B -\u0026gt; A : Hi @enduml Use hide footbox\nRef: Docs\n1 2 3 4 @startuml hide footbox title Foot Box Removed @enduml Lifeline (2024/10/15)\nLifeline of a participant can be activate (or ++) and deactivate (or --), while Participant can be ** (created) and !! (deleted).\nreturn with autoactive on opened will deactivate the most recent lifeline. Ref: Sequence - Lifeline Activation and Destruction\n","date":"2024-10-12T12:45:00Z","image":"https://cdn.icon-icons.com/icons2/2107/PNG/512/file_type_plantuml_icon_130258.png","permalink":"http://blog.zichen.uk/post/writenotes/lang/b-note-plantuml/","title":"Memo: Lang - PlantUML | Usages"},{"content":"(2024/11/16)\nFeature image is from: iDoka/awesome-linbus Included by lin · GitHub Topics · GitHub Searched by LIN Protocol — One Wire Protocol for Automotive Applications - Electronics Lab in Google image Init search by lin protocol in DDG ➊ Overview References:\nLIN Bus Explained - A Simple Intro (2023) - CSS Electronics Chinese LIN总线简介 - bilibili - 广州虹科电子 Found by searching lin总线 in biliibli. LIN protocol 2.2A- Specification Package - lin-cia.org Notes:\nSingle signal wire: Master sends request to the bus, and slaves send response to the bus.\nExample of the Left-hand side Front door module controls the rest 3 windows r1-bili: Only one task exclusively occupies the bus at a time.\nA suppliment bus of CAN\nControl window, wiper, door\nOne master task and 16 slave tasks (including master node)\nMaster task means publishing header, which can only be done by the master node.\nSlave task means sending data. Slave node can only publish data, but they can\u0026rsquo;t publish header.\nLow-cost, low-speed with potential delay, requiring response space\n➋ Frame Structure References:\n4.25 LIN线控制的车窗玻璃如何下降 - bilibili - 虹科Pico汽车示波器 Followed by v2 【汽车总线技术】LIN总线技术基础讲解合集 - bilibili - 广州虹科电子 - 33:50 LIN Protocol Specification Revision 2.2A (Dec 31, 2010) Pages: p.30 Notes:\n(2024-10-07)\nA complete packet can be divided into 5 parts:\nBreak field (A long period of dominant value, at least 13 dominant bits) for letting slaves be aware of there is a frame on the bus.\nOne break field followed by 4 to 11 byte fields r3-Docs-p.29, including: Sync, PID, 1-8 Data, checksum Sync: 1 byte 0x55 (0101_0101). Note that the waveform on bus is reversed: 1010,1010 r2-Bili\nSlaves\u0026rsquo; clock are syncronized based the falling edges. That\u0026rsquo;s why the 1010,1010 is used. Protected ID\nData field\nChecksum\nPacket diagram:\npacket-beta title A LIN Frame carried 2 bytes of data 0-12: \"Break Field\" 13: \"Break delimiter\" 14: \"Start bit (0)\" 15-22: \"Sync (0x55)\" 23: \"Stop bit (1)\" 24: \"Start bit\" 25-32: \"PID (8 bit)\" 33: \"Stop bit\" 34: \"start bit\" 35-42: \"1 data (8 bit)\" 43: \"Stop bit\" 44: \"Start bit\" 45-52: \"1 byte data\" 53: \"Stop bit\" 54: \"Start bit\" 55-62: \"Checksum\" 63: \"Stop bit\" Terminologies:\nEvery byte field is enclosed by a Start bit 0 ahead and a Stop bit 1 at the end. r3-Docs-p.29.\nByte fields refer to 4 types of field: Sync field, Protected id field, each Data field. Whereas the Break field is at least 14 bits, which is the only field that doesn\u0026rsquo;t comply the structure of a byte field. r3-Docs-p.30.\nFrame length should be calculated as a sum of two portaions: Header + Response, as the response can be published by a slave r3-Docs-p.32.\nThe time of sending a frame counts not only sending bits, but also the \u0026ldquo;Response space\u0026rdquo; and \u0026ldquo;Inter-byte spaces\u0026rdquo;. There is an \u0026ldquo;inter-byte space\u0026rdquo; betwen every 2 \u0026ldquo;Byte file\u0026rdquo;. The gap betwen Break field and the Sync field is not an \u0026ldquo;inter-byte field\u0026rdquo;, because the Break field is not a \u0026ldquo;Byte field\u0026rdquo;. 1 2 0 1 3 B + r 1 … e a b k i t H 𝟎 e a 1 1 d e 0 b r y 1 t S e 0 y n ( 1 c 8 0 b i 1 t s 0 ) 𝟏 0 P i 1 r d o e b t n y e t t c i e t f e i d e r 1 0 1 1 R D b e a y s t t p a e o n s 1 e 0 1 C h b e y c t k e 1 0 is dominant value, while 1 is recessive value.\nIf the data field is empty, this packet is called a Hearbeat packet, used to inform salaves that the master is online and keep the salves aweak.\nDistinguish master and slave from the waveform: the master is of lower voltage, while the slave\u0026rsquo;s lower bound is higher.\nThe bottom edge of the packet published by the master node is flat. While the packet carrying slave\u0026rsquo;s data has a lader. + 1 2 0 H p n e a M o a c a r k s d t e t a b t e t e r a a t … M i a n s s t t e r r u c w t R e S s p p a … o c n e s e M b s a r y s e c t k , e , i r d S r l e a s v p e o n s e A complete dialog (cycle) between master and a slave consists of 4 packets:\n1 2 0 H p ( n e a a M o a c w a r k e s d t e a t a b t k e t e ) r a a t … M ( a p u M i s a s a n t c r s s r k t t e c e r R t m r u e d c q ) … M b s a r y S ( s e c l f t k , a p e e , i v a e r d e c d k b S r R e a l e e t c a s s k v p p ) e n s … ( M i M p u a n a a s s s s c r t t t k e r e e c r u r t m c d ) Sequence diagram for a conversation:\nsequenceDiagram Master-\u003e\u003eSlave: Heartbeat packet Note over Master,Slave: Wait Master-\u003e\u003eSlave: Instruction packet Note over Master,Slave: Wait Master--\u003e\u003eSlave: Request packet header Slave--\u003e\u003eMaster: Response Note over Master,Slave: Wait Master-)Slave: Following Instruction Protected ID References:\nLIN通讯_报文帧结构的PID及数据段 - HappytoShare Followed by v1 Notes:\nA 1-byte (8 bits) protected identifier field includes: 2 bits for parity and 6 bits for frame identifier\nFrame ID ranges from 0 to 63, corresponding to hex code: 0x00 ~ 0x3F, where the 2 frames: 0x3C (60) and 0x3B (61) are used to carry diagnostic and configuration data. And 0x3E and 0x3D are reserved for future protocol enhancement.\n6 frame types: FrameID 0-59 60 Type Unconditional Each frame id corresponds to a unique Protected ID, so a lookup table can be used in programs. LIN总线代码详解-1 - 20:52\nF F P r r r a a o m m t e e c i i i d d d D H H e e e c x x : : : | P 1 P 0 F I 3 D 2 5 3 I 1 D 6 4 I D 8 3 I D 4 2 F F I D 2 1 I D 1 0 2 Parity calculation based on Exclusive OR ⊕ and NOT ¬:\nP0 = ID0 ⊕ ID1 ⊕ ID2 ⊕ ID4\nP1 = ¬ (ID1 ⊕ ID3 ⊕ ID4 ⊕ ID5)\nThe frame identifier ranges 0 ~ 63, resulting 64 types of frames.\nTransmission starts from ID0 upwards to P1\nExamples:\nGiven a frame identifier: 38 (10_0110)\nP0 = 0 ⊕ 1 ⊕ 1 ⊕ 0 = 0\nP1 = ¬ (1 ⊕ 0 ⊕ 0 ⊕ 1) = 1\nProtected Id = 1010,0110\nThe wave on bus:\n0 S t a r 0 1 1 0 P 0 I D 1 ' 0 1 1 S t 0 p Data Field A frame can carry 0 ~ 8 bytes of data. Each data is a byte field.\nA byte field is 10-bit length (SCI format):\nS b t i 1 o t p d 7 0 d 6 1 d 1 5 0 0 b d i 4 1 t s d 3 1 d 2 1 d 1 1 d 0 0 S b 0 t i a t r t Bit transmission starts from d0 and ends at d7\nFor example, a data is 0101,1110\nThe wave on bus is:\n1 2 0 1 I b s 0 S t a t 0 1 1 1 1 D a 0 t a 1 0 1 S t p Checksum (2024-10-09)\nTwo types of checksums:\nClassic checksum is a sum of all Data bytes without including the protected id field;\nEnhanced checksum sums up all Data bytes including the protected id.\nThe frames in LIN version 1.x use Classic checksum. Whereas the frames LIN version 2.x use Enhanced checksum.\nNote the diagnostic frames (0x3C \u0026amp; 0x3D) always use the classic checksum because their IDs are fixed. HKACO - 41:00\nChecksum calculation when assembling the frame before transmission:\nSum up all Data fields\nSince the checksum is only of 1 byte (8 bits), once an intermediate sum got larger than 255 (more than 1 byte), a 255 (0xFF) needs to be subtracted. Flip all bits of the sum.\nValidate the checksum once the receiver received the frame.\nSum up all Data, but not followed by flipping all bits; Add the Data sum to the frame checksum, expecting their sum is 0xFF. Example:\nGiven a frame to be published on the bus:\nBreak field: 13 bits of 0 and 1 bit of 1; Sync field (8 bits): 0101,0101 (0x55); Protected id (8 bits): 1010,0110 (0xA6), if the frame id is 38; Data (length varies from 8 to 64 bits), e.g., 1001,0011 (0x93, Decimal: 147) 0 B f r i 0 e e a l … k d 0 B D i r e m 0 k l t 0 S t a r 1 0 S H 1 y e n a 0 c d e 1 f r i 0 e l 1 d 0 1 S t p 1 I b s 0 S t a r 0 P 1 r o 1 t e 0 c t 0 e d 1 i 0 d 1 1 S t p 1 R o S … e n p s s a 1 p e c 0 S t a r 1 1 0 D 0 a t 1 a 0 1 0 R e 1 s p 1 S t p o n 1 I b s s e 0 S t a r 0 0 C 1 h e 1 c k 0 s u 1 m 1 0 1 S t p Then, the checksum to be appended is:\nSum all Data: 1001,0011 (smaller than 0xFF, which is not needed to be subtracted)\nFlip all bits\nExample 1: Two Unconditional frames with LIN 2.2A HappytoShare - 07:47\nFrameID PID Data1 Data2 Data3 Data4 Data5 Data6 Data7 Data8 V2 DL 0x30 0xF0 0x00 0xFF 0x0F 2 0x31 0xB1 0x00 0x00 0x59 0x53 0x7f 0x14 0x00 0xff 0x0E 8 Extended checksum:\n+data2 +PID Inverted Add 00 00 F0 + FF + F0 \u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash; Sum FF F0 0F Sub 00 Frame ID: 0x31\n+data2 +data3 +data4 +data5 +data6 +data7 +PID Inverted Add 00 00 59 AC 2C 40 40 F1 + 00 + 59 + 53 + 7f + 14 + ff + B1 \u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash; - - - - - - Sum 00 59 AC 12B 40 - F1 Sub 2C 40 0E Subtracting 0xFF (256) can be figuratively performed by: removing the highest 1 (meaning 256) and add 1. HKACO - 42:20\nFlip 0xF1 (1111,0001) -\u0026gt; 0000,1110 (E)\nWaveform Problem:\nUse the logic analyzer: \u0026ldquo;Saleae Pro 8\u0026rdquo; to prob a LIN header transmitted by the Master RL78/F24 References:\n逻辑分析仪 Saleae Logic 16使用 - 程序员老吴的文章 - 知乎 Searched by 逻辑分析仪 用法 in DDG logic-lin-data/README.md at master · ma-lwa-re/logic-lin-data - GitHub Searched by Saleae for LIN protocol in DDG Notes:\n(2024-12-16)\nProb a LIN frame using Saleae logic analyzer r1-知乎\nActions: Extension for parsing LIN frame r2-GitHub\nOscilloscope\nReferences:\nLIN PID Protected ID LUT Look Up Table - 곰집사노리터 Actions:\nSend LIN frame Results:\n(2025-07-16T17:06)\nA 0x29 frame and a 0x47 frame without response.\n1 bit duration is around 50~60 us 1 2 3 4 5 6 7 8 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 | | | | | | | | | S S S S S S S S S t t t t t t t t t o a o a o a o a o p r p r p r p r p t t t t Sync PID=0xDD Data 0 = F0 Data 1 = 1E PID = 0xDD, corresponding to the frame ID 29 r1-LUT\nThe PID field in the second frame is 0b01101111 (i.e., reversed 0b1111,0110), 0x6F, corresponding to frame ID 47\n(2025-07-17T09:37)\nA 0x50 frame\nNote:\nThe waveform displayed on the osilloscope is reversed from the signal. There might be some recessive bits (1) after the end bit of the last byte field. And the following first 0 is the start of the next byte field. 1 2 3 4 5 6 7 8 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 | | | | | | | | | s s e s s s s s s t t n t t t t t t o a d a o a o a o p r r p r p r p t t t t Sync = 0x55 PID=0x32 Data 0 = 0 Data 1 = 0 Sync field: 0101,0101 (0x55) PID: 0011,0010 (2025-08-19T10:30)\nMaster header + Master data followed by Master header + Slave data\nLIN messages: The time interval between the end of a command frame and the start of a response frame\nTime interval between the start of a command frame to the start of response data\nA Master frame occupies the bus for 5ms, A Request frame take up the bus for 8ms:\nTime length of a Master frame\nTime length of a Response frame\nTime from the end of the command frame to the start of the response frame\nTime after the master releases the bus\nTime length of 13 bits\nTime length of 8 bits: 1010_1010\n➌ Frame Transfer (2024-10-09)\nSequence:\nRequest frame (header + data) published by the master node.\nWait some time for slaves recognizing and processing\nMaster node publish a header asking information\nOne of the 15 slave nodes publishs its data on the bus.\nSleep and Wakeup ➍ Diagnostic References:\n【汽车总线技术】LIN总线技术基础讲解合集 02 汽车LIN总线诊断及节点配置规范 - 广州虹科电子 【汽车总线技术】LIN总线技术基础讲解合集 04 LIN自动化测试软件高级功能使用 - 广州虹科电子 LIN Diagnostic Spec - Revision 2.2A - December 31, 2010; Page 73 LIN Diagnostic Spec - Revision 2.2A - December 31, 2010; Page 85 Frame Structure Overall:\nBoth MasterReq and SlaveResp can be a single frame SF or multiple frames (first frame FF + consecutive frame CF)\n1 2 3 4 5 6 7 .-- SF .-- MasterReq-+ Diagnostic | \u0026#39;-- FF + CF frame -+ | .-- SF \u0026#39;-- SlaveResp-+ \u0026#39;-- FF + CF Single Frame (2025-02-24)\nNotes:\nData field contains 8 bytes P02-11:52:\nN 0 A D P 1 C I B y 2 t e 0 B y 3 t e 1 B y 4 t e 2 B y 5 t e 3 B y 6 t e 4 B y 7 t e 5 The first 2 data bytes are occupied by NAD (Node address) and PCI (protocol control information), so one frame can only carry 6 bytes data (48 bits) at most. A MasterReq is a single frame:\nN A D P C I S I D D 1 D 2 D 3 D 4 D 5 Byte0 is SID (service ID) A SlaveResp is a single frame:\nN A D P C I R S I D D 1 D 2 D 3 D 4 D 5 NAD: Node Address\nNAD Description 0 Sleep Command 1 - 125 (0x01 - 0x7D) Physical address of slave node 126 (0x7E) Function Address, 只在诊断报文中使用，不允许用在节点配置中 127 (0x7F) Broadcast address 128-255 (0x80-0xFF) Customized PCI indicates the type of 3 kinds of PDU: SF (single frame), FF (first frame), and CF (consecutive frame) r1-虹科\ntype b7, b6, b5, b4 b3, b2, b1, b0 SF 0 0 0 0 有效数据字节数 FF 0 0 0 1 有效数据字节数的高 4 位, 会与后面的 LEN 组成一个 12 位的data: max:FFF CF 0 0 1 0 包编号 If the data is less than 8 bytes, a SF is enough. Otherwise, there will be FF and CF\n第一个 CF 的包编号是 0x1\nExamples: 0x06= SF + 6 Bytes, 0x10 = FF + less than 255 (FF) Bytes, 0x21 = CF + pdu index is 1.\nSID (Service Identifier) P2-25:52\nUsed in MasterReq\nTwo kinds of services: Diagnostic and Node configuration\n|Values | Services |\nFirst, Consecu Frame A MasterReq is a First Frame + multiple Consecutive frame:\nN A D P C I L E N S I D D 1 D 2 D 3 D 4 LEN is an additional segment added in the single frame, while reducing one data byte field.\nSimilarly, the First Frame for a SlaveResp is included LEN based on the single frame of a SlaveResp\nN A D P C I L E N R S I D D 1 D 2 D 3 D 4 The consecutive frame CF structure in the MasterReq and SlaveResp is the same: containing 6 data byte field\npacket-beta title CF (Data Field) 0-7: \"NAD\" 8-15: \"PCI\" 16-23: \"D1\" 24-31: \"D2\" 32-39: \"D3\" 40-47: \"D4\" 48-55: \"D5\" 56-63: \"D6\" Example log in SimpleConf console:\nFrameID Data 0x3C 0x7f, 0x10, 0x19, 0xaa, 0x01, 0x02, 0x03, 0x04 0x3C 0x7f, 0x21, 0x05, 0x06, 0x07, 0x08, 0x09, 0x10 0x3C 0x7f, 0x22, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16 0x3C 0x7f, 0x23, 0x17, 0x18, 0x19, 0x20, 0x21, 0x22 0x3C 0x7f, 0x24, 0x23, 0x24, 0xff, 0xff, 0xff, 0xff 0x7f : NAD = Broadcast address, informing all nodes 0x10 : PCI = First frame, no more than 0xFF bytes data 0x19 : LEN, Length (Number of byte of data) = 25 bytes 0xaa : SID, Service ID 0x01 - 0x04 : Data = 4 byte in the First Frame 0x21: PCI = CF, PDU index is 0x1 25 bytes data is from 0xaa to 0x24 Node Configuration Services (2025-02-24)\nObjective: Modify the NAD or Protected ID using the 0x3C frame with Service IDs ranging from 0xB0 ~ 0xB7 r1-02\nEach service corresponds to a master request, and is transmitted as a single frame SF r3-Docs Example 22 Service Notes:\n(2025/02/24)\nSID = 22 r1-02-1:10:19 UDS Services Notes:\nUnified Diagnostic Service r4-Docs\nSID list Wikipedia Send Diagnostic Frames (2025-02-14)\nNotes:\n\u0026ldquo;RAW\u0026rdquo; vs \u0026ldquo;DTL\u0026rdquo; r1-02\nRAW: Type 8 bytes for each frame manually. DTL: Enter the total number of data bytes, followed by inputing the data values only, without repeatedly typing the NAD and PCI, as they are identical for a diagnostic frame. The entire data will be split into a First Frame and multiple Consecutive Frames. Protocol \u0026gt; Execute service r2-虹科\n➎ Physical Layer Clock Freq Lin Driver Receiver Clock Freq (2024-10-10)\nBit Timing\nT0 is the first falling edge, and T1 is the last falling edge in the sync field.\nTherefore, the length of a bit is:\n$\\frac{(T1-T0)}{8}$\n1 B D r e T 0 S t a t 0 1 | 0 | 1 | 0 | 1 | 0 | 1 T 0 1 1 S t 0 p If the buad rate is 19,200, the bit timing is 52 us Lin Driver Receiver Linworks (2024-10-11)\n搭配硬件：上位机 （Babylin） STM32-LIN (2024-10-08)\nReferences:\nLIN总线代码详解-1 - bilibili - bili_17503743935 Followed by v1) RL78/F23 References:\nRL78/F23, F24 Setup Procedure for LIN Communication in Slave Mode (Guidance) Searched by No option for R7F123xxx in renesas lin configurator in Google RL78/F23, F24 Setup Procedure for LIN Communication in Master Mode (Guidance) Searched by RL78/F23, F24 Setup Procedure for LIN Communication in Master Mode (Guidance) in Google Inspired by r1 How to Create a LIN Project with Smart Configurator on e² studio - YouTube - RenesasPresents Shared by Kaushik RL78/F2x RLIN3 Module- Software Integration System Generated by Smart Configurator Notes:\n(2024-11-13)\nThe Master-mode settings of a RL78 in the r2-Docs p.3 is consistent with the video: r3-YouTube\nfCLK = 40M MHz, Use Pin13 \u0026amp; Pin14 Example: R78/F12 UART References:\nRL78G13 Utilising the Serial Array Unit (SAU) in LIN Communications Sample Code - R01AN0912EG0100 (Nov 23, 2011) Searched by Renesas LIN function example code in DDG Also can be searched by the document number in Search | Renesas RL78/F12 LIN Slave Mode (UARTF) - Renesas - R01AN1839ED0100 (Sep 25, 2013) Searched by s1 Also can be searched by LIN communication slave mode in Search | Renesas Pages: p.3 p.4 Search | Renesas Answered by Where to find sample code for RL78/G14 UART - Forum - RL78 MCU - Renesas Engineering Community Searched by Code Utilising the Serial Array Unit (SAU) in LIN Communications site:renesas.com in DDG Universal asynchronous receiver-transmitter - Wikipedia LIN Protocol Specification - Revision 2.2A (December 31, 2010) Notes:\n(2024-11-19)\nTutorial for a CubeSuite+ project (LIN.mtpj) demonstrates the usage of a SAU module implementing LIN communication r1-SAU.\nSample code of the UARTF module on an RL78/F12 (Slave mode) communicating with an RL78/F14 (Master) through the LIN Driver r2-RL78/F12.\n(2024-11-20)\nUART is different from LIN r2-p.3 in terms of aspects for a protocol r4-Wiki:\nFeatures UART LIN #wires 2 (Tx-Rx) 1 (Mst-Slv) Direction Full-duplex Half-duplex Syncronize Baud rate Baud rate Signals TTL Distance 40 m Dominant They both have No Clock wires, so the waveform-parsing syncronization is achieved by the baud rate between the two parties of the communication.\nAs LIN is used for the situation that one master controlling multiple slaves, so it only has a single wire.\nSince there is only one single wire shared by the many \u0026ldquo;users\u0026rdquo;,\nOnly the master can send frame Headers. When the master is about to publish a frame header, it pull down the bus for 13 dominant bits followed by at leat 1 recessive bit as a break delimiter.\nThe header issued by the master includes two fields: a Sync field and a PID. r5-Lin-p29\nThe baud rate is inicated by the \u0026ldquo;Sync break field\u0026rdquo; (0x55) to enable the target slave to match up its baud rate.\nThe PID field is used for generating interruptions, which can be generated after a slave received the PID completely (as PID only can be sent by the Master) r-F12-p4.\nThe slave response is of 9 bytes at most:\nThe reponse consists of two portions:\n8 bytes data at the most 1 byte of Checksum Sample Code:\n(2024-11-19)\ninit function\nReasons:\nAnalogous to Arduino\u0026rsquo;s Serial.begin(19200).\nNo user operations are required. The code on the slave will keep running: waiting for the header from the Master.\nThe LIN transceiver must be use with a UART?\nThe UART of an MCU need to be initialized: Clock, baud rate, enable interrupts, header format.\nCall APIs References:\nRLIN3 Module Software Integration System 【AutoSAR】只讲干货!一文看懂LIN通信_lin通讯-CSDN博客 Searched by l_sys_init in baidu Select the word: l_sys_init in the r1-Manual P.100 \u0026gt; 豆包桌面版: \u0026ldquo;AI搜索\u0026rdquo; LIN Middleware - GitHub Pages - Infineon.github.io Searched by #define ld_send_message(ch, length, NAD, data) ApMLinDiag_vogSendMessage_DMY(ch, length, NAD, data) in DDG LIN 入门 - Renesas Electronics (Oct 25, 2010) Searched by l_sys_init() LIN 通信代码 in Google Notes:\n(2024-12-04)\nThe functions of the library rlin3 are not many r4-Manual Ch8. (2024-12-26)\nA scheduler is necessary to master r1-SIS.\nA scheduler can be implemented with a timer (2024-12-27)\nAdd the the lin conf (/src/r_rlin3_lib/r_lin_drv/Channel0/conf) into compiler. Otherwise, the following code will run into an error: Undefined external symbol \u0026quot;_l_sys_init\u0026quot;\nActions:\nThe following code can be built without errors:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026#34;r_smc_entry.h\u0026#34; #include \u0026#34;conflin_0.h\u0026#34; int main (void); int main(void) { EI(); // Initialize the LIN core if (l_sys_init()) { // Initialization successful } else { // Initialization failed } // Your application code here return 0; } Execute processes based on the returned initialization result\nReasons\nl_sys_init() is an alias for the ApMLin_u1gInitSys() (hintted by VSCode copilot).\nPrompt: \u0026ldquo;@workspace How to use this function: l_sys_init()?\u0026rdquo; Call the LIN2.1 API in an individual source file for a channel.\nReasons:\nThe r1-Manual P.33 mentioned the source code for each channel should be separated.\nL c c A d I S ( C S p f I o o d e n o U a o a u N n n d f c u s l u r n f f i l r i l r t c 2 i i c n u c n c t . n n o i d e g f e ( i 1 _ _ m t e u I o 0 0 p i c L n c n n C D . . i o o I c o c ) L r r h c l n d N t d l I L e i e . e i e u N I a v r 2 o d N t e f . n f i A e r o o 1 s o n p C p r r g p o L t A l n i i c P c m i f b o h I o a c i r n a ) m i a g a n m n t u r n o ( i r y e n ) o a l n t ( o C 0 r H C 0 r ) e a t e | c c A d I S ( C o o d e n o U a L n n d f c u s l I f f i l r i l N i i c n u c n n n o i d e g f 2 _ _ m t e u . 1 1 p i c L n 1 . . i o o I c h c l n d N t D e . e i r r 2 o i f . n v o o 1 s e p r r t A i c P L o h I i n a ) b n r n a e r l y 1 ( C H 1 ) | Actions:\nCreat a hardware layer source file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // hw/lin_channel_0.c #include \u0026#34;conflin_0.h\u0026#34; u1 init_lin_ch0(void) { if (l_sys_init()) { // return L_FAIL; } else { return L_SUCCESS; } } The difference between l_bytes_wr and ld_send_message()\nReasons:\nThe r1-Manual doesn\u0026rsquo;t contain explanation about ld_send_message()\nconflin_0.h:\n1 2 3 #define ld_send_message(ch, length, NAD, data) ApMLinDiag_vogSendMessage_DMY(ch, length, NAD, data) #define l_bytes_wr ApLin_vogWriteBytesSig Actions:\nl_bytes_wr is used to write a byte to a signal, contributes to a frame r4-入门 P.51. ld_send_message is used to send a diagnostic frame r3-infineon Analysis:\nI guess the frames are sent automatically according to the schedule: l_sch_tick(). (2024-12-29)\nA schedule is created and the variable is generated by LIN Configurator, defined in conflin_0.h\nReasons:\nCall l_sch_set(LIN_CHANNEL0, SCH0, 0U); after initializing LIN r1-Manual p40.\nThe LIN transceiver needs +12V battery\n","date":"2024-10-07T08:24:00Z","image":"https://github.com/iDoka/awesome-linbus/raw/main/lin_logo.png","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-comm-lin/","title":"Memo: EE - Comm | LIN Bus Protocol"},{"content":"Install (2024-10-02)\nIt has versions for Linux. Downloads Include Libraries References:\n(2024-10-17)\nAdding Libraries in Kicad - YouTube - Electronics-PCB-Hardware Design Searched by How to include 3rd party libraries into KiCAD in DDG Notes:\nClick: \u0026lsquo;Preferences\u0026rsquo; -\u0026gt; \u0026lsquo;Manage Symbol Libraries\u0026rsquo; -\u0026gt; Click the folder icon -\u0026gt; Select path where the component library is: C:\\Users\\ZichenWang\\Documents\\KiCad\\8.0\\3rdparty\\LIB_STM8AF5288TCX\\STM8AF5288TCX\\KiCad\nInclude Footprints References:\nHow to add a .pretty directory (footprint library) - KiCad.info Forums Searched by KiCad include footprints in DDG. Notes:\nFootprint editor. Docs Draw Schematic Search Symbols References:\n(2024-10-16)\nDemo Project: From shematics to PCB routing and to manufacturer Learn KiCad 8 in 45 minutes - From idea to upload in one video - YouTube - AnotherMaker Searched by kicad 8 tutorial in DDG What component to use for \u0026ldquo;pin headers\u0026rdquo;? - KiCad.info Forums Searched by KiCad symbol 4 Header in DDG Schematic Editor | 8.0 | English | Documentation - KiCad\nNotes:\nSearch keyword: \u0026ldquo;CONN\u0026rdquo; for Connecters r1 TestPoint Start: Don\u0026rsquo;t know what this symbol is:\nReference:\n(2024-10-18)\nCreate bottom test points on the circuit board as \u0026hellip; - KiCad.info Forums Qingjun said: Those are 测试点 Search: KiCad add testing point in DDG Notes:\nTestPoint symbol and footprint Forum\nFootprint for TestPoint Footprint Library - TestPoint - GitHub Pages (Searched by s1)\nHierarchical Sheets Reference:\n(2024-10-18)\nKiCad 5 #16 Multisheet Schematics - YouTube - John\u0026rsquo;s Basement I wanted to draw an exactly same copy of the sample schematic, which spilt the entire circuit into multiple cells. Search: how to split the schematic diagram into cells in KiCAD in Google Multiple schematic pages in one project - KiCad.info Forums Searched by Kicad add additional page in DDG KiCad Docs\nNotes:\nFeels it\u0026rsquo;s like a \u0026ldquo;hyperlink\u0026rdquo; to enable jumping to another sheet. r1\nTricks:\nConnecting two components with a wire, allowing streching when moving around.\nNumbering components across sheets by x100\nDelete debug markers: \u0026ldquo;Electrical Rules Checker\u0026rdquo; -\u0026gt; \u0026ldquo;Delete All Markers\u0026rdquo; 12:19\nKiCad doesn\u0026rsquo;t support multiple sheets, in favor of hierarchical sheets. r2\nLabels Reference:\n(2024-10-18)\n开源EDA软件kicad6.0讲解之十二-原理图连接方式，网络标签，总线放置，标签语法详解 -bilibili- 含羞草的367朵花 Search kicad 绘制原理图 in bilibili Open: 开源EDA软件kicad6.0讲解之十三-多原理图设计，原理图分页，全局标签，分模块设计 - 含羞草的367朵花 Went to the previous video laying out basics. Connecting nets without wires on schematic - Page 1 - EEVblog Searched by KiCad connection name and Net name in DDG Documentation | KiCad Searched by KICad connection name in DDG Docs - Schematic Creation and Editing Searched by kicad net name in DDG Notes:\nSame text means a common connection. r1\nPressing L to add a Local label across the current sheet.\nTricks:\nPressing Ins key to adding repeating objects. Pressing ~ to highlight the network. Create a \u0026ldquo;Net Name\u0026rdquo; for a connection by the tool: \u0026ldquo;Add Label\u0026rdquo; r2:\n(2024-10-19)\nIs a \u0026ldquo;GND\u0026rdquo; symbol not connected with a \u0026ldquo;GND\u0026rdquo; net name?\nGND Symbol GND Label Power symbol is global, such as GND r2. While Add Label is adding a local label across the sheet, such as /GND. r3\nFor connections between sheets, use global or hierarchical labels.\nMarks on Sheet Reference:\n(2024-10-18)\n#4 How To Prettify Your Schematic with Kicad 7.0 | #PCBCupid - YouTube Directed by the video: #8 How To Use Constraints And Net Class Directive tool in KiCad 7.0 | #PCBCupid - YouTube, which shows a schematic with cells layout. Then, I went to check their other videos in this playlist. This video is searched by \u0026ldquo;Kicad add net class directive\u0026rdquo; in DDG, when I wonder what\u0026rsquo;s the button \u0026lsquo;Add Net Class Directive\u0026rsquo; below \u0026lsquo;Add Label\u0026rsquo;, which I was about to try. Symbol Graphics - KiCad Docs\nNotes:\nEach functional unit can be fenced by \u0026ldquo;Adding a rectangle\u0026rdquo; forming a \u0026ldquo;cell\u0026rdquo;.\nSimulation (2024-10-16)\nStart: I first saw the word \u0026ldquo;Simulation\u0026rdquo; is in the title: \u0026ldquo;Simulation Workflow Best-Practise for a Complete Project\u0026rdquo; (Searched by STM8AF5288 in kicad project in DDG)\nStory: Although the chip STM8AF6223, which is available in KiCad, doesn\u0026rsquo;t have USART, I can implement the LIN circuit first.\nThen, I started thinking if there is a possible way to verfiy my circuit\u0026hellip; How can I see the wave shapes? Can I use a virtual oscilloscope to probe the output? I remember I did some simulation experiments using a software called \u0026ldquo;Proteus\u0026rdquo; in my undergraduate course.\nCan KiCad do simulation? Right, it possibly does, as I just saw the searched result.\nSearches\n(2024-10-16)\nSearch: Kicad 8 simulation in DDG\nIntroductory videos on simulation with ngspice in KiCad 8 - Forum\nTwo resistors series connection. Power supply used the VDC in the Simulation_SPICE library.\nOscilloscope visualization the input and output voltage across an operational amplifier. ngspice in KiCad 8: inverting amplifier with generic opamp\nA descriptive introduction post: KiCad 8: Working with Circuit Simulations! - element14 Community\nNE555 LED Reference:\n(2024/10/17)\nKicad tutorial 31: Design and Simulation of 555 timer flashing led circuit - YouTube - Circuit Generator Searched by KiCad 8 simulation for LED in DDG SPICE libraries download links:\nNE555.lib\nMissing Model Simu Files (2024-10-17)\nSearch: Kicad spice simulation file in DDG\nGitHub - kicad-spice-library/KiCad-Spice-Library: Centralized repo to \u0026hellip; Kaushik:\nMake a package by yourself\nSearch in websites:\nDigiKey Component Search Engine Attach simulation model\nI2C Simu (2024-10-17)\nSearch I2C simulation in KiCad 8 in DDG\nGitHub - samjkent/encoder-i2c: KiCAD projects and schematics for I2C \u0026hellip; Make Symbol Reference:\n(2024-10-18)\nSymbol for a passive component (Inductor): KiCad初学者 - 第2集 创建元件符号（A） - Michael_AU Recommended by Qingjun. Symbol for an IC (Controller for Boost): KiCad初学者 - 第3集 创建元件符号（B）- Michael_AU\n(2024-10-22)\nGetting Started in KiCad - Creating New Symbols Notes:\n(2024-10-19)\nSteps:\nSymbol Editor\nFile -\u0026gt; New Library -\u0026gt; Add To Library Table -\u0026gt; Global (For being searchable in other projects)\nSymbol file path: C:\\Users\\ZichenWang\\Documents\\ECU_Practice-Zichen-2024-10-16\\3rd_Party_Libs\\Conn_Zichen.kicad_sym\nNew Symbol -\u0026gt; Fill out properties:\nSymbol name: TC2030-MCP-NL\nDefault reference designator: J\nThe rest attributes are left as default\nAdd Pin (P) -\u0026gt; Set properties:\nPin name:\nPin number:\nElectrical type:\nInductor is a passive component; While pins of IC require Electrical Types. Power and Analog GND are supposed to be \u0026ldquo;Power Input\u0026rdquo; type. Orientation\nDraw graphical shapes\nEdit Symbol Properties:\nReference: J\nValue: TC2030-MCP-NL\nFootprint:\nDatasheet: https://ww1.microchip.com/downloads/aemDocuments/documents/OTH/ProductDocuments/UserGuides/TC2030-MCP-NL_PCB_Footprint_RevD.pdf\nMPN (Manufacturer part number? for exporting BOM)\nManufacturer: Microchip Technology\nDescription: PLUG-OF-NAILS 6-PIN W/O LEGS\nDigiKey Part Number: TC2030-MCP-NL-ND\nThe \u0026ldquo;Pin Table\u0026rdquo; in \u0026ldquo;Symbol Editor\u0026rdquo; provides an overview for all pins. 07:46\nModify graphical shape properties by double-clicking it\nFill with body background color Make Footprints References:\n(2024-10-19)\nKiCad初学者 - 第4集 创建元件封装 - Michael_AU\nImages of TC2030 are from:\nmicrochip Tag-Connect Notes:\nSteps (r1):\nOpen \u0026ldquo;Footprint Editor\u0026rdquo; 05:48\nSet up plotting workspace 06:01\nSpecify unit (inches,mils,mm) and grid size\nThe positive direction of y is down.\nFile -\u0026gt; New Library -\u0026gt; Global 07:27\nCustom library foder (.pretty) storage path: C:\\Users\\ZichenWang\\Documents\\ECU_Practice-Zichen-2024-10-16\\3rd_Party_Libs\\Conn_Zichen.pretty\nThe newly created lib may not be inlcuded in search path yet (KiCad 5.1.10).\nFile -\u0026gt; New Footprint 09:11\nRead datasheet for pads sizes and layout\nEnter footprint name for this component: TC2030-MCP-NL-FP\nFootprint type: SMD\nTo move texts, pressing key \u0026lsquo;M\u0026rsquo;\nPlace pads: \u0026ldquo;Add Pad\u0026rdquo; 11:42\nThe 3 outer triangle vertices seems through hole for \u0026ldquo;Alignment pins\u0026rdquo;\nInterface Footprint Outlook Pins of TC2030 RJ12 for ICD \u0026ldquo;Add Pads\u0026rdquo; with type: Through-hole for the 3 \u0026ldquo;locating holes\u0026rdquo;.\nAdd 6 pads with type SMD (Surface mounted 贴片)\nDraw frames (placeholder) on the layer: F.Silkscreen (Front of Silkscren 丝印层的顶层) 14:24\nDatasheet instruction: \u0026ldquo;No other track or signal within 0.020\u0026rdquo; of any contact pad.\u0026quot;\nI drew the illustration for \u0026ldquo;Optional Bottom Layer\u0026rdquo;\nAdjust the texts position\nAdd 3D model (.step) file 16:25\nObtain 3D model from the seller official website and models download website\n\u0026ldquo;Footprint Properties\u0026rdquo; -\u0026gt; \u0026ldquo;3D Models\u0026rdquo;\nLook around the 3D model to check whether the pins match the pads\nLink the footprint to the symbol 19:50\n\u0026ldquo;Symbol Editor\u0026rdquo; -\u0026gt; \u0026ldquo;Symbol Properties\u0026rdquo; -\u0026gt; \u0026ldquo;Footprint\u0026rdquo;.\nMake footprint for an IC chip with \u0026ldquo;templates\u0026rdquo; 21:58\nFile -\u0026gt; \u0026ldquo;Create Footprint\u0026rdquo; (Create a new footprint using the Footprint Wizard) Edit the predefined pads parameters according to the datasheet. 23:15\nExport footprint to editor 24:35\nAdd 3D model file: \u0026ldquo;Footprint Properties\u0026rdquo; -\u0026gt; \u0026ldquo;3D Models\u0026rdquo;\nRename the footprint: \u0026ldquo;Footprint Properties\u0026rdquo; -\u0026gt; \u0026ldquo;Footprint name\u0026rdquo;\nLink the footprint to a symbol in \u0026ldquo;Symbol Editor\u0026rdquo; -\u0026gt; \u0026ldquo;Symbol Properties\u0026rdquo;\n","date":"2024-10-02T11:55:00Z","permalink":"http://blog.zichen.uk/post/writenotes/embed/b-cad-kicad_usages/","title":"Memo: EE - ECAD | KiCAD Usages"},{"content":" Homepage Live 1 课程内容概览 \u0026amp; 建立领域视野 ","date":"2024-09-13T11:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/d-vid-games003-scitut/","title":"Watch: SciTut - GAMES003 | 科研素养课"},{"content":"Method References:\nL07-论文图表的设计 - GAMES003-图形视觉科研基本素养 - GAMES-Webinar Notes:\n(2024-11-30)\nGuideline:\nClearly state the input and output\nThe writing sequence of the Method section follows the steps of the pipeline illustration r1-07:32.\nCharts Notes:\n(2024-11-30)\nTeaser images objectives:\nShow the problem to be tackled in this paper. “我们其实都是用 ppt 来作图的” r1-17:15\n对齐：Arange \u0026gt; Align Center Detailed caption under the pipeline diagram r1-22:07.\nRendering 3D objects: HTDerekLiu/BlenderToolbox r1-36:45\nThe main aim for images is to show your novelty. r1-41:10\nUse charts to show quantitative result.\nAdd labels of axes.\nUse big fonts in images\nTable\nUse tables to compare features of different methods. r1-47:38\nHighlight numbers in a table with package \\usepackage[table]{xcolor}: jonbarron/tablize\nIntro Video Tools:\nPPT (Transition + Animation) + iMoive r1-1:00:09\nResults displayed in paper -\u0026gt; 360-degree videos CMU Guide References:\ncmu-ci-lab/writing - Writing suggestions and resources for CIRL(May12,25) Mentioned by Xinge Yang in Wechat group ","date":"2024-08-22T00:00:00Z","image":"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcQ2If0SVeJ4WupNc56kFo36k4gMPHcKsE2T1HcaG95HtO0pn7gR","permalink":"http://blog.zichen.uk/post/writenotes/model/acadmodel/b-note-paper_writing/","title":"Memo: Acad - Paper | Writing Guidence"},{"content":"Single-hidden-layer neural network:\nD a X t a I n w p e u i I t g W h l t a s y e r H f i e H d a d t e u n r e O u w p e 𝛃 u i t g h l t a s y e r = T a T r g e t Optimization objective: Refining the $IW$ (input-layer weights), such that the hidden feature $H$ gets refined.\nIteration 1\nForward:\n$$ \\begin{aligned} H = IW ⋅ X \\\\\\ Y_{pred} = \\bm β ⋅ H \\end{aligned} $$ Compute error E:\n$$ E = T - Y_{pred} $$ Considering there is an imaginary data $P$ resulting in the error $E$ formulated by the equation: $E = \\bm β ⋅ P$\nThus, the data $P$ can be solved by the pseudo-inverse of $\\bm β$:\n$$ P = \\bm β⁺ ⋅ E $$ Using the $P$ to solve a \u0026ldquo;supplemental IW\u0026rdquo; $IW_{supp}$ by considering the relationship: $P = IW_{supp} ⋅ X$\n$$ IW_{supp} = X⁺ ⋅ P $$ Update the input-layer weight by adding the supplemental $IW_{res}$ to the initial $IW$\n$$ IW = IW + IW_{supp} $$ Update $\\bm β$ based on the updated $IW$ and the equation $T = \\bm β ⋅ H$:\n$$ \\begin{aligned} H = IW ⋅ X \\\\\\ \\bm β = H⁺ ⋅ T \\end{aligned} $$ Compute the error at present:\n$$ \\begin{aligned} Y_{pred} = \\bm β ⋅ H \\\\\\ E = T - Y_{pred} \\end{aligned} $$ Iteration 2:\nCompute $P$ Compute supplemental $IW_{res}$ Update $IW$ Update $β$ Compute $E$ Iteration 3:\nPerform the same 5 steps The following may be wrong This morning, I forgot the model architecture consists of 2 weight matrices. So, what I told you this morning is only refining a single weight matrix:\nGiven an equation: $Y_{pred} = A ⋅ X$, one wants to find the coefficient matrix A.\n\\begin{algorithm} \\caption{SNN} \\begin{algorithmic} \\STATE \\COMMENT{The optimal A sovled by least squares with Moore-Penrose inverse:} \\STATE $A = X⁺ ⋅ Y_{pred}$ \\STATE \\COMMENT {There are still some errors E:} \\STATE $E = T - Y_{pred}$ \\STATE \\COMMENT {By considering the E is attributed to an imaginary data P, there is: $E = A ⋅ P$} \\STATE \\COMMENT {The data P can be solved as:} \\STATE $P = A⁺ ⋅ E$ \\STATE \\COMMENT {To fit the error E, we can use another coeff. matrix A₂ and the equation: $E = A_2 ⋅ P$} \\STATE \\COMMENT {So, the A₂ can be solved as:} \\STATE $A_2 = P⁺ ⋅ E$ \\STATE \\COMMENT {Update A:} \\STATE $A = A + A_2$ \\STATE \\COMMENT {Compute the new error:} \\STATE $E = T - A ⋅ X$ \\STATE Go to line \\#5. \\end{algorithmic} \\end{algorithm} (2024-09-30)\n听到武老师夸奖 Arash：This is the progress I really like. 我进展太慢了，我昨天发的周报，武老师连 Thanks 也不回了。 Don\u0026rsquo;t take it personally. 我承认我做的是挺慢的，那也只能一点一点做，没事，别气馁。\nArash 做的什么东西啊？难道这么快就发第二篇文章？他用的栋哥的方法？我对分类任务没什么兴趣啊。\n训练经典的神经网络中的参数，是使用反向传播和梯度下降这一套优化方法， 而训练子网络是不断引入新的权重，每一次迭代是用新权重去“解释”残差，最后把所有的权重合并起来。\nDoubt Priviledge Question:\nWhat are the fundamental difference betwen deep neural networks and single layer wide networks?\nWhich one is better?\nWhy people all like deep nets, rather than wide nets?\nSpculations:\n(2024-11-11)\nAn accuate inverse matrix is not easy to obtained.\nSLFNN needs to calculate the inverse of a giant matrix, which is difficult and not accurate.\nAlso, the weight matrix could be singular. this may bring error when computing inverse, although a singular matrix can be inversible by using a regularization term.\nHence, the precison of the optimized parameters could be low.\n","date":"2024-08-12T12:34:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/rsnn_recap/","title":"Illustration for SNN"},{"content":"(2024-08-03)\n我看群里大家都在投稿文章，心里有点难过，唉，气馁，时间我是花进去了，但是做出的东西只是误差范围内的提升。 我做不出东西来，我都想退坑了。我突然感觉我走得很慢啊，文献也没看多少，我最近在干嘛啊？我最近连B站动态也没看，我时间花哪里去了？\n追求理想就是很难的 看到 00 大佬推荐这篇文章 (NRC)：Real-time Neural Radiance Caching for Path Tracing - Thomas Muller - 2021 - Nvidia\n他说：“别搞 Nerf 了，来搞这个”\n西山彻回复：但是这作者后来也转去做 NeRF，搞出了 instant-ngp Representation Particle 3D Gaussian References:\nfudan-zvg/gaussian-raytracing Mentioned in WEChat group and QQ group. (2024-12-01)\n3D Gaussian Ray Tracing r1-Fudan\nRelated:\nPdf: Nvidia fudan-zvg/gaussian-raytracing ","date":"2024-08-03T11:48:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/invrender/c-symp-render/","title":"Sympo: Render - RT"},{"content":"(2024-05-28)\nEPFL 图形实验室——关于可微分渲染，我能告诉你的一切 - boxAI博士 Modeling Scale References:\n第十六章 高级渲染 - pku.edu.cn Searched by 量子力学 与 光线追踪 in DDG Notes:\n(2024-11-11)\nThe requirements determine the ascpet to be modeled / simulated / formulated / approximated. Various scenario applications require different focusand problems to be taken into consideration.\nFor example, the interaction of light with atoms requires quantum dynamics. While game rendering just need geometric optics. r1-PKU\n线偏振渲染\n11-09 8:43\n【首个】线偏振渲染器-哔哩哔哩\n偏振：两个方向相反的偏正边叠在一起没有光透过\n11-15 08:43 PM 复旦大佬开源的 3DGS+Ray Tracing 项目，转给大家\n素履向心： 我们组也开源了一个自己实现的3D Gaussian Ray Tracing: https://github.com/fudan-zvg/gaussian-raytracing 以及一个封装好的gaussian tracer库: https://github.com/fudan-zvg/gtracer\nRT的优势： 可以按光线渲染，实现一些ray-based effects\n鱼眼相机，inverse rendering\u0026hellip;\nRay Marching References:\nうろうろシェーダー 241201 by TheBoiledHotWate - SharderToy Notes:\n(2024-12-04)\nSharderToy has a tag: raymarching Shader References:\nShadertoy for absolute beginners - YouTube - The Art of Code Notes:\n(2024-11-29)\nTo design the color of each pixel on a canvas r1-YouTube.\n1 0 ( 8 0 0 , 0 ) 1 9 2 0 1 0 ( 8 0 0 , 0 ) 1 9 2 0 × a s p = 1 0 8 0 1 float ratio = iBoundary.y / iBoundary.x; Soft shadow achieved by soft\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void mainImage( out vec4 fragColor, in vec2 fragCoord ) { // Normalized pixel coordinates (from 0 to 1) vec2 uv = fragCoord/iResolution.xy; uv.x *= iResolution.y/iResolution.x; //uv.x -= 0.5; //uv.y -= 0.5; vec3 col = vec3(uv.x * uv.x + uv.y * uv.y); // Output to screen fragColor = vec4(col,1.0); } Cherno Tut Problems:\nIt\u0026rsquo;s hard for me to read the book. I want to watch videos and get hands dirty.\nFound Cherno Tutorial r1-翻译\n视频项目源码 https://github.com/TheCherno/RayTracing 原视频地址 https://www.youtube.com/playlist?list=PLlrATfBNZ98edc5GshdBtREv5asFW3yXl ::: aside\nReferences 1. [【TheCherno - Ray Tracing(AI翻译)】 01-Welcome to Ray Tracing - VotiveHing](https://www.bilibili.com/video/BV1C3pBe8Emn/) Searched by `ray tracing code` at [bilibili](https://search.bilibili.com/all?vt=15324283\u0026keyword=ray%20tracing%20code\u0026from_source=webtop_search\u0026spm_id_from=333.1007\u0026search_source=5) }}} ::: ","date":"2024-08-03T10:09:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/b-note-raytracing/","title":"Memo: Vis - Gfx | Ray Tracing"},{"content":"Polarize (2024-07-30)\nSource video: 【官方双语】光的起源，散射，以及偏振 | 彩虹灯柱 Part 2\n线偏光与圆偏光的区别在电子的运动方式不同。线偏光是电子沿 一个轴 振动，圆偏光是电子在 一个平面 内运动。 09:27 Refraction Ref:\nSource video: 【官方双语】光为什么会“变慢”，又为什么跟彩虹的颜色有关呢？ | 光学谜题 Part 3\nRef: The Feynman Lectures on Physics #31\nGists:\n(2024-07-30)\n折射是因为 波长 被压缩了啊！ 07:40\n波长压缩是因为每穿过“一层”介质相位就会滞后一些。\n频率不变，因为一个时间段内振动的 次数 没变。\n看起来像被打破的镜子一样，“镜中的像支离破碎”\n波向右传播击中一层介质，介质右侧波的相位比左侧波的相位 慢，所以波就好像是被向 左 拽了一段。\n光一边向右传播一边延迟相位，直到入射波的一个完整的周期进入介质，此时“波前”被延迟的相位就是，这个介质对入射波相位的滞后量\n介质的厚度也会影响出射波的相位吧？因为波的第一点每穿过一层介质就会滞后一点相位，\n光的照射会激发介质的电子的振动，这个振动对应一个“次生”的波，会向介质表面的两侧传播， 向介质外面传播的光就是 反射波，向介质里面传播的波会与入射波 叠加起来。 11:53\n次生波的振幅的 大小 决定了入射波的相位 滞后 多少角度 把一个波想象成一个在平面上 旋转的向量 14:21\n具体地：把一个正弦波想象成一个在x-y平面上绕原点旋转的向量的 y 分量。 (同理，余弦波就是这个旋转向量的 x 分量。)\n波的相位是这个旋转向量的 初始角度。\n两个波的叠加就是两个旋转向量的 向量和。 14:53\n大桥就像一个秋千，秋千的频率与外力的频率的 差异 会影响秋千振幅的大小。 21:48\nSnell\u0026rsquo;s law: The sine of the angle divided by the light\u0026rsquo;s speed is a constant.\n( V G m a l e c a d u s i u s u m m ) ⋱ θ ⋱ ₁ θ ₂ The oscillation of the glass atom are driven by the incident light, and the frequency of the wave inside the glauss after stabilzed equals the frequency of the one of the incident light. ","date":"2024-07-30T11:58:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem/b-note-refract/","title":"Memo: Calc - Opt | Refraction"},{"content":"(2024-07-30)\n","date":"2024-07-30T09:41:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/b-note-psf/","title":"Memo: Vis - Opt | PSF"},{"content":"Searches (2024-07-20)\nI want to find some open-source library for computational camera research.\nSearch: \u0026ldquo;open computational cameras\u0026rdquo; in DDG\nComputational Camera and Photography | Media Arts and Sciences | MIT \u0026hellip;\nNo video, only mp3 DiffuserCam: Lensless Single-exposure 3D Imaging\nBen Mildenhall !!!! Search: \u0026ldquo;open computational cameras library\u0026rdquo;\nAn open-source C++ computer-vision library for plenoptic cameras \u0026hellip; Search: \u0026ldquo;open source code computational cameras library\u0026rdquo;\nOpen Source » Computational Imaging Lab - UCB\ncomputational-photography · GitHub Topics · GitHub\nGitHub - vccimaging/MegapixelAO: This is the open source repository for \u0026hellip;\nQiang Fu Welcome — libcamera\nSearch: \u0026ldquo;differentiable optics\u0026rdquo; in DDG (Listened from Lec4-0:01:00)\ndO: A differentiable engine for Deep Lens design of computational \u0026hellip;\nKUST Differentiable Compound Optics and Processing Pipeline Optimization for End-to-end Camera Design\nCanada company: Algolux, Montreal Differentiable Optics Seminar: Louis Desdoigts - GitHub Pages\nHe has a open source library: dLux - louisdesdoigts.github.io Learning to Model and Calibrate Optics via a Differentiable Wave Optics Simulator\nCode Chromatix : Differentiable wave optics using JAX! - Read the Docs\nused for computational optics Ethan Tseng - GitHub Pages\nHe has two open-source repos. Search: \u0026ldquo;computational optics\u0026rdquo;\nEE367 / CS448I: Computational Imaging - Stanford University\nComputational Optical Imaging: Principle and Technology - Springer\nResearch Overview - Toronto Computational Imaging Group\n他们也研究 NeRF， Search: \u0026ldquo;光学 学习路线\u0026rdquo; in DDG\n如何学好光学？ - 知乎 基础阶段：电磁学—基础光学（几何和波动的简单入门）——电动力学——量子力学——激光原理——固体物理（最好还学一下热力学与统计物理） Communicate with xinge Yang:\nWe now have a WeChat group. Please contact Xinge Yang (singeryang1999) to join the discussion!\nWeChat:\nMe: 杨博您好，非常感谢您的工作[Joyful]\nYang: 感谢你的兴趣！\nMe: 嗯嗯，最近对 diff optics 很感兴趣，想研究一下 dO[Worship]\nYang: 可以啊，不过dO 代码很久没维护了，可以用deeplens\nMe: 嗯嗯好的，太感谢您的建议了[Worship]\nMe: 杨博，可以请您稍微指点一下我应该如何进入 diff optics 这个领域吗？ 我之前是研究三维重建技术：NeRF 和 3DGS，光学方面的知识我只在本科上过电磁场理论。 如果想看懂 deeplens 的代码的话，大致需要哪些知识呢？希望您可以给一些提示[Worship]\nYang: 没问题！deeplens 本质上在做成像模拟，成像一般有两种模型，一个是光线追踪渲染，一个是psf 卷积，其中psf 卷积也可以用光线追踪来计算\nYang: 所以只需要学会光线追踪就行了\nYang: 而光线追踪本质上也只是直线和镜片求相交，以及求折射\nMe: 哇，太感谢杨博的热心回复啦！好，我去研究下[Joyful]\n(2024-07-30)\n我想起来之前在 B 站看到过有人搬运从零开始写光追，的一系列视频，想去“稍后再看”里面找找。\n没想到我之前添加过 3B1B 的：【官方双语】折射率4问4答 | 光学谜题4\n这个光学专题的 Part1 应该是: 【官方双语】这个演示考验你对光的理解 | 彩虹灯柱 Part1\n【官方双语】为何糖总是将光向右偏转：糖的旋光性-SteveMould Optical Design (2024-08-03)\nCurriculum learning for ab initio deep learned refractive optics Nature Commu | Xinge Yang ","date":"2024-07-20T11:41:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/invrender/c-coll-do/","title":"Coll: Differentiable Optics"},{"content":"(2024-07-18)\nTechBeat人工智能社区-生成模型相关Talk\nYilun Xu\nGenPhys: From Physical Processes to Generative Models Arxiv'23 ","date":"2024-07-18T17:41:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/genphys/c-symp-genphys/","title":"Sympo: Gen - Phys | "},{"content":"Searching (2024-07-14)\n20210509_王磊_流模型计算物理视角 机器学习与科学应用 - bilibili space 20210731_董彬_Learning and Learning to Solve PDEs 我赞同这条评论：“感觉这套方法真正落地到比如科学计算上可能主要的困难反而是缺data。。。？”\n机器学习与科学应用 - c2sml.cn\n应用数学新时代的曙光 - 鄂维南 简洁优雅的物理模型虽然精准，但不利于实际生产应用 机器学习可帮助开发简化近似的物理模型，从而可以应用于实际生产 高维逼近：用简单的基本对象逼近给定对象。抽象地说：逼近的难度就是给定对象复杂度 以给定对象能被多少个基本函数近似来衡量该给定对象的复杂度，不适用于高维函数对象。 正确的是复杂度衡量是能否被神经网络逼近 用相应的机器学习模型逼近对应的偏微分方程的解。 应用数学的组成部分：基于第一性原理的数学建模，Data-drive 机器学习方法，算法。 纯数学的组成部分：代数，分析，几何，拓扑。 物理学的组成部分：经典力学，统计力学，电磁学，量子力学 建模包括两个部分：模型的物理原理和分析这些模型的数学工具。前者教给数学家基础物理知识。后者是应用分析，包括常微分方程和偏微分方程，变分法，概率论，渐近分析和随机分析。两者都可以被一个一年的课程所涵盖。 Search: \u0026ldquo;鄂维南 site:bilibili.com\u0026rdquo; in DDG 鄂维南《AI for Science：一场正在发生的科技革命》｜理解未来科学讲座实录AI for Science系列03期\n鄂维南：发生在当下的科技革命\n【240626院士学术报告会】北大鄂维南院士：数学与人工智能- 环境工人失眠版 (Found in s2-eweinan -\u0026gt; 视频笔记\n鄂维南：再谈AI for Science - 北京科学智能研究院\nSearch: \u0026quot;\u0026quot;\nBook (Chinese) in progress - SJTU Book: 深度学习导论与理解 本书以现象驱动介绍深度学习的一些基本知识，以及提供理解 Nature Mother Collision 刚体 Snow\n(2024-12-06)\n如何模拟：一个雪团从天上垂直落下，打在灌木丛上，然后碎开，变成一大一小，还有一些更碎的雪， 有点像彗星尾，也有点像两个小行星碰撞，撞碎了。 Data Dataset References:\nThe Well：可用于机器学习研究的15TB 物理模拟数据集 - 科学数据之美 Pushed by WeChat (Personlzd Subscr. Recommd.) Notes:\n(2024-12-29)\n在科学数据集训练，预测物理现象. The Well 有 PyTorch 接口 r1-Well ","date":"2024-07-14T10:46:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/simu/c-symp-mlphys/","title":"Coll: Phys | ML-based Physics"},{"content":"Solver References:\nFluent GPU求解器:前所未有的速度和规模的CFD研究 | Simulation World - Ansys中国 Directed by the video: Ansys把大模型GPT和自家的产品结合了，已发布AnsysGPT - 曾导SJTU Notes:\n(2024/10/27)\nA promotional introduction of the \u0026lsquo;Fluent\u0026rsquo; GPU-based solver by Ansys. (r1-Ansys)\nI found the slogon of \u0026lsquo;Ansys中国\u0026rsquo; is: \u0026ldquo;奔向确定性的未来。\u0026quot;。 Interesting, I feel is a cue for AI. Searching (2024-07-14)\nSearch: \u0026ldquo;计算物理\u0026rdquo; in DDG\nSearch \u0026ldquo;course code for computational physics\u0026rdquo; in DDG\n(2024-08-03)\nSearch \u0026ldquo;电磁仿真求解器\u0026rdquo; in bilibili Courses Computational Physics - GitHub\n(Found in s2)\nCompPhysics/ComputationalPhysics\nComputational Physics course (PHYS 6350) - GitHub - at the University of Houston\n(Found in s2)\nAlexander2116/ComputationalPhysics\n计算物理(2022/秋季) - 中国科学技术大学\n(Found in s1)\nVideos 【北京大学】计算物理学（全46讲）\n(Found in s1)\nTalk (2024-08-03)\n华为AI科学计算！华为诺亚方舟实验室：可微物理仿真器助力逆问题求解 -邵云峰 - 创新指南针 (Found in s3)\n原来 Ray Tracing 也可以做电磁仿真。 神经网络输出的结果倾向于低频？那我就用 Ray Tracing 算高频的部分？\n他们用可微物理做天线设计：\n前向用麦克斯韦方程计算； 反向用 RNN 和 CNN 替代有限元方法。 “通过发射和接收信号反演物体” 14:33 ","date":"2024-07-14T09:56:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/simu/c-coll-compphys/","title":"Coll: Phys | Computational Physics"},{"content":"减论 (2024-07-05)\n一副图像是一个数据，是一个样本。一个图像是一个 整体，而不是对像素分析。\n所以“图像”这种维度为 (3xHxW) 的 数据 的分布，就是生成模型要估计的分布。\n【减论系列专栏】从分布到生成（一）：什么是图像的分布？本集我们通过与最简单的伯努利分布的类比，尝试回答如下问题：当我们在说图像分布的时候，我们在说什么？\n","date":"2024-07-05T17:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/d-vid-imagegen/","title":"watch: Image Generation"},{"content":"(2024-06-24)\nSearch (祝哥的论文) \u0026ldquo;太赫兹频段冰云体散射特性研究\u0026rdquo; in DDG\n太赫兹冰云辐射散射特性研究和探测参数设计 遥感学报\nRelated reading\n基于神经网络的太赫兹冰云探测反演算法研究 遥感学报 ","date":"2024-06-25T00:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/misc/cem/c-coll-remotsens/","title":"coll: Remote Sensing Technology"},{"content":"PDF | SemScho\nNotes (2024-06-02)\nAbstract\nTask: Display point set surface.\nSolution:\nDefine a smooth manifold surface based on local maps and MLS.\nlocal map from differential geometry: \u0026ldquo;local\u0026rdquo;: neighbors Upsampling and downsampling to fit the points spacing with the screen space resolution.\nThe approximation error is bounded.\nAdvantage: Applicable for any point set\nIntroduction Task: Use point set as the shape representation\nWhy matter?\nPoint sets data are getting popular.\nRepresenting a highly detailed surface with primitives requires substantial small unit that\u0026rsquo;s smaller than a single pixel.\nProblems:\nAlthough point set is an economical representation, point sets contain noisy and redundant points.\nPoint cloud is discrete without explicit surface.\nSolution:\nAdjust the density of points to reconstruct a smooth surface.\nUse polynomials to approximate surface with MLS\nPolynomials: What are basis functions are used to fit the surface function? What does the surface function look like? Projection (affine transformation): weighted sum / linear combination of x,y coordinate\n$$ \\begin{bmatrix} a_1 \\\\\\ a_2 \\\\\\ a_3 \\end{bmatrix} [1 \\quad x \\quad x^2] [^x_y] $$ MLS: The partial derivative of error w.r.t. coefficient matrix 𝐀 equals 0.\nExplain idea:\nGenerating new points is a sampling process on the hidden surface.\nDoubt: Can generative models be applied? as this is a sampling process. The error has a limited bound. Rendering a point set is achieved by up-sampling and down-sampling the point set.\nThe sampling error is bounded. How to prove? Reduce the input point set $P=\\\\{𝐩ᵢ\\\\}$ to \u0026ldquo;representation points\u0026rdquo; $R=\\\\{𝐫ᵢ\\\\}$ defining an MLS surface $S_R$, which approximates the original surface $S_P$.\nSounds like Variational Inference: Let an assumed posterior distribution of z $q(z|x)$ with learnable params (μ,σ) to approximate the true intractable posterior distribution of z $P(z|x)$, through minimizing the KL-divergence.\nOn the other hand, if analyzing ELBO, the goal of maximizing the log-likelihood of x is transformed into maximizing the variational lower bound: ELBO.\nFiguratively, use a proxy object to approximate the true object.\nRelated Work\n(2024-06-07)\nConsolidation\nA practical point cloud includes multiple scans to capture the complete geometry for a nontrivial object.\nTriangulation techniques.\nShortcomings Weighted averaging of multiple scans based on implicit function\nWeighted averaging points directly\nTrianglulation\nDoubt: Perhaps I should compare my Exp: PC01 with this kind of methods. Surface fitting using polynomial\nResampling\nUsing physically-based particle systems to sample an implicit surface\nRepulsive forces\nMLS + Projection + Iterations\nExtending k neighbors to a \u0026ldquo;fan\u0026rdquo;\nPoint Sample Rendering\nConverting geometric models into point-sampled data sets.\nUsing a hierarchy of spheres of different radii to model a high-resolution model\ntime-critical rendering Principal curvatures\nGlobal illumination effects -\u0026gt; ray tracing technique\nResampling the surface in object space during rendering to adapt the display resolution.\nHybrid triangle-point approaches\nQSplat Method A surface is defined by a projection procedure: through which the surface is obtained by projecting the input points.\nBasic assumption: The input point set represent a surface implicitly.\nMain idea:\n$r$ is a raw point deviating from the actural surface due to noise\n$p_i$ is a sampled point on the ground-truth surface.\n$S_p$ is the approximated surface estimated from $p_i$\nFind a local plane for $r$, which serves as a x-y plane for a 3D coordinate system. The local plane is a linear regression for r\u0026rsquo;s neighboring points.\nThen, find surface with referencing the local plane.\nProjection Two steps of the Projection Procedure:\nStep Ⅰ: For an raw input point 𝐫, compute a local plane H:\n$$ H =\\\\{𝐱 | ⟨𝐧,𝐱⟩-D = 0, 𝐱∈ ℝ³\\\\}, 𝐧∈ ℝ³,‖𝐧‖=1 $$The plane H is determined by minimizing a Weighted Sum of Squared Distances from each neighboring point 𝐩ᵢ of the point 𝐫 to the approximate plane H, and solved by weighted least squares.\nThe weight is a function θ of the distance between the projection 𝐪 of 𝐫 on the plane to each of its neighboring point 𝐩ᵢ.\n$$ ∑ᵢ₌₁ᴺ \\underset{Error}{(⟨𝐧,𝐩ᵢ⟩-D)²} ⋅ \\underset{Weights}{θ(‖𝐩ᵢ-𝐪‖)} $$ H 𝐫 𝐧 𝐪 ⋰ ⋰ θ ⋰ D ∙ 𝐩 ᵢ ⋰ O P o f r ⋮ ⋮ ⋮ ⋮ l f r i a f o g ⟨ n s m i 𝐧 e e n , t s 𝐩 ᵢ ⟩ - D Using distances between $pᵢ$ and $q$ aims to optimize the position of the plane H based on the grount-truth point set $pᵢ$.\nθ is a smooth, monotone decreasing function, positive on the whole space\n$N$ is the number of neighbors of 𝐫.\nA plane in 3D is defined with its normal vector 𝐧 and a point 𝐱 within the plane, that deviates from the origin by a distance D along the direction of normal: $⟨𝐧,𝐱⟩=D$\n𝐧 p 𝐱 l a n e D ∙ O r i g i n The distance from 𝐩ᵢ to the plane is the inner product of 𝐧 and 𝐩ᵢ: $⟨𝐧, 𝐩ᵢ⟩$.\nFurther, represent the projection 𝐪 with a parameter t:\n$$𝐪 = 𝐫 + t𝐧$$The loss function: encouraging neighboring points 𝐩ᵢ close to the H, becomes:\n$$ \\begin{aligned} \u0026 ∑ᵢ₌₁ᴺ (⟨𝐧, 𝐩ᵢ⟩-D)² ⋅ θ(‖𝐩ᵢ- 𝐫 - t𝐧‖) \\\\\\ \u0026= ∑ᵢ₌₁ᴺ (⟨𝐧, 𝐩ᵢ⟩-⟨𝐧,𝐪⟩)² ⋅ θ(‖𝐩ᵢ- 𝐫 - t𝐧‖) \\\\\\ \u0026= ∑ᵢ₌₁ᴺ (⟨𝐧, 𝐩ᵢ-𝐪⟩)² ⋅ θ(‖𝐩ᵢ- 𝐫 - t𝐧‖) \\\\\\ \u0026= ∑ᵢ₌₁ᴺ ⟨𝐧, 𝐩ᵢ - 𝐫 - t𝐧⟩^2 ⋅ θ(\\\\| 𝐩ᵢ - 𝐫 - t𝐧 \\\\|) \\end{aligned} $$ H t 𝐧 𝐧 𝐫 𝐪 ⋰ ⋮ ⋮ ∙ ⋰ D θ ⋰ 𝐩 ᵢ ⋰ ⋮ ⋮ ⋮ ⋮ ⟨ 𝐧 , 𝐩 ᵢ - 𝐫 - t 𝐧 ⟩ Denote the target plane H with the set of projections based on raw input points r:\n$$Q(t) = 𝐪 = 𝐫 + t𝐧$$ The t reminds me the sampling in NeRF: $𝐨 + t𝐝$. After rescaling the range of [near,far] to [-1,1], the t ranges from [0,1] to sample points starting from the near plane.\nThe t is not the depth z, but the steps that are compatible for both LLFF scene (infinite boundary) and Blender scene (bounded).\nSimilarly, Point-MVSNet used steps to refine the depth of the point along a ray.\nThe plane H will serve as a \u0026ldquo;ground\u0026rdquo; for measuring the height of each 𝐩ᵢ, using the coordinates (x,y) on the plane H for indexing.\nStep Ⅱ: Approximate surface based on the local plane H:\nThe projection 𝓟 of 𝐫 onto the target surface $S_P$ is a sum of the projection of 𝐫 on the plane H and the \u0026ldquo;residual\u0026rdquo; approximed with a bivariate polynomial: g(x,y),\n$$ \\begin{aligned} \\cal{P}(𝐫) \u0026= 𝐪 + g(0,0)𝐧 \\\\\\ \u0026= 𝐫 + (t+g(0,0))𝐧 \\end{aligned} $$ Similarly, PointFlow in Point-MVSNet also predicts the depth residual to tweak a point.\nA surface equation is bivariate because it only has two dimensions: x and y.\nThe $g(x,y)$ gets optimized by minimizing the weighted sum of neigboring pionts 𝐩ᵢ height error:\n$$∑ᵢ₌₁ᴺ (g(xᵢ,yᵢ) - fᵢ)² ⋅ θ(\\\\| 𝐩ᵢ - 𝐪 \\\\|)$$ l p f o l o c a r a n S l e 𝐫 p H g 𝐫 ( ( 0 0 ⋮ ⋮ 𝐪 , , e 0 0 ⋰ ) ) ⋰ θ ⋰ g 𝐩 ⋰ ( ( ᵢ x x ⋮ ⋮ * ⋮ ⋮ ⋮ ⋮ 𝐪 ᵢ ᵢ ↑ ↓ 𝐧 ᵢ , , E f ⋅ y y r ᵢ ( ᵢ ᵢ r 𝐩 ) ) ᵢ - 𝐪 ) $fᵢ$ is the height of the point 𝐩ᵢ w.r.t. the plane H:\n$$fᵢ= 𝐧⋅(𝐩ᵢ - 𝐪)$$ The projection 𝐪 of 𝐫 is the origin of the plane-H coordinate system.\nThe projection of 𝐫 on the original surface can be represented as: $𝐪+g(0,0)⋅𝐧$\nProperties Properties of the Projection Procedure\n(2024-06-09)\nThe equation holds, when the plane H is the original surface $S_P$:\n$$\\cal{P}(\\cal{P}(𝐫)) = \\cal{P}(𝐫)$$ l p f o l o c a r a n l e 𝐫 g H ( 0 , 0 𝐫 ( ) 0 = 𝐪 , 0 0 ⋰ ) ⋰ w ⋰ g 𝐩 ⋰ ( ( ᵢ x x 𝐪 ᵢ ᵢ ᵢ , , f y y ᵢ ᵢ ᵢ ) ) O s = r u 0 i r g f i a n c a e l S p Compute Projection Computing the Projection\nTradeoff Data Structures and Tradeoffs\nResults Resampling Upsampling based on Voronoi diagram\nhttps://i.ibb.co/2jd5S0V/Paper-computing-and-rendering-point-set-surface-7-Figure8-1.png\nActive Recall LLM Chat Problems:\nI want to find a teacher to correct my understanding References:\nChatGPT Notes:\n(2025-04-06)\nChat with free ChatGPT\nSupports:\nClaude.ai has length limit - Cannot upload paper (neither pdf/html/md).\nYuanbao.tencent.com has a bad image understanding of my \u0026ldquo;hand-draw\u0026rdquo; (whiteboard) image\nDeepSeek.com does not support image\nOptimization in Common Problems:\nThe common features among optimization-based methods Issues:\nPoints displacement References:\nNotes:\n(2025-04-06)\nPoints displacement is the model output\nSupports:\nPoint-MVSNet: Network ➔ ray march 3DGS: BP+GD ➔ dx,dy,dz MLS: Least Square ➔ Height residual Play MLS in PCL Ref (2024-07-20)\nPCL MLS論文Computing and Rendering Point Set Surfaces研讀筆記 - keineahnung2345 - CSDN\n(Found: Search image for \u0026ldquo;voxel grid dilation\u0026rdquo; in DDG ➡ Images from blog: PCL - MLS代碼研讀（十五）- VOXEL_GRID_DILATION上採樣方法 ➡ He has 50 posts for PCL: keineahnung2345的博客 ➡ Since he has a careful study on MLS, he must read the paper.)\n","date":"2024-06-02T10:47:00Z","image":"https://d3i71xaburhd42.cloudfront.net/b7e0ebc678153eb2c341702247d2fafb872f0465/3-Figure3-1.png","permalink":"http://blog.zichen.uk/post/writenotes/model/points/b-note-voronoimls/","title":"Read: Points - Surface | Voronoi + MLS"},{"content":"Source Video: Reflection laws proof using Huygen\u0026rsquo;s principle | Wave optics | Physics | Khan Academy\n","date":"2024-04-12T21:46:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/waveoptics/khan_waveoptics/","title":"watch: WaveOptics - Khan Academy India"},{"content":"camtools Plot the 49 Camera Poses in DTU\nReferences:\n(2024-03-28)\nyxlao/camtools - Github Supports:\nActions:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import numpy as np import os def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min import camtools as ct import open3d as o3d Ks, Ts= [], [] for i in range(49): intrinsics, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan1\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) Ks.append(intrinsics) Ts.append(extrinsics) cameras = ct.camera.create_camera_frames(Ks, Ts) o3d.visualization.draw_geometries([cameras]) Results:\nResult Demo in README Don\u0026rsquo;t know how to add camera frames like his. 1 2 3 4 5 6 0 1 2 3 4 10 9 8 7 6 5 11 12 13 14 15 16 17 18 27 26 25 24 23 22 21 20 19 28 29 30 31 32 33 34 35 36 37 48 47 46 45 44 43 42 41 40 39 38 CameraViewer (2024-03-28)\nxt4d/CameraViewer found by DDG when searching \u0026ldquo;how to visualize camera pose\u0026rdquo;\nIt uses plotly to visualize cameras internally.\n(2024-03-28)\nTest the poses in DTU.\nThe extrinsics (w2c) don\u0026rsquo;t appear on the canvas if keeping the translation vectors.\nI set the translation (camera pose), i.e., 4th column in extrinsics to all 0, then the rotations are shown.\n1 2 3 4 5 # /mnt/data2_z/MVSNet_testing/dtu/scan1/cams/{0:08d}_cam.txt array([[ 0.970263 , 0.00747983, 0.241939 , 0. ], [-0.0147429 , 0.999493 , 0.0282234 , 0. ], [-0.241605 , -0.030951 , 0.969881 , 0. ]], dtype=float32) The reason could be that the translation vector is too large: [-191.02, 3.28832, 22.5401].\nBy reducing it by 1/100 times (extrinsics[:,3] = extrinsics[:,3]/100): [-1.9102, 0.0328832, 0.225401], the camera appears.\nThe w2c (extrinsics) can be prepared as npy files:\n1 2 3 4 5 for i in range(49): intrinsics, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan1\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) extrinsics = extrinsics[:3] extrinsics[:,3] = extrinsics[:,3]/100 np.save(f\u0026#39;/mnt/data2_z/Poses_CamViewer/obj/poses/{i:03d}\u0026#39;, extrinsics) BTW, writing json file manually is a time black hole.\nAs long as the filenames of poses and images are the same, it\u0026rsquo;s okay. The indexing doesn\u0026rsquo;t matter.\n1 ~/Downloads/CameraViewer$ python app.py --root /mnt/data2_z/Poses_CamViewer/obj/ --type w2c --image_size 128 If omitting the argument --type, the program will use poses.json. Otherwise, the program will read directories: poses/ and images/. 49 cameras for scan1\nThe above figure shows the original poses. And the principal axis is facing away from the object. pytransform3d (2024-03-29)\nPlot mesh and cameras:\nVisualizing camera trajectory in Open3D #148 (Found when searching \u0026ldquo;open3d visualize camera poses\u0026rdquo; DDG)\nThe mesh in the image is produced by Meshroom. And then use Figure.plot_camera()\npytransform3d.camera.plot_camera() Example\nCode for plotting pose 1 of DTU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import matplotlib.pyplot as plt import pytransform3d.camera as pc import pytransform3d.transformations as pt w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) c2w = np.linalg.inv(w2c) intrinsics = np.array([ [ 2.89233051e+03, -2.48063349e-04, 8.23205273e+02], [ 0.00000000e+00, 2.88317528e+03, 6.19070918e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]) sensor_size = np.array([1600, 1200]) # image size virtual_image_distance = 1 ax = pt.plot_transform(A2B=c2w, s=0.2) ax.set_xlim(186, 188) ax.set_ylim(3, 5) ax.set_zlim(20, 22) pc.plot_camera( ax, cam2world=c2w, M=intrinsics, sensor_size=sensor_size, virtual_image_distance=virtual_image_distance) plt.show() pytransform3d matplotlib In the matplotlib code, I have corrected the camera position to -extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1]. The 2 results are the same. Plot basis and camera plane:\nCamera Extrinsic Matrix with Example in Python - Part2\n1 2 3 4 5 6 7 # plot the global basis and the transformed camera basis ax = pr.plot_basis(ax) ax = pr.plot_basis(ax, R, offset) # plot the original and transformed image plane ax.plot_surface(xx, yy, Z, alpha=0.75) ax.plot_surface(xxt, yyt, Zt, alpha=0.75) Matplotlib (2024-03-30)\nCode {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 %matplotlib widget import os import numpy as np def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min poses_list = [] for i in range(49): _, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan23\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) poses_list.append({ # \u0026#34;position\u0026#34;: extrinsics[:,-1][:-1], # Wrong \u0026#34;position\u0026#34;: - extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1], \u0026#34;rotation\u0026#34;: extrinsics[:-1][:,:-1], }) import matplotlib.pyplot as plt from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection def plot_camera_poses(poses, axis_length=0.1): fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # Plot each camera pose for pose in poses: # Extract camera position and orientation cam_position = pose[\u0026#39;position\u0026#39;] cam_orientation = pose[\u0026#39;rotation\u0026#39;] # Plot camera position ax.scatter(cam_position[0], cam_position[1], cam_position[2], c=\u0026#39;r\u0026#39;, marker=\u0026#39;.\u0026#39;) axes_endpoints = cam_position + axis_length * cam_orientation # Plot camera canvas determined by 4 corners (forming 2 triangles) corner_back_1 = axes_endpoints[2] + 0.3*axis_length * cam_orientation[1] + 0.3*axis_length * cam_orientation[0] corner_back_2 = axes_endpoints[2] - 0.3*axis_length * cam_orientation[1] + 0.3*axis_length * cam_orientation[0] corner_back_3 = axes_endpoints[2] + 0.3*axis_length * cam_orientation[1] - 0.3*axis_length * cam_orientation[0] corner_back_4 = axes_endpoints[2] - 0.3*axis_length * cam_orientation[1] - 0.3*axis_length * cam_orientation[0] verts = np.array([[ corner_back_1, corner_back_2, corner_back_4]]) tri = Poly3DCollection(verts, alpha=0.3, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) verts = np.array([[ corner_back_1, corner_back_3, corner_back_4]]) tri = Poly3DCollection(verts, alpha=0.3, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) # Hull of the camera ax.plot3D([cam_position[0], corner_back_1[0]], [cam_position[1], corner_back_1[1]], [cam_position[2], corner_back_1[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_2[0]], [cam_position[1], corner_back_2[1]], [cam_position[2], corner_back_2[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_3[0]], [cam_position[1], corner_back_3[1]], [cam_position[2], corner_back_3[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_4[0]], [cam_position[1], corner_back_4[1]], [cam_position[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) # Camera plane edges ax.plot3D([corner_back_1[0], corner_back_2[0]], [corner_back_1[1], corner_back_2[1]], [corner_back_1[2], corner_back_2[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_3[0], corner_back_1[0]], [corner_back_3[1], corner_back_1[1]], [corner_back_3[2], corner_back_1[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_3[0], corner_back_4[0]], [corner_back_3[1], corner_back_4[1]], [corner_back_3[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_2[0], corner_back_4[0]], [corner_back_2[1], corner_back_4[1]], [corner_back_2[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) lines = [] for idx in range(len(poses)-1): lines.append([poses[idx][\u0026#39;position\u0026#39;], poses[idx+1][\u0026#39;position\u0026#39;]]) # Create a line collection lc = Line3DCollection(lines, colors=\u0026#39;b\u0026#39;, linewidths=1, label=\u0026#39;Camera Trajectory\u0026#39;) ax.add_collection3d(lc) # Set plot limits and labels ax.set_box_aspect([1, 1, 1]) ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) plt.show() return ax # Plot camera poses ax_prev = plot_camera_poses(poses_list, axis_length=60) # Plot point cloud import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/mnt/data2_z/SampleSet/MVS Data/Points/stl/stl023_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vs = np.asarray(pcd.points) samples = vs[np.random.choice(vs.shape[0],100)] x = samples[:,0] y = samples[:,1] z = samples[:,2] ax_prev.scatter(x, y, z, color=\u0026#39;gray\u0026#39;, alpha=0.4) }}} The Line3DCollection usage is seen from an chatGPT-generated anwser: How to visualize colmap export that Camera parameters -SO open3d camera hull (2024-03-29)\ncreate_camera_visualization()\nSample code: Is there a way to draw a camera in a visualizer? #3876 (Found when searching \u0026ldquo;open3d draw cameras\u0026rdquo; DDG)\ncamera moves Iterate Multiple Camera Poses (Extrinsics)\nReferences:\n(2024-03-31)\nSetting the extrinsic matrix for ViewControl #2121 - Open3D Searched by \u0026ldquo;open3d camera extrinsic set_extrinsic\u0026rdquo; at DDG Supports:\nActions:\nIterate multiple camera poses (extrinsics):\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import numpy as np import os def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min poses_list = [] for i in range(49): _, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan23\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) poses_list.append({ \u0026#34;position\u0026#34;: - extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1], \u0026#34;rotation\u0026#34;: extrinsics[:-1][:,:-1], \u0026#34;extrinsics\u0026#34;: extrinsics, }) import time import itertools import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window(window_name=\u0026#34;Playback\u0026#34;, visible=True) vis.get_render_option().background_color = np.asarray([0, 0, 0]) vis.add_geometry(pcd) ctr = vis.get_view_control() cam = ctr.convert_to_pinhole_camera_parameters() for view_idx, camParams in zip(itertools.count(), poses_list): cam.extrinsic = camParams[\u0026#39;extrinsics\u0026#39;] ctr.convert_from_pinhole_camera_parameters(cam, True) vis.poll_events() vis.update_renderer() time.sleep(0.1) vis.destroy_window() Note: The argument allow_arbitrary=True is required in convert_from_pinhole_camera_parameters(cam, True) (using 0.18.0), Custom Animation Customized visualization - Open3D Docs\nCode from View Control No Effect in 0.17 #6098\n1 2 3 4 5 6 7 8 9 import open3d as o3d def rotate(vis): ctr = vis.get_view_control() ctr.rotate(5, 0) return False frame = o3d.geometry.TriangleMesh.create_coordinate_frame() o3d.visualization.draw_geometries_with_animation_callback([frame], rotate) Others demul/extrinsic2pyramid\nOpenCV has example code. How to plot the camera and image positions from camera calibration data? pytorch3d/docs/tutorials/utils/camera_visualization.\nsxyu/nerfvis: NeRF visualization library under construction\nBlender add-on: Photogrammetry-Importer How to visualize colmap export \u0026lsquo;images.txt\u0026rsquo; in blender? -SO\nkaolin.render.camera — Kaolin documentation - Read the Docs\nWebGL Visualizing the Camera\nOpenCV (2024-04-01)\nOpenCV: cv::viz::WCameraPosition Class Reference\nDraw coordinates axes\nHow to draw 3D Coordinate Axes with OpenCV for face pose estimation? - SO\n1 2 scale = 0.1 img = cv2.drawFrameAxes(img, K, distortion, rotation_vec, translation_vec, scale) cvtkit nburgdorfer/cvtkit\n(Found: nburgdorfer/confidence-based-fusion -\u0026gt; github.io -\u0026gt; cvtkit)\n(2024-07-18)\nFunctions: Point cloud ➡ GIF, Video; Mesh ➡ GIF, Video; Depth map ➡ Visibility map. Tutorials 1 pip install cvt imageio-ffmpeg ply to mp4 (2024-07-19)\nThe input camera poses can use the ones of DTU dataset.\n1 2 3 4 python script_ply2gif_cvtkit.py \\ -p /home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/points/scan001_l3.ply \\ -c /mnt/data2_z/MVSNet_testing/dtu/scan1/cams \\ -n 48 # Use the 49 poses The default value for the argument --video_file is None. There will be an error if without setting it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 (AIkui) yi@yi-Alienware:~/Experiments/PointCloud/log$ python script_ply2gif_cvtkit.py -p /home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/points/scan001_l3.ply -c /mnt/data2_z/MVSNet_testing/dtu/scan1/cams [Open3D INFO] EGL headless mode enabled. FEngine (64 bits) created at 0x55882c2a12c0 (threading is enabled) EGL(1.5) OpenGL(4.1) Traceback (most recent call last): File \u0026#34;/home/yi/OneDrive/Exercises/Experiments/PointCloud/log/script_ply2gif_cvtkit.py\u0026#34;, line 72, in \u0026lt;module\u0026gt; main() File \u0026#34;/home/yi/OneDrive/Exercises/Experiments/PointCloud/log/script_ply2gif_cvtkit.py\u0026#34;, line 59, in main with imageio.get_writer(video_file, mode=\u0026#34;I\u0026#34;, fps=ARGS.fps) as writer: File \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/imageio/v2.py\u0026#34;, line 163, in get_writer image_file = imopen(uri, \u0026#34;w\u0026#34; + mode, **imopen_args) File \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/imageio/core/imopen.py\u0026#34;, line 240, in imopen raise err_type(err_msg) ValueError: ImageIO does not generally support reading folders. Limited support may be available via specific plugins. Specify the plugin explicitly using the `plugin` kwarg, e.g. `plugin=\u0026#39;DICOM\u0026#39;` Similar problem discussed in: One-Shot_Free-View_Neural_Talking_Head_Synthesis - Issue#41 ","date":"2024-03-28T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/pkg-camera_plots/","title":"Memo: Vis - 3D | Visualize Camera Poses"},{"content":"(Feature image from: 3D point set processing in python: a quick overview - Medium)\nRead-Write plyfile (2024-03-27)\nRead ply file (Polygon File Format): Docs\n1 2 3 4 5 6 7 import numpy from plyfile import PlyData, PlyElement with open(\u0026#39;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#39;, \u0026#39;rb\u0026#39;) as f: plydata = PlyData.read(f) np.array(plydata.elements[0].data)[0] Output:\n1 (49.720848, -54.11675, 672.04956, 0.9649841, -0.08213623, -0.24911714, 102, 70, 44) The returned tuple is a single data, whose datatype has 9 members. I want to only take the first 3 values: x,y,z. But it\u0026rsquo;s not allowed to use syntax like [:3] to slice it.\nWrite a ply file\nExample in 3DGS:\nRead Numer of Points Problems:\nGiven a .ply file, obtain the number of pints Open3D (2024-03-27)\nCompare open3d, plyfile, pyntcloud, and meshio\nopen3d: has good docs. How To Read and Write ply File in Python - Jdhao\n1 pip install open3d plyfile: lightweight. Could be slow when reading a large ply file. Python plyfile vs pymesh - SO\nRead Various file formats\nOpen3d read .ply file\n1 2 3 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) ::: aside\nReferences: Docs ::: Open3D converts pcd to np.array and visualize it: Docs\n1 2 3 4 import numpy as np xyz_load = np.asarray(pcd.points) # (2880879, 3) print(f\u0026#39;xyz_load:\\n {xyz_load}\u0026#39;) o3d.visualization.draw_geometries([pcd]) Setting camera directions:\n1 2 3 4 lookat = np.array([[500.],[500.], [500.]]) up = np.array([[0.853452],[-0.447425], [0.267266]]) front = np.array([[0.417749],[0.893913],[0.162499]]) o3d.visualization.draw_geometries([pcd], width=500, height=500, lookat=lookat, up=up, front=front, zoom=1.0) open3d.visualization.draw_geometries\nlookat is the window center. set_lookat(), set_front(), set_up() usage of VisualControl #2139\nVisualize Open3D visualizes a point cloud from a specified camera pose:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.add_geometry(pcd) vis.get_render_option().background_color = np.asarray([1,1,1]) view_ctl = vis.get_view_control() w2c = np.array([[-0.636298, -0.727666, 0.25618, -143.534], [0.0315712, 0.307237, 0.951109, -579.42], [-0.770797, 0.613276, -0.172521, 759.831], [ 0.0, 0.0, 0.0, 1.0]]) # cam 38 cam = view_ctl.convert_to_pinhole_camera_parameters() cam.extrinsic = w2c view_ctl.convert_from_pinhole_camera_parameters(cam, True) vis.run() vis.destroy_window() Note: crete_window() must precedes add_geometry() Options for rendering\n(2024-07-08)\nSet zoom of the visualizer. r1-Docs\n1 view_ctl.set_zoom(0.14) Set window size r2:\n1 2 vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window(width=800, height=800) Capture the current window:\n1 2 3 # vis.run() vis.capture_screen_image(\u0026#34;PC01_upPC_240709.png\u0026#34;, do_render=True) vis.destroy_window() ::: aside\nReferences: {{{\nDocs (Found in DDG:open3D set camera distance No luck. ➔ DDG: set fov ➔ Tutorial ➔ DDG: set zoom) Found in DDG }}} :::\nShortcuts Open3D Shortcut keysr1-Docs\n(2024-07-10)\nPress H will show help message in terminal or output cell. ::: aside\nReferences: Visualization — Open3D 0.9.0 documentation Search by \u0026ldquo;open3d visualizer keyboard behaviors collection\u0026rdquo; at DDG ::: Help info (0.18) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 -- Mouse view control -- Left button + drag : Rotate. Ctrl + left button + drag : Translate. Wheel button + drag : Translate. Shift + left button + drag : Roll. Wheel : Zoom in/out. -- Keyboard view control -- [/] : Increase/decrease field of view. R : Reset view point. Ctrl/Cmd + C : Copy current view status into the clipboard. Ctrl/Cmd + V : Paste view status from clipboard. -- General control -- Q, Esc : Exit window. H : Print help message. P, PrtScn : Take a screen capture. D : Take a depth capture. O : Take a capture of current rendering settings. Alt + Enter : Toggle between full screen and windowed mode. -- Render mode control -- L : Turn on/off lighting. +/- : Increase/decrease point size. Ctrl + +/- : Increase/decrease width of geometry::LineSet. N : Turn on/off point cloud normal rendering. S : Toggle between mesh flat shading and smooth shading. W : Turn on/off mesh wireframe. B : Turn on/off back face rendering. I : Turn on/off image zoom in interpolation. T : Toggle among image render: no stretch / keep ratio / freely stretch. -- Color control -- 0..4,9 : Set point cloud color option. 0 - Default behavior, render point color. 1 - Render point color. 2 - x coordinate as color. 3 - y coordinate as color. 4 - z coordinate as color. 9 - normal as color. Ctrl + 0..4,9: Set mesh color option. 0 - Default behavior, render uniform gray color. 1 - Render point color. 2 - x coordinate as color. 3 - y coordinate as color. 4 - z coordinate as color. 9 - normal as color. Shift + 0..4 : Color map options. 0 - Gray scale color. 1 - JET color map. 2 - SUMMER color map. 3 - WINTER color map. 4 - HOT color map. Other Libs pyminiply claims that it\u0026rsquo;s faster than open3d.\nply-parser\n(2024-05-10)\nzishun/awesome-geometry-processing\n(Found by Perplexity: \u0026ldquo;Compare 3 libraries: CGAL (Computational Geometry Algorithms Library), Open3D, and PointCloudLibrary (PCL)\u0026rdquo;)\nWis3D (2024-04-07)\nzju3dv/Wis3D\nPCL PCL Tutorial(2014) Jeff Delmerico\nHe have projects on volumetric reconstruction Jeff\u0026rsquo;s repo: jeffdelmerico/pointcloud_tutorial\nCompile on Ubu 22.04 ✅ (2024-05-11)\nReference: Docs\nSystem specs:\nUbuntu 22.04.4 LTS x86_64, Kernel: 6.5.0-28-generic, gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0 Cudatoolkit: cuda_11.6.r11.6/compiler.30794723_0 Nvidia Driver: 545.23.08 GPU: 3090Ti CPU: AMD Ryzen 7 5700X (16) @ 3.400GHz Board: X570 AORUS PRO WIFI -CF. Memory: 16GB. Steps:\nDownload source.tar.gz (pcl-1.14.1)\nUncompress: tar xvf source.tar.gz\nBuild:\n1 2 cd pcl cmake -B ./build -DCMAKE_BUILD_TYPE=RelWithDebInfo Compile and install:\n1 2 3 cd ./build make -j8 sudo make -j8 install The library is installed in /usr/lib and /usr/include/\nInstall paths {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 -- Installing: /usr/local/lib/libpcl_kdtree.so.1.14.1 -- Installing: /usr/local/lib/libpcl_kdtree.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_kdtree.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_kdtree.so -- Installing: /usr/local/lib/pkgconfig/pcl_kdtree.pc -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/kdtree.h -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/io.h -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/kdtree_flann.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/impl/io.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/impl/kdtree_flann.hpp -- Installing: /usr/local/lib/libpcl_octree.so.1.14.1 -- Installing: /usr/local/lib/libpcl_octree.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_octree.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_octree.so -- Installing: /usr/local/lib/pkgconfig/pcl_octree.pc -- Installing: /usr/local/include/pcl-1.14/pcl/octree/octree_base.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/octree/impl/octree_base.hpp ... -- Installing: /usr/local/lib/libpcl_search.so.1.14.1 -- Installing: /usr/local/lib/libpcl_search.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_search.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_search.so -- Installing: /usr/local/lib/pkgconfig/pcl_search.pc -- Installing: /usr/local/include/pcl-1.14/pcl/search/search.h -- Installing: /usr/local/include/pcl-1.14/pcl/search/kdtree.h ... -- Installing: /usr/local/lib/libpcl_sample_consensus.so.1.14.1 -- Installing: /usr/local/lib/libpcl_sample_consensus.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_sample_consensus.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_sample_consensus.so -- Installing: /usr/local/lib/pkgconfig/pcl_sample_consensus.pc -- Installing: /usr/local/include/pcl-1.14/pcl/sample_consensus/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/sample_consensus/impl/mlesac.hpp -- Installing: /usr/local/lib/libpcl_filters.so.1.14.1 -- Installing: /usr/local/lib/libpcl_filters.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_filters.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_filters.so -- Installing: /usr/local/lib/pkgconfig/pcl_filters.pc -- Installing: /usr/local/include/pcl-1.14/pcl/filters/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/filters/impl/farthest_point_sampling.hpp -- Installing: /usr/local/lib/pkgconfig/pcl_2d.pc -- Installing: /usr/local/include/pcl-1.14/pcl/2d/convolution.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/2d/impl/convolution.hpp ... -- Installing: /usr/local/lib/pkgconfig/pcl_geometry.pc -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/eigen.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/triangle_mesh.h -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/impl/polygon_operations.hpp -- Installing: /usr/local/lib/libpcl_io_ply.so.1.14.1 -- Installing: /usr/local/lib/libpcl_io_ply.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_io_ply.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_io_ply.so -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/byte_order.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/io_operators.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/ply.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/ply_parser.h -- Installing: /usr/local/lib/libpcl_io.so.1.14.1 -- Installing: /usr/local/lib/libpcl_io.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_io.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_io.so -- Installing: /usr/local/lib/pkgconfig/pcl_io.pc -- Installing: /usr/local/include/pcl-1.14/pcl/io/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/eigen.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/compression/octree_pointcloud_compression.h -- Installing: /usr/local/include/pcl-1.14/pcl/compression/color_coding.h -- Installing: /usr/local/include/pcl-1.14/pcl/compression/compression_profiles.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/io/impl/ascii_io.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/io/impl/pcd_io.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/io/impl/octree_pointcloud_compression.hpp -- Installing: /usr/local/lib/libpcl_features.so.1.14.1 -- Installing: /usr/local/lib/libpcl_features.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_features.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_features.so -- Installing: /usr/local/lib/pkgconfig/pcl_features.pc -- Installing: /usr/local/include/pcl-1.14/pcl/features/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/features/boundary.h -- Installing: /usr/local/include/pcl-1.14/pcl/features/range_image_border_extractor.h -- Installing: /usr/local/include/pcl-1.14/pcl/features/impl/board.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/features/impl/boundary.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/features/impl/range_image_border_extractor.hpp -- Installing: /usr/local/lib/libpcl_ml.so.1.14.1 -- Installing: /usr/local/lib/libpcl_ml.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_ml.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_ml.so -- Installing: /usr/local/lib/pkgconfig/pcl_ml.pc -- Installing: /usr/local/include/pcl-1.14/pcl/ml/feature_handler.h -- Installing: /usr/local/include/pcl-1.14/pcl/ml/multi_channel_2d_comparison_feature.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/dt/decision_forest.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/ferns/fern.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/impl/dt/decision_forest_evaluator.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/impl/svm/svm_wrapper.hpp -- Installing: /usr/local/lib/libpcl_segmentation.so.1.14.1 -- Installing: /usr/local/lib/libpcl_segmentation.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_segmentation.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_segmentation.so -- Installing: /usr/local/lib/pkgconfig/pcl_segmentation.pc -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/extract_clusters.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/impl/extract_clusters.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/impl/cpc_segmentation.hpp -- Installing: /usr/local/lib/libpcl_surface.so.1.14.1 -- Installing: /usr/local/lib/libpcl_surface.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_surface.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_surface.so -- Installing: /usr/local/lib/pkgconfig/pcl_surface.pc -- Installing: /usr/local/include/pcl-1.14/pcl/surface/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/surface/eigen.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/surface/poisson.h -- Installing: /usr/local/include/pcl-1.14/pcl/surface/3rdparty/poisson4/allocator.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/surface/impl/gp3.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/surface/impl/grid_projection.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/surface/impl/marching_cubes.hpp ... -- Installing: /usr/local/lib/libpcl_registration.so.1.14.1 -- Installing: /usr/local/lib/libpcl_registration.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_registration.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_registration.so -- Installing: /usr/local/lib/pkgconfig/pcl_registration.pc -- Installing: /usr/local/include/pcl-1.14/pcl/registration/eigen.h -- Installing: /usr/local/include/pcl-1.14/pcl/registration/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/transforms.h -- Installing: /usr/local/include/pcl-1.14/pcl/registration/transformation_estimation.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/gicp.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/impl/default_convergence_criteria.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/registration/impl/correspondence_estimation.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/impl/transformation_estimation_3point.hpp -- Installing: /usr/local/lib/libpcl_keypoints.so.1.14.1 -- Installing: /usr/local/lib/libpcl_keypoints.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_keypoints.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_keypoints.so -- Installing: /usr/local/lib/pkgconfig/pcl_keypoints.pc -- Installing: /usr/local/include/pcl-1.14/pcl/keypoints/keypoint.h -- Installing: /usr/local/include/pcl-1.14/pcl/keypoints/narf_keypoint.h -- Installing: /usr/local/include/pcl-1.14/pcl/keypoints/sift_keypoint.h ... -- Installing: /usr/local/lib/libpcl_tracking.so.1.14.1 -- Installing: /usr/local/lib/libpcl_tracking.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_tracking.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_tracking.so -- Installing: /usr/local/lib/pkgconfig/pcl_tracking.pc -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/tracking.h -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/tracker.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/impl/tracking.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/impl/tracker.hpp ... -- Installing: /usr/local/lib/libpcl_recognition.so.1.14.1 -- Installing: /usr/local/lib/libpcl_recognition.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_recognition.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_recognition.so -- Installing: /usr/local/lib/pkgconfig/pcl_recognition.pc -- Installing: /usr/local/include/pcl-1.14/pcl/recognition/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/recognition/ransac_based/auxiliary.h ... -- Installing: /usr/local/lib/libpcl_stereo.so.1.14.1 -- Installing: /usr/local/lib/libpcl_stereo.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_stereo.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_stereo.so -- Installing: /usr/local/lib/pkgconfig/pcl_stereo.pc -- Installing: /usr/local/include/pcl-1.14/pcl/stereo/stereo_grabber.h -- Installing: /usr/local/include/pcl-1.14/pcl/stereo/stereo_matching.h ... -- Installing: /usr/local/bin/pcl_pcd_convert_NaN_nan -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd_convert_NaN_nan\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd2ply -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd2ply\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_xyz2pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_xyz2pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pclzf2pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pclzf2pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd2vtk -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd2vtk\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_add_gaussian_noise -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_add_gaussian_noise\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd_change_viewpoint -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd_change_viewpoint\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_concatenate_points_pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_concatenate_points_pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_demean_cloud -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_demean_cloud\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_generate -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_generate\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_convert_pcd_ascii_binary -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_convert_pcd_ascii_binary\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd_introduce_nan -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd_introduce_nan\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_hdl_grabber -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_hdl_grabber\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2obj -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2obj\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2ply -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2ply\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2raw -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2raw\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_plyheader -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_plyheader\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_plane_projection -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_plane_projection\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_compute_hausdorff -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_compute_hausdorff\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_compute_cloud_error -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_compute_cloud_error\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_voxel_grid -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_voxel_grid\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_passthrough_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_passthrough_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_radius_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_radius_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_outlier_removal -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_outlier_removal\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_fast_bilateral_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_fast_bilateral_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_morph -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_morph\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_local_max -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_local_max\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_grid_min -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_grid_min\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_cluster_extraction -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_cluster_extraction\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_progressive_morphological_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_progressive_morphological_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_mls_smoothing -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_mls_smoothing\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_uniform_sampling -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_uniform_sampling\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_sac_segmentation_plane -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_sac_segmentation_plane\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_train_unary_classifier -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_train_unary_classifier\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_unary_classifier_segment -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_unary_classifier_segment\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_crf_segmentation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_crf_segmentation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_train_linemod_template -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_train_linemod_template\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_normal_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_normal_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_boundary_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_boundary_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_fpfh_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_fpfh_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_vfh_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_vfh_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_spin_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_spin_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_extract_feature -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_extract_feature\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_marching_cubes_reconstruction -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_marching_cubes_reconstruction\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_gp3_surface -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_gp3_surface\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_poisson_reconstruction -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_poisson_reconstruction\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_icp -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_icp\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_icp2d -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_icp2d\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_elch -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_elch\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_lum -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_lum\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ndt2d -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ndt2d\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ndt3d -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ndt3d\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_transform_point_cloud -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_transform_point_cloud\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_transform_from_viewpoint -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_transform_from_viewpoint\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_match_linemod_template -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_match_linemod_template\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_linemod_detection -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_linemod_detection\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; }}}\n(2024-06-25)\nI can\u0026rsquo;t cmake the project: dtcMLOps/upsamplingCloudPCL, with errors shown below.\nError: {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 (base) zichen@homepc:~/Downloads/upsamplingCloudPCL$ cmake -Bbuild ========================================= Project: upsampling_cloud ========================================= CMake Warning (dev) at /usr/local/share/pcl-1.14/Modules/FindFLANN.cmake:45 (find_package): Policy CMP0144 is not set: find_package uses upper-case \u0026lt;PACKAGENAME\u0026gt;_ROOT variables. Run \u0026#34;cmake --help-policy CMP0144\u0026#34; for policy details. Use the cmake_policy command to set the policy and suppress this warning. CMake variable FLANN_ROOT is set to: /usr For compatibility, find_package is ignoring the variable, but code in a .cmake module might still use it. Call Stack (most recent call first): /usr/local/share/pcl-1.14/PCLConfig.cmake:250 (find_package) /usr/local/share/pcl-1.14/PCLConfig.cmake:295 (find_flann) /usr/local/share/pcl-1.14/PCLConfig.cmake:551 (find_external_library) CMakeLists.txt:29 (find_package) This warning is for project developers. Use -Wno-dev to suppress it. -- FLANN found (include: /usr/include, lib: /usr/lib/x86_64-linux-gnu/libflann_cpp.so) -- Could NOT find Pcap (missing: PCAP_LIBRARIES PCAP_INCLUDE_DIRS) CMake Error at /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/patches/99/FindHDF5.cmake:241 (try_compile): Unknown extension \u0026#34;.c\u0026#34; for file /home/zichen/Downloads/upsamplingCloudPCL/build/CMakeFiles/hdf5/cmake_hdf5_test.c try_compile() works only for enabled languages. Currently these are: CXX See project() command to enable other languages. Call Stack (most recent call first): /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/patches/99/FindHDF5.cmake:596 (_HDF5_test_regular_compiler_C) /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/VTK-vtk-module-find-packages.cmake:444 (find_package) /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/vtk-config.cmake:150 (include) /usr/local/share/pcl-1.14/PCLConfig.cmake:264 (find_package) /usr/local/share/pcl-1.14/PCLConfig.cmake:313 (find_VTK) /usr/local/share/pcl-1.14/PCLConfig.cmake:548 (find_external_library) CMakeLists.txt:29 (find_package) CMake Error at /usr/local/share/pcl-1.14/PCLConfig.cmake:335 (string): string sub-command REGEX, mode REPLACE needs at least 6 arguments total to command. Call Stack (most recent call first): /usr/local/share/pcl-1.14/PCLConfig.cmake:548 (find_external_library) CMakeLists.txt:29 (find_package) -- Checking for module \u0026#39;libusb-1.0\u0026#39; -- No package \u0026#39;libusb-1.0\u0026#39; found -- Could NOT find libusb (missing: libusb_LIBRARIES libusb_INCLUDE_DIR) -- Found Qhull version 8.0.2 CMake Error at /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/patches/99/FindHDF5.cmake:241 (try_compile): Unknown extension \u0026#34;.c\u0026#34; for file /home/zichen/Downloads/upsamplingCloudPCL/build/CMakeFiles/hdf5/cmake_hdf5_test.c try_compile() works only for enabled languages. Currently these are: CXX See project() command to enable other languages. Call Stack (most recent call first): /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/patches/99/FindHDF5.cmake:596 (_HDF5_test_regular_compiler_C) /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/VTK-vtk-module-find-packages.cmake:444 (find_package) /usr/lib/x86_64-linux-gnu/cmake/vtk-9.1/vtk-config.cmake:150 (include) /usr/local/share/pcl-1.14/PCLConfig.cmake:264 (find_package) /usr/local/share/pcl-1.14/PCLConfig.cmake:313 (find_VTK) /usr/local/share/pcl-1.14/PCLConfig.cmake:548 (find_external_library) CMakeLists.txt:29 (find_package) CMake Error at /usr/local/share/pcl-1.14/PCLConfig.cmake:335 (string): string sub-command REGEX, mode REPLACE needs at least 6 arguments total to command. Call Stack (most recent call first): /usr/local/share/pcl-1.14/PCLConfig.cmake:548 (find_external_library) CMakeLists.txt:29 (find_package) -- PCL status: -- version: 1.14.1 -- directory: /usr/local/share/pcl-1.14 CMake Warning (dev) at /usr/local/share/cmake-3.28/Modules/FetchContent.cmake:1331 (message): The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is not set. The policy\u0026#39;s OLD behavior will be used. When using a URL download, the timestamps of extracted files should preferably be that of the time of extraction, otherwise code that depends on the extracted contents might not be rebuilt if the URL changes. The OLD behavior preserves the timestamps from the archive instead, but this is usually not what you want. Update your project to the NEW behavior or specify the DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this robustness issue. Call Stack (most recent call first): cmake/functions.cmake:5 (FetchContent_Declare) CMakeLists.txt:39 (fetch_project) This warning is for project developers. Use -Wno-dev to suppress it. CMake Warning (dev) at build/_deps/cloudparse-src/CMakeLists.txt:18 (find_package): Policy CMP0074 is not set: find_package uses \u0026lt;PackageName\u0026gt;_ROOT variables. Run \u0026#34;cmake --help-policy CMP0074\u0026#34; for policy details. Use the cmake_policy command to set the policy and suppress this warning. CMake variable PCL_ROOT is set to: /usr/local For compatibility, CMake is ignoring the variable. This warning is for project developers. Use -Wno-dev to suppress it. -- PCL status: -- version: 1.14.1 -- directory: /usr/local/share/pcl-1.14 CMake Warning (dev) at /usr/local/share/cmake-3.28/Modules/FetchContent.cmake:1331 (message): The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is not set. The policy\u0026#39;s OLD behavior will be used. When using a URL download, the timestamps of extracted files should preferably be that of the time of extraction, otherwise code that depends on the extracted contents might not be rebuilt if the URL changes. The OLD behavior preserves the timestamps from the archive instead, but this is usually not what you want. Update your project to the NEW behavior or specify the DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this robustness issue. Call Stack (most recent call first): cmake/functions.cmake:5 (FetchContent_Declare) CMakeLists.txt:44 (fetch_project) This warning is for project developers. Use -Wno-dev to suppress it. ========================================= Project: upsampling_cloud COMPILED WITH CMAKE 3.28.0-rc3 ========================================= -- Configuring incomplete, errors occurred! }}}\nAttempts:\nI installed libvtk9-dev: sudo aptitude install libvtk9-dev and sudo apt-get install libpcap-dev libusb-1.0-0-dev\nCompile again.\nAnd I\nEnable C language: Add the following line near the top of your CMakeLists.txt file, right after the project() command: enable_language(C) (Following the instrction of Claude3.5 Sonnet)\nError: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 (base) zichen@homepc:~/Downloads/upsamplingCloudPCL$ cmake -B build -- The CXX compiler identification is GNU 9.5.0 -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- The C compiler identification is GNU 9.5.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done ========================================= Project: upsampling_cloud ========================================= CMake Warning (dev) at /usr/local/share/pcl-1.14/Modules/FindFLANN.cmake:45 (find_package): Policy CMP0144 is not set: find_package uses upper-case \u0026lt;PACKAGENAME\u0026gt;_ROOT variables. Run \u0026#34;cmake --help-policy CMP0144\u0026#34; for policy details. Use the cmake_policy command to set the policy and suppress this warning. CMake variable FLANN_ROOT is set to: /usr For compatibility, find_package is ignoring the variable, but code in a .cmake module might still use it. Call Stack (most recent call first): /usr/local/share/pcl-1.14/PCLConfig.cmake:250 (find_package) /usr/local/share/pcl-1.14/PCLConfig.cmake:295 (find_flann) /usr/local/share/pcl-1.14/PCLConfig.cmake:551 (find_external_library) CMakeLists.txt:30 (find_package) This warning is for project developers. Use -Wno-dev to suppress it. -- Checking for module \u0026#39;flann\u0026#39; -- Found flann, version 1.9.1 -- Found FLANN: /usr/lib/x86_64-linux-gnu/libflann_cpp.so -- FLANN found (include: /usr/include, lib: /usr/lib/x86_64-linux-gnu/libflann_cpp.so) -- Found OpenMP_CXX: -fopenmp (found version \u0026#34;4.5\u0026#34;) -- Found OpenMP: TRUE (found version \u0026#34;4.5\u0026#34;) found components: CXX -- Found Pcap: /usr/lib/x86_64-linux-gnu/libpcap.so -- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \u0026#34;1.2.11\u0026#34;) -- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \u0026#34;1.6.37\u0026#34;) CMake Error at /usr/local/share/pcl-1.14/PCLConfig.cmake:335 (string): string sub-command REGEX, mode REPLACE needs at least 6 arguments total to command. Call Stack (most recent call first): /usr/local/share/pcl-1.14/PCLConfig.cmake:548 (find_external_library) CMakeLists.txt:30 (find_package) -- Checking for module \u0026#39;libusb-1.0\u0026#39; -- Found libusb-1.0, version 1.0.25 -- Found libusb: /usr/lib/x86_64-linux-gnu/libusb-1.0.so -- Found Qhull version 8.0.2 CMake Error at /usr/local/share/pcl-1.14/PCLConfig.cmake:335 (string): string sub-command REGEX, mode REPLACE needs at least 6 arguments total to command. Call Stack (most recent call first): /usr/local/share/pcl-1.14/PCLConfig.cmake:548 (find_external_library) CMakeLists.txt:30 (find_package) -- Found PCL_COMMON: /usr/local/lib/libpcl_common.so -- Found PCL_KDTREE: /usr/local/lib/libpcl_kdtree.so -- Found PCL_OCTREE: /usr/local/lib/libpcl_octree.so -- Found PCL_SEARCH: /usr/local/lib/libpcl_search.so -- Found PCL_SAMPLE_CONSENSUS: /usr/local/lib/libpcl_sample_consensus.so -- Found PCL_FILTERS: /usr/local/lib/libpcl_filters.so -- Found PCL_2D: /usr/local/include/pcl-1.14 -- Found PCL_GEOMETRY: /usr/local/include/pcl-1.14 -- Found PCL_IO: /usr/local/lib/libpcl_io.so -- Found PCL_FEATURES: /usr/local/lib/libpcl_features.so -- Found PCL_ML: /usr/local/lib/libpcl_ml.so -- Found PCL_SEGMENTATION: /usr/local/lib/libpcl_segmentation.so -- Found PCL_SURFACE: /usr/local/lib/libpcl_surface.so -- Found PCL_REGISTRATION: /usr/local/lib/libpcl_registration.so -- Found PCL_KEYPOINTS: /usr/local/lib/libpcl_keypoints.so -- Found PCL_TRACKING: /usr/local/lib/libpcl_tracking.so -- Found PCL_RECOGNITION: /usr/local/lib/libpcl_recognition.so -- Found PCL_STEREO: /usr/local/lib/libpcl_stereo.so -- PCL status: -- version: 1.14.1 -- directory: /usr/local/share/pcl-1.14 CMake Warning (dev) at /usr/local/share/cmake-3.28/Modules/FetchContent.cmake:1331 (message): The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is not set. The policy\u0026#39;s OLD behavior will be used. When using a URL download, the timestamps of extracted files should preferably be that of the time of extraction, otherwise code that depends on the extracted contents might not be rebuilt if the URL changes. The OLD behavior preserves the timestamps from the archive instead, but this is usually not what you want. Update your project to the NEW behavior or specify the DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this robustness issue. Call Stack (most recent call first): cmake/functions.cmake:5 (FetchContent_Declare) CMakeLists.txt:40 (fetch_project) This warning is for project developers. Use -Wno-dev to suppress it. CMake Warning (dev) at build/_deps/cloudparse-src/CMakeLists.txt:18 (find_package): Policy CMP0074 is not set: find_package uses \u0026lt;PackageName\u0026gt;_ROOT variables. Run \u0026#34;cmake --help-policy CMP0074\u0026#34; for policy details. Use the cmake_policy command to set the policy and suppress this warning. CMake variable PCL_ROOT is set to: /usr/local For compatibility, CMake is ignoring the variable. This warning is for project developers. Use -Wno-dev to suppress it. -- PCL status: -- version: 1.14.1 -- directory: /usr/local/share/pcl-1.14 CMake Warning (dev) at /usr/local/share/cmake-3.28/Modules/FetchContent.cmake:1331 (message): The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is not set. The policy\u0026#39;s OLD behavior will be used. When using a URL download, the timestamps of extracted files should preferably be that of the time of extraction, otherwise code that depends on the extracted contents might not be rebuilt if the URL changes. The OLD behavior preserves the timestamps from the archive instead, but this is usually not what you want. Update your project to the NEW behavior or specify the DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this robustness issue. Call Stack (most recent call first): cmake/functions.cmake:5 (FetchContent_Declare) CMakeLists.txt:45 (fetch_project) This warning is for project developers. Use -Wno-dev to suppress it. ========================================= Project: upsampling_cloud COMPILED WITH CMAKE 3.28.0-rc3 ========================================= -- Configuring incomplete, errors occurred! Then I suspect the pcl version mismatch, as that project used 1.12.1. So I want to find the previous verions in their \u0026ldquo;Downloads\u0026rdquo; and found there is an option for linux: sudo apt install libpcl-dev.\nBut my Ubuntu 22.04 run into errors:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (base) zichen@homepc:~/Downloads/upsamplingCloudPCL$ dpkg -l | grep pcl (base) zichen@homepc:~/Downloads/upsamplingCloudPCL$ (base) zichen@homepc:~/Downloads/upsamplingCloudPCL$ sudo apt install libpcl-dev Reading package lists... Done Building dependency tree... Done Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: libqt5webkit5-dev : Depends: libqt5webkit5 (= 5.212.0~alpha4-15ubuntu1) but it is not installable qttools5-dev : Depends: qttools5-dev-tools (= 5.15.3-1) E: Unable to correct problems, you have held broken packages. So I try to install it on Ubuntu 20.04. It can be installed, but the error for cmake the upsamplingCloudPCL persists.\n(2024-06-27)\nRecompile pcl after vtk installed.\nInstall dependencies:\nIf vtk or libopenni-dev are missing, ccmake .. will stop at the log page (press l), where you can see which packages are not found. Set env: export VTK_DIR=~/vtk/build 1 2 3 4 cd build ccmake -DCMAKE_BUILD_TYPE=RelWithDebInfo .. make -j8 sudo make install Compiling with CUDA and GPU options turned on under cuda-11.8 fails:\ncuda header cannot found:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ 51%] Built target pcl_outofcore [ 51%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/record_tsdfvolume.cpp.o [ 51%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/capture.cpp.o [ 51%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/evaluation.cpp.o [ 51%] Linking CXX shared library ../lib/libpcl_surface.so In file included from /home/zichen/Downloads/pcl/gpu/kinfu/tools/../src/internal.h:42, from /home/zichen/Downloads/pcl/gpu/kinfu/tools/kinfu_app.cpp:84: /home/zichen/Downloads/pcl/gpu/kinfu/tools/../src/safe_call.hpp:40:10: fatal error: cuda_runtime_api.h: No such file or directory 40 | #include \u0026#34;cuda_runtime_api.h\u0026#34; | ^~~~~~~~~~~~~~~~~~~~ compilation terminated. make[2]: *** [gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/build.make:76: gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/kinfu_app.cpp.o] Error 1 make[2]: *** Waiting for unfinished jobs.... [ 51%] Building CXX object tools/CMakeFiles/pcl_viewer.dir/pcd_viewer.cpp.o In file included from /home/zichen/Downloads/pcl/gpu/kinfu/tools/../src/internal.h:42, from /home/zichen/Downloads/pcl/gpu/kinfu/tools/record_tsdfvolume.cpp:52: /home/zichen/Downloads/pcl/gpu/kinfu/tools/../src/safe_call.hpp:40:10: fatal error: cuda_runtime_api.h: No such file or directory 40 | #include \u0026#34;cuda_runtime_api.h\u0026#34; | ^~~~~~~~~~~~~~~~~~~~ compilation terminated. Recompiling pcl without turning on CUDA and GPU options succeeds.\nCompile on Ubu 20.04 ✅ Compile PCL for leveraging Nvidia GPU: Tutorials\nI\u0026rsquo;m concerning GPU version is not good for debugging.\n(2024-05-13)\nSystem info:\nOS: Ubuntu 20.04.6 LTS x86_64, Kernel: 5.15.0-105-generic CPU: Intel i7-9700 (8) @ 4.700GHz GPU: NVIDIA GeForce GTX 1050 Ti; GPU: Intel UHD Graphics 630 Memory: 5010MiB / 15809MiB gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 Cudatoolkit: Cuda compilation tools, release 11.6, V11.6.55. Build cuda_11.6.r11.6/compiler.30794723_0 1 2 3 4 5 sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt install g++-7 -y sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 60 --slave /usr/bin/g++ g++ /usr/bin/g++-7 sudo update-alternatives --config gcc Open GPU options in ccmake, which is an interactive interface How do I install ccmake? - SE\n1 2 3 4 5 # install ccmake sudo apt-get install cmake-curses-gui mkdir build; cd build ccmake .. I turned ON 2 options: BUILD_CUDA and BUILD_GPU\nPress c again to finish configurations and then press g to generate makefiles.\n1 2 3 # cd build make sudo make install Fix install vtk (2024-06-25)\nI can\u0026rsquo;t cmake the project: dtcMLOps/upsamplingCloudPCL on alienware. I though I missed some dependencies.\nInstall (developer-version) dependencies:\nopengl: How to Install OpenGL Library on Ubuntu 20.04 LTS (Focal Fossa) DDG\n1 2 3 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade sudo apt install freeglut3-dev libpcap-dev libusb-1.0-0-dev dpkg -L freeglut3-dev vtk: Getting Started-vtk\n1 sudo apt install libvtk7-dev After them, I didn\u0026rsquo;t compile the pcl again. I found there a pre-built libpcl-dev provided for linux, which is said \u0026ldquo;the recommended installation method\u0026rdquo; Download page\n1 (base) yi@yi-Alienware-Aurora-R8:~$ sudo apt install libpcl-dev The version is 1.10.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 (base) yi@yi-Alienware-Aurora-R8:~$ dpkg -l | grep pcl ii libdapclient6v5:amd64 3.20.5-1 amd64 Client library for the Network Data Access Protocol ii libpcl-apps1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - apps library ii libpcl-common1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - common library ii libpcl-dev 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - development files ii libpcl-features1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - features library ii libpcl-filters1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - filters library ii libpcl-io1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - I/O library ii libpcl-kdtree1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - kdtree library ii libpcl-keypoints1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - keypoints library ii libpcl-ml1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - ml library ii libpcl-octree1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - octree library ii libpcl-outofcore1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - outofcore library ii libpcl-people1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - people library ii libpcl-recognition1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - recognition library ii libpcl-registration1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - registration library ii libpcl-sample-consensus1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - sample consensus library ii libpcl-search1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - search library ii libpcl-segmentation1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - segmentation library ii libpcl-stereo1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - stereo library ii libpcl-surface1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - surface library ii libpcl-tracking1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - tracking library ii libpcl-visualization1.10:amd64 1.10.0+dfsg-5ubuntu1 amd64 Point Cloud Library - visualization library And pcl-1.10 can be found:\n1 2 (base) yi@yi-Alienware-Aurora-R8:~$ whereis pcl-1.10 pcl-1: /usr/include/pcl-1.10 I know its name beacuse I saw the folder pcl-1.10 in /usr/include. I was inspire by this anwser: Ubuntu 20.04 can\u0026rsquo;t find PCL because of incorrect include directory after installing it by sudo apt install libpcl-dev ( Found in DDG ) cmake has no error:\ncmake output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/upsamplingCloudPCL$ cmake -Bbuild -- The CXX compiler identification is GNU 9.4.0 -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done ========================================= Project: upsampling_cloud ========================================= CMake Warning (dev) at /usr/local/share/pcl-1.14/Modules/FindFLANN.cmake:45 (find_package): Policy CMP0144 is not set: find_package uses upper-case \u0026lt;PACKAGENAME\u0026gt;_ROOT variables. Run \u0026#34;cmake --help-policy CMP0144\u0026#34; for policy details. Use the cmake_policy command to set the policy and suppress this warning. CMake variable FLANN_ROOT is set to: /usr For compatibility, find_package is ignoring the variable, but code in a .cmake module might still use it. Call Stack (most recent call first): /usr/local/share/pcl-1.14/PCLConfig.cmake:260 (find_package) /usr/local/share/pcl-1.14/PCLConfig.cmake:305 (find_flann) /usr/local/share/pcl-1.14/PCLConfig.cmake:570 (find_external_library) CMakeLists.txt:29 (find_package) This warning is for project developers. Use -Wno-dev to suppress it. -- Checking for module \u0026#39;flann\u0026#39; -- Found flann, version 1.9.1 -- Found FLANN: /usr/lib/x86_64-linux-gnu/libflann_cpp.so -- FLANN found (include: /usr/include, lib: /usr/lib/x86_64-linux-gnu/libflann_cpp.so) -- Found OpenMP_CXX: -fopenmp (found version \u0026#34;4.5\u0026#34;) -- Found OpenMP: TRUE (found version \u0026#34;4.5\u0026#34;) found components: CXX -- Found Pcap: /usr/lib/x86_64-linux-gnu/libpcap.so -- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \u0026#34;1.2.11\u0026#34;) -- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \u0026#34;1.6.37\u0026#34;) -- The imported target \u0026#34;vtkParseOGLExt\u0026#34; references the file \u0026#34;/usr/bin/vtkParseOGLExt-7.1\u0026#34; but this file does not exist. Possible reasons include: * The file was deleted, renamed, or moved to another location. * An install or uninstall procedure did not complete successfully. * The installation package was faulty and contained \u0026#34;/usr/lib/cmake/vtk-7.1/VTKTargets.cmake\u0026#34; but not all the files it references. -- The imported target \u0026#34;vtkRenderingPythonTkWidgets\u0026#34; references the file \u0026#34;/usr/lib/x86_64-linux-gnu/libvtkRenderingPythonTkWidgets.so\u0026#34; but this file does not exist. Possible reasons include: * The file was deleted, renamed, or moved to another location. * An install or uninstall procedure did not complete successfully. * The installation package was faulty and contained \u0026#34;/usr/lib/cmake/vtk-7.1/VTKTargets.cmake\u0026#34; but not all the files it references. -- The imported target \u0026#34;vtk\u0026#34; references the file \u0026#34;/usr/bin/vtk\u0026#34; but this file does not exist. Possible reasons include: * The file was deleted, renamed, or moved to another location. * An install or uninstall procedure did not complete successfully. * The installation package was faulty and contained \u0026#34;/usr/lib/cmake/vtk-7.1/VTKTargets.cmake\u0026#34; but not all the files it references. -- The imported target \u0026#34;pvtk\u0026#34; references the file \u0026#34;/usr/bin/pvtk\u0026#34; but this file does not exist. Possible reasons include: * The file was deleted, renamed, or moved to another location. * An install or uninstall procedure did not complete successfully. * The installation package was faulty and contained \u0026#34;/usr/lib/cmake/vtk-7.1/VTKTargets.cmake\u0026#34; but not all the files it references. -- Checking for module \u0026#39;libusb-1.0\u0026#39; -- Found libusb-1.0, version 1.0.23 -- Found libusb: /usr/lib/x86_64-linux-gnu/libusb-1.0.so -- Found Qhull: /usr/lib/x86_64-linux-gnu/libqhull_r.so -- QHULL found (include: /usr/include, lib: /usr/lib/x86_64-linux-gnu/libqhull_r.so) -- Found PCL_COMMON: /usr/local/lib/libpcl_common.so -- Found PCL_KDTREE: /usr/local/lib/libpcl_kdtree.so -- Found PCL_OCTREE: /usr/local/lib/libpcl_octree.so -- Found PCL_SEARCH: /usr/local/lib/libpcl_search.so -- Found PCL_SAMPLE_CONSENSUS: /usr/local/lib/libpcl_sample_consensus.so -- Found PCL_FILTERS: /usr/local/lib/libpcl_filters.so -- Found PCL_2D: /usr/local/include/pcl-1.14 -- Found PCL_GEOMETRY: /usr/local/include/pcl-1.14 -- Found PCL_IO: /usr/local/lib/libpcl_io.so -- Found PCL_FEATURES: /usr/local/lib/libpcl_features.so -- Found PCL_ML: /usr/local/lib/libpcl_ml.so -- Found PCL_SEGMENTATION: /usr/local/lib/libpcl_segmentation.so -- Found PCL_SURFACE: /usr/local/lib/libpcl_surface.so -- Found PCL_REGISTRATION: /usr/local/lib/libpcl_registration.so -- Found PCL_KEYPOINTS: /usr/local/lib/libpcl_keypoints.so -- Found PCL_TRACKING: /usr/local/lib/libpcl_tracking.so -- Found PCL_RECOGNITION: /usr/local/lib/libpcl_recognition.so -- Found PCL_STEREO: /usr/local/lib/libpcl_stereo.so -- Found PCL_CUDA_COMMON: /usr/local/include/pcl-1.14 -- Found PCL_CUDA_FEATURES: /usr/local/lib/libpcl_cuda_features.so -- Found PCL_CUDA_SEGMENTATION: /usr/local/lib/libpcl_cuda_segmentation.so -- Found PCL_CUDA_SAMPLE_CONSENSUS: /usr/local/lib/libpcl_cuda_sample_consensus.so -- Found PCL_GPU_CONTAINERS: /usr/local/lib/libpcl_gpu_containers.so -- Found PCL_GPU_UTILS: /usr/local/lib/libpcl_gpu_utils.so -- Found PCL_GPU_OCTREE: /usr/local/lib/libpcl_gpu_octree.so -- Found PCL_GPU_FEATURES: /usr/local/lib/libpcl_gpu_features.so -- Found PCL_GPU_KINFU: /usr/local/lib/libpcl_gpu_kinfu.so -- Found PCL_GPU_SEGMENTATION: /usr/local/lib/libpcl_gpu_segmentation.so -- PCL status: -- version: 1.14.1 -- directory: /usr/local/share/pcl-1.14 CMake Warning (dev) at /usr/local/share/cmake-3.28/Modules/FetchContent.cmake:1331 (message): The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is not set. The policy\u0026#39;s OLD behavior will be used. When using a URL download, the timestamps of extracted files should preferably be that of the time of extraction, otherwise code that depends on the extracted contents might not be rebuilt if the URL changes. The OLD behavior preserves the timestamps from the archive instead, but this is usually not what you want. Update your project to the NEW behavior or specify the DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this robustness issue. Call Stack (most recent call first): cmake/functions.cmake:5 (FetchContent_Declare) CMakeLists.txt:39 (fetch_project) This warning is for project developers. Use -Wno-dev to suppress it. CMake Warning (dev) at build/_deps/cloudparse-src/CMakeLists.txt:18 (find_package): Policy CMP0074 is not set: find_package uses \u0026lt;PackageName\u0026gt;_ROOT variables. Run \u0026#34;cmake --help-policy CMP0074\u0026#34; for policy details. Use the cmake_policy command to set the policy and suppress this warning. CMake variable PCL_ROOT is set to: /usr/local For compatibility, CMake is ignoring the variable. This warning is for project developers. Use -Wno-dev to suppress it. -- PCL status: -- version: 1.14.1 -- directory: /usr/local/share/pcl-1.14 CMake Warning (dev) at /usr/local/share/cmake-3.28/Modules/FetchContent.cmake:1331 (message): The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is not set. The policy\u0026#39;s OLD behavior will be used. When using a URL download, the timestamps of extracted files should preferably be that of the time of extraction, otherwise code that depends on the extracted contents might not be rebuilt if the URL changes. The OLD behavior preserves the timestamps from the archive instead, but this is usually not what you want. Update your project to the NEW behavior or specify the DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this robustness issue. Call Stack (most recent call first): cmake/functions.cmake:5 (FetchContent_Declare) CMakeLists.txt:44 (fetch_project) This warning is for project developers. Use -Wno-dev to suppress it. ========================================= Project: upsampling_cloud COMPILED WITH CMAKE 3.28.0-rc3 ========================================= -- Configuring done (2.1s) CMake Warning at CMakeLists.txt:57 (add_executable): Cannot generate a safe runtime search path for target upsampling_cloud because files in some directories may conflict with libraries in implicit directories: runtime library [libfreetype.so.6] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libz.so.1] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libexpat.so.1] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libpng16.so.16] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libtiff.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libpython3.8.so.1.0] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libsz.so.2] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libxml2.so.2] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Widgets.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Gui.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Sql.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Core.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in: /home/yi/anaconda3/lib Some of these libraries may not be found correctly. CMake Warning at build/_deps/cloudparse-src/CMakeLists.txt:31 (add_library): Cannot generate a safe runtime search path for target cloudparse because files in some directories may conflict with libraries in implicit directories: runtime library [libpng16.so.16] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libexpat.so.1] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libtiff.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Widgets.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Gui.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Sql.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libQt5Core.so.5] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libsz.so.2] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libxml2.so.2] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libpython3.8.so.1.0] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libz.so.1] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libfreetype.so.6] in /usr/lib/x86_64-linux-gnu may be hidden by files in: /home/yi/anaconda3/lib runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in: /home/yi/anaconda3/lib Some of these libraries may not be found correctly. -- Generating done (0.0s) -- Build files have been written to: /home/yi/Downloads/upsamplingCloudPCL/build But make has error:\n1 2 3 4 5 6 7 8 9 10 11 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/upsamplingCloudPCL/build$ make [ 25%] Building CXX object _deps/cloudparse-build/CMakeFiles/cloudparse.dir/src/parser.cpp.o In file included from /home/yi/Downloads/upsamplingCloudPCL/build/_deps/cloudparse-src/include/cloudparse/parser.hpp:19, from /home/yi/Downloads/upsamplingCloudPCL/build/_deps/cloudparse-src/src/parser.cpp:1: /home/yi/Downloads/upsamplingCloudPCL/build/_deps/cloudparse-src/include/cloudparse/concrete_parses.hpp:13:10: fatal error: pcl/io/vtk_lib_io.h: No such file or directory 13 | #include \u0026lt;pcl/io/vtk_lib_io.h\u0026gt; | ^~-~-~-~-~-~-~-~-~-~ compilation terminated. make[2]: *** [_deps/cloudparse-build/CMakeFiles/cloudparse.dir/build.make:76: _deps/cloudparse-build/CMakeFiles/cloudparse.dir/src/parser.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:144: _deps/cloudparse-build/CMakeFiles/cloudparse.dir/all] Error 2 make: *** [Makefile:156: all] Error 2 Maybe I need to re-compile pcl. Re-compile pcl:\nError: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 [ 42%] Linking CXX executable ../bin/pcl_vtk2ply [ 42%] Built target pcl_vtk2ply Scanning dependencies of target pcl_viewer [ 42%] Building CXX object tools/CMakeFiles/pcl_viewer.dir/pcd_viewer.cpp.o [ 42%] Built target pcl_hdl_viewer_simple Scanning dependencies of target pcl_pcd2png [ 43%] Building CXX object tools/CMakeFiles/pcl_pcd2png.dir/pcd2png.cpp.o [ 43%] Linking CXX shared library ../lib/libpcl_surface.so [ 43%] Linking CXX executable ../bin/pcl_compute_cloud_error [ 43%] Built target pcl_compute_cloud_error Scanning dependencies of target pcl_convert_pcd_ascii_binary [ 43%] Linking CXX executable ../bin/pcl_obj2vtk [ 44%] Linking CXX executable ../bin/pcl_ply2pcd [ 44%] Building CXX object tools/CMakeFiles/pcl_convert_pcd_ascii_binary.dir/convert_pcd_ascii_binary.cpp.o [ 44%] Built target pcl_obj2vtk Scanning dependencies of target pcl_pcd_introduce_nan [ 44%] Building CXX object tools/CMakeFiles/pcl_pcd_introduce_nan.dir/pcd_introduce_nan.cpp.o [ 44%] Built target pcl_ply2pcd Scanning dependencies of target pcl_outofcore [ 44%] Building CXX object outofcore/CMakeFiles/pcl_outofcore.dir/src/cJSON.cpp.o [ 44%] Linking CXX executable ../bin/pcl_compute_hausdorff [ 44%] Built target pcl_surface Scanning dependencies of target pcl_record_tsdfvolume [ 44%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/record_tsdfvolume.cpp.o [ 45%] Building CXX object outofcore/CMakeFiles/pcl_outofcore.dir/src/outofcore_node_data.cpp.o [ 45%] Built target pcl_compute_hausdorff Scanning dependencies of target pcl_kinfu_app [ 45%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/kinfu_app.cpp.o In file included from /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/internal.h:42:0, from /home/yi/Downloads/pcl/gpu/kinfu/tools/record_tsdfvolume.cpp:52: /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/safe_call.hpp:40:10: fatal error: cuda_runtime_api.h: No such file or directory #include \u0026#34;cuda_runtime_api.h\u0026#34; ^~-~~-~-~-~-~-~-~-~- compilation terminated. make[2]: *** [gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/build.make:63: gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/record_tsdfvolume.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:2346: gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/all] Error 2 make[1]: *** Waiting for unfinished jobs.... [ 45%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/capture.cpp.o In file included from /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/internal.h:42:0, from /home/yi/Downloads/pcl/gpu/kinfu/tools/kinfu_app.cpp:84: /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/safe_call.hpp:40:10: fatal error: cuda_runtime_api.h: No such file or directory #include \u0026#34;cuda_runtime_api.h\u0026#34; ^~-~~-~-~-~-~-~-~-~- compilation terminated. make[2]: *** [gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/build.make:63: gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/kinfu_app.cpp.o] Error 1 make[2]: *** Waiting for unfinished jobs.... [ 45%] Building CXX object outofcore/CMakeFiles/pcl_outofcore.dir/src/outofcore_base_data.cpp.o [ 45%] Linking CXX executable ../bin/pcl_convert_pcd_ascii_binary [ 45%] Built target pcl_convert_pcd_ascii_binary [ 45%] Linking CXX executable ../bin/pcl_pcd2png [ 45%] Built target pcl_pcd2png [ 45%] Linking CXX shared library ../lib/libpcl_outofcore.so [ 45%] Built target pcl_outofcore make[1]: *** [CMakeFiles/Makefile2:2379: gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/all] Error 2 [ 45%] Linking CXX executable ../bin/pcl_pcd_introduce_nan [ 45%] Built target pcl_pcd_introduce_nan [ 45%] Linking CXX executable ../bin/pcl_viewer [ 45%] Built target pcl_viewer [ 45%] Linking CXX shared library ../lib/libpcl_sample_consensus.so [ 45%] Built target pcl_sample_consensus make: *** [Makefile:152: all] Error 2 I suspect the problem is the libvtk7-dev installed before: During ccmake configuration, the vtk is detected, so cuda will be called to build for some functions. But I don\u0026rsquo;t know why cuda can\u0026rsquo;t be found. (2024-06-26)\nChange conda env doesn\u0026rsquo;t work\nChange a conda env can avoid some warnings about lib hidden when ccmake configuration. 1 2 3 4 5 6 7 8 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl$ conda activate casmvsnet_pl (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl$ cd build/ (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ rm -r * rm: cannot remove \u0026#39;*\u0026#39;: No such file or directory (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ ls (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ ccmake .. (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ make Set the following env variables doesn\u0026rsquo;t work:\n1 2 3 4 5 6 7 8 9 10 # CUDA export CUDA=11.6 export PATH=/usr/local/cuda-$CUDA/bin${PATH:+:${PATH}} export CUDA_PATH=/usr/local/cuda-$CUDA export CUDA_HOME=/usr/local/cuda-$CUDA export LIBRARY_PATH=$CUDA_HOME/lib64:$LIBRARY_PATH export LD_LIBRARY_PATH=/usr/local/cuda-$CUDA/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH export NVCC=/usr/local/cuda-$CUDA/bin/nvcc export CFLAGS=\u0026#34;-I$CUDA_HOME/include $CFLAGS\u0026#34; Copy from Fatal error: cuda_runtime_api.h: No such file or directory ( Found in DDG ) Then I wonder \u0026ldquo;how to verify vtk installed on my system\u0026rdquo; (DDG) Remove libvtk7-dev and re-compile pcl:\n1 (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ sudo apt remove --purge libvtk7-dev Recompile again, but this time VTK can\u0026rsquo;t be found at ccmake configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CMake Warning at cmake/pcl_find_vtk.cmake:31 (find_package): By not providing \u0026#34;FindVTK.cmake\u0026#34; in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \u0026#34;VTK\u0026#34;, but CMake did not find one. Could not find a package configuration file provided by \u0026#34;VTK\u0026#34; with any of the following names: VTKConfig.cmake vtk-config.cmake Add the installation prefix of \u0026#34;VTK\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;VTK_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;VTK\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): CMakeLists.txt:398 (include) Therefore, the vtk-dev is required for some options.\nThe functions related to VTK are supposed to be ignored, so the compile suceeded (make -j8).\nInstall vtk from src code: Building - VTK doc\n1 2 (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/vtk/build$ cmake --build ~/vtk/build [5091/5091] Creating library symlink lib/libvtkFiltersFlowPaths-9.3.so.1 lib/libvtkFiltersFlowPaths-9.3.so Recompile pcl\nThe warning still occurs: CMake Warning at cmake/pcl_find_vtk.cmake:31 (find_package):\nI need to specify the path to vtk:\n1 2 3 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ export VTK_DIR=/home/yi/vtk/build (base) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ conda activate casmvsnet_pl (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/pcl/build$ ccmake .. The cuda headers: cuda_runtime_api.h still can\u0026rsquo;t be found:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Scanning dependencies of target pcl_oni_viewer [ 45%] Building CXX object tools/CMakeFiles/pcl_oni_viewer.dir/oni_viewer_simple.cpp.o In file included from /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/internal.h:42:0, from /home/yi/Downloads/pcl/gpu/kinfu/tools/record_tsdfvolume.cpp:52: /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/safe_call.hpp:40:10: fatal error: cuda_runtime_api.h: No such file or directory #include \u0026#34;cuda_runtime_api.h\u0026#34; ^~-~~-~-~-~-~-~-~-~- compilation terminated. make[2]: *** [gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/build.make:63: gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/record_tsdfvolume.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:2346: gpu/kinfu/tools/CMakeFiles/pcl_record_tsdfvolume.dir/all] Error 2 make[1]: *** Waiting for unfinished jobs.... [ 45%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/capture.cpp.o [ 45%] Building CXX object outofcore/CMakeFiles/pcl_outofcore.dir/src/outofcore_base_data.cpp.o [ 46%] Linking CXX executable ../bin/pcl_pcd2png [ 46%] Built target pcl_pcd2png [ 46%] Building CXX object gpu/kinfu/tools/CMakeFiles/pcl_kinfu_app.dir/evaluation.cpp.o In file included from /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/internal.h:42:0, from /home/yi/Downloads/pcl/gpu/kinfu/tools/kinfu_app.cpp:84: /home/yi/Downloads/pcl/gpu/kinfu/tools/../src/safe_call.hpp:40:10: fatal error: cuda_runtime_api.h: No such file or directory #include \u0026#34;cuda_runtime_api.h\u0026#34; ^~-~~-~-~-~-~-~-~-~- compilation terminated. Compilation succeeds when I don\u0026rsquo;t turn on CUDA and GPU options, so I installed a CPU version.\nI may try to paste the cuda envs into .bashrc and compile again, can it found?\nThe header \u0026lt;pcl/io/vtk xxx\u0026gt; still can\u0026rsquo;t be found when make the project [upsamplingCloudPCL].\n1 2 3 4 5 6 7 8 (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/upsamplingCloudPCL/build$ make [ 25%] Building CXX object _deps/cloudparse-build/CMakeFiles/cloudparse.dir/src/parser.cpp.o In file included from /home/yi/Downloads/upsamplingCloudPCL/build/_deps/cloudparse-src/include/cloudparse/parser.hpp:19:0, from /home/yi/Downloads/upsamplingCloudPCL/build/_deps/cloudparse-src/src/parser.cpp:1: /home/yi/Downloads/upsamplingCloudPCL/build/_deps/cloudparse-src/include/cloudparse/concrete_parses.hpp:13:10: fatal error: pcl/io/vtk_lib_io.h: No such file or directory #include \u0026lt;pcl/io/vtk_lib_io.h\u0026gt; ^~-~~-~-~-~-~-~-~-~- compilation terminated. Should I create a symblic link: /usr/include/pcl instead of /usr/include/pcl-1.14 (containing pcl/), similar to the symblic link /usr/local/cuda directly contains dirs like bin/ wihouth an outer extra intermediate folder? So, when I\u0026rsquo;m checking the folders, I found the date of the pcl folder is 24-05-13. I realized I haven\u0026rsquo;t install the pcl after compilaiton.\nI sudo make install the pcl (Log file)\nThe vtk_lib_io.h can be found!\nAnother error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 (casmvsnet_pl) yi@yi-Alienware-Aurora-R8:~/Downloads/upsamplingCloudPCL/build$ make [ 25%] Building CXX object _deps/cloudparse-build/CMakeFiles/cloudparse.dir/src/parser.cpp.o [ 50%] Linking CXX shared library libcloudparse.so [ 50%] Built target cloudparse [ 75%] Building CXX object CMakeFiles/upsampling_cloud.dir/src/main.cpp.o In file included from /home/yi/Downloads/upsamplingCloudPCL/src/main.cpp:5:0: /home/yi/Downloads/upsamplingCloudPCL/build/_deps/argparse-src/include/argparse/argparse.hpp:36:10: fatal error: charconv: No such file or directory #include \u0026lt;charconv\u0026gt; ^~-~-~-~-~ compilation terminated. make[2]: *** [CMakeFiles/upsampling_cloud.dir/build.make:76: CMakeFiles/upsampling_cloud.dir/src/main.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:118: CMakeFiles/upsampling_cloud.dir/all] Error 2 make: *** [Makefile:156: all] Error 2 Claude3.5 remindes me this is a feature of C++17. I remember I had set g++ to 7 for compiling pcl.\nOnce I switch g++ to 9, this error is gone. And the compilation for upsamplingCloudPCL succeeds. (Log file)\nSummary:\n1 2 3 4 ccmake -DCMAKE_BUILD_TYPE=Debug .. # Use the default options without turnning on CUDA and GPU make -j8 sudo make -j8 install Convert ply pcd (2024-05-11)\nRef: Convertion of .ply format to .pcd format - SO\nPCD File Format: Tutorial 1 2 3 import open3d as o3d ply = o3d.io.read_point_cloud(\u0026#34;source_pointcloud.ply\u0026#34;) o3d.io.write_point_cloud(\u0026#34;sink_pointcloud.pcd\u0026#34;, ply) Then use loadPCDFile to a PCLPointCloud2 template point cloud.\n1 2 pcl::PCLPointCloud2 cloud_blob; pcl::io::loadPCDFile (\u0026#34;../pcl/test/bun0.pcd\u0026#34;, cloud_blob); In this way, however, there is no triangle formed, as shown in the last line of the file \u0026ldquo;mesh.vtk\u0026rdquo;: POLYGONS 0 0. VTK file formats\nSimilarly, the following point cloud only produced 9 triangles. GreedyProjectionTriangulation · Issue #4123 (Search: \u0026ldquo;PointCloudLibrary triangulation cannot form polygons\u0026rdquo; in DDG) (2024-05-13)\nThe greedy projection is not good.\nhow to create mesh in delaunay triangulation using pcl library #6225 (Surfaced by searching \u0026ldquo;pointcloudlibrary triangulation\u0026rdquo;)\nRead Ply file (2024-05-12)\nDocs\n1 2 #include \u0026lt;pcl/io/ply_io.h\u0026gt; pcl::io::loadPLYFile (\u0026#34;../CasMVSNet_pl-comments/results/dtu/image_ref/scan1/points3d.ply\u0026#34;, cloud_blob); Debug Repo-Github\n(2024-05-12)\nThe C/C++ extension in VSCode can\u0026rsquo;t cannot open source file \u0026quot;pcl/point_types.h\u0026quot;C/C++(1696)\nSet IncludePath referring to Visual Studio Code cannot open source file \u0026ldquo;iostream\u0026rdquo; - SO\nBTW, To find the path to C++: gcc -v -E -x c++ -\nCreate c_cpp_properties.json by pressing Ctrl+Shift+p and select C/C++: Edit Configurations (JSON)\nAppend includePath\n1 2 3 4 5 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/include/pcl-1.14\u0026#34;, \u0026#34;/home/zichen/.local/include/eigen3\u0026#34; ] Eigen must be installed: cannot open source file \u0026quot;Eigen/StdVector\u0026quot; (dependency of \u0026quot;pcl/io/pcd_io.h\u0026quot;)C/C++(1696)\nInstallation Guide: Eigen - GitHub Pages\nIt's installed in .local/include/eigen/ {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 -- Configured Eigen 3.3.7 -- -- Some things you can do now: -- --------------+-------------------------------------------------------------- -- Command | Description -- --------------+-------------------------------------------------------------- -- make install | Install Eigen. Headers will be installed to: -- | \u0026lt;CMAKE_INSTALL_PREFIX\u0026gt;/\u0026lt;INCLUDE_INSTALL_DIR\u0026gt; -- | Using the following values: -- | CMAKE_INSTALL_PREFIX: /home/zichen/.local -- | INCLUDE_INSTALL_DIR: include/eigen3 -- | Change the install location of Eigen headers using: -- | cmake . -DCMAKE_INSTALL_PREFIX=yourprefix -- | Or: -- | cmake . -DINCLUDE_INSTALL_DIR=yourdir -- make doc | Generate the API documentation, requires Doxygen \u0026amp; LaTeX -- make check | Build and run the unit-tests. Read this page: -- | http://eigen.tuxfamily.org/index.php?title=Tests -- make blas | Build BLAS library (not the same thing as Eigen) -- make uninstall| Removes files installed by make install -- --------------+-------------------------------------------------------------- -- -- Configuring done (3.0s) -- Generating done (1.1s) -- Build files have been written to: /tmp/eigen/build + make install Install the project... -- Install configuration: \u0026#34;Release\u0026#34; -- Installing: /home/zichen/.local/include/eigen3/signature_of_eigen3_matrix_library -- Installing: /home/zichen/.local/share/pkgconfig/eigen3.pc -- Installing: /home/zichen/.local/share/eigen3/cmake/Eigen3Targets.cmake -- Installing: /home/zichen/.local/share/eigen3/cmake/UseEigen3.cmake -- Installing: /home/zichen/.local/share/eigen3/cmake/Eigen3Config.cmake -- Installing: /home/zichen/.local/share/eigen3/cmake/Eigen3ConfigVersion.cmake -- Installing: /home/zichen/.local/include/eigen3/Eigen/Cholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/CholmodSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/Core -- Installing: /home/zichen/.local/include/eigen3/Eigen/Dense -- Installing: /home/zichen/.local/include/eigen3/Eigen/Eigen -- Installing: /home/zichen/.local/include/eigen3/Eigen/Eigenvalues -- Installing: /home/zichen/.local/include/eigen3/Eigen/Geometry -- Installing: /home/zichen/.local/include/eigen3/Eigen/Householder -- Installing: /home/zichen/.local/include/eigen3/Eigen/IterativeLinearSolvers -- Installing: /home/zichen/.local/include/eigen3/Eigen/Jacobi -- Installing: /home/zichen/.local/include/eigen3/Eigen/LU -- Installing: /home/zichen/.local/include/eigen3/Eigen/MetisSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/OrderingMethods -- Installing: /home/zichen/.local/include/eigen3/Eigen/PaStiXSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/PardisoSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/QR -- Installing: /home/zichen/.local/include/eigen3/Eigen/QtAlignedMalloc -- Installing: /home/zichen/.local/include/eigen3/Eigen/SPQRSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/SVD -- Installing: /home/zichen/.local/include/eigen3/Eigen/Sparse -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseCholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseCore -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseLU -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseQR -- Installing: /home/zichen/.local/include/eigen3/Eigen/StdDeque -- Installing: /home/zichen/.local/include/eigen3/Eigen/StdList -- Installing: /home/zichen/.local/include/eigen3/Eigen/StdVector -- Installing: /home/zichen/.local/include/eigen3/Eigen/SuperLUSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/UmfPackSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Householder ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/DenseBase.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/functors/StlFunctors.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/CwiseTernaryOp.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/products -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/products/TriangularMatrixVector_BLAS.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/Transpositions.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/NoAlias.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/util -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/util/BlasUtil.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/DenseStorage.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/ZVector -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/ZVector/Complex.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/Default -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/Default/Settings.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/Default/ConjHelper.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX/Complex.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/SSE -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/SSE/Complex.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/SSE/MathFunctions.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/CUDA -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/CUDA/Complex.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/CUDA/MathFunctions.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AltiVec -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AltiVec/Complex.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX512 -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX512/MathFunctions.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX512/PacketMath.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON/Complex.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON/MathFunctions.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON/PacketMath.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PardisoSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PardisoSupport/PardisoSupport.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/CholmodSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/CholmodSupport/CholmodSupport.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Jacobi -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Jacobi/Jacobi.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Eigenvalues -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Eigenvalues/Tridiagonalization.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/misc -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/misc/Image.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SuperLUSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SuperLUSupport/SuperLUSupport.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/OrderingMethods -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/OrderingMethods/Ordering.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCore -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCore/SparseDot.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/IterativeLinearSolvers -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/IterativeLinearSolvers/IncompleteCholesky.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/QR -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/QR/CompleteOrthogonalDecomposition.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/StlSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/StlSupport/StdList.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU/Determinant.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU/arch -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU/arch/Inverse_SSE.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PaStiXSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PaStiXSupport/PaStiXSupport.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SVD -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SVD/JacobiSVD.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Geometry -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Geometry/Umeyama.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCholesky/SimplicialCholesky.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCholesky/SimplicialCholesky_impl.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/plugins -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/plugins/BlockMethods.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/MetisSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/MetisSupport/MetisSupport.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky/LLT_LAPACKE.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky/LDLT.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky/LLT.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseLU -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseLU/SparseLU_column_dfs.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseQR -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseQR/SparseQR.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/AdolcForward ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/LevenbergMarquardt ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/FFT ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/EulerAngles -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/EulerAngles/EulerAngles.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/EulerAngles/EulerSystem.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MatrixFunctions -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Eigenvalues -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Eigenvalues/ArpackSelfAdjointEigenSolver.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff/AutoDiffScalar.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff/AutoDiffJacobian.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff/AutoDiffVector.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/BVH -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/BVH/BVAlgorithms.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/BVH/KdBVH.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/NumericalDiff -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/NumericalDiff/NumericalDiff.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Splines ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/SpecialFunctions ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/SparseExtra ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Polynomials ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MoreVectorization -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MoreVectorization/MathFunctions.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/KroneckerProduct -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/KroneckerProduct/KroneckerTensorProduct.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Skyline ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/NonLinearOptimization ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/IterativeSolvers ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/Tensor -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/TensorSymmetry -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/ThreadPool -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/src -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/src/Tensor ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/src/util ... }}}\nCloudCompare Homepage | Download page\nInstall:\n1 2 3 4 5 6 7 8 9 10 11 sudo apt install flatpak # Add the Flathub repository flatpak remote-add --if-not-exists flathub https://dl.flathub.org/repo/flathub.flatpakrepo # Install CloudCompare flatpak install flathub org.cloudcompare.CloudCompare # Run the app flatpak run org.cloudcompare.CloudCompare # (base) yi@Alienware:~$ flatpak run org.cloudcompare.CloudCompare It pops to warn about Python 3.11 requirement. But it\u0026rsquo;s okay to run in my conda env with Python 3.8. Open a .vtk file:\nThe mesh.vtk result of greedy_projection.cpp for dtu scan1 point cloud (.ply) indeed doesn\u0026rsquo;t have mesh:\n1 2 [19:15:38] [VTK] vtk output [19:15:39] An error occurred while loading \u0026#39;mesh\u0026#39;: nothing to load MeshLab Remeshing (2024-05-13)\nVersion: MeshLab 64bit dp v2023.12d built on Dec 12 2023 with GCC 9.4.0 and Qt 5.15.2.\nTutorial: MeshLab: Point Cloud to Mesh - Design Support - Greenwich Blogs (Search: \u0026ldquo;meshlab convert point cloud to mesh\u0026rdquo;)\nFilters -\u0026gt; Remeshing, Simplification and Reconstruction -\u0026gt; Surface Reconstruction: Screened Poisson (or Ball Pivoting)\nPoisson (defalut) Ball Pivoting (default) Recon depth: 8 Clustering radius: 20% The .ply file is generated by casmvsnet_pl Triangulation Ball-Pivoting Open3D open3d.geometry.TriangleMesh - Docs\nRef: 5-Step Guide to generate 3D meshes from point clouds with Python - Medium - Florent Poux, Ph.D\nBasic code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import open3d as o3d import numpy as np pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/image_ref/scan1/points3d.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) distances = pcd.compute_nearest_neighbor_distance() avg_dist = np.mean(distances) # The radius of the ball should be larger than the avg distance between points radius = 3 * avg_dist # Ball-Pivoting Algorithm bpa_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(pcd,o3d.utility.DoubleVector([radius, radius * 2])) dec_mesh = bpa_mesh.simplify_quadric_decimation(100000) dec_mesh.remove_degenerate_triangles() dec_mesh.remove_duplicated_triangles() dec_mesh.remove_duplicated_vertices() dec_mesh.remove_non_manifold_edges() o3d.io.write_triangle_mesh(\u0026#34;bpa_mesh.ply\u0026#34;, dec_mesh) Open the .ply file with ParaView:\nThe ball-pivoting algorithm result is better than possion on this point cloud. Retrieve triangles vertices:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 print(np.asarray(dec_mesh.triangles).shape) print(dec_mesh.triangles[0]) print(np.asarray(dec_mesh.vertices).shape) print(dec_mesh.vertices[100567]) # print(np.asarray(dec_mesh.triangle_uvs).shape) print(dec_mesh) vertices = np.asarray(dec_mesh.vertices) # (220_583, 3) vertices = torch.from_numpy(vertices) triangles = np.asarray(dec_mesh.triangles) # (100_000, 3) triangles = torch.from_numpy(triangles) K = 3 # number of vertices # Coordinates of 3 vertices of each triangle nnCoords = vertices.gather(dim=0, index=triangles.reshape(-1,1).expand(-1,3).type(torch.int64)).view(-1,K,3) # (100_000,K,3) Poisson Open3D Basic Code:\n1 2 3 4 5 6 poisson_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=8, width=0, scale=1.1, linear_fit=False)[0] bbox = pcd.get_axis_aligned_bounding_box() p_mesh_crop = poisson_mesh.crop(bbox) o3d.io.write_triangle_mesh(\u0026#34;p_mesh_c.ply\u0026#34;, p_mesh_crop) Visualization in ParaView\nDon\u0026rsquo;t know if the normals of the input .ply caused the Poisson reconsturction bad. It seems like a sheet is blown up. Delaunay (2024-06-21)\nSearch: \u0026ldquo;open3d delaunay triangulation\u0026rdquo; in DDG\ndelaunay-triangulation · GitHub Topics\nDelaunay Triangulation - SCIco - Youtube\nExample: Delaunay Triangulation - GitHub Pages\nGCaptainNemo/Delaunay-Triangulation\nPCL (2024-05-13)\nSearch: \u0026ldquo;pointcloudlibrary triangulation\u0026rdquo; in DDG\n\u0026ldquo;how to create mesh in delaunay triangulation using pcl library\u0026rdquo; IntelRealSense-github An alternative is using CloudCompare. Palissy ware project Open3D (2024-06-21)\nSearch: \u0026ldquo;open3d delaunay triangulation\u0026rdquo; in DDG doesn\u0026rsquo;t reveal Docs related to delaunay.\nI asked perplexity: \u0026ldquo;Doesn\u0026rsquo;t the Open3D library have an implementation of Delaunay triangulation?\u0026rdquo;\nOpen3D does have since 0.8.0 geogram (2024-06-21)\nRepo: BrunoLevy/geogram (Found in DDG)\nscipy (2024-06-21)\nHow to visualize 3D delaunay triangulation - SO (Found in DDG)\nscipy.spatial.Delaunay — SciPy Manual\nPyVista (2024-06-21)\n3D point set processing in python: a quick overview - Medium (Found in DDG) Projection Greedy Projection Code: PCL-Tutorial\nChange the path to .pcd file:\n1 pcl::io::loadPCDFile (\u0026#34;../../pcl/test/bun0.pcd\u0026#34;, cloud_blob); Save point cloud as .vtk file:\n1 pcl::io::saveVTKFile (\u0026#34;mesh.vtk\u0026#34;, triangles); Build and compile:\n1 2 cmake -B ./build -DCMAKE_BUILD_TYPE=Debug make -C ./build Execute: ./greedy_projection will generate a mesh.vtk\nUse ParaView to open .vtk files.\nUncompress:\n1 tar xvf ParaView-5.12.0-MPI-Linux-Python3.10-x86_64.tar.gz Execute:\n1 ./home/jack/Programs/ParaView-5.12.0-MPI-Linux-Python3.10-x86_64/bin/paraview Open the vtk file and open the \u0026ldquo;eye\u0026rdquo;.\nStitch Clouds (2024-04-01)\nStitching point clouds from multiple cameras - camcalib\nResampling MLS PCL (2024-05-28)\nInterpolate point cloud base on Moving Least Squares. Tutorial\n1 2 3 4 mkdir -p pcl_resampling \u0026amp;\u0026amp; cd $_ nvim resampling.cpp cmake -B ./build -DCMAKE_BUILD_TYPE=Debug make -C ./build Python version for PCL MLS: syedjameel/MovingLeastSquares (Found when search: \u0026ldquo;code example for PCL MovingLeastSquares UpsamplingMethod\u0026rdquo; in DDG) This repo: dtcMLOps/upsamplingCloudPCL enable specify options through input arguments. ","date":"2024-03-27T18:14:00Z","image":"https://miro.medium.com/v2/resize:fit:786/format:webp/1*YWpRKfA-RRQgjhdG_3EP4g.png","permalink":"http://blog.zichen.uk/post/writenotes/vis/pkg-pointcloud/","title":"Memo: Lib - Points | Processing Point Cloud"},{"content":"(Feature image from Blender Artists Community - Donut 3.0)\nNeRF Synthetic The dataset nerf/data/nerf_synthetic/lego composes of 3 things:\nRGB images (train/),\npoint cloud (points3d.ply),\nassociated camera poses (transforms_train.json), where each frame contains 3 attributes: file_path, rotation, and c2w transform_matrix\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \u0026#34;camera_angle_x\u0026#34;: 0.6911112070083618, \u0026#34;frames\u0026#34;: [ { \u0026#34;file_path\u0026#34;: \u0026#34;./train/r_0\u0026#34;, \u0026#34;rotation\u0026#34;: 0.012566370614359171, \u0026#34;transform_matrix\u0026#34;: [ [ -0.9999021887779236, 0.004192245192825794, -0.013345719315111637, -0.05379832163453102 ], [ -0.013988681137561798, -0.2996590733528137, 0.95394366979599, 3.845470428466797 ], [ -4.656612873077393e-10, 0.9540371894836426, 0.29968830943107605, 1.2080823183059692 ], [ 0.0, 0.0, 0.0, 1.0 ] ] }, Colmap Sparse (2024-03-14)\nBlender Add-On (2024-03-20)\nmaximeraafat/BlenderNeRF - Github Found by DDG searching \u0026ldquo;how to make nerf blender\u0026rdquo; AI RENDERING from Blender to NeRF | BlenderNeRF Tutorial - Youtube NeRFStudio Nerfstudio-Docs Creating VFX with NeRFs - Nerfstudio Blender Add-On Tutorial - Youtube DTU (2024-03-21)\nAdapt the DTU dataset to the NeRF synthetic style dataset format. Concretely, the extrinsics (w2c) in DTU (aligned with OpenCV) are required to be modified to c2w of OpenGL.\nDTU dataset already provides point cloud (ply) and multi-view images.\n(2024-04-02)\nThe extrinsics in DTU is w2c. So, c2w = np.linalg.inv(extrinsics)\nAnd because the camera coordinate system in OpenCV (used by DTU) is RDF, the 4 columns in the c2w are [Right | Down | Front | CamCenter]\nHowever, in OpenGL, the c2w matrix should be: [Right | Up | Back | CamCenter]\n1 2 3 4 5 6 7 8 9 Trsfm = np.array([[ 9.70263e-01, 7.47983e-03, 2.41939e-01, -1.91020e+02], [-1.47429e-02, 9.99493e-01, 2.82234e-02, 3.28832e+00], [-2.41605e-01,-3.09510e-02, 9.69881e-01, 2.25401e+01], [ 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00]]) c2w_opencv = np.linalg.inv(Trsfm) print(c2w_opencv) c2w_opengl = c2w_opencv c2w_opengl[:,1] = - c2w_opencv[:,1] c2w_opengl[:,2] = - c2w_opencv[:,2] The other way is adjusting w2c: Each row in the rotation matrix of w2c is a camera axes. And the 4-th column is $-R_{c2w}^{-1} t_{c2w} = - R_{w2c} C$, where the C is the camera center.\nTo align with the OpenGL camera coord. system (from RDF to RUB), the 2nd and 3rd rows in w2c need flips:\n1 2 3 4 5 w2c_opencv = Trsfm w2c_opencv[1] *=-1 w2c_opencv[2] *=-1 c2w_opengl_ = np.linalg.inv(w2c_opencv) print((c2w_opengl_ ==c2w_opengl).all()) The c2w_opengl_ equals to the c2w_opengl above.\n","date":"2024-03-20T12:31:00Z","image":"https://blenderartists.org/uploads/default/optimized/4X/6/9/e/69e15630871293570c19e16b01524870e2526a06_2_562x750.jpeg","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-test-make_dataset/","title":"test: NeRF | Make Datasets"},{"content":"colmap/colmap - Github\nA sparse reconstruction model consists of 3 .txt files: cameras.txt, images.txt, points3D.txt Docs\nForge Colmap Result Data Code for writing data: colmap / scripts / python / read_write_model.py\ncameras.txt\nEach line is the intrinsic parameters of a camera.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def write_cameras_text(cameras, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::WriteCamerasText(const std::string\u0026amp; path) void Reconstruction::ReadCamerasText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; HEADER = ( \u0026#34;# Camera list with one line of data per camera:\\n\u0026#34; + \u0026#34;# CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\\n\u0026#34; + \u0026#34;# Number of cameras: {}\\n\u0026#34;.format(len(cameras)) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, cam in cameras.items(): to_write = [cam.id, cam.model, cam.width, cam.height, *cam.params] line = \u0026#34; \u0026#34;.join([str(elem) for elem in to_write]) fid.write(line + \u0026#34;\\n\u0026#34;) images.txt\nOne camera has two lines. The 1-st line includes extrinsic parameters. The 2-nd line lists the coordinates of 2D keypoints.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def write_images_text(images, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::ReadImagesText(const std::string\u0026amp; path) void Reconstruction::WriteImagesText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; if len(images) == 0: mean_observations = 0 else: mean_observations = sum( (len(img.point3D_ids) for _, img in images.items()) ) / len(images) HEADER = ( \u0026#34;# Image list with two lines of data per image:\\n\u0026#34; + \u0026#34;# IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\\n\u0026#34; + \u0026#34;# POINTS2D[] as (X, Y, POINT3D_ID)\\n\u0026#34; + \u0026#34;# Number of images: {}, mean observations per image: {}\\n\u0026#34;.format( len(images), mean_observations ) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, img in images.items(): image_header = [ img.id, *img.qvec, *img.tvec, img.camera_id, img.name, ] first_line = \u0026#34; \u0026#34;.join(map(str, image_header)) fid.write(first_line + \u0026#34;\\n\u0026#34;) points_strings = [] for xy, point3D_id in zip(img.xys, img.point3D_ids): points_strings.append(\u0026#34; \u0026#34;.join(map(str, [*xy, point3D_id]))) fid.write(\u0026#34; \u0026#34;.join(points_strings) + \u0026#34;\\n\u0026#34;) points3D.txt\nEach line is a 3D point.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def write_points3D_text(points3D, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::ReadPoints3DText(const std::string\u0026amp; path) void Reconstruction::WritePoints3DText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; if len(points3D) == 0: mean_track_length = 0 else: mean_track_length = sum( (len(pt.image_ids) for _, pt in points3D.items()) ) / len(points3D) HEADER = ( \u0026#34;# 3D point list with one line of data per point:\\n\u0026#34; + \u0026#34;# POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\\n\u0026#34; + \u0026#34;# Number of points: {}, mean track length: {}\\n\u0026#34;.format( len(points3D), mean_track_length ) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, pt in points3D.items(): point_header = [pt.id, *pt.xyz, *pt.rgb, pt.error] fid.write(\u0026#34; \u0026#34;.join(map(str, point_header)) + \u0026#34; \u0026#34;) track_strings = [] for image_id, point2D in zip(pt.image_ids, pt.point2D_idxs): track_strings.append(\u0026#34; \u0026#34;.join(map(str, [image_id, point2D]))) fid.write(\u0026#34; \u0026#34;.join(track_strings) + \u0026#34;\\n\u0026#34;) Problems I didn\u0026rsquo;t manage to realize the method as the following problems:\nEach image entry requires specify 2D keypoints.\nNot sure how to obtain them yet.\nAre the X-Y-Z axes of DTU camera aligned with right-bottom-front in COLMAP ?\nThis may require visulization to verify.\nInstall Ubuntu 22.04 ✅ (2024-04-17)\nOS: Ubuntu 22.04.4 LTS x86_64, Kernel: 6.5.0-27-generic\nInstall dependencies:\nPackages list {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 sudo apt-get install \\ git \\ cmake \\ ninja-build \\ build-essential \\ libboost-program-options-dev \\ libboost-filesystem-dev \\ libboost-graph-dev \\ libboost-system-dev \\ libeigen3-dev \\ libflann-dev \\ libfreeimage-dev \\ libmetis-dev \\ libgoogle-glog-dev \\ libgtest-dev \\ libsqlite3-dev \\ libglew-dev \\ qtbase5-dev \\ libqt5opengl5-dev \\ libcgal-dev \\ libceres-dev }}}\nQt dependencies has conflicts.\nError message: {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 The following packages have unmet dependencies: libqt5opengl5 : Depends: qtbase-abi-5-15-3 qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) E: Unable to correct problems, you have held broken packages. }}}\nSome posts suggests that the libqt5opengl5 has already been included into qt\nReference:\ncannot install qtbase-abi-5-5-1 on ubuntu 17.10 -SO\n1 2 (base) zichen@homepc:~$ apt-cache search qtbase-abi libqt5core5a - Qt 5 core module ubuntu22.04安装软件出现qtbase错误_qtbase-abi-5-15-3-CSDN博客\n这依赖捞不着啊- Community - Deepin Technology\nI want to skip those 2 packages and compile directly. However, got error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 (base) zichen@homepc:~/Downloads/colmap/build$ cmake .. -GNinja -- The C compiler identification is GNU 9.5.0 -- The CXX compiler identification is GNU 9.5.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found version \u0026#34;1.74.0\u0026#34;) found components: filesystem graph program_options system CMake Error at cmake/FindFLANN.cmake:89 (message): Could not find FLANN Call Stack (most recent call first): cmake/FindDependencies.cmake:17 (find_package) CMakeLists.txt:96 (include) -- Configuring incomplete, errors occurred! Found this issue: Colmap cmake .. error #1451\nI indeed have not installed the libmetis-dev and other packages except for the above 2 missing packages:\nPackages list {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sudo apt-get install \\ git \\ cmake \\ ninja-build \\ build-essential \\ libboost-program-options-dev \\ libboost-filesystem-dev \\ libboost-graph-dev \\ libboost-system-dev \\ libeigen3-dev \\ libflann-dev \\ libfreeimage-dev \\ libmetis-dev \\ libgoogle-glog-dev \\ libgtest-dev \\ libsqlite3-dev \\ libglew-dev \\ libcgal-dev \\ libceres-dev }}}\nLinking failed:\n1 2 3 4 5 6 7 8 9 /home/zichen/anaconda3/lib/libQt5OpenGL.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Widgets.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Gui.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2 -lcudadevrt -lcudart_static -lrt -lpthread -ldl \u0026amp;\u0026amp; : /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldTag@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldName@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldReadCount@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldPassCount@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldDataType@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `_TIFFDataSize@LIBTIFF_4.0\u0026#39; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. Uninstall libtiff\nAccording to this issue: /usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/libfreeimage.so: undefined reference to `_TIFFDataSize@LIBTIFF_4.0\u0026rsquo; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. #1803\n1 2 3 4 ninja clean conda uninstall libtiff cmake .. -GNinja ninja I have libtiff:\n1 2 3 4 5 (base) zichen@homepc:~/Downloads/colmap/build$ conda list libtiff # packages in environment at /home/zichen/anaconda3: # # Name Version Build Channel libtiff 4.5.0 h6a678d5_2 https://repo.anaconda.com/pkgs/main Another error about libQt5:\n{{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 [213/213] Linking CXX executable src/colmap/exe/colmap FAILED: src/colmap/exe/colmap : \u0026amp;\u0026amp; /usr/bin/c++ -Wno-maybe-uninitialized -Wall -O3 -DNDEBUG src/colmap/exe/CMakeFiles/colmap_main.dir/feature.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/sfm.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/colmap.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/database.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/gui.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/image.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/model.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/mvs.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/vocab_tree.cc.o -o src/colmap/exe/colmap -L/usr/local/cuda-11.6/targets/x86_64-linux/lib/stubs -L/usr/local/cuda-11.6/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-11.6/targets/x86_64-linux/lib:/home/zichen/anaconda3/lib: src/colmap/controllers/libcolmap_controllers.a src/colmap/retrieval/libcolmap_retrieval.a src/colmap/scene/libcolmap_scene.a src/colmap/sfm/libcolmap_sfm.a src/colmap/util/libcolmap_util.a src/colmap/util/libcolmap_util_cuda.a src/colmap/mvs/libcolmap_mvs_cuda.a src/colmap/ui/libcolmap_ui.a src/colmap/util/libcolmap_util_cuda.a src/colmap/controllers/libcolmap_controllers.a src/colmap/retrieval/libcolmap_retrieval.a src/colmap/sfm/libcolmap_sfm.a src/colmap/mvs/libcolmap_mvs.a src/thirdparty/PoissonRecon/libcolmap_poisson_recon.a /usr/lib/x86_64-linux-gnu/libgmpxx.so /usr/lib/x86_64-linux-gnu/libmpfr.so /usr/lib/x86_64-linux-gnu/libgmp.so src/colmap/estimators/libcolmap_estimators.a src/colmap/feature/libcolmap_feature.a /usr/lib/x86_64-linux-gnu/libflann.so /usr/lib/x86_64-linux-gnu/liblz4.so src/thirdparty/SiftGPU/libcolmap_sift_gpu.a /usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-11.6/targets/x86_64-linux/lib/libcurand.so /usr/lib/x86_64-linux-gnu/libGLEW.so src/colmap/optim/libcolmap_optim.a /usr/lib/x86_64-linux-gnu/libboost_program_options.so.1.74.0 src/colmap/image/libcolmap_image.a src/colmap/scene/libcolmap_scene.a src/colmap/feature/libcolmap_feature_types.a src/colmap/geometry/libcolmap_geometry.a src/colmap/math/libcolmap_math.a /usr/lib/x86_64-linux-gnu/libmetis.so /usr/lib/x86_64-linux-gnu/libboost_graph.so.1.74.0 /usr/lib/x86_64-linux-gnu/libboost_regex.so.1.74.0 src/colmap/sensor/libcolmap_sensor.a src/colmap/util/libcolmap_util.a /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.74.0 /usr/lib/x86_64-linux-gnu/libsqlite3.so /usr/lib/x86_64-linux-gnu/libGLX.so /usr/lib/x86_64-linux-gnu/libOpenGL.so /usr/lib/libceres.so.2.0.0 /usr/lib/x86_64-linux-gnu/libglog.so.0.4.0 /usr/lib/x86_64-linux-gnu/libunwind.so /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.2 -lpthread src/thirdparty/VLFeat/libcolmap_vlfeat.a /usr/lib/gcc/x86_64-linux-gnu/9/libgomp.so /usr/lib/x86_64-linux-gnu/libpthread.a /usr/lib/x86_64-linux-gnu/libfreeimage.so src/thirdparty/LSD/libcolmap_lsd.a /home/zichen/anaconda3/lib/libQt5OpenGL.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Widgets.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Gui.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2 -lcudadevrt -lcudart_static -lrt -lpthread -ldl \u0026amp;\u0026amp; : /usr/bin/ld: warning: libicui18n.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libicuuc.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libicudata.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_errorName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_setMillis_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getAlias_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_inDaylightTime_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_strToLower_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getStandardName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_strToUpper_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_setSubstChars_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getMaxCharSize_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getTimeZoneDisplayName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_fromUnicode_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getDefaultName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getDefaultTimeZone_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_clone_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getDSTSavings_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_strcoll_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_countAvailable_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openCountryTimeZones_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_compareNames_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getAvailableName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openTimeZoneIDEnumeration_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_setAttribute_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openTimeZones_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `uenum_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_countAliases_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_getSortKey_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_get_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `uenum_next_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_toUnicode_58\u0026#39; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. }}}\nReference:\nLinkage against libQt5Core - SO\nBuild from source error libQt5Core.so.5.15.2: undefined reference to #3829\nI already have libicu-dev installed.\nInstall qtbase5-dev\nI noticed the error said: \u0026ldquo;warning: libicudata.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found\u0026rdquo;.\nAnd the binary couldn\u0026rsquo;t find symbols: \u0026ldquo;/home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to u_errorName_58'\u0026rdquo;\nI didn\u0026rsquo;t have Qt? Is this why there is the suggestion?\n1 sudo aptitude install qtbase5-dev Someone used aptitude to install the above 2 packages. Ubuntu20.04安装colmap从零开始全过程记录（包括CUDA/CUDNN/ceres/anaconda） - CSDN I forgot to check Qt before installing qtbase5: How to find Version of Qt? - SO\n1 2 3 (base) zichen@homepc:~/Downloads/colmap$ QT_SELECT=5 qmake -v QMake version 3.1 Using Qt version 5.15.2 in /home/zichen/anaconda3/lib (2024-04-19) I accepted the first solution provided by the aptitude. But, I didn\u0026rsquo;t notice that the 1st solution is to keep everything unchanged and give up installing qtbase5. Qt Pkgs Matter (2024-04-19)\nThe \u0026ldquo;undefined reference\u0026rdquo; errors persist:\n1 /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_errorName_58\u0026#39; I uninstalled anaconda (removed ~/anaconda3 to /mnt), based on this answer: OpenCV undefined references for libQt5Core.so.5 - raggot (Found by search the above error).\nI tested later that rename the ~/anaconda3 works as well. 【超详细】安装了anaconda后，Ubuntu18+COLMAP配置疯狂踩坑踩至魔怔的记录 - CSDN Then cmake .. -GNinja. It could not find Qt5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CMake Error at cmake/FindDependencies.cmake:150 (find_package): By not providing \u0026#34;FindQt5.cmake\u0026#34; in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \u0026#34;Qt5\u0026#34;, but CMake did not find one. Could not find a package configuration file provided by \u0026#34;Qt5\u0026#34; (requested version 5.4) with any of the following names: Qt5Config.cmake qt5-config.cmake Add the installation prefix of \u0026#34;Qt5\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;Qt5_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;Qt5\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): CMakeLists.txt:96 (include) Qt5 has been existed on the system:\n1 2 3 zichen@homepc:~/Downloads/colmap$ QT_SELECT=5 qmake -v QMake version 3.1 Using Qt version 5.15.3 in /usr/lib/x86_64-linux-gnu Specifing path to Qt5 doesn\u0026rsquo;t work:\n1 2 3 cmake .. -DQt5_DIR=/usr/lib/x86_64-linux-gnu/ -GNinja cmake .. -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/ -GNinja cmake -B ./build -DCMAKE_PREFIX_PATH=\u0026#34;/usr/lib/x86_64-linux-gnu/qt5;/usr/lib/qt5;/usr/share/qt5\u0026#34; -GNinja I really don\u0026rsquo;t have Qt?\n\u0026ldquo;ubuntu check qt version\u0026rdquo; DDG\n1 2 zichen@homepc:~/Downloads/colmap$ qmake -v qmake: could not find a Qt installation of \u0026#39;\u0026#39; The following packages already existed on the system: (qt4 - How to find Version of Qt? - SO)\nqt related pkgs on my computer: {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 zichen@homepc:~/Downloads/colmap$ dpkg -l | grep qt ri libfcitx-qt5-1:amd64 1.2.7-1.2build1 amd64 Free Chinese Input Toy of X - D-Bus client libraries for Qt5 ri libfcitx-qt5-data 1.2.7-1.2build1 all Free Chinese Input Toy of X - data files for Qt5 integration ii libgsettings-qt1:amd64 0.2-4 amd64 library to access GSettings from Qt (shared libraries) ii libqt5core5a:amd64 5.15.8.1-1+dde amd64 Qt 5 core module ii libqt5core5a:i386 5.15.8.1-1+dde i386 Qt 5 core module ii libqt5dbus5:amd64 5.15.8.1-1+dde amd64 Qt 5 D-Bus module ii libqt5dbus5:i386 5.15.8.1-1+dde i386 Qt 5 D-Bus module ii libqt5gui5:amd64 5.15.8.1-1+dde amd64 Qt 5 GUI module ii libqt5network5:amd64 5.15.8.1-1+dde amd64 Qt 5 network module ri libqt5positioning5:amd64 5.15.8-1+dde amd64 Qt Positioning module ri libqt5printsupport5:amd64 5.15.8.1-1+dde amd64 Qt 5 print support module ii libqt5qml5:amd64 5.15.8.1-1+dde amd64 Qt 5 QML module ii libqt5qmlmodels5:amd64 5.15.8.1-1+dde amd64 Qt 5 QML Models library ii libqt5quick5:amd64 5.15.8.1-1+dde amd64 Qt 5 Quick library ii libqt5quickwidgets5:amd64 5.15.8.1-1+dde amd64 Qt 5 Quick Widgets library ri libqt5webchannel5:amd64 5.15.8-1+dde amd64 Web communication library for Qt ii libqt5widgets5:amd64 5.15.8.1-1+dde amd64 Qt 5 widgets module ri libqt5x11extras5:amd64 5.15.8-1+dde amd64 Qt 5 X11 extras ii qt5-qmake:amd64 5.15.3+dfsg-2ubuntu0.2 amd64 Qt 5 qmake Makefile generator tool ii qt5-qmake-bin 5.15.3+dfsg-2ubuntu0.2 amd64 Qt 5 qmake Makefile generator tool — binary file ii qtchooser 66-2build1 amd64 Wrapper to select between Qt development binary versions ii qttranslations5-l10n 5.15.3-1 all translations for Qt 5 zichen@homepc:~/Downloads/colmap$ }}}\nHow to make sure that Qt5.4.2 is installed properly\n1 2 zichen@homepc:~/Downloads/colmap$ whereis qt5 qt5: /usr/lib/x86_64-linux-gnu/qt5 /usr/lib/qt5 /usr/share/qt5 I have qmake:\n1 2 3 4 zichen@homepc:~/Downloads/colmap$ export QT_SELECT=qt5-x86_64-linux-gnu zichen@homepc:~/Downloads/colmap$ qmake -v QMake version 3.1 Using Qt version 5.15.3 in /usr/lib/x86_64-linux-gnu Only 1 binary qmake exists:\n1 2 3 4 5 6 zichen@homepc:~/Downloads/colmap$ tree /usr/lib/qt5 /usr/lib/qt5 └── bin └── qmake 1 directory, 1 file There should be other libraries or binaries for development. apt can find qtbase5-dev, but I can\u0026rsquo;t install: How do you install qt on ubuntu22.04\nLog {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 zichen@homepc:~/Downloads/colmap$ apt-cache search qt zichen@homepc:~/Downloads/colmap$ sudo apt-get install qtbase5-dev Reading package lists... Done Building dependency tree... Done Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) Recommends: libqt5opengl5-dev (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed E: Unable to correct problems, you have held broken packages. }}}\nqt5-default can\u0026rsquo;t be installed neither:\n1 2 3 4 5 6 7 8 9 10 zichen@homepc:~/Downloads/colmap$ sudo apt-get install qt5-default [sudo] password for zichen: Reading package lists... Done Building dependency tree... Done Reading state information... Done Package qt5-default is not available, but is referred to by another package. This may mean that the package is missing, has been obsoleted, or is only available from another source E: Package \u0026#39;qt5-default\u0026#39; has no installation candidate Others packages related qtbase5:\n1 2 3 4 5 zichen@homepc:~/Downloads/colmap$ apt-cache search qtbase5-dev qtbase5-dev - Qt 5 base development files qtbase5-dev-tools - Qt 5 base development programs qtbase5-gles-dev - Qt 5 base development files — OpenGL ES variant qtbase5-private-gles-dev - Qt 5 base private development files — OpenGL ES variant Some of them requires qtbase-abi-5-15-3\nBut, I can\u0026rsquo;t install it:\n1 E: Package \u0026#39;qtbase-abi-5-15-3\u0026#39; has no installation candidate Select the second solution offered by aptitude:\nOutput {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 zichen@homepc:~/Downloads/colmap$ sudo aptitude install qtbase5-dev The following NEW packages will be installed: qtbase5-dev{b} 0 packages upgraded, 1 newly installed, 0 to remove and 15 not upgraded. Need to get 1,135 kB of archives. After unpacking 15.7 MB will be used. The following packages have unmet dependencies: qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable The following actions will resolve these dependencies: Keep the following packages at their current version: 1) qtbase5-dev [Not Installed] Accept this solution? [Y/n/q/?] . The following actions will resolve these dependencies: Remove the following packages: 1) libqt5core5a:i386 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now)] 2) libqt5dbus5:i386 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now)] 3) libqt5positioning5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] 4) libqt5webchannel5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] 5) libqt5x11extras5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] Install the following packages: 6) libqt5sql5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 7) libqt5sql5-odbc [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 8) libqt5test5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 9) libqt5xml5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 10) qtbase5-dev-tools [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] Downgrade the following packages: 11) libqt5concurrent5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 12) libqt5core5a [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 13) libqt5dbus5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 14) libqt5gui5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 15) libqt5network5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 16) libqt5printsupport5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 17) libqt5qml5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 18) libqt5qmlmodels5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 19) libqt5quick5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 20) libqt5quickwidgets5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 21) libqt5widgets5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] Accept this solution? [Y/n/q/?] Y The following packages will be DOWNGRADED: libqt5concurrent5 libqt5core5a libqt5dbus5 libqt5gui5 libqt5network5 libqt5printsupport5 libqt5qml5 libqt5qmlmodels5 libqt5quick5 libqt5quickwidgets5 libqt5widgets5 The following NEW packages will be installed: libqt5sql5{a} libqt5sql5-odbc{a} libqt5test5{a} libqt5xml5{a} qtbase5-dev qtbase5-dev-tools{a} The following packages will be REMOVED: libqt5core5a:i386{a} libqt5dbus5:i386{a} libqt5positioning5{a} libqt5webchannel5{a} libqt5x11extras5{a} The following packages are RECOMMENDED but will NOT be installed: libqt5svg5 qt5-gtk-platformtheme 0 packages upgraded, 6 newly installed, 11 downgraded, 5 to remove and 15 not upgraded. Need to get 15.1 MB of archives. After unpacking 6,725 kB will be used. Do you want to continue? [Y/n/?] }}}\nAfter installation, there are more binary files:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 zichen@homepc:~/Downloads/colmap$ tree /usr/lib/qt5/bin/ /usr/lib/qt5/bin/ ├── fixqt4headers.pl ├── moc ├── qdbuscpp2xml ├── qdbusxml2cpp ├── qlalr ├── qmake ├── qvkgen ├── rcc ├── syncqt.pl ├── tracegen └── uic 0 directories, 11 files The Qt5OpenGL is required as well:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CMake Error at /usr/lib/x86_64-linux-gnu/cmake/Qt5/Qt5Config.cmake:28 (find_package): Could not find a package configuration file provided by \u0026#34;Qt5OpenGL\u0026#34; with any of the following names: Qt5OpenGLConfig.cmake qt5opengl-config.cmake Add the installation prefix of \u0026#34;Qt5OpenGL\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;Qt5OpenGL_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;Qt5OpenGL\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): cmake/FindDependencies.cmake:150 (find_package) CMakeLists.txt:96 (include) Similarly, use aptitude to install it:\n1 2 3 4 5 6 zichen@homepc:~/Downloads/colmap$ sudo aptitude install libqt5opengl5-dev The following NEW packages will be installed: libqt5opengl5{a} libqt5opengl5-dev 0 packages upgraded, 2 newly installed, 0 to remove and 27 not upgraded. Need to get 195 kB of archives. After unpacking 927 kB will be used. Do you want to continue? [Y/n/?] Y Finally, compilation succeeded:\n1 2 3 zichen@homepc:~/Downloads/colmap$ cmake -B ./build -GNinja cd build ninja (2024-04-18)\nCUDA requires GCC setup under Ubuntu 22.04. Docs\n1 2 3 4 # sudo apt-get install gcc-10 g++-10 export CC=/usr/bin/gcc-10 export CXX=/usr/bin/g++-10 export CUDAHOSTCXX=/usr/bin/g++-10 Installing nvidia-cuda-toolkit failed via apt install. I don\u0026rsquo;t install it since I already have CUDA-11.6 installed by downloading package manually.\nVerified Practice (2024-04-19)\nRename anaconda3 folder, e.g., anaconda3_1. Otherwise, the qt5 called by header and cmake mismatch. issue-sofa-fredroy\nUse aptitude to install unavailable dependecies, as apt install cannot install qtbase5-dev and libqt5opengl5-dev on Ubuntu 22.04.\nSelect an alternative solutions provided by aptitude.\nSpecify gcc and g++ versions for Ubuntu 22.04:\n1 2 3 4 # sudo apt-get install gcc-10 g++-10 export CC=/usr/bin/gcc-10 export CXX=/usr/bin/g++-10 export CUDAHOSTCXX=/usr/bin/g++-10 Compile colmap:\n1 2 3 4 5 # cd colmap cmake -B ./build -GNinja cd build ninja sudo ninja install The colmap is installed in /usr/local\n{{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 [0/1] Install the project... -- Install configuration: \u0026#34;Release\u0026#34; -- Installing: /usr/local/share/applications/COLMAP.desktop -- Installing: /usr/local/lib/libcolmap_controllers.a -- Installing: /usr/local/lib/libcolmap_estimators.a -- Installing: /usr/local/lib/libcolmap_exe.a -- Installing: /usr/local/lib/libcolmap_feature_types.a -- Installing: /usr/local/lib/libcolmap_feature.a -- Installing: /usr/local/lib/libcolmap_geometry.a -- Installing: /usr/local/lib/libcolmap_image.a -- Installing: /usr/local/lib/libcolmap_math.a -- Installing: /usr/local/lib/libcolmap_mvs.a -- Installing: /usr/local/lib/libcolmap_optim.a -- Installing: /usr/local/lib/libcolmap_retrieval.a -- Installing: /usr/local/lib/libcolmap_scene.a -- Installing: /usr/local/lib/libcolmap_sensor.a -- Installing: /usr/local/lib/libcolmap_sfm.a -- Installing: /usr/local/lib/libcolmap_util.a -- Installing: /usr/local/lib/libcolmap_lsd.a -- Installing: /usr/local/lib/libcolmap_poisson_recon.a -- Installing: /usr/local/lib/libcolmap_vlfeat.a -- Installing: /usr/local/lib/libcolmap_ui.a -- Installing: /usr/local/lib/libcolmap_util_cuda.a -- Installing: /usr/local/lib/libcolmap_mvs_cuda.a -- Installing: /usr/local/lib/libcolmap_sift_gpu.a -- Installing: /usr/local/share/colmap/colmap-config.cmake -- Installing: /usr/local/share/colmap/colmap-config-version.cmake -- Installing: /usr/local/share/colmap/colmap-targets.cmake -- Installing: /usr/local/share/colmap/colmap-targets-release.cmake -- Installing: /usr/local/include/colmap -- Installing: /usr/local/include/colmap/estimators -- Installing: /usr/local/include/colmap/estimators/utils.h -- Installing: /usr/local/include/colmap/estimators/pose.h -- Installing: /usr/local/include/colmap/estimators/coordinate_frame.h -- Installing: /usr/local/include/colmap/estimators/two_view_geometry.h -- Installing: /usr/local/include/colmap/estimators/generalized_absolute_pose.h -- Installing: /usr/local/include/colmap/estimators/bundle_adjustment.h -- Installing: /usr/local/include/colmap/estimators/affine_transform.h -- Installing: /usr/local/include/colmap/estimators/generalized_absolute_pose_coeffs.h -- Installing: /usr/local/include/colmap/estimators/fundamental_matrix.h -- Installing: /usr/local/include/colmap/estimators/euclidean_transform.h -- Installing: /usr/local/include/colmap/estimators/alignment.h -- Installing: /usr/local/include/colmap/estimators/homography_matrix.h -- Installing: /usr/local/include/colmap/estimators/absolute_pose.h -- Installing: /usr/local/include/colmap/estimators/similarity_transform.h -- Installing: /usr/local/include/colmap/estimators/triangulation.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix_poly.h -- Installing: /usr/local/include/colmap/estimators/cost_functions.h -- Installing: /usr/local/include/colmap/estimators/generalized_relative_pose.h -- Installing: /usr/local/include/colmap/estimators/generalized_pose.h -- Installing: /usr/local/include/colmap/estimators/translation_transform.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix_coeffs.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix.h -- Installing: /usr/local/include/colmap/controllers -- Installing: /usr/local/include/colmap/controllers/feature_matching_utils.h -- Installing: /usr/local/include/colmap/controllers/automatic_reconstruction.h -- Installing: /usr/local/include/colmap/controllers/bundle_adjustment.h -- Installing: /usr/local/include/colmap/controllers/hierarchical_mapper.h -- Installing: /usr/local/include/colmap/controllers/incremental_mapper.h -- Installing: /usr/local/include/colmap/controllers/image_reader.h -- Installing: /usr/local/include/colmap/controllers/feature_extraction.h -- Installing: /usr/local/include/colmap/controllers/feature_matching.h -- Installing: /usr/local/include/colmap/controllers/option_manager.h -- Installing: /usr/local/include/colmap/math -- Installing: /usr/local/include/colmap/math/math.h -- Installing: /usr/local/include/colmap/math/matrix.h -- Installing: /usr/local/include/colmap/math/graph_cut.h -- Installing: /usr/local/include/colmap/math/polynomial.h -- Installing: /usr/local/include/colmap/math/random.h -- Installing: /usr/local/include/colmap/exe -- Installing: /usr/local/include/colmap/exe/gui.h -- Installing: /usr/local/include/colmap/exe/image.h -- Installing: /usr/local/include/colmap/exe/database.h -- Installing: /usr/local/include/colmap/exe/feature.h -- Installing: /usr/local/include/colmap/exe/model.h -- Installing: /usr/local/include/colmap/exe/mvs.h -- Installing: /usr/local/include/colmap/exe/vocab_tree.h -- Installing: /usr/local/include/colmap/exe/sfm.h -- Installing: /usr/local/include/colmap/optim -- Installing: /usr/local/include/colmap/optim/sampler.h -- Installing: /usr/local/include/colmap/optim/random_sampler.h -- Installing: /usr/local/include/colmap/optim/least_absolute_deviations.h -- Installing: /usr/local/include/colmap/optim/support_measurement.h -- Installing: /usr/local/include/colmap/optim/combination_sampler.h -- Installing: /usr/local/include/colmap/optim/ransac.h -- Installing: /usr/local/include/colmap/optim/progressive_sampler.h -- Installing: /usr/local/include/colmap/optim/sprt.h -- Installing: /usr/local/include/colmap/optim/loransac.h -- Installing: /usr/local/include/colmap/sensor -- Installing: /usr/local/include/colmap/sensor/models.h -- Installing: /usr/local/include/colmap/sensor/database.h -- Installing: /usr/local/include/colmap/sensor/bitmap.h -- Installing: /usr/local/include/colmap/sensor/specs.h -- Installing: /usr/local/include/colmap/util -- Installing: /usr/local/include/colmap/util/types.h -- Installing: /usr/local/include/colmap/util/ply.h -- Installing: /usr/local/include/colmap/util/string.h -- Installing: /usr/local/include/colmap/util/threading.h -- Installing: /usr/local/include/colmap/util/sqlite3_utils.h -- Installing: /usr/local/include/colmap/util/base_controller.h -- Installing: /usr/local/include/colmap/util/endian.h -- Installing: /usr/local/include/colmap/util/eigen_alignment.h -- Installing: /usr/local/include/colmap/util/logging.h -- Installing: /usr/local/include/colmap/util/version.h -- Installing: /usr/local/include/colmap/util/timer.h -- Installing: /usr/local/include/colmap/util/cuda.h -- Installing: /usr/local/include/colmap/util/controller_thread.h -- Installing: /usr/local/include/colmap/util/cache.h -- Installing: /usr/local/include/colmap/util/testing.h -- Installing: /usr/local/include/colmap/util/opengl_utils.h -- Installing: /usr/local/include/colmap/util/misc.h -- Installing: /usr/local/include/colmap/util/cudacc.h -- Installing: /usr/local/include/colmap/mvs -- Installing: /usr/local/include/colmap/mvs/mat.h -- Installing: /usr/local/include/colmap/mvs/meshing.h -- Installing: /usr/local/include/colmap/mvs/cuda_rotate.h -- Installing: /usr/local/include/colmap/mvs/cuda_texture.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat_ref_image.h -- Installing: /usr/local/include/colmap/mvs/image.h -- Installing: /usr/local/include/colmap/mvs/cuda_flip.h -- Installing: /usr/local/include/colmap/mvs/patch_match_cuda.h -- Installing: /usr/local/include/colmap/mvs/cuda_transpose.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat_prng.h -- Installing: /usr/local/include/colmap/mvs/depth_map.h -- Installing: /usr/local/include/colmap/mvs/normal_map.h -- Installing: /usr/local/include/colmap/mvs/workspace.h -- Installing: /usr/local/include/colmap/mvs/fusion.h -- Installing: /usr/local/include/colmap/mvs/patch_match.h -- Installing: /usr/local/include/colmap/mvs/model.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat.h -- Installing: /usr/local/include/colmap/mvs/consistency_graph.h -- Installing: /usr/local/include/colmap/image -- Installing: /usr/local/include/colmap/image/undistortion.h -- Installing: /usr/local/include/colmap/image/warp.h -- Installing: /usr/local/include/colmap/image/line.h -- Installing: /usr/local/include/colmap/sfm -- Installing: /usr/local/include/colmap/sfm/incremental_mapper.h -- Installing: /usr/local/include/colmap/sfm/incremental_triangulator.h -- Installing: /usr/local/include/colmap/retrieval -- Installing: /usr/local/include/colmap/retrieval/utils.h -- Installing: /usr/local/include/colmap/retrieval/vote_and_verify.h -- Installing: /usr/local/include/colmap/retrieval/inverted_file_entry.h -- Installing: /usr/local/include/colmap/retrieval/inverted_file.h -- Installing: /usr/local/include/colmap/retrieval/geometry.h -- Installing: /usr/local/include/colmap/retrieval/inverted_index.h -- Installing: /usr/local/include/colmap/retrieval/visual_index.h -- Installing: /usr/local/include/colmap/scene -- Installing: /usr/local/include/colmap/scene/synthetic.h -- Installing: /usr/local/include/colmap/scene/point2d.h -- Installing: /usr/local/include/colmap/scene/two_view_geometry.h -- Installing: /usr/local/include/colmap/scene/reconstruction_manager.h -- Installing: /usr/local/include/colmap/scene/reconstruction_io.h -- Installing: /usr/local/include/colmap/scene/image.h -- Installing: /usr/local/include/colmap/scene/scene_clustering.h -- Installing: /usr/local/include/colmap/scene/database.h -- Installing: /usr/local/include/colmap/scene/database_cache.h -- Installing: /usr/local/include/colmap/scene/camera.h -- Installing: /usr/local/include/colmap/scene/track.h -- Installing: /usr/local/include/colmap/scene/reconstruction.h -- Installing: /usr/local/include/colmap/scene/correspondence_graph.h -- Installing: /usr/local/include/colmap/scene/projection.h -- Installing: /usr/local/include/colmap/scene/visibility_pyramid.h -- Installing: /usr/local/include/colmap/scene/camera_rig.h -- Installing: /usr/local/include/colmap/scene/point3d.h -- Installing: /usr/local/include/colmap/tools -- Installing: /usr/local/include/colmap/feature -- Installing: /usr/local/include/colmap/feature/utils.h -- Installing: /usr/local/include/colmap/feature/types.h -- Installing: /usr/local/include/colmap/feature/matcher.h -- Installing: /usr/local/include/colmap/feature/sift.h -- Installing: /usr/local/include/colmap/feature/extractor.h -- Installing: /usr/local/include/colmap/geometry -- Installing: /usr/local/include/colmap/geometry/pose.h -- Installing: /usr/local/include/colmap/geometry/gps.h -- Installing: /usr/local/include/colmap/geometry/sim3.h -- Installing: /usr/local/include/colmap/geometry/rigid3.h -- Installing: /usr/local/include/colmap/geometry/homography_matrix.h -- Installing: /usr/local/include/colmap/geometry/triangulation.h -- Installing: /usr/local/include/colmap/geometry/essential_matrix.h -- Installing: /usr/local/include/colmap/ui -- Installing: /usr/local/include/colmap/ui/qt_utils.h -- Installing: /usr/local/include/colmap/ui/image_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/colormaps.h -- Installing: /usr/local/include/colmap/ui/bundle_adjustment_widget.h -- Installing: /usr/local/include/colmap/ui/feature_extraction_widget.h -- Installing: /usr/local/include/colmap/ui/model_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/thread_control_widget.h -- Installing: /usr/local/include/colmap/ui/point_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/project_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_options_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_manager_widget.h -- Installing: /usr/local/include/colmap/ui/license_widget.h -- Installing: /usr/local/include/colmap/ui/point_painter.h -- Installing: /usr/local/include/colmap/ui/media -- Installing: /usr/local/include/colmap/ui/undistortion_widget.h -- Installing: /usr/local/include/colmap/ui/render_options.h -- Installing: /usr/local/include/colmap/ui/movie_grabber_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_stats_widget.h -- Installing: /usr/local/include/colmap/ui/shaders -- Installing: /usr/local/include/colmap/ui/match_matrix_widget.h -- Installing: /usr/local/include/colmap/ui/feature_matching_widget.h -- Installing: /usr/local/include/colmap/ui/options_widget.h -- Installing: /usr/local/include/colmap/ui/main_window.h -- Installing: /usr/local/include/colmap/ui/log_widget.h -- Installing: /usr/local/include/colmap/ui/database_management_widget.h -- Installing: /usr/local/include/colmap/ui/triangle_painter.h -- Installing: /usr/local/include/colmap/ui/automatic_reconstruction_widget.h -- Installing: /usr/local/include/colmap/ui/line_painter.h -- Installing: /usr/local/include/colmap/ui/dense_reconstruction_widget.h -- Installing: /usr/local/include/colmap/ui/render_options_widget.h -- Installing: /usr/local/include/colmap/thirdparty -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/LiteWindow.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/GlobalUtil.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidGL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidCL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/CuTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftPyramid.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/GLTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCG.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftMatchCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftMatch.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftGPU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/CLTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/FrameBufferObject.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ShaderMan.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramGLSL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramGPU.h -- Installing: /usr/local/include/colmap/thirdparty/LSD -- Installing: /usr/local/include/colmap/thirdparty/LSD/lsd.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/fisher.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/qsort-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/generic.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/sift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop_sse2.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/heap-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/gmm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/ikmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/slic.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/stringop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/quickshift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/hog.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/lbp.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/float.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/kmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop_avx.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/host.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/dsift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/homkermap.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/array.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/pgm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/svmdataset.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/shuffle-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/svm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mser.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/liop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/scalespace.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/imopv_sse2.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/rodrigues.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/vlad.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/covdet.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/imopv.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/aib.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/hikmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/getopt_long.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/random.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/kdtree.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SurfaceTrimmer.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Polynomial.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BinaryNode.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.IsoSurface.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Geometry.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BSplineData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/CmdLineParser.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Allocator.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.Evaluation.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Factor.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BSplineData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Polynomial.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PPolynomial.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SparseMatrix.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/FunctionData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MarchingCubes.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SparseMatrix.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Array.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/FunctionData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Array.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MAT.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PoissonRecon.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MAT.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Geometry.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Octree.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.SortedTreeNodes.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MemoryUsage.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.WeightedSamples.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Hash.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PPolynomial.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Octree.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PointStream.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/CmdLineParser.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.System.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Ply.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MyTime.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PointStream.h -- Installing: /usr/local/share/colmap/cmake -- Installing: /usr/local/share/colmap/cmake/FindLZ4.cmake -- Installing: /usr/local/share/colmap/cmake/FindMetis.cmake -- Installing: /usr/local/share/colmap/cmake/FindFLANN.cmake -- Installing: /usr/local/share/colmap/cmake/FindGlew.cmake -- Installing: /usr/local/share/colmap/cmake/FindFreeImage.cmake -- Installing: /usr/local/share/colmap/cmake/FindGlog.cmake -- Installing: /usr/local/share/colmap/cmake/FindDependencies.cmake -- Installing: /usr/local/bin/colmap -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/colmap\u0026#34; to \u0026#34;\u0026#34; }}}\nUbuntu 20.04 ❌ (2024-04-18)\nSystem info:\nOS: Ubuntu 20.04.6 LTS x86_64; Kernel: 5.15.0-101-generic Host: Alienware Aurora R8 1.0.6 CPU: Intel i7-9700 (8) @ 4.700GHz, GPU: Intel UHD Graphics 630 GPU: NVIDIA GeForce GTX 1050 Ti Memory: 16GB gcc version 9.4.0 Cuda compilation tools, release 11.6, V11.6.55; Build cuda_11.6.r11.6/compiler.30794723_0 (nvcc -V) Nvidia Driver Version: 545.23.06 Dependecies: Docs\nThe installation has no error reported:\nLog {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 (base) yi@yi-Alienware-Aurora-R8:~/Downloads$ sudo apt-get install \\ \u0026gt; git \\ \u0026gt; cmake \\ \u0026gt; ninja-build \\ \u0026gt; build-essential \\ \u0026gt; libboost-program-options-dev \\ \u0026gt; libboost-filesystem-dev \\ \u0026gt; libboost-graph-dev \\ \u0026gt; libboost-system-dev \\ \u0026gt; libeigen3-dev \\ \u0026gt; libflann-dev \\ \u0026gt; libfreeimage-dev \\ \u0026gt; libmetis-dev \\ \u0026gt; libgoogle-glog-dev \\ \u0026gt; libgtest-dev \\ \u0026gt; libsqlite3-dev \\ \u0026gt; libglew-dev \\ \u0026gt; qtbase5-dev \\ \u0026gt; libqt5opengl5-dev \\ \u0026gt; libcgal-dev \\ \u0026gt; libceres-dev [sudo] password for yi: Reading package lists... Done Building dependency tree Reading state information... Done libboost-filesystem-dev is already the newest version (1.71.0.0ubuntu2). libboost-filesystem-dev set to manually installed. libboost-program-options-dev is already the newest version (1.71.0.0ubuntu2). libboost-program-options-dev set to manually installed. libboost-system-dev is already the newest version (1.71.0.0ubuntu2). libboost-system-dev set to manually installed. libboost-graph-dev is already the newest version (1.71.0.0ubuntu2). libboost-graph-dev set to manually installed. libeigen3-dev is already the newest version (3.3.7-2). libglew-dev is already the newest version (2.1.0-4). build-essential is already the newest version (12.8ubuntu1.1). build-essential set to manually installed. git is already the newest version (1:2.25.1-1ubuntu3.11). The following additional packages will be installed: cmake-data googletest libaec-dev libatlas3-base libblas-dev libbtf1 libceres1 libcxsparse3 libflann1.9 libfreeimage3 libgflags-dev libgflags2.2 libgmp-dev libgmpxx4ldbl libgoogle-glog0v5 libgraphblas3 libhdf5-mpi-dev libhdf5-openmpi-dev libjxr0 libklu1 liblapack-dev libldl2 liblz4-dev libmongoose2 libmpfr-dev libqt5opengl5 librbio2 librhash0 libspqr2 libsuitesparse-dev qt5-qmake qt5-qmake-bin qtbase5-dev-tools qtchooser Suggested packages: cmake-doc liblapack-doc libmpfi-dev libntl-dev gmp-doc libgmp10-doc libhdf5-doc libmpfr-doc sqlite3-doc default-libmysqlclient-dev firebird-dev libpq-dev unixodbc-dev The following NEW packages will be installed: cmake cmake-data googletest libaec-dev libatlas3-base libblas-dev libbtf1 libceres-dev libceres1 libcgal-dev libcxsparse3 libflann-dev libflann1.9 libfreeimage-dev libfreeimage3 libgflags-dev libgflags2.2 libgmp-dev libgmpxx4ldbl libgoogle-glog-dev libgoogle-glog0v5 libgraphblas3 libgtest-dev libhdf5-mpi-dev libhdf5-openmpi-dev libjxr0 libklu1 liblapack-dev libldl2 liblz4-dev libmetis-dev libmongoose2 libmpfr-dev libqt5opengl5 libqt5opengl5-dev librbio2 librhash0 libspqr2 libsqlite3-dev libsuitesparse-dev ninja-build qt5-qmake qt5-qmake-bin qtbase5-dev qtbase5-dev-tools qtchooser 0 upgraded, 46 newly installed, 0 to remove and 47 not upgraded. Need to get 39.1 MB of archives. After this operation, 323 MB of additional disk space will be used. Do you want to continue? [Y/n] }}}\nCompile\nSame error pertain to libtiff as above.\n1 2 3 4 5 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/colmap/build$ conda list libtiff # packages in environment at /home/yi/anaconda3: # # Name Version Build Channel libtiff 4.5.1 h6a678d5_0 Uninstall libtiff\nHowever, conda uninstall libtiff hangs forever.\nDocker Image System Info Problems:\nApplications are not preserved through a system reinstallation, and backing up the bin directory is inefficient due to its size.\nCompiling COLMAP is cumbersome due to missing dependencies and configurations.\nDocker can create the correct environment for a (prebuilt) application, with all its dependencies properly configured r1-Gemini.\nColmap provides a pre-built Docker image with CUDA support r2-Docs\n::: aside\nReferences: {{{ Gemini 2.5P - Docker: Application Deployment Solution Installation — COLMAP 3.13.0.dev0 | a5332f46 (2025-07-05) documentation GitHub - colmap/docker/setup-ubuntu.sh }}} ::: Supports\nCheck Install Requirements for Docker Image Build\nlsb_release -a: Ubuntu 22.04.5 LTS\ndocker --version: Docker version 28.4.0, build d8eb465\n./setup-ubuntu.sh: Check host CUDA version to update Dockerfile\nneofetch\n{{{ 1 2 3 4 5 6 7 8 9 zichen@zichen-X570-AORUS-PRO-WIFI --------------------------------- OS: Ubuntu 22.04.5 LTS x86_64 Host: X570 AORUS PRO WIFI -CF Kernel: 6.8.0-83-generic Shell: bash 5.1.16 CPU: AMD Ryzen 7 5700X (16) @ 3.400GHz GPU: NVIDIA 09:00.0 NVIDIA Corporation Device 2203 Memory: 25734MiB / 128754MiB }}}\ngcc --version: gcc (Ubuntu 10.5.0-1ubuntu1~22.04.2) 10.5.0\nnvcc -V: NVIDIA (R) Cuda compiler driver, release 11.3, V11.3.58, cuda_11.3.r11.3/compiler.29745058_0\nnvidia-smi: Nvidia Driver Version 550.163.01\nThe UBUNTU_VERSION identified in the setup-ubuntu.sh is for the Docker image, not related to the host system.\nActions:\nDownload COLMAP source code\n1 2 3 zichen@zichen-X570-AORUS-PRO-WIFI:~/Programs$ git clone --depth=1 https://github.com/colmap/colmap cd colmap/docker Run setup-ubuntu.shr3-GitHub to get CUDA version number\n1 ./setup-ubuntu.sh That script performs: Update nvidia driver, Identify compatible CUDA version, Install nvidia-container-toolkit, Run a test container (I already have the package nvidia-container-toolkit installed.)\nNvidia driver upgraded to 580.65, and reboot is required.\nRun the script again after rebooting\nError:\n1 2 3 🔍 Finding latest patch version for CUDA 12.9... ❌ No CUDA 12.9 images found for Ubuntu 24.04, trying Ubuntu 22.04... ❌ No compatible CUDA images found The target image is not found due to the URL request only fetchs links on page=1, but the link to the target image (12.9.0-base-ubuntu22.04) is located on the page=2.\nModify Sh for Dynm Pages Problems:\nThe execution of ./setup-ubuntu.sh failed because no image was found for CUDA 12.9. ::: aside\nReferences: {{{ GPT5 - Explain command function Gemini 2.5P - Docker Hub Pagination for CUDA }}} ::: Supports:\nDocker images for old CUDA are superseded by new versions, making it impossible to retrieve a link to an image from the latest 100 images.\nSolution: Implement Dynamic Pagination Handling. r2-Gemini\njq extracts specified fields from JSON.\nActions:\nAdd a function to check all pages on Docker Hub\nUse the sleep command to implement a delay between requests within the loop to avoid hitting rate limits. Code Modification {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Function to search for a CUDA image tag, sending progress to stderr find_cuda_image_tag() { local cuda_ver=$1 local ubuntu_ver=$2 local page_num=1 local url=\u0026#34;https://registry.hub.docker.com/v2/repositories/nvidia/cuda/tags/?page=${page_num}\u0026amp;page_size=100\u0026#34; # This progress message now goes to stderr echo \u0026#34;⏳ Searching for CUDA ${cuda_ver} on Ubuntu ${ubuntu_ver}...\u0026#34; \u0026gt;\u0026amp;2 while [ -n \u0026#34;$url\u0026#34; ]; do # This progress message also goes to stderr echo \u0026#34; - Checking page ${page_num}...\u0026#34; \u0026gt;\u0026amp;2 response=$(curl -s \u0026#34;$url\u0026#34;) if ! echo \u0026#34;$response\u0026#34; | jq empty 2\u0026gt;/dev/null; then echo \u0026#34; - Warning: Could not retrieve or parse page ${page_num}.\u0026#34; \u0026gt;\u0026amp;2 break fi found_tag=$(echo \u0026#34;$response\u0026#34; | jq -r \u0026#39;.results[].name\u0026#39; | grep -E \u0026#34;^${cuda_ver}\\.[0-9]+-base-ubuntu${ubuntu_ver}$\u0026#34; | head -1) if [ -n \u0026#34;$found_tag\u0026#34; ]; then # This is the RESULT. It goes to stdout so it can be captured by the variable. echo \u0026#34;$found_tag\u0026#34; return 0 fi url=$(echo \u0026#34;$response\u0026#34; | jq -r \u0026#39;.next // empty\u0026#39;) page_num=$((page_num + 1)) if [ -n \u0026#34;$url\u0026#34; ]; then sleep 1 fi done echo \u0026#34; - Search complete. No matching tag found.\u0026#34; \u0026gt;\u0026amp;2 return 1 } echo \u0026#34;🔍 Finding latest patch version for CUDA $COMPATIBLE_CUDA...\u0026#34; # 1. Try to find a matching image for Ubuntu 24.04 by searching all pages AVAILABLE_VERSIONS=$(find_cuda_image_tag \u0026#34;$COMPATIBLE_CUDA\u0026#34; \u0026#34;24.04\u0026#34;) if [ -n \u0026#34;$AVAILABLE_VERSIONS\u0026#34; ]; then # Extract full version (e.g., \u0026#34;12.9.1\u0026#34; from \u0026#34;12.9.1-base-ubuntu24.04\u0026#34;) FULL_CUDA_VERSION=$(echo \u0026#34;$AVAILABLE_VERSIONS\u0026#34; | cut -d\u0026#39;-\u0026#39; -f1) UBUNTU_VERSION=\u0026#34;24.04\u0026#34; echo \u0026#34;✅ Found CUDA version: $FULL_CUDA_VERSION for Ubuntu $UBUNTU_VERSION\u0026#34; else echo \u0026#34;❌ No CUDA $COMPATIBLE_CUDA images found for Ubuntu 24.04, trying Ubuntu 22.04...\u0026#34; # 2. If not found, fall back to searching for an Ubuntu 22.04 image AVAILABLE_VERSIONS=$(find_cuda_image_tag \u0026#34;$COMPATIBLE_CUDA\u0026#34; \u0026#34;22.04\u0026#34;) if [ -n \u0026#34;$AVAILABLE_VERSIONS\u0026#34; ]; then FULL_CUDA_VERSION=$(echo \u0026#34;$AVAILABLE_VERSIONS\u0026#34; | cut -d\u0026#39;-\u0026#39; -f1) UBUNTU_VERSION=\u0026#34;22.04\u0026#34; echo \u0026#34;✅ Found CUDA version: $FULL_CUDA_VERSION for Ubuntu $UBUNTU_VERSION\u0026#34; else echo \u0026#34;❌ No compatible CUDA images found for CUDA $COMPATIBLE_CUDA on supported Ubuntu versions.\u0026#34; exit 1 fi fi }}}\nRun script ./setup-ubuntu-pages_1.sh\nTerminal output {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 📦 Configure Docker INFO[0000] Loading config from /etc/docker/daemon.json INFO[0000] Wrote updated config to /etc/docker/daemon.json INFO[0000] It is recommended that docker daemon be restarted. 🔍 Finding latest patch version for CUDA 12.9... ⏳ Searching for CUDA 12.9 on Ubuntu 24.04... - Checking page 1... - Checking page 2... ✅ Found CUDA version: 12.9.1 for Ubuntu 24.04 🧪 Testing with automatically detected compatible CUDA version: 12.9... Unable to find image \u0026#39;nvidia/cuda:12.9.1-base-ubuntu24.04\u0026#39; locally 12.9.1-base-ubuntu24.04: Pulling from nvidia/cuda 32f112e3802c: Already exists 644e9b203583: Pull complete 02559cd4bc8d: Pull complete 2cd52cbb1ebe: Pull complete 6e8af4fd0a07: Pull complete Digest: sha256:29e5e3425e2e0f5a4e97c9fb4695ba4887cd78210a43cf94c3bcafc6ab01c5e6 Status: Downloaded newer image for nvidia/cuda:12.9.1-base-ubuntu24.04 Sun Sep 28 05:19:19 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 580.65.06 Driver Version: 580.65.06 CUDA Version: 13.0 | +-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 Ti Off | 00000000:09:00.0 Off | Off | | 0% 27C P8 23W / 450W | 134MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ ✅ GPU support working with CUDA 12.9.1! ✅ Updated Dockerfile to use CUDA 12.9.1 }}}\nExecute Colmap Problems:\nRun colmap for reconstruction from multi-view images Supports:\nStart container from the folder containing input images\n1 ./run.sh /path/to/folder Colmap Used by 3DGS Problems:\n3DGS initializes the scene point cloud using COLMAP via convert.py r1-README\n1 colmap_command = \u0026#39;\u0026#34;{}\u0026#34;\u0026#39;.format(args.colmap_executable) if len(args.colmap_executable) \u0026gt; 0 else \u0026#34;colmap\u0026#34; ::: aside\nReferences: {{{ graphdeco-inria/gaussian-splatting - Processing your own Scenes }}} ::: Supports:\nIn my experiment: The input images for CasMVSNet_pl were also processed by COLMAP for comparison.\n┌ │ │ └ ─ I I ─ ─ n m ─ ─ p a ─ ─ u g ┬ │ └ ─ t e ─ ─ ─ s ─ ─ ─ ─ ─ ┐ ├ │ ┘ ─ ─ ─ ─ ─ ▷ ▷ ┌ │ │ └ ┌ │ └ ─ C ─ ─ C ─ ─ a ─ ─ O ─ ─ s ─ ─ L ─ ─ M ─ ─ M ─ ─ V ─ ─ A ─ ─ S ─ ─ P ─ ─ N ─ ─ ─ ─ e ─ ┐ ├ ┘ ─ t ─ ─ ─ _ ─ ─ ─ p ─ ─ ─ l ─ ─ ┐ ├ │ ┘ ─ ─ ─ ─ ─ ▷ ─ ┌ │ │ └ ─ ─ P ─ ─ ─ o ─ ─ ─ i ─ ─ ─ n ─ ─ ─ t △ │ ┘ ─ ─ ─ C ─ ─ l ─ ─ o ─ ─ u ─ ─ d ─ ─ ─ ┐ │ │ ┘ The input images were saved to /mnt/Seagate4T/04-Projects/CasMVSNet_pl-comments/results/dtu/image_ref/scan1/img\nSource: CasMVSNet_pl-comments/eval_refine.py, Line #353\n1 cv2.imwrite(f\u0026#39;results/{args.dataset_name}/image_ref/{scan}/img/{ref_vid:08d}.png\u0026#39;, image_ref[:,:,::-1]) Actions:\nUse the script CasMVSNet_pl-comments/test_scripts/mvDTU_3dgsColmap.sh to copy img folders to /mnt/Seagate4T/04-Projects/gaussian-splatting/ directory, then run convert.py for each scan to reconstruct a sparse point cloud using COLMAP.\nCOLMAP inputs:\n1 2 3 4 5 6 \u0026lt;location\u0026gt; |---input | |---\u0026lt;image 0\u0026gt; | |---\u0026lt;image 1\u0026gt; | |---... | Execute\n1 python convert.py -s \u0026#34;/mnt/Seagate4T/04-Projects/gaussian-splatting/DTU_testScans/scan1\u0026#34; Convert Sparse bin to Ply (2024-07-10)\nUse readColmapSceneInfo() function in 3DGS.\n--source_path=\u0026quot;DTU_scan1\u0026quot;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 gaussian-splatting | |---scene | |---temp.ipynb | |---DTU_scan1 |---images | |---\u0026lt;image 0\u0026gt; | |---\u0026lt;image 1\u0026gt; | |---... |---sparse |---0 |---cameras.bin |---images.bin |---points3D.bin Function call:\n1 2 3 4 # --- temp.ipynb --- from scene.dataset_readers import readColmapSceneInfo readColmapSceneInfo(\u0026#39;DTU_scan1\u0026#39;, \u0026#39;images\u0026#39;, False) Visualize Sparse Point Cloud (2024-07-10)\nThe sparse point cloud of DTU scan1 generated by colmap only has only 388 points. They are observable in Open3D if the camera pose is appropriate.\nThe translation vector (the 4th column) in the original extrinsics matrix from the DTU dataset mismatch the scale in Open3D. I guess Open3D has rescaled the translation vector according to the size of the input point cloud.\nThe original pose43 from DTU dataset won\u0026rsquo;t show any point of this sparse point cloud, although the dense point cloud restored from CasMVSNet can be displayed correctly:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;DTU_scan1/sparse/0/points3D.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) # pcd = o3d.io.read_point_cloud(\u0026#34;../CasMVSNet_pl-comments/results/dtu/points/scan001_l3.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) import numpy as np vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window(width=800, height=800) vis.add_geometry(pcd) vis.get_render_option().background_color = np.asarray([1,1,1]) w2c = np.array([[0.311619, -0.853452, 0.417749, -285.581], [0.0270351, 0.447425, 0.893913, -541.214], [-0.949823, -0.267266, 0.162499, 545.452], [0.0, 0.0, 0.0, 1.0]]) # cam 43 extrinsic view_ctl = vis.get_view_control() cam = view_ctl.convert_to_pinhole_camera_parameters() cam.extrinsic = w2c view_ctl.convert_from_pinhole_camera_parameters(cam, True) vis.run() vis.destroy_window() The viewpoint parameters saved by pressing: Ctrl+c cannot be reloaded with vis.get_render_option().load_from_json().\n(Search: \u0026ldquo;open3d display colmap point cloud\u0026rdquo; in Google ➔ Visualization — Open3D latest (664eff5) documentation)\nSave the copied json content into a file \u0026ldquo;o3d_render_scan1.json\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;class_name\u0026#34; : \u0026#34;ViewTrajectory\u0026#34;, \u0026#34;interval\u0026#34; : 29, \u0026#34;is_loop\u0026#34; : false, \u0026#34;trajectory\u0026#34; : [ { \u0026#34;boundingbox_max\u0026#34; : [ 2.0850396156311035, 8.7633571624755859, 31.446575164794922 ], \u0026#34;boundingbox_min\u0026#34; : [ -9.2711105346679688, -2.8082370758056641, 20.023061752319336 ], \u0026#34;field_of_view\u0026#34; : 60.0, \u0026#34;front\u0026#34; : [ 0.57468924964379176, 0.44402843879692444, -0.68743800584737935 ], \u0026#34;lookat\u0026#34; : [ -3.5930354595184326, 2.9775600433349609, 25.734818458557129 ], \u0026#34;up\u0026#34; : [ -0.33054624782933378, -0.64249516212987978, -0.69133142897285405 ], \u0026#34;zoom\u0026#34; : 0.69999999999999996 } ], \u0026#34;version_major\u0026#34; : 1, \u0026#34;version_minor\u0026#34; : 0 } This viewpoint should\u0026rsquo;t be loaded with get_render_option().load_from_json(), like:\n1 2 3 4 5 6 7 8 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;DTU_scan1/sparse/0/points3D.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.Visualizer() vis.create_window(width=800, height=800) vis.add_geometry(pcd) vis.get_render_option().load_from_json(\u0026#34;o3d_render_scan1.json\u0026#34;) vis.run() vis.destroy_window() Loading camera parameters with get_render_option() will pop an \u0026ldquo;unsupport\u0026rdquo; error:\n1 [Open3D WARNING] ViewTrajectory read JSON failed: unsupported json format. Because RenderOption is not ViewControl (camera parameters). (Directed by Google)\nI drag the visualizer to a similar viewpoint to the pose43, and save the camera parameter with write_pinhole_camera_parameters() as a json file.\n(Found in DDG ➔ How to load viewpoint in Visualization? #567)\n1 2 3 4 5 6 7 8 9 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;DTU_scan1/sparse/0/points3D.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.Visualizer() vis.create_window(width=800, height=800) vis.add_geometry(pcd) vis.run() # user changes the view and press \u0026#34;q\u0026#34; to terminate param = vis.get_view_control().convert_to_pinhole_camera_parameters() o3d.io.write_pinhole_camera_parameters(\u0026#34;o3d_render_scan1_save.json\u0026#34;, param) vis.destroy_window() The json file contains extrinsic and intrinsic. {{{ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 { \u0026#34;class_name\u0026#34; : \u0026#34;PinholeCameraParameters\u0026#34;, \u0026#34;extrinsic\u0026#34; : [ 0.65176050659153895, 0.35039808285127577, -0.67262874275612927, 0.0, -0.68560130823217202, 0.65140983980868639, -0.32498625624902339, 0.0, 0.3242824204268171, 0.67296835299631441, 0.66479659119730228, 0.0, -3.962131546638247, -17.999337566932006, -4.527720017212074, 1.0 ], \u0026#34;intrinsic\u0026#34; : { \u0026#34;height\u0026#34; : 800, \u0026#34;intrinsic_matrix\u0026#34; : [ 692.82032302755101, 0.0, 0.0, 0.0, 692.82032302755101, 0.0, 399.5, 399.5, 1.0 ], \u0026#34;width\u0026#34; : 800 }, \u0026#34;version_major\u0026#34; : 1, \u0026#34;version_minor\u0026#34; : 0 } }}}\nThen, the identical viewpoint can be recovered by loading this json file with read_pinhole_camera_parameters:\n1 2 3 4 5 6 7 8 9 10 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;DTU_scan1/sparse/0/points3D.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.Visualizer() vis.create_window(width=800, height=800) ctr = vis.get_view_control() param = o3d.io.read_pinhole_camera_parameters(\u0026#34;o3d_render_scan1_save.json\u0026#34;) vis.add_geometry(pcd) ctr.convert_from_pinhole_camera_parameters(param) vis.run() vis.destroy_window() I write the extrinsic in the above json to a w2c matrix:\n1 2 3 4 w2c = np.array([[0.65176050659153895, -0.68560130823217202, 0.3242824204268171, -3.962131546638247], [0.35039808285127577, 0.65140983980868639, 0.67296835299631441, -17.999337566932006], [-0.67262874275612927, -0.32498625624902339, 0.66479659119730228, -4.527720017212074], [0.0, 0.0, 0.0, 1.0]]) Using this w2c, the sparse point cloud can be displayed correctly.\nReplace the translation vector of pose43 with the above one, while keep the rotation 3x3 part unchanged:\n1 2 3 4 w2c = np.array([[0.311619, -0.853452, 0.417749, -3.96], [0.0270351, 0.447425, 0.893913, -17.999], [-0.949823, -0.267266, 0.162499, -4.52], [0.0, 0.0, 0.0, 1.0]]) # cam 43 With using this w2c, points are visible.\nPyCOLMAP Supports:\nColmap Documentation ","date":"2024-03-14T23:40:00Z","image":"https://colmap.github.io/_images/sparse.png","permalink":"http://blog.zichen.uk/post/writenotes/vis/pkg-colmap/","title":"Memo: Lib - COLMAP | Install and Usages"},{"content":"Pip Install References:\nRequirements File Format pip install - pip documentation Issues:\npip install Notes:\nextra_index_url argument\n1 pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 fr2-Docs\npip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # environment.yaml for conda name: CONDA_ENV_NAME channels: - pytorch - conda-forge - defaults dependencies: - python=3.8 - pip - matplotlib=3.4.3 - numpy - pip: - torch==1.9.0+cu111 - torchvision==0.10.0+cu111 - -f https://download.pytorch.org/whl/torch_stable.html git\n-U Upgrade specified packagesr2-Docs cache (2024-04-07)\npip uses incorrect cached package version, instead of the user-specified version - SO\n1 2 pip cache list # stored wheel files previously compiled pip install --no-cache-dir \u0026lt;package\u0026gt; # re-download to read (2024-05-21)\nPython 包管理工具 pip 详解 - muzing\nPoetry Problems:\n王树义\u0026rsquo;s project uses poetry installr1-Bilibili. What is poetry? References:\n如何安装使用我的 Web 界面 AI 工作流？ workflows_with_litellm_pub 设置运行详解 - Bilibili - 王树义老师 wshuyi/workflows_with_litellm_pub - GitHub [DeepSeek](https://chat.deepseek.com/a/chat/s/d42679bc-3281-40fa-9616-2f4c292ef5e3) Notes:\n(2025-04-03)\nThe similarities between poetry and condar3-DS. GUI tkinter Widgets\nReferences:\nPython GUI Programming: Your Tkinter Tutorial - Real Python Searched by python gui tkinter in DDG Gemini 2.5P - Python 文件间 Tkinter 按钮控制 Supports:\n(2025-07-31T22:21)\nWidgets have similar attributes: text, foreground, background, size r1-RealPython Events\nReferences:\nPython GUI Programming: Your Tkinter Tutorial - Real Python Gemini 2.5P - Python 文件间 Tkinter 按钮控制 Supports:\n(2025-08-01T07:10)\ncommand r1-RealPython (2025-08-01T17:10)\nNeed a separate stop signal for each function to achieve stop a function while starting another r2-Gemini Protocol\nReferences:\nGemini 2.5P - Execute Function Before Tkinter Window Close Supports:\n(2025-08-01T17:12)\nExecute functions before closing the window r2-Gemini pyinstaller pyinstaller\nReferences:\ndhruvansh-patel/Python-GUI-to-EXE-Builder - GitHub Searched by python gui build to exe in DDG ","date":"2024-03-11T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/python_packages/","title":"Memo: Lang - Python | Packages"},{"content":"Code | Arxiv\nNotes The editing feature is based on Embedded deformation for shape manipulation\nThe transformation of each Gaussian in the entire point cloud is an expaction of the transformations of the K nearest control points.\nPlay Environment (2024-03-11)\nUbuntu 20.04, cuda-11.6\n1 2 3 4 5 conda create -n SC-GS python=3.10 conda activate SC-GS pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 # I also change the torch version to cu116 in requirements.txt pip install -r requirements.txt If I directly run pip install -r requirements.txt, the following error about pip compiling occurs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 (SC-GS) yi@yi:~/Downloads/SC-GS-comments$ pip install -r requirements.txt ... Collecting git+https://github.com/facebookresearch/pytorch3d.git (from -r requirements.txt (line 14)) Cloning https://github.com/facebookresearch/pytorch3d.git to /tmp/pip-req-build-2ndb6zwl Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-2ndb6zwl Resolved https://github.com/facebookresearch/pytorch3d.git to commit 7566530669203769783c94024c25a39e1744e4ed Preparing metadata (setup.py) ... error error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─\u0026gt; [6 lines of output] Traceback (most recent call last): File \u0026#34;\u0026lt;string\u0026gt;\u0026#34;, line 2, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;pip-setuptools-caller\u0026gt;\u0026#34;, line 34, in \u0026lt;module\u0026gt; File \u0026#34;/tmp/pip-req-build-2ndb6zwl/setup.py\u0026#34;, line 15, in \u0026lt;module\u0026gt; import torch ModuleNotFoundError: No module named \u0026#39;torch\u0026#39; [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed × Encountered error while generating package metadata. ╰─\u0026gt; See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Refer issue 10 and issue 15\nCompilin Pillow requires: sudo apt-get install libjpeg-dev. Otherwise, error occus:\n1 2 3 4 5 6 7 8 9 10 11 The headers or library files could not be found for jpeg, a required dependency when compiling Pillow from source. Please see the install instructions at: https://pillow.readthedocs.io/en/latest/installation.html note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for Pillow Running setup.py clean for Pillow Failed to build Pillow ERROR: Could not build wheels for Pillow, which is required to install pyproject.toml-based projects PIL error: no attribute 'ANTIALIAS'\n1 2 3 File \u0026#34;/home/yi/anaconda3/envs/SC-GS/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py\u0026#34;, line 486, in make_image image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS) AttributeError: module \u0026#39;PIL.Image\u0026#39; has no attribute \u0026#39;ANTIALIAS\u0026#39; ChatGPT: The api has changed, the function should be called like:\nDidn't try 1 2 3 from PIL import Image, ImageFilter # ... image = image.resize((scaled_width, scaled_height), ImageFilter.ANTIALIAS) This error is solved by reinstalling the conda environment with python=3.8, pip will download the packages compatible (cp) with python 3.8. And Pillow cp38 still have the same api.\nTrain Train on 1050Ti (4GB)\n1 2 3 4 5 6 # Train with terminal only (for the resolution of 400*400 with best PSNR) CUDA_VISIBLE_DEVICES=0 python train_gui.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512 \\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 80000 iterations cost 1:47:24.\nDeformation Once the model has been trained, launch GUI with the output dir (just add --gui):\n1 2 3 4 5 CUDA_VISIBLE_DEVICES=0 python train_gui.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512\\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 --gui Unlike LBS, the model can be teared intio pieces.\nAnd the deformation may be anti-physical.\nAfter adjusting the poses, clik play to watch animation.\nTo debug the deformation code, once selected keypoints (A+Left Clik), Pause the debugger first, and then step by step inspect.\nPress D + Right-click drag will trigger callback_keypoint_drag\nClick Init Graph will tigger callback_animation_initialize to assign the attribute animate_tool by instantiating the class LapDeform\n1 dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Right, callback=callback_keypoint_drag) The command for eval only change train_gui.py to render.py:\n1 2 3 4 5 CUDA_VISIBLE_DEVICES=0 python render.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512 \\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 ","date":"2024-03-11T00:00:00Z","image":"https://arxiv.org/html/2312.14937v2/x2.png","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/b-note-sc-gs/","title":"Read: SC-GS"},{"content":"remap Docs - OpenCV: Remapping\nUse cv2.remap to do image warpping:\nHomography mapping determines the projection location on the source image for a reference image.\nr e ( ( ( f 0 1 2 , , , p 0 0 0 i ) ) ) x e ( ( ( l 0 1 2 , , , c 1 1 1 o ) ) ) o r ( ( ( d 0 2 2 s , , , 2 2 2 ) ) ) p o ( ( ( i x x x n y y y t z z z s ) ) ) w ( ( ( o x x x r y y y l z z z d ) ) ) c ( ( ( o x x x o y y y r z z z d ) ) ) s ( ( ( p x x x r , , , o y y y j ) ) ) s ( ( ( o x x x n , , , y y y s ) ) ) r c ( ( ( x x x i , , , m y y y g ) ) ) By fetching the pixels of the source image to the grid, according to the projection coordinates, a warpped source image is obtained.\nTherefore, remap requires the (x,y) \u0026ldquo;new arrange pattern\u0026rdquo; of the source image as input, along with the source image.\nExample in CasMVSNet-pl:\n1 2 3 4 5 6 7 8 9 xy_ref = np.mgrid[:img_wh[1],:img_wh[0]][::-1].astype(np.float32) xy_src = xy_ref2src(xy_ref, depth_ref, P_world2ref, depth_src, P_world2src, img_wh) # Sample the depth of xy_src using bilinear interpolation depth_src2ref = cv2.remap(depth_src, xy_src[0].astype(np.float32), xy_src[1].astype(np.float32), interpolation=cv2.INTER_LINEAR) resize (2024-03-14)\n1 2 3 4 5 6 7 8 9 10 img_ref = cv2.imread(os.path.join(root_dir, # channel is BGR f\u0026#39;Rectified/{scan}/rect_{vid+1:03d}_3_r5000.png\u0026#39;)) # scale to specified size img_ref = cv2.resize(img_ref, tuple(args.img_wh), interpolation=cv2.INTER_LINEAR)[:,:,::-1] # to RGB # scaling with factor, h and w both increase 4 times proba_ref = cv2.resize(proba_ref, None, fx=4, fy=4, interpolation=cv2.INTER_LINEAR) ","date":"2024-03-07T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/pkg-opencv/","title":"memo: OpenCV"},{"content":"Parallel programming model - Wikipedia\nReduction 归约 (2024-03-01)\nSource video: CUDA编程模型系列八(原子操作 / 规约 / 向量元素求和) - Ken He Code The parallism design depends on the operations to be performed. For example, given a task N-number summation, the operation executed on each thread in parallel is addition.\nThreads reduce by half every time.\nAs the \u0026ldquo;plus\u0026rdquo; operation computes 2 numbers, the data sequence is bisected.\n1 2 3 4 5 6 7 8 9 10 11 source[8]: 0 1 2 3 4 5 6 7 step 1: thread 0: source[0] + source[4] -\u0026gt; source[0] thread 1: source[1] + source[5] -\u0026gt; source[1] thread 2: source[2] + source[6] -\u0026gt; source[2] thread 3: source[3] + source[7] -\u0026gt; source[3] step 2: thread 0: source[0] + source[2] -\u0026gt; source[0] thread 1: source[1] + source[3] -\u0026gt; source[1] step 3: thread 0: source[0] + source[1] As shown above, the number of inital threads allocated is a half of the total data items. And in the following steps, the number of launched threads is a half of the number of threads used last time.\nSpecificaly, in the 1st round, 4 threads for 8 items, and the 2nd round only uses 2 threads for 4 results of the last step, and the final round only uses 1 thread.\n8 4 4 2 2 1 r e i t i t i t s t h t h t h u e r e r e r l m e m e m e t s a s a s a : : d : d : d s s : : : 0 0 0 0 1 1 1 2 2 3 3 4 5 6 7 If using CPU, there will be 7 plus operation, however, on GPU, there are only 3 steps.\nWhen the total number of operations is larger than the allocated threads, the Grid stride loop trick can be used.\nFor example, there are 32 operations need to be executed, but only 8 threads are allocated. Therefore, each thread has to be reused 4 times.\nBased on this fact, accumulate the 4 loops at first and then perform summation within a block (8 threads).\nOnly consider the behavior of one thread: what values will it use?\nFor a thread in a block, the sum of values assigned to it during 4 loops is computed as:\n3 A 8 r 2 c e c t s e h u l l r l e o e t : o d p : 0 0 0 l 1 1 o o 2 2 p ⋯ ⋯ 1 7 7 8 l 9 o o 1 p 0 2 ⋯ 1 5 1 6 l 1 o 7 o p 1 8 3 ⋯ 2 3 2 4 2 l 5 o o 2 p 6 4 ⋯ 3 1 In this way, multiple steps are compressed into a single block (8 threads).\nShared memory is very fast, so it can be used for those memory that is frequently accessed.\nAs the accumulated sums of 4 loops for each thread requires summation across the BLOCK_SIZE at the end, they can be stored in shared memory for later frequent reading.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // grid loop: accumulate loops first // allocate the same size as a block __shared__ int acc_tmp[BLOCK_SIZE]; int shared_tmp = 0; // Necessary to get correct result // Each thread adds the thread after n_thrd_cur for(int ele_id=blockDim.x * blockIdx.x + threadIdx.x; ele_id\u0026lt;num_items; ele_id+=blockDim.x*gridDim.x){ shared_tmp += d_in[ele_id]; } // __syncthreads(); // Sometimes lead to wrong result acc_tmp[threadIdx.x] = shared_tmp; // assign shared mem __syncthreads(); // Necessary Note:\nIf directly using the shared memory to do accumulation like: acc_tmp[threadIdx.x] += d_in[ele_id];, the result could be wrong because it\u0026rsquo;s in a loop, where mutliple thread may access the same memory at the same time, due to shared memory is accessible for all threads in a block.\nHowever, the local variable (shared_tmp) reside in register is private for a thread, and other threads can\u0026rsquo;t access it. So modifying the shared_tmp is safe and necessary.\nThe last __syncthreads(); cannot be put right after the for loop, and must be after shared memory assignment (on 1050Ti). Otherwise, the result could be wrong. (1080Ti is ok.)\nSo far, only the \u0026ldquo;block\u0026rdquo; of shared memory acc_tmp needs to compute the sum.\nThe threads reduction is performed through a for loop to adjust the number of threads step-by-step:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Sum numbers in the shared memory with size of BLOCK_SIZE // Threads reduce by half each step // Initial number of threads is a half of the total data for (n_thrd_cur=BLOCK_SIZE/2; n_thrd_cur\u0026gt;=1; n_thrd_cur/=2){ // Let a thread to do an operations: plus // Temporary variable is necessary for memory safety: int sum_tmp = 0; // Only use threads required if (threadIdx.x \u0026lt; n_thrd_cur){ sum_tmp = acc_tmp[threadIdx.x] + acc_tmp[threadIdx.x + n_thrd_cur]; } __syncthreads(); // Necessary // as write after read for the same memory // Can\u0026#39;t reside in if or other brach syntax // Write result back to memory if (threadIdx.x \u0026lt; n_thrd_cur){ acc_tmp[threadIdx.x] = sum_tmp; } __syncthreads(); // Necessary for 1050Ti, 1080Ti } Finally, the sum of a block (shared memory) is stored in acc_tmp[0].\n__syncthreads() is used when a memory is read followed by writing/modification to avoid data Hazard-wiki (Race condition-wiki, Memory safety-wiki)\n__syncthreads() can\u0026rsquo;t reside in if because it\u0026rsquo;s a branch. Otherwise, when multiple threads run in parallel, threads may go different branches, consequently, leading to errors.\natomicAdd guarantees the read/write to an address won\u0026rsquo;t be disrupted by other threads.\nWhen adding the summation of each block acc_tmp[0] (shared memory) upto d_out (global memory), multiple threads access the same global memory d_out, so atomicAdd is applied:\n1 2 3 4 5 6 7 8 // Accumulate all blocks in the grid // The sum of each block was stored in acc_tmp[0] // Each block uses 1 thread to add its sum to the total sum of all blocks if (blockIdx.x * blockDim.x \u0026lt; num_items){ if (threadIdx.x == 0){ atomicAdd(d_out, acc_tmp[0]); } } Full code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 #include \u0026lt;stdio.h\u0026gt; #define N 10000000 // number of data. No = and column ; #define BLOCK_SIZE 256 // blockDim.x #define GRID_SIZE 32 // gridDim.x, the total threads allocated: 8192 __managed__ int source[N]; // allocate memory for input data __managed__ int result_gpu[1] = {0}; // output the sum of N items // Remeber to declare types for args __global__ void sum_gpu(int* d_in, int num_items, int* d_out){ // grid loop __shared__ int acc_tmp[BLOCK_SIZE]; int shared_tmp = 0; for (int ele_id=blockIdx.x * blockDim.x + threadIdx.x; ele_id \u0026lt; num_items; ele_id+=BLOCK_SIZE*GRID_SIZE){ shared_tmp += d_in[ele_id]; } acc_tmp[threadIdx.x] = shared_tmp; __syncthreads(); // threads reduction int sum_tmp = 0; for (int thd_cur=BLOCK_SIZE/2; thd_cur\u0026gt;=1; thd_cur/=2){ if (threadIdx.x \u0026lt; thd_cur){ sum_tmp = acc_tmp[threadIdx.x] + acc_tmp[threadIdx.x + thd_cur]; } __syncthreads(); if (threadIdx.x \u0026lt; thd_cur){ acc_tmp[threadIdx.x] = sum_tmp; } __syncthreads(); } // accumulate all blocks if (blockIdx.x * blockDim.x \u0026lt; num_items){ if (threadIdx.x == 0){ // d_out[0] += acc_tmp[0]; atomicAdd(d_out, acc_tmp[0]); } } } int main(){ // Initialize source data // Can\u0026#39;t use: for(int\u0026amp; i :sourace) for (int i=0; i\u0026lt;N; i++) source[i] = rand()%10; // record time cudaEvent_t start, stop_gpu, stop_cpu; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop_gpu); cudaEventCreate(\u0026amp;stop_cpu); cudaEventRecord(start); cudaEventSynchronize(start); for (int i=0; i\u0026lt;20; i++){ // avg time for 20 rounds result_gpu[0] = 0; // clear value sum_gpu\u0026lt;\u0026lt;\u0026lt;GRID_SIZE, BLOCK_SIZE\u0026gt;\u0026gt;\u0026gt;(source, N, result_gpu); cudaDeviceSynchronize(); // wait gpu } cudaEventRecord(stop_gpu); cudaEventSynchronize(stop_gpu); // cpu execution int result_cpu = 0; for (int i=0; i\u0026lt;N; i++) result_cpu += source[i]; cudaEventRecord(stop_cpu); cudaEventSynchronize(stop_cpu); float time_gpu, time_cpu; cudaEventElapsedTime(\u0026amp;time_gpu, start, stop_gpu); cudaEventElapsedTime(\u0026amp;time_cpu, stop_gpu, stop_cpu); cudaEventDestroy(start); cudaEventDestroy(stop_gpu); cudaEventDestroy(stop_cpu); printf(\u0026#34;Time on gpu: %.2f, Time on cpu: %.2f\\n\u0026#34;, time_gpu/20, time_cpu); printf(\u0026#34;%s\\n\u0026#34;, (result_gpu[0] == result_cpu) ? \u0026#34;Equal\u0026#34; : \u0026#34;Error\u0026#34;); printf(\u0026#34;Sum on gpu: %d, Sum on cpu: %d\\n\u0026#34;, *result_gpu, result_cpu); return 0; } Output:\n1 2 3 4 5 (base) yi@yi:~/Downloads/CUDA_Study$ nvcc Tut_KenHe/8_reduction.cu (base) yi@yi:~/Downloads/CUDA_Study$ ./a.out Time on gpu: 1.15, Time on cpu: 26.45 Equal Sum on gpu: 45011704, Sum on cpu: 45011704 ","date":"2024-03-01T18:20:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cuda/tut_%E4%BD%95%E7%90%A8/","title":"watch: CUDA - 何琨 | CUDA Programming Model"},{"content":"Open3D (2024-05-13)\nJust found Open3D also can find nearest neighbors: 5-Step Guide to generate 3D meshes from point clouds with Python - Medium\n1 2 3 4 5 6 7 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/image_ref/scan1/points3d.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) distances = pcd.compute_nearest_neighbor_distance() avg_dist = np.mean(distances) radius = 3 * avg_dist CUDA Impl (2024-03-05)\nExample from simple-knn:\nUse thread block to divide the sorted points\nGiven 100K points, using only 1D grid and 1D block which length is 1024 (SO), the number of blocks is $\\frac{100K + 1024-1}{1024} = 97$\nHow many threads can a Nvidia GPU launch?, found by DDG Therefore, the kernel launch configuration parameters are \u0026lt;\u0026lt;\u0026lt;97, 1024\u0026gt;\u0026gt;\u0026gt;\nConvert 3D cartesian coordinates to Morton code\nMake up key-value pairs, where Morton code is key and points indices are values.\nSuch that, the points (indices) are sorted and similar points will gather together.\n1 2 3 4 5 6 7 8 thrust::device_vector\u0026lt;uint32_t\u0026gt; indices(P);\t// indices for points thrust::sequence(indices.begin(), indices.end());\t// indices: 0, 1, 2, 3. ... P thrust::device_vector\u0026lt;uint32_t\u0026gt; indices_sorted(P);\t// the sorted indices cub::DeviceRadixSort::SortPairs(nullptr, temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); temp_storage.resize(temp_storage_bytes); cub::DeviceRadixSort::SortPairs(temp_storage.data().get(), temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); Once the points are spread onto blocks, points within a block are similar to each other.\nFind the min and max coordinates combinations in each block\n1 boxMinMax \u0026lt;\u0026lt; \u0026lt;num_boxes, BOX_SIZE \u0026gt;\u0026gt; \u0026gt; (P, points, indices_sorted.data().get(), boxes.data().get()); Each thread has a point index. The indices indices_sorted of points are already sorted based on the Morton (z-order) code. So points within a block are similar to each other, and their distances are supposed to be small.\nG r a i i B n d l p d o o e c i x k n 1 t 0 0 2 m 4 i n t , h r m e a ⋯ a x d s B l o c k 1 1 0 2 m 4 i n t , h r m e a ⋯ a x d s ⋯ B l o m c i k n , 9 7 m a x The vector boxes records each block\u0026rsquo;s min and max coordinates:\nb 2 l x k y 0 z b 2 l x k y 1 z b 2 l x k y 2 z b 2 l x k y n z Based on the min and max coordinates, a point can quickly determine whether a block contains potential nearest neighbors.\nSpecifically, if the distance of a point to the min or max of a block is larger than reject, the block won\u0026rsquo;t be searched for possible nearest neighbors.\n1 2 3 4 5 for (int b = 0; b \u0026lt; (P + BOX_SIZE - 1) / BOX_SIZE; b++){ MinMax box = boxes[b]; float dist = distBoxPoint(box, point); if (dist \u0026gt; reject || dist \u0026gt; best[2]) continue; The reject value is the distance of the 3-rd nearest neighbor based on the initially sorted points sequence.\n1 2 3 4 5 6 7 8 9 int idx = cg::this_grid().thread_rank(); float3 point = points[indices[idx]]; float best[3] = { FLT_MAX, FLT_MAX, FLT_MAX }; for (int i = max(0, idx - 3); i \u0026lt;= min(P - 1, idx + 3); i++){ if (i == idx) continue; updateKBest\u0026lt;3\u0026gt;(point, points[indices[i]], best); } In this way, quiet amount of points are filtered out to save computation.\nIterate every points in the potential block\n1 2 3 4 5 6 7 8 9 for (int b = 0; b \u0026lt; (P + BOX_SIZE - 1) / BOX_SIZE; b++){ ... for (int i = b * BOX_SIZE; i \u0026lt; min(P, (b + 1) * BOX_SIZE); i++){ if (i == idx) continue; updateKBest\u0026lt;3\u0026gt;(point, points[indices[i]], best); } } dists[indices[idx]] = (best[0] + best[1] + best[2]) / 3.0f; Compute distance every two points:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 __device__ void updateKBest(const float3\u0026amp; ref, const float3\u0026amp; point, float* knn) { float3 d = { point.x - ref.x, point.y - ref.y, point.z - ref.z };\t// diff float dist = d.x * d.x + d.y * d.y + d.z * d.z;\t// sequare error for (int j = 0; j \u0026lt; K; j++)\t// K-nearest { if (knn[j] \u0026gt; dist)\t// closer than j { float t = knn[j];\t// tmp for previous j knn[j] = dist;\t// replace the previous j dist = t;\t// the previous j is compared with remaining neighbors } } } Test (2024-03-06)\nUbuntu 20.04, 1050Ti, cuda-11.6 (nvcc -V)\nClone: git clone https://gitlab.inria.fr/bkerbl/simple-knn.git\nEnvironment: conda env create -f environment.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 name: PCRefine channels: - pytorch - conda-forge - defaults dependencies: - python=3.10 - pip - pip: - torch==1.12.1+cu116 - torchvision==0.13.1+cu116 - --extra-index-url https://download.pytorch.org/whl/cu116 - ipykernel - plyfile Compile based on the setup.py: pip install .\nCall from python:\n1 2 3 4 5 6 7 import numpy as np import torch from simple_knn._C import distCUDA2 from utils import fetchPly pcd = fetchPly(\u0026#34;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/points3d.ply\u0026#34;) meanDist = distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()) Output the mean distance from neighbors to each point:\n1 2 (PCRefine) yi@yi:~/Downloads/simple-knn-comments$ python test_knn.py tensor([0.0013, 0.0020, 0.0008, ..., 0.0023, 0.0017, 0.0007], device=\u0026#39;cuda:0\u0026#39;) Indices (2024-03-08)\nModify the original code to return the neighbors\u0026rsquo; indices:\n1 2 3 4 5 6 7 8 (casmvsnet_pl) yi@yi:~/Downloads/CasMVSNet_pl-comments/submodules/simple-knn-comments$ python test_knn.py tensor([[41057, 23085, 16403], [21674, 59181, 31901], [18699, 99481, 15716], ..., [17352, 51604, 48154], [90929, 45350, 94932], [ 6797, 62182, 78410]], device=\u0026#39;cuda:0\u0026#39;, dtype=torch.int32) pytorch3d (2024-03-12)\nSet up environment to compile pytorch3d:\n1 2 3 4 5 conda create -n casmvsnet_pl python=3.8 pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 pip install git+https://github.com/facebookresearch/pytorch3d.git pip install -r requirements.txt Example from SC-GS\n1 nn_dist, nn_idxs, _ = pytorch3d.ops.knn_points(init_pcl[None], init_pcl[None], None, None, K=K+1) Use pytorch3d to find KNN in the lego point cloud:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np from plyfile import PlyData from typing import NamedTuple import torch import pytorch3d.ops class BasicPointCloud(NamedTuple): points : np.array colors : np.array normals : np.array # Code from: https://github.com/graphdeco-inria/gaussian-splatting def fetchPly(path): plydata = PlyData.read(path) vertices = plydata[\u0026#39;vertex\u0026#39;] # coordinates (x,y,z), normal (nx,ny,nz), r,g,b of 100K points positions = np.vstack([vertices[\u0026#39;x\u0026#39;], vertices[\u0026#39;y\u0026#39;], vertices[\u0026#39;z\u0026#39;]]).T # xyz: (3, 100K) -\u0026gt; (100K,3) colors = np.vstack([vertices[\u0026#39;red\u0026#39;], vertices[\u0026#39;green\u0026#39;], vertices[\u0026#39;blue\u0026#39;]]).T / 255.0 # rgb: (3, 100K) -\u0026gt; (100K, 3) normals = np.vstack([vertices[\u0026#39;nx\u0026#39;], vertices[\u0026#39;ny\u0026#39;], vertices[\u0026#39;nz\u0026#39;]]).T # normal xyz: (100K,3) return BasicPointCloud(points=positions, colors=colors, normals=normals) # NamedTuple pcd = fetchPly(\u0026#34;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/points3d.ply\u0026#34;) init_pcl = torch.from_numpy(np.asarray(pcd.points)).float().cuda() K=3 nn_dist, nn_idxs, _ = pytorch3d.ops.knn_points(init_pcl[None], init_pcl[None], None, None, K=K+1) # Both are (1,100k,4) print(nn_dist[0][:3]) print(nn_idxs[0][:3]) pytorch3d\u0026rsquo;s function can return nearest neighbors distance and indices at the same time. And the results are the same as the above cuda implementation:\n1 2 3 4 5 6 tensor([[0.0000, 0.0008, 0.0016, 0.0017], [0.0000, 0.0012, 0.0021, 0.0026], [0.0000, 0.0006, 0.0008, 0.0009]], device=\u0026#39;cuda:0\u0026#39;) tensor([[ 0, 41057, 23085, 16403], [ 1, 21674, 59181, 31901], [ 2, 18699, 99481, 15716]], device=\u0026#39;cuda:0\u0026#39;) I also tested the point cloud predicted by casmvsnet_pl, but this time the pytorch3d is so much slower than the cuda code (1h). Dont\u0026rsquo;t know why.\nResults are the same: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 scan1 contains 9.03 M points # cuda result nn indices: tensor([[2654648, 181848, 2654649], [1825888, 2654649, 182085], [1825889, 2025037, 182085], ..., [1788118, 5669911, 1996297], [2195795, 2814578, 1568896], [2814579, 2597015, 2195796]], device=\u0026#39;cuda:0\u0026#39;, dtype=torch.int32) # pytorch3d result nn indices: (1, 9030200, 4) tensor([[[ 0, 2654648, 181848, 2654649], [ 1, 1825888, 2654649, 182085], [ 2, 1825889, 2025037, 182085], ..., [9030197, 1788118, 5669911, 1996297], [9030198, 2195795, 2814578, 1568896], [9030199, 2814579, 2597015, 2195796]]], device=\u0026#39;cuda:0\u0026#39;) print(nn_dist) tensor([[[0.0000, 0.0137, 0.0167, 0.3456], [0.0000, 0.2516, 0.3126, 0.3652], [0.0000, 0.0262, 0.0991, 0.2446], ..., [0.0000, 0.0775, 0.0991, 0.1228], [0.0000, 0.0577, 0.0749, 0.0777], [0.0000, 0.0449, 0.1124, 0.1350]]], device=\u0026#39;cuda:0\u0026#39;) In simple-knn, the distance dist is a square error:\n1 2 float3 d = { point.x - ref.x, point.y - ref.y, point.z - ref.z }; float dist = d.x * d.x + d.y * d.y + d.z * d.z; ","date":"2024-02-29T11:50:00Z","permalink":"http://blog.zichen.uk/post/writenotes/algo/knn/","title":"memo: algo | KNN"},{"content":"(Feature image is from: CasMVSNet_pl)\nRestoring point cloud: Fuse depth maps of multi-views, and then unproject the depth map to 3D points.\nMVSNet-PyTorch Source code: MVSNet-PyTorch\nDownload pretrained model checkpoint\n1 gdown 1j2I_LNKb9JeCl6wdA7hh8z1WgVQZfLU9 -O ./checkpoints/pretrained/MVSNet-Pytorch_model_000014.ckpt Download preprocessed testing data of MVSNet\n1 2 gdown 135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ -O /data2/zichen/MVSNet_testing_dtu.zip unzip /data2/zichen/MVSNet_testing_dtu.zip -d /data2/zichen/MVSNet_testing Modify data paths in eval.sh, and uncomment the line#302: save_depth() in \u0026ldquo;eval.py\u0026rdquo; to generate depth map first. And then run the script: ./eval.sh.\nThis program requires 15GB VRAM. so 1080Ti can\u0026rsquo;t run it.\nThe resolution of the test images (3x1184x1600; Feat map: 32x296x400) is too high to be processed by 1080Ti with 11 GB VRAM.\nWhen processing the 2nd source view, OOM occurs at:\n1 2 3 warped_src_fea = F.grid_sample( src_fea, grid.view(batch, num_depth * height, width, 2), mode=\u0026#39;bilinear\u0026#39;, padding_mode=\u0026#39;zeros\u0026#39;) (2024-02-23) If executing the eval.py with the training images, the Dataset data_yao_eval may be mismatched with the training data folder.\nVisualization: \u0026ldquo;outputs/mvsnet001_l3.ply\u0026rdquo;\nThere are noise point around the round edge of the bowl.\nUse Matlab to compute the quantitative Metrics:\nThe DTU dataset (\u0026ldquo;DTU_SampleSet.zip\u0026rdquo;) provides the \u0026ldquo;Matlab evaluation code/\u0026rdquo; to assess the quality of point clouds.\nBased on the SampleSet directory structure:\n1 2 3 4 5 6 7 8 9 10 11 (base) yi@yi:~$ tree /mnt/data2_zichen/SampleSet/ -d -L 2 /mnt/data2_zichen/SampleSet/ ├── Matlab evaluation code │ └── MeshSupSamp_web └── MVS Data ├── Calibration ├── Cleaned ├── ObsMask # 3D parts used for evaluation ├── Points # ground truth point clouds for each scan ├── Rectified └── Surfaces # Poisson reconstruction To evaluate point clouds of all scans, the folder \u0026ldquo;Points\u0026rdquo; needs to be replaced with the full version \u0026ldquo;Points/\u0026rdquo;.\nCreate a symbolic link Points in SampleSet/MVS Data for the folder Points/:\n1 2 ln -s Points/ SampleSet/MVS\\ Data/Points # Or: ln -s Points/ SampleSet/MVS\\ Data/ Modify the paths and specify the scan to be evaluated in the matlab code:\n1 2 3 4 5 6 7 8 % ground-truth data: dataPath=\u0026#39;/mnt/data2_zichen/SampleSet/MVS Data\u0026#39;; % the dir storing testing .ply files: plyPath=\u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained\u0026#39;; % .mat fill will be saved here: resultsPath=\u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained\u0026#39;; - - - UsedSets=[1] Run BaseEvalMain_web in matlab:\nOutput 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt;\u0026gt;\u0026gt; BaseEvalMain_web cSet = 1 DataInName = \u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained/mvsnet001_l3.ply\u0026#39; EvalName = \u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrainedmvsnet_Eval_1.mat\u0026#39; /mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained/mvsnet001_l3.ply ans = 19 14 Elapsed time is 58.298360 seconds. downsample factor: 1.6901 Elapsed time is 18.930595 seconds. Computing Data 2 Stl distances Elapsed time is 83.304838 seconds. Computing Stl 2 Data distances Distances computed Elapsed time is 42.517009 seconds. Saving results Elapsed time is 42.864935 seconds. Elapsed time is 52.741538 seconds. ans = 19 18 mean/median Data (acc.) 0.254528/0.180807 mean/median Stl (comp.) 0.254594/0.218734 The samller the values, the better. CasMVSNet-pl (2024-02-23)\nSource code: CasMVSNet_pl - AIkui\nEnvironment Depth_raw:\nThe MVSNeRF inherited some codes of this repo, thus, in MVSNeRF\u0026rsquo;s training dataset, the directory \u0026ldquo;Depths/\u0026rdquo; under \u0026ldquo;mvs_training/dtu/\u0026rdquo; is also replaced with the folder \u0026ldquo;Depths/\u0026rdquo; unzipped from \u0026ldquo;Depth_raw.zip\u0026rdquo;\nCreate environment:\n(2024-04-04)\n1 2 3 4 conda create -n casmvsnet_pl python=3.8 conda activate casmvsnet_pl pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 pip install -r requirements.txt Python version: Environment casmvsnet_pl requires python 3.7 for pytorch 1.4.0, and the packages listed in requirements.txt. If using python 3.10, the available pytorch version is higher than 1.11. (2024-04-07)\nEnv on Ubuntu 22.04 ctk-11.6 in China:\n1 2 3 4 conda create -n casmvsnet_pl python=3.8 # pip always breaks when downloading torch. conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.6 -c pytorch -c conda-forge pip install -r requirements.txt # comment torch entries I cannot compile inplace-abn via pip with erros:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/utils/cpp_extension.py:820: UserWarning: There are no g++ version bounds defined for CUDA version 11.6 warnings.warn(f\u0026#39;There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}\u0026#39;) building \u0026#39;inplace_abn._backend\u0026#39; extension gcc -pthread -B /home/zichen/anaconda3/envs/casmvsnet_pl/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn.cpp -o build/temp.linux-x86_64-cpython-38/src/inplace_abn.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14 cc1plus: warning: command-line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ gcc -pthread -B /home/zichen/anaconda3/envs/casmvsnet_pl/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn_cpu.cpp -o build/temp.linux-x86_64-cpython-38/src/inplace_abn_cpu.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14 cc1plus: warning: command-line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ /usr/local/cuda-11.6/bin/nvcc -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn_cuda.cu -o build/temp.linux-x86_64-cpython-38/src/inplace_abn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -std=c++14 /home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/c10/core/SymInt.h(84): warning #68-D: integer conversion resulted in a change of sign /usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with ‘...’: 435 | function(_Functor\u0026amp;\u0026amp; __f) | ^ /usr/include/c++/11/bits/std_function.h:435:145: note: ‘_ArgTypes’ /usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’: 530 | operator=(_Functor\u0026amp;\u0026amp; __f) | ^ /usr/include/c++/11/bits/std_function.h:530:146: note: ‘_ArgTypes’ error: command \u0026#39;/usr/local/cuda-11.6/bin/nvcc\u0026#39; failed with exit code 1 Then, I tried to install it through compiling the source code. Unfortunately, the same error ocurred.\nI compiled the source code on Ubuntu 20.04 and ctk-11.6 (I had created a same conda env using the above 2 lines. When using the pip install manner, it will find the previous cache of the inplace-abn package on the machine.). Superisingly, the compilation suceeded.\nThe crucial difference between the 2 trials could be the gcc version gcc --version. Their return values are:\n1 2 3 gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 After I changed to gcc-9 on Ubuntu 22.04, the compilation for inplace-abn succeeded.\nI was reminded to check gcc due to ChatGPT\u0026rsquo;s response (prompted with the above error), and the issuse CUDA version (12.1) #232 and this issue asked gcc: failed install #143\nThere is another solution: inplace_abn安装报错？来看看这篇避坑指南吧！ But I didn\u0026rsquo;t try it.\nHow to change gcc version:\nRef: How to switch between multiple GCC and G++ compiler versions on Ubuntu 22.04 LTS Jammy Jellyfish\nThere are multiple gcc versions under /usr/bin/.\nBut, they didn\u0026rsquo;t show up:\n1 2 (casmvsnet_pl_) z@homepc:~/Downloads/inplace_abn$ update-alternatives --list gcc update-alternatives: error: no alternatives for gcc Install GCC 9: sudo apt-get install gcc-9 g++-9.\nIt\u0026rsquo;s said the 2 lines below are creating list for multiple gcc and g++:\n1 2 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 9 But once I exectuted them, the gcc --version has changed to gcc 9.5.0\nThe gcc and g++ will revert to 11.4 by executing:\n1 2 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 11 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 11 (2024-05-13)\nInstall gcc-7 I want to use gcc-7 to compile PCL. I installed g++-7 as below, and specify it as default:\n1 2 3 sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update \u0026amp;\u0026amp; apt install g++-7 -y sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 60 --slave /usr/bin/g++ g++ /usr/bin/g++-7 However, an error prompted when install g++-9, because it can't be master\n1 2 3 (base) yi@Alien:~$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 (base) yi@Alien:~$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 9 update-alternatives: error: alternative g++ can\u0026#39;t be master: it is a slave of gcc So, I use a one-line command similar to the above gcc-7:\n1 (base) yi@Alien:~$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 --slave /usr/bin/g++ g++ /usr/bin/g++-9 (Ref: Update gcc alternatives in Ubuntu 18.04 - g++ cannot be slave of gcc)\nLater, the versions can be chosen from a list:\nsudo update-alternatives --config gcc\nNot found GPU:\nAlthough the Pytorch 1.4.0 (installed with pip) was compiled with cuda 10.1 (download), the cudatoolkit (nvcc -V) on the PATH is not neccessary to match the cuda version.\nFor example, with using the same conda environemnt, Alien-PC (Ubuntu 20.04) using cuda-11.6 and the lambda server (Ubuntu 18.04) using cuda-10.2 both can detect GPU and run.\nThe error: torch.cuda.is_available() returns Falsehere is not due to cuda, but my debug settings!!! Fuck!! the environment variable CUDA_VISIBLE_DEVICES in launch.json was set to 6 for server, which should be 0 on my PC.\n(2024-04-13)\nCreate an environment again on Lambda server: Ubuntu 18.04, cudatoolkit 11.6 (nvcc -V), and gcc, g++ both are 7.5.0 (gcc -v)\n1 2 3 4 5 6 7 8 9 10 (casmvsnet_pl_py38) z@lambda-server:~/Downloads/CasMVSNet_pl-comments$ g++ -v Using built-in specs. COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper OFFLOAD_TARGET_NAMES=nvptx-none OFFLOAD_TARGET_DEFAULT=1 Target: x86_64-linux-gnu Configured with: ../src/configure -v --with-pkgversion=\u0026#39;Ubuntu 7.5.0-3ubuntu1~18.04\u0026#39; --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu Thread model: posix gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04) The pytorch is not install separately, just using the requirements.txt as below. Git commit\n1 2 3 conda create -n casmvsnet_pl_py38 python=3.8 conda activate casmvsnet_pl_py38 pip install -r requirements.txt Evaluation Evaluation and Depth Map Fusion: evalutions/\nDownload pretrained model checkpoint\n1 2 3 4 mkdir -p ckpts wget https://github.com/kwea123/CasMVSNet_pl/releases/download/v1.0/exp2.zip -P ckpts/ unzip ckpts/exp2.zip -d ckpts/ wget https://github.com/kwea123/CasMVSNet_pl/releases/download/v1.0/_ckpt_epoch_10.ckpt -P ckpts/exp2 Specify testing data\nI only left scan9 in \u0026ldquo;lists/test.txt\u0026rdquo; for a quick peek.\nThe code has some path templates that don\u0026rsquo;t match the DTU testing data directory structure.\nCode Modifications File: datasets/dtu.py, build_metas() (line #40-42):\n1 2 3 pair_file = \u0026#34;pair.txt\u0026#34; # Make 49 * 22 testing pairs for scan in self.scans: # add `scan` into path with open(os.path.join(self.root_dir, scan, pair_file)) as f: File: datasets/dtu.py, build_proj_mats (Line #58-59):\n1 2 3 # all scans have the same cams, so hardcard \u0026#39;scan1\u0026#39; here: proj_mat_filename = os.path.join(self.root_dir, \u0026#39;scan1\u0026#39;, f\u0026#39;cams/{vid:08d}_cam.txt\u0026#39;) File: datasets/dtu.py, __getitem__() (Line #165-166):\n1 2 3 # Add `scan` into path: img_filename = os.path.join(self.root_dir, scan, f\u0026#39;images/{vid:08d}.jpg\u0026#39;) File: eval.py, read_image() (line #87):\n1 2 if dataset_name == \u0026#39;dtu\u0026#39;: return cv2.imread(os.path.join(root_dir, scan, f\u0026#39;images/{vid:08d}.jpg\u0026#39;)) File: eval.py, (Line #352): set name format to align the matlab code (of the MVSNet_pytorch).\n1 PlyData([el]).write(f\u0026#39;{point_dir}/scan{int(scan[4:]):03d}_l3.ply\u0026#39;) Execute eval.py. (1080Ti is okay.)\n1 CUDA_VISIBLE_DEVICES=5 python eval.py --root_dir /data2/zichen/MVSNet_testing/dtu Visualization:\nImport the result point cloud \u0026ldquo;results/dtu/points/scan9.ply\u0026rdquo; into MeshLab:\ncasmvsnet mvsnet Some floaters will be exposed when looking at the point cloud from novel views.\nCasMVSNet-pl has less noise points than MVSNet-pytorch.\nModify the paths in \u0026ldquo;BaseEvalMain_web.m\u0026rdquo; (of the MVSNet_pytorch):\n1 2 3 4 5 6 dataPath=\u0026#39;/mnt/data2_zichen/SampleSet/MVS Data\u0026#39;; % GT plyPath=\u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points\u0026#39;; % pred resultsPath=\u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results\u0026#39;; % store .mat method_string=\u0026#39;scan\u0026#39;; - - - UsedSets=[9] Output for scan9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cSet = 9 DataInName = \u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points/scan009_l3.ply\u0026#39; EvalName = \u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/resultsscan_Eval_9.mat\u0026#39; /mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points/scan009_l3.ply ans = 0 35 Elapsed time is 412.622045 seconds. downsample factor: 5.5621 Elapsed time is 285.285496 seconds. Computing Data 2 Stl distances Elapsed time is 101.764399 seconds. Computing Stl 2 Data distances Distances computed Elapsed time is 106.225475 seconds. Saving results Elapsed time is 106.982512 seconds. Elapsed time is 126.089258 seconds. ans = 0 52 mean/median Data (acc.) 0.357586/0.229884 mean/median Stl (comp.) 0.304593/0.177679 Evaluate point clouds of all testing scans with matlab and save outputs into write.txt for calculating the average performance:\n1 2 3 4 5 6 7 8 9 10 11 12 import re import numpy as np means_acc = [] means_comp = [] with open(\u0026#34;/mnt/Server/Downloads/MVSNet_pytorch-comments/evaluations/dtu/write.txt\u0026#34;, \u0026#34;r\u0026#34;) as file: for line in file.readlines(): if \u0026#39;acc.\u0026#39; in line: means_acc.append( eval(re.findall(\u0026#39;\\d+\\.\\d+|\\d+\u0026#39;,line)[0]) ) elif \u0026#39;comp.\u0026#39; in line: means_comp.append( eval(re.findall(\u0026#39;\\d+\\.\\d+|\\d+\u0026#39;,line)[0]) ) print(np.mean(means_acc), np.mean(means_comp)) Output: 0.36399536, 0.36997940\nResults of every scan. scans accuracy Completeness num pts 1 0.235012/0.177113 0.219228/0.176128 29.40M 4 0.267118/0.198705 0.376820/0.192314 23.06M 9 0.312725/0.196551 0.204539/0.169905 25.38M 10 23.63M 11 19.57M 12 21.47M 13 22.35M 15 26.06M 23 29.97M 24 24.95M 29 19.67M 32 22.06M 33 16.67M 34 28.18M 48 15.82M 49 19.90M 62 22.97M 75 18.60M 77 7.46 M 110 27.34M 114 0.223317/0.169992 0.249666/0.178679 31.52M 118 0.243320/0.183991 0.403758/0.195397 30.18M avg 22 (2024-04-03)\nVisualization for scan1:\nThe aliasing: Moiré pattern (摩尔纹) resulted from insufficient sampling frequency is wave interference (波的干涉).\nWave interference is a characteristic of wave behaviors: when two waves encounters, they form a new wave?\nEval LLFF Data (2024-04-15)\nConvert LLFF dataset to the format of MVSNet.\nUse Colmap to solve the LLFF images: Colmap Tutorial; Ytb (The camera poses in NeRF are also recovered by Colmap with imgs2poses), and then use the script colmap2mvsnet.py provided by MVSNet to obtain matched format.\nEavl T\u0026amp;T (2024-05-25)\nDataset: Tanks and Temples\nThe F-score of the testing sets (without GT published) should be evaluated by submitting the point cloud (.ply) to their webpage. (Github)\nFrom the leaderboard, the results of MVSNet and casmvsnet_pl can be found there.\n3DGS shows the PSNR, rather than F-score.\nThe training sets have GT provided.\nMVSNet Sec5.2 evaluates its f-score on the intermediate set (Family, Francis, Horse, Lighthouse, M60, Panther, Playground, Train) of T\u0026amp;T.\nThe intermediate.zip only contains .mp4 videos.\nN = 5, W = 1920, H = 1056 and D = 256\nMVSNet 418\t43.48\tN.A.\t55.99\t28.55\t25.07\t50.79\t53.96\t50.86\t47.9\t34.69\nCasMVSNet_pl is tested on T\u0026amp;T with the default parameter in eval.py.\nCasMVSNet_pl 296.25\t55.09\tN.A.\t76.4\t52.83\t49.08\t49.72\t56.24\t51.99\t53.87\t50.63 Point-MVSNet 390.50 48.27 N.A. 61.79 41.15 34.20 50.79 51.97 50.85 52.38 43.06 didn\u0026rsquo;t exhibit the T\u0026amp;T results in their paper.\nDepth map fusion (24/07/18) This paragraph has been revised in my thesis for fluency and including references.\nCode: CasMVSNet_pl\n(2024-03-06) (2024-07-08)\nAll 49 depth maps are used to produce the point cloud for a scan by default. The filtering is performed based on 10 neighboring source depth maps.\nThe photometric consistency removes the pixel whose value is lower than 0.8 on the probability map. The geometric consistency can be depicted as follows. The reference depth map is projected onto each source viewpoint, obtaining the corresponding coordinates on the source depth map. Equivalently, the source depth map is warped to the reference viewpoint. Then, the warped source depth map is projected back to the reference depth map. If the reprojected coordinates on the reference depth map are 1 pixel away from the original coordinates, and the difference between the depth at the reprojected and the original pixel coordinates on the reference depth map is smaller than 0.01 * original depth. The point associated with a pixel on the reference view is considered valid. If a pixel is finally marked as valid more than 3 times, it will be retained on the reference depth map.\nEach reference depth map applies the above photometric and geometric consistencies given a source depth map, resulting in a masked reference depth map. All the masked reference depth maps are aggregated together and the average masked depth map is unprojected to form a point cloud for this reference view.\nAs a scan has a total of 49 images, the final point cloud is a combination of the 49 point clouds produced from each of these images.\n\\begin{algorithm} \\caption{Depth map filter and fusion} \\begin{algorithmic} \\FOR{view=1 \\TO 49 (\\texttt{args.max\\_ref\\_views})} \\STATE Read reference image to compute the average RGB \\STATE \\COMMENT{Photometric consistency} \\STATE Read probability map for the reference image \\IF {Pixel prob $\u003e$ 0.8} \\STATE mask\\_conf = 1 \\ENDIF \\STATE \\COMMENT{Geometric consistency} \\FOR{src=1 \\TO 10} \\STATE Warp src depth map to be seen from the ref view by \\texttt{xy\\_ref2src} \\STATE Reproject the warped dMap\\_src onto dMap\\_ref by \\texttt{xy\\_src2ref} \\IF {Pixel diff $\u003c$ 1 pix \\AND depth error $\u003c$ 0.01} \\STATE mask\\_geo = 1 \\ENDIF \\ENDFOR \\STATE Average ref depth maps and images reprojected from src views \\STATE Unproject the mean ref depth map to point cloud in world space \\ENDFOR \\STATE Output 49 point clouds \\end{algorithmic} \\end{algorithm} Procedure description refer to paper MVSNet sec 4.2 (2024-05-16)\nAs long the predicted depth is accurate, warping the source view based on the homography will result in the same scale as the ref view.\nTherefore, the same pixel coordinate on the warped src image and the ref image correspond to a same 3D point. In other words, the warped src view and the ref view can overlap.\nWarping is done by sampling pixels from the source depth map according to the homography from the ref view to a src view.\nThe fused depth map of a ref view is the average of refined depth maps after applying the photometric and geometric constraints on each pair of the ref and a src view\nApply the final mask on the depth_est_averaged, then unproject pixels on dpeth map to world space.\nBecause there are 49 views for a scan, the output .ply file is a combination of 49 point clouds.\nCasMVSNet depth map fusion process: Plotting code snippet 1 2 3 4 5 6 7 import numpy as np import matplotlib.pyplot as plt fig, axes = plt.subplots(2,3,figsize=(9,5),dpi=200) plt.tight_layout() image_ref = np.load(\u0026#34;./image_ref.npy\u0026#34;) axes[0][0].imshow(image_ref) axes[0][0].set_title(\u0026#34;image_ref: scan_1 view0\u0026#34;) With substituing the predicted depth into homography, the warped source image aligns with the reference image pretty much.\nSeen from the valid-depth mask, some highlight spots are missed.\nCasMVSNet-pl averaged the color of 10 source views that have performed homography transformation. (desc)\n1 2 image_refined_ = \\ np.sum(image_src2refs, 0)/np.expand_dims((mask_geo_sum+1), -1) Whereas, MVSNet-pytorch only casts the color of the refernce view.\n1 color = ref_img[1:-16:4, 1::4, :][valid_points] # hardcoded for DTU dataset ","date":"2024-02-21T09:00:00Z","image":"https://github.com/kwea123/CasMVSNet_pl/raw/master/assets/demo.png","permalink":"http://blog.zichen.uk/post/writenotes/model/depth/b-test-mvs_ply/","title":"Test: Points - MVSNets | Restore Point Cloud from Depth Map"},{"content":"Create Matrix mat3 (2024-02-01)\nType: typedef mat3x3 mat3 API - GLM: Types\nPassing a single scalar will initialize the diagonal. OpenGL Mathematics (GLM)\nOpenGL矩阵运算——GLM库的使用 - CSDN\nExample: 3DGS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;glm/glm.hpp\u0026gt; #include\u0026lt;glm/gtx/string_cast.hpp\u0026gt; #include \u0026lt;iostream\u0026gt; int main(){ glm::mat3 S = glm::mat3(1.0f); // identity matrix // Make scaling matrix S[0][0] = 2; S[1][1] = 3; S[2][2] = 4; std::cout \u0026lt;\u0026lt; glm::to_string(S) \u0026lt;\u0026lt; std::endl; return 0; } Output 1 2 3 (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ g++ test_glm.cpp (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ ./a.out mat3x3((2.000000, 0.000000, 0.000000), (0.000000, 3.000000, 0.000000), (0.000000, 0.000000, 4.000000)) Refer to How do I print vector values of type glm::vec3 that have been passed by reference? - SO\nUse glm::to_string(). Episode 18 - OpenGL Math - Introduction to the GLM Library - Modern OpenGL - Mike Shah\n","date":"2024-02-01T14:50:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/glm/","title":"memo: glm | Basics"},{"content":"Introducing CUDA UnBound (CUB) - Microway\nInclusiveSum Docs\n(2024-01-27)\n1 2 d_in: [8 , 6 , 7 , 5 , 3 , 0 , 9 ] d_out: [8 , 14 , 21 , 26 , 29 , 29 , 38 ] Example from 3DGS\n1 2 3 4 5 6 cub::DeviceScan::InclusiveSum( nullptr, // No memory is allocated geom.scan_size, // num of bytes needed to reserve geom.tiles_touched, // input sequence geom.tiles_touched, // becomes prefix sums P) // number of items in the sequence, 100K points Given a block of P bytes of memory pointed to by geom.tiles_touched, obtain geom.scan_size indicating the number of bytes for temporay storage required to reserve for executing the function: SortPairs Docs\n(2024-02-03)\nSort key-value pairs according to keys\nExample in diff-rast-gass\n1 cub::DeviceRadixSort::SortPairs (2024-03-05)\nExample in simple-KNN of 3DGS\nClustering points (indices) based on morton codes morton:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Determine the number of bytes for temp_storage cub::DeviceRadixSort::SortPairs( nullptr, temp_storage_bytes, morton.data().get(), // in: keys morton_sorted.data().get(), // out: sorted keys indices.data().get(), // in: values indices_sorted.data().get(), // out: sorted values P); // Allocate memory for temp_storage temp_storage.resize(temp_storage_bytes); // Sort the pairs based on keys cub::DeviceRadixSort::SortPairs( temp_storage.data().get(), temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); Reduce min (2024-02-28)\nTo execute this function, temp_storage_bytes is required to be reserved. Therefore, this function needs to run twice:\nGet the number of required temporary bytes, with specifying d_temp_storage as nullptr;\nExecute the function again given the correct address d_temp_storage\nA simplest code: \u0026ldquo;cub_reduce.cu\u0026rdquo;\nFull code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include \u0026lt;cub/cub.cuh\u0026gt; // CustomMin functor struct CustomMin { template \u0026lt;typename T\u0026gt; __device__ __forceinline__ T operator()(const T \u0026amp;a, const T \u0026amp;b) const { return (b \u0026lt; a) ? b : a; } }; // Declare, allocate, and initialize device-accessible pointers for // input and output int N = 7; // num of items int d_in[7] = {8, 6, 7, 5, 3, 1, 9}; int d_out[1] = {0}; // the min value CustomMin min_op; int init = INT_MAX; // To be compared initially int main(){ int *in, *out; cudaMalloc(\u0026amp;in, sizeof(int)*N); cudaMalloc(\u0026amp;out, sizeof(int)); cudaMemcpy(in, d_in, sizeof(int)*N, cudaMemcpyHostToDevice); // Determine temporary device storage requirements void *d_temp_storage = NULL; size_t temp_storage_bytes = 0; cub::DeviceReduce::Reduce( d_temp_storage, temp_storage_bytes, in, out, N, min_op, init); cudaDeviceSynchronize(); // Allocate temporary storage cudaMalloc(\u0026amp;d_temp_storage, temp_storage_bytes); // Run reduction cub::DeviceReduce::Reduce( d_temp_storage, temp_storage_bytes, in, out, N, min_op, init); cudaDeviceSynchronize(); int min; cudaMemcpy(\u0026amp;min, out, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;The min is: %d\\n\u0026#34;, min); // 1 // Free allocated memory cudaFree(d_temp_storage); return 0; } Notes:\nIf the suffix is .cpp and when compiling with nvcc cub_reduce.cpp, there will be errors about Debug:\n1 2 3 4 5 6 /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/block/specializations/../../block/../util_debug.cuh:101:43: error: ‘Debug’ is not a member of ‘cub’ 101 | #define CubDebug(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__) | ^~~~~ /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/util_allocator.cuh:695:17: note: in expansion of macro ‘CubDebug’ 695 | if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error; | ^~~~~~~~ __host__ __forceline function cannot be called, need __device__\n1 2 3 4 5 6 /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/device/dispatch/dispatch_reduce.cuh(145): error: calling a __host__ function(\u0026#34;T1 CustomMin::operator ()\u0026lt;int\u0026gt; (const T1 \u0026amp;, const T1 \u0026amp;) const\u0026#34;) from a __global__ function( \u0026#34;cub::DeviceReduceSingleTileKernel\u0026lt; ::cub::DeviceReducePolicy\u0026lt;int, int, int, ::CustomMin\u0026gt; ::Policy600, int *, int *, int, ::CustomMin, int\u0026gt; \u0026#34;) is not allowed /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/device/dispatch/dispatch_reduce.cuh(145): error: identifier \u0026#34;CustomMin::operator ()\u0026lt;int\u0026gt; const\u0026#34; is undefined in device code The customize min_op (or CustomMin()) can be changed to cub::Sum() to solve the sum of input data.\nRef:\ncub::DeviceReduce — CUB 104.0 documentation\nCUDA高性能计算经典问题（一）—— 归约（Reduction） - Will Zhang的文章 - 知乎\nCUDA编程模型系列八(原子操作 / 规约 / 向量元素求和) - 扫地的小何尚 (何琨) Found when searching \u0026ldquo;cuda cub 例子\u0026rdquo; in DDG\nsum (2024-03-04)\nMove the input data onto GPU first! Otherwise, it won\u0026rsquo;t be processed and the output variable won\u0026rsquo;t update and remains the initialized value, e.g., 0.\nMove the output data back to host for printing! Otherwise, the gpu memory is not allowed to access and return segment fault.\nFull code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #include \u0026lt;cub/cub.cuh\u0026gt; int N=7; // num of items int d_in[7] = {8, 6, 7, 5, 3, 0, 9}; int d_out[1]={0}; int main(){ int *in, *out; cudaMalloc(\u0026amp;in, sizeof(int)*N); // allocate memory cudaMalloc(\u0026amp;out, sizeof(int)); cudaMemcpy(in, d_in, sizeof(int)*N, cudaMemcpyHostToDevice); // Determine temporary device storage requirements void *d_temp_storage = NULL; size_t temp_storage_bytes = 0; cub::DeviceReduce::Sum( d_temp_storage, temp_storage_bytes, in, out, N); // Allocate temporary storage cudaMalloc(\u0026amp;d_temp_storage, temp_storage_bytes); // Run sum-reduction cub::DeviceReduce::Sum( d_temp_storage, temp_storage_bytes, in, out, N); int sum=0; cudaMemcpy(\u0026amp;sum, out, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;%d\\n\u0026#34;, sum); } Ref:\nDocs-Snippest\nExample Code Reference by this SO question: Sum reduction with CUB surfaced by DDG with searching: \u0026ldquo;cub::DeviceReduce::Reduce example tutorial\u0026rdquo;\n","date":"2024-01-27T15:35:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cuda/cub/","title":"Memo: Lang - CUDA | CUB"},{"content":"White background Code from 3DGS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pathlib import Path from PIL import Image image_path = Path(path) / cam_name # \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/./train/r_0.png\u0026#39; image_name = Path(cam_name).stem # r_0 image = Image.open(image_path) # \u0026lt;class \u0026#39;PIL.PngImagePlugin.PngImageFile\u0026#39;\u0026gt; im_data = np.array(image.convert(\u0026#34;RGBA\u0026#34;)) # (800,800,4) bg = np.array([1,1,1]) if white_background else np.array([0, 0, 0]) # Normalize to [0,1] norm_data = im_data / 255.0 # Re-composite img_arr = norm_data[...,:3] * norm_data[...,-1:] + bg * (1-norm_data[...,-1:]) # Convert to image img_rgb = Image.fromarray(np.array(img_arr * 255.0, dtype=np.byte), \u0026#39;RGB\u0026#39;) Scaling image Zooming image requires changing the focal lengths together, while cropping image doesn\u0026rsquo;t need.\nDownsize h and w, the focal also downscales, see NeRF:\n1 2 3 4 5 6 factor 4 # shrink raw image to 1/4 args = \u0026#39; \u0026#39;.join([\u0026#39;mogrify\u0026#39;, \u0026#39;-resize\u0026#39;, f\u0026#39;{100./factor}%\u0026#39;, \u0026#39;-format\u0026#39;, \u0026#39;png\u0026#39;, \u0026#39;*.{}\u0026#39;.format(ext)]) ... # the 5th column is hwf poses[:2, 4, :] = np.array(sh[:2]).reshape([2, 1]) # hw poses[2, 4, :] = poses[2, 4, :] * 1./factor # focal A b l e t f o s e e 4 b l k s . C l o s e r : o ½ n f l y 1 t i l e f i l l s i n e y e s Example: CasMVSNet has 3 levels of feature maps, so the first 2 rows of the camera intrinsics are scaled up along with the image size increases:\n1 2 for l in reversed(range(self.levels)): intrinsics[:2] *= 2 # 1/4-\u0026gt;1/2-\u0026gt;1 Crop a patch doesn\u0026rsquo;t affect focals referring to GNT\nToTensor 1 from torchvision import transforms fov Code from 3DGS\ni m g f p l a n e l e f t z n _ e n a e r a r p l a n e r i g h t Field of view: fovX = $2* arctan(\\frac{width}{2f})$ Near plane\u0026rsquo;s right boundary: $z_{near} * tan(fovX)$ Convert fov to focal\n1 2 def fov2focal(fovX, width): # 1111.11103, 800 return width / (2 * math.tan(fov / 2)) Near plane computed from fov\n1 2 3 4 5 6 7 tanHalfFovY = math.tan((fovY / 2)) tanHalfFovX = math.tan((fovX / 2)) top = tanHalfFovY * znear bottom = -top right = tanHalfFovX * znear left = -right Pixel Coords (2024-03-14)\nnp.mgrid. Example from casmvsnet_pl\n1 2 3 xy_ref = np.mgrid[:args.img_wh[1],:args.img_wh[0]][::-1] # (2, args.img_h, args.img_w) # restore depth for (x,y): xyz_ref = np.vstack((xy_ref, np.ones_like(xy_ref[:1]))) * depth_refined[ref_vid] # (3:xyz, h,w) np.meshgrid. Example form MVSNet_pytorch\n1 2 3 4 5 xx, yy = np.meshgrid(np.arange(0, width), np.arange(0, height)) print(\u0026#34;yy\u0026#34;, yy.max(), yy.min()) yy = yy.reshape([-1]) xx = xx.reshape([-1]) X = np.vstack((xx, yy, np.ones_like(xx))) torch.meshgrid. Example form MVSNet_pytorch\n1 2 3 4 5 6 y, x = torch.meshgrid([torch.arange(0, height), torch.arange(0, width)]) y, x = y.contiguous(), x.contiguous() y, x = y.view(height * width), x.view(height * width) xyz = torch.stack((x, y, torch.ones_like(x))) # [3, H*W] xyz = torch.unsqueeze(xyz, 0).repeat(batch, 1, 1) # [B, 3, H*W] torch.cartesian_prod referred by Docs\n1 2 3 h, w = ref.shape[:2] vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed Write Image (2024-04-02)\nExample of cv2 in casmvsnet_pl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from PIL import Image # pillow import cv2 import numpy as np root_dir = \u0026#34;/mnt/data2_z/MVSNet_testing/dtu\u0026#34; img_path = f\u0026#39;{root_dir}/scan1/images/00000000.jpg\u0026#39; img = np.array(Image.open(img_path)) # RGB fig, ax = plt.subplots(1,2) ax[0].imshow(img) cv2.imwrite(f\u0026#39;1.png\u0026#39;, img[:,:,::-1]) # save ax[1].imshow(cv2.imread(img_path)) # nd array, BGR img_read = cv2.imread(img_path)[:,:, ::-1] # RGB print((img_read == img).all()) ","date":"2024-01-22T17:25:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/image_rw/","title":"memo: Vis | Image Read/Write"},{"content":"(Feature image credits: GS big picture, please say thanks to GS.#419 - yuedajiong)\ngaussian-splatting python Source code\nDefine Params (2024-01-22)\nCreate PyTorch tensors for Gaussians\u0026rsquo; parameters at GaussianModel() by __init__ them with torch.empty(0):\n_xyz (Gaussian center), _features_dc and _features_rest (SH coeffs), _scaling (vec3), _rotation(quaternion), sh_degree (color), _opacity (unweighted alpha)\nEach covariance matrix 𝚺 is built from a scaling vector and a quaternion:\ns q c u a a l t i e n r g n i v o e n c S R t o r t e a c t h i i o n n g m m a a t t r r i i x x 𝐑 𝐒 ↗ ↘ 𝚺 = 𝐑 𝐒 𝐒 ᵀ 𝐑 ᵀ t l r o i w a e n r g ? l e ` s y m m ` symm is [Σ₀₀, Σ₀₁, Σ₀₂, Σ₁₁, Σ₁₂, Σ₂₂]\nRead point cloud and cameras at Scene() by calling __init__\nSceneInfo:\nBasic point cloud: points(x,y,z), normals (nx,ny,nz), colors (r,g,b)\nCameraInfo: extrinsics (R,T), fov (FovX,FovY), gt images (image, image_name, image_path, width, height)\nnerf_normalization (avg_cam_center (translate), max displacement from the avg cam (radius)),\nCameras list train_cameras of 300 Camera() is made up by repeatly loadCam()\nResize GT image for each camera (i.e., 300 views).\nViewing transform (world➡camera): self.world_view_transform\nProjection matrix (camera➡clip): self.projection_matrix\n$$ 𝐏 = \\begin{bmatrix} \\frac{2n}{r-l} \u0026 0 \u0026 \\frac{r+l}{r-l} \u0026 0 \\\\\\ 0 \u0026 \\frac{2n}{t-b} \u0026 \\frac{t+b}{t-b} \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\frac{f}{f-n} \u0026 \\frac{-fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\end{bmatrix} $$By denoting sight width as $W$, the above $r=\\frac{W}{2},\\ l=-\\frac{W}{2}$, so $(r-l)=W$. The sight center (principle center) (cx,cy) is (r,l), i.e., (W/2, H/2) The clip coordinate of $x$ produced by multiplying 𝐏 will result in $x_{NDC} ∈ [-1,1]$\nBy using this 𝐏, the final $z$ axis of ND coordinate ranges in [0,1], unlike the usual NDC cube of [-1,1]\n$$ near_{NDC} = \\frac{ \\frac{f * near}{f-n} + \\frac{-fn}{f-n} }{n} = 0 \\\\\\ far_{NDC} = \\frac{ \\frac{f*far}{f-n} + \\frac{-fn}{f-n} }{f} = 1 $$Therefore, this 𝐏 leads to a cuboid NDC space, rather than a cube NDC space. Figuratively, in the clip space, the points satisfying $0","date":"2024-01-22T00:00:00Z","image":"https://user-images.githubusercontent.com/52232153/283030293-f9f3589c-fa59-431f-8bd0-478337426933.png","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/b-note-3dgs-code/","title":"Read: 3DGS | Code Understanding"},{"content":"CUDA Tutorials I Profiling and Debugging Applications - NVIDIA Developer\n(2024-01-20)\nSource video: GPU L16: Support: cuda-gdb - YouTube - HPC Education (Rupesh Nasre 2021)\nIt\u0026rsquo;s a gdb extension for real hardware (not a simulator). Comparing with Nsight having GUI, CUDA-GDB is CLI. Regretfully, cuda-gdb doesn\u0026rsquo;t have TUI. Capture Last Error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Filename: test_cuda-gdb.cu #include \u0026lt;cuda_runtime.h\u0026gt; // to synchronize #include \u0026lt;cstdio\u0026gt; __global__ void kernel(int* x) { *x = 0; printf(\u0026#34;%d\\n\u0026#34;, *x); } int main() { int* x; kernel\u0026lt;\u0026lt;\u0026lt;2, 10\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); // Capture error cudaError_t err = cudaGetLastError(); printf(\u0026#34;err=%d, %s, %s\\n\u0026#34;, err, cudaGetErrorName(err), cudaGetErrorString(err) ); return 0; } Build: nvcc test_cuda-gdb.cu. Execution: ./a.out\nNothing is printed out, although 0 is supposed to show.\nAnd no error is reported, because the CPU sometimes isn\u0026rsquo;t aware of the error (e.g., SegFault) that happens on the GPU.\nTo identify whether the error occurred on the GPU, cudaGetLastError()\n1 2 yi@yi-Alien:~/Downloads/CUDA_Study/Debug_CUDA$ ./a.out err=700, cudaErrorIllegalAddress, an illegal memory access was encountered x requires GPU memory allocated:\n1 2 3 4 5 6 7 8 9 10 11 int main() { int* x; cudaMalloc( (void**)\u0026amp;x, 1*sizeof(int) ); kernel\u0026lt;\u0026lt;\u0026lt;2,2\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); cudaFree(x); cudaError_t err = cudaGetLastError(); printf(\u0026#34;err=%d, %s, %s\\n\u0026#34;, err, cudaGetErrorName(err), cudaGetErrorString(err) ); return 0; } Output 1 2 3 4 5 6 yi@yi-Alien:~/Downloads/CUDA_Study/Debug_CUDA$ ./a.out 0 0 0 0 err=0, cudaSuccess, no error cudaError Homework: Write programs to invoke these errors.\nRef:\nProper CUDA Error Checking - Lei Mao\u0026rsquo;s Log Book\nCUDA DEBUGGING - Bob Crovella, 9/14/2021\nNVIDIA CUDA Library: cudaError\nCUDA-GDB CLI Set flags to include the symbol information (variable name, function name) into the binary file:\nNames of variables and functions are used only for programming, as execution is instructed by memory addresses. So, symbols will be discarded for efficiency after compilation by default. 1 nvcc -g -G main.cu -g is for __host__ functions, compiled by gcc.\n-G is for __device__ functions, compiled by nvcc.\nDisable optimizations (preventing remove unused code) for debugging line-by-line.\nDebugging with cuda-gdb:\n1 cuda-gdb a.out Given the erroneous code:\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;cuda.h\u0026gt; __global__ void kernel(int* x) { *x = 0; printf(\u0026#34;%d\\n\u0026#34;, *x); } int main() { int* x; kernel\u0026lt;\u0026lt;\u0026lt;2, 2\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); return 0; } Build: nvcc test_cuda-gdb.cu. Debug: cuda-gdb a.out.\nrun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (cuda-gdb) run Starting program: /home/yi/Downloads/CUDA_Study/Debug_CUDA/a.out [Thread debugging using libthread_db enabled] Using host libthread_db library \u0026#34;/lib/x86_64-linux-gnu/libthread_db.so.1\u0026#34;. [New Thread 0x7ffff5d9b000 (LWP 2434197)] [New Thread 0x7ffff4ab1000 (LWP 2434198)] [Detaching after fork from child process 2434199] [New Thread 0x7fffeef3d000 (LWP 2434215)] [New Thread 0x7fffed533000 (LWP 2434216)] CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x100002ede48 Thread 1 \u0026#34;a.out\u0026#34; received signal CUDA_EXCEPTION_14, Warp Illegal Address. [Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] 0x00000100002ede78 in kernel(int*)\u0026lt;\u0026lt;\u0026lt;(2,1,1),(2,1,1)\u0026gt;\u0026gt;\u0026gt; () LWP: Light weight process Switching focus to a specific thread info cuda kernels Intro to GPU: 06 Debugging on GPU - YouTube - NERSC\n","date":"2024-01-20T17:20:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cuda/debug_gdb/","title":"memo: CUDA | Debug with CUDA-GDB"},{"content":" Source video: 【并行计算】CUDA在现代C++中如何运用？看这一个就够了！- 双笙子佯谬 - bilibili parallel101/course - Github Testing repo Textbook; pdf Enable CUDA in CMake 1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.10) set(CMAKE_CXX_STANDARD 17) set(CMAKE_BUILD_TYPE Release) # add CUDA project(hellocuda LANGUAGES CXX CUDA) add_executable(main main.cu) CUDA syntax is compatible with C++, so nvcc can compile a C++ project by chaning all .cpp files renamed to .cu. The nvcc can compile CPU and GPU code jointly. CPU-GPU Asyncronous For the sake of efficiency, after CPU tells GPU to run the kernel function (decorated by __global__), CPU proceeds to the next line of code without waiting for the GPU to finish the computation.\nTherefore, in the following code, the printf won\u0026rsquo;t be executed because programs returns directly after CPU pushes the task to GPU execution queue. However, the GPU didn\u0026rsquo;t have time to execute and return results.\n1 2 3 4 5 6 7 8 9 10 11 // Filename: test_async.cu #include \u0026lt;cstdio\u0026gt; __global__ void kernel() { printf(\u0026#34;Hello World!\\n\u0026#34;); } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(); return 0; } Compile: nvcc test_async.cu. Execute application: ./a.out. Set the program to wait for GPU completing all the tasks in its queue:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;iostream\u0026gt; __global__ void kernel() { printf(\u0026#34;Hello World!\\n\u0026#34;); // std::cout \u0026lt;\u0026lt; \u0026#34;out\u0026#34; \u0026lt;\u0026lt; std::endl; } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); return 0; } std::cout and std::endl are \u0026ldquo;host (CPU) functions\u0026rdquo;, which can\u0026rsquo;t be executed on GPU.\n1 test_async.cu(7): error: calling a __host__ function (\u0026#34;std::basic_ostream\u0026lt;char, st...\u0026#34;) from a __global__ function(\u0026#34;kernel\u0026#34;) is not allowed __host__ functions are compiled to callable only for other host functions. NV Forums\nFunction types Docs - Sec 7.1 Function Execution Space Specifiers\n__global__ function: called from the host or other devices, and executed on the device.\n__device__ function: called from other __device__ (or __global__) functions and executed on device.\n__host__ function: called from __host__ functions and executed on CPU.\nA function without decorated by any execution space specifier is compiled as a __host__ function.\nCalling a __device__ function (from other devices) doesn\u0026rsquo;t need \u0026lt;\u0026lt;\u0026lt; \u0026gt;\u0026gt;\u0026gt;, as it\u0026rsquo;s called on the GPU interally:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;cstdio\u0026gt; __device__ void say_hello() { printf(\u0026#34;hello\\n\u0026#34;); } __global__ void kernel() { say_hello(); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); return 0; } Since __gloabl__ functions are asyncronous and won\u0026rsquo;t return immediately, their return type must be void.\nHowever, the __device__ can have return value, like a normal function.\nA function can be called from either GPU or CPU with using both specifier: __host__ __device__\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; __host__ __device__ void say_hello() { printf(\u0026#34;hello~\\n\u0026#34;); } __global__ void kernel() { say_hello(); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); // gpu version cudaDeviceSynchronize(); say_hello(); // cpu version } Wil the computation in say_hello executed both by CPU and GPU?\nThe constexpr keyword can be replaced with __host__ __device__ by nvcc compiler to enable a constexpr function (e.g., math function) can be called from either a host or a device.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; constexpr const char* cuthead(const char* p) { return p + 1; } __global__ void kernel() { printf(cuthead(\u0026#34;Hello World!\\n\u0026#34;)); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); print(cuthead(\u0026#34;ABC\\n\u0026#34;)); return 0; } By decorating with __host__ __device__, the constexpr function will be inlined automatically.\nEnable the nvcc flag --expt-relaxed-constexpr with a \u0026ldquo;CMake的生成器表达式来实现只对 .cu 文件有效，而不会在 gcc 编译 .c 文件时生效，不然给到 gcc 就出错了\u0026rdquo; (?):\n1 2 add_executable(main main.cu foo.cpp) target_compile_options(main PUBLIC $\u0026lt;$\u0026lt;COMPILE_LANGUAGE:CUDA\u0026gt;:--expt-relaxed-constexpr\u0026gt;) However, on the contrary, __host__ __device__ can\u0026rsquo;t be replaced with constexpr, because constexpr function cannot call printf and GPU-specific functions, like _syncthreads. inline device function Docs - Sec 7.1.5\nIf appropriate, the compiler will inline __device__ functions automatically.\nWhen the function body is too big, the compiler may won\u0026rsquo;t insert code. __noinline__ declares a function that won\u0026rsquo;t be inserted into the place where it\u0026rsquo;s called.\nAnd __forceinline__ is the opposite.\n","date":"2024-01-19T14:58:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cuda/tut_%E5%BD%AD%E4%BA%8E%E6%96%8C/","title":"watch: Parallel101 - 彭于斌 | CUDA Programming"},{"content":"(2024-02-14)\nDerivative is the amount of change in a target object caused by a variable\u0026rsquo;s change.\nA row a matrix consists of the coefficient of each term in a linear equation. And based on the \u0026ldquo;sum rule\u0026rdquo; of derivative ($(f+g)'=f'+g'$), the derivative of the linear equation w.r.t. a variable is the summation of the derivative of each element in the row w.r.t. the variable.\nd Ax (2024-01-13)\nSource video: Derivative of a Matrix : Data Science Basics - ritvikmath\nMatrix 𝐀() stands for a linear transformation (function). And only the derivative of a function (𝐀𝐱) makes sense.\nMatrix is a representation of linear systems. $$ \\begin{aligned} f(x) \u0026= 𝐀𝐱 \\\\\\ \u0026= \\begin{bmatrix} 1 \u0026 2 \\\\\\ 3 \u0026 4 \\end{bmatrix} \\begin{bmatrix} x₁ \\\\\\ x_2 \\end{bmatrix} \\\\\\ \u0026= \\begin{bmatrix} x₁ + 2 x₂ \\\\\\ 3x₁ + 4x₂ \\end{bmatrix} ⇒ \\begin{bmatrix} f₁(x₁,x₂) \\\\\\ f₂(x₁,x₂) \\end{bmatrix} \\end{aligned} $$$$ \\frac{d𝐀𝐱}{d𝐱} = \\begin{bmatrix} ∂f₁/∂x₁ \u0026 ∂f₁/∂x₂ \\\\\\ ∂f₂/∂x₁ \u0026 ∂f₂/∂x₂ \\end{bmatrix}= \\begin{bmatrix} 1 \u0026 2 \\\\\\ 3 \u0026 4 \\end{bmatrix} $$The derivative of the linear transformation 𝐀𝐱 w.r.t. x is A. It analog to single-variable function.\nA matrix $A$ is a \u0026ldquo;scalar\u0026rdquo;. More concretely, it\u0026rsquo;s a collection of scalars in a box.\nTherefore, the derivative of A means the derivative of a constant, which would be 0. So, it doesn\u0026rsquo;t make any sense.\nThereby, we are not calculating the derivative of a matrix, but the derivative of a linear transformation 𝐀𝐱 w.r.t. 𝐱.\nd xᵀAx $$ \\begin{aligned} 𝐱ᵀ𝐀𝐱 \u0026= \\begin{bmatrix} x₁ \u0026 x₂ \\end{bmatrix} \\begin{bmatrix} a₁₁ \u0026 a₁₂ \\\\\\ a₂₁ \u0026 a₂₂ \\end{bmatrix} \\begin{bmatrix} x₁ \\\\\\ x₂ \\end{bmatrix} \\\\\\ \u0026= \\begin{bmatrix} x₁ \u0026 x₂ \\end{bmatrix} \\begin{bmatrix} a₁₁x₁+ a₁₂x₂ \\\\\\ a₂₁x₁ + a₂₂x₂ \\end{bmatrix} \\\\\\ \u0026= a₁₁x₁²+ a₁₂x₁x₂ + a₂₁x₁x₂ + a₂₂x₂² ⇒ f(x₁,x₂) \\end{aligned} $$Consider 𝐀 is a symmetric matrix, so a₂ = a₃. Then, $𝐱ᵀ𝐀𝐱 = a₁₁x₁²+ a₁₂x₁x₂ + a₂₁x₁x₂ + a₂₂x₂² = f(x₁,x₂)$\nThe derivative of the linear transformation 𝐱ᵀ𝐀𝐱:\n$$ \\begin{aligned} \\frac{d𝐱ᵀ𝐀𝐱}{d𝐱} \u0026= \\begin{bmatrix} ∂f/∂x₁ \\\\\\ ∂f/∂x₂ \\end{bmatrix} \\\\\\ \u0026= \\begin{bmatrix} 2a₁₁x₁+2a₁₂x₂ \\\\\\ 2a₁₂x₂ + 2a₂₂x₂ \\end{bmatrix} \\\\\\ \u0026= 2 \\begin{bmatrix} a₁₁ \u0026 a₁₂ \\\\\\ a₁₂ \u0026 a₂₂ \\end{bmatrix} \\begin{bmatrix} x₁ \\\\\\ x₂ \\end{bmatrix} \\\\\\ \u0026= 2𝐀𝐱 \\end{aligned} $$It\u0026rsquo;s an analog to quadratic of matrix operations.\n3 cases Source article: The derivative matrix - Math Insight\nA matrix 𝐀 contains elements that are functions of a scalar x.\nThe $\\frac{d𝐀}{dx}$ is a matrix of the same size as 𝐀.\nRefer to Definition 5 in Matrix Differentiation - Department of Atmospheric Sciences\nThe derivative of a multi-variable scalar-valued function $f$ is a matrix of partial derivatives of each function with respect to each variable.\nDerivative of 𝐟 w.r.t. each coordinate axis. $\\frac{df}{d𝐱} = [ \\frac{∂f}{∂x₁}\\ \\frac{∂f}{∂x₂}\\ ⋯ \\ \\frac{∂f}{∂xₙ} ]$ A matrix 𝐀 contains elements that are functions of a vector 𝐱.\n$𝐀(𝐱) = 𝐟(𝐱) = (f_1(𝐱),\\ f_2(𝐱),\\ ..., f_m(𝐱)) = \\begin{bmatrix} f_1(𝐱) \\\\\\ f_2(𝐱) \\\\\\ ⋮ \\\\\\ f_m(𝐱) \\end{bmatrix}$\nThe $\\frac{d𝐀}{d𝐱}$ is a matrix with the size of mxn:\n$$ \\frac{d𝐀}{d𝐱} = \\begin{bmatrix} \\frac{f_1}{x_1} \u0026 \\frac{f_1}{x_2} \u0026 ⋯ \u0026 \\frac{f_1}{xₙ} \\\\\\ \\frac{f_2}{x_1} \u0026 \\frac{f_2}{x_2} \u0026 ⋯ \u0026 \\frac{f_2}{xₙ} \\\\\\ ⋮ \u0026 ⋮ \u0026 ⋮ \u0026 ⋮ \\\\\\ \\frac{f_m}{x_1} \u0026 \\frac{f_m}{x_2} \u0026 ⋯ \u0026 \\frac{f_m}{xₙ} \\\\\\ \\end{bmatrix} $$ Matrix derivative (2023-02-12)\nMatrix derivatie is in terms of the whole matrix, instead of each element. Whereas partial derivatives of a matrix\nGiven a matrix $[^{a\\ b}\\_{d\\ c}]$,the derivative of its inverse matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}\\_{-d\\ a}]$ w.r.t. the original matrix is the \u0026ldquo;coefficient\u0026rdquo; in their relation:\n$$ \\underbrace{ \\begin{bmatrix} c \u0026 -b \\\\\\ -d \u0026 a \\end{bmatrix} \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026 -b \\\\\\ -d \u0026 a \\end{bmatrix} }\\_{\\text{Coefficient}} \\begin{bmatrix} a \u0026 b \\\\\\ d \u0026 c \\end{bmatrix} = \\begin{bmatrix} c \u0026 -b \\\\\\ -d \u0026 a \\end{bmatrix} $$ This transformation can be understood as that the original matrix first times its inverse $\\frac{1}{ac-bd}[^{\\ c\\ -b}\\_{-d\\ a}]$ to become the identity matrix $[^{1\\ 0}_{0\\ 1}]$, which gets multiplied by $[^{\\ c\\ -b}\\_{-d\\ a}]$ to yield the inverse matrix.\nTherefore, the coefficient is:\n$$ \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026 -b \\\\\\ -d \u0026 a \\end{bmatrix} \\begin{bmatrix} c \u0026 -b \\\\\\ -d \u0026 a \\end{bmatrix} = \\frac{1}{ac-bd} \\begin{bmatrix} c² + bd \u0026 -bc-ab \\\\\\ -cd-ad \u0026 bd+a²\\end{bmatrix} $$In this case, is the optimizing objective the whole matrix $[^{a\\ b}_{d\\ c}]$, with its coefficient serving as the gradient?\nperplexity\nOn the other hand, the partial derivatives of the inverse matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}\\_{-d\\ a}]$ with respect to each element a, b, c, d can be conceptualized as:\nhow does changes in the 4 \u0026ldquo;variables\u0026rdquo; $a,\\ b,\\ c,\\ d$ affect the matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}\\_{-d\\ a}]$\n$$ \\begin{aligned} \\frac{ ∂\\frac{1}{ac-bd} \\begin{bmatrix} c \u0026 -b \\\\\\ -d \u0026 a \\end{bmatrix}}{∂a} \u0026= \\begin{bmatrix} \\frac{∂}{∂a} (\\frac{c}{ac-bd} ) \u0026 \\frac{∂}{∂a} (\\frac{-b}{ac-bd}) \\\\\\ \\frac{∂}{∂a} (\\frac{-d}{ac-bd}) \u0026 \\frac{∂}{∂a} (\\frac{a}{ac-bd} ) \\\\\\ \\end{bmatrix} \\\\\\ \u0026= \\begin{bmatrix} \\frac{-c²}{(ac-bd)²} \u0026 \\frac{bc}{(ac-bd)²} \\\\\\ \\frac{dc}{(ac-bd)²} \u0026 \\frac{-bd}{(ac-bd)²} \\\\\\ \\end{bmatrix} \\end{aligned} $$The total change of the matrix magnitude caused by moving $a$ by one unit would be:\n$$\\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}\\_{-d\\ a}] )}{∂a} = \\frac{-c² + bc + dc - bd}{(ac-bd)²} $$ Particularly, with this derivative, $a$ can be optimized via gradient descent. Similarly, the partial derivatives of the matrix w.r.t. $b,\\ c,\\ d$ are:\n$$ \\begin{aligned} \\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}\\_{-d\\ a}] )}{∂b} \u0026= \\frac{cd-ac-d²+ad}{(ac-bd)²} \\\\\\ \\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}\\_{-d\\ a}] )}{∂c} \u0026= \\frac{-bd+ba+da-a²}{(ac-bd)²} \\\\\\ \\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}\\_{-d\\ a}] )}{∂d} \u0026= \\frac{cb-b²-ac+ab}{(ac-bd)²} \\\\\\ \\end{aligned} $$ (2024-02-13)\nMatrix Derivatives: What\u0026rsquo;s up with all those transposes ? - David Levin\nGradient: Matrix form -\u0026gt; indices form -\u0026gt; matrix form\nMatrix Calculus - Online\nXᵀwX (2024-04-06)\n拆分成：向量函数 + 多元函数\n空间的基可以是多项式函数, 幂函数, 所以线性方程可以表示非线性函数\n【微积分和线性代数碰撞的数学盛宴：最小二乘法公式推导！】-晓之车高山老师 - bilibili\n(2024-05-15)\n【高等数学笔记】多元向量值函数的导数与微分_- CSDN - seh_sjlj (2024-07-22)\nSource video: 手推机器学习1⃣️\u0026mdash;矩阵求导 - S-WangZ(2024-05-24)\nScalar-value function $f: \\\\R^n → \\\\R$\nDefined with field and vector space: Scalar-valued function definition - SE (Searched by \u0026ldquo;scalar function\u0026rdquo; in DDG)\nA field $k$ comprises one set k and two operations: addition and multiplicaiton. $k = (k, +, ⋅)$\nA vector space $V$ comprises two sets $k$ and $V$ and two operations: addition and multiplicaiton. $V = (V, +, k, ⋅)$\nAn element in the set k is a scalar. An element in the set V is a vector. Scalar-field funtion f maps a vector space to a scalar: $f: V → k$ \u0026ldquo;Scalar function is a function with one-dimensional scalar output\u0026rdquo; Scalar Function, Definition of Scalar - Statistics How To\n\u0026ldquo;A scalar-value function is a function that takes one or more values and returns a single value.\u0026rdquo; World Web Math: Vector Calculus: Scalar Valued Functions - MIT\n","date":"2024-01-13T09:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/matrix_derivative/","title":"memo: Calc | Derivative of \"Matrix\""},{"content":"Code | arXiv | Vickie Ye\nSurfaced by NeRF\u0026amp;Beyond 12.5日报(CustomNeRF，VideoRF，网格引导编辑，SANeRF-HQ，GPS-Gaussian，两个GaussianAvatar，SpalTAM，gsplat） - Jason陪你练绝技的文章 - 知乎 Feature image from: Understanding the Covariance Matrix - Janakiev Updates:\n(2024-06-17)\nI suspect I may have previously mixed up the directions of \u0026ldquo;back\u0026rdquo; and \u0026ldquo;front\u0026rdquo; after reviewing the ewa splatting paper.\nThe \u0026ldquo;back\u0026rdquo; is supposed to be close to the eye (camera), which looks at the \u0026ldquo;front\u0026rdquo;. As said in the ewa paper:\nthe final image is computed by compositing the sheets back to front.\nSo, I changed all the \u0026ldquo;front\u0026rdquo; to \u0026ldquo;back\u0026rdquo; in this post, and vice versa.\nProjection Old Notes on 2023-12-05 NDC is the de-homogeneous clip coordinates and ranges [-1,1].\nDe-homogeneous means the z value has been divided.\nMapping the coordinates of point (x,y,z) in camera space to clip coordinates, i.e., a cube of [-1,1] can be decomposed to two operations: perspective projection and scaling ranges, and then compound them.\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\\\ y_{clip} \\\\\\ z_{clip} \\\\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\end{bmatrix} \\begin{bmatrix}x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{bmatrix} \\end{array} $$The individual perspective projection:\n$$ \\begin{bmatrix} fₓ \u0026 0 \u0026 0 \\\\\\ 0 \u0026 f_y \u0026 0 \\\\\\ 0 \u0026 0 \u00261 \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y \\\\\\ z \\end{bmatrix} $$After that, the plane coordinates are (u, v), wher $u = \\frac{fₓx}{z}, v=\\frac{f_yy}{z}$.\nThen scaling the ranges:\nScale the range of u from [-w/2,w/2] to [-1,1] through a linear mapping: α u + β\nα and β be solved based on two points.\n$$ \\begin{array}{cc} \\begin{cases} α (-w/2) + β = -1 \\\\\\ α w/2 + β = 1 \\end{cases} ⇒ \\begin{cases} α = 2/w \\\\\\ β = 0 \\end{cases} \\end{array} $$ Similarly, scale the range of v from [-h/2,h/2] to [-1,1] through a linear mapping: α v + β\nThus, $α= 2/h, β=0$\nSo far, the first 2 rows are determined:\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\\\ y_{clip} \\\\\\ z_{clip} \\\\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2fₓ/w \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 2f_y/h \u0026 0 \u0026 0 \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\end{bmatrix} \\begin{bmatrix}x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{bmatrix} \\end{array} $$ When scaling z, it has nothing to do with x and y. Thus, the 3rd row is 0 0 □ □.\nBeacuse NDC is the de-homogeneous clip coordinates, which requires divide by z to become NDC. Therefore, the 4-th row is 0 0 1 0.\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\\\ y_{clip} \\\\\\ z_{clip} \\\\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2fₓ/w \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 2f_y/h \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 A \u0026 B \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\end{bmatrix} \\begin{bmatrix}x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{bmatrix} \\end{array} $$With denoting the two unknowns as A and B, the NDC of z dimension is: $\\frac{A z + B}{z}$.\nAccording to the range constraint [-1,1], the A and B can be solved from:\n$$ \\begin{array}{cc} \\begin{cases} \\frac{A n + B }{n} = -1 \\\\\\ \\frac{A f + B }{f} = 1 \\end{cases} ⇒ \\begin{cases} A = (f+n)/(f-n) \\\\\\ B = -2fn/(f-n) \\end{cases} \\end{array} $$ Finally, the mapping from the camera coordinates of a point to corresponding clip coordinates is:\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\\\ y_{clip} \\\\\\ z_{clip} \\\\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2fₓ/w \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 2f_y/h \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\frac{f+n}{f-n} \u0026 \\frac{-2fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\end{bmatrix} \\begin{bmatrix}x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{bmatrix} \\end{array} $$ NDC Mapping (2024-01-01)\nThe perspective division (a 3D pixel coordinates are divided by the 3rd dimension) should be performed as the final step in the transformation pipeline, as it\u0026rsquo;s a non-linear operation.\nGiven a 3D point (x,y,z)ᵀ located in the camera space, the perspective projection and scaling are carried out in sequence to obtain its clip coordinates (not NDC yet).\n$$ \\begin{array}{c} \\text{[Scaling Matrix] [Perspective Projection] [Camera space] = [Clip space]} \\\\\\ \\\\\\ \\begin{bmatrix} □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\end{bmatrix} \\begin{bmatrix} fₓ \u0026 0 \u0026 cₓ \u0026 0 \\\\\\ 0 \u0026 f_y \u0026 c_y \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x_c \\\\\\ y_c \\\\\\ z_c \\\\\\ w_c \\end{bmatrix} \\end{array} $$The scaling matrix is built with the goal of mapping the view frustum to a [-1,1] NDC-space cube encompassing only valid points, whose clip coordinates satisfy: $-w_c \u003c x_c,y_c,z_c \u003c w_c$. Specifically, the projected coordinates are scaled and then perform perspective division to become the NDC.\nPerspective projection:\n$$ \\begin{bmatrix} fₓ \u0026 0 \u0026 cₓ \u0026 0 \\\\\\ 0 \u0026 f_y \u0026 c_y \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fₓx + cₓ z \\\\\\ f_y y + c_y z \\\\\\ z \\\\\\ 1 \\end{bmatrix} $$ ( ( 0 0 , , 0 H ) … ) c … ₓ … … ⋮ ⋮ ∙ ( c 0 y , 0 ) ( W , 0 ) Scaling projected coordinates u and v to [-1, 1]\nTo scale u, the 1st row of the scaling matrix is A 0 B 0.\nPerform scaling first, followed by perspective division, to obtain the x in NDC space: $\\frac{A (fₓ x + cₓ z) + Bz}{z} = A (\\frac{fₓ x}{z} + cₓ) + B ∈ [-1,1]$\nSince $(\\frac{fₓ x}{z} + cₓ) = u ∈ [0,W]$\n$$ \\begin{cases} A0 + B = -1 \\\\\\ AW + B = 1 \\end{cases} ⇒ \\begin{cases} A = \\frac{2}{W} \\\\\\ B = -1 \\end{cases} $$Therefore, the first 2 rows are:\n$$ \\begin{bmatrix} 2/W \u0026 0 \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 2/H \u0026 -1 \u0026 0 \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\\\\\ □ \u0026 □ \u0026 □ \u0026 □ \\end{bmatrix} \\begin{bmatrix} fₓx + cₓ z \\\\\\ f_y y + c_y z \\\\\\ z \\\\\\ 1 \\end{bmatrix} $$ Scaling frustum $z ∈ [n, f]$ to [-1,1]\nz is independent to x and y, so the 3rd row only has 2 unknows: 0 0 A B.\nScaling first, then perspective division, thereby the z in NDC space is: $\\frac{A z + B}{z} ∈ [-1, 1]$\nSubstituting z = n and f:\n$$ \\begin{cases} \\frac{A n + B}{n} = -1 \\\\\\ \\frac{A f + B}{f} = 1 \\end{cases} ⇒ \\begin{cases} A = \\frac{f+n}{f-n} \\\\\\ B = \\frac{-2fn}{f-n} \\end{cases} $$Finally, since the denominators are z (i.e., the w of a point\u0026rsquo;s clip coordinates is z), the 4th row is 0 0 1 0:\n$$ \\begin{bmatrix} 2/W \u0026 0 \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 2/H \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\frac{f+n}{f-n} \u0026 \\frac{-2fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\end{bmatrix} \\begin{bmatrix} fₓx + cₓ z \\\\\\ f_y y + c_y z \\\\\\ z \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2 (fₓx + cₓ z)}{W} - z \\\\\\ \\frac{2 (f_y y + c_y z) }{H} - z \\\\\\ \\frac{f+n}{f-n} z - \\frac{2fn}{f-n} \\\\\\ z \\end{bmatrix} $$The result coordinates are in the clip space, which will become ND coordinates after perspective division, and only points within the cube of [-1,1] in NDC space will be rendered.\n(2024-07-09)\n相机系下点的坐标乘以下面那个 Projection 矩阵，但还没做透视除法，这时 z 方向的取值范围是 [-z, z]，也就是 clip space 坐标系。\n(2024-07-21)\nAfter projection matrix, the range of depth in the clip space is $[\\frac{f+n}{f-n} n - \\frac{2fn}{f-n}, \\frac{f+n}{f-n} f - \\frac{2fn}{f-n}]$. In the clip space, those points whose x and y are larger than their z (or smaller than -z), will be filtered out.\nClipping 做完之后，再做透视除法，这样3个方向的坐标范围就都是 [-1,1] 了，也就是 ND 坐标系了。\nIn summary, the Projection Matrix (GL_PROJECTION) transforming camera-space (x,y,z)ᵀ to clip coordinates is: $$ P = \\begin{bmatrix} \\frac{2}{W} \u0026 0 \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 \\frac{2}{H} \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\frac{f+n}{f-n} \u0026 \\frac{-2fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\end{bmatrix} \\begin{bmatrix} fₓ \u0026 0 \u0026 cₓ \u0026 0 \\\\\\ 0 \u0026 f_y \u0026 c_y \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2fₓ}{W} \u0026 0 \u0026 (\\frac{2cₓ}{W}) -1 \u0026 0 \\\\\\ 0 \u0026 \\frac{2f_y}{H} \u0026 (\\frac{2c_y}{H}) -1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\frac{f+n}{f-n} \u0026 \\frac{-2fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\end{bmatrix} $$ When cx and cy are W/2 and H/2, it\u0026rsquo;s in the form of Ye\u0026rsquo;s article.\nWhen cx and cy are r and t, it\u0026rsquo;s in the form of songho.\n(2025-05-15T23:16)\n透视除法之后的 x,y,z 才是 [-1,1]，即 NDC 坐标，没有做透视除法之前是 Clip 坐标\nProjection Matrix 作用在点在 Camera space 下的坐标上，得到 Clip 坐标\nClip 坐标再做透视除法才是 NDC 坐标， NDC 坐标的取值范围需要是 [-1,1]\n因为 NDC 坐标是通过除以 $Z_{Cam}$ 得来，所以 $Z_{Cam}$ 就决定了一个点是否位于 ND 空间内（被渲染）\nx _ c l i p / z _ c a m \u0026lt; - 1 - 1 1 x _ c l i p / z _ c a m \u0026gt; 1 (2024-01-02)\nProject mean vector 𝛍 A mean vector 𝛍 in world space is changed to pixel space as follows:\nW C o o 𝛍 r o l r d d V t 𝐓 i r e a w n s f C C a o m o 𝐭 e r r d a P + e r S 𝐏 s c p a l p i r n o g j C C l o i o 𝐭 p r ' d C l P ↑ i e d p ϕ r i p a ₖ s v i c ( p i n c 𝐭 e s g o ' c i u ) t o n i n → t v s e f N C ( o D o C r C o u r b d e S ) c a l e P C i o 𝛍 x o ' e r l d 𝐭 refers to coordinates of 𝛍 in the camera space as:\n$$ 𝐭 = \\begin{bmatrix} 𝐑_{w2c} \u0026 𝐭_{w2c} \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} \\bm μ \\\\\\ 1 \\end{bmatrix} $$ Note the translation vector is denoted as $𝐭_{w2c}$.\nAnd the extrinsics is represented as $𝐓_{w2c}$.\nThe clip coordinates of 𝛍 is $𝐭' = 𝐏𝐭$\nThe nonlinear perspective division is approximated by the projective transformation $ϕₖ(𝐭')$.\nPoints\u0026rsquo; coordinates conversion from world space to clip space:\n$$ \\begin{aligned} 𝐭' \u0026= 𝐏⋅ 𝐓_{w2c}⋅ [^𝛍_1] \\\\\\ \u0026=\\begin{bmatrix} \\frac{2fₓ}{W} \u0026 0 \u0026 (2cₓ/W) -1 \u0026 0 \\\\\\ 0 \u0026 \\frac{2f_y}{H} \u0026 (2c_y/H) -1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\frac{f+n}{f-n} \u0026 \\frac{-2fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\end{bmatrix} \\begin{bmatrix} R_{w2c} \u0026 t_{w2c} \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} μₓ \\\\\\ μ_y \\\\\\ μ_z \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} tₓ' \\\\\\ t_y' \\\\\\ t_z' \\\\\\ t_w' \\end{bmatrix} \\end{aligned} $$ $t_w'$ is the point\u0026rsquo;s camera-space depth $t_z$, which is \u0026gt; 0. Frustum clipping (clip-space culling) filters points that won\u0026rsquo;t appear in the frustum based on clip coordinates before perspective division:\nc C - n c e a 1 e a n m ⋅ m t - a e s r p a r c f e a r i f c n r o u o s r 1 d s N C C ➔ D l a i m N c p e D o r o c a c r o o d o c o : D r P o r i d r o d ( v : o r i x j d n ₙ b ( : a , y x + t _ ( e y w c s x o s ₙ _ , c , z u , c a y ₙ t y l , \u0026gt; z _ e z ❌ 1 o ₙ c , f , , 1 ) f 1 z r ) _ ∈ u ∈ c s [ , [ t 0 0 u , w , m _ ∞ c ∞ ] ) ] → ∈ [ [ - 0 1 ↑ ↑ , , C 1 D l ∞ ] i i ] v p p w i n g ❗ View Frustum Clipping - UofTexas - Lec9\nView frustum clipping aims to reduce computation. Essentially, it filters points by comparing the clip coordinate $w_c$ with $x_c, y_c, z_c$ for each point.\nIn the previous derivation, clip coordinates are the scaled projection coordinates: (A*ProjCoord+B). NDC space is defined by $w$, as the NDC cube is constrained by bounds where the clip coordinates divided by 𝑤 equals 1 (𝑤 is the benchmark), such as $\\frac{A (f_x x+c_x z) + Bz}{w} = 1$\nIn other words, the NDC planes wrap around points whose clip coordinates $x_c,y_c,z_c$ less than or equal to $w_c$. In addition, $w_c$ must be the camera-space depth z for the final perspective division. Thus, if the clip coordinate of a point is bigger than w or less than $-w$, the \u0026ldquo;quotient\u0026rdquo; will be outside of [-1,1], i.e., the point is not located in the camera-space view frustum or the NDC-space cube.\nFrustum clipping retains points that satisfy: $-w_c \\leq x_c, y_c, z_c \\leq w_c$. Conversely, those points whose w (equals camera-space depth) is smaller than x,y,z will be filtered out.\nAlthough ND Coordinates are also able to identify points for clipping, to reduce the number of perspective divisions (executed at final), clipping is performed in the clip space for efficiency.\nOn the other hand, since the ND Coordinates of the Left, Right, Bottom, Top, Near, and Far frustum planes are -1 and 1, the clip coordinates of points located within the Frustum satisfy the relation: $-1 \u003c \\frac{x_c}{w_c}, \\frac{y_c}{w_c}, \\frac{z_c}{w_c} \u003c1$\nConsequently, only points in the frustum (i.e., NDC-space cube: $-1 \u003c xₙ, yₙ, zₙ \u003c 1$) are survived.\n(2024-01-03)\nIt is the view frustum clipping that makes ND coordinates can be regarded as a cube space, because out-of-the-cube points have been disregared. (2024-06-22)\nCan I use the mnemonic: If the z-coord of a point is larger than its x,y, it will be killed? (I should refer that video to see the code.) Performing perspective division on the clip coordinates resulting in NDC, whose all components ranges in [-1,1]:\n$$ NDC = \\begin{bmatrix} tₓ'/t_w' \\\\\\ t_y'/t_w' \\\\\\ t_z'/t_w' \\\\\\ 1 \\end{bmatrix} ∈ [-1,1] $$ (2024-02-08) NDC is 3D, including zₙ coord besides the 2D pixel coords. Scaling NDC to obtain pixel coordinates 𝛍\u0026rsquo; (viewport transformation): songho\n$$ [-1, 1] \\overset{×W}{→} [-W, W] \\overset{+1}{→} [-W+1,W+1] \\overset{÷2}{→} [\\frac{-W+1}{2}, \\frac{W+1}{2}] \\\\\\ \\overset{+c_x}{→} [0.5, W+0.5] (\\text{ if $c_x =\\frac{W}{2}$}) $$Therefore, the final pixel coordinate 𝛍\u0026rsquo; of a world-space mean vector 𝛍 is: $$ \\bm \\mu' = \\begin{bmatrix} (W⋅tₓ'/t_w' + 1) /2 + c_x \\\\\\ (H⋅t_y'/t_w' + 1) / 2 + c_y \\end{bmatrix} $$ Project Covariance (2024-01-03)\nBecause the perspective projection is not a linear operation due to division, the 2D projection of a 3D Gaussian is not a 2D Gaussian:\nsnowball splat diffuse Is it a 2D Gaussian? Conic sections EWA Splatting doesn\u0026rsquo;t scale the projected coordinates to the [-1,1] NDC-space cube. It directly transforms points from camera space onto screen (or viewing-ray space) by dividing z. The result coordinates range in [0,W] and [0,H].\nProjective transformation ϕ(𝐭) in EWA Splatting: Converting an arbitrary point\u0026rsquo;s camera-space coordinates $𝐭=(tₓ,t_y,t_z)ᵀ$ to the coordinates in 3D ray space (pixel coordinates x0,x1 + \u0026ldquo;new depth\u0026rdquo; x2) has 2 steps:\nPixel coords = perspective projection + perspective division. The new depth is set to the L2 norm of the point\u0026rsquo;s camera-space coordinates. $$ ϕ(𝐭) = \\begin{bmatrix} \\frac{fₓ tₓ}{t_z} + cₓ \\\\\\ \\frac{f_y t_y}{t_z} + c_y \\\\\\ \\sqrt{t_x^2 + t_y^2 + t_z^2} \\end{bmatrix} = \\begin{bmatrix} x_0 \\\\\\ x_1 \\\\\\ x_2 \\end{bmatrix} $$Because EWA Splatting doesn\u0026rsquo;t consider frustum clipping, the approximation is based on the camera coordinates 𝐭.\n(2024-02-16) The clip coordinates shouldn\u0026rsquo;t be used in EWA splatting because the Gaussian center is used in the Jacobian that approximates the perspective projection, where the camera-space coordinates are supposed to be used.\nWhereas, 3DGS (or gsplat) requires to determine whether the point (Gaussian center) is in the frustrum to be rendered, the clip coordinates are utilized for frustum clipping. Therefore, the world coordinates 𝐱 are involved into 2 procedures: the covertion for covariance matrix from world space to ray space (𝐉𝐖𝚺𝐖ᵀ𝐉ᵀ), and the projection for Gaussian center from world space onto the screen.\nConsequently, the derivative of Loss w.r.t. the world coordinates ($\\frac{∂L}{∂𝐱}$) has 2 portions.\nHowever, gsplat uses the clip space to filter points outside the camera frustum. So, after perspective projection and scaling for NDC with matrix 𝐏, points are transferred into clip space as 𝐭\u0026rsquo; for clipping.\nAfter clipping, the nonlinear perspective division and x₂ reassignment in ϕ(𝐭), are approximated with an affine transformation based on the clip coordinates 𝐭\u0026rsquo;. Therefore, the projective transformation $ϕ(𝐭)$ that maps camera space to ray space becomes a mapping from clip space to the ray space $ϕ(𝐭')$.\n$$ \\begin{aligned} 𝐭 \u0026→ 𝐭'=𝐏𝐭 = \\begin{bmatrix} \\frac{2 (fₓ t_x + cₓ t_z)}{W} - t_z \\\\\\ \\frac{2 (f_y t_y + c_y t_z) }{H} - t_z \\\\\\ \\frac{f+n}{f-n} z - \\frac{2fn}{f-n} \\\\\\ t_z \\end{bmatrix} = \\begin{bmatrix}t_x' \\\\\\ t_y' \\\\\\ t_z' \\\\\\ t_z\\end{bmatrix} \\\\\\ \u0026→ ϕ(𝐭') = \\begin{bmatrix} tₓ'/t_z' \\\\\\ t_y'/t_z' \\\\\\ ‖𝐭'‖₂ \\end{bmatrix} = \\begin{bmatrix} x_0 \\\\\\ x_1 \\\\\\ x_2 \\end{bmatrix} \\end{aligned} $$The affine approximation is the first 2 terms of $ϕ(𝐭')$\u0026rsquo;s Taylor expansion evaluated at a Gaussian\u0026rsquo;s mean vector $𝐭ₖ' = (t_{k,x}', t_{k,y}', t_{k,z}')ᵀ$ in the clip space:\n$$ \\begin{aligned} \u0026 ϕ(𝐭') ≈ ϕₖ(𝐭') = ϕ(𝐭ₖ') + 𝐉_{𝐭ₖ'} ⋅ (𝐭' - 𝐭ₖ') \\\\\\ \u0026 = \\begin{bmatrix} t_{k,x}'/t_{k,z}' \\\\\\ t_{k,y}'/t_{k,z}' \\\\\\ ‖𝐭ₖ'‖² \\end{bmatrix} + \\begin{bmatrix} 1/t_{k,z}' \u0026 0 \u0026 -t_{k,x}/{t_{k,z}'}^2 \\\\\\ 0 \u0026 1/t_{k,z}' \u0026 -t_{k,y}/{t_{k,z}'}^2 \\\\\\ t_{k,x}'/‖𝐭ₖ'‖₂ \u0026 t_{k,y}'/‖𝐭ₖ'‖₂ \u0026 t_{k,z}'/‖𝐭ₖ'‖₂ \\end{bmatrix} (𝐭' - 𝐭ₖ') \\end{aligned} $$ If using the camera-space coordinates 𝐭 to express the projective transformation as ϕ(𝐭), focal lengths will be exposed:\n$$ \\begin{aligned} ϕ(𝐭) ≈ ϕₖ(𝐭) \u0026= ϕ(𝐭ₖ)+ 𝐉_{𝐭ₖ} ⋅ (𝐭 - 𝐭ₖ) \\\\\\ \u0026=\\begin{bmatrix} 2(fₓ t_{k,x}/t_{k,z} + cₓ)/W -1 \\\\\\ 2(f_y t_{k,y}/t_{k,z} + c_y)/H -1 \\\\\\ ‖𝐭ₖ‖₂ \\end{bmatrix} + 𝐉_{𝐭ₖ} ⋅ (𝐭 - 𝐭ₖ) \\end{aligned} $$If $c_x=W/2,\\ c_y=H/2$, then $ϕₖ(𝐭) = \\begin{bmatrix} \\frac{2f_x t_{k,x}}{W⋅ t_{k,z}} \\\\\\ \\frac{2f_y t_{k,y}}{H⋅ t_{k,z}} \\\\\\ ‖𝐭ₖ‖₂ \\end{bmatrix}$\nand the Jacobian $𝐉_{𝐭ₖ}$ will be:\n$$ 𝐉_{𝐭ₖ} = \\begin{bmatrix} (2/W)⋅fₓ/t_{k,z} \u0026 0 \u0026 (2/W)⋅-fₓ t_{k,x} / {t_{k,z}}^2 \\\\\\ 0 \u0026 (2/H)⋅f_y/t_{k,z} \u0026 (2/H)⋅-f_y t_{k,y} / {t_{k,z}}^2 \\\\\\ t_{k,x}/‖𝐭ₖ‖₂ \u0026 t_{k,y}/‖𝐭ₖ‖₂ \u0026 t_{k,z}/‖𝐭ₖ‖₂ \\end{bmatrix} $$ If the camera-film coords x∈[0,W] and y∈[0,H] are not scaled to [-1,1], the scaling factors 2/W and 2/H won\u0026rsquo;t exist.\nThus, the Jacobian evaluated at center 𝐭ₖ in the camera space is:\n$$ 𝐉_{𝐭ₖ} = \\begin{bmatrix} fₓ/t_{k,z} \u0026 0 \u0026 -fₓ t_{k,x} / {t_{k,z}}^2 \\\\\\ 0 \u0026 f_y/t_{k,z} \u0026 -f_y t_{k,y} / {t_{k,z}}^2 \\\\\\ t_{k,x}/‖𝐭ₖ‖₂ \u0026 t_{k,y}/‖𝐭ₖ‖₂ \u0026 t_{k,z}/‖𝐭ₖ‖₂ \\end{bmatrix} $$ By expressing the projective transformation ϕₖ() with the camera coordinates 𝐭, the relation between ray-space coordinates and camera-space coordinates is constructed. Thereby, the derivative of Gaussian center ϕₖ(𝐭ₖ) in the ray space is derived from camera-space coordinates 𝐭ₖ, with the clip coordinates 𝐭ₖ\u0026rsquo; skipped.\nFor the case of 2D projection, only x and y dimensions of the covariance matrix need consideration, with the 3rd row and column are omitted. Because of the affine approximation, the projective transformation ϕ(𝐭\u0026rsquo;) becomes a linear operation. Thereby, a 3D Gaussian after perspective division is a 2D Gaussian.\nFiguratively, points surrounding the 3D Gaussian center in clip space will fall into an ellipse on the 2D screen. In the 3DGS code, the 2D ellipse is further simplified as a circle to count the overlapped tiles. The covariance matrix 𝚺ₖ\u0026rsquo; of the 2D Gaussian in the pixel space corresponding to the 3D Gaussian (𝚺ₖ) in the world space can be derived based on properties of Gaussians as:\n$$\\bm Σₖ' = 𝐉ₖ 𝐑_{w2c} \\bm Σₖ 𝐑_{w2c}ᵀ 𝐉ₖᵀ$$ Because covariance matrix 𝚺\u0026rsquo; is symmetric, it can be decomposed to a stretching vector (diagonal matrix) and a rotation matrix by SVD, analogous to describing the configurations of an ellipsoid ^3DGS. (Essentially, the covariance matrix depicts a data distribution.)\nThe rotation matrix is converted to a quaternion during optimization.\nIn summary: A 3D Gaussian (𝛍ₖ,𝚺ₖ) in world (object) space is transformed into 3D ray space (or the screen with the 3rd dim omitted), resulting in:\nMean vector: $\\bm μₖ' = ϕ(𝐭ₖ')$, where $𝐭ₖ' = 𝐏⋅ [^{𝐑_{w2c} \\ 𝐭_{w2c}}_{0 \\quad\\ 1}] ⋅[^{\\bm μₖ}_1]$ is clip coordinates.\nCovariance matrix: $\\bm Σₖ' = 𝐉ₖ⋅ 𝐑_{w2c}⋅ \\bm Σₖ⋅ 𝐑_{w2c}ᵀ⋅ 𝐉ₖᵀ$\nA point 𝐭\u0026rsquo; in clip space within the Gaussian distribution is projected to ray space: $ϕₖ(𝐭') = ϕ(𝐭ₖ') + 𝐉ₖ⋅(𝐭' - 𝐭ₖ')$. The discrepancy between the approximated and the real projected locations is $ϕₖ(𝐭')-ϕ(𝐭')$.\nS c r e ⬯ 𝛍 e 𝐱 ' n , 𝚺 ' C l i 𝛍 p 𝐭 ᶜ ' , s 𝚺 p ᶜ a c e O s b p j a e c ⬯ 𝐗 𝛍 c e , t 𝚺 Rasterizing Sorting Kernels (2024-01-09)\nSort Gaussians within each 16x16 tile based on depth\nEvery pixel has a perpendicular dot line, and the discs intersected with the line are visible to the pixel.\nThe depths of those discs are $x₂ = ‖𝐭'‖₂ = \\sqrt{ {t₀'}² + {t₁'}² + {t₂'}²}$, L2 norm of the clip coordinates.\nThe disc closer to the screen is more prominent.\nDifferent images are obtained given different viewing rays, as the opacities of the discs change in tandem with viewing rays. (Specifically, the opacity of a disc is an integral for the Gaussian in the 3D ray space along a viewing ray.)\nIn contrast, volume rendering method changes point-wise colors on different viewing rays.\n(2024-01-22)\nIn splatting, image formation still relies on alpha compositing. Distinct from NeRF where a pixel-ray originates from the camera optical center, in splatting, a pixel emits a perpendicular ray from the screen. And it is the incoming viewing rays determine varying alpha (opacity) of the filters on the pixel-ray path. Such that the screen displays diverse colors with various viewing rays.\n(2024-04-20)\nI think the \u0026ldquo;screen\u0026rdquo; is indeed the camera film. The \u0026ldquo;viewing ray\u0026rdquo; doesn\u0026rsquo;t hit the screen\u0026quot;. The viewing ray is just required to pass through 3D Gaussians to calculate each Gaussian\u0026rsquo;s opacity by integrating the 3D Gaussian over the intersecting segment.\nRendering a pixel in the splatting method also emitting a ray from a pixel and compositing discs on the ray. The difference is that the opacity has been precomputed by splatting procedure: integrating the viewing ray.\nThus, it is the screen, i.e., pixels that shoot lines.\nIn the implementation of 3DGS, the splatting process is omitted, since the opacity of each Gaussian is accquired by optimizing it iteratively.\nSplatting is just one of the ways to get the opacity. As long as the opacities are obtained, any rendering method can be applied to form an image, e.g., rasterization, ray marching/tracing.\nAlpha Blending (2024-01-05)\nEWA splatting equation for N kernels existing in the space:\n$$ \\underset{\\substack{↑\\\\\\ \\text{Pixel}\\\\\\ \\text{color}}}{C} = ∑_{k∈N} \\underset{\\substack{↑\\\\\\ \\text{Kernel}\\\\\\ \\text{weight}}}{wₖ}⋅ \\underset{\\substack{↑\\\\\\ \\text{Kernel} \\\\\\ \\text{color}}}{cₖ}⋅ \\underset{\\substack{↑\\\\\\ \\text{Accumulated} \\\\\\ \\text{transmittance}}}{oₖ}⋅ (\\underset{\\substack{↑\\\\\\ \\text{Kernel}\\\\\\ \\text{opacity}}}{qₖ}⊗ \\underset{\\substack{↑\\\\\\ \\text{Loss-pass}\\\\\\ \\text{filter}}}{h}) (\\underset{\\substack{↑\\\\\\ \\text{2D} \\\\\\ \\text{coords}}}{𝐱}) $$In 3DGS, each 3D Gassian in the object space has 4 learnable parameters:\nColor (cₖ): SH for \u0026ldquo;directional appearance component of the radiance field\u0026rdquo;\nOpacity ($qₖ⊗ h$). It results in accu. transmittance oₖ as $∏_{m≤k}(1-\\text{opacity}ₘ)$.\nPosition 𝛍: determines the kernel\u0026rsquo;s weight wₖ, as wₖ is an evaluation of the projected kernel (2D Gaussian distribution) at the pixel.\nCovariance matrix: the stretching matrix and rotaion matrix (quaternion) jointly determine wₖ as well.\nThe splatting equation can be reformulated as alpha compositing used in 3DGS:\n$$C = ∑_{n≤N} Tₙ⋅αₙ⋅cₙ, \\text{ where } Tₙ = ∏_{m \u003c n}(1-αₘ)$$ Alpha can be expressed with sigma, which is an exponent of e, akin to NeRF.\n$$αₙ = oₙ ⋅ \\exp(-σₙ), \\text{where } σₙ = ½\\bm Δₙᵀ \\bm Σ'⁻¹ \\bm Δₙ$$ oₙ is a kernel\u0026rsquo;s opacity, i.e., the above qₖ⊗ h. And the negative exponential term is a scalar scaling factor. σₙ is Mahalanobis distance.\n(2024-02-16)\nGaussian\u0026rsquo;s opacity oₙ is fixed after splatting with a specific viewing ray, so during alpha compositing, the variation in alpha among different Gaussian results from the different positions of a target rendering pixel relative to various Gaussians.\nWhen performing alpha compositing, the alpha of a Gaussian is the Gaussian\u0026rsquo;s opacity scaled by the \u0026ldquo;probability\u0026rdquo; for the position of the rendering pixel in the Gaussian distribution.\nHowever, the alpha value in NeRF is $αᵢ = 1- exp(-σᵢδᵢ)$ and serves as the opacity in $∑ᵢ₌₁ᴺ Tᵢ αᵢ cᵢ$. Alpha is a converted point opacity ranging in [0,1].\nIn other words, alpha $αₙ$ equals a kernel\u0026rsquo;s opacity oₙ scaled by a weight Gₙ (the above wₖ), i.e., the evaluation of 2D Gaussian Gₙ at the viewing pixel:\n$$αₙ = oₙ ⋅ Gₙ, \\text{where } Gₙ = e^{-\\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σ'}}$$ oₙ is the opacity (the footprint qₙ) of the n-th 3D Gaussian. In EWA splatting, the qₙ is an integral of the Gaussian in the 3D ray space along the viewing ray: $qₙ(𝐱) = ∫rₙ'(𝐱,x₂)dx₂$.\noₙ in 3DGS will get optimized directly via gradient descent during training.\nGₙ is a 2D Gaussian with the normalization factor omitted. Its mean and covariance matrix will get optimized.\nΔₙ is the displacement of a pixel center from the 2D Gaussian\u0026rsquo;s mean.\nInitially, the opacity of an arbitrary 3D location in the object space is considered as an expectation of the contributions from all 3D Gaussians on that location.\nAfter performing 3 steps: ❶ substituting the perspectively projected (\u0026ldquo;squashed\u0026rdquo;) kernel within the 3D ray space into the rendering equation, ❷ switching the sequence of integral and expectation, ❸ and applying simplifying assumptions, the opacity becomes a 1-D (depth) integral along the viewing ray for each kernel in the 3D ray space.\nIn summary, the changes of opacity before and after perspective projection:\nAspect Original form Post-projection Venue Object space 3D ray space, or screen Intuition Opacity combination Discs stack Scope Location-wise Ellipsoid-wise Operation Expectation Integral Formula $f_c(𝐮) = ∑_{k≤N} wₖ rₖ(𝐮)$ Footprint $qₖ(𝐱) = ∫_{x₂=0}^L rₖ'(𝐱,x₂) dx₂$ Basis Gauss. mixture Scene locality Locality: 3D positions are grouped into different ellipsoids. (2024-01-06) A 3D Gaussian (datapoint) in the camera space (or clip space) is \u0026ldquo;perspectively\u0026rdquo; projected (thrown) onto the screen (or the ray space, as its x2 is independtly assigned beside the screen coordinates x0,x1), and results in a 2D Gaussian (with applying the Taylor expansion to approximate the nonlinear perspective effects).\nThe viewing ray in camera space will be projected into the 3D ray space remaining a straight line segment (due to the linear approximation), and then the 3D line is projected onto the screen orthogonally.\nOrthogonal projection is because the 3D Gaussians have already been projected onto the screen (the location has been determined as x,y divided by z and covariance matrix 𝚺\u0026rsquo;= 𝐉𝐖 𝚺𝐖ᵀ 𝐉ᵀ) yielding 2D Gaussians, so each pixel is only derived from those projected kernels (2D Gaussians) that overlaps with it (\u0026ldquo;Overlapping\u0026rdquo; refer to 3DGS.), like a stack of filters in alpha compositing. That implies the alpha compositing is performed in the screen space, or the ray sapce, as the ray space is equivalent with screen (EWA paper: \u0026ldquo;the transformation from volume-data to ray space is equivalent to perspective projection.\u0026rdquo;).\nA p i ■ x e → l = = d k S i e u s ■ ↑ ⬭ n m c l 1 1 o f ✚ 2 d k D i e s ■ ↑ ⬯ n G c l a 2 2 u s s ✚ ⋯ i a n d k s i e s ■ ↑ ⬭ n O c l v N N e r l a o p ( m ( p 2 i 3 i D t D n ↑ g S \" R c d a w r e y i e p t e t s h n h p ) \" a i c t x e ₂ ) With the \u0026ldquo;orthogonal correspondence\u0026rdquo; between the ray space and the screen, the ray integral (footprint function, or kernel\u0026rsquo;s opacity) in the 3D ray space becomes (??Not sure) an integral on the 2D screen plane, i.e., an integral of a 2D Gaussian.\nAnd the ray in the 3D ray space corresponds to a line on the screen, as rays in the 3D ray space are parallel (i.e., orthogonal projection). Thus, the opacity is an\nAlpha of an arbitrary point in screen (or 3D ray space) is a 2D-Gaussian mixture over all kernels. (Not the screen, object space is opacities combination, whereas the screen space is filter stacking.)\nThe alpha of a 3D location is calculated in each 3D Gaussian based on the distance to the center. And the final alpha on the location is the expectation of all the evaluations. (No location-wise alpha was calculated.)\nThe color of a pixel on the screen is a 2D-Gaussian mixture:\nG a i 2 u a D s n s s S - c r e e n ∫ D V e i V p d e o t i w l h f i . \" u f n ~ m h e g e e r r e r d e n a a t y t i a s v 3 e D h ‖ i • a 𝐭 w ( s ' i V ‖ n i b ₂ g e e w e f r i n o a n r y g ~ t s ) . \" h c . r a R o l C a w c o y n u v l s o a m p . n t a a \" • \" t i t c . o n r e g i t x h o e p i P p a s e r s c r o c i 𝚺 s j r t ' p e y ₃ e e ₓ c n g ₃ C S . i . a p ⬭ v m a e e c n r e a ⬯ The alpha compositing process for a pixel is illustrated below:\nOpacities (o₁,o₂,o₃) of different kernels are various-length integral along the viewing ray in the 3D ray space.\nNot sure whether the integral in 3D ray space equal the integral on screen. Weight (w₁,w₂,w₃) of a kernel\u0026rsquo;s opacity is its evaluation at the pixel.\nAlpha (α₁,α₂,α₃) of a kernel equals its opacity multiplied with its weight.\nAccumulated transmittance (T₁,T₂,T₃) equals the product of previously passed kernels\u0026rsquo; transmittance.\nPixel color is the sum of each kernel\u0026rsquo;s color scaled by alpha.\nThere is no volume rendering as there is no sampling points on the viewing ray. Pixel is a summation of visible 2D discs (referring to 3DGS). Only alphas of the explicitly existent discs require to be computed, unlike volume rendering where every sampling location need to compute alpha.\nGradient wrt Composite (2023-01-07)\nS P k 3 p r e D l o r a j n r t e e a t ⋯ c l y i t s n ⋯ e s g ⋯ d i p : ⋯ ⋯ n a ⋯ c U ⋯ ⋯ e s c ⋯ ⋯ e o ⋯ m ⋯ ⋯ ⋯ V p ⋯ i u ⋯ ⋯ e t w e i n o g p a r c a i y t i t e o s F F d 𝛍 o r i ' c r o s ■ ₁ ₁ w n c , , a t 1 𝚺 o r ' ₁ d ✚ ₁ : d r i 𝛍 e s ■ ' c n B c ₂ ₂ d a 2 , , e c 𝚺 o r k ' ₂ ✚ ⋯ ₂ p ( i f x i d e r i 𝛍 l s s ■ ' c t c ₙ ₙ c N , , o b 𝚺 o l e = ' ₙ o p ₙ r s i ■ e x e n ) The dot lines in the left figure represent orthogonal correspondence, not projection. A pixel can only see 2D Gaussians located on its perpendicular line. Those visible 2D Gaussians to a pixel are sorted based on depth, and then their colors are composited from near to far with multiplying with their opacities that computed as an integral along the viewing ray. A synthesized pixel is a weighted sum of the related (overlapping) kernels\u0026rsquo; color in the whole space:\n$$C_{pred} = ∑_{n≤N} Tₙ⋅αₙ⋅cₙ, \\quad \\text{where } αₙ=oₙ⋅e^{-\\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σ'}}$$Loss:\n$$L = ‖C_{targ} - C_{pred}‖₂$$The paper used Frobenius product to analyze. A Frobenius inner product is like a linear layer:\n$$⟨𝐗,𝐘⟩ = \\operatorname{vec}(𝐗)ᵀ \\operatorname{vec}(𝐘)$$ 1 2 3 fc = nn.Linear(3, 10) # fc.weight is (10,3) x = torch.range(2,3) fc(x) # X⋅Wᵀ = (2,3)⋅(3,10) = (2,10) is ⟨Xᵀ,Wᵀ⟩ Chain rule:\nx A ₘ ₓ ₚ ( x ) Y ₚ ₓ ₙ ( x ) X ₘ ₓ ₙ f $$\\begin{aligned} \\frac{∂f}{∂x} \u0026= \\frac{∂f}{∂X}⋅\\frac{∂X}{∂AY}⋅\\frac{∂AY}{∂x} \\\\\\ \u0026= \\frac{∂f}{∂X}⋅\\frac{∂X}{∂AY}⋅(\\frac{∂A}{∂x}Y + A\\frac{∂Y}{∂x})\\\\\\ \\end{aligned}$$ Since the passed kernels on the ray path (starting from a pixel) have influences on the next kernel\u0026rsquo;s contribution, which is scaled by the previous accumulated transmittance, the order of solving derivatives should start from the most rear kernel, and then sequentially calculate the derivatives of backer kernels in the reverse order of the forward pass.\nS G p r s l F F a o a o r d l t r N o ⬮ v t w n - f e i a t - i d n r ⬮ r g d - s : : - t ⬮ V P - i i - e x ⬮ w e - T i i l - h n n ⬮ e g r - t e B - t h r n 1 a ⬮ ↑ o e a d c p y e k = p c r e o ↘ i ■ s l n t o h g r i c t ← o s s l t o a s r c c k r e e n B ⬮ N D a a - e l c ✚ - r l k ⬮ i w - v d a ✚ - a o r ⬮ t w d - i n : ✚ - n v - ⬮ + e s s - 1 t u - c r m ⬮ n o e - m m g - e r ⬮ s c a o d = f l i r o e ■ o r n m s t s The viewing ray travels from front to back and hits the screen. But the color nearest to the screen is the first to be seen by (or shown on) the camera (or eye).\nThe toppest color is based on downstream colors, so, its a function of all the preceding colors.\nIn the color stack, the color above depends on color below. Thus, the derivatives at the bottom should be sovled first.\n(2024-01-16) The toppest color is the base of all downstream colors, so, its derivative is contributed by all the behind colors.\nColor, Opacity (2024-01-08)\nGiven $\\frac{∂L}{∂Cᵢ(k)}$, the partial derivatives of the predicting pixel color $Cᵢ$ w.r.t. each parameter of a Gaussian Gₙ (in the ray space) that contributes to the pixel are:\nThe parital derivative of Cᵢ w.r.t. the kernel Gₙ\u0026rsquo;s color cₙ, based on the forward pass: $Cᵢ = T₁⋅α₁⋅c₁+ T₂⋅α₂⋅c₂ + ⋯ Tₙ⋅αₙ⋅cₙ+ Tₙ₊₁⋅αₙ₊₁⋅cₙ₊₁ +⋯ + T_N⋅α_N⋅c_N$:\n$$\\frac{∂Cᵢ(k)}{∂cₙ(k)} = αₙ⋅Tₙ$$ k represents one channel of RGB.\nThe furthest $T_N$ from the screen is saved at the end of the forward pass. And then the $T_{N-1}$ in back of it is calculated as $T_{N-1} = \\frac{T_N}{1-α_{N-1}}$. The points in back follow this relation.\n3DGS Code\n1 2 T = T / (1.f - alpha); const float dchannel_dcolor = alpha * T; Alpha αₙ\nTo solve the partial derivative of pixel color $C_i$ w.r.t. the kernel Gₙ\u0026rsquo;s αₙ, only consider the kernels that follow Gₙ, as the transmittances of all the subsequent kernels rely on the currenct kernel Gₙ: $$Tₙ₊₁ = (1-αₙ)Tₙ$$Thereby, the behind kernels will provide derivatives to the current kernel Gₙ\u0026rsquo;s alpha.\nFor example, the color of the next kernel, Gₙ₊₁, behind Gₙ is:\n$$\\begin{aligned} Cₙ₊₁ \u0026= cₙ₊₁⋅αₙ₊₁⋅Tₙ₊₁ \\\\\\ \u0026= cₙ₊₁⋅αₙ₊₁⋅ (1-αₙ)Tₙ \\\\\\ \u0026= cₙ₊₁⋅αₙ₊₁⋅(1-αₙ)⋅ \\frac{ Tₙ₊₁}{1-αₙ} \\\\\\ \\end{aligned}$$Thus, the color $Cₙ₊₁$ contributes to the total partial derivative $\\frac{∂Cᵢ}{∂αₙ}$ with the amount: $-\\frac{cₙ₊₁⋅αₙ₊₁⋅Tₙ₊₁}{1-αₙ}$ .\nContinuously, the following color Cₙ₊₂ can be represented with αₙ:\n$$ Cₙ₊₂ = cₙ₊₂⋅αₙ₊₂⋅Tₙ₊₂ \\\\\\ = cₙ₊₂⋅αₙ₊₂ ⋅\\cancel{(1-αₙ₊₁)} (1-αₙ) \\frac{Tₙ₊₂}{ \\cancel{(1-αₙ₊₁)} (1-αₙ)}$$ Thus, $\\frac{∂Cₙ₊₂}{∂αₙ} = -\\frac{cₙ₊₂⋅αₙ₊₂⋅Tₙ₊₂}{1-αₙ}$ Similarly, the subsequent kernel Gₘ, with m\u0026gt;n, also contribute to the overall partial derivative $\\frac{∂Cᵢ}{∂αₙ}$.\nThereby, the ultimate partial derivatives of the pixel color $Cᵢ$ w.r.t. the Gₙ\u0026rsquo;s alpha αₙ is:\n$$\\frac{∂Cᵢ}{∂αₙ} = cₙ⋅Tₙ - \\frac{∑_{m\u003en}cₘ⋅αₘ⋅ Tₘ}{1-αₙ}$$ Opacity oₙ, mean 𝛍\u0026rsquo;, and covariance 𝚺':\nAccording to $αₙ = oₙ e^{-σₙ}$, where $σₙ = \\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σₙ'}$ (a scalar), and 𝚫ₙ is the offset from the pixel center to the 2D Gaussian Gₙ\u0026rsquo;s mean $\\bm μ'$, such as $\\bm Δₙ = \\bm μₙ' - 𝐱ᵢ$.\nPartial derivative of αₙ w.r.t. opacity oₙ:\n$$ \\frac{∂αₙ}{∂oₙ} = e^{-\\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σₙ'}} $$ Partial derivative of αₙ w.r.t. the \u0026ldquo;exponent\u0026rdquo; sigma $σₙ$:\n$$ \\frac{∂αₙ}{∂σₙ} = -oₙ e^{-σₙ}$$ Partial derivative of sigma σₙ w.r.t. 2D mean 𝛍ₙ':\nBecause $\\bm Δₙ$ is a function of 𝛍ₙ\u0026rsquo;, computing derivaties w.r.t. 𝛍ₙ\u0026rsquo; is equivalent to $\\bm Δₙ$.\nThe Jacobian of σₙ is:\n$$\\frac{∂σₙ}{∂\\bm μₙ'} = \\frac{∂σₙ}{∂\\bm Δₙ'} = \\frac{∂(½\\bm Δₙ'ᵀ \\bm Σ'⁻¹ \\bm Δₙ')}{∂\\bm Δₙ'} = \\bm Σₙ'⁻¹ \\bm Δₙ'$$ Partial derivative of sigma σₙ w.r.t. 2D covariance matrix 𝚺':\n$$\\frac{∂σₙ}{∂\\bm Σₙ'} =$$ Gradient wrt Projection (2024-01-10)\nS 2 c D r e ⬯ e n 𝐱 ⋯ O 𝛁 | ⋯ m 𝓛 i R ( ⋯ t b a ϕ 𝛍 a y ₖ ₖ ⋯ d c 3 ( ' k D s 𝐭 , ⬮ p p ' 𝚺 r a ) ₖ o c ' ϕ + ( p e ) ₖ D a ( 𝐉 i g 𝐭 ⋅ v a ' ( t ) 𝐭 a e | = ' p ϕ p C ( r l ( 𝛍 𝛍 o i 𝛍 ₖ ₖ x p ₖ ᶜ ᶜ . 𝐭 ᶜ ) ) ) S ' , ⬮ p 𝚺 a ₖ c ᶜ e ) | 𝐏 C a m e ( r 𝐭 a ₖ 𝐭 , ⬮ s 𝚺 p ₖ a ) c ⌈ ⌊ e 𝐑 0 ʷ | ² ᶜ O ' b 𝐭 1 j ʷ e ² c ( ᶜ t 𝛍 ⌉ ⌋ 𝐗 ₖ s , p 𝚺 ⬮ a ₖ c ) e | A 3D Gaussian distribution centered at 𝛍ₖ with a covariance matrix 𝚺ₖ in the object space will be projected onto 2D screen through the splatting step, resulting in a 2D Gaussian distribution, with applying the affine transformation approximation for the nonlinear perspective division.\nThe 3D ray space (or the screen) is constructed based on the perspective division (x,y divided by z), which however is non-linear. Therefore, the projective transformation $ϕ(𝐭')$ (i.e., perspective division + new depth) converting the clip space to the 3D ray space is approximated by a linear mapping: $ϕₖ(𝐭') = ϕ(𝐭ₖ') + 𝐉ₖ⋅(𝐭' - 𝐭ₖ')$, where 𝐭ₖ\u0026rsquo; = 𝛍ₖᶜ, the mean vector in clip space.\nThe effects of the approximated affine mapping ϕₖ(𝐭\u0026rsquo;) are as follows:\nThe transformed 2D Gaussian\u0026rsquo;s center 𝛍ₖ\u0026rsquo; is the exact projective transformation ϕ(𝛍ₖᶜ), i.e., ϕₖ(𝐭ₖ\u0026rsquo;)=ϕ(𝐭ₖ\u0026rsquo;), without any error, with the 3rd dimension omitted.\n$$ \\underset{(3D)}{\\bm μₖ'} = ϕ(\\bm μₖᶜ) = \\begin{bmatrix} f_x⋅μ_{k,x}ᶜ/μ_{k,z}ᶜ + c_x \\\\\\ f_y⋅μ_{k,y}ᶜ/μ_{k,z}ᶜ + c_y \\\\\\ \\sqrt{ {μ_{k,x}ᶜ}^2 + {μ_{k,y}ᶜ}^2 + {μ_{k,z}ᶜ}^2} \\end{bmatrix} \\overset{\\text{Omit 3rd dim}}{\\longrightarrow} \\underset{(2D)}{\\bm μₖ'}= \\begin{bmatrix} \\frac{f_x⋅μ_{k,x}ᶜ}{μ_{k,z}ᶜ} + c_x \\\\\\ \\frac{f_y⋅μ_{k,y}ᶜ}{μ_{k,z}ᶜ} + c_y \\end{bmatrix} $$ However, the approximated transformation ϕₖ(𝐭\u0026rsquo;) of an arbitrary point 𝐭\u0026rsquo; around the clip-space 3D Gaussian center 𝐭ₖ' will deviate from the precise perspective projections ϕ(𝐭\u0026rsquo;) gradually, as the (𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;) increases in the approximated mapping:\n$$ \\begin{aligned} ϕ(𝐭') ≈ ϕₖ(𝐭') \u0026= ϕ(𝐭ₖ') + 𝐉ₖ⋅(𝐭' - 𝐭ₖ') \\\\\\ \u0026= \\bm μₖ' + 𝐉ₖ⋅(𝐭' - 𝐭ₖ') \\end{aligned} $$ The projected 2x2 covariance matrix on screen is the 3x3 matrix in the ray space: $\\bm Σₖ' = 𝐉ₖ⋅ 𝐑_{w2c}⋅ \\bm Σₖ⋅ 𝐑_{w2c}ᵀ⋅ 𝐉ₖᵀ$, with the 3rd row and column omitted.\n(2024-01-11)\n⭐Note: The following $𝐭ₖ$ is the coordinates of a Gaussian center in the camera space:\n$$𝐭ₖ = [^{𝐑_{w2c} \\ 𝐭_{w2c}}_{0 \\quad\\ 1}] ⋅[^{\\bm μₖ}_1]$$ where 𝛍ₖ³ᕽ¹ is the coordinates of the mean vector in world space.\n$𝐭'$ is the clip coordinates, which is camera coordinates times the projection matrix 𝐏: $𝐭' = 𝐏𝐭$\n𝐏 maps the camera-space coordinates to camera film (homogeneous) and scales for letting the ND Coordinates of points located within the camera frustum range in [-1,1]. With using clip coordinates, points whose w (i.e., z) is smaller than x,y,z are deleted.\nThe approximated projective transformation $ϕₖ(𝐭')$ fulfills perspective division after frustum clipping. Therefore, the 2D screen coordinates ${\\bm μₖ'}_{(2D)} = ϕₖ(𝐭ₖ')_{(2D)}$ are NDC ∈ [-1,1].\nThen the Gaussian center\u0026rsquo;s ND coordinates are scaled back to the screen size, yielding pixel coordinates 𝛍ₖ\u0026rsquo;₍ₚᵢₓ₎ represented with clip coordinates as:\n$$ {𝛍ₖ'}_{(\\text{pix})} = \\begin{bmatrix} (W⋅t_{k,x}'/t_{k,w}' + 1) /2 + c_x \\\\\\ (H⋅t_{k,y}'/t_{k,w}' + 1) / 2 + c_y \\end{bmatrix} $$This relationship enables propagating gradients from pixel coordinates $\\bm μₖ'₍ₚᵢₓ₎$ to the clip coordinates $𝐭ₖ'$ directly, without the 2D screen coordinates ${\\bm μₖ'}_{(2D)}$ involved.\nCenter (2024-01-12)\nBecause ellipse center 𝛍ₖ\u0026rsquo;₍ₚᵢₓ₎ and covariance 𝚺ₖ\u0026rsquo; on the 2D screen both are functions of 3D Gaussian center 𝐭ₖ, the partial derivatives of the loss w.r.t. 𝐭ₖ³ᕽ¹ is a sum:\n$$ \\frac{∂L}{∂𝐭ₖ} = \\frac{∂L}{∂ \\bm μₖ'₍ₚᵢₓ₎} \\frac{∂\\bm μₖ'₍ₚᵢₓ₎}{∂𝐭ₖ} + \\frac{∂L}{∂ {\\bm Σₖ'}_{(2D)}} \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂𝐭ₖ} \\\\\\ $$ The partial derivative of 2D Gaussian\u0026rsquo;s mean $\\bm μₖ'₍ₚᵢₓ₎$ w.r.t. the camera coordinates of 3D Gaussian center 𝐭ₖ:\n$$ \\begin{aligned} \\frac{∂\\bm μₖ'₍ₚᵢₓ₎}{∂𝐭ₖ} \u0026= \\frac{∂\\bm μₖ'₍ₚᵢₓ₎}{∂ {\\bm μₖ'}_{(2D)}} ⋅ \\frac{∂ {ϕₖ(𝐭ₖ')}_{(2D)}}{𝐭ₖ'} ⋅ \\frac{∂𝐭ₖ'}{∂𝐭ₖ} \\qquad \\text{(Full process)} \\\\\\ \u0026= \\frac{∂\\bm μₖ'₍ₚᵢₓ₎}{∂𝐭ₖ'} ⋅ \\frac{∂𝐭ₖ'}{∂𝐭ₖ} \\qquad \\text{(Clip→Pix, skip screen coords)} \\\\\\ \u0026= \\frac{1}{2} \\begin{bmatrix} W/t_{k,w}' \u0026 0 \u0026 0 \u0026 -W⋅t_{k,x}'/{t_{k,w}'}^2 \\\\\\ 0 \u0026 H/t_{k,w}' \u0026 0 \u0026 -H⋅t_{k,y}'/{t_{k,w}'}^2 \\end{bmatrix}⋅ 𝐏 \\end{aligned} $$Based on the properties of the Frobenius inner product, eq. (23) is obtained.\nThe partial derivative of the 2D Gaussian\u0026rsquo;s covariance 𝚺ₖ\u0026rsquo; w.r.t. the camera coordinates of 3D Gaussian center 𝐭ₖ:\n$$ \\frac{∂\\bm Σₖ'}{∂𝐭ₖ} = \\frac{∂(𝐉ₖ⋅ 𝐑_{w2c}⋅ \\bm Σₖ⋅ 𝐑_{w2c}ᵀ⋅ 𝐉ₖᵀ)_{2D} }{∂𝐭ₖ} $$(2024-01-13) Derivation refers to 3D Gaussian Splatting中的数学推导 - 八氨合氯化钙的文章 - 知乎\nLetting $𝐔 = 𝐉ₖ⋅ 𝐑_{w2c}$ (3DGS code refers to it as T.), the Gaussian covariance 𝚺ₖ\u0026rsquo; in the 3D ray space derived from the projective transformation ϕₖ(𝐭\u0026rsquo;) is:\n$$\\bm Σₖ' = 𝐔 ⋅ \\bm Σₖ⋅𝐔ᵀ = \\\\\\ \\begin{bmatrix} U₁₁ \u0026 U₁₂ \u0026 U₁₃ \\\\\\ U₂₁ \u0026 U₂₂ \u0026 U₂₃ \\\\\\ U₃₁ \u0026 U₃₂ \u0026 U₃₃ \\end{bmatrix} \\begin{bmatrix} σ₁₁ \u0026 σ₁₂ \u0026 σ₁₃ \\\\\\ σ₂₁ \u0026 σ₂₂ \u0026 σ₂₃ \\\\\\ σ₃₁ \u0026 σ₃₂ \u0026 σ₃₃ \\end{bmatrix} \\begin{bmatrix} U₁₁ \u0026 U₂₁ \u0026 U₃₁ \\\\\\ U₁₂ \u0026 U₂₂ \u0026 U₃₂ \\\\\\ U₁₃ \u0026 U₂₃ \u0026 U₃₃ \\end{bmatrix} = \\\\\\ \\begin{bmatrix} U₁₁ \u0026 U₁₂ \u0026 U₁₃ \\\\\\ U₂₁ \u0026 U₂₂ \u0026 U₂₃ \\\\\\ U₃₁ \u0026 U₃₂ \u0026 U₃₃ \\end{bmatrix} \\begin{bmatrix} \\boxed{σ₁₁}U₁₁+σ₁₂U₁₂+σ₁₃U₁₃ \u0026 \\boxed{σ₁₁}U₂₁+σ₁₂U₂₂+σ₁₃U₂₃ \u0026 \\boxed{σ₁₁}U₃₁+σ₁₂U₃₂+σ₁₃U₃₃ \\\\\\ σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃ \u0026 σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃ \u0026 σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃ \\\\\\ σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃ \u0026 σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃ \u0026 σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃ \\end{bmatrix} = \\\\\\ \\Big[ \\begin{array}{c|c|c} U₁₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₁₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₁₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026 U₁₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₁₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₁₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \u0026 U₁₁(σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃) + U₁₂(σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃) + U₁₃(σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃) \\\\\\ U₂₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₂₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₂₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026 U₂₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₂₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₂₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \u0026 U₂₁(σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃) + U₂₂(σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃) + U₂₃(σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃) \\\\\\ U₃₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₃₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₃₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026 U₃₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₃₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₃₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \u0026 U₃₁(σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃) + U₃₂(σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃) + U₃₃(σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃) \\end{array} \\Big] $$The 3rd row and column of the 𝚺ₖ\u0026rsquo; (in the 3D ray space) are omitted due to the orthogonal correspondence between the 3D ray space and the 2D screen (\u0026quot;⟼\u0026quot;). Thus, ${\\bm Σₖ'}_{(2D)}$ is only the upper-left 2×2 elements of the 3D 𝚺ₖ\u0026rsquo;, contributing to the gradient of 2D loss $L(𝐜ₖ, oₖ, {\\bm μₖ'}_{(2D)}, {\\bm Σₖ'}_{(2D)})$, while the remaining 5 elements of 𝚺ₖ\u0026rsquo;₍₃ₓ₃₎ make no contributions.\n$$ {\\bm Σₖ'}_{(2D)} = \\\\\\ \\left[ \\begin{array}{c|c} U₁₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₁₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₁₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026 U₁₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₁₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₁₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \\\\\\ U₂₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₂₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₂₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026 U₂₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₂₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₂₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \\end{array} \\right] $$ Each element of ${\\bm Σₖ'}_{(2D)}$ is a \u0026ldquo;sub-\u0026rdquo; function, which is taken derivative w.r.t. each variable: σ₁₁, σ₁₂, σ₁₃, σ₂₂, σ₂₃, σ₃₃, to backpropagate the gradient $\\frac{∂L}{∂{\\bm Σₖ'}_{(2D)}}$ to 𝚺ₖ. (Only these 6 elements of 𝚺ₖ need computation as 𝚺ₖ₍₃ₓ₃₎ is symmetric.)\nIt\u0026rsquo;s not proper to think of the derivative of a \u0026ldquo;matrix\u0026rdquo; w.r.t. a matrix. Instead, it\u0026rsquo;s better to consider the derivative of a function w.r.t. variables, as essentially a matrix stands for a linear transformation.\nThe partial derivative of ${\\bm Σₖ'}_{(2D)}$ w.r.t. $\\bm Σₖ$ (the 3D covariance matrix in world space):\n$$ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₁₁} = \\begin{bmatrix} U₁₁U₁₁ \u0026 U₁₁U₂₁ \\\\\\ U₂₁U₁₁ \u0026 U₂₁U₂₁ \\end{bmatrix} = \\begin{bmatrix} U₁₁ \\\\\\ U₂₁ \\end{bmatrix} \\begin{bmatrix} U₁₁ \u0026 U₂₁ \\end{bmatrix} \\\\\\ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₁₂} = \\begin{bmatrix} U₁₁U₁₂ \u0026 U₁₁U₂₂ \\\\\\ U₂₁U₁₂ \u0026 U₂₁U₂₂ \\end{bmatrix} = \\begin{bmatrix} U₁₁ \\\\\\ U₂₁ \\end{bmatrix} \\begin{bmatrix} U₁₂ \u0026 U₂₂ \\end{bmatrix} \\\\\\ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₁₃} = \\begin{bmatrix} U₁₁U₁₃ \u0026 U₁₁U₂₃ \\\\\\ U₂₁U₁₃ \u0026 U₂₁U₂₃ \\end{bmatrix} = \\begin{bmatrix} U₁₁ \\\\\\ U₂₁ \\end{bmatrix} \\begin{bmatrix} U₁₃ \u0026 U₂₃ \\end{bmatrix} \\\\\\ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₂₂} = \\begin{bmatrix} U₁₂U₁₂ \u0026 U₁₂U₂₂ \\\\\\ U₂₂U₁₂ \u0026 U₂₂U₂₂ \\end{bmatrix} = \\begin{bmatrix} U₁₂ \\\\\\ U₂₂ \\end{bmatrix} \\begin{bmatrix} U₁₂ \u0026 U₂₂ \\end{bmatrix} \\\\\\ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₂₃} = \\begin{bmatrix} U₁₂U₁₃ \u0026 U₁₂U₂₃ \\\\\\ U₂₂U₁₃ \u0026 U₂₂U₂₃ \\end{bmatrix} = \\begin{bmatrix} U₁₂ \\\\\\ U₂₂ \\end{bmatrix} \\begin{bmatrix} U₁₃ \u0026 U₂₃ \\end{bmatrix} \\\\\\ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₃₃} = \\begin{bmatrix} U₁₃U₁₃ \u0026 U₁₃U₂₃ \\\\\\ U₂₃U₁₃ \u0026 U₂₃U₂₃ \\end{bmatrix} = \\begin{bmatrix} U₁₃ \\\\\\ U₂₃ \\end{bmatrix} \\begin{bmatrix} U₁₃ \u0026 U₂₃ \\end{bmatrix} $$ σ₁₁, σ₂₂, σ₃₃ are on the diagonal, while σ₁₂, σ₁₃, σ₂₃ are off-diagonal. The partial derivative of the loss L w.r.t. each element of 𝚺ₖ:\n$$ \\begin{aligned} \\frac{∂L}{∂ {\\bm Σₖ'}_{(2D)}} \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₁₁} \u0026= ∑_{row}∑_{col}{ \\begin{bmatrix} \\frac{∂L}{∂a} \u0026 \\frac{∂L}{∂b} \\\\\\ \\frac{∂L}{∂b} \u0026 \\frac{∂L}{∂c} \\end{bmatrix} ⊙ \\begin{bmatrix} U₁₁U₁₁ \u0026 U₁₁U₂₁ \\\\\\ U₂₁U₁₁ \u0026 U₂₁U₂₁ \\end{bmatrix} } \\\\\\ \u0026= \\frac{∂L}{∂a} U₁₁U₁₁ + 2× \\frac{∂L}{∂b} U₁₁U₂₁ + \\frac{∂L}{∂c} U₂₁U₂₁ \\\\\\ \\frac{∂L}{∂ {\\bm Σₖ'}_{(2D)}} \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₁₂} \u0026= ∑_{row}∑_{col}{ \\begin{bmatrix} \\frac{∂L}{∂a} \u0026 \\frac{∂L}{∂b} \\\\\\ \\frac{∂L}{∂b} \u0026 \\frac{∂L}{∂c} \\end{bmatrix} ⊙ \\begin{bmatrix} U₁₁U₁₂ \u0026 U₁₁U₂₂ \\\\\\ U₂₁U₁₂ \u0026 U₂₁U₂₂ \\end{bmatrix} } \\\\\\ \u0026= \\frac{∂L}{∂a} U₁₁U₁₂ + \\frac{∂L}{∂b} U₁₁U₂₂ +\\frac{∂L}{∂b}U₂₁U₁₂ + \\frac{∂L}{∂c} U₂₁U₂₂ \\end{aligned} $$ ⊙ is Hadamard product (element-wise product). $∑_{row}∑_{col}$ means summation of all elements in the matrix.\n(2024-02-17) In this step, the derivative w.r.t. a matrix is determined by calculating the derivative w.r.t. each element individually, rather than the entire matrix. Thus, the multiplication between two \u0026ldquo;derivative matrices\u0026rdquo; is hadamard product, as essentially it\u0026rsquo;s the derivative w.r.t. a single scalar (in contrast to vector or matrix). However, for example, if $\\frac{∂\\bm Σ}{∂𝐌}$ is the derivative of 𝚺 w.r.t. the matrix 𝐌, the multiplication with the incoming upstream \u0026ldquo;derivative matrix\u0026rdquo; should be a normal matmul.\nWithin a chain of differentiation, the 2 manners of solving derivative for a matrix by computing the derivative for the entire matrix or calculating the derivative for each element can coexist simultaneously.\nNote: The $\\frac{∂L}{∂b}$ in the 3DGS code has been doubled. And each off-diagonal element is multiplied by 2, as the symmetrical element has the same gradient contribution.\nIn 3DGS code, the derivative of loss w.r.t. each element of 3D covariance matrix 𝚺ₖ₍₃ₓ₃₎ in the world space is computed individually:\n1 2 3 4 5 6 7 8 9 10 11 dL_da = denom2inv * (...); dL_dc = denom2inv * (...); dL_db = denom2inv * 2 * (...); dL_dcov[0] = (T[0][0]*T[0][0]*dL_da + T[0][0]*T[1][0]*dL_db + T[1][0]*T[1][0]*dL_dc); dL_dcov[3] = (T[0][1]*T[0][1]*dL_da + T[0][1]*T[1][1]*dL_db + T[1][1]*T[1][1]*dL_dc); dL_dcov[5] = (T[0][2]*T[0][2]*dL_da + T[0][2]*T[1][2]*dL_db + T[1][2]*T[1][2]*dL_dc); dL_dcov[1] = 2*T[0][0]*T[0][1]*dL_da + (T[0][0]*T[1][1] + T[0][1]*T[1][0])*dL_db + 2*T[1][0]*T[1][1]*dL_dc; dL_dcov[2] = 2*T[0][0]*T[0][2]*dL_da + (T[0][0]*T[1][2] + T[0][2]*T[1][0])*dL_db + 2*T[1][0]*T[1][2]*dL_dc; dL_dcov[4] = 2*T[0][2]*T[0][1]*dL_da + (T[0][1]*T[1][2] + T[0][2]*T[1][1])*dL_db + 2*T[1][1]*T[1][2]*dL_dc; The 2D covariance matrix ${\\bm Σₖ'}_{(2D)}$ is Not equivalent to the calculation where the 3rd row and column of 𝚺ₖ are omitted from the beginning, because σ₁₃, σ₂₃, σ₃₃ are also involved in the projected covariance 𝚺ₖ\u0026rsquo;. However, the derivatives w.r.t. them ($\\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₁₃},\\ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₂₃},\\ \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂σ₃₃}$) can\u0026rsquo;t be derived from the following expression:\n$$ \\begin{aligned} {\\bm Σₖ'}_{(2D)} \u0026= \\begin{bmatrix} U₁₁ \u0026 U₁₂ \\\\\\ U₂₁ \u0026 U₂₂ \\end{bmatrix} \\begin{bmatrix} σ₁₁ \u0026 σ₁₂ \\\\\\ σ₂₁ \u0026 σ₂₂ \\end{bmatrix} \\begin{bmatrix} U₁₁ \u0026 U₂₁ \\\\\\ U₁₂\u0026 U₂₂ \\end{bmatrix} \\\\\\ \u0026= \\begin{bmatrix} (U₁₁σ₁₁ + U₁₂σ₂₁)U₁₁ + (U₁₁σ₁₂ + U₁₂σ₂₂) U₁₂ \u0026 (U₁₁σ₁₁ + U₁₂σ₂₁)U₂₁ + (U₁₁σ₁₂ + U₁₂σ₂₂) U₂₂ \\\\\\ (U₂₁σ₁₁ + U₂₂σ₂₁)U₁₁ + (U₂₁σ₁₂ + U₂₂σ₂₂) U₁₂ \u0026 (U₂₁σ₁₁ + U₂₂σ₂₁)U₂₁ + (U₂₁σ₁₂ + U₂₂σ₂₂) U₂₂ \\end{bmatrix} \\end{aligned} $$ The partial derivative of the 2D Gaussian covariance matrix ${\\bm Σₖ'}_{(2D)}$ w.r.t. the 3D Gaussian center 𝐭ₖ in the camera space:\n$$ \\begin{aligned} \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂𝐔ₖ} \\frac{∂𝐔ₖ}{∂𝐭ₖ} = \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂𝐔ₖ} \\frac{∂(𝐉ₖ⋅ 𝐑_{w2c})}{∂𝐭ₖ} \\\\\\ \\end{aligned} $$ $\\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂𝐔ₖ}$ (corresponding to $\\frac{∂\\bm Σₖ'}{∂T}$ inside $\\frac{∂L}{∂T}$ of eq.(25) in the gsplat paper.)\n$$ \\begin{aligned} \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₁₁} \u0026= \\begin{bmatrix} 2σ₁₁U₁₁+(σ₁₂+σ₂₁)U₁₂+(σ₁₃+σ₃₁)U₁₃ \u0026 σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃ \u0026 σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃ \\\\\\ σ₁₁U₂₁ + σ₂₁U₂₂ + σ₃₁U₂₃ \u0026 0 \u0026 0 \\\\\\ σ₁₁U₃₁ + σ₂₁U₃₂ + σ₃₁U₃₃ \u0026 0 \u0026 0 \\end{bmatrix} \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₁₂} \u0026= \\begin{bmatrix} U₁₁σ₁₂+σ₂₁U₁₁+2σ₂₂U₁₂+σ₂₃U₁₃ +U₁₃σ₃₂ \u0026 σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃ \u0026 σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃ \\\\\\ U₂₁σ₁₂ + U₂₂σ₂₂+U₂₃σ₃₂ \u0026 0 \u0026 0 \\\\\\ U₃₁σ₁₂ + U₃₂σ₂₂+U₃₃σ₃₂ \u0026 0 \u0026 0 \\end{bmatrix} \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₁₃} \u0026= \\begin{bmatrix} U₁₁σ₁₃+U₁₂σ₂₃+σ₃₁U₁₁+σ₃₂U₁₂+2σ₃₃U₁₃ \u0026 σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃ \u0026 σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃ \\\\\\ U₂₁σ₁₃+U₂₂σ₂₃+U₂₃σ₃₃ \u0026 0 \u0026 0 \\\\\\ U₃₁σ₁₃+U₃₂σ₂₃+U₃₃σ₃₃ \u0026 0 \u0026 0 \\end{bmatrix} \\\\\\ \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₂₁} \u0026= \\begin{bmatrix} 0 \u0026 U₁₁σ₁₁+U₁₂σ₂₁+U₁₃σ₃₁ \u0026 0 \\\\\\ σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃ \u0026 2σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃+U₂₂σ₂₁+U₂₃σ₃₁ \u0026 σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃ \\\\\\ 0 \u0026 U₃₁σ₁₁+U₃₂σ₂₁+U₃₃σ₃₁ \u0026 0 \\end{bmatrix} \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₂₂} \u0026= \\begin{bmatrix} 0 \u0026 U₁₁σ₁₂+U₁₂σ₂₂+U₁₃σ₃₂ \u0026 0 \\\\\\ σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃ \u0026 U₂₁σ₁₂+σ₂₁U₂₁+2σ₂₂U₂₂+σ₂₃U₂₃+U₂₃σ₃₂ \u0026 σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃ \\\\\\ 0 \u0026 U₃₁σ₁₂+U₃₂σ₂₂+U₃₃σ₃₂ \u0026 0 \\end{bmatrix} \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₂₃} \u0026= \\begin{bmatrix} 0 \u0026 U₁₁σ₁₃+U₁₂σ₂₃+U₁₃σ₃₃ \u0026 0 \\\\\\ σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃ \u0026 U₂₁σ₁₃+U₂₂σ₂₃+σ₃₁U₂₁+σ₃₂U₂₂+2σ₃₃U₂₃ \u0026 σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃ \\\\\\ 0 \u0026 U₃₁σ₁₃+U₃₂σ₂₃+U₃₃σ₃₃ \u0026 0 \\end{bmatrix} \\\\\\ \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₃₁} \u0026= \\begin{bmatrix} 0 \u0026 0 \u0026 U₁₁σ₁₁+U₁₂σ₂₁+U₁₃σ₃₁ \\\\\\ 0 \u0026 0 \u0026 U₂₁σ₁₁+U₂₂σ₂₁+U₂₃σ₃₁ \\\\\\ σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃ \u0026 σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃ \u0026 2σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃+ U₃₂σ₂₁ + U₃₃σ₃₁ \\\\\\ \\end{bmatrix} \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₃₂} \u0026= \\begin{bmatrix} 0 \u0026 0 \u0026 U₁₁σ₁₂+U₁₂σ₂₂+U₁₃σ₃₂ \\\\\\ 0 \u0026 0 \u0026 U₂₁σ₁₂+U₂₂σ₂₂+U₂₃σ₃₂ \\\\\\ σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃ \u0026 σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃ \u0026 U₃₁σ₁₂+ σ₂₁U₃₁+2σ₂₂U₃₂+σ₂₃U₃₃+ U₃₃σ₃₂ \\\\\\ \\end{bmatrix} \\\\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₃₃} \u0026= \\begin{bmatrix} 0 \u0026 0 \u0026 U₁₁σ₁₃+U₁₂σ₂₃+U₁₃σ₃₃ \\\\\\ 0 \u0026 0 \u0026 U₂₁σ₁₃+U₂₂σ₂₃+U₂₃σ₃₃ \\\\\\ σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃ \u0026 σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃ \u0026 U₃₁σ₁₃ + U₃₂σ₂₃ + σ₃₁U₃₁+σ₃₂U₃₂+2σ₃₃U₃₃ \\\\\\ \\end{bmatrix} \\end{aligned} $$ $\\frac{∂𝐔ₖ}{∂𝐭ₖ} = \\frac{∂(𝐉ₖ⋅ 𝐑_{w2c})}{∂𝐭ₖ} = \\frac{∂𝐉ₖ}{∂𝐭ₖ}⋅ 𝐑_{w2c} + \\cancel{ 𝐉ₖ⋅\\frac{∂𝐑_{w2c}}{∂𝐭ₖ} }$,\n(2024-01-16)\nThe derivative of 𝐉ₖ w.r.t. the camera-space center 𝐭ₖ could be obtained from the representation of 𝐉ₖ in terms of 𝐭ₖ, which includes focals fx,fy more than the representation with clip coordinates 𝐭ₖ\u0026rsquo;. In this way, the projection matrix P isn\u0026rsquo;t involved as 𝐭ₖ\u0026rsquo;=𝐏𝐭ₖ.\n$$ 𝐉_{𝐭ₖ} = \\begin{bmatrix} fₓ/t_{k,z} \u0026 0 \u0026 -fₓ t_{k,x} / {t_{k,z}}^2 \\\\\\ 0 \u0026 f_y/t_{k,z} \u0026 -f_y t_{k,y} / {t_{k,z}}^2 \\\\\\ t_{k,x}/‖𝐭ₖ‖ \u0026 t_{k,y}/‖𝐭ₖ‖ \u0026 t_{k,z}/‖𝐭ₖ‖ \\end{bmatrix} $$The derivative of $𝐉_{𝐭ₖ}$ w.r.t. each component of 𝐭ₖ:\n$$ \\begin{aligned} \\frac{∂𝐉_{𝐭ₖ}}{∂t_{k,x}} = \\begin{bmatrix} 0 \u0026 0 \u0026 -f_x/t_{k,z}² \\\\\\ 0 \u0026 0 \u0026 0 \\\\\\ 1/‖𝐭ₖ‖ - t_{k,x}^2/‖𝐭ₖ‖^3 \u0026 -t_{k,y}t_{k,x}/‖𝐭ₖ‖^3 \u0026 -t_{k,z}t_{k,x}/‖𝐭ₖ‖^3 \\end{bmatrix} \\\\\\ \\frac{∂𝐉_{𝐭ₖ}}{∂t_{k,y}} = \\begin{bmatrix} 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 -f_y/t_{k,z}^2 \\\\\\ -t_{k,x}t_{k,y}/‖𝐭ₖ‖^3 \u0026 1/‖𝐭ₖ‖ - t_{k,y}^2/‖𝐭ₖ‖^3 \u0026 -t_{k,z}t_{k,y}/‖𝐭ₖ‖^3 \\end{bmatrix} \\\\\\ \\frac{∂𝐉_{𝐭ₖ}}{∂t_{k,z}} = \\begin{bmatrix} -f_x/t_{k,z}^2 \u0026 0 \u0026 2 f_x t_{k,x}/t_{k,z}^3 \\\\\\ 0 \u0026 -f_y/t_{k,z}^2 \u0026 2 f_y t_{k,y}/t_{k,z}^3 \\\\\\ -t_{k,x}t_{k,z}/‖𝐭ₖ‖^3 \u0026 -t_{k,y}t_{k,z}/‖𝐭ₖ‖^3 \u0026 1/‖𝐭ₖ‖ - t_{k,z}^2/‖𝐭ₖ‖^3 \\end{bmatrix} \\end{aligned} $$ The derivative of 𝚺ₖ\u0026rsquo; w.r.t. 𝐭ₖ\n(2024-01-17)\n$$ \\begin{aligned} \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂𝐔ₖ} \\frac{∂𝐔ₖ}{∂t_{k,x}} \\end{aligned} $$ Based on the projective projection $ϕ(𝐭) ≈ ϕ(𝐭ₖ) + 𝐉ₖ⋅(𝐭 - 𝐭ₖ)$,\nwhere\nThe extrinsics of camera: $𝐓_{w2c} = [^{𝐑_{w2c} \\ 𝐭_{w2c}}_{0 \\quad\\ 1}]$\n𝐭 is the mean vector represented in the camera space: $𝐭 = 𝐓_{w2c} ⋅[^{\\bm μₖ}_1]$\nThe Jacobian of the projective transformation evaluated at 𝛍ₖ:\n$$𝐉ₖ = \\begin{bmatrix} fₓ/μ_{k,z} \u0026 0 \u0026 -fₓ μₓ / μ_{k,z}^2 \\\\\\ 0 \u0026 f_y/μ_{k,z} \u0026 -f_y μ_y / μ_{k,z}^2 \\\\\\ μₖₓ/‖\\bm μₖ‖₂ \u0026 μ_{k,y}/‖\\bm μₖ‖₂ \u0026 μ_{k,z}/‖\\bm μₖ‖₂ \\end{bmatrix} $$ Therefore, the partial derivatives of the loss w.r.t.\nThe partial derivative of the loss 𝓛 w.r.t. the 3D Gaussian center 𝐭 in the world space:\n$$ \\frac{∂L}{∂𝐭} = \\frac{∂L}{∂\\bm μ'} \\frac{∂\\bm μ'}{∂𝐭} + \\frac{∂L}{∂ {\\bm Σₖ'}_{(2D)}} \\frac{∂ {\\bm Σₖ'}_{2D}}{∂𝐭} $$ The partial derivative of the loss 𝓛 w.r.t. the 3D Gaussian covariance matrix 𝚺ₖ in the world space:\n$$ \\frac{∂L}{∂\\bm Σₖ} = \\frac{∂L}{∂ {\\bm Σₖ'}_{(2D)}} \\frac{∂ {\\bm Σₖ'}_{(2D)}}{∂\\bm Σₖ} $$ Covariance Because covariance matrix is symmetric (𝚺ₖ = 𝚺ₖᵀ), it\u0026rsquo;s a square matrix, so it\u0026rsquo;s diagonalizable\n\u0026ldquo;Diagonalizable matrix 𝐀 can be represented as: 𝐀 = 𝐏𝐃𝐏⁻¹.\u0026rdquo;\n\u0026ldquo;A diagonalizable matrix 𝐀 may (?) be decomposed as 𝐀=𝐐𝚲𝐐ᵀ\u0026rdquo;\n\u0026ldquo;Quadratic form can be regarded as a generalization of conic sections.\u0026rdquo; Symmetric matrix\nSince the covariance matrix 𝚺 is a symmetric matrix, its eigenvalues are all real. By arranging all its eigenvectors and eigenvalues into matrices, there is:\n$$\\bm Σ 𝐕 = 𝐕 𝐋$$ where each column in 𝐕 is an eigenvector, which are orthogonal to each other.\n𝐋 is a diagonal matrix. For example:\n$$ \\bm Σ 𝐕 = 𝐕 𝐋 = \\begin{bmatrix} a \u0026 d \u0026 g \\\\\\ b \u0026 e \u0026 h \\\\\\ c \u0026 f \u0026 j \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 2 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 3 \\end{bmatrix} = \\begin{bmatrix} a \u0026 2d \u0026 3g \\\\\\ b \u0026 2e \u0026 3h \\\\\\ c \u0026 2f \u0026 3j \\end{bmatrix} $$ If 𝐕 is invertible, 𝚺 can be represented as 𝚺=𝐕𝐋𝐕⁻¹\nThe eigenvectors matrix 𝐕 and eigenvalues matrix 𝐋 corresponds to the rotation matrix 𝐑 and the stretching matrix 𝐒 squared, which are solved from SVD: 𝚺=𝐑𝐒𝐒ᵀ𝐑ᵀ.\nThe rotation matrix rotates the original space to a new basis, where each axis points in the direction of the highest variance, i.e., the eigenvectors. And the stretching matrix indicates the magnitude of variance along each axis, i.e., the square root of the eigenvalues $\\sqrt{𝐋}$. janakiev-Blog\nAfter obtaining the magnitude of the variance in each direction, the extent range on each axis can be calculated based on the standard deviation according to the 3-sigma rule.\nTo optimize the 3D Gaussians in the world space based on rendered image, the derivative chain is like:\nI l m o a s g s e s s c p r a 𝚺 e c ' e e n ( 2 D ) R s a p y a 𝚺 c ' e ( 3 D ) c s a p m a e c 𝚺 r e a w s o p 𝚺 r a l c d e ","date":"2024-01-01T12:45:00Z","image":"https://janakiev.com/assets/covariance-matrix_files/covariance_visualization.jpg","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/b-note-3dgs-math/","title":"Read: 3DGS | Math Derivation"},{"content":"Code | Arxiv | Oral | GScholar | Rui Chen, Hao Su\nNotes (2023-12-24)\nAbs \u0026amp; Intro Use point cloud to refine depth map rather than regularizing (\u0026ldquo;squashing\u0026rdquo;) cost volumes.\nThe points move along the ray corresponding to each pixel. The step size is determined by neighbors. Refine the coarse point cloud by iteratively estimating the residual from the current depth to the target depth.\nThe target depth is obtained from the ground-truth piont cloud. F V e o a l t u u m r e e P p e r r o s j p F p e o a i t n u t r e c - l a o u u g d m e n t e d N N D r e e p s t i h d u a l P d o i i s n p t l a c e m e n t Output: a dense point cloud, as opposed to sparse point cloud from SfM (COLMAP) used by 3DGS.\nDepth map yielded from point cloud representation doesn\u0026rsquo;t suffer from the problem of resolution limitation.\nOptimization objective: minimize the distance from point to the surface, with the supervision of depth map\nDepth estimation doesn\u0026rsquo;t involve opacity for rendering, so can the RGB appearance be used as loss function? PointFlow: Move point towards the surface. In contrast, 3DGS split or clone a Gaussian along the position\u0026rsquo;s gradient with some hyper-parameters setting.\nHow is the generalizability of this method?\nPipeline Point cloud initialization from a small coarse depth map for reference image.\nS m a i l n l r C e o f s t c a V m o e l r u a m e 3 c D l a U s N s e i t f y d C e o p a t r h s e m a U p n p r o j P w o o i r n l t d c s l p o a u c d e 48 homographies (depths) for warping a source feature map with the 1/8 size of the original image. Assign 2D and 3D context feature to each point\n2D feature: projecting each point onto 3-level feature maps of each view with camera focals and cx,cy scaled.\nW o r l d s U p P p a o d c i a e n t t i s n g F i x f e e d a t 3 p u - y r s r e c a a m m l i a e d p s The retrieved feature vectors of N views are merged into a variance for a certain level j of the feature maps:\n$$𝐂ʲ = \\frac{\\sum (𝐅ʲ - \\bar{𝐅ʲ}) }{N}$$ Only the feature vectors at the projection locations are taken, rather than processing the entire feature map, thereby improving efficiency. 3D feature: normalized point coordinates $𝐗ₚ$\nConcatenate features: $Cₚ = 𝐂¹ ⊕ 𝐂² ⊕ 𝐂³ ⊕ 𝐗ₚ$\nDefine displacement steps with point hypotheses.\nThe unprojected 3D point cloud from 2D depth map is determined. The displacement direction has a lot of freedom. Thus, each point is confined to move along the ray emitted from the reference camera. Such that the per-pixel depth can be refined.\n→ R - e □ f ↓ ↑ □ □ c - a ← m In the above figure, • is a real point, and o is a point hypothesis, denoted as $\\tilde{𝐩ₖ}$, which is associated with a read point $𝐩ₖ$:\n$$\\tilde{𝐩ₖ} = 𝐩ₖ + k s 𝐭, \\quad k = -m, …, m$$where k is the number of steps, s is the step size along the ray direction 𝐭.\nm = 1, so\nAggregate features of the n nearest neighbors\nUse Dynamic Graph CNN to aggregate neighboring points\nUse an MLP to map the aggregated feature to probabilities of point hypotheses.\nThe PointFlow module requires iterations to approach the surface iteratively.\nThe predicted depth residual against the target depth is a probabilistic weighted sum (expectation) of all predefined hypothetical steps.\n$$Δdₚ = 𝐄(ks) = ∑_{k=-m}^m ks × Prob( \\tilde{𝐩ₖ} )$$ Upsampling the refined depth map and shrink the interval between point hypotheses.\nLoss: Accumulated absolute error between refined depth map and the ground-truth depth map over all previous iterations.\n$$L = ∑_{i=0}^l \\Big( \\frac{1}{s^{(i)}} ∑_{p∈𝐏_{valid}} \\\\| 𝐃_{GT}(p) - 𝐃^{(i)}(p) \\\\|₁ \\Big)$$ Method Feature concatenation is similar to PixelNeRF: point ocoordinates + feature vectors. But here the points position isn\u0026rsquo;t consistent, resulting in that different feature vectors that are sampled on the feature pyrimid.\nBecause the 2D CNN also needs to be trained, the feature pyrimid is changing as well.\nPlay Environment (2023-12-26)\nInstallation on Lambda server (Ubuntu 18.04.6 LTS, nvcc -V return 10.2, Driver Version: 470.103.01):\n1 2 bash install_dependencies.sh bash compile.sh Error: No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\nTraceback 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 No CUDA runtime is found, using CUDA_HOME=\u0026#39;/usr/local/cuda\u0026#39; running build_ext Traceback (most recent call last): File \u0026#34;setup.py\u0026#34;, line 20, in \u0026lt;module\u0026gt; \u0026#39;build_ext\u0026#39;: BuildExtension File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/setuptools/__init__.py\u0026#34;, line 153, in setup return distutils.core.setup(**attrs) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/core.py\u0026#34;, line 148, in setup dist.run_commands() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/dist.py\u0026#34;, line 955, in run_commands self.run_command(cmd) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/dist.py\u0026#34;, line 974, in run_command cmd_obj.run() File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/setuptools/command/build_ext.py\u0026#34;, line 79, in run _build_ext.run(self) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/command/build_ext.py\u0026#34;, line 339, in run self.build_extensions() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 404, in build_extensions self._check_cuda_version() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 777, in _check_cuda_version torch_cuda_version = packaging.version.parse(torch.version.cuda) File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/pkg_resources/_vendor/packaging/version.py\u0026#34;, line 49, in parse return Version(version) File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/pkg_resources/_vendor/packaging/version.py\u0026#34;, line 264, in __init__ match = self._regex.search(version) TypeError: expected string or bytes-like object Search the error with DDG.\ntorch.cuda.is_available() returns False. NV forums\npytorch is of cpu version. issue\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; conda list cpuonly 2.0 0 pytorch pytorch 1.10.2 py3.6_cpu_0 pytorch pytorch-mutex 1.0 cpu pytorch torchvision 0.11.3 py36_cpu [cpuonly] pytorch Reinstall with newer packages:\n\u0026ldquo;install_dependencies.sh\u0026rdquo;:\n1 2 3 4 5 6 #!/usr/bin/env bash conda create -n PointMVSNet python=3.10 source activate PointMVSNet conda install pytorch==1.12.1 torchvision==0.13.1 cudatoolkit=10.2 -c pytorch conda install -c anaconda pillow pip install -r requirements.txt Error: identifier \u0026quot;AT_CHECK\u0026quot; is undefined.\nFull message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 building \u0026#39;dgcnn_ext\u0026#39; extension creating build creating build/temp.linux-x86_64-cpython-310 creating build/temp.linux-x86_64-cpython-310/csrc /usr/local/cuda/bin/nvcc -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/zichen/anaconda3/envs/PointMVSNet/include/python3.10 -c csrc/gather_knn_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/gather_knn_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1011\\\u0026#34; -DTORCH_EXTENSION_NAME=dgcnn_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++14 /home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign /home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/ATen/Context.h(25): warning: attribute \u0026#34;__visibility__\u0026#34; does not apply here csrc/gather_knn_kernel.cu(34): error: identifier \u0026#34;AT_CHECK\u0026#34; is undefined csrc/gather_knn_kernel.cu(106): error: identifier \u0026#34;AT_CHECK\u0026#34; is undefined csrc/gather_knn_kernel.cu(125): error: identifier \u0026#34;THArgCheck\u0026#34; is undefined csrc/gather_knn_kernel.cu(145): error: identifier \u0026#34;THCudaCheck\u0026#34; is undefined csrc/gather_knn_kernel.cu(84): error: identifier \u0026#34;TH_INDEX_BASE\u0026#34; is undefined 5 errors detected in the compilation of \u0026#34;/tmp/tmpxft_00002cb5_00000000-6_gather_knn_kernel.cpp1.ii\u0026#34;. error: command \u0026#39;/usr/local/cuda/bin/nvcc\u0026#39; failed with exit code 1 Perplexity (GPT4): Identifiers have been deprecated after PyTorch 1.0.\nIn PyTorch 1.5.0 and later, AT_CHECK has been replaced with TORCH_CHECK. Similarly, THArgCheck and THCudaCheck are no longer used in newer versions of PyTorch. The identifier TH_INDEX_BASE is also undefined because it\u0026rsquo;s no longer used in PyTorch 1.0 and later.\nDowngrade pytorch:\nAI葵 used torch 1.4. Issue: \u0026ldquo;TH_INDEX_BASE\u0026rdquo; is undefined #1\n1 2 3 4 5 6 #!/usr/bin/env bash conda create -n PointMVSNet python=3.8 source activate PointMVSNet conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch conda install -c anaconda pillow pip install -r requirements.txt torchvision older than v0.6.1 doesn\u0026rsquo;t exist in conda:\nFull message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels: - torchvision==0.5.0 Current channels: - https://conda.anaconda.org/pytorch/linux-64 - https://conda.anaconda.org/pytorch/noarch - https://conda.anaconda.org/conda-forge/linux-64 - https://conda.anaconda.org/conda-forge/noarch - https://repo.anaconda.com/pkgs/main/linux-64 - https://repo.anaconda.com/pkgs/main/noarch - https://repo.anaconda.com/pkgs/r/linux-64 - https://repo.anaconda.com/pkgs/r/noarch To search for alternate channels that may provide the conda package you\u0026#39;re looking for, navigate to https://anaconda.org and use the search bar at the top of the page. Use pip to install torch 1.4. (perplexity)\n1 2 3 4 5 conda create -n PointMVSNet python=3.8 source activate PointMVSNet conda install -c anaconda pillow pip install torch==1.4.0 torchvision==0.5.0 pip install -r requirements.txt And modify TH_INDEX_BASE -\u0026gt; 0 in \u0026ldquo;gather_knn_kernel.cu\u0026rdquo;.\nIt can be compiled successfully.\nP.S.:\n3DGS also has knn code: simple-knn Open3D also can find the nearest neighbors. Train \u0026amp; Eval Place the dataset (DTU) in the specified directory:\n1 2 mkdir data ln -s /home/zichen/Downloads/mvs_training/dtu/ ./data Training for 16 epochs cost 3 days approximately:\n1 2 export CUDA_VISIBLE_DEVICES=4,5,6,7 python pointmvsnet/train.py --cfg configs/dtu_wde3.yaml Evaluate\nThe \u0026ldquo;Recified\u0026rdquo; dataset can\u0026rsquo;t be download from the official website by clicking the hyperlink. It may be due to broswer limitations as the zip file is 123 GB (analyzed by Perplexity).\nWget works:\n1 wget http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Download message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (base) zichen@lambda-server:/data/zichen$ wget http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip wget: /home/zichen/anaconda3/envs/GNT/lib/libuuid.so.1: no version information available (required by wget) --2023-12-30 19:30:31-- http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Resolving roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)... 130.225.69.128 Connecting to roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)|130.225.69.128|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip [following] --2023-12-30 19:30:32-- https://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Connecting to roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)|130.225.69.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 129593443783 (121G) [application/zip] Saving to: ‘Rectified.zip’ Rectified.zip 100%[===========================\u0026gt;] 120.69G 3.48MB/s in 14h 18m 2023-12-31 09:48:33 (2.40 MB/s) - ‘Rectified.zip’ saved [129593443783/129593443783] Unzip it\n","date":"2023-12-16T21:30:00Z","image":"https://pic2.zhimg.com/v2-557251caddb35bac39401c9440615105_r.jpg","permalink":"http://blog.zichen.uk/post/writenotes/model/depth/b-note-point-mvsnet/","title":"Read: Depth - MVG | Point-MVSNet"},{"content":"Code | Arxiv | GScholar | mvsnet pytorch\nNotes (2024-10-11)\n可微单应性变换: \u0026ldquo;Differentiable\u0026rdquo; means \u0026ldquo;Matrix multiplication\u0026rdquo; 多视图三维重建技术发展 - bilibili推荐 slides 是用 XMind 做的 Task The model predicts a single depth map at once instead of an entire scene:\nMVSNet can reconstruct a large scene by performing per-view depth map estimation repeatedly.\n\u0026ldquo;Per-view\u0026rdquo; also appears in Point-Based Neural Rendering with Per-View Optimization - Inria.\nThe estimated depth map of the reference view is a weighted sum of multiple predefined depth values, where the weights are regressed from the variance of multi-view cost volumes by a 3D UNet-like network.\nA cost volume is a structure-fixed container (file holder) in the reference camera space, with each compartment holding a source view\u0026rsquo;s feature map that requires warping to align with the camera pose and depth before being inserted into each slot.\nC a r c o r e a s e f m t e e i r r V n e a o n l t c s u h e p m e a e c S s e a m e d e p t h - s l i c i n g A s f o o b u r u r n c o d e t l h e f e e r o a f t V u o w r l a e u r m p m e e a s d p s (2023-12-13) In contrast, NeRF lacks a unified spatial structure to facilitate multi-view geometry consistency.\nPer-ray sampling \u0026 Scene-specific Each view samples depths and maintains a point cloud in its own frustum for rendering (not for reconstructing the global geometry), and the fine-stage sampling results in each ray having different sampled points.\n(2024-02-19) The sampled points don\u0026rsquo;t have standardized geometric structure, because NeRF is a \u0026ldquo;continuous\u0026rdquo; filed, whose continuity is achieved by an MLP. NeRF encompassed multi views into a Radiance filed, so the scene geometry is not directly modeled and constrained. Consequently, multi-view (geometry) inconsistency (floater) is incurred.\nv i 1 e w v i e w 2 v i 3 e w NeRF samples per ray rather than the whole space because it employs the volume rendering method to generate view-dependent images. The primary goal is achieving accurate pixels by optimizing the alignment of volume density σ and voxel color c on an arbitrary piont due to the volume rendering equaiton, while the overall geometry is not focused specifically.\nIn other words, since what NeRF learned is the match between density and voxel color, even if the densities (geometry) are wrong, with colors compensated, the composition could be plausible. Despite the multi-view consistency constraint during training, the geometry estimation (surface) is inherently biased. (NeuS)\nOn the other hand, because the correspondence between volume density and voxel color is different for various scenes, the MLP network of NeRF is scene-specific.\nSimilarly, PixelNeRF regress densities and colors from pixel-wise image features in the frustum of each viewpoint as well, instead of the world space. And multiple views\u0026rsquo; results get aggregated by taking average finally.\nPer-ray and fine-stage sampling decides NeRF is a scene-specific representation because each region is not reconstructed equally. Although some generalizable NeRFs predict density and color from image features, the precision won\u0026rsquo;t be high if only image features are input without providing explicit geometry structures.\n(2023-12-15) The whole space to be reconstructed is split (divide-and-conquer strategy) with some assumption instead of estimating from null. CVPR2023|基于深度学习MVS的最新进展 GeoMVSNet - 3D视觉工坊\nThe resolution of a cost volume is fixed (point-mvsnet). A cost volume is a voxel grid to some extent.\nThe pipeline of MVSNet:\nR i f e m e f g a S v f t S v f r i e r i e c e a c e a w t w t R 1 e 2 p l i c C a o s W s a t r p V e o ✚ d l u v i f m a n e e r a : i r V v t o e o a s w u f l r a s - u i r c m a p d a e n e e m c d p e t s f h p + e s a s a c 3 U o t e D N f s e t t m a a t x R e V 1 f o 9 ' l 2 s u m d p e e r p o s b D E e m x p a p t p e h c r e f i n e Implicit Camera Pose \u0026ldquo;Arbitrary camera geometries\u0026rdquo; prevent the direct comparision among all multi-view feature maps (due to the enormous disparity in observations?).\nIn contrast, 2-view stereo is free from that problem because after pre-rectification, the matching cost of a pair of feature maps is only attributed to the disparity between the pair of images. (?)\n\u0026ldquo;Arbitrary camera geometries\u0026rdquo; means that the epipolar constraint served as the base of two-view stereo could fail, e.g. when no overlapping regions exist in the 2 views. In such a case, however, homography still holds. homography - Carleton University\nTherefore, MVSNet transforms all feature maps into the reference-view camera space.\nHomography warping injects camera geometries into the 2D CNN implicitly.\nBecause the convolution and homography (warping) both are linear operations, they can be switched. Thus, warping the feature map is equivalent to warp the source images followed by convolution. Consequently, the convolution layer (feature extractor) is trained on an image set that implies the camera geometries.\n(2023-12-19) Note: The source image (or feature map) is not mapped onto the depth plane. They\u0026rsquo;re mapped into the reference camera viewpoint to watch the scene.\nBy applying different homographies solved with the predefined depth planes, the source image is warped uniquely. Thus, the features at the same location vary.\nOverall, as depth increases, the source images are go backward.\n(2024-03-17) The above effect is like that a source image is seen from the reference view. It\u0026rsquo;s like watching TV from another angle. This is because when restoring depth values for all pixels, they are assigned with a common depth value to become 3D points (on a depth plane) Code. During training, there are 192 assumed depth values to be assigned on pixels, and then the network is required to predict which depth each pixel is, resulting in a correct pixel-wise depth.\nWith the correct depth map, applying homography will lead to overlapping for the common area between src and ref images.\nThis differentiates homography from epipolar geometry, where a point is projected onto different views to retrieve feature vectors.\n(2023-12-20) As ray marches, for epipolar geometry, the projected pixel is moving along the epipolar line. However, for homography, the source image keeps warping using the increasing depth, resulting in the same effect that the projected location moves along the epipolar line.\nAnd the directions of data flow are opposite. Epipolar is from 3D points at different depths to a source image, whereas homography is from different warped source images to a common 3D point.\nThe reason of performing homography warping on feature map rather than source images could be reducing indexing time with a small resolution?\n(2023-12-15) Maybe to avoid performing too many convolutions. A source image does convolution once and the feature map is warped 192 times.\n(2023-12-23) Maybe the author got inspired by the CNN-based 2-view stereo matching, where camera parameters are disregarded when applying CNN onto 2 pre-rectified images. In contrast, MVSNet incorporates camera poses implicitly after CNN. In other words, \u0026ldquo;rectification\u0026rdquo; is performed on feature maps instead of original images, so as to encode the camera poses into the 2D CNN.\nIf warping the source image first, many black empty areas will appear and disrupt the following convolution. Thus, perform CNN on source images first and then sample feature maps.\nBecause the cost volume is built in the reference camera frustum rather than the world space, the coordinates transformation applied is homography $𝐊₁\\[𝐑₁|𝐭₁\\](𝐊ᵢ\\[𝐑ᵢ|𝐭ᵢ\\])⁻¹$ rather than only unprojection $(𝐊ᵢ\\[𝐑ᵢ|𝐭ᵢ\\])⁻¹$. The \u0026ldquo;implicit\u0026rdquo; camera pose embedding manner is proposed to generalize the 2-view stereo to multi-view stereo.\nOur 3D cost volume is built upon the camera frustum instead of the regular Euclidean space.\nConvolution fuses the neighboring pixels into one feature vector for dense matching. A feature vector is a compact representation of a patch of pixels.\nIn this way, image size reduced but without losing context information required by matching. Hence, the efficiency is higher than matching the original images directly. The high-dimensional descriptor can lead to correct matching if they\u0026rsquo;re well-optimized.\nEvery source view passes the identical 2D CNN, so the differences between cost volumes indicate the probabilities of each depth for each pixel.\nEach \u0026ldquo;depth slot\u0026rdquo; within a cost volume stores a 32-channel feature map, which will be regularized to a 1-channel probability map by 3D UNet and softmax.\nEstimate by Variance The matching cost is measured by element-wise (pixel-wise, depth-wise, and channel-wise) variance of cost volumes, such that the number of source views is not limited.\nA property can be inferred from the mean of multiple views, serving as a synthesized representation. For example, PixelNeRF used the average of projected features from all source views to regress rgbσ per point. However, MVSNet maps the explicit difference (measured by variance) among the same-position feature vectors to a property, i.e., probability of each hypothetical depth for a pixel.\nThe feature vectors at the same location on the warped feature maps from different source images don\u0026rsquo;t correspond to the same 3D point, because a 3D point projected onto different camera films gets different pixel coordinates.\nThus, the homologous feature vectors (\u0026ldquo;allele\u0026rdquo;) at the same relative position from the 3 input images are distinct evidently.\nCyan squares are all at the same location [100:150, 165:215].\nThe ground truth depth is around 776:\n(2023-12-15) The variance for each channel of each matched feature vector from 3 cost volumes, for 192 depths is calculated as:\nF A e c a R r t e o s f s s f o S 3 r r c C d 1 o e s p t t S h r V 1 c o 2 l u m F e e s a t R s e f f o S r r c d 1 e p t S h r 2 c 2 ⋯ ⋯ F e a t R s e f f o S r r c d 1 e p 1 S 9 r 2 c 2 The 192 32-channel variance vectors will be fused to 1-D scalars by a 3D CNN.\nConv along the depth dimension, kernel size=2 and stride=1: aggregate every two variance vector at adjacent depths.\nOutput channel is 1: combine the covered vectors by channel-wise weighted summation only once.\nIn a kernel, weighted sum the same channels, and then sum up all channels to produce one of the output channels.\no 1 □ u V t c a □ : h r v a ↓ a t ⋯ r ⋯ s 1 D u 1 m □ V a r a s t t ⋯ r ⋯ D i 2 d e = 1 ⋯ o 1 V □ u a t c r □ : h a v t ↓ a ⋯ r D ⋯ s 1 1 u 9 9 m 2 1 □ V a r a t D ⋯ 1 9 2 192 variance scalars will be normalized by softmax to 192 probabilities for each depth value.\nDepth map is a summation of 192 preset depths weighted by probabilities.\nThe variance of the \u0026ldquo;position-matched\u0026rdquo; feature vectors from a pair of source feature maps at each depth is interpreted as each depth\u0026rsquo;s probability for a pixel:\nA source image is warped using a homography, solved with a certain depth $dₙ$, to supplement the reference view with information from a lateral perspective, although the scale doesn\u0026rsquo;t match:\nIf the scale matches, the pixels on the ref and source views projected from a common 3D point will overlapped, and the depth of that 3D point is found. (Refer to the test result in another post.)\nThe feature map of the source image is sampled into a warped feature map.\nFeature vectors at the same location are matched? do not match a common 3D point.\nThe variance of the feature vectors pertaining to a depth is calculated\nHigh variance means the patches that a point projected onto each view are distinct, leading to different feature vectors.\n3D CNN aggregates adjacent variance vectors along 3 directions into a scalar variance.\n3D CNN is a smooth constraint to filter the noise in the cost volume caused by non-Lambertian surfaces and object occlusions.\nNormalize 192 variance by Softmax to obtain the probability distribution of depths.\nWhy does the correct depth has the highest variance?\nWhy use variance? high variance means high probability.\n为什么高方差意味着高概率呢？？？\n把一组照片的 homography 效果做出来看看 Read: Rethink depth esti and IS-MVSNet Google search: \u0026ldquo;why does MVSNet use the variance to regress the probability of depths?\u0026rdquo; Because the exact depth for a pixel results in other views\u0026rsquo; \u0026ldquo;deny\u0026rdquo; since the matching pixel on other source images doesn\u0026rsquo;t correspond to that depth due to the viewpoint shifts.\nchanging viewpoint will change distance from the observing point to the camera as well. a feature vector represents a patch of pixels, the area with high rgb-variance means geometry changes.\n(2023-12-16) Each variance vector is attributed to 3 feature vectors:\nepipolar homography The epipolar reminds me the view fusion in GNT, where multi-view features are fused by \u0026quot; subtraction attention\u0026quot;.\n(2023-12-20) However, MVSNet is not epipolar. The feature 3 vectors are at the same location with the underlying feature maps changing.\nBut essentially, homography is epipolar in terms of variance vectors ( not actual 3D points). The feature vectors on the epipolar line are shifted to the location of the reference feature.\nSuccinctly, in epipolar, the projection changes (moves), while in homography, the feature map changes (warps).\nI probably thought of (on 2023-12-16) that after warping, the feature vectors at the same location correspond to a same 3D point, like eipolar. But that\u0026rsquo;s not right even for the warping with the accurate depth value (776 mm) as shown in the above demo. The image (or feature map) moved indeed after warping, but not got the exact position corresponding to the common 3D point.\nR f s f □ e e s f o e f a o e u a t u a r t 1 r t 2 c c e m e m a 3 a p 2 p d c 1 h n l s 3 □ □ 2 d 2 c □ □ h n l s □ □ d 3 1 2 9 2 c h n l s However, if the epipoles are at infinite (no overlapping observation), the variance matching is poor (under-determined system has inifinitely many solutions normally), and a reference pixel is falsely matched resulting in false probabilities distribution: Multiple depth values are plausible and gain similar probabilities. The correct warping correspond to the correct depth, and vice versa.\n(2023-12-20) 3D CNN is a classifier to find the most possible depth value among 192 hypotheses based on the input variance, which is produced from a 2D CNN.\nTherefore, the job of the 2D CNN is to make the features cooresponding to the correct depth having the biggest variance. Such that the aggregated variance will still be the most salient after softmax. And finally, it takes the highest weight.\n(2023-12-20) If the above analysis that the homography is equivalent to epipolar geometry for building variance vectors is true, then the reference pixel should find the matched pixel on the epipolar line projected from the correct depth.\nThe source feature map is warped differently according to various depth values. The feature vectors at a common location of all the input images differs. Their variance is the matching cost for a hypothetical depth. Since a feature vector indicates the context around a pixel, the high variance means the behind pixels are not similar. So they should not be matched. But they use softmax to identify the highest variance.\ndoubt: I still believe the variance should be minimized at the correct depth. 计算机视觉中cost-volume的概念具体指什么？ - 知乎\n(2024-02-19) The PointMVSNet paper said MVSNet used \u0026ldquo;soft argmin\u0026rdquo; in sec3.1.\nCost Volume A cost volume 𝐕ᵢ corresponding to a source view\u0026rsquo;s feature map 𝐅ᵢ constitutes multiple warped feature maps 𝐕ᵢ(d), which can be obtained by specifying a depth and a [𝐑|𝐭] that transforms the reference camera to the source view camera.\nAs shown below, a cost volume for a source feature map contains 192 warped feature maps at corresponding depths, while the cost volume for the reference feature map is a replication of itself 192 times.\nF o M S s f z N R F 𝐅 e f a a o e o e e ₁ a p m u a a r f a t r p p r t x m t r e e l c i a i e m f D d e e 𝐕 s l m m p a e d ᵢ g a l p v p g ( o v ' p i i t r d f e s c 𝐅 e 𝐊 h i ) c a ₁ w ᵢ : d R t s [ e o 𝐑 1 ▦ f r ᵢ | c o 𝐭 a f ᵢ 2 ▦ m ] e p ( r l 𝐊 a a ₁ 3 ▦ n [ e 𝐑 s ₁ ⋯ | : t : 𝐭 1 ( o ( ₁ 9 C C w C C ] 2 o = a o = ) s 3 r s 3 ⁻ t 2 d t 2 ¹ , s , V V o N c o N l _ a l _ u d m u d m e e m e e p r e p t a t o h o h f = f = 1 1 v 9 R 9 i 2 e 2 e , f , w H v H i , i , W e W ) w ) A feature map has 32 channels, so a cost volume is a 4-D tensor: (C=32, D=192, H=128, W=160).\n(2023-12-12) Homography isn\u0026rsquo;t used to map the pixel of reference-view feature map to source-view feature map, but instead warp the source feat to reference feat via sampling cooresponding matching points.\n(2024-04-28)\nWhen searching \u0026ldquo;深度\u0026rdquo; in the QQ group, I found the comment of AURORA on mvsnet on 23/10/08: “（MVSNet只在 DTU 数据集上训的，为啥在其他数据集上也有效果）因为 costvolume 相当于训练了一个 feature extractor”\nI think the inputs are already feature maps. The network gotten trained is the 3D UNet, that maps variance volume to probability volume.\nEven if the 3D UNet is really a \u0026ldquo;feature extractor\u0026rdquo;, the feature is extracted from the variance volume.\nCode Understand A complete demo of homography warping: gist\nAccording to the principle of homography, pixels on the reference view\u0026rsquo;s feature map are mapped linearly onto each source view. Thus, a warped source feature map is sampled (F.grid_sample) from the original source feature map at the mapped pixels.\nConstruct coordinates of pixels:\n1 2 3 4 h, w = 128, 160 vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed uv1 = torch.cat([uv, torch.ones(len(uv), 1)], dim=-1) # (hw,3) Map coordinates on the reference image to a source image.\nThe mapping matrix is 𝐊ᵢ\n\\[𝐑ᵢ|𝐭ᵢ\\] (𝐊₁[𝐑₁|𝐭₁])⁻¹. Therefore, each view calculates its own proj 𝐊ᵢ\n\\[𝐑ᵢ|𝐭ᵢ\\] in advance.\nUse $[\\\\^𝐑|\\\\^𝐭]$ to represent the mapping for a pixel from the reference image to a source image, so that the pixel-wise (u,v,1) and depth-wise (d) matmul is:\n$$ \\begin{bmatrix} u' d' \\\\\\ v' d' \\\\\\ d' \\\\\\ 1 \\end{bmatrix}= \\begin{bmatrix} \\\\^𝐑 \u0026| \\\\^𝐭 \\\\\\ 0 \u0026| 1 \\end{bmatrix} \\begin{bmatrix} ud \\\\\\ vd \\\\\\ d \\\\\\ 1 \\end{bmatrix} = \\\\^𝐑\\_{3×3} \\begin{bmatrix} u \\\\\\ v \\\\\\ 1 \\end{bmatrix}⋅d + \\begin{bmatrix} t₁ \\\\\\ t₂ \\\\\\ t₃ \\\\\\ 1 \\end{bmatrix} $$ where [ud, vd, d,1]ᵀ is homogeneous coordinates for translation, and [u, v, 1]ᵀ is homogeneous coordinates for perspective division to present a 3D scene on a 2D image.\nThe translation $\\\\^𝐭$ is not affected by d, so it\u0026rsquo;s separated.issue 10\n1 2 3 4 5 6 7 8 src_KRt = intrisics @ extrinsics[:3,:4] proj = src_KRt @ torch.inverse(ref_KRt) rot, trans = torch.split(proj, [3,1], dim=-1) # (4,3), (4,1) rot_uv1 = rot[:3] @ uv1.t() # (3, hw) # depth_values: d = 425. + 1.06*2.5* torch.arange(192).view(1,-1,1) # (1,192,1) rot_uvd = rot_uv1.unsqueeze(1).expand(3,192,-1) *d # (3,192,hw) pix_proj = rot_uvd + trans[:3].unsqueeze(1).expand(3,192,1) The above procedures of determining projection locations are not involving learnable feature vectors. Thus, with torch.no_grad(): runtime context isn\u0026rsquo;t actually effective. And the \u0026ldquo;differentiable\u0026rdquo; is achieved by F.grid_sample(). Sample the source feature map at the mapped coordinates\n1 2 3 4 u_src = 2*(pix_proj[0] / pix_proj[2]) / (w-1) - 1 # (192, hw) v_src = 2*(pix_proj[1] / pix_proj[2]) / (h-1) - 1 uv_src = torch.stack([u_src, v_src], dim=-1) # (192, hw, 2) warped_feat = F.grid_sample(src_feat, uv_src.view(bs, 192*h, w, 2)) Merge all feature maps\u0026rsquo; cost volume into their variance:\n$$ \\begin{aligned} \\frac{ ∑_{i=1}^N (𝐕ᵢ - \\bar{𝐕})² }{N} \u0026= \\frac{∑_{i=1}^N (𝐕ᵢ² - 2𝐕ᵢ \\bar{𝐕} + \\bar{𝐕}² ) }{N} \\\\\\ \u0026= \\frac{∑_{i=1}^N 𝐕ᵢ²}{N} - \\frac{∑_{i=1}^N 2𝐕ᵢ \\bar{𝐕} }{N} + \\frac{∑_{i=1}^N \\bar{𝐕}² }{N} \\\\\\ \u0026= \\frac{∑_{i=1}^N 𝐕ᵢ²}{N} - \\frac{ 2\\bar{𝐕} ∑_{i=1}^N 𝐕ᵢ }{N} + \\frac{N \\bar{𝐕}²}{N} \\\\\\ \u0026= \\frac{∑_{i=1}^N 𝐕ᵢ²}{N} - \\bar{𝐕}² \\end{aligned} $$ 1 c = volume_sq_sum / n_src - (volume_sum / n_src)**2 Compress the 3D 32-channel volume variance to 1-channel scalar for each depth through a 3D UNet:\n3 2 → 8 ↓ 1 6 → 1 ↓ 3 6 2 → 3 ↓ 6 2 4 ✚ ✚ + → 3 6 2 ↑ 4 = 3 1 ↑ 2 6 = 1 8 ↑ 6 = 8 → 1 Note: The original UNet concats the feature maps on the same level, whereas here the feature vectors are added up, like skip connections in restnet. Then, the logits will be normalized by softmax to become a vector of probabilities.\nThe depth of one pixel is a weighted sum (expectation) of 192 depth values.\nA d d e e p p p i t t x h h e s l = : ' s p 4 1 2 5 ✚ p 4 2 2 7 ✚ . 6 p 3 4 3 ✚ 0 ⋯ ✚ p 9 1 3 9 1 2 . 1 Kind of like \u0026ldquo;alpha compositing\u0026rdquo;, where the opacity corresponds to the probability and the color of each filter is the depth value here.\nOr it can be interpreted as a linear interpolation for 192 depth values.\nEach pixel has a distinct depth-probability distribution derived from feature maps.\nPhotometric consistency\nprob_volume → prob_volume_sum4: 4 * average of every 4 depths\u0026rsquo; prob → retrieve the \u0026ldquo;4 times average prob\u0026rdquo; from prob_volume_sum4 according to the depth_index, which is a weighted sum (expectation) of 0 ~ 192 using the prob_volume.\n1 9 2 p r A T o ⋯ v i b g m e e s v e 4 r . * y [ 0 4 p s - ; r u 1 o m 9 b 4 2 ] D e p t h p R e e ⋯ p r t l r a p i n i e e x v e e i l n d e x P c h o o n t f o i m d e e t n r c i e c The quality of depth estimation is measured by the sum of 4 nearby probabilities for a predicted depth. If this probability sum is high, the estimation is reliable. Refine Network\nd ⊕ e p r ↓ 4 t g h b 3 2 3 2 3 2 1 ✚ = 1 T\u0026amp;T dataset doesn\u0026rsquo;t provide normal information to generate mesh surface, and render images for each viewpoint, so the depth map cannot be finetuned by the reference image. Point Cloud Filter Depth Map based on photometric (probability map\u0026gt;0.8), and geometric (re-projection error less than 1 pixel) consistencies\nr □ e f □ s r c The function for \u0026ldquo;geometric consistency\u0026rdquo;:\nInput: 10 source views of a reference view,\nOutput: A mask for the reference image.\nGeometric consistency implementations:\ncheck_geo_consistency() in kwea123/CasMVSNet_pl/eval.py\ncheck_geometric_consistency() in xy-guo/MVSNet_pytorch/eval.py\nFuse multi-view depth maps based on visibility, and then unproject the unified depth map.\nActice Recall Exercise (2025-04-10)\n不要画相机，不要想象相机，MVSNet 方法的 setting 中，所涉及到的对象只有 图片(平面)，只是从不同方向去看图片。\n方法的核心是代价体\nPlay Profiler The training is slow, almost as slow as NeRF. Why?\nThere is a profile() function provided.\n(2023-12-09)\nDTU The preprocessed DTU includes 79 training scans (scenes) and 22 testing scans. Each scan has the same 49 camera poses with 7 different light conditions.\nTherefore, each scan has 79×49×7 = 27097 pictures serving as reference images. Each reference image is assigned with 10 images taken with nearest poses under the same light condition.\nDuring training, 3 source images are used to estimate the depth map of the corresponding reference image, while 5 source images are used during testing.\nThe depth range is determined based on the depth_min (425.0) and depth_interval * interval_scale (2.5 × 1.06) between two adjacent fronto-parallel planes. For example, if there are 192 planes, the max depth is 425 + (2.5 × 1.06) × (192-1) = 931.15\n1 2 |------|------|------ ... ---| depth: 425.0 427.65 430.3 931.1488 Code warps the source feat map in a backward way by reversing the target coordinates back to the source feat map and then sampling, rather than computing the target coordinates directly from the homography represented with planes.\nThe pixel transferring from the (target) reference plane to a source plane is performed by 3 steps: rotate first, then assign depths, finally add translation.\n1 2 rot@ *d +t (u,v,1) -\u0026gt; (u\u0026#39;,v\u0026#39;,w\u0026#39;) -\u0026gt; (u\u0026#39;d, v\u0026#39;d, w\u0026#39;d) -\u0026gt; (u\u0026#39;d, v\u0026#39;d, w\u0026#39;d) + t $$ \\begin{bmatrix} u \\\\\\ v \\\\\\ 1 \\end{bmatrix} → \\begin{bmatrix} u' \\\\\\ v' \\\\\\ w' \\end{bmatrix} → \\begin{bmatrix} u'd \\\\\\ v'd \\\\\\ w'd \\end{bmatrix} → \\begin{bmatrix} u'd+t \\\\\\ v'd+t \\\\\\ w'd+t \\end{bmatrix} → \\begin{bmatrix} \\frac{u'd+t}{w'd+t} \\\\\\ \\frac{v'd+t}{w'd+t} \\\\\\ 1 \\end{bmatrix} $$Refer to eq.(11) in Multi-View Stereo中的平面扫描(plane sweep) - ewrfcas的文章 - 知乎\n$$ R $$ Read GT Depth (2023-12-19)\nFind the ground-truth depth map for an input image (512, 640).\nThe preprocessced DTU (\u0026ldquo;dtu_training.rar\u0026rdquo;) only contains 1/4 depth maps with size 128x160, aligned with the size of feature maps. So, each file (\u0026ldquo;DTU_Depths_raw/scan1_train/depth_map_0000.pfm\u0026rdquo;) is 80K.\nWhile the folder \u0026ldquo;DTU_Depths_raw/\u0026rdquo; (\u0026ldquo;Depth_raw.zip\u0026rdquo; used in MVSNeRF) includes full-size pfm with the same resolution 1200x1600 as the DTU images. An example: \u0026ldquo;DTU_Depths_raw/scan1/depth_map_0000.pfm\u0026rdquo;: 7.32M\nRead depth map with function read_pfm():\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import imageio import matplotlib.pyplot as plt from pathlib import Path from datasets.data_io import read_pfm depth_path = Path(\u0026#34;../DTU_Depths_raw/scan4_train/depth_map_0008.pfm\u0026#34;) depth_pfm, scale = read_pfm(depth_path) # (128,160), 1.0 depth_png = imageio.imread(\u0026#34;../DTU_Depths_raw/scan4_train/depth_visual_0008.png\u0026#34;) orig_rgb = imageio.imread(\u0026#34;../mvs_training/dtu/Rectified/scan4_train/rect_009_3_r5000.png\u0026#34;) orig_rgb = orig_rgb[::4,::4,:] # reduce 4 times q_x, q_y = int(190/4), int(125/4) # query point plt.text(x=q_x, y=q_y, s=f\u0026#39;{depth_pfm[q_x, q_y]:.2f}\u0026#39;) plt.vlines(x=q_x, ymin=q_y-12.5, ymax=q_y+12.5) plt.hlines(y=q_y, xmin=q_x-12.5, xmax=q_x+12.5) plt.imshow(depth_pfm) # plt.imshow(depth_png, alpha=1) plt.imshow(orig_rgb, alpha=0.2) plt.title(str(depth_path).split(\u0026#39;/\u0026#39;)[2:]) ### LLFF\nAnalyses:\nWhat\u0026rsquo;s the key of its generalizability?\nEnvironment (2024-02-21)\nReferring to this issue#101 and issue#149\n1 2 conda create --name MVSNet python=3.7 cudatoolkit=9.0 cudnn=7.6.4 -c conda-forge pip install -r requirements.txt # tf==1.15.0 Download pretrained model: 1 gdown 1-1JyFT9ClqPO0kz0d_5I1_IHX05paS4h Validate:\n1 2 3 4 python mvsnet/validate.py --regularization \u0026#39;3DCNNs\u0026#39; --validate_set dtu --max_w 640 --max_h 512 --max_d 128 \\ --pretrained_model_ckpt_path \u0026#39;tf_model_dtu/3DCNNs/model.ckpt-150000.data-00000-of-00001\u0026#39; \\ --dtu_data_root \u0026#39;~/Downloads/mvs_training/dtu\u0026#39; \\ --validation_result_path \u0026#39;validation\u0026#39; Related Mutli-view stereo多視角立體重建技術介紹 - AI葵\nMVSNet系列 - MEGVII 黄百川\n【代码精读】开山之作MVSNet PyTorch版本超详细分析 - doubleZ的文章 - 知乎\n【论文精读5】MVSNet系列论文详解-Point-MVSNet - CSDN博客\n多视图几何三维重建实战系列- Cascade-MVSNet - 哔哩哔哩\nFollow-up 2019-2020. 基于深度学习的三维重建——MVSNet系列论文解读 - 机器人3D感知的文章 - 知乎\n","date":"2023-12-03T00:49:00Z","image":"https://ar5iv.labs.arxiv.org/html/1804.02505/assets/x1.png","permalink":"http://blog.zichen.uk/post/writenotes/model/depth/b-note-mvsnet/","title":"Read: Depth - MVG | MVSNet"},{"content":"(Feature image from: A Comprehensive Overview of Gaussian Splatting - Medium - Kate Yurkova)\nSurveys A Survey on 3D Gaussian Splatting\nArxiv\n(2024-01-10)\nReview papers until Jan 2024 Not very detailed. 3D Gaussian as a New Vision Era: A Survey\n(2024-02-13)\nPapaers in 2023. Render Quality Sorting StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering SIG'24 | Lukas Radl, Bernhard Kerbl, TU Graz\n(Mentioned by will)\nCode\nTask: Rending with splatting\nWhy matter?: Real-time rendering\nProblem: 3D Gaussians are not sorted carefully for varing view directions.\nSolution:\nProgressively find the first intersection with a 3D Gaussian on the ray Optical flow Conclusion\nRethink\nSidenotes\nMLP Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering CVPR'24 | Tao Lu, Bo Dai\nCode | Live: 【CVPR大佬讲paper第二期】上海AI实验室鲁涛大佬讲Scaffold-GS, GSDF, OctreeGS录制内容\n(2024-05-04)\n显隐结合 Quantum Physics (2024-09-25)\nJason wechat group (24/09/02): 清华大学的2DGH，从量子物理中汲取灵感，提出使用高斯-埃尔米特核作为高斯分层中的新基元， 在几何重建和新视图合成任务中的非凡性能。\nReduction EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS\nCode | Arxiv\n(2023-12-08)\nVector-Quantized to encode each Gaussian\u0026rsquo;s color and rotation attributes to a discrete vector to reduce memory usage. LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS\nCode\n(2023-12-09)\nCompressed 3D Gaussian Splatting for Accelerated Novel View Synthesis\nEmergent\n(2024-01-08)\nSensitivity-aware clustering, quantization-aware fine-tuning, and entropy encoding. Learned codebooks, compress 31x Compact 3D Gaussian Representation for Radiance Field\nCode | Emergent | ProjPage | Joo Chan Lee\n(2024-01-15)\nReduce the number of Gaussians by learnable masking. use MLP to enhance the Gaussians\u0026rsquo; color. GES emergent\n(2024-02-18)\nReplace Gaussian distribution with Generalized Exponential Function PBR Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing\nProjPage\n(2025-04-17T23:40:22)\nWill\u0026rsquo;s collection:\n3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes Moenne-Loccoz等 Radiant Foam: Real-Time Differentiable Ray Tracing Govindarajan 等 RaySplats: Ray Tracing based Gaussian Splatting Byrski 等 REdiSplats:Ray Tracing for Editable Gaussian Splatting Byrski 等 Reflective Gaussian Splatting Yao 等\nGeometry Mesh Recon Trim 3D Gaussian Splatting for Accurate Geometry Representation Arxiv | Lue Fan, Zhaoxiang Zhang, CASIA\nNo Code | src:QChatGrp\n(2024-06-12)\nTask: Geometry reconstruction Problem: redundant and inaccurate Gaussians Solution: Contribution-based trimming and small Gaussians\u0026rsquo; scale Conclusion: Remove false Gaussian and preserve correct structures. SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering\nCode | Arxiv | Coolpaper\n(2023-12-17)\nAlign Gaussians with surface NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance\nArxiv\n(2024-02-20)\nJoint optimization (3DGS can\u0026rsquo;t perform gradient descent simultaneously with a neural network.) QQ chat 2024-02-20: 除了 Sugar，现在还有什么新的 3DGS 提 mesh 的方法嘛\nwill：GaMeS，GSIR，Mesh-based Gaussian splatting for real-time large-scale deformation， 都得把 gs 退化成椭圆，目前应该只有 sugar 开了源。 还有一个 NeuSG 是 nerf+GS 联合出 mesh 的。 gs 本身的拓扑关系不强，提 mesh 还是不如 nerf，更别说比过传统方法了。\n2D Gaussian Splatting for Geometrically Accurate Radiance Fields\nArxiv\n(2024-04-16)\n2D Gaussians compose of the surface. (2024-04-24)\nMath derivation (Chinese): will-zzy/2dgs-non-official python imple - Colab GOF\nSingle-View Splatter Image: Ultra-Fast Single-View 3D Reconstruction\nCode | arXiv | brief\n(2023-12-30)\nOne image is input into a UNet to reconstruct \u0026ldquo;images\u0026rdquo; that is interpreted as parameters (opacity, RGB, Covariance, positon) of all Guassians. Each pixel corresponds to a Guassian. Cross-view transformer Compared with PixelNeRF AGG: Amortized Generative 3D Gaussians for Single Image to 3D\nNo Code | CoolPaper | Emergent | Dejia Xu, UofTexas\n(2024-01-10)\nComparing with optimization-based single image to 3D method, such as leveraging diffusion model, AGG obtains Gaussian-wise color and opacity from augmented image features.\nCoarse representation: DINOv2 image features followed by 2 transformers that encode features to Gaussian locations and a tri-plane texture field.\nThe Gaussian locations + texture feature are mapped to color and opacity for each Gaussian by an MLP\nWhile in NeRF, MLP is used to output each point\u0026rsquo;s rgb and density. In fine stage, Gaussian\u0026rsquo;s coarse color and opacity are augmented with the DINOv2 image feature, and perform super-resolution on the low-resolution feature map, which will be decoded by MLP.\n(2024-04-12)\nTriplane is a kind of implicit representation. While Gaussians is an explicit representation. 显隐结合\nUpsampling Gaussians is a densification? (Heard from 【3D AIGC论文串讲】单视图3D重建-从单视图中预测3D模型】)\nTriplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers\nGradio Demo | Code | ProjPage | VAST\n(2024-01-20)\nThis model (on huggingface) is trained on Objaverse-LVIS (~45K synthetic objects) only.\nIs their multi-view consistency derivated from geometry prior contained in the huge dataset?\nImage -\u0026gt; pre-trained ViT \u0026ndash;\u0026gt; Upsample (SnowflakeNet) \u0026ndash;\u0026gt; point cloud -\u0026gt; Tri-plane -\u0026gt; Gaussians\u0026rsquo; parameters \u0026ndash;\u0026gt; Splatting\nGamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction\nArxiv\n(2024-04-16)\nSingle view -\u0026gt; DINO features -\u0026gt; Mamba-based sequential network -\u0026gt; 3D Gaussians Touch-GS IROS'24\nCode\n(2024-07-09)\n斯坦福开源Touch-GS！视觉-触觉有监督的3D高斯泼溅-计算机视觉life Bayesian. How to introduce probability? Sparse views Mask GaussianObject GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting\nCode | Arxiv | CoolPapers | Chen Yang, Wei Shen, SJTU\n(2024-02-24)\nMask for an object. Not reconstructing a scene. Train a diffusion model for repairing. (2024-04-09)\nTwo problems and two solutions:\nProblem 1: Overfitting to sparse views resulting in fragmented structure.\nVisual hull of objects for structure prior to restrict Gaussian kernel within object outline. Problem 2: Information missing.\nTrain a 2D diffusion model, with leave-one-out training and noise-added gaussians, to predict what a corrupt rendered image should have looked like. (for arbitrary viewpoint?) The predicted images are used to refine 3DGS. (2024-07-26)\nThe project: SAM has been archived. I\u0026rsquo;m confused as it\u0026rsquo;s so popular.\nDownload the \u0026ldquo;Dataset Pt. 1\u0026rdquo; of Mip-NeRF 360 (11.7G): wget http://storage.googleapis.com/gresearch/refraw360/360_v2.zip\nCheck: unzip -l 360_v2.zip\nDepth Regularized DNGaussian DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization ~ CVPR 2024\nCode | Arxiv | Jiahe Li, Beihang\nSupports\n(2024-04-14)\nLess training time and less memory footprint.\nGeometry degradation can be mitigate by dpeth constraint.\nLoss: hard- and soft-depth regularization; Global- and local-depth normalization.\nFreeze Gaussians\u0026rsquo; shape (covariance), and apply depth regularization to position and opacity of Gaussians.\nHard depth: Gaussians on the surface are the outest hull of the point cloud, they can be identified by setting a large opacity, and when rendering depth map, only their depths are revealed.\nSmall depth variation matters for 3DGS -\u0026gt; normalization\nA depth map is split into patches. Normalize each patch with local patch-level depth variance, and global image-level depth variance separately.\nColors are predicted by a MLP.\nDepth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images\nArxiv | Jaeyoung Chung\n(2023-11-30)\nFew shot but without overfitting Pre-trained monocular depth estimation model Representing a 3D scene by combining numerous Gaussian splats has yielded outstanding visual quality.\nNexusGS References:\nNexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting Code Supports:\nProblem solving\n┌ │ └ ─ D ─ ─ e ─ ─ n ─ ─ s ─ ─ i ─ ─ f ─ ─ y ─ ┐ ├ ┘ ─ ─ ▷ ┌ │ └ ─ P ─ ─ l ─ ─ a ─ ─ c ─ ─ e ─ ─ ─ ─ p ─ ─ o ─ ─ i ─ ─ n ─ ─ t ─ ─ s ─ ┐ ├ ┘ ─ ─ ▷ ┌ │ └ ─ R ─ ─ e ─ ─ d ─ ─ u ─ ─ c ─ ─ e ─ ─ ─ ─ r ─ ─ a ─ ─ n ─ ─ d ─ ─ o ─ ─ m ─ ─ n ─ ─ e ─ ─ s ─ ─ s ─ ┐ │ ┘ Implementation\n┌ │ └ ─ D ─ ─ e ─ ─ p ─ ─ t ─ ─ h ─ ─ ─ ─ E ─ ─ m ─ ─ b ─ ─ e ─ ─ d ─ ─ d ─ ─ i ─ ─ n ─ ─ g ─ ┐ ├ ┘ ─ ─ ▷ ┌ │ └ ─ A ─ ─ c ─ ─ c ─ ─ u ─ ─ r ─ ─ a ─ ─ t ─ ─ e ─ ─ ─ ─ D ─ ─ e ─ ─ p ─ ─ t ─ ─ h ─ ┐ ├ ┘ ─ ─ ▷ ┌ │ └ ─ A ─ ─ c ─ ─ c ─ ─ u ─ ─ r ─ ─ a ─ ─ t ─ ─ e ─ ─ ─ ─ D ─ ─ e ─ ─ p ─ ─ t ─ ─ h ─ ─ ─ ─ m ─ ─ a ─ ─ p ─ ┐ ├ ┘ ─ ─ ▷ ┌ │ └ ─ O ─ ─ p ─ ─ t ─ ─ i ─ ─ c ─ ─ a ─ ─ l ─ ─ ─ ─ f ─ ─ l ─ ─ o ─ ─ w ─ ─ , ─ ─ ─ ─ c ─ ─ a ─ ─ m ─ ─ e ─ ─ r ─ ─ a ─ ─ ─ ─ p ─ ─ o ─ ─ s ─ ─ e ─ ┐ │ ┘ Introduction:\nProblem Analyzing\nflowchart LR a(\"Densification\") a --\u003e b1(\"The number of points\") a --\u003e b2(\"Their position accuracy\") b1 --\u003e c1(\"FSGS\") b2 --\u003e c2(\"Depth\") Competition\n\u0026ldquo;NexusGS for Novel View Synthesis\u0026rdquo;\nContinuous camera pose rendering\nMetrics: PSNR, SSIM, LPIPS\nSparse input views\nApproach\nflowchart LR a(\"Depth map\") -- constrainted --\u003e b(\"Optical Flow,\nEpipolar Line\") b --\u003e c(\"Point insert\") ┌ │ └ ─ D ─ ─ e ─ ─ p ─ ─ t ─ ─ h ─ ─ ─ ─ m ─ ─ a ─ ─ p ─ ─ ─ ┐ ├ ┘ ─ c o n s t r a i n t e d ─ ▷ ┌ │ └ ─ O ─ ─ p ─ ─ t ─ ─ i ─ ─ c ─ ─ a ─ ─ l ─ ─ ─ ─ F ─ ─ l ─ ─ o ─ ─ w ─ ─ , ─ ─ \u0026lt; ─ ─ b ─ ─ r ─ ─ \u0026gt; ─ ─ ─ ─ E ─ ─ p ─ ─ i ─ ─ p ─ ─ o ─ ─ l ─ ─ a ─ ─ r ─ ─ ─ ─ L ─ ─ i ─ ─ n ─ ─ e ─ ─ ─ ─ ─ ┐ ├ ┘ ─ ─ ▷ ┌ │ └ ─ P ─ ─ o ─ ─ i ─ ─ n ─ ─ t ─ ─ ─ ─ i ─ ─ n ─ ─ s ─ ─ e ─ ─ r ─ ─ t ─ ─ ─ ┐ │ ┘ Image Features CoherentGS CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians\nNo Code | Arxiv | Avinash Paliwal, Texas A\u0026amp;M\n(2024-04-06)\n(2024-04-16)\nRegularizer: single\u0026amp;multiview convolution decoder + total variance loss + flow-based loss. Initilization: Monocular depth estimation model pixelSplat pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction\nCode | CoolPapers | David Charatan, Vincent Sitzmann\n(2024-04-15)\n2 views\nSample from a distribution with reparameterization trick.\nFeed-forward: reference-image colors are fused into novel-views colors.\nFeed-forward (Generalizable) methods require training on large datasets, e.g., RealEstate10K, which is also used by: IBRNet, GNT, MuRF, to accquire a general 3D scene prior.\nMVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images\nCode | Arxiv | Yuedong Chen, Monash\n(2024-04-13)\nMVSNet + 3DGS I guess this paper has been rejected like his last project Match-NeRF, because as Jiayuan said, \u0026ldquo;just changed a dataset\u0026rdquo;. MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo\nNoCode | Arxiv | Tianqi Liu, HUST\n(2024-05-27)\n\u0026ldquo;MVSNet\u0026rdquo; -\u0026gt; depth map -\u0026gt; pixel-aligned feature -\u0026gt; Gaussian parameters.\nChinese post\nFrequency RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting\nCode | Jaewoo Jung, KoreaU\n(2024-04-18)\nAccurate Densification Revising Densification in Gaussian Splatting\n(2024-04-10)\nX - Zhenjun Bao\nGradient Direction Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting\nCode | Arxiv | Zheng Zhang, UoHK\n(2024-05-09)\nImprove the clone and split with focus on gradients. (2024-05-27)\nThe Gaussian growth is based on the gradient. Thus, controling the growth requires to control the gradients. Use the number of visible views as the weights of gradients This reminds me the NeuRay which uses \u0026ldquo;visibility\u0026rdquo; to facilitate consistent geometry. Scale the gradient according to the distance to the camera to avoid the tendency that folaters exist near the camera. AbsGS: Recovering Fine Details for 3D Gaussian Splatting\nNo Code | Arxiv\n(2024-04-17)\nThe gradients of various Gaussians have different directions, so the gradients from covered pixels for a big Gaussian get canceled each other out. Thus, the big Gaussians won\u0026rsquo;t split Regularization GaussianPro: 3D Gaussian Splatting with Progressive Propagation\nCode | Arxiv | CoolPapers\n(2024-04-17)\nSupplement Gaussians with high uncertainty from the rendered normal maps.\nRegularization term: planer normal map loss\nPropagate: fuse neighboring pixels\u0026rsquo; normals\nAccurate geometry: evaluate new Gaussians with photometric consistancy.\nInterpolate FSGS: Real-Time Few-Shot View Synthesis using Gaussian Splatting\nCode | ProjPage | Zhiwen Fan, UoTexas\n(2023-12-04)\n3 views Unpooling the existing Gaussians to densify the sparse point cloud resulting from sparse views Depth regularization from a pre-trained monocular depth estimation model. Stochastic Process References:\n3D Gaussian Splatting as Markov Chain Monte Carlo Arxiv (Was trying to find the img of densifying. Search: \u0026ldquo;3D Gaussian Splatting ar5iv\u0026rdquo; in DDG) (2024-07-19)\nTask:\nRethink:\n感觉 densification 就是点云上采样的问题，可以单独研究上采样的问题，和 3DGS 结合就是蹭热点。 3DGS 的“本质”是：渲染快。 Attack Poison Problem Investigating\nMake a computation-intense attack on the service vendor server ::: aside\nReferences: Talk | 新加坡国立大学博士生卢嘉昊：Poison-Splat：针对3D高斯溅射的计算成本攻击 - TechBeat人工智能社区 ::: Supports\nf Extend\nHow to densify wisely?\nflowchart LR a(\"Wise densification\") a -- reverse --\u003e b(\"Suppress bad densification\") b --\u003e c(\"Defence computational-intense attack\") Solutions\nflowchart LR a(\"抵制下毒：计算成本的增加\") a --\u003e b(\"分类问题：判别一个 “高斯基元是否对场景重要？”\") a --\u003e c(\"去噪问题：消除输入图片中的（高频）噪声\") Attack in another way?\nflowchart LR a(\"攻击\") --\u003e b(\"加噪\") --\u003e c(\"DDPM\") Deblur Deblurring 3D Gaussian Splatting\nEmergent\n(2024-01-05)\nMLP \u0026ldquo;re-fuses\u0026rdquo; quaternion (covariance matrix), scaling matrix and position SLAM Generic 3DGS optimize a pre-generated point cloud.\nGaussian Splatting SLAM\nArxiv | ProjPage\n(2023-12-14)\nIncrementally construct the point cloud Optimize camera pose alongside point cloud via gradient descent according to the Jacobian of camera pose w.r.t. the 3D Gaussian map (screen function).\nConfine geometry consistency via\nPoint cloud growing and pruning\n哈工大博士分享：基于Gaussian Splatting的SLAM新发展与新论文（上）- bilibili - 计算机视觉life\nHow NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey\n(2024-02-29)\nAnti-alias TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering\nCode | Emergent | ProjPage | Linus Franke\n(2024-01-14)\nscreen-space image pyramid Light-weight MLP Edit GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting\nCode | arxiv | Yiwen Chen, Guosheng Lin\n(2024-01-21)\nIntegrated various papers. Gaussian semantic tracing ? Hierarchical Gaussian Splatting ? Diffusion Prior InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior\nCode | Arxiv\n(2024-04-18)\nSplatting GS++: Error Analyzing and Optimal Gaussian Splatting\nArxiv | Letian Huang, NJU\n(2024-02-04)\nMinimize the Taylor approximation error in the projective transformation\nResearch aspect: point cloud storage, performance , and robustness in sparse viewpoints\n360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming\nArxiv | Jiayang Bai, NJU\n(2024-02-04)\nSpherical surface, analogy to NeRF++ Simulation Dynamic 3DGS GMix.ai Dynamic Gaussian Splatting post Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle\nProjPage |Youtian Lin, Yao Yao\n(2024-02-21)\nMovement in a timestep GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting\n| Surfaced by Jason\n(2024-02-29)\n网格面的顶点对每个高斯分量进行参数化\nSC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes ~ CVPR 2024\nProjPage | Yi-hua Huang, Xiaojuan Qi\n(2024-03-06)\nSparse control points Decompose motion and appearance MLP predict 6DoF movement. MLP can\u0026rsquo;t reach accurate result, so it works for large-scale movement. And can it represent the multi-object interaction? like collision? Material Points References:\nPhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics NeuMA: Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics - NeuIPS‘24 Mentioned in Jason\u0026rsquo;s WeChat group. (2024-03-06)\nPhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics\nRelated:\nCode Pdf: Xuan Li, Chenfanfu Jiang Reasons:\nMPM solver! This method may produce reasonable effects as the underling physics simulation. (2024-12-02)\nNeuMA r2-Nips Depth Regularized EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting Emergent | Code\n(2024-02-10)\nDepth-guided supervision for handling occulusion Text23D GSGEN: Text-to-3D using Gaussian Splatting\nCode | Zilong Chen\n(2024-02-29)\nNo Pose InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds\nProjPage | Zhiwen Fan, UTAustin\n(2024-04-01)\n3DGS + DUSt3R\nCOLMAP-Free 3D Gaussian Splatting ~ CVPR 2024\nNo Code | Arxiv | Yang Fu, Xiaolong Wang, UCSD\n(2024-04-09)\nGiven a video, each frame produces a depth map and a local Gaussian set, which will be merged into a global Gaussian set.\nOptimize the camera pose affine transformation between 2 adjacent frams: the current frame and its previous frame.\nSpeed NeRF Prior RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS\nNo Code | CoolPapers | Michael Niemeyer ;\n(2024-04-16)\nTraining 3DGS with NeRF as supervision. Texture Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity Arxiv | Zhentao Huang, Minglun Gong, UofGuelph\n(2024-07-20)\nTask: The color attribute of each 3D Gaussian\nWhy matter: Rendering surface\nProblem: SH coeffs\nSolution:\nSH coeffs -\u0026gt; RGB,opacity\nInit Point Cloud VGGT Replace Colmap Motivation\nColmap functionality: Generate sparse point cloud\nVGGT functionality:\nImplementation: VGGT + 3DGSr1-快速\nUse Brush ::: aside\nReferences: VGGT＋3DGS快速生成高质量场景 - Yuinix ArthurBrussee/brush :::\nResults\nSpeed up\nProcess more input images\nVGGT Bad Pose Supports:\nIt is said the pose are not usable for gs r1-Dscd ::: aside\nReferences: {{{ Discord - MrNeRF \u0026amp; Brush | camera-pose-estimation }}} ::: VGGT + BA Problem:\n(2025-09-22T12:55)\nBundle Adjustment can improve the poses generated by VGGT\nAlthough the poses are not as accurate as those from COLMAP, it could be an alternative when Colmap fails. r1-Dscd ::: aside\nReferences: {{{ Discord - MrNeRF \u0026amp; Brush | camera-pose-estimation }}} ::: Number of Points Problems\n空间中的高斯基元数量是否指定？\n有的 3dgs 实现需要指定高斯基元的数量：LichtFeld (MCMC)\n而如果不指定数量，即允许随场景规模扩张增加的话，这会是一个潜在的安全漏洞: 计算成本攻击\n::: aside\nReferences: MrNeRF/LichtFeld-Studio ::: ","date":"2023-11-30T13:30:00Z","image":"http://blog.zichen.uk/post/writenotes/model/splats/c-symp-3dgs/feat_img-nerf_vs_3dgs.webp","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/c-symp-3dgs/","title":"Sympo: Render - Points | 3DGS"},{"content":" 换元法 The area of a cube in orthogonal coordinate system and the spherical coordinate system.\n直角坐标与极坐标的互化中，为什么 dxdy=rdrdθ？ - 予一人的回答 - 知乎\n立体角\n如何推导柱坐标，球坐标的体积元和极坐标的面积元？ - 半个冯博士的回答 - 知乎\nChange of variables | MIT 18.02SC Multivariable Calculus, Fall 2010 - Youtube\n(2024-06-17)\nI think the change of variables is used for integral, specifially, when a small area, e.g., $dxdy$, is replaced with another facet $dudv$ of another space.\n$$ \\iint_R (4x^2 - y^2) dxdy \\overset{\\substack{u=2x+y; \\\\\\ v=2x-y}}{→} \\int_0^2 \\int_{-v}^0 (uv)^2 \\frac{1}{4} du dv $$ y 2 2 x 2 + x y - = y 2 = x 0 - 2 ⋱ ⋱ ⋱ ⋱ V 2 u However, when changing the variable 𝑢 in a function $f(𝑢)$ to another form, e.g., $u = 2(t+1)$, the variable-changed function $f_u(t)$ is just replacing 𝑢 with $2(t+1)$:\nGiven $f(u) = u^2 + 1$, the form represented by 𝑡 is $f_u(t) = (2(t+1))^2+1 = 4t^2+8t+5$. There is no infinitesimal, such as 𝑑𝑢, that requires maintaining equal area when it gets replaced with the variable t.\nThe region to be integrated over can be simplified by doing the change of variable.\nChange of variables (single integral and substitution) | Lecture 30 | Vector Calculus for Engineers\nChange of Variables and the Jacobian - Serpentine Integral\nAn infinitesimal area varies in different coordinate system because shapes of a unit area are distinct. In other words, different coordinate systems have varous scales.\nIn polar coordinate system:\n$$ x = r⋅cosθ \\\\\\ y = r⋅sinθ $$Jacobian matrix is a function that depicts how much each dimension of the source space should be scaled to align with another coordinate system at an arbitrary position.\nThe scale factor is the ratio between a target dimension and a source dimension. Thus, by multiplying those factors, the source space will be scaled to the target space.\n$$ 𝐉 = \\begin{bmatrix} \\frac{dx}{dr} \u0026 \\frac{dx}{dθ} \\\\\\ \\\\\\ \\frac{dy}{dr} \u0026 \\frac{dy}{dθ} \\end{bmatrix} $$Because an infinitesimal area $dx⋅dy$ only has magnitude (without direction), the area scaling factor should be just a positive real number.\nTo make the area in the source space to be the same in the target space, the scaling factor should be the ratio of two unit areas.\n$$ \\frac{d(x,y)}{d(r,θ)} $$That is $f(x,y) = g(r,θ) dx dy$, so the integrated areas equals:\n$$ ∬_D f(x,y) dx dy = ∬_E g(r,θ) dx dy = ∬_E g(r,θ) ∂(x,y)/∂(r,θ) drdθ $$And the calculation of an area is just the cumulative product of every dimensions.\nTherefore, the abosolute value of the determinant of the Jacobian matrix is taken.\n$$ |det(𝐉)| = |dx/dr * dy/dθ - dx/dθ * dy/dr | = |cosθ*rcosθ - rsinθ*(-sinθ)| = |r| $$ 11.9: Change of Variables - Mathematics LibreTexts\n(2023-11-28)\nThe determinant of Jacobian matrix is responsible for the infinitesimal area, not the integrand.\nThe new integrand is just substituting the old variable with the tranformation with new variable.\nf(x,y) -\u0026gt; g(rcosθ, rsinθ)\ndxdy -\u0026gt; r drdθ\nf(x,y) = g(rcosθ, rsinθ) * r\nThe change of variables theorem has two aspects:\nf ( x , y ) C M h u a l n t g i i p n l g y v | a d r e i t a ( b J l ) e | g ( r , θ ) (2024-07-07)\n形象理解雅可比行列式与多重积分-工程形象 ","date":"2023-11-27T19:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/change_of_variables/","title":"memo: Calc | Change of Variables Theorem"},{"content":"Source video: Continuous time convolution with impulses - ProfKathleen\nConvolution For example, there is a man keep eating all the time. $f(t)$ is the food eaten at time t, and $h(t)$ is the food mass changing function for a period of digestion time t. For example, after the duration $(t-τ)$ of digestion, the amount of food eaten at time $τ$ becomes $h(t-τ)$.\nTherefore, the total food reamined in stomach at time t is f convolving with h:\n$$ y(t) = f(t) * h(t) = ∫_{-∞}^{+∞}f(τ)⋅h(t-τ) dτ $$Intuitively, the value $x(τ)$ is scaled by a factor from τ time ago. Thus, the function $h(t)$ looks like in reverse.\nGiven $f = x(t)$ and $h = δ(t)$, the existing amount at time t is:\n$$ y(t) = f * h = ∫_{-∞}^{+∞} x(τ) h(t-τ) dτ $$ Delta function $δ(τ)$:\n0 1 τ $δ(τ-3)$: Right shift $δ(τ)$ by 3.\nWhere the 0 was becomes -3. Therefore, all the coordinates minus 3, i.e., τ becomes τ-3.\n0 3 1 τ $δ(-τ-3)$: Rverse $δ(τ-3)$:\n- 3 1 0 τ $δ(t-τ-3)$: Shift $δ(τ-3)$ to t, which is a constant.\nWhere the τ-3 = 0 becomes t, so all coordinate plus t:\nt - 3 1 t τ Convolve with Impulse Given $x(t)$\nt The existing amount of a system containing two functions x(t) and δ(t-3) at time t is:\n$$ y(t) = x(t) * δ(t-3) = ∫_{-∞}^{+∞} x(τ) δ(t-τ-3) dτ $$Multiply x(τ) by the reversed impluse:\nt - 3 t τ Delta function is 0 except for τ = t-3, therefore, only x(t-3) will be computed:\n$$ y(t) = x(t) * δ(t-3) = ∫_{-∞}^{+∞} x(t-3) δ(t-τ-3) dτ $$And in that integral, x(t-3) has nothing to do with τ, so it can be pulled outside the integral:\n$$ y(t) = x(t) * δ(t-3) = x(t-3) ∫_{-∞}^{+∞} δ(t-τ-3) dτ \\\\\\ = x(t-3)*1 \\\\\\ = x(t-3) $$By convolving with an impulse function, x(t) is shifted (based on the origin) to where the impulse is.\nThis conclusion can be generalized to any f.\nBox function 0 1 f ( t ) 3 t ✶ 0 1 δ ( t - 1 ) t = 0 1 y 2 ( t ) 4 t Two impulses Convolution is linear. Compute separately and sum together.\n0 1 f ( 1 t ) 3 t ✶ 0 1 1 δ ( t - 1 ) 1 4 / 2 t = 1 2 1 y ( 4 t ) 1 6 / 2 t Impulse train Convolving with a infinite sum of delta function: $Σ_{n=-∞}^{+∞} δ(t-n)$\nThe replicas of the signal overlaps:\n0 0 1 1 f y 2 ( 2 ( t t 3 ) 3 ) t t ✶ s u m - 1 3 2 0 0 δ 1 ( t 1 2 - 1 2 y 3 ) ( 3 t ) t = t ","date":"2023-11-20T11:40:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/convol_impulses/","title":"memo: Calc | Convolution with Impulses"},{"content":"(Feature image from 3D Gaussian Splatting中的数学推导 - 八氨合氯化钙的文章 - 知乎)\nEWA Splatting - UMD\nTL;DR (2024-08-11)\nQ群有个人（小虫 24/08/04 20:19）的总结到位：\nSplatting 就是对 3D 高斯的投影的一个线性近似，其他还是计算机视觉的知识。\n(2024-08-23)\nIllustion from Yifan\u0026rsquo;s demo:\nProjection points (discs for surface splatting) in the object space onto an image plane (from a certain viewpoint) to render images. These discs\u0026rsquo; opacities are retrieved from the footprint table (screen space).\n2 regularization terms: projection (for smooth normal) and repulsion (for uniform position distribution)\n(2023-11-20)\nAbs \u0026amp; Intro (2023-11-29)\nThis work combines elliptical splat kernels with a low-pass filter to avoid aliasing without introducing much blurring.\nFootprint function is an integral across a 3D reconstruction kernel along the line of sight, so it computes the contribution of an individual 3D kernel to a screen point. Those contributions form a splat, which can be precomputed, thus improving efficiency of volume rendering.\nTwo types of volume rendering approaches can be distinguished based on the availability of volume data.\nR a S p y N a o e m i C R p n a F l t n p s e s e i t a x i r e n l g f a r V P P o i o F S l x P i r p u e r n o l m l e t n a e c s F t t e o o s d m m r a i p w t t u a a s t r e d r d a R y s e S p n p l d l a e I ⬯ ⬯ a t r m ⬯ ⬯ t f t i a t o i S n g i r n c g B e n w g r a g a e c p r e k l d n a n ( e s f p o l p o a i t t x p e r l i n c t o l t o a r b l e ) In NeRF, each sampled point is queried out from a MLP (sampling an implicit field), and then a pixel is synthesized by alpha-compositing along the viewing ray. Thus, there isn\u0026rsquo;t physical volume data in NeRF.\nIn splatting, each volume data (as a continuous distribution) is projected onto the screen perspectively yielding various splats (滩涂) before rendering (i.e., A pixel emits a ray). Splats don\u0026rsquo;t result in pixels directly as they still need to do combination (to produce the final weight of the 3D point corresponding to the pixel).\nEach splat is integrated along line-of-sights, resulting in footprint (opacity, =alpha here) functions that are recorded on the screen in advance. And when rendering from an arbitrary viewpoint, the expectation of all splats\u0026rsquo; contributions is calculated, forming a function that maps a position to an rendered attribute (color).\n(2024-01-21) The comparison between ray casting and splatting is somewhat like that of a CPU and GPU:\nVideo (May05,25): Mythbusters Demo GPU versus CPU - YouTube - Angelo Tadres Two ways of rendering a point dataset:\nPoints ➔ triangle meshes ➔ \u0026ldquo;surface rendering\u0026rdquo; ➔ image Points ➔ \u0026ldquo;splatting\u0026rdquo; ➔ image P o i n t s T l r a i t a i n o g n u S p l a m t e t s i h n e g s R e d u c t i o n I m a g e Gaussian is a universal rendering primitive for volume splatting and surface splatting.\nPrevious Work Slice-based volume rendering\nProgressive\nfor an arbitrary position in 3D, the contributions from all kernels must sum up to one.\nSplatting (2023-11-22)\nA point cloud (data set) is a discrete sampling of a continuous scene function.\nIf a point is considered as a continuous function, then the point cloud corresponds to a continuous representation on the 2-D screen space corresponding to the 3D scene.\nThus, an image (pixel grid) reflecting the scene comes from sampling the intemediate reconstructed continuous screen, instead of directly sampling the scene, that is, \u0026ldquo;resampling\u0026rdquo; (sampling again).\n(2024-03-16)\nAfter reading the code, I figured out the splatting and alpha compositing (forward) are 2 separate procedure. The point cloud is a sample of the scene. By performing splatting, each point is converted into a disc in the (viewing-) ray space. Such that, the opacities of each Gaussian have been determined. When rendering images, given a camera, each pixel emits a ray, and then the sampling points on a ray query their opacities from the ray sapce, to perform alpha compositing. In this way, an image is formed. Therefore, an image is a sampling of the ray space. By considering each point in dataset as a reconstruction kernel, which is a continuous attribute-weight function (e.g., opacity: the weight of color) in the object space, the continuous scene function can be constructed by aggregating all kernels.\nFormally, the weight assigned to a 3D point 𝐮 in the object space for a certain attribute (e.g., color) $f_c(𝐮)$ is a mixture (weighted sum) of weights calculated from each reconstruction kernel:\n$$f_c(𝐮) = ∑_{k∈IN} wₖ rₖ(𝐮)$$ $rₖ$ is a reconstruction kernel centered at a data point $𝐮ₖ$. A kernel is a function mapping a position to a weight (opacity) for a property (color).\nIn this work, rₖ is chosen to be a 3D Gaussian whose mean vector is the position of a data point 𝐮ₖ, and\u0026hellip; what\u0026rsquo;s the variance?\n$rₖ(𝐮)$ is the \u0026ldquo;weight of an attribute\u0026rdquo; (opacity of a color) of an arbitrary voxel 𝐮 inside the kernel $rₖ$.\nA kernel center 𝐮ₖ will perform transformations as follows:\n⌈ ⌊ P e c x x r c e ₀ ₁ o t n ⌉ ⌋ S j i t c - o e ⬯ r n r e e ★ n ` F p i ⋅ o r n o i t x ` t n e ₂ - t g ⋅ r a ⬯ l L f o i g w l - t ⊗ p e a r h s s ⌈ | ⌊ R s ( s x x x 𝐱 a p N c ₀ ₁ ₂ ₖ y a D a ⌉ | ⌋ c C l e i w n 𝐭 ϕ t / g + r o ) 𝐉 p a ⋅ r n ∆ o s 𝐭 j f ⌈ | ⌊ C s ( t t t 𝐭 a p P ₀ ₁ ₂ ₖ m a e ⌉ | ⌋ e c r r e s a p 𝐖 φ t ( e r E c 𝐮 v a y t + i n e i 𝐝 e s ) v w f e ) ⌈ | ⌊ u u u O s ₀ ₁ ₂ 𝐮 b p ⌉ | ⌋ ₖ j a e c c e t Prime $'$ indicates the ray space.\nx₂ is the Euclidean distance (L2 norm) from the 3D point 𝐱 to the projection center for integrating the total opacity given by a kernel.\nRay space is a \u0026ldquo;plane\u0026rdquo; because the x₂ is not a practical depth anymore and only used for object culling. And the screen is just a footprint table:\nThus essentially, ray space is the projection plane, while screen is another space recording the contributions of a 3D kernel to each 2D point.\nRoughly, the main focus on the ray space is on the near plane. For example, the pixel covered by a kernel projection is determined on the near plane.\n(2024-07-10)\nMaybe the \u0026ldquo;footprint\u0026rdquo; means a kernel\u0026rsquo;s footprint on the screen (Lookup Table). All kernels accumulate their respective opacities by intergrating along the line of sight (not sunlight), resulting in a footprint value ($qₖ(𝐱) = ∫₀ᴸ rₖ'(𝐱,ξ)dξ$) on the screen. During rendering, an image pixel\u0026rsquo;s color is calculated based on these footprints: a weighted sum of footprints, where the weight $wₖ$ is determined based on the distance between the image pixel and the kernel center.\nSplatting makes a discretization comparing the normal volume rendering. A pixel color is an integral over particles on a camera ray in the volume rendering equation (VRE), while splatting computes the pixel color as a summation of multiple kernels (footprints). So, an integral operation is replaced with a summation.\nThis simplification is achieved by regarding a position as a weighted summation of multiple reconstruction kernels, and applying with some approximations.\nThis reminds me the low-rank decomposition. A continuous signal is decomposed into multiple components. By switching the sequence of integration and the summation operations, the footprint function of a kernel is introduced. And the integral over the line of sight changes to a summation of kernels. Thereby, the weighted footprint: $wₖqₖ$ of a kernel corresponds to the alpha of a particle in the normal VRE. And the accumulated transmittance of a particle in the normal VRE corresponds to the product of all nearer-to-camera kernels\u0026rsquo; transmittances: $\\prod_{j=0}^{k-1} (1-wₖqₖ)$\nIn summary, the color of an image pixel rendered from multiple kernels is: $c = \\sum_{k\\in \\\\N} cₖ wₖqₖ \\prod_{j=0}^{k-1} (1-wⱼqⱼ)$, with some approximations applied.\nIn contrast, the summation in NeRF is derived from the point-sampling operation on a ray. (2024-07-15)\nIn NeRF, the opacity of a point on a ray is used as the alpha (self-occlusion) of the point, because points on a ray directly contribute to a pixel. The point\u0026rsquo;s alpha in NeRF = 1 minus e raised to the negative power, which is the product of volume density and the length of a segment of the ray.\nSuccinctly, for a point i, its alpha is its opacity: $$αᵢ = 1-e^{-σᵢδᵢ}$$ The accumulated transmittance of each sample point is a cumulative production of previous (1-alpha)s.\nWhereas, in splatting, the alpha of a point contributing to a pixel color on a ray is a product of a weight and a line integration over the segment of the ray intersected with a kernel: $$αₖ = wₖqₖ$$ , where the line integration qₖ is the opacity of a kernel. In other words, the alpha of a point at the intersection of a kernel and a ray for a certain pixel is the kernel\u0026rsquo;s opacity scaled by a weight.\n(2025-05-05T22:22)\n在 NeRF 中，采样点本身就是 发光粒子，而 splatting 在渲染时，采样点自己无颜色和不透明度，采样点上的颜色/不透明度是 重建核的分量。\n所以，NeRF中的采样点的 “mask” (alpha) 就是它的不透明度 opacity，就是它的体密度 sigma\n(2024-06-17)\nW is the rotation matrix in w2c, d is the translation vector.\nThe projective transformation ϕ includes two operations: perpective projection and reassigning the x₂ coordinate.\nEach Gaussian\u0026rsquo;s opacity is computed in the ray space through an integral over the line of sight sunlight.\nThe screen (in the screen space) serves as a bound record for integrating a Gaussian along the line of sight sunlight.\nResampling (2023-11-24)\n\u0026ldquo;Ideal sampling\u0026rdquo; means no aliasing and the original continuous signal can be reconstucted from the sampled signal exactly.\nAntialiasing Spatial signal (Screen function) ➔ Samples (Image) ➔ Frequency response (Coefficient of each basis) given by Fourier Transform.\n0 f ₁ f ₂ f ₃ T f ₙ x 2 π ⁄ T ⋮ w Sampling in time (or spatial) domain: the continuous signal is multiplied by an impulse train with a period T:\n$$ y(t) = x(t) ⋅ ∑_{k=-∞}^{∞} δ(t-kT) = ∑_{k=-∞}^{∞} x(t) ⋅ δ(t-kT) $$ Multiplication in time domain corresponds to convolution ⊗ in frequency domain.\n$$ \\begin{aligned} Y(ω)\u0026= \\frac{1}{2π} \\left(X(ω) ⊗ \\frac{2π}{T} ∑_{k=-∞}^{∞} δ(ω-k \\frac{2π}{T}) \\right)\\\\\\ \u0026= \\frac{1}{T} ∫_{-∞}^{∞} X(λ)⋅∑_{k=-∞}^{∞} δ(ω-k \\frac{2π}{T}-λ) dλ \\\\\\ \u0026= \\frac{1}{T} ∑_{k=-∞}^{∞} X(ω-k\\frac{2π}{T}) \\end{aligned} $$ The frequency spectrum Y(ω) of the sampled signal is a sum of the original signal\u0026rsquo;s spectrum X(ω) shifted according to the corresponding impulse train in the frequcy domain.\nRefer to: Fourrier transform of an impulsion train - Inria\nSampling with an impulse train: Lecture 12 - Stanford University\n$2π/T$ is the frequency of the sampling impulse. So, the space between 2 spectrum replicas is $\\frac{2π}{T}$.\nDenoting the highest frequency of the temporal signal as $w_m$, to fully separate 2 neighbor spectrums, the interval $2π/T$, i.e., the sampling frequency $w_s$, must be larger than (no equal) $2w_m$. Otherwise, spectrum replicas will overlap and an individual intact spectrum can\u0026rsquo;t be selected out by a box function to reconstruct the temporal signal.\nConversely, given a temporal impulse train with a sampling frequency $w_s$, the highest frequency of the continuous signal to be sampled shouldn\u0026rsquo;t exceed $\\frac{w_s}{2}$, which is called the Nyquist frequency.\nTo match the Nyquist frequency of the discrete desired grid, the time-domin signal can pass a low-pass filter before sampling, in contrast to increasing sampling frequency causing more computation cost.\nMoreover, the width of the low-pass filter to be convolved is a tradeoff for computation efficiency. Thus, aliasing is inevitable.\nMip-Splatting (231127) adds a constrains to the size of kernels based on the $w_s$ induced by the input views, and changes the 2D Gaussian low-pass filter to a 2D Mip filter.\nThus, alliviate the dilation effect when zooming out and the high frquency artifacts when zooming in.\nCont. Screen The \u0026ldquo;weights-mixture\u0026rdquo; function $f_c(𝐮)$ determinating an attribute\u0026rsquo;s weight at an arbitrary position 𝐮 in source space are transformed as follows:\nI D m i a s g c e r e t A C M I T e l o u m r p m l p a h p t u i a o i l n s p s i l e t y 2 C e g D o _ n c S t ' c i ( r n 𝐱 e u ) e o I k n u n e s t r e n g e r l a s t e L f o i w l - t p e a r s s ⦿ ⦿ ⦿ ⦿ g 3 S ⦿ _ D p ⦿ c a ⦿ ( R c ⦿ 𝐱 a e ) y P P e r r o s j p e e c c t t i v e ⦿ ⦿ ⦿ 3 C ⦿ f D o ⦿ _ n ⦿ c s t ⦿ ( c . ⦿ 𝐮 e ) n e The coordinates 𝐱 in the 3D ray space contain the 2D perspectively-projected screen coordinates (t0/t2, t1/t2) and the 3rd dimension is set to the L2 norm of 𝐭, which is the coordinates in the camera space.\n𝐱 is also used to refer to the corresponding screen point.\nThree operations in splatting equation:\nFor splatting approach, a scene consists of reconstruction kernels spreading out the space. Thus, a volume at 𝐮 in the source space is a combination of contributions from all reconstruction kernels:\n$$ f_c(𝐮) = ∑_{k∈ N} wₖ rₖ(𝐮) $$ 𝐮 is a position in the source (object) space; $rₖ$ is a reconstruction kernel centered at 𝐮ₖ. $rₖ(𝐮)$ is the weight for an attribute stored in the location 𝐮 computed according to $rₖ$; $wₖ$ is the coefficient of each weight to produce a unified weight for an attribute on the location 𝐮; An attribute-weight function $f_c$ in the source space will be finally projected onto the screen as a 2D continuous function that produces the weight for an attribute on any screen position 𝐱:\n$$ g_c(𝐱) = \\\\{ P( f_c ) \\\\}(𝐱) = \\left\\\\{ P \\left( ∑_{k∈ N} wₖ rₖ \\right) \\right\\\\}(𝐱) $$ 𝐱 is not a pixel, because the screen is still continuous. A discrete image is sampled from the screen via an impulse train: $g(𝐱) = g_c(𝐱) i(𝐱)$ I use curly braces to imply a function. Because a non-affine projection operation can be approximated as a linear transformation through the 1st-order Taylor expansion, the weighted sum and linear projecting operations can be flipped:\n$$ g_c(𝐱) = ∑_{k∈ N} wₖ⋅ P( rₖ) ) (𝐱) $$ Thereby, each kernel performs projection first, and then combined together. Commutative: switching operators means switching the sequence of operations. To avoid aliasing, the screen function before being sampled to an image needs to pass a low-pass filter $h$ to meet the Nyquist frequency:\n$$ \\begin{aligned} \\\\^g_c(𝐱) \u0026= g_c(𝐱) ⊗ h(𝐱) \\\\\\ \u0026= ∫_{-∞}^{∞} g_c(\\bm η)⋅h(𝐱- \\bm η) d \\bm η \\\\\\ \u0026= ∫_{-∞}^{∞} ∑_{k∈ N} wₖ⋅ P(rₖ)(\\bm η)⋅h(𝐱-\\bm η) d\\bm η \\\\\\ \u0026= ∑_{k∈ N} wₖ ∫_{-∞}^{∞} P(rₖ) (\\bm η) ⋅ h(𝐱-\\bm η) d\\bm η \\\\\\ \u0026= ∑_{k∈ N} wₖ ⋅ P(rₖ) (𝐱) ⊗ h(𝐱) \\end{aligned} $$ Every projected kernel (mapping a position to a weight) is filtered by h. The weighted sum is done after projection and convolution. An ideal resampling kernel: $$ ρₖ(𝐱) = ( P(rₖ) ⊗ h )(𝐱) $$ Therefore, any location in the 2D continuous screen space is a combination of the projected and filtered reconstruction kernels $ρₖ$ evaluated at that location.\nRendering References:\n【学习记录】论文精读 A Hierarchical 3D Gaussian Representation for Real-Time Rendering（全） - 来点盐巴吧 (2024-07-09)\nReview:\n(Volume) Rendering process differs from splatting process.\nRendering composites colors and opacities at multiple \u0026ldquo;positions\u0026rdquo; on the ray emitted from the camera (eye) by an integral, resulting in a pixel\u0026rsquo;s color.\nWhereas, splatting computes the opacity of each kernel as an integral over a segment on the sunlight path projected onto the screen.\nThe direction of rendering integration is from near to far, while the direction of splatting integration is from far to near.\nRendering can be of arbitrary viewpoints. Whereas, splatting has a fixed direction, i.e., the direction of incident sunlight.\nRendering is performed after splatting. Splatting calculates opacities for kernels. Then, the rendering process queries the opacities at various \u0026ldquo;positions\u0026rdquo; on the camera ray from these reconstruction kernels .\nThe opacity at any \u0026ldquo;position\u0026rdquo; in the ray space is a weighted sum of all the kernels\u0026rsquo; opacities.\nBy substituting the weighted sum into the volume rendering equation, a version specifically tailored for splatting is derived.\n(2024-07-11)\nVolume rendering performs alpha compositing to form images. Splatting is calculating the Gaussian\u0026rsquo;s opacity.\nThe fundamental image formation principles in NeRF and 3DGS both are alpha compositing. Their difference is the scene representation: NeRF is stratified sampled points (coarse+fine sampling), while 3DGS uses ellipsoids.\nIn NeRF, each sample point on the ray has an alpha value.\nFor splatting, each 3D Gaussian is first converted to a disc with a consistent opacity across the entire disc. The opacity of a disc is an integration over the x2 direction of the Gaussian in ray space along the Line of sight.\nUsing an integral to calculate a disc\u0026rsquo;s opacity is because of the switch between the Gaussians\u0026rsquo; mixture and the volume rendering integral (along the pixel ray marching). After this switch, a pixel ray becomes a combination of all 3D Gaussians in the space (although only Gaussians intersected with the pixel ray have contributions to the pixel color). And the integration of a 3D Gaussian over the intersection segment with line of sight can be regarded as the opacity of a disc corresponding to the 3D Gaussian.\nThe pixel ray intersects with multiple discs. Each disc has an alpha value. The alpha value is a weighted disc\u0026rsquo;s opacity: wₖqₖ, where the weight wₖ depends on the distance from the pixel to the disc center.\nIn summary, a disc represents a portion of a 3D Gaussian. The portion is a line integral over the segment of the line of sight intersected with the 3D Gaussian.\n(2025-05-11T14:16)\nSplatting 是 投影 3D 椭球，而 Rendering 是为各 像素计算颜色r1-盐巴 (2025-05-14T22:00)\n(Notes on Cursive) (2023-11-25)\n(In NeRF,) Using opacity (self-occlusion) $αₜ$ to compute the color based on alpha compositing:\n$$\\rm c = ∫_{t=n}^f Tₜ αₜ cₜ dt, \\quad Tₜ = ∏_{s=n}^{t-1} (1-αₛ) ds $$If using volume density σ to compute, then the pixel color is:\n$$ \\rm c = ∫_{t=n}^f Tₜ (1 - e^{-σₜ δ}) cₜ dt, \\quad Tₜ = e^{- ∫_{s=n}^{t-1} σₜ δ ds} $$ Alpha = $1 - e^{-σδ}$, when the volume density σ is 0, the alpha (opacity) is 0: The filter itself doesn\u0026rsquo;t show its color. It\u0026rsquo;s as if there were no particles there, and the rays pass through without any changes.\nδ is a unit interval on the ray. (The width of a filter.)\nNeRF can use 1 pixel to observe all points in the space, while splatting use all points to composite 1 pixel.\nC a m e r N a e □ R s F p a c e R a E y y S e ⦿ p s ⦿ - l p - a a ⦿ ⦿ t c - t e S - - - - i c ❚ ❚ ❚ ❚ □ n r g e P e r n o j I e n c t t e i g o r n a t c e e n t e r This paper uses the low albedo approximation of the volume rendering, so that the intensity (a 1D attribute) of a screen point 𝐱 corresponding to a ray with length 𝐿 is computed as:\n$$ I(𝐱) = ∫_{ξ=0}^L c(𝐱,ξ) ⋅f_c'(𝐱,ξ) ⋅ e^{-∫_{μ=0}^ξ f_c'(𝐱,μ)dμ}dξ $$ L is the distance from eye to the projection center on screen.\n▹ $c(𝐱,ξ)$ is the intensity on a 3D point (𝐱,ξ) in ray space. 𝐱 is (x0,x1), ξ is x2.\n▹ $f_c'(𝐱,ξ)$ is the point\u0026rsquo;s opacity (weight for color) in the ray space. ▹ The exponential term is the accumulated transmittance prior to the point at ξ. Why is it in this form? Is a transmittance $e^{-f_c'(𝐱,ξ)}$?\nK e r n ( I l s e x 3 n i i l ₀ D t n g , e e h r x R g t ₖ ₁ a r o , ' , y a f ( ξ t L 𝐮 ) ℮ S e ) ᵀ y p ℮ a ⦿ c - e ⦿ - - - ⦿ - - - - - S | • ❚ ❚ ❚ | c ↙ r P 𝐱 e r : e o ( n j c S x e e c ₀ c n V r , t t i e x i e e e ₁ o r w n ) n i ᵀ n c g o o r r a d y i n a t e s The 3D ray space is not perspective because depth has been divided and then reset to the distance from the projection center. Thus, kernels in ray space are projected onto the screen orthogonally by just omitting their depth.\nAnd the probing line L from eye to the projection center are projected orthogonally onto the screen as well.\nTherefore, integrating along L in 3D space corresponds to integrating the viewing ray between (x₀,x₁) and the projection center on 2D screen.\nRewrite the volume rendering equation using projected kernels in the ray space.\nSince the position 𝐱 in the ray space is the projection of position 𝐮 in the source space after a viewing transformation φ (w2c) and a projective transformation ϕ (Note: NDC= projection with depths kept + scaling. Here is NDC without scaling), there are:\n$$ \\begin{aligned} 𝐱 \u0026= ϕ(φ(𝐮)) \u0026 \\text{Project a location to ray space} \\\\\\ 𝐮 \u0026= φ⁻¹(ϕ⁻¹(𝐱)) \\\\\\ f_c(𝐮) \u0026= ∑_{k∈IN} wₖ rₖ(𝐮) \u0026\\text{Combination in object space} \\\\\\ f_c'(𝐱) \u0026= ∑_{k∈IN} wₖ rₖ(φ⁻¹(ϕ⁻¹(𝐱))) \u0026\\text{Change of variable} \\\\\\ rₖ'(𝐱) \u0026= rₖ(φ⁻¹(ϕ⁻¹(𝐱))) \u0026 \\text{Kernel k in the ray space} \\\\\\ f_c'(𝐱) \u0026= ∑_{k∈IN} wₖ rₖ'(𝐱) \u0026\\text{Written in a consistent form} \\end{aligned} $$ Weights-mixture (opacities-mixture) function $f_c'(𝐱)$ is the corresponding representation of $f_c(𝐮)$ in the ray space, merely substituting variables without scaling the axes.\nThus, the weight for an attribute on 𝐱 is a summation of all projected kernels. For example, the assigned opacity at 𝐱 in the ray space is a weighted sum of all opacities evaluated from each kernel. (Gassian mixture model)\nSubstituting the opacity $f_c'(𝐱)$ into the rendering equation:\n$$ \\begin{aligned} I(𝐱) \u0026= ∫_{ξ=0}^L c(𝐱,ξ)⋅f_c'(𝐱,ξ) ⋅ e^{-∫_{μ=0}^ξ f_c'(𝐱,μ)dμ}dξ \\\\\\ \u0026= ∫_{ξ=0}^L c(𝐱,ξ)⋅ ∑_{k∈ℕ} wₖ ⋅rₖ'(𝐱,ξ) ⋅ e^{-∫_{μ=0}^ξ ∑_{j∈ℕ} wⱼ rⱼ'(𝐱,μ) dμ} ⋅dξ \\\\\\ \u0026= ∑_{k∈ℕ} wₖ \\left( ∫_{ξ=0}^L c(𝐱,ξ)⋅ rₖ'(𝐱,ξ) ⋅ e^{-∑_{j∈ℕ} wⱼ ⋅∫_{μ=0}^ξ rⱼ'(𝐱,μ) dμ} ⋅dξ \\right) \\\\\\ \u0026= ∑_{k∈ℕ} wₖ \\left( ∫_{ξ=0}^L c(𝐱,ξ)⋅ rₖ'(𝐱,ξ) ⋅ ∏_{j∈ℕ} e^{- wⱼ⋅∫_{μ=0}^ξ rⱼ'(𝐱,μ) dμ} ⋅dξ \\right) \\end{aligned} $$ A point (𝐱,ξ) in ray space indicates the 3rd dim (x₂) is ξ and the first 2 dims (x₀,x₁) is 𝐱.\nThe rendered color is a composition of each point\u0026rsquo;s color c(𝐱,ξ) multiplied by a weight given by the weights-mixture function $f_c'$ at the point on the line of sight 𝐿. And the weights-mixture function $f_c'$ is a combination of the evaluations of all kernels $rₖ$ in the ray space.\nCommutative: Sum all kernels evaluated at a point followed by integrating all points on a ray = integrate all the points in each kernel followed by summation for all kernels.\nThe integral for the evaluations at all ray points, considering rₖ as the kernel to be evaluated: $qₖ(𝐱) = ∫₀ᴸ rₖ'(𝐱,ξ)dξ$ is called the footprint function of rₖ, which is used as the approximate opacity for all voxels in the support of a kernel.\nAnd the exponential product term can be approximated as $∏ⱼᵏ⁻¹(1-wⱼqⱼ(𝐱))$ by applying Taylor expansion (see below), such that it can be regarded as the accumulated transmittance very harmoniously.\n(2023-12-31)\nIn this way, the rendering equation aligns with the alpha compositing: equation opacity transmittance volume rendering α (1-α) splatting wₖqₖ (1-wₖqₖ) A projected rendered attribute in the screen space, such as the above $I(𝐱)$, is collectively referred to as $g_c(𝐱)$.\nApproximations To simplify the computation of $g_c(𝐱)$, 4 approximations are applied.\nLocal support: the regime of each 3D kernel is range-limited and not overlapped with other kernels on the ray.\nlocal support\nL s 3 i i D N n g k o e h e R t r a O o , n y v f L e L s e l S o u r s E p c p l y a a p a e c l o p ⦿ e r t ⦿ S - c • ❚ ❚ ❚ ❚ ❚ ❚ r e P c E e r e x n o n t j t e e e n c r t t i o n The attribute (e.g., color) in the local support region of a 3D kernel along a ray is assumed to be constant. Specifically, the $c(𝐱,ξ)$ over the line of sight (for all ξ) is constant, while other rays (i.e., 𝐱) passing through the kernel can have different colors. Hence, c(𝐱,ξ) can be put outside of the ray integral.\n$$g_c(𝐱) = ∑_{k∈IN} wₖ cₖ(𝐱) \\left( ∫_{ξ=0}^L rₖ'(𝐱,ξ) ⋅ ∏_{j∈IN} e^{- wⱼ⋅∫_{μ=0}^ξ rⱼ'(𝐱,μ) dμ} ⋅dξ \\right)$$ Assume the attenuation factor of voxels in a kernel along a ray is constant, i.e., the transmittance isn\u0026rsquo;t dependent on previous voxels passed through by the ray, but equals the accumulation of all voxels. Thus, the upper bound of the integral becomes L:\n$$\\rm exp(-∫_{μ=0}^{ξ} f_c'(𝐱, μ) dμ) → exp(-∫_{μ=0}^{L} f_c'(𝐱, μ) dμ)$$In this way, the transmittance isn\u0026rsquo;t restricted by ξ of the outer rendering integral and can be pulled outside.\n$$g_c(𝐱) = ∑_{k∈IN} wₖ cₖ(𝐱) \\left( ∫_{ξ=0}^L rₖ'(𝐱,ξ)dξ \\right) ⋅ ∏_{j∈IN} e^{- wⱼ⋅∫_{μ=0}^L rⱼ'(𝐱,μ) dμ} $$The integral of opacities of all voxels rₖ\u0026rsquo;(𝐱,ξ) on the line of sight L belonging to a kernel is denoted as:\n$$ qₖ(𝐱) = ∫_{x₂=0}^L rₖ'(𝐱,x₂) dx₂ $$ qₖ(𝐱) is the footprint function for a 3D kernel $rₖ'(𝐱,x₂)$ in the ray space.\nqₖ(𝐱) is a 2D function that specifies the contribution (total opacity) of a 3D kernel to a screen location.\nR a y s e I p y n a e t c e e g r a 3 t D - e ⋯ s ⋯ r u ⋯ ₖ p ⋯ ( p ⋯ 𝐱 o ⦿ ⋯ , r ⋯ ξ t ⋯ ) ⋯ o ⋯ a f ⋯ l - o r n ₖ g t h e s l c i r ⌈ ⌊ n e x x e e ₀ ₁ n ⌉ ⌋ o f s i g h t The 3rd dimension (x₂) of a kernel has been integrated out. Thus, inputting a 2D coordinates, it returns an attribute\u0026rsquo;s weight resulting from the corresponding kernel.\n(2023-12-31) The objects to be blended differ between EWA and NeRF.\na r e p r o P j o e c i c a n t m t e e s d r a ( i 1 n v e r s e l y ) L p V i i r i n n o s e e j c w 3 e r i D s c e ∫ i n e t e l s g r g e n i a m d n f r y e e o a n o o y s t n s t s p s t e p a o g r i c a m i n e r s e n t e c n t e r t g e r e a n l s (2024-01-04)\nIn NeRF, points on a ray are composited, whereas EWA blends \u0026ldquo;line segments\u0026rdquo;. (refer to 八氨)\nIn EWA, scene voxels are grouped into different ellipsoids, and then rendering for ellipsoids, instead of volume-wise compositing. The scene element is a line integral of an ellipsoid, analogous to a particle in the volume rendering, thereby boosting efficiency.\nIn this ellipsoidal scenario, the alpha of each ellipsoid is the footprint function (opacity integral over the line of sight).\nAnd 3D kernels in the dataset can be preintegrated during splatting phase before the rendering phase.\nWith the opacity precomputed, the alpha compositing is a 2D convolution over the line of sight L for splatting, whereas ray-casting needs 3D convolutions. Thus, splatting is more efficient.\nWith this approximation, the volume rendering equation becomes adapted for splatting representation of a 3D scene:\n$$g_c(𝐱) = ∑_{k∈IN} wₖ ⋅cₖ(𝐱)⋅ qₖ(𝐱) ⋅ ∏_{j∈IN} e^{- wⱼ⋅qⱼ(𝐱)} $$ Assume all kernels are ordered back to front (statements inconsistent in the paper), so that when computing the transmittance, only the opacities of kernels prior to the current kernel need to be accumulated.\ne y e b a c r k ' ₖ ⦿ ₋ ₁ ( r 𝐱 ' ⦿ ⋅ , ₖ ξ ) r ' ₖ ⦿ ₊ f ₁ r o n t s c r e e n $$g_c(𝐱) = ∑_{k∈IN} wₖ ⋅cₖ(𝐱)⋅ qₖ(𝐱) ⋅ ∏_{j=0}^{k-1} e^{- wⱼ⋅qⱼ(𝐱)} $$ The exponential term is approximated by its 1st-order Taylor expansion based on $e^{-x} = 1-x$ evaluated at x=0:\n$$ e^{- wⱼ⋅qⱼ(𝐱)} \\approx 1-wⱼ qⱼ(𝐱) \\\\\\ g_c(𝐱) = ∑_{k∈IN} wₖ ⋅cₖ(𝐱)⋅ qₖ(𝐱) ⋅ ∏_{j=0}^{k-1} (1-wⱼ qⱼ(𝐱)) $$ The $g_c(𝐱)$ is the splatting equation representing the continuous screen.\nConsequently, with the above 4 assumptions, the point-based splatting becomes the same form as the NeRF-style volumetric rendering, because they\u0026rsquo;re both based on alpha compositing (image formation model). (Refer to 3DGS)\nCombine Filter Screen is a continuous 2D representation of the scene. A discrete image grid can be obtained by sampling it with an impulse train. To avoid aliasing when sampling the screen function, each projected 2D splat needs to be filtered to the Nyquist frequency of the output image by passing a proper loss-pass filter $h(𝐱)$.\n$$ \\begin{aligned} \\^g_c(𝐱) \u0026= g_c(𝐱) ⊗ h(𝐱) \\\\\\ \u0026= ∫_{η=-∞}^{∞} ∑_{k∈IN} wₖ ⋅ cₖ(\\bm η) ⋅ qₖ(\\bm η) ⋅ ∏_{j=0}^{k-1} (1-wⱼ qⱼ(\\bm η)) ⋅ h(𝐱-\\bm η) d\\bm η \\\\\\ \u0026= ∑_{k∈IN} wₖ ⋅ ∫_{η=-∞}^{∞} cₖ(\\bm η) ⋅ qₖ(\\bm η) ⋅ ∏_{j=0}^{k-1} (1-wⱼ qⱼ(\\bm η)) ⋅ h(𝐱-\\bm η) d\\bm η \\\\\\ \\end{aligned} $$ cₖ(𝛈) is the color (emission coefficient) of the ray point (𝛈,ξ) calculated relative to the kernel rₖ. It\u0026rsquo;s a function of rays 𝛈.\nqₖ(𝛈) is the contribution (total opacity) of the kernel rₖ to the screen point 𝛈.\nThe cumulative product term is the transmittance (attenuation) of each voxel (𝛈,ξ) in the kerenel rₖ.\nBecause the color cₖ(𝛈) and the transmittance of a voxel (𝛈,ξ) in a kernel have no explicit formula to be integrated, two approximations are introduced to reach an analytical expression to compute.\nColor of any voxel on any ray 𝛈 in the support region of a 3D kernel rₖ is a constant cₖ:\n$$cₖ(\\bm η) = cₖ$$ Transmittance of each voxel in the kernel rₖ is a constant.\n$$∏_{j=0}^{k-1} (1-wⱼ qⱼ(\\bm η)) \\approx oₖ $$ The transmittance variation inside a 3D kernel is omitted, so the sole splatting equation can\u0026rsquo;t avoid edge aliasing, which needs to be solved by other techniques. Therefore after filtering, the footprint function becomes band-limited in frequency domain:\n$$∫_{η=-∞}^{∞} qₖ(\\bm η) h(𝐱-\\bm η)d\\bm η$$And the antialiased splatting equation (screen function):\n$$ \\begin{aligned} \\^g_c(𝐱) \u0026= g_c(𝐱) ⊗ h(𝐱) \\\\\\ \u0026≈ ∑_{k∈IN} wₖ⋅cₖ⋅oₖ⋅∫_{η=-∞}^{∞} qₖ(\\bm η) ⋅ h(𝐱-\\bm η) d\\bm η \\\\\\ \u0026= ∑_{k∈IN} wₖ⋅cₖ⋅oₖ⋅ (qₖ ⊗ h)(𝐱) \\end{aligned} $$The formula can be interpreted as a weighted sum (combination, expectation) of footprint function in the 2D screen space. Thus, the primitives are the projected, prefiltered reconstruction kernels, so called ideal volume resampling filter:\n$$ρₖ(𝐱) = cₖ⋅oₖ⋅(qₖ ⊗ h)(𝐱)$$ Instead of band limiting the output function $g_c(𝐱)$ directly, we band limit each footprint function qₖ separately.\n(2023-11-26)\nEWA Splats The ideal volume resampling filter (splat primitive) is obtained after 3 steps: projection (viewing + projective transformation), footprint function, and convolving with a Gaussian loss-pass filter.\nGaussian will yields a Gaussian after affine transformations, convolving with another Gaussian, and integrating along one of its dimensions.\nTherefore, 3D Gaussian is used as 3D reconstruction kernels (in object space) to produce an analytical expression of the 2D Gaussian resampling filter $ρₖ(𝐱)$ in screen space. So that splatting equation is an elliptical weighted average (EWA).\nA reconstruction kernel centered at datapoint 𝐮ₖ in the object space is defined as the 3D Gaussian distribution with a mean vector 𝐮ₖ and a covariance matrix 𝐕ₖ:\n$$rₖ(𝐮) = \\frac{1}{(2π)^{3/2} det(𝐕ₖ)^{1/2}} e^{-½(𝐮-𝐮ₖ)^T 𝐕ₖ^{-1} (𝐮-𝐮ₖ)}$$ (2023-11-28)\nProject Kernels Project a 3D Gaussian distribution from object space to ray space through a viewing transformation and a projective transformation.\nViewing transformation φ Transform a Gaussian distribution from source (object) space to camera space through an affine mapping: $𝐭=𝐖 𝐮+𝐝$. If 𝐖 is invertible, there is $𝐮 = 𝐖⁻¹(𝐭-𝐝)$.\nGiven a probability density function $f_𝐮$ of the variable 𝐮, and a linear mapping 𝐭=𝐖 𝐮+𝐝, by substituting the 𝐮 of $f_𝐮(𝐮)$ with 𝐖⁻¹(𝐭-𝐝), the result expression is a new distribution $f_𝐭$ represented in 𝐭\u0026rsquo;s space:\n$$f_𝐭(𝐭) = \\frac{1}{(2π)^{3/2} det(𝐕ₖ)^{1/2}} e^{-½(𝐖⁻¹(𝐭-𝐝)-𝐮ₖ)^T 𝐕ₖ^{-1} (𝐖⁻¹(𝐭-𝐝)-𝐮ₖ)}$$ Clarify: Subscripts indicate which variable\u0026rsquo;s distribution the function is depicting. The argument in parentheses is the variable building the function (in the associate space). On the other hand, to represent the PDF $f_𝐮$ in another space with a new variable, e.g., 𝐭 which associates with $f_𝐮(𝐭)$, rather than the generic $f_𝐮(𝐮)$, 2 steps are required: substituting and scaling. After substituting variable, the result representation in 𝐭\u0026rsquo;s space is $f_𝐭(𝐭)$ as stated above.\nBut the scale is mismatched. According to the Change of Variables theorem, the \u0026ldquo;absolute value of the determinant of the Jacobian matrix\u0026rdquo; must be multiplied as a scaling factor to align different units between 𝐭 and 𝐮.:\n$$f_𝐮(𝐭) = f_𝐭(𝐭) ⋅|det(\\frac{∂𝐮}{∂𝐭})| = f_𝐭(𝐭)⋅ |det(𝐖⁻¹)|$$And the new variable 𝐭\u0026rsquo;s PDF can be written as: $$f_𝐭(𝐭) = \\frac{1}{|det(𝐖⁻¹)|} f_𝐮(𝐭)$$, corresponding to the equation (21) in the paper: $G_𝐕^n (ϕ⁻¹(𝐮) - 𝐩) = \\frac{1}{|𝐌⁻¹|} G_{𝐌 𝐕 𝐌ᵀ}^n(𝐮-ϕ(𝐩))$\n(2023-11-28)\ndoubt: I\u0026rsquo;m confused in eq. (21), is the LHS the $f_𝐭(𝐭)$? Why do they care the unscaled distribution? (2023-12-02)\nIf the |det(J)| is missing and just substituting variable, the new distribution $f_𝐭(𝐭)$ can\u0026rsquo;t be written as a Gaussian (?). So, the author used the transformed \u0026ldquo;non-\u0026ldquo;Gaussian divided by the factor, such that the new distribution is a Gaussian as well. (2023-12-31)\nI forgot the meaning of the above comment on 2023-12-02. The eq. (21) can be understood as:\nGiven an affine mapping: 𝐮 = 𝐌 𝐱+c = ϕ(𝐱), and define its inverse as: 𝐱 = 𝐌⁻¹(𝐮-c) = ϕ⁻¹(𝐮)\nGiven the distribution of 𝐱 is $G_𝐕^n (𝐱 - 𝐩)$, after applying the affine mapping 𝐌 𝐱+c, the mean and variance will become: ϕ(𝐩) and 𝐌 𝐕 𝐌ᵀ. So the new distribution is represented as $G_{𝐌 𝐕 𝐌ᵀ}^n (𝐮 - ϕ(𝐩))$.\nAccording to Change of Variables, the relations are:\n$$ Gⁿ_𝐕(𝐱-𝐩) \\overset{substitute}{→} Gⁿ_𝐕(ϕ⁻¹(𝐮) - 𝐩) \\overset{times |det(J)|}{→} G_𝐕^n(ϕ⁻¹(𝐮) - 𝐩) |𝐌⁻¹| = G_{𝐌 𝐕 𝐌ᵀ}^n (𝐮 - ϕ(𝐩)) $$So the eq.(21) is indeed the unscaled transformed Gaussian in another space.\ndoubt: The unscaled, merely variable-changed distribution $Gⁿ_𝐕(ϕ⁻¹(𝐮) - 𝐩)$ is also a Guassian (?), as the 3 axes are stretched by the affine mapping linearly and separately, so the overall shape of Gaussian will be kept. And the |det(J)| is just a scalar coefficient ensuring consistent area quantity.\nSimilarly, the case of projecting a kernel from source space into camera space ($𝐭=𝐖 𝐮+𝐝$) is changing mean vector and covariance matrix:\n$rₖ(𝐭)$ is the transformed representation of the kernel $k$ in the camera space, matching the above $f_𝐮(𝐭)$, so it equals the variable-changed $rₖ(𝐮)$ multiplied with $|det(∂𝐮/∂𝐭)|$:\n$$ \\begin{aligned} \u0026rₖ(𝐭) = rₖ(𝐖⁻¹(𝐭-𝐝)) ⋅ |det(𝐖⁻¹)| \\\\\\ \u0026= \\frac{1}{(2π)^{3/2} \\sqrt{det(𝐕ₖ)} |det(𝐖)|} e^{-½(𝐖⁻¹(𝐭-𝐝)-𝐮ₖ)ᵀ𝐕ₖ⁻¹(𝐖⁻¹(𝐭-𝐝)-𝐮ₖ )} \\\\\\ \u0026= \\frac{1}{(2π)^{3/2} \\sqrt{det(𝐕ₖ)} |det(𝐖)|} e^{-½(𝐖⁻¹(𝐭-𝐝-𝐖𝐮ₖ))ᵀ𝐕ₖ⁻¹(𝐖⁻¹(𝐭-𝐝-𝐖𝐮ₖ) )} \\\\\\ \u0026= \\frac{1}{(2π)^{3/2} \\sqrt{det(𝐕ₖ)} |det(𝐖)|} e^{-½(𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)))ᵀ𝐕ₖ⁻¹(𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)) )} \\\\\\ \u0026= \\frac{1}{(2π)^{3/2} ⋅ \\sqrt{det(𝐕ₖ)} ⋅ |det(𝐖ᵀ)|^½ ⋅ |det(𝐖)|^½} \\\\\\ \u0026\\quad ⋅ e^{-½(𝐭-(𝐖𝐮ₖ+𝐝))ᵀ(𝐖⁻¹)ᵀ𝐕ₖ⁻¹𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)) } \\\\\\ \u0026= \\frac{1}{(2π)^{3/2} \\sqrt{|det(𝐖 𝐕ₖ 𝐖ᵀ)}|} e^{-½(𝐭-(𝐖𝐮ₖ+𝐝))ᵀ(𝐖ᵀ)⁻¹𝐕ₖ⁻¹𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)) } \\\\\\ \u0026= N(𝐖𝐮ₖ+𝐝, 𝐖 𝐕ₖ 𝐖ᵀ) \\end{aligned} $$Hence, after performing an affine transformation, the distribution in the new space has a new mean vector 𝐖𝐮ₖ+𝐝, i.e., the original mean 𝐮ₖ is shifted by the affine mapping, and the variance matrix becomes 𝐖 𝐕ₖ 𝐖ᵀ.\nDerivation refers to: Linear Transformation of Gaussian Random Variable (Found by DDG with keywords: \u0026ldquo;affine transform for Gaussian distribution\u0026rdquo;)\nFact: $(Wᵀ)⁻¹ = (W⁻¹)ᵀ$\nAffine property for x~N(0,1): STAT 830 The Multivariate Normal Distribution - SFU\nThe Multivariate Gaussian Distribution - Stanford\nProjective transformation ϕ (2024-01-04) Summary: The non-linear projective transformation is approximated via Taylor expansion. In this paper, a 3D point (x,y,z) is projected onto a plane perspectively as follows: the x, y coordinates are divided by z, and the z value is then reset to the Euclidean norm $‖𝐭‖$ for object culling.\nIn this way, points in the camera space are transformed into the 3D ray space.\n$$ \\begin{bmatrix}x₀ \\\\\\ x₁ \\\\\\ x₂ \\end{bmatrix} = ϕ \\left( \\begin{bmatrix}t₀ \\\\\\ t₁ \\\\\\ t₂ \\end{bmatrix} \\right) = \\begin{bmatrix}t₀/t₂ \\\\\\ t₁/t₂ \\\\\\ \\|(t₀,t₁,t₂)ᵀ\\| \\end{bmatrix} $$ After perspective division with pixel coords obtained, x₂ is supposed to be 1, but it\u0026rsquo;s assigned with ‖𝐭‖₂.\nDenote the L2 norm of 𝐭 as l: $‖(t₀,t₁,t₂)ᵀ‖ = l = \\sqrt{t₀²+t₁²+t₂²}$, i.e., the magnitude of the line connecting a point 𝐭 to the projection center on the screen.\nThe dpeth is not confined to [-1,1] like NDC. The points on a single ray all have the same (t₀,t₁)ᵀ, thus, the depth $x₂=\\sqrt{t₀²+t₁²+t₂²}$ only depends on t₂.\nAlthough $y=\\sqrt{1+x²}$ is not linear, it\u0026rsquo;s approaching linear: $y=x$ as x grows.\n(Plotted by Perplexity-Wolfram)\nBy using the L2 norm of the camera-space coordinates 𝐭 as the depth, the evenly sampled points on the line of sight in the ray space will almost remain evenly spaced after this non-linear projection (due to perspective division) from 𝐭 to 𝐱, close to directly using the t₂ of camera space as the depth:\n⌈ ⌊ S x x c ₀ ₁ r ⌉ ⌋ e e n M a ↓ i n ↓ t R a a ↓ i y n ℮ s e p v a x e c ₂ n e ⦿ k w t s ( r e a h p x ₖ r t e a ₀ ' n c c , e h r i x l i a n ₁ n y g , i g x s s ₂ p ) a ᵀ c e (2024-01-01) Unlike directly using t₂ as the depth in the ray space, with which the uniform intervals can be exactly kept (as perspective projection with homogeneous coords is linear), this approach considers both t₀ and t₁. After viewing transformation, it is the kernels that are viewing the screen. Thus, the screen seen by a kernel is perspective (i.e., near large, far small, as shown below Fig.3 bottom) after projective transformation. Each ray emitted from the kernel is parallel to each other in the ray space.\nMap a kernel from camera space to ray space.\n(2024-01-06)\nThe transformation ϕ from camera space to ray space should be understood as the datapoints are projected into the ray space (or the screen by disregarding the x₂) perspectively, because we want to display 3D world in a 2D plane. (Note: x₂ is independtly assigned beside the screen coordinates x0,x1, so ray space = screen + x₂.)\nShortly, the near-large-far-small effect is desired. But it\u0026rsquo;s a nonlinear function, so its linear approximation is applied to simulate those \u0026ldquo;curves\u0026rdquo;. As shown in Fig. 9, the correct perspective curving effects (bottom-right) aren\u0026rsquo;t accurately represented by an linear projection (bottom-left). Intuitively, one might think that all points are using a common depth value, but for the exact perspective projection, each point should use its individual depth.\nIn addition, the x₂ (‖𝐭‖₂, depth in the ray space) can be disregarded, because the datapoints have already been projected onto the screen, i.e., (x₀,x₁). The existence of ray space may just for introducing the footprint function (ray integral). The \u0026ldquo;projective transformation ϕ\u0026rdquo; is solely assigning x₂ based on the perspective projection.\nThe statement that \u0026ldquo;rays are parallel\u0026rdquo; may be misleading, as the perspective projection (x,y divided by z) has been performed, and one can focus on the screen directly. \u0026ldquo;Parallel\u0026rdquo; doesn\u0026rsquo;t mean that the kernel is projected onto the screen orthogonally, because they already are on the screen. The ray space is set for defining the footprint function of each kernel as an integral along a certain viewing ray (sunlight), which remains a straight line in the 3D ray space after perspective projection due to the linear approximation.\nAnd it is the viewing ray that will be orthogonally projected onto the screen. Thereby, the ray integral in the 3D ray space becomes a line integral on the 2D screen plane, i.e., an integral for a 2D Gaussian.\n(2024-01-07)\nThe 3D Gaussian integral in the ray space equals the 2D Gaussian integral in the screen space (?), because I think that the \u0026ldquo;voxels\u0026rdquo; perpendicular to a pixel at different depths are replicas of that pixel, which is projected by a certain kernel. S p c i r x e e ■ □ e l n ( v ■ 1 ( □ R v ■ a 2 y □ v ■ s 3 p □ a ⋯ c e v ■ ⋯ N ) □ ) K K e e r r n n e e l l 1 2 The depth dimension x₂ is ‖𝐭‖₂, which is independent to x0, x1. So ray space is \u0026ldquo;orthogonal\u0026rdquo; to the screen. In other words, a screen pixel is a summation of only kernels that overlaps with it, because object-space 3D Gaussian have already been projected perspectively onto the screen (corresponding to 2D center and covariance matrix), and the ray space is constructed after that projection as \u0026ldquo;rays space = screen + x₂\u0026rdquo;. In short, ray space is for viewing rays, not for datapoints.\n(2024-01-07)\nThe viewing ray is only used to compute each kernel\u0026rsquo;s opacity, which is the contribution coefficient to the target pixel\u0026rsquo;s color.\nKernels contributing to the target pixel are determined by whether a kernel overlaps with the pixel after being thrown onto the screen. In essence, the contributing kernel in the ray space is located on the perpendicular line to the target pixel, and its squashed flat disc covers the target pixel.\nThe role of viewing rays in splatting differs from volume rendering, where a viewing ray yields a pixel color, but in splatting, color is generated from kernels and a viewing ray determines the weights of kernels.\n(2024-01-22)\nThe 2D location (x₀,x₁) of a Gaussian center on the screen and the 2D covariance matrix are determined by perspective projection, while the opacity of a Gaussian is determined by computing the integral over x₂ along the viewing ray in the 3D ray space. However, this perspective projection is not affine (linear) because the depth t₂ is divided and x₂ is reassigned. Therefore, the first-order Taylor expansion of this transformation matrix is used as its linear approximation.\n(2024-01-05)\nThe reassignment of x₂ is not the reason for the approximation. x₂ is just used for integration along the viewing ray. And after approximation, the shape of the 2D Gaussian doesn\u0026rsquo;t depend on depth, which (the 3rd row, col) will be omitted to obtain the 2D covariance matrix of the projected 2D ellipse. The Jacobian matrix of ϕ(𝐭) is:\n$$ (\\begin{bmatrix}t₀/t₂ \\\\\\ t₁/t₂ \\\\\\ ‖(t₀,t₁,t₂)ᵀ‖ \\end{bmatrix})'= 𝐉 = \\begin{pmatrix} 1/t₂ \u0026 0 \u0026 -t₀/t₂² \\\\\\ 0 \u0026 1/t₂ \u0026 -t₁/t₂² \\\\\\ t₀/l \u0026 t₁/l \u0026 t₂/l \\end{pmatrix} $$The first-order Taylor expansion evaluated at the kernel center point 𝐭ₖ is called the local affine approximation:\n$$ \\begin{aligned} \u0026 ϕₖ(𝐭) = ϕ(𝐭ₖ) + 𝐉ₖ ⋅ (𝐭-𝐭ₖ) \\\\\\ \\\\\\ \u0026 𝐉ₖ = \\frac{∂ϕ}{∂𝐭}(𝐭ₖ) = \\begin{pmatrix} 1/tₖ,₂ \u0026 0 \u0026 -tₖ,₀/tₖ,₂² \\\\\\ 0 \u0026 1/tₖ,₂ \u0026 -tₖ,₁/tₖ,₂² \\\\\\ tₖ,₀/‖𝐭ₖ‖ \u0026 tₖ,₁/‖𝐭ₖ‖ \u0026 tₖ,₂/‖𝐭ₖ‖ \\end{pmatrix} \\end{aligned} $$(2024-01-10)\nIf the 𝐭 is 𝐭ₖ, (𝐭-𝐭ₖ)=0. So, there is no approximation, the projection of 𝐭ₖ is exact ϕ(𝐭ₖ). This is what the following sentence in the paper means.\n\u0026ldquo;As illustrated in Fig. 9, the local affine mapping is exact only for the ray passing through tk or xk, respectively.\u0026rdquo;\nAnd for the point far away from the Guassian center, the approximation result has a noticeble deviation from the actual case, as (𝐭-𝐭ₖ) is large.\nBy concatenating the viewing transform φ(𝐱) and projective transform ϕ(𝐭), the conversion for a point 𝐮 in source space to 𝐱 in ray space is an affine mapping 𝐦(𝐮):\n$$𝐭 = φ(𝐮) = 𝐖 𝐮+𝐝 \\\\\\ 𝐱 = 𝐦ₖ(𝐮) = ϕₖ(𝐭) = ϕ(𝐭ₖ) + 𝐉ₖ ⋅ (𝐭-𝐭ₖ) \\\\\\ = 𝐱ₖ + 𝐉ₖ ⋅ (𝐖 𝐮+𝐝 -𝐭ₖ) = 𝐉ₖ⋅𝐖 𝐮 + 𝐱ₖ + 𝐉ₖ⋅(𝐝-𝐭ₖ)$$Therefore, for this compound affine mapping, the multiplier is $𝐉ₖ⋅𝐖 $, and the bias is $𝐱ₖ + 𝐉ₖ⋅(𝐝-𝐭ₖ)$.\nAccording to the aforementioned viewing transformation derivation, the kernel in the 3D ray sapce rₖ\u0026rsquo;(𝐱) has a shifted mean 𝐮ₖ and a scaled variance 𝐕ₖ:\n$$ rₖ'(𝐱) ∼ N(𝐦(𝐮ₖ), 𝐉ₖ𝐖 𝐕ₖ 𝐖 ᵀ𝐉ₖᵀ) $$(2023-12-02)\ndoubt: Does the eq. (30) mean that the unscaled representation in ray space is the desired projected kernel?\nThe normalization factor of the transformed Gaussian is canceled, so that its integral isn\u0026rsquo;t a unit. Is that they want?\nIntegrate Kernels The footprint function is an integral over the voxels in a 3D kernel along the ray.\nGiven a 3D kernel in the ray space rₖ\u0026rsquo;(𝐱) as above, the footprint function is the integral over depth:\nConvolve with Filter The anti-aliased splatting equation is achieved by using the band-limited footprint function under 2 assumptions described above.\nConvolving the footprint function with a Gaussian low-pass filter:\n","date":"2023-11-18T21:52:00Z","image":"https://pic2.zhimg.com/80/v2-7cbe3b0c3b67ce80593fad0d73a814b5_720w.webp","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/b-note-ewa_splatting/","title":"Read: Render - Points | EWA Splatting"},{"content":"Example Source video: 【cmake教程】为你的项目引入外部第三方库(以真实项目partio为例) - 只喝白开水\nPrepare: Download the code of partio.\n1 git clone https://github.com/wdas/partio.git --depth=1 Directory structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (base) yi@yi:~/Downloads/partio$ tree -L 2 . ├── build_wheels.sh ├── CMakeLists.txt ├── Dockerfile ├── LICENSE ├── Makefile ├── README.md ├── setup.cfg ├── setup.py └── src ├── data ├── doc ├── lib ├── Makefile ├── py ├── tests └── tools 7 directories, 9 files Four steps for testing the introduced library:\nCopy codes\nOnly the codes partio/src/lib/ are needed in this example., so copy it to your own project, and place it under the directory example_cmake/external/partio/ as shown below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (base) yi@yi:~/Downloads/example_cmake$ tree . ├── CMakeLists.txt ├── external │ └── partio │ ├── CMakeLists.txt # partio/src/lib/CMakeLists.txt │ ├── core │ ├── io │ ├── PartioAttribute.h │ ├── Partio.h │ ├── PartioIterator.h │ ├── PartioVec3.h │ └── test │ └── test_helloWorld.cpp └── src ├── CMakeLists.txt └── main.cpp Inherit (Copy) the original CMakeListst settings:\nCopy all contents of the top-level CMakeLists.txt of the original partio project to the front of the copied lib\u0026rsquo;s external/partio/CMakeLists.txt, i.e., the previous partio/src/lib/CMakeLists.txt, as it\u0026rsquo;s included first.\nSo as to retain some macro settings and variables for compiling. And then delete unnecessary settings.\nThe top-level CMakeLists.txt of your own project is:\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.20) project(learn-cmake LANGUAGES CXX) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) # each sub-directory has its own CMakeLists.txt: add_subdirectory(external/partio) add_subdirectory(src) The src/ folder stores your own code, which will be built to an executable program. So, the executable in the src/CMakeLists.txt is your own main.cpp:\n1 add_executable(main main.cpp) An example of src/main.cpp:\n1 2 3 int main(int argc, char const *argv[]){ return 0; } Write an testing executable program (test_helloWorld.cpp) inside the external/partio.\nIn this case, test_helloWorld.cpp calls functions in external/partio \u0026ldquo;natively\u0026rdquo;. However, this isn\u0026rsquo;t the case for calling its functions from another project.\n1 2 3 4 #include \u0026#34;Partio.h\u0026#34; int main(){ Partio::ParticlesDataMutable* data = Partio::read(\u0026#34;bgeoFile\u0026#34;); std::cout \u0026lt;\u0026lt; \u0026#34;Number of \u0026#34; \u0026lt;\u0026lt; std::endl;} Set the external/partio/CMakeLists.txt to compile it as an executable application:\n1 2 3 4 add_library(partio ${PARTIO_LIBRARY_TYPE} ${io_cpp} ${core_cpp}) # Setting target_include_directories .... add_executable(test_helloWorld ${CMAKE_CURRENT_LIST_DIR}/test/test_helloWorld.cpp) target_link_libraries(test_helloWorld PRIVATE partio) After compiling, a binary file test_helloWorld will be generated under \u0026ldquo;external/partio\u0026rdquo;.\nThis test code has nothing to do with the other subdirectories\u0026rsquo; \u0026ldquo;src/\u0026rdquo;, where the main application will be build based on its own CMakeLists.txt.\nThe CMakeLists.txt of different projects are independent to each other.\nThe most commonly used rule is \u0026ldquo;One CMakeLists.txt per target\u0026rdquo;. cmake: add_subdirectory() vs include() - SO\nTest DiffRast I want to debug the library diff-gaussian-rasterization.\nI can create a project, and make the diffRast as an external library to call its methods.\n1 2 3 4 mkdir debug_diff_rast \u0026amp;\u0026amp; cd debug_diff_rast git init mkdir external \u0026amp;\u0026amp; cd external git submodule add https://github.com/graphdeco-inria/diff-gaussian-rasterization Directory structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 (base) yi@yi:~/Downloads/debug_diff_rast$ tree . ├── CMakeLists.txt ├── main.cpp ├── README.md └── external └── diff-gaussian-rasterization ├── CMakeLists.txt ├── cuda_rasterizer │ ├── auxiliary.h │ ├── backward.cu │ ├── backward.h │ ├── config.h │ ├── forward.cu │ ├── forward.h │ ├── rasterizer.h │ ├── rasterizer_impl.cu │ └── rasterizer_impl.h ├── diff_gaussian_rasterization │ └── __init__.py ├── ext.cpp ├── LICENSE.md ├── rasterize_points.cu ├── rasterize_points.h ├── README.md ├── setup.py └── third_party ├── glm └── stbi_image_write.h The original diff-gaussian-rasterization/CMakeLists.txt would build the project to a library (CudaRasterizer), not an executable application.\nThe command add_subdirectory() is used to add an external project, which will automatically build according to its own CMakeLists.txt when the root CMakeLists.txt starts building.\nIf without add_subdirectory, an error would occur: CMake Error at CMakeLists.txt:17 (target_include_directories): Cannot specify include directories for target which is not built by this project. Hence, the library CudaRasterizer is linked as a static library through target_link_libraries() to an executable program MyApp based on the main.cpp, and debug it.\nIn summary, the top-level (root) CMakeLists.txt needs to link 2 libraries: libtorch and CudaRasterizer, as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cmake_minimum_required(VERSION 3.20 FATAL_ERROR) project(MyApp) # ${PROJECT_NAME} find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_subdirectory(external/diff-gaussian-rasterization) add_executable(MyApp main.cpp) set_property(TARGET MyApp PROPERTY CXX_STANDARD 17) target_link_libraries(MyApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) target_link_libraries(MyApp CudaRasterizer) # rasterization_points.cu and ~.h aren\u0026#39;t in their CMakeLists.txt, so need: target_sources(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization/rasterize_points.cu) target_include_directories(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization) In the diff-gaussian-rasterization/CMakeLists.txt, the header files inside the folder cuda_rasterizer/ are included and set to PUBLIC, such that when the library is linked to the target, those headers will be appended to the target\u0026rsquo;s INCLUDE_DIRECTORIES. So they can be included via their filenames, for example #include \u0026quot;forward.h\u0026quot;, without relative path. (Not sure. 2023-11-16)\nHowever, the rasterize_point.h isn\u0026rsquo;t included into the library (CudaRasterizer), so it has to be included using its relative path.\nSpecifically, in the main.cpp:\n1 2 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026#34;external/diff-gaussian-rasterization/rasterize_points.h\u0026#34; With adding that header file (declaration) to executable target, the compilation of main.cpp can pass. However, if the function body rasterize_points.cu (function implementation) isn\u0026rsquo;t added to the executable in CMakeLists.txt, there will be a linking error:\n1 2 /usr/bin/ld: CMakeFiles/MyApp.dir/main.cpp.o: in function `main\u0026#39;: main.cpp:(.text.startup+0x383): undefined reference to `RasterizeGaussiansCUDA(at::Tensor const\u0026amp;, ...)\u0026#39; Two modifications for source code of diff-gaussian-rasterization:\nrasterize_point.cu isn\u0026rsquo;t included into the CudaRasterizer library, because it\u0026rsquo;s not specified within the add_library() function in the diff-gaussian-rasterization/CMakeLists.txt.\nTherefore, when the project diff-gaussian-rasterization is built by executing cmake -B ./build, the .cu file rasterize_point.cu isn\u0026rsquo;t compiled.\nThus, the following error of missing Python.h won\u0026rsquo;t be tiggered.\nrasterize_point.cu includes header \u0026lt;torch/extension.h\u0026gt;, which is used for interacting with Python. Based on that, the library CudaRasterizer can be called in Python scripts as a PyTorch extension.\nTherefore, the Python development headers must be present to compile it. Otherwise, an error would occur:\n1 2 3 4 5 6 7 8 9 In file included from /usr/local/libtorch/include/torch/csrc/Device.h:3, from /usr/local/libtorch/include/torch/csrc/api/include/torch/python.h:8, from /usr/local/libtorch/include/torch/extension.h:6, from /home/yi/Downloads/diff-gaussian-rasterization/rasterize_points.h:13, from /home/yi/Downloads//diff-gaussian-rasterization/test_DiffRast.cpp:2: /usr/local/libtorch/include/torch/csrc/python_headers.h:10:10: fatal error: Python.h: No such file or directory 10 | #include \u0026lt;Python.h\u0026gt; | ^~-~~-~~-~~-~~ compilation terminated. Previously, my trivial experiments only used LibTorch tensors without interacting with python. Therefore, no error was reported when building without specifying the path of Python.\nBut the Python development headers is already installed:\n1 2 sudo apt-get install python3-dev apt list python3-dev A possible solution is to include python in CMakeLists.txt: Similar issure\nSince here I just want to debug it as a C++ project, instead of for python, and I have LibTorch installed.\nSo I changed #include \u0026lt;torch/extension.h\u0026gt; to #include \u0026lt;torch/torch.h\u0026gt; in the 2 files: \u0026ldquo;rasterize_points.cu\u0026rdquo; and \u0026ldquo;rasterize_points.h\u0026rdquo;.\nBuild the top-level project \u0026ldquo;debug_diff_rast\u0026rdquo;:\n1 2 3 4 5 6 cd ~/Downloads/debug_diff_rast # configure: cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja # build: cmake --build ./build ./build/MyApp Ref:\n【公开课】现代CMake高级教程（持续更新中）- 双笙子佯谬 - bilibili\nInclude Python cmake (2024-01-29)\nThe direct solution for Python.h: No such file or directory could be include Python in the CMakeLists.txt,\nThis is inspired by Example debugging mixed Python C++ in VS Code - Nadiah Pardede Kristensen, where #include \u0026lt;Python.h\u0026gt; will be reported the error by Intellisense, if without specifying \u0026ldquo;includePath\u0026rdquo; for Python in the .vscode/c_cpp_properties.json.\nCMake built-in module: FindPython3 mentioned in How do I get cmake to find python3.9 instead of python3.10 on ubuntu22.04\ng++ CLI (2024-01-31)\nCheck if the Python Development Libraries has been installed: dpkg -l | grep python3-dev. devicetests Install it: sudo apt-get install python3-dev\nFind where it is: find / -type f -iname 'python.h' 2\u0026gt;/dev/null Debian / Ubuntu: Fatal error: Python.h: No such file or Directory\nThe directory of Python is also required to be included: g++ -I/usr/include/python3.8 main.cpp. GeeksforGeeks\nA complete example command is:\n1 2 3 4 5 6 7 8 9 10 11 12 (AIkui) yi@yi-Alien:~/Downloads/CppCudaExt_PT_Tut_AIkui$ g++ \\ -g -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ -I/home/yi/anaconda3/envs/AIkui/include/python3.10 \\ -I./include \\ # Custom functions declarations -L/usr/local/libtorch/lib \\ -Wl,-rpath,/usr/local/libtorch/lib \\ test_None_tensor.cpp \\ # main function definition interpolation.cpp \\ # Custom functions definitions -ltorch -ltorch_cpu -lc10 A demo Refer to Nathan Sebhastian\n1 2 3 4 5 6 7 8 #include\u0026lt;Python.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } Compile: g++ test_python.cpp, an error occurs:\n1 2 3 4 5 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/Cpp_Study$ g++ test_python.cpp test_python.cpp:1:9: fatal error: Python.h: No such file or directory 1 | #include\u0026lt;Python.h\u0026gt; | ^~~~~~~~~~ compilation terminated. Locate header file:\n1 2 3 sudo apt install mlocate sudo updatedb locate Python.h Include Python dir: gcc test_python.cpp -I/usr/include/python3.8\n","date":"2023-11-16T11:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cmake_3rdparty/","title":"Memo: Lang - CMake | Include 3rd-Party Library"},{"content":"Build add_executalbe() is for building a executable program. While add_library() is for building a library.\nBoth the source files and header files need to be added to executable target.\nOld fasion: How can i include header files from a directory into cmake\nMordern style: parallel101/course - 彭于斌\n1 2 3 add_executable(MyApp) file(GLOB sources *.cpp *.h) target_source(main PUBLIC ${sources}) (2023-11-16)\nCcache Speed up recompilation. Docs\nUsage refer to parallel101/course\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.15) project(hellocmake) find_program(CCACHE_PROGRAM ccache) if (CCACHE_PROGRAM) message(STATUS \u0026#34;Found CCache: ${CCACHE_PROGRAM}\u0026#34;) set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM}) set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK ${CCACHE_PROGRAM}) endif() ","date":"2023-11-15T22:05:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cmake_misc/","title":"memo: CMake | Misc"},{"content":"Debug Config Open \u0026ldquo;launch.json\u0026rdquo; ➡ Clik blue button \u0026ldquo;Add Configuration\u0026hellip;\u0026rdquo; at right-bottom corner ➡ Select \u0026ldquo;C/C++: (gdb) Launch\u0026rdquo;.\nThen, a snippest of configuration with name \u0026quot;(gdb) Launch\u0026quot; is inserted into the \u0026ldquo;configurations\u0026rdquo; field in the config file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(gdb) Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/test\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;stopAtEntry\u0026#34;: false, \u0026#34;cwd\u0026#34;: \u0026#34;${fileDirname}\u0026#34;, \u0026#34;environment\u0026#34;: [], \u0026#34;externalConsole\u0026#34;: false, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true }, { \u0026#34;description\u0026#34;: \u0026#34;Set Disassembly Flavor to Intel\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-gdb-set disassembly-flavor intel\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;g++ compile\u0026#34; }, ] } This newly created configuration can be selected in the drop-down box beside the gear.\nAdd a task in tasks.json.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;g++ compile\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;g++\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;${workspaceFolder}/test.cpp\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;test\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, ] } The code \u0026ldquo;test.cpp\u0026rdquo; to be debugged is as follows:\n1 2 3 4 5 6 7 8 9 10 11 int main(){ int x = 1; int exampleArray[5]; for (int i=0; i\u0026lt;5; i++) exampleArray[i] = i; int* exampleArray_heap = new int[5]; for (int i=0; i\u0026lt;5; i++) exampleArray_heap[i] = i; delete[] exampleArray_heap } View Memory After debugging started,\nFind the address of a variable x:\nAdd expression \u0026amp;x into \u0026ldquo;Watch\u0026rdquo; panel. SO\nUse VS Code\u0026rsquo;s generic memory view\nInstall extention Hex Editor; Clik the binary data icon $[^{01}_{10}]$ following the address (not variables). Another ways to view memory\nUsing extension Memmory View\nPress F1, MemoryView: Add new memory view (for debugger)\nType the address of x: 0x7fffffffdbf0. Then, \u0026ldquo;MEMORY\u0026rdquo; section appers in the bottom panel.\nUse GDB command -exec x/64xb 0x7fffffffdc00\n1 2 3 -exec x/64xb 0x555558854e20 0x555558854e20:\t0x03\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x555558854e28:\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 The byte at 0x555558854e20 is 3. Inspect array Array on stack is the pointer to the array. The pointer is stored in: \u0026amp;exampleArray= 0x7fffffffdc00\nAfter initializing, memory at exampleArray is 0,1,2,3,4:\n1 2 00007fffffffdc00 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 00007fffffffdc10 04 00 00 00 ff 7f 00 00 00 50 58 2b fa da fc 53 This array takes 16 bytes. Array on heap is the address of pointer, which is stored in: \u0026amp;exampleArray_heap= 0x7fffffffdbf8\n1 00007fffffffdbf0 01 00 00 00 04 00 00 00 c0 b2 56 55 55 55 00 00 That\u0026rsquo;s an address: 00 00 55 55 55 56 b2 c0 (0x000055555556b2c0), because the endianness of computer is in reverse order.\nAfter initializing by the for loop, heap memory of the array is:\n1 2 000055555556b2c0 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 000055555556b2d0 04 00 00 00 Read Address Pointer\u0026rsquo;s value is an address.\nUse for loop to read 8 bytes one-by-one:\n1 2 3 4 5 6 7 8 // Read the \u0026#34;address\u0026#34; (0x55555556b2c0) from memory (0x7fffffffdbf0) in bytes unsigned char* bytePointer = (unsigned char*)\u0026amp;exampleArray_heap; const int numBytesToRead = 8; unsigned char buffer[numBytesToRead]; for (int i=0; i\u0026lt;numBytesToRead; i++) buffer[i] = *(bytePointer + i); for (int i=0; i\u0026lt;numBytesToRead; i++) std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; static_cast\u0026lt;int\u0026gt;(buffer[i]) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; Output:\n1 c0 b2 56 55 55 55 0 0 Another way is converting the type of pointer to (long long*)\n1 std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; *(long long*)\u0026amp;exampleArray_heap \u0026lt;\u0026lt; std::endl; And it will automatically reverse the number:\n1 55555556b2c0 Python Calls Cpp (2024-01-29)\nAttach the GDB used for C++ application to the running Python debugger, following steps in Example debugging mixed Python C++ in VS Code - Nadiah Pardede Kristensen\nPreparation: Compile the C++ code to a python package:\nMake a folder for compiling the cpp package:\n1 2 3 4 5 6 7 8 9 10 11 (AIkui) yi@yi-Alien:~/Downloads/Cpp_Study$ tree . ├── cpp_ext_myadd | ├── debug_w_cpp.py │ ├── myAdd.cpp │ ├── setup.cfg │ └── setup.py └── .vscode ├── c_cpp_properties.json ├── launch.json └── settings.json where the myAdd.cpp file refers to code\nThe \"setup.py\" looks like: 1 2 3 4 5 6 7 8 from distutils.core import setup, Extension def main(): setup(name=\u0026#34;myadd\u0026#34;, ext_modules=[Extension(\u0026#34;myadd\u0026#34;, [\u0026#34;myAdd.cpp\u0026#34;])], ) if __name__ == \u0026#34;__main__\u0026#34;: main() Install package:\n1 (AIkui) yi@yi-Alie:~/Downloads/Cpp_Study$ pip install ./cpp_ext_myadd/ The customize package myadd will be installed in the current environment. So, VS Code requires \u0026ldquo;Python:Select Interpreter\u0026rdquo; by pressing F1 to match the envrionment.\nPyhon code: 1 2 3 4 import myadd print(\u0026#34;going to ADD SOME NUMBERS\u0026#34;) x = myadd.myadd(5,6) print(x) Add a breakpoint at line 2 before getting into package functions. And add a breakpoint (at z = x + y;) in the cpp file. Otherwise, the debugger won\u0026rsquo;t pause.\nThe method that needs to manually find the process ID:\nAdd 2 configurations for \u0026ldquo;Python\u0026rdquo; and \u0026ldquo;C/C++: gdb (Attach)\u0026rdquo; separately in the launch.json.\nDefault contents generated after clicking the blue button \"Add Configuration...\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(gdb) Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;/home/yi/anaconda3/envs/AIkui/bin/python3.10\u0026#34;, \u0026#34;processId\u0026#34;: \u0026#34;${command:pickProcess}\u0026#34;, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true }, { \u0026#34;description\u0026#34;: \u0026#34;Set Disassembly Flavor to Intel\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-gdb-set disassembly-flavor intel\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Python: Current File\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${file}\u0026#34;, \u0026#34;console\u0026#34;: \u0026#34;integratedTerminal\u0026#34;, \u0026#34;justMyCode\u0026#34;: true }, ] } With focusing on debug_w_cpp.py, select the dubugger \u0026ldquo;Python: Current File\u0026rdquo;, and then click the green start button to kick off the debugging.\nWhen the debugger hits the breakpoint in debug_w_cpp.py, find its process ID:\n1 2 3 4 5 6 7 8 9 10 (base) yi@yi-Alien:~$ ps aux | grep python root 905 0.0 0.0 42744 1680 ? Ss 2023 0:00 /usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggers root 1278 0.0 0.0 121200 1436 ? Ssl 2023 0:00 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal yi 1605315 0.0 0.1 38840 26336 pts/5 S+ 16:17 0:04 /usr/bin/python3.8 -O /usr/bin/ranger yi 1698762 2.9 0.9 1179385796 150624 ? Sl 18:42 0:04 /usr/share/code/code --ms-enable-electron-run-as-node /home/yi/.vscode/extensions/ms-python.vscode-pylance-2023.12.1/dist/server.bundle.js --cancellationReceive=file:bad7d71dedf58b5bb22a36398d8eb2bcf4447338c7 --node-ipc --clientProcessId=1698586 yi 1700442 0.4 0.1 840012 18472 ? Sl 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter yi 1700450 0.3 0.1 250260 18376 pts/6 Sl 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher 45763 -- /home/yi/Downloads/Cpp_Study/cross_lang_debug_host.py yi 1700456 3.1 0.1 414788 30468 pts/6 Sl+ 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy --connect 127.0.0.1:55977 --configure-qt none --adapter-access-token 2bb19c19a6a1592229fd27c7c8e3e0a37c002b320568e3b3c8fe450a3d343404 /home/yi/Downloads/Cpp_Study/cross_lang_debug_host.py yi 1700778 0.0 0.0 12116 2776 pts/7 S+ 18:45 0:00 grep --color=auto python The desired process is the one attached with a token: 1700456\nSwitch the debugger to \u0026ldquo;gdb (Attach)\u0026rdquo;, and then click the green start button again, it will ask the process ID just found.\nThen, a prompt pops:\n1 2 (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ /usr/bin/env /bin/sh /tmp/Microsoft-MIEngine-Cmd-00cvveuw.5cg Superuser access is required to attach to a process. Attaching as superuser can potentially harm your computer. Do you want to continue? [y/N] However, after I input \u0026ldquo;y\u0026rdquo;, the terminal said executing GDB requires elevated permission, but it didn\u0026rsquo;t prompt me to enter password.\n1 2 3 4 5 ==== AUTHENTICATING FOR org.freedesktop.policykit.exec === Authentication is needed to run `/usr/bin/gdb\u0026#39; as the super user Authenticating as: Yi Cao,,, (yi) Password: [1] + Stopped (tty output) /usr/bin/pkexec \u0026#34;/usr/bin/gdb\u0026#34; --interpreter=mi --tty=${DbgTerm} 0\u0026lt;\u0026#34;/tmp/Microsoft-MIEngine-In-tkwd0bog.wdj\u0026#34; 1\u0026gt;\u0026#34;/tmp/Microsoft-MIEngine-Out-n1p4gbjg.vvb\u0026#34; You have stopped jobs. Extension: Python C++ Debugger eliminates the need of manually entering process ID.\nIts default configuration in launch.json is:\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Python C++ Debug\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;pythoncpp\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;pythonConfig\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;cppConfig\u0026#34;: \u0026#34;default (gdb) Attach\u0026#34;, } ] } However, the authentication error persists.\nBypass the authentication:\n1 echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope Refer to: Debugging mixed Python C++ in VS Code. Can\u0026rsquo;t enter sudo password - SO\n\u0026ldquo;This solution doesn\u0026rsquo;t require reboot, but it\u0026rsquo;s not permanent.\u0026rdquo; , as explained in Attaching gdb (C++ debugger) to remote python process in VSCode - SO (OP is Mark Harris) (Found by Perplexity)\nOriginal answer 1 2 3 4 5 6 7 $ gdb --pid=30428 ... Attaching to process 30428 Could not attach to process. If your uid matches the uid of the target process, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try again as the root user. For more details, see /etc/sysctl.d/10-ptrace.conf ptrace: Operation not permitted. In this example, I found that the debugger cannot step downward after entering the cpp file, instead, it gets stuck at the line following the breakpoint.\nHowever, the method works in the project of AIkui\u0026rsquo;s CUDA extension tutorial, where the debugger moves line-by-line after jumping into \u0026ldquo;interpolation_kernel.cu\u0026rdquo; files (function: trilinear_fw_cu()), whereas it can\u0026rsquo;t step into kernel functions, which may require CUDA-GDB, rather than GDB.\nIt seems that after GDB concludes the execution of C++ programm, it will stay at the last line, and won\u0026rsquo;t automatically return to the Python debugger. So, I have to click the \u0026ldquo;disconnect\u0026rdquo; button on the control bar to free the GDB. Other problems:\nIf the file currently open in the editor is not the Python file \u0026ldquo;debug_w_cpp.py\u0026rdquo;, but rather \u0026ldquo;myAdd.cpp\u0026rdquo;, an error pops upon clicking the start button:\nThe following command for installing cpp packege, referring How to pass \u0026ndash;debug to build_ext when invoking setup.py install?, requires the path to source file relative path to workspaceFolder in the \u0026ldquo;setup.py\u0026rdquo;: ext_modules=[Extension(\u0026quot;myadd\u0026quot;, [\u0026quot;./cpp_ext_myadd/myAdd.cpp\u0026quot;])]\n1 (AIkui) yi@yi-Alienware:~/Downloads/Cpp_Study$ python3.10 ./cpp_ext_myadd/setup.py build_ext --debug install (2024-04-18)\nWill\u0026rsquo;s practice: 使用PythonCppDebugger联合调试Python与Cpp，以及应用到3DGS的若干尝试 - will的文章 - 知乎\n利用vscode的插件PythonCppDebugger，gdb选用cuda拓展的cuda-gdb； 相比于：“利用vscode的插件PythonCppDebugger，gdb选用cpp拓展的cppdbg”， 程序会突然崩溃，(chatGPT)怀疑是gs本身显存管理过于复杂，长时间的debug容易崩溃所致。 最终由于实在不稳定，笔者放弃使用该方法，选用笨办法： 把python端数据存成txt，然后另起一个c++项目调试cuda代码。 另可见笔者的issue： Question aboud cuda-gdb extension in attach process · Issue #32 · benibenj/vscode-pythonCpp\nIllustrate A C Project Call Graph References:\nCall Graph - Visual Studio Marketplace - LuoZhihao Asked ChatGPT: What\u0026rsquo;s the best illustration for an embedded project written in C-lang? I want to show the calling relationship of functions and show which functions are defined in which files. GitHub - chaudron/cally The term \u0026ldquo;Call Graph\u0026rdquo; can be surfaced by keywords: How to illustrate calling relationship in a C project in DDG 请问支持c/c++吗 - issue #11 - beicause/call-graph Drived by the error when trying to run its command. Crabviz - Visual Studio Marketplace - Chan HoCheung Searched by call graph in VSCode Extensions Notes:\n(2024-12-17)\nCall Graph is for Java r1-Mrktp.\nI can\u0026rsquo;t used it for a C project, although it\u0026rsquo;s based on VSCode call hierarchy language feature. r3-Issue\nRun command: CallGraph.showOutgoingCallGraph will prompt an error: This command is not found.\nMany tools are there:\nCally r2-GitHub, a C project call graph generator using GCC\u0026rsquo;s generated Register Transfer Language (RTL) files Crabvizr4-Mrktp, interative call graph generator\nCall Graph of Embedded Software\nReferences:\nDisplay Calling Relationships between Functions (Call Graph) | CS+ V4.01.00 Supports\n(2025-07-08T09:46)\nRenesas CS+ has call graph panel Code Understand AI Chat DeepWiki\nReferences:\nDeepWiki | AI documentation you can talk to, for every repo Mentioned in discord: TheCW\u0026rsquo;s Server Supports:\n(2025-08-20T08:26)\nDeepWiki was developed by TheCW. It has contained some popular repos r1-DeepWiki ","date":"2023-11-13T11:49:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/c++_debug/","title":"memo: C++ | Debug with VS Code"},{"content":"C++ Extensions - Docs\nInstall LibTorch libtorch is the C++ API for PyTorch. Guide: INSTALLING C++ DISTRIBUTIONS OF PYTORCH\nDownload the binary files with modified URL for specified version:\n1 2 3 4 5 6 7 # Download here (cxx11 ABI): wget https://download.pytorch.org/libtorch/cu116/libtorch-cxx11-abi-shared-with-deps-1.12.1%2Bcu116.zip unzip libtorch-cxx11-abi-shared-with-deps-1.12.1+cu116.zip # Optional sudo mv libtorch/ /usr/local/ Create folder \u0026ldquo;example-app\u0026rdquo;, and write code into \u0026ldquo;example-app.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;iostream\u0026gt; int main(){ torch::Tensor tensor = torch::rand({2,3}); std::cout \u0026lt;\u0026lt; tensor \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size of Tensor type: \u0026#34;\u0026lt;\u0026lt; sizeof(torch::Tensor) \u0026lt;\u0026lt; std::endl; } Set dependence for LibTorch via CMakeLists.txt:\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.18 FATAL_ERROR) project(MyLibTorchApp) # name find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_executable(MyLibTorchApp main.cpp) target_link_libraries(MyLibTorchApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) set_property(TARGET MyLibTorchApp PROPERTY CXX_STANDARD 17) Build (externally) inside the dir \u0026ldquo;example-app/build\u0026rdquo;\n1 2 3 4 # mkdir build # cd build cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch .. cmake --build . --config Release (Optional) For C/C++ extension, set up includePath in \u0026ldquo;c_cpp_properties.json\u0026rdquo; for correct intellisense:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/libtorch\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], \u0026#34;defines\u0026#34;: [], \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/bin/g++\u0026#34;, \u0026#34;cStandard\u0026#34;: \u0026#34;c17\u0026#34;, \u0026#34;cppStandard\u0026#34;: \u0026#34;gnu++14\u0026#34;, \u0026#34;intelliSenseMode\u0026#34;: \u0026#34;linux-gcc-x64\u0026#34; } ], \u0026#34;version\u0026#34;: 4 } Debug LibTorch (2023-11-12)\nlaunch.json Modify launch.json in .vscode to debug C/C++ binary file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;LibTorch Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/build/MyLibTorchApp\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, } ] } program is the compiled binary file, not the main.cpp file. Debug C++ in Visual Studio Code - Docs General attributes: Options for Debugging task.json Set up \u0026ldquo;tasks.json\u0026rdquo; for building project with cmake:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;cmake-configure\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-DCMAKE_BUILD_TYPE=Debug\u0026#34;, \u0026#34;-DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch\u0026#34;, \u0026#34;..\u0026#34;, ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;cmake-build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--build\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;--config\u0026#34;, \u0026#34;Debug\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;dependsOn\u0026#34;: [\u0026#34;cmake-configure\u0026#34;, \u0026#34;cmake-build\u0026#34;] } ] } Multiple tasks can be combined to one, which then is able tobe passed to preLaunchTask in \u0026ldquo;launch.json\u0026rdquo;. (Can\u0026rsquo;t pass 2 tasks at once.)\nIntegrate with External Tools via Tasks - Docs\nThe above 2 tasks both are custom tasks executed in shell.\nCMake requires two steps, so 2 tasks are needed.\nThe tag -DCMAKE_BUILD_TYPE=Debug is needed for generating debug info. Otherwise, the breakpoints won\u0026rsquo;t take effect:\n1 Module containing this breakpoint has not yet loaded or the breakpoint address could not be obtained. However, sometimes, camke build failed:\nExpand the error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 * Executing task: cmake --build . --config Debug -- Caffe2: CUDA detected: 11.6 -- Caffe2: CUDA nvcc is: /usr/local/cuda-11.6/bin/nvcc -- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.6 CMake Error at /usr/local/libtorch/share/cmake/Caffe2/public/cuda.cmake:88 (message): Caffe2: Couldn\u0026#39;t determine version from header: Change Dir: \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Run Build Command(s): /usr/local/bin/cmake -E env VERBOSE=1 /usr/bin/make -f Makefile cmTC_a6e2f/fast make[1]: Entering directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; /usr/bin/make -f CMakeFiles/cmTC_a6e2f.dir/build.make CMakeFiles/cmTC_a6e2f.dir/build make[2]: Entering directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Building CXX object CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o /usr/bin/c++ -I/usr/local/cuda-11.6/include -o CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o -c /home/yi/Downloads/example-app/build/detect_cuda_version.cc Assembler messages: Fatal error: can\u0026#39;t create CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o: No such file or directory make[2]: *** [CMakeFiles/cmTC_a6e2f.dir/build.make:78: CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o] Error 1 make[2]: Leaving directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; make[1]: *** [Makefile:127: cmTC_a6e2f/fast] Error 2 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include) /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:4 (find_package) -- Configuring incomplete, errors occurred! make: *** [Makefile:179: cmake_check_build_system] Error 1 * The terminal process \u0026#34;/usr/bin/bash \u0026#39;-i\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cmake --build . --config Debug\u0026#39;\u0026#34; terminated with exit code: 2. I tried g++ command, but I had problem with linking torch libraries (-L) and including header files (-I).\n1 2 3 4 5 6 7 * Executing task: C/C++: g++ build active file Starting build... /usr/bin/g++ -fdiagnostics-color=always -g /home/yi/Downloads/example-app/main.cpp -o /home/yi/Downloads/example-app/main /home/yi/Downloads/example-app/main.cpp:1:10: fatal error: torch/torch.h: No such file or directory 1 | #include \u0026lt;torch/torch.h\u0026gt; | ^~~-~~-~~-~~-~~-~~ Ref:\nCMake+VSCode编译运行C++程序简单教程 - 迷楼的文章 - 知乎\nLinux环境下使用VScode调试CMake工程 - Colorful的文章 - 知乎\nlibtorch 常用api函数示例（史上最全、最详细）- 无左无右 - 博客园\nInspect Tensor (2023-11-19)\nSave tensors to a file.\n(LibTorch -\u0026gt; PyTorch tensors) How to save a C++ Libtorch Tensor and load it into a Python Pytorch project? In GDB: *(float_t*)x.data_ptr()\n1 2 int main(){ const torch::Tensor bkg = torch::full({3}, 0., torch::device(torch::kCUDA)); GDB: *(float_t*)bkg.data_ptr() return 0.\nIs it possible to view values of a at::Tensor in Visual codes debug variable view (linux)?\n","date":"2023-11-09T20:04:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/libtorch_debug/","title":"memo: LibTorch | Debug"},{"content":"Create Tensor shape (2024-01-24)\n.sizes() is an \u0026ldquo;vector-like\u0026rdquo; object of class: IntArrayRef. It can be created with curly braces, e.g., {5.2}, or an std::vector\u0026lt;int64_t\u0026gt;{1,2,4}.\n1 2 3 4 5 6 7 #include \u0026lt;cassert\u0026gt; std::vector\u0026lt;int64_t\u0026gt; myVec = {5,2}; assert(torch::ones({5,2}).sizes() == myVec); // pass std::cout \u0026lt;\u0026lt; \u0026#34;Equal\u0026#34; \u0026lt;\u0026lt; std::endl; c10::IntArrayRef myArrRef = {5,2}; assert(myVec == myArrRef); // pass \u0026ldquo;Create vector out of the IntArrayRef constructor, , otherwise the vector is destroyed immediately afterward.\u0026rdquo; How to compare a torch::tensor shape against some other shapes? - SO Use tensor.size(i) (better than tensor.sizes()[i]) to access one of dimensions. Docs\n1 2 3 std::cout \u0026lt;\u0026lt; torch::ones(5).sizes() \u0026lt;\u0026lt; std::endl; // [5] std::cout \u0026lt;\u0026lt; torch::ones({5,2}).sizes() \u0026lt;\u0026lt; std::endl; // [5, 2] std::cout \u0026lt;\u0026lt; myTensor.size(1) \u0026lt;\u0026lt; std::endl; // 2 Use a Lambda function to reshape a tensor and return the updated shape:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 std::function\u0026lt;c10::IntArrayRef(c10::IntArrayRef newSize)\u0026gt;getResizedShape(torch::Tensor\u0026amp; t) { auto lambda = [\u0026amp;t](c10::IntArrayRef newSize){ t.resize_(newSize); return t.sizes(); }; return lambda; } int main() { torch::Tensor myTensor = torch::rand({1,2,3}); std::cout \u0026lt;\u0026lt; myTensor \u0026lt;\u0026lt; std::endl; auto getNewSize = getResizedShape(myTensor); std::cout \u0026lt;\u0026lt; getNewSize({3,2}) \u0026lt;\u0026lt; std::endl; } Output: 1 2 3 4 5 (1,.,.) = 0.9838 0.7854 0.6991 0.8325 0.1196 0.3780 [ CPUFloatType{1,2,3} ] [3, 2] Create from factory func Tensor Creation API — PyTorch main documentation Test repo General schema:\n1 torch::\u0026lt;factory-func-name\u0026gt; (\u0026lt;func-specific-args\u0026gt;, \u0026lt;sizes\u0026gt;, \u0026lt;tensor-opt\u0026gt;) \u0026lt;factory-func-name\u0026gt; e.g., arange, empty, \u0026hellip; Create a tensor from the factory function torch::rand()\n1 2 3 4 5 6 7 #include \u0026lt;torch/torch.h\u0026gt; // unzipped to /usr/local/libtorch int main(){ const torch::Tensor a = torch::randint(1, 9, {1,2,3}); std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt;\u0026#34;size:\u0026#34; \u0026lt;\u0026lt; a.sizes() \u0026lt;\u0026lt; std::endl; } CMakeLists.txt\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.18 FATAL_ERROR) project(MyLibTorchApp) # name find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_executable(${PROJECT_NAME} main.cpp) target_link_libraries(${PROJECT_NAME} \u0026#34;${TORCH_LIBRARIES}\u0026#34;) set_property(TARGET ${PROJECT_NAME} PROPERTY CXX_STANDARD 17) Build:\n1 2 3 4 mkdir -p build cd build cmake -DCMAKE_PREFIX_PATH=/usr/local/libtorch .. cmake --build . --config Release Or in a modern way (under the workspace; no need cd to ./build):\n1 2 cmake -B build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja cmake --build build # build in ./build Execute it: ./MyLibTorchApp\nOutput: 1 2 3 4 5 (1,.,.) = 8 7 6 2 7 2 [ CPUFloatType{1,2,3} ] size: [1, 2, 3] Create with 4 Properties Pass an instance TensorOptions to the factory function:\n1 2 3 4 5 6 7 8 torch::TensorOptions options = torch::TensorOptions().dtype(torch::kFloat32) .layout(torch::kStrided) .device(torch::kCUDA, 0) .requires_grad(true); torch::Tensor a = torch::full({3,4}, 123, options); std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; a.device() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; a.requires_grad() \u0026lt;\u0026lt; std::endl; Only float and complex can .requires_grad. full(...) is not implemented for sparse layout Output: 1 2 3 4 5 6 123 123 123 123 123 123 123 123 123 123 123 123 [ CUDAFloatType{3,4} ] cuda:0 1 Omitting torch::TensorOptions(), which will be pre-configured and returned if calling the 4 properties directly from torch:: namespace.\n1 torch::Tensor a = torch::arange(1,9, torch::dtype(torch::kInt32).device(torch::kCUDA, 0)); If only one property needs to be specified, its property name (torch::dtype()) can be omitted even further.\n1 torch::Tensor a = torch::arange(8, torch::kInt32); Convert tensor by .to Use TensorOptions and .to() to create a new tensor on new memory based on a source tensor.\nConvert dtype:\n1 2 3 4 5 6 7 8 torch::Tensor src_tensor = torch::randn({3,2}); torch::Tensor a = src_tensor.to(torch::kInt32); // combinational torch::Tensor a = src_tensor.to(torch::dtype(torch::kInt32).device(torch::kCUDA,0)); auto opts = a.options(); std::cout \u0026lt;\u0026lt; opts \u0026lt;\u0026lt; std::endl; What does \u0026ldquo;new\u0026rdquo; mean? Options Alteration 1 2 3 4 5 6 torch::Tensor a = torch::randn(3); // change the property of dtype in the TensorOptions object auto int_opts = a.options().dtype(torch::kInt32); auto float_opts = a.options().dtype(torch::kFloat32); size of a tensor (2024-01-24)\nLibTorch sizeof tensor - SO\n1 2 3 4 5 6 torch::Tensor myTensor = torch::rand({1,2,3}, torch::kFloat32); int sizeOfFloat = torch::elementSize(torch::typeMetaToScalarType(myTensor.dtype())); std::cout \u0026lt;\u0026lt; \u0026#34;size of the kFloat32 type: \u0026#34; \u0026lt;\u0026lt; sizeOfFloat \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Number of elements in the tensor: \u0026#34; \u0026lt;\u0026lt; myTensor.numel() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Bytes occupied by the tensor: \u0026#34; \u0026lt;\u0026lt; myTensor.numel() * sizeOfFloat \u0026lt;\u0026lt; std::endl; Output: 1 2 3 size of the kFloatt32 type: 4 Number of elements in the tensor: 6 Bytes occupied by the tensor: 24 Manipulate Tensor ATen means \u0026ldquo;A Tensor Library\u0026rdquo;. The Tensor class under its namespace at:: lays the base for all tensor operations. ezyang\u0026rsquo;s blog\nResize (2023-11-12)\nAPI: Class Tensor in Namespace ATen - Docs\nReshape a tensor in place:\n1 2 3 4 torch::Tensor t = torch::arange(6).resize_({1,2,3}); std::cout \u0026lt;\u0026lt; t \u0026lt;\u0026lt; std::endl; t.resize_({6}); std::cout \u0026lt;\u0026lt; t \u0026lt;\u0026lt; std::endl; Output: 1 2 3 4 5 6 7 8 9 10 11 (1,.,.) = 0 1 2 3 4 5 [ CPULongType{1,2,3} ] 0 1 2 3 4 5 [ CPULongType{6} ] It can be resized to more than its elements:\n1 2 3 4 5 6 7 torch::Tensor t = torch::arange(6).resize_({1,2,3}); t.resize_({10}); std::cout \u0026lt;\u0026lt; \u0026#34;Allocated bytes:\u0026#34; \u0026lt;\u0026lt; t.numel() * torch::elementSize(torch::typeMetaToScalarType(t.dtype())) \u0026lt;\u0026lt; std::endl; for (size_t i = 0; i \u0026lt; t.numel(); ++i) { std::cout \u0026lt;\u0026lt; t[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Output 1 2 3 4 5 6 7 8 9 10 11 Allocated bytes:80 0 [ CPULongType{} ] 1 [ CPULongType{} ] 2 [ CPULongType{} ] 3 [ CPULongType{} ] 4 [ CPULongType{} ] 5 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] Flatten Reshape a tensor to 1D and return the pointer to it. Code from 3DGS\nUse a lambda function to resize the tensor and return the data pointer.\n.data_ptr() points to data of the tensor x, while x doesn\u0026rsquo;t point to data directly.\nreinterpret_cast\u0026lt;char*\u0026gt; converts the tensor-type pointer .data_ptr() to a char-type pointer pResizedX, which will read memory byte by byte.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdio\u0026gt; std::function\u0026lt;char*(size_t N)\u0026gt; resizeFunctional(torch::Tensor\u0026amp; t){ std::cout \u0026lt;\u0026lt; \u0026#34;size of the reference of the input tensor: \u0026#34; \u0026lt;\u0026lt; sizeof(t) \u0026lt;\u0026lt; std::endl; auto lambda = [\u0026amp;t](size_t N){ // Number of elements t.resize_({ (long long) N}); // shape: {N} std::cout \u0026lt;\u0026lt; \u0026#34;N is: \u0026#34; \u0026lt;\u0026lt; N \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size of t: \u0026#34; \u0026lt;\u0026lt; sizeof(t) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;dtype of t: \u0026#34; \u0026lt;\u0026lt; t.dtype() \u0026lt;\u0026lt; std::endl; return reinterpret_cast\u0026lt;char*\u0026gt;(t.contiguous().data_ptr()); // read memory byte by byte }; return lambda; } int main(){ torch::Tensor a = torch::arange(33,40, torch::kByte).resize_({1,2,3}); std::cout \u0026lt;\u0026lt; \u0026#34;Test tensor: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Tensor is a ptr, so its size is: \u0026#34; \u0026lt;\u0026lt; sizeof(torch::Tensor) \u0026lt;\u0026lt; std::endl; auto resizer = resizeFunctional(a); // lambda expression char* pTensor = resizer(a.numel()); // pointer to tensor\u0026#39;s data // Memory address printf(\u0026#34;char*: %p \\n\u0026#34;, pTensor); std::cout \u0026lt;\u0026lt; \u0026#34;size of pointer of a char: \u0026#34; \u0026lt;\u0026lt; sizeof(pTensor) \u0026lt;\u0026lt; std::endl; // The return address is the data_ptr() printf(\u0026#34;data_ptr(): %p \\n\u0026#34;, a.data_ptr()); // Print out the data stored in the returned address // Since a data is only 1 byte, the 1st byte is the 1st data. char data = *pTensor; // the first byte. // Note: unicode of 0-31 are invisible, so I test char 33-40 printf(\u0026#34;The first byte: %c \\n\u0026#34;, data); std::cout \u0026lt;\u0026lt; data \u0026lt;\u0026lt; std::endl; // Convert value (char) to integer printf(\u0026#34;Decimal: %d \\n\u0026#34;, data); // 33 std::cout \u0026lt;\u0026lt; \u0026#34;Convert 1st byte to int: \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;int\u0026gt;(*pTensor) \u0026lt;\u0026lt; std::endl; // Indexing elements like an array: std::cout \u0026lt;\u0026lt; \u0026#34;Use [0]: \u0026#34; \u0026lt;\u0026lt; pTensor[0] \u0026lt;\u0026lt; std::endl; // ! for (size_t i = 0; i \u0026lt; 6; ++i) { std::cout \u0026lt;\u0026lt; static_cast\u0026lt;char\u0026gt;(pTensor[i]) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; } Output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 (base) yi@yi:~/Downloads/LibTorch_Study$ ./build/MyLibTorchApp Test tensor: (1,.,.) = 33 34 35 36 37 38 [ CPUByteType{1,2,3} ] Tensor is a ptr, so its size is: 8 size of the reference of the input tensor: 8 N is: 6 size of t: 8 dtype of t: unsigned char char*: 0x55f202301640 size of pointer of a char: 8 data_ptr(): 0x55f202301640 The first byte: ! ! Decimal: 33 Convert 1st byte to int: 33 Use [0]: ! ! \u0026#34; # $ % \u0026amp; N is the total number of elements in the tensor t.\nresize_ requires the shape argument size to be c10::IntArrayRef type, which is an array of int64_t, i.e., signed 8-byte integer.\nTherefore, from the unsigned long int size_t (N) to a signed int64_t is a narrowing conversion.\nlong is at least 32-bit. In my computer, long is 8-byte. And long long is at least 64-bit. Because the signedness modifier is omitted, both long and long long are signed. Thus, the type casting (long long) N is equivalent to (int64_t) N\nint64_t is exact 8 bytes for all compilers, unlike long somewhere is 4-bytes. Definition of int64_t - SO\n1 2 3 4 5 6 std::cout \u0026lt;\u0026lt; sizeof(size_t) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(signed long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(unsigned long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(long long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(int64_t) \u0026lt;\u0026lt; std::endl; // 8 Attributes of tensor x:\n--- title: tensor x --- classDiagram direction RL class T[\"at::TensorBase\"]{ + c10::intrusive_ptr impl_ } class P[\"c10::intrusive_ptr\"]{ + c10::TensorImpl* target_ } note for P \"0x555557729610\" P --\u003e T View the memory via GDB command -exec:\n1 2 3 -exec x/64xb 0x555557729610 0x555557729610:\t0x60\t0x35\t0xfb\t0xf7\t0xff\t0x7f\t0x00\t0x00 0x555557729618:\t0x01\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x7FFFF7FB3560 is not the address storing x\u0026rsquo;s data. Char pointer pResizedX = 0x555557729500 points to the memory storing the x\u0026rsquo;s data:\n1 2 3 4 5 6 7 8 9 -exec x/64b 0x555557729500 0x555557729500:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729508:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729510:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729518:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729520:\t0\t0\t0\t0\t0\t0\t0\t0 0x555557729528:\t81\t0\t0\t0\t0\t0\t0\t0 0x555557729530:\t0\t0\t0\t0\t0\t0\t0\t0 0x555557729538:\t16\t80\t87\t85\t85\t85\t0\t0 There are four 3. A tensor takes 8-byte integer? DEBUG CONSOLE panel:\n1 2 x.data_ptr {void *(const at::TensorBase * const)} 0x55555555b392 \u0026lt;at::TensorBase::data_ptr() const\u0026gt; Don\u0026rsquo;t know what that address is? Get value (2023-11-12)\nReturn the pointer to data: Tensor.data\u0026lt;T\u0026gt;(), which is deprecated and changed to Tensor.data_ptr\u0026lt;T\u0026gt;() internally. Source code\n1 2 3 4 5 int main(){ torch::Tensor x = torch::full({1,3}, 2, torch::dtype(torch::kFloat)); std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x.contiguous().data\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x.contiguous().data_ptr\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl;} Output: 1 2 3 4 5 ~/l/build$ ./MyLibTorchApp 2 2 2 [ CPUFloatType{1,3} ] 0x557d9beab500 0x557d9beab500 .item\u0026lt;dtype\u0026gt;() can get scalar data, not vector. Torch C++: Getting the value of a int tensor by using *.data() - SO\n1 2 3 4 5 6 int main(){ torch::Tensor x = torch::randn({1,3}); std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x[0][0].item\u0026lt;int\u0026gt;() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x[0][0].item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; } Output: 1 2 3 4 5 ~/l/build$ ./MyLibTorchApp -0.6926 -0.2304 1.2920 [ CPUFloatType{1,3} ] 0 -0.692582 Use a vector to hold result tensor after inference: Part-2 Garry\u0026rsquo;s Blog\n1 2 3 4 5 6 7 8 9 // Extract size of output (of the first and only batch) // and preallocate a vector with that size auto output_size = output.sizes()[1]; auto output_vector = std::vector\u0026lt;float\u0026gt;(output_size); // Fill result vector with tensor items using `Tensor::item` for (int i = 0; i \u0026lt; output_size; i++) { output_vector[i] = output[0][i].item\u0026lt;float\u0026gt;(); } Copy cv::Mat to a tensor: Part-3 Garry\u0026rsquo;s Blog\n1 2 3 4 torch::Tensor tensor = torch::empty({mat.row, mat.cols, mat.channels()}, torch::TensorOptions().dtype(torch::kByte).device(torch::kCPU)); std::memcpy(tensor.data_ptr(), reinterpret_cast\u0026lt;void*\u0026gt;(mat.data), tensor.numel() * sizeof(at::kByte)); A more detailed post: Data Transfer to and from PyTorch - SimonWenkel.com libtorch 常用api函数示例（史上最全、最详细） - 博客园\nAllentDan/LibtorchTutorials\nFrom PyTorch to Libtorch: tips and tricks - Marc Lalonde - Medium\nAnnouncing a series of blogs on PyTorch C++ API - Kushashwa Ravi Shrimali\nempty tensor (2024-01-28)\nIn 3DGS, the project diff-gaussian-rasterization is built as an cpp extension according to setup.py, which is called in Python program. Whereas the CMakeList.txt serves for building the project as a static library (.so) to be inserted into the C++ executable application.\nOriginally, I want to debug the diff-gaussian-rasterization as a static library, so I need to construct input tensors that mimic those passed from Python, where some tensors are assigned as None, such as cov3D_precomp.\nHowever, I don\u0026rsquo;t know how to create a \u0026ldquo;None\u0026rdquo; tensor in the C++ program (Perplexity said: \u0026ldquo;You can\u0026rsquo;t directly set a tensor to NULL as you would do in Python by setting a variable to None.\u0026rdquo;).\nI have tried torch::empty({0}), but its data_ptr() is not the desired nullptr. Consequently, a if judge statement later won\u0026rsquo;t enter into the branch that would happened when the extension is called by Python.\n(2024-01-31) It turns out that I forgot the re-build and make the application again. So, CUDA-GDB still steps through the old application.\nThe .data_ptr() of torch::empty({0}) and torch::full({0},0) both are nullptr.\n(2024-01-30)\nJust found the None Python tensors in 3DGS are reassigned with torch.Tensor([]):\n1 2 if cov3D_precomp is None: cov3D_precomp = torch.Tensor([]) The torch.Tensor([]) will be passed into the C++ package function: _C.rasterize_gaussians() (i.e., the forward method RasterizeGaussiansCUDA)\nA demo where Python calls C++ package referring to AIkui\u0026rsquo;s CUDA extension tutorial:\nExpand codes The code can be evaluated by commands: chmod +x test.sh and ./test.sh P t y o t r h c o h n . T e n s o r ( [ ] ) p a s s t o r L c i h b : T : o e r m c p h t y ( { 0 } ) r e t u r n t o r P c y h t . h T o e n n s o r ( [ ] ) ","date":"2023-11-09T19:40:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/libtorch_tensor/","title":"memo: LibTorch | Tensor APIs and Examples"},{"content":"Pointer (2023-11-09) Review:\nA pointer variable and a regular variable are 2 ways of indexing memory.\nA pointer variable stores the address of the start byte of a variable. And its type indicates how many bytes the complete data takes.\nPointer variable is an address, a unsigned integer, whose size depends on system architecture (32-/64-bit addr). While a variable represents the whole data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // \u0026#34;pointer.c\u0026#34; #include \u0026lt;stdio.h\u0026gt; int main(){ int a = 1; int* p = \u0026amp;a; printf(\u0026#34;a = %d \\n\u0026#34;, a); printf(\u0026#34;p = %p \\n\u0026#34;, p); printf(\u0026#34;address of a = %p \\n\u0026#34;, \u0026amp;a); printf(\u0026#34;#bytes of a = %ld \\n\u0026#34;, sizeof(a)); printf(\u0026#34;#bytes of p = %ld \\n\u0026#34;, sizeof(p)); int b[2][2] = {1,2,3,4}, *ptr_b = \u0026amp;b[2][2]; printf(\u0026#34;#bytes of b = %ld \\n\u0026#34;, sizeof(b)); printf(\u0026#34;#bytes of ptr_b = %ld \\n\u0026#34;, sizeof(ptr_b)); } Build gcc pointer.c -o pointer and run ./pointer.\nOutput: 1 2 3 4 5 6 7 a = 1 p = 0x7ffe45d7c88c address of a = 0x7ffe45d7c88c #bytes of a = 4 #bytes of p = 8 #bytes of b = 16 #bytes of ptr_b = 8 The *p (dereferenced p) and i are equivalent at all time.\n1 2 3 int i = 1; int* p = \u0026amp;i; bool cf_res = (*p == i); // 1 (2023-11-11) char* indicates the direction: from pointer * to char. Similarly, char** means from a pointer * to another pointer *, then to char.\nPOINTERS in C++ - YouTube - The Cherno\nvoid* ptr = 0. Address 0 is NULL or nullptr. void means dismissing the type of the data it points to.\nReference Note: C doesn\u0026rsquo;t have reference. SO\nA reference variable is an alias. Compared to pointer, it\u0026rsquo;s an already dereferenced address.\n1 2 3 4 5 int i = 10 int* ptr = \u0026amp;i; // an address int\u0026amp; ref = i; // an alias Declaration and initialization must be performed at the same time.\n1 2 3 4 5 int\u0026amp; ref; // incorrect ref = i; int* ptr; ptr = \u0026amp;i; // ok Reference variable cannot be reassigned.\n1 2 3 4 5 6 int i =0, j =1; int\u0026amp; ref1 = i; int\u0026amp; ref1 = j; // error: redeclaration of ‘int\u0026amp; ref1’ int\u0026amp; ref2 = ref1; // ok std::cout \u0026lt;\u0026lt; ref2 \u0026lt;\u0026lt; std::endl; // 0 Reference shares the same memory as the variable, not occupying another memory.\nPass reference into function to modify the source data directly. Pointer needs deferencing to access the data.\nPython only has reference without pointer.\n1 2 3 4 5 6 7 8 9 10 11 #include\u0026lt;iostream\u0026gt; int main(){ int i = 0; std::cout \u0026lt;\u0026lt; \u0026amp;i \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e74 int* ptr = \u0026amp;i; std::cout \u0026lt;\u0026lt; \u0026amp;ptr \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e78 int\u0026amp; ref = i; std::cout \u0026lt;\u0026lt; \u0026amp;ref \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e74 } Use reference for function parameters and return types.\nUse references when you can, and pointers when you have to. C++ FAQ\nIteration:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; int main(){ vector\u0026lt;int\u0026gt; myVec; myVec = {1,2,3,4,5}; // for (int i:myVec) // will copy item to i for (int\u0026amp; i : myVec) // use reference to avoid copy data std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } Pass reference to function to avoid copying:\n1 2 3 4 5 6 7 8 9 void Function(const std::vector\u0026lt;int\u0026gt;\u0026amp; myVec){ for (int i : myVec) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt;std::endl; } int main(){ std::vector\u0026lt;int\u0026gt; myVec={1,2,3}; Function(myVec); } A pointer to a class/struct uses -\u0026gt; to access its members, whereas a reference uses . (Same as python.) What are the differences between a pointer variable and a reference variable? - SO\nRef:\nPointers vs References in C++ - GeeksforGeeks Raw Array (2023-11-11)\nThe variable name exampleArray is an address of the starting byte, so it\u0026rsquo;s a pointer.\n1 f = Array is a row of contiguous memory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #include \u0026lt;iostream\u0026gt; int main(){ // Create an array on stack, which // will be destoryed when getting out of the scope int exampleArray[4]; for (int i=0; i\u0026lt;5; i++) exampleArray[i] = i; cout \u0026lt;\u0026lt; exampleArray \u0026lt;\u0026lt; \u0026#34; is the address of the array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026amp;(exampleArray[0]) \u0026lt;\u0026lt; \u0026#34; is the address of the first element\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; *exampleArray \u0026lt;\u0026lt; \u0026#34; is the first element by dereferencing the pointer\u0026#34; \u0026lt;\u0026lt; endl; // Get the 3rd elements can do arithmatic on pointer, // where the size of the type will be multiplied automatically. cout \u0026lt;\u0026lt; *(exampleArray + 2) \u0026lt;\u0026lt; \u0026#34; is the 3rd element\u0026#34;\u0026lt;\u0026lt; endl; // If access the 2nd elements by calculating in bytes, // cast integer pointer to char pointer with is 1-byte type. // After locating the address, cast back to integer pointer for assigning a integer value. *(int*)((char*)exampleArray + 8) = 30; cout \u0026lt;\u0026lt; exampleArray[2] \u0026lt;\u0026lt; \u0026#34; is the modified 3rd element\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; sizeof(exampleArray) \u0026lt;\u0026lt; \u0026#34; bytes taken by the entire array\u0026#34;\u0026lt;\u0026lt; endl; // For array created on stack, the number of elements can be known because stack pointer contains offset: int num_elements = sizeof(exampleArray) / sizeof(int); cout \u0026lt;\u0026lt; num_elements \u0026lt;\u0026lt; \u0026#34; elements in the array on stack\u0026#34; \u0026lt;\u0026lt; endl; // Create an array on heap with the `new` keyword, // This array is the same as exampleArray, except for its lifetime that lasts until calling `delete[] arrayname`. // So if an array created inside function needs to be returned, it must be created on heap int* exampleArray_heap = new int[4]; // is a pointer (address 0x55555556b2c0) to heap, not the 1st element 0, for (int i=0; i\u0026lt;4; i++) exampleArray_heap[i] = i; // Read the \u0026#34;address\u0026#34; from memory in bytes std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; *(long long*)\u0026amp;exampleArray_heap \u0026lt;\u0026lt; std::endl; cout \u0026lt;\u0026lt; \u0026amp;exampleArray_heap \u0026lt;\u0026lt; \u0026#34; stores the 8-byte address (the pointer) to the array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; exampleArray_heap \u0026lt;\u0026lt; \u0026#34; is the address (pointer) to the array on heap\u0026#34; \u0026lt;\u0026lt; endl; // cast type to integer pointer cout \u0026lt;\u0026lt; *(exampleArray_heap+0) \u0026lt;\u0026lt; \u0026#34; is the first element in the array\u0026#34; \u0026lt;\u0026lt; endl; // one more jump will reduce performance // exampleArray_heap is a pointer of pointer (memory indirection). An 32-bit address taking 4 bytes cout \u0026lt;\u0026lt; sizeof(exampleArray_heap) \u0026lt;\u0026lt; \u0026#34; bytes for the pointer of the array on heap\u0026#34; \u0026lt;\u0026lt; endl; // delete[] exampleArray_heap; // If array is not created on stack, or only has the pointer to an array, // the number of elements can\u0026#39;t be computed as sizeof(array)/sizeof(int). So it must be maintained. static const int exampleSize = 5; // must be known at compile-time int myArray[exampleSize]; } 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 b f 5 6 Create an array on heap.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Entity{ public: int* exampleArray_obj = new int[4]; Entity(){ // construction function for (int i=0; i\u0026lt;4; i++) exampleArray_obj[i] = i; } }; int main(){ Entity e; cout \u0026lt;\u0026lt; e.exampleArray_obj \u0026lt;\u0026lt; \u0026#34; is the address (pointer) to array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; *(e.exampleArray_obj) \u0026lt;\u0026lt; \u0026#34; is the 1st element of array\u0026#34; \u0026lt;\u0026lt; endl; // 0 } std::array has .size().\n1 2 3 4 5 6 7 8 9 #include \u0026lt;iostream\u0026gt; #include \u0026lt;array\u0026gt; int main(){ std::array\u0026lt;int, 4\u0026gt; myArray; for(int i=0; i\u0026lt;myArray.size(); i++) myArray[i] = i; } Ref:\nArrays in C++ - The Cherno Pointer Alignment (2023-11-14)\n(Unsure about my naming for this.)\nBecause the algorithm specifies processing 128 data at a time, i.e., the factor alignment = 128. Thus, each time 128*sizeof(dtype) memory will be accessed.\nTherefore, the pointers to data should all be multiples of 128.\nThe way to round a number to a multiple of 128 is setting its last 8 bits to 1000_0000 (128).\nThe type of pointer affects its arithmetic:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(){ // int-type pointer: int* chunk = nullptr; // 8-byte address: 0 0 0 0 0 0 0 0 int* ptr = chunk + 127; // 0 + 127*sizeof(int) = 508 = 0x1fc std::cout \u0026lt;\u0026lt; ptr \u0026lt;\u0026lt; std::endl; // 0x1fc // char-type pointer: char* ptr2 = (char*) chunk + 127; // 0 + 127*sizeof(char) = 127 = 0x7f std::cout \u0026lt;\u0026lt; static_cast\u0026lt;void*\u0026gt;(ptr2) \u0026lt;\u0026lt; std::endl; // 0x7f } Convert a pointer to an integer: SO\n1 2 char* chunk = nullptr; uint var = reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(chunk); // ok Ensure offset a multiple of 128:\nIf the currect pointer chunk is not a multiple of 128, add another 127, and then truncate the last 8 bits to 1000_000 to make it a multiple of 128.\n1 2 3 4 5 6 int main(){ char* chunk = (char*)130; std::size_t alignment = 128; // uint // 130 + 127 \u0026amp; ~127 = 256 std::size_t offset = (reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(chunk) + alignment - 1) \u0026amp; ~(alignment - 1); // uint } Bitwise operation:\n1 2 3 4 5 chunk: 0000_0000_1000_0010 alignment-1: 0000_0000_0111_1111 ~(alignment-1): 1111_1111_1000_0000 chunk + (alignment-1): 0000_0001_0000_0001 (257) (257)\u0026amp;~(127) = 256: 0000 0001 0000 0000 Code from 3DGS\nPossible related articles:\nWhat exactly is an \u0026lsquo;aligned pointer\u0026rsquo;? size_t (2024-01-24)\nsize_t is the unsigned integer type. For example, if it\u0026rsquo;s 8 bytes (64-bit unsigned long), its value ranges in [0, 2⁶⁴-1]. In other words, if a number is larger than 2⁶⁴-1, it can\u0026rsquo;t be declared as the size_t type.\nUsage: Given a variable that stores the number of elements of an array, or number of bytes of an object, it can be set as the size_t type, as the name indicated that size_t is used for representing a \u0026ldquo;size\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;cstddef\u0026gt; // For size_t #include \u0026lt;typeinfo\u0026gt; int main() { size_t arraySize = 10; // unsigned int (16 bits at least) int myArray[arraySize]; // a 10-integer array size_t sizeOfArray = sizeof(myArray); // 40 bytes // The type of `arraySize` is unsigned integer const std::type_info\u0026amp; typeInfo = typeid(arraySize); std::cout \u0026lt;\u0026lt; typeInfo.name() \u0026lt;\u0026lt; std::endl; // return: m } m refers to unsigned long integer. The returned strings are defined differently across various compilers. So, the correct meaning should be found in the implementation. Strange output of std::typeid::name() - SO Do not use typeid The returned string is not consistent; Need to check the documents.\nRef: typeid(xxxx).name() \u0026ldquo;m\u0026rdquo; returned? - C++ Forum\ncppreference.com Returning the Name and Value of a C Type Using typeid() Reserve Memory (2024-01-26)\nPad the lastCount value to a multiple of the alignment 128, and then allocate the following count bytes, which is the number of elements of the target data pointed by dataPtr:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // \u0026#34;reserve_memory.cpp\u0026#34; #include \u0026lt;torch/torch.h\u0026gt; template \u0026lt;typename T\u0026gt; // T can be int, float..., deduced when compiling void accum(char*\u0026amp; lastCount, T*\u0026amp; dataPtr, size_t count, size_t alignment) { // Decimal arithmetic size_t offset = (reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(lastCount) + alignment-1) \u0026amp; ~(alignment-1); // Update the pointer to the data to be filled dataPtr = reinterpret_cast\u0026lt;T*\u0026gt;(offset); // Accumulate the number of bytes of the data lastCount = reinterpret_cast\u0026lt;char*\u0026gt;(dataPtr+count); } int main(){ char* size = nullptr; // Count from 0x0 float* depth; // sizeof(float) = 4 bytes float* color; accum(size, depth, 100, 128); // 0 + 100*4 = 400 bytes = 0x190 bytes printf(\u0026#34;%p \\n\u0026#34;, size); // 0x190 accum(size, color, 100*3, 128); // 512 + 300*4= 1712 bytes = 0x6b0 bytes printf(\u0026#34;%p \\n\u0026#34;, size); // 0x6b0 return 0; } char*\u0026amp; size = nullptr: the variable size is an alias of the nullptr, sharing the same memory.\nSuch that the pointer (lastCount and dataPtr) is changed directly.\nCompilation. Ref: Troubles while compiling C++ program with PyTorch, HElib and OpenCV - SO\n1 2 3 4 5 6 7 8 g++ -g -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ -L/usr/local/libtorch/lib \\ -Wl,-rpath,/usr/local/libtorch/lib \\ reserve_memory.cpp \\ -ltorch -ltorch_cpu -lc10 # Order matters: After .cpp file doubt: If I add -O2 option, the libraries -ltorch -ltorch_cpu -lc10 can be omitted. Don\u0026rsquo;t know why. With -O2 optimization, debugging will become difficult.\nThe original anwser: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 g++ -g -O2 -std=c++17 \\ -pthread \\ -march=native \\ -I/home/lulu/helib_install/helib_pack/include \\ -I/usr/include/opencv4 \\ -I/home/lulu/libtorch/include \\ -I/home/lulu/libtorch/include/torch/csrc/api/include \\ -I/home/lulu/libtorch/include/torch \\ -L/home/lulu/helib_install/helib_pack/lib \\ -L/usr/include/opencv4 \\ -L/home/lulu/libtorch/lib \\ -Wl,-rpath,/home/lulu/libtorch/lib \\ prova.cpp \\ -lopencv_core -lopencv_highgui -lopencv_imgcodecs \\ -lhelib -lntl -lgmp -lm \\ -ltorch -ltorch_cpu -lc10 \\ -o prova ","date":"2023-11-09T18:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/c++_pointer/","title":"memo: C++ | Pointer Usages"},{"content":"Compiler Switch g++ Versions Update Alternatives Problems:\nCUDA requires g++ to be compatible\nThe g++ needs to change version alongside CUDA version Switch.\nSupports:\nChange system-wide g++ Actions:\nInstall the target version of g++\n1 sudo apt-get install g++-10 -y Install\n1 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-10 20 Select the target version of g++\n1 2 3 sudo update-alternatives --display g++ sudo update-alternatives --list g++ sudo update-alternatives --config g++ Docker Container for g++ auto Compiler will evaluate the expression and use the result\u0026rsquo;s type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // \u0026#34;auto1.cpp\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;typeinfo\u0026gt; using namespace std; int main(){ int a = 1, b = 2; auto s = a + b; // compiler will compute a+b and assign the type of result to s. cout \u0026lt;\u0026lt; \u0026#34;Sum = \u0026#34; \u0026lt;\u0026lt; sum \u0026lt;\u0026lt;endl; cout \u0026lt;\u0026lt; typeid(s).name() \u0026lt;\u0026lt; endl; return 0; } Build: g++ auto1.cpp -o auto1\nMultiple variables followed by auto should be initialized with a common type.\n1 2 3 4 auto i = 0, *p = \u0026amp;i; // pointer variable p stores the address of i, // And the type of i is deduced from the value 0, auto* p = \u0026amp;i; // ok Reference:\nCodes from The auto Type Specifier in C++ - Neso Academy Placeholder type specifiers (since C++11) - cppreference Vector (2023-11-10)\n1 2 3 4 5 6 7 8 9 #include \u0026lt;vector\u0026gt; void main(){ std::vector\u0026lt;int\u0026gt; myVec; myVec = {1,2,3,4,5}; for (int i:myVec) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } std::vector is a class template (class maker), from which the specific class is derived with additional specifications.\nTemplate in C++ is like the parent class in Python to some extent: compile-time polymorphism v.s. runtime polymorphsim. But templates are not functions or classes.\nSpecifications follow the template name and are enclosed by angle brackets.\nA std::vector is a container for a sequence of objects.\nA std::vector\u0026rsquo;s definition doesn\u0026rsquo;t need number of elements, because it\u0026rsquo;s a dynamic array with adjustable size, achieved by allocating new memory and copying data as needed.\nPass a customized type to std::vector\u0026lt;custom_type\u0026gt;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; struct Vertex{ float x,y,z; }; // Overload the \u0026lt;\u0026lt; operator for Vertex struct std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; stream, const Vertex\u0026amp; v){ stream \u0026lt;\u0026lt; v.x \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; v.y \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; v.z; return stream; } int main(){ std::vector\u0026lt;Vertex\u0026gt; vertices; vertices.push_back({1,2,3}); vertices.push_back({4,5,6}); // Range-based for loop // for (Vertex v : vertices) // will copy each vertex for (const Vertex\u0026amp; v : vertices) // reference won\u0026#39;t copy std::cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; std::endl; // Delete the 2nd element vertices.erase(vertices.begin() + 1); for (int i=0; i \u0026lt; vertices.size(); i++) std::cout \u0026lt;\u0026lt; vertices[i] \u0026lt;\u0026lt; std::endl; // Clear the vector vertices.clear(); // size = 0 std::cout \u0026lt;\u0026lt; vertices.size() \u0026lt;\u0026lt; std::endl; } Reference:\nThe Vector Type in C++ - Youtube - Neso Academy Dynamic Arrays in C++ (std::vector) - Youtube - The Cherno vector insert() Function in C STL - GeeksforGeeks Iterator Ref:\nITERATORS in C++ - The Cherno Raw Function Pointer A representation for a function with an alias. For example, myFunc(int i) is represented as: void (*alias)(int)\nA similar thing in Python:\n1 2 3 4 5 6 norm = torch.nn.LayerNorm myModel = nn.Sequential( nn.Linear(3,5), norm(5) ) The class name is assigned to a variable, which is an alias (reference). And objects are instantiated later by specifying arguments.\nThe instantiation is deferred.\nFuntion pointer stores a function\u0026rsquo;s address in the compiled binary code.\nIn C++, to define a function pointer variable, its type can be deduced by auto:\n1 2 3 4 5 6 7 8 9 10 #include\u0026lt;iostream\u0026gt; void MyFunc(int a){ std::cout \u0026lt;\u0026lt; \u0026#34;Hello\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; } int main(){ auto pFunc = MyFunc; // No (), otherwise calling. pFunc(1); } The \u0026ldquo;Function Pointer\u0026rdquo; type is void(*)(args). And a function pointer variable is void(*var_name)(args).\n1 2 3 4 int main(){ void(*pFunc)(int) = MyFunc; pFunc(1); } And it can be rewritten as a type by typedef:\n1 2 3 4 5 int main(){ typedef void(*MyFunc_type)(int); MyFunc_type pFunc = MyFunc; pFunc(1); } When passing a function to another function, the function pointer parameter should be defined as void (*func_name)(args_list):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; void PrintInt(int i){ std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, void(*func)(int) ){ # Apply func to each element in the vector for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,2,3}; ForEach(myVec, PrintInt); } For compactness, the function PrintInt can be written as a one-line (anonymous) lambda function :\n1 2 3 4 5 6 7 8 9 void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, void(*func)(int) ){ for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,2,3}; ForEach(myVec, [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;} ); } Pass an outside variable a into the lambda function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;string\u0026gt; void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, const std::function\u0026lt;void(int)\u0026gt;\u0026amp; func){ for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {1,3,5,6,0}; std::string a = \u0026#34;Current element:\u0026#34;; ForEach(myVec, [\u0026amp;a](int i){ std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}); } [\u0026amp;a] captures the outside variable a by reference without copying data.\nOutput 1 2 3 4 5 6 7 yi@yi:~/Downloads/Cpp_Study$ g++ function_pointer.cpp yi@yi:~/Downloads/Cpp_Study$ ./a.out Current element:1 Current element:3 Current element:5 Current element:6 Current element:0 Ref:\nFunction Pointers in C++ - YouTube - The Cherno Lambda expression (2023-11-11)\n[](){}:\nSquare brackets [] is for variables outside the scope of {} lambda function\n[\u0026amp;] all variables are captures by references. [=] all variables are captured by values. Parentheses () enclose parameter to be used in {} lambda function\nCurly braces {} enclose operations.\nUsage:\nIf there is a function pointer, lambda function can be in place of it. Example using lambda expression as a bool:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,5,4,3}; auto it = std::find_if(myVec.begin(), myVec.end(), [\u0026amp;](int val){return val \u0026gt; 3;} ); std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; std::endl; // Output: 5 } std::find_if examins myVec to find the fisrt element that is larger than 3 and returns an iterator it.\nIterator object it is the address of the first element in the container. And it+1 points to the 2nd element.\nSo, it needs to be dereferenced to get the value.\nRef:\nLambdas in C++ - The Cherno Lambda expressions - cppreference std::find, std::find_if, std::find_if_not - cppreference.com std::function (2023-11-11)\nA representation for a kind of callable objects, such as normal function, functor, lambda expression, struct.\n\u0026ldquo;Same kind\u0026rdquo; refers to having the same return type and input arguments, as indicated by the function signature.\n1 2 3 4 5 6 7 8 9 // Normal function can be represented as a std::function: void MyFunc(int x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; }; std::function\u0026lt;void(int)\u0026gt; f = MyFunc; // Lambda expression is assigned to a variable: std::function\u0026lt;void(int)\u0026gt; fl = [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}; // Equivalent to a function pointer: auto pf = [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}; The std::function\u0026lt;void(int)\u0026gt; represents any callable object that has no return value and only one int input argument.\nTherefore, it can be used as the type for a formal parameter when defining a function that takes as input a specific kind of function.\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; void funcsPrintNum(int x, std::function\u0026lt;void(int)\u0026gt; func){ func(x); } int main(){ auto myLambda = [](int i){ std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; }; funcsPrintNum(1, myLambda); } A vector storing multiple callable objects that have an identical function signature.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;vector\u0026gt; void aNormalFunc(int i){ std::cout \u0026lt;\u0026lt; i + 5 \u0026lt;\u0026lt; std::endl;} int main(){ std::vector\u0026lt;std::function\u0026lt;void(int)\u0026gt;\u0026gt; vf; vf.push_back([m=5](int i){std::cout \u0026lt;\u0026lt; m \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}); vf.push_back(aNormalFunc); vf[0](1); // Call the 1st function with the argument 1 vf[1](4); // Call the 2nd function } Output 1 2 51 9 Calling a function from a std::function is slower than calling it natively. This is the cost of unifying representation for callable objects with the same input and output.\nRef:\nC++ std::function Next Level Function Polymorphism - The Builder Template (2023-11-14)\nThe template argument typename T (a type) will be deduced when compiling and the corresponding function will be created to link.\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; template \u0026lt;typename T\u0026gt; // before the return type of the func void Print(T value){ std::cout \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; } int main(){ Print(5); Print(5.5f); Print(\u0026#34;A string\u0026#34;); } When calling a template in program, the teamplate argument \u0026lt;T\u0026gt; can be omitted as it can be deduced by compiler.\nOutput 1 2 3 5 5.5 A string The following int N is required at compile time, because an array created on stack needs size known when compiling. And it can be deduced when evaluating the template:\n1 2 3 4 5 6 7 8 9 10 11 12 template \u0026lt;typename T, int N\u0026gt; class ArrayClass{ private: // visible inside the class T m_Array[N]; public: // visible outside the class int GetSize() const {return N;} }; int main(){ ArrayClass\u0026lt;int, 5\u0026gt; myArray; std::cout \u0026lt;\u0026lt; myArray.GetSize() \u0026lt;\u0026lt; std::endl; } Ref:\nTemplates in C++ - YouTube - The Cherno Class vs Struct \u0026ldquo;Struct is the same as class.\u0026rdquo; \u0026ndash; CLASSES vs STRUCTS in C++ - YouTube - The Cherno\nDefault members (without setting visibility) of a class are all private.\nIn contrast, members in a struct are all public by default.\nStruct also can set members (variables and methods) to private.\nStruct can do inheritance as well, but not common. Struct is often used to group variables.\ndoubt: Why does a struct can include itself? (2024-01-27) Not itself. fromChunk is a function and its return type is the stuct. The code is from 3DGS.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct GeometryState { size_t scan_size; float* depths; char* scanning_space; bool* clamped; int* internal_radii; float2* means2D; float* cov3D; float4* conic_opacity; float* rgb; uint32_t* point_offsets; uint32_t* tiles_touched; static GeometryState fromChunk(char*\u0026amp; chunk, size_t P); }; Bitwise Operator (2023-11-14)\n~\n1 2 3 4 5 6 7 8 9 #include \u0026lt;iostream\u0026gt; #include \u0026lt;bitset\u0026gt; int main(){ std::size_t alignment = 128; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;(alignment) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;(alignment-1) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;( ~ (alignment-1)) \u0026lt;\u0026lt; std::endl; } Output:\n1 2 3 00000000000000000000000010000000 00000000000000000000000001111111 11111111111111111111111110000000 Ref:\nhow to output an int in binary? - SO #define (2023-11-15)\nShort for a for loop\n1 2 3 4 5 #define fori(x) for(int i=0; i\u0026lt;x; i++) fori(20){ ... } Ref: Why use #define instead of a variable\n#define 100_000 won\u0026rsquo;t work. #define 100000 is ok.\nHeader files (2023-11-15)\nFunctions\u0026rsquo; signature must be written in the file to use them.\nThose declarations can be stored in header files.\nThe preprocess directive #include will copy and paste a header file to there.\nHeader files can form a chain, whereas a potential probelm is repeated difinations. (e.g. structs need unique names.)\nThe header guide #pragma once marks a header file won\u0026rsquo;t be included multiple times into a single translation unit (cpp file).\nOld fasion is wrapping the entire header file with #ifndef symbol and #endif\n(2024-06-03)\nInclude Guards\n1 2 3 4 5 6 7 8 //x.h #ifndef __X_H_INCLUDED__ // if x.h hasn\u0026#39;t been included yet... #define __X_H_INCLUDED__ // #define this so the compiler knows it has been included class X { }; #endif Suffix .h is used to differentiate \u0026ldquo;C standard library\u0026rdquo; (\u0026lt;stdio.h\u0026gt;) and \u0026ldquo;C++ standard libray\u0026rdquo; (\u0026lt;iostream\u0026gt;)\nAngular brackets \u0026lt;fname\u0026gt; will search the \u0026ldquo;standard include directories\u0026rdquo; (of compiler) for the file named fname.\nWhile double qutoes \u0026quot;../myHeader.h\u0026quot; normaly enclose a relative path, although it can enclose a file that resides in \u0026ldquo;standard include directories\u0026rdquo; as well (e.g., \u0026quot;iostream\u0026quot;).\nDo not use namespace in a header file. Why I don\u0026rsquo;t \u0026ldquo;using namespace std\u0026rdquo; - The Cherno Ref:\nC++ Header Files - The Cherno Source file inclusion - cppreference.com (2024-05-13)\nRef: C/C++头文件里有什么, 什么是接口与实现分离, 为什么这么干? 代码知识 - 不停感叹的老林\n头文件的意义：多处声明，一处定义 (2024-06-03)\nRef: Headers and Includes: Why and How - C++ Articles - Disch (Found by DDG)\nSince each source file is compiled individually to object files (which are binary too) before linking them together, the header files are needed to make the function interface recognizable for the compiler, and keep the implementation individual.\nKeeping each file self-contained facilitates making modifications.\nAs each source file is compiled separately, the implementation of a function is unknown in another source file. So the header file is a \u0026ldquo;messenger\u0026rdquo; between them by telling funcitons\u0026rsquo; interface to each other.\nThus, functions\u0026rsquo; interface and their implementation are separated.\nSpliting the program to multiple piece files instead of a single comprehensive file helps debugging and facilitate compilation (only compile the needed file).\nThe compiler compiles (translate) each source file into a binary (object file), and the linker finds function implementations, and connects the object files and precompiled libraries together. 0.5 — Introduction to the compiler, linker, and libraries #include is copy \u0026amp; paste during preprocessing.\nHeader files are pasted (#included) and not compiled, whereas source files are compiled and not included. Do not #include source files.\nOnly #include the necessary things in header files.\nTry to avoid #include the full .h file:\nDo nothing ❮ Forward declare A ❮ #include \u0026ldquo;a.h\u0026rdquo;\nForward declared dependencies: class aClass;\nThe right way to include: \u0026ldquo;Forward declare when you can, don\u0026rsquo;t #include unless it\u0026rsquo;s necessary\u0026rdquo;\nUse a pointer or reference to an object aClass* a, instead of instantiating a full object: aClass a, and then use forward declaration: class aClass; to avoid the circular inclusion problem where using #include \u0026quot;a.h\u0026quot; is necessary.\ninline functions require their function body to exist in the every cpp file that calls them.\nTheir function bodies can be put in a header file.\nThe forward declaration for a template class requires typedef, which could cause inconvenience as all files that use the template class need to be modified manually when the template class changes.\nThe solution is putting the typedef of the template class in a header file and then #include it in another header file.\n(2024-06-04)\nRef: How a compiler knows from a header file, that a source file exists somewhere? - SO (Found in DDG)\n#include \u0026quot;myfile.h\u0026quot; has a broader searching scope than #include \u0026lt;stdio.h\u0026gt;\nThe \u0026quot;\u0026quot; will search in the current working folder besides the predefined standard well-known directories.\nIncluding a header #include math.h but without linking the library libm.a into the executable binary, by specifying it in the gcc build command: -lm, the functions in libm.a can\u0026rsquo;t be called\nGcc convention: For a precompiled library: lib\u0026lt;name\u0026gt;.a, the argument to link it: -l\u0026lt;name\u0026gt;. -I and -L specify the additional searching paths to headers and libraries.\nRef: C/C++ Headers and Source Files: How Do They Work? - codeproject (Not accurate enough)\nEverything (function, struct, variable) in cpp needs a declaration before using it, so header files are pasted in the source file with #include.\nHeader files are declarations informing the compiler. Semicolon after the function signature indicates to the compiler that this is a function prototype declaration, rather than the definition.\nCompilation only process source files, as header files already have been pasted into source files.\nCompiling \u0026amp; Linking Compiling:\nConvert cpp files (translation units) to binary object file.\nPreprocess statements (directives) will be done when compiling.\n#include just copy and paste to where it is. #define replace symbol #if 1 (or 0) and #endif to use a code snippest or not. Once all preprocess statements are finished, the preprocessed \u0026ldquo;full\u0026rdquo; code will be tranformed to an object file\nMultiple cpp files can be combined into a single cpp file by #include, and then only one translation unit will be generated.\nHowever, if every cpp file doesn\u0026rsquo;t include others, each cpp file will have an translation unit.\nEach translation unit yields a object file\nconstant folding: constant arithmetics will be solved at compile time, such that there is only 1 asm instruction: put a number into a register.\nRef: How the C++ Compiler Works\nLinking:\nFind the binary code of functions, symbols and entry point main, according to their \u0026ldquo;function signatures\u0026rdquo;, and then form a executable file.\nCompiling is to preprocess and generate object file. Linking is organizing binary code to a executable fiel. Build = Compiling + Linking Link error and compile error\nError code starts with \u0026ldquo;LNK\u0026rdquo; means it\u0026rsquo;s an linking error:\nFunction signature mismatch with the definition (name, return type, parameters), thus the binary code can\u0026rsquo;t be found: LNK2019 Unresolve externel symbol\nNo entry point definition (e.g., main()) for Application(.exe).\nDuplicate symbols (function signature, variables) are found in the project\u0026rsquo;s obj files. LNK1169 One or more funcname defined symbols found\nEven if a function isn\u0026rsquo;t called by main(), it could be called in other cpp files.\nTherefore, linker also needs to link those \u0026ldquo;uncalled\u0026rdquo; functions, unless it has static keysword representing it will only be called internally in the cpp file it exists.\nError code starts with \u0026ldquo;C\u0026rdquo; means it\u0026rsquo;s an compiling error:\nSyntax errors, e.g., missing ; (C2143) No declaration or definition for functions used in a cpp file. Multiple definitions in a single file. Multiple definition if a function is defined in a header file without static or inline limitation, because #include is just pasting header file.\n\u0026ldquo;Log.h\u0026rdquo;:\n1 2 3 4 5 #pragma once #include \u0026lt;iostream\u0026gt; void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } \u0026ldquo;Log.cpp\u0026rdquo;\n1 2 3 4 #include \u0026#34;Log.h\u0026#34; // Log definition will be here void InitLog(){ Log(\u0026#34;Initialization\u0026#34;); } \u0026ldquo;main.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 #include \u0026#34;Log.h\u0026#34; // Log definition will be here int Multiply(int\u0026amp; a, int\u0026amp; b){ Log(\u0026#34;Multiplication:\u0026#34;); return a*b; } int main(){ int c = Multiply(5, 2); } There won\u0026rsquo;t be compiling error, since each cpp file knows functions to be used. But this project cannot be linked since there two Log function with the same signature, and the linker don\u0026rsquo;t know which one should be used.\nThere are 3 soulutions:\nstatic will limit the function used only in the file it resides in and won\u0026rsquo;t be visible to any other obj files.\n1 2 3 static void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } That means even though \u0026ldquo;Log.cpp\u0026rdquo; and \u0026ldquo;main.cpp\u0026rdquo; both have the definition of void Log(const char*), they use their own Log. The linker won\u0026rsquo;t be confused.\ninline will replace the calls of the function with its body:\n1 2 3 inline void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } Define the function in one of translation units. And header file only contain declarations.\nMove the definition into \u0026ldquo;Log.cpp\u0026rdquo;:\n1 2 3 void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } Ref: How the C++ Linker Works - The Cherno\n(2024-06-04)\nHow C++ Works: Understanding Compilation | Toptal (Found in DDG)\nPreprocess: Copy/paste header files to source files:\n\u0026ldquo;test-compile.cpp\u0026rdquo;\n1 2 3 #include \u0026lt;iostream\u0026gt; int main(int argc, char* argv[] ){ std::cout \u0026lt;\u0026lt; \u0026#34;Hello\u0026#34; \u0026lt;\u0026lt; std::endl;} Add -E option to stop the compiler after preprocessing: (Found by perplexity)\n1 2 gcc -E test-compile.cpp -o test-compile.ii wc test-compile.ii The preprocessing and compiling in c++ is similar to c lang. Compile each source file separately to generate a object file: (sec2)\nGiven a c file: \u0026ldquo;sum.c\u0026rdquo;:\n1 2 int sumI(int a, int b){ return a+b;} float sumF(float a, float b){return a+b;} -c stops the compiler after compiling before linking. manual (SO)\n1 yi@Alien:~/Downloads/Cpp_Study$ gcc -c test_compile.cpp -o sum.o nm lists symbols from object files.\n1 2 3 yi@Alien:~/Downloads/Cpp_Study$ nm sum.o 0000000000000018 T _Z4sumFff 0000000000000000 T _Z4sumIii No symbol is imported. Two symbols are exported as part of the .text segment T. (Don\u0026rsquo;t understand)\n_Z4sumFff and _Z4sumIii are the names used by other source files calling the 2 functions.\n#include header files is declaring the 2 functions\nMix calling C and C++ functions\nFunction symbols in cpp enable the feature of overload when several functions have the same name but different input arguments. Link all the object files to generte a binary file.\n(2024-06-06)\nCompile 的中文翻译：编译 = 翻译 + 编纂。编纂的意思是把多个文件放到一起。\npile 是“一摞”, compile 是把一摞文件放到一起。\nFind Next MSB (2023-11-17)\nFind the next-highest bit of the MSB (Most Significant Bit)\nRight shift:\n1 2 3 4 5 6 7 #include \u0026lt;iostream\u0026gt; #include \u0026lt;bitset\u0026gt; int main(){ uint16_t x = 32767; // 01111111_11111111 std::cout \u0026lt;\u0026lt; (x \u0026gt;\u0026gt; 16) \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; (x \u0026gt;\u0026gt; 12) \u0026lt;\u0026lt; std::endl; // 0000000_0000111 } Parse Args (2023-11-03)\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(int argc, char **argv) { std::cout \u0026lt;\u0026lt; \u0026#34;Number of input arguments: \u0026#34; \u0026lt;\u0026lt; argc \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt;= argc-1; i++) { std::cout \u0026lt;\u0026lt; argv[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } return 0; } Scope Resolution (2024-02-01)\nScope resolution operator: :: | Microsoft Learn :: : Scope resolution operator in C++ - GeeksforGeeks\nDifferentiate global and locatl variables with the same name; Identify classes with the same name in different namespace. Define a member function outside the class Access a class\u0026rsquo;s static member Distinguish members with the same name reside in multiple classes with inheritances. Refer to nesting class. . is used for member of object. Member access operators - cppreference\na-\u0026gt;b, where a is a pointer, equals to ((*a).b). In contrast, a.b where a is an object. member access operators - C++ Forum\nType Casting (2024-02-03)\n(uint32_t) is forceful casting in terms of the bits reading. whereas static_cast\u0026lt;uint32_t\u0026gt; cannot be applied on data with bits interpretion mismatched. Type conversions and type safety - Microsoft Learn\n1 2 3 4 5 6 7 #include \u0026lt;iostream\u0026gt; int main(){ float depth = 15.7; std::cout \u0026lt;\u0026lt; (uint32_t)depth; // 15 std::cout \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(depth); // 15 std::cout \u0026lt;\u0026lt; reinterpret_cast\u0026lt;uint32_t\u0026gt;(depth); //error } Files Process Save location (2024-06-26)\nUse command arguments to specify saving locations.\nRef:\ndtcMLOps/upsamplingCloudPCL Claude3.5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;filesystem\u0026gt; // ... (rest of the code) int main(int argc, char** argv) { std::filesystem::path output_dir; if (arg_parser.get\u0026lt;std::string\u0026gt;(\u0026#34;--output-dir\u0026#34;) == \u0026#34;-\u0026#34;) { output_dir = std::filesystem::current_path(); } else { output_dir = arg_parser.get\u0026lt;std::string\u0026gt;(\u0026#34;--output-dir\u0026#34;); } if (!std::filesystem::exists(output_dir)) { std::filesystem::create_directories(output_dir); } std::filesystem::path output_file = output_dir / \u0026#34;upsampled_cloud.pcd\u0026#34;; pcl::io::savePCDFile(output_file.string(), *output_cloud); } Full demo Filename format (2024-06-26)\nUse regex\nQuestion:\nI want to make up a filename with the arg_parser.get\u0026lt;std::string\u0026gt;(\u0026quot;--cloudfile\u0026quot;), rather than hardcode \u0026ldquo;upsampled_cloud.pcd\u0026rdquo;.\nFor example, if the \u0026ndash;cloudfile is \u0026ldquo;/home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/image_ref/scan1/points3d.ply\u0026rdquo;, I want the filename to be \u0026ldquo;scan001_l3.ply\u0026rdquo;. Another example: If the \u0026ndash;cloudfile is \u0026ldquo;/home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/image_ref/scan4/points3d.ply\u0026rdquo;, I want the filename to be \u0026ldquo;scan004_l3.ply\u0026rdquo;. Another example: If the \u0026ndash;cloudfile is \u0026ldquo;/home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/image_ref/scan118/points3d.ply\u0026rdquo;, I want the filename to be \u0026ldquo;scan118_l3.ply\u0026rdquo;. How to write code?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #include \u0026lt;filesystem\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;regex\u0026gt; int main(int argc, char** argv) { std::string input_filepath = arg_parser.get\u0026lt;std::string\u0026gt;(\u0026#34;--cloudfile\u0026#34;); // Use regex to extract the scan number std::regex scan_regex(R\u0026#34;(scan(\\d+)/points3d.ply)\u0026#34;); std::smatch match; std::string scan_number; if (std::regex_search(input_filepath, match, scan_regex)) { scan_number = match[1].str(); // Pad the scan number with zeros std::ostringstream padded_num; padded_num \u0026lt;\u0026lt; std::setw(3) \u0026lt;\u0026lt; std::setfill(\u0026#39;0\u0026#39;) \u0026lt;\u0026lt; std::stoi(scan_number); scan_number = padded_num.str(); } else { // If no scan number found, use a default scan_number = \u0026#34;---\u0026#34;; } // Construct the new filename std::string output_filename = \u0026#34;scan\u0026#34; + scan_number + \u0026#34;_l3.ply\u0026#34;; } R\u0026quot;()\u0026quot; means raw string which doesn\u0026rsquo;t need \\ converting some characters to be escape characters.\nThe regex given by chatGPT4o is scan_regex(\u0026quot;scan(\\\\d+)/points3d\\\\.ply\u0026quot;), which works either.\nRef:\nClaude3.5\nregex101: build, test, and debug regex ( Found in DDG )\nStatic Issues:\nNotes:\n(2024-12-02)\nFive places essentially perform 3 effects\nReferences: {{{\n5分钟讲透C++的static（5分钟实在讲不完，超时了很抱歉【手动狗头】）- bilibili - 打洞哥不打洞 Found in his collection 白玉莫寻的个人主页 Noticed his dynamic avatar in the comments of 【官方自制】CANoe/CANalyzer基础教程合集（已完结）共18P - bilibili - 维克多汽车技术 }}} Supports:\n(2025-06-27T13:17)\n使类成员全局唯一，限制一个全局变量或一个普通函数只在本文件内生效，延长局部变量的生命周期 r1-打洞哥\n如果类的一个成员变量是静态的，那么这个变量与类挂钩，而不与实例化出来的对象挂钩； 如果该成员变量被修改，所有对象的该变量都会改变。 可以通过类名访问这个成员变量\n如果类的一个成员函数是静态的，那这个成员函数是唯一的，是被所有对象共享的，所有对象的这个成员函数都对应同一个函数。 就可以通过类找到这个成员函数。\n“通过类找到的成员函数” 与一个普通函数就类似了：都是只需用函数名就可以调用函数\n函数是一个代码段的首地址，调用函数时会跳转到那个地址。 如果类的一个成员函数是非静态的，不同对象的该成员函数对应的也是 同一个地址， 但是不同对象调用该成员函数时，有一个传入的隐含参数不同：该对象的 this 指针 （不同对象的 this 不同）。\n换句话说，调用非静态成员函数时，需要指定对象的指针，不然不知道是哪个对象。\n对于 不在类中的变量，即全局变量，如果需要在另一个文件中使用它，需要在那个文件中声明它时，用 extern 修饰。 在链接时，使用的是同一个变量。\n如果两个文件中有两个相同名字的全局变量，并且它们都被用 static 修饰，它们在链接时，不会是同一个变量， 而是对应不同的变量。\n对于一个普通函数，只能在当前文件生效，若要在其他文件中使用它，需要 include 头文件（即声明它）\n在函数内部定义的变量是局部变量，如果不用 static 修饰，则此局部变量会在 函数结束后释放。 下一次调用函数时，会重新创建局部变量。 如果使用了 static 修饰，创建后直到 程序结束才释放，所以当再次运行到创建语句时， 发现该变量已经存在，便不会再次创建，所以它可以持续被修改\nStatic Variables on Memory\nReferences: {{{\nCPU眼里的：静态、全局、临时变量 - bilibili - 阿布编程 Found in the recommendation of r1-打洞哥 RL78 F23, F24 User\u0026rsquo;s Manual Hardware-Rev110-r01uh0944ej0110-rl78f23-f24 }}} Supports:\n(2025-06-27T16:55)\n初始化区与未初始化区 (2025-07-02T00:38)\nTwo files can contain variables with the same name, because their symbol are distinguished by their filenames appended to them. This allows the linker to recognize them as separate variables. Conditional Compilation Pre-processor Directives\nReferences: {{{\n范懿 - Slides4C:ConditionalCompilationDirectives Explain the pre-processor directives in C language - TutorialsPoint PRE PROCESSOR DIRECTIVES IN C LANGUAGE. - Carleton University Herbert Schildt - C++: The Complete Reference, 4th Edition, Page 243 }}} Supports:\n(2025-08-25T13:55)\nConditional compilation directive r2-Tutorial, r3-Carleton, r4-Herbert\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;stdio.h\u0026gt; #define MAX 100 int main(void) { #if MAX\u0026gt;99 printf(\u0026#34;Compiled for array greater than 99.\\n\u0026#34;); #endif return 0; } ","date":"2023-11-09T15:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cpp-misc/","title":"memo: Lang - C++ | Misc"},{"content":" The 2 submodules are CUDA projects that should be debugged separately from the python project, because the python debugger can\u0026rsquo;t access the PyTorch CUDA extensions. Debug Settings (2023-11-08)\nVSCode Intellisense To enable intellisense, the configuration of \u0026ldquo;C/C++ extension\u0026rdquo; must be put in the \u0026ldquo;.vscode/\u0026rdquo; of (top-level) current working directory (.vscode/c_cpp_properties.json), not in submodule\u0026rsquo;s \u0026ldquo;.vscode/\u0026rdquo; (submodules/diff-gaussian-rasterization/.vscode/c_cpp_properties.json).\nOtherwise, the settings won\u0026rsquo;t be loaded.\nCUDA syntax (e.g.,__global__) and header (\u0026lt;cooperative_groups/reduce.h\u0026gt;, min) won\u0026rsquo;t be recognized, if includePath and compiler are incorrect,\nDoubtful Attempts Put the includes paths of torch before cuda\u0026rsquo;s include. Otherwise #include \u0026lt;cooperative_groups/reduce.h\u0026gt; in \u0026ldquo;forward.cu\u0026rdquo; can\u0026rsquo;t be found.\nSet the includePath in \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo; as:\n1 2 3 4 5 6 7 \u0026#34;includePath\u0026#34;: [ \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34;, \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/cuda-11.6/include\u0026#34; ], Error in \u0026ldquo;rasterizer_impl.cu\u0026rdquo;: cannot open source file glm/glm.hpp (dependency of cub/cub.cuh)\n1 sudo apt-get install libglm-dev Why can\u0026rsquo;t C++ find GLM headers? - SO\nError in \u0026ldquo;backward.cu\u0026rdquo;: namespace \u0026quot;cooperative_groups\u0026quot; has no member \u0026quot;this_grid\u0026quot;\nChange CMakelist: SO\nDidn\u0026rsquo;t fix\nError: identifier \u0026quot;min\u0026quot; is undefinedC/C++(20)\nDidn\u0026rsquo;t fix\nSolution: Use nvcc as compiler. Verified \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/include/python3.7m\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], \u0026#34;defines\u0026#34;: [], \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/local/cuda-11.6/bin/nvcc\u0026#34;, \u0026#34;cStandard\u0026#34;: \u0026#34;c17\u0026#34;, \u0026#34;cppStandard\u0026#34;: \u0026#34;gnu++14\u0026#34;, \u0026#34;intelliSenseMode\u0026#34;: \u0026#34;linux-gcc-x64\u0026#34; } ], \u0026#34;version\u0026#34;: 4 } Debug diffRast (2023-11-18)\nAction: Write a \u0026ldquo;main.cpp\u0026rdquo; that calls the methods of the compiled library \u0026ldquo;CudaRasterizer\u0026rdquo; for debugging. Repo for debugging DiffRast: debug_diff_rust\n\u0026ldquo;CMakeLists.txt\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cmake_minimum_required(VERSION 3.20 FATAL_ERROR) project(MyApp) # ${PROJECT_NAME} find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_subdirectory(external/diff-gaussian-rasterization-comments) add_executable(MyApp main.cpp) set_property(TARGET MyApp PROPERTY CXX_STANDARD 17) target_link_libraries(MyApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) target_link_libraries(MyApp CudaRasterizer) target_sources(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization-comments/rasterize_points.cu) target_include_directories(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization-comments) \u0026ldquo;launch.json\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DiffRast Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/build/MyApp\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, } ] } \u0026ldquo;tasks.json\u0026rdquo;\nopen 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;cmake-configure\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-DCMAKE_BUILD_TYPE=Debug\u0026#34;, \u0026#34;-DCMAKE_PREFIX_PATH=/usr/local/libtorch\u0026#34;, \u0026#34;..\u0026#34;, ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;cmake-build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--build\u0026#34;, \u0026#34;.\u0026#34;, // \u0026#34;--config\u0026#34;, // \u0026#34;Debug\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;dependsOn\u0026#34;: [\u0026#34;cmake-configure\u0026#34;, \u0026#34;cmake-build\u0026#34;] } ] } Enter CUDA Kernels Building project as above won\u0026rsquo;t allow to step into CUDA kernels, e.g., preprocessCUDA.\n(2024-01-20)\nEnable CUDA language in CMAKE and set for debugging:\n1 2 3 4 5 project(MyApp LANGUAGES CXX CUDA) set(CMAKE_BUILD_TYPE Debug) if(CMAKE_BUILD_TYPE STREQUAL \u0026#34;Debug\u0026#34;) set(CMAKE_CUDA_FLAGS \u0026#34;${CMAKE_CUDA_FLAGS} -g -G\u0026#34;) endif() Build to binary file:\n1 2 3 # cd ~/Downloads/debug_diff_rast cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja cmake --build ./build Launch cuda-gdb: cuda-gdb ./build/MyApp. It\u0026rsquo;s convenient to use cuda-gdb in the terminal of vscode, where I can jump to the code by clicking the path.\nAdd breakpoint inside preprocessCUDA: (cuda-gdb) b forward.cu:182 Not sure if it worked.\n(2024-01-26) Nsight (CUDA-GDB)\nGenerate Makefile with cmake:\n1 2 cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; cmake --build ./build To let cmake produce Makefile, do not use -GNinja. Why isn\u0026rsquo;t the command \u0026ldquo;cmake .\u0026rdquo; generating a makefile? - SO (2024-04-23)\nError: \u0026ldquo;Caffe2: Cannot find cuDNN library\u0026rdquo;\nSolved by installing cudnn-xxx.deb Error Message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 (gaussian_splatting) yi@yi-Alien:~/Downloads/debug_diff_rast$ cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; -- The CXX compiler identification is GNU 9.4.0 -- The CUDA compiler identification is NVIDIA 11.6.55 -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Detecting CUDA compiler ABI info -- Detecting CUDA compiler ABI info - done -- Check for working CUDA compiler: /usr/local/cuda-11.6/bin/nvcc - skipped -- Detecting CUDA compile features -- Detecting CUDA compile features - done -- Found CCache: /usr/local/bin/ccache -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found CUDA: /usr/local/cuda-11.6 (found version \u0026#34;11.6\u0026#34;) -- Caffe2: CUDA detected: 11.6 -- Caffe2: CUDA nvcc is: /usr/local/cuda-11.6/bin/nvcc -- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.6 -- Caffe2: Header version is: 11.6 -- Could NOT find CUDNN (missing: CUDNN_LIBRARY_PATH CUDNN_INCLUDE_PATH) CMake Warning at /usr/local/libtorch/share/cmake/Caffe2/public/cuda.cmake:120 (message): Caffe2: Cannot find cuDNN library. Turning the option off Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include) /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:19 (find_package) -- /usr/local/cuda-11.6/lib64/libnvrtc.so shorthash is 280a23f6 -- Autodetected CUDA architecture(s): 6.1 -- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61 CMake Error at /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:96 (message): Your installed Caffe2 version uses cuDNN but I cannot find the cuDNN libraries. Please set the proper cuDNN prefixes and / or install cuDNN. Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:19 (find_package) -- Configuring incomplete, errors occurred! I didn\u0026rsquo;t encounter this error before.\nThere is no libcudnn.so under directory: /usr/local/cuda-11.6/lib64/\nlibcudnn.so does\u0026rsquo;t apper in the output of python -m torch.utils.collect_env as shown in issue#30\nDownload cuDNN Library for CUDA11 Ubuntu 20.04. Referring to this answer: spconv - issues#277\n1 2 3 4 5 wget https://developer.download.nvidia.com/compute/cudnn/9.1.0/local_installers/cudnn-local-repo-ubuntu2004-9.1.0_1.0-1_amd64.deb sudo dpkg -i cudnn-local-repo-ubuntu2004-9.1.0_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu2004-9.1.0/cudnn-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cudnn-cuda-11 The libcudnn and cudnn are installed under /usr/lib/ and /usr/include:\n1 2 3 4 5 6 7 8 9 10 11 12 13 (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ whereis libcudnn libcudnn: /usr/lib/x86_64-linux-gnu/libcudnn.so (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ whereis cudnn cudnn: /usr/include/cudnn.h (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ dpkg -l | grep cudnn ii cudnn-local-repo-ubuntu2004-9.1.0 1.0-1 amd64 cudnn-local repository configuration files ii cudnn9-cuda-11 9.1.0.70-1 amd64 NVIDIA cuDNN for CUDA 11 ii cudnn9-cuda-11-8 9.1.0.70-1 amd64 NVIDIA cuDNN for CUDA 11.8 ii libcudnn9-cuda-11 9.1.0.70-1 amd64 cuDNN runtime libraries for CUDA 11.8 ii libcudnn9-dev-cuda-11 9.1.0.70-1 amd64 cuDNN development headers and symlinks for CUDA 11.8 ii libcudnn9-static-cuda-11 9.1.0.70-1 amd64 cuDNN static libraries for CUDA 11.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 (base) yi@yi-Alien:~$ apt-cache search cudnn cudnn9-cuda-11-8 - NVIDIA cuDNN for CUDA 11.8 cudnn9-cuda-11 - NVIDIA cuDNN for CUDA 11 cudnn9-cuda-12-4 - NVIDIA cuDNN for CUDA 12.4 cudnn9-cuda-12 - NVIDIA cuDNN for CUDA 12 cudnn9 - NVIDIA CUDA Deep Neural Network library (cuDNN) cudnn - NVIDIA CUDA Deep Neural Network library (cuDNN) libcudnn9-cuda-11 - cuDNN runtime libraries for CUDA 11.8 libcudnn9-cuda-12 - cuDNN runtime libraries for CUDA 12.4 libcudnn9-dev-cuda-11 - cuDNN development headers and symlinks for CUDA 11.8 libcudnn9-dev-cuda-12 - cuDNN development headers and symlinks for CUDA 12.4 libcudnn9-samples - cuDNN samples libcudnn9-static-cuda-11 - cuDNN static libraries for CUDA 11.8 libcudnn9-static-cuda-12 - cuDNN static libraries for CUDA 12.4 cudnn-local-repo-ubuntu2004-9.1.0 - cudnn-local repository configuration files After installation, the cmake configuration works fine.\nI didn\u0026rsquo;t copy it to /usr/local/cuda/lib64 or /usr/local/cuda/include, like: CuDNN not found while compiling PyTorch C++ extension - Forum\nI didn\u0026rsquo;t set environment variable neither.\n(2024-05-01)\nLambda server met the same problem. The current cudnn 9.1 doesn\u0026rsquo;t have deb option for Ubuntu 18.04. I downloaded the history version cudnn 8.9.7 (2023/12/05): Local Installer for Ubuntu18.04 x86_64 (Deb) (839MB)\nwget cannot download it. Have to use browser.\n1 wget https://developer.nvidia.com/downloads/compute/cudnn/secure/8.9.7/local_installers/11.x/cudnn-local-repo-ubuntu1804-8.9.7.29_1.0-1_amd64.deb/ Install the deb package with reference to cudnn 9.1:\n1 2 3 sudo dpkg -i cudnn-local-repo-ubuntu1804-8.9.7.29_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu1804-8.9.7.29/cudnn-local-AE31B5F1-keyring.gpg /usr/share/keyrings/ sudo apt-get update The name maybe mismatched:\n1 2 3 4 5 root@lambda-server:/data2/zi# sudo apt-get -y install cudnn-cuda-11 Reading package lists... Done Building dependency tree Reading state information... Done E: Unable to locate package cudnn-cuda-11 There are several cudnn package could be installed:\n1 2 3 4 5 6 7 root@lambda-server:/data2/zi# apt-cache search cudnn libcudnn8 - cuDNN runtime libraries libcudnn8-dev - cuDNN development libraries and headers libcudnn8-samples - cuDNN samples libcudnn7-dev - cuDNN development libraries and headers libcudnn7 - cuDNN runtime libraries cudnn-local-repo-ubuntu1804-8.9.7.29 - cudnn-local repository configuration files Search: How to install cudnn Local Installer for Ubuntu18.04 x86_64 (Deb) on DDG:\nFound: NVIDIA cuDNN - NVIDIA Documentation Hub\nReferring to the pdf docs, specify the versions:\n1 2 3 4 5 root@lambda-server:/data2/zi# sudo apt-get install libcudnn8-dev=8.9.7.29-1+cuda11.6 Reading package lists... Done Building dependency tree Reading state information... Done E: Version \u0026#39;8.9.7.29-1+cuda11.6\u0026#39; for \u0026#39;libcudnn8-dev\u0026#39; was not found The installation succeed without adding version:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 root@lambda-server:/data2/zichen# sudo apt-get install libcudnn8-dev The following additional packages will be installed: libcudnn8 The following NEW packages will be installed: libcudnn8 libcudnn8-dev 0 upgraded, 2 newly installed, 0 to remove and 337 not upgraded. Need to get 0 B/878 MB of archives. After this operation, 2,366 MB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 file:/var/cudnn-local-repo-ubuntu1804-8.9.7.29 libcudnn8 8.9.7.29-1+cuda11.8 [441 MB] Get:2 file:/var/cudnn-local-repo-ubuntu1804-8.9.7.29 libcudnn8-dev 8.9.7.29-1+cuda11.8 [437 MB] Selecting previously unselected package libcudnn8. (Reading database ... 231804 files and directories currently installed.) Preparing to unpack .../libcudnn8_8.9.7.29-1+cuda11.8_amd64.deb ... Unpacking libcudnn8 (8.9.7.29-1+cuda11.8) ... Selecting previously unselected package libcudnn8-dev. Preparing to unpack .../libcudnn8-dev_8.9.7.29-1+cuda11.8_amd64.deb ... Unpacking libcudnn8-dev (8.9.7.29-1+cuda11.8) ... Setting up libcudnn8 (8.9.7.29-1+cuda11.8) ... Setting up libcudnn8-dev (8.9.7.29-1+cuda11.8) ... update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v8.h to provide /usr/include/cudnn.h (libcudnn) in auto mode The debugger can step into kernel functions. However, the variables are not visible in the panel: Cannot instantiate printer for default visualizer\nCreate tasks.json\nFollowing this tutorial: Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;label\u0026#34;: \u0026#34;CUDA Make\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;make dbg=1\u0026#34;, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true }, \u0026#34;problemMatcher\u0026#34;: [ \u0026#34;$nvcc\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Makefile }, }, And then click the menu bar in vscode \u0026ldquo;Terminal\u0026rdquo; -\u0026gt; \u0026ldquo;Run Build Task \u0026hellip;\u0026rdquo; -\u0026gt; select \u0026ldquo;CUDA Make\u0026rdquo;.\nCreate launch.json:\n1 2 3 4 5 6 { \u0026#34;name\u0026#34;: \u0026#34;CUDA: Debug with CUDA-GDB\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/MyApp\u0026#34;, }, Select the configuration: \u0026ldquo;CUDA: Debug with CUDA-GDB\u0026rdquo; beside the \u0026ldquo;Run\u0026rdquo; button. Then, click \u0026ldquo;Run\u0026rdquo; to debug.\nThe program can hit the breakpoint in the kernel function preprocessCUDA, for example: float3 p_view; at \u0026ldquo;diff-gaussian-rasterization/cuda_rasterizer/forward.cu#192\u0026rdquo;\n(2024-04-24) On Ubuntu 20.04, CUDA-11.6, gcc 9.4.0\nRe-verified practice: 2 steps enable debugging with breakpoints set inside kernel functions.\nCompile with cmake:\n1 2 (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ cmake --build ./build without clicking: Terminal -\u0026gt; Run Build Task... -\u0026gt; CUDA Make\nLaunch debugger with the above launch.json: \u0026ldquo;CUDA: Debug with CUDA-GDB\u0026rdquo;\n","date":"2023-11-08T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/b-note-3dgs-debug/","title":"Read: 3DGS | Debug Code"},{"content":"Source video: 21. Eigenvalues and Eigenvectors - Gilbert Strang - MIT OpenCourseWare\n(2023-11-06) The 1st-time recitation note. Haven\u0026rsquo;t re-check.\nOut direction same as in Matrix multiply vector acting like MLP, where all current dimensions are combined in different weights to produce each dimension in another space.\n$$ 𝐀 𝐗 \\\\\\ \\begin{bmatrix} y_1 \\\\\\ y_2 \\\\\\ y_3 \\end{bmatrix} = 𝐀 \\begin{bmatrix} x_1 \\\\\\ x_2 \\end{bmatrix} $$ Matrix 𝐀 is like a function that takes as input 𝐗 and output 𝐀𝐗. Typically, 𝐀𝐗 points in different direction from the input 𝐗. Eigenvectors are those 𝐗 make 𝐀𝐗 parallels to 𝐗. Parallel means scaling: 𝐀𝐗 = λ𝐗, where the multiplier λ is eigenvalue. Given a matrix 𝐀, how to solve its eigenvalues λ and eigenvectors 𝐗?\nIf λ = 0, 𝐗 solved from 𝐀𝐗 = 0 by elimination is 𝟎, which is useless. Some hints can be found through the following examples\nProjection matrix Given a projection matrix 𝐏, a 3D vector is projected on to a plane.\nz x p l a n e y Eigenvectors 𝐗 should parallel to the projected vectors 𝐏𝐗. So the vectors located in the plane all are eigenvectors, as $𝐏𝐗=𝐗$, with λ=1.\nVectors perpendicular to the plane statisfy: $𝐏𝐗=0$, i.e., λ=0.\nThose two sets of eigenvectors are perpendicular.\nPermutation matrix Given the 2D permutation matrix $𝐀=[^{0 \\ 1}_{1 \\ 0}]$, which vector can make 𝐀𝐗=𝐗, i.e., identical after permutation.\n𝐗 = $[^1_1]$, with λ=1.\nAs 𝐀 is a 2D matrix, there should be 2 eigenvalues. Which vector can statisfy λ=-1 ?\n𝐗 = $[^{-1}_1]$\nThe 2 sets of eigenvectors 𝐗 are perpendicular as well.\nFact: The sum of eigenvalues equals to the sum of elements on diagonal of 𝐀.\nHere, there is 1 + (-1) = 0 + 0.\nHow to solve λ, 𝐗 Trick: Rearrange to 𝐀𝐗 - λ𝐗 = 0.\nIf there is $(𝐀 - λ𝐈)𝐗 = 0$ and 𝐗, a non-zero vector, becomes 0 after multiplied by (𝐀 - λ𝐈), the (𝐀 - λ𝐈) must be sigular: determinant is 0.\nThe formula $𝐀 - λ𝐈 = 0$ doesn\u0026rsquo;t include 𝐗, so eigenvalues λ can be solved first, as in the example below.\nPlus multiple identity Given a matrix 𝐀 = $[^{3 \\ 1}_{1 \\ 3}]$, to calculate λ, solve:\n$$ \\begin{aligned} | 𝐀 - λ𝐈 | = 0 \\\\\\ \\begin{bmatrix} 3-λ \u0026 1 \\\\\\ 1 \u0026 3-λ \\end{bmatrix} = 0 \\\\\\ (3-λ)^2 -1 = 0 \\\\\\ λ^2 - 6λ + 8 = 0 \\\\\\ (λ-4)(λ-2) = 0 \\end{aligned} $$Two roots: λ₁=4, λ₂=2. Then solve the 2 sets of eigenvectors.\nFor λ₁=4,\n$$ \\begin{aligned} (𝐀 - λ𝐈)𝐗 = 0 \\\\\\ \\begin{bmatrix} -1 \u0026 1 \\\\\\ 1 \u0026 -1 \\end{bmatrix} 𝐗 = 0 \\\\\\ \\end{aligned} $$By letting the free variable to 1, 𝐗 can be solved as $[^1_1]$.\nHence, one of eigenvectors is $[^1_1]$\nFor λ₂=2,\n$$ \\begin{aligned} (𝐀 - λ𝐈)𝐗 = 0 \\\\\\ \\begin{bmatrix} 1 \u0026 1 \\\\\\ 1 \u0026 1 \\end{bmatrix} 𝐗 = 0 \\\\\\ \\end{aligned} $$Hence, one of eigenvectors is $[^{-1}_1]$\nThe sum of the two eigenvalues is the trace of 𝐀, and their product is the determinant of 𝐀.\nComparing the matrix $[^{3 \\ 1}\\_{1 \\ 3}]$ and the permutation matrix $[^{0 \\ 1}\\_{1 \\ 0}]$, there is:\nA $[^{3 \\ 1}\\_{1 \\ 3}]$ $[^{0 \\ 1}\\_{1 \\ 0}]$ λ 4 and 2 1 and -1 𝐗 $[^1\\_1]$ and $[^{-1}\\_1]$ $[^1\\_1]$ and $[^{-1}\\_1]$ If $𝐀𝐗 =λ𝐗$, then $(𝐀+3𝐈)𝐗= 𝐀𝐗+3𝐈𝐗 = (λ+3𝐈)𝐗$\nThat means, if 𝐀 plus 3𝐈, then eigenvalues λ will plus 3𝐈, while eigenvectors doesn\u0026rsquo;t change.\nCan\u0026rsquo;t generalize However, th above property (for multiple identity) can\u0026rsquo;t be generalized to plusing an arbitrary matrix 𝐁.\nMatrix addition doesn\u0026rsquo;t imply eigenvalues addition, because the eigenvector of 𝐁 is typically not 𝐗. Therefore, the following addition can\u0026rsquo;t be performed:\n$$ 𝐀𝐗 =λ𝐗 \\\\\\ 𝐁𝐗 =α𝐗 \\\\\\ (𝐀+𝐁)𝐗 = (λ+α)𝐗 $$Complex eigenvalues Considering rotation matrix:\n$$ \\begin{aligned} \\begin{bmatrix} cosθ \\\\\\ sinθ \\end{bmatrix} = \\begin{bmatrix} cosθ \u0026 -sinθ \\\\\\ sinθ \u0026 cosθ \\end{bmatrix} \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} \\end{aligned} $$Let θ be 90°, the rotation matrix is $[^{0 \\ -1}\\_{1 \\ 0}]$.\nHowever, according to the theory \u0026ldquo;eigenvector outcomes in the direction that it went in\u0026rdquo;, there seems to be no eigenvector intuitively, as any vector gets in will rotate 90 degree by this matrix.\nSolve its eigenvalues:\n$$ \\begin{aligned} | 𝐀 - λ𝐈 | = 0 \\\\\\ |\\begin{bmatrix} 0 \u0026 -1 \\\\\\ 1 \u0026 0 \\end{bmatrix} - \\begin{bmatrix} λ \u0026 0 \\\\\\ 0 \u0026 λ \\end{bmatrix}| = \\begin{vmatrix} -λ \u0026 -1 \\\\\\ 1 \u0026 -λ \\end{vmatrix} = 0 \\\\\\ λ^2 + 1 = 0 \\end{aligned} $$Two roots: λ₁=i, λ₂=-i\nAnti-symmetric matrix has imaginary eigenvalues which are always in pairs, as they\u0026rsquo;re complex conjugate. While a symmetrix matrix\u0026rsquo;s eigenvalues are all real numbers.\nSolve eigenvectors 𝐗:\nFor λ₁=i,\n$$ \\begin{aligned} (𝐀 - λ𝐈) 𝐗 = 0 \\\\\\ \\begin{bmatrix} 0-λ₁ \u0026 -1 \\\\\\ 1 \u0026 0-λ₁ \\end{bmatrix} \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} = 0 \\\\\\ \\begin{bmatrix} -i \u0026 -1 \\\\\\ 1 \u0026 -i \\end{bmatrix} \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} = 0 \\\\\\ \\end{aligned} $$Let the free variable b=1, then $[^a_b] = [^i_1]$\nFor λ₁= -i,\n$$ \\begin{aligned} \\begin{bmatrix} i \u0026 -1 \\\\\\ 1 \u0026 i \\end{bmatrix} \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} = 0 \\\\\\ \\end{aligned} $$Let the free variable b=1, then $[^a_b] = [^{-i}_1]$\nRepeated eigenvalues Eigenvalues of a triangular matrix is obvious: the diagonal, as the determinant is directly factorized.\nFor example, given matrix $𝐀 = [^{3 \\ 1}_{0 \\ 3}]$:\nSolve its eigenvalues:\n$$ \\begin{aligned} | 𝐀 - λ𝐈 | = 0 \\\\\\ \\begin{vmatrix} 3-λ \u0026 1 \\\\\\ 0 \u0026 3-λ \\end{vmatrix} = 0 \\\\\\ (3-λ)(3-λ) \\end{aligned} $$Two roots: λ₁=3, λ₂= 3\nSolve eigenvectors:\nFor λ₁=3,\n$$ \\begin{aligned} (𝐀 - λ𝐈) 𝐗 = 0 \\\\\\ \\begin{bmatrix} 3-λ₁ \u0026 1 \\\\\\ 0 \u0026 3-λ₁ \\end{bmatrix} \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} = 0 \\\\\\ \\begin{bmatrix} 0 \u0026 1 \\\\\\ 0 \u0026 0 \\end{bmatrix} \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} = 0 \\end{aligned} $$So $[^a_b]$ can be $[^1_0]$\nFor λ₂= 3, same formulas appear. There isn\u0026rsquo;t the 2nd (set of) independent eigenvectors.\nBut there are supposed to be two eigenvalues. It\u0026rsquo;s incomplete yet.\n(2024/06/22)\n视频 | 特征向量和特征值的应用 - 遇见数学\nZach-Youtube\n","date":"2023-11-06T13:51:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/eigenvalues_vectors/","title":"watch: LA - G.S. 21 | Eigenvalues \u0026 Eigenvectors"},{"content":"Feature image from: Understanding Diffusion Probabilistic Models (DPMs) | by Joseph Rocca - Medium ( Searched by diffusion model in DDG image )\nCollections References:\nmoatifbutt/awesome-diffusion-iclr-2025 - GitHub Surfaced when searching the paper of IC-Light in DDG Variational Diffusion Models ~ NIPS 2021\nArxiv\n(2023-11-04)\nSyncDreamer: Generating Multiview-consistent Images from a Single-view Image\nCode | ProjPage\nZero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model\nProjPage\n(2023-12-26)\nDM for single-image depth estimation NeRF HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion\n【Diffusion生成NeRF】TUM, Apple提出HyperDiffusion，用Diffusion计算神经场权重，统一框架下生成3D权重或4D动画\n(2023-07-16)\nUse DM to generate a NeRF.\nComment: \u0026ldquo;思路不如 shapE 宽。shapE的encoder不仅把3d assets压缩为MLP， 而且同时支持Nerf和DMTet的表征，在MLP上做diffusion还是conditional的。这篇文章相比起来还不太清楚卖点在哪\u0026rdquo;\nRCG Self-conditioned Image Generation via Generating Representations Code | brief\n(2023-12-30)\nThe distribution of image is learned by a pre-trained encoder, used as the condition for image generation.\nRepresentative Diffusion model: Sampling from the representation distribution\nPixel generater: convert samples to pixel\nFID (Frechet Inception Distance): 3.31, IS (Inception score): 253.4\n2D to 3D MVDD: Multi-View Depth Diffusion Models\nArxiv | Emergent\n(2023-12-31)\nUse DM to generate multi-view depth maps for point cloud generation.\n20K+ points. The number of valid points may no larger than the resolution of an image, because depth and geometry consistencies needs to be checked like the point cloud fusion performed in MVSNet.\nDepth map fusion\nEpipolar attention affects the denosing steps.\nEpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion\nCode | Emergent\n(2023-12-31) (可能是 美貌与智慧并重 他们做的，他在VAST?)\nDM conditioned by a single image for generating multi-view images.\nRestrict the frozen diffusion model with an epipolar cross-view attention\nReminds me MVDiffusion Generate 16 multi-view images in 12 seconds\nWhat is the resolution? What is the device? Adjusting feature maps to control image generation\nNo 3D geometry. I believe explicit structure is necessary for multi-view consistency especially in views with large-baselines. Text to 3D RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D\nEmergent\n(2024-01-05)\ngeneralizable Normal-Depth diffusion model, PBR Multi-view Cameras as Rays: Pose Estimation via Ray Diffusion ~ ICLR 2024 (Oral)\nProjPage | Code | CMU\n(2024-03-01)\nGenerate ray moments and ray directions by diffusion model. Control Illumination Editing Light Transport References:\nScaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport - OpenReview Surfaced by WeChat subscription: ICLR 惊现 10,10,10,10 满分论文，ControlNet 作者新作，Github 5.8k 颗星 - 机器之心 Style2Paints Research Lvmin Zhang (Lyumin Zhang) Training Model From Scratch:\n(2024-12-01)\nIC-Light r1-OpenReview\nRelated:\nPdf: Lvmin Zhang lllyasviel/IC-Light Reasons:\nThis paper draw my attention as it involves light transportation. Q\u0026amp;A:\nHow does this method combine with Light Transport?\nIs the training process similar to NeRF, which integrated differentiable rendering into the \u0026ldquo;pipeline to fulfill the task\u0026rdquo;, i.e., volumetric rendering.\nBonds:\n\u0026ldquo;in-the-wild data\u0026rdquo; reminds me NeRF-in-the-wild, which separates transient and consistant contents using two gates.\n\u0026ldquo;linear blending\u0026rdquo; of lighting effects under each single illumination condition.\nWeighted sum, which the NN is good at.\nI remember the word prompts to diffusion model have arithmatic characteristic, demonstrated in the short course of DLAI (Andrew Ng).\nDiffusion-baed illumination editing method\nLvmin commits himself to help artists r2-Paints. Ideas:\nInproper training constraints result in a \u0026ldquo;Structure-guided random image generator\u0026rdquo;.\nComplex illumination \u0026gt; Mixture of illumination \u0026gt; Approximated with $k$ diffusion model.\nQuestions:\nCan the Mixture of diffusion models be replaced with Gaussian mixture model?\nWhat are the similarity between the Mixture of diffusion models and Gaussian mixture model?\n","date":"2023-11-04T16:30:00Z","image":"https://miro.medium.com/v2/resize:fit:1358/0*hRF_dQv3uSGuS_IU.gif","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/diffusion/c-symp-misc/","title":"Sympo: Diffusion | Misc"},{"content":"vars() Create a class ParamGroup to store args into instance variables, then use vars(self) to read memebers from self.__dict__.\nAdd each argument into parser through parser.add_argument().\nSet shorthand to the initial character, e.g., --source_path use -s\nExample from gaussian-splatting:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class ParamGroup: def __init__(self, parser: ArgumentParser, name : str, fill_none = False): group = parser.add_argument_group(name) for key, value in vars(self).items(): shorthand = False if key.startswith(\u0026#34;_\u0026#34;): shorthand = True key = key[1:] t = type(value) value = value if not fill_none else None if shorthand: if t == bool: group.add_argument(\u0026#34;--\u0026#34; + key, (\u0026#34;-\u0026#34; + key[0:1]), default=value, action=\u0026#34;store_true\u0026#34;) else: group.add_argument(\u0026#34;--\u0026#34; + key, (\u0026#34;-\u0026#34; + key[0:1]), default=value, type=t) else: if t == bool: group.add_argument(\u0026#34;--\u0026#34; + key, default=value, action=\u0026#34;store_true\u0026#34;) else: group.add_argument(\u0026#34;--\u0026#34; + key, default=value, type=t) class ModelParams(ParamGroup): def __init__(self, parser, sentinel=False): self.sh_degree = 3 self._source_path = \u0026#34;\u0026#34; self._model_path = \u0026#34;\u0026#34; self._images = \u0026#34;images\u0026#34; self._resolution = -1 self._white_background = False self.data_device = \u0026#34;cuda\u0026#34; self.eval = False super().__init__(parser, \u0026#34;Loading Parameters\u0026#34;, sentinel) (2024-04-03)\nIn this way, there won\u0026rsquo;t be a long list of parser.add_argument() declaiming all arguments.\nInstead, related arguments are arranged into a group.\nCustomize Parsing (2023-09-27)\nAnalyse string manually\nRefer to Match-NeRF for an example.\nSpecify Args in Scripts (2024-07-19)\nSpecifying default values for arguments for debugging.\nInitialize a parser and then change it:\n1 2 3 4 5 6 7 8 9 10 11 parser = argparse.ArgumentParser(description=\u0026#34;A Func\u0026#34;) parser.add_argument(\u0026#34;-f\u0026#34;, \u0026#34;--filename\u0026#34;, default=\u0026#34;f.txt\u0026#34;, type=str, help=\u0026#34;File name.\u0026#34;) def main(): # . . . return if __name__ == \u0026#34;__main__\u0026#34;: # args = parser.parse_args() args, unknown = parser.parse_known_args() args.filename = \u0026#39;b.txt\u0026#39; The following examples all pass the args into the main function, like: eval(args)\nGNT/eval.py\nRepMLP/main_repmlp.py\nContext-Cluster/segmentation/tools/onnx2tensorrt.py\nContext-Cluster/segmentation/tools/torchserve/test_torchserve.py\n","date":"2023-11-04T13:40:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/python_args_parse/","title":"Memo: Python | Parse Input Args"},{"content":"From C to CUDA Source article: CUDA 01 | 第一个程序 - Master KangKang的文章 - 知乎\nCUDA extends Cpp to GPU.\nC lang: \u0026ldquo;add.c\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 #include\u0026lt;stdio.h\u0026gt; int add(int a, int b){ int c = a + b; return c; } int main(){ int c = add(2, 3); printf(\u0026#34;c = %d\\n\u0026#34;, c); return 0; } CUDA: \u0026ldquo;add.cu\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include\u0026lt;stdio.h\u0026gt; __global__ void add(int a, int b, int *c){ *c = a + b; } int main(){ int c; // Allocate Unified Memory - accessible from CPU or GPU int* c_cuda; // a memory for a int data cudaMalloc((void**)\u0026amp;c_cuda, 1*sizeof(int)); //return pointer // Launch kernel on 1 block containing 1 thread add\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(2, 3, c_cuda); // Transfer data between GPU VRAM and CPU RAM cudaMemcpy(\u0026amp;c, c_cuda, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;c=%d\\n\u0026#34;, c); // Free the allocated unified memory cudaFree(c_cuda); return 0; } Adapt C to CUDA (kernel function):\nThe function add is declared as a __global__ function, and then it becomes a kernel, which is called from CPU and executed on GPU.\nKernels have no return value, as results are left in memory.\nThus, the GPU memory for output data must be allocated (by cudamalloc) and passed into the kernel. And free it (with cudaFree) at the end. A kernel needs execution configuration: \u0026lt;\u0026lt;\u0026lt;blocks, threads, shared_mem, stream\u0026gt;\u0026gt;\u0026gt; to sequentialize the host code before host compilation. Docs\nCompile\n1 2 3 4 5 6 7 8 # For C project gcc add.c -o add_c # For CUDA project nvcc add.cu -o add_cuda # Profiler nvprof ./add_cuda Blocks \u0026amp; Threads References:\nTutorial 01: Say Hello to CUDA - CUDA Tutorial - Read the Docs\nTriple chevrons: How is the CUDA\u0026laquo;\u0026lt;\u0026hellip;\u0026raquo;\u0026gt;() kernel launch syntax implemented - SO\nNotes:\n(2025-02-21)\n并行编程就是管理线程运行哪些 kernel，如何索引到一个线程：三级：grid 》 block〉 thread (2023-11-03)\nCUDA locates a thread via blocks.\nNumber of threads in a block is a multiple of 32, e.g., 256 threads. An Even Easier Introduction to CUDA - Nvidia Blog - Mark Harris, Jan25, 2017\nTotal threads can be reshaped to 2D or 3D, and accordingly the kernel needs to modify the threads indexing.\nNumber of blocks must be larger than the total elements.\nGiven N elements, there needs n = (N + blkDim.x - 1)/blkDim.x\nd a t a 0 b l 1 k 1 2 3 b l 4 k 2 5 ⋯ ⋯ ⋯ b l k n - 1 N - 1 b l ⋯ ⋯ k ⋯ ⋯ ⋯ ⋮ ⋯ n ⋯ ⋯ - ⋯ ⋯ ⋮ ' Index of a thread is blockIdx.x * blockDim.x + threadIdx.x.\nAnd a grid includes all the threads = gridDim.x * blockDim.x. (grdiDim in the following figure may be wrong.)\nExample Source article: An Easy Introduction to CUDA C and C++ - Nvidia Blog - Mark Harris, Oct31, 2012\nEach thread handles a single element in an array. Glossary SM: Streaming Multiprocessor Docs\ndim3: An integer vector based on uint3. Programming Guide\nTo do\nGetting Started With CUDA for Python Programmers\ncudaMemcpy (2024-02-02)\nDoc: NVIDIA CUDA Library | Example from 3DGS\nSpecify dest and src pointers, number of bytes to be transferred, and direction of copy.\n1 2 3 4 5 int num_rendered; // Declare and Allocate memory on the host cudaMemcpy(\u0026amp;num_rendered, // points to destination area geomState.point_offsets + P - 1, // points to source area sizeof(int), // copy an signed int (4 bytes) cudaMemcpyDeviceToHost) // direction Shared Memory Docs\n(2024-02-05)\nShared by threads within a thread block. Synchronization Docs-#7.6\n__syncthreads is a \u0026ldquo;checkpoint\u0026rdquo; to wait all thread arrive at this point.\nNVIDIA CUDA Tutorial 8: Intro to Shared Memory -Ytb - Creel\n","date":"2023-11-03T12:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cuda/basic/","title":"memo: CUDA | Basics"},{"content":"Docs of Nsight: Getting Started with the CUDA Debugger\nDebug Demo (2023-11-03)\nEnvironment: Ubuntu 20.04, cuda-11.6 (in /usr/local/cuda-11.6), GPU 1050Ti.\nnvcc\n1 2 3 4 5 6 (base) yi@yi-Alienware:~/Downloads/CUDA_Study/Debug_CUDA$ nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_Mar__8_18:18:20_PST_2022 Cuda compilation tools, release 11.6, V11.6.124 Build cuda_11.6.r11.6/compiler.31057947_0 Prerequisite:\nInstall 2 extensions: Nsight and C/C++\nCreate 2 debugging configuration files: launch.json and tasks.json under \u0026ldquo;.vscode/\u0026rdquo;\nSelect debugger: CUDA C++ (CUDA-GDB) Example with nvcc Testing repo\n\u0026ldquo;test.cu\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(int argc, char **argv) { std::cout \u0026lt;\u0026lt; \u0026#34;Number of input arguments: \u0026#34; \u0026lt;\u0026lt; argc \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt;= argc-1; i++) { std::cout \u0026lt;\u0026lt; argv[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } return 0; } \u0026ldquo;launch.json\u0026rdquo; for debug configurations:\nSet program as the output binary program to be debugged:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;CUDA C++: Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/test.bin\u0026#34;, // binary file \u0026#34;preLaunchTask\u0026#34;: \u0026#34;mynvcc\u0026#34; }, // no need to change this: { \u0026#34;name\u0026#34;: \u0026#34;CUDA C++: Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34; } ] } \u0026ldquo;tasks.json\u0026rdquo; for building configurations:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;mynvcc\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;nvcc\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;${file}\u0026#34;, \u0026#34;-g\u0026#34;,\u0026#34;-G\u0026#34;, \u0026#34;-o\u0026#34;,\u0026#34;${fileDirname}/test.bin\u0026#34;, ] } ] } Error: /usr/bin/bash: nvcc: command not found\nAdd command on PATH before running startup scripts. tasks - VSCode - Docs\nTried add export PATH=\u0026quot;$PATH:/usr/local/cuda-12.3/bin\u0026quot; into \u0026ldquo;/etc/environment\u0026rdquo;, \u0026ldquo;/etc/profile\u0026rdquo;, \u0026ldquo;/etc/xprofile\u0026rdquo;, \u0026ldquo;/etc/bash.bashrc\u0026rdquo; all doesn\u0026rsquo;t work. How to permanently set $PATH on Linux/Unix -SO\nSolution: Set integrated terminal in user settings.json (My vscode version: 1.83.1, 2023-11-03)\n1 2 3 4 5 6 7 8 9 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { \u0026#34;bash\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;bash\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-i\u0026#34; ] } }, \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, Ref: VSCode tasks error: /bin/bash: npm: command not found (Found by DDG with searching \u0026ldquo;vscode debug tasks.json /usr/bin/bash: nvcc: command not found\u0026rdquo;)\nThen, with the \u0026ldquo;test.cu\u0026rdquo; file opening in the editor, click the start button to initiate debugging.\n(2024-01-26) I still don\u0026rsquo;t know how to include headers for libtorch in the CLI of nvcc. So, I didn\u0026rsquo;t manage to compile the 3DGS project with nvcc as above.\nPotentially useful:\nAn example: Include path problems for GPU library - SO Docs of nvcc: NVIDIA CUDA Compiler Driver NVCC (2024-01-27)\nBased on Troubles while compiling C++ program with PyTorch, HElib and OpenCV - SO, and reminded by perplexity, -Wl and -rpath are used in GCC for linking and specifying runtime library search path. In contrast, nvcc has -Xlinker for linking during compilation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Compile CUDA source files into object files with nvcc nvcc --compile -g -G -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ main_copy.cu \\ -o main.o # Link object files into an executable with g++ g++ main.o \\ -L/usr/local/libtorch/lib \\ -L/usr/local/cuda/lib64 \\ -Wl,-rpath,/usr/local/libtorch/lib \\ -ltorch -ltorch_cpu -lc10 -lcudart \\ -o my_executable Compiling is OK. But linking reports error:\n1 2 3 /usr/bin/ld: main.o: in function `main\u0026#39;: /home/yi/Downloads/debug_diff_rast/main_copy.cu:31: undefined reference to `RasterizeGaussiansCUDA(at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, float, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, float, float, int, int, at::Tensor const\u0026amp;, int, at::Tensor const\u0026amp;, bool, bool)\u0026#39; collect2: error: ld returned 1 exit status Not successful yet. Need to compile the library with nvcc.\nExample with Makefile Nvidia Tutorial Clip: Debugging CUDA kernels with VS Code\nMicrosoft VS CUDA Support in Visual Studio Code with Julia Reid\n(2024-01-26)\nUse cmake to produce Makefile, otherwise, error occurs: make *** no targets specified and no makefile found. stop\nAnd then edit the launch.json and tasks.json following this article: Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\nRefer to my repo for debugging 3DGS.\nExample with CMake Debugging CUDA kernels with VS Code\nRef\nSource articles: CUDA 番外篇 | Visual Studio Code的CUDA环境 - Master KangKang的文章 - 知乎 Adapted demo: vscode远程调试Linux CUDA程序- oushaojun2 - CSDN Debug Cuda Samples (2023-11-02)\nDownload sample project: NVIDIA/cuda-samples for 12.3\n1 git clone https://github.com/NVIDIA/cuda-samples.git Make 12.3 failed with 11.6\n1 2 cd ./cuda-samples make dbg=1 Error: /usr/bin/ld: cannot find -lglut\nNeed: sudo apt-get install freeglut3 freeglut3-dev\nError:\n1 2 3 4 5 6 /usr/bin/ld: simpleCUFFT_callback.o: in function `main\u0026#39;: /home/yi/Downloads/cuda-samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu:103: undefined reference to `cudaGetDeviceProperties_v2\u0026#39; collect2: error: ld returned 1 exit status make[1]: *** [Makefile:373: simpleCUFFT_callback] Error 1 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback\u0026#39; make: *** [Makefile:45: Samples/4_CUDA_Libraries/simpleCUFFT_callback/Makefile.ph_build] Error 2 cudaGetDeviceProperties_v2 is not existed in cuda 11.x, but appear in cuda 12.2. SO\ncuda-sample-11.6 Download zip: Release pkg 11.6; Git tag-11.6\nmake\n1 2 3 4 5 6 7 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/cuda-samples-11.6$ make dbg=1 make[1]: Entering directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/3_CUDA_Features/ptxjit\u0026#39; /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -g -G --threads 0 --std=c++11 -gencode arch=compute_35,code=compute_35 -o ptxjit.o -c ptxjit.cpp nvcc fatal : Unsupported gpu architecture \u0026#39;compute_35\u0026#39; make[1]: *** [Makefile:396: ptxjit.o] Error 1 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/3_CUDA_Features/ptxjit\u0026#39; make: *** [Makefile:45: Samples/3_CUDA_Features/ptxjit/Makefile.ph_build] Error 2 Devices with compute capacity (cc) 3.x have been dropped by cuda 12.x. Solution is removing the requests of compute_35 in the make file. Forum Nv\nCuda Toolkit is compatible the devices with lower cc than it supports. CUDA 11.x supports a maximum cc of 8.x. CSDN\n(2023-11-02) Remove cc of 35 and 37 SO:\nReplace all the pattern SMS ?= 35 37 with SMS ?= through VSCode. Replace all the pattern compute_35 with compute_61 Make failed:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -g -G --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o reduction_kernel.o -c reduction_kernel.cu reduction_kernel.cu(558): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(558): error: expected an identifier __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(558): warning #1835-D: attribute \u0026#34;__shared__\u0026#34; does not apply here __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ Remark: The warnings can be suppressed with \u0026#34;-diag-suppress \u0026lt;warning-number\u0026gt;\u0026#34; reduction_kernel.cu(558): error: expected a \u0026#34;;\u0026#34; __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(561): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name auto cta = cg::experimental::this_thread_block(scratch); ^ reduction_kernel.cu(561): error: identifier \u0026#34;scratch\u0026#34; is undefined auto cta = cg::experimental::this_thread_block(scratch); ^ reduction_kernel.cu(563): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name auto multiWarpTile = cg::experimental::tiled_partition\u0026lt;MultiWarpGroupSize\u0026gt;(cta); ^ 6 errors detected in the compilation of \u0026#34;reduction_kernel.cu\u0026#34;. make[1]: *** [Makefile:358: reduction_kernel.o] Error 255 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/2_Concepts_and_Techniques/reduction\u0026#39; make: *** [Makefile:45: Samples/2_Concepts_and_Techniques/reduction/Makefile.ph_build] Error 2 VSCode didn\u0026rsquo;t find header with red underlines:\n1 2 3 #include errors detected. Please update your includePath. Squiggles are disabled for this translation unit (/home/yi/Downloads/cuda-samples-11.6/Samples/2_Concepts_and_Techniques/reduction/reduction_kernel.cu).C/C++(1696) cannot open source file \u0026#34;cooperative_groups/reduce.h\u0026#34;C/C++(1696) Edit \u0026ldquo;c_cpp_properties.json\u0026rdquo; as:\n1 2 3 4 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/cuda-11.6/include\u0026#34; ], Didn\u0026rsquo;t solve. And the header is there and can be found.\n(2023-11-03) Intellisense erros for CUDA syntax.\nEnsure selecting lang as \u0026ldquo;CUDA C++\u0026rdquo; rather than \u0026ldquo;C++\u0026rdquo;. VSCode Nsight Intellisense not detecting functions and datatypes for *cu; *cpp works - NV forum\nRed underlines disappeared after the C/C++ extension got disabled and only \u0026ldquo;Nsigh\u0026rdquo; extension left. But these 2 extensions both are installed in every tutorial.\n(2023-11-03)No error in a folder contains only .cu files. Thus, Python code can\u0026rsquo;t co-exist with CUDA code?\n(2023-11-08) ✅ Refering the \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo; in the CUDA sample: \u0026ldquo;Samples/0_Introduction/matrixMul\u0026rdquo;, the compiler should be nvcc, not \u0026ldquo;/usr/bin/gcc\u0026rdquo;.\n1 \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/local/cuda/bin/nvcc\u0026#34;, Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\nSame error about reduction here issue#201. But he was 11.8.\nRe-install cuda toolkit 11.8 and test samples of 11.8.\nNote: the final line of the installing scripts provided on official site should be: sudo apt-get -y install cuda-11-8 instead of sudo apt-get -y install cuda\nError persists at reduction_kernel.cu.\nBuild with CMake (2023-11-17)\nThe library \u0026ldquo;diff-gaussian-rasterization-comments/cuda_rasterizer\u0026rdquo; is built according to CXX standard.\nHowever, I cannot step into the CUDA kernels when debugging.\nMaybe using nvcc to build can enable debugging.\nCreate a CMakeLists.txt using nvcc?\n","date":"2023-11-02T17:44:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/cuda/debug/","title":"memo: CUDA | Debugging"},{"content":"CosAnelWrmRst Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import torch from torch import nn from torch.utils.data import TensorDataset, DataLoader from matplotlib import pyplot as plt import numpy as np inps = torch.arange(50.).expand(10,-1).reshape(100, 5) tgts = torch.arange(50.).expand(10,-1).reshape(100, 5) dataset = TensorDataset(inps, tgts) loader = DataLoader(dataset, batch_size=1, pin_memory=True) model = nn.Sequential(nn.Linear(5,5)) criterion = torch.nn.MSELoss() optimizer = torch.optim.AdamW(model.parameters()) scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( optimizer, T_0=6, # First cycle is 6 epochs T_mult=2, # next cycle will be 2x epochs (int) eta_min=1e-5, # minimum lr last_epoch=-1, # set when resuming verbose=False) # print lr eps = np.arange(100) lrs = [] for idx, batch in enumerate(loader): x = batch[0] y = batch[1] y_pred = model(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step() lrs.append(scheduler.get_last_lr()) scheduler.step() plt.plot(eps, lrs) plt.title(\u0026#34;CosineAnnealingWarmRestarts\u0026#34;) scheduler.step(0) will set lr to the value at epoch 0. WarmUp + CosAnelWrmRst Ref: firstelfin/WarmUpLR\nThe original CosineAnnealingWarmRestarts doesn\u0026rsquo;t have warmup. WarmUpLR followed by CosineAnnealingWarmRestarts\n1 2 cosine = CosineAnnealingWarmRestarts(**param) warm_up_lr = WarmUpLR(cosine) The first 9 epochs use WarmUpLR, and the following use CosineAnnealingWarmRestarts.\nGallery A Visual Guide to Learning Rate Schedulers in PyTorch - Medium - Leonie Monigatti\n(2023-10-30)\nMax_lr Decay qu-gg/pytorch-cosine-annealing-with-decay-and-initial-warmup\nFound by github searching \u0026ldquo;CosineAnnealingWarmRestart\u0026rdquo;. Results\n","date":"2023-10-24T17:53:52Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-lr_sched/","title":"memo: PyTorch | LR \u0026 Scheduler"},{"content":"Code | Arxiv(2303) | ProjPage\nBriefs Author\u0026rsquo;s Talk Source Video: 【Talk | ICLR'23 Oral 美国东北大学马旭：图像亦是点集（Image as Set of Points）】\nInsights:\nUse clustering to extract image features.\nFusing cluster members through Collapse followed by Reconstruction\nPaper Explain Source Video: 【ICLR 2023】Image as Set of Points.计算机视觉新范式，利用聚类的思想实现图像建模。在多个下游任务上不输ViT和ConvNets】\nImage features are refined through multiple aggregation and dispatching using attention for pixels. Paper Notes Abstract Is the picture natively well-clustered?\nIf so, this method essentially is same as convolution, which extracts features by fusing pixels in the kernel.\nBut Cluster is discreate and sparse, can it capture high-level feature?\nClusters appeals to segmentation and interpretability.\nIntro Is this method designed for a single image?\nHowever, MVS can directly leverage epipolar lines on source views more concisely, without fusing neighbor pixels features..\nPixels with same color can imply fundamentally difference?\n\u0026ldquo;Similar pixels are grouped, but they are fundamentally different.\u0026rdquo; ?\nI think same color means rays hit same spatial geometry.\nMethod Pipeline:\nInitial feature of raw pixels are 5-D, including color (r,g,b) ∈ [0,255] and normalized pixel center (x,y) ∈ [0, 1]-0.5\nPixels are reduced to 𝑟 times at each stage\u0026rsquo;s start by fusing k (=4 or 9) neighbors around the 𝑟 evenly distributed anchors across the image, i.e., concatenating their channels and projecting to specified dimension by FC, unlike maxpooling.\nConv layer can perform points reduction as well if points\u0026rsquo; organization aligned with the raw image.\nIn their implementation, function PointReducer uses a conv layer instead of FC.\nFor model coc_base_dim64, before stage-0, image is reduced to 1/16 with kernels of size=4, i.e., 16 points are fused to 1. and 5-D features are mixed to 64-D.\n1 2 # Point Reducer is implemented by a layer of conv since it is mathmatically equal. nn.Conv2d(5, 64, kernel_size=4, stride=4, padding=0) Output points require reording for downstream pixel-wise tasks, like segmentation.\nCoC Block:\nEach stage repeats Context Cluster block several time. A blocks processes points set in 3 steps:\nPixels clustered $c$ groups in the feature space\nc centers are evenly distributed and form voronoi based on the cosine similarity of each point to each center feature.\nCenters are chose through nn.AdaptiveAvgPool2d((proposal_w, proposal_h)), which will set kernel and stride automatically for AvgPool2d.\nCenter feature is the average of k nearest neighbors, after placing the c centers.\nIntial features are 5-D including RGB and position (x,y).\nAggregation: Add features of all members to an aggregated feature g:\n$$g = \\frac{1}{C} (v_c + ∑_{i=1}^m sig(α s_i + β) * v_i )$$The aggregated feature g is computed by plusing center\u0026rsquo;s feature and the weighted sum of feature vectors of all m points vᵢ in a cluster, scaled by a tunable factor $sig(α sᵢ + β)$ ∈ (0,1), where s is similarity to the center feature and α,β are nn.parameters.\nThe denominator $C = 1+ ∑_{i=1}^m sig(α sᵢ+ β)$ aims to limit the magnitude.\nDispatching: Each member updates its feature from the aggregated feature, so as to fuse all other points and realize spatial interaction.\n$$p_i' = p_i + FC(sig (α sᵢ+ β) * g )$$The amount of g assigned to a member is determined by the adaptive similarity again, inversing the summation.\nCommunication between pixel in a cluster is like server-client in a centralized network.\nCenters\u0026rsquo;s positions are fixed for efficiency, so it emphasizes locality.\ndoubt: Advanced postional embedding could be applied.\ndoubt: Will different selection strategies affect model performance? They mentioned Farthest Point Sampling (FPS) mehtod in appx.D\nArchitecture:\nContext Cluster is a hierarchical model composed 4 stages and points are reduced to 1/4 (ie, h/2, w/2) after each stage.\nPlay Model can be comprehended by debugging the file \u0026ldquo;context_cluster\u0026rdquo;, using environment of \u0026ldquo;AIM\u0026rdquo;.\n\\begin{algorithm} \\begin{algorithmic} \\STATE PointReducer: Conv2d(x), downsample 16 times, 256 dim \\STATE Partition feature maps: rearrange(x) \\STATE Centers feature from x: AdaptiveAvgPool2d((2,2))(x) \\STATE Simlarity matrix: vectors' inner product with multi-head \\STATE Clustering: .scatter\\_ \\STATE Aggregate feature $g$: sum members' feat based on similarity \\STATE Dispatch $g$ to members \\STATE Reverse partition \\STATE Project to out\\_dim: Conv2d \\STATE FFN: Mlp, out\\_dim → hidden → out\\_dim \\end{algorithmic} \\end{algorithm} Similarity and points\u0026rsquo; features are optimized separately:\nSimilarity: x → center → sim\nFeatures: x → value → val_center → aggregated feature out\nsim and out are decoupled.\n","date":"2023-10-15T13:40:00Z","image":"https://pic2.zhimg.com/80/v2-2aac08c8a30f4726f3eb32b608a95589_720w.webp","permalink":"http://blog.zichen.uk/post/writenotes/model/misc/b-note-image_as_points/","title":"read: Image as Set of Points"},{"content":" (Discussed in QQ group 706949479) Code | Arxiv | ProjPage\nAuthor\u0026rsquo;s blog: ICCV 2023 NeRF提点的Magic Loss 即插即用 —— S3IM随机结构相似性 - Summer Clover的文章 - 知乎 Notes Abs Previous NeRF didn\u0026rsquo;t utilize structural information on image level, but train and predict point-wise.\nMethod = 3 1 = 6 1 9 = 2 3 = = 2 5 = 3 2 S 5 - 4 = 3 - 1 3 ‖ ‖ ‖ ‖ ‖ ‖ ‖ ⬇ I - = - M 8 - 0 = 9 - 4 ₁ 7 - = 1 - = = 2 7 = 4 6 0 = 6 8 R e o r d e r = 9 7 = 6 2 1 = 8 0 = = 0 4 = 4 8 S - 6 = - 7 3 ‖ ‖ ‖ ‖ ‖ ‖ ‖ ⬇ I - = - M 2 - 2 = 6 - ₂ 1 - 5 = 2 - 3 = = 5 3 = 1 1 4 3 = 9 3 R e o r d e r Steps:\nApply SSIM on the randomly selected training pixel patch with a kernel size $K$ (=2) and stride size S (=K).\nRepeatedly reorder the predicted and target pixel patchs, and calculate S3IM multiple ($M$=10) times.\nThe final loss term is the average of them multiplied with a weight factor (hyperparameter) $λ$.\n$$ \\rm L_{S3IM} = λ ⋅ (1 - \\frac{1}{M} \\sum_{m=1}^M SSIM(Patch_{rendered}, Patch_{target}) ) $$ Compare with SSIM:\nS3IM applied on random pixel patches significantly outperforms SSIM applied on local continuous patches.\nThe authors explain this as the SSIM can only capture the local similarity, whereas S3IM can compare the nonlocal structural similarity over all training images.\nTraining NeRF with local continuous patches will hurt the performance (as stated at the end of section 3.1).\nPlay Code\n1 2 3 4 5 6 7 8 9 s3im_func = S3IM(kernel_size=args.s3im_kernel, stride=args.s3im_stride, repeat_time=args.s3im_repeat_time, patch_height=args.s3im_patch_height, patch_width=args.s3im_patch_width).cuda() if args.s3im_weight \u0026gt; 0: s3im_pp = args.s3im_weight * s3im_func(rgb_map, rgb_train) total_loss += s3im_pp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class S3IM(torch.nn.Module): r\u0026#34;\u0026#34;\u0026#34;Implements Stochastic Structural SIMilarity(S3IM) algorithm. Arguments: kernel_size (int): kernel size in ssim\u0026#39;s convolution(default: 4) stride (int): stride in ssim\u0026#39;s convolution(default: 4) repeat_time (int): repeat time in re-shuffle virtual patch(default: 10) patch_height (height): height of virtual patch(default: 64) patch_width (height): width of virtual patch(default: 64) \u0026#34;\u0026#34;\u0026#34; def __init__(self, kernel_size=4, stride=4, repeat_time=10, patch_height=64, patch_width=64): super(S3IM, self).__init__() self.kernel_size = kernel_size self.stride = stride self.repeat_time = repeat_time self.patch_height = patch_height self.patch_width = patch_width self.ssim_loss = SSIM(window_size=self.kernel_size, stride=self.stride) def forward(self, src_vec, tar_vec): r\u0026#34;\u0026#34;\u0026#34; src_vec: (ray_batch_size=4096=64*64, 3) \u0026#34;\u0026#34;\u0026#34; loss = 0.0 index_list = [] for i in range(self.repeat_time): if i == 0: tmp_index = torch.arange(len(tar_vec)) # (4096) index_list.append(tmp_index) else: ran_idx = torch.randperm(len(tar_vec)) index_list.append(ran_idx) res_index = torch.cat(index_list) # (M * ray_bs = 10*4096) tar_all = tar_vec[res_index] # (10*4096, 3) src_all = src_vec[res_index] tar_patch = tar_all.permute(1, 0).reshape(1, 3, self.patch_height, self.patch_width * self.repeat_time) src_patch = src_all.permute(1, 0).reshape(1, 3, self.patch_height, self.patch_width * self.repeat_time) loss = (1 - self.ssim_loss(src_patch, tar_patch)) return loss ","date":"2023-10-14T10:30:00Z","image":"https://miro.medium.com/v2/resize:fit:1400/1*SBVkh54RJZrMsozG-vQdCQ.png","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-s3im/","title":"read: NVS - NeRF | S3IM Loss for NeRF"},{"content":"(2023-10-15)\nTalk-221121 Source video: 丁霄汉：结构重参数化是怎么来的【深度学习】【直播回放】-bilibili\nRepVGG: (2021)\nVGG has bad performance with fast inference because single stream can run in parallel efficientlly.\nAnd multiple branches means multiple sets of parameters, which help achieve better precision.\nIf a set of parameters can be transformed equivalently to another set of parameters, the corresponding structure would changed naturally.\nTherefore, the multi-branches architecture during training can be transformed to a single branch model in inference period.\nMethodology: Kernel size can vary while computation remains, e.g., a 1×1 kernel can be reshaped to 3×3\nThus, 3 branches with a 3×3 kernel, a 1×1 kernel, and a 3×3 identity kernel can be added to a single 3x3 kernel based on the property of linearity of convolution: $x * K_a + x * K_b = x * (K_a+K_b)$ Centripetal SGD: (2017)\nInception: Compressing (pretrained?) models by pruning redundent channels in feature maps. To create identical channels, let optimizer (SGD) guide some channels to become similar. Two same channels merged to a comprehensive channel, model gets concise while performance unchanged. Linear Redundancy Unit (Obsolete)\nMerge 2 feature maps: Training with two 3×3 kernels and merge the 2 kernels after training. This method brought marginal improvement though.\nThis indicates that two models with the same final structure, but experienced different training processes in different architectures, have different performances.\nAsymmetic Convolution Block (2019)\nBranches are different: 3×3 + 1×3 + 3×1 Research on Simple Models: (2020)\nUse identity branch to eliminate some shortcuts in ResNet.\nHow to make a ultimate simple yet powerful model without shortcut? ▶ RepVGG Multiple branches like InceptionNet in just a single kernel.\nWhy can it work in an arbitrary model? ▶ Diverse Branch Block (DBB) RepMLP: (2022)\nInject locality into MLP (CNN is a special MLP) by transforming an arbitrary conv kernel to a FC kerenel. RepLKNet: (2022)\nLarge kernel: 31x31 + 5x5 Misc:\nResRep for channel pruning.\nOutput channels can be controled through a 1x1 kernel after the original 3x3 kernel. Such that channel pruning can be performed on the 1x1 kernel. RepOptimizer: generalize to gradient reparameterization for fast training.\nIncorperating the prior knowledge (inductive bias) into optimizer instead of model structure. RepVGGplus: principles behind RepVGG Ideas moving forward:\nConnect Structural Rep with every element in a general vision model:\nTopology (RepVGG), Component (ACNet, DBB), Width (ResRep), Globality v.s. locality (RepMLP), Kernel size (RepLKNet), Optimizer (RepOptimizer)\nRethink classical problems.\nSimple model, like VGG, doesn\u0026rsquo;t work? (RepVGG) Can\u0026rsquo;t train a super deep model without shortcut? (RepVGGplus) Inception Net is too complex to be abandoned? (DBB) MLP can\u0026rsquo;t handle image tasks? (RepMLP) Large kernels are less effective? (RepLKNet) Related works:\nNon-deep Network; RepNAS, YOLO v6\u0026amp;v7, DyRep, Scaling up Kernels in 3D GNNs, RepUNet, RepSR (superres), De-IReps. Talk-220426 Source video: 【论文连讲：用重参数化赋予MLP网络局部性、超大卷积核架构【CVPR2022】【基础模型】】- bilibili\n(2023-10-16)\nRepMLPNet \u0026ldquo;一种采用重参数化技术引入局部性的分层 MLP 网络\u0026rdquo;\nCode | Arxiv MLP has no locality, only global capacity, thus it\u0026rsquo;s not favorable to do linear projection on 2D images.\nLocality means the surrounding pixels of a input pixel should have larger contributions due to stronger correlation compared to distant pixels.\nHowever, MLP treats all pixels on the image equally without considering relative positions, resulting in that MLP is difficult to converge for images data due to high dimensionailty and individually training for each pixel.\nCNN perserves this inductive bias through kernels. But CNN doesn\u0026rsquo;t have long-range dependencies because different regions share the same parameters: the kernel. Thus, a CNN stacks multiple layers for a large receptive field. In contrast, MLP is a function of positions, sensitive to location.\nHence, one approach to inject locality is by creating parallel branches with various conv kernels (for different dimensions) alongside the fc layer.\nBy supplementing conv kernels, the model is competent both at long-range dependency and locality for 2D images.\nThe side effect is the mutiple disunified branches will hinder computation parallelism, and impair the inference efficiency finally.\nThe solution to maintain the inference efficiency and perserve conv branches is Structural Reparameterization:\nMerging multiple auxiliary branches to a single FC stream can be realized by transforming their parameters after training into one FC kernel, such that the inference speed and precision are unchanged.\n通过参数的等价转换实现结构的等价转换。\nGeneric CNNs with conv kernels include massive parameters. And multiple branch of conv kernels may be unfeasible if without reducing parameters.\nThere are three branches: FC3, 3x3 \u0026amp; 1x1 kernels, and \u0026ldquo;Identity\u0026rdquo; Identity branch performs FC1+FC2 after maxpooling shrinks (H,W) to only (1,1).\nThus, the FC layer only need 1 parameter. Plusing 4 parameters in BatchNorm (mean, std, scale factor, bias), this branch only has 5 parameters.\nThis branch functions like a SE block (Squeeze-Excitation) providing channel-wise \u0026ldquo;overall scaling\u0026rdquo;.\n3x3 and 1x1 conv layer perform \u0026ldquo;set-sharing\u0026rdquo; (depth-wise conv + group convolution), where total of C channels are split to S groups.\nC S c g h r n o l u s p s 0 1 2 3 4 5 6 7 8 9 1 0 1 1 Then the number of parameters in a conv layer reduced from (C×H×W)² to S×(H×W)².\nThe main branch performs FC3 after depth-wise convolution for input feature maps.\nThe equivalent FC layer for a conv layer is required for adding conv layers to FC layer.\nFC kernel is the 2D weight matrix $W_{dₒ×dᵢ}$ in a linear layer.\nA 3D Conv kernel is a special FC kernel represented as a Toeplitz matrix, containing lots of shared parameters, so its associated FC kernel must exist.\nThen, 2 FC kernels can add up directly based on linearity.\nA FC layer processes a feature map through 4 steps: (n,c,h,w) ➔ (n, c×h×w) ➔ FC kernel ➔ (n, o×h×w) ➔ (n,o,h,w), denoted as: $\\rm MMUL(featmap, W_{dₒ×dᵢ})$, where dᵢ = c×h×w.\nA Conv layer with a 3D conv kernel $F$ and padding $p$ processes the feature map is denoted as $\\rm CONV(featmap, F,p)$\nThus, the problem is how to convert a 3D kernel to a 2D kernel.\nGiven the corresponding FC kernel of a conv kernel $W^{(F,p)}$, two operations are equivalent: $\\rm MMUL(featmap, W^{(F,p)}) = CONV(featmap,F,p)$\nConsidering a linear layer, it projects vectors: $\\rm V_{n×dₒ} = V_{n×dᵢ} ⋅W^{(F,q)\\ T}$\nInsert an identity matrix I:\n$$ V_{n×dₒ} = V_{n×dᵢ} ⋅I ⋅ W^{(F,q)\\ T} = V_{n, dᵢ} ⋅(I_{dᵢ×dᵢ} ⋅ W^{(F,q)\\ T}) $$Then, the term $(I⋅W^{(F,q)\\ T})$ can be regarded as a convolution operation.\nA conv operation must be a Mat-Mul, but a Mat-Mul may not be a conv operation.\nWhat kind of Mat-Mul (FC layer) is a conv operation? It\u0026rsquo;s when the weight matrix is a Toeplitz matrix transformed from a conv kernel.\nBecause $W^{(F,p)}$ is transformed indeed from conv kernel, the Mat-Mul $\\rm I⋅W^{(F,p)}$ is a convolution operation for sure.\n$$\\rm I_{dᵢ×dᵢ}⋅W^{(F,p)} ⇔ CONV(F,p,featmap)$$In the convolution $I_{dᵢ×dᵢ}⋅W^{(F,p)}$, $I_{dᵢ×dᵢ}$ is convoled. Thus, it\u0026rsquo;s supposed to be the featmap in CONV(). i.e., the $I_{dᵢ×dᵢ}$ is reshaped from featmap $I_{(c×h×w, c, h, w)}$\nAdditional reshaping is needed to match the dimensionality:\n$$\\rm I_{dᵢ×dᵢ}⋅W^{(F,p)} = CONV(F,p,featmap).reshape(chw, c, h, w)$$ From the above equation, the desired FC kernel $W^{(F,p)}$ is the result feature map of convolving the kernel F with a blank featmap:\n$$\\rm W^{(F,p)} = CONV(F,p,I_{(c×h×w, c, h, w)}).reshape(chw, c, h, w)$$For example, if the conv kernel F is (c, o, (3,3)), then the corresponding FC kernel $W^{(F,p)}$ has shape: (o, h-3+2×p+1, w-3+2×p+1) = (c×h×w, o,h,w).\nThis \u0026ldquo;3D FC kernel\u0026rdquo; has finished the \u0026ldquo;sum\u0026rdquo; computation and gets waiting for Mat-Mul with the input feature maps.\nTo align with the squashed 2D input feature maps (n, c×h×w), it needs to be reshaped to 2D: (c×h×w, o×h×w).\nFinally, a 3D conv kernel becomes a 2D kernel.\nThe equivalent FC kernel of a conv kernel is the result of convolution on an identity matrix with proper reshaping.\nFuse the parameters (μ,σ,γ,β) of BatchNorm into convolution layer based on linearity.\n$$M' = γ⋅[(MF -μ)/σ] + β = γ⋅(MF)/σ + (β - γ⋅μ/σ)$$So new kernel and bias: $F' = γ⋅F/σ, \\quad b' = (β - γ⋅μ/σ)$\nAfter that, bias-added conv kernels are converted to 2D kernels, which can be added up the main stream: FC3 kernel for inference with only MLP layers.\nResMLP-Net\nHierarchical design mimic popular vision models\nRepMLPBlock and FFN alternate.\nCan be used as the backbone for downstream tasks.\nAdjust the amount of parameters in each stage through \u0026ldquo;set-sharing\u0026rdquo;.\nNo need for large datasets (JFT300M) or many epochs (300~400) to train. (IN for 100 epochs).\nThroughput is higher than conventional CNN models. Speed has not much relation with the number of FLOPs.\nRepMLP is suitable for highly parallelized devices (GPU) rather than devices with lower computation capacity, like mobile.\n\u0026ldquo;Identity\u0026rdquo; branch is necessary for the performance with providing information in different scale and dimensions.\n\u0026ldquo;set-sharing\u0026rdquo; increase the number of groups will bring precision.\nLocality can be observed on the feature maps.\nRepMLPNet is robust for discontinuity between split patches from big images.\nThe resolution of Cityscapes dataset doesn\u0026rsquo;t match the pretrained model. They devided an entire image to small patches.\n1 2 3 4 5 class RepMLPNet: RepMLPNetUnit RepMLPBlock RepMLPBlock cannot resume training after model.locality_injection() because sub-modules have been deleted. Therefore, .locality_injection should be called with a new model before inference.\nBlog-210426 Source: 结构重参数化：利用参数转换解耦训练和推理结构 - 丁霄汉的文章 - 知乎\nBlog-210517 解读模型压缩6：结构重参数化技术：进可暴力提性能，退可无损做压缩 - 科技猛兽的文章 - 知乎\n矩阵乘法可以看做卷积：一个 2D 数据矩阵乘以 $W^{(F,p)}$，相当于这个数据矩阵先 reshape 成 4D 的 feature map 做卷积，结果再 reshape 成 2D. Papers FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization\nCode\n","date":"2023-10-13T20:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/misc/c-symp-struct_reparam/","title":"sympo: Structural Reparameterization"},{"content":"watch: CppExt - AI葵 01 | Cpp Bridges PyTorch \u0026amp; CUDA\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 1 - English CC -\nCode Instructions The pure purpose of CUDA extensions is to make PyTorch programs faster.\nCUDA extensions are more efficient than PyTorch in two scenarios:\nProcedures can\u0026rsquo;t be executed in parallel, e.g., each ray has different numbers of points.\nMany sequential computations, like a nn.Sequential module including lots of conv layers. C++ can fuse multiple layers to a single function.\nRelations: PyTorch will call a C++ function, which will call the CUDA extension.\nP y T o r c h \" B C r p i p d g e \" C U D A Environment conda create -n cppcuda python=3.8\nLatest PyTorch: conda install pytorch==1.12.1 cudatoolkit=10.2 -c pytorch\nVersion of the (compiled) PyTorch needs to match the local CUDA version (checked by nvcc -V).\nUpgrade pip for building cpp programs: python -m pip install pip -U\nPybind11 The code: \u0026ldquo;interpolation.cpp\u0026rdquo; acts like the main function that calls the C++ function, and python will call the \u0026ldquo;main\u0026rdquo; function. The \u0026ldquo;main\u0026rdquo; function receives input tensors from PyTorch and return output tensors from CUDA code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Declare PyTorch #include \u0026lt;torch/extension.h\u0026gt; // Define starts with the type of return values torch::Tensor trilinear_interpolate( torch::Tensor features, // 8 corners torch::Tensor point // target point coord. No comma at the end ){ return features; } // API for Python PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ // Function name in python and the cpp function m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); } (2023-10-18) Didn\u0026rsquo;t update the includePath for PyTorch as follows because I didn\u0026rsquo;t find the entry \u0026ldquo;C/C++: Edit Configurations (JSON)\u0026rdquo; after pressing F1. It seems like VSCode finds PyTorch automatically.\n1 2 3 4 5 6 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], (2023-10-27) However, error intellisense occurs after I installed the \u0026lsquo;C/C++ Extension Pack\u0026rsquo; for VSCode. So setting includePath is necessary.\npybind11 connects Python and C++11 codes.\n1 2 pip install pybind11 pip install ninja Pip compile Build the cpp codes to a python package.\nCreate a \u0026ldquo;setup.py\u0026rdquo; for building settings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CppExtension setup( name=\u0026#34;my_cppcuda_pkg\u0026#34;, # python package name version=\u0026#34;0.1\u0026#34;, description=\u0026#34;cppcuda example\u0026#34;, long_description=\u0026#34;cpp-cuda extension\u0026#34;, author=\u0026#34;z\u0026#34;, author_email=\u0026#34;luckily1640@gmail.com\u0026#34;, ext_modules=[ CppExtension( name=\u0026#39;my_cppcuda_pkg\u0026#39;, sources=[\u0026#34;interpolation.cpp\u0026#34;,]) # code files ], cmdclass={ # commands to be executed \u0026#34;build_ext\u0026#34;:BuildExtension } ) Build and install the package:\n1 2 3 4 pip install . # setup.py path is cwd (since pip 21.3) # Or adding an arg to avoid the deprecation warning: pip install . --use-feature=in-tree-build (2024-03-06) Pybind11 module also can be compiled with cmake: 如何在Python中调用C++代码？pybind11极简教程 - HexUp\nPyTorch Call \u0026ldquo;test.py\u0026rdquo; will call the cpp program.\nPackage torch must to be imported before the cuda extensions. 1 2 3 4 5 6 7 8 9 10 import torch import my_cppcuda_pkg features = torch.ones(8,1) point = torch.zeros(1,2) out = my_cppcuda_pkg.trilinear_interpolate(features, point) print(out) title: \u0026ldquo;watch: CppExt - AI葵 02 | Kernel Function\u0026rdquo; date: 2023-10-23T20:20:00\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 2 - English CC -\nDocs: CUDA C++ Programming Guide\nGPU Parallsiam Kernel → Grid → Block → Thread\nC P K n U e e r l d a t a ⋮ G P G U r i B d l T o c h k r 0 e a d B l T o c h k r 1 e a d ⋅ ⋅ ⋅ A thread is the smallest computation unit that executes element arithmatic independently.\nThe number of threads in a block is limited up to 1024. To multiply the amount of threads, many Block are placed together in a Grid. Docs\nThe number of Blocks can be $(2^{31}-1) × 2^{16} × 2^{16}$\nIntroducetion to GPUs - NYU\nTrilinear Interpolate Each corner is sumed up with a weight which is the product of normalized distance from the point to the opposite side.\nAnalogy to Bilinear interpolation:\nf f ₁ ₃ = = = = = u = = 1 = - 1 v = v ‖ 1 - u f 1 f ₂ ₄ $$\\rm f(u,v) = (1-u)(1-v)⋅f₁ + u(1-v)⋅f₂ + (1-u)v⋅f₃ +uv⋅f₄$$ For Trilinear interpolation, each weight is the product of 3 normalized distances to the opposite plane.\n$$ \\begin{aligned} \\rm f(u,v,w) =\u0026 (1-u)(1-v)(1-w)f₁ + u(1-v)(1-w)f₂ + (1-u)v(1-w)f₃ + uv(1-w)f₄ \\\\\\ \u0026+ (1-u)(1-v)w f₅ + u(1-v)w f₆ + (1-u)vw f₇ + uvwf₈ \\\\\\ \u0026 \\\\\\ =\u0026\\rm (1-u) [ (1-v)(1-w)f₁ + v(1-w)f₃ +(1-v)wf₅ +vw f₇ ] \\\\\\ \u0026\\rm + u [ (1-v)(1-w)f₂ + v(1-w)f₄ + (1-v)w f₆ + vwf₈] \\end{aligned} $$ f f ₁ ₃ f f ₅ ₇ u w V f f ₄ ₂ f f ₆ ₈ Input-Output Input: features (N, 8, F) and points coordinates in each cube (N, 3)\nOutput: features at points (N, F).\nOperations can be performed in parallel\nEach point can be computed individually; Each feature can be computed individually. Code Notes:\nIf input variables of CUDA kernel are torch.Tensor, they must be checked whether they\u0026rsquo;re on cuda and contiguous, because threads needs to read/write data without jumping.\nWhile if input variables are not tensor, the checking is not required.\ncpp Cpp: \u0026ldquo;trilinear_interpolate.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;torch/extension.h\u0026gt; #include \u0026#34;utils.h\u0026#34; torch::Tensor trilinear_interpolate( const torch::Tensor features, const torch::Tensor points ){ // Check input tensors for building successfully CHECK_INPUT(features); CHECK_INPUT(points); // Call the cuda kernel return trilinear_fw_cu(features, points); } PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); } header Header: \u0026ldquo;include/utils.h\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;torch/extension.h\u0026gt; // \u0026#34;one-line functions\u0026#34; // Any tensor must reside on cuda device. #define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \u0026#34; must be a CUDA tensor\u0026#34;) // Next element in x corresponds 1 step for R/W head, // thus, a multi-dim tensor is indexed like a flatten tensor. // Workers are contiguous, so tensor must be as well. #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \u0026#34; must be contiguous\u0026#34;) // Combine two conditions: #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x) // Declear the cuda kernel torch::Tensor trilinear_fw_cu( const torch::Tensor feats, const torch::Tensor points ); cu CUDA kernel: \u0026ldquo;interpolation_kernel.cu\u0026rdquo;\nSource video: part-3 Source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 #include \u0026lt;torch/extension.h\u0026gt; // kernel function template \u0026lt;typename scalar_t\u0026gt; // for type of scalar_t __global__ void trilinear_fw_kernel( // no return value // input variables are packed_accessor const torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; feats, const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; points, torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; feat_interp ){ // index thread along x for samples: const int n = blockIdx.x * blockDim.x + threadIdx.x; // index thread along y for features: const int f = blockIdx.y * blockDim.y + threadIdx.y; // Terminate exceeded threads without input data if (n \u0026gt;= feats.size(0) || f \u0026gt;= feats.size(2)) return; // Put results into output variable // normalized coordinates in each cell, [-1,1] -\u0026gt; [0,1] const scalar_t u = (points[n][0]+1)/2; const scalar_t v = (points[n][1]+1)/2; const scalar_t w = (points[n][2]+1)/2; // factors const scalar_t a = (1-v)*(1-w); const scalar_t b = v*(1-w); const scalar_t c = (1-v)*w; const scalar_t d = v*w; // Each thread will perform: feat_interp[n][f] = (1-u) * (a*feats[n][0][f] + b*feats[n][1][f] + c*feats[n][2][f] + d*feats[n][3][f]) + u * (a*feats[n][4][f] + b*feats[n][5][f] + c*feats[n][6][f] + d*feats[n][7][f]); } // foward pass torch::Tensor trilinear_fw_cu( torch::Tensor feats, // (N=20, 8, F=10) torch::Tensor points // (N=20, 3) ){ const int N = points.size(0); const int F = feats.size(2); // Initialize the output data residing on the same devices // as the input data torch::Tensor feat_interp=torch::empty({N,F}, feats.options()); // Allocate threads and blocks // #Threads per block: 256 (Rule of thumb). // Threads can be 3-D (cube) at most, where each dim can be set as proportional as the data\u0026#39;s shape. // Two dimensions will run in parallel: N (20) and F (10) const dim3 threads(16, 16, 1); // total 256. // #Blocks is determined by repeating `threads` to sufficiently cover the output data. const dim3 blocks( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ); // Launch threads to compute for each \u0026#34;voxel\u0026#34; in the \u0026#34;cube\u0026#34; of block AT_DISPATCH_FLOATING_TYPES(feats.type(), \u0026#34;trilinear_fw_cu\u0026#34;, ([\u0026amp;] { // call kernel function with passing input and output trilinear_fw_kernel\u0026lt;scalar_t\u0026gt;\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;( feats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;(), points.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), feat_interp.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;() ); } ) ); return feat_interp; } Since 2 dimensions of the output tensor both require parallism, 256 threads in a block are resized to a square of (16, 16).\nTo ensure each element of the output tensor assigned with a thread, 2 by 1 (2,1) blocks are required.\nSuch that each element will be computed by a thread (\u0026ldquo;box\u0026rdquo;) individually.\n2 - 0 ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' = = □ □ □ □ □ = □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ = 1 = □ □ □ □ □ = □ □ □ □ □ = 0 = □ □ □ □ □ = □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ = 1 = □ □ □ □ □ = □ □ □ □ □ = 6 = □ □ □ □ □ = □ □ □ □ □ = □ □ □ = = □ □ □ □ □ = □ □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ □ = . ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' 1 6 I ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' s = o = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = l = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = a T = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = t h = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = e r = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = e = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = 1 U a = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = 6 n d = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = u s ⊠ ⊠ ⊠ = s = ⊠ ⊠ ⊠ ⊠ ⊠ = ⊠ ⊠ ⊠ ⊠ ⊠ ⊠ = e = ⊠ ⊠ ⊠ ⊠ ⊠ = ⊠ ⊠ ⊠ ⊠ ⊠ ⊠ = d = ⊠ ⊠ ⊠ ⊠ ⊠ = ⊠ ⊠ ⊠ ⊠ ⊠ ⊠ = . ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' 1 6 Notes:\nthreads and blocks are not assigned with tuples:\n1 2 const dim3 threads = (16, 16, 1); // total 256. const dim3 blocks = ( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ) They\u0026rsquo;re object instantiated from classes:\n1 2 const dim3 threads(16, 16, 1); // total 256. const dim3 blocks( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ) If multiple tensors need return, the return type of the func should be std::vector\u0026lt;torch::Tensor\u0026gt;. And the end syntax: return {feat_interp, points};\npytorch.org/cppdocs/ Docs: CUSTOM C++ AND CUDA EXTENSIONS Kernel func Source video: P4\nAT_DISPATCH_FLOATING_TYPES got passed data type and a name for error prompt.\nscalar_t is used to allow various float types of input data to kernel function trilinear_fw_kernel, as AT_DISPATCH_FLOATING_TYPES can recieve float16, float32, float64.\nSpecify sepcific dtype rather than scalar_t and size_t:\n1 2 3 4 5 6 trilinear_fw_kernel\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;( feats.packed_accessor\u0026lt;float, 3, torch::RestrictPtrTraits\u0026gt;(), points.packed_accessor\u0026lt;float, 2, torch::RestrictPtrTraits\u0026gt;(), feat_interp.packed_accessor\u0026lt;float, 2, torch::RestrictPtrTraits\u0026gt;(), var_not_tensor // packed_accessor is only for tensor ) packed_accesor indicates how to index elements by stating \u0026ldquo;datatype\u0026rdquo; (scalar_t) and \u0026ldquo;number of dimensions\u0026rdquo; (3) for each input. And size_t means shape of an index aligned with scalar_t.\ntorch::RestrictPtrTraits: Memory is independent to any other variables.\nKernel trilinear_fw_kernel doesn\u0026rsquo;t return any value (void), with directly changing the memory of output data. Thus, output must be passed.\n__global__ means kernel function is called on cpu and excecuted on cuda devices.\n__host__ for functions called on cpu and run on cpu. __device for functions called and run both on cuda device. Indexing samples by n and indexing features by f.\nIf threads accessed empty area, program returns.\nsetup.py Building CudaExtension: \u0026ldquo;setup.py\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from setuptools import setup from torch.utils.cpp_extension import CUDAExtension, BuildExtension from pathlib import Path ROOT_DIR = Path.cwd() exts = [\u0026#34;.cpp\u0026#34;, \u0026#34;.cu\u0026#34;] sources = [str(p) for p in ROOT_DIR.rglob(\u0026#39;*\u0026#39;) if p.suffix in exts] include_dirs = [ROOT_DIR / \u0026#34;include\u0026#34;] setup( name=\u0026#34;my_cppcuda_pkg\u0026#34;, version=\u0026#34;0.1\u0026#34;, description=\u0026#34;cppcuda example\u0026#34;, long_description=\u0026#34;cpp-cuda extension\u0026#34;, author=\u0026#34;z\u0026#34;, author_email=\u0026#34;luckily1640@gmail.com\u0026#34;, ext_modules=[ CUDAExtension( name=\u0026#39;my_cppcuda_pkg\u0026#39;, sources=sources, # code files include_dirs=include_dirs, extra_compile_args={\u0026#39;cxx\u0026#39;: [\u0026#39;-O2\u0026#39;], \u0026#39;nvcc\u0026#39;: [\u0026#39;-O2\u0026#39;]} ) ], cmdclass={ # commands to be executed \u0026#34;build_ext\u0026#34;:BuildExtension } ) Build and install: pip install . Delete failed building history manually: \u0026ldquo;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/my_cppcuda_pkg-0.1.dist-info\u0026rdquo; test.py Python function: \u0026ldquo;test.py\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 import torch from my_cppcuda_pkg import trilinear_interpolate N = 65536; F = 256 feats = torch.rand(N,3, F, device=\u0026#39;cuda\u0026#39;) points = torch.rand(N,3, device=\u0026#39;cuda\u0026#39;)*2-1 # [0,1] -\u0026gt; [-1,1] out = trilinear_interpolate(feats, points) print(out.shape) title: \u0026ldquo;watch: CppExt - AI葵 05 | Validate\u0026rdquo; date: 2023-10-28T12:05:00\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 5 - English CC - Source code To validate if cuda kernel yields correct results, impelement a PyTorch version.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch import my_cppcuda_pkg import time def trilinear_interpolate_py(feats, points): r\u0026#34;\u0026#34;\u0026#34; feats: (N, 8, F), features on 8 vertices points: (N, 3) , coordinates [-1,1] \u0026#34;\u0026#34;\u0026#34; u,v,w = (points[:,0:1]+1)/2, (points[:,1:2]+1)/2, (points[:,2:3]+1)/2 a,b,c,d = (1-v)*(1-w), v*(1-w), (1-v)*w, v*w feat_interp = (1-u) * (a*feats[:,0] + b*feats[:,1] + c*feats[:,2] + d*feats[:,3]) \\ + u*(a*feats[:,4] + b*feats[:,5] + c*feats[:,6] + d*feats[:,7]) return feat_interp # (N,F) if __name__ == \u0026#34;__main__\u0026#34;: N=65536; F=256 feats = torch.rand(N,8,F, device=\u0026#34;cuda\u0026#34;).requires_grad_(True) points = torch.rand(N,F, device=\u0026#34;cuda\u0026#34;)*2-1 t = time.time() out_cuda = my_cppcuda_pkg.trilinear_interpolate(feats, points) torch.cuda.synchronize() print(f\u0026#39;CUDA time: {time.time()-t} s\u0026#39;) t = time.time() out_py = trilinear_interpolate_py(feats, points) torch.cuda.synchronize() print(f\u0026#39;PyTorch time: {time.time()-t} s\u0026#39;) print(f\u0026#34;fw all close? {torch.allclose(out_cuda, out_py)}\u0026#34;) print(f\u0026#34;Cuda has grad? {out_cuda.requires_grad}\u0026#34;) title: \u0026ldquo;watch: CppExt - AI葵 06 | Backward\u0026rdquo; date: 2023-10-28T16:40:00\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 6 反向傳播 - English CC -\nSource code Compute Partial Derivatives When loss L comes, the partial derivatives of L w.r.t. every trainable input variable of the function are required.\nTrilinear interpolation:\n$$ \\begin{aligned} f(u,v,w) = (1-u) * [ \u0026 (1-v)(1-w)f₁ + v(1-w)f₃ + (1-v)wf₅ + vw f₇ ] \\\\\\ + u * [ \u0026 (1-v)(1-w)f₂ + v(1-w)f₄ + (1-v)w f₆ + vwf₈ ] \\end{aligned} $$ u,v,w are coordinates, which are constant (requires_grad is False). So only vertices features f₁, f₃, f₅, f₇, f₂, f₄, f₆, f₈ need optimizing.\nGiven interpolated result f, their gradients for this operation are:\n$$ \\begin{aligned} \u0026\\frac{∂f}{∂f₁} = (1-u)(1-v)(1-w); \u0026\\frac{∂f}{∂f₂} \u0026= u(1-v)(1-w); \\\\\\ \u0026\\frac{∂f}{∂f₃} = (1-u)v(1-w); \u0026\\frac{∂f}{∂f₄} \u0026= uv(1-w); \\\\\\ \u0026\\frac{∂f}{∂f₅} = (1-u)(1-v)w; \u0026\\frac{∂f}{∂f₆} \u0026= u(1-v)w; \\\\\\ \u0026\\frac{∂f}{∂f₇} = (1-u)vw \u0026\\frac{∂f}{∂f₈} \u0026= uvw \\end{aligned} $$ The derivatives of L w.r.t. features f₁, f₂, f₃, f₄, f₅, f₆, f₇, f₈ are:\n$$ \\frac{∂L}{∂f} \\frac{∂f}{∂f₁}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₂}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₃}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₄}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₅}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₆}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₇}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₈} $$ Bw Kernel Write host function trilinear_bw_cu based on trilinear_fw_cu in \u0026ldquo;interpolation_kernel.cu\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 torch::Tensor trilinear_bw_cu( const torch::Tensor dL_dfeat_interp, // Inputs const torch::Tensor feats, const torch::Tensor points ){ const int N = points.size(0); const int F = feats.size(2); torch::Tensor dL_dfeats=torch::empty({N,8,F}, feats.options()); // output data const dim3 threads(16,16); const dim3 blocks((N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y); // Launch kernel function AT_DISPATCH_FLOATING_TYPES(feats.type(), \u0026#34;trilinear_bw_cu\u0026#34;, ([\u0026amp;] { trilinear_bw_kernel\u0026lt;scalar_t\u0026gt;\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;( dL_dfeat_interp.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), feats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;(), points.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), dL_dfeats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;() ); } ) ); return dL_dfeats; } Write kernel function trilinear_bw_kernel based on trilinear_fw_kernel in \u0026ldquo;interpolation_kernel.cu\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 template \u0026lt;typename scalar_t\u0026gt; __global__ void trilinear_bw_kernel( const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; dL_dfeat_interp, const torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; feats, const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; points, torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; dL_dfeats ){ const int n = blockIdx.x * blockDim.x + threadIdx.x; const int f = blockIdx.y * blockDim.y + threadIdx.y; if (n \u0026gt;= points.size(0) || f\u0026gt;= feats.size(2)) return; // Define helper variables const scalar_t u = (points[n][0]+1)/2; const scalar_t v = (points[n][1]+1)/2; const scalar_t w = (points[n][2]+1)/2; const scalar_t a = (1-v)*(1-w); const scalar_t b = v*(1-w); const scalar_t c = (1-v)*w; const scalar_t d = v*w; // Compute derivatives dL_dfeats[n][0][f] = dL_dfeat_interp[n][f]*(1-u)*a; dL_dfeats[n][1][f] = dL_dfeat_interp[n][f]*(1-u)*b; dL_dfeats[n][2][f] = dL_dfeat_interp[n][f]*(1-u)*c; dL_dfeats[n][3][f] = dL_dfeat_interp[n][f]*(1-u)*d; dL_dfeats[n][4][f] = dL_dfeat_interp[n][f]*u*a; dL_dfeats[n][5][f] = dL_dfeat_interp[n][f]*u*b; dL_dfeats[n][6][f] = dL_dfeat_interp[n][f]*u*c; dL_dfeats[n][7][f] = dL_dfeat_interp[n][f]*u*d; } Add the function signature into header file \u0026ldquo;include/utils.h\u0026rdquo;\n1 2 3 4 5 torch::Tensor trilinear_bw_cu( const torch::Tensor dL_dfeat_interp, const torch::Tensor feats, const torch::Tensor points ); Add a cpp function to call the backward method trilinear_bw_cu in \u0026ldquo;interpolation.cpp\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 torch::Tensor trilinear_interpolate_bw( const torch::Tensor dL_dfeat_interp, const torch::Tensor feats, const torch::Tensor points ){ CHECK_INPUT(dL_dfeat_interp); CHECK_INPUT(feats); CHECK_INPUT(points); return trilinear_bw_cu(dL_dfeat_interp, feats, points); } Give the function trilinear_interpolate_bw a name in PYBIND as a method of the package:\n1 2 3 4 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); m.def(\u0026#34;trilinear_interpolate_bw\u0026#34;, \u0026amp;trilinear_interpolate_bw); } Encapsulate Wrap forward and backward by a subclass inherited from torch.autograd.Function in \u0026ldquo;test.py\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class trilinear_interpolate_cuda(torch.autograd.Function): @staticmethod def forward(ctx, feats, points): feat_interp = my_cppcuda_pkg.trilinear_interpolate(feats, points) ctx.save_for_backward(feats, points) return feat_interp @staticmethod def backward(ctx, dL_dfeat_interp): # The number of input vars corresponds to return values of forward pass. # i.e., inputs are gradients of Loss w.r.t the forward\u0026#39;s outcomes. feats, points = ctx.saved_tensors dL_dfeats = my_cppcuda_pkg.trilinear_interpolate_bw( dL_dfeat_interp.contiguous(), feats, points) return dL_dfeats, None # return gradients of Loss w.r.t each input data forward Notes:\nThe nubmer of return values needs to match the input to forward pass. If some input doesn\u0026rsquo;t require grad, return a None.\nctx is mandatory for storing intermeidate data.\nVerify Graident Test the gradient of backward:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # test.py import torch import my_cppcuda_pkg import time def trilinear_interpolate_py(feats, points): r\u0026#34;\u0026#34;\u0026#34; feats: (N, 8, F), features on 8 vertices points: (N, 3) , coordinates [-1,1] \u0026#34;\u0026#34;\u0026#34; u,v,w = (points[:,0:1]+1)/2, (points[:,1:2]+1)/2, (points[:,2:3]+1)/2 a,b,c,d = (1-v)*(1-w), v*(1-w), (1-v)*w, v*w feat_interp = (1-u) * (a*feats[:,0] + b*feats[:,1] + c*feats[:,2] + d*feats[:,3]) \\ + u*(a*feats[:,4] + b*feats[:,5] + c*feats[:,6] + d*feats[:,7]) return feat_interp # (N,F) if __name__==\u0026#34;__main__\u0026#34;: N = 1024; F=256 feats = torch.rand(N,8,F, device=\u0026#34;cuda\u0026#34;) feats_py = feats.clone().requires_grad_() feats_cu = feats.clone().requires_grad_() points = torch.rand(N,3, device=\u0026#34;cuda\u0026#34;)*2-1 t = time.time() out_py = trilinear_interpolate_py(feats_py, points) torch.cuda.synchronize() print(f\u0026#34;py: {time.time() - t}\u0026#34;) t = time.time() out_cuda = trilinear_interpolate_cuda.apply(feats_cu, points) torch.cuda.synchronize() print(f\u0026#34;cu: {time.time() - t}\u0026#34;) loss_py = out_py.sum() loss_cuda = out_cuda.sum() loss_py.backward() loss_cuda.backward() print(f\u0026#34;Grad all close? {torch.allclose(feats_py.grad, feats_cu.grad)}\u0026#34;) ","date":"2023-10-11T16:23:00Z","image":"https://img.youtube.com/vi/l_Rpk6CRJYI/maxresdefault.jpg","permalink":"http://blog.zichen.uk/post/writenotes/lang/cuda/tut_ai%E8%91%B5/","title":"watch: CppExt - AI葵 | CUDA Extension for PyTorch"},{"content":"Yannic Source video: Retentive Network: A Successor to Transformer for Large Language Models (Paper Explained)\nRemove softmax outside the attention scores, then no all the results have to be hold and wait for softmax.\nT p r a a r n a i l n l L T f g i i r o s n a r m e n m a s e r - r T R r I L e a n o t n f w N s e e f r C t o e o r n s m c t e e r R N e e c t p u w S e r o t r r r r f e k o o n n r t g m a n c e RetNet is a kind of linear transforemr, like RWKV.\nRecurrent network each time train only 1 token because once the next work has been predicted, the backpropagation has to be done to optimize previous hidden states.\nw h s o i t r d a d d t s e e : n s b p a r c o k p Recurrent network cannot be trained parallelly because the non-linearity activation function\nG(c( G(b( G(ax+γ)+γ )+γ) +γ) )\nHidden state is a shared buffer. The hidden state contains all the previous information, so the memory cost is consistent during training.\n$$ \\begin{aligned} ax+\\gamma = \\gamma \\\\\\ by + \\gamma = \\gamma \\\\\\ cz + \\gamma = \\gamma \\\\\\ \\end{aligned} $$ Transformer can\u0026rsquo;t be recurrent because the existence of softmax, which requires all the attention scores (\u0026ldquo;hidden states\u0026rdquo;) not to be abandoned.\nRetNet achieved training parallism through matrix multiplication, like a Linear layer.\nTime-scaling mask replaces causal mask (blocking the subsequent words when doing attention in parallel)\nRetNet by chunks is a trade-off between recurrent and parallel.\n秋刀鱼 Source video: 【论文速览】 RetNet: A Successor to Transformer for Large Language Models2307.08621\nEquentions explaination and code walkthrough.\nA global state is maintained like recurrent network. With that, expand the equation of attention: Q K V Apply singular decomposition \u0026hellip;. ","date":"2023-10-10T23:41:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/d-vid-retnet/","title":"watch: RetNet"},{"content":" Source video: 第一次在美國直播（講解Gaussian Splatting的cuda code）- AI葵\nkwea123/gaussian_splatting_notes\nSelected comments:\n\u0026ldquo;3D Gaussian Mixture Model.\u0026rdquo; GMM?!\n\u0026ldquo;EWA Splatting paper (2001) contains all the necessary derivations and math.\u0026rdquo; - Matias Turkulainen (gsplat contributor).\nforward.cu Trade-off:\nThe 2D-projections of 3D ellipsoids are circles rather than ellipses to reduce shading tiles.\nDetermine visibility of tiles instead of pixels (shaded by circle shadows) for fast rasterization.\nMain steps:\nDetermine the radii of circles shadows (preprocessCUDA)\nProject a 3D ellipsoid will yield a 2x2 covariance matrix\nSolve the 2 Eigenvalues for the covariance matrix, and the bigger one is the length of the major axis.\nUse the major axis as the radius of the circle.\nDetermine pixels covered by the projected circles.\nIf the distance betwen a pixel to the circle center is smaller than the circle radius, the pixel is visible to the circle (a disc corresponding to a 3D Gaussian in the ray space).\nGetRect Sort 3D Gaussians (discs) by depths\nEach tile is shaded by multiple ellipsoids, i.e., visible to multiple Gaussians.\nPair each pixel with each contributing ellipsoid, and form a 64-bit identifier for each pair.\ni.e., stitching the index (32-bit) of a pixel and the depth (32-bit) of a ellipsoid.\nSort all the identifier, and obtain a sequence\nFor example:\n0 2 c a b 1 3 tile-0 has 2 pairs: 0-a, 0-c tile-1 has 3 pairs: 1-a, 1-b, 1-c Suppose the depths of 3 Gaussians are b \u0026gt; a \u0026gt; c, the sequence of pairs is as follows:\n1 2 3 4 5 0-c 0-a 1-c 1-a 1-b Alpha compositing for each pixel\u0026rsquo;s color (renderCUDA)\nImplementation tricks:\nA tile is a Block, in which each pixel is a worker. Every pixel in a tile uses the same Gaussian distributions, so those data are stored in __share__ memory. Docs Calculate alpha, which is proportional to the probability in a Gaussian distribution.\nThe probability is calculated according to the expression of 2D Gaussian.\nBlend alpha * color (SH) of each ellipsoid front-to-back.\nbackward.cu output input pixels\u0026rsquo; color (p,3) Gaussians\u0026rsquo; color (g,3) Gaussians\u0026rsquo; alpha Gaussians\u0026rsquo; position (g,3) Gaussians\u0026rsquo; rotation (g,4) Gaussians\u0026rsquo; length of axis (g,3) p is number of pixels; g is number of 3D Gaussians Loss = color + SSIM\nA full image is produced at once, so image metrics, like SSIM, can be added. Write parital derivative for each input tensor.\n","date":"2023-10-10T16:43:00Z","image":"https://img.youtube.com/vi/1buFrKUaqwM/maxresdefault.jpg","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/d-vid-3dgs-explain-%E8%91%B5/","title":"watch: 3DGS | AI葵 Cuda Code Walkthrough"},{"content":"Metapost ChatGPT - code interpreter Data visualization\nProcesson Networks Architecture\n你的科研能力从什么时候开始突飞猛进的？ - 平凡的回答 - 知乎\n","date":"2023-10-07T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/latex_draw/","title":"memo: Plotting for Academics"},{"content":"Nvidia apex An example project using it is AIM.\ntorch amp An example: Automatic Mixed Precision recipe\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 use_amp = True net = make_model(in_size, out_size, num_layers) opt = torch.optim.SGD(net.parameters(), lr=0.001) scaler = torch.cuda.amp.GradScaler(enabled=use_amp) for epoch in range(epochs): for input, target in zip(data, targets): with torch.autocast(device_type=\u0026#39;cuda\u0026#39;, dtype=torch.float16, enabled=use_amp): output = net(input) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(opt) scaler.update() opt.zero_grad() # set_to_none=True here can modestly improve performance 【pytorch distributed】amp 原理，automatic mixed precision 自动混合精度\n","date":"2023-09-19T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_amp/","title":"memo: PyTorch | Automatic Mixed Precision"},{"content":"Source video: CLIP 论文逐段精读【论文精读】- 跟李沐学AI ~ Bilibili 2022-02-10\nCLIP (Contrastive Language-Image Pre-Training) Code\nFeatures Large-scale dataset: 4e8 pairs of image and caption.\nSelf-supervised learning strategy (pretext task): Given an image, find the matched text vector from candidates\nContrastive learning needs positive and negative samples.\nThere is only one correct text vector for an image, while the remaining text vectors are served as negative samples.\nLoosen the target: pairing rather than predicting next word\nGood transferability: Able to generalize to unseen classes based on the text prompts.\nLeverage text to enhance image features with semantic understanding \\begin{algorithm} \\caption{CLIP} \\begin{algorithmic} \\STATE If = ImageEncoder(I) $\\quad$ \\COMMENT{(n,h,w,c)→(n, di)} \\STATE Tf = TextEncoder(T) $\\quad$ \\COMMENT{(n,l)→(n,dt)} \\STATE Ie = Linear projection (If) $\\quad$ \\COMMENT{(n,de)} \\STATE Te = Linear projection (Tf) $\\quad$ \\COMMENT{(n,de)} \\STATE logits = Inner Product (Ie, Te.T) \\STATE labels = np.arange(n) \\STATE lossᵢ = CrossEntropy(logits, labels, axis=0) \\STATE lossₜ = CrossEntropy(logits, labels, axis=1) \\STATE loss = (lossᵢ + lossₜ)/2 \\end{algorithmic} \\end{algorithm} Experiments Backbone model: The image encoder can be ResNet or ViT, text encoder is a transformer\nZero-shot transfer: No downstream task adaptation, apply the pre-trained model directly onto the unseen data.\nFew-shot transfer: Given a few images, fine-tune or linearly probe the pre-trained model. CLIP outperforms all the previous pre-trained models supervised by labels.\nFull-data transfer: Better than other zero-shot model.\nThe features extracted by previous pre-trained models only have the image modality, while the image features of CLIP are learned under the instructions of text description, so the image features have fused with text modality and guided to semantic understanding.\nMix precision training can save half of memory without losing performance.\nPrompt engineering: Fit the label into a sentence by putting it into prompt templates to close gap with the training set, i.e., image-caption pairs.\nThey made 80 templates for describing different situations in images, such that more specific context is confined to help find the solution in a small possible range.\nUnrealistic and abstract datasets, like MNIST, counting number of objects, are difficult for CLIP because they are hard to describe with language. Otherwise, as long as the describable object exists in the image, CLIP can recognize it.\nLimitations CLIP is not the SOTA on ImageNet, but only in the zero-shot task.\nCannot understanding abstract concepts: \u0026ldquo;abnormal\u0026rdquo;, \u0026ldquo;safe\u0026rdquo;\nOut-of-distribution when performing zero-shot inference will ruin the generaliability of CLIP: MNIST (different from natural images) isn\u0026rsquo;t included in the training set.\nZero-shot inference of CLIP requires the \u0026ldquo;new label\u0026rdquo; is provided in the candidates to do a multiple choice question.\nBy contrast, let model generate caption from image will make the data loop. But that is infesible because massive computation with low-efficient training techinics.\nData utilization is inefficient with too many training images. Dataloader spitting image one-by-one needs long time.\nDatasets bias: Hyperparameter tunning is based on ImageNet; The testing performance is based on chosen 27 datasets.\nTraining set is from internet without filtering, so the model may learned malicious information.\nPerformance of few-shot learning sometimes is inferior to zero-shot scenario weirdly.\nFooter:\nThe pre-trained method isn\u0026rsquo;t open-source. But the model is open source. Code Repo\nInstall CLIP:\n1 2 3 conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0 pip install ftfy regex tqdm pip install git+https://github.com/openai/CLIP.git Zero-shot classification:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch import clip from PIL import Image device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) image = preprocess(Image.open(\u0026#34;CLIP.png\u0026#34;)).unsqueeze(0).to(device) text = clip.tokenize([\u0026#34;a diagram\u0026#34;, \u0026#34;a dog\u0026#34;, \u0026#34;a cat\u0026#34;]).to(device) with torch.no_grad(): image_features = model.encode_image(image) text_features = model.encode_text(text) logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\u0026#34;Label probs:\u0026#34;, probs) # prints: [[0.9927937 0.00421068 0.00299572]] ","date":"2023-08-29T12:12:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transfer/d-vid-clip_paper/","title":"watch: CLIP Paper Walkthrough"},{"content":"Code | Arxiv | ProjPage | OpenReview | Ytb\nNote Abs Transfer between different modalities: image classification and video understanding\nAdd 3 new layers inside each transformer block, not at the very end of the model.\n3 new layers:\nSpatial adapter is after self-attention; Temporal adapter is after two self-attention, Joint adapter is a bypass branch of the MLP layers. Frozen pre-trained parameters and optimize only the new layers to transfer the pre-trained model onto another task.\nIntro Two directions: adding temporal module onto or inflating an image transformer model both have drawbacks: heavy-computation full fine-tunning is required. Related work Pre-trained image models have good transferability. Fully fine-tuning a transformer-based image model is uneconomical. Parameter-efficient finetuning was applied on LLM for downstream tasks. Method ViT consists of 12 encoder blocks (MSA and MLP).\nAn image is split into N pathes, which will be projected to D channels;\nThe input to MSA is each patch attached class token channel and added positional encoding.\nSpace-only model (baseline, no temporal modelong): Apply pre-trained frozen ViT onto video by processing each frame independently.\nEach frame will be represented by the final class token.\nThe token of each frames are averaged to form a vector for predicting\nSpatial adaptation is adding an adapter after the self-attention (pre-trained MSA) fuses N+1 patches.\nAn adapter is a bottleneck, i.e, Reduce-Act-Expand with skip connection.\nThis can achieve comparable performance compared with space-only baseline, because image model learns spatial feature well.\nTemporal modeling reused the self-attention parameters again, whereas the T frames got fused by reshapeing the tensor.\nAnother adapter is appended for adapting the generated temporal features.\nTemporal modeling is performed ahead of spatial modeling, so the adapter is removed skip connection and initialized as zero to avoid disrupting the perfomance of the original model.\nBy reusing the MSA, the number of parameters is maintained.\nJoint adapation jointly fits the temporal features and spatial features.\nThis adapter also doesn\u0026rsquo;t has skip connection.\nAverage the final class token of each frame and pass it to classification head.\nExperiments Task: classification video?\n8 frames Memory: AIM based on Swin-B pre-trained with IN-21K occupies 9GB. Underperform on temporal-heavy video because the temporal modeling is simply reusing the spatial modeling parameters. Discussion Deeper layer needs adaptation for task-specific features, while shallow layer may not. Conclusion Transfer models trained with other sequence data, like text and audio for video action recognition. flowchart TD input(\"Image (224,224,3)\") --\u003e cls(\"Class token (1,768)\") \u0026 pe(\"Position Embedding (197,768)\") input --\u003e feat(\"Conv2d (16x16,s16) (14,14)\") cls \u0026 feat --\u003e Cat pe \u0026 Cat --\u003e add1(\"Add\") add1 --\u003e msa1(\"MSA\") --\u003e Tadap --\u003e msa2(\"MSA\") --\u003e Sadap Sadap --\u003e ineck(\"Inverse bootleneck\") Sadap --\u003e Jadap add1 \u0026 ineck \u0026 Jadap --\u003e add2(\"Add\") --\u003e x Play Debug code with experiment settings in \u0026ldquo;run_exp.sh\u0026rdquo;\nEnvironment 1 2 3 4 5 6 7 8 9 10 11 12 13 conda env create -f ./environment.yml conda activate AIM # install CLIP pip install git+https://github.com/openai/CLIP.git # install mmaction2 python setup.py develop # install apex git clone https://github.com/NVIDIA/apex cd apex pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\u0026#34;--cpp_ext\u0026#34; --global-option=\u0026#34;--cuda_ext\u0026#34; ./ Dataset diving48 To prepare the dataset diving48 , I downloaded the repo MMAction2 Documentaions\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 conda create --name openmmlab python=3.8 -y conda activate openmmlab conda install pytorch torchvision -c pytorch # Step 1: pip install -U openmim mim install mmengine mim install mmcv mim install mmdet mim install mmpose # Step 2: git clone https://github.com/open-mmlab/mmaction2.git cd mmaction2 pip install -v -e . Following \u0026ldquo;Download from Official Source\u0026rdquo; section.\nDownload annotations using their shell script. bash download_annotations.sh Download videos \u0026ldquo;Diving48_rgb.tar.gz\u0026rdquo; (9.6G) Only extract the rgb frames: bash extract_rgb_frames_opencv.sh Generate file list using program: bash generate_videos_filelist.sh Make a symbolic link to \u0026ldquo;mmaction2/data\u0026rdquo; in \u0026ldquo;adapt-image-models\u0026rdquo;: ln -s /home/zichen/Downloads/mmaction2/data/ ./\nFormat\nannotation file \u0026ldquo;data/diving48/diving48_train_list_videos.txt\u0026rdquo; includes: filename and class label of each video Config for 1080Ti Train with 1 video cannot make the acc increase\nDefault configs (8 videos, 32 frames) will cause 1 1080Ti OOM. (\u0026ldquo;configs/recognition/vit/vitclip_large_diving48.py\u0026rdquo;)\nOverride the number of videos in config file with args:\n1 2 3 \u0026#34;args\u0026#34;:[ \u0026#34;--cfg-options\u0026#34;, \u0026#34;data.videos_per_gpu=1\u0026#34; ] But the top1_acc didn\u0026rsquo;t grow:\n1 2 3 4 5 2023-08-31 12:22:11,768 - mmaction - INFO - Epoch [1][4180/15027] lr: 6.003e-05, eta: 6 days, 8:30:03, time: 0.709, data_time: 0.001, memory: 5659, top1_acc: 0.0500, top5_acc: 0.3500, loss_cls: 3.4383, loss: 3.4383 \u0026quot;data.videos_per_gpu=2\u0026quot; will OOM.\nReduce num_frames\n.vscode/launch.json is made based on \u0026ldquo;run_exp.sh\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34;, \u0026#34;data.videos_per_gpu=8\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, // The follwings cannot change // \u0026#34;train_pipeline[1].clip_len=3\u0026#34;, // \u0026#34;val_pipeline[1].clip_len=3\u0026#34; \u0026#34;--train_clip_len\u0026#34;, \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; ] (2023-09-06) The cfg.data.train['pipeline']['clip_len'] didn\u0026rsquo;t changed, which still equals 32. Consequently, the images x passed to forward(self, x) of model ViT_CLIP has the shape (256, 197, 768)\nHowever, the instance variable self.num_frames of the backbone model ViT_CLIP was changed to 3.\nThen, the einops.rearrange cannot parse the dimensionality in: x = rearrange(x, '(b t) n d -\u0026gt; (b n) t d', t=self.num_frames)\n1 2 einops.EinopsError: Shape mismatch, can\u0026#39;t divide axis of length 256 in chunks of 3 Dataset is built based the key cfg.data.train, thus, its values are also required to update:\n1 2 3 cfg.merge_from_dict(dict(train_pipeline=args.train_clip_len, val_pipeline=args.train_clip_len)) update_option = {\u0026#39;data\u0026#39;: {\u0026#39;train\u0026#39;: {\u0026#39;pipeline\u0026#39;: args.train_clip_len}, \u0026#39;val\u0026#39;: {\u0026#39;pipeline\u0026#39;: args.train_clip_len}}} cfg.merge_from_dict(update_option) Start training:\n1 2 3 4 5 6 7 8 9 10 11 12 13 export CUDA_VISIBLE_DEVICES=4 python -m torch.distributed.launch \\ --nproc_per_node=1 --master_port=29500 \\ tools/train.py \\ \u0026#34;configs/recognition/vit/vitclip_base_diving48.py\u0026#34; \\ --launcher=\u0026#34;pytorch\u0026#34; \\ --test-last \\ --validate \\ --cfg-options model.backbone.pretrained=\u0026#34;openaiclip\u0026#34; \\ work_dir=\u0026#34;work_dirs_vit/diving48/debug\u0026#34; \\ data.videos_per_gpu=8 \\ model.backbone.num_frames=3 \\ --train_clip_len \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; Optimization Souce code\nAdamW: lr=3e-4, weight_decay=0.05, LR scheduler: CosineAnnealing Pseudocode With backbone: ViT_CLIP\n\\begin{algorithm} \\caption{main()} \\begin{algorithmic} \\PROCEDURE{Config}{cfg, args} \\STATE args = parse\\_args() \\PROCEDURE{Config.fromfile}{args.config} \\STATE model settings \\STATE dataset settings: ann\\_file, train\\_pipeline,... \\STATE optimizer settings \\STATE learning policy \\STATE runtime settings \\ENDPROCEDURE \\ENDPROCEDURE \\STATE $\\newline$ \\PROCEDURE{build-model}{cfg.model} \\COMMENT{Construct ViT with Adapters added} \\PROCEDURE{build-localizer}{cfg} \\PROCEDURE{LOCALIZERS.build}{cfg} \\PROCEDURE{BaseRecognizer}{} \\STATE $\\newline$ \\PROCEDURE {builder.build-backbone}{backbone} \\STATE BACKBONES.build(cfg) \\ENDPROCEDURE \\STATE $\\newline$ \\PROCEDURE {init-weights}{} \\STATE self.backbone.init\\_weights() \\COMMENT{Load pretrained state\\_dict} \\ENDPROCEDURE \\STATE $\\newline$ \\ENDPROCEDURE \\ENDPROCEDURE \\ENDPROCEDURE \\ENDPROCEDURE \\STATE $\\newline$ \\STATE datasets = [build\\_dataset(cfg.data.train)] \\STATE $\\qquad$ build\\_from\\_cfg(cfg, DATASETS) \\STATE $\\qquad$ 11 transforms operations \\STATE Freeze params.requires\\_grad=False \\STATE $\\newline$ \\PROCEDURE{train-model}{model,datasets,cfg,...} \\STATE dataloader\\_settings \\STATE data\\_loaders = build\\_dataloader(dataset, dataloader\\_setting) \\STATE optimizer = build\\_optimizer(model, cfg.optimizer) \\STATE amp settings \\STATE fp16 settings \\STATE register DistOptimizerHook \\STATE build validation dataset and dataloader \\STATE $\\newline$ \\PROCEDURE{runner.run}{data\\_loaders, cfg.workflow, cfg.total\\_epochs,**runner\\_kwargs} \\STATE DistOptimizerHook.before\\_run(self, runner): \\STATE $\\qquad$ runner.optimizer.zero\\_grad() \\STATE BaseRecognizer.train\\_step(self, data\\_batch,) \\STATE losses = self(imgs, label) \\PROCEDURE {Recognizer3D.forward-train}{img, label} \\STATE x = BaseRecognizer.extract\\_feat(imgs) \\STATE $\\qquad$ self.backbone(imgs) \\COMMENT{ViT\\_CLIP.forward()} \\ENDPROCEDURE \\STATE $\\qquad$ self.forward\\_test(img, label) \\ENDPROCEDURE \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} Debug VideoSwin The pretrained weights of ViT_CLIP are obtained from an initialized clip model:\n1 2 3 4 5 clip_model, preprocess = clip.load(\u0026#34;ViT-B/16\u0026#34;, device=\u0026#34;cpu\u0026#34;) pretrain_dict = clip_model.visual.state_dict() # param del clip_model del pretrain_dict[\u0026#39;proj\u0026#39;] msg = self.load_state_dict(pretrain_dict, strict=False) Source code\nHowever, the weights of Swin Transformer needs to be loaded from file. Source code\nReminded by this issue MMCV load pretrained swin transformer\nPretrained Swin Transformer (Swin-B 224x224, \u0026ldquo;swin-base_3rdparty_in21k.pth\u0026rdquo;) of open-mmlab (mmpretrain) doesn\u0026rsquo;t have the key: \u0026lsquo;model\u0026rsquo;, so it mismatches the code.\n1 2 3 def inflate_weights(self, logger): checkpoint = torch.load(self.pretrained, map_location=\u0026#39;cpu\u0026#39;) state_dict = checkpoint[\u0026#39;model\u0026#39;] While the pretrained swin from microsoft can be successfully loaded. The Swin Transformer has not been trained with CLIP, only on ImageNet21K.\nThe author adds adapters to \u0026ldquo;Swin-B_IN-21K\u0026rdquo; SwinTransformer2D (\u0026ldquo;swin2d.py\u0026rdquo;) in \u0026ldquo;mmaction/models/backbones/ swin2d_adapter.py\u0026rdquo; as clarified in issue18.\nThe \u0026ldquo;swin2d_adapter\u0026rdquo; is compared with SwinTransformer3D (VideoSwin, \u0026ldquo;swin_transformer.py\u0026rdquo;) in Table 6. And most of their experiments are based on ViT_CLIP and compared with TimeSformer.\nSwinTransformer2D is adapted by settings: \u0026ldquo;configs/recognition/swin/ swin2d_adapter_patch244_window7_kinetics400_1k.py\u0026rdquo;.\nWhereas, the config file: \u0026ldquo;configs/recognition/swin/ swin_base_patch244_window877_kinetics400_1k.py\u0026rdquo; is for the original VideoSwin SwinTransformer3D.\nArguments pretrained: str and pretrained2d: bool of class SwinTransformer3D originate in VideoSwin, which adapted pretrained 2D swin transfromer to 3D.\nAIM codes are based on VideoSwin.\nFollowing VideoSwin, pretrained is supposed to be a path to the pretrained model, which should be downloaded in advance. An example is KeyError: \u0026lsquo;patch_embed.proj.weight\u0026rsquo; #22\nBased on the above, the args in launch.json should be set as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Swin-B settings \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;1\u0026#34;, // GPUs \u0026#34;--master_port\u0026#34;, \u0026#34;29600\u0026#34;, \u0026#34;tools/train.py\u0026#34;, \u0026#34;configs/recognition/swin/swin2d_adapter_patch244_window7_kinetics400_1k.py\u0026#34;, \u0026#34;--launcher\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;--test-last\u0026#34;, \u0026#34;--validate\u0026#34;, \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=work_dirs_swin/swin_base_patch4_window7_224_22k.pth\u0026#34;, \u0026#34;work_dir=work_dirs_swin/K400/debug\u0026#34;, \u0026#34;data.videos_per_gpu=8\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, \u0026#34;--train_clip_len\u0026#34;, \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; ] Dataset SSv2 AIM-Swin only has configuration file for K400 and ssv2 datasets. K400 has 240K training videos, which are massive. So I choose the smaller one, SSv2, which has 169K training videos.\nRefer to the guide of SSv2 - mmaction2\nAnnotations: Once signed in your Qualcomm account, download \u0026ldquo;Labels\u0026rdquo; into \u0026ldquo;data/sthv2/annotations/\u0026rdquo; from homepage (Need to acknowledge the agreement before jumping to the download page)\n1 2 3 4 5 6 7 unzip 20bn-something-something-download-package-labels.zip # Rename to match the python code \u0026#34;parse_file_list.py\u0026#34; mv data/sthv2/annotations/labels/train.json data/sthv2/annotations/something-something-v2-train.json mv data/sthv2/annotations/labels/validation.json data/sthv2/annotations/something-something-v2-validation.json mv data/sthv2/annotations/labels/test.json data/sthv2/annotations/something-something-v2-test.json mv data/sthv2/annotations/labels/labels.json data/sthv2/annotations/something-something-v2-labels.json Videos: Download 20 files into \u0026ldquo;mmaction2/data/sthv2/\u0026rdquo;.\nBy executing the following 2 commands, 220847 webm videos (19G) are extracted into the folder: \u0026ldquo;sthv2/20bn-something-something-v2\u0026rdquo;\n1 2 3 4 5 unzip 20bn-something-something-v2-\\??.zip cat 20bn-something-something-v2-?? | tar zx # Rename to match the script below and configs in AIM mv 20bn-something-something-v2/ videos/ Split: Generate list\n1 2 cd mmaction2/tools/data/sthv2/ bash generate_videos_filelist.sh Two .txt files \u0026ldquo;sthv2_train_list_videos.txt\u0026rdquo; and \u0026ldquo;sthv2_val_list_videos.txt\u0026rdquo; are created under \u0026ldquo;data/sthv2/\u0026rdquo;.\nTo debug AIM-swin with SSv2, specify the config file as \u0026ldquo;configs/recognition/swin/swin2d_adapter_patch244_window7_sthv2_1k.py\u0026rdquo; in \u0026ldquo;launch.json\u0026rdquo;.\n2023-09-12 15:41:48,166 - mmaction - INFO - Epoch [1][28160/84457]\tlr: 6.601e-05, eta: 21 days, 10:31:39, time: 0.365, data_time: 0.001, memory: 1420, loss_cls: 4.3897, loss: 4.3897\nForward swin \\begin{algorithm} \\caption{SwinTransformer2d\\_Adapter} \\begin{algorithmic} \\PROCEDURE{forward}{x: (B,T,D,H,W)} \\STATE Conv3d extracts feat maps: (B, C, num\\_Ttokens, H', W') \\STATE $\\newline$ \\PROCEDURE{SwinTransformer2d-Adapter}{B*num\\_Ttokens, H*W, C} \\STATE 2 SwinTransformerBlock \\STATE $\\quad$ rearrange \\STATE $\\quad$ LN1 \\STATE $\\quad$ Temporal MSA mix \"num\\_Ttokens\" of feat maps \\COMMENT{even blks} \\STATE $\\quad$ Temporal Adapter \\STATE $\\quad$ rearrange back \\STATE $\\newline$ \\STATE $\\quad$ LN1 \\STATE $\\quad$ Shift window rows and cols \\STATE $\\quad$ window\\_partition \\COMMENT{reshape} \\STATE $\\quad$ WindowAttention mix \"pixels\" in each window \\STATE $\\quad$ Spatial Adapter \\STATE $\\quad$ window\\_reverse \\STATE $\\quad$ Shift window rows and cols \\STATE $\\newline$ \\STATE $\\quad$ Squash feat maps to 1D \\STATE $\\quad$ Skip connect with the features before S\\_adap \\STATE $\\quad$ LN2 \\STATE $\\quad$ MLP + Joint Adapter \\STATE PatchMerging: (B*num\\_Ttokens, H'/2*W'/2, 2*C) \\STATE $\\newline$ \\STATE 2 SwinTransformerBlock \\STATE PatchMerging: (B*num\\_Ttokens, H'/4*W'/4, 4*C) \\STATE $\\newline$ \\STATE 18 SwinTransformerBlock \\STATE PatchMerging: (B*num\\_Ttokens, H'/8*W'/8, 8*C) \\STATE $\\newline$ \\STATE 2 SwinTransformerBlock \\ENDPROCEDURE \\STATE $\\newline$ \\STATE LN \\STATE rearrange to (B,C,T,H,W) \\STATE cls\\_head, i.e. I3DHead (A linear layer) \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} The reason of setting window_size to 7 may be that the resolution of feature maps is (56,56), which can shrink gradually to (7,7).\nAdapter: Pass the attended features to a bottleenck (2-layer MLP) for adapting them.\nAdapted Swin Differences of the adapted Swin (\u0026ldquo;swin2d_adapter.py\u0026rdquo;) from the baseline model SwinTransformer2D (\u0026ldquo;swin_transformer.py\u0026rdquo;):\n1 2 diff mmaction/models/backbones/swin2d_adapter.py \\ mmaction/models/backbones/swin2d.py swin2d has a temporal adapter more than swin_transformer\nswin2d_adapter has\nNo joint adapter\n","date":"2023-08-23T00:00:00Z","image":"https://adapt-image-models.github.io/method.JPG","permalink":"http://blog.zichen.uk/post/writenotes/model/transfer/b-note-aim-video/","title":"read: Transfer - Adapter | AIM for Video"},{"content":"Code-pytroch | Arxiv | OpenReview\nQ\u0026amp;A Condition image vs target image? Abstract A img2img diffusion model is conditioned with pose and a single source view to generate multiviews.\nStochastic conditioning: Randomly select a view from avaliable views as condition image at each denoising step during sampling?, rather than using only the given view.\nReconstruct a NeRF to measure 3D consistency of multi-views.\nNeRF is not their ultimate objective. Intro Regressive methods for NVS from sparse views based on NeRF are still not generalizable enough or able to produce high-quality completion for the occluded parts.\nRegularized NeRF (RegNeRF) suffer from artifacts when only few views are given because they and didn\u0026rsquo;t apply the features of commen prior of multiple scenes.\nRegressing a NeRF from image feataures (pixel-NeRF) tend to get blurred images.\nGeometry-free methods for NVS obtain colors that aren\u0026rsquo;t directly derived from volume rendering.\nLight field network Scene Representation Transformer EG3D combines StyleGAN and volume rendering 3D diffusion model is a generative and geometry-free method.\nUse pairs of images of the same scene to train a diffusion model. During training, one of them serves as the original, and the other is the condition image. The trained model can produce a multi-view set of a scene given one condition image. Model They consider multiple views from a scene are not independent, but follow the distribution of the training views, to enhance multi-view consistency.\nThe distributions of different views, given a scene with a total observation set 𝐒, $p(𝐱|S)$ are conditionally independent (different).\nNeRF solves NVS under an even strict condition: each ray in the scen is conditionally independent.\nHowever, with this nature, the diffusion model cannot guarantee the samplings (generated images), conditioned with different source view, follow a common distribution, i.e., the diffusion model needs a unique distribution to learn.\nIdeally, the common distribution should be p(S), but it\u0026rsquo;s difficult to approximate the entire scene based on sparse views. (Not sure, my guess.)\nThat\u0026rsquo;s why they reused the generated views previously for later condition.\nPose-conditioned Given the data distribution p(𝐱₁, 𝐱₂), diffusion model learns the distribution of one of the two images conditioned on the other image and both poses.\nNoise schedule involving signal-to-noise ratio λ. Loss function of DDPM Stochastic condition Figure 3: Stochastic conditioning sampler\nMarkovian model didn\u0026rsquo;t perform well, where the next image is conditioned on (k) previously generated views. Thus, a scene can be represented as $p(𝐗) = ∏ᵢp(𝐱ᵢ|𝐱_{","date":"2023-08-12T09:40:00Z","image":"https://ar5iv.labs.arxiv.org/html/2210.04628/assets/figures/training.png","permalink":"http://blog.zichen.uk/post/writenotes/model/nvs/b-note-nvs-dm-posecond/","title":"read: NVS with Pose-conditioned Diffusion Models"},{"content":"Code | Arxiv (2307) | ProjPage\nAbs \u0026amp; Intro Fine-tune the pre-trained text-to-image diffusion model (SD) Insert cross-attention blocks between UNet blocks; Generate multiple views in parallel using a SD, and fuse multi views by attention; Freeze pre-trained weights while training the attention blocks Solving problems:\nGenerating panorama Extrapolate one perspective image to a full 360-degree view Preliminary MVDiffusion derives from LDM (Latent Diffusion Model¹), which contains 3 modules:\nVAE for transfering the generation process to a latent space, denoising model (UNet) for sampling from the distribution of the inputs\u0026rsquo; latent codes, condition encoder for providing descriptors. Loss function is similar to original diffusion model: the MSE between original noise and predicted noise, which conditioned on noisy latents, timestep, and featues.\n$$L_{LDM} = ∑$$Convolution layers are insert in each UNet block:\nFeature maps at each level will be added into UNet blocks. Figure 2\nPipeline pixel-to-pixel correspondences enable the multi-view consistent generation of panorama and depth2img, because they have homography matrix and projection matrix to determine the matched pixel pairs in two images.\nPanorama Text-conditioned model: generate 8 target images from noise conditioned by per-view text prompts.\nThe final linear layer in CAA blocks are initialized to zero to avoid disrupt the SD\u0026rsquo;s original capacity.\nMultiple latents will be predict by UNet from noise images and then restored to images by the pre-trained VAE\u0026rsquo;s decoder.\nImage\u0026amp;Text-conditioned model: generate 7 target images based on 1 condition image and respective text prompts.\nBased on SD\u0026rsquo;s impainting modle as it takes 1 condition image.\nDon\u0026rsquo;t impaint the condition image by concatenating the noise input with a all-one mask (4 channels in total)\nCompeletly regenerate the input image by concatenating the noise input with a all-zero mask (4 channels in total)\nConcatenating all-one and all-zero channel during training make the model learn to apply different processes to condition image and target image.\ndepth2img S d \u0026amp; T e e e q p p x u t o t e h s n e p c m s r e a o p m o s p f t I A m n a T y g M S k e e i u e x t \u0026amp; d b y t m w i T m d s - o o m e o l e f c d a x d e t r o e c g t e a n l o e - l i o m d n s c m f e i s o a s t e n g i c d e o u i n t t e i i d v o e n e d k e i y m - a f g r e a s m e This two-stage design is because SD\u0026rsquo;s impainting modle doesn\u0026rsquo;t support depth map condition.\nSo the Text-conditioned model is reused to generate condition images. Then the Image\u0026amp;Text-conditioned model interpolate the two condition images. Correspondence-aware attention Aggregate the features of KxK neighbor pixels on every target feature maps to each pixel their own feature vector.\nThe source pixel $s$ perform positional encoding γ(0)\nThe neighbor pixel $t_\\*^l$ around the corresponding pixel $t^l$ on the target image $l$ perform position encoding $γ(s_\\*^l-s)$, which means the neighbor pixel $t_\\*^l$ need to be warpped back to source feature map to find the distance from $s_\\*^l$ to the source pixel $s$.\nFigure 3\nRef High-Resolution Image Synthesis with Latent Diffusion Models - Robin Rombach ","date":"2023-08-10T20:40:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/nvs/b-note-mvdiffusion-scene/","title":"read: MVDiffusion generates multi-view images"},{"content":"Arxiv\nAbstract Embed diffusion model into stereo matching network Adopt multi-level network for high-resolution input Fuse generated depth map to reconstruct 3D human model. Introduction Sparse-view methods, which predict geometry based on appearance, cannot produce detailed human model because of lacking sufficient multiview stereo matching.\nContinuous models are basically obtained from traditional stereo methods based on a continuous varitional formulation, which can solved by diffusion model.\nPipeline:\nReconstruct coarse field first by using DoubleField; Render depth maps from multiple viewpoints Compute disparity flow masks Refine disparity flow with diffusion model Level 1: Use CNN to extract feature maps of disparity flow masks Level 2: Condition diffusion model with feature maps Fuse 3D points through interpolation. ","date":"2023-08-10T18:40:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/nvs/b-note-diffustereo-human/","title":"read: DiffuStereo reconstruct 3D human"},{"content":" (2023-07-28)\ntaku Source video: 你必须拥有RWKV，以及其他模型为何拉胯，NLP杂谈 - taku的交错电台 - bilibili\nSequentially generating mimics the human speaking behavior naturally;\nNo need to perform Positional Encoding, RNN won\u0026rsquo;t mess up the order of tokens,\nbecause the next token always derives from the previous hidden state; RNN internally has time order. Vanilla transformer doesn\u0026rsquo;t include Positional Encoding. Cannot generate a sentence parallelly, but only word-by-word,\nbecause the \u0026ldquo;fusion matrix multiplication\u0026rdquo;, i.e., attn @ V has simplied to a RNN; Less multiplication: RNN module only consider previous two hidden states;\nAnd the matmul of Q*V and feedfoward module are kept. Locally optimizing can be done by giving a hidden state.\nTransformer has the ultimate precision because it attends all the token in the sequence, but it\u0026rsquo;s not necessary if the required performance can be met in some way.\nRWKV may forget former tokens along inputting. RWKV is good for inference and Transformer is good for training.\nRWKV incorporates RNN into transformer;\nSuch that RNN is combined with residual connection: hidden_state = hidden_state + attention(hidden_state)\nPrevious RNN is combine with attention but without residual connection;\nTransformer is attention + residual\nRWKV has a consistent memory cost at inference,\nbecause each generation only attends to the last two hidden states. That means it can accept infinite-long sequence when inference.?\nHowever, the memory cost grows up linearly when tranining, because the intermediate hidden states are required to store for calculating gradients.\nLarge model ranking\nSu, Jianlin Source article: Google新作试图“复活”RNN：RNN能否再次辉煌？- 苏剑林\nOnly if the sequence length is significantly longer than hidden size, the standard attention will become slower quickly because its quadratic complexity. Otherwise, it\u0026rsquo;s almost linear complexity. So, it\u0026rsquo;s not necessary to make attention linear.\nOn LM (Language Model) tasks, RNN underperform attention may suffer from the hidden size.\nYannic Kilcher Source video: RWKV: Reinventing RNNs for the Transformer Era (Paper Explained) - Yannic Kilcher\n","date":"2023-07-28T17:59:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/c-sum-rwkv/","title":"sum: RWKV"},{"content":"ResNet Deep Residual Learning for Image Recognition ~ 2015 MSRA CVPR arxiv\nNetwork Architectures: Bottleneck Block: Figure 5: A deeper residual function ℱ for ImageNet. Left: a building block (on 56×56 feature maps) as in Fig. 3 for ResNet-34. Right: a “bottleneck” building block for ResNet-50/101/152.\nCode: torchvision\nMobileNet MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications ~ 2017 Google arxiv\nFigure 3: Left: Standard convolutional layer with batchnorm and ReLU. Right: Depthwise Separable convolutions with Depthwise and Pointwise layers followed by batchnorm and ReLU.\nDepthwise + Pointwise convolution reduces FLOPs and parameters. Accuracy is slightly inferior to fully CNN MobileNet V3 Searching for MobileNetV3 ~ 2019 Google ICCV arxiv\nArchitecture of MobileNetV3-Large:\nTable 1 Specification For Mobilenetv3-Large. SE Denotes Whether there Is A Squeeze-and-Excite In That Block. NL Denotes the Type of Nonlinearity Used. Here, HS Denotes H-Swish and RE Denotes Relu. NBN Denotes No Batch Normalization. S Denotes Stride.\nTutorial-bili\nCode: torchvision\nEfficientNet EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ~ 2019 Google ICML arxiv\nFigure 2:Model Scaling. (a) is a baseline network example; (b)-(d) are conventional scaling that only increases one dimension of network width, depth, or resolution. (e) is our proposed compound scaling method that uniformly scales all three dimensions with a fixed ratio.\nSummary:²\nScale proportionally the resolution and channels of feature maps, and number of blocks in a model. Use NAS (Neural Architecture Search) to search a structure for smaller models. Architecture:\nMBConv block is similar to MobileNetV3 InvertedResidualBlock. 8\nCode: torchvision\nVision Transformer An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ~ 2020 Google ICLR arxiv\nImage from 7\nConvert an image (224x224) to a vector using a 16x16 kernel with a stride of 16 and then flattening the result feature map (14x14) into a sequence (196).\nBy performing 728 times convolution for getting 728 vectors, an image is represented by a matrix (196, 728).\nTo encode this image\u0026rsquo;s information into a vector for classification, the class label are concatenated onto each vector and Positional encoding is added element-wise onto each vector.\nThen this matrix (197, 728) gets passed through Multi-head Self-Attention so that each token (728) will obtain a vector recording the similarity (mutual information?) between it and each other token.\nOnly taking out the vector belonging to the class label, it will be projected to the number of target categories for classifing.\nThe hybrid model didn\u0026rsquo;t downsample the image by a 16x16 kernel, but use ResNet50 to shrink the 224x224 image to 14x14.\nWhen training fewer epochs (7), hybrid model has higher accuracy than standard ViT. However, more epochs will make ViT better than hybrid model. ViT needs pre-traine on a large dataset (Google JFT) to perform better on ImageNet. However, if it\u0026rsquo;s trained on ImageNet-1K directly, the result won\u0026rsquo;t be good.\nSwin Transformer Swin Transformer: Hierarchical Vision Transformer using Shifted Windows ~ 2021 MSRA ICCV arxiv\nA unit component of swin transformer has two blocks: a Window MSA and a Shifted Window MSA.\nPatch merging\nImage from 5\nWindow MSA\nInstead of performing MSA for the all patches (sequence) of a feature map, a feature map is divided into finer grid, where severl patches is a group and a group of patches perform MSA.\nThis way reduces computation.\nDisadvantage is that the context between different group isn\u0026rsquo;t built.\nShifted Window MSA\nMove the grid (H/2, W/2) patches to the bottom right, then different groups can be fused through MSA.\nTo enhence parallelization, top row of patches are moved to the bottom and the left-most column of patches are moved the right-most.\nTo avoid fuse non-neighbor patches that are not adjacent in the original feature maps, masked MSA is used.\nThe masks are added onto the q-k weights corresponding to the non-neighbor patches for the current patch (q).\n(2023-09-28)\nSplit windows 1 image, 2 channels, H=4, W=6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 a = torch.arange(48).reshape(1,2,4,6) [[[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]] # a.is_contiguous() == True b = rearrange(a, \u0026#34;B C (nh H) (nw W) -\u0026gt; B C nh H nw W\u0026#34;, nh=2, nw=2) # b.is_contiguous() == True c = rearrange(b, \u0026#34;B C nh H nw W -\u0026gt; B nh nw C H W\u0026#34;) # c.is_contiguous() == False d = rearrange( c, \u0026#34;B nh nw C H W -\u0026gt; (B nh nw) C H W\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 b (1,2,2,2,2,3) c (1,2,2,2,2,3) d (4,2,2,3) [[[[[[ 0, 1, 2], [[[[[[ 0, 1, 2], [[[[ 0, 1, 2], [ 3, 4, 5]], [ 6, 7, 8]], [ 6, 7, 8]], [[ 6, 7, 8], [[24, 25, 26], [[24, 25, 26], [ 9, 10, 11]]], [30, 31, 32]]], [30, 31, 32]]], [[[12, 13, 14], [[[ 3, 4, 5], [[[ 3, 4, 5], [15, 16, 17]], [ 9, 10, 11]], [ 9, 10, 11]], [[18, 19, 20], [[27, 28, 29], [[27, 28, 29], [21, 22, 23]]]], [33, 34, 35]]]], [33, 34, 35]]], [[[[24, 25, 26], [[[[12, 13, 14], [[[12, 13, 14], [27, 28, 29]], [18, 19, 20]], [18, 19, 20]], [[30, 31, 32], [[36, 37, 38], [[36, 37, 38], [33, 34, 35]]], [42, 43, 44]]], [42, 43, 44]]], [[[36, 37, 38], [[[15, 16, 17], [[[15, 16, 17], [39, 40, 41]], [21, 22, 23]], [21, 22, 23]], [[42, 43, 44], [[39, 40, 41], [[39, 40, 41], [45, 46, 47]]]]]] [45, 46, 47]]]]]] [45, 46, 47]]]] The above 3 steps are equivalent to: e = rearrange(a, \u0026quot;B C (nh H) (nw W) -\u0026gt; (B nh nw) H W, nh=2, nw=2)\n= 1 1 0 6 0 6 = 2 8 = = 1 1 1 7 1 7 = 3 9 = = 1 2 2 8 2 - 8 = 4 - 0 ‖ ‖ ‖ ‖ ‖ ‖ ‖ - = 1 - 2 2 3 3 - 9 = 5 - 1 4 0 = 1 = 1 2 2 3 4 0 = 6 2 5 1 = 1 = 1 2 2 3 5 1 = 7 3 6 3 3 9 4 1 2 3 = 3 4 0 4 0 = 6 2 = 5 1 2 3 = 3 4 1 5 1 = 7 3 = 2 3 2 3 = 3 4 7 3 6 - 2 = 8 - 4 ‖ ‖ ‖ ‖ ‖ ‖ ‖ 2 3 2 - 3 = 3 - 4 8 4 7 - 3 = 9 - 5 = 2 3 2 3 = 4 4 9 5 8 4 = 0 6 = 1 1 2 3 = 4 4 2 8 9 5 = 1 7 1 1 3 9 1 2 4 0 3 4 6 2 3 4 7 3 3 4 8 4 1 2 5 1 1 2 6 2 1 2 7 3 3 4 9 5 4 4 0 6 4 4 1 7 Restore feat maps .permute() changed .stride(), which can\u0026rsquo;t return to the structure that matches with the tensor\u0026rsquo;s original shape anymore.\nTherefore, .contiguous() is necessary before tensor .view() to the original size.\n1 2 e = d.view(1, 2, 2, 2, 2, 3) # (B, nh, nw, C, H, W) merge = e.permute(0, 3, 1, 4, 2, 5).contiguous().view(1, 2, 4, 6) # [B, C, H, W] Code from MatchNeRF.\nConvNeXt A ConvNet for the 2020s ~ 2022 FAIR CVPR arxiv\nModify ResNet50 according to Swin Transformer: 4\nStages: [3, 4, 6, 3] ➡ [3, 3, 9, 3] (Tiny) ³ Stem: First conv1 (kernel=7x7, stride=2, pad=3) and maxpool (stride=2) ➡ Conv2dNormActivation (kernel=4x4, stride=4) Depthwise Conv: groups=1 ➡ groups=input_channels Expand Input Chanls: Stage 04=(64,256,512,1024,2048) ➡ Stage 04=(96,192,384,768) Expand Middle Chanls: Bottleneck (256➞64➞64➞256) ➡ Inverted Bottleneck (96➞384➞384➞96) Conv First: fc + conv + fc ➡ conv + fc + fc. Because in a transformer block, attention is ahead of fc. Large Kernel: 3 ➡ 7 Activation: ReLU ➡ GELU Fewer Activation: After each Conv2d ➡ After 1st 1x1 conv Fewer Norms: After each Conv2d ➡ After 1st 7x7 conv Norms: BatchNorm ➡ LayerNorm Downsample Layer: Conv(stride=2) ➡ LayerNorm + Conv2d(k=2,s=2) A ConvNext block mimics a transformer block: attention + feedforward (MLP), so Multi-Head Self-Attention corresponds to Depthwise Conv, and feedforward corresponds to 1x1 conv + activation. 6\nNetwork Architecture:\nCode flowchart from 4\nCodes: torchvision | csdn-AI浩 | official\nReference ConvNeXt实战：使用ConvNeXt实现植物幼苗分类（自创，非官方）- AI浩 细品EfficientNet - 沈景兵的文章 - 知乎 ConvNext | Less is More (Found this under the Images section of DDG with searching \u0026ldquo;convnext model\u0026rdquo;)\nConvNeXt网络详解 - 太阳花的小绿豆 - CSDN Swin-Transformer网络结构详解 - 太阳花的小绿豆 - csdn A Basic Introduction to Separable Convolutions - Medium Vision Transformer详解 - 太阳花的小绿豆 - csdn ","date":"2023-07-25T16:00:00Z","image":"https://pic2.zhimg.com/80/v2-35966819d100014901f3c819b7252c65_720w.webp","permalink":"http://blog.zichen.uk/post/writenotes/model/misc/c-symp-vision_champions/","title":"sum: Champions on Vision"},{"content":"Source video: PyTorch Hooks Explained - In-depth Tutorial - Elliot Waite\nHooks for tensor 1 a.register_hook(hook_func) This will add a property _backward_hooks for the tensor a. And hooks for tensors only take effect when back-propagating (when the gradient is calculated).\nhook_func can be normal function or a lambda function, which takes as input the gradient grad for this tensor a coming from the last node, and pass the current gradient to the later backwards graph.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def hook_func(grad: Tensor): print(grad) return grad + 1 # the grad passed to the next operation increased by 1. a.register_hook(hook_func) # Second hook function a.register_hook(lambda grad: print(grad)) # Third hook function: changed grad a.register_hook(lambda grad: grad * 100) # Fourth hook function: save gradient for an intermediate node a.retain_grad() _backward_hooks is an OrderDict so it can contain multiple functions, and they\u0026rsquo;ll be executed according to their definition sequence.\nRegistering a hook for an intermediate node tensor will notify the corresponding tensor in the backwards graph.\nInside the associated node on the backwards graph, there will be an additional property: pre_hooks list, which will call the hook property a._backward_hooks of that tensor, before the grad getting into the method backward.\nSo the hook will be executed ahead of backward property during the back-propagating. backward will use the gradients returned from the hooks.\nHowever, when setting a hook for a leaf node, the hook function will only add the hook func into the _backward_hooks OrderDict of that leaf node.\nAnd the associated AccumulateGrad node of that leaf node will check if the leaf node has hook function needed to be executed before assigning grad from previous calculations.\nEach hook function has a handle index, which will be returned after executing the hook function, e.g., h = c.register_hook(hook_func) A hook can be removed via the handle index: h.remove()\nCaveat: Change grad in-place in the hook functions may affect other tensors\u0026rsquo; gradients, so later backward pass will be mess up.\nFor example, as for the grad_fn of operation e = c+d, the output gradients for tensor c and d are supposed to be the same. If the grad of d has changed, the grad of c will also changed.\ne.g. Gradient clipper ¹ Clamp the gradient of each tensor in a certain range by registering a hook for each parameter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torchvision import models def gradient_clipper(model: nn.Module, val: float) -\u0026gt; nn.Module: for parameter in model.parameters(): # in-place changing the gradient parameter.register_hook(lambda grad: grad.clamp_(-val, val)) return model resnet_clipped = gradient_clipper(models.resnet50(), val=0.01) dummy_input = torch.ones(1, 3, 224, 224) pred = resnet_clipped(dummy_input) loss = pred.log().mean() loss.backward() print(resnet_clipped.fc.bias.grad[:25]) Hooks for modules Hooks registered for modules can be automatically triggered before or after a nn.module.forward is called (even if a layer), so a hook can modify the input and output tensors of a nn.module\nHooks before forward register_forward_pre_hook(hook_func), where the hook_func can access the module and its positional input.\n1 2 3 4 5 def hook_func_pre_forward(module: nn.Module, inputs: Tensor): a, b = inputs return a+2, b myModel.register_forward_pre_hook(hook_func_pre_forward) Hooks after forward: register_forward_hook(hook_func), where the hook_func will recieve 3 arguments: the module, its input, and its output.\n1 2 3 4 def hook_func_forward(module: nn.Module, inputs: Tensor, output: Tensor): return output + 10 myModel.register_forward_hook(hook_func_forward) Hooks after backward: register_backward_hook() has been deprecated in favor of register_full_backward_hook()\ne.g. Inspect a model ¹ Printing the shape of output tensors after each layer by registering a hook for each layer in an external wrapper, rather than adding print inside the model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch from torch import nn, Tensor from torchvision import models class VerboseExecution(nn.Module): def __init__(self, model: nn.Module): super().__init__() self.model = model # Register a hook for each layer for name, module in self.model.named_children(): # conv1, bn1, relu, maxpool, layer1, ... module.__name__ = name module.register_forward_hook(self.print_shape()) def print_shape(self): def hook_func(module, inputs, output): print(f\u0026#34;{module.__name__}: {output.shape}\u0026#34;) return hook_func def forward(self, x: Tensor) -\u0026gt; Tensor: return self.model(x) # Print intermediate shape in ResNet50 resnet_verbose = VerboseExecution(models.resnet50()) dummy_input = torch.ones(1, 3, 224, 224) _ = resnet_verbose(dummy_input) e.g. Extract feature maps ¹ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import torch from torch import nn, Tensor from torchvision import models from typing import Dict, Iterable, Callable class FeatureExtractor(nn.Module): def __init__(self, model: nn.Module, layer_ls: Iterable[str]): super().__init__() self.model = model self.layers_ex = layer_ls # Define a dict to store feature maps self._features = {layer: torch.empty(0) for layer in layer_ls} for name in layer_ls: # Pick out the selected layers by their names from a dictionary layer = dict([*self.model.named_modules()])[name] # Register a hook for each layer layer.register_forward_hook(self.save_outputs(name)) def save_outputs(self, layer_name: str) -\u0026gt; Callable: def hook_func(module, inputs, output): self._features[layer_name] = output return hook_func def forward(self, x: Tensor) -\u0026gt; Dict[str, Tensor]: _ = self.model(x) return self._features # Extract feature maps at each level before \u0026#34;avgpool\u0026#34; and \u0026#34;fc\u0026#34; resnet50 = models.resnet50() resnet_features = FeatureExtractor( resnet50, layer_ls = list(resnet50._modules)[:-2] ) dummy_input = torch.ones(1, 3, 224, 224) feature_maps = resnet_features(dummy_input) print({name: output.shape for name, output in feature_maps.items()}) Reference How to Use PyTorch Hooks ","date":"2023-07-22T15:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-hooks/","title":"memo: PyTorch | Hooks"},{"content":"How to extract features of an image from a trained model - PyTorch Forum\nHow can l load my best model as a feature extractor/evaluator?\nLoad and call torchvision.models | ResNet50 Docs\n1 2 3 4 5 6 7 8 9 10 11 import torch from torch import nn from torchvision import models, transforms import PIL resnet50 = models.resnet50(weights=\u0026#39;DEFAULT\u0026#39;) resnet50.eval() im_tensor = transforms.ToTensor()(PIL.Image.open(\u0026#39;data/nerf_llff_data/fern/images_4/image000.png\u0026#39;)) output = resnet50(im_tensor[None, ...]) Or using the weights object:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from torchvision import models from torchvision.io import read_image im = read_image(\u0026#34;data/nerf_llff_data/fern/images_4/image000.png\u0026#34;) # Step 1: Initialize model with the best available weights weights = models.ResNet50_Weights.DEFAULT model = models.resnet50(weights=weights) model.eval() # Step 2: Initialize the inference transforms preprocess = weights.transforms() # Step 3: Apply inference preprocessing transforms batch = preprocess(im).unsqueeze(0) # Step 4: Use the model and print the predicted category prediction = model(batch).squeeze(0).softmax(0) category_id = prediction.argmax().item() score = prediction[category_id].item() category_name = weights.meta[\u0026#34;categories\u0026#34;][category_id] print(f\u0026#34;{category_name}: {100*score:.1f}%\u0026#34;) triceratops: 11.5%\nBut the prediction seems not to be accurate.\nAdjust image input example\n1 2 3 4 5 6 # Initialize the Weight Transforms weights = ResNet50_Weights.DEFAULT preprocess = weights.transforms() # Apply it to the input image img_transformed = preprocess(img) (2023-07-23)\nInspect model\u0026rsquo;s modules Use hook to print layer name and the shape of their outputs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import torch from torch import nn, Tensor from torchvision import models class InspectModel(nn.Module): def __init__(self, model: nn.Module): super().__init__() self.model = model self.hook_handles = [] # print layers and their outputs\u0026#39; shape for name, module in self.model.named_children(): module.__name__ = name handle = module.register_forward_hook( lambda module, inputs, output: print(f\u0026#34;{module.__name__}: {output.shape}; Op: {module._get_name()}\u0026#34;)) self.hook_handles.append(handle) def forward(self, x: Tensor): self.model(x) for handle in self.hook_handles: handle.remove() Another ugly way is using a for loop:\n1 2 for name, module in self._modules.items(): print(name) Extract \u0026amp; Intrpl feature maps Retrieve specified layers\u0026rsquo;s feature maps and interpolate them to the same size.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import torch from torch import nn, Tensor from torchvision import models from typing import Callable, Iterable, Tuple import torch.nn.functional as F class ExtractIntrplFeatures(nn.Module): def __init__(self, model: nn.Module, layer_names: Iterable[str] = None, chnl_dim: int = 1) -\u0026gt; None: super().__init__() self.model = model self.layer_names = layer_names self._features = {} # Register a hook for each layer if layer_names is None: layer_names = list(model._modules) for layerName in layer_names: layer = dict([*self.model.named_children()])[layerName] layer.register_forward_hook(self.save_features( layerName, chnl_dim)) def save_features(self, layerName, chnl_dim) -\u0026gt; Callable: # print(layerName) def hook_func(module, inputs, output): if chnl_dim != 1: perm_order = list(range(len(output.shape))) perm_order.remove(chnl_dim) perm_order = [0, chnl_dim] + perm_order[1:] output = output.permute(*perm_order) self._features[layerName] = output return hook_func def forward(self, x: Tensor, size: Tuple[int] = None) -\u0026gt; Tensor: self.model(x) # Interpolate to the same size as the first conv feature for i in range(0, len(self._features)): layerName = list(self._features.keys())[i] if size is None: size = list(self._features.values())[0].shape[-2:] self._features[layerName] = F.interpolate( input=self._features[layerName], size=size, mode=\u0026#34;bilinear\u0026#34;, align_corners=True, ) return torch.cat(list(self._features.values()), dim=1) (2023-08-08)\nFeature extraction Get feature map at certain layers through create_feature_extractor() torchvision.models.feature_extraction — Torchvision 0.11.0 documentation I guess it cannot realize fine-tuning the pre-trained model.\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from torchvision import models from torchvision.models.feature_extraction import get_graph_node_names from torchvision.models.feature_extraction import create_feature_extractor model = models.resnet50( weights=\u0026#34;DEFAULT\u0026#34;) train_nodes, eval_nodes = get_graph_node_names(model) print(train_nodes) featExtractor = create_feature_extractor(model, return_nodes={\u0026#39;layer4.2.relu_2\u0026#39;:\u0026#39;layer4_feat\u0026#39;}) inp = torch.ones(2, 3,224,224) with torch.no_grad(): out = featExtractor(inp) # dict (2023-08-08)\n.modules vs ._modules convnext.modules is a method. Its output content is in a sepcific format.\nconvnext.modules() is a generator. Docs - nn.Module(); Docs - Modules\nHowever, if I traverse it like for _ in convnext.modules(): print(_), it will repeatly print all the modules in the model.\nAnd chatGPT answer:\nmodules() function also iterates through sub-modules of each module, resulting in duplicate prints. To avoid this, you can use the children(). Using children() will only give you the immediate sub-modules of the features module\n1 2 3 4 5 import torchvision.models as models convnext = models.convnext_tiny(weights=\u0026#39;DEFAULT\u0026#39;) for module in convnext.features.children(): print(module) convnext._modules is an OrderedDict\nlist(convnext._modules) only has the keys (name of the modules).\nTruncate pre-trained model Question for chatGPT:\n\u0026ldquo;Given a pre-trained multiple-layer neural network in PyTorch, how to run a part of it and stop at certain layer?\u0026rdquo;\nJust extract features up to a certain layer, without performing classification or regresion steps.\n1 2 3 4 5 6 7 8 # 1. Load model import torch.nn as nn from torchvision import models pre_model = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) # 2. Create a new model sliced_model = nn.Sequential(*list(pre_model.features.children())[:5]) sliced_model.eval() The new model will inherit the pre-trained weights.\nAlexNet Source code\nAlexNet.features contains 13 modules\n1 2 3 4 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) alexnet_debug = InspectModel(alexnet.features) dummy_input = torch.ones(1,3,224, 224) alexnet_debug(dummy_input) Get pre-logits vector Retrieve the feature vector before it gets compressed to 1_000 categories, i.e., removing the last Linear layer of \u0026lsquo;classifier\u0026rsquo; module in alexnet and making a new model\n1 2 3 4 5 6 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) # drop the last \u0026#39;linear layer\u0026#39; in classifier module new_classifier = nn.Sequential(*list(alexnet.classifier.children())[:-1]) alexnet.classifier = new_classifier Get feature maps Make a new model that stops at a certain feature map\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class AlexNetConv4(nn.Module): def __init__(self, original_alexnet): super().__init__() # stop at conv4 layer_list = list(original_alexnet.features.children())[:-3] self.features = nn.Sequential(*layer_list) def forward(self, x): x = self.features(x) return x model = AlexNetConv4(models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;)) dummy_input = torch.ones(1, 3, 224, 224) features = model(dummy_input) # (1, 256, 13, 13) Retrieve the feature maps after ReLU each time:\n1 2 3 4 5 6 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) alexnet_features = ExtractIntrplFeatures( alexnet.features, layer_names=[\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;11\u0026#39;] ) dummy_input = torch.ones(1, 3, 224, 224) feat_alexnet = alexnet_features(dummy_input) # (1, 1152, 55, 55) ResNet Get feat map of resnet34 Pixel-NeRF obtains the feature map by copying the forward method until the layer4 and concat feature maps of each layer along the channels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class SpatialEncoder(nn.Module): def __init__(self, backbone): super().__init__() self.model = getattr(torchvision.model, backbone)(weights=\u0026#39;DEFAULT\u0026#39;) # Modifying model here doesn\u0026#39;t affect forward method. self.model.avgpool = nn.Sequential() self.model.fc = nn.Sequential() def forward(self, img): # DTU img: (1, 3, H=300, W=400) ... x = self.model.conv1(x) # (B, 64, H/2, W/2) x = self.model.bn1(x) x = self.model.relu(x) latents = [x] # store feature maps after different times of convlution if self.num_layers \u0026gt; 1: if self.use_first_pool: x = self.model.maxpool(x) # (B, 64, H/4, W/4) x = self.model.layer1(x) # (B, 64, H/4, W/4) latents.append(x) if self.num_layers \u0026gt; 2: x = self.model.layer2(x) # (B, 128, H/8, W/8) latents.append(x) if self.num_layers \u0026gt; 3: x = self.model.layer3(x) # (B, 256, H/16, W/16) latents.append(x) if self.num_layers \u0026gt; 4: x = self.model.layer4(x) # (B, 512, H/32, W/32) latents.append(x) self.latents = latents align_corners = None if self.index_interp == \u0026#34;nearest \u0026#34; else True latent_sz = latents[0].shape[-2:] # (H/2, W/2) # expand the feature maps to the original size for i in range(len(latents)): latents[i] = F.interpolate( input=latents[i], size=latent_sz, # (H/2, W/2) mode=self.upsample_interp, # bilinear align_corners=align_corners, ) self.latent = torch.cat(latents, dim=1) # (B, 64+64+128+256, H/2, W/2) self.latent_scaling[0] = self.latent.shape[-1] # W, 200 self.latent_scaling[1] = self.latent.shape[-2] # H, 150 self.latent_scaling = self.latent_scaling / (self.latent_scaling - 1) * 2.0 # tensor([200., 150.]) -\u0026gt; tensor([2.0101, 2.0134]) return self.latent # (B, 512, H/2, W/2) There is a flatten step in the forward method (Source code), so even the avgpool and fc are canceled as self.model.fc = nn.Sequential(), the output after calling the modified model will still become a vector, but not a feature map (planes).\nThe solution is to create a model whose forward method doesn\u0026rsquo;t contain the torch.flatten operation.\n1 2 3 4 5 resnet34 = models.resnet34(weights=\u0026#39;DEFAULT\u0026#39;) resnet_feat = nn.Sequential(*list(resnet34.children())[:-2]) im_tensor = transforms.ToTensor()(PIL.Image.open(\u0026#39;path/to/png\u0026#39;))[None,...] resnet_feat(im_tensor) However, if want to keep the feature map at every level, the forward method has to be rewritten. hooks for modules can realize this.\n(2023-07-23)\nHook feat maps 1 2 3 4 5 6 7 8 resnet34 = models.resnet34(weights=\u0026#39;DEFAULT\u0026#39;) # save feature maps after: relu, layer1, layer2, layer3. resnet_features = ExtractIntrplFeatures( resnet34, [\u0026#34;relu\u0026#34;, *list(resnet34._modules)[4:-3]] ) im_tensor = transforms.ToTensor()( PIL.Image.open(\u0026#39;data/nerf_llff_data/fern/images_4/image000.png\u0026#39;)) feat = resnet_features(im_tensor.unsqueeze(0)) The result is identical to pixel-NeRF\u0026rsquo;s SpatialEncoder, which I copied its definition to a ipynb and instantiate it:\n1 2 3 pixelNeRFEncoder = SpatialEncoder() feature_maps = pixelNeRFEncoder(im_tensor.unsqueeze(0)) torch.eq(feat, feature_maps).detach().numpy().all() (2023-07-21)\nMobileNet v3 MobileNetV3 contains 3 components: features, avgpool, and classifier.\nThe MobileNetV3.features starts with a Conv2dNormActivation layer followed by 15 InvertResidual blocks, and ends with a Conv2dNormActivation layer.\nAn InvertResidual module is a nn.Sequential model of 3 Conv2dNormActivation() layers, corresponding to \u0026ldquo;expand\u0026rdquo;, \u0026ldquo;depthwise\u0026rdquo;, and \u0026ldquo;project\u0026rdquo;. Source code\nInspect mobilenet 1 2 3 4 mobilenetv3 = models.mobilenet_v3_large(weights=\u0026#39;DEFAULT\u0026#39;) mobilenet_inspect = InspectModel(mobilenetv3.features) dummy_input = torch.ones(1, 3, 224, 224) mobilenet_inspect(dummy_input) Hook feature maps 1 2 3 4 5 6 mobilenetv3 = models.mobilenet_v3_large(weights=\u0026#39;DEFAULT\u0026#39;) mobilenet_feats = ExtractIntrplFeatures( mobilenetv3.features, layer_names=list(mobilenetv3.features._modules) ) feat_mbnet = mobilenet_feats(dummy_input) # (1,1224,112,112) faster-rcnn Obj Detect Everything you need to know about TorchVision’s MobileNetV3 implementation - PyTorch blog\nDocs | Faster RCNN Code\nMobileNet feature maps \u0026ndash;\u0026gt; Feature Pyramid Network \u0026ndash;\u0026gt; FasterRCNN detector for locating bounding box\n1 2 3 4 high_res = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) high_res.eval() out = high_res([im_tensor[0]])[0] out.keys() 1 2 3 4 5 6 7 low_res = models.detection.fasterrcnn_mobilenet_v3_large_320_fpn( weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT) low_res.eval() x = [im_tensor, torch.rand(3, 500, 400)] predictions = low_res(x) print(predictions[0].keys()) lraspp segment Outputs are masks for different classes. I guess these masks may be not suitable to serve as feature maps.\n1 2 3 4 5 6 lraspp = models.segmentation.lraspp_mobilenet_v3_large(pretrained=True) # input: (B, C, H, W); output: (B, C\u0026#39;, H, W) output = lraspp(im_tensor) out = output[\u0026#39;out\u0026#39;].detach().numpy() plt.imshow(out[0][3]) deeplabv3 segment 1 2 deeplabv3 = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True) deeplabv3(im_tensor) (2023-07-22)\nReplace forward of predefined model How can I replace the forward method of a predefined torchvision model with my customized forward function?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import torchvision.models as models import torch def new_forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) return x # define a resnet instance resent = models.resnet18() # add new_forward function to the resnet instance as a class method bound_method = new_forward.__get__(resent, resent.__class__) setattr(resent, \u0026#39;forward\u0026#39;, bound_method) (2023-07-22)\nExtract \u0026amp; interpolate feature maps Question: I want to extract and interpolate all the features maps after each stage of convolution to the same size form a pytorch pretrained model, for example, mobilenet v3. Please give the code.\nBing chat Here is an example code that extracts and interpolates all the feature maps after each stage of convolution to the same size from a PyTorch pre-trained model, MobileNet V3.\nClick to expand the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torch import torch.nn as nn import torch.nn.functional as F from torchvision.models.mobilenetv3 import mobilenet_v3_large class FeatureExtractor(nn.Module): def __init__(self): super(FeatureExtractor, self).__init__() self.features = nn.ModuleList([ nn.Sequential( m.conv, m.bn, nn.ReLU(inplace=True), m.hs, m.se, m.conv_pw, m.bn_pw, nn.ReLU(inplace=True), m.hs_pw, ) for m in mobilenet_v3_large().features[:-1] ]) def forward(self, x): out = [] for f in self.features: x = f(x) out.append(x) return out model = FeatureExtractor() x = torch.randn(1, 3, 224, 224) out = model(x) This code creates a FeatureExtractor class that extracts all the feature maps after each stage of convolution to the same size from a PyTorch pre-trained model, MobileNet V3. The forward method takes an input tensor x and returns a list of feature maps. The out variable contains all the feature maps after each stage of convolution to the same size.\nI hope this helps! Let me know if you have any other questions.\nRun it:\nAttributeError: \u0026lsquo;Conv2dNormActivation\u0026rsquo; object has no attribute \u0026lsquo;conv\u0026rsquo;\nChatGPT To extract and interpolate all the feature maps after each stage of convolution in a PyTorch pre-trained model like MobileNetV3, you can use the intermediate_layers functionality to access intermediate outputs and the nn.Upsample module for interpolation. Here\u0026rsquo;s a step-by-step guide on how to achieve this:\nLoad the pre-trained MobileNetV3 model. Modify the model to store intermediate feature maps during forward pass. Use the nn.Upsample module to interpolate the feature maps to a common size. First, make sure you have PyTorch and torchvision installed: pip install torch torchvision\nNow, let\u0026rsquo;s create the code:\nClick to expand the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import torch import torch.nn as nn import torchvision.models as models class MobileNetV3WithIntermediateOutputs(nn.Module): def __init__(self, model_name=\u0026#39;mobilenet_v3_large\u0026#39;, pretrained=True): super(MobileNetV3WithIntermediateOutputs, self).__init__() self.model = models.__dict__[model_name](pretrained=pretrained) # Register hooks to save intermediate outputs self.intermediate_outputs = [] self.hook_handles = [] for module in self.model.modules(): handle = module.register_forward_hook(self.hook_fn) self.hook_handles.append(handle) def hook_fn(self, module, input, output): self.intermediate_outputs.append(output) def forward(self, x): self.intermediate_outputs = [] # Clear previous intermediate outputs _ = self.model(x) return self.intermediate_outputs def interpolate_to_common_size(feature_maps, target_size): _, _, H, W = feature_maps[-1].size() if (H, W) == target_size: return feature_maps interpolated_feature_maps = [] for fmap in feature_maps: interpolated_feature_maps.append(nn.functional.interpolate(fmap, size=target_size, mode=\u0026#39;bilinear\u0026#39;, align_corners=False)) return interpolated_feature_maps # Example usage: model_name = \u0026#39;mobilenet_v3_large\u0026#39; input_size = 224 target_size = (56, 56) # Specify the target size you want to interpolate the feature maps to # Load the MobileNetV3 model with intermediate outputs model = MobileNetV3WithIntermediateOutputs(model_name=model_name, pretrained=True) # Set the model to evaluation mode model.eval() # Create a random input tensor (batch size 1, 3 channels, height=input_size, width=input_size) input_tensor = torch.rand(1, 3, input_size, input_size) # Get the intermediate feature maps intermediate_feature_maps = model(input_tensor) # Interpolate the feature maps to the common size interpolated_feature_maps = interpolate_to_common_size(intermediate_feature_maps, target_size) # The \u0026#39;interpolated_feature_maps\u0026#39; now contain all the intermediate feature maps, each resized to the \u0026#39;target_size\u0026#39;. In the code above, we create a MobileNetV3WithIntermediateOutputs class that inherits from nn.Module. This class will hold the pre-trained MobileNetV3 model and includes a hook function to store intermediate feature maps during forward pass.\nThe interpolate_to_common_size function takes a list of feature maps and resizes each one to the specified target_size using bilinear interpolation. If the size of the feature maps already matches the target size, they are left unchanged.\nFinally, we load the MobileNetV3 model with intermediate outputs, perform a forward pass with a random input tensor, and then interpolate all the intermediate feature maps to the common size specified by target_size.\nPlease note that the example assumes you want to interpolate the feature maps to a target size of (56, 56). You can modify the target_size to your desired dimensions. Additionally, you can change the model_name parameter to use other versions of MobileNetV3, such as \u0026lsquo;mobilenet_v3_small\u0026rsquo;, etc.\nRun it:\n1 2 3 4 5 6 7 /home/zichen/Downloads/GNT-comments/temptest.ipynb Cell 4 in interpolate_to_common_size(feature_maps, target_size) 25 def interpolate_to_common_size(feature_maps, target_size): ---\u0026gt; 26 _, _, H, W = feature_maps[-1].size() 27 if (H, W) == target_size: 28 return feature_maps ValueError: not enough values to unpack (expected 4, got 2) It used hook, but it registered a hook for each module. But it didn\u0026rsquo;t stop before torch.flatten when forwarding, so the last feature_maps is a vector. Maybe that\u0026rsquo;s my bad in prompting.\n(2023-07-24)\nConvNeXt-tiny Print the shape of output after each layer:\n1 2 3 4 convnext = models.convnext_tiny(weights=\u0026#39;DEFAULT\u0026#39;) convnext_inspect = InspectModel(convnext) dummy_input = torch.ones(1,3,754,1008) convnext_inspect(dummy_input) Extract specified feature maps and interpolate them to specified size:\n1 2 3 convnext_features = ExtractIntrplFeatures(convnext.features, [\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;]) H_maps, W_maps = dummy_input.size()[-2:] features = convnext_features(dummy_input, (H_maps, W_maps)) # (1,192,754,1008) ","date":"2023-07-18T19:25:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_pretrained_models/","title":"memo: PyTorch | Pre-trained models"},{"content":" Source Video: Diffusion Models | Paper Explanation | Math Explained - Outlier Code: dome272/Diffusion-Models-pytorch (2023-08-02)\nIdea \u0026amp; Theory Diffusion model is a generative model, so it learns the distribution of data 𝐗. (Discrimitive model learns labels. And MLE is a strategy to determine the distribution through parameters 𝚯)\nThe essential idea is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. [1]\nForward diffusion process:\nSample noise from a normal distribution¹ and add it to an image iteratively, until the original distribution of the image has been completely destroyed, becoming the same as the noise distribution.\nThe noise level (mean, variance) of each timestep is scaled by a schedule to avoid the variance explosion along with adding more noise.\nThe image distribution should be destroyed slowly, and the noise redundency at the end stage should be reduced.\nOpenAI ³ proposed Cosine schedule in favor of the Linear schedule in DDPM².\nReverse diffusion process:\nPredict the noise of each step.\nDo not predict the full image in one-shot because that\u0026rsquo;s intractable and results in worse results¹.\nPredicting the mean of the noise distribution and predicting the noise in the image directly are equivalent, just being parameterized differently ².\nPredict noise directly, so it can be subtracted from image.\nThe variance σ² of the normal distribution followed by the noise can be fixed². But optimizing it together with the mean, the log-likehood will get improved ³.\nArchitecture DDPM used UNet like model:\nMulti-level downsample + resnet block ➔ Low-res feature maps ➔ Upsample to origianl size Concatenate RHS feature maps with the LHS feature maps of the same resolution to supplement the location information for features at each pixel. Attend some of LHS and RHS feature maps by attention blocks to fuse features further. Time embedding is added to each level of feature maps for \u0026ldquo;identifying\u0026rdquo; the consistent amount of noise to be predicted during a forward pass at a timestep. OpenAI 2nd paper(4) made improvement by modifying the model in:\nIncrease levels, reduce width; More attention blocks, more heads; BigGAN residual block when downsampling and upsampling; Adaptive Group Normalization after each resnet block: GroupNorm + affine transform ( Time embedding * GN + Label embedding) Classifier guidance is a separate classifier that helps to generate images of a certain category. Math Derivation (2024-04-17)\nVAE and diffuseion model both follows MLE strategy to find the parameter corresponding to the desired data distribution. VAE solves the dataset distribution P(𝐗) by approximating ELBO; While diffusion model solves the dataset distribution P(𝐗) by minimizing the KL Divergence. (2023-08-04)\nVAE VAE also wants to get the distribution of dataset P(𝐗), and an 𝐱 is generated by a latent variable 𝐳.\nTherefore, based on Bayes theorem, p(𝐱) = p(𝐳) p(𝐱|𝐳) / p(𝐳|𝐱), where p(𝐳) is the prior.\nAnd p(𝐳|𝐱) is intractable because in p(𝐳|𝐱) = p(𝐳)p(𝐱|𝐳) / p(𝐱), the p(𝐱) can\u0026rsquo;t be computed through ∫f(𝐱,𝐳)d𝐳 since 𝐳 is high dimensional continuous.\nBy introducing an approximated posterior q(𝐳|𝐱), log p(𝐱) = ELBO + KL-divergence.\n$log p(𝐱) = E_{q(𝐳|𝐱)} log \\frac{p(𝐱,𝐳)}{q(𝐳|𝐱)} + ∫ q(𝐳|𝐱) log \\frac{q(𝐳|𝐱)}{p(𝐳)} d𝐳$\nThe KL-divergence can be integrated analytically.\nELBO is an expectation w.r.t q(𝐳|𝐱), which can technically be estimated using Monte Carlo sampling directly.\nBut when sampled q(𝐳ⁱ|𝐱) is around 0, the variance of $log\\ q(𝐳|𝐱)$ would be high and make its gradint unstable, then cause the optimization difficult. And the function to be estimated $log(\\frac{p_θ(𝐱,𝐳)}{q_φ(𝐳|𝐱)})$ involves two approximate models containing a lot error.\nThus, it\u0026rsquo;s not feasible to approximate ELBO directly.\nTo approximate ELBO, we analyse the generative model (Decoder) p(𝐱|𝐳).\nBase on Bayes theorem, p(𝐱|𝐳) = p(𝐱) p(𝐳|𝐱)/p(𝐳).\nBy introducing the posterior approximation q(𝐳|𝐱), p(𝐱|𝐳) can derive: E(log p(𝐱|𝐳)) = ELBO + KL-divergence, i.e.,\n$E_{q(𝐳|𝐱)}[ log p(𝐱|𝐳)] = E_{q(𝐳|𝐱)}[ log(\\frac{p(𝐳|𝐱) p(𝐱)}{q(𝐳|𝐱)})] + ∫ q(𝐳|𝐱) log \\frac{q(𝐳|𝐱)}{p(𝐳)} d𝐳$\nGiven 𝐳, the likelihood p(𝐱|𝐳) is supposed to be maximized. (The probiblity that the real 𝐱 is sampled should be maximum.)\nTherefore, the parameters θ of generative model p(𝐱|𝐳) should be optimized via MLE (cross-entropy) loss.\nNow since ELBO = E(log p(𝐱|𝐳)) - KL-divergence and KL-div is known, ELBO will be obtained by just computing E(log p(𝐱|𝐳)).\nE(log p(𝐱|𝐳)) can be estimated by MC: sample a 𝐳 then compute log p(𝐱|𝐳), and repeat N times, take average.\nThe approximated E(log p(𝐱|𝐳)) should be close to the original 𝐱, so there is a MSE loss to optimize the parameters ϕ of the distribution of 𝐳.\n(2023-10-30) 𝐳\u0026rsquo;s distribution needs to be learned as well for sampling 𝐱. But MC sampling is not differentiable, so ϕ cannot be optimized through gradient descent.\nTherefore, reparameterization considers that 𝐳 comes from a differentiable determinstic transform of ε, a random noise, i.e., 𝐳 = μ + σε.\nThen, parameters (μ, σ²) of 𝐳\u0026rsquo;s distribution (Encoder) will be optimized by MSE.\nForward process The forward diffusion process is like the \u0026ldquo;Encoder\u0026rdquo; p(𝐳|𝐱) in VAE:\n$$q(𝐳|𝐱) ⇒ q(𝐱ₜ | 𝐱ₜ₋₁)$$The distribution of image 𝐱ₜ at timestep t is determined by the image 𝐱ₜ₋₁ at the previous timestep, where smaller t means less noise.\nSpecifically, 𝐱ₜ follows a normal distribution with a mean of $\\sqrt{1-βₜ}𝐱ₜ₋₁$ and a variance of $\\sqrt{βₜ}𝐈$:\n$$q(𝐱ₜ | 𝐱ₜ₋₁) = N(𝐱ₜ; \\sqrt{1-βₜ} 𝐱ₜ₋₁, \\sqrt{βₜ}𝐈)$$𝐱ₜ is similar to 𝐱ₜ₋₁ because its mean is around 𝐱ₜ₋₁.\nAn image 𝐱 is a \u0026ldquo;vector\u0026rdquo;, and each element of it is a pixel.\nAs timestep t increase, βₜ increases and (1-βₜ) decreases, which indicates the variance gets larger and the mean value gets smaller.\nIntuitively, the value of the original pixel xₜ₋₁ is fading and more pixels become outliers resulting in a wider range of variation around the mean.\nBy introducing a notation $α = 1-βₜ$, the t-step evolution from 𝐱₀ to 𝐱ₜ can be simplied to a single expression instead of sampling t times iteratively.\nReplace (1-βₜ) with α, the distribution becomes:\n$q(𝐱ₜ | 𝐱ₜ₋₁) = N(𝐱ₜ; \\sqrt{αₜ} 𝐱ₜ₋₁, (1-αₜ)𝐈)$\nBased on the reparameterization trick, a sample from the distribution is:\n$𝐱ₜ = \\sqrt{αₜ} 𝐱ₜ₋₁ + \\sqrt{1-αₜ} ε$\nSimilarly, $𝐱ₜ₋₁ = \\sqrt{αₜ₋₁} 𝐱ₜ₋₂ + \\sqrt{1-αₜ₋₁} ε$, and plug it into 𝐱ₜ.\nThen $𝐱ₜ = \\sqrt{αₜ₋₁} ( \\sqrt{αₜ} 𝐱ₜ₋₂ + \\sqrt{1-αₜ₋₁} ε ) + \\sqrt{1-αₜ} ε$. Now, the mean becomes $\\sqrt{αₜαₜ₋₁} 𝐱ₜ₋₂$\nGiven variance = 1 - (mean/𝐱)² in the above normal distribution $N(𝐱ₜ; \\sqrt{αₜ} 𝐱ₜ₋₁, (1-αₜ)𝐈)$, and here mean = $\\sqrt{αₜαₜ₋₁} 𝐱ₜ₋₂$,\nthe standard deviation should be $\\sqrt{1 - αₜαₜ₋₁}$, then 𝐱ₜ becomes:\n$𝐱ₜ = \\sqrt{αₜαₜ₋₁} 𝐱ₜ₋₂ + \\sqrt{1 - αₜαₜ₋₁} ε$\nRepeatedly substituting intermediate states, the 𝐱ₜ can be represented with 𝐱₀ :\n$𝐱ₜ = \\sqrt{αₜαₜ₋₁ ... α₁} 𝐱₀ + \\sqrt{1 - αₜαₜ₋₁ ... α₁} ε$\nDenote the cumulative product \u0026ldquo;αₜαₜ₋₁ \u0026hellip; α₁\u0026rdquo; as $\\bar aₜ$, the 𝐱ₜ can be reached in one-shot.\n$𝐱ₜ = \\sqrt{\\bar aₜ} 𝐱₀ + \\sqrt{1 - \\bar aₜ} ε$\nThe distribution of 𝐱ₜ given 𝐱₀ is:\n$q(𝐱ₜ | 𝐱₀) = N(𝐱ₜ; \\sqrt{\\bar aₜ} 𝐱₀, (1 - \\bar aₜ)𝐈)$\nWith this expression, the deterministic forward process is ready-to-use and only the reverse process needs to be learned by a network.\nThat\u0026rsquo;s why in the formula below, they \u0026ldquo;reverse\u0026rdquo; the forward q(𝐱ₜ|𝐱ₜ₋₁) to q(𝐱ₜ₋₁|𝐱ₜ) resulting in the equation only containing \u0026ldquo;reverse process\u0026rdquo;: 𝐱ₜ₋₁|𝐱ₜ, which then can be learned by narrowing the gap between q(𝐱ₜ₋₁|𝐱ₜ) and p(𝐱ₜ₋₁|𝐱ₜ). Reverse process The reverse diffusion process is like the Decoder in VAE.\n$$p(𝐱|𝐳) ⇒ p(𝐱ₜ₋₁𝐱ₜ₋₂..𝐱₀ | 𝐱ₜ)$$ Given a noise image 𝐱ₜ, the distribution of less-noise image 𝐱ₜ₋₁ is\n$p(𝐱ₜ₋₁ | 𝐱ₜ) = N(𝐱ₜ₋₁; μ_θ(𝐱ₜ, t), Σ_θ(𝐱ₜ, t))$\nwhere the variance can be a fixed schedule as βₜ, so only the mean $μ_θ(𝐱ₜ, t)$ needs to be learned with a network.\nVLB VLB is the loss to be minimized. VLB gets simplied by:\nApplying Bayes rule to \u0026ldquo;reverse\u0026rdquo; the direction of the forward process, which becomes \u0026ldquo;forward denoising\u0026rdquo; steps q(𝐱ₜ₋₁ | 𝐱ₜ), because it\u0026rsquo;s from a noise image to a less-noise image;\nAdding extra conditioning on 𝐱₀ for each \u0026ldquo;forward denosing\u0026rdquo; step q(𝐱ₜ₋₁ | 𝐱ₜ, 𝐱₀).\nDerivation by step:\nDiffusion model wants a set of parameter 𝛉 letting the likelihood of the original image 𝐱₀ maximum.\n$$\\rm θ = arg max_θ\\ log\\ p_θ(𝐱₀)$$With adding a minus sign, the objective turns to find the minimum:\n-log p(𝐱₀) = -ELBO - KL-divergence\n$$ \\begin{aligned} \u0026 -log p(𝐱₀) \\left( = -log \\frac{p(𝐱₀, 𝐳)}{p(𝐳|𝐱₀)} \\right) \\\\\\ \u0026= -log \\frac{p(𝐱_{1:T}, 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)} \\\\\\ \u0026 \\text{(Introduce \"approximate posterior\" q :)} \\\\\\ \u0026= -(log \\frac{ p(𝐱_{1:T}, 𝐱₀) }{ q(𝐱_{1:T} | 𝐱₀)} \\ + log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)}) ) \\\\\\ \\end{aligned} $$ Note that $q(𝐱_{1:T} | 𝐱₀)$ represents a joint distribution of N conditional distributions 𝐱ₜ and 𝐱ₜ₋₁.\nIt is the step-by-step design that makes training a network to learn the data distribution possible. Meanwhile, the sampling process also has to be step-by-step.\nCompute expection w.r.t. $q(𝐱_{1:T} | 𝐱₀)$ for both side.\n$$ E_{q(𝐱_{1:T} | 𝐱₀)} [ -log p(𝐱₀) ] \\\\\\ \\ = E_{q(𝐱_{1:T} | 𝐱₀)} \\left[-log \\frac{ p(𝐱_{0:T}) }{ q(𝐱_{1:T} | 𝐱₀)}\\right] \\ + E_{q(𝐱_{1:T} | 𝐱₀)} \\left[-log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right] $$Expectation is equivalent to integration.\n$$ \\begin{aligned} \u0026 \\text{LHS:} ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * (-log p(𝐱₀)) d𝐱_{1:T} = -log p(𝐱₀) \\\\\\ \u0026 \\text{RHS:} \\ = E_{q(𝐱_{1:T} | 𝐱₀)} \\left[-log \\frac{ p(𝐱_{0:T}) }{ q(𝐱_{1:T} | 𝐱₀)}\\right] \\\\\\ \u0026 + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * \\left(-log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right) d𝐱_{1:T} \\end{aligned} $$ Since KL-divergence is non-negative, there is:\n-log p(𝐱₀) ≤ -log p(𝐱₀) + KL-divergence =\n$$ \\begin{aligned} \u0026 -log p(𝐱₀) + D_{KL}( q(𝐱_{1:T} | 𝐱₀) || p(𝐱_{1:T} | 𝐱₀) ) \\\\\\ \u0026= -log p(𝐱₀) \\ + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * \\left(log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right) d𝐱_{1:T} \\end{aligned} $$ Break apart the denominator $p(𝐱_{1:T} | 𝐱₀)$ of the argument in the KL-divergence\u0026rsquo;s logarithm based on Bayes rule:\n$$p(𝐱_{1:T} | 𝐱₀) = \\frac{p(𝐱_{1:T}, 𝐱₀)}{p(𝐱₀)} = \\frac{p(𝐱_{0:T})}{p(𝐱₀)}$$Plug it back to KL-divergence:\n$$ \\begin{aligned} \u0026∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * \\left( log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right) d𝐱_{1:T} \\\\\\ \u0026= ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log (\\frac{q(𝐱_{1:T} | 𝐱₀) p(𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T} \\\\\\ \u0026= ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * [ log(p(𝐱₀) + log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})})] d𝐱_{1:T}\\\\\\ \u0026= ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log(p(𝐱₀) d𝐱_{1:T} \\\\\\ \u0026\\quad + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T} \\\\\\ \u0026= log p(𝐱₀) + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T} \\end{aligned} $$ Plug this decomposed KL-divergence into the above inequality, and the incomputable log-likelihood (-log p(𝐱₀)) can be canceled, resulting in the Variational Lower Bound (VLB):\n$$-log p(𝐱₀) ≤ ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀)\\ log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T}$$The argument of log is a ratio of the forward process and the reverse process.\nThe numerator is the distribution of $𝐱_{1:T}$ given the starting point 𝐱₀. To make the numerator and denominator have symmetric steps, the starting point of the reverse process $p(𝐱_T)$ can be separated out.\nSeparate out $p(𝐱_T)$ from the denominator by rewriting the conditional probability as a cumulative product:\n$$ p(𝐱_{0:T}) = p(𝐱_T) Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ) $$Plug it back into the logarithm of the VLB, and break the numerator joint distribution as a product of N-1 steps as well:\n$$ log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_T) Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ)}) \\= log \\frac{ Π_{t=1}^T q(𝐱ₜ|𝐱ₜ₋₁)}{ p(𝐱_T) Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ)} \\\\\\ \\= log \\frac{ Π_{t=1}^T q(𝐱ₜ|𝐱ₜ₋₁)}{ Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ)} - log p(𝐱_T) \\\\\\ \\= ∑_{t=1}^T log (\\frac{q(𝐱ₜ|𝐱ₜ₋₁)}{p(𝐱ₜ₋₁|𝐱ₜ)}) - log\\ p(𝐱_T) $$This form includes every step rather than only focusing on the distribution of the all events $𝐱_{1:T}$.\n(2023-08-11) DM wants the data distribution, but it doesn\u0026rsquo;t rebuild the distribution transformation directly from Gaussian to data distribution, but approachs the corruption process step-by-step to reduce the difficulty (variance).\nSeparate the first item (first step, t=1) from the summation, so that the other terms can be conditioned on 𝐱₀, thus reducing the variance:\n$$ log \\frac{q(𝐱₁|𝐱₀)}{p(𝐱₀|𝐱₁)} + ∑_{t=2}^T log (\\frac{q(𝐱ₜ|𝐱ₜ₋₁)}{p(𝐱ₜ₋₁|𝐱ₜ)}) - log\\ p(𝐱_T) $$ Reformulate the numerator $q(𝐱ₜ|𝐱ₜ₋₁)$ based on Bayes rule:\n$$ q(𝐱ₜ|𝐱ₜ₋₁) = \\frac{q(𝐱ₜ₋₁|𝐱ₜ)q(𝐱ₜ)}{q(𝐱ₜ₋₁)} $$In this form, forward adding noise $q$ and reverse denoising $p$ become the same process from 𝐱ₜ to 𝐱ₜ₋₁. Such that, in one pass, the model can both perform forward process and reverse process once.\nMake each step conditioned on 𝐱₀ to reduce the variance (uncertainty).\n$$ q(𝐱ₜ|𝐱ₜ₋₁) = \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)q(𝐱ₜ| 𝐱₀)}{q(𝐱ₜ₋₁| 𝐱₀)} $$And this distribution $q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)$ has a closed-form solution.\nHere is why the first step is separated out: If t=1, the $q(𝐱₁|𝐱₀)$ conditioned on 𝐱₀ is:\n$$ q(𝐱₁|𝐱₀) = \\frac{q(𝐱₀|𝐱₁, 𝐱₀)q(𝐱₁|𝐱₀)}{q(𝐱₀|𝐱₀)} $$There is a loop of $q(𝐱₁|𝐱₀)$ if 𝐱₀ exists, and other terms $q(𝐱₀|𝐱₁, 𝐱₀)$ and $q(𝐱₀|𝐱₀)$ don\u0026rsquo;t make sense.\nPlug the newly conditioned numerator back to the fraction, and break it apart based on log rule:\n$$ ∑_{t=2}^T log \\frac{q(𝐱ₜ|𝐱ₜ₋₁)}{p(𝐱ₜ₋₁|𝐱ₜ)} \\ = ∑_{t=2}^T log \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)q(𝐱ₜ| 𝐱₀)}{p(𝐱ₜ₋₁|𝐱ₜ)q(𝐱ₜ₋₁| 𝐱₀)} \\\\\\ \\ = ∑_{t=2}^T log \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)}{p(𝐱ₜ₋₁|𝐱ₜ)} + ∑_{t=2}^T log \\frac{q(𝐱ₜ| 𝐱₀)}{q(𝐱ₜ₋₁| 𝐱₀)} \\\\\\ $$The second term will be simplied to $log \\frac{q(𝐱_T| 𝐱₀)}{q(𝐱₁| 𝐱₀)}$\nThen, the variational lower bound becomes:\n$$ D_{KL}(q(𝐱_{1:T}|𝐱₀) || p(𝐱_{0:T})) = \\\\\\ log \\frac{q(𝐱₁|𝐱₀)}{p(𝐱₀|𝐱₁)} \\ + ∑_{t=2}^T log \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)}{p(𝐱ₜ₋₁|𝐱ₜ)} \\ + log \\frac{q(𝐱_T| 𝐱₀)}{q(𝐱₁| 𝐱₀)} \\ - log\\ p(𝐱_T) \\\\\\ \\ \\\\\\ \\ = ∑_{t=2}^Tlog \\frac{ q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) }{ p(𝐱ₜ₋₁|𝐱ₜ)} \\ + log \\frac{q(𝐱_T| 𝐱₀)}{p(𝐱₀|𝐱₁)} \\ - log\\ p(𝐱_T) \\\\\\ \\ \\\\\\ \\ = ∑_{t=2}^T log \\frac{ q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) }{ p(𝐱ₜ₋₁|𝐱ₜ)} \\ + log \\frac{q(𝐱_T| 𝐱₀)}{p(𝐱_T)} \\ - log p(𝐱₀|𝐱₁) $$Write this formula as KL-divergence, so that a concrete expression can be determined later.\nHow are those two fractions written as KL-divergence? $$ \\begin{aligned} \u0026 ∑_{t=2}^T D_{KL} (q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) || p(𝐱ₜ₋₁|𝐱ₜ)) \\\\\\ \u0026 + D_{KL} (q(𝐱_T| 𝐱₀) || p(𝐱_T)) \\\\\\ \u0026 - log\\ p(𝐱₀|𝐱₁) \\end{aligned} $$ Loss function The VLB to be minimized is eventually derived as a MSE loss function between the actual noise and the predicted noise.\n$D_{KL} (q(𝐱_T| 𝐱₀) || p(𝐱_T))$ can be ignored.\n$q(𝐱_T| 𝐱₀)$ has no learnable parameters because it just adds noise following a schedule. And $p(𝐱_T)$ is the noise image sampled from normal distribution. Since $q(𝐱_T| 𝐱₀)$ is the eventual image which is supposed to follow the normal distribution, this KL-divergence should be small. Then, the loss only contains the other two terms:\n$$L = ∑_{t=2}^T D_{KL} (q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) || p(𝐱ₜ₋₁|𝐱ₜ)) - log\\ p(𝐱₀|𝐱₁)$$ $D_{KL} (q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) || p(𝐱ₜ₋₁|𝐱ₜ))$ is the MSE between the actual noise and the predicted noise.\nFor the reverse pass, the distribution of the denoised image $p(𝐱ₜ₋₁|𝐱ₜ)$ has a parametric expression:\n$$p(𝐱ₜ₋₁|𝐱ₜ) = N(𝐱ₜ₋₁; μ_θ(𝐱ₜ,t), Σ_θ(𝐱ₜ,t)) \\\\\\ = N(𝐱ₜ₋₁; μ_θ(𝐱ₜ,t), β𝐈)$$where Σ is fixed as βₜ𝐈, and only the mean $μ_θ(𝐱ₜ,t)$ will be learned and represented by a network (output) through the MSE loss of noise as below.\nFor the (\u0026ldquo;reversed\u0026rdquo;) forward pass, the distribution of noise-added image $q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)$ has a closed-form solution, which can be written as a similar expression as p(𝐱ₜ₋₁|𝐱ₜ): What\u0026rsquo;s the derivation?\n$$ q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) = N(𝐱ₜ₋₁; \\tilde μₜ(𝐱ₜ,𝐱₀), \\tilde βₜ𝐈) \\\\\\ \\ \\\\\\ \\tilde βₜ = \\frac{1- \\bar αₜ₋₁}{1-\\bar αₜ} ⋅ βₜ \\\\\\ \\ \\\\\\ \\tilde μ(𝐱ₜ,𝐱₀) = \\frac{\\sqrt{αₜ} (1-\\bar αₜ₋₁) }{1-\\bar αₜ} 𝐱ₜ \\ + \\frac{\\sqrt{\\bar αₜ₋₁} βₜ}{1-\\bar αₜ} 𝐱₀ \\\\\\ \\ \\\\\\ \\rm Is there \\sqrt{αₜ} or \\sqrt{\\bar αₜ} ? $$where the $\\tilde βₜ$ is fixed, so only consider the $\\tilde μ(𝐱ₜ,𝐱₀)$, which can be simplified by the one-step forward process expression: $𝐱ₜ = \\sqrt{\\bar αₜ} 𝐱₀ + \\sqrt{1 - \\bar αₜ} ε$\n$$ 𝐱₀ = \\frac{𝐱ₜ - \\sqrt{1 - \\bar αₜ} ε}{\\sqrt{\\bar αₜ}} $$Plug 𝐱₀ into $\\tilde μ(𝐱ₜ,𝐱₀)$, then the mean of the noise-added image doesn\u0026rsquo;t depend on 𝐱₀ anymore:\n$$ \\begin{aligned} \\tilde μ(𝐱ₜ,𝐱₀) \u0026 = \\frac{\\sqrt{αₜ} (1-\\bar αₜ₋₁) }{1-\\bar αₜ} 𝐱ₜ \\ + \\frac{\\sqrt{\\bar αₜ₋₁} βₜ}{1-\\bar αₜ} \\ \\frac{𝐱ₜ - \\sqrt{1 - \\bar αₜ} ε}{\\sqrt{\\bar αₜ}} \\\\\\ \\ \\\\\\ \u0026 = ???\\ How \\ to \\ do? \\ ??? \\\\\\ \u0026 = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε) \\end{aligned} $$The mean of the distribution from which the noise-added image (𝐱ₜ,𝐱₀) at timestep t get sampled out is subtracting some random noise from image 𝐱ₜ.\n𝐱ₜ is known from the forward process schedule, and the $\\tilde μ(𝐱ₜ,𝐱₀)$ is the target for the network to optimize weights to make the predicted mean $μ_θ(𝐱ₜ,t)$ same as $\\tilde μ(𝐱ₜ,𝐱₀)$.\nSince network only output μ, the KL-divergence in the loss function can be simplified in favor of using MSE:\n$$ Lₜ = \\frac{1}{2σₜ²} \\\\| \\tilde μ(𝐱ₜ,𝐱₀) - μ_θ(𝐱ₜ,t) \\\\|² $$This MSE indicates that the noise-added image in the forward process and the noise-removed image in the reverse process should be as close as possible.\nSince the actual mean $\\tilde μ(𝐱ₜ,𝐱₀) = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε)$, where 𝐱ₜ is known, as it\u0026rsquo;s the input to the network. So the model is essentially estimating the actual $ε$ (random noise) every time.\nHence, the predicted mean $μ_θ(𝐱ₜ,t)$ by the model can be written in the same form as $\\tilde μ(𝐱ₜ,𝐱₀)$, where only the noise $ε_θ$ has parameters:\n$$μ_θ(𝐱ₜ,t) = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ((𝐱ₜ,t)))$$Therefore, the loss term becomes:\n$$ Lₜ = \\frac{1}{2σₜ²} \\\\| \\tilde μ(𝐱ₜ,𝐱₀) - μ_θ(𝐱ₜ,t) \\\\|² \\\\\\ \\ = \\frac{1}{2σₜ²} \\left\\\\| \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε) \\ - \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t)) \\right\\\\|² \\\\\\ \\ = \\frac{βₜ²}{2σₜ² αₜ (1-\\bar αₜ)} \\\\|ε - ε_θ(𝐱ₜ,t) \\\\|² $$Disregarding the scaling factor can bring better sampling quality and easier implementation, so the final loss for the KL-divergence is MSE between actual noise and predicted noise at time t:\n$$\\\\|ε - ε_θ(𝐱ₜ,t) \\\\|²$$ Once the mean $μ_θ(𝐱ₜ,t)$ has predicted out based on 𝐱ₜ and t, a \u0026ldquo;cleaner\u0026rdquo; image can be sampled from the distribution:\n$$ N(𝐱ₜ₋₁; μ_θ(𝐱ₜ,t), \\sigma_θ(𝐱ₜ,t)) = N(𝐱ₜ₋₁; \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t), βₜ𝐈) $$ By using reparameterization trick, this sampled image is: $$ 𝐱ₜ₋₁ = μ_θ(𝐱ₜ,t) + σε \\ = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t) + \\sqrt{βₜ}ε $$ The last term $log p(𝐱₀|𝐱₁)$ in the VLB is the predicted distribution for the original image 𝐱₀. Its goodness is measured by a probability that the original image $𝐱₀$ gets sampled from the estimated distribution $N(x; μ_θⁱ(𝐱₁,1), β₁)$.\nThe probability of an image should be a product of total D pixels. And the probability a pixel should be an integral over an interval [δ₋, δ₊] of the PDF curve:\n$$ p_θ(𝐱₀|𝐱₁) = ∏_{i=1}^D ∫_{δ₋(x₀ⁱ)}^{δ₊(x₀ⁱ)} N(x; μ_θⁱ(𝐱₁,1), β₁) dx $$ where $x₀$ is the pixel\u0026rsquo;s ground-truth. $N(x; μ_θⁱ(𝐱₁,1), β₁)$ is the distribution to be integrated. This interval is determined based on the actual pixel value as:\n$$ δ₊(x) = \\begin{cases} ∞ \u0026 \\text{if x = 1} \\\\\\ x+\\frac{1}{255} \u0026 \\text{if x \u003c 1} \\end{cases}, \\quad δ₋(x) = \\begin{cases} -∞ \u0026 \\text{if x = -1} \\\\\\ x-\\frac{1}{255} \u0026 \\text{if x \u003e -1} \\end{cases} $$ The original pixel range [0,255] has been normalized to [-1, 1] to align with the standard normal distribution $p(x_T) \\sim N(0,1)$\nIf the actual value is 1, the integral upper bound in the distribution is ∞, and the lower bound is 1-1/255 = 0.996, the width of the interval is from 0.996 to infinity.\nIf the actual value is 0.5, the upper bound is 0.5+1/255, and the lower bound is 0.5-1/255, the width of the interval is 2/255.\npic: area of the true pixel region in two predicted distributions.\nIf the area around the actual pixel value under the predicted distribution PDF curve is large, the predicted distribution is good. Howerver, if the area around real pixel value is small, the estimated mean is wrongly located.\nHence, this probability (log-likelihood) should be maximized, and by condering the minus sign in front of it, the corresponding loss term comes.\nHowever, the authors got rid of this loss term $-log p(𝐱₀|𝐱₁)$ when training the network. And the consequense is at inference time, the final step from 𝐱₁ to 𝐱₀ doesn\u0026rsquo;t add noise, because this step wasn\u0026rsquo;t get optimized. Therefore, The difference from other sampling steps is that the predicted 𝐱₀ doesn\u0026rsquo;t plus random noise.\n$$ \\begin{aligned} \\text{t\u003e1:}\\quad 𝐱_{t-1} \u0026= \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t) + \\sqrt βₜ ε) \\\\\\ \\text{t=1:}\\quad 𝐱_{t-1} \u0026= \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t)) \\end{aligned} $$A simple reason is that we don\u0026rsquo;t want to add noise to the final denoised clear output image 𝐱₀. Otherwise, the generated image is low-quality.\nThe complete loss function is MSE:\n$$ \\begin{aligned} \\rm L_{simple} \u0026= E_{t,𝐱₀,ε} [ || ε - ε_θ(𝐱ₜ,t)||² ] \\\\\\ \u0026= E_{t,𝐱₀,ε} [ || ε - ε_θ( \\sqrt{\\bar aₜ} 𝐱₀ + \\sqrt{1 - \\bar aₜ} ε, t) ||² ] \\end{aligned} $$ t is sampled from a uniform distribution between 1 and t; 𝐱ₜ is the one-step forward process. Algorithms DDPM paper\nTraining a model:\n\\begin{algorithm} \\caption{Training} \\begin{algorithmic} \\REPEAT \\STATE Sample a t from U(0,T) \\STATE Select an input image 𝐱₀ from dataset \\STATE Sample a noise from N(0,𝐈) \\STATE Perform gradient descent with loss: \\\\\\\\ $||ε - ε_θ(\\sqrt{\\bar aₜ} 𝐱₀ + \\sqrt{1 - \\bar aₜ} ε, t)||²$ \\UNTIL{converge} \\end{algorithmic} \\end{algorithm} Sampling from the learned data distribution by means of reparameterization trick:\n\\begin{algorithm} \\caption{Sampling} \\begin{algorithmic} \\STATE Sample a noise image $𝐱_T \\sim N(0,𝐈)$ \\FOR{t = T:1} \\COMMENT{Remove noise step-by-step} \\IF{t=1} \\STATE ε=0 \\ELSE \\STATE ε ~ N(0,𝐈) \\ENDIF \\STATE $𝐱ₜ₋₁ = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{1-αₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t) + \\sqrt{σₜ}ε$ \\COMMENT{Reparam trick} \\ENDFOR \\RETURN 𝐱₀ \\end{algorithmic} \\end{algorithm} In this reparametrization formula change βₜ and $\\sqrt{βₜ}$ to 1-αₜ and σₜ, which are different from the above equation.\nTraining and Sampling share the common pipeline:\n𝐱 t ₜ ‖ U ε N ⋮ - e ε t _ θ ‖ ² ε _ θ ⋯ ▶ μ _ θ ( 𝐱 ₜ , t ) ⋯ ▶ 𝐱 ₜ ₋ ₁ Improvements Improvements from OpenAI\u0026rsquo;s 2021 papers.\nLearn a scale factor for interpolating the upper and lower bound to get a flexible variance:\n$$Σ_θ(xₜ,t) = exp(v\\ log βₜ +(1-v)\\ log(1- \\tilde{βₜ}))$$v is learned by adding an extra loss term $λ L_{VLB}$, and λ=0.001.\n$$L_{hybrid} = E_{t,𝐱₀,ε} [ || ε - ε_θ(𝐱ₜ,t)||² ] + λ L_{VLB}$$ Use cosine noise schedule $f(t)=cos(\\frac{t/T+s}{1+s}⋅π/2)²$ in favor of linear schedule.\nReference Deep Unsupervised Learning using Nonequilibrium Thermodynamics, 2015 Denoising Diffusion Probabilistic Models, 2020 Improved Denoising Diffusion Probabilistic Models, 2021 Feb Diffusion Models Beat GANs on Image Synthesis, 2021 May ","date":"2023-07-14T21:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/diffusion/d-vid-outlier/","title":"watch: Diffusion - Outlier | Explain 4 papers"},{"content":"Explicit Correspondence Matching for Generalizable Neural Radiance Fields\nCode | Arxiv\nNotes Idea Take the difference between reference image features of a 3D point as geometry prior.\n(2023-10-25) Method is like a simplified GPNR: feature differences between views measued by cosine similarity (dot product). However, GPNR is more ultimate, while MatchNeRF only computes differences for each two views and differences are taken averaged as the input feature.\n(2023-12-09) Image features alone cannot constrain geometry of the unseen scenes well. In contrast, MVSNet preset depths explicitly.\nAnd MVSNet used the variance of features to measure the depth misalignment among all feature maps, whereas MatchNeRF used differences of each 2 feature maps.\nPipeline CNN extract image feature (1/8 reso), which will be upsampled to (1/4 reso), for each reference view.\nSelect a pair of reference views and Mix their feature maps by cross-attention (GMFlow).\nProject a 3D point onto this pair of interacted feature maps.\nMeasuring the difference of feature vectors by cosine similarity (dot prodcut)\nThe feature difference indicates whether the 3D point is at surface, so that it provides a geometry prior. Dot product of two feature vectors is a scalar, which will lost much information. So they divided the total channel into many groups to do \u0026ldquo;group dot product\u0026rdquo;. And concatenate the dot product of each group as a vector 𝐳.\nAlso, for the 1/4 feature map, there is a \u0026ldquo;dot-products\u0026rdquo; vector $\\\\^𝐳$ for a pair of reference views.\nGiven 𝑁 reference views, there are 𝑁(𝑁-1)/2 pairs of reference views, corresponding to 𝑁(𝑁-1)/2 \u0026ldquo;difference\u0026rdquo; vectors, which will merge together by taking their element-wise average as a single 𝐳\nThis \u0026ldquo;feature difference\u0026rdquo; vector 𝐳 (geometry prior) is fed along with the 3D point\u0026rsquo;s position and viewdir into decoder (MLP and ray-transformer), which regresses the color and volume density.\nExperiments Settings are following MVSNeRF.\nDatasets:\nStage Data Contents Resolution N_views Train DTU 88 scenes 512x640 49 Test DTU 16 scenes 3 Test NeRF real 8 scenes 640x960 4 Test Blender 8 scenes 800x800 4 Device: 16G-V100 Play (2023-08-24)\nCompare with GNT The architectures of Match-NeRF and GNT are similar.\n(2023-12-09) Overview: Souce images\u0026rsquo; features are extracted, mixed and regressed to rgbσ. Match-NeRF is trained only on DTU dataset, while GNT can be trained on multiple datasets (gnt_full).\nGNT merges multiple source views via subtract attention, while Match-NeRF fuses multi-view feature maps before getting into the model.\nMatch-NeRF mixes the entire feature maps for each two reference views, and then project 3D points onto the fused feature maps to index feature vectors.\nHowever, GNT directly mixes point\u0026rsquo;s feature vectors coming from each feature maps.\nDifferent training settings:\nHyper-params GNT MatchNeRF #rays for grad-descnt 2048 1024 #source views 8~10 3 1080Ti only supports --nerf.rand_rays_train=512 for MatchNeRF.\nThe opts.batch_size will be divided evenly to each gpu (self.opts.batch_size // len(self.opts.gpu_ids)), so bs (num of images)=1 cannot be split to multiple GPUs.\nAnd if setting bs=2, each card still have to process 1024 rays selected from an image.\nTesting with 1 1080Ti:python test.py --yaml=test --name=matchnerf_3v --nerf.rand_rays_test=10240\n(2023-09-27)\nCode Details GMFlow uses 6 transformer blocks consisting of self_attn and cross_attn for fusing windows, where the 1st and odd blocks perform window shift.\nMatchNeRF fully-finetuned the pre-trained GMFlow.\nDoes Inner product of a pair of features come from GMFlow?\n(2023-10-25) I guess it\u0026rsquo;s a simplified attention only for pairs, instead of among all views.\n(2023-12-09) Inferring geometry from the difference in high-dimensional features may have been present even earlier than MVSNet.\nSelf-attn and cross-attn for two samples data1 and data2 can be done in a single transformer block of GMFlow by concating 2 samples in the batch dimension twice in different order, i.e., source=[\u0026lsquo;data1\u0026rsquo;,\u0026lsquo;data2\u0026rsquo;] and target=[\u0026lsquo;data2\u0026rsquo;,\u0026lsquo;data1\u0026rsquo;].\nSuch that self-attn is performed on source and source. And cross-attn is source and target. If fused source returned after a block, the order requires reverse again to form the new target.\nviewdir didn\u0026rsquo;t perform positional embedding (same as PixelNeRF).\nRay transformer (MHA) in decoder mixes 16-dim feature vectors. (Unexpectedly tiny)\nTake the nearest views.\nCoordinates of 3D points are projected onto the image plane of the source view 0 to do positional embedding. Code\nThe encoder is supposed to provide the overall (geometry) prior, so they emphasized in the paper:\ndo not tune encoder for per-scene fine tuning\n","date":"2023-07-14T14:59:00Z","image":"https://i.imgur.com/rQN8Eg5.png","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-matchnerf/","title":"read: Render - NVS | Match-NeRF"},{"content":"Source video: 英伟达高俊: AI高质量三维内容生成（内容生成系列【一】） 北京智源大会2023 视觉与多模态大模型\nThe representation of 3D objects Implicit field is in favor of neural network, where it can be optimized by gradient. mesh can achieve real-time rendering and is handy for downstream creation, and good topology. Marching cube is not fully differentiable DMTet: A differentiable iso-surfacing is an implict field, and also a mesh.\nAn field where only the location at surface has value? a field only has one mesh? Diff-render 2D images supervise 3D generation 2D GAN advantages:\nvarious discriminator architecture powerful generator GAN3D\nThe latent codes of geometry and texture are sampled from 3D gaussian as prior 3D generator: Tri-plane consistute the implicit field. Get a mesh by DMTet from the generated geometry and texture, then render it to 2D image Use GAN to discriminate if the render is real and backward the gradient of loss Limitation: class label conditioned. One model can only can generate 1 category of objects. Text prompts generate 3D objects 2D diffusion used socre function to encourage high-fadality images score function needs a full image, but NeRF are trained batch-by-batch of rays, not a full image. Dream fusion can only render 64x64 images, so its geometry is low-quality. Coarse to fine: Use instant-ngp generate a rough geometry based on low-resolution diffusion model， then use DMTet convert the geometry to mesh; So that a highe-resolution image can be rendered, which can offer a strong gradient for fine geometry Future work a universal model can generate any category of objects. composite objects to form a scene dynamic objects ","date":"2023-07-13T18:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/shapes/%E9%AB%98%E8%B4%A8%E9%87%8F3d%E5%86%85%E5%AE%B9%E5%88%9B%E9%80%A0-%E9%AB%98%E4%BF%8A/","title":"watch: Jun Gao | ML for 3D content generation"},{"content":"Course page\nSampling Steps:\nSample a random noise image from normal distribution;\nUse the trained network to predict the noise as opposed to the meaningful object for one step;\nUse DDPM algorithm to compute noise-level scaling factors given a timestep: s1, s2, s3 = ddpm_scaling(t)\nSubtract the predicted noise from noise image and add extra noise, sample = s1 * (sample - s2 * predicted_noise) + s3 * extra_noise\nRepeat steps 2 to 4 to remove noise progressively.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 samples = torch.randn(N_imgs, 3, height, height) timesteps = 500 for i in range(timesteps, 0, -1): # reshape to match input image (N_imgs, 3, height, height) t = torch.tensor( [i/timesteps] )[:, None, None, None] # extra noise (except for the first step) z = torch.randn_like(samples) if i \u0026gt; 1 else 0 # predict noise eps = nn_model(samples, t) # remove noise and add extra noise samples = denoise_add_noise(samples, i, eps, z) Adding extra noise before removing noise in the next step avoids collapsing to the average thing of the training dataset.\nUNet UNet can output images of the same size as the input, and assign the image feature onto each pixel.\nCompress image for compact representation; Down sampling once, number of channel doubles Also UNet allows incorporating addtional information during the decoding period.\nEach time up-sampling, the sample is multiplied with context embeddings and plus time embeddings.\nTime embedding indicates timestep of the feature vector, so with that, the \u0026ldquo;time-dependent\u0026rdquo; noise level can be determined.\nContext embedding can be text description, so the UNet will be guided to generate specific output.\nf e ( h a 4 i t d u c d e h e r n n l m s a ) p s u a p m - p l e ( e e 2 C m m u o b T b p c n e i e 1 h t d m d n e d e d l x i i s n n ) g g ⨂ ⨁ s u a p m - p l e ( 1 u p c 2 h n l ) 1 2 3 4 5 # embed context and timestep cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1) temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1) up2 = self.up(cemb1 * up1 + temb1, down2) (2023-07-10)\nTraining Train the UNet to identify the noise that was applied to the image.\nUNet can segment image (classify each pixel), so here is it used to identify whether every pixel is noise or not?\nNo, it\u0026rsquo;s used to make each pixel carried with extracted or introduced features. Training steps:\nSample a random timestep (noise-level) to make noise; Add the known noise onto a random training image; UNet takes as input the noise image and predicts the applied noise as output; Loss is the difference between the true noise and the predicted noise 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for ep in range(n_epoch): for x in dataloader: # perturb data t = torch.randint(1, timesteps + 1, (x.shape[0],) ).to(device) noise = torch.randn_like(x) x_pert = perturb_input(x, t, noise) # use network to recover noise pred_noise = nn_model(x_pert, t / timesteps) # loss is MSE loss = F.mse_loss(pred_noise, noise) optim.zero_grad() loss.backward() optim.step() (2023-07-11)\nControling Use embedding vector to control the predicted noise.\nEmbedding is a vector (a set of numbers) that is a representation for something in another space.\nEmbeddings can perform arithmetic operations.\nParis : - France : + England : = London : Noise is what should be removed from the image.\nOnce the noise is fully subtracted out, what left is the generated image. By injecting context embeddings into decoder, the output feature vector of the predicted noise becomes specific for that given context.\nFor example, the noise corresponding to \u0026ldquo;A ripe avocado\u0026rdquo; is pixels that are not \u0026ldquo;A ripe avocado\u0026rdquo;, and they\u0026rsquo;ll be removed eventually.\n' a A v o r c i n i a m o p d g i e o ' ⨁ E C m o b n e U t d - e d N x i e n t g p n r o e i d l o s s And because the embedding vectors can be mixed, once the mixed noise is removed, what is left is the combination of two objects, i.e. the thing that the context embedding stands for.\nFor example, an embedding vector of \u0026ldquo;Avocado armchair\u0026rdquo; has the information of both \u0026ldquo;avocado\u0026rdquo; and \u0026ldquo;armchair\u0026rdquo;, so its context will lead the model to predict the noise that is neither \u0026ldquo;avocado\u0026rdquo; nor \u0026ldquo;armchair\u0026rdquo;.\n' a A r v m o c n c h o a a i d i o r ' C E U o m - n b N t e e e d t x d i n g s t n e o p i ⊖ i m g Context can be one-hot encoded vector for indicating categories, which will result in a specific class of images.\nSpeeding Up DDIM skips some timesteps, so it breaks Markov chain process, where each timestep is probablisticly dependent on the previous one.\nThere is a hyper-parameter step_size to decide how many timesteps are skipped.\nDDIM performs better than DDPM under 500 timesteps. The quality of images from DDIM may differs as opposed to DDPM.\nDenoising Diffusion implict model predicts a \u0026ldquo;rough sketch\u0026rdquo; of the final output, and then it refines it with the denoising process.\n","date":"2023-07-09T17:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/diffusion/d-vid-dlai/","title":"watch: DM - DLAI | How Diffusion Models Work"},{"content":"Code-matlab | Code-python | paper\nIntuitive from Shakil Goal Find the true matched keypoints and identify the outliers.\nAffinity matrix Measure the difference between any two edges in two graphs.\nG r a 1 2 p h - P G r a a b c p h - Q c is the outlier. Nodes can be SIFT feature points.\n$$W_{1a,2b} = 12 - ab$$The index of each element is a notation representing if i was a and if j was b.\nW 1a 1b 1c 2a 2b 2c 1a 0 1b 0 1c 0 2a 0 2b 0 2c 0 Random walk Iterations will not walk out of the area, i.e., the middle area has higher probability.\na b c 1 1 0 0 2 0 1 0 probability node 0.75 a 0.1 b 0.05 c 0.25 a 0.5 b 0.25 c The initial probility for each point is the same: 1/6:\nprobability node 0.16 a 0.16 b 0.16 c 0.16 a 0.16 b 0.16 c Associate graph (2023-07-09)\nAbstract Association graph consistitue of nodes representing correspondence between two graphs. Random walk select nodes to enforce real correspondence on the association graph. Introduction Previous work didn\u0026rsquo;t make a objective function\nIQP is NP hard, so its solution needs approximation.\nRandom walk view: rank the nodes on the association graph.\nPrevious methods\nTensor eigen decomposition Problem Each graph have a set of nodes (vertices) 𝐕, edges 𝐄, and attributes 𝐀.\nnode is local appearance of feature edge is geometrical relationship of two nodes. Matching two graphs is to find the correspondence of nodes in two graphs\nAffinity matrix 𝐖 is recording the compatibility of pairs of edges\ndiagonal is unary affinity, e.g. $𝐖_{ia;ia}$, between one correspondence and itself. non-diagonal $𝐖_{ia;jb}$ is the affinity of a pair of correspondence: $(vᵢᴾ,\\ vₐ^Q)$ and $(vⱼᴾ,\\ v_b^Q)$ W ia ib ic ja jb jc ia ib ic ja jb jc Each row is fixing one correspondence in one pair of correspondences, and changing the other correspondence.\nFor example, 1 corresponds to a is fixed, then the neighbor of 1 and a is changing:\nf i 1 1 x c h a 1 2 n g e f i a a a x c h a a b c n g e The correspondence assignment is stored in assignment matrix 𝐗;\nwhere 1 means matched correspondence, while 0 means the two nodes are not matched. x a b c i j Column vector 𝐱 is the reshape of matrix 𝐗 with length of $nᴾ×n^Q$\nx_ia x_ib x_ic x_ja x_jb x_jc Indicator vector 𝐱* is the target by maximizing the score of 𝐱ᵀ𝐖𝐱\nRandom Walks for Graph Matching Convert the affinity matrix to an association graph for random walk\nAssociation graph $G^{rw}$ is made up by nodes that represents a correspondence between $Gᴾ$ and $G^Q$.\nFor example, the correspondence $(vᵢᴾ,\\ vₐ^Q)$ is node $v_{ia}$ on the association graph.\nso the edge attributes are the elements of affinity matrix 𝐖 ;\nFor example, the edge $e_{ia; jb}$ on the association graph is the affinity $𝐖_{ia;jb}$\nRanking the nodes of association graph by random walk process\nAffinity preserving random walk Random walk process:\nA walker starts off with an arbitrary node and select as the next step one of its out-going edges based on the Markov transition kernel.\nInternet democracy:\nTotal vote that every webpage has is 1. This is realized by dividing the weight of its every out-going edge by the total number of its out-going edges.\nRow stochastic:\nOn to the association graph, its edges set 𝐖 is supposed to be normalized by 𝐃. That is each row is divided by the sum of the affinity values in that row. Then the normalized affinity matrix is row stochastic matrix 𝐏 = 𝐃⁻¹, where 𝐃 is a diagonal matrix.\nHowever, because there are outliers on the association graph, which are suppose to have small weight, the Internet democracy doesn\u0026rsquo;t suit here.\nOutliers on the association graph are the mismatched correspondence.\nFor example, the actual corespondences are $(v_1^P, v_1^Q)$, and $(v_2^P, v_2^Q)$, which are nodes $v_{11}$ and $v_{22}$ on the association graph.\nTherefore, on the association graph, the outliers are nodes other than $v_{11}$ and $v_{22}$, i.e., $v_{12},\\ v_{13},\\ v_{23},\\ v_{21}$\nG ^ 1 2 P 1 G ^ Q 2 3 A s s o 1 c 3 i a t i o n g r a p h ","date":"2023-07-06T00:00:00Z","image":"https://cv.snu.ac.kr/research/~RRWM/paper_teaser.jpg","permalink":"http://blog.zichen.uk/post/writenotes/model/misc/b-note-rrwm/","title":"read: RRWM"},{"content":"Source video: Lesson 3: Complex conjugates and dividing complex numbers - Khan Academy\nGiven a complex number\n$$ z = \\underset{↑}{2} + \\underset{↑}{\\underline{3}} i \\\\\\ \\quad \\quad Re(z) \\ Im(z) $$where $2$ is a real number, $3i$ is an imaginary number. The real part of z $Re(z)$ is 2, while the imaginary part of z $Im(z)$ is 3.\nComplex conjugate The complex conjugate of z is\n$$ \\bar z \\text{ or } z^* = 2 - 3i = \\overline{2+3i} $$where the real part Re(z*) remains the same, while the imaginary part Im(z*) has the opposite sign.\n- I b 0 b m a z z * R e This act like mirror reflecting over Real axis.\nThe sum of a complex number and its complex conjugate is two times of its real part, 2Re(z):\n$$ z + \\bar z = a + \\cancel bi + a - \\cancel bi = 2a = 2 Re(z) = 2 Re(\\bar z) $$Graphically, vector addition\n- I b 0 b m a z z * 2 ( a z + z * ) R e The product of a complex number and its complex conjugate is a real number, and it\u0026rsquo;s equal to the magnitude of the complex number squared:\n$$ z ⋅ \\bar z = (a+bi) ⋅ (a-bi) = a² - (bi)² = a² + b² = |z|² $$which is useful in the division of comple numbers: multiply the numerator and denominator by the conjugate of the denominator to convert the division to one complex number\nFor example:\n$$ \\begin{aligned} \u0026 \\frac{1+2i}{4-5i} \\\\\\ \u0026 = \\frac{1+2i}{4-5i} ⋅ \\frac{4+5i}{4+5i} \\\\\\ \u0026 = \\frac{(1+2i)(4+5i)}{4²-(5i)²} \\\\\\ \u0026 = \\frac{4+5i + 8i-10}{16+25} \\\\\\ \u0026 = \\frac{-6}{41} + \\frac{13i}{41} \\end{aligned} $$ Factoring sum of squares We can factor a difference of squares as:\n$$ x² - y² = (x-y)(x+y) $$But the sum of squares x² + y² cannot be factorized if without considering imaginary unit i.\n$$ \\begin{aligned} x² + y² \u0026 = x² - (-y²) \\\\\\ \u0026 = x² - (- 1 y²) \\\\\\ \u0026 = x² - (i² y²) \\\\\\ \u0026 = x² - (iy)² \\\\\\ \u0026 = (x-iy) (x+iy) \\end{aligned} $$ Modulus of complex value Source page Lesson 5\nThe absolute value (modulus) of a number is the distance away from zero.\nA complex number $3-4i$ plotted on the complex plane:\n4 I m 3 | 3 - 4 i | R e The absolute value of 3-4i is the hypotenuse of the right triangle.\nBased on the Pythagorean theorem, |3-4i|= √(3²+4²) = 5. (Because distance is positive, so only take the positive square root).\npolar \u0026amp; rectangular forms Source page: Lesson 5\nA complex number can be represented in two forms:\nRead number + imaginary number: a + bi Exponential form Both of them have the same diagram, and just are described in different coordinates:\nb I m r φ a z R e The two arguments of complex number z can be (r, φ) or (a,b)\nMagnitude: r = |z| = √(a²+b²)\nArgument (Polar angle): φ = arctan(b/a)\na = r⋅cosφ\nb = r⋅sinφ\nz = a+bi = r⋅cosφ + r⋅sinφi = r(cosφ + sinφ i) = $r e^{iφ}$\nwhere $cosφ + sinφ i = e^{iφ}$ can be derived by using Taylor series.\nMultiplying complex number Source page Lesson 7\nGiven z,\n3z has the same direction as z, but three times it\u0026rsquo;s magnitude;\n-3z is in the opposite direction, and has three times modulus of z\n-3iz = $1e^{iπ} ⋅ 3e^{i⋅π/2} ⋅ re^{iφ} = 3r e^{i(π+π/2+φ)}$ which turns into the opposite direction then rotates another 90 degrees in counter-clock wise.\nOr this can be derived from the loop of 1 -\u0026gt; i -\u0026gt; -1 -\u0026gt; -i with multiplying i each time.\nz⋅(-1-i) : the angle and modulus of $-1-i$ take effect on z separately.\nThe angle of -1-i is 225 degrees (observed from the complex plane), so it will rotate the z by 225 degrees.\nThe magnitude of -1-i is √2, so z will be scaled by √2.\n(I think it as a vector addition: z⋅(-1-i) = -z-zi,, but there\u0026rsquo;re too many steps).\nOperations in polar form Source page: Lesson 8\nMultiplication Given: $$ w₁ = 3(cos(330°) + i sin(330°)) \\\\\\ w₂ = 2(cos(120°) + i sin(120°)) $$What is w₁⋅w₂ ?\n$3 e^{i⋅330°} ⋅ 2 e^{i⋅120°} = 6 e^{i(450°)}$\nw₁⋅w₂ can be viewed as w₁ transforming w₂, i.e., w₂ is transformed by multiplying w₁\nThe modulus of w₂ is scaled by the modulus of w₁ (2), so |w₁⋅w₂| = 3⋅2 = 6;\nThe argument (angle) of w₁ is rotated by the argument of w₁ (330°), so arg(w₁⋅w₂) = 120° + 330° = 450° = 90°\nSo w₁⋅w₂ = 6 (cos(90°) + i sin(90°)) = 6i\nDivision $$ w₁ = 8(cos( \\frac{4π}{3} ) + i sin( \\frac{4π}{3} )) \\\\\\ w₂ = 2(cos( \\frac{7π}{6} ) + i sin( \\frac{7π}{6} )) \\\\\\ $$What is $\\frac{w₁}{w₂}$ ?\n$8e^{ i\\frac{4π}{3} } / (2e^{ i\\frac{7π}{6} }) = 4 e^{i(π/6)}$\nAnother way to think about it is that w₁ is transformed by w₂:\nthe modulus of w₁ is divided by the modulus of w₂;\nthe argument of w₁ is rotated clock-wise by the argument of w₂.\nmodulus argument |w₁|=8 arg(w₁)=4π/3 |w₂|=2 arg(w₂)=7π/6 |w₁/w₂|=4 arg(w₁/w₂) = 4π/3 - 7π/6 = π/6 So $\\frac{w₁}{w₂} = 4(cos(π/6) + i sin(π/6)) = 4(√3/2 + i/2)$\nPowers of complex number Consider the complex number $z = -1 + i \\sqrt 3$. Find $z^4$ in polar and rectangular form.\nModulus of z is 2, so its modulus is times itself four times: 2⁴ = 16 Argument of z is φ = $\\rm arctan(\\sqrt 3)$=60°=120°, so its angle rotate by 4 times of its argument: φx4 = 480° = 120° The polar form is $z⁴ = 16 (cos(120°) + i sin(120°))$, so the rectangular form is $z⁴ = 16(1/2 + i √3/2) = 8 + i8√3 $\nComplex number equations Given equation x³=1, find all of the real and/or complex roots of this equation.\nFor a real number: $z= 1 = 1 + 0i = 1e^{i0°}$, its argument arg(z) can be 0, 2π, 4π, \u0026hellip;, i.e., $1 = e^{i0} = e^{i2π} = e^{i4π} = e^{i6π}$ \u0026hellip;\nPlug these exponential form into x³=1:\nx³=1 x³=$e^{i2π}$ x³=$e^{i4π}$ x³=$e^{i6π}$ cube root x=1 x=$e^{i2π/3}$ x=$e^{i4π/3}$ x=$e^{i6π/3}$ modulus 1 1 1 1 angle 0 2π/3 = 120° 4π/3 = 240° 2π root x₁ x₂ x₃ redundant a+bi 1 -1/2 + i√3/2 -1/2 - i√3/2 1 x x ₂ ₃ I m x 1 ₁ R e Fundamental theorem of Algebra Source page: Lesson 9\nThe Fundamental theorem of Algebra: a n-th degree polynomial has n roots.\n$P(x) = ax^n + bx^{n-1} + ... + K$\n","date":"2023-06-26T11:29:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/complex_conjugates/","title":"watch: Khan | Complex Number"},{"content":"Real Python Tutorial\nConstruct path Make the filename an object instead of strings.\n1 2 3 4 5 6 7 from pathlib import Path # Instantiate an object: datadir = Path(\u0026#34;./datadir\u0026#34;) # Joining Paths new_path = datadir / \u0026#34;sampled.npy\u0026#34; Iterate dir .iterdir() method iterates over all the files in the given directory\n1 2 3 4 5 from pathlib import Path from collections import Counter # count number of files of different types Counter(path.suffix for path in Path.cwd().iterdir()) Return: Counter({'.md': 2, '.txt': 4, '.pdf': 2, '.py': 1})\n.glob(\u0026quot;*.txt\u0026quot;) returns all the files with a .txt suffix in the current directory.\n1 2 \u0026gt;\u0026gt;\u0026gt; Counter(path.suffix for path in Path.cwd().glob(\u0026#34;*.p*\u0026#34;)) Counter({\u0026#39;.pdf\u0026#39;: 2, \u0026#39;.py\u0026#39;: 1}) .rglob() recursively find all the files in both the directory and its subdirectories.\n1 2 3 4 5 6 def tree(Path(directory)): print(f\u0026#34;+ {directory}\u0026#34;) for path in sorted(directory.rglob(\u0026#34;*\u0026#34;)): depth = len(path.relative_to(directory).parts) spacer = \u0026#34; \u0026#34; * depth print(f\u0026#34;{spacer}+ {path.name}\u0026#34;) glob two patterns python - How to glob two patterns with pathlib? - Stack Overflow\n1 2 3 4 5 6 7 8 9 10 11 12 13 from pathlib import Path exts = [\u0026#34;.jl\u0026#34;, \u0026#34;.jsonlines\u0026#34;] mainpath = \u0026#34;/path/to/dir\u0026#34; # Same directory files = [p for p in Path(mainpath).iterdir() if p.suffix in exts] # Recursive files = [p for p in Path(mainpath).rglob(\u0026#39;*\u0026#39;) if p.suffix in exts] # \u0026#39;files\u0026#39; will be a generator of Path objects, to unpack into strings: list(files) ","date":"2023-06-25T15:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/python_pathlib/","title":"memo: Python | pathlib"},{"content":"Overview Source video: Fourier Analysis: Overview\nMost major breakthrough started with coordinate transformation. (e.g., Theory of relativity)\nFourier transform is another coordinate transformation\nFourier derived Fourier series as a way of approximating the solutions of partial differential equations, e.g., Heat equations.\nA rectangular metal has a temperature distribution u(x,y,t) in 2 dimensions, which is governed by the heat equation:\n$$ \\frac{∂u}{∂t} = α (\\frac{∂²u}{∂x²} + \\frac{∂²u}{∂y²}) = α ∇²u $$Fourier Transform diagonalize the laplacian operator in the heat equation. That means the laplacian operator has eigen values and eigen functions, just like other linear operators do.\nThe eigen functions are sines and cosines with a particular frequency determined by the boundary conditions and geometry of this object. The eigen values are those spatial frequencies. An arbitrary function can be approximated by a sum of sines and cosines of increasing frequencies\nf s ( i x n ) ( w t ) ‖ ✚ ✚ ⋮ These sines and cosines form an orthogonal basis for the space of possible functions. (读作“一个基”，而不是“一组基”) For example, in a vector space, the x-axis and y-axis form the basis for 2-D vector space.\nFast Fourier Transform (FFT) computes Fourier series efficiently to analyze and process data.\nFourier Series-1 Source video: Fourier Series: Part 1\n(2023-11-19) Series means a list of number, where the meaning of each term is implied. Thus in Fourier Series, each term stands for a basis, which is a trigonometric functions of a increasingly high frequence.\nWhile Fourier Transform is a function that gives an aribitrary number in the \u0026ldquo;series\u0026rdquo;.\nFourier series approximates arbitrary functions f(x) as a infinite sum of sines and cosines of increasingly high frequency.\n(2023-11-19) For 1D signal, sine (or cosine) is enough. For 2D signal, sine and cosine are both needed as they\u0026rsquo;re orthogonal. Considering f(x) is 2π-periodic:\nf ( x ) = - π π The function f(x) can be represented as a sum from k=1 to infinity of 2π-periodic sines and cosines of increasingly high frequency:\n$$ f(x) = A₀/2 + ∑ₖ₌₁^∞ (Aₖcos(kx) + Bₖsin(kx) ) $$where\nAₖ, Bₖ are Fourier coefficients, which indicate how much of each sine and cosine is needed to add up to recover f(x);\nk stands for k-th frequency.\nWhen k=0, cos0=1, sin0=0, so there\u0026rsquo;s a constant A₀\nThese sine and cosine waves are 2π periodic.\nCoefficients are inner products These Fourier coefficients can be computed as inner products (in Hilbert space) of the function f(x) with particular wave:\n$$ Aₖ = 1/π ∫_{-π}^π f(x) cos(kx) dx\\\\\\ Bₖ = 1/π ∫_{-π}^π f(x) sin(kx) dx $$More explicitly, Aₖ, Bₖ are the inner products of f(x) with the k-th wave function normalized by the magnitude of (co)sine function\ndoubt: Isn\u0026rsquo;t the magnitude should be square-rooted? Why is it divided by the norm squared? (2023-07-11) 傅里叶系数的归一化除以了两个基向量的模长，其中一个来自“求傅里叶系数”时，f 要与 单位 基向量做点积， 第二个是在“重建”信号时，傅里叶系数要与 单位 基向量相乘以得到某一方向上的分量。 $$ Aₖ = \\frac{1}{‖cos(kx)‖²} ⋅ ⟨ f(x), cos(kx) ⟩ \\\\\\ \\ \\\\\\ Bₖ = \\frac{1}{‖sin(kx)‖²} ⋅ ⟨ f(x), sin(kx) ⟩ $$ When the function f(x) is projected into the direction cos(kx), the unit length of that direction is needed, so the norm of cos(kx) is divided.\n‖cos(kx)‖² can be computed as the inner product of cos(kx) with itself, which is π.\nProjection \u0026amp; reconstruction 𝐮 ` ` ` ` ` 𝐲 , 𝐟 , , , , 𝐯 𝐱 A test vector 𝐟 can be represented separately in two sets of orthogonal basis (2D vector space, 2D coordinate system: 𝐱-𝐲 and 𝐮-𝐯):\n$$ 𝐟 = ⟨𝐟, 𝐱⟩ ⋅ \\frac{𝐱}{‖𝐱‖²}\\ + ⟨𝐟, 𝐱⟩ ⋅ \\frac{𝐲}{‖𝐲‖²} \\\\\\ \\ \\\\\\ 𝐟 = ⟨𝐟, 𝐮⟩ ⋅ \\frac{𝐮}{‖𝐮‖²}\\ + ⟨𝐟, 𝐯⟩ ⋅ \\frac{𝐯}{‖𝐯‖²} $$The definition of Fourier Series is the same as projecting the vector 𝐟 on an orthogonal basis in a 2D vector space.\nThe coefficients are the projections of 𝐟 in each basis direction (i.e., inner products), and then they\u0026rsquo;re multiplied with the unit vectors, and add them up.\nThat means, in Fourier Series, the sines and cosines are the orthogonal basis functions. Coefficients indicate how much 𝐟 is in each direction.\nFourier Series-2 Source video: Fourier Series: Part 2\nArbitrary period L Generalize the period from (-π, π) to (0, L)\n$$ f(x) = \\frac{A₀}{2} \\ + ∑ₖ₌₁^∞ (Aₖcos( \\frac{2πkx}{L} ) + Bₖsin( \\frac{2πkx}{L} ) ) $$where:\ncos( 2πkx/L ) and sin( 2πkx/L ) are periodic between 0 and L.\nThat is when x=0, 2πkx/L=0, and when x=L, 2πkx/L=2πk, i.e., the wave returns to the start.\nThese cos( 2πkx/L ) and sin( 2πkx/L ) are orthogonal basis for Hilbert space of function f.\nThe Fourier coefficients change their bounds from (-π, π) to (0, L):\n$$ Aₖ = \\frac{2}{L} ∫_0^L f(x) cos(2πkx/L) dx\\\\\\ Bₖ = \\frac{2}{L} ∫_0^L f(x) sin(2πkx/L) dx $$Approximation is periodic The approximation $\\\\^f(x)$ is L-periodic as follows, because the f(x) is defined from 0 to L and all sines and cosines are L-periodic\n⋅ ⋅ ⋅ - L 0 L 2 L ⋅ ⋅ ⋅ The Fourier approximation $\\\\^f(x)$ is periodic just repeating forever the pattern of the target function $f(x)$, which is defined on an interval.\nInner Products in Hilbert Space Source video: Inner Products in Hilbert Space\nInner products of functions It\u0026rsquo;s consistent with the definition of inner products of vectors.\nGiven two functions f(x) and g(x),\nf g ₁ : ₁ : x a ₁ f g ₂ : ₂ : x ₂ f : g : : : x ₃ ₃ ₃ f : g : : : : x ₙ ₙ ₙ b f g ( ( x x x ) ) The inner product between these two functions is defined as:\n$$ ⟨f(x), g(x)⟩ = ∫ₐᵇ f(x) g(x) dx $$If they\u0026rsquo;re complex-valued functions (like $e^{iwx}$), then this inner product would use the complex conjugate (same real part, opposite imaginary part) of g(x): $⟨f(x), g(x)⟩ = ∫ₐᵇ f(x) \\bar g(x) dx$\nThis inner product indicates how similar these two functions are, just like the inner product of vectors.\nBy sampling these two functions at n evenly distributed x locations, a function is represented by a data vector.\nThe interval between 2 sampling x is Δx = (b-a)/(n-1).\nAs n increases, more points are sampled and the interval is getting infinitely small, the continuous function can be recovered by a series of points.\n$$ \\underline 𝐟 = \\begin{bmatrix} f₁ \\\\\\ f₂ \\\\\\ ⋮ \\\\\\ fₙ \\end{bmatrix} ; \\quad \\underline 𝐠 = \\begin{bmatrix} g₁ \\\\\\ g₂ \\\\\\ ⋮ \\\\\\ gₙ \\end{bmatrix} $$Compute the inner product of these two vectors:\n$$ ⟨\\underline 𝐟, \\underline 𝐠⟩ = \\underline 𝐠ᵀ ⋅ \\underline 𝐟 \\ = ∑ₖ₌₁ⁿ fₖ gₖ $$ where 𝐠ᵀ is a row vector, 𝐟 is a column vector.\nIf data is complex-valued, 𝐠ᵀ would be complex conjugate transpose 𝐠* and gₖ will be complex conjugate $\\bar gₖ$:\n$⟨\\underline 𝐟, \\underline 𝐠⟩ = \\underline 𝐠^* ⋅ \\underline 𝐟 = ∑ₖ₌₁ⁿ fₖ \\bar gₖ$\nIf the number of samples (resolution) is doubled, this sum will get twice as large, which is not correct because the \u0026ldquo;similarity\u0026rdquo; of two functions doesn\u0026rsquo;t change.\nSo the inner product should be normalized by Δx. (Expectation)\n$$ ⟨\\underline 𝐟, \\underline 𝐠⟩ Δx = ∑ₖ₌₁ⁿ f(xₖ) \\bar g(xₖ) Δx $$The righ-hand term is the Riemann approximation of the continuous integral.\nIf take the limit as Δx goes to 0, the resolution becomes infinitely fine corresponding to infinitely tall vectors, and the Riemann approximation becomes continuous integral:\n$$ ⟨f(x), g(x)⟩ = ∫ₐᵇ f(x) \\bar g(x) dx $$ 再往后是先看一遍，然后默写\n(2023-06-27)\nComplex Fourier Series Source video: Complex Fourier Series\nFourier series uses a infinite sum of sines and cosines functions of increasingly high frequencies to approximate an arbitrary periodic function.\nIf the function is complex-valued, the Fourier series should be reformulated to accomondate imaginary numbers.\nBy leveraging to Euler\u0026rsquo;s formula $e^{ikx} = cos(kx) + i sin(kx)$, the complex Fourier series can be written as:\n$$ f (x) = ∑_{-∞}^{+∞} Cₖ eⁱᵏˣ \\\\\\ \\ = ∑_{-∞}^{+∞} (α+βi) (cos(kx) + i sin(kx)) $$where Cₖ is a complex coefficient.\nReal number is a subset of complex number, so this formula also adapt to read-valued functions, with subjecting to a constraint\nif f(x) is real-valued, $Cₖ = \\bar C₋ₖ$.\nThis can be derived by expanding and kill all the imaginary terms:\n$$ f(x) = ∑_{-∞}^{+∞} (α+βi) (cos(kx) + i sin(kx)) \\\\\\ \\ = ∑_{-∞}^{+∞} αcos(kx) + \\cancel{ αi sin(kx) } + \\cancel{ βicos(kx)} - βsin(kx) \\\\\\ \\ = ∑_{-∞}^{+∞} αcos(kx) - βsin(kx) \\\\\\ \\ = ∑_{-∞}^{+∞} (α-βi) (cos(-kx) + i sin(-kx) ) \\\\\\ $$eⁱᵏˣ are Orthogonal basis eⁱᵏˣ with taking different integer k stands for different basis function. And these basis functions are orthogonal, which are unique directions and form an infinite-dimensional function space.\nOrthogonal means the inner products against each other are 0, but the inner product with itself is its norm squared.\nLet eⁱᵏˣ be defined as Ψₖ (eⁱᵏˣ ≔ Ψₖ), then the inner product in the Hilbert space of any two basis functions can be computed as an integral (the period is [-π,π]):\n$$ ⟨Ψⱼ, Ψₖ⟩ = ∫_{-π}^π Ψⱼ ⋅ \\bar Ψₖ dx \\\\\\ \\ = ∫_{-π}^π eⁱʲˣ ⋅ e⁻ⁱᵏˣ dx \\\\\\ \\ = ∫_{-π}^π eⁱ⁽ʲ⁻ᵏ⁾ˣ dx \\\\\\ \\ = \\frac{1}{i(j-k)} ⋅ [eⁱ⁽ʲ⁻ᵏ⁾ˣ ]_{-π}^π $$if (k-j) is non-zero, but an integar d, and because eⁱᵈ$^π$ = eⁱᵈ$^{-π}$, the inner product (definite integral) is 0.\nWhile if (k-j)=0, the inner product is 0/0:\n$$ \\frac{ [eⁱ⁽ʲ⁻ᵏ⁾ˣ]_{-π}^π }{ i(j-k) } = \\frac{0}{0} $$According to Loptial\u0026rsquo;s Rule, \u0026quot; the limit of the ratio of two functions is the same after we take the derivative of each function.\u0026quot;\nSo take the derivative of numerator and denominator w.r.t. (j-k):\n$$ \\begin{aligned} \u0026 lim_{(j-k)→0} \\frac{ [eⁱ⁽ʲ⁻ᵏ⁾ˣ ]\\_{-π}^π }{ i(j-k) } \\\\\\ \u0026= lim_{(j-k)→0} \\frac{e^{i(j-k)π} - e^{i(j-k)(-π)} }{i(j-k)} \\\\\\ \u0026= lim_{(j-k)→0} \\frac{ iπ e^{i(j-k)π} - (-πi) e^{i(j-k)(-π)}}{i} \\\\\\ \u0026= 2π \\end{aligned} $$Actually, it\u0026rsquo;s not necessary to take this limit.\nSince $⟨Ψⱼ, Ψₖ⟩ = ∫_{-π}^π eⁱ⁽ʲ⁻ᵏ⁾ˣ dx$, this integral = 2π when (j-k) = 0.\nTherefore, the inner product is\n$$ ⟨Ψⱼ, Ψₖ⟩ = \\frac{1}{i(j-k)} ⋅ eⁱ⁽ʲ⁻ᵏ⁾ˣ |_{-π}^π \\\\\\ \\ = \\begin{cases} 0 \u0026 \\text{if j $\\neq$ k} \\\\\\ 2π \u0026 \\text{if j=k} \\end{cases} $$The norm squared of a basis function Ψⱼ (‖Ψⱼ‖²) is 2π.\nAnalogy to regular 2D vector space above, a function can be written as a sum of projections that the function is projected onto each basis function:\n$$ 𝐟(x) = ∑_{-∞}^{+∞} Cₖ eⁱᵏˣ = ∑_{k=-∞}^{+∞} \\frac{⟨𝐟(x), Ψₖ⟩}{2π} ⋅Ψₖ $$doubt: Is Ψₖ unit length? Otherwise, Ψₖ needs normalization?, because the 1/2π is for energy measurement instead of getting the unit length of Ψₖ?\nThe intuitive application of Fourier series is that: given a function, the approximated version of it can be obtained by truncating the summation of basis direction weighted by coefficients. And the coefficients is the projection (inner product) of the function onto each basis.\n(2023-06-28)\nFourier Transform Double integral for x then for w\n还是内积（投影），x的范围是负无穷到正无穷，投影到一个特定频率 w 的基函数上，得到 Fourier coefficient，这就是 Fourier Transform\nFourier Series\u0026rsquo;s counterpart is inverse Fourier Transform\n","date":"2023-06-20T21:23:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ddse-steven/02_fourier/","title":"watch: Steven | Fourier Analysis"},{"content":"Source video: 傅立叶变换如何理解？美颜和变声都是什么原理？李永乐老师告诉你\n变换 向量可以从图形“变换”成数，也可以从数“逆变换”回图形\ny B A ( ( 1 2 , , C - 4 2 ) ( ) 4 , 2 ) x 变 换 = A B C ( ( 2 1 ( , , 3 , 4 - ) 2 2 ) ) 向量的图像与数组是一一对应的。向量相加对应数组相加。\n向量 A 被表示为 (2,4)，其含义为 A 在 x 方向上有 2 个单位长度 ex，在 y 方向上有 4 个单位长度 ey。\n因为：\n𝒆x 和 𝒆y 的长度（自己与自己的内积）都等于 1 ： 𝒆x⋅ex = ey⋅ey = 1\nex 与 ey 正交（垂直），两者内积等于 0： ex ⋅ ey = |ex|⋅|ey|⋅cos90° = 0\n所以 ex 和 ey 是一组标准正交基。 空间中的任何一个向量都可以变成标准正交基的组合。\n傅里叶级数 任何一个周期性的函数 f(t) 都可以变换为一系列正余弦函数的和。 正弦余弦函数就是一组正交基。\n它的逆变换是显而易见的，因为 sin 和 cos 都是周期性的，它们组合出来的函数也一定是周期性的。\n对于一个周期性函数，按照不同频率 w 分解出不同的正余弦函数（波）：\n如上图，在频率方向上，一个周期性波可以分解成一个频率为 w 的正弦函数，与一个频率为 2w 的分量（振幅比较大），再与一个频率为 3w 的分量（振幅比较小）相加。\n从后方观察，以时间为坐标系，只能看到信号随时间的变化； 从侧面观察，以频率为坐标系，一个信号是多个信号的累加。\n因为分量是有限个，所以频谱图上是一条条竖线，不同的高度代表振幅的大小。 不同频率的分量，除了振幅不同，起始点也不同，所以还需要一个轴表示相位：\n时域里的信号 f 变换到频域里的三个维度：频率 w，振幅 F(f)，相位 φ。\n如果已知频域中的 w, F(f), φ，就可以把它们组合起来，逆变换到时域中的信号。\n$$ f(t) = \\frac{a_0}{2} + \\sum_i a_n sin(n w t + φ_n) \\\\\\ \\quad = \\frac{a_0}{2} + \\sum_i a_n sin(n w t) + \\sum_i b_n cos(n w t) $$n 是整数，nw 是不同的频率，aₙ 是振幅，φₙ 是相位。 按照第2行的写法，标准正交基有 3 个：1， sin(nwt), cos(nwt)\n任何一个周期性的函数都可以分解成以上 3 个标准正交基的线性组合。 （sin(nwt) 与 cos(nwt) 是一对，拆不开？）\n连续傅里叶变换 非周期信号可以看作是周期无穷大的周期性信号。\n欧拉公式 用一个平面坐标系表示虚数，则横轴表示实部，纵轴表示虚部。 横轴的单位为1，纵轴的单位为虚数单位 i，则单位圆上任何一点 A 可以表示为： cosθ + i⋅sinθ。\n令 θ = wt，w 取不同的频率，对应 A 点在圆上逆时针转动不同的角度\n根据欧拉公式：\n$$ cosθ + i⋅sinθ = e^{iθ} \\\\\\ cos(wt) + i⋅sin(wt) = e^{iwt} $$所以 A 在逆时针旋转时，每时每刻都代表了两个正交基 cosθ, sinθ。 如果是 $e^{-iwt}$ 就是顺时针旋转。\nFT: 用正交基\u0026quot;摘\u0026quot;分量 一个非周期性的时域信号肯定也包含各种频率的信号分量， 又因为标准正交基与自己的内积为 1，与其他正交基的内积为 0， 所以可以用各个正交基与该信号内积，则留下的就只是在这个基向量上的分量。\n非周期性时域信号的频率分量有无穷个，所以傅里叶变换是用积分：\n$$ \\\\^F_T(w) = ∫_{-∞}^{+∞} f(t) e^{-jwt} dt \\\\\\ \\ \\\\\\ f(t) e^{-jwt} \\begin{cases} =0 \u0026 \\text{f(t) 不含 sin(wt),cos(wt)} \\\\\\ \\neq 0 \u0026 \\text{f(t) 含有 sin(wt),cos(wt)} \\end{cases} $$其中 $e^{-jwt}$ 是一组正交基 sin(wt) 和 cos(wt)。\n$\\\\^F_T(w)$ 是个复数（带 i），实部表示振幅，虚部是相位，而且是连续的 ，意味着每个频率的正（余）弦函数都有分量，\nIFT: 正交基加权和 $F_T(w)$ 是各正交基（正弦余弦函数）的”系数“，即在各正交基上的”长度“。\n从频域信号逆变换回时域信号，就把系数乘到基向量上，再加起来：\n$$ f(t) = ∫_{-∞}^{+∞} F_T(w) e^{-jwt} dw $$应用 声音信号是时域，横坐标是时间；图像信号是空间域，横坐标是空间位置\n低频分量是图像的轮廓，高频分量是细节\n","date":"2023-06-20T15:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/d-vid-ft-%E6%9D%8E%E6%B0%B8%E4%B9%90/d-vid-ft-%E6%9D%8E%E6%B0%B8%E4%B9%90/","title":"watch: FT - 李永乐 | Fourier Transform"},{"content":"NO code | arxiv\nFound by google scholar with searching: \u0026ldquo;fourier transform neural radiance field\u0026rdquo; 3. Generalized PlenOctree Fusion silhouettes 剪影 Pipeline generalized NeRF Ψ: averaging the density σ and\nΨ are queried at every position with several view directions to predicted density and color\nPlenOctree stores the averaged density and color\nFilter out leaves having low density ($σ \u003c 1e-3$) by \u0026ldquo;averaging\u0026rdquo; the queried results of 100 rendered views at the points, where the (cumulated) transmittance $T \u003e 1e-3$ (close to the camera?).\n\u0026ldquo;Coarse\u0026rdquo; means sparse adjacent views of a target view, while \u0026ldquo;fine\u0026rdquo; means dense adjacent views\n4. Fourier PlenOctree \u0026ldquo;adopt PlenOctree to dynamic scenes by compressing time-variant information in the frequency domain\u0026rdquo;\n4D Scene Representation: position and time (x,y,z,t)\nhigh dimensional frequency domain: Mapping the position to Fourier Transform coefficients of the density σ(t) and each SH coefficient 𝐳(t)\n$$Φ(x,y,z) = 𝐤^σ, 𝐤^𝐳$$where $𝐤^σ ∈ ℝ^{n₁}$ (a sigma corresponds to n₁ DFT coefficients),\nand $𝐤^𝐳 ∈ ℝ^{n₂ × (l_{max} + 1)^2 × 3 }$ (a point has $(l_{max} + 1)^2 × 3$ SH coefficients and each SH coefficient has n₂ DFT coefficients.)\nReconstruct density σ at time t by summing n₁ DFT coefficients of the Fourier PlenOctree with orthogonal basis:\n$$\\rm σ(t; 𝐤^σ) = ∑ᵢ₌₀^{n₁-1} 𝐤^σᵢ ⋅ IDFTᵢ(t)$$where IDFTᵢ(t) = $\\\\{^{cos(\\frac{i\\pi}{T} t) \\quad \\text{if i is even}} _{sin(\\frac{(i+1)\\pi}{T} t) \\quad \\text{if i is odd}}$\nReconstruct each SH coefficient at time t by summing n₂ DFT coefficients of the Fourier PlenOctree:\n$$\\rm z_{m, l}(t; 𝐤^𝐳) = ∑ᵢ₌₀^{n₂-1} 𝐤^𝐳_{m,l,i} ⋅ IDFTᵢ(t)$$ Generalized PlenOctree Fusion Aggregating the PlenOctrees of different frames at T times.\nLeaves of different PlenOctrees at the same position are stacked, so the density σ and SH coefficients 𝐳(t) are stacked along the time axis.\nThe stacked vector performs DFT becoming Fourier coefficients $𝐤^σ, 𝐤^𝐳$, which are stored in Fourier PlenOctree.\nFourier PlenOctree Fine-tunning Based on PlenOctree fusion \u0026ldquo;training\u0026rdquo;, the Fourier PlenOctree can be continuous optimizing via gradient descent.\n","date":"2023-06-19T12:16:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-fourier_plenoctrees/","title":"read: Fourier PlenOctrees for Dynamic"},{"content":"(2023-11-22)\nDiscrete Convolve Source video: 【官方双语】那么……什么是卷积？- 3B1B\nTwo list of numbers are combined and form a new list: Make a table, calculate pairwise product for each cell and sum up all anti-diagonals.\n$(a * b)\\_n = ∑_{\\substack{i,j \\\\\\ i+j=n}} a_i * b_j$\nThe sum of indecies of the 2 number in each pair to be summed up is the same, just distinct combinations. Reverse the second list, slide it from left to righ, and sum the products of each pair aligned vertically.\nConvolution for image is for smoothing by averaging nearby pixels.\nThe multiplication of two polynomials is convolution, because multiply each two term and collect term with the same order.\n(2024-01-12) 父亲寿命 和 儿子的年龄做卷积，等于一起生活的岁月\n【官方双语】卷积的两种可视化|概率论中的X+Y既美妙又复杂 - 3B1B\n【官方双语】但是什么是中心极限定理？- 3B1B\n【官方双语】为什么正态分布里会有一个π？（不止是积分技巧）- 3B1B\n(2023-11-07)\nCovariance Matrix is PSD Variance is the average of the squared distance between the mean and point for a single dimension.\n方差是偏离期望的平方的期望。\nVariance is a scalar: the averaged squared mangitude of the distance vector between a certain vector and the mean vector in a data space.\n$$Var(x) = \\frac{∑(x-μ)²}{n}$$ Covariance is the product of 2 distances betwee mean and point for the 2 dimensions respectively.\n$$Covar(x,y) = \\frac{∑(x-μ_x)(y-μ_y)}{n}$$Covariance is a real number and indicates positive- or negtive correlation of 2 dimensions by its sign.\nCovariance matrix is a symmatric matrix recording covariance for each pair of dimensions.\n$$ \\begin{array}{c|ccc} 𝐱 \u0026 d₁ \u0026 d₂ \u0026 d₃ \\\\\\ \\hline \\\\\\ d₁ \u0026 \\frac{∑(x_{d₁}-μ_{d₁})(x_{d₁}-μ_{d₁})}{n} \u0026 \\frac{∑(x_{d₁}-μ_{d₁})(x_{d₂}-μ_{d₂})}{n} \u0026 \\frac{∑(x_{d₁}-μ_{d₁})(x_{d₃}-μ_{d₃})}{n} \\\\\\ \\\\\\ d₂ \u0026 \\frac{∑(x_{d₂}-μ_{d₂})(x_{d₁}-μ_{d₁})}{n} \u0026 \\frac{∑(x_{d₂}-μ_{d₂})(x_{d₂}-μ_{d₂})}{n} \u0026 \\frac{∑(x_{d₂}-μ_{d₂})(x_{d₃}-μ_{d₃})}{n} \\\\\\ \\\\\\ d₃ \u0026 \\frac{∑(x_{d₃}-μ_{d₃})(x_{d₁}-μ_{d₁})}{n} \u0026 \\frac{∑(x_{d₃}-μ_{d₃})(x_{d₂}-μ_{d₂})}{n} \u0026 \\frac{∑(x_{d₃}-μ_{d₃})(x_{d₃}-μ_{d₃})}{n} \\end{array} $$Thus, element on the main diagonal is the variance of each variable.\nCovariance matrix is always positive semi-definite, symmetric, square.\nThe eigenvalues λ and eigenvectors 𝐯 of covariance matrix 𝐄(𝐱𝐱ᵀ) can be solved from:\n𝐄(𝐱𝐱ᵀ) ⋅ 𝐯 = λ𝐯\nSince 𝐄(𝐱𝐱ᵀ) is a symmetric matrix, λs are all real numbers.\nλ = 𝐄(𝐱𝐱ᵀ) = $\\frac{∑(xᵢ-μᵢ)²}{n}$ ≥ 0,\nwhere n is the number of datapoints in the dataset. xᵢ is one of dimensions of 𝐱. xᵢ is a column vector containing n points.\nCheck the definition of positive semi-definite:\n𝐯ᵀ𝐄 𝐯 = λ𝐯ᵀ𝐯 = λ[a b c] $[^a\\_{^b\\_c}]$ = λ(a²+b²+c²) ≥ 0.\nThus, covariance matrix 𝐄 is positive semi-definite.\ntodo: This could be wrong, need to watch Dr. Strang\u0026rsquo;s lecture.\nThe following proof is from Is a sample covariance matrix always symmetric and positive definite? - SE\nCovariance matrix E(𝐱𝐱ᵀ) = (𝐱-𝛍)(𝐱-𝛍)ᵀ/n\nCheck the definition of positive semi-definite:\n𝐯ᵀ E(𝐱𝐱ᵀ) 𝐯 = $\\frac{1}{n}$ 𝐯ᵀ (𝐱-𝛍)(𝐱-𝛍)ᵀ 𝐯 = ((𝐱-𝛍)ᵀ 𝐯)²/n ≥ 0. A positive scalar.\nOther Proofs:\nProof: Positive semi-definiteness of the covariance matrix Since covariance matrix is positive semi-definite, there is global minima for all axis:\nHeatmap of a covariance matrix - Gowri Shankar\nCategorizing Quadratic Forms - Ximera, OSU - Categorizing Quadratic Forms\n(2023-11-08)\nRadius of 3D Gaussian If a dataset scattered as an ellipse following 2D Gaussian is circumscribed by a circle, to cover the most points, the radius of the circle could be $r = 3σ$, 3 times the standard deviation.\nThe standard deviation σ is the square root of the variance, which is the element on the main diagonal of the covariance matrix.\nAnd variances are the eigenvalues of the covariance matrix.\nGiven a covariance matrix $𝐂=[^{a \\ b}_{b \\ c}]$, its eigenvalue λ and eigenvector 𝐯 satisfy: 𝐂 𝐯 = λ𝐯.\nEigenvalues λs can be solved from: $\\rm det(𝐂 - λ𝐈) = 0$:\n$$ \\begin{vmatrix} a-λ \u0026 b \\\\\\ b \u0026 c-λ\\end{vmatrix} = 0 \\\\\\ (a-λ)(c-λ) - b^2 = 0 \\\\\\ λ^2 - (a+c)λ + ac-b^2 =0 $$λ₁ = $\\frac{(a+c) + \\sqrt{(a+c)^2 - 4(ac-b^2)} }{2}$, λ₂ = $\\frac{(a+c) - \\sqrt{(a+c)^2 - 4(ac-b^2)} }{2}$\n","date":"2023-06-17T22:14:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/gaussian/","title":"memo: Calc | Gaussian"},{"content":"FNet: Mixing Tokens with Fourier Transforms - NAACL 2022\nCode: arxiv appendix | official-jax; | keras code\n(2023-07-05) Other implementations found by asking bing chat: \u0026ldquo;Could you give its pytorch code?\u0026rdquo;\nrishikksh20/FNet-pytorch erksch/fnet-pytorch1 jaketae/fnet vgundecha/fnet-google-pytorch (2023-06-16)\nVideo Intro Source video: FNet: Mixing Tokens with Fourier Transforms (Machine Learning Research Paper Explained) - Yannic Kilcher\n(2023-07-07)\nAbstract Use linear transformations replace self-attention sublayers resulting in speeding up;\nUse unparameterized Fourier Transform replace self-attention sublayers achieving over 90% accuracy of BERT counterparts.\nFNet has a light memory footprint (because it doesn\u0026rsquo;t have parameters?)\nIntroduction Attention connects each token by the relevance weights of every other token in the input.\nAnd more complex mixing help capture the relationship between tokens.\nCan attention, the relevance-based \u0026ldquo;token-mixer\u0026rdquo;, be replaced by simpler linear transformation (𝐗𝐖⁻¹+𝐛)?\nDecent results are gived by replacing attention with twice parametrized (optimizable) matrix multiplications, which are mixing the sequence dimension and then mixing hidden dimension.\nA sequence containing 5 tokens, which are 4-dimensional.\nS e q u e n c e d i m e n t i o n h d i i d m d e e n n s i o n Use the faster, structured linear transformation FFT without parameters, yielding similar performance of dense layer mixing and good scalability.\nContributions:\nattention may not be a necessary component. Hence, seeking new mixing mechanisms is valuable. FNet uses FFT to mix token speeding up the training while losing some accuracy. Attention do help increase accuracy to some extent. FNet scales well to long inputs. Code from: rishikksh20/FNet-pytorch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import torch from torch import nn class FeedForward(nn.Module): def __init__(self, dim, hidden_dim, dropout = 0.): super().__init__() self.net = nn.Sequential( nn.Linear(dim, hidden_dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout) ) def forward(self, x): return self.net(x) class PreNorm(nn.Module): def __init__(self, dim, fn): super().__init__() self.norm = nn.LayerNorm(dim) self.fn = fn def forward(self, x, **kwargs): return self.fn(self.norm(x), **kwargs) class FNetBlock(nn.Module): def __init__(self): super().__init__() def forward(self, x): # \u0026#34;2-D fft\u0026#34;? row-wise first, then column-wise. x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real return x class FNet(nn.Module): def __init__(self, dim, depth, mlp_dim, dropout = 0.): super().__init__() self.layers = nn.ModuleList([]) for _ in range(depth): self.layers.append(nn.ModuleList([ PreNorm(dim, FNetBlock()), PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)) ])) def forward(self, x): for attn, ff in self.layers: x = attn(x) + x x = ff(x) + x return x ","date":"2023-06-16T16:23:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/b-note-fnet/","title":"read: FNet"},{"content":"Arxiv | Code-keras | Code-cvae\nAnother paper: An Introduction to Variational Autoencoders Recognition model with parameters φ, $q_φ(𝐳|𝐱)$, approximates the intractable posterior distribution;\nGenerative model with parameters θ, $p_θ(𝐳)p_θ(𝐱|𝐳)$, maps a latent variable to a \u0026ldquo;sample\u0026rdquo;.\nAbstract directed probabilistic model\nThe distribution of an i.i.d. dataset with latent variables P(𝐗,𝐙)\ncontinuous latent variables with intractable posterior distributions\nThe latent variable 𝐳 per datapoint is continuous, so its posterior distribution $p_θ(𝐳|𝐱)$ cannot be computed explicitly.\nreparameterization of the variational lower bound yields a lower bound estimator\nLower bound estimator can be optimized using SGD,\nIntroduction Variational Bayesian approach involves the optimization of an approximation to the intractable posterior.\nUse q(𝐳) to approximate $p_θ(𝐳|𝐱)$\nMean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior\n𝔼_{q(𝐳)} [ ]\nq(𝐳) is also intractable\nMethod Problem scenario Each sample is generated by two steps:\nsample a 𝐳 from prior distribution $p_{θ^*}(𝐳)$ Sample an 𝐱 from the conditional distribution $p_{θ^*}(𝐱|𝐳)$ where θ* is the true parameters.\nIntractability:\nThe integral of the marginal likelihood $p_θ(𝐱) = ∫ p_θ(𝐳) p_θ(𝐱|𝐳) d𝐳 = ∫ p_θ(𝐱,𝐳) d𝐳$ cannot be computed, because 𝐳 is a high-dimensional continuous variable.\nSuch that the true posterior density $p_θ(𝐳|𝐱) = \\frac{ p_θ(𝐱|𝐳) p_θ(𝐳) }{ p_θ(𝐱) }$ is also intractable, because the denominator marginal likelihood cannot be computed.\nThat means the EM algorithm cannot be used, because in each iteration the introduced prior distribution q(𝐳) needs to be equal to $p_θ(𝐳|𝐱)$, which however is intractable.\nAnd any reasonable mean-field variational bayesian algorithms are intractable, where the $p_θ(𝐳|𝐱)$ is used as the objective of approximating by q(𝐳), but it doesn\u0026rsquo;t work for $p_θ(𝐳|𝐱)$ that cannot be computed.\nA large dataset:\nSampling-based methods, e.g. Monte Carlo EM, would be too slow, because sampling is performed on every datapoint.\nThree meaningful problems:\nEstimating the parameter θ of the distribution via MLE or MAP can allow mimicking the data-generating process: Sample 𝐳 first then sample the x from the conditional distribution\nMLE try to find the θ that makes the probability of dataset X given Z maximum. MLE is to estimate parameter θ, while VAE is to estimate the distribution of X.\nMAP believe that the θ having the maximum posterior probability given a dataset likelihood p(X|θ) and prior probability p(θ) according to, $p(θ|X) = \\frac{p(X|θ) p(θ)}{p(X)}$, is the most possible.\nEM algorithm uses MLE to find the parameter θ of a probabilistic model involving latent variable, where assume the prior probability q(𝐳) = posterior probability $p_θ(𝐳|𝐱)$ of the latent variable, then apply MLE find optimal θ.\nApproximating the posterior probability $p_θ(𝐳|𝐱)$ given an observed value 𝐱 and a choice of parameters θ is useful for encoding data.\nThe latent variable 𝐳 can be regarded as a latent representation of a datapoint 𝐱.\nApproximating the marginal likelihood $p_θ(𝐱)$ enable to perform those inference tasks where the prior p(𝐱) is required, e.g. image denoising and inpainting.\nThe above three goals can be tackled by a recognition model $q_φ(𝐳|𝐱)$, which is an approximation to the intractable posterior probability $p_θ(𝐳|𝐱)$\nThe recognition model $q_φ(𝐳|𝐱)$ is a probabilistic encoder, which produces a distribution over all possible values of 𝐳 with given a datapoint 𝐱.\n(Each datapoint 𝐱 corresponds to a distribution $q_φ(𝐳|𝐱)$ with some parameters (e.g., μ,σ).)\np ( 𝐳 D | i ( 𝐱 s b f ) t y r r o i g m b i u v E t e n i n c o o n a d e o 𝐱 r f ) 𝐳 𝐳 p ( 𝐱 | D 𝐳 i ( ) s b f t y r r o i g m b i u v D t e e i n c o o n a d e o 𝐳 r f ) 𝐱 𝐱 A 𝐳 produces a conditional distribution p(𝐱|𝐳), from which a datapoint 𝐱 is generated. So the generative model $p_θ(𝐱|𝐳)$ is called probabilistic decoder.\n(2023-07-09)\nSo a training step is: sampling a 𝐳 from distribution p(𝐳|𝐱), then with this 𝐳, find the parameter θ that makes the probability p(𝐱|𝐳) largest based on MLE. That\u0026rsquo;s why the loss funciton has a cross-entropy term that measures the likelihood of output 𝐱\u0026rsquo;, i.e., log p(𝐱|𝐳).\nAccording to KL-divergence $-q(𝐳) log \\frac{p(𝐳|𝐱)}{q(𝐳)}$, the approximated posterior q(𝐳) is supposed to equal the real posterior p(𝐳|𝐱), which, however, is intractable. So the authors use network and reparameterization to estimate the parameters (mean, variance) of the posterior.\n(2024-07-17)\nI always mix up the sign of KL divergence. I\u0026rsquo;m not 100% sure if I can use the following mnemonic: The entropy of a system is defined as the expectation of the entropy of each event in this system. Therefore, q(z|x) is the weight, and the negative log term should be an event\u0026rsquo;s entropy. The argument of the log should the probability q(z|x), but, since here is a relative entropy (KL divergence), the probability in log should be a \u0026ldquo;variant\u0026rdquo; of q(z|x), i.e., a portion of q(z|x). Thus, the probability in log should be a fraction: $\\frac{p(x|z)}{q(z|x)}$. Thereby, KL divergence is $-\\int q(z|x) log ( \\frac{p(z|x)}{q(z|x)})$ (I think) 𝐳 sampled from posterior can be one value or multiple times and doing average.\n2.2 The Variational bound The marginal likelihood of the dataset with N datapoints is a sum over the marginal likelihoods of individual datapoints:\n$log\\ p_θ(𝐱⁽¹⁾, ..., 𝐱⁽ᴺ⁾ ) = ∑ᵢ₌₁ᴺ log\\ p_θ(𝐱⁽ⁱ⁾)$\nRewrite it by introducing posterior approximation $q_φ(𝐳|𝐱⁽ⁱ⁾)$ to make up KL divergence:\n$$ \\begin{aligned} \u0026log\\ p_θ(𝐱⁽ⁱ⁾) \\\\\\ \u0026= log\\ (\\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{p_θ(𝐳|𝐱⁽ⁱ⁾)} ) \\\\\\ \u0026= log\\ (p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)) - log\\ p_θ(𝐳|𝐱⁽ⁱ⁾) \\\\\\ \u0026= log\\ (p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)) - log\\ p_θ(𝐳|𝐱⁽ⁱ⁾) \\\\\\ \u0026\\quad + log\\ q_φ(𝐳|𝐱⁽ⁱ⁾) - log\\ q_φ(𝐳|𝐱⁽ⁱ⁾) \\\\\\ \u0026= log\\ \\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} - log\\ \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)}\\\\\\ \\end{aligned} $$ Compute expectations w.r.t. the approximate posterior $q_φ(𝐳|𝐱)$ for both side:\n$$ ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log\\ p_θ(𝐱⁽ⁱ⁾) d𝐳 = \\\\\\ ∫q_φ(𝐳|𝐱⁽ⁱ⁾) \\left[log\\ \\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} - log\\ \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} \\right] d𝐳 $$ Left-hand side remains marginal likelihood $log\\ p_θ(𝐱⁽ⁱ⁾)$ (i.e. $𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [ log\\ p_θ(𝐱⁽ⁱ⁾) ]$) because p(x) has nothing to do with z.\nWhile right-hand side is the lower bound plus KL divergence: $$ \\begin{aligned} \u0026 log\\ p_θ(𝐱⁽ⁱ⁾) = \\\\\\ \u0026 ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log\\ \\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} d𝐳 \\\\\\ \u0026\\quad - ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log\\ \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} d𝐳 \\\\\\ \u0026 = ℒ(θ,φ; 𝐱⁽ⁱ⁾) + D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳|𝐱⁽ⁱ⁾ ) \\\\\\ \u0026 \\\\\\ \u0026 = 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾,𝐳) - log\\ q_φ(𝐳|𝐱⁽ⁱ⁾)] \\\\\\ \u0026\\quad + D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳|𝐱⁽ⁱ⁾ ) \\quad (2) \\end{aligned} $$ In another way, the lower bound can also be written as eq.(3): 𝓛$(θ,φ; 𝐱⁽ⁱ⁾) = -D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳) ) + 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾|𝐳)]$\nwhose derivation starts from the conditional probability $p_θ(𝐱⁽ⁱ⁾|𝐳)$:\nThe likelihood of the i-th datapoint: $$ \\begin{aligned} \u0026 log(p_θ(𝐱⁽ⁱ⁾|𝐳)) \\\\\\ \u0026= log( \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾))}{p_θ(𝐳)} )\\\\\\ \u0026= log( \\frac{ p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾)* q_φ(𝐳|𝐱⁽ⁱ⁾) ) }{ p_θ(𝐳)*q_φ(𝐳|𝐱⁽ⁱ⁾) } ) \\\\\\ \u0026= log( \\frac{ p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} ) - log( \\frac{p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} ) \\end{aligned} $$ Compute the expectation w.r.t. approximate posterior $q_φ(𝐳|𝐱⁽ⁱ⁾)$:\n$$ \\begin{aligned} \u0026 ∫q_φ(𝐳|𝐱⁽ⁱ⁾)\\ log(p_θ(𝐱⁽ⁱ⁾|𝐳)) d𝐳 = \\\\\\ \u0026 ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log( \\frac{ p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)})d𝐳\\\\\\ \u0026 \\quad - ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log( \\frac{p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} ) d𝐳 \\\\\\ \u0026 \\\\\\ \u0026 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾|𝐳))] = \\\\\\ \u0026 \\quad ℒ(θ,φ; 𝐱⁽ⁱ⁾) + D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾ || p_θ(𝐳)) \\end{aligned} $$ However, using Monte Carlo to estimate gradient of 𝓛 (an expectation) will bring high variance.\n2.3 SGVB estimator and AEVB algo A practical estimator of the lower bound 𝓛 of the likelihood and its derivatives w.r.t. the parameters.\n(With some mild conditions for a selected approximate posterior distribution $q_φ(𝐳|𝐱)$,)\nConsider a variable $\\\\~𝐳$ that comes from $\\\\~𝐳 = g_φ$(𝛆,𝐱) with 𝛆 ~ p(𝛆), follows the posterior distribution $\\\\~𝐳 \\sim q_φ(𝐳|𝐱)$ (or not conditioned distribution $q_φ(𝐳)$). And $g_φ$(𝛆,𝐱) is a deterministic differentiable transformation of an (auxiliary) noise variable 𝛆.\nSince $\\\\~𝐳$ is a transformation of 𝛆, $\\\\~𝐳$ follows the distribution p(𝛆) as well.\nMonte Carlo estimation\nTherefore, using Monte Carlo (i.e., averaging the L sampled values) to approximate an expectation of some function $f(𝐳)$ w.r.t. the posterior approximation $q_φ(𝐳|𝐱⁽ⁱ⁾)$ becomes:\n$$ 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [f(𝐳)] = 𝔼_{p(ε)} [f( g_φ(ε, 𝐱⁽ⁱ⁾) )] \\\\\\ ≃ \\frac{1}{L} ∑ₗ₌₁ᴸ f( g_φ(ε⁽ˡ⁾, 𝐱⁽ⁱ⁾) ), $$ where 𝛆⁽ˡ⁾~ p(𝛆).\nApproximate lower bound with eq. (2)\nThe version A of SGVB estimator comes from eq. (2), the lower bound 𝓛 should be finally equal to that ELBO expectation, 𝓛ᴬ (𝛉,𝛗,𝐱⁽ⁱ⁾) ≃ 𝓛 (𝛉,𝛗,𝐱⁽ⁱ⁾) i.e., the KL divergence=0.\nAnd that expectation can be approximated via Monte Carlo (sampling), so the lower bound is approximated as:\n$$ \\\\~\\mathcal L^A (θ,φ,𝐱⁽ⁱ⁾) = \\\\\\ \\frac{1}{L} ∑ₗ₌₁ᴸ \\left[ log\\ p_θ(𝐱⁽ⁱ⁾, 𝐳⁽ⁱ'ˡ⁾) - log\\ q_φ(𝐳⁽ⁱ'ˡ⁾| 𝐱⁽ⁱ⁾) \\right] \\\\\\ $$where $𝐳⁽ⁱ'ˡ⁾= g_φ(ε⁽ⁱ'ˡ⁾, 𝐱⁽ⁱ⁾))$ and 𝛆⁽ˡ⁾~ p(𝛆)\nApproximate lower bound with eq. (3)\nSince the KL-divergence $D_{KL}( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳) )$ of eq. (3) can be integrated analytically, it can be substituted into eq. (3), then only that expectation (\u0026ldquo;expected reconstruction error\u0026rdquo; $𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾|𝐳))]$) is approximated, so the second version of lower bound approximation: 𝓛ᴮ (𝛉,𝛗,𝐱⁽ⁱ⁾) ≃ 𝓛 (𝛉,𝛗,𝐱⁽ⁱ⁾) is more relatively accurate than the version A.\n$$ \\tilde{\\mathcal L^B} = -D_{KL}( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳) ) \\\\\\ \\qquad + \\frac{1}{L} ∑ₗ₌₁ᴸ log\\ p_θ(𝐱⁽ⁱ⁾ | 𝐳⁽ⁱ'ˡ⁾) $$where $𝐳⁽ⁱ'ˡ⁾= g_φ(ε⁽ⁱ'ˡ⁾, 𝐱⁽ⁱ⁾))$ and 𝛆⁽ˡ⁾~ p(𝛆).\nThe KL-divergence there can be interpreted as regularizer, and the optimization objective is enlarging the \u0026ldquo;expected negative reconstruction error\u0026rdquo;, i.e., recover original 𝐱 from code 𝐳.\nTrain with minibatches\nGiven a dataset 𝐗 with N datapoints, each time M datapoints are drawn as a minibatch, then the marginal likelihood lower bound of the full dataset batch-by-batch is estimated as:\n$$ \\mathcal L(\\pmb{θ,φ},𝐱) \\simeq \\mathcal L^M (\\pmb{θ,φ},𝐗ᴹ) = \\frac{N}{M} ∑ᵢ₌₁ᴹ \\tilde{\\mathcal L} (\\pmb{θ,φ},𝐱⁽ⁱ⁾) $$ 2.4 Reparameterization trick Previously, 𝐳 is sampled directly, then input to model $p_θ(𝐱|𝐳)$, but the parameter φ of the distribution of 𝐳 is not able to be optimized via gradient descent, since Monte Carlo isn\u0026rsquo;t differentiable.\np ( z z ) p r i o r , q ᵩ ( z ) s a z M m . p z C l . i n g z - θ - x Instead of sampling the 𝐳 directly, but the 𝐳 is derived from the sampled 𝛆, based on the deterministic differentiable transformation: 𝐳 = gᵩ(𝛆, 𝐱).\np ( 𝛆 ) 𝛆 d i s t r i b u t i o s n a M m . p 𝛆 𝛆 C l . i n g z = μ + σ 𝛆 z - θ - x (The non-differentiable operation (M.C.) is put ahead of the leave nodes on the computational graph.)\nThen the parameters (e.g. μ,σ²) of 𝐳\u0026rsquo;s distribution can be learned by a network with parameters φ.\nx - φ l μ o g σ ² z p ( = 𝛆 ) 𝛆 μ d + i s σ s t m r 𝛆 a i p b l u i t n i g o n L z - 𝛆 θ - x R e c c l o t o n i s s o s t n r u - Such that the Monte Carlo estimate of the expectation is differentiable w.r.t. φ.\nThe reasoning is as follows:\nFor infinitesimals, there is $$q_φ(𝐳|𝐱)∏ᵢdzᵢ = p(\\pmb ε)∏ᵢdεᵢ$$; where zᵢ is one of dimensions, d𝐳 = ∏ᵢ dzᵢ\nTherefore, $∫ q_φ(𝐳|𝐱) f(𝐳) d𝐳$ = ∫p(𝛆) f(𝐳) d𝛆 = ∫ p(𝛆) $f(g_φ$(𝛆,𝐱)) d𝛆\nUse Monte Carlo sampling approximation: $∫ q_φ(𝐳|𝐱) f(𝐳) d𝐳$ ≃ 1/L ∑ₗ₌₁ᴸ f( gᵩ(𝛆⁽ˡ⁾, 𝐱)), where 𝛆⁽ˡ⁾ ~ p(𝛆)\nThe transformation (φ) from 𝛆 to 𝐳 can be learned by back-propagation and gradient descent from the reconstruction loss, since the transformation differentiable.\nSo the parameters (e.g. μ,σ) of the approximate posterior distribution $q_φ(𝐳|𝐱)$ of 𝐳 can be optimized through φ, and the parameter θ of the generative model is trained jointly.\nThis\u0026rsquo;s just a trick, the essence of the algorithm is still the coordinate ascent:\nSample a 𝐳 by sampling an 𝛆:\n1 2 3 4 def reparameterize(mu, logvar): # logvar is log𝛔² std = torch.exp(0.5 * logvar) # (e^{log𝛔²})^½ eps = torch.randn_like(std) return mu + eps * std Use 𝐳 to generate 𝐱 with model $p_θ(𝐱|𝐳)$,\n1 2 3 def decode(z): result = nn.linear(latent_dim, hidden_dims)(z) return result then use 𝐱 to produce 𝐳 with model $q_φ(𝐳|𝐱)$\n1 2 3 4 5 def encode(input): result = nn.linear(input_dim, hidden_dim)(input) mu = nn.Linear(hidden_dim, latent_dim)(result) logvar = nn.Linear(hidden_dim, latent_dim)(result) return [mu, logvar] With using reparameterization trick, both the two models can be optimized by gradient descent.\n(2024-04-16)\nReparameterization trick is a sampling method. It\u0026rsquo;s similar to inverse transform sampling: using a known distribution (uniform) to sample an unknown distribution.\nReparameterization trick makes the sampling from an unknown distribution differentiable, and enables the parameters of the distribution to be optimized.\nLoss func Loss function contains two parts: KL divergence and reconstruction error.\nKL divergence (i.e., cross entropy) can be computed with given the prior and assumed posterior of 𝐳.\nFor example, let the prior $\\rm p_θ(𝐳) = N(𝐳,𝟎,𝐈)$ and assume the posterior qᵩ(𝐳|𝐱) = N(𝐳; 𝛍, 𝛔²𝐈), then cross entropy can be derived by plugging them into Gaussian expression.\nThe reconstruction error is the log likelihood of the input datapoint log p(𝐱|𝐳).\nFor a datapoint obeying multivariate Bernoulli (Yes or No), the log likelihood of 𝐱 is\n$$ log p(𝐱|𝐳) = log ∏ᵢ₌₁ᴰ yᵢˣⁱ (1-yᵢ)¹⁻ˣⁱ \\\\\\ \\ = ∑ᵢ₌₁ᴰ [ xᵢlog yᵢ + (1-xᵢ)log (1-yᵢ) ] $$where yᵢ should be like a probability (need to do sigmoid activation). So in this case, this loss term is a cross entropy.\nFor a datapoint following multivariate Gaussian distribution N(𝐱; 𝛍, 𝛔²𝐈), refer to Su, Jianlin\n$$ \\begin{aligned} \u0026p(𝐱|𝐳) = \\frac{1}{∏ᵢ₌₁ᴰ \\sqrt{2πσᵢ²(𝐳)}} \\rm exp(-\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖²) \\\\\\ \\ \\\\\\ \u0026log\\ p(𝐱|𝐳) \\\\\\ \u0026= log \\frac{1}{∏ᵢ₌₁ᴰ \\sqrt{2πσᵢ²(𝐳)}} \\ -\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\\\ \u0026= -∑ᵢ₌₁ᴰ log \\sqrt{2πσᵢ²(𝐳)} -\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\\\ \u0026= -∑ᵢ₌₁ᴰ [\\frac{1}{2} (log2π + logσᵢ²(𝐳))] - \\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\\\ \u0026= -\\frac{D}{2} log2π -∑ᵢ₌₁ᴰlogσᵢ²(𝐳) -\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\\\ \\end{aligned} $$Normally, the variance 𝛔² will be fixed, so this loss term is only related with mean 𝛍(𝐳):\n-log p(𝐱|𝐳) ~ $\\frac{1}{2\\pmb σ²}\\\\| 𝐱-\\pmb μ(𝐳) \\\\|²$\nTherefore, this loss term is MSE.\n","date":"2023-06-07T10:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/vae/b-note-vae/","title":"read: Gen - ELBO | VAE"},{"content":"Code | Arxiv\nNotes Abstract (2023-05-24)\nModel the radiance field as a 4D tensor.\nOptimize a 4D tensor? Optimize a 4D matrix? MLP (network) is a 2D matrix? 𝐗 @ 𝐖ᵀ = 𝐘; Nonlinear regession cannot be solved by linear algebra? Gauss-Newton algorithm (2023-05-29)\nTerminology:\n\u0026ldquo;4D scene tensor\u0026rdquo;: (x, y, z, feature). Each voxel is allocated with a sigma and appearance feature. (Alike a feature field?).\n(2023-10-19) Features on each point derived from the coefficients of each vector in each basis. \u0026ldquo;factorize\u0026rdquo;: Encode the data into \u0026ldquo;coordinates\u0026rdquo; (coefficients) in the principle components;\n\u0026ldquo;multiple compact\u0026rdquo;: Principle components are orthogonal between each other;\n\u0026ldquo;low-rank\u0026rdquo;: The number of principle components is small. Rank=1 is a vector, rank\u0026gt;1 will depend on the rank of the matrix;\n\u0026ldquo;component\u0026rdquo;: The most \u0026ldquo;iconic sample\u0026rdquo;, which can be reshaped (reconstructed) to a data that has the original dimensionality,\nIn other words, it\u0026rsquo;s a \u0026ldquo;space\u0026rdquo; constituted by several directions; e.g., an image space has X and Y 2 directions.\nIn TensoRF, a vec + a mat is a component.\nFor a scene placed in a cube, one of components is an equal-sized cube, and the 3 directions are the X,Y,Z axes.\n(2023-10-19) Because the scene is the reconstruction goal, it served as the template of each basis. If the scene isn\u0026rsquo;t be reconstructed directly, for example, reconstring each ray (ro,rd,l,s), what\u0026rsquo;s the basis then?\nA 1D signal\u0026rsquo;s basis is sin+cos.\n(2023-10-19) Voxels are discrete samples from a continuous scene function. Coefficients on vectors of a basis are inner product between the scene function and each basis. Given some sample points (voxels), their coefficients are the coordinates of their projection onto the basis.\n\u0026ldquo;mode\u0026rdquo;: A principle component, a coordinate system, a space, a pattern;\nIntroduction \u0026ldquo;CAN-DECOMP/PARA-FAC\u0026rdquo;: Every direction in a component is a 1-dimensional vector.\n\u0026ldquo;Vector-matrix decomposition\u0026rdquo;: two directions out of 3 are jointly represented by a \u0026ldquo;frame\u0026rdquo; (plane),\nSo the obtained factors for 1 component are 1 vector and 1 matrix.\nVM decomposition spends more computation for more expressive components.\nThe \u0026ldquo;similarity\u0026rdquo; is computed \u0026ldquo;frame-by-frame\u0026rdquo;, so it needs more calculation. And the original structure is more kept than 2 dimensions are analyzed individually, so the components could be more representitive and less components are needed.\nBetter space complexity: O(n) with CP or O(n²) with VM,\nComparing with optimizing each voxel directly, which is O(n³), optimizing factors takes less memory.\nGradient descent\nThey\u0026rsquo;re not encoding the radiance field into factors because the feature grid/tensor is unknown. They decompose the feature grid to simplify the optimization, which then turns to optimizing factors.\nImplementation A scene is a radiance field.\nRadiance field (4D tensor) = Volume density grid (3D tensor: X,Y,Z) + Appearance feature grid (4D tensor: X,Y,Z,feat)\nRadiance field is a tensor of (X,Y,Z,σ+feature), where volume density σ is 1D, appearance feature is 27D;\n1D Volume density (feature) is decomposed to 3 vectors (CP) or 3 vector-matrix combo (VM) for each component.\n27D Appearance feature is amortized into 3 vector-matrix combos for 16 components.\nThese components are coefficients to fuse the data \u0026ldquo;basis vectors\u0026rdquo; in different ways, which is acting like a network.\n1D density and 27D features are optimized jointly with coefficients.\n(2023-10-28) Features and coefficients are optimized simultaneously. R a d i a n c e f i e l d = V o l u m e d e n s i t y ✚ S c S ( o ( m a f p p 2 e o p 7 a n D t e f n e t a ✶ t u r e 3 R s D G ✶ B c c o o = m m p p o o n n e e n n t t ) ⟩ where S is two-layer MLP: 150 → 128 → 128 → 3\n(2023-10-28) I guess the authors came up with tensor decomposition because they realized that Positional Encoding is Fourier decomposition, And NeRF then used MLP to learn the coefficients for the decomposed \u0026ldquo;Fourier basis\u0026rdquo;: sin and cos. Interpolate vector \u0026amp; matrix Instead of performing tri-linear interpolation at a point by computing 8 corners, a point at arbitrary position is computed by interpolating th vector and matrix.\nt r i - l i n e a r V . S . l i n e a r × b i - l i n e a r Code Notes Steps overview Dataset includes all_rgbs and corresponding ro, and normalized rd under world space all_rays, (N_imgs×H×W, 3)\nSplit the bounding box $[^{[-1.5, -1.5,\\ -1.5]}_{[1.5,\\quad 1.5,\\quad 1.5]}]$ into a voxel grid of the given resolution [128,128,128] determined by the number of voxels args.N_voxel_init\n1 reso_cur = N_to_reso(args.N_voxel_init, aabb) - 1 . 5 , - 1 . 5 , - 1 . 5 V ) o x ╰ e ╌ l ╌ ╌ s ╌ i ╌ z ╌ e ╌ ╌ 3 3 ╌ / z ╌ 1 ╌ 2 ╌ 8 ╌ , y ╌ ╌ s ╯ t e p S i z ( x e 1 . i 5 s , a 1 . h 5 a , l f 1 . 5 ) Sampling number nSamples = voxel grid diagnoal ∛(128²+128²+128²) / stepSize\nLearnable Parameters init_svd_volume() creates basis vectors and matrices. A vector and a matrix are orthogonal because they\u0026rsquo;re a side and a plane of a cube.\nParameters to be optimized: 3 Vectors and 3 Matrices\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def init_one_svd(self, n_component, gridSize, scale, device): plane_coef, line_coef = [], [] # 3 kinds of vec-mat combo, each combo has 16 components; for i in range(len(self.vecMode)): # vecMode: [2, 1, 0] vec_id = self.vecMode[i] # matMode: [(0,1),(0,2),(1,2)] mat_id_0, mat_id_1 = self.matMode[i] # (1, 16, 128, 128) plane_coef.append(torch.nn.Parameter( scale * torch.randn((1, n_component[i], gridSize[mat_id_1], gridSize[mat_id_0])))) # (1, 16, 128, 1) line_coef.append(torch.nn.Parameter( scale * torch.randn((1, n_component[i], gridSize[vec_id], 1)))) return torch.nn.ParameterList(plane_coef).to(device), torch.nn.ParameterList(line_coef).to(device) Given a 3D tensor, it can be decomposed as Vector-Matrix combo in three directions:\n= ✚ ✚ Each direction has 16 components. In other words, an observation of the cube from a direction can be reconstructed by summing those components up.\ndoubt: These 3 directions are orthogonal because the cube is viewed from distinct directions, but how are those 16 channels guaranteed to be orthogonal?\n(2023-10-17) Based on the theory of PCA? (2023-10-28) Is it possible that 16 components are parallel instead of orthogonal? They\u0026rsquo;re summed directly, similar to an FC layer with 16 neurons representing 16 ways of combining features. doubt: Are those components parallel to each other? Do they have different importance or priority?\n(2023-06-22) I guess no. They\u0026rsquo;re just added together simply. A scene is decomposed to a set of components, then a scene can be reconstructed using a set of coefficients of those components.\nSepcificlly, each voxel is a summation of the products for corresponding projections on vector and matrix in 3 directions and 16 components.\nBased on those vector-matrix components, with the help of interpolation, the value at any location can be obtained.\nFiltering rays Filter the effective rays based on the ratio (deviation) betweem the direction of rays and the direction of bounding box corners.\nMask those rays inside bbox by compareing the ratio of the rd to the direction of the two bbox corners:\n1 2 3 4 5 6 7 8 9 10 11 12 13 if bbox_only: # Avoid denominator of 0, normalized rd, (chunk, 3) vec = torch.where(rays_d == 0, torch.full_like(rays_d, 1e-6), rays_d) # ratio of the direction of bbox corner xyz_min to testing ray rate_a = (self.aabb[1] - rays_o) / vec # (chunk, 3) # ratio of the direction of bbox corner xyz_max to testing rd rate_b = (self.aabb[0] - rays_o) / vec t_min = torch.minimum(rate_a, rate_b).amax(-1) # [chunk] t_max = torch.maximum(rate_a, rate_b).amin(-1) # [chunk] # rays located inside the bbox mask_inbbox = t_max \u0026gt; t_min An effective ray should end up inside the bounding box,\nr o v e c ` x y ` z _ r ` m d i ✩ n x y z _ m a x ✩ is an effective ray, while ◯ is a non-effective ray because it\u0026rsquo;s out of the bbox.\nReconstruct sigma The sigma value on each voxel is the summation of 16 components of all 3 directions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1D scalar: (args.batch_size*N_samples) sigma_feature = torch.zeros((xyz_sampled.shape[0],), device=xyz_sampled.device) # traverse 3 modes for idx_plane in range(len(self.density_plane)): # interpolate tensor (1,16,128,128) at coordinates (batch_size*N_samples, 1, 2) plane_coef_point = F.grid_sample(self.density_plane[idx_plane], coordinate_plane[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1]) # (16, batch_size*N_samples) # interpolate tensor (1,16,128,1) at coordinates:(bs*N_samples, 1, 2) line_coef_point = F.grid_sample(self.density_line[idx_plane], coordinate_line[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1]) # (16, batch_size*N_samples) # accumulate 16 components of 3 directions for each voxel, (batch_size*N_samples) sigma_feature = sigma_feature + torch.sum(plane_coef_point * line_coef_point, dim=0) The factors grid is composed of 3 planes self.density_plane and 3 vectors self.density_line\nThe factors corresponding to the sampled 3D points are obtained by F.grid_sample():\np l a n e \" 0 d - e 1 s \" n i t ⬣ 1 y 6 p c l ⬣ h a a n n e n c ⬣ e ( o l 1 o s 2 r 8 d ⬣ , i 1 n 2 a 8 t ) e _ 0 p l a n 2 e \" 1 1 - 2 \" c \" o 0 o \" r , d \" i 1 n \" a , t d e o e _ r n ( l s 1 i \" i 2 n 2 t 8 e \" y , 1 l ) i n e Retrieve the \u0026ldquo;factor in the vector direction\u0026rdquo; for each sample voxel is like the above figure right.\ns f i e g a m t a u r e = p v r ( e o 1 V c j 6 e t e c o c c r t o - i m 2 o ＋ ⋮ p x × n o n p e m r n M a o t a t j s t r e ) i c x t - i 0 o 1 n ✚ p v r e o ( V c j 1 e t e 6 c o c r t c - i ＋ o 1 o ⋮ m x × n p o p n m r e M a o n a t j t t r e s i c ) x t - i 0 o 2 n ✚ p v r e o ( V c j 1 e t e 6 c o c r t ＋ c - i o 0 o ⋮ m x × n p o p n m r e M a o n a t j t t r e s i c ) x t - i 1 o 2 n doubt: Were the coefficients not multiplied with basis vector, but simply summed up together as the reconstruction?\n(2023-06-29) TensoRF is not projecting voxel onto each basis vector (matrix), but retrieving the coefficients from the factor grid.\nWhat the TensoRF retrieved is the coefficient * vector because it samples the factor grid directly. The factor grid satifies orthogonality naturally, so a coefficient inside is equivalent to having already multiplied with basis vectors.\nReconstruct appear. feature For appearance feature of each voxel, the vector projection and matrix projection in 3 directions are concatenated together, then multiply together:\n1 2 3 4 5 6 7 8 9 10 11 12 plane_coef_point, line_coef_point = [],[] for idx_plane in range(len(self.app_plane)): plane_coef_point.append(F.grid_sample(self.app_plane[idx_plane], coordinate_plane[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1])) line_coef_point.append(F.grid_sample(self.app_line[idx_plane], coordinate_line[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1])) plane_coef_point, line_coef_point = torch.cat(plane_coef_point), torch.cat(line_coef_point) return self.basis_mat((plane_coef_point * line_coef_point).T) 1 1 l 4 p 4 i 4 l 4 f n a e 2 e c n c a a 7 _ h e h p t c a _ a p u d o n c n r i e n o n e m f e e e = = l = f l s s ( 4 ( ❲ p 8 p 4 v r m r 8 V e o c a o 1 e c j ⋮ o t j ⋮ c 4 c t e m r e o 4 o c p i c m × x r t o x t p d - i n - i o i M 2 o e 0 o n m a n n 1 n e t t n s t ❳ ) s ) × ⊕ ⊕ ( 1 4 ( 4 p 8 m p 4 4 v r a r 8 x e o c t o 2 c j ⋮ o r j ⋮ c 7 t e m i e o o c p x c m b r t o - t p a - i n 0 i o s 1 o e 2 o n i n n n e s t n s t m ) s a ) t ⊕ ⊕ ( 4 ( p 8 m p 4 v r a r 8 e o c t o c j ⋮ o r j ⋮ c t e m i e o o c p x c m r t o - t p - i n 1 i o 0 o e 2 o n n n n e t n s t ) s ) Then RGB is mapped from the appearance featrue:\nr g 3 b = M L P ❲ f e a a 2 p t 7 p u r e ⊕ v d i i 3 e r w ⊕ a p p e p f 1 e 0 ⊕ a 8 t v i p 1 e e 2 w ❳ Optimizing The learnable parameters includes:\n16 vectors and 16 matrices for density in 3 directions; 48 vec and 48 mat for app feature in 3 directions; linear layer transforming 48x3 dim appearance feat to 27D; linear layer mapping 27D feat+viewdir to 3-dim rgb. flowchart LR x(\"Sample a point\") y(\"Reconstruct its sigma and rgb by aggregating its components\") b(\"Use BP+GD (Adam) to optimize those parameters\") x --\u003e y --\u003e b Losses: L2 rendering loss + L1 norm loss + Total Variation loss.\nTV loss benefits real datasets with few input images, like LLFF. Q\u0026amp;A How does it ensure that the components are orthogonal during training? (2023-06-10) Reference {论文代码阅读}TensoRF: Tensorial Radiance Fields - 爱睡觉的人人\n","date":"2023-05-29T18:18:00Z","image":"https://apchenstu.github.io/TensoRF/img/pipeline.png","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-tensorf/","title":"read: Render - NVS | TensoRF"},{"content":"Repo\nStandalone Use (2024/06/16)\nDownload package and Install go\n1 2 3 4 5 6 cd Programs/ # dir of the package su # installing it requires root rm -rf /usr/local/go \u0026amp;\u0026amp; tar -C /usr/local -xzf go1.22.4.linux-amd64.tar.gz nvim ~/.bashrc # write: export PATH=$PATH:/usr/local/go/bin go version Install goat:\n1 go install github.com/blampe/goat/cmd/goat@latest Check the usage of goat:\n1 ./go/bin/goat -h Use goat to convert a txt file to svg image: (Asked chatGPT how to use goat. Althgouh it answered the usage of the npm version, I saw the -o argument.)\n1 ./go/bin/goat -i test.txt -o test.svg Notes:\nSome unicode symbols that are wider than 2:1 aspect ratio, such as ✚, ⊖, cannot be correctly converted to svg. And it will cause overleaf to prompt errors. (2023-05-29)\nThis diagram is supported by Hugo natively.\nMarkdeep can do more complicated, but it writes .html file. (Mentioned in Edge cases don\u0026rsquo;t render correctly #11) Ascii-art Archive\nTrees 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 Overlaps Line Decorations Line Ends Dot Grids · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · Large Nodes A 1 2 3 4 B 5 C 6 8 7 D Small Grids a b a b a b a b Big Grids A B A B A B Complicated \u0026amp; A M S i o i q f b B x u j o e a ( - x d r a \u0026gt; e f R \u0026gt; c o C n u o b ( n r ) ) d n e e J d r o s i n N o R D t o i u a a n g d d l o i t n e D i a g o n a l s C V u e r r v t e i d c a l n o t A N C : o u l d r r i A a / I v n s i n e e h s t d - e - t r l B i h i i s i o n ' s r e q n . u * o o b t t o e l a s d ' * l i n e D o n S e e ? a r c 3 h Ascil Art Cube , 1 2 8 9 6 9 6 ` ` . . ` ` ` ` . . ` ` . . ` . ` ` . ` . Coordinates z x p l a n e y f g ₁ : ₁ : x a ₁ f g ₂ : ₂ : x ₂ f : g : : : x ₃ ₃ ₃ f : g : : : : x ₙ ₙ ₙ b f g ( ( x x x ) ) f ₁ x ₁ f f ( ₂ x x ₂ ) f x ∑ ₃ ₃ _ { k ∈ ℤ } δ ( f x t ₙ ₙ - k T ) x w w ( x ₀ , x S ₁ c ) r ᵀ e e n I k n e x t r ₂ e n g e ( P c ( r l O r e x a s r o n ₀ l s t j t , ϕ R p h e e x ( a a o c r ₁ φ y c g t , ( e o i r x 𝐮 n o ' ₂ ) a n ( ) ) l 𝐱 ᵀ ϕ t ) ) 𝐖 r N a 𝐮 D n + C s ( b f P r φ C s o ( a p s 𝐮 m a p ) e c e r e c a t φ t i 𝐌 r v v a e 𝐮 i n ) + e s c w f K O e b r j n e e c l t P i s f r n p e a ⊗ f c i e h l t e r 𝐮 NeRF vs EWA:\nP r o j c e a c m t e r p a o i 1 n t s P r o j e c t R p r a a a l s y r y i c s a n r l s e e ∫ i a l p e L s r e a s n i e l c e n f e g e o i s o n s t e p g r i n t PixelNeRF:\nP r ◸ o j e c v t i e p w o i 1 v n i t e s w 2 Ellipses . \" ~ • - - - o ~ . \" \" - _ \" - - - • - - - ∘ - . - _ \" - \" . \" ∘ • \" . Splat\nG a i 2 u a D s n s s S - c r e e n . \" • \" . V D o e l p u t m h e h . \" d e ~ a r t e a i h s a s ‖ 𝐭 b ‖ • e ₂ e . n C t o h v r o m ~ w a . \" n t r o i n x t o i s t h 𝚺 e ' . s c r e e n ","date":"2023-05-29T09:58:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/go_goat/","title":"memo: GoAT Ascii Diagram"},{"content":"Course page: AM301\nSingular Value Decomposition Source video: Lecture: The Singular Value Decomposition (SVD) - AMATH301\nlinalg: Matrix acts on vector All what matrix multiplication does is it rotates and stretch the vectors.\nPrincipal Component Analysis Source video: Lecture: Principal Componenet Analysis (PCA)\nPhysical example Considering a physical system as the following, a rod suspends a mass via a spring.\nr o d m s p k r i | n g f ( z ) Pull the mass down and let it oscillate up and down.\nThe displacement (position) of the mass oscillation can be measured by a function f(z). Then f(z) can be solved based on Newton\u0026rsquo;s second law:\n$$ \\begin{aligned} F \u0026= ma = m \\frac{d²f(z)}{dz²} \\\\\\ \\ \\\\\\ -w²f(z) \u0026= m \\frac{d²f(z)}{dz²} f(z) = Acos(wz + w₀) \\end{aligned} $$Data-driven approach can solve it too.\nSuppose the law $F=ma$ is unknown, infer the F=ma from the data alone. First of all, the complexity of this system should be figured out.\nMeasure this system using cameras\nc a m e r a 1 r o d m s p k r i | n g f ( z ) c a c m a e m r e a r a 3 2 Every camera records the mass coordinates on their projection plane:\nCamera 1: (𝐱$_a$, 𝐲$_a$); Camera 1: (𝐱$_b$, 𝐲$_b$); Camera 1: (𝐱$_c$, 𝐲$_c$).\nArrange them into a data matrix 𝐗 (6-rows):\n$$ 𝐗 = \\begin{bmatrix}𝐱_a \\\\\\ 𝐲_a \\\\\\ 𝐱_b \\\\\\ 𝐲_b \\\\\\ 𝐱_c \\\\\\ 𝐲_c \\end{bmatrix} $$Two fundamental issuses associated with these data need to be addressed.\nNoise\nData with noise on top of it is not a good representation of the system.\nRedundancy\nMeasurements are not independent to each other, i.e., x and y are related. Different cameras take the similar infomation just from different angles.\nThe movement is only one degree of freedom, but the observed data has six sets.\nOne doesn\u0026rsquo;t know how to take the perfect observation ahead of time. PCA will reveal which camera at which angle is enough to describe the whole system.\nVariance and Covariance Assumption: big variance score means that vector is chaning a lot. It has a lot stuff happening.\nIf the diagonal terms that are big in the covariance matrix , those vectors are matter. Vectors having small variance don\u0026rsquo;t change much. MAKE DIAGNOAL (SVD) is remove all the redundancy\nPCA for Face Recognition Source video: Lecture: PCA for Face Recognition\nA 2D image are flattened into a vector. So each column in the U returned from SVD is an image. So a column essentially contains two directions: x and y. ","date":"2023-05-27T22:20:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ala-nathan/02_svd/","title":"watch: Nathan 02 | SVD"},{"content":"Jacobian matrix Khan Academy\nPrerequisite Matrix is a linear transformation of space by moving the basis vectors to new landing spot.\n$$ \\begin{bmatrix} 2 \u0026 -3 \\\\\\ 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} x \\\\\\ y \\end{bmatrix} \\rightarrow \\begin{bmatrix} 2x + (-3)y \\\\\\ 1x+1y \\end{bmatrix} $$ \u0026ldquo;linear\u0026rdquo;: grid lines remain parallel, evenly spaced and straight lines after transformation.\nTransform\nThe two basis vectors $[^1_0]$ and $[^0_1]$ are moved to $[^2_1]$ and $[^{-3}_{1}]$, which are the columns of the matrix.\n$$ \\begin{array}{ccc} \\begin{bmatrix} 2 \u0026 -3 \\\\\\ 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2+0 \\\\\\ 1+0 \\end{bmatrix} \\\\\\ \\\\\\ \\begin{bmatrix} 2 \u0026 -3 \\\\\\ 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0-3 \\\\\\ 0+1 \\end{bmatrix} \\end{array} $$ Reconstruct\nThe basis vectors $𝐢 = [i₁,i₂], 𝐣 = [j₁,j₂]$ of the new space are completely made up by original basis vectors $[^1_0]$ and $[^0_1]$:\n$$ \\begin{array}{ccc} 𝐢 = \\begin{bmatrix} i_1 \\\\\\ i_2 \\end{bmatrix} = \\begin{bmatrix} 2×1 + (-3)×0 \\\\\\ 1×1 + 1×0 \\end{bmatrix} \\begin{matrix} \\text{first component}\\\\\\ \\text{second component}\\end{matrix} \\\\\\ \\\\\\ 𝐣 = \\begin{bmatrix} j_1 \\\\\\ j_2 \\end{bmatrix} = \\begin{bmatrix} 2×0 + (-3)×1 \\\\\\ 1×0 + 1×1 \\end{bmatrix} \\begin{matrix} \\text{first component}\\\\\\ \\text{second component}\\end{matrix} \\end{array} $$ Record\nMatrix records synthesising factor for transforming each original basis vector to the new basis vector:\nThe first row of the matrix is the combination factors for the original first basis vector to form the first component of the new first basis vector.\nThe second row corresponds to the second component of the new first basis vector, by re-combining the original first basis vector. And so forth.\nLiner Operator\nMatrix operator 𝑳 takes in a vector and spit out a vector.\nBased on the properties of linearity: scaling and adding, i.e.,\nL(a𝐯) = aL(𝐯) L(𝐯+𝐰) = L(𝐯) + L(𝐰) Applying the operator on a vector $[^x_y]$ can be represented as:\n$$ \\begin{aligned} L(\\begin{bmatrix} x \\\\\\ y \\end{bmatrix}) \u0026= L(x \\begin{bmatrix} 1\\\\\\ 0 \\end{bmatrix} + y \\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix}) \\\\\\ \u0026= x L(\\begin{bmatrix} 1 \\\\\\ 0\\end{bmatrix}) + y L(\\begin{bmatrix} 0 \\\\\\ 1 \\end{bmatrix}) \\end{aligned} $$Let $[^x_y]$ equals to $[^2_1]$, the result vector should be 2 times of $L([^1_0])$ plus 3 times of $L([^0_1])$, i.e., $[^1_3]$\nThat means matrix transforms the space by modifying the original basis vectors. The coordinates of the given vector have no change either in the original space or the new space.\nSo the transformation can be found by determining the coefficient of the change of each component.\nThe first column in the transformation matrix is landing spot of the first basis vector (i.e., x axis)\nlocal linearity Nonlinear function $f(\\[^x_y\\]) = \\rm [^{x+sin(y)}_{y+sin(x)}]$ has local linearity. Therefore, the local linear transformation can be represented by a 2x2 matrix\nJacobian matrix There are multiple functions and multiple variables. Jacobian records the effects of each variable on each function. Its organization comes down to local linearity, which allows the nonlinear transformation to be represented as a linear transformation.\nJacobian determinant Jacobian determinant is the factor of the neighbor area around a point scratched or squished by the local \u0026ldquo;linear transformation\u0026rdquo;.\ntodo: What is Jacobian? | The right way of thinking derivatives and integrals - Mathemaniac\nGauss-Newton algorithm 最优化算法之高斯牛顿法-path-int\n用雅克比矩阵的乘积 J(𝐱) J(𝐱)ᵀ 代替 Hessian 矩阵\n-拼凑梦想-\n初值会影响能否走到全局最优\n视觉slam 十四讲第6章\nGauss-Newton algorithm for solving non linear least squares explained\n（干货）《雅可比矩阵是什么东西》3Blue1Brown，搬自可汗学院。 【自制中文字幕】\n(2023-12-24) \u0026ldquo;In [8], an LSTM [18] is used to model the Levenberg-Marquardt (LM) algorithm and predicts the update at each step directly.\u0026rdquo; mentioned in Fast-MVSNet.\nLevenberg Marquadt code example\npaper METHODS FOR NON-LINEAR LEAST SQUARES PROBLEMS\n","date":"2023-05-26T13:58:00Z","image":"https://img.youtube.com/vi/Vnga_psnCAo/maxresdefault.jpg","permalink":"http://blog.zichen.uk/post/writenotes/calc/nonlinear_least_squares/","title":"memo: Calc | Nonlinear Least Squares"},{"content":"Book site; book\nⅠ. Curve Fitting Sec 4.1: Classic Curve Fitting and Least-Squares Regression\nRegression ≈ curve fitting ≈ 𝐀𝐱=𝐛 (Least-square fitting), where 𝐀 is the data, 𝐱 is the parameters of the model, 𝐛 is the target.\nRegression: Fitting a model to some data with some parameters\nOver- and Under-determined Models could be Over- and Under-determined.\nOver-determined system normally has No solution.\nThere\u0026rsquo;re massive constraints (equations, samples) given, but the complexity (#variables) of the system is not enough to describe the existing data.\nAn over-determined system can be \u0026ldquo;No solution\u0026rdquo; or \u0026ldquo;Infinitely many solution\u0026rdquo; Over~ wiki\nHomogeneous case: (no bias)\nThe coefficient matrix is a tall, skinny matrix, the \u0026ldquo;all-zero solution\u0026rdquo; always holds. If there\u0026rsquo;re enough equations are dependent, and after eliminating #non-zero row $\u003c$ #cols in the coefficient matrix in row-echelon form, this over-determined homogeneous system has \u0026ldquo;infinitely many solutions\u0026rdquo; (including all-0 solution). Otherwise, \u0026ldquo;all-zero\u0026rdquo; is the only solution. Non-homogeneous case:\nIf the last non-zero row of the augmented matrix in row echelon form is only having the constant entry of the last column (giving an equaltion 0=c), this system has \u0026ldquo;No solution\u0026rdquo;.\nSince it\u0026rsquo;s a tall matrix, this case is likely to happen:\n$$ \\left[ \\begin{array}{cc|c} 2 \u0026 1 \u0026 -1 \\\\\\ -3 \u0026 1 \u0026 -2 \\\\\\ -1 \u0026 1 \u0026 1 \\\\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc|c} 2 \u0026 1 \u0026 -1 \\\\\\ 0 \u0026 5/2 \u0026 -7/2 \\\\\\ 0 \u0026 3/2 \u0026 1/2 \\\\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc|c} 2 \u0026 1 \u0026 -1 \\\\\\ 0 \u0026 5/2 \u0026 -7/2 \\\\\\ 0 \u0026 0 \u0026 13/5 \\\\\\ \\end{array} \\right] $$\nSince the equations are way more than unknowns, the coefficient matrix in row echelon form is most likely not having: #non-zero rows = #cols. So the case \u0026ldquo;single unique solution\u0026rdquo; is almost impossible, but there\u0026rsquo;ll be \u0026ldquo;Infinite many solutions\u0026rdquo;.\nUnless, enough rows can be eliminated (lines overlap), and the remaining coefficient matrix has the same rank as the augmented matrix, and also the #non-zero rows = #cols in the coefficient matrix, this system has \u0026ldquo;single unique solution\u0026rdquo;.\n(2024-05-31)\n如果约束太多，就变成“超定系统”了，通常无解； 此时就要引入更多的“未知量”，变成一个“欠定系统”，就有无穷多解了\n之前我说老弟顾忌的东西太多，无解；所以要引入“变量”。多看多尝试不同的东西，有人就有变数，变则通。\nUnder-determined system normally has infinitely many solutions.\nBecause the #parameters is more than equations (samples), some variables are not restricted, which caused the infinitude of solutions. In other words, there\u0026rsquo;re not enough equations to uniquely determine each unknown. So the problems is how to determine which solution is the best.\nAn underdetermined linear system has either No solution or Infinitely many solutions.Under~ wiki\nHomogeneous case: The coefficient matrix is a short, fat matrix, #non-zero rows $\u003c$ #cols. So the system always has \u0026ldquo;Infinitely many solutions\u0026rdquo; (including all-0 solution). Non-homogeneous case: Rank of augmented matrix \u0026gt; Rank of coefficient matrix: No solution Rank of augmented matrix = Rank of coefficient matrix: There must be some solutions. But since the #row is already less than #cols for the coefficient matrix, the single unique solution (except for all-0) is impossible. So this system indeed has infinitely many potential solutions. $$ \\left[ \\begin{array}{ccc|c} 1 \u0026 1 \u0026 1 \u0026 1 \\\\\\ 1 \u0026 1 \u0026 2 \u0026 3 \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{ccc|c} 1 \u0026 1 \u0026 1 \u0026 1 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 2 \\end{array} \\right] $$ How to solve If there\u0026rsquo;re infinite optional solutions, then the best model should be selected according to other constraints, i.e., regularizers g(𝐱).\nFor instance, the penalty for L2 norm of the parameters vector ‖𝐱‖₂ can lead to the model with smallest mse; And the penalty for L1 norm of the parameters vector ‖𝐱‖₁ can lead to the model with sparse number of parameters.\nTherefore, the objective is:\n$argmin_𝐱 g(𝐱)$ subject to ‖𝐀𝐱-𝐛‖₂=0.\nIf some error can be tolerated to just find the model that has the minimum L2-norm params, the constraint can be relaxed by a small error, like: ‖𝐀𝐱-𝐛‖₂≤ε\nIf there\u0026rsquo;s no solution, the expected model should have the minimum error.\nTherefore, the objective is: $argmin_𝐱$ (‖𝐀𝐱-𝐛‖₂).\nAdding regularizers can confine the optimizer to find the right model, so the objective can be\n$argmin_𝐱$ (‖𝐀𝐱-𝐛‖₂ + λg(𝐱))\nOrdinary least squares gives the approximate solutions (when no exact solution exists) or the exact solution (when it exists) by minimizing the square error:\n$argmin_𝐱$ ‖𝐀𝐱-𝐛‖\nIts solution is 𝐱 = (𝐀ᵀ𝐀)⁻¹𝐀ᵀ𝐛. And using QR factorization of A to solve the least squares can achieve good numerical accuracy.\nMore broadly, this generic architecture can be generalized to non-linear regressions by replacing ‖𝐀𝐱-𝐛‖₂ to non-linear constraints f(𝐀,𝐱,𝐛):\n$$ argmin_𝐱 ( f(𝐀,𝐱,𝐛) + λg(𝐱)) \\quad \\text{(Over-determined) or} \\\\\\ argmin_𝐱 g(𝐱) \\ \\text{subject to } f(𝐀,𝐱,𝐛) ≤ ε \\quad \\text{(Under-determined)} $$An over-determined non-linear system can be solved by Gauss-Newton iteration\nOver- and Under-fitting Two canonical issues:\nOver-fitting: With enhencing the model complexity, the training error keeps dropping, but the evaluating error on withhold data goes up.\nUnder-fitting: No matter how you increase the model complexity (parameters), the training error doesn\u0026rsquo;t drop, either that\u0026rsquo;s a bad model or there\u0026rsquo;s not enough data.\nRegression framework Generic Regression: 𝐘 = 𝑓(𝐗,𝛃), input 𝐗 into model 𝑓, then target 𝐘 can be obtained.\nRegression needs 2 things: select a model 𝑓 and find its parameters beta that can map X to Y.\nDifferent norms are used to measure the error: L∞ norm, L1 norm, L2 norm. And the selected norm (error metric) has big impact on the \u0026ldquo;Goodness of Fit\u0026rdquo;.\nⅡ. Nonlinear Regression Sec 4.2: Nonlinear Regression and Gradient Descent\nⅢ. Regularization Sec 4.3: Over- and Under-determined Systems; Code demo\nⅣ. Optimization Sec 4.4: Optimization for regression\nOptimization is the cornerstone of regression.\nOptimization is to minimize some objective function. Regression is finding the parameters of the given model that maps the input to the output. Regularization is Critical\nRegularizers are what determine the type solutions obtained. Simple Example The data is generated from parabola + noise $f(x) = x^2 + 𝐍(0,σ)$. However, the model in practice is unknown, and regularization can help find better models.\nGiven 100 Realizations of data. And the model is represented as $𝐘 = 𝑓(𝐗,β)$\nSmall noise results complex predicted models.\nDifferent regression 1 2 3 f = (x.^2 + 0.2 * rand(1,n)).\u0026#39;; lambda = 0.1; phi pinv(): Pseudo-inverse (Least square with the minimum L2 norm) \\ (bashslash): QR decomposition lasso() robustfit() () Least-square fit pinv get very different model for samll pertubation every time and all the coefficients are functioning. The uncertainty of this solver is high.\nThe high-dgree terms are highly volatile, which makes the model unstable.\nParsimony (Pareto front) Keep adding higher degree polynomials to the model, the error will no longer decrease at some point and instead even rise up.\nPareto front Sec 4.7: The Pareto front and Lex Parsimoniae\nSmall number of parameters can bring interperity\nConnect NNs Sec Neural Networks: 1-Layer Networks\n","date":"2023-05-23T17:24:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ala-nathan/10_regression_model_selection/","title":"watch: Nathan 10 | Regression and Model Selection"},{"content":"Course page: AM584\nDescovered by Ytb search: CANDECOMP/PARAFAC (CP) decomposition\nTensor Decomposition Source video Applied Linear Algebra: Tensor Decompositions\nTensor is the generalization of matrix. Tensor decomposition is doing multiple times SVD to extract the corelation in each dimension.\nOne piece of data could be a matrix, e.g., the temperture distribution in a region of latitude and longitude.\nBy stacking the data matrices of different time points, the obtained data forms a cube.\nX ⱼ ` ` . . ` ` X ` ` . . Vectorizing Vectorize a (n-dimensional) array is to reshape it into a vector.\nStacking the columns on top of each other.\nX ⱼ : X ⱼ O r ` ` . . ` ` ` ` . . : X ⱼ Vectorizing doesn\u0026rsquo;t bother the similarity against each smaples? because the total amount of difference is fixed? No matter using which kind of metric, the normalized distinctions between each other are fixed?\nSVD By flattening each sample, and arrange them into a matrix, then SVD can be applied.\nReconstruct the matrix A with low-rank structures?\nm n A = σ ₁ u 1 v 1 + σ ₂ u ₂ v ₂ + σ ₃ u ₃ v ₃ + ⋯ ⋯ To avoid vectorization (flattening) messes up or mixes together the independent attributes (temperture, pressure, and humidity) and keep them in their own dimensions separately, SVD needs to be applied for every dimension.\nDecompose a data cube 𝓜 by performing SVD in 3 directions:\nc σ ⱼ ₁ a ₁ a ⱼ b ₁ c ₁ b ⱼ = + σ ₂ a ₂ b ₂ c ₂ + σ ₃ a ₃ b ₃ c ₃ + ⋯ ⋯ A dominant vector is responsible for its homologous vectors, having the same dimensionality.\nFor example, the above 5-dimensional vector is representing for the vectors parallel to it.\nAs the following hand illustration shows, Align the fingers with the vector and move towards the direction that the palm is facing. The vectors on the hand path will be represented.\n(image comes from here)\nReconstruct The r-rank approximation to the data cube M is the sum of the outer product of the dominant vectors in 3 directions.\n𝓜 = ∑ⱼ₌₁ʳ σⱼ∘aⱼ∘bⱼ∘cⱼ\nAll individual dominant vectors aⱼ constitute 𝓐ᵣ. And 𝓑ᵣ is the collection of all the bⱼ, and so do 𝓒ᵣ.\n𝓜 = 𝓐ᵣ ∘ 𝓑ᵣ ∘ 𝓒ᵣ\nInner and Outer product: Given two column vectors: $𝐮=[^{{u_1}}\\_{^{u_2}\\_{u_3}}]$ and $𝐯=[^{v_1}\\_{^{v_2}\\_{v_3}}]$,\nInner product (equals dot product):\n$$ \\rm uᵀv = (u₁,u₂,u₃) [^{v_1}\\_{^{v_2}\\_{v_3}}] = u₁v₁ +u₂v₂ + u₃v₃ \\text{= scalar} $$ Outer prodcut: $$ \\rm 𝐮⊗𝐯 = uvᵀ = [^{{u_1}}\\_{^{u_2}\\_{u_3}}] (v₁,v₂,v₃) \\\\\\ = \\begin{bmatrix} \\rm u₁v₁ \u0026 \\rm u₁v₂ \u0026 \\rm u₁v₃ \\\\\\ \\rm u₂v₁ \u0026 \\rm u₂v₂ \u0026 \\rm u₂v₃ \\\\\\ \\rm u₃v₁ \u0026 \\rm u₃v₂ \u0026 \\rm u₃v₃ \\end{bmatrix} $$Any two vectors (with difference lengths) can perform outer product:\n$$ \\rm (^{v₁}_{v₂}) ⊗ (^{w₁}\\_{^{w₂}\\_{w₃}}) = \\begin{bmatrix} \\rm v₁w₁ \u0026 \\rm v₁w₂ \u0026 \\rm v₁w₃ \\\\\\ \\rm v₂w₁ \u0026 \\rm v₂w₂ \u0026 \\rm v₂w₃ \\end{bmatrix} $$Each entry in the matrix is a scalar and can be indexed by: (𝐯 ⊗ 𝐰)ᵢⱼ= 𝐯ᵢ⋅𝐰ⱼ, the product of an element in the first vector with an element in the second vector.\nVector-matrix outer product\n1 2 3 4 5 6 7 8 9 \u0026gt;\u0026gt;\u0026gt; v = torch.tensor([0,1]) # [2] \u0026gt;\u0026gt;\u0026gt; m = torch.tensor([[2,0],[1,3]]) # [2, 2] \u0026gt;\u0026gt;\u0026gt; torch.einsum(\u0026#39;p,qr-\u0026gt; pqr\u0026#39;,v,m) tensor([[[0, 0], [0, 0]], [[2, 0], [1, 3]]]) Pytorch batch matrix vector outer product - SO\nThe SVD decomposition can be interpreted as the sum of outer products of each left (𝐮ₖ) and right (𝐯ₖ) singular vectors, scaled by the corresponding nonzero signular value σₖ wiki\n𝐀 = 𝐔 𝚺 𝐕ᵀ = $∑_{k=1}^{rank(A)}$ (𝐮ₖ ⊗ 𝐯ₖ)σₖ\nCode Demo Source video: Applied Linear Algebra: Implementing Tensor Decompositions\nDataset The data cube is generated by a spatial temporal function. It\u0026rsquo;s like a video having x and y directions and chaning in time.\ny x t = σ ₁ m o t d ₁ e 1 x ₁ y ₁ + σ ₂ m o a d ₂ e 2 x ₂ y ₂ The function mixes two fundamental features (modes) together. So the decomposition method should pull out the the two features back.\nFeature 1: $f₁(x,y,t) = e^{-(x²+0.5*y²)} cos(2t)$, the spatial structure is a elliptical shaped gaussian, and this bump oscillate up and down in time. Feature 2: $f₂(x,y,t) = sech(x)tanh(x) e^{-0.2y^2} sin(t)$, x direction is the shape -∿-, y direction is gaussian bell, and it oscillates with sine t Data matrix: F = f₁ + f₂ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 clear all; close all; clc % Basic domains (each frame is a rectangle) x = -5:0.1:5; y = -6:0.1:6; t = 0:0.1:10*pi; [X, Y, T] = meshgrid(x, y, t); % Data cube f1 = exp(-(X.^2 + 0.5*Y.^2)).*(cos(2*T)); % (Y,X,t): (121, 101, 315) f2 = (sech(X).*tanh(X).*exp(-0.2*Y.^2)).*sin(T) A = f1 + f2; % Tensor % Visualizing the shapes for j = 1:length(t): surfl(x, y, f1(:,:,j); %surfl(x, y, f2(:,:,j); %axis([-5,5], [-6,6], [-1,1]); colormap(hot) shading interp; drawnow end % Vectorizing (for performing standard SVD) nx = length(x); ny = length(y); for j = 1:length(t): % Every flattened column vector is appended to the matrix Af Af(:,j) = reshape(A(:,:,j), nx*ny, 1); % (nx*ny, t) end % Make matrix Af back to a tensor for j = 1:length(t) At(:,:,j) = reshape(Af(:,j), nx, ny) end % n-way array tensor decomposition (parallel factorization algorithm) % Given a hypercube of data, it will find the dominant factors in each direction. % Input tensor and rank (# of dominant components truncated) model = parafac(A,2); % Determine factors in each directions from each low-rank projections % If A is a 10-dim tensor, A1,A2,...,A10 directions are expected. % A1 has 2 factors, A2 has 2 factors, ... [A1, A2, A3] = fac2let(model); % Plot the 2 factors (vectors) in each component. subplot(3,1,1), plot(y, A1, \u0026#39;Linewidth\u0026#39;, [2]) % subplot(3,1,2), plot(x, A2, \u0026#39;Linewidth\u0026#39;, [2]) subplot(3,1,3), plot(t, A3, \u0026#39;Linewidth\u0026#39;, [2]) In y direction has two gaussian: $e^{-0.5y^2}$ and $e^(-0.2y^2)$\nIn x direction, there are a gaussian $e^(-x^2)$ and $sech(x)*tanh(x)$\nIn t direciton, cos(2t) and sin(t) are pulled out.\n","date":"2023-05-22T18:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ala-nathan/09_tensor_decompositions/","title":"watch: Nathan 09 | Tensor Decompositions"},{"content":"Code | Arxiv(2308) | ProjPage\nCode: wanmeihuali/taichi_3d_gaussian_splatting\nVideo Explain (2023-09-11)\nSource video: 【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！- 意の茗\nCG technique: Splatting\nEach point is an anisotropic 3D Gaussian distribution:\nmean is point location (xyz), covariance matrix determined the shape of the 3D Gaussian Optimize points\u0026rsquo; distribution:\nLocations are at mean, which are needless to learn; Co-variance matrix is converted to quaternion 3D Gaussian distribution can do clone and split to fit complex geometry.\nTile-based fast rendering rather volume rendering.\n3D Gaussian (2023-10-28)\n1D Gaussian distribution:\nGiven a scalar $x \\sim N(μ,σ²)$, its PDF:\n$$ p(x) = \\frac{1}{\\sqrt{2πσ²}} e^{-\\frac{(x-μ)²}{2σ²}} $$ 3D Gaussian composed of 3 independent 1D Gaussian in 3 directions can be represented as below.\nGiven a vector 𝐯: [a,b,c], its PDF: p(𝐯) = p(a) p(b) p(c)\n$$ \\begin{aligned} p(𝐯) \u0026= \\frac{1}{(2π)^{3/2}σₐ σ_b σ_c} exp(-\\frac{(a-μₐ)²}{2σₐ²} -\\frac{(b-μ_b)²}{2σ_b²} -\\frac{(c-μ_c)²}{2σ_c²}) \\\\\\ (\\text{vectorize}) \u0026= \\frac{1}{(2π)^{3/2} |Σ|^½} exp(-½⋅(𝐯-\\bm μ)ᵀ(𝐯-\\bm μ) Σ⁻¹) \\end{aligned} $$ Where the $σₐ σ_b σ_c$ is the square root of the determinant of covariance matrix Σ:\n$$ |Σ|^{½} = \\begin{vmatrix} σₐ² \u0026 0 \u0026 0 \\\\\\ 0 \u0026 σ_b² \u0026 0 \\\\\\ 0 \u0026 0 \u0026 σ_c² \\end{vmatrix}^{1/2} = σₐ σ_b σ_c $$ The exponent can be derived as:\n$$ \\begin{array}{ccc} \\begin{bmatrix} a-μ_a \\\\\\ b-μ_b \\\\\\ c-μ_c \\end{bmatrix} \\begin{bmatrix} a-μ_a \u0026 b-μ_b \u0026 c-μ_c \\end{bmatrix} \\begin{bmatrix} 1/σₐ² \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1/σ_b² \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1/σ_c² \\end{bmatrix} \\\\\\ = \\frac{(a-μ_a)²}{σₐ²} + \\frac{(b-μ_b)²}{σ_b²} + \\frac{(c-μ_c)²}{σ_c²} \\end{array} $$ If those 3 1D Gaussian are all $N(μ=0, σ²=1)$, the 3D Gaussian becomes simpler:\n$$\\rm p(𝐯) = \\frac{1}{(2π)^{3/2}} exp(-\\frac{a²+b²+c²}{2})$$ Derivation for arbitrary 3D Gaussians:\n3D Gaussian is a product of three 1D Gaussian. If each 1D Gaussian is N(0,1), then 3D Gaussian is:\n$$ \\begin{aligned} p(𝐱) \u0026= p(x)p(y)p(z) \\\\\\ \u0026= \\frac{1}{(2π)^{3/2}} exp(-\\frac{x²+y²+z²}{2}) = \\frac{1}{(2π)^{3/2}} exp(-\\frac{𝐱ᵀ𝐱}{2}) \\\\\\ \\end{aligned} $$ For an arbitrary vector 𝐱 = [x,y,z], where 3 Gaussians could be in various shapes. it will be moved to the origin of world space by subtracting the mean vecotr, and the 3 directions (covariance matrix) will be scaled to 1 (identity matrix) through a transformation matrix 𝐀.\n$$\\rm 𝐱' = 𝐀(𝐱-\\bm μ)$$Because the mean vector is point\u0026rsquo;s position. This step means all points are moved to world\u0026rsquo;s origin and reshaped into a unit space for later optimization.\nPlug this \u0026ldquo;normalized\u0026rdquo; 𝐱\u0026rsquo; into 3D Gaussian :\n$$p(𝐱') = \\frac{1}{(2π)^{3/2}} exp(-\\frac{(𝐱-\\bm μ)ᵀ𝐀ᵀ𝐀(𝐱-\\bm μ)}{2})$$ To reach a form about 𝐱, integrate p(𝐱\u0026rsquo;):\n$$1 = ∭_{-∞}^{+∞} \\frac{1}{(2π)^{3/2}} exp(-\\frac{(𝐱-\\bm μ)ᵀ𝐀ᵀ𝐀(𝐱-\\bm μ)}{2}) d𝐱'$$ Substitude $d𝐱'$ with $d𝐀(𝐱-\\bm μ) = |𝐀|d𝐱$\n$$1 = ∭_{-∞}^{+∞} \\frac{|𝐀|}{(2π)^{3/2}} exp(-\\frac{(𝐱-\\bm μ)ᵀ𝐀ᵀ𝐀(𝐱-\\bm μ)}{2}) d𝐱$$The function being integrated is p(𝐱). Since 𝐱 follows Gaussian, it can be rewritten with a mean vector 𝛍 and a covariance matrix 𝚺.\nThe covariance matrix 𝚺 is a symmetric matrix, which can be decomposed by SVD:\n$$ \\begin{aligned} Σ \u0026= U Λ Uᵀ \\\\\\ \u0026= UΛ^{½} Λ^{½T} Uᵀ \\\\\\ \u0026= UΛ^{½} (UΛ^{½})ᵀ \\end{aligned} $$ On a 2D plane, SVD is strecting and rotating, represented separately by a stretch matrix (diagnoal) $S = [^{s₁ \\ 0}\\_{0\\ s₂}]$, and a rotate matrix (UUᵀ=1) $R = [^{cosθ \\ -sinθ}\\_{sinθ \\ cosθ}]$.\nEach column of U is orthogonal to each other and of magnitude 1. For example, when U is R:\n⌈ ⌊ c s x o i s n θ θ - R s c y i o n s θ θ ⌉ ⌋ ⌈ ⌊ p 1 0 P ₁ ⌉ ⌋ t s ⌈ ⌊ p 0 1 ₂ ⌉ ⌋ = ⌈ ⌊ c s R p o i o ₁ s n t ' θ θ a ⌉ ⌋ t e ⌈ ⌊ d - c p s o P ₂ i s t ' n θ s θ ⌉ ⌋ In 3D space, matrices are 3×3.\n𝐔 is a basis (a component, a coordinate system). By multiplying it with a diagnoal matrix, $UΛ^½$ is a linear transformation, denoted as the transformation matrix 𝐀. Thus, 𝚺 = 𝐀𝐀ᵀ.\nIn other words, an identity matrix (basis) will be transformed to another basis $UΛ^½$ by 𝐀.\nTherefore, 𝐀ᵀ𝐀 = 𝚺⁻¹, which will reverse an arbitrary covariance matrix 𝚺 to identity matrix 𝐈, i.e., putting the ellipsoid into the \u0026ldquo;unit\u0026rdquo; space $[^{100}_{^{010}\\_{001}}]$, where the modulus of each axis is 1.\nSubstitute 𝐀ᵀ𝐀 with 𝚺⁻¹, and $|𝐀| = |Σ|^½$, the 3D Gaussian is the function being integrated:\n$$\\frac{1}{(2π)^{3/2}|Σ|^½} exp(-\\frac{(𝐱-\\bm μ)ᵀΣ⁻¹(𝐱-\\bm μ)}{2})$$ The 3D Gaussian used in this work is simplified as:\n$$ G(x) = exp(-\\frac{(𝐱)ᵀΣ⁻¹(𝐱)}{2}) \\tag{4} $$ Omit the mean vector 𝛍, becuase it\u0026rsquo;s 0. Every 3D Gaussian distribution\u0026rsquo;s center has been shifted to the origin. Once the optimization finished, ellipsoids will be reverted to the world space for rasterization.\nThe front fraction is omitted as the integral (\u0026ldquo;volume\u0026rdquo;) of 3D Gaussian isn\u0026rsquo;t limited to 1 to be a probability distribution, and considering a 3D Gaussian can be any size.\n(2024-05-15)\nseh_sjij 解释为: 系数$\\frac{1}{(2π)^{\\frac{3}{2}}|Σ|^½}$被包含到了 opacity 中，然后 opacity 会自己优化。 CSDN 𝐀 is for rotating and stretching an ellipsoid. Thus, 𝐀=𝐑𝐒. And then 𝚺 = 𝐑𝐒𝐒ᵀ𝐑ᵀ\nThe covariance matrix 𝚺 gets optimized during training, such that the shape and direction of ellipsoids get adjusted to fit the scene.\nOptimization 3D Gaussian representation based on point cloud from SfM.\nEach 3D Gaussian contains properties: 3D position (xyz), shape (transform matrix 𝐀), SH coeffs (color), opacity (α).\nOptimize rotate matrix 𝐑 using Quaternion instead of 3×3 matrix\nPoint cloud optimization\nRemove points whose opacity lower than threshold after a certain epochs;\nHigh positional gradients are inferred as a point is hard to reconstruct the geometry. So, small Gaussian do clone (for faster training), while large Gaussian do split (for recovering bkg).\nReset opacity to 0 periodically to remove floater around camera\nTile-based rasterization.\nAn image is split into 16×16 patches. Sorting 3D ellipses observed by a patch based on depth. Terminate alpha compositing on a pixel when opactiy reaches 1. Each tile has a CUDA block, and each pixel has a CUDA thread. Read Notes (2023-05-14)\nThree elements:\nBased on sparse points (colmap) and 3D Gaussians representation Point cloud optimization Fast rendering algorithm with GPU sorting (tile-based rasterization) How much meomery does the 1-5 million (1e6) Gaussians (for all scenes tested) cost?\nComparing with Mip-NeRF360 of 8.6MB, 3DGS has 523MB after 7K iterations on dataset \u0026ldquo;Mip-NeRF360\u0026rdquo;. Chinese translation: 3DGS笔记 - bo233的文章 - 知乎\nAbs (2023-10-29)\nNVS based on radiance field without neural network.\nSfM point cloud and 3D Gaussian representation. Point cloud adjustment: add and remove based on gradients. Tile-based rasterizer leveraging depth and cuda. Intro Points cloud format is chosen for rasterization. Then, 3D Gaussian is chosen to make point cloud a continuous field.\nEfficient method based on continuous representation (MLP) most performed interpolation.\nNeural nets representation is convinent to be optimized, but hinder fast rendering.\nRelated Works Splatting made point-based rendering more efficient by extending the rasterization beyond a single pixel to cover a spread-out area.\nSome methods used CNN to render.\nNeRF requires extensive sampling around the entire space.\nPulsar is \u0026ldquo;order-independent\u0026rdquo;, whereas alpha-blending for a pixel is performed based on visibility order.\ndiffuse - diffusion model\nOverview The key to the efficiency of our method is our tile-based rasterizer.\nExplicit scene representation appeals fast rendering without inferencing neural network.\nGradients are backpropagated to concrete 3D Gaussian.\nDifferentiable 3DGS \u0026ldquo;Unstructured\u0026rdquo; is opposite to \u0026ldquo;regular volume grid\u0026rdquo;. 3DGS is unstructured but able to do volume rendering. Point cloud data + 3DGS primitive -\u0026gt; scene representation\nComparing with small planar circle representation for each point, 3D Gaussian doesn\u0026rsquo;t need normal. And normals are intractable for a sparse (SfM) point cloud.\ncovariance matrices have physical meaning only when they are positive semi-definite\nThus, they use a decomposed, equivalent form: 𝚺 = 𝐑𝐒𝐒ᵀ𝐑ᵀ , i.e., stretching and rotating an ellipsoid.\nScaling matrix 𝐒 is represented by a 3D vector; Rotation 𝐑 is represented by a quaternion. Reparameterization is changing spaces. For example, spherical coords (θ,φ) of viewdir is recombinded to cartisian coords (x,y,z) in NeRF\u0026rsquo;s code. However, SVD doesn\u0026rsquo;t alter the number of dimensions, as a single space is split into 2 spaces. (2024-07-09)\n3DGS 中优化高斯分布的协方差矩阵时，将其分解成了 scaling 和 rotation 两个矩阵，这样做是因为直接优化协方差矩阵很难收敛吗？ (文中说梯度下降无法保持协方差矩阵的半正定性质。) 不好优化是因为约束不够（？），“先分解再合成”就相当于是增加了额外的约束吗？\n让我想起 TensoRF 先做了 tensor decomposition：把一个三维场景分解成 3 个一维向量（CP）或者 1 个向量加 1 个平面（VM）， 再去优化各个 basis (component) 的 basic vector/plane\n\u0026ldquo;把多个 pattern 叠加起来\u0026quot;就是重建，那 volume rendering 不也是这个意思？像素射线上的每个点是一个 component。 3DGS 是把相机射线上的椭球叠加起来，那一个椭球就是这个像素的component。\nOptimization Create, delete or move points (3D Gaussian) to fit geometry.\nThe quality of covariance of 3D Gaussians is critical for the compactness.\nSGD performed by CUDA. Alpha comes from sigmoid. Covariance has done exponential activation.\nThe fast rasterization is critical in the efficiency of our optimization.\nLoss function: 𝓛 = (1-λ)𝓛₁ + λ𝓛_dssim\nSince the output is a complete image, SSIM is measured holistically between the gt image and predicted image. In contrast, S3IM is for random-ray patches.\nUse absolute error of rgbs, instead of MSE.\nAdaptive Density Control Densify to better represent scene and remove transparent 3D Gaussians (points) every 100 iterations.\nTotal volume?\nReset alpha value close to zero every 3k iterations to reduce floaters near cameras.\nTile-based Gaussian Split image into 16x16 patches ➔ Cull 3D Gaussians intersected with frustum ➔ Project 3D Gaussians to 2D ➔ Assign depth observed by a patch to each 3D Gaussian ➔ Sorting Gaussians based on depth for each patch.\nA block is assigned to a patch and loads packets of Gaussians into shared memory.\nEach pixel has a thread to do alpha compositing.\nReuse the per-tile lists for each pixel in the tile.\nImplementation Central-object photos taken from the entire hemisphere without angular regions missing result in good SH coefficients.\nHowever, for incomplete observation, e.g., corners or \u0026ldquo;inside-out\u0026rdquo; photos, the 0th order SH prone to be corrupt during optimization.\nSo they first optimize only the 0-order coefficient for 1K iterations, then add 1 more coeff every 1K iters until all 4 orders are supplemented.\nResults As fast as instantNGP, and as good as MipNeRF360. Synthetic dataset can use 100K random intialized point cloud in the bbox. More compact than other point-based methods. Limitations Hight memory cost due to point cloud representation. Tradeoff of model size for speed.\nCompress point cloud. Defects occurs in the scene not well observed.\nPlay (2023-11-01)\nEnvironment Ubuntu 18.04 Lambda Server: nvcc 10.2, Ubuntu 18.04, driver 470.103.01 with support for CUDA 11.4 at highest.\nClone needs \u0026ndash;recursive: git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive\nOtherwise, there will be an error when running conda env create --file environment.yml:\n1 ERROR: Directory \u0026#39;submodules/diff-gaussian-rasterization\u0026#39; is not installable. Neither \u0026#39;setup.py\u0026#39; nor \u0026#39;pyproject.toml\u0026#39; found. Or fetch and install submodules individually:\n1 2 3 4 conda activate gaussian_splatting git submodule update --init --recursive pip install submodules/diff-gaussian-rasterization pip install submodules/simple-knn Submodule install error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Pip subprocess error: error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─\u0026gt; [12 lines of output] Traceback (most recent call last): File \u0026#34;\u0026lt;string\u0026gt;\u0026#34;, line 36, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;pip-setuptools-caller\u0026gt;\u0026#34;, line 34, in \u0026lt;module\u0026gt; File \u0026#34;/home/z/Downloads/gaussian-splatting/submodules/diff-gaussian-rasterization/setup.py\u0026#34;, line 13, in \u0026lt;module\u0026gt; from torch.utils.cpp_extension import CUDAExtension, BuildExtension File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/__init__.py\u0026#34;, line 201, in \u0026lt;module\u0026gt; _load_global_deps() File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/__init__.py\u0026#34;, line 154, in _load_global_deps ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL) File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/ctypes/__init__.py\u0026#34;, line 364, in __init__ self._handle = _dlopen(self._name, mode) OSError: /home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/lib/../../../../libcublas.so.11: symbol cublasLtGetStatusString version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference [end of output] Submodules (\u0026ldquo;diff-gaussian-rasterization\u0026rdquo;) needs compilation locally with CUDA Toolkit. issue#45\nAlternative steps refer to issue#406\n(2024-04-05)\nI previously thought that the driver 470 is insufficient to install cuda 11.6, but I found that driver 470 is able to install CUDA 11.x on Docs. However, the problem persists when using cuda 11.6.\nThe problem is that my PATH has been modified previously. I once added a line in my .bashrc: export LD_LIBRARY_PATH=/home/z/anaconda3/envs/GNT/lib\nThis caused the libcublas points to mismatched library. Refer to the answer of eval\nI remove that line, and having the nvidia-driver 470 + ctk 11.6 on ubuntu 18.04, the env installation succeeds with just one line: conda env create --file environment.yml\nHe said by setting env virable LD_LIBRARY_PATH=\u0026lt;anaconda dir\u0026gt;/python3.7/site-packages/torch/lib/nvidia/cublas/lib/:$LD_LIBRARY_PATH So that \u0026ldquo;the dlopen will firstly look for .so files in that directory.\u0026rdquo; (I didn\u0026rsquo;t try that at present.)\nSubmodules compile error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /usr/local/cuda/bin/nvcc -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/TH -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/z/anaconda3/envs/3dgs/include/python3.9 -c cuda_rasterizer/backward.cu -o build/temp.linux-x86_64-cpython-39/cuda_rasterizer/backward.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -I/home/z/Downloads/gaussian-splatting/submodules/diff-gaussian-rasterization/third_party/glm/ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1011\\\u0026#34; -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++14 cuda_rasterizer/backward.cu:15:10: fatal error: cooperative_groups/reduce.h: No such file or directory #include \u0026lt;cooperative_groups/reduce.h\u0026gt; ^~~------------------------- compilation terminated. ERROR: Could not build wheels for diff-gaussian-rasterization, which is required to install pyproject.toml-based projects And compiling simple_knn.h will encounter:\n1 2 simple_knn.cu:17:10: fatal error: cub/cub.cuh: No such file or directory #include \u0026lt;cub/cub.cuh\u0026gt; Possible method: Add cub to CmakeLists.txt. Nvidia-forum\nnvcc mismatch and maybe 10.2 doesn\u0026rsquo;t have that function yet.\nUbuntu 20.04 Install new cudatookit on Alien PC (Ubuntu 20.04) CUDA Toolkit 12.3 Downloads | NVIDIA Developer\n1 2 3 4 5 6 7 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2004-12-3-local_12.3.0-545.23.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-12-3-local_12.3.0-545.23.06-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2004-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda-toolkit-12-3 Install driver:\n1 2 sudo apt-get install -y cuda-drivers # reboot is required But nvcc -V still returns 10.2.\nI found there are multiple \u0026ldquo;cuda\u0026rdquo; on my Alien PC. Nv-Install Guide Linux\nUnder dir \u0026ldquo;/usr/local/\u0026rdquo;, there are \u0026ldquo;cuda/\u0026rdquo;, \u0026ldquo;cuda11/\u0026rdquo;, \u0026ldquo;cuda11.6/\u0026rdquo;, \u0026ldquo;cuda12/\u0026rdquo;, \u0026ldquo;cuda12.3/\u0026rdquo;.\nOnly one could be used. SO. And the default version may can be checked by: update-alternatives --display cuda Docs-Nv\nAdd 11.6 to PATH:\n1 2 3 export PATH=/usr/local/cuda-11.6/bin${PATH:+:${PATH}} export CUDADIR=/usr/local/cuda-11.6 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.6/lib64 Then nvcc -V change to 11.6.\nSuccess to create env on Alien PC with just the one line: conda env create --file environment.yml\nFor Lambda Server, although there is \u0026ldquo;cuda-11.2\u0026rdquo; folder in \u0026ldquo;/usr/local/\u0026rdquo;, but \u0026ldquo;bin/\u0026rdquo; doesn\u0026rsquo;t exist inside, so it can\u0026rsquo;t be added into PATH.\nTherefore, the full cudatoolkit 11.6 and compatible driver (as above) are required, instead of the subset cudatoolkit for runtime installed by conda.\nUbuntu 22.04 (2023-11-03)\nCUDA Toolkit 11.6 doesn\u0026rsquo;t have version for Ubuntu 22.04 CUDA Toolkit 11.8 corresponds PyTorch which is higher than 2.0.0 1 2 3 4 5 6 7 # nvcc -V # 11.8, Ubuntu 22.04, nvidia driver 545, 3090Ti conda create -n 3dgs python=3.9 conda activate 3dgs conda install pytorch==2.0.0 torchvision==0.15.0 pytorch-cuda=11.8 -c pytorch -c nvidia pip install plyfile tqdm ninja pip install submodules/diff-gaussian-rasterization/ pip install submodules/simple-knn/ SIBR viewer install fails (Ubuntu 22.04) #151\n(2024-05-17)\nThe glm module missed because I forgot add --recursive (when cloning submodule depth-diff-gaussian-rasterization solely: git submodule update --init --recursive). And the error about pyproject appears either:\n1 2 3 4 5 6 7 8 9 10 11 Compiling objects... - - - - - In file included from /home/zichen/Downloads/CasMVSNet_pl-comments/_3DGS/submodules/depth-diff-gaussian-rasterization/cuda_rasterizer/backward.cu:12: /home/zichen/Downloads/CasMVSNet_pl-comments/_3DGS/submodules/depth-diff-gaussian-rasterization/cuda_rasterizer/backward.h:19:10: fatal error: glm/glm.hpp: No such file or directory 19 | #include \u0026lt;glm/glm.hpp\u0026gt; | ^~-~-~-~-~-~-~-~-~-~-~-~ compilation terminated. - - - - - ERROR: Could not build wheels for diff_gaussian_rasterization, which is required to install pyproject.toml-based projects Train NeRF Synthetic\n1 python train.py -s ../nerf/data/nerf_synthetic/lego -r 8 DTU (2024-04-15)\nTrain with DTU dataset:\nInstall Colmap.\nTo get the CUDA support on Linux, a local compilation is required. Docs\nUse COLMAP to prepare training dataset with convert.py:\n(2024-05-26)\n1 2 3 4 5 6 7 8 9 zichen@homepc:~/Downloads/gaussian-splatting$ tree . ├── convert.py ⋮ └── DTU_scan1 └─── input ├── 00000000.jpg ├── 00000002.jpg └── 00000004.jpg Execute:\n1 python convert.py -s DTU_scan1 The generated point cloud is a binary file: sparse/0/points3D.bin, which will be read with read_points3D_binary().\nTrain:\n1 (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ python train.py -s DTU_scan1 The result point cloud doesn\u0026rsquo;t perserve a good geometry:\ninput.ply iter30K \u0026gt;}} Given 3 views, COLMAP produced very sparse points.\nThe gaussian clone and split maybe not accurate enough for recovering the geometry.\nT\u0026amp;T (2024-05-26)\nRef:\n3DGS guide Pixel-GS Steps:\nUse COLMAP to estimate camera poses from input images:\n1 2 3 4 5 6 7 8 9 zichen@homepc:~/Downloads/gaussian-splatting$ tree . ├── convert.py ⋮ └── TnT_Barn └─── input ├── 000010.jpg ├── 000179.jpg └── 000184.jpg Execute:\n1 python convert.py -s TnT_Barn --resize I removed the magick_command variable when constructing commands (at L105, L112, L119): exit_code = os.system(\u0026quot; mogrify -resize 50% \u0026quot; + destination_file) as Ubuntu 22.04 doesn\u0026rsquo;t have magick. Train:\n1 (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ python train.py -s TnT_Barn Result model of Barn with 2 views (010.jpg and 184.jpg) input:\nAs Pixel-GS descriped (in abstract): there are lots of \u0026ldquo;needle-like artifacts\u0026rdquo;. SIBR viewer SIBR Core - Git Lab\nUbuntu 20.04 On Ubuntu 20.04, 1050Ti:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Get into submodule: cd SIBR_viewers # For Ubuntu 20.04, otherwise opencv version (4.2) mismatch 4.5: git checkout fossa_compatibility # Dependencies sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev # Project setup cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release # add -G Ninja to build faster # Don\u0026#39;t add \u0026#39;-j24\u0026#39;, otherwise crash? cmake --build build --target install Some details:\nInstall cmake by compiling source code:\nDownload tar.gz (cmake-.tar.gz), such as cmake-3.29.1.tar.gz, and extract: tar -zxvf cmake-{version number}.tar.gz\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Dependecy for OpenSSL sudo apt-get install libssl-dev cd cmake-{version number} # Check required dependencies: ./bootstrap # Build package: make # Install sudo make install cmake --version Ref How to Install CMake on Ubuntu 22.04 or 20.04 - LinuxCapable\nOpenCV issue on Ubuntu 20.04 resolved by switching branch to \u0026ldquo;fossa_compatibility\u0026rdquo;. Installing SIBR - cannot find correct version of OpenCV #10\nsm_30 error\nWith cudatoolkit 11.6 added to PATH, an error occurs:\n1 2 3 4 #$ ptxas -arch=sm_30 -m64 \u0026#34;tmp/CMakeCUDACompilerId.ptx\u0026#34; -o \u0026#34;tmp/CMakeCUDACompilerId.sm_30.cubin\u0026#34; ptxas fatal : Value \u0026#39;sm_30\u0026#39; is not defined for option \u0026#39;gpu-name\u0026#39; Check if old-version cuda exists: apt-cache policy nvidia-cuda-toolkit. 【已解决】 Compilation error ptxas fatal :\n1 2 3 4 5 6 7 nvidia-cuda-toolkit: Installed: 10.1.243-3 Candidate: 10.1.243-3 Version table: *** 10.1.243-3 500 500 http://ca.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages 100 /var/lib/dpkg/status Remove it: sudo apt remove nvidia-cuda-toolkit\nThen, re-build works.\nRun local viewer\n1 ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/c777580e-9 Error (Real-time viewer requires CC \u0026gt; 7.0):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 (gaussian_splatting) yi@yi-Alienware-Aurora-R8:~/Downloads/gaussian-splatting$ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/c777580e-9/ --no_interop [SIBR] -- INFOS --: Initialization of GLFW [SIBR] -- INFOS --: OpenGL Version: 4.6.0 NVIDIA 545.23.06[major: 4, minor: 6] Number of input Images to read: 300 Number of Cameras set up: 300 [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego. [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego.ply. [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego.obj. LOADSFM: Try to open /home/yi/Downloads/nerf/data/nerf_synthetic/legopoints3D.bin [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/legopoints3D.bin. [SIBR] !! WARNING !!: FILE /home/yi/Downloads/gaussian-splatting/SIBR_viewers/src/core/scene/ProxyMesh.cpp LINE 29, FUNC loadFromData proxy model not found at /home/yi/Downloads/nerf/data/nerf_synthetic/lego [SIBR] ## ERROR ##: FILE /home/yi/Downloads/gaussian-splatting/SIBR_viewers/src/projects/gaussianviewer/renderer/GaussianView.cpp LINE 339, FUNC GaussianView Sorry, need at least compute capability 7.0+!terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): See log for message errors Aborted (core dumped) Even the 1080 Ti\u0026rsquo;s compute capacity is only 6.1 ? Your GPU Compute Capability Ubuntu 22.04 (2023-11-04)\nInstall:\n1 2 3 4 5 6 # Dependencies sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev # Project setup cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release -G Ninja cmake --build build -j24 --target install Train: python train.py -s ../Dataset/nerf_synthetic/lego\nReal-time viewer: sudo ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/4ee28b3c-9\nRun Network viewer:\n1 2 3 4 5 6 7 8 (3dgs) z@homepc:~/Downloads/gaussian-splatting/SIBR_viewers$ ./install/bin/SIBR_remoteGaussian_app [SIBR] -- INFOS --: Initialization of GLFW [SIBR] ## ERROR ##: FILE /home/z/Downloads/gaussian-splatting/SIBR_viewers/src/core/graphics/Window.cpp LINE 30, FUNC glfwErrorCallback X11: The DISPLAY environment variable is missing terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): See log for message errors Aborted (core dumped) (2024-05-09)\nUse SIBR to view the trained model with point cloud initialization from CasMVSNet\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m ../CasMVSNet_pl-comments/output/no_densify_0509/ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app: error while loading shared libraries: libassimp.so.5: cannot open shared object file: No such file or directory (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev 有一些软件包无法被安装。如果您用的是 unstable 发行版，这也许是 因为系统无法达到您要求的状态造成的。该版本中可能会有一些您需要的软件 包尚未被创建或是它们已被从新到(Incoming)目录移出。 下列信息可能会对解决问题有所帮助： 下列软件包有未满足的依赖关系： libminizip-dev : 依赖: libminizip1 (= 1.1-8build1) 但是 1.1.1-1+rebuild 正要被安装 E: 无法修正错误，因为您要求某些软件包保持现状，就是它们破坏了软件包间的依赖关系。 (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ sudo aptitude install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m ../CasMVSNet_pl-comments/output/no_densify_0509/ Then, it works:\nUbuntu 18.04 (2024-04-05)\nThe current cmake on server is 3.10, which is too old.\nUsing the pre-built cmake cmake-3.29.1-linux-x86_64.tar.gz will lead to error.\n1 2 3 4 5 6 wget https://github.com/Kitware/CMake/releases/download/v3.29.1/cmake-3.29.1-linux-x86_64.tar.gz tar -zxvf cmake-3.29.1-linux-x86_64.tar.gz # Build SIBR cd ./Downloads/gaussian-splatting/SIBR_viewers /home/z/Downloads/cmake-3.29.1-linux-x86_64/bin/cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release Error 1 2 3 4 5 6 7 8 CMake Error at /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Call Stack (most recent call first): /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindGLEW.cmake:242 (find_package_handle_standard_args) cmake/linux/dependencies.cmake:69 (FIND_PACKAGE) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Using local cmake to compile remote folder, which is mounted via sshfs doesn\u0026rsquo;t work neither:\n1 2 3 4 (base) yi@yi:/mnt/Server/Downloads/gaussian-splatting/SIBR_viewers$ cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release CMake Error: The current CMakeCache.txt directory /mnt/Server/Downloads/gaussian-splatting/SIBR_viewers/CMakeCache.txt is different than the directory /home/z/Downloads/gaussian-splatting/SIBR_viewers where CMakeCache.txt was created. This may result in binaries being created in the wrong place. If you are not sure, reedit the CMakeCache.txt Clone the CMake repo and build it. git clone https://github.com/Kitware/CMake.git\nInstall lib for ssl: sudo apt-get install libssl-dev to avoid SSL error:\n1 2 3 4 -- Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the system variable OPENSSL_ROOT_DIR (missing: OPENSSL_CRYPTO_LIBRARY OPENSSL_INCLUDE_DIR) CMake Error at Utilities/cmcurl/CMakeLists.txt:644 (message): Could not find OpenSSL. Install an OpenSSL development package or configure CMake with -DCMAKE_USE_OPENSSL=OFF to build without OpenSSL. 1 2 3 cd CMake/ mkdir build \u0026amp;\u0026amp; cd build ../bootstrap \u0026amp;\u0026amp; make The using cmake to compile project:\n1 2 cd ~/Downloads/gaussian-splatting/SIBR_viewers ../../CMake/build/bin/cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release But this still encounters the same error as above.\n1 2 CMake Error at /home/z/Downloads/CMake/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Copy the compiled executable files onto system.\n1 2 cd sudo make install Cmake 3.29 gets installed (copied) into:\n1 2 3 4 5 6 7 8 9 10 11 /usr/local/doc/cmake-3.29/cmsys/Copyright.txt ... ... ... -- Installing: /usr/local/share/cmake-3.29/Modules/ ... -- Installing: /usr/local/share/cmake-3.29/Templates ... -- Installing: /usr/local/share/vim/vimfiles/indent ... -- Installing: /usr/local/share/emacs/site-lisp/cmake-mode.el -- Installing: /usr/local/share/aclocal/cmake.m4 -- Installing: /usr/local/share/bash-completion/completions/ Relaunch a terminal. Check the version:\n1 2 3 4 (base) z@lambda-server:~/Downloads/gaussian-splatting$ cmake --version cmake version 3.29.20240405-gc2949db CMake suite maintained and supported by Kitware (kitware.com/cmake). Build SIBR:\n1 2 3 4 5 git checkout fossa_compatibility git checkout master cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release The error persists!! as below: (Note: The following error corresponds to the master branch.)\n1 2 3 4 5 6 7 8 CMake Error at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Call Stack (most recent call first): /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) /usr/local/share/cmake-3.29/Modules/FindGLEW.cmake:245 (find_package_handle_standard_args) cmake/linux/dependencies.cmake:69 (FIND_PACKAGE) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Install dependencies:\n1 sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev # libembree-dev It prompts that E: Unable to locate package ibembree-dev.\nThen, the building will prompts error about libboost:\n1 2 3 4 5 6 7 8 9 10 CMake Error at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find Boost: Found unsuitable version \u0026#34;1.65.1\u0026#34;, but required is at least \u0026#34;1.71.0\u0026#34; (found /usr/include, found components: system chrono filesystem date_time) Call Stack (most recent call first): /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:598 (_FPHSA_FAILURE_MESSAGE) /usr/local/share/cmake-3.29/Modules/FindBoost.cmake:2394 (find_package_handle_standard_args) cmake/dependencies.cmake:173 (find_package) cmake/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) The header and libraries of libboost are in:\n1 2 3 -- Found Boost: /usr/include (found version \u0026#34;1.65.1\u0026#34;) -- Boost_INCLUDE_DIRS: /usr/include -- Boost_LIBRARY_DIRS: /usr/lib/x86_64-linux-gnu These information is return by adding command into CMakeLists.txt (guided by chatGPT):\n1 2 3 find_package(Boost REQUIRED) message(STATUS \u0026#34;Boost_INCLUDE_DIRS: ${Boost_INCLUDE_DIRS}\u0026#34;) message(STATUS \u0026#34;Boost_LIBRARY_DIRS: ${Boost_LIBRARY_DIRS}\u0026#34;) I don\u0026rsquo;t know how to update libboost.\nI found the CMakeLists.txt varies between different branch.\nThe branch of SIBR used by 3DGS is gaussian_code_release_union (4ae964a267):\n1 2 3 git checkout gaussian_code_release_union cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release Warnings about embree, and errors for OpenCV4.5:\nError 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Warnings CMake Warning at cmake/linux/dependencies.cmake:127 (find_package): By not providing \u0026#34;Findembree.cmake\u0026#34; in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \u0026#34;embree\u0026#34;, but CMake did not find one. Could not find a package configuration file provided by \u0026#34;embree\u0026#34; (requested version 3.0) with any of the following names: embreeConfig.cmake embree-config.cmake Add the installation prefix of \u0026#34;embree\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;embree_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;embree\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) # Errors: There is no provided OpenCV library for your compiler, relying on find_package to find it CMake Error at cmake/linux/dependencies.cmake:248 (find_package): Could not find a configuration file for package \u0026#34;OpenCV\u0026#34; that is compatible with requested version \u0026#34;4.5\u0026#34;. The following configuration files were considered but not accepted: /usr/share/OpenCV/OpenCVConfig.cmake, version: 3.2.0 Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Then I change to branch fossa_compatibility:\n1 2 3 git checkout fossa_compatibility cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release The error of OpenCV4 occurs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CMake Warning (dev) at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:438 (message): The package name passed to `find_package_handle_standard_args` (EMBREE) does not match the name of the calling package (embree). This can lead to problems in calling code that expects `find_package` result variables (e.g., `_FOUND`) to follow a certain pattern. Call Stack (most recent call first): cmake/linux/Modules/Findembree.cmake:87 (FIND_PACKAGE_HANDLE_STANDARD_ARGS) cmake/linux/dependencies.cmake:127 (find_package) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) This warning is for project developers. Use -Wno-dev to suppress it. -- EMBREE wasn\u0026#39;t found correctly. Set EMBREE_DIR to the root SDK installation directory. (missing: EMBREE_INCLUDE_DIR EMBREE_LIBRARIES) There is no provided OpenCV library for your compiler, relying on find_package to find it CMake Error at cmake/linux/dependencies.cmake:248 (find_package): Could not find a configuration file for package \u0026#34;OpenCV\u0026#34; that is compatible with requested version \u0026#34;4\u0026#34;. The following configuration files were considered but not accepted: /usr/share/OpenCV/OpenCVConfig.cmake, version: 3.2.0 Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Authors said the SIBR has no support for Ubuntu 18. Related issues with searching \u0026ldquo;18.04\u0026rdquo;\n","date":"2023-05-14T13:41:00Z","image":"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.bfDD9_L28UK7AsADDzB8vwHaEo%26pid%3DApi\u0026f=1\u0026ipt=3b34b7952e1e03305e5c085185ede1ad09e8edc71e9c1161a33c17b6b9f2b3ac\u0026ipo=images","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/b-note-3dgs-read/","title":"Read: NVS - 3DGS | 3D Gaussian Splatting"},{"content":"DataLoader (2024-05-29)\nPyTorch DataLoader工作原理可视化 collate_fn\ntorch.utils.data.IterableDataset pytorch forum 2020-02-26; Docs\nConcatDataset Docs\nExample 1: Pytorch DataLoader multiple data source - SO;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import os import torch.utils.data as data class SingeJsonDataset(data.Dataset): # implement a single json dataset here... ... list_of_datasets = [] for j in os.path.listdir(root_dir): if not j.endswith(\u0026#39;.json\u0026#39;): continue # skip non-json files list_of_datasets.append(SingeJsonDataset(json_file=j, root_dir=root_dir, transform=None)) # once all single json datasets are created you can concat them into a single one: multiple_json_dataset = data.ConcatDataset(list_of_datasets) Example 2: PyTorch forum - Praateek\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class LazyTextDataset(Dataset): def __init__(self, filename): self._filename = filename self._total_data = int(subprocess.check_output(\u0026#34;wc -l \u0026#34; + filename, shell=True).split()[0]) - 1 def __getitem__(self, idx): line = linecache.getline(self._filename, idx + 1) csv_line = csv.reader([line]) return next(csv_line) def __len__(self): return self._total_data path = /where_csv_files_are_dumped/ files = list(map(lambda x : path + x, (filter(lambda x : x.endswith(\u0026#34;csv\u0026#34;), os.listdir(path))))) datasets = list(map(lambda x : LazyTextDataset(x), files)) dataset = ConcatDataset(datasets) Comments of Thomans Ahle:\nThe problem with ConcatDataset is that it doesn\u0026rsquo;t work with multiprocessing. It calls len(ds) on each dataset in it\u0026rsquo;s initializer, so you end up loading every dataset in the main process.\nnp.load(path, mmap_mode=\u0026lsquo;r\u0026rsquo;) Load multiple .npy files (size \u0026gt; 10GB) in pytorch - SO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np import torch from bisect import bisect import os, psutil # used to monitor memory usage class BigDataset(torch.utils.data.Dataset): def __init__(self, data_paths, target_paths): self.data_memmaps = [np.load(path, mmap_mode=\u0026#39;r\u0026#39;) for path in data_paths] self.target_memmaps = [np.load(path, mmap_mode=\u0026#39;r\u0026#39;) for path in target_paths] self.start_indices = [0] * len(data_paths) self.data_count = 0 for index, memmap in enumerate(self.data_memmaps): self.start_indices[index] = self.data_count self.data_count += memmap.shape[0] def __len__(self): return self.data_count def __getitem__(self, index): memmap_index = bisect(self.start_indices, index) - 1 index_in_memmap = index - self.start_indices[memmap_index] data = self.data_memmaps[memmap_index][index_in_memmap] target = self.target_memmaps[memmap_index][index_in_memmap] return index, torch.from_numpy(data), torch.from_numpy(target) # Test Code if __name__ == \u0026#34;__main__\u0026#34;: data_paths = [f\u0026#39;data/d{index}.npy\u0026#39; for index in range(10)] target_paths = [f\u0026#39;data/s{index}.npy\u0026#39; for index in range(10)] process = psutil.Process(os.getpid()) memory_before = process.memory_info().rss dataset = BigDataset(data_paths, target_paths) used_memory = process.memory_info().rss - memory_before print(\u0026#34;Used memory:\u0026#34;, used_memory, \u0026#34;bytes\u0026#34;) dataset_size = len(dataset) print(\u0026#34;Dataset size:\u0026#34;, dataset_size) print(\u0026#34;Samples:\u0026#34;) for sample_index in [0, dataset_size//2, dataset_size-1]: print(dataset[sample_index]) ","date":"2023-04-21T09:57:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/load_multi_files/","title":"memo: Datasets load multiple data files"},{"content":"Callbacks I asked chat-GPT: \u0026ldquo;How to save the intermediate tensor in a tensorflow model?\u0026rdquo;.\ntf.keras.callbacks.LambdaCallback class will call a custom function after each epoch or each batch or the begining/ending of traininng. Docs.\nCallbacks are useful to get a view on internal states and statistics of the model during training. Docs: Writing your own callbacks\nExample of callbacks in model.fit() refer to: 7;\nHowever, the NeRF model constitutes low-level operations without implementing the method model.fit(). Then the callback functions need to be called manually at the appropriate points in the training loop:\nAsking Chat-GPT: \u0026ldquo;Please do not use model.fit(). Make the model and callback in low-level options. Please given an example of using callbacks and using TensorFlow 1.15.\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras.callbacks import LambdaCallback tf.enable_eager_execution() # Define the model architecture model = tf.keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation=\u0026#39;relu\u0026#39;), Dense(10) ]) # Define the loss function and optimizer loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = tf.keras.optimizers.Adam() # Define the learning rate decay schedule def lr_decay(epoch, lr): if epoch % 10 == 0 and epoch \u0026gt; 0: return lr * 0.1 else: return lr # Define the callback to update the learning rate lr_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: optimizer.lr.assign(lr_decay(epoch, optimizer.lr))) # Define a function to save the intermediate tensor def save_intermediate_tensor(batch, logs): intermediate_tensor = model.layers[2].output tf.summary.scalar(\u0026#39;intermediate_tensor\u0026#39;, intermediate_tensor, step=epoch) save_callback = LambdaCallback(on_batch_end=save_intermediate_tensor) # Load the MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # Normalize the pixel values x_train = x_train[:100] / 255.0 x_test = x_test[:100] / 255.0 y_test = y_test[:100] # Compile the model for model.evaluate() model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\u0026#39;accuracy\u0026#39;]) # Train the model for epoch in range(10): for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train, y_train)): x_batch_train = x_batch_train.reshape((-1, 28, 28)) y_batch_train = y_batch_train.reshape((-1,)) with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Call the save callback at the end of each batch save_callback.on_batch_end(batch,{}) # Call the learning rate callback at the end of each epoch lr_callback.on_epoch_end(epoch, {\u0026#39;lr\u0026#39;: optimizer.lr.numpy()}) # Evaluate the model on the test set test_loss, test_acc = model.evaluate(x_test, y_test) print(\u0026#39;Epoch {}, Test Loss: {:.4f}, Test Accuracy: {:.4f}\u0026#39;.format(epoch+1, test_loss, test_acc)) Error: couldn\u0026rsquo;t find summary writer\nNotFoundError: Resource localhost/logdir:./logdir/exp1/N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:FlushSummaryWriter]\nSolution: Restart the python kernel. tf issue#47100 tf.io.write_file() How to save the value of a tensor in Tensorflow found by 1\n1 2 one_string = tf.strings.as_string(tensor) tf.io.write_file(filename, one_string) Problem: the rank of tensor has to be 0? tf TFRecordWriter How do you save a Tensorflow dataset to a file? found by 1\nThis is for making (x,y) dataset. io_ops._save() Is there a way to save an intermediate output in Tensorflow to a file? found by 2\nMay have been deprecated. Create a new model Obtaining output of an Intermediate layer in TensorFlow/Keras found by 2\nGet values of KerasTensor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import tensorflow as tf def init_nerf_model(): ... print(output.numpy()) # AttributeError: \u0026#39;KerasTensor\u0026#39; object has no attribute \u0026#39;numpy\u0026#39; model = tf.keras.Model(inputs=inputs, outputs=outputs) return model model= init_nerf_model(input_ch=63, input_ch_views=27, use_viewdirs=True) # model.layers # Check memory address of each layer temp_mode = tf.keras.Model(model.input, model.layers[11].output) temp_mode.summary() # print description of each layer Oiginal NeRF model summary:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Model: \u0026#34;model\u0026#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 90)] 0 [] tf.split (TFOpLambda) [(None, 63), 0 [\u0026#39;input_1[0][0]\u0026#39;] (None, 27)] dense (Dense) (None, 256) 16384 [\u0026#39;tf.split[0][0]\u0026#39;] dense_1 (Dense) (None, 256) 65792 [\u0026#39;dense[0][0]\u0026#39;] dense_2 (Dense) (None, 256) 65792 [\u0026#39;dense_1[0][0]\u0026#39;] dense_3 (Dense) (None, 256) 65792 [\u0026#39;dense_2[0][0]\u0026#39;] dense_4 (Dense) (None, 256) 65792 [\u0026#39;dense_3[0][0]\u0026#39;] tf.concat (TFOpLambda) (None, 319) 0 [\u0026#39;tf.split[0][0]\u0026#39;, \u0026#39;dense_4[0][0]\u0026#39;] dense_5 (Dense) (None, 256) 81920 [\u0026#39;tf.concat[0][0]\u0026#39;] dense_6 (Dense) (None, 256) 65792 [\u0026#39;dense_5[0][0]\u0026#39;] dense_7 (Dense) (None, 256) 65792 [\u0026#39;dense_6[0][0]\u0026#39;] dense_9 (Dense) (None, 256) 65792 [\u0026#39;dense_7[0][0]\u0026#39;] tf.concat_1 (TFOpLambda) (None, 283) 0 [\u0026#39;dense_9[0][0]\u0026#39;, \u0026#39;tf.split[0][1]\u0026#39;] dense_10 (Dense) (None, 128) 36352 [\u0026#39;tf.concat_1[0][0]\u0026#39;] dense_11 (Dense) (None, 3) 387 [\u0026#39;dense_10[0][0]\u0026#39;] dense_8 (Dense) (None, 1) 257 [\u0026#39;dense_7[0][0]\u0026#39;] tf.concat_2 (TFOpLambda) (None, 4) 0 [\u0026#39;dense_11[0][0]\u0026#39;, \u0026#39;dense_8[0][0]\u0026#39;] ================================================================================================== Total params: 595,844 Trainable params: 595,844 Non-trainable params: 0 Only keep the MLP part:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Model: \u0026#34;model_1\u0026#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 90)] 0 [] tf.split (TFOpLambda) [(None, 63), 0 [\u0026#39;input_1[0][0]\u0026#39;] (None, 27)] dense (Dense) (None, 256) 16384 [\u0026#39;tf.split[0][0]\u0026#39;] dense_1 (Dense) (None, 256) 65792 [\u0026#39;dense[0][0]\u0026#39;] dense_2 (Dense) (None, 256) 65792 [\u0026#39;dense_1[0][0]\u0026#39;] dense_3 (Dense) (None, 256) 65792 [\u0026#39;dense_2[0][0]\u0026#39;] dense_4 (Dense) (None, 256) 65792 [\u0026#39;dense_3[0][0]\u0026#39;] tf.concat (TFOpLambda) (None, 319) 0 [\u0026#39;tf.split[0][0]\u0026#39;, \u0026#39;dense_4[0][0]\u0026#39;] dense_5 (Dense) (None, 256) 81920 [\u0026#39;tf.concat[0][0]\u0026#39;] dense_6 (Dense) (None, 256) 65792 [\u0026#39;dense_5[0][0]\u0026#39;] dense_7 (Dense) (None, 256) 65792 [\u0026#39;dense_6[0][0]\u0026#39;] dense_9 (Dense) (None, 256) 65792 [\u0026#39;dense_7[0][0]\u0026#39;] ================================================================================================== Total params: 558,848 Trainable params: 558,848 Non-trainable params: 0 Ref DDG search: save tensorflow tensor to file DDG search: save intermediate tensor from tensorflow model A Guide to TensorFlow Callbacks ","date":"2023-04-17T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf_save_tensors/","title":"memo: TF | Save Tensors"},{"content":"Via conda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Check the version of the CUDA: $ conda list cudatookit # Name Version Build Channel cudatoolkit 10.1.243 h8cb64d8_10 conda-forge $ conda list cudnn: # Name Version Build Channel cudnn 7.6.5.32 hc0a50b0_1 conda-forge # install/update CUDA and CUDNN through conda: $ conda install -c anaconda cudatoolkit $ conda install -c anaconda cudnn # show the highest supported CUDA version? $ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 | # $ nvcc --version Refer to 1; 3\nAnother way to check the version of cuDNN 4:\n1 cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2 Via PyTorch 1 2 3 import torch print(torch.version.cuda) \u0026gt;\u0026gt;\u0026gt; 10.2 Via TensorFlow 1 2 3 4 5 import tensorflow as tf sys_details = tf.sysconfig.get_build_info() cuda_version = sys_details[\u0026#34;cuda_version\u0026#34;] print(cuda_version) Not quiet right. Refer to How to check CUDA version in TensorFlow - gcptutorials\nCompatible combinations for TF Refer to Docs; 2;\nCompatible CUDA for cards Determine the driver version first and then determine the cuda version. Genearlly, cuda is backward compatible with the driver. Nvidia-page\nRef get the CUDA and CUDNN version on windows with Anaconda installed searched by DDG: \"What is the version of CUDA and cuDNN\" Which TensorFlow and CUDA version combinations are compatible? How to get the CUDA version? [NV] How to check CUDA and cuDNN version | by totokk | Medium ","date":"2023-04-09T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/check_cuda_version/","title":"memo: Check Versions of CUDA and cuDNN"},{"content":"tf 1.x enable_eager_execution() Invoke the function tf.enable_eager_execution() at program startup, and then you can use the .numpy() method. ¹\n1 2 3 import tensorflow as tf tf.compat.v1.enable_eager_execution() pred_id = tf.multinomial(tf.exp(preds), num_samples=1)[0][0].numpy() (eager execution is enabled by default in TensorFlow 2.0.)\ntf.get_static_value(x) Answer from ¹. Link to docs\n1 tf.get_static_value( tensor, partial=False ) Create sess Without enabling the eager execution, a tensor can be evaluated in a session. The session can be created as a runtime context by with tf.Session() as sess:, or using InteractiveSession() in place ⁴.\nThe value of a Tensor can only be obtained after calling .eval(), because Tensor (in TF) is of lazy evaluation. ²\n1 2 3 4 5 6 7 8 9 10 import tensorflow as tf a = tf.constant([1, 1.5, 2.5], dtype=tf.float32) b = tf.constant([1, -2, 3], dtype=tf.float32) c = a * b with tf.Session() as sess: result = c.eval() print(result) Use InteractiveSession() Initialize an InteractiveSession as the default session. The following snippet is from 2.\n1 2 3 4 5 6 7 8 9 import tensorflow as tf a = tf.constant([1, 1]) # SparseTensor，只存储值及其对应的index b = tf.constant([2, 2]) c = tf.add(a, b) # Tensor，完整的矩阵 sess = tf.InteractiveSession() # v1.15 print(\u0026#34;a[0]=%s, a[1]=%s\u0026#34; % (a[0].eval(), a[1].eval())) print(\u0026#34;c = %s\u0026#34; % c.eval()) sess.close() Session.run() 通过 Session.run() 获取变量的值 6, 4\n1 2 3 4 5 6 x = tf.placeholder(tf.float32) y = tf.placeholder(tf.float32) bias = tf.Variable(1.0) y_pred = x**2 + bias loss = (y - y_pred)**2 print(\u0026#39;Loss(x,y) = %.3f\u0026#39; % session.run(loss, {x:3.0, y:9.0})) # 程序开头已定义sess 报错：\n1 2 3 4 5 6 FailedPreconditionError: 2 root error(s) found. (0) Failed precondition: Attempting to use uninitialized value Variable_1 [[{{node Variable_1/read}}]] [[pow_3/_33]] (1) Failed precondition: Attempting to use uninitialized value Variable_1 [[{{node Variable_1/read}}]] 原因： bias is a tf.Variable which needs initialization 5, like:\n1 2 3 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(session.run(loss, {x:3.0, y:9.0}) tf.keras.backend.get_value(x) Answer form DesiKeki⁴. Link to docs\n1 2 3 4 print(product) \u0026gt;\u0026gt;tf.Tensor([[12.]], shape=(1, 1), dtype=float32) print(tf.keras.backend.get_value(product)) \u0026gt;\u0026gt;[[12.]] tf.print() Python print() cannot be run in a tf.Graph, but tf.print() can be. tf_guide\n1 2 3 4 5 @tf.function def double(a): print(\u0026#34;Tracing with\u0026#34;, a) # only be executed when (re)tracing tf.print(\u0026#34;Executing with\u0026#34;, a) # executed every time the graph is called return a + a doubt: tf.print 用法？ \u0026ldquo;可以在计算一个变量的同时，指定打印一组变量\u0026rdquo;(不懂) tensorflow杂记 2016\nUse tf.print (instead of deprecated tf.Print). It returns None when executing eagerly. how to use tf.print (not tf.Print) in high-level Estimator api\nAttributeError: 'Tensor' object has no attribute '_datatype_enum' when executing: tf.print(inputs, output_stream=sys.stderr)\n(Deprecated) tf.Print() The pythonic print(feat_tensor) will only print the description of tensors once when the input function graph is constructed.\ntf.Print() is a graph node that splices (打结) the \u0026ldquo;print call\u0026rdquo; into the graph by taking the tensor you wanna print as input, and output a \u0026ldquo;same\u0026rdquo; variable that will be passed to the later node in the graph.\n1 2 3 4 5 6 7 def input_fn(dataset): def _fn(): feat_tensor = tf.constant(dataset.data) feat_tesnor = tf.Print(feat_tensor, data=[feat_tensor, tensor2, tensor3], # feat_tensor will be printed \u0026amp; returned, tensor2,tensor3 are printed btw. message=\u0026#34;Inputs are: \u0026#34;) feat = {feat_name: feat_tensor} Using tf.Print() in TensorFlow -Yufeng Guo\n(2023-04-16)\nLambdaCallback doesn\u0026rsquo;t work sumNotes\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras.callbacks import LambdaCallback tf.enable_eager_execution() # Define the model architecture model = tf.keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation=\u0026#39;relu\u0026#39;), Dense(10) ]) # Define the loss function and optimizer loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = tf.keras.optimizers.Adam() # Define a function to save the intermediate tensor def save_intermediate_tensor(epoch,): intermediate_tensor = model.layers[0].output # Replace this with the layer whose output you want to save print(intermediate_tensor) # only show the description once # tf.print(intermediate_tensor) # AttributeError: \u0026#39;Tensor\u0026#39; object has no attribute \u0026#39;_datatype_enum\u0026#39; # np.savez(f\u0026#39;vec{epoch}\u0026#39;, intermediate_tensor.numpy()) # AttributeError: \u0026#39;Tensor\u0026#39; object has no attribute \u0026#39;numpy\u0026#39; (with egar execution enabled) # tf.io.write_file(f\u0026#39;vec{epoch}\u0026#39;, tf.strings.as_string(intermediate_tensor)) # rank has to be 0 save_callback = LambdaCallback(on_epoch_end=save_intermediate_tensor) # Load the MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # Normalize the pixel values x_train = x_train[:100] / 255.0 # Compile the model model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\u0026#39;accuracy\u0026#39;]) # Train the model for epoch in range(10): for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train, y_train)): x_batch_train = x_batch_train.reshape((-1, 28, 28)) y_batch_train = y_batch_train.reshape((-1,)) with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) save_callback.on_epoch_end(epoch) DDG search: \u0026lsquo;Tensor\u0026rsquo; object has no attribute \u0026rsquo;numpy\u0026rsquo;\nTF 2.0 \u0026lsquo;Tensor\u0026rsquo; object has no attribute \u0026rsquo;numpy\u0026rsquo; while using .numpy() although eager execution enabled by default#27519\nTF 2.x @tf.function Create a dataflow graph for a Python function. Then its outputs can be printed. Docs\n1 2 3 4 5 6 @tf.function def foo(): a = tf.random.uniform([1], maxval=5, seed=1) b = tf.random.uniform([1], maxval=5, seed=1) return a+b print(foo()) # 2*a Ref AttributeError: 'Tensor' object has no attribute 'numpy' - SO Tensorflow 的 Tensor 和 OpKernel 分析-高鹏 在TensorFlow中怎么打印Tensor对象的值 How to print the value of a Tensor object in TensorFlow? - SO FailedPreconditionError: Attempting to use uninitialized in Tensorflow - SO Tensorflow之调试(Debug)及打印变量 - Shiyu_Huang -博客园 ","date":"2023-04-08T10:48:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf_print_tensor/","title":"memo: TF | Print Tensors"},{"content":"Source video: 【俗说矩阵】逆矩阵的内涵原来这么丰富！快来看看吧！\nInverse of diagonal matrix A diagonal matrix with the given values: 2,3,4 as follows.\n$$ 𝐀 = \\text{diag\\\\{2, 3, 4\\\\}} = \\begin{bmatrix} 2 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 3 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 4 \\end{bmatrix} $$The augmented matrix is constructed and perform elementary row operations as:\n$$ 𝐀|𝐈 = \\begin{bmatrix} 2 \u0026 0 \u0026 0 \u0026 | 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 3 \u0026 0 \u0026 | 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 4 \u0026 | 0 \u0026 0 \u0026 1 \\end{bmatrix} \\overset{1/2r1,1/3r2, 1/4r3}{⇢} \\begin{bmatrix} 1 \u0026 0 \u0026 0 \u0026 | 1/2 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \u0026 | 0 \u0026 1/3 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 | 0 \u0026 0 \u0026 1/4 \\end{bmatrix} $$From the results, the inverse matrix 𝐀⁻¹ of a diagonal matrix is also a diagonal matrix. And its elements on the main diagonal are the reciprocal of elements on the original diagonal, where the element cannot be 0.\n$$ 𝐀⁻¹ = \\begin{bmatrix} 1/2 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1/3 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1/4 \\end{bmatrix} = \\text{diag\\\\{1/2, 1/3, 1/4\\\\}} $$Inverse of a scalar matrix If all elements on the diagonal are the same, the matrix is a schalar matrix. 𝚲 = diag{λ, \u0026hellip;, λ}, λ≠0.\nIts inverse matrix is a scalar matrix where the elements on the diagonal are all 1/λ, 𝚲⁻¹ = diag{λ⁻¹, \u0026hellip;, λ⁻¹}, λ≠0.\nIf λ = 1, the scalar matrix is identity matrix 𝐈. So the inverse of a identity matrix is itself: 𝐈⁻¹ = 𝐈.\nInverse of the elementary matrix The elementary matrix is definitely invertible. So their inverse matrices must exist.\nInverse of a row-switching matrix For a row-switching matrix (permutation matrix) switching the first 2 rows of the identity matrix:\n$$ 𝐅₁ = \\begin{bmatrix} 0 \u0026 1 \u0026 0 \\\\\\ 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$Write the augmented matrix: $$ 𝐅₁|𝐈 = \\begin{bmatrix} 0 \u0026 1 \u0026 0 \u0026| 1 \u0026 0 \u00260\\\\\\ 1 \u0026 0 \u0026 0 \u0026| 0 \u0026 1 \u00260\\\\\\ 0 \u0026 0 \u0026 1 \u0026| 0 \u0026 0 \u00261 \\end{bmatrix} $$The inverse matrix can be obtained by switching the first two rows and letting the left part become a identity matrix. Then the inverse matrix is the right part:\n$$ 𝐅₁⁻¹ = \\begin{bmatrix} 0 \u0026 1 \u00260\\\\\\ 1 \u0026 0 \u00260\\\\\\ 0 \u0026 0 \u00261 \\end{bmatrix} $$It indicates that the inverse of a row-switching matrix is itself. (Just like the identity matrix, its inverse is itself.)\nInverse of a row-multiplying matrix For a row-multiplying matrix scaling the second rows by 2 times:\n$$ 𝐅₂ = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 2 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$Therefore, its inverse is obtained by multiplying 1/2 onto the 2nd row of the identity matrix.\n$$ 𝐅₂⁻¹ = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1/2 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$That means the inverse of a row-multiplying matrix will still be a row-multiplying matrix. But the scale factor becomes the reciprocal.\nInverse of a row-addition matrix For a row-addition matrix: the second row is added by 2-times first row. $$ 𝐅₃ = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 2 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$Its inverse is obtained by lettingthe 2nd row in the identity matrix add the -2-times of the first row:\n$$ 𝐅₃⁻¹ = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ -2 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$Thus, its inverse is just changing the scale factor to its opposite.\nIn summary, the inverse matrices of elementry matrices are the same type.\nThe inverse of a row-switching matrix is itself. The inverse of a row-multiplication matrix is changing the scale factor to its reciprocal. The inverse of a row-addition matrix is changing the scale factor to its opposite. Inverse of second-order matrix The number of rows and columns of a second-order matrix are both 2.\n","date":"2023-04-03T16:17:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/13_%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E6%89%A9%E5%B1%95%E5%BA%94%E7%94%A8/","title":"watch: LA - 高山 13 | Application of Inverse Matrix"},{"content":"Source video: 【俗说矩阵】初等变换求逆矩阵原来是这个原理！数学老师看后大赞！\n𝐀 can perform 9 elementary row operations to reach the identity matrix 𝐈.\n$$ 𝐀 = [^{\\_{1\\ 3\\ 5}} \\_{^{2\\ 4\\ 6} \\_{1\\ 4\\ 9}}] ➔ [^{\\_{1\\ 3\\ 5}} \\_{^{1\\ 2\\ 3} \\_{1\\ 4\\ 9}}] ➔ [^{\\_{1\\ 2\\ 3}} \\_{^{1\\ 3\\ 5} \\_{1\\ 4\\ 9}}] ➔ [^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 1\\ 2} \\_{1\\ 4\\ 9}}] ➔ [^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 1\\ 2} \\_{0\\ 2\\ 6}}] ➔ [^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 1\\ 2} \\_{0\\ 0\\ 2}}] ➔ [^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 1\\ 2} \\_{0\\ 0\\ 1}}] ➔ [^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}] ➔ [^{\\_{1\\ 2\\ 0}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}] ➔ [^{\\_{1\\ 0\\ 0}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}] = 𝐈 $$𝐀 is multiplied with 9 elementary matrix on the left: 𝐈 = 𝐅₉𝐅₈\u0026hellip;𝐅₁𝐀\nThe product of these 9 elementary matrix is called 𝐀⁻¹, the inverse of 𝐀. So 𝐀⁻¹𝐀 = 𝐈, and 𝐀⁻¹ = 𝐅₉𝐅₈\u0026hellip;𝐅₁.\n𝐀⁻¹ will not change if it\u0026rsquo;s multiplied with 𝐈: 𝐀⁻¹ = 𝐅₉𝐅₈\u0026hellip;𝐅₁𝐈\nThis equation indicates that an identity matrix 𝐈 ($[^{\\_{1\\ 0\\ 0}}\\_{^{0\\ 1\\ 0}\\_{0\\ 0\\ 1}}]$) can perform exactly the same 9 elementary row operations to reach the inverse matrix 𝐀⁻¹ = $[^{\\_{-3\\ 7/4\\ 1/2}} \\_{^{ 3\\ -1\\ -1}\\_{-1\\ 1/4\\ 1/2}}]$.\n$$ \\begin{array}{c} 𝐀 = \\begin{bmatrix} 1 \u0026 3 \u0026 5 \\\\\\ 2 \u0026 4 \u0026 6 \\\\\\ 1 \u0026 4 \u0026 9 \\end{bmatrix} \\rm \\overset{\\text{9 elementary row}}{operations→} 𝐈 = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\\\\\ 𝐈 = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\rm \\overset{\\text{9 elementary row}}{operations→} 𝐀⁻¹ = \\begin{bmatrix} -3 \u0026 7/4 \u0026 1/2 \\\\\\ 3 \u0026 -1 \u0026 -1 \\\\\\ -1 \u0026 1/4 \u0026 1/2 \\end{bmatrix} \\end{array} $$If an matrix 𝐀 can perform multiple elementary row operations to become an identity matrix 𝐈, then 𝐈 can perform the same 9 elementary row operations simultaneously to reach the inverse matrix 𝐀⁻¹.\nIn another word, the transformation from 𝐈 to 𝐀⁻¹ replicates the 9 elementary row opertations from 𝐀 to 𝐈.\nAugmented matrix performs elementary row operations To keep the sequence of operations the same, the augmented matrix 𝐀|𝐈 is leveraged.\n$$ 𝐀|𝐈 = \\begin{bmatrix} 1 \u0026 3 \u0026 5 \u0026 | 1 \u0026 0 \u0026 0 \\\\\\ 2 \u0026 4 \u0026 6 \u0026 | 0 \u0026 1 \u0026 0 \\\\\\ 1 \u0026 4 \u0026 9 \u0026 | 0 \u0026 0 \u0026 1 \\end{bmatrix} $$This augmented matrix 𝐀|𝐈 can perform 9 elementary matrix to make the left part of the vertical line become a identity matrix. Such that the right part of the vertical line becomes the inverse matrix 𝐀⁻¹.\n$$ 𝐀|𝐈 = 𝐈|𝐀⁻¹ $$This method is applicable to the matrix that its elements are given.\nExample Given a matrix 𝐀 = $[^{\\_{1\\ 1\\ 2}} \\_{^{1\\ 2\\ 3} \\_{2\\ 4\\ 5}}]$, solve the inverse 𝐀⁻¹.\nConstruct the augmented matrix:\n$$ 𝐀|𝐈 = \\begin{bmatrix} 1 \u0026 1 \u0026 2 \u0026 | 1 \u0026 0 \u0026 0 \\\\\\ 1 \u0026 2 \u0026 3 \u0026 | 0 \u0026 1 \u0026 0 \\\\\\ 2 \u0026 4 \u0026 5 \u0026 | 0 \u0026 0 \u0026 1 \\end{bmatrix} $$ Perform elementary row operations on this augmented matrix to make the left part become 𝐈\n$$ \\begin{bmatrix} 1 \u0026 1 \u0026 2 \u0026 | 1 \u0026 0 \u0026 0 \\\\\\ 1 \u0026 2 \u0026 3 \u0026 | 0 \u0026 1 \u0026 0 \\\\\\ 2 \u0026 4 \u0026 5 \u0026 | 0 \u0026 0 \u0026 1 \\end{bmatrix} →^{\\text{row echelon}}\\_{form}: \\begin{bmatrix} 1 \u0026 1 \u0026 2 \u0026 | 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 1 \u0026 | -1 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 | 0 \u0026 2 \u0026 -1 \\end{bmatrix} →^{\\text{clear top-}}\\_{\\text{right part}}: \\begin{bmatrix} 1 \u0026 0 \u0026 0 \u0026 | 2 \u0026 -3 \u0026 1 \\\\\\ 0 \u0026 1 \u0026 0 \u0026 | -1 \u0026 -1 \u0026 1 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 | 0 \u0026 2 \u0026 -1 \\end{bmatrix} $$ Row echelon form: Clear the values on the lower left side of the main diagonal from top to bottom Clear the values on the top right side of the main diagonal from bottom to top The right-hand side of the vertical line individually is the inverse matrix 𝐀⁻¹.\n$$ \\begin{bmatrix} 1 \u0026 0 \u0026 0 \u0026 | 2 \u0026 -3 \u0026 1 \\\\\\ 0 \u0026 1 \u0026 0 \u0026 | -1 \u0026 -1 \u0026 1 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 | 0 \u0026 2 \u0026 -1 \\end{bmatrix} = 𝐈|𝐀⁻¹ $$ So,\n$$ 𝐀⁻¹ = \\begin{bmatrix} 2 \u0026 -3 \u0026 1 \\\\\\ -1 \u0026 -1 \u0026 1 \\\\\\ 0 \u0026 2 \u0026 -1 \\end{bmatrix} $$ 2x2 (2024-02-12)\nThe inverse of $[^{a\\ b}_{d \\ c}]$ is:\n$$ \\underbrace{ \\begin{bmatrix} 1 \u0026 -\\frac{b}{a} \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{d} \u0026 0 \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \\\\\\ 0 \u0026 \\frac{a}{ac-bd} \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \\\\\\ -1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} \\frac{d}{a} \u0026 0 \\\\\\ 0 \u0026 1 \\end{bmatrix} }\\_{A⁻¹} \\begin{bmatrix} a \u0026 b \\\\\\ d \u0026 c \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\\\ 0 \u0026 1 \\end{bmatrix} $$The combinatio nof 5 matrices is the inverse matrix:\n$$ A⁻¹ = \\begin{bmatrix} \\frac{c}{ac-bd} \u0026 -\\frac{b}{ac-bd} \\\\\\ -\\frac{d}{ac-bd} \u0026 \\frac{a}{ac-bd} \\end{bmatrix} = \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026 -b \\\\\\ -d \u0026 a \\end{bmatrix} $$ Hence, the 1/determinant is a part of the inverse matrix. Matlab A is invertible:\n1 2 3 4 5 6 7 8 A = [1, 1, 2; 1, 2, 3; 2, 4, 5] inv(A) ans = 2 -3 1 -1 -1 1 0 2 -1 B is not invertible:\n1 2 3 4 5 6 7 8 9 10 B = [1, 1, 1; 2, 2, 2; 3, 3, 3] inv(B) Warning: Matrix is singular to working precision. ans = Inf Inf Inf Inf Inf Inf Inf Inf Inf C is not a square matrix:\n1 2 3 4 5 C = [1, 2, 3; 4, 5, 6] inv(C) Error using inv Matrix must be square. ","date":"2023-04-03T12:31:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/12_%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E6%B3%95/","title":"watch: LA - 高山 12 | How to find the inverse matrix?"},{"content":"Matrix multiplication doesn\u0026rsquo;t have commutative property (交换律). But some square matrices satisfy 𝐀𝐁=𝐁𝐀\nMatrix multiplication has Associative property (结合律)\n(𝐀𝐁)𝐂 = 𝐀(𝐁𝐂)\nCompund linear mapping\nTranspose and multiplication (𝐀𝐁)ᵀ ≠ 𝐀ᵀ𝐁ᵀ\n(𝐀𝐁)ᵀ = 𝐁ᵀ𝐀ᵀ\nSquare matrix multiplication The number of rows = the number of columns\n𝐀 = $[^{1\\ 2} \\_{1\\ 4}]$ ; 𝐁 = $[^{4\\ -2} \\_{-1\\ 1}]$\nThere is 𝐀𝐁 = 𝐁𝐀.\nSo these two matrices 𝐀, 𝐁 are commutative 1. 𝐀,𝐁 must be the square matrices with the same size, and the result also has the identical-size square matrix.\nPower of square matrix 𝐀ᵏ = 𝐀⋅𝐀ᵏ⁻¹ = 𝐀ᵏ⁻¹⋅𝐀, where 𝐀 and 𝐀ᵏ⁻¹ are commutative because of the commutative property of matrix multiplication.\nTherefore, the two positive integar powers of matrix 𝐀 are always commutative.\nPower of the matrix only applicable to square matrices. Otherwise, the mismatched dimensions prevent performing matrices multiplication.\n𝐀¹=𝐀 ; 𝐀⁰ = 𝐈\nThus, the range of power k is the set of natural numbers k∈ ℕ\nPower of diagonal matrix $$ 𝐀 = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 2 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 3 \\end{bmatrix} = diag\\\\{1, 2, 3\\\\} $$Each elements on the main diagonal is multiplied with itself many times.\n2\nScalar matrix $$ 𝐀 = \\begin{bmatrix} λ \u0026 0 \u0026 0 \\\\\\ 0 \u0026 λ \u0026 0 \\\\\\ 0 \u0026 0 \u0026 λ \\end{bmatrix} = diag\\\\{λ, λ, λ\\\\} $$If λ = 1, then 𝐀 = 𝐈. So 𝐈ᵏ=𝐈, and this is also applicable to zero matrix 𝟎ᵏ= 𝟎 (k≠0)\nRef When are two matrices A and B: AB = BA? - StackExchange 11. Powers of matrices - Massachusetts Institute of Technology ","date":"2023-04-02T14:55:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/08_%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E7%9A%84%E6%80%A7%E8%B4%A8/","title":"watch: LA - 高山 08 | Properties of matrix multiplication"},{"content":"Review Zero matrix 𝐀 = [^{4\\ 5\\ 6} _{5\\ 6\\ 7}]; 𝟎 = [^{_{0\\ 0}} _{^{0\\ 0} _{0\\ 0}}]₃ₓ₂\n𝐀𝟎 = [^{0\\ 0} _{0\\ 0}] = 𝟎₂ₓ₂\nNon-homogeneous system has no zero solution 𝐀𝐁 ≠ 0 means A, B both cannot be zero.\nDiagonal matrix and identical matrix 𝐁 is a diagonal matrix.\n𝐀𝐁 is scaling each column of 𝐀 by the value of element on the diagonal times.\nI = []\nVectors multiplication Rank relation 𝐀 = [^{1\\ 2} _{0\\ 3}]; 𝐁 = [^{2\\ 3} _{0\\ 4}]\n𝐀𝐁 = [^{2\\ 11} _{0\\ 12}]\nr(𝐀) = 2; r(𝐁) = 2; r(𝐀𝐁) = 2\nIf 𝐁 = [^{1\\ 0} _{0\\ 0}], r(𝐁) = 1, then 𝐀𝐁 = [^{1\\ 0}_{0\\ 0}]; r(𝐀𝐁) = 1.\nFurther letting 𝐀 = [^{1\\ 2} _{0\\ 0}], r(𝐀) = 1, then 𝐀𝐁 = [^{1\\ 0}_{0\\ 0}]; r(𝐀𝐁) = 1.\nIn addition, if 𝐀 = [^{1\\ 2} _{0\\ 0}], 𝐁 = [^{0\\ -2} _{0\\ 1}], then 𝐀𝐁 = [^{0\\ 0}_{0\\ 0}], r(𝐀𝐁) = 0\nConclusion: the rank of the product matrix is not greater than the rank of any multiplier.\nr(𝐀𝐁) ≤ min{ r(𝐀), r(𝐁)}\n","date":"2023-04-02T14:29:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/07_%E7%89%B9%E6%AE%8A%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E6%B3%95/","title":"watch: LA - 高山 07 | Extension for matrix multiplication"},{"content":"Source video: 【俗说矩阵】初识矩阵的秩，从最简单的例子讲起！\nFind the rank Rank of a matrix 𝐀 equals to the number of the non-zero rows of a matrix in row echelon form (after performing elementary row operations).\nDefinition of the rank determinant\nProperties The matrices appearing in the transforming process are able to reach the same row echelon form, so they have the same rank. In another word, the elementary row operations don\u0026rsquo;t change the rank of matrices. The rank of a matrix is no bigger than the number of rows or columns. Thus, the solution of a linear equation system can be represented as its rank.\nSolution judged by rank Homogeneous linear equation system:\nOnly zero solution: r(𝐀) = n, the rank of the coefficient matrix equals to the number of unknowns Exist non-zero solution: r(𝐀) \u0026lt; n, Non-homogeneous linear equation system:\nSingle unique solution: r(𝐀|𝐛) = r(𝐀) = n, the rank of the augmented matrix = the rank of the coefficient matrix = the number of the unknowns; Inifinitely many solutions: r(𝐀|𝐛) = r(𝐀) \u0026lt; n; No solution: r(𝐀|𝐛) ≠ r(𝐀) r(𝐀) = 0 𝐀 = $[^{\\_{0\\ 0\\ 0}} \\_{^{0\\ 0\\ 0} \\_{0\\ 0\\ 0}}]$\nAll of its elements are 0. There is no non-zero rows. So r(𝐀) = 0.\nThis is the so-called zero matrix noted as 𝐀 = 𝟎\nr(𝐀) = 0 and 𝐀=𝟎 are equivalent, because if any one of elements is not 0, then there is a non-zero row resulting the r(𝐀) ≠ 0.\nr(𝐀) = 1 𝐀 = $[^{\\_{1\\ 2\\ 3}} \\_{^{2\\ 4\\ 6}\\_{3\\ 6\\ 9}}]$ ➔ $[^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 0\\ 0}\\_{0\\ 0\\ 0}}]$\nThe rows in the matrix of r(𝐀) = 1 are proportional to each other. In fact, the columns are also proportional. Their ratio factor can be 0, and also there must be at least 1 element which is not zero in the matrix. Otherwise, it\u0026rsquo;s the zero matrix. 𝐚= [1 2 3] and 𝐛 = $[^{_1} \\_{^2\\_3}]$ are 1x1 matrix with rank = 1.\nVectors are the matrix with only 1 row or 1 column.\nRow vector with at least 1 non-zero value is of rank=1.\nColumn vector has multiple rows with a single value. If it\u0026rsquo;s not a zero vector, the later rows can be reduced to 0 by adding the row1 multiplying with different factors. 𝐛 = $[^{_1} \\_{^2\\_3}]$ ➔ $[^{_1} \\_{^1\\_1}]$\nFor all the non-zero vector, no matter row vector or column vector, their rank = 1.\nVector 𝐚 ≠ 0 and r(𝐚) = 1 are equivalent.\n","date":"2023-04-02T12:13:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/04_%E5%88%9D%E8%AF%86%E7%9F%A9%E9%98%B5%E7%9A%84%E7%A7%A9/","title":"watch: LA - 高山 04 | Rank of matrix"},{"content":"The 1-17 tips come from Faster Deep Learning Training with PyTorch – a 2021 Guide - LORENZ KUHN\n1. Consider using another learning rate schedue torch.optim.lr_scheduler.CyclicLR torch.optim.lr_scheduler.OneCycleLR 2. Use multiple workers and pinned memory in DataLoader torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=4, pin_memory=pin_memory) DataLoader-Docs\nnum_workers Rule of thumb: set the num_workers to four times the number of available GPUs. Note that increasing num_workers will increase RAM consumption.\npin_memory When using a GPU it’s better to set pin_memory=True, this instructs DataLoader to use pinned (page-locked) memory and enables faster and asynchronous memory copy from the host to the GPU. Tutorial-Szymon Migacz\npin_memory avoid one implicit CPU-to-CPU (\u0026ldquo;Pageable Memory\u0026rdquo; to \u0026ldquo;Pinned Memory\u0026rdquo;) copy when perform a.cuda() operation. As the illustration shows in Nvidia Blog I fogot where I got this inspiration: \u0026ldquo;点对点复制\u0026rdquo;.\nWith pinned memory tensors, the copy process a.cuda(non_blocking=True) is asynchronous with respect to host (CPU) SO. If the code is structured as:\na.cuda(non_bloking=True) # copy from cpu to gpu Perform some CPU operations Perform GPU operations using a. The step 1 and step 2 can proceed parallelly. Hence, the maximum time can be saved is the duration of step 2.\n3. Max out the batch size Other hyperparameters, like learning rate, have to be adjusted. Rule of thumb: double the learning rate as double the batch size May cause worse generalization performance. 4. Use Automatic Mixed Precision (AMP) The optimizations of some operations use semi-precision (FP16) rather than single-precision (FP32) 5. Consider using another optimizer AdamW outperform Adam resulting from weight decay (rather than L2-regularization). 6. Turn on cudNN benchmarking Tutorial-Szymon Migacz\n7. Avoid unnecessary CPU-GPU synchronizations (Tutorial-Szymon Migacz):\ntensor.cpu() or tensor.cuda(), tensor.to() tensor.item() or tensor.numpy() print(cuda_tensor) cuda_tensor.nonzero() retrieves the indices of all non-zero elements; Avoid python control based on cuda tensors, e.g., if (cuda_tensor != 0).all() The good practice should let the CPU run ahead of the accelerator as much as possible to make sure that the accelerator work queue contains may operations.\nRef ","date":"2023-03-23T14:11:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_train_faster/","title":"memo: PyTorch | Tricks for Faster Training"},{"content":"Authors: Wandong Zhang, Jonathan Wu, Yimin Yang Neurocomputing (2020-07-18)\nAbstract Wide hierarchical subnetwork-based neural network (Wi-HSNN) Iterative training by adding subnetnetwork nodes into modle one-by-one batch-by-batch training instead of processing the entire dataset (one-batch) Place365 has 1.8 million samples 1 Introduction 3 Proposed Wi-HSNN Loss function of SLFN is MSE: min J= ½ ‖𝐓 - g(𝐗, 𝐰₁, 𝐛)⋅𝐰₂‖²\nThe output weights can be initialized from the Target data (letting 𝐇 denote g(𝐗, 𝐰₁, 𝐛)):\n𝐰₂ = ( 𝐇ᵀ𝐇 + I/C )⁻¹ 𝐇ᵀ 𝐓\nIn this paper, the input data 𝐗 is first transformed to 𝐅ₑₙ, which then passes a SNN (SLFN, subnetwork node) and becomes g(𝐅ₑₙ⋅𝐚ₑₓ + bₑₓ). Next, the input 𝐗 is fed into multiple SNN sequentially. And their outputs are accumulated with the weight 𝐚ₜ.\nTherefore, if there are L SNNs, the loss function for this problem is:\nmin J = ½ ‖𝐓 - ∑ᵢ₌₁ᴸ g(𝐅ₑₙⁱ⋅𝐚ₑₓⁱ + bₑₓⁱ) ⋅ 𝐚ₜᴸ‖²\n3.2 Training the Wi-HSNN Feedforward with randomly initialized (𝐚ₑₙ¹, bₑₙ¹) and (𝐚ₑₓ¹, bₑₓ¹) and calculate the optimal output weights based on pseudo-inverse:\n𝐅ₑₙ¹ = g(𝐗⋅𝐚ₑₙ¹ + bₑₙ¹) 𝐅ₜ¹ = 𝐅ₑₓ¹ = g(𝐅ₑₙ¹ ⋅𝐚ₑₓ¹ + bₑₓ¹) 𝐚ₜ¹ = (𝐅ₜᵀ𝐅ₜ + I/C)⁻¹ 𝐅ₜᵀ 𝐓\nObtain the feedback error (\u0026ldquo;feature H\u0026rdquo;) matrix 𝐏ₑₓ¹ and 𝐏ₑₙ¹ for solving the weights 𝐚ₑₓ (=𝐰₂), 𝐚ₑₙ (=𝐰₁) of next iteration.\n𝐏ₑₓ¹ = g⁻¹ (𝐞¹ ⋅ (I/C + (𝐚ₜ¹)ᵀ⋅𝐚ₜ¹)⁻¹ ⋅ (𝐚ₜ¹)ᵀ ) 𝐏ₑₙ¹ = g⁻¹ (𝐏ₑₓ¹ ⋅ (I/C + (𝐚ₑₓ¹)ᵀ ⋅ 𝐚ₑₓ¹ )⁻¹ ⋅ (𝐚ₑₓ¹)ᵀ )\nCalculate the 𝐚ₑₓ, 𝐚ₑₙ for next SNN:\n𝐚ₑₙⁱ = (𝐗ᵀ𝐗 + I/C )⁻¹ 𝐗ᵀ 𝐏ₑₓⁱ⁻¹ (Entrance layer weights) 𝐚ₑₓⁱ = ( (𝐅ₑₙⁱ)ᵀ𝐅ₑₙⁱ + I/C )⁻¹ (𝐅ₑₙⁱ)ᵀ 𝐏ₑₙⁱ⁻¹ (Exit layer weights)\nSummarize outputs 𝐅ₜⁱ from all exisiting SNN, and update output weight 𝐚ₜⁱ.\n𝐅ₜⁱ = ∑ₖ₌₁ⁱ 𝐅ₑₓᵏ 𝐚ₜⁱ = ( (𝐅ₜⁱ)ᵀ𝐅ₜⁱ + I/C )⁻¹ (𝐅ₜⁱ)ᵀ 𝐓\nObtain the feedback error 𝐏ₑₓⁱ and 𝐏ₑₙⁱ:\n𝐏ₑₓⁱ = g⁻¹ (𝐞ⁱ ⋅ (I/C + (𝐚ₜⁱ)ᵀ⋅𝐚ₜⁱ)⁻¹ ⋅ (𝐚ₜⁱ)ᵀ ) 𝐏ₑₙⁱ = g⁻¹ (𝐏ₑₓⁱ ⋅ (I/C + (𝐚ₑₓⁱ)ᵀ ⋅ 𝐚ₑₓⁱ )⁻¹ ⋅ (𝐚ₑₓⁱ)ᵀ )\nRepeat step 3 to step 5 L-2 times. 𝐅ₜᴸ is the final encoding. And 𝐘 = 𝐅ₜᴸ 𝐚ₜᴸ is the final classification prediction.\n3.4 Batch-by-batch scheme with parallelism strategy The entire feature set 𝐅ᴺᕽᵈ (i.e., 𝐇) and the target set 𝐓 are split into p subsets:\n𝐇 = $\\[^{𝐇(𝐱₁)}\\_{^{...}\\_{𝐇(𝐱ₚ)}}\\]$, 𝐓 = $\\[^{𝐓(𝐱₁)}\\_{^{...}\\_{𝐓(𝐱ₚ)}}\\]$\nThe desired weights matrix 𝐰₂ (i.e., \u0026ldquo;𝛃\u0026rdquo;) represented weith pseudo-inverse matrix becomes\n𝐰₂ = (𝐇ᵀ𝐇 + I/C)⁻¹𝐅ᵀ⋅𝐓\n𝐰₂ = ([𝐇₁ᵀ,\u0026hellip;, 𝐇ₚᵀ] $\\[^{\\_{𝐇(𝐱₁)}} \\_{^{...}\\_{𝐇(𝐱ₚ)}}\\] + ^I\\_{^-\\_C}$)⁻¹ [𝐇₁ᵀ,\u0026hellip;, 𝐇ₚᵀ] $[ ^{\\_{𝐓(𝐱₁)}}\\_{^{...}\\_{𝐓(𝐱ₚ)}} ]$\n𝐰₂ = ([𝐇(𝐱₁)ᵀ𝐇(𝐱₁) + \u0026hellip; + 𝐇(𝐱ₚ)ᵀ𝐇(𝐱ₚ)] + I/C)⁻¹ ⋅ [𝐇(𝐱₁)ᵀ𝐓(𝐱₁) + \u0026hellip; + 𝐇(𝐱ₚ)ᵀ𝐓(𝐱ₚ) ] 𝐰₂ = (∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐇(𝐱ᵢ) + I/C)⁻¹ ⋅ ∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐓(𝐱ᵢ)\nFirst, calculate (∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐇(𝐱ᵢ) + I/C)⁻¹.\nThe matrices are accumulated batch by batch in the code. After the 1st iteration, K=(𝐇₁ᵀ𝐇₁ + I/C)⁻¹ has obtained and returned.\nWhen next batch 𝐇₂ is retrieved:\nK_new = (𝐇₂ᵀ𝐇₂ + 𝐇₁ᵀ𝐇₁ + I/C)⁻¹ = (𝐇₂ᵀ𝐇₂ + K⁻¹)⁻¹ # analogy to (UBV + A)⁻¹, where U=𝐇₂ᵀ, V=𝐇₂, B=I, A=K⁻¹ Woodbury: \u0026ldquo;A⁻¹ - A⁻¹⋅U⋅(I+BV⋅A⁻¹⋅U)⁻¹ BV A⁻¹ \u0026quot; 1 = K - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ K = (I - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ ) ⋅ K Let Kₚ = (I - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ ). So K_new = Kₚ ⋅ K\nThen, for the second item ∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐓(𝐱ᵢ),\nIn the first batch, 𝛃=K⋅𝐇₁ᵀ⋅𝐓₁ is obtained and returned.\nWhen the second batch coming, there should be (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂)\n𝛃_new = K_new ⋅ (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂) = K_new ⋅ (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂) = Kₚ ⋅ K ⋅ 𝐇₁ᵀ⋅𝐓₁ + K_new ⋅ 𝐇₂ᵀ⋅𝐓₂ = Kₚ ⋅ 𝛃 + K_new ⋅ 𝐇₂ᵀ⋅𝐓₂\n矩阵之和的逆 (DDG search: \u0026ldquo;矩阵之和的逆\u0026rdquo;) 两个矩阵相加后求逆 - 海棠依旧 - CSDN\n关于两个矩阵之和逆阵的讨论 - docin\n(Google search: \u0026ldquo;两矩阵和的逆\u0026rdquo;) 兩矩陣和的逆矩陣 - 線代啟示錄 - 周志成(阳明交大)\n矩阵之和的逆 不等于 逆矩阵的和 (A+B)⁻¹ ≠ A⁻¹ + B⁻¹\n∵ (A+B)(A⁻¹ + B⁻¹) = E + BA⁻¹ + AB⁻¹ + E ≠ E\n最简单的例子：取 A=B=E，(A+B)(A⁻¹ + B⁻¹) = E + BA⁻¹ + AB⁻¹ + E = 4E ≠ E 矩阵和的逆矩阵 逆矩阵的和 相等吗 -百度知道\nRef 兩矩陣和的逆矩陣 - 線代啟示錄 - 周志成(阳明交大) ","date":"2023-03-15T20:13:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-snn-batch-train/","title":"Read: Optim - SLFN | Wi-HSNN for DR"},{"content":"Inverse of Sigmoid 1 2 def inverse_sigmoid(x): return torch.log(x/(1-x)) Inverse of Softmax Determining Existence Only monotonic function has its inverse function. ¹\nThe softmax function is not invertible? Any constant C will work. Or C=(1−log(x⋅y⋅z))/3) Code Implement a layer to invert the softmax by tensorflow: ²\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import tensorflow as tf def inv_softmax(x, C): return tf.math.log(x) + C import math input = tf.keras.layers.Input(shape=(1,10)) x = tf.keras.layers.Lambda(lambda x : inv_softmax(x, math.log(10.)),name=\u0026#39;inv_softmax\u0026#39;)(input) model = tf.keras.Model(inputs=input, outputs=x) a = tf.zeros([1, 1, 10]) a = tf.nn.softmax(a) a = model(a) print(a.numpy()) Ref 反函数的定义及求法 - 半个冯博士的文章 - 知乎; (Searched by DDG: \"求和函数的反函数\") How to create a layer to invert a softmax (TensforFlow,python)? - StackOverflow Can we have an explicit representation of inverse of a softmax function? -StackExchange (searched by DDG: \"softmax function inverse\") Invert the softmax function - StackExchange What is an intuitive interpretation for the softmax transformation?\n一文详解Softmax函数 - 触摸壹缕阳光的文章 - 知乎 (searched by DDG: \"sotmax\") (2023-02-23)\nInverse of Sum Is the summation operation invertible?\n(DDG search: \u0026ldquo;inverse summation\u0026rdquo; or \u0026ldquo;invert a summation\u0026rdquo;)\n","date":"2023-02-19T11:00:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/inverse_softmax/","title":"memo: Calc | Inverse Function of Activations"},{"content":"Authors: Wandong Zhang et. al. Publish date: 2020-03-30 (Finished in 2019)\nIEEE Trans. Industrial Informatics | G.Drive | G.Scholar\nTry to summary (2023-02-26):\nDifferent features are concatenated and then fed into a \u0026ldquo;I-ELM with subnetwork nodes\u0026rdquo;. What is optimized it the combination weights, but the feature vectors themselves are not changed. It is the weights (IW, 𝛃) are refined. Sepecifically, the new R-SNN node is improved by adding a part of unlearned wights accquired from the residual error of the last node. That is the weights are accumulated on the newest node. So the ultimate R-SNN node contains all the previous training outcomes. What we kept is only the last R-SNN node, i.e., a SLFN. Does that require the performance of the final R-SNN is the best among the former nodes. (In code) The update process of a SLFN is as follows:\nflowchart LR subgraph node1[SLFN1] h1((h1)) \u0026 h2((h2)) \u0026 he((he)) \u0026 hd((hd)) end In[\"X\\n data\\n matrix\"] --\u003e IW1 --\u003e h1 \u0026 h2 \u0026 he \u0026 hd node1 --- beta1(\"𝛃1\") --\u003e Yout1 --\u003e Error1 Target --\u003e|\"pinv\"| beta1 beta1 \u0026 Error1 --\u003e|\"inv: 𝛃⋅P=𝐞₁\"| P[\"P\\n ( The H\\n yielding\\n Error1) \"] P \u0026 In --\u003e IW'[\"residual\\n IW\"] IW' \u0026 IW1 --\u003esum((+)) --\u003e IW2 subgraph node2[SLFN2] h21((h1)) \u0026 h22((h2)) \u0026 h2e((he)) \u0026 h2d((hd)) end IW2 --\u003e h21 \u0026 h22 \u0026 h2e \u0026 h2d node2 --- beta2(\"𝛃2\") Target --\u003e |\"pinv\"| beta2 beta2 --\u003e Yout2 Abstract A supervised multi-layer subnetwork-based feature refinement and classification model for representation learning. Expand the width for a generalized hidden layer rather than stack more layers to go deeper One-shot solution for finding the meaningful latent space to recognize the objects rather than searching separate spaces to find a generalized feature space. Multimodal fusion fusing various feature sources into a superstate encoding instead of a unimodal feature coding in the traditional feature representation methods. Ⅰ. Introduction (Task \u0026amp; Application \u0026amp; List of ralated research field \u0026amp; Problem \u0026amp; Existing solutions brif)\nTask: high-dimensional data processing and learning Problem definition: selecting the optimal feature descriptors 2 branch of solutions: hand-crafted descriptors and deep-learning-based features. (Criticize the former feature extraction solutions and introduce proposed method:)\nFeatures derived from approaches of those 2 categories are too inflexible to contribute a robust model. This method \u0026ldquo;encodes and refines these? raw features from multiple sources to improve the classification performance\u0026rdquo;. For example: 4 extracted features (from AlexNet, ResNet, HMP, and SPF) are concatenated into 1 vector taken as the input to a \u0026ldquo;3-layer\u0026rdquo; model, where only a single \u0026ldquo;genearlized\u0026rdquo; hidden layer (latent space) bridges the raw feature space (transformation ax+b) and the final target space (residual error). (Recap deep learning models and mention the theory base of this work)\nDeep networks often get \u0026ldquo;trapped in local minimum and are sensitive to the learning rate\u0026rdquo; because their training fundation is BP. Regression-based feature learning. Least-squares representation learning methods. (Problems to be solved) Drawbacks of regression-based approaches:\n\u0026ldquo;block\u0026rdquo; models? don\u0026rsquo;t perform one-shot training philosophy based on the relation between raw data and the target. A model trained by some \u0026ldquo;designed\u0026rdquo; process has a inferious generalizatio n capacity than the model derived from one-shot training strategy (least-squares). Drawbacks of multilayer neural networks \u0026amp; solution Deeper layer-stacked models suffer from overfitting with limited training samples. Network-in-network structure enhances the network\u0026rsquo;s generalization capacity for learning feature. ELM with subnetwork nodes. Contributions: Subnetwork neural nodes (SNN) realized multilayer representation learning. Unlike the ensembled network, the SNN is trained based on the error term. Feature space transformation and the classification are solved together by searching iteratively the optimal encoding space (hidden layer). Concatenation of multiple features result more discriminative representations for samples. Ⅱ. Literature review A. Conventional Feature Coding \u0026quot; Supervised method of learning representaiton evaluates the importance of a specific feature through the correlation between features and categories.\u0026quot;\nConventional feature coding of images depends on prior knowledge of the problem. Thus, the features are not complete representations.\nThis paper enhances the feature by fusing (discriminative) hand-crafted features and (class-specific) CNN-based features.\nB. Least-Squares Encoding Methods The least-squares approximation methods, such as random forest and alternating minimization, have been exhaustively investigated in single-layer neural networks.\nRelated works: Moore-Penrose inverse; Universal approximation capacity of I-ELM, ELM autoe-ncoder14, Features combined with subnetwork nodes 18\nEach SNN is applied as a local feature descriptor. Hence, the subspace features can be extracted? from the original data independently, and the useful features are generated via the combination of these features.\nⅢ. Proposed Method A. Algorithmic Summary Two steps:\nPreprocessing: concatenate various feature vectors into a single \u0026ldquo;supervector\u0026rdquo;. Train the width-growth model: Terminology: layer name marker params in out input Entrance (feature) layer 𝑓 𝐖ᵢᶠ, 𝐛ᵢᶠ random vct linear combination 𝐇 hidden Refinement layer/subspace 𝑟 𝐖ᵢʳ, 𝐛ᵢʳ (𝐚,b) 𝐇 partial feature Ψ output Least square learning layr 𝑣 𝐖ᵢᵛ (𝛃) Ψ sum up all partial features: 𝚪 residual error 𝐞 (An entrance layer and a refinement layer both are \u0026ldquo;SNN\u0026rdquo;, and their combination is a \u0026ldquo;R-SNN\u0026rdquo;)\nInitialization: For the 1st R-SNN, 𝐖₁ᶠ, 𝐖₁ʳ are random generating a false feature Ψ. Then the first least-square method (pseudoinverse) is performed to calculate 𝐖₁ᵛ based on target 𝐘 and Ψ.\nIteratively add the R-SNN (2≤ i≤ L) (refinement subspace) into the hidden layer (optimal feature space)\nflowchart TB subgraph In[input feature] x1((1)) \u0026 x2((2)) \u0026 xe((\"⋮\")) \u0026 xn((n)) end EnW(\"Entrance layer\\n 𝐖ᵢᶠ, 𝐛ᵢᶠ\\n random\") subgraph H[\"entrance feature 𝐇\"] h1((1)) \u0026 h2((2)) \u0026 he((\"⋮\")) \u0026 hD((D)) end RefineW(\"Refinement layer\\n 𝐖ᵢʳ, 𝐛ᵢʳ\") subgraph Psi[partial feature Ψ] Ψ1((1)) \u0026 Ψ2((2)) \u0026 Ψe((\"⋮\")) \u0026 Ψd((d)) end OW(\"Output layer\\n 𝐖ᵢᵛ\") subgraph Out[\"Output vector\"] o1((1)) \u0026 o2((2)) \u0026 oe((\"⋮\")) \u0026 om((m)) end x1 \u0026 x2 \u0026 xe \u0026 xn --\u003e EnW --\u003e h1 \u0026 h2 \u0026 he \u0026 hD --\u003e RefineW --\u003e Ψ1 \u0026 Ψ2 \u0026 Ψe \u0026 Ψd --\u003e OW --\u003e o1 \u0026 o2 \u0026 oe \u0026 om Out --\u003e|\"- 𝐞ᵢ₋₁\"| erri[\"𝐞ᵢ\"] erri \u0026 OW -.-\u003e|pinv| newΨ(\"𝐏 \\n yielding\\n 𝐞ᵢ\") subgraph H1[\"entrace feature 𝐇ᵢ₊₁\"] h11((1)) \u0026 h12((2)) \u0026 h1e((\"⋮\")) \u0026 h1D((D)) end In --\u003e EnW1(\"Entrance layer\\n 𝐖ᵢ₊₁ᶠ, 𝐛ᵢ₊₁ᶠ\\n random\") --\u003e h11 \u0026 h12 \u0026 h1e \u0026 h1D H1 --\u003e RefineW1(\"Refinement layer\\n 𝐖ᵢ₊₁ʳ, 𝐛ᵢ₊₁ʳ\") %%-.-|solved by P| newΨ newΨ -.-\u003e RefineW1 subgraph Psi1[partial feature Ψ] Ψ11((1)) \u0026 Ψ12((2)) \u0026 Ψ1e((\"⋮\")) \u0026 Ψ1d((d)) end RefineW1 --\u003e Ψ11 \u0026 Ψ12 \u0026 Ψ1e \u0026 Ψ1d --\u003e OW1(\"Output layer\\n 𝐖ᵢ₊₁ᵛ\") %%OW1 -.-|solved by| erri erri -.-\u003e OW1 subgraph Out1[\"Output vector\"] o11((1)) \u0026 o12((2)) \u0026 o1e((\"⋮\")) \u0026 o1m((m)) end OW1 --\u003e o11 \u0026 o12 \u0026 o1e \u0026 o1m Out1 --\u003e|\"- 𝐞ᵢ\"| erri+1[\"𝐞ᵢ₊₁\"] --\u003e newP B. Model Definition SLFN solves the regression problem can be expressed as:\nMLNN has nested transformation:\nProposed method is a generlized SLFN:\nminimize J = ½ ‖𝐘-f(𝐇ᵢᶠ, 𝐖ᵢʳ, 𝐛ᵢʳ)⋅𝐖_Lᵛ‖²,\nf(𝐇ᵢᶠ, 𝐖ᵢʳ, 𝐛ᵢʳ) = ∑ᵢ₌₁ᴸ g(𝐇ᵢᶠ ⋅ 𝐖ᵢʳ + 𝐛ᵢʳ): sum all R-SNN 𝐇ᵢᶠ = g(𝐖ᵢᶠ, 𝐛ᵢᶠ, 𝐗) 𝐘 ∈ ℝᴺᕽᵐ: expected output, target feature 𝐗 ∈ ℝᴺᕽⁿ: input matrix L : number of R-SNN node g : activateion function 3 differences from other least-squares-based MLNNs\nSNN combines each dimension of the feature vector serving as local feature descriptor. While the R-SNN is the basic unit to refine feature vectors.\nOptimal feature is the aggregation of R-SNN added one by one. R-SNN is densly connected to input vector and output layer containing twice linear projection. Different R-SNNs are independent because they learn from different error.\nThe latent space is the aggregation of all R-SNN nodes subspace. So the parameters training has no block-wise communication between different spaces. That means the feature refinement and classification are doen together.\nC. Proposed Width-Growth Model Input weights and bias 𝐖ᵢᶠ, 𝐛ᵢᶠ: randomly initialized; Entrance feature: 𝐇ᵢᶠ = g(𝐗𝐖ᵢᶠ+ 𝐛ᵢᶠ); Refined partial feature: Ψᵢ=g(𝐇ᵢᶠ𝐖ᵢʳ+ 𝐛ᵢʳ), where 𝐛ᵢʳ is random; Output weights: 𝐖ᵢᵛ=(𝐈/C + 𝚪ᵀ𝚪)⁻¹𝚪ᵀ⋅𝐘, where C is hyperparameter for regularization, and (𝐈/C + 𝚪ᵀ𝚪)⁻¹𝚪ᵀ is the pseudoinverse of output vector 𝚪 (label?) Error: 𝐞ᵢ = 𝐘 - 𝐖ᵢᵛ\n𝐏 is the desired matrix generating 𝐞ᵢ by: 𝐏ᵢ⋅𝐖ᵢᵛ=𝐞ᵢ, so 𝐏ᵢ = 𝐞ᵢ⋅(I/C + (𝐖ᵢᵛ)ᵀ𝐖ᵢᵛ)⁻¹(𝐖ᵢᵛ)ᵀ\nRefinement layer weights of next R-SNN: 𝐖ᵢ₊₁ʳ = (I/C +𝐇ᵢᵀ𝐇ᵢ)⁻¹𝐇ᵢᵀ ⋅ g⁻¹(𝐏ᵢ), because g(𝐇ᵢ₊₁⋅𝐖ᵢ₊₁ʳ+ 𝐛ᵢ₊₁ʳ) = 𝐏ᵢ. Next partial feature: Ψᵢ₊₁ = g(𝐇ᵢ₊₁⋅𝐖ᵢ₊₁ʳ+ 𝐛ᵢ₊₁ʳ)\nAccumulate the partial feature to the optimal feature: 𝚪ᵢ₊₁ = 𝚪ᵢ + Ψᵢ₊₁ Update error: 𝐞ᵢ₊₁ = 𝐞ᵢ-𝐖ᵢᵛ𝚪ᵢ\nRepeat steps 4-6 L-2 times, and the final feature 𝚪$_L$ is the generalized feature correponding to the best output parameter 𝐖 $_Lᵛ$ for classification.\nRef L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, “Representational learning with extreme learning machine for big data,” IEEE Intell. Syst.,Dec. 2013. Y. Yang and Q. J. Wu, “Features combined from hundreds of midlayers: Hierarchical networks with subnetwork nodes,” IEEE Trans. Neural Netw. Learn. Syst., Nov. 2019. J. Tang, C. Deng, and G.-B. Huang, “Extreme learning machine for multilayer perceptron,” IEEE Trans. Neural Netw. Learn. Syst., Apr. 2016. ","date":"2023-02-15T12:40:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-snn-refine-weights/","title":"Read: Optim - SLFN | Width-Growth Model with SNN"},{"content":"Authors: Yimin Yang; Yaonan Wang; Xiaofang Yuan\nCode | TNNLS (2012-06-20)\nTry to summary: (2023-02-15) B-ELM is a variant of I-ELM by dividing the hidden nodes into 2 types: odd and even, which differ in wehther the input parameters (𝐚,b) is randomly generated or calculated by twice inverse operations.\nSpecifically, the first node is solved from the target 𝐭. Then for subsequent nodes, the even node is solved based on the residual error of its previous odd node.\nThe output weights of the odd nodes are calculated as: 𝐇₂ₙ₋₁ʳ + e₂ₙ₋₂ (or 𝐭) ➔ 𝛃₂ₙ₋₁ (➔e₂ₙ₋₁) The input weights 𝐚 and bias b of even nodes are solved based on the residual error: e₂ₙ₋₁ ➔ 𝐇₂ₙᵉ ➔ 𝐚,b (➔ ^𝐇₂ₙᵉ ➔ 𝛃₂ₙ ➔ e₂ₙ) Note: the superscript r and e stand for \u0026ldquo;random\u0026rdquo; and \u0026ldquo;error\u0026rdquo; marking the source of H. Abstract This algorithm tends to reduce network output error to 0\nby solving the input weights 𝐚 and bias b based on the network residual error. (In other words, the residual error is represented by a, b of subsequent nodes. Or the error is absorbed by others parameters besides 𝛃.)\nⅠ. Introduction For ELM with a fixed structure, the best number of hidden nodes need to ffound by trial-and-error, because the residual error is not always decreasing when there are more hidden nodes in an ELM.\nFor incremental ELM, the hidden node is added one by one, so the residual error keeps decreasing. But the model training has to do multiple iterations, i.e., calculating inverse matrix is needed after adding each node.\nCompared with other incremental ELM, this method is\nFaster and with fewer hidden nodes showing the relationship between the network output residual error 𝐞 and output weights 𝛃, which is named \u0026ldquo;error-output weights ellipse\u0026rdquo; The hidden layer (input weights) with determined parameters instead of random numbers would make the error reduce, or improve the accuracy. Ⅱ. Preliminaries and Notation A. Notations and Definitions ⟨u,v⟩ = ∫ₓu(𝐱)v‾(𝐱)d𝐱 is the Frobenius inner product of two matrices u,v, where the overline denotes the complex conjugate.\nB. I-ELM Lemma 1(proved by Huang⁸): indicated that (For the incremental ELM,) the target function can be approximated with more nodes added into the network by reducing the residual error to 0, as the 𝛃 of each new node is calculated based on the error of the network last status eₙ₋₁ as:\n𝛃 = $\\frac{⟨eₙ₋₁, 𝐇ₙʳ⟩}{‖𝐇ₙʳ‖²}$\nThe numerator is the inner product measuring the distance from the nth (random) hidden node 𝐇ₙʳ to be added and the error of the network with n-1 hidden nodes.\nSo the output of the newly added hidden nodes 𝐇ₙʳ are getting smaller and smaller, because they are learning something from the residual error.\nⅢ. Proposed Bidirectional ELM Method A. Structure of the Proposed Bidirectional ELM Method Two types of node, the node with odd index {2n-1} has random 𝐚,b, while the 𝐚,b of the node with even index {2n} are calculated based on the residual error of the network with an odd number of nodes at the last moment.\nTheir output weights both are calculated based on Lemma 1. The 𝐇 of the even node is from residual error, not from the random a,b.\nThe odd node aims to approximate the target through 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁, where 𝐇₂ₙ₋₁ is yield based on random a,b;\nThe odd node 2n-1 approximates the previous residual error with random generated a,b;\nBut the even node approximates the residual error 𝐞₂ₙ₋₁ through 𝐇₂ₙᵉβ₂ₙ with calculated a,b, where 𝐇₂ₙᵉ is yield with the weights a,b solved based on the residual error 𝐞₂ₙ₋₁ from the target (the job hasn\u0026rsquo;t done by all the previous nodes)\nBi-direction means the approximation is learned from both the target and the error, where the odd node (β₂ₙ₋₁) solved by the target, while the even node (β₂ₙ) calculated by the error. So a pair of odd node and even node is a complete step toward to the target.\nBi-directional means H₂ₙᵉ is calculated first from eq.(6); Then it is calculated again using the ^a,^b, which are solved based on H₂ₙᵉ, to get the updated ^H₂ₙᵉ, which is used to calculate the output weight for the next random odd node based on the Lemma 1.\nflowchart LR subgraph in[inputs] x1 \u0026 xe[\"⋮\"] \u0026 xn end rand((\"random\\n 𝐚,b\")) calculated((\"solved\\n 𝐚,b\")) subgraph hid[hidden] H1 \u0026 he[\"⋮\"] \u0026 h2n-1 \u0026 h2n end x1 \u0026 xe \u0026 xn --\u003e rand --\u003e h2n-1[\"𝐇₂ₙ₋₁ʳ\"] ---|Lemma 1| β2n-1((\"β₂ₙ₋₁\"))--\u003e e2n-1[\"e₂ₙ₋₁\"] x1 \u0026 xe \u0026 xn --\u003e calculated --\u003e h2n[\"𝐇₂ₙᵉ\"] ---|Lemma 1| β2n((\"β₂ₙ\"))--\u003e e2n[\"e₂ₙ\"] h2n -.- |\"⬅ inv of\\n β₂ₙ₋₁\"| e2n-1 calculated -.-|\"⬅ inv of\\n 𝐱\"| h2n The dot lines represent the inverse calculation. β₂ₙ₋₁ is derived from the network residual error of the last status.\nBlock diagram:\nflowchart TB init[Initialization: \\n Given training set,\\n expect accracy η and \\n let #hidden nodes L=0] --\u003e incre[L = L+1] --\u003e OdEv{\"Is L \\n odd or even?\"} OdEv --\u003e|L=2n+1| I-ELM --\u003e calcE[Calculate \\n residual error E] OdEv --\u003e|L=2n| Theorem2 --\u003e update[\"Update ^H_L = ^𝐚_L 𝐱 + ^b)\\n and calculate the output weight \\n β_L based on eq.(7)\"] --\u003e calcE subgraph Theorem2 direction TB calcH[\"Calculate output matrix \\n H_L basd on eq.(6)\"] --\u003e calcab[\"Calculate hidden-node parameters \\n (^𝐚_L,^b_L) based on eq.(14)\"] end subgraph I-ELM direction TB rand[\"Randomly assign hidden-node\\n parameters (𝐚_L,b_L) \\n and obtain\\n output matrix H_L\"] --\u003e |Lemma 1| outW[\"Calculate the output weight \\n βL according to eq.(8)\"] end calcE --\u003e thres{\"‖E‖\u003cη\"} --\u003e |Yes| END %%thres ----\u003e|No| incre %% mess up the chart incre o---o|No| thres B. Bidirectional ELM method Theorem 1 states the target function 𝑓 is approximated by the existing network 𝑓₂ₙ₋₂ plus the last two nodes: 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁ and 𝐇₂ₙᵉβ₂ₙ, when n→∞. On the other hand, the residual error the network is 0:\n$lim\\_{n→∞}‖𝑓-(𝑓₂ₙ₋₂ + 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁ + 𝐇₂ₙᵉβ₂ₙ)‖ = 0$\nwhere the sequence of the 𝐇₂ₙᵉ (the output of the even node calculated from the feedback error) is determined by the inverse of last output weight β₂ₙ₋₁:\n$𝐇₂ₙᵉ = e₂ₙ₋₁ ⋅(β₂ₙ₋₁)⁻¹$ (6)\nThat means $𝐇₂ₙᵉ ⋅ β₂ₙ₋₁ = e₂ₙ₋₁$, this even node is approaching the last residual error based on the known output weight β₂ₙ₋₁.\nThen its output weight can be calculated based on the Lemma 1 as:\n$β₂ₙ = \\frac{⟨e₂ₙ₋₁, 𝐇₂ₙᵉ⟩}{‖𝐇₂ₙᵉ‖²}$ (7)\nOnce this even node is determined, the corresponding residual error e₂ₙcan be generated, and also the output weight of the next odd node (with 𝐇₂ₙ₊₁ʳ from random weights) can be calculated based on Lemma 1:\n$β₂ₙ₊₁ = \\frac{⟨e₂ₙ, 𝐇₂ₙ₊₁ʳ⟩}{‖𝐇₂ₙ₊₁ʳ‖²}$ (8)\nTheorem 2 further states the updated ^𝐇₂ₙᵉ calculated with the optimal ^a, ^b solved based on 𝐇₂ₙᵉ by the least-square (pseudo inverse) can also let the residual error converge to 0.\nRemark 1 clarifies the differences between this method and I-ELM, that is the input weights and bias of even nodes are calculated, not randomly generated. And the output weights are set similarly based on Lemma 1.\nBased on the eq. 6, the Δ = ‖e₂ₙ₋₁‖² + ‖e₂ₙ‖² can be reformalized to an ellipse curve.\nCode Training needs to store parameters for each node. And testing needs to query each node sequentially.\nReference G. B. Huang, L. Chen, and C. K. Siew, “Universal approximation using incremental constructive feedforward networks with random hidden nodes,” IEEE Trans. Neural Netw., Jul.2006. ","date":"2023-02-10T19:29:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-elm-bidirec/","title":"Read: Optim - SLFN | B-ELM"},{"content":"(Featured image credits: Least Squares Regression - Math Is Fun Found by Google Lens )\nPseudo Inverse Source video: 深度学习-啃花书0103伪逆矩阵最小二乘 (2021-07-22) - 科研小灰灰\n(2023-02-04)\nSolve linear regression For a n-dimensional linear regression problem,\nThere are N input samples: x₁,x₂,\u0026hellip;,$x_N$, where each sample is a n-dimension vector xᵢ∈ ℝⁿ Their corresponding target outputs are: y₁,y₂,\u0026hellip;,$y_N$, where each output is a scalar yᵢ∈ ℝ Hence, these data can be represented as a linear system:\n$$ \\begin{cases} x₁₁a₁ + x₁₂a₂ + ... + x₁ₙaₙ = y₁ \\\\\\ x₂₁a₁ + x₂₂a₂ + ... + x₂ₙaₙ = y₁ \\\\\\ \\vdots \\\\\\ x_{N1}a₁ + x_{N2}a₂ + ... + x_{Nn}aₙ = y_N \\\\\\ \\end{cases} $$Further, this system can be represented with matrix and vector:\n$$ \\begin{bmatrix} x₁₁ \u0026 x₁₂ \u0026 … \u0026 x₁ₙ \\\\\\ x₂₁ \u0026 x₂₂ \u0026 … \u0026 x₂ₙ \\\\\\ ⋮ \u0026 ⋮ \u0026 ⋱ \u0026 ⋮ \\\\\\ x_{N1} \u0026 x_{N2} \u0026 … \u0026 x_{Nn} \\end{bmatrix} \\begin{bmatrix} a₁ \\\\\\ a₂ \\\\\\ ⋮ \\\\\\ aₙ \\end{bmatrix} = \\begin{bmatrix} y₁ \\\\\\ y₂ \\\\\\ ⋮ \\\\\\ y_N \\end{bmatrix} $$The objective of the linear regression is to solve the weights vector: $\\[ ^{^{a₁}\\_{a₂}} \\_{^{\\ ⁞}\\_{aₙ}} \\]$ from the linear equation: $𝐗_{N×n} 𝐚_{n×1} = 𝐘_{N×1}$.\nIf N=n (the coefficient matrix is a square matrix), and the data matrix $𝐗_{N×n}$ is a invertible matrix, then there will be 𝐚=𝐗⁻¹𝐘, such that the weights vector is determined directly.\nBut in general, the number of samples N is not equal to the number of features n (N ≠ n), that is 𝐗 is not invertible and 𝐚 cannot be represented as 𝐗⁻¹𝐘.\nShift from 𝐗 to 𝐗ᵀ𝐗 Therefore, when the 𝐚 cannot be reached directly, the solution should be as close to the optimal as possible.\nThat means the objective is to minimize the distance of two vectors:\nJ=‖𝐗𝐚-𝐘‖² (without constraints)\nAnd the optimal solution is obtained when\n∂J/∂𝐚 = 𝐗ᵀ(𝐗𝐚-𝐘) = 0 ⇒ 𝐗ᵀ𝐗𝐚 = 𝐗ᵀ𝐘.\nNow, the previous 𝐗 is shifted to here 𝐗ᵀ𝐗 ∈ ℝⁿᕁⁿ, which is a square matrix. And if 𝐗ᵀ𝐗 is invertible, then the optimal 𝐚 can be calculated in one-shot.\nIs 𝐗ᵀ𝐗 invertible? An invertible matrix has to satisfy 2 conditions: it\u0026rsquo;s a square matrix and its rank equals to the number of variables n (#columns).\nAccording to this video, there are two cases:\nIf N \u0026gt; n, for example N=5, n=3, then (𝐗ᵀ𝐗)₃ₓ₃ is inverible generally. So\n𝐚=(𝐗ᵀ𝐗)⁻¹𝐗ᵀ𝐘,\nwhere the coefficient in front of 𝐘, (𝐗ᵀ𝐗)⁻¹𝐗ᵀ, is called the pseudo-inverse matrix. And 𝐚 = (𝐗ᵀ𝐗)⁻¹𝐗ᵀ𝐘 is called the least-square solution. (最小二乘解)\n(Because 𝐗 has no inverse matrix, so we find its \u0026ldquo;pseudo\u0026rdquo; inverse matrix. Or if 𝐗 is invertible, 𝐗⁻¹=(𝐗ᵀ𝐗)⁻¹𝐗ᵀ, they\u0026rsquo;re equivalent, but the latter suits more general scenarios.). If $N \u003c n$, for example N=3, n=5, then (𝐗ᵀ𝐗)₅ₓ₅ is not invertible, because:\nrank(𝐗ᵀ𝐗) ≤ rank(𝐗₃ₓ₅) ≤ N=3 $\u003c$ n=5.\nIn this case, 𝐚 cannot be calculated as (𝐗ᵀ𝐗)⁻¹𝐗ᵀ𝐘.\nThe problem can be understood that there are too many parameters (n is too high). When the parameters are much more than samples, there will be overfitting. And one of the solutions is regularization.\nEffect of the regularization term Since the reason why the optimal solution of the loss function J cannot be solved in one-shot is that there are too many parameters, a regularization is added to the loss funciton as follows:\n$$J = ‖𝐗𝐚-𝐘‖² + λ‖𝐚‖², λ\u003e0$$Then the derivative becomes:\n∂J/∂𝐚 = 𝐗ᵀ𝐗𝐚 - 𝐗ᵀ𝐘 + λ𝐚 = 0.\nBy moving items, the equation becomes:\n(𝐗ᵀ𝐗 + λ𝐈)𝐚 = 𝐗ᵀ𝐘,\nwhere the (𝐗ᵀ𝐗 + λ𝐈) is invertible. The proof is as follows.\nProof Since 𝐗ᵀ𝐗 is a symmetric matrix, it can be diagonalized. Thus, 𝐗ᵀ𝐗 can be written as:\n$$ 𝐗ᵀ𝐗 = 𝐏⁻¹ \\begin{bmatrix} λ₁ \u0026 \u0026 \\\\\\ \u0026 ⋱ \u0026 \\\\\\ \u0026 \u0026 λₙ \\end{bmatrix} 𝐏 $$The determinant of 𝐗ᵀ𝐗 is: |𝐗ᵀ𝐗| = |𝐏⁻¹| ⋅ |$^{^{λ₁}\\_{\\quad ⋱}} \\_{\\qquad λₙ}$| ⋅ |𝐏| = λ₁ ⋅ λ₂ … ⋅ λₙ\nAnd λ₁, λ₂ …, λₙ are the eigen values for the 𝐗ᵀ𝐗\nThen the invertibility can be judged from this determinant:\nIf |𝐗ᵀ𝐗| = 0, then 𝐗ᵀ𝐗 is not invertible, because there are some zero lines in the matrix (after elementary row operations), that means rank(𝐗ᵀ𝐗) \u0026lt; n.\nBut if |𝐗ᵀ𝐗| \u0026gt; 0, the matrix 𝐗ᵀ𝐗 is invertible, because it has full rank, which is equal to the number of lines of rows. 【俗说矩阵】行列式等于0意味着什么？你一定要了解哦~ - 晓之车高山老师-bilibili\nAnalyze the two cases without and with adding the regularization term:\nOnly the 𝐗ᵀ𝐗:\nLet this matrix be enclosed by a non-zero real row vector 𝛂ᵀ and its column vector 𝛂 to constuct a quadratic form: 𝛂ᵀ(𝐗ᵀ𝐗)𝛂, which is used to characterize the definiteness of 𝐗ᵀ𝐗. Definite matrix -wiki\nBased on the combination law of the matrix multiplication, it can be written as: (𝛂ᵀ𝐗ᵀ)(𝐗𝛂) = (𝐗𝛂)ᵀ(𝐗𝛂), which is the norm of the vector 𝐗𝛂. Because the norm is ≥ 0 definitely, the 𝛂ᵀ(𝐗ᵀ𝐗)𝛂 ≥ 0 (indicating 𝐗ᵀ𝐗 is positive semi-definite matrix).\nBased on the properties of quadratic form, eigen values λᵢ of 𝐗ᵀ𝐗 are all ≥ 0\nThen the above determinant is |𝐗ᵀ𝐗|= λ₁ ⋅ λ₂ … ⋅ λₙ ≥ 0, so when |𝐗ᵀ𝐗| = 0, the rank of the matrix 𝐗ᵀ𝐗 is not full (≠ n), so the matrix 𝐗ᵀ𝐗 is not invertible.\nFor 𝐗ᵀ𝐗+λ𝐈 (where λ is a hyper-parameter), it can be considered as a matrix:\nA quadratic form is constructed as: 𝛂ᵀ(𝐗ᵀ𝐗+λ𝐈)𝛂 = (𝛂ᵀ𝐗ᵀ)(𝐗𝛂) + λ𝛂ᵀ𝛂. The second item is always \u0026gt;0 because 𝛂 is not a zero vector, so its norm 𝛂ᵀ𝛂 \u0026gt; 0. Therefore, the matrix (𝐗ᵀ𝐗+λ𝐈) is a positive definite matrix. The eigen values λᵢ of (𝐗ᵀ𝐗+λ𝐈) are all \u0026gt; 0. The determinant of 𝐗ᵀ𝐗+λ𝐈 is the product of its eigen values, i.e., the determinant |𝐗ᵀ𝐗+λ𝐈|\u0026gt;0. So the matrix 𝐗ᵀ𝐗+λ𝐈 has full rank, and 𝐗ᵀ𝐗+λ𝐈 is invertible.\nTherefore, the optimal solution can be solved as 𝐚 = (𝐗ᵀ𝐗 + λ𝐈)⁻¹ 𝐗ᵀ𝐘. This is called the \u0026ldquo;least square - least norm solution\u0026rdquo;. (最小二乘-最小范数解)\nL2 regularization is originally added to make the 𝐗ᵀ𝐗 invertible.\n(bilibili search: \u0026ldquo;伪逆矩阵\u0026rdquo;)\ntodo: 01 2、最小二乘与pca（新） - 深度之眼官方账号-bilibili\ntodo: 【熟肉】线性代数的本质 - 06 - 逆矩阵、列空间与零空间 - 3B1B -bili\ntodo: 【26 深入理解逆矩阵】- cf98982002 -bili\nGPU Solve Inverse Use GPU to solve inverse faster?\n(DDG search: \u0026ldquo;矩阵求逆 gpu\u0026rdquo;)\nThe acceleration ratio of GPU to CPU is more than 16 times. ⁴. GPU矩阵计算是否会更快？（基于Pytorch） - 半个冯博士的文章 - 知乎 How to solve inverse matrix? 求逆矩阵的4种方法? - 疯狂绅士的回答 - 知乎 Gaussian Eliminate LU decomposition, commenly used by computer because it can be performed parallelly. SVD decomposition QR decomposition Estimate Polynomial by LS (2024-06-10)\nLeast-square can be used to approximate the coefficients of a polynomial.\nThe optimal polynomial corresponds to the minimum error between the observed and predicted values.\nExample\nGiven a point set as below, use a linear polynomial to fit the data:\n1 2 x = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] y = [0, 4, 5, 14, 15, 14.5, 14, 12, 10, 5, 4] The general form of a linear polynomial is: ax + b*1 = y, a and b are the coefficients to be estimated:\n$$ \\begin{bmatrix} 0 \u0026 1 \\\\\\ 0.1 \u0026 1 \\\\\\ 0.2 \u0026 1 \\\\\\ 0.3 \u0026 1 \\\\\\ ⋯ \\end{bmatrix} \\begin{bmatrix} a \\\\\\ b \\end{bmatrix} = \\begin{bmatrix} 0 \\\\\\ 4 \\\\\\ 5 \\\\\\ 14 \\\\\\ ⋯ \\end{bmatrix} $$$$XA = Y$$ Obviously, the 2 axes (column space) of X cannot be \u0026ldquo;recombined\u0026rdquo; to produce Y space. In other words, the $[^a_b]$ that makes the equation hold doesn\u0026rsquo;t exist.\nTherefore, the objective is shifted to minimize the error between Y_pred and target Y.\nProject the target Y to the column space of X resulting in Y\u0026rsquo; (that will be approximated by A\u0026rsquo;), and the error: Y\u0026rsquo;-Y is orthogonal to the column space of X, as the error can\u0026rsquo;t be explained by the X\u0026rsquo;s space (Don\u0026rsquo;t know a theoretical explanation yet).\nSince Y\u0026rsquo;-Y is orthogonal to X, there is:\n$$\\begin{aligned} X^T(Y'-Y)=0 \\\\\\ X^T(XA'-Y) = 0 \\end{aligned}$$The best coefficients A\u0026rsquo; is:\n$$A'=(X^T X)^{-1} X^TY$$ Use a quadratic polynomial $\\rm ax²+bx+c=y$ to fit the data set:\n$$ \\begin{bmatrix} 0 \u0026 0 \u0026 1 \\\\\\ 0.01 \u0026 0.1 \u0026 1 \\\\\\ 0.04 \u0026 0.2 \u0026 1 \\\\\\ 0.09 \u0026 0.3 \u0026 1 \\\\\\ ⋯ \\end{bmatrix} \\begin{bmatrix} a \\\\\\ b \\\\\\ c \\end{bmatrix} = \\begin{bmatrix} 0 \\\\\\ 4 \\\\\\ 5 \\\\\\ 14 \\\\\\ ⋯ \\end{bmatrix} $$ x² (e.g., 0.01), x (e.g., 0.1), and 1 are the 3 basis functions that are combined with various ratios to form multiple basiss. On the other hand, the optimal coefficients of a polynomial can also be found by solving the equation that the partial derivative of the error w.r.t. each coefficient equals 0.\nDerivation referring to: An As-Short-As-Possible Introduction to the Least Squares, Weighted Least Squares and Moving Least Squares Methods for Scattered Data Approximation and Interpolation (may 2004) - Andy Nealen\nUse a quadratic bivariate polynomial to fit a point set (𝐱ᵢ,yᵢ)\n$$ \\\\|Y' - Y\\\\|^2 $$$$ α = \\left[ ∑_{i=1}^N \\big( b(xᵢ)⋅ b(xᵢ)^T \\big) \\right]^{-1} ⋅ ∑_{i=1}^N \\big( b(xᵢ) ⋅ yᵢ \\big) $$ b means a basis (a coordinate system), i.e., a series (combination) of basis functions. Weighted Least Squares (2024-06-11)\nThe value at each new point is predicted based on an approximated polynomial, which is found by minimizing the difference between the training point and predicted values, and the differences are weighted according to the distance from the new point to each training point.\n(2024-06-18)\nExample: Given 7 points: (0, 2.5), (1, 6.5), (2, 18.5), (3, 32.5), (4, 48), (5, 70.5), (6, 90.5), (data is made up from 2x²+4x+2.5), use WLS to estimate the value at x=0.5.\nLS is featured by its one-shot solution, but I\u0026rsquo;m not sure what\u0026rsquo;s the algorithm in np.polyfit. Just illustrating the general 2 steps below: Approximate a polynomial for only first 4 points:\nCode generated by chatGPT4o - Diagrams \u0026 Data Ask: Use a quadratic function to fit the sequence: f(0) =2.5, f(1) = 6.5, f(2) = 18.5, f(3) = 32.5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import numpy as np import matplotlib.pyplot as plt from numpy.polynomial.polynomial import Polynomial # Given data points x_data = np.array([0, 1, 2, 3]) y_data = np.array([2.5, 6.5, 18.5, 32.5]) # Fit a quadratic polynomial (degree 2) coefs = np.polyfit(x_data, y_data, 2) # Create the polynomial function from the coefficients p = np.poly1d(coefs) # Generate x values for plotting the fitted curve x_fit = np.linspace(-1, 4, 400) y_fit = p(x_fit) # Plot the original data points plt.scatter(x_data, y_data, color=\u0026#39;red\u0026#39;, label=\u0026#39;Data points\u0026#39;) # Plot the fitted quadratic function plt.plot(x_fit, y_fit, label=f\u0026#39;Fitted quadratic function:\\ $f(x) = {coefs[0]:.2f}x^2 + {coefs[1]:.2f}x + {coefs[2]:.2f}$\u0026#39;) plt.title(\u0026#39;Quadratic Fit to 4 Neighboring Points\u0026#39;) plt.xlabel(\u0026#39;$x$\u0026#39;) plt.ylabel(\u0026#39;$f(x)$\u0026#39;) plt.legend() plt.grid(True) # Plot other points plt.scatter(0.5, p(0.5), color=\u0026#39;g\u0026#39;, label=\u0026#34;New point\u0026#34;) plt.scatter([4,5,6], [48, 70.5, 90.5], color=\u0026#39;r\u0026#39;) plt.legend() plt.show() Use the estimated polynoimal to infer the value at 0.5:\n(2024-05-31)\nConsidering some samples (errors) are more important, and the importance is determined by the sample\u0026rsquo;s neighbors, the loss function becomes: (移动最小二乘法在点云平滑和重采样中的应用-博客园-楷哥)\n$$J_{LS} = \\sum_{i=1}^N wᵢ (\\\\^yᵢ - yᵢ)^2$$ Assume the target function has a \u0026ldquo;Compact Support\u0026rdquo;. Thus, each sample is only affected by its neighbors within the local support.\nUsing the variance around a point as the weight.\nBy only considering a narrow region, each sample has a variance $σᵢ²$. And the weights can be the reciprocal of the variance $\\frac{1}{σᵢ²}$:\nIn other words, WLS takes the correlation of adjacent errors into account. Wikipedia\nOther reasons analysis: Lecture 24-25: Weighted and Generalized Least Squares - CMU (Found in DDG)\nExamples of fitting circle, line, curves with IRLS: 最小二乘法，加权最小二乘法，迭代重加权最小二乘法 (含代码)【最小二乘线性求解】- CSDN\nMLS for Resampling (2024-06-18)\nMLS performs WLS repeatedly for all 𝑥 of the function to be approximated, whereas WLS is used to compute the optimal coefficients.\nMLS is proposed for fitting surface from discrete data in Surfaces Generated by Moving Least Squares Methods - AMS.\n(2024-06-10)\nThe value of a new (querying) point is computed based on its neighboring points in the input (training) point set.\nSteps of MLS: (Ref example)\nGiven a point set: (x₁,y₁), (x₂,y₂), (x₃,y₃), \u0026hellip;($x_N,y_N$);\nPredict the value y\u0026rsquo; of a new point x\u0026rsquo; based on the approximated polynomial, which is found by minimizing the error between the y_pred (evaluated by the approximated polynomial) and the true y values for N points. This minimization process can be performed with a one-shot expression, i.e., pseudo inverse in least squares.\nThe error of each point is scaled by a weight:\n$$\\sum_{i=1}^N w_i (y_i - y')^2$$where the weight is determined based on the distance from the new point to the training input point:\n$$ w = \\begin{cases} 0, \u0026 \\text{dist\u003c0} \\\\\\ \\frac{2}{3} - 4 dist^2 + 4 dist^3, \u0026 \\text{0≤dist≤0.5} \\\\\\ \\frac{4}{3} - 4 dist + 4 dist^2 - \\frac{4}{3} dist^3, \u0026 \\text{0.5","date":"2023-02-04T12:23:00-05:00","image":"https://www.mathsisfun.com/data/images/least-squares2.svg","permalink":"http://blog.zichen.uk/post/writenotes/calc/pseudo_inverse/","title":"Memo: Calc - Data | Pseudo-Inverse"},{"content":"Source video: 【俗说矩阵】矩阵乘法不神秘，揭开面纱很容易！\nGiven two linear systems:\n$$ \\begin{array}{cc} \\begin{cases} x₁ + 2x₂ = y₁ \\\\\\ 3x₁ + 4x₂ = y₂ \\end{cases} \u0026 \\begin{cases} 5y₁ + 6y₂ = 23 \\\\\\ 7y₁ + 8y₂ = 31 \\end{cases} \\end{array} $$Unlike the previous example, the constant terms of the left system is not fixed, but variables y₁, y₂. And the variables in the right system are y₁, y₂.\nNow, How to solve x₁, x₂ based on these 2 systems (compound equation system)?\nThe first step is to eliminate the intermediate variables y₁, y₂. One of methods is substituting the y₁, y₂ into the left system, so as to get a compound matrix:\n$$ \\begin{cases} 5(x₁ + 2x₂) + 6(3x₁ + 4x₂) = 23 \\\\\\ 7(x₁ + 2x₂) + 8(3x₁ + 4x₂) = 31 \\end{cases} ⇒ \\begin{cases} 23x₁ + 34x₂ = 23 \\\\\\ 31x₁ + 46x₂ = 31 \\end{cases} $$The above three system of equations can be represented with matrices and vectors:\n$$ \\begin{array}{ccc} \\begin{array}{c} \\\\{^{x₁ + 2x₂ = y₁} \\_{3x₁ + 4x₂ = y₂} \\\\\\ ⇓\\\\\\ [^{1\\ 2}\\_{3\\ 4}] [^{x₁}\\_{x₂}] = [^{y₁}\\_{y₂}] \\\\\\ ⇓\\\\\\ f([^{x₁}\\_{x₂}]) = [^{y₁}\\_{y₂}] \\end{array} \u0026 \\begin{array}{c} \\\\{^{5y₁ + 6y₂ = 23} \\_{7y₁ + 8y₂ = 31} \\\\\\ ⇓\\\\\\ [^{5\\ 6}\\_{7\\ 8}] [^{y₁}\\_{y₂}] = [^{23}\\_{31}] \\\\\\ ⇓\\\\\\ g([^{y₁}\\_{y₂}]) = [^{23}\\_{31}] \\end{array} \u0026 \\begin{array}{c} \\\\{^{23x₁ + 34x₂ = 23} \\_{31x₁ + 46x₂ = 31}\\\\\\ ⇓\\\\\\ [^{23\\ 34}\\_{31\\ 46}] [^{x₁}\\_{x₂}] = [^{23}\\_{31}] \\\\\\ ⇓\\\\\\ g(f([^{x₁}\\_{x₂}])) = g∘f([^{x₁}\\_{x₂}]) = [^{23}\\_{31}] \\end{array} \\end{array} $$In fact,the coefficient matrix maps the unknowns vector to the outcome vector. And the above 3rd system is a compound mapping from $\\[^{x₁}\\_{x₂}\\]$ to $\\[^{23}\\_{31}\\]$\nTherefore, each coefficient matrix is a mapping function: $[^{1\\ 2}\\_{3\\ 4}] ⇔ f$; $[^{5\\ 6}\\_{7\\ 8}]⇔ g$; and $[^{23\\ 34}\\_{31\\ 46}] ⇔ g∘f$\nCompound mapping matrix The relation between the compound mapping and its component mapping matrix.\nThe compound mapping matrix comes from \u0026ldquo;merging\u0026rdquo; two system through multiplication and add as follow:\n$$g∘f ⇔ [^{23\\ 34}\\_{31\\ 46}] = [^{5×1+6×3 \\quad 5×2+6×4} \\_{7×1+8×3 \\quad 7×2+8×4} ] = [^{5\\ 6}\\_{7\\ 8}]∘[^{1\\ 2}\\_{3\\ 4}] $$Therefore, this kind of compound operation rules between 2 matrices can be defined as matrix multiplication.\nFor the sake of convenience, the ring operator is omitted, then the matrix multiplication is written as: $[^{5\\ 6}\\_{7\\ 8}] [^{1\\ 2}\\_{3\\ 4}]$. Obviously, the result of 2-matrices multiplication is still a matrix.\nLaw of matrix multiplication $$ \\begin{array}{c} \\_{} \\\\\\ ^{r\\_{L1}} \\_{r\\_{L2}} [^{5\\ 6} \\_{7\\ 8} ] \\end{array} \\begin{array}{c} \\_{c_{L1}}\\ \\_{c_{L2}} \\\\\\ [^{1\\ 2} \\_{3\\ 4} ] \\end{array} \\begin{array}{c} \\_{}\\\\\\ = \\end{array} \\begin{array}{c} \\_{} \\\\\\ [^{5×1+6×3 \\quad 5×2+6×4} \\_{7×1+8×3 \\quad 7×2+8×4} ] \\end{array} $$r denotes row; c denotes column; L means left matrix; R means the right matrix. And use $[^{a₁₁\\ a₁₂} \\_{a₂₁\\ a₂₂} ]$ to represent the 4 elements in the outcome matrix.\na₁₁ is the sum of the products of the corresponding elements in the first row in the left matrix and the first column in the right matrix, i.e., the inner product of two vectors: $a₁₁ = r_{L1}⋅c_{R1} = (5,6)⋅(1,3)$\nSimilarly,\n$a₁₂ = r_{L1}⋅c_{R2} = (5,6)⋅(2,4)$; $a₂₁ = r_{L2}⋅c_{R1} = (7,8)⋅(1,3)$; $a₂₂ = r_{L2}⋅c_{R2} = (7,8)⋅(2,4)$; Rule: Take a row vector from the left matrix, take a column vector from the right matrix, and calculate the inner product of them, then the element position is determined based on the index of row and column.\n04:18/08:58\n","date":"2023-02-03T11:00:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/06_%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/","title":"watch: LA - 高山 06 | Matrix multiplication"},{"content":"Source video: 【俗说矩阵】用矩阵求非齐次线性方程组轻轻松松，几步就能搞定！\n3 cases of solutions There are 3 cases of solutions for a non-homogeneous linear system:\nA single unique solution Infinitely many solutions No solution Augmented matrix When performing Gaussian elimination, the equations results will also change. Hence, the coefficient matrix and the \u0026ldquo;results\u0026rdquo; column(s) are composed together forming the augmented matrix. Such that the \u0026ldquo;two matrices\u0026rdquo; will perform the same elementary row operations.\n$$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 5 \\\\\\ x₁ + 6x₂ + 7x₃ = 9 \\\\\\ x₁ + 10x₂ + 6x₃ = 8 \\\\\\ \\end{cases} $$$$ \\begin{array}{cc} A = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\\\ 1 \u0026 6 \u0026 7 \\\\\\ 1 \u0026 10 \u0026 6 \\end{bmatrix} \u0026 A|b = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \u0026 | 5 \\\\\\ 1 \u0026 6 \u0026 7 \u0026 | 9 \\\\\\ 1 \u0026 10 \u0026 6 \u0026 | 8 \\end{bmatrix} \\end{array} $$The augmented matrix can be leveraged to determine the case of the solution.\nSingle unique solution Performing elementary row operations on the augmented matrix:\nEliminate the unknown x₁ in eq. 2 and eq. 3 by performing -r1 + r2 and -r1+r3: $$ \\begin{bmatrix} 1 \u0026 2 \u0026 3 \u0026 | 5 \\\\\\ 0 \u0026 4 \u0026 4 \u0026 | 4 \\\\\\ 0 \u0026 8 \u0026 3 \u0026 | 3 \\end{bmatrix} $$ Cancel the common factor 4 for the 2nd line by multiplying the 2nd line with 1/4. $$ \\begin{bmatrix} 1 \u0026 2 \u0026 3 \u0026 | 5 \\\\\\ 0 \u0026 1 \u0026 1 \u0026 | 1 \\\\\\ 0 \u0026 8 \u0026 3 \u0026 | 3 \\end{bmatrix} $$ Eliminate the unknown x₂ in eq. 3 by performing -8r2 + r3 : $$ \\begin{bmatrix} 1 \u0026 2 \u0026 3 \u0026 | 5 \\\\\\ 0 \u0026 1 \u0026 1 \u0026 | 1 \\\\\\ 0 \u0026 0 \u0026 -5 \u0026 | -5 \\end{bmatrix} $$ After that, the augmented matrix becomes row echelon form.\nNext, the unknowns in the system can be solved from bottom to top by substituting x₃ into eq. 2, and then substituting x₂, x₃ to eq. 1. $$ \\begin{cases} x₁+\u0026 2x₂ +\u0026 3x₃ = 5 \\\\\\ \u0026 x₂ +\u0026 x₃ = 1 \\\\\\ \u0026 \u0026 x₃ = 1 \\\\\\ \\end{cases} $$Therefore, the unique solution for this non-homogeneous linear system is: 𝐱 = $[^{\\_{x₁}}\\_{^{x₂}\\_{x₃}} ] = [^{\\_{2}}\\_{^{0}\\_{1}} ]$\nAlso, by looking at the coefficient matrix and the augmented matrix after elementary row operations, the number of their non-zeros lines are both 3, which equals to the number of unknowns.\nThus, the conclusion is that if the augmented matrix has the same number of non-zeros lines of the coefficient matrix, where they both are in row echelon form, but also the number of non-zeros lines equals to the number of unknowns, then this non-homogeneous linear system has a single unique solution.\nInfinitely many solutions $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 5 \\\\\\ x₁ + 3x₂ + 4x₃ = 6 \\\\\\ x₁ + 4x₂ + 5x₃ = 7 \\\\\\ \\end{cases} $$ Performing elementary row operations on the augmented matrix as follows:\nEliminate x₁ of row2 and row3 by -r1+r2 and -r1+r3 : $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{1\\ 3\\ 4\\ |6}\\_{1\\ 4\\ 5\\ |7}}]$ ➔ $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 2\\ 2\\ |2}}]$\nCancel the commen factor of row3 by multiplying with 1/2 : $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 2\\ 2\\ |2}}]$ ➔ $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 1\\ 1\\ |1}}]$\nEliminate the unknowns x₂, x₃ in row3 by performing -r2+r3: $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 1\\ 1\\ |1}}]$ ➔ $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 0\\ 0\\ |0}}]$\nThen the equations become: $$ \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ = 5 \\\\\\ \u0026 x₂ +\u0026 x₃ = 1 \\\\\\ \u0026 \u0026 0 = 0 \\\\\\ \\end{cases} $$By representing x₁, x₂ with x₃, the solution vector becomes: 𝐱 = $[^{\\_{3-x₃}}\\_{^{1-x₃}\\_{x₃}} ]$\nThat means x₃ can be any value, and x₁ and x₂ can be any value too. Therefore, this non-homogeneous linear system has infinitely many solutions.\nHere, the number of non-zero lines in coefficient matrix and augmented matrix is less than the number of unknowns: r(𝐀|𝐛) = r(𝐀) = 2 \u0026lt; 3\nSo the conclusion is if the augmented matrix and the coefficient matrix in row echelon form have the same number of non-zero rows, which is less than #unknowns, then the non-homogeneous linear system has infinitely many solutions.\nNo solutions $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 5 \\\\\\ x₁ + 3x₂ + 4x₃ = 6 \\\\\\ x₁ + 4x₂ + 5x₃ = 9 \\\\\\ \\end{cases} $$$[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{1\\ 3\\ 4\\ |6}\\_{1\\ 4\\ 5\\ |9}}]$ $^{-r1+r2}\\_{-r1+r3}$ ➔ $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 2\\ 2\\ |4}}]$ ½×r3 ➔ $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 1\\ 1\\ |2}}]$ -r2+r3 ➔ $[^{\\_{1\\ 2\\ 3\\ |5}} \\_{^{0\\ 1\\ 1\\ |1}\\_{0\\ 0\\ 0\\ |1}}]$\nThe corresponding equations are: $$ \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ = 5 \\\\\\ \u0026 x₂ +\u0026 4x₃ = 6 \\\\\\ \u0026 \u0026 0 = 1 \\\\\\ \\end{cases} $$The 3rd equation is a fault. That means no matther what values the unknowns are, this linear equation system cannot be satisfied. Hence, there is no solution.\nIn this situation, the number of non-zero rows in the coefficient matrix is not equal to the augmented matrix: r(𝐀|𝐛) = 3 ≠ r(𝐀) = 2.\nThe conclusion is if the number of non-zero lines in the augmented matrix mismatch with the coefficient matrix in row echelon form, then the non-homogeneous linear equation system has no solution.\nGeneral solution of infinitely many solution $$ \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ = 5 \\\\\\ \u0026 x₂ +\u0026 x₃ = 1 \\\\\\ \u0026 \u0026 0 = 0 \\\\\\ \\end{cases} $$Its general solution being represented with x₃ is 𝐱 = $[^{\\_{3-x₃}}\\_{^{1-x₃}\\_{x₃}} ]$\nTo structure the solution, the constant terms are separated out and x₃ is replaced by k.\n𝐱 = $[^{\\_{3-x₃}}\\_{^{1-x₃}\\_{x₃}} ]$ = x₃$[^{\\_{-1}}\\_{^{-1}\\_{1}} ] + [^{\\_{3}}\\_{^{1}\\_{0}} ]$ = k$[^{\\_{-1}}\\_{^{-1}\\_{1}} ] + [^{\\_{3}}\\_{^{1}\\_{0}} ]$\nThis general solution consistitute two components: random number times a column vector k$[^{\\_{-1}}\\_{^{-1}\\_{1}} ]$ and a constant vector.\nIf the $k [^{\\_{-1}}\\_{^{-1}\\_{1}} ]$ is substituted into equation system individually, the results of all equations are 0s.\n$$ \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ = 0 \\\\\\ \u0026 x₂ +\u0026 x₃ = 0 \\\\\\ \u0026 \u0026 0 = 0 \\\\\\ \\end{cases} $$So this term is the general solution for the homogeneous linear equations system.\nAnd the constant vector can just make the system satisfied. Hence, it\u0026rsquo;s the specific solution of the non-homogeneous linear equation system.\nTherefore, the general solution for a non-homogeneous linear equation system is made up by two parts: the general solution of the corresponding homogeneous linear equation system and the specific solution of the non-homogeneous linear equation system.\nThe method to find the general solution of the corresponding homogeneous linear equation system has been introduced in the last video.\nFind the specific solution Based on the augmented matrix in row echelon form, the leading variables are x₁ and x₂, while x3 is the free variable.\nTheoratically, any one of the solution satifying the non-homogeneous linear equation system is a specific solution. So, the free varaible can be assigned with any value. And in practice, free variables are all set to 0 for the purpose of solving convinently.\nIf Letting x3 = 0, then x2=1, x1 =3. Thus, the specific vector 𝛈 = $[^{\\_{3}} \\_{^{1}}\\_{0}]$\nSteps of solving Write the augmented matrix 𝐀|𝐛 based on the equation system and determine the number of unknowns n.\n$$ \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ +\u0026 x₄ \u0026= 3 \\\\\\ x₁ -\u0026 4x₂ -\u0026 x₃ -\u0026 x₄ \u0026= 1 \\\\\\ 2x₁ +\u0026 x₂ +\u0026 4x₃ +\u0026 x₄ \u0026= 5 \\\\\\ x₁ -\u0026 x₂ +\u0026 x₃ \u0026 \u0026= 2 \\\\\\ \\end{cases} $$𝐀|𝐛 = $$ \\begin{bmatrix} 1 \u0026 2 \u0026 3 \u0026 1 \u0026 | 3 \\\\\\ 1 \u0026 -4 \u0026 -1 \u0026 -1 \u0026 | 1 \\\\\\ 2 \u0026 1 \u0026 4 \u0026 1 \u0026 | 5 \\\\\\ 1 \u0026 -1 \u0026 1 \u0026 0 \u0026 | 2 \\end{bmatrix} $$ , n = 4\nTransform the augmented matrix to row echelon form by performing elementray row operations. $[^{^{1\\ 2\\ 3\\ 1\\ | 3}\\_{1\\ -4\\ -1\\ -1\\ |1}} \\_{^{2\\ 1\\ 4\\ 1\\ | 5}\\_{1\\ -1\\ 1\\ 0\\ | 2}}]$ $^{\\_{-(-r1+r2)}} \\_{^{-(-2r1+r3)} \\_{-(-r1+r4)}}$ ➔ $[^{^{1\\ 2\\ 3\\ 1\\ | 3} \\_{0\\ 6\\ 4\\ 2\\ |2}} \\_{^{0\\ 3\\ 2\\ 1\\ | 1}\\_{0\\ 3\\ 2\\ 1\\ | 1}}]$ ½r2 ➔ $[^{^{1\\ 2\\ 3\\ 1\\ | 3} \\_{0\\ 3\\ 2\\ 1\\ |1}} \\_{^{0\\ 3\\ 2\\ 1\\ | 1}\\_{0\\ 3\\ 2\\ 1\\ | 1}}]$ $^{-(-r2+r3)} \\_{-(-r2+r4)}$ ➔ $[^{^{1\\ 2\\ 3\\ 1\\ | 3} \\_{0\\ 3\\ 2\\ 1\\ |1}} \\_{^{0\\ 0\\ 0\\ 0\\ | 0}\\_{0\\ 0\\ 0\\ 0\\ | 0}}]$\nCheck the number of non-zero rows r(𝐀|𝐛) of the obtained simplified augmented matrix and r(𝐀) for the coefficient matrix. r(𝐀|𝐛) = r(𝐀) = 2 \u0026lt; 3\nDetermine the case of the situations:\nr(𝐀|𝐛) = r(𝐀) = n, single unique solution r(𝐀|𝐛) = r(𝐀) \u0026lt; n, inifinitely many solutions r(𝐀|𝐛) ≠ r(𝐀), no solution Because r(𝐀|𝐛) = r(𝐀) = 2 \u0026lt; 4, this non-homogeneous equation system has infinitely many solutions.\nFor the case of infinitely many solutions, determine the leading variables and free variables first, and solve the fundamental system of solution, then make up the general solution for the non-homogeneous equation system: General solution of the corresponding homogeneous equation system\nSpecific solution of the non-homogeneous equation system The leading variables are x1 and x2, while x3 and x4 are free variables.\nSolve the general solution of the homogeneous equation system by letting the result of all equations be 0. $$ \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ +\u0026 x₄ \u0026= 0 \\\\\\ \u0026 3x₂ +\u0026 2x₃ +\u0026 x₄ \u0026= 0 \\\\\\ 0 \u0026= 0 \\\\\\ 0 \u0026= 0 \\\\\\ \\end{cases} $$ Transform the coefficient matrix to row echelon form $[^{^{1\\ 2\\ 3\\ 1\\ | 3} \\_{0\\ 3\\ 2\\ 1\\ |1}} \\_{^{0\\ 0\\ 0\\ 0\\ | 0}\\_{0\\ 0\\ 0\\ 0\\ | 0}}]$ Determine the leading variables and free variables. By assigning values to free variables orthogonally, the general solution for homogeneous equation system can be constructed. • Take x3,x4 as 0,1, then x2=-1/3, x1=-1/3; • Take x3,x4 as 1,0, then x2=-2/3, x1=-5/3; • The fundamental system of solution constitute $[^{^1\\_1}\\_{^0\\_{-3}}]$ and $[^{^5\\_2}\\_{^{-3}\\_0}]$ • General solution for homogeneous equation system: 𝐱\u0026rsquo; = k₁ξ₁ +k₂ξ₂ = k₁$[^{^1\\_1}\\_{^0\\_{-3}}]$ + k₂ $[^{^5\\_2}\\_{^{-3}\\_0}]$ Find the specific solution of the non-homogeneous equation system by letting all free variables be 0 and solving leading variables from bottom to top: Take x3,x4 as 0, then x2=1/3, x1=7/3. So the specific solution is 𝛈=$[^{^{7/3}\\_{1/3}}\\_{^0\\_0}]$\nAdd the above two parts together to get the final general solution of the non-homogeneous equation system. 𝐱 = 𝐱\u0026rsquo; + 𝛈 = k₁$[^{^1\\_1}\\_{^0\\_{-3}}]$ + k₂ $[^{^5\\_2}\\_{^{-3}\\_0}]$ + $[^{^{7/3}\\_{1/3}}\\_{^0\\_0}]$\n","date":"2023-02-03T10:39:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/03_%E8%A7%A3%E9%9D%9E%E9%BD%90%E6%AC%A1%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/","title":"watch: LA - 高山 03 | Solve Non-homogeneous Linear System"},{"content":"Source video：【俗说矩阵】 用矩阵解齐次线性方程组这要这么几步就搞定了！真的好简单 - 晓之车高山老师 - bilibili\nThere are 2 cases of solutions for homogeneous linear system:\nHave and only have the zero solution Exist non-zero solution besides the zero solution How to judge the case of the solution? Given a homogeneous linear system in 3 variables:\n$$ \\begin{cases} x₁ + \u0026 2x₂ + \u00263x₃ = 0 \\\\\\ \u0026 4x₂ + \u0026 5x₃ = 0 \\\\\\ \u0026 \u0026 x₃ = 0 \\end{cases} $$Substitute the bottom variable to the topper equations to solve other variables. And the solved result is a zero vector: $𝐱 = \\begin{bmatrix} 0\\\\\\ 0\\\\\\ 0 \\end{bmatrix}$\nThe coefficient matrix is in row echelon form resulting from a Gaussian elimination.\n$$ \\begin{bmatrix} 1 \u0026 2 \u0026 3\\\\\\ 0 \u0026 4 \u0026 5\\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$In the above matrix, the first elements next to the right side of the steps line are all non-zero elements, while on the left-hand side of the steps, the elements are all 0.\nAll three rows are not all zeros. That means the number of rows that are not 0 is 3, which is the same as the number of the unknowns. Hence, only if the number of non-zero rows in the echelon-form coefficient matrix is equal to the number of unknowns, the homogeneous linear system has only the zero solution. case1: only zero solution Given the following homogeneous linear system: $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 0 \\\\\\ x₁ + 4x₂ + 5x₃ = 0 \\\\\\ x₁ + 10x₂ + x₃ = 0 \\end{cases} $$Its coefficient matrix ($\\[^{\\_{1\\ 2\\ 3}}\\_{^{1\\ 6\\ 8}\\_{1\\ 10\\ 6}}\\]$) is not in row echelon form. But this linear system can be simplized through the (row) Gaussian elimination, i.e., the elementary row operations applied on its coefficients matrix.\nWith the row echelon form as the objective, the elementary row operations are operated on the coefficients matrix.\nUse row-1 to cancel the first element (1) in the row2 and row3 by multiplying the 1st row with -1 and adding it to the 2nd and 3rd rows. $\\[^{\\_{1\\ 2\\ 3}}\\_{^{1\\ 6\\ 8}\\_{1\\ 10\\ 6}}\\]$ ➔ $\\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 8\\ 3}}\\]$\nUse row-2 to cancel the second element (8) in the 3rd row by multiplying the 2nd row with -2 and adding it to the 3rd rows. $\\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 8\\ 3}}\\]$ ➔ $\\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 0\\ -7}}\\]$\nMultiply the 3rd row with -1/7. $\\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 0\\ -7}}\\] ➔ \\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 0\\ 1}}\\]$\nConclusion: Perform the elementary row operations on the coefficient matrix to transform it as in the row echelon form. Then if the number of non-zero rows is equal to the number of unknowns, the homogeneous linear system only has zero solution.\ncase2: infinitely many solution A homogeneous linear system is as follow: $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 0 \\\\\\ x₁ + 6x₂ + 8x₃ = 0 \\\\\\ x₁ + 10x₂ + 14x₃ = 0 \\end{cases} $$Eliminate the elements from bottom to top and from left to righ:\nUse row-1 to cancel the first element of the 2nd and 3rd rows by \u0026ldquo;-r1+r2\u0026rdquo; and \u0026ldquo;-r1+r3\u0026rdquo;. $\\[^{\\_{1\\ 2\\ 3}}\\_{^{1\\ 6\\ 8}\\_{1\\ 10\\ 13}}\\]$ ➔ $\\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 8\\ 10}}\\]$\nUse row-2 to cancel the second element of the 3rd row by \u0026ldquo;-2r2+r3\u0026rdquo;. $\\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 8\\ 10}}\\] ➔ \\[^{\\_{1\\ 2\\ 3}}\\_{^{0\\ 4\\ 5}\\_{0\\ 0\\ 0}}\\]$\nThe bottom row of this echelon-form matrix is all 0. Hence, there are only 2 independent equations essiensially: $$ \\begin{cases} x₁ + \u0026 2x₂ +\u0026 3x₃ = 0 \\\\\\ \u0026 6x₂ +\u0026 8x₃ = 0 \\\\\\ \u0026 \u0026 0 = 0 \\end{cases} $$If the number of non-zero rows (2) is less than the number of unknowns (3), then there exists non-zero solutions besides the zero solution.\nAnd we can give a general solution for this system.\nWhat is general solution? Suppose the homogeneous linear system is: $$ \\begin{cases} x₁+x₂=0\\\\\\ 2x₁+2x₂=0 \\end{cases} $$As long as the 2 variables have opposite signs: $\\rm \\\\{^{x₁=0}\\_{x₂=0}\\ or\\ \\\\{^{x₁=1}\\_{x₂=-1}\\ or\\ \\\\{^{x₁=2}\\_{x₂=-2}\\ or ... $,\nthe system is satisfied.\nSo the solutions has a general form: $\\\\{^{x₁=k}\\_{x₂=-k}$\nWrite the solution as a column vector: $𝐱 = \\[^{x₁}\\_{x₂} \\] = k \\[^{\\ 1}\\_{-1}\\]$\nHow to find the general solution? How to solve the general solution when there are infinitely many solutions?\n$$ \\begin{cases} x₁ + \u0026 2x₂ +\u0026 3x₃ = 0 \\\\\\ \u0026 6x₂ +\u0026 8x₃ = 0 \\\\\\ \u0026 \u0026 0 = 0 \\end{cases} $$Similarly, solve the variables from bottom to top. Fix the x₃, then x₁ and x₂ can be represented with x₃. Thus the solution is written as: $\\begin{bmatrix} -1/2⋅x₃\\\\\\ -5/4⋅x₃\\\\\\ x₃ \\end{bmatrix} = x₃⋅\\begin{bmatrix}-1/2\\\\\\ -5/4\\\\\\ 1 \\end{bmatrix}$\nHere, any x₃ can be the solution of this system. That purely numerical column vector constitutes the fundamental system of solutions(基础解系) for this homogeneous linear system.\nThe fundamental system can be scaled by any factor. For example, let x₃=-4k, it becomes a scaled fundamental system. Then the solution $k \\begin{bmatrix}2\\\\\\ 5\\\\\\ -4 \\end{bmatrix}$ containing only integars is the general solution. 2 free variables Given a homogeneous linear system: $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 0 \\\\\\ 2x₁ + 4x₂ + 6x₃ = 0 \\\\\\ 3x₁ + 6x₂ + 9x₃ = 0 \\end{cases} $$Perform the elementary row operations for its coefficient matrix: $\\[^{\\_{1\\ 2\\ 3}} \\_{^{2\\ 4\\ 6}\\_{3\\ 6\\ 9}} \\]$ by \u0026ldquo;-2r1+r2\u0026rdquo; and \u0026ldquo;-3r1+r3\u0026rdquo;. The echelon-form matrix is $\\[^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 0\\ 0}\\_{0\\ 0\\ 0}} \\]$, where the 2nd and 3rd rows are all 0. That means only the 1st equation is valid. $\\\\{^{x₁ + 2x₂ + 3x₃ = 0}\\_{^{0=0}\\_{0=0}}$\nTherefore, only if the x₂ and x₃ both are set, then x₁ can be determined.\nIn other words, x₁ has to be represented by x₂ and x3 collectively: $\\begin{bmatrix} -2x₂-3x₃\\\\\\ x₂\\\\\\ x₃\\end{bmatrix}$\nTo clarify the representation, it can be broken down to separate different variables:\n$$ \\[^{\\_{-2x₂-3x₃}} \\_{^{x₂}\\_{x₃}} \\] = \\[^{\\_{-2x₂}} \\_{^{x₂}\\_{0}} \\] + \\[^{\\_{-3x₃}} \\_{^{0}\\_{x₃}} \\] = x₂\\[^{\\_{-2}} \\_{^{1}\\_{0}} \\] + x₃\\[^{\\_{-3}} \\_{^{0}\\_{1}} \\] $$Then the general solution is a linear combination of two numerical column vectors with x₂,x₃ as their coefficients. And the x₁ in these 2 vector (-2 and -3) are the solution when x₂,x₃ are the corresponding values.\nThat means when x₂,x₃ are assigned (orthogonally) repeatedly with (1,0) and (0,1), two x₁ can be determined and then 2 numerical column vectors are generated, which constitute the fundamental system of solutions for this linear system.\nBy substituting x₂,x₃ with constants k₁,k₂, the genearl solution is the linear combination of all the numerical column vector in the fundamental system: $$ k₁\\begin{bmatrix} -2\\\\\\ 1\\\\\\ 0\\end{bmatrix} + k₂\\begin{bmatrix} -3\\\\\\ 0\\\\\\ 0\\end{bmatrix} $$Comparing the above two general solutions: $k \\[^2\\_{^5\\_{-4}} \\]$ and $k₁\\[^{\\_{-2}} \\_{^{1}\\_{0}} \\] + k₂\\[^{\\_{-3}} \\_{^{0}\\_{1}} \\]$, there is only 1 vector in the fundamental system of solutions for the former system, and there are 2 vectors in the fundamental system of solutions for the latter system. Obviously, this is related to the number of non-zero rows in the matrix in the row-echelon form.\nLeading variables \u0026amp; free variables Commonly, for a matrix in the row-echelon form, the first non-zero element next to the steps line corresponds to a leading variable. And also in the simplified system of equations, the leading variables are embodied as the coefficient of the first variable is not zero. The variables exposed at the start of each line are leading variables; And the other variables are called free variables.\n$$ \\begin{array}{cc} \\begin{array}{c} \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ = 0\\\\\\ \u0026 4x₂ +\u0026 5x₃=0\\\\\\ \u0026 \u0026 0=0 \\end{cases} \\\\\\ ⇓\\\\\\ \\[^{1\\ 2\\ 3} \\_{^{0\\ 4\\ 5}\\_{0\\ 0\\ 0}} \\] \\end{array} \u0026 \\begin{array}{c} \\begin{cases} x₁ +\u0026 2x₂ +\u0026 3x₃ = 0\\\\\\ \u0026 \u0026 0=0\\\\\\ \u0026 \u0026 0=0 \\end{cases} \\\\\\ ⇓\\\\\\ \\[^{1\\ 2\\ 3} \\_{^{0\\ 0\\ 0}\\_{0\\ 0\\ 0}} \\] \\end{array} \\end{array} $$In the above left system, x₁ and x₂ are leading variables. While in the right system, only x₁ is the leading variable.\nleading variables are variables whose column contains the leading entry of some row; And free variables are all the other variables. MST10030 notes wiki\nA few related conclusions:\nThe number of leading variables is equal to the number of non-zero rows in the matrix in the row echelon form, which is written as r(𝐀)\nIf there are total n unknowns, then the number of the free variables is t = n - r(𝐀).\nAll free variables need to be assigned orthogonally.\nIf there is only 1 free variable, then just assign it with 1 and calculate the other values (leading variables) in the numerical column vector constituting the fundamental system of solutions. If there are 2 free variables, they can be assigned with (1,0) and (0,1) twice separately\u0026hellip;. i.e., the number of free variables is the number of orthogonal assignmens required separately. The number of free variables is the number of vectors constituting the fundamental system of solutions.\nTo calculate the fundamental system of solutions, the free variables have to be assigned first, and then solve the other leading variables from bottom to top.\nComplete solution steps The steps of solving homogeneous linear system based on the coefficient matrix.\n$$ \\begin{cases} x₁ + 2x₂ + 3x₃ + x₄ = 0 \\\\\\ x₁ - 4x₂ - x₃ - x₄ = 0 \\\\\\ 2x₁ + x₂ + 4x₃ + x₄ = 0 \\\\\\ x₁ - x₂ + x₃ = 0 \\end{cases} $$ Write down the coefficient matrix 𝐀 and the number of unknowns n=4 based on the given equations system. 𝐀 = $\\[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}\\_{1\\ -4\\ -1\\ -1}} \\_{^{2\\ \\ 1\\ \\ 4\\ 1}\\_{1\\ -1\\ 1\\ 0}} \\]$\nPerform the elementary on the coefficient matrix 𝐀 = $\\[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}\\_{1\\ -4\\ -1\\ -1}} \\_{^{2\\ \\ 1\\ \\ 4\\ 1}\\_{1\\ -1\\ 1\\ 0}} \\]$ to reach the matrix in the row-echelon form.\nUse row-1 to cancel the first element in 2nd, 3rd, and 4th rows by \u0026ldquo;-r1+r2\u0026rdquo;, \u0026ldquo;-2r1+r3\u0026rdquo;, \u0026ldquo;-r1+r4\u0026rdquo;:\n𝐀 = $\\[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}\\_{0\\ -6\\ -4\\ -2}} \\_{^{0\\ -3\\ -2\\ -1}\\_{0\\ -3\\ -2\\ -1}} \\]$ Multiply 2nd, 3rd, and 4th rows with some numbers to make common factor (\u0026quot;-0.5r2\u0026quot;, \u0026ldquo;-r3\u0026rdquo;, \u0026ldquo;-r4\u0026rdquo;)\n𝐀 = $\\[^{^{1\\ 2\\ 3\\ 1}\\_{0\\ 3\\ 2\\ 1}} \\_{^{0\\ 3\\ 2\\ 1}\\_{0\\ 3\\ 2\\ 1}} \\]$ Use row-2 to cancel the 3rd and 4th rows by \u0026ldquo;-r2+r3\u0026rdquo;, \u0026ldquo;-r2+r4\u0026rdquo;: 𝐀 = $\\[^{^{1\\ 2\\ 3\\ 1}\\_{0\\ 3\\ 2\\ 1}} \\_{^{0\\ 0\\ 0\\ 0}\\_{0\\ 0\\ 0\\ 0}} \\]$ Check the number of non-zero rows in this echelon-form matrix, r(𝐀)=2\nIf r(𝐀) = n, then this system only has zero solution.\nOtherwise, if r(𝐀) \u0026lt; n, this system exists non-zero solution. Find out the leading varibles based on the position of the down edges of the steps line: x₁, x₂. So the free variables are x₃ and x₄. Then x₃,x₄ are assigned with (1,0) and (0,1) repeatedly to get the 2 (=n-r(𝐀)) numerical column vector constituting the fundamental system of solutions.\nWhen $\\\\{^{x₃=1}\\_{x₄=0}$, the leading variables are $\\\\{^{x₁=-5/3}\\_{x₂=-2/3}$. Thus, one of the vectors in the fundamental system of solutions is $ξ₁ = \\[^{^{5}\\_{2}} \\_{^{-3}\\_{0}} \\]$ When $\\\\{^{x₃=0}\\_{x₄=1}$, the leading variables are $\\\\{^{x₁=-1/3}\\_{x₂=-1/3}$. Thus, one of the vectors in the fundamental system of solutions is $ξ₂ = \\[^{^{1}\\_{1}} \\_{^{0}\\_{-3}} \\]$ Combine linearly those two numerical column vectors by constants to get the general solution:\n𝐱 = k₁ξ₁ + k₂ξ₂ = $k₁\\[^{^{5}\\_{2}} \\_{^{-3}\\_{0}} \\] + k₂\\[^{^{1}\\_{1}} \\_{^{0}\\_{-3}}\\]$\n","date":"2023-02-01T20:21:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/02_%E8%A7%A3%E9%BD%90%E6%AC%A1%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/","title":"watch: LA - 高山 02 | Solve Homogeneous Linear System"},{"content":"Source video: 【俗说矩阵】初等矩阵可一点也不初等，它居然有这么重要的意义！\n3 elementary row operations 换行：交换两行的位置 数乘：给某一行乘以非零常数k 倍加：将某一行的 k 倍加到另一行上 By means of these 3 kind of elementary row operations, a matrix can be transformed to row echelon form, which is useful in analysing and solving the system of linear equations.\nIdentical matrix $$ I = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$The elements on the main diagonal are all 1 and the rest of elements in the martrix are all 0.\nIf multiplying a matrix with a identical matrix, the matrix doesn\u0026rsquo;t change. So the effect of I is similar to the 1 in numbers.\nA identical matrix can perform elementary row operations.\nElementary matrix 对单位矩阵 I 实施一次初等行变换。实施2次就不是了。wikipedia\n$$ I = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} ➔ \\begin{cases} \\begin{bmatrix} 0 \u0026 1 \u0026 0 \\\\\\ 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \u0026 \\text{Row switch: r1 ⟷ r2} \\\\\\ \\\\\\ \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 3 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \u0026 \\text{Row multiplication: 3r1 ➔ r1} \\\\\\ \\\\\\ \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 3 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \u0026 \\text{Row addition: 3r1+r2 ➔ r2} \\end{cases} $$初等矩阵只有3种：置换阵，数乘阵，倍加阵\nElementary matrices connect the matrix multiplication and elementary row operations.\nRow-switching transformations Row-multiplying transformations Row-addition transformations 对矩阵实施一次初等行变换 左乘一个初等矩阵，就是把对初等矩阵的初等行变换，施加到矩阵上。\n","date":"2023-02-01T12:41:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/10_%E5%88%9D%E7%AD%89%E7%9F%A9%E9%98%B5/","title":"watch: LA - 高山 10 | Elementary Matrix"},{"content":"Original Video：【俗说矩阵】矩阵原来这么简单！从二元一次方程组开始教你~ - 晓之车高山老师 - bilibili\nLinear equations system Linear equation in 2 variables (二元一次方程): An equation has 2 unknown variables with an exponent of 1.\nTwo linear equations in 2 variables can make up a system of linear equations (线性方程组). $$ \\begin{cases} x + y = 3 \\\\\\ 2x + 3y = 8 \\end{cases} $$Gaussian elimination Two kinds of Gaussian elimination to solve this linear system:\nReformalize the 1st equation and substitute it into the 2nd one, such that it comes down to a linear equation in 1 variable. (代入消元法) $\\begin{cases}y=3-x\\\\\\ 2x+3y=8 \\end{cases}$ ➔ 2x+3(3-x)=8 ➔ $\\\\{^{x=1}\\_{y=2}$\nMultiply the equation with a certain number and add the 2 equations together to cancel a variable, such that only 1 linear equation is left. (加减消元法) $\\begin{cases}-2x-2y=-6\\\\\\ 2x+3y=8 \\end{cases}$ ➔ y=2 ➔ $\\\\{^{x=1}\\_{y=2}$\nGeometric interpretation A linear equation in 2 variables is a line on a plane. The solution of the linear system is the intersection of these lines.\nTwo kinds of linear system:\nHomogeneous system: all of the constant terms are zeroes. $\\begin{cases} x+y = 0\\\\\\ 2x+3y = 0 \\end{cases}$\nNonhomogeneous system: The constant term is not all zero. $\\begin{cases} x+y = 3\\\\\\ 2x+3y = 8 \\end{cases}$\nSolution of Homogeneous system has 2 cases The lines are all passing through the origin.\nIf the slopes are different ($\\\\{^{x+y=0}\\_{2x+3y=0}$ ➔ x=y=0), the only intersection of them is the origin. This means the homogenous system has only zero solution. If the slopes of them are identical ($\\\\{^{x+y=0}\\_{2x+2y=0}$ ➔ x=-y), the lines are overlapping. The solution set is a line: any point on the line is a solution of this system. That means there are infinitely many non-zero solutions besides the zero solution. There exists non-zero solution in this system. Solution of Non-homogeneous system has 3 cases Lines have distinct slopes ($\\\\{^{x+y=3}\\_{2x+3y=8}$), so there is only one intersection corresponding to the single unique solution.\nLines are overlapping ($\\\\{^{x+y=3}\\_{2x+2y=6}$). Any point on the line is a solution of this system. The system has infinitely many solutions.\nLines are parallel without any intersection ($\\\\{^{x+y=3}\\_{2x+2y=4}$), this system has no solution. (Its solution must not be zero.)\nTherefore, the assertion \u0026ldquo;The number of unknowns is the number of equations needed.\u0026rdquo; works only for the situation of \u0026ldquo;only zero solution\u0026rdquo; in a homogeneous system and \u0026ldquo;single unique solution\u0026rdquo; in a non-homogeneous system.\nIn the scenario with more variables, or the numbers of unknowns and equations are not equal, even if the gaussian elimination can be applied, the correct steps or the direction of elimination are hard to determine.\nHow to solve a linear system? A general homogeneous system is a combination of m linear equations in n unknowns.\n$$ \\begin{cases} a₁₁x₁ + a₁₂x₂ + ... + a₁ₙxₙ = 0 \\\\\\ a₂₁x₁ + a₂₂x₂ + ... + a₂ₙxₙ = 0 \\\\\\ \\dots \\\\\\ aₘ₁x₁ + aₘ₂x₂ + ... +aₘₙxₙ = 0 \\end{cases} $$where\nx₁, x₂, \u0026hellip;,xₙ are the n unkowns,\nThe coefficient aₘₙ, the first subscript represents the ordinal number of the equation, and the second subscript stands for the unknown that this coefficient multiplies with.\nFor example a₃₂ is the coefficient in the 3rd equation for the unknown x₃.\nCoefficients of each row cannot be all 0 and all of the parameters of an unknown cannot be all 0, so these m equations and n unknowns are effective.\nHomogeneous Linear system with 3 variables $$ \\begin{cases} x₁ + 3x₂ + 5x₃ = 0 \\\\\\ 2x₁ + 4x₂ + 6x₃ = 0 \\\\\\ 2x₁ + 5x₂ + 8x₃ = 0 \\end{cases} $$Each row is a inner product of two 3-dimensional vectors:\nx₁ + 3x₂ + 5x₃ = 0 ➔ (1 3 5)(x₁ x₂ x₃) = 0 2x₁ + 4x₂ + 6x₃ = 0 ➔ (2 4 6)(x₁ x₂ x₃) = 0 2x₁ + 5x₂ + 8x₃ = 0 ➔ (2 5 8)(x₁ x₂ x₃) = 0\nThere are 2 types of vector:\nRow vector (1 2 3) Column vector $\\[^1\\_{^2\\_3}\\]$ or (1 2 3)ᵀ Stack the above 3 expressions: $\\begin{bmatrix}1 \u0026 3 \u0026 5\\\\\\ 2 \u0026 4 \u0026 6\\\\\\ 2 \u0026 5 \u0026 8 \\end{bmatrix} ⋅ (x₁\\ x₂\\ x₃) = \\begin{bmatrix}0\\\\\\ 0\\\\\\ 0\\end{bmatrix}$\nThe left array is matrix. This linear system can be regarded as a function: $f( (x₁\\ x₂\\ x₃) ) = \\begin{bmatrix} 0\\\\\\ 0\\\\\\ 0 \\end{bmatrix}$\nwhich maps 3 variables to 3 zeros, i.e., maps a unknown (row) vector to a zero (column) vector 𝟎.\nTo normalize the notations, the unknown vector is written as a column vector 𝒙, such that the linear system can be represented by matrix:\n$$ \\begin{bmatrix} 1 \u0026 3 \u0026 5\\\\\\ 2 \u0026 4 \u0026 6\\\\\\ 2 \u0026 5 \u0026 8 \\end{bmatrix} \\begin{bmatrix} x₁\\\\\\ x₂\\\\\\ x₃ \\end{bmatrix} = \\begin{bmatrix} 0\\\\\\ 0\\\\\\ 0 \\end{bmatrix} $$Therefore, the expression represents the mapping linearly from a column vector to another column vector.\n$f(\\begin{bmatrix} x₁\\\\\\ x₂\\\\\\ x₃ \\end{bmatrix}) = \\begin{bmatrix} 0\\\\\\ 0\\\\\\ 0 \\end{bmatrix}$\nHere, the matrix is arranging all the coefficients into an array based on their positions. Hence, it\u0026rsquo;s called coefficient matrix 𝑨.\nThen this linear system can be expressed consiscely as: 𝑨𝒙=𝟎.\nFrom the view of function, the function $f(\\begin{bmatrix} x₁ \\\\\\ x₂ \\\\\\ x₃ \\end{bmatrix})=\\begin{bmatrix} 0\\\\\\ 0\\\\\\ 0 \\end{bmatrix}$ can be written as f(𝒙)=𝟎.\nThe coefficient matrix 𝑨 plays the role of function f, that means the coefficient matrix maps the unknown vector to zero vector.\nNon-homogeneous linear system A non-homogeneous linear system contains m linear equations with n unknowns, where their constant termsare not all 0.\n$$ \\begin{cases} a₁₁x₁ + a₁₂x₂ + ... + a₁ₙxₙ = b₁ \\\\\\ a₂₁x₁ + a₂₂x₂ + ... + a₂ₙxₙ = b₂ \\\\\\ \\dots \\\\\\ aₘ₁x₁ + aₘ₂x₂ + ... + aₘₙxₙ = bₙ \\\\\\ \\end{cases} $$The notation is the same as homogeneous system and b₁, b₂,\u0026hellip;, bₙ are not all 0.\nFor instance, the following non-homogeneous linear system can be represented as matrix:\n$$ \\begin{cases} x₁ + 3x₂ + 5x₃ = 2 \\\\\\ 2x₁ + 4x₂ + 6x₃ = 4 \\\\\\ 2x₁ + 5x₂ + 8x₃ = 4 \\\\\\ \\end{cases} \\\\\\ ⇓ \\\\\\ \\begin{bmatrix} 1 \u0026 3 \u0026 5\\\\\\ 2 \u0026 4 \u0026 6\\\\\\ 2 \u0026 5 \u0026 8 \\end{bmatrix} \\begin{bmatrix} x₁\\\\\\ x₂\\\\\\ x₃ \\end{bmatrix} = \\begin{bmatrix} 2\\\\\\ 4\\\\\\ 4 \\end{bmatrix} \\\\\\ ⇓ \\\\\\ 𝑨𝒙=𝐛 $$Here, the coefficient matrix 𝑨 maps the column vector to a known non-zero column vector: $$f(\\begin{bmatrix} x₁\\\\\\ x₂\\\\\\ x₃ \\end{bmatrix})=\\begin{bmatrix} 2\\\\\\ 4\\\\\\ 4 \\end{bmatrix} ➔ f(𝒙)=𝐛$$ Elementary row operations are equivalent to Gaussian elimination.\n","date":"2023-02-01T12:19:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/01_%E5%88%9D%E8%AF%86%E7%9F%A9%E9%98%B5/","title":"watch: LA - 高山 01 | Walk into the Matrix"},{"content":"Source video: 【俗说矩阵】逆矩阵原来要这么学！数学老师直呼内行！\nRow echelon form ➔ Identity matrix 𝐊₆ includes 6 elementary row operations that transform the matrix 𝐀 to row echelon form.\n$$ 𝐀₆ = 𝐊₆𝐀 \\\\\\ \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\\\ 0 \u0026 1 \u0026 2 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 0 \u0026 1/2 \u0026 0 \\\\\\ 1 \u0026 -1/2 \u0026 0 \\\\\\ -1 \u0026 1/4 \u0026 1/2 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 3 \u0026 5 \\\\\\ 2 \u0026 4 \u0026 6 \\\\\\ 1 \u0026 4 \u0026 9 \\end{bmatrix} $$𝐀₆ can further perform elementary row operations to make the elements at the top right of the main diagonal 0, i.e., reaching an identity matrix.\n𝐀₆: $[^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 1\\ 2} \\_{0\\ 0\\ 1}}]$ $\\overset{-2×r3+r2:\\ [^{\\_{1\\ 0\\ 0}} \\_{^{0\\ 1\\ -2} \\_{0\\ 0\\ 1}}]}{{⇢}}$ 𝐀₇: $[^{\\_{1\\ 2\\ 3}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}]$ $\\overset{-3×r3+r1:\\ [^{\\_{1\\ 0\\ -3}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}]}{{⇢}}$ 𝐀₈: $[^{\\_{1\\ 2\\ 0}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}]$ $\\overset{-2×r2+r1:\\ [^{\\_{1\\ -2\\ 0}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}]}{{⇢}}$ 𝐀₉: $[^{\\_{1\\ 0\\ 0}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}]$\nwhere,\n$$ [^{\\_{1\\ -2\\ 0}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}] [^{\\_{1\\ 0\\ -3}} \\_{^{0\\ 1\\ 0} \\_{0\\ 0\\ 1}}] [^{\\_{1\\ 0\\ 0}} \\_{^{0\\ 1\\ -2} \\_{0\\ 0\\ 1}}] 𝐊₆𝐀 = 𝐊₉𝐀 = 𝐈 \\\\\\ 𝐊₉ = [^{\\_{-3\\ 7/4\\ 1⁄2}} \\_{^{3\\ -1\\ -1}\\_{-1\\ ¼\\ 1⁄2}}] $$The above example indicates that there exist a matrix that can modify the matrix 𝐀 to 𝐈.\nLet 𝐀⁻¹ denote the 𝐊₉, and 𝐀⁻¹ is called the inverse matrix of 𝐀.\nSuch that there is 𝐀⁻¹𝐀 = 𝐈. Since the matrix multiplication doesn\u0026rsquo;t has commutative property, what does 𝐀𝐀⁻¹ equal?\n𝐀𝐀⁻¹ = [^{_{1\\ 3\\ 5} _{^{2\\ 4\\ 6}_{1\\ 4\\ 9}}}] [^{_{-3\\ 7/4\\ 1⁄2}} _{^{3\\ -1\\ -1}_{-1\\ ¼\\ 1⁄2}}] = 𝐈\nIn fact, 𝐀 and 𝐀⁻¹ are a pair of commutative matrices: 𝐀⁻¹𝐀 = 𝐀𝐀⁻¹ = 𝐈. Also, 𝐀 and 𝐀⁻¹ are the inverse matrix of each other: (𝐀⁻¹)⁻¹ = 𝐀\nAnalogy to reciprocal Given two numbers a, b, if ab = ba = 1, then b = a⁻¹, a≠0.\nAnd the identity matrix 𝐈 has the same effect as the number 1.\nTherefore, given two matrices 𝐀, 𝐁, if 𝐀𝐁 = 𝐁𝐀 = 𝐈, then 𝐁 = 𝐀⁻¹\nNo all matrix has its inverse matrix, which is like the number 0 doesn\u0026rsquo;t has its reciprocal.\nSo what kind of matrix has an inverse? And is the inverse unique? How to solve the inverse matrix?\nInvertible matrix An invertible matrix can perform multiple elementary row operations to become an identity matrix.\nIdentity matrix is a square matrix (#row = #colmuns) and its rank = #rows (= #cols). And the elementary row operations do not change the size and rank note4.\nTherefore, an invertible matrix must be an square matrix and its rank = #rows.\nBased on the commutative property that 𝐀⁻¹ and 𝐀 are the inverse matrices of each other, the 𝐀⁻¹ must also be a square matrix with the same rank of 𝐈.\n可逆的2个条件：1-方阵， 2-秩=列(行)数\nSince elementary row operation is that 𝐀 is multiplied by elementary matrix on the left. And the product of the series of elementary row operations that transformed 𝐀 to 𝐈 is represented as 𝐀⁻¹.\nTherefore, an invertible matrix and its inverse are both able to represented as the multiplication of multiple elementary matrix.\nSince the product of a series of elementary matrix must be invertible, a special case is that there is only a single elementary matrix. So, any of the elementary matrix is invertible.\nInverse is unique Let 𝐀𝐁 = 𝐁𝐀 = 𝐈 and 𝐀𝐂 = 𝐂𝐀 = 𝐈, then 𝐁 = 𝐁𝐈 = 𝐁(𝐀𝐂) = (𝐁𝐀)𝐂 = 𝐈𝐂 = 𝐂\n","date":"2023-02-01T11:00:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/11_%E9%80%86%E7%9F%A9%E9%98%B5%E6%A6%82%E5%BF%B5/","title":"watch: LA - 高山 11 | Concept of the Inverse Matrix"},{"content":" Wikipedia-ELM Controversy: RBF (1980s) raised the similar idea of ELM. Dispute about the originality of ELM: Origins of ELM Portal of ELM python toolbox: hpelm Facts ELM is ⁽¹⁾\na type of single hidden layer feedforward neural network (SLFN).\nThe parameters (𝐰,b) between input layer and hidden layer are set randomly. Thus, for N input n-dimensional samples and L hidden nodes, the output of the hidden layer is $𝐇 = 𝐗_{N×n} 𝐖_{n×L}+𝐛\\_{n×L}$\nOnly the number of hidden nodes needs to be predefined manually without other hyper-parameters.\nThe output weights are initialized randomly and solved based on the pseudo inverse matrix in one-shot.\nFor a n-dimensional sample 𝐱ⱼ and its target 𝐭ⱼ=[tᵢ₁, tᵢ₂, \u0026hellip;, tᵢₘ]ᵀ∈ ℝᵐ, the output of ELM with L hidden nodes is 𝐨ⱼ = ∑ᵢ₌₁ᴸ 𝛃ᵢ g(𝐰ᵢᵀ⋅𝐱ⱼ + bᵢ), where g(⋅) is activation function; 𝛃ᵢ is the weights of the ith ouput unit: 𝛃ᵢ=[βᵢ₁, βᵢ₂, \u0026hellip;, βᵢₙ]ᵀ; 𝐰ᵢ is input weight: 𝐰ᵢ=[wᵢ₁, wᵢ₂, \u0026hellip;, wᵢₙ]ᵀ; 𝐱ⱼ is a n-dimensional input: 𝐱ⱼ=[xᵢ₁, xᵢ₂, \u0026hellip;, xᵢₙ]ᵀ∈ ℝⁿ; bᵢ is the bias of the ith hidden unit; 𝐨ⱼ is a m-dimensional vector: 𝐨ⱼ=[oᵢ₁, oᵢ₂, \u0026hellip;, oᵢₘ]ᵀ∈ ℝᵐ; The ideal parameters (𝐰,b,𝛃) should satisfy: ∑ᵢ₌₁ᴸ 𝛃ᵢ g(𝐰ᵢᵀ⋅𝐱ⱼ + bᵢ) = 𝐭ⱼ For total N samples, this mapping can be reforomalized with matrices: $𝐇\\_{N×L} \\pmb\\beta\\_{L×m} = 𝐓\\_{N×m}$, where\n𝐇 is the output of the hidden layer for N samples: $$𝐇(𝐰₁,...,𝐰_L, b₁,...,b_L, 𝐱₁,...𝐱_L) = \\\\\\ \\begin{bmatrix} g(𝐰₁⋅𝐱₁+b₁) \u0026 \\dots \u0026 g(𝐰_L⋅𝐱₁+b\\_L)\\\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots\\\\\\ g(𝐰₁⋅𝐱_N+b₁) \u0026 \\dots \u0026 g(𝐰_L⋅𝐱_N+b\\_L) \\end{bmatrix}_{N×L}$$ 𝛃 is the output weights matrix: [ 𝛃₁ᵀ ; \u0026hellip; ; 𝛃$\\_Lᵀ ]_{L×m}$\nTarget data: 𝐓 = $\\begin{bmatrix}𝐓₁ᵀ\\\\\\ \\vdots \\\\\\𝐓_Nᵀ\\end{bmatrix}_{N×m}$\nGenerally, $𝐇\\_{N×m}$ is not a square matrix (not invertible). Hence, 𝛃=𝐇⁻¹𝐓 cannot be applied. However, the optimal 𝛃 can be approached by minimizing the traning error iteratively: ∑ⱼ₌₁ᴺ‖𝐨ⱼ-𝐭ⱼ‖.\nBest estimation: $\\\\^𝐰ᵢ, \\\\^bᵢ$, ^𝛃ᵢ satisfy: ‖𝐇(^𝐰ᵢ, ^bᵢ)⋅^𝛃ᵢ- 𝐓‖ = min_{𝐰ᵢ, bᵢ, 𝛃ᵢ} ‖𝐇(𝐰ᵢ, bᵢ)⋅𝛃ᵢ- 𝐓‖, where i=1,\u0026hellip;,L\nLoss function: J = ∑ⱼ₌₁ᴺ (∑ᵢ₌₁ᴸ 𝛃ᵢ⋅g(𝐰ᵢ⋅𝐱ⱼ + bᵢ) - 𝐭ⱼ)²\nSolve 𝛃 based on the ∂J/∂𝛃=0, such that the optimal parameter is: ^𝛃 = $𝐇^† 𝐓$ = (𝐇ᵀ𝐇)⁻¹𝐇ᵀ 𝐓, where $𝐇^†$ is the Moore-Penrose inverse (Pseudo-inverse) of 𝐇. It can be proved that the norm of ^𝛃 is the smallest and unique solution (for a set of random (𝐰ᵢ, bᵢ)).\nMoore-Penrose inverse Also called pseudoinverse or generalized inverse ⁽²⁾.\n(bilibili search: \u0026ldquo;伪逆矩阵\u0026rdquo;) 深度学习-啃花书0103伪逆矩阵最小二乘\n(DDG search: \u0026ldquo;伪逆矩阵\u0026rdquo;)\n伪逆矩阵的意义及求法？ - 知乎\nnumpy.linalg.pinv() pinv(𝐗) = (𝐗ᵀ 𝐗)⁻¹ 𝐗ᵀ pinv(𝐗) 𝐗 = 𝐈 python之numpy之伪逆numpy.linalg.pinv - 千行百行 - CSDN Example Code This matlab code ⁽¹⁾ trains and tests a ELM on the NIR spectra dataset (regression) and the Iris dataset (classification).\nNote that each column is a sample, and each row is an attribute/feature. Notations:\nQ: number of samples R: input features S: output features $P\\_{R×Q}$: input pattern matrix $T\\_{S×Q}$: target data matrix N: number of hidden nodes TF: transfer function $IW\\_{N×R}$: input weights matrix $B\\_{N×Q}$: bias matrix $LW\\_{N×S}$: transposed output weights matrix Train (calculate the LW):\n$tempH\\_{N×Q} = IW\\_{N×R}⋅P\\_{R×Q} + B\\_{N×Q}$ $H\\_{N×Q} = TF(tempH)$ $LW\\_{S×N} = T\\_{S×Q}$⋅ pinv(H), based on: 𝛃$\\_{S×N} 𝐇\\_{N×Q} = 𝐓\\_{S×Q}$ Test:\n$tempH\\_{N×Q} = IW\\_{N×R}⋅P\\_{R×Q} + B\\_{N×Q}$ $H\\_{N×Q} = TF(tempH)$ $Y\\_{S×Q} = LW\\_{S×N}⋅H\\_{N×Q}$ Example code (py) Build an Extreme Learning Machine in Python | by Glenn Paul Gara \u0026hellip; searched by DDG: \u0026ldquo;incremental elm python\u0026rdquo;\nI-ELM incremental just means adding neurons?\ngithub\nOS-ELM On-line elm\nDeep incremental RVFL Deep incremental random vector functional-link network: A non-iterative constructive sketch via greedy feature learning\nReference 极限学习机(Extreme Learning Machine, ELM)原理详解和MATLAB实现 - 奔跑的Yancy - CSDN Moore-Penorse - Wikipedia (Back to Top)\n","date":"2023-01-31T15:49:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/c-sum-elm/","title":"sum: ELM"},{"content":"下一代国际华人青年学子面对面 第6期 2022年10月20日 周四\nOverview Research work Suggestions for graduate student (1st year) Research Contribution:\nNon-iterative learning strategy for training neural networks including single-layer networks, multi-layer networks, autoencoders, hierarchical networks, and deep networks. All the related publications in this category are my first-author papers\nThe proposed methods for pattern recognition related applications: Image Recognition, Video Recognition, Hybrid System Approximation, Robotics System Identification, EEG-brain Signal Processing. Most the related publications in this category are co-author papers with my HQPs.\nMy research works In the past 10 years, the works about Artificial neural networks：\nTheoretical Contributions to ANN Machine Learning based Applications Single-layer network with non-iterative learning Data Analysis and Robotics System Identification (Ph.D.) Hierarchical NN with Subnetwork Neurons Image Recognitions (Post Doctoral Fellow) Deep Networks without iterative learning Pattern Recognition (since 2018) Ⅰ. Single-layer network with non-iterative learning Starting with a small idea The labtorary mainly studies robots, control, mechanics. After 2008 Chinese Winter Storms, they got funding for creating Powerline De-icing robots.\nThe supervisor (Yaonan Wang): \u0026ldquo;Can you find a Neural Network for Identifying Robotics Dynamic Systems?\u0026rdquo; (in 2009 winter)\nLater, I found the following paper: \u0026ldquo;Universal approximation using incremental constructive feedforward networks with random hidden nodes\u0026rdquo;, By Huang, Guang-Bin et.al (Cannot be found on IEEE) version on elm portal\nWhat is the Neural Network? Single hidden layer feedforward NN:\nflowchart BT subgraph in[Input Layer] x1((1)) \u0026 xe((\"...\")) \u0026 xn((n)) end subgraph hid[Hidden Layer] h1((\"𝛂₁,♭₁,𝛃₁\")) \u0026 h2((\"𝛂₂,♭₂,𝛃₂\")) \u0026 he((\"......\")) \u0026 hL((\"𝛂L,♭L,𝛃L\")) end subgraph out[Output Layer] y1((1)) \u0026 ye((\"...\")) \u0026 ym((m)) end x1 \u0026 xn --\u003e h1 \u0026 h2 \u0026 hL --\u003e y1 \u0026 ym Output of additive hidden neurons: G(𝐚ᵢ, bᵢ, 𝐱) = g(𝐚ᵢ⋅𝐱+bᵢ) Output of RBF hidden nodes: G(𝐚ᵢ, bᵢ, 𝐱) = g‖𝐱-𝐚ᵢ‖ The output function of SLFNs is: fₗ(𝐱) = ∑ₗ₌₁ᴸ 𝛃ᵢ⋅G(𝐚ᵢ, bᵢ, 𝐱) Network training Advantage: Approximate unknown system through learnable parameters.\nMathematical Model:\nApproximation capability: Any continuous target function f(x) can be approximated by Single-layer feedforward network with appropriate parameters (𝛂,♭,𝛃). In other words, given any small positive value e, for SLFN with enough number of hidden nodes, we have: ‖f(𝐱)-fₗ(𝐱)‖ \u0026lt; e\nIn real applications, target function f(𝐱) is usually unknown. One wishes that unknown f could be approximated by the trained network fₗ(𝐱).\nWhat is Extreme Learning Machine? Feed forward random network without using BP to train, such that it has a good real-time performance. And it fits the real-time robot task exactly.\nB-ELM (2011) \u0026ldquo;Bidirectional ELM for regression problem and its learning effectiveness\u0026rdquo;, IEEE Trans. NNLS., 2012 paper\n(23-02-10) This the inception of his subnetwork series work, and I was supposed to read this brief firt. paperNote\nMotivation\nOriginal ELM has 3 kinds of parameters: 𝐚 is called the \u0026ldquo;input weights\u0026rdquo;, b is the bias, 𝛃 is the \u0026ldquo;output weights\u0026rdquo;, which are consistent with earlier feedfoward network, though current single-layer feedfoward network has removed the 𝛃.\nThe 1st layer in ELM is generated randomly and the 2nd layer is constructed based on Moore-Penrose inverse without iteration.\nflowchart LR in((X)) --\u003e lyr1[\"Layer1:\\n 𝐚ⁿ, b\\n Random\\n Generated\"] --\u003e weightSum1((\"X⋅𝐚ⁿ + b\")) --\u003e act[Activation\\n Function\\n g] --\u003e z((\"g(X⋅𝐚ⁿ+b)\")) --\u003e lyr2[\"Layer2:\\n 𝛃\"] --\u003e weightSum2((\"O =\\n 𝛃⋅g(X⋅𝐚ⁿ+b)\\n =T\")) -.-\u003e|\"𝛃 =\\n g(X⋅𝐚ⁿ+b)⁻¹T\"| lyr2 classDef lyrs fill:#ff9 class lyr1,act,lyr2 lyrs; Bidirectional ELM\nIn original ELM, the 𝐚ⁿ,b are random numbers, but they can be yield if pulling the error back further, that is doing twice more inverse computation. Therefore, in order to calculate the 𝐚ⁿ,b, there are 3 times inverse computation: for output weights 𝛃, activation function g(⋅) and activation z (X⋅𝐚ⁿ+b) respectively.\n%%{ init: { 'flowchart': { 'curve': 'bump' } } }%% flowchart LR in((X)) --\u003e lyr1[\"Layer1:\\n 𝐚ⁿ, b\\n Random\\n Generated\"] --\u003e weightSum1((\"X⋅𝐚ⁿ + b\")) --\u003e act[Activation\\n Function\\n g] --\u003e z((\"g(X⋅𝐚ⁿ+b)\")) --\u003e lyr2[\"Layer2:\\n 𝛃\"] --\u003e out((\"O =\\n 𝛃⋅g(X⋅𝐚ⁿ+b)\\n =T\")) -.-\u003e|\"𝛃 =\\n g(X⋅𝐚ⁿ+b)⁻¹T\"| lyr2 classDef lyrs fill:#ff9 class lyr1,act,lyr2 lyrs; out -.-\u003e |\"X⋅𝐚ⁿ+b =\\n g⁻¹ T 𝛃⁻¹\"|weightSum1 out -.-\u003e |\"𝐚ⁿ =\\n X⁻¹ (g⁻¹ T 𝛃⁻¹ -b ),\\n b = mse(O-T)\"|lyr1 𝛃 = [g(X⋅𝐚ⁿ+b)]⁻¹T X⋅𝐚ⁿ+b = g⁻¹ T 𝛃⁻¹ 𝐚ⁿ = X⁻¹ (g⁻¹ T 𝛃⁻¹ -b ), and b = mse(O-T) The error is surprisingly small even with few hidden nodes. Compared with the original ELM, the required neurons in this method are reduced by 100-400 times, and the testing error reduced 1%-3%, and also the training time reduced 26-250 times over 10 datasets.\nMajor differences\nRandomized Networks Bidirectional ELM Classifier ELM; Echo State Netowrk;\nRandom Forest;\nVector Functional-link Network Only works for regression task Performance Similar performance;\nFaster speed;\nLess required neurons Learning strategy (Semi-)Randomized input weights;\nNon-iterative training;\nSingle-layer network Non-iterative training;\nSingle-layer network;\nCalculated weights in a network Ⅱ. Hierarchical NN with Subnetwork Neurons Single-Layer Network with Subnetwork Neurons In 2014, deep learning is becoming popular. How to extend the B-ELM as a multi-layer network? \u0026ldquo;Extreme Learning Machine With Subnetwork Hidden Nodes for Regression and Classification\u0026rdquo;. paper; paperNote\nMotivation\nflowchart TB base[Bidirectional ELM] --\u003e theory[Theoretical Contributions on ANN] \u0026 app[Industrial Applications] theory --\u003e multilyr[1. Two-layer Neural Networks?\\n 2. Hierarchical Neural Networks?] app --\u003e tasks[1. Feature Extraction\\n 2. Dimension Reduction\\n 3. Image Recognition] Pull the residual error back to multiple B-ELMs sequentially:\nflowchart LR in((X)) --\u003e lyr11 \u0026 lyr21 \u0026 lyrN1 subgraph net1[B-ELM 1] lyr11[\"𝐚¹,b¹\"] ==\u003e|\"X⋅𝐚¹+b¹\"| act1[g] ==\u003e|\"g(X⋅𝐚¹+b¹)\"|lyr12[\"𝛃¹\"] ==\u003e|\"𝛃¹⋅g(X⋅𝐚¹+b¹)\"| out((\"O ➔ T\\n E=T\")) -.-\u003e lyr12 -.-\u003e act1 -.-\u003e lyr11 lyr11 --\u003e act1 --\u003e lyr12 end lyr12 --\u003e out1((\"O¹➔T\\n E=T-O¹\")) -.-\u003e lyr22 subgraph net2[B-ELM 2] lyr22[\"𝛃²\"] -.-\u003e act2[g] -.-\u003e lyr21[\"𝐚²,b²\"] lyr21 --\u003e|\"X⋅𝐚² + b²\"|act2 --\u003e|\"g(X⋅𝐚²+b²)\"|lyr22 end lyr22--\u003e out2((\"O²+O¹➔T\\n E=T-(O²+O¹)\")) out2 -.-\u003e|\"Solve\\n multiple\\n B-ELMs\"| outn-1((\"E=T-∑ᵢ₌₁ᴺ⁻¹ Oⁱ\"))-.-\u003elyrN2 %%out2 -.-\u003e lyrn2 %%subgraph nets[multiple B-ELMs] %%lyrn2[\"𝛃ⁿ\"] -.-\u003e actn[g] -.-\u003e lyrn1[\"𝐚ⁿ,bⁿ\"] %%lyrn1 --\u003e|\"X⋅𝐚ⁿ + bⁿ\"|actn --\u003e|\"g(X⋅𝐚ⁿ+bⁿ)\"|lyrn2 %%end %%lyrn2 --\u003e outn-1((\"E=T-∑ᵢ₌₁ᴺ⁻¹ Oⁱ\"))-.-\u003elyrN2 subgraph netN[\"B-ELM N\"] lyrN2[\"𝛃ᴺ\"] -.-\u003e actN[g] -.-\u003e lyrN1[\"𝐚ᴺ,bᴺ\"] lyrN1 --\u003e|\"X⋅𝐚ᴺ + bᴺ\"|actN --\u003e|\"g(X⋅𝐚ᴺ+bᴺ)\"|lyrN2 end lyrN2--\u003e outN((\"∑ᵢ₌₁ᴺ Oⁱ➔T\")) classDef lyrs fill:#ff9 class lyr11,act1,lyr12,lyr21,act2,lyr22,lyrN1,actN,lyrN2 lyrs; linkStyle 9,10,11,15,16,17,22,23,24 stroke:#0af,stroke-width:3px %%classDef node font-size:20px; Dotted links are computation with inverse. Cyan links is the second feedforward using the updated parameters to give a trustworthy result O¹. The objective is to approach the target T, so there is a residual error E=T-O¹. Then another B-ELM (with same structure) is used to reduce the error continuously. And this time, the prediction is O²+O¹, which is the approximation of T. Here, the residual error is E=T-(O²+O¹)\nRepeatedly pulling the residual error to a new B-ELM N times is equivalent to N SLFNs. But B-ELM is fast without iteration and less computation with a few hidden nodes in each SLFN.\nBased on original SLFN structure, each node contains a SLFN.\nTwo-Layer Network with Subnetwork Neurons (2015) How to extend the Single-layer network with subnetwork nodes system to a two-layer network system?\nA general two-layer system was built in paper: \u0026ldquo;Multilayer Extreme Learning Machine with Subnetwork Hidden Neurons for Representation Learning\u0026rdquo; paper; paperNote\nThough it only contains two \u0026ldquo;general\u0026rdquo; layers, this system includes hundreds of networks, and it\u0026rsquo;s fast due to the modest quantity and no iteration.\nCompared with ELM and B-ELM, it got better performance over 35 datasets:\nClassification (vs ELM) Regression (vs B-ELM) Required Neurons Reduced 2-20 times Reduced 13-20% Metrics Accuracy increase 1-17% Testing error reduced 1-8% Training Speed faster 25-200times faster 5-20% Two-layer network system can perform image compression or reconstruciton, etc. This method is better than Deep Belief Network on small datasets. But it\u0026rsquo;s inferior than deep learning with transfer learning technics on huge datasets.\nHierarchical Network with Subnetwork Neurons \u0026ldquo;Features combined from hundreds of mid-layers: Hierarchical networks with subnetwork nodes\u0026rdquo; IEEE Trans. NNLS, 2019. paper\nFrom a single-layer network with subnetwork neurons to the multi-layer network, and then to a neural network system, these 3 papers cost 5 years or so.\nCompared with deep learning network, it\u0026rsquo;s extremely fast and performs well on small datasets, like Scene15, Caltech101, Caltech256. But for large datasets, deep learning is the winner.\n\u0026ldquo;Somewhat regretfully, I turned to deep learning a bit late. But been hesitant to do research along this approach.\u0026rdquo;\nMajor differences between ours and Deep Networks\nSGD based methods in DCNN Moore-Penrose inverse matrix based methods Hyper params lr; momentum; bs; L2 regulariation; epochs L2 regularization (non-sensitive) Performance higher performance in Computer Vision tasks (with huge datasets);\nGPU-based computation resource;\nMore parameters;\nMore required training time Faster learning speed/less tuning;\nPromising performance in Tabular Datasets;\nLess over-fitting problem. Ⅲ. Deep Networks without iterative learning Since 2018: How to combine the non-iterative method (M-P inverse matrix) with deep convolutional network to gain advantages? This took 2-3 years.\nThis is the age of Deep Learning.\nInteresting 20 years of cycles\nRosenblatt\u0026rsquo;s Perceptron proposed in mid of 1950s, sent to \u0026ldquo;winter\u0026rdquo; in 1970s. Back-Propagation and Hopfield Network Proposed in 1970s, reaching research peak in mid of 1990s. Support vector machines proposed in 1995, reaching research peak early this century. There are exceptional cases:\nMost basic deep learning algorithms proposed in 1960s-1980s, becoming popular only since 2012 (for example, LeNet proposed in 1989). ImageNet pushed deep learning, because only when the huge network structure of deep learning meets the matched huge dataset, it can achive good performance.\nThe success of deep learning enlist three factors: 1. NN structure and algorithm; 2. Big data; 3. GPU availability.\nHundreds of layers result in tedious training time. \u0026ldquo;The study intensity is infinitely small and the study duration is infinitely large.\u0026rdquo;\n\u0026ldquo;The improvement space of deep neural network is limited. So can we introduce non-iterative learning strategies for training deep networks\u0026rdquo;\nTraining speed is more important for scientific research than accuracy. And also it\u0026rsquo;s necessary to reduce the dependence on GPUs and the involvement of undeterministic hyper parameters (lr,bs,\u0026hellip;)\nRetraining DCNN with the non-iterative strategy (2019): \u0026ldquo;Recomputation of the dense layers for the performance improvement of DCNN.\u0026rdquo; IEEE TPAMI, 2020. link\nMotivation\ngraph LR base[\"1. Two-layer Neural Networks\\n 2. Hierarchical Neural Networks\"] --\u003e explore[\"1. Deep learning withe non-iterative method\"] In a DCNN, the first few layers are convolutional layers, maxpooling, then there\u0026rsquo;re 3 or 1 dense layer.\nIf I cannot train all of the hundreds layers in my non-iterative method, can I train only certain layers that are easy trained with my method, rather the SGD?\nOnly the fully-connected layers are trained by non-iterative method (inverse matrix), and the rest of layers are trained by gradient descent (SGD, SGDM, Adam).\nOn some medium-size datasets(CIFAR10, CIFAR100, SUN397), this approach brought a moderate improvement because there are only 3 dense layer out of a 50/100-layer network (most of layers are trained with SGD), but speeds up the training.\nOne layer can be trained within 1-2 seconds.\n","date":"2023-01-27T10:19:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/","title":"Watch: SNN - 杨易旻 | WeChat Live"},{"content":"IEEE Cybernetics(2015-11-02) | Google Drive | G.Scholar\nThis is the first paper of ELM with subnetwork nodes by Yimin Yang. The second paper in the series is MltLyr ELM with subnetwork nodes The outline of Yang\u0026rsquo;s research works: Yang-WeChatLive-20221020; Abstract Learning effectiveness and speed of SLFN are bottleneck. ELM is fast. Grow subnetwork nodes by pulling back residual network error to the hidden layer. Better generalization performance with fewr hidden nodes. Ⅰ. Introduction Bring out the subject: FNN (universial approximator) ➔ SLFN\nWhat is an SLFN? Input layer + hidden layer + output layer\nMath description: For N arbitary distinct samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, where 𝐱ᵢ∈ 𝐑ⁿ and 𝐭ᵢ∈ 𝐑ᵐ, the network output is:\n$𝐟\\_L(𝐱)$ = ∑ᵢ₌₁ᴸ 𝛃ᵢh(𝐚ᵢ⋅𝐱ⱼ + bᵢ) = ∑ᵢ₌₁ᴸ 𝐇ᵢ⋅𝛃ᵢ, j=1,\u0026hellip;,N (1)\nSLFN output is the weighted sum of 𝐿 hidden nodes (perceptrons) with the factor 𝛃. The ith perceptron receives the weighted sum of 𝑁 inputs through its parameters (𝐚ᵢ, bᵢ), 𝐚ᵢ∈ 𝐑ⁿ, b∈ 𝐑, and performs activation function h. Its contribution ratio to the all output nodes is 𝛃ᵢ. ELM traits: NN (all params are adjustable) ➔ partial random networks ➔ ELM is a full-random learning method, where the input weights and bias (𝐚, b) are generated randomly and independent of training data. (Will the Glorot normalization has no effect?)\nELM advantages: An unification of FNNs and SVM/LS-SVM\nELM application: CV, da,\u0026hellip;, online learning\nProblems:\nThe choice of the regularization parameter C which affects the generalization performance of ELM mainly relies on trial-and-error method.\nHow many neurons should be used in ELM. Although Huang suggested to use more than 1000 hidden nodes, whether the number of hidden nodes can be further reduced without affecting learning effectiveness for large-size/high dimension data set. Several improved ELM methods, like B-ELM pulls the network residual error back to the hidden layer but it only works for regression task, and other methods bring a higher computation complexity when compared to standard ELM.\nSolution: Growing subnetwork hidden nodes to the exisiting network by pulling back the network residual error to hidden layers. A hidden node itself can be formed by several hidden nodes.\nContributions:\nFaster than BP, SVM and other ELMs and compatible to regreesion and classification problems. The regularized parameter C do not affect the generalization performance of this method. This method with m hidden nodes (the desired output dimensionality) can achieve better training accuracy than the original ELM with a large number of hidden nodes. Ⅱ. Definitions and Basic-ELM A. Notations and Definitions 𝐑 : set of real numbers\n{(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ : N arbitrary distinct samples,\n𝐱ᵢ = [xᵢ₁,xᵢ₂,\u0026hellip;,xᵢₙ]ᵀ : n-dim input data, 𝐱ᵢ∈ 𝐑ⁿ is a column vector;\n𝐭ᵢ : m-dim desired output data, 𝐭ᵢ∈ 𝐑ᵐ\n𝐱 : input data matrix, 𝐱=[𝐱₁,\u0026hellip;𝐱_N], 𝐱ᵢ∈ 𝐑ⁿᕁᴺ\n𝐭 : desired output data matrix, 𝐭=[𝐭₁,\u0026hellip;𝐭_N], 𝐭∈ 𝐑ᵐᕁᴺ\n(^𝐚ₙ,^bₙ) : parameters of the 𝑛th subnetwork hidden node, ^𝐚ₙ∈ 𝐑ⁿᕁᵐ, ^bₙ∈ 𝐑 (suppose the number of hidden nodes equals to the output dimension m, thus the mapping is from n to m.)\n^𝐚ₙ = [𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ], n×m weights matrix (for a n-dimension sample 𝐱ᵢ), 𝐚ₙₘ∈ 𝐑ⁿ\n𝐞ₙ : residual error of current network output 𝑓ₙ with 𝑛 hidden nodes (for N samples), i.e., 𝐞ₙ=𝐭-𝑓ₙ, 𝐞ₙ∈ 𝐑ᵐᕁᴺ.\n𝐇 : output matrix of the hidden layer (of SLFN) for tarining set {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐇 = [h(𝐱₁),\u0026hellip;,h(𝐱_N)]ᵀ, 𝐇∈ 𝐑ᴺᕁᵐ, 𝐇 = g(𝐱ᵀ𝐚+𝐛)???\nh(𝐱) : activation function. ELM feature mapping (or Huang\u0026rsquo;s transform)\n𝐇ᵢ : the 𝑖th hidden node output w.r.t. inputs, i.e., the 𝑖th column of 𝐇\n𝐈 : unit matrix\nsum(𝐞) : the sum of all elements of the matrix 𝐞\nB. Basic-ELM ELM is proposed for single-hidden-layer feedforward networks (SLFNs).\nThe output function of ELM with L hidden nodes for SLFNs is:\n$𝑓\\_L(𝐱)$ = ∑ᵢ₌₁ᴸ βᵢ⋅h(𝐚ᵢ⋅𝐱ⱼ + bᵢ) = ∑ᵢ₌₁ᴸ 𝐇ᵢ⋅𝛃ᵢ, j=1,\u0026hellip;,N.\nwhere h(⋅) denotes an activation function, (𝐚ᵢ, bᵢ), 𝐚ᵢ∈ 𝐑ⁿ, bᵢ∈ 𝐑, denotes the ith hidden node parameters, and 𝛃ᵢ is the ith output weight between the ith hidden node and the output nodes.\nBased on Bartlett\u0026rsquo;s theory, ELM theory aims to reach not only the smallest training error, but also the smallest norm of output weights (Least square-least norm solution, where the regularization makes an invertible matrix, such that a special solution can be determined.):\nMinimize: ‖𝛃‖² + C⋅‖𝐇𝛃 - 𝐭‖²\n\u0026ldquo;then the generalization performance depends on the size of weights rather than the number of nodes.\u0026rdquo;\nLemma 1 (proved by Huang):\nGiven an SLFN with nonconstant piecewise continuous hidden nodes 𝐇(𝐱, 𝐚, b), then for any continuous target function 𝑓 and any function sequence 𝐇ₙʳ(𝐱) = 𝐇(𝐱, 𝐚ₙ, bₙ) randomly generated based on any continuous sampling distribution,\nlim$\\_{n➝∞}$ ‖𝑓 - (𝑓ₙ₋₁ + 𝐇ₙʳ⋅𝛃ₙ)‖ = 0\nholds with probabitliy 1 if: 𝛃ₙ = ⟨𝐞ₙ₋₁, 𝐇ₙʳ⟩ / ‖𝐇ₙʳ‖² (the weight of the 𝑛th node)\nwhere\n\u0026ldquo;⟨ , ⟩\u0026rdquo; stands for \u0026ldquo;dot product\u0026rdquo; (Frobenius inner product) of two matrices and is a scalar. n is the number of hidden nodes in the hidden layer. 𝐞ₙ₋₁ is the residual error of the last iteration, i.e., when there were n-1 hidden nodes. 𝐇ₙʳ is the output matrix of the current hidden layer (activated but havn\u0026rsquo;t scaled by 𝛃). Intuitively, as the residual error reduces, the weight of the newer node gets smaller.\nⅢ. Proposed ELM Method With Subnetwork Hidden Nodes A. Structure of the Proposed Method Motivations:\nSelecting an appropriate number of neurons can resort to optimization algorithms. The generalization performance depends on the size of the weights rather than the number of weights. Inspiration:\n\u0026ldquo;A hidden node itself can be a subnetwork formed by several nodes. And these subnetwork hidden nodes and output weights itself should be the smallest norm, and also aim to reach the smallest training error.\u0026rdquo;\nObjectives:\nGiven N training samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐱ᵢ∈ 𝐑ⁿ, 𝐭ᵢ∈ 𝐑ᵐ, generated from the same continuous system, if activation function h is invertible, the objectives are:\n‖𝐞ₙ₋₁‖ ≥ ‖𝐞ₙ₋₁ - h(^𝐚ₙ,𝐱)‖ ≥ ‖𝐞ₙ₋₁ - 𝐇ₙ‖ ≥ ‖𝐞ₙ₋₁ - 𝐇ₙ⋅𝛃ₙ‖ (residual error is decreasing.) ‖h(^𝐚ₙ,𝐱) - 𝐞ₙ₋₁‖ = min_{𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ} ‖h(𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ) - 𝐞ₙ₋₁‖ (minimize weights inside nodes) ‖𝐇ₙ⋅^𝛃ₙ - 𝐞ₙ₋₁‖ = min_{𝛃} ‖𝐇ₙ⋅𝛃 - 𝐞ₙ₋₁‖ (minimize the weights outside nodes) where\n^𝐚ₙ and ^𝛃ₙ are the optimal (the ultimate status) parameters with the smallest norm among all the least squares solutions. 𝐇ₙ = h(^𝐚ₙ, ^bₙ, 𝐱) is the output of the nth hidden node with the optimal parameters. If activation function h is invertible, subnetwork hidden nodes in SLFN can be calculated by pulling back network residual error to hidden layers.\nFor example, with sine function as the activation function, training a subnetwork hidden node (^𝐚) is equivalent to finding a least-square solution ^𝐚ₙ (letting the derivative of MSE=0) with the least norm for the linear system:\n[𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ]⋅𝐱 = arcsin(𝐞ₙ₋₁), 𝐞ₙ₋₁∈ (0,1],\nsuch that the optimal ^𝐚ₙ satifies:\n‖sin(^𝐚ₙ, 𝐱) - 𝐞ₙ₋₁‖ = min_{𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ} ‖sin(𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ, 𝐱) - 𝐞ₙ₋₁‖,\nThat means the output of the nth \u0026ldquo;subnetwork hidden node\u0026rdquo; ^𝐚ₙ is approaching the residual error 𝐞ₙ₋₁ of the last status.\nThe input weights ^𝐚ₙ of a node for this model is a matrix (instead of a vector), beacuse each \u0026ldquo;subnetwork (general) hidden node\u0026rdquo; contains a standard SLFN (several hidden nodes) internally.\nDifferences with standard ELM\nELM with subnetwork standard ELM hidden node m neurons: $𝐚_f∈ 𝐑ⁿᕁᵐ, 𝐛_f∈ 𝐑ᵐ$ single neuron:\n𝐚∈ 𝐑ⁿ, b∈ 𝐑 construct calculated generated randomly # hidden nodes L x m (m ⟂ L, m = #output dim) L B. Proposed Method Lemma 2: Given a bounded nonconstant piecewise continuous activation function h, there is: lim$\\_{(𝐚,b)→(𝐚₀,b₀)}$ ‖h(𝐚⋅𝐱+b) - h(𝐚₀⋅𝐱+b₀)‖ = 0 (连续性)\nTheorem 1: Given N arbitrary distinct samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐱ᵢ∈ 𝐑ⁿ, 𝐭ᵢ∈ 𝐑ᵐ, a sigmoid or sine activation function h, and then for any continuous desired outputs 𝐭, the limit of error converges to 0:\nlim$\\_{n➝∞}$ ‖ 𝐭-{ u⁻¹(h(^𝐚⋅𝐱+b))} ‖ = 0\nProof:\nProve the sequence ‖𝐞ₙ‖ is decreasing with 0 as the lower bound and it converges.\nFor the 𝑛th subnetwork hidden node containing m hidden nodes, the linear mapping is:\n𝛌ₙ = [𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ]⋅𝐱, 𝛌ₙ∈ 𝐑ᵐ\nThen 𝛌ₙ passes through the activation function. Because the target is error, which should become 0 at the end, the error at present is the output of activation function:\n𝐞ₙ₋₁ = h(𝛌ₙ) ∈ 𝐑ᵐ\nThe inverse function of h is h⁻¹, and its input value should range from (0,1]. Therefore, if trying to solve 𝛌ₙ from 𝐞ₙ₋₁, every element in 𝐞ₙ₋₁ should be scaled to the range of (0,1] by the normalized function u(⋅). Then, 𝛌ₙ can be calculated through:\n𝛌ₙ = h⁻¹(u(𝐞ₙ₋₁))\nFurther, the input weights of this subnetwork hidden node can be solved:\n$$\\\\^𝐚ₙ = [𝐚ₙ₁,...,𝐚ₙₘ] = h⁻¹(u(𝐞ₙ₋₁))⋅𝐱⁻¹$$ For different activation functions, there will be:\n$$\\rm \\\\{^{\\\\^𝐚ₙ = arcsin(u(𝐞ₙ₋₁))⋅𝐱⁻¹,\\quad sine} \\_{\\\\^𝐚ₙ = -log( (1/u(𝐞ₙ₋₁)) - 1)⋅𝐱⁻¹,\\quad sigmoid}$$ (This work is a continuation on ELM, that is once the 𝛃 calculated based on target 𝐭 and 𝐇⁻¹, the residual error is also fixed, so it serves as the target for 𝐚,b. Still, applying least-square, the optimial a can be calculated based on \u0026ldquo;target\u0026rdquo; e and 𝐱⁻¹.)\n^b is the mean of hidden nodes.\nThe error can be reduced by adding the bias\nDo feedforward using the calculated ^𝐚ₙ and ^bₙ, so this time the output of the hidden layer is:\n$$\\\\^𝐇ₙᵉ = u⁻¹(h(\\\\^𝐚ₙ⋅𝐱+\\\\^bₙ))$$Because 𝐞ₙ₋₁ is the last output of the activation fuction, 𝐞ₙ₋₁ and $\\\\^𝐇ₙᵉ$ are the same things. So they can subtract from each other. Then the residual error for this time is:\nΔ = ‖𝐞ₙ₋₁‖² - ‖𝐞ₙ₋₁ - ^𝐇ₙᵉ‖² = ‖𝐞ₙ₋₁‖² - (‖𝐞ₙ₋₁‖² - 2‖𝐞ₙ₋₁‖‖^𝐇ₙᵉ‖ + ‖^𝐇ₙᵉ‖²) = 2‖𝐞ₙ₋₁‖‖^𝐇ₙᵉ‖ - ‖^𝐇ₙᵉ‖² = 2⟨𝐞ₙ₋₁, ^𝐇ₙᵉ⟩ - ‖^𝐇ₙᵉ‖² = ‖^𝐇ₙᵉ‖² ( 2⟨𝐞ₙ₋₁, ^𝐇ₙᵉ⟩/‖^𝐇ₙᵉ‖² - 1 )\nΔ is ≥ 0\nProve the limit converges to 0 when n tends to infinity.\nThe target value is approximated while the error is decreased.\nThe final estimation is the summation of the ouput of d subnetwork hidden nodes\nThe VC dimension is lower than standard ELM, i.e., the dimension of feature space m≪ L, so the generalization ability of this method is better.\n(Back to top)\n","date":"2023-01-20T11:46:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-elm-subnet/","title":"Read: Optim - SLFN | ELM with SNN"},{"content":"Authors: Yimin Yang, and Q. M. Jonathan Wu IEEE Cybernetics; Publish Date: 2015-10-09.\nThis is the 2nd paper in his series, and the first paper is this.\n(感觉Intro写得不错，逻辑性强，信息量大；但后面method部分好多typo)\nAbstract Representation learning of multilayer ELM with subnetwork nodes outperform conventional feature learning methods.\nI. Introduction model performance ➔ data representaiton/features ➔ processing pipelines design and data transformations ➔ data representation ➔ effective learning\nFeature reduction and extraction techniques can be conducted in a supervised, unsupervised or semi-supervised manner.\nELMs learn representations of data to extract useful information when building classifiers or predictors.\nELMs provide a unified learning framework for \u0026ldquo;generalized\u0026rdquo; single-hidden layer feedforward NNs (SLFNs).\nIn ELM methods, the hidden layer parameters of NN need not be tuned during training, but generated randomly. ML-ELM is adding (multiple) general hidden nodes (subnetwork nodes) to existing single-hidden-layer ELM networks.\nA versatile platform with faster speed and better generalization performance on feature extraction. Its generalization performance is not sensitive to the parameters of the networks in the learning process. ML-ELM has universal approximation capability and representation learning. II. Preliminaries and Basic-ELM A. Notations 𝐑 : set of real numbers {(𝐱ᵢ,𝐲ᵢ)ᵢ₌₁ᴹ} (𝐱ᵢ∈ 𝐑ⁿ,𝐲ᵢ∈ 𝐑ᵐ) : M arbitrary distinct samples, 𝐱 : an input data matrix 𝐱∈ 𝐑ⁿᕁᴹ 𝐲 : desired output data matrix 𝐲∈ 𝐑ᵐᕁᴹ 𝛂ᵢ : weight vector connecting the 𝑖th hidden nodes and the input nodes ♭ᵢ : bias of the 𝑖th hidden nodes βᵢ : output weight between the 𝑖th hidden node and the output node 𝐞 : residual error of current network output, i.e., 𝐞=𝐲-𝐟 𝐈 : unit matrix sum(𝐞) : the sum of all elements of the matrix 𝐞 g : sigmoid or sine activation function (TABLE 1)\n(𝛂,♭) : a hidden node (in basic ELM) (𝐚,𝑏) : a general hidden node (or subnetwork node) ^𝐚ʲ_f : input weight of the jth general hidden node in feature mapping layer. ^𝐚ʲ_f∈ 𝐑ᵈᕁⁿ ^bʲ_f : bias of the 𝑗th general hidden node in feature mapping layer ^bʲ_f∈ 𝐑 (𝛂ᵢʲ_f,♭ʲ_f) : the 𝑖th general hidden node in the 𝑗th general hidden node. (^𝐚ₕ,^𝑏ₕ) : hidden nodes in ELM learning layer and ^𝐚ₕ∈ 𝐑 ᵐᕁᵈ uⱼ : normalized function in the 𝑗th general node, uⱼ(⋅):𝐑 ➔ (0,1], uⱼ⁻¹ represent its reverse function 𝐇ʲ_f : feature data generated by 𝑗general nodes in a feature mapping layer, i.e., 𝐇ʲ_f = ∑ᵢ₌₁ʲ uᵢ⁻¹ ⋅ g(𝐱, ^𝐚ⁱ_f, ^bⁱ_f), 𝐇ʲ_f∈ 𝐑ᵈᕁᴹ 𝐇ʲⁱ_f : feature data generated by the 𝑖th feature mapping layer M : number of training samples n : input data dimension m : output data dimension d : feature data dimension 𝐞_L : the residual error of current two-layer network (L general nodes in the first layer and (𝐚ₕ,𝑏ₕ) in the second layer) 𝐞ʲ_L : the residual error of current two-layer network (L general nodes in the first layer and 𝑗general nodes in the second layer) L : the numbers of general hidden nodes B. Basic-ELM The output function of ELM for SLFNs fed with input matrix 𝐱 is: fₙ(𝐱)=∑ᵢ₌₁ⁿ βᵢ g(𝐱, 𝛂ᵢ, ♭ᵢ).\n\u0026ldquo;ELM theory aims to reach the smallest training error but also the smallest norm of output weights\u0026rdquo; (regularization term?), so the objective is to minimize: ‖βᵢ‖ₚᶣ¹ + C⋅‖∑ᵢ₌₁ⁿ βᵢ g(𝐱, 𝛂ᵢ, ♭ᵢ) - 𝐲‖ᶣ²_q, i=1,\u0026hellip;,n. (ᶣ signifies μ)\nwhere μ₁\u0026gt;0, μ₂\u0026gt;0, p,q = 0, ½, 1, 2, \u0026hellip;, +∞, C is a positive value, g(𝐱, 𝛂, ♭) is referred to as ELM feature mapping (linear projection+activation) or Huang\u0026rsquo;s transform.\n(Convergence proved by Huang et al.)\nLemma 1: Given M aribitrary distinct samples {(𝐱, 𝐲)}, 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ sampled from a continuous system, an activation function g, then for any continous target function 𝐲 and any function sequence g(𝐱, 𝛂ₙʳ, ♭ₙʳ) randomly generated based on any continuous sampling distribution, lim_{n➝∞} ‖𝐲-（fₙ₋₁ + g(𝐱, 𝛂ₙʳ, ♭ₙʳ)）‖=0 holds with probabiltiy one if βₙ = ⟨𝐞ₙ₋₁, g(𝐱, 𝛂ₙʳ, ♭ₙʳ)⟩ / ‖g(𝐱, 𝛂ₙʳ, ♭ₙʳ)‖²,\nwhere (𝛂ₙʳ, ♭ₙʳ) represesnts the 𝑛th random hidden node, and 𝐞ₙ₋₁ = 𝐲-fₙ₋₁\nIII. Proposed Method A. ELM With Subnetwork Nodes A hidden node can be a subnetwork formed by several hidden nodes. Hence, a single mapping layer can contain multiple networks.\nComparision of the feature mapping layer:\nflowchart LR subgraph A[basic ELM] direction BT x1[\"x₁\"]--\u003e h1((\"𝛂₁,♭₁, β₁\")) \u0026 he1((...)) \u0026 hL((\"𝛂L,♭L, βL\")) xe[x...]--\u003e h1((\"𝛂₁,♭₁, β₁\")) \u0026 he1((...)) \u0026 hL((\"𝛂L,♭L, βL\")) xn[\"xₙ\"]--\u003e h1((\"𝛂₁,♭₁, β₁\")) \u0026 he1((...)) \u0026 hL((\"𝛂L,♭L, βL\")) h1 --\u003e y1[\"y₁\"] \u0026 ye[...] \u0026 ym[\"yₘ\"] he1--\u003e y1[\"y₁\"] \u0026 ye[...] \u0026 ym[\"yₘ\"] hL --\u003e y1[\"y₁\"] \u0026 ye[...] \u0026 ym[\"yₘ\"] subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 he1 \u0026 hL end end subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 he1 \u0026 hL end subgraph B[ELM with subnetwork nodes] direction BT x1_[\"x₁\"] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL xe_[x...] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL xn_[\"xₙ\"] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL ghn1--\u003e y1_[\"y₁\"] \u0026 ye_[...] \u0026 ym_[\"yₘ\"] ghne--\u003e y1_[\"y₁\"] \u0026 ye_[...] \u0026 ym_[\"yₘ\"] ghnL--\u003e y1_[\"y₁\"] \u0026 ye_[...] \u0026 ym_[\"yₘ\"] end subgraph ghn1[\"^𝛂¹_f, ^♭¹_f, with weight u₁⁻¹\"] direction TB n11((\"𝛂¹_f1,\\n ♭¹_f1\")) \u0026 n1e((...)) \u0026 n1m((\"𝛂¹_fm,\\n ♭¹_fm\")) end subgraph ghne[\"general hidden nodes\"] direction TB ne1((1)) \u0026 nee((...)) \u0026 nem((m)) end subgraph ghnL[\"^𝛂ᴸ_f, ^♭_f, with weight u\\_L⁻¹\"] direction TB nL1((\"𝛂ᴸ_f1,\\n ♭ᴸ_f1\")) \u0026 nLe((...)) \u0026 nLm((\"𝛂ᴸ_fm,\\n ♭ᴸ_fm\")) end Three differences between ELM feature mapping layer and this feature mapping layer.\nDifference Standard ELM ELM with subnetwork nodes hidden node single hidden node generated\none by one general hidden node having subnetwork # hidden node Independent to the output dim 𝑚 In a subnetwork, it equals to the output dim relation A special case of the subnetwork case B. Proposed Method for Representation Learning 1) Optimal Projecting Parameters and Optimal Feature Data flowchart LR x1[\"x₁\"]--\u003e h1((\"^𝛂¹_f,^♭¹_f, β¹\")) \u0026 h2((\"^𝛂²_f,^♭²_f, β²\")) \u0026 he1((\"⋮\")) \u0026 hL((\"^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ\")) xe[x...]--\u003e h1((\"^𝛂¹_f,^♭¹_f, β¹\")) \u0026 h2((\"^𝛂²_f,^♭²_f, β²\")) \u0026 he1((\"⋮\")) \u0026 hL((\"^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ\")) xn[\"xₙ\"]--\u003e h1((\"^𝛂¹_f,^♭¹_f, β¹\")) \u0026 h2((\"^𝛂²_f,^♭²_f, β²\")) \u0026 he1((\"⋮\")) \u0026 hL((\"^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ\")) h1 \u0026 h2 \u0026 he1 \u0026 hL --\u003e feat[\"d-dimension\\n Feature data\"] feat --\u003e n1 \u0026 n2 \u0026 ne \u0026 nm --\u003e y1_[\"y₁\"] \u0026 ye_[\"⋮\"] \u0026 ym_[\"yₘ\"] subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 h2 \u0026 he1 \u0026 hL end subgraph elm[\"ELM-learning layer\"] n1((\"𝛂ₕ₁,^♭ₕ\")) \u0026 n2((\"𝛂ₕ₂,^♭ₕ\")) \u0026 ne((\"⋮\")) \u0026 nm((\"𝛂ₕₘ,^♭ₕ\")) end Objective of representation learning: Represent the input features meaningfully in several different representations as follows.\nRepresen-\ntation feat dim (𝑑) vs\nin-dim (𝑛) feature target output Dimension Reduction 𝑑 \u0026lt; 𝑛 H_f ∈ 𝐑ᵈᕁᴹ 𝐲=label (Supervise)\nor 𝐲=𝐱 (Unsp~) Expanded Dimension 𝑑 \u0026gt; 𝑛 H_f ∈ 𝐑ᵈᕁᴹ 𝐲=label (Supervise)\nor 𝐲=𝐱 (Unsp~) The feature data is 𝐇_f(𝐱ₖ, ^𝐚_f, ^b_f), where the weights of feature mapping layer ^𝐚ʲ_f, j=1,\u0026hellip;,L belongs to 𝐑ᵈᕁⁿ.\nDefinition 1: Given a nonlinear piecewise continous activation function g, we call {(^𝐚ʲ_f, ^bʲ_f)ⱼ₌₁ᴸ} (^𝐚ʲ_f ∈ 𝐑ᵈᕁⁿ) the 𝐿 optimal general hidden nodes and 𝐇⃰ ⃰_f= ∑ᵢ₌₁ᴸ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) the optimal feature data if it satisfies: ‖𝐞_L‖ ≤ min_{𝐇⃰ᴸ_f∈ 𝐑ᵈᕁᴹ} ( min_{𝐚ₕ∈ 𝐑 ᵐᕁᵈ} ‖𝐲-uₕ⁻¹ g(𝐇⃰ᴸ_f, 𝐚ₕ, bₕ)‖ ) (4)\nwhere 𝐞_L = ‖𝐲-uₕ⁻¹ g(𝐇⃰ᴸ ⃰_f, ^𝐚ₕ, ^bₕ)‖ and sequence ‖𝐞_L‖ is decreasing and bounded below by zero.\nRemark 1: If the optimal projecting parameters are obtained in the feature mapping layer {(^𝐚ʲ_f, ^bʲ_f)ⱼ₌₁ᴸ} (where ^𝐚_f ∈ 𝐑ᵈᕁⁿ), the original n-dimension data points 𝐱 will be converted to d-dimension data points: 𝐇⃰_f= ∑ⱼ₌₁ᴸ g(𝐱ₖ, ^𝐚ʲ_f, ^bʲ_f), which satisfy the inequality (4).\nThus the purpose is to find optimal projecting parameters that make the inequality (4) true for all data points.\n2) Learning Steps Based on the inverse of the activation function.\nGiven M arbitrary distinct training samples {(𝐱ₖ,𝐲ₖ)ₖ₌₁ᴹ}, 𝐱ₖ∈ 𝐑ⁿ, 𝐲ₖ∈ 𝐑ᵐ, which are sampled from a continuous system.\nSet j=1 to initialize a general node of the feature mapping layer randomly as: 𝐇⃰ʲ_f = g(^𝐚ʲ_f⋅𝐱 + ^bʲ_f), (^𝐚ʲ_f)ᵀ⋅^𝐚ʲ_f=𝐈, (^bʲ_f)ᵀ⋅^bʲ_f=1,\nwhere ^𝐚ʲ∈ 𝐑ᵈᕁⁿ, ^bʲ_f∈ 𝐑 is the orthogonal random weight and bias of feature mapping layer. 𝐇⃰ʲ_f is current feature data.\nGiven a sigmoid or sine activation function g, for any continous desired outputs 𝐲, the parameters in the (general) ELM learning layer are obtained as:\n^𝐚ₕ = g⁻¹(uₙ(𝐲)) ⋅ (𝐇⃰ʲ_f)⁻¹, ^𝐚ʲₕ∈ 𝐑ᵈᕁᵐ, ^bₕ = √mse(^𝐚ₕʲ ⋅ 𝐇⃰ʲ_f - g⁻¹(uₙ(𝐲)) ), ^bʲₙ∈ 𝐑, $g⁻¹(⋅) = \\\\{^{arcsin(⋅) \\quad if\\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \\quad if\\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _ where 𝐇⃰⁻¹ = 𝐇⃰ᵀ( (C/𝐈) + 𝐇⃰ 𝐇⃰ᵀ)⁻¹; C is a positive value; uₙ is a normalized function uₙ(𝐲): 𝐑➔(0,1]; g⁻¹ and uₙ⁻¹ represent their reverse function.\nUpdate the output error 𝐞ⱼ as 𝐞ⱼ = 𝐲 - uₙ⁻¹ g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ) So the error feedback data is 𝐏ⱼ = g⁻¹(uₙ(𝐞ⱼ))⋅(^𝐚ₕ)⁻¹\nSet j=j+1, add a new general node (^𝐚ʲ_f, ^bʲ_f) in the feature mapping layer by\n^𝐚ʲ_f = g⁻¹( uⱼ(𝐏ⱼ₋₁) ) ⋅ 𝐱⁻¹, ^𝐚ʲ_f∈ 𝐑ⁿᕁᵈ ^bʲ_f = √mse(^𝐚ʲ_f ⋅ 𝐱 - 𝐏ⱼ₋₁), ^bʲ∈ 𝐑 and update the feature data 𝐇⃰ʲ_f = ∑ᵢ₌₁ʲ uₗ⁻¹ g(𝐱, ^𝐚ˡ_f, ^bˡ_f)\nRepeat step 2-4 𝐿-1 times. (Finally, 𝐿 nodes are added into feature mapping layer.) The set of parameters {^𝐚ʲ_f,^bʲ_f}ⱼ₌₁ᴸ are the optimal projecting parameters and the feature data 𝐇⃰ᴸ_f = ∑ⱼ₌₁ᴸ uⱼ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) = 𝐇⃰ ⃰_f are the optimal feature data.\nC. Proof of the Proposed Method (Proof of Convergence)\nGiven M arbitrary distinct samples {(𝐱ₖ,𝐲ₖ)}ₖ₌₁ᴹ (𝐱ₖ∈ 𝐑ⁿ, 𝐲ₖ∈ 𝐑ᵐ)\nLemma 2: Given a bounded nonconstant piecewise continuous activation function g, we have lim_{(𝛂,♭)→(^𝛂,^♭)} ‖g(𝐱,𝛂,♭) - g(𝐱,^𝛂,^♭)‖ = 0 where the (^𝛂,^♭) is one of the least-squares solutions of a general linear system 𝛂⋅𝐱+♭.\nRemark 2:\nLemma 2 shows that SLFN training problem can be considered as finding optimal hidden parameters which satisfy: g(^𝛂₁,^♭₁) + \u0026hellip; + g(^𝛂_L,^♭_L) → 𝐲. 𝛂 (alpha) stands for basic ELM hidden node.\nThus training an SLFN is equivalent to finding a least-square general input weight ^𝐚ₕ of the (linear+activation) system g(^𝐚ₕ⋅𝐱) = 𝐲.\nIf activation function g is invertible, the input weights matrix can be obtained by pulling back the residual error to the hidden layer.\nFor example, if g is a sine function,\nThe output of the hidden layer matrix is 𝐲=sin(𝐚ₕ ⋅ 𝐱). Thus, 𝐚ₕ⋅𝐱 = arcsin(𝐲), 𝐲∈ (0,1]. The smallest norm least-squares solution of the linear system sin(𝐚ₕ⋅𝐱)=𝐲 is: ^𝐚ₕ = arcsin(𝐲)⋅𝐱⁻¹, where 𝐱⁻¹ is the Moore-Penrose generalized inverse of matrix 𝐱. 𝐱⁻¹ = 𝐱ᵀ( (C/𝐈) + 𝐱𝐱ᵀ)⁻¹ Theorem 1: Given M arbitrary distinct samples {(𝐱ᵢ,𝐲ᵢ)ᵢ₌₁ᴹ}, (𝐱ᵢ∈ 𝐑ⁿ, 𝐲ᵢ∈ 𝐑ᵐ) and a sigmoid or sine activation function g, for any continuous desired outputs 𝐲, we have:\nthe optimal weights ^𝐚ₕ = argmin_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹(g(𝐱,𝐚ₕ)) - 𝐲‖ least square error ‖g(𝐱,^𝐚ₕ,^bₕ) - 𝐲‖ ≤ min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹(g(𝐚ₕ⋅𝐱)) - 𝐲‖ if the parameters are obtained by (similar to Algorithm step-2):\n^𝐚ₕ = g⁻¹( u(𝐲))⋅𝐱⁻¹, ^𝐚ₕ ∈ 𝐑ᵐᕁⁿ ^bₕ = √mse(^𝐚ₕ⋅𝐱 - g⁻¹(u(𝐲))), ^bₕ∈ 𝐑 $g⁻¹(⋅) = \\\\{^{arcsin(⋅) \\quad if\\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \\quad if\\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _ Proof:\nLet 𝛌=𝐚ₕ⋅𝐱, and 𝛌 satisfy g(𝛌) = 𝐲. Normalizing 𝐲 to (0,1] by u(𝐲) to let 𝛌∈ 𝐑. Thus, for a sine hidden node, 𝛌 = g⁻¹(u(𝐲)) = arcsin(u(𝐲)). While for a sigmoid hidden node, 𝛌 = g⁻¹(u(𝐲)) = -log(1/u(𝐲) - 1).\n^𝐚ₕ is the solution for the linear system (g(𝐚ₕ⋅𝐱)=𝐲). For sine activation: ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ = arcsin(u(𝐲))⋅𝐱⁻¹. For sigmoid activation: ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ = -log(1/u(𝐲) - 1)⋅𝐱⁻¹\nOne of the least-squares solutions of a general linear system 𝐚ₕ⋅𝐱=𝛌 is ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹, which means the smallest error can be reached by this solution: ‖^𝐚ₕ⋅𝐱 -𝛌ₙ‖ = min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖𝐚ₕ⋅𝐱 - g⁻¹( u(𝐲) )‖ (18)\nThe special solution ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ has the smallest norm among all the least-squares solutions of 𝐚ₕ⋅𝐱 = 𝛌. The error can be further reduced by adding bias bₙ: ^bₕ = √mse(^𝐚ₕ⋅𝐱 - h⁻¹( u(𝐲) ))\nBased on eq. (18) and Lemma2, optimization by minimizing the L2-loss can be reformulated as: min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹( g(𝐚ₕ⋅𝐱) ) - u⁻¹( g(𝛌))‖ = ‖u⁻¹( g(^𝐚ₕ⋅𝐱) ) - u⁻¹( g(𝛌))‖ ≥ ‖u⁻¹( g(^𝐚ₕ⋅𝐱 + ^bₕ) ) - 𝐲‖ (20)\nBased on eq. (18) and eq. (20), the optimal weights is proved as: ^aₕ = arg min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖g(𝐱,𝐚ₕ) - 𝐲‖ And it satisfy: ‖g(𝐱,^𝐚ₕ,^bₕ) - 𝐲‖ ≤ min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹( g(^𝐚ₕ⋅𝐱) ) - 𝐲 ‖\nBased on Lemma 2 and Theorem 1, Theorem 2 is given:\nTheorem 2: Given M arbitrary distinct samples (𝐱, 𝐲), 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ, a sigmoid or sine activation function g, and the initial orthogonal random weights ^𝐚¹_f and bias ^b¹_f. For any continuous desired output 𝐲, the optimal feature data is: 𝐇⃰ᴸ⃰ _f(𝐱, (^𝐚¹_f, \u0026hellip;, ^𝐚ᴸ_f), (^b¹_f,\u0026hellip;,^bᴸ_f)) = ∑ⱼ₌₁ᴸ uⱼ⁻¹ g(^𝐚ʲ_f ⋅ 𝐱 + ^bʲ_f) which satisfy: ‖𝐞_L‖ ≤ min_{^𝐚ʲ_f∈ 𝐑ⁿᕁᵈ} ( min_{𝐚ₕ∈ 𝐑 ᵐᕁᵈ} ‖𝐲-uₙ⁻¹ g(𝐇⃰ᴸ_f, 𝐚ₕ, bₕ)‖) (21)\nand ‖𝐞_L‖ is decreasing and bounded below by zero if these parameters are obtained by:\n𝐇⃰ʲ_f = ∑ᵢ₌₁ʲ uᵢ⁻¹ g(𝐱, ^𝐚ⁱ_f, ^bⁱ_f), ^𝐚ₕ = g⁻¹(uₙ(𝐲)) ⋅ (𝐇⃰ʲ_f)⁻¹, ^𝐚ₕ∈ 𝐑 ᵐᕁᵈ, ^bₕ = √mse(^𝐚ₕ⋅𝐇⃰ʲ_f - g⁻¹( u(𝐲) )), ^bₕ∈ 𝐑 g⁻¹(⋅) = \\{^{arcsin(⋅) \\quad if\\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \\quad if\\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _ 𝐞ⱼ = 𝐲 - uₙ⁻¹( g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ), 𝐏ⱼ = g⁻¹(uₙ(𝐞ⱼ))⋅(^𝐚ₕ)⁻¹ ), ^𝐚ʲ_f = g⁻¹(uⱼ(𝐏ⱼ₋₁)) ⋅ 𝐱⁻¹, ^𝐚ʲ∈ 𝐑ⁿᕁᵈ ^bʲ_f = √mse(^𝐚ʲ_f ⋅ 𝐱 - 𝐏ⱼ₋₁), ^bʲ_f∈ 𝐑 Proof:\nBase on Theorem 1, the validity of (21) is obvious. So here, we just prove that the error ‖𝐞_L‖ is decreasing and bounded below by zero.\nLet Δ = ‖eⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ)‖² (last error-current output), and take the newest item apart: = ‖eⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g( (∑ᵢ₌₁ʲ⁻¹ uᵢ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) + uᵢ⁻¹g(𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ) ‖² (24)\nLet ^Tʲ = uₙ⁻¹g(uⱼ⁻¹g(𝐱, ^𝐚ʲ_f, ^bʲ_f), ^𝐚ₕ, ^bₕ). Because activation function is sigmoid or sine function, eq. (24) can be simplified as: Δ ≥ ‖𝐞ⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g( (∑ᵢ₌₁ʲ⁻¹ uᵢ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ) - ^Tʲ‖² = ‖𝐞ⱼ₋₁‖² - ‖𝐞ⱼ₋₁ - ^Tʲ‖² (unfold) = ‖𝐞ⱼ₋₁‖² - (‖𝐞ⱼ₋₁‖² - 2\u0026lt;eⱼ₋₁, ‖^Tʲ‖\u0026gt; + ‖^Tʲ‖²) (\u0026quot;\u0026lt;\u0026gt;\u0026quot; is dot product of 2 matrices: Frobenius inner product)\n= 2\u0026lt;𝐞ⱼ₋₁, ‖^Tʲ‖\u0026gt; - ‖^Tʲ‖² = ‖^Tʲ‖² ( 2\u0026lt;𝐞ⱼ₋₁, ‖^Tʲ‖\u0026gt; / ‖^Tʲ‖² - 1 ) (25)\nWe set ^Tʲ = uₙ⁻¹g(uⱼ⁻¹g( 𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ ) = 𝐞ⱼ₋₁ ± σ. (σ is variance, and 𝐞ⱼ₋₁ is the expectation) So 𝐞ⱼ₋₁ = ^Tʲ ± σ. Then \u0026lt;𝐞ⱼ₋₁, ‖^Tʲ‖\u0026gt; = \u0026lt;^Tʲ± σ, ‖^Tʲ‖\u0026gt; = \u0026lt;‖^Tʲ‖² ± \u0026lt;‖^Tʲ‖,σ\u0026gt; \u0026gt;\nHence, eq. (25) can be reformulated: Δ ≥ ‖^Tʲ‖² ( 2\u0026lt; ‖^Tʲ‖² ± \u0026lt;‖^Tʲ‖,σ\u0026gt; \u0026gt; / ‖^Tʲ‖² - 1 ) = ‖^Tʲ‖² ( 1 ± 2‖σ⋅(^Tʲ)ᵀ‖/‖^Tʲ‖²) (Wandong thinks there should be a 2.) ≥\nIn addition, based on Theorem 1 and eq. (7), there will be:\n‖^Tʲ - 𝐞ⱼ₋₁‖ ≤ min_{^𝐚ʲ_f∈ 𝐑ᵈᕁⁿ} ‖uₙ⁻¹g(uⱼ⁻¹g( 𝐱, ^𝐚ʲ_f, ^bʲ_f), ^𝐚ₕ, ^bₕ) -𝐞ⱼ₋₁‖ ‖σ‖ ≤ ‖^Tʲ‖ Thus Δ ≥ 0 can be proved as: Δ ≥ ‖^Tʲ‖² (1 ± ‖σ‖ / ‖^Tʲ‖) ≥ 0 (28)\nEq. (28) means ‖𝐞ⱼ₋₁‖ ≥ ‖𝐞ⱼ‖ and ‖𝐞‖ is decreasing and bounded below by zero.\nBased on Theorem 2, Theorem 3 is given:\nTheorem 3: Given M arbitrary distinct samples (𝐱, 𝐲), 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ, a sigmoid or sine activation function g, and optimal feature data 𝐇⃰ᴸ_f obtained by Algorithm 1, then lim_{j➝+∞} ‖𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - \u0026hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖ = 0 holds with probability one if :\n𝐚ⱼ = g⁻¹( u(𝐲) ) ⋅ (𝐇⃰ᴸ_f)⁻¹, ^𝐚ⱼ∈ 𝐑ᵐᕁⁿ bⱼ = √mse(^𝐚ⱼ⋅(𝐇⃰ᴸ_f) - g⁻¹(u(𝐲))), ^bⱼ∈ 𝐑 βⱼ = ⟨𝐞ⱼ₋₁, g(𝐇⃰ᴸ_f, 𝐚ⱼ, bⱼ)⟩ / ‖g(𝐇⃰ᴸ_f, 𝐚ⱼ, bⱼ)‖², βⱼ∈ 𝐑 Proof:\nFirst prove that the sequence ‖𝐞ⱼᴸ‖ is decreasing and bounded below by zero. Then prove that the lim_{j➝+∞} ‖𝐞ⱼᴸ‖ = 0\nBased on Theorem 1 and Lemma 1, the network output error satisfies: ‖𝐞ⱼᴸ‖ = ‖𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - \u0026hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖ ≤ ‖𝐲 -u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁)‖ = ‖𝐞₁ᴸ‖\nBased on Theorem 2, there will be: ‖𝐞ⱼᴸ‖ ≤ ‖𝐞ⱼᴸ⁻¹‖ ≤ \u0026hellip; ≤ ‖𝐞ⱼ¹‖\nThus, ‖𝐞ⱼᴸ‖ ≤ ‖𝐞ⱼᴸ⁻¹‖ ≤ \u0026hellip; ≤ ‖𝐞ⱼ¹‖ ≤ \u0026hellip; ≤ ‖𝐞₁¹‖ and ‖𝐞ⱼᴸ‖ is decreasing and bounded below by 0.\nBased on Lemma 1, when all hidden nodes randomly generated based on any continuous sampling distribution, lim_{n➝∞} ‖f - (fₙ₋₁ + βₙ⋅g(𝐱, 𝛂ₙʳ, ♭ₙʳ) )‖ = 0 holds with probability one if βₙ = ⟨𝐞ₙ₋₁, g(𝐇⃰ᴸⱼ, 𝛂ₙʳ, ♭ₙʳ)⟩ / ‖g(𝐇⃰ᴸⱼ, 𝛂ₙʳ, ♭ₙʳ)‖².\nIn addition, ELM theories have shown that almost any nonlinear piecewise continuous random hidden node can be use in ELM, and the resultant networks have universal approximation capbilities. According to the definition of general hidden neurons, (a general hidden node contains m (basic) hidden node), a general hidden node (𝐚,b) = (𝛂ʳ₁, \u0026hellip;, 𝛂ʳₘ, bʳ₁, \u0026hellip;, bʳₘ), . Thus its output is g(𝐇⃰ᴸ_f, 𝐚ⱼʳ, ♭ⱼʳ) ≡ ∑ᵢ₌₁ᵐ g(𝐇⃰ᴸ_f, 𝛂ʳᵢ, bʳᵢ).\nTherefore, lim_{j➝∞} ‖ 𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - \u0026hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖\n= lim_{n➝∞} ‖f- (fₙ₋₁ + βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼʳ, 𝑏ⱼʳ)) ‖ = 0\nD. Proposed Method With Multinetwork Structures %%{ init: { 'flowchart': { 'curve': 'basis' } } }%% flowchart LR subgraph in[\"input feature\"] x1((x1)) \u0026 xe((\"⋮\")) \u0026 xn((xn)) end subgraph net1[\"Layer 1\"] l11((a,b)) \u0026 l1e((\"⋮\")) \u0026 l1L((a,b)) end subgraph net2[\"Layer 2\"] l21((a,b)) \u0026 l2e((\"⋮\")) \u0026 l2L((a,b)) end subgraph netC[\"Layer C\"] lC1((a,b)) \u0026 lCe((\"⋮\")) \u0026 lCL((a,b)) end x1 \u0026 xn --\u003e l11 \u0026 l1L l11 \u0026 l1L --\u003e l21 \u0026 l2L l21 \u0026 l2L -.-\u003e lC1 \u0026 lCL subgraph out[\"output y\"] direction LR y1((1)) \u0026 ye((\"⋮\")) \u0026 ym((m)) end subgraph belm1[\"Basic ELM 1\"] direction LR b11((\"aₕ,bₕ\")) \u0026 b1e((\"⋮\")) \u0026 b1m((aₕ,bₕ)) end subgraph belm2[\"Basic ELM 2\"] direction LR b21((aₕ,bₕ)) \u0026 b2e((\"⋮\")) \u0026 b2m((aₕ,bₕ)) end net1 --\u003e feat1[\"Feature\\n data\\n 𝐇¹f\"] --\u003e belm1 --\u003e out feat1 --\u003e net2 --\u003e feat2[\"Feature\\n data\\n 𝐇²f\"] --\u003e belm2 --\u003e out netC --\u003e featC[\"Feature\\n data\\n 𝐇ᶜf\"] subgraph MultiLayer ELM in \u0026 net1 \u0026 net2 \u0026 netC end %% inverse for initialization weights linkStyle 12,13,14,16,17,18 stroke:#f0f Pink links will do inverse to calculate the weights for corresponding layers.\n","date":"2023-01-18T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-elm-mltlyr/","title":"Read: Optim - SLFN | Multilayer SNN"},{"content":"When generating random numbers, tensorflow calls:\nDDG search: \u0026ldquo;_pywrap_tensorflow.TFE_Py_FastPathExecute()\u0026rdquo;\nSimilar function: \u0026ldquo;Tile\u0026rdquo; Question about how TensorFlow API link with C++ code - reddit. The TF API name Tile is used to map it to C++ class or function name by a table.\nSimilar function \u0026ldquo;MatMul\u0026rdquo; Where can I find exactly how Tensorflow does matrix multiplication? - reddit:\n1 _result = _pywrap_tensorflow.TFE_Py_FastPathExecute( _ctx._context_handle, _ctx._eager_context.device_name, \u0026#34;MatMul\u0026#34;, name, _ctx._post_execution_callbacks, a, b, \u0026#34;transpose_a\u0026#34;, transpose_a, \u0026#34;transpose_b\u0026#34;, transpose_b) Run \u0026rsquo;nm \u0026ndash;demangle\u0026rsquo; on _pywrap_tensorflow_internal.so grep for MatMul, and get: tensorflow::SparseMatMulOp file: \u0026ldquo;tensorflow/tensorflow/core/kernels/sparse_matmul_op.cc\u0026rdquo; code \u0026ldquo;看python到C++调用关系\u0026rdquo; tensorflow二次开发 - 沉思语录20190227. Take matmul as an example：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 me@Server:~$ cd /mnt/Server/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core/python me@Server:~/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core/python$ grep -rni \u0026#34;tf_export.*matmul\u0026#34; # 这个函数需要用 tf_export 导出 ops/math_ops.py:2565:@tf_export(\u0026#34;linalg.matmul\u0026#34;, \u0026#34;matmul\u0026#34;) ops/math_ops.py:2859:tf_export(v1=[\u0026#34;sparse_matmul\u0026#34;])(sparse_matmul) ops/gen_nn_ops.py:10155:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBias\u0026#34;)(QuantizedMatMulWithBias) ops/gen_nn_ops.py:10306:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBiasAndRelu\u0026#34;)(QuantizedMatMulWithBiasAndRelu) ops/gen_nn_ops.py:10471:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize\u0026#34;)(QuantizedMatMulWithBiasAndReluAndRequantize) ops/gen_sparse_ops.py:3078:tf_export(\u0026#34;raw_ops.SparseTensorDenseMatMul\u0026#34;)(SparseTensorDenseMatMul) ops/gen_linalg_ops.py:2531:tf_export(\u0026#34;raw_ops.TridiagonalMatMul\u0026#34;)(TridiagonalMatMul) ops/linalg/linalg_impl.py:552:@tf_export(\u0026#39;linalg.tridiagonal_matmul\u0026#39;) ops/sparse_ops.py:2188:@tf_export(\u0026#34;sparse.sparse_dense_matmul\u0026#34;, ops/gen_math_ops.py:1618:tf_export(\u0026#34;raw_ops.BatchMatMul\u0026#34;)(BatchMatMul) ops/gen_math_ops.py:1726:tf_export(\u0026#34;raw_ops.BatchMatMulV2\u0026#34;)(BatchMatMulV2) ops/gen_math_ops.py:6150:tf_export(\u0026#34;raw_ops.MatMul\u0026#34;)(MatMul) ops/gen_math_ops.py:7610:tf_export(\u0026#34;raw_ops.QuantizedMatMul\u0026#34;)(QuantizedMatMul) ops/gen_math_ops.py:10010:tf_export(\u0026#34;raw_ops.SparseMatMul\u0026#34;)(SparseMatMul) Read the usage description at math_ops.py:2565. It calls gen_math_ops.batch_mat_mul or gen_math_ops.mat_mul.\nGo to tensorflow.python.ops/gen_math_ops.py (This file maybe generated when compiling.)\nThe function batch_mat_mul calls:\n1 2 3 4 _result = _pywrap_tensorflow.TFE_Py_FastPathExecute( _ctx._context_handle, _ctx._thread_local_data.device_name, \u0026#34;BatchMatMul\u0026#34;, name, _ctx.post_execution_callbacks, x, y, \u0026#34;adj_x\u0026#34;, adj_x, \u0026#34;adj_y\u0026#34;, adj_y) So the Op function in C++ should be \u0026ldquo;BatchMatMul\u0026rdquo;.\nSeach all the place registering this Op by searching the definition of op in the source code/repo:\n1 2 3 4 5 6 7 8 # Cannot find anything in the python package installed by conda # yi@PC:/mnt/Server/anaconda3/pkgs/tensorflow-base-1.15.0-gpu_py37h9dcbed7_0$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; # yi@PC:/mnt/Server/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; yi@PC:~/Downloads/tensorflow_1.15$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; tensorflow/core/ops/math_ops.cc:946:REGISTER_OP(\u0026#34;MatMul\u0026#34;) tensorflow/compiler/mlir/tfr/resources/decomposition_lib.mlir:83:// REGISTER_OP(\u0026#34;MatMul\u0026#34;) tensorflow/c/experimental/ops/README.md:15:since `REGISTER_OP(\u0026#34;MatMul\u0026#34;)` appears in ***core/math_ops.cc***, the \u0026#34;MatMul\u0026#34; Search the kernel implementation of this Op:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 yi@PC:~/Downloads/tensorflow_1.15$ grep -rni \u0026#34;Name(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; tensorflow/core/transforms/remapper/tests/contraction.mlir:38: %MatMul, %ctl_1 = MatMul(%Placeholder, %Const) device(\u0026#34;/device:CPU:0\u0026#34;) name(\u0026#34;MatMul\u0026#34;) {T = f32, transpose_a = false, transpose_b = false} : (tensor\u0026lt;*xf32\u0026gt;, tensor\u0026lt;*xf32\u0026gt;) -\u0026gt; (tensor\u0026lt;*xf32\u0026gt;) tensorflow/core/transforms/remapper/tests/onednn_contraction.mlir:76: %MatMul, %ctl_1 = MatMul(%Placeholder, %Const) device(\u0026#34;/device:CPU:0\u0026#34;) name(\u0026#34;MatMul\u0026#34;) {T = f32, transpose_a = false, transpose_b = false} : (tensor\u0026lt;*xf32\u0026gt;, tensor\u0026lt;*xf32\u0026gt;) -\u0026gt; (tensor\u0026lt;*xf32\u0026gt;) tensorflow/core/grappler/utils/pattern_utils_test.cc:42: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), input, weight); tensorflow/core/grappler/optimizers/remapper_test.cc:1225: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/remapper_test.cc:1433: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/remapper_test.cc:1610: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/mkl_remapper_test.cc:466: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), input, filter); tensorflow/core/grappler/optimizers/mkl_remapper_test.cc:667: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/constant_folding_test.cc:2909: Output matmul = ops::MatMul(scope.WithOpName(\u0026#34;matmul\u0026#34;), a, b); tensorflow/core/grappler/optimizers/arithmetic_optimizer_test.cc:1155: auto matmul_op = s.WithOpName(\u0026#34;matmul\u0026#34;); tensorflow/core/grappler/optimizers/arithmetic_optimizer_test.cc:1227: Output matmul = ops::BatchMatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), trans_a, trans_b); tensorflow/core/grappler/costs/analytical_cost_estimator_test.cc:79: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), flat, w2); tensorflow/core/kernels/matmul_op_test.cc:107: root.WithOpName(\u0026#34;matmul\u0026#34;), tensorflow/core/kernels/matmul_op_test.cc:126: root.WithOpName(\u0026#34;matmul\u0026#34;), tensorflow/core/kernels/mkl/mkl_fused_ops_test.cc:931: Output next_op = ops::MatMul(root.WithOpName(\u0026#34;matmul\u0026#34;), input_op, tensorflow/core/kernels/matmul_op_impl.h:881: Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_CPU).TypeConstraint\u0026lt;TYPE\u0026gt;(\u0026#34;T\u0026#34;), \\ tensorflow/core/kernels/matmul_op_impl.h:892: Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_GPU).TypeConstraint\u0026lt;TYPE\u0026gt;(\u0026#34;T\u0026#34;), \\ tensorflow/core/framework/op_kernel_test.cc:1062:REGISTER_KERNEL_BUILDER(Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_CPU), DummyKernel); tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:427: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), feed, const_1); tensorflow/compiler/tf2xla/kernels/matmul_op.cc:102:REGISTER_XLA_OP(Name(\u0026#34;MatMul\u0026#34;).TypeConstraint(\u0026#34;T\u0026#34;, kMatmulTypes), MatMulOp); DDG search: \u0026ldquo;How to read tensorflow c++ source code\u0026rdquo;\n","date":"2023-01-16T18:19:49-05:00","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf_randomuniform/","title":"memo: TF1.15 RandomUniform in C++"},{"content":"Statistical dispersion (2024-07-08)\nVariance: $\\sum_{i=1}^N(x_i - \\bar x)^2 / N$. The division by N is done to account for varying numbers of sampling data in different trials.\nFor example, one group has 100 data points, whereas another group has only 10 data points.\nIf variance uses \u0026ldquo;absolute error\u0026rdquo; instead of \u0026ldquo;squared error\u0026rdquo;,it becomes the mean absolute difference (平均绝对离差). MAD typically refers to the median absolute difference. wikipedia\nApplying various metrics requires considering the specific scenario.\nSquaring magnifies distances larger than 1 and diminishes distances smaller than 1. Therefore, variance is more sensitive to outliers.\n0 3 1 0 - 1 3 0 2 2 0 2 2 Variance: 20/5 \u0026gt; 16/5. MeanAD: 8/5 = 8/5\nIf using the MeanAD, the 2 sets of data have the same variability. Whereas considering variance, the first group has a larger variability.\nMeanAD is more affected by \u0026ldquo;众数\u0026rdquo; than by outliers.\n0 3 2 ½ 0 - ½ - 2 - 3 0 3 1 . 4 1 . 4 0 - 1 . 4 - 1 . 4 - 3 Variance: 26.5/7 \u0026gt; 25.84/7. Conversely, MeanAD: 11/7 $\u003c$ 11.6/7\nVariance has better mathematical properties than MeanAD.\nThe squaring operation is differentiable everywhere, whereas taking the absolute value is not differentiable at \u0026ldquo;0\u0026rdquo;.\nBut variance is not always superior to MeanAD in all cases.\nFor example, when one wants to weaken the impact of rare extreme outliers without deleting them (as they may have potential meaning), using MeanAD is appropriate.\nCounting all people is impossible, so the \u0026ldquo;sample variance\u0026rdquo; is used to approximate the \u0026ldquo;population variance\u0026rdquo;.\nHowever, sample variance tends to be lower than the population variance when sampling multiple times. Figuratively, the narrow perspective of a small group of samples may underestimates the overall variance:\nTo estimate the variance of a distribution, one can sample from this distribution randomly, and use the sample variance as an estimate of the distribution\u0026rsquo;s variance. However, performing this sampling only once is definitely not accurate. Therefore, sampling must be conducted many times, and the average of these sample variances is more reliable.\nSome sampling trials result in a higher variance than the population variance, while others result in a lower variance. Therefore, the expectation of these sampling results is calculated, which is always samller than the population variance.\nTo alleviate the tendency of the sample variance being smaller than the population variance, use (N-1) as the denominator rather than N.\nThis adjustment, known as Bessel’s correction, which can be accounted for the degrees of freedom in the data.\nRef: 方差全面解疑！最有梗的一次数学讲解视频，有你想听的！-有趣的理工男\nNormalize Dataset (2023-11-23)\nNormalizing a dataset means making the variance to 1.\n方差是一个标量（偏离期望的平方的期望）: the average of squared difference between each value in a dataset from the mean value.\nSquare ignores the sign of deviations, and only focuses on magnitudes.\nGiven 2 datasets with different scales: 1,2,6 and 10,20,60.\n1 2 -*--*--▲------* V.S. -*--*--▲------* 1 2 3 6 10 20 30 60 Their variance are 4.66 and 46.6. After normalization, their variance will both become 1. (Plain number, without any practical meaning.)\nTransform the variance to 1: subtract mean and divide by std for each sample.\n$$ Var = \\frac{∑(\\frac{x-μ}{σ} - 0)²}{n} = \\frac{∑\\frac{(x-μ)²}{σ²}}{n} = \\frac{1}{σ²} σ² = 1 $$ The consequences of normalization have 2 aspects:\nBecause all the squared deviations (x-μ)² are scaled by their average: σ², the measuring unit is eliminated (消除量纲). Hence, different attributes (dimensions) can be compared equally.\nOn the other hand, the sum of the scaled squared deviation becomes $n$. Thus, the new variance is 1.\n$$ Var = \\frac{∑\\frac{(x-μ)²}{σ²}}{n} = \\frac{∑\\frac{(x-μ)²}{ \\frac{∑(x-μ)²}{n} }}{n} = \\frac{n ∑\\frac{(x-μ)²}{∑(x-μ)²} }{n} = \\frac{n* 1}{n} =1 $$ Analogy: $\\frac{a}{a+b+c} + \\frac{b}{a+b+c} + \\frac{c}{a+b+c}=1$ Variance can be scaled to any value. Taking 1 is for interpretability and simplifying calculations (?) (2023-01-14)\nBatchNorm BatchNorm: 前向过程中每层的输入的分布一直在变化，不满足独立同分布，导致内部协变量偏移Internal Covariate Shift问题， 所以对每层的激活值在输入下一层之前，把这一个batch的分布调整为0均值，方差为1的标准正态分布， 即固定每个隐层节点的激活输入分布，让输入的激活值落在激活函数梯度较大的区域，加快收敛与避免梯度消失。 Batch Normalization导读-张俊林-知乎\n(2024-02-21) 不是“正态分布”，标准化不会改变数据的分布类型，数据原来是什么分布，做完 normalization 还是什么分布。 (mean=0, sigma=1) 只是一个 标准，之后经过一些运算，分布的参数发生变化了，还可以回到这个标准。 todo: L11.2 How BatchNorm Works\ntodo: L11.4 Why BatchNorm Works\nNormalization layer 对数据减均值，除以标准差。以下的 N 是一个 batch 中的样本个数(batch size, B)， 图片batch：(N,C,H,W)，序列batch: (N, embed_dim, seq_len)。 1d,2d,3d方法的区别在于 input 的维度。 Learnable parameters 是γ 和 β 用于对 μ和σ 做 affine 变换。 running mean 和 running var 在训练时不断使用在前向时得到的 x 的mean 和 var 做加权更新，权重为动量 momentum\nBatchNorm 让1个batch的，每个通道的均值为0，方差为1（整个batch的μ也=0）； 指定通道数量input.shape[1]: bn=nn.BatchNorm1d(num_features=C)\nLayerNorm 让每个样本的全部通道（最后几维，一个单词的特征向量，一幅图片的feature map）的均值为0，方差为1，layer指的是 fc net 的一层；\n指定最后几维: ln=nn.LayerNorm(normalized_shape=[C,H,W])）\nInstanceNorm 让每个样本的每个通道的均值为0，方差为1，同一batch内的样本没有联系，用于风格迁移；\n指定通道数量: in=nn.InstanceNorm1d(num_features=C)\nGroupNorm 介于 LayerNorm 和 Instance Norm之间，通道分组=1 就是LN，通道分组=C 就是IN；\n指定通道分组: gn=nn.GroupNorm(num_groups=2, num_channels=C) lecture 7b 神经网络的训练(归一化，迁移学习）-ranchlai-bili; github repo\nWu\u0026He ECCV2018 Weight Norm\n(2023-08-03)\nLayerNorm Customize LayerNorm, as referring to ConvNeXt-meta\u0026rsquo;s nn.LayerNorm, can only normalized the last few channels.\nF.layer_norm can be used to normlize one of the middle dimensions, as in ConvNeXt-torchvision\nLoop of ChatGPT\u0026rsquo;s Wrong Solutions:\nQ: In the following code, why is the grad of self.weight None after loss.backward()?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class LayerNorm(nn.Module): r\u0026#34;\u0026#34;\u0026#34; Input data: (batch_size, C, H, W) Use F.layer_norm to do normalization for the dim of channels. \u0026#34;\u0026#34;\u0026#34; def __init__(self, eps=1e-6): super().__init__() self.normalized_shape = (4, ) # C = 3, placeholder self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) self.eps = eps def forward(self, x): r\u0026#34;\u0026#34;\u0026#34; Since the `normalized_shape` (#channels) of x is unknown when initializing the model, implement normalization for the channels dimension here. \u0026#34;\u0026#34;\u0026#34; if self.normalized_shape != (x.shape[1],): self.weight = nn.Parameter(torch.ones(x.shape[1])) self.bias = nn.Parameter(torch.zeros(x.shape[1])) mean = torch.mean(x, dim=1, keepdim=True) var = torch.var(x, dim=1, correction=0, keepdim=True) # biased estimation x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x x = torch.ones(1,3,224,224) myLN = LayerNorm() myLN.weight.register_hook(lambda grad: print(grad.shape)) normalized_x = myLN(x) target_x = torch.rand(1,3,224,224) loss = nn.MSELoss()(normalized_x, target_x) loss.backward() Answer:\nThe redefined variables self.weight, self.bias inside the forward method are not part of the computation graph, so their gradients won\u0026rsquo;t be computed during the backward pass.\nAnd the new created parameter tensors have no connection to the original self.weight and self.bias (created during initialization), which are not used in the forward pass. so their grad attribute remain None.\nFinally, he suggested me don\u0026rsquo;t redefine in the forward.\nQ: But I don\u0026rsquo;t have information about the shape of self.weight to initialize it inside the init method.\nAnswer:\nInitialize them as None in the __init__ method and then initialize them with the correct shape during the first forward pass.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class LayerNorm(nn.Module): def __init__(self, eps=1e-6): super().__init__() self.eps = eps self.normalized_shape = None # Placeholder for the shape of self.weight self.weight = None # Placeholder for the weight parameter self.bias = None # Placeholder for the bias parameter def forward(self, x): if self.normalized_shape is None: # Initialize self.normalized_shape and create weight and bias with the correct shape self.normalized_shape = (x.shape[1],) self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) print(self.bias.shape) mean = torch.mean(x, dim=(2, 3), keepdim=True) var = torch.var(x, dim=(2, 3), correction=0, keepdim=True) x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x This code has an ValueError: optimizer got an empty parameter list\nAnwser:\nApologies for the oversight. The error is happening because the LayerNorm class does not define any learnable parameters (weights and biases) in its __init__ method. As a result, when you try to pass myLN.parameters() to the optimizer, it doesn\u0026rsquo;t find any parameters to optimize, leading to the \u0026ldquo;optimizer got an empty parameter list\u0026rdquo; error.\nIn order to use an optimizer like Adam, you need to define at least one learnable parameter within your model that will be optimized during training.\nSince you don\u0026rsquo;t have information about the shape of the input during initialization, you can create a placeholder for self.weight and self.bias in the __init__ method, and then initialize them with the correct shape during the first forward pass. To do this, we can use the register_buffer method, which allows us to create tensors that are not optimized during training but still part of the model.\nHere\u0026rsquo;s the updated code with the use of register_buffer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class LayerNorm(nn.Module): def __init__(self, eps=1e-6): super().__init__() self.eps = eps self.register_buffer(\u0026#39;normalized_shape\u0026#39;, torch.tensor([])) self.register_buffer(\u0026#39;weight\u0026#39;, torch.tensor([])) self.register_buffer(\u0026#39;bias\u0026#39;, torch.tensor([])) def forward(self, x): if self.normalized_shape.numel() == 0: # Initialize self.normalized_shape and create weight and bias with the correct shape self.normalized_shape = torch.tensor([x.shape[1]]) self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) print(self.bias.shape) mean = torch.mean(x, dim=(2, 3), keepdim=True) var = torch.var(x, dim=(2, 3), correction=0, keepdim=True) x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x Now the model will contain learnable parameters that can be optimized using an optimizer like Adam. The grad attribute of self.weight should be available and updated after loss.backward().\nAdditionally, you can also use torch.nn.init functions to initialize the weight and bias tensors in a more standard way. For example, you can use torch.nn.init.ones_ and torch.nn.init.zeros_ to initialize the self.weight and self.bias tensors, respectively.\nregister_buffer doesn\u0026rsquo;t add parameters to optimizer to optimize neither.\nIt may be necessary to know the number of channels.\nJust sum up the attribute out_chnnels of each layer, e.g., alexnet.features[0].out_channels. PyTorch Forum\nBut Swin transformer doesn\u0026rsquo;t have Conv2d.\nMaybe I can only hard code the numbers of channels.\nIf you add the norm layer following the convolution layer, the normalized_shape is the out_channels of the convolution layer.\nBut here, I want to normalize the stacked extracted feature maps. So I need to know the total channels of the incoming feature maps.\nIf the #channels is known, I can use F.layer_norm() directly.\n(2023-10-17)\nAbsorb BN Normalization is performed to keep each \u0026ldquo;stage\u0026rdquo; (layer) in a network maintain the same distribution, such that the training could be more efficient due to\nLeaving out modeling distributions shift of each layer\u0026rsquo;s input data.\nStabilizing optimization by suppressing \u0026ldquo;outliers\u0026rdquo; (high value) in the output feature map of each layer to avoid big variations in the final prediction.\nRefer to: BN、BN同步、吸收BN - EveK的文章 - 知乎\nSpecifically, input data has (mean=0, std=1), but after a conv layer, the outcome feature maps may don\u0026rsquo;t persist (mean=0, std=1).\nIdeally, featue maps\u0026rsquo;s distribution should be transformed to (mean=0, std=1) to align with the input data. And that transformation requires its whitening matrix , which transforms co-variance matrix of the feature maps an identity matrix, meaning each dimension is unrelated. CSDN\nHowever, solving the whitening matrix for a high-dimension tensor is time consuming.\nTherefore, the \u0026ldquo;whitening\u0026rdquo; is performed only for each channel.\nOn the other hand, it\u0026rsquo;s difficult to get the exact distributions at once, because the data is trained batch-by-batch.\nTherefore, the distribution to be corrected pertains only to the small batch of data. (Or involving previous data by using moving average of history mean and std)\nAnd only 2 parameters (mean, std) of their distribution are considered to keep the distributions consistent.\nOnce a feature map $M_{(N, C, H, W)}$ spit out from a conv layer, BatchNorm normalizes the data at the same channel for all N samples in the batch:\n\\begin{algorithm} \\begin{algorithmic} \\FOR {i=0 \\TO M.size(1)} \\PROCEDURE{BN}{ M, meanʰ, stdʰ, scale, bias} \\STATE mean = M[:, i, :, :].sum() / N \\STATE std = ( (M[:, i, :, :] - mean)² / N ).sqrt() \\STATE M⁰¹ = (M[:, i, :, :] - mean) / std \\STATE M' = scale * M⁰¹ + bias \\ENDPROCEDURE \\ENDFOR \\end{algorithmic} \\end{algorithm} meanʰ and stdʰ are from history.\nThe scale factor γ and bias β are for the situation where the variation is very small. Then the differences can be magnified through scaling to avoid representation capacity degradation.\nAnd γ,β are required to be learnable to automatically find the appropriate feature levels. Otherwise, bias will grow to infinity as explained in paper.\nCombine BN into wights and bias of the last layer\n$$ M' = γ \\frac{M - mean}{std} + β = \\frac{γ⋅M}{std} \\left( β - \\frac{γ ⋅ mean}{std} \\right) $$If M is calculated as $M = w⋅x + b$, by substituting it, the equation becomes:\n$$ M'= \\frac{γ⋅w}{std} x + \\left( β - \\frac{γ⋅mean}{std} + \\frac{γ⋅b}{std} \\right) $$Thus, w becomes $\\frac{γ⋅w}{std}$ and b becomes $( β - \\frac{γ⋅mean}{std} + \\frac{γ⋅b}{std} )$.\nGiven a conv layer with $C_i$ input channels and $C_o$ output channels, BN for this layer has $C_i × C_o$ parameters, where $C_i$ is for the history channels, $C_o$ is for target channels.\n","date":"2023-01-14T12:54:12-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/dl-normalization/","title":"memo: DL | Normalization"},{"content":"The maximum gradient of the sigmoid activation function is 0.25, which may cause partial derivative of loss with respect to the earlier weight w very small after passing throught multipler layers. And scaling the weights down can mitigate the gradient decrease.\nBased on the chain rule, the derivative of the weight in the fisrt layer (l=1) is ∂loss/∂w¹ = ∂loss/∂o ⋅ ∂o/∂a² ⋅ ∂a²/∂a¹ ⋅ ∂a¹/∂w¹, where a = g(z), g is an activation function.\nIf the weight w¹ is small, the activation z¹ is small (around zero), so ∂a¹/∂z¹ corresponds to the highest derivative. And also if w² is small, ∂a²/∂a¹ = ∂a²/∂z²⋅ ∂z²/∂a¹, where ∂a²/∂z² will be a big derivative. Hence, ∂loss/∂w¹ can maintain a high derivative.\nSo it\u0026rsquo;s importance to initialize the weights centered at zero with small variance for getting the maximum gradient. L11.5 Weight Initialization \u0026ndash; Why Do We Care?\nActivation z is a sum of wᵢxᵢ, so it may be exploding or vanishing quickly if the W doesn\u0026rsquo;t have constriant. Weight Initialization in a Deep Network (C2W1L11) - Andrew Ng\nXavier (Glorot) initialization L11.6 Xavier Glorot and Kaiming He Initialization - Sebastian Raschka\nStep 1: Initialize weights from Gaussian or uniform distribution Step 2: Scale the weights proportional to the number of input features to the layer In particular, the weights of layer l is defined as: 𝐖 ⁽ˡ⁾ ≔ 𝐖 ⁽ˡ⁾⋅ √(1/m⁽ˡ⁻¹⁾), where m is the number of input units of the previous layer (𝑙-1) to the next layer (𝑙).\n𝐖 is initialized from Gaussion (or uniform) distribution: Wᵢⱼ⁽ˡ⁾~N(μ=0, σ²=0.01)\nRationale behind this scaling factor:\nHe (Kaiming) initialization Usage Three different commonly used initialization techniques. Here are what their variants need to be set to and which activation functions they work best with.\nInitialization Activation function Variance (σ²) Mean Glorot Linear; Tanh; Sigmoid; Softmax σ² = 1/(½⋅(fanᵢₙ+fanₒᵤₜ)) 0 He ReLu; Variants of ReLU σ² = 2/fanᵢₙ 0 LeCun SELU σ² = 1/fanᵢₙ 0 Weight Initialization for Deep Feedforward Neural Networks\n","date":"2023-01-14T12:24:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/dl-weight_initialization/","title":"memo: DL | Weight initialization"},{"content":"视频封面：一个统一视角下的概率论+统计学+信息论\n原视频：1. 从头开始，把概率、统计、信息论中零散的知识统一起来-王木头学科学\n概率论最基础的问题：用数学的方式描述\u0026quot;不确定性\u0026quot;（或\u0026quot;可能性\u0026quot;）\n把所有事件及其发生的可能性写到一个表 f 里，每次通过查表就能知道可能性是多少: f(S) = K。\nidx S K 1 事件a 数值1 2 事件b 数值2 3 事件c 数值3 \u0026hellip; \u0026hellip;.. \u0026hellip;.. 这里的“可能性” 满足一些限制：\n数值要满足事件可能性的相对关系。比如，如果事件 a 的可能性 \u0026gt; 事件 b 的可能性，则 数值1 \u0026gt; 数值2 数值需要满足事件的包含关系。比如，事件 c = {事件a，事件b}（a,b中任意一个发生），则 数值3 = 数值1 + 数值2 注意，这里并未要求所有数值归一化，所以还不是概率值 这种方式只是把“可能性”做了数学符号化，并不是“数学化”。除了要保证定义出来的这个体系自洽之外，还要尽可能简约。 比如上面的事件 c 并不需要单独定义，它的数值可以从事件 a 和 b 推导出来。 所以表格中并不需要列举所有的事件，只需要包含不可再分的原子事件及其可能性。\nV 2.0\n表1：\nidx S K 1 原子事件a 数值1 2 原子事件b 数值2 3 原子事件c 数值3 4 原子事件d 数值4 \u0026hellip; \u0026hellip;.. \u0026hellip;.. 它可以定义出：f(S) = K。只留下原子事件后，就可以确定“可能性”的最大值为所有原子事件“合”在一起组成的事件发生的可能性（只要有1个原子事件发生，这件事就算发生）： max(∑K) = ∑_{s∈all} f(s)\n有了最大可能性之后，可以定义归一化的数值 K = K/(∑_{s∈all} f(s)) ∈ [0,1]\n从上面的原子事件可推导出以下表2:\nP(S) ∑K {1,2} 数值1+数值2 {1,2,3} 数值1+数值2+数值3 {3,4} 数值3+数值4 {2,4} 数值2+数值4 \u0026hellip;\u0026hellip;. \u0026hellip;.. P(S) 表示集合 S 的幂集。\n但是原子事件对于不同的问题，不好确定。对于离散问题（掷骰子），原子事件就是点数。 但对于连续的变量（温度），原子事件可以取一个小区间。因为可以无限细分，当区间趋近于无穷小时，它发生的可能性就趋于0。 如果所有原子事件的可能性都是0，就无法从表 1 推导出表 2。\n可以从表 2 建立数学体系。\nV 3.0\n对于连续的情况，原子事件就是一个个点，如表 3，它们对应的数值并不代表发生的可能性，而是有其他意义，因为我们要用表 4 建立定义，所以表 3 的数值是由表 4 推导出来的。\n表 3：\nℝ S K 0.0 点a 数值1 \u0026hellip; 点b 数值2 \u0026hellip; 点c 数值3 \u0026hellip; 点d 数值4 \u0026hellip; \u0026hellip;. \u0026hellip;.. 表 4：\nP(ℝ) K 0.0 [1,2] f[数值1,数值2] \u0026hellip; [1,3] f[数值1,数值3] \u0026hellip; [3,4] f[数值3,数值4] \u0026hellip; [1,2]∪[3,4] f[数值1,数值2]+f[数值3,数值4] \u0026hellip; \u0026hellip;. \u0026hellip;.. ","date":"2023-01-08T16:09:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/22_%E6%A6%82%E7%8E%87%E8%AE%BA1-%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%81%94/","title":"watch: DL - 王木头 22 | Overview of Probability theory, Statistics, Information theory"},{"content":"变分自编码器（三）：这样做为什么能成？\n采样一次就够 先推断学习 z 的后验分布 p(z|x) 的参数，再从分布中采样一个 z，用它计算 x 的后验分布 p(x|z) 的参数，再算从x的后验分布中采样得到 x‘的概率，\n在 VAE 的损失函数：𝓛 = 𝔼_pᐢ(x) [ KL( p(z|x)||q(z) ) + 𝔼_p(z|x) [ -ln q(x|z) ] ]中，\nKL散度的计算是使用神经网络拟合出的z的后验分布（正态分布）的期望和方差， 而第 2 项只采了一个样本 x 做近似，所以这一项变为：-ln q(x|z), z~p(z|x)\n因为 KL 散度也可写成期望：E_p(z|x) [ ln (p(z|x)/q(z)) ]，所以它也可以只采样一个点来近似。所以损失函数就可写为：\n𝓛 = 𝔼_pᐢ(x) [ ln p(z|x) - ln q(z) - ln q(x|z) ], z~p(z|x) (5)\n苏神说，以上的损失函数也能收敛到相似的结果。\n似然不能只用一个采样点估计 极大似然的公式可以写成期望： q(x|z) = arg max_q(x|z) ∫ pᐢ(x) ln (∫ q(x|z) q(z) dz) dx ，数值计算，要乘x的概率\n= arg max_q(x|z) 𝔼_pᐢ(x) [ ln ( ∫ q(x|z) q(z) dz) ] ，采样近似求积分\n= arg max_q(x|z) 𝔼_pᐢ(x) [ ln (1/K ∑_ₖ₌₁ᴷ q(x|zₖ) ], z1,z2,\u0026hellip;,2ₖ～q(z)\n对这个期望近似：先从先验 q(z) 中采 k 个 z，算积分（求和），再采 1 个 x，求它的概率的对数就行了。\nshuhuai 说是因为 log 的方差大，所以采样太少会失效。 苏神说，因为z 和 x 是一一对应的，如果没有采到 zₖ，那它对应的 xₖ 也就采不出来，概率就算不出来，因为采样是随机的，不能保证每次采的 k 个 z，包含了本 batch 中所有 x 对应的 z，所以容易失效。\nVAE采一个点确实够了 根据对数据集的了解，数据集X本身带有很强的约束，真正独立的维度很少，所以数据集可以被投影到低维空间的一个隐变量上。 这和普通的自编码器一样，也就是 z 与 x 一一对应，也就意味这 p(z|x) 和 q(x|z) 的方差为0。 在引入标准正态形式的先验分布 q(z) 后，粗略地看，只是对隐变量空间做了平移和缩放，所以方差也可以不大。\n因为 x 的后验分布的方差很小，每次采的结果都一样，都是均值 μ(z)。 因为 z 与 x 是一一对应的，所以 z 的后验分布的方差也很小，所以每次从中采的 z 都相同。 所以采样一次，与采样多次没什么差别，期望都是一样的。\n后验之妙 直接从先验 q(z) 中采样不可行，但在后验分布 p(z|x) 中采样一个点就够了， 因为自编码器里的方差为0，引入 z 的先验（标准正态分布），方差也不会太大。\n耿直的IWAE 重要性加权自编码器 Importance Weighted AutoEncoders arxiv\n对 p(x) 做了等价变换：乘一个p(z|x),除一个 p(z|x)\np(x) = ∫ q(x|z) q(z) dz = ∫ p(z|x) ⋅ [q(x|z) ⋅ q(z)] / p(z|x) dz = 𝔼_p(z|x) [ q(x|z) ⋅ q(z) / p(z|x) ]\n这样，从 q(z) 中采样就变成了从 p(z|x) 中采样，此前已论述了后验分布 p(z|x) 方差较小，所以采样几个点就够了：\n∫ q(x|z) q(z) dz = 1/k ∑ₖ₌₁ᴷ [ q(x|zₖ)q(zₖ)/p(zₖ|x) ], z1,z2,..,zₖ～p(z|x)\n代入似然函数：\nq(x|z) = arg max_q(x|z) 𝔼_pᐢ(x) [ ln ( ∫ q(x|z) q(z) dz) ] = arg max_q(x|z) 𝔼_pᐢ(x) [ ln ( 1/k ∑ₖ₌₁ᴷ [ q(x|zₖ)q(zₖ)/p(zₖ|x) ] ) ]\n= arg min_{q(x|z),p(z|x)} 𝔼_pᐢ(x) [ -ln ( 1/k ∑ₖ₌₁ᴷ [ q(x|zₖ)q(zₖ)/p(zₖ|x) ] ) ], z1,z2,..,zₖ～p(z|x) ，加个负号，求极小值。\n当 k=1 时，与 (5) 式一样，从这个角度看，IWAE 是VAE的升级版。\n其实，等价变换可以使用 z 的任意分布（只要能采出z就行），\n“选择 p(z|x) 只是因为它有聚焦性，便于采样。而当 k 足够大时，p(z|x) 的具体形式就不重要了”\nIWAE 中削弱了推断模型 p(z|x) 的作用，不去近似后验分布\n","date":"2023-01-01T22:48:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/vae/d-note-vae_3-su/","title":"read: Blog - 苏剑林 | VAE-3"},{"content":"Use the built-in dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torchvision import datasets, transforms # Root directory for the dataset data_root = \u0026#39;data\u0026#39; # Spatial size of training images, images are resized to this size. image_size = 64 celeba_data = datasets.CelebA( data_root, download=True, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) ] ) ) This will download and extract the zip file.\nSOURCE CODE FOR TORCHVISION.DATASETS.CELEBA\nExtract the zip file 1 2 3 4 5 6 7 8 9 import zipfile data_root = \u0026#39;data/celeba\u0026#39; # Add shortcut of dataset to your google drive zip_path = \u0026#39;/content/drive/MyDrive/CelebA/Img/img_align_celeba.zip\u0026#39; with zipfile.ZipFile(zip_path, \u0026#39;r\u0026#39;) as ziphandler: ziphandler.extractall(\u0026#39;data\u0026#39;) How do I load the CelebA dataset on Google Colab, using torch vision, without running out of memory?\nZipFile - GfG\nDownload with gdown Install the package: pip install gdown. Copy the URL in the address bar. 1 2 3 4 import gdown url = \u0026#34;https://drive.google.com/u/0/uc?id=1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ\u0026amp;export=download\u0026#34; output = \u0026#34;celeba.zip\u0026#34; gdown.download(url, output) Then unzip it and its subfolder:\n1 2 3 unzip celeba.zip cd celeba unzip img_align_celeba.zip (Python) Use the gdown package to download files from Google Drive\n(2024-02-21)\ngdown 4.7.1 cannot download large dataset dtu.zip 554 MB with the following error reported:\n1 2 3 4 5 6 7 8 9 (base) zi@lambda-server:~/Downloads$ gdown 135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ Access denied with the following error: Cannot retrieve the public link of the file. You may need to change the permission to \u0026#39;Anyone with the link\u0026#39;, or have had many accesses. You may still be able to access the file from the browser: https://drive.google.com/uc?id=135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ Update gdown to 5.1.0 to avoid it: issue\n1 pip install --upgrade gdown Download GoogleDrive (2024-05-25)\nisl-org/TanksAndTemples has a function about downloading datasets from google drive.\n","date":"2022-12-31T23:36:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/load_celeba_colab/","title":"memo: load CelebA on Colab"},{"content":"笔记 for 变分自编码器（二）：从贝叶斯观点出发\n数值计算 vs 采样计算 数值计算是先给个数列，对里面的每个数求概率p(x⁽ⁱ⁾)，再加权求和 ∑ᵢ₌₀ⁿ x⁽ⁱ⁾p(x⁽ⁱ⁾) (x⁽ⁱ⁾-x⁽ⁱ⁻¹⁾)； 采样计算是先从分布中采样，求采样点概率的平均，所以不需要再乘样本点出现的概率：E[x]≈1/n⋅∑ᵢ₌₀ⁿ x⁽ⁱ⁾, x⁽ⁱ⁾∼p(x)。\n推导VAE 的损失函数 苏神从逼近联合概率 p(x,z) 出发，而不是从逼近z的后验 p(z|x) 出发。 也许是因为沿着 EM 的思路走，就需要对后验 p(z|x) 求近似，所以很多人聚焦于推导 p(z|x)。\n因为想求样本集合 x的分布，\n但是难以直接描述复杂分布，所以通过引入隐变量把x的分布变成条件分布的叠加，而后可以对隐变量的分布和条件分布做适当简化（比如都假设为正态分布），并且可以用深度学习模型近似求（隐变量）条件分布的参数，即“深度概率图模型”。\n假设 x 是由 z 生成的，所以求 p(x) 可以通过把联合分布 p(x,z) 中的 p(z) 积掉求得： p(x) = ∫p(x,z) dz = ∫ p(x|z)p(z) dz\n目标是用一个 q(x,z) 逼近 p(x,z)，又因为 p(z) 是先验（已知），所以当 q(x,z)≈p(x,z) 时，生成模型 p(x|z) 也就学到了，“一举两得”。\n通过最小化 KL 散度逼近：KL( p(x,z) || q(x,z) ) = ∫∫ p(x,z) [ ln (p(x,z)/q(x,z)) ] dz dx，是一个二重积分\n把 p(x,z) 写成 pᐢ(x)⋅p(z|x)，也就是推断过程，由x的先验推出z：\nKL( p(x,z) || q(x,z) ) = ∫∫ pᐢ(x)⋅p(z|x) [ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) ] dz dx = ∫ pᐢ(x) [ ∫ p(z|x) ⋅ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) dz ] dx = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) dz]\n可以蒙特卡罗采样近似求这个期望，也就是把每个样本 x⁽ⁱ⁾ 代入上面中括号里的函数（代入概率密度公式可算出概率值），把函数值求均值。\n这个期望可以进一步简化，把 ln 拆开： ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) = ln pᐢ(x) + ln (p(z|x)/q(x,z))\n𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) dz ] = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln pᐢ(x) dz] + 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln (p(z|x)/q(x,z)) dz]\n上面第 1 个期望： 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln pᐢ(x) dz] = 𝔼_pᐢ(x) [ ln pᐢ(x)⋅∫ p(z|x) dz ] = 𝔼_pᐢ(x) [ ln pᐢ(x) ]\n这里的 pᐢ(x) 是根据样本 x⁽⁰⁾, x⁽¹⁾,\u0026hellip;, x⁽ⁿ⁾ 确定的关于 x 的先验分布，是已知的确定的，所以这一项是一个常数。 所以 KL 散度 = 常数 + 一个期望：\nKL( p(x,z) || q(x,z) ) = 常数 + 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln (p(z|x)/q(x,z)) dz]\n所以最小化KL散度，对应目标函数 𝓛 就是第2个期望：\n𝓛 = KL( p(x,z) || q(x,z) ) - 常数，则𝓛 的下界就是\u0026quot;-常数\u0026quot;: -𝔼_pᐢ(x) [ ln pᐢ(x) ]， 其中 pᐢ(x) 不一定是概率，在连续情况时，pᐢ(x) 是概率密度函数，它可以大于1 也可以小于1，所以下界不一定是非负的，即 loss 可能是负数。\n再把 𝓛 里的 ln 和 q(x,z) 展开：\n𝓛 = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( p(z|x) / q(x,z) ) dz] = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( p(z|x) / (q(x|z)q(z)) dz ] = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( p(z|x)/q(z) ) dz - ∫ p(z|x) ⋅ ln q(x|z) dz]\n把里面的积分写成期望：\n𝓛 = 𝔼_pᐢ(x) [ KL( p(z|x)||q(z) ) + 𝔼_p(z|x) [ -ln q(x|z) ] ]\n括号里的就是 VAE 的损失函数：KL散度（正则化项）+ x的后验按照 z 的后验求期望 shuhuai008-30VAE\n不能把括号里面的两项分开看或分开最小化。 如果只令 KL( p(z|x)||q(z) )=0，即每个后验都是标准正态分布，与x无关，导致生成的 x 不准，概率 q(x|z) 会很小，-ln q(x|z) 就会很大。 而如果 -ln q(x|z) 很小，即x后验概率 q(x|z) 大，后验分布 p(z|x) 的峰肯定集中在 x 附近，即 p(z|x) 的方差小，与标准正态的差距大，KL( p(z|x)||q(z) ) 不会小。 所以这两部分 loss 是相互拮抗的，𝓛 要以整体来看。 也就是要推断过程与生成过程相互博弈。\n算法设计 损失函数中未知的分布包括：z 的先验 q(z)，z 的后验 p(z|x)，x 的后验 q(x|z) （x 的先验pᐢ(x) 是已知的）\n为了便于采样，假设 z 的先验分布为标准多元正态分布：q(z)=N(0,I)\n用神经网络拟合 z 的后验 p(z|x) 和 x 的后验 q(x|z)。\n计算 z 的后验是推断过程，对应 EM 的 E步：近似求得 p(z|x)；计算 x 的后验是生成过程，对应 EM 的 M步：把z的近似后验代入似然函数，求极大似然时，对应的模型参数。 （EM中用于逼近 p(z|x) 的神经网络的参数是 ϕ；用于逼近 q(x|z) 的神经网络（也可直接求导）的参数是 θ）\n后验分布近似 假设 z 的后验是（各分量独立的）一般正态分布，所以需要神经网络逼近它的期望和方差。期望和方差都由 x 决定，即是 x 的函数 μ_ϕ(x), Σ_ϕ(x)\n然后 KL 散度就可以写出来了: 1/2(-logσ² + μ² + σ² -1)，已在VAE第一篇推导过。变分自编码器（一）：原来是这么一回事\n生成模型近似 对于生成模型部分 q(x|z) 的假设，原作者在论文《Auto-Encoding Variational Bayes》中，给出了两种方案：二项分布或正态分布。“既要满足概率的定义（归一化），又要容易算，没多少选择”\n二项分布只有一个参数：\u0026ldquo;抛硬币向上的概率 ρ\u0026rdquo;。所以对于一个 D 维的样本 x，x 的每一维都是个二值的，所以一个输入样本 x 在 z 成立的情况下，发生的概率就是： q(x|z) = ∏ₖ₌₁ᴰ (ρₖ(z))ˣᵏ (1-ρₖ(z))¹⁻ˣᵏ\n此时的 -ln q(x|z) = ∑ₖ₌₁ᴰ [ -xₖ ln ρₖ(z) - (1-xₖ) ln(1-ρₖ(z)) ]\n也就是说神经网络的输出 ρ(z) 需要是在 0～1 之间（比如用 sigmoid 激活），然后用交叉熵作为损失函数。\n如果假设 q(x|z) 是正态分布，用神经网络估计它的期望 μ_θ(z) 和方差 Σ_θ(z)，于是： -ln q(x|z) = ½ || (x-μ_θ(z)) / Σ_θ(z) ||² + D/2 ln 2π + ½∑ₖ₌₁ᴰ lnΣ_θ(z)。\n很多时候，训练时方差会固定为一个较小的常数（每次采样都会采到μ），所以神经网络只需估计μ，也就是把 μ 当作生成的 x\u0026rsquo;，则上式重构误差可简化为： -ln q(x|z) = ½ || (x-μ_θ(z)) / Σ_θ(z) ||²\nx \u0026ndash;\u0026gt; z \u0026ndash;\u0026gt; x'\n综上，对于二值数据，假设 q(x|z) 是伯努利分布（二项分布），可以对 decoder （第2个神经网络）的输出用 sigmoid 激活，并用交叉熵作为损失函数； 而对于一般数据，假设 q(x|z) 是正态分布，则使用 mse 作为损失函数。\n从后验中采样 z 的技巧 损失函数的第 2 项是：𝔼_p(z|x) [ -ln q(x|z) ]，根据蒙特卡罗的思想，这个期望用均值近似：先采样 z，用 z 计算 x 的后验分布 q(x|z)，再从中采样 x 计算它出现概率的对数：-ln q(x|z)，再求均值：\n𝔼_p(z|x) [ -ln q(x|z) ] = -1/n ∑ᵢ₌₁ᴺ ln q(x|zᵢ), zᵢ～p(z|x)\n假设了 p(z|x) 是正态分布，它的参数 μ_ϕ, Σ_ϕ 已由神经网络算出，再使用重参数化技巧就能采样出 z 。\n但是要采样多少个合适呢？因为每个 z 是专属于 1 个 x，所以只从 p(z|x) 中采一个 z 来计算 x 的分布 q(x|z)，再计算 -ln q(x|z)，就是loss值。\n最终：𝓛 = 𝔼_pᐢ(x) [ KL( p(z|x)||q(z) ) -ln q(x|z) ] , z～p(z|x)\n因为每次 epoch 的隐变量都是随机生成的，因此当 epoch 数足够多时，可以保证采样的充分行。苏神试过采样多个的情形，感觉生成的样本没有明显变化。\n","date":"2022-12-30T20:07:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/vae/d-note-vae_2-su/","title":"read: Blog - 苏剑林 | VAE-2"},{"content":"(2023-10-11)\n一维高斯 数据 的空间分布像是一个中间重两头轻的铁棒 (线)； 二维高斯数据的分布像一个 椭圆 (面)； 服从三维高斯分布的数据的空间分布像一个 椭球 (体)。\nSource video: 机器学习-白板推导系列(二)-数学基础\n1. 高斯分布1-极大似然估计 P1\n高斯分布在（统计）机器学习中非常重要。比如线性高斯模型是一整套体系，卡尔曼滤波就是一种线性高斯模型。 隐变量与隐变量，隐变量与观测变量之间服从高斯分布。\n1 2 3 z1 -\u0026gt; z2 -\u0026gt; ... -\u0026gt; zN ↓ ↓ ↓ ↓ x1 -\u0026gt; x2 -\u0026gt; ... -\u0026gt; xN 比如从 z⁽ᵗ⁾ ➔ z⁽ᵗ⁺¹⁾ 的转移服从 线性高斯：z⁽ᵗ⁺¹⁾=Az⁽ᵗ⁾+B+ε，先做线性变换再加上 ε 高斯噪声。\n再比如 P-PCA（概率PCA）中，原数据是 p 维的 x∈ℝᵖ 要降到 q 维空间 z∈ℝ^q， 假设 x 与 z 之间的变换为：x=Wz+μ+ε，ε～N(0,σ²⋅I)，0均值的各向同性的（即 Σ 是对角矩阵，并且对角线上的值都是σ²）高斯分布（标准正态分布）。 如果对角线上的值不相同，就变成了因子分析\n而且线性高斯模型有很多特性：比如一个高维的随机变量 x∈ℝᵖ 服从高斯分布 x～N(μ,Σ)，将它分成两个小组 x₁∈ℝᵐ, x₂∈ℝⁿ, m+n=p。 则 x₁ 也服从高斯分布，条件概率 x₂|x₁ 也服从高斯分布。\n参数估计 给定数据 𝐗：N个样本，每个样本 x 是 p 维的 x∈ℝᵖ，𝐗 就是一个 N x p 的矩阵：\n𝐗 = (x₁, x₂,\u0026hellip;, xₙ)ᵀ = $\\[^{^{ x₁ᵀ}\\_{x₂ᵀ}} \\_{^{⋱}\\_{ xₙᵀ}}]$ₙₓₚ\n$$ \\pmb X = (x_1, x_2, ...,x_N)^T \\\\\\ = \\begin{pmatrix} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1p} \\\\\\ x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2p} \\\\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \\\\\\ x_{p1} \u0026 x_{p2} \u0026 \\cdots \u0026 x_{pp} \\end{pmatrix}^T \\\\\\ = \\begin{pmatrix} x_{11} \u0026 x_{21} \u0026 \\cdots \u0026 x_{p1} \\\\\\ x_{12} \u0026 x_{22} \u0026 \\cdots \u0026 x_{p2} \\\\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \\\\\\ x_{1p} \u0026 x_{2p} \u0026 \\cdots \u0026 x_{pp} \\end{pmatrix} $$ $x_i \\in \\R^p$ : p维实数向量空间（p维欧氏空间），列向量 假设样本 xᵢ 之间是独立同分布，都是从一个高斯分布中抽出来的: xᵢ～N(μ,Σ)。\n令参数 θ = (μ,Σ) （Σ是协方差矩阵：对角线上是σ²，μ 是位置参数，σ是尺度参数）\nθ 的极大似然估计 (MLE)：θₘₗₑ = arg max_θ P(X|θ)\n为简化计算：令 p=1 (一维)，则 θ = (μ,Σ=σ²)，一维高斯分布的概率密度函数: p(x) = 1/(√(2π)⋅σ) ⋅ exp(-(x-μ)²/2σ²)\n$$ p(x) = \\frac{1}{\\sqrt{2π}⋅σ} exp(-\\frac{(x-μ)²}{2σ²}) $$高维（p维）的高斯分布的概率密度函数 (PDF)：p(x)=1/(2πᵖᐟ²⋅|Σ|¹ᐟ²) ⋅ exp(-½(x-μ)ᵀ⋅Σ⁻¹⋅(x-μ))\n$$ p(𝐱) = \\frac{1}{(2π)^{p/2} |Σ|^½} exp(-\\frac{(𝐱-\\bm μ)ᵀ(𝐱-\\bm μ)}{2Σ}) $$log P(X|θ) = log ∏ᵢ₌₁ᴺ p(xᵢ|θ) ，独立同分布，联合概率写成连乘 = ∑ᵢ₌₁ᴺ log p(xᵢ|θ) ，log把连乘变连加\n= ∑ᵢ₌₁ᴺ log 1/(√(2π)⋅σ) ⋅ exp(-(xᵢ-μ)²/2σ²)，代入高斯分布\n= ∑ᵢ₌₁ᴺ [ log 1/√(2π) + log 1/σ -(xᵢ-μ)²/2σ² ]\n先求最佳的 μ: 当对数似然最大时，μ 等于多少？\nμₘₗₑ = arg max_μ P(X|θ) = arg max_μ ∑ᵢ₌₁ᴺ [ log 1/√(2π) + log 1/σ -(xᵢ-μ)²/2σ² ] = arg max_μ ∑ᵢ₌₁ᴺ [ -(xᵢ-μ)²/2σ² ] ，只保留与μ相关的项 = arg min_μ ∑ᵢ₌₁ᴺ (xᵢ-μ)² ，σ² 一定是正的 对 μ 求偏导，令其等于0：\n∂ ∑ᵢ₌₁ᴺ (xᵢ-μ)² / ∂μ = ∑ᵢ₌₁ᴺ -2(xᵢ-μ) = 0 ∑ᵢ₌₁ᴺ (xᵢ-μ) = 0 ∑ᵢ₌₁ᴺ xᵢ - ∑ᵢ₌₁ᴺ μ = 0 ∑ᵢ₌₁ᴺ xᵢ = Nμ ，μ 与 i 无关 μₘₗₑ = 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ ，最优的 μ 就是样本的均值。 μₘₗₑ 是无偏的，因为 μₘₗₑ 的期望：\nE[μₘₗₑ] = 1/N ⋅ ∑ᵢ₌₁ᴺ [μₘₗₑ] = 1/N ⋅ ∑ᵢ₌₁ᴺ [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ ] = 1/N ⋅ ∑ᵢ₌₁ᴺ [μ] ，因为样本iid,服从高斯分布 = 1/N ⋅ Nμ = μ (真实的 μ)\nμₘₗₑ 的期望是无偏的，但每次的 μₘₗₑ 并不是真实的 μ，每次估出来的可能比μ大或者小，这个 μ 无法通过一次估计得到，只能是随着样本量的增加，把每次试验的结果求平均才会无限近似真实的 μ。 （但是样本不可能有无限个，所以实际上真实均值是无法得到的）\n求最优的 σ² 用类似的过程:\nσ²ₘₗₑ = arg max_σ² P(X|θ) = arg max_σ² ∑ᵢ₌₁ᴺ [ log 1/√(2π) + log 1/σ -(xᵢ-μ)²/2σ² ] = arg max_σ² ∑ᵢ₌₁ᴺ [ -log σ -(xᵢ-μ)²/2σ² ] ，只保留与σ²相关的项 目标函数对 σ 求偏导：\n∂ ∑ᵢ₌₁ᴺ [ -log σ -(xᵢ-μ)²/2σ² ] / ∂σ = ∑ᵢ₌₁ᴺ [-1/σ - (xᵢ-μ)²/2 ⋅ -2σ⁻³ = 0 ，两边同乘σ³ ∑ᵢ₌₁ᴺ [-σ² + (xᵢ-μ)² = 0 ，把∑ 带进去 ∑ᵢ₌₁ᴺ σ² = ∑ᵢ₌₁ᴺ (xᵢ-μ)² ，σ² 与 i 无关 N σ² = ∑ᵢ₌₁ᴺ (xᵢ-μ)² σ²ₘₗₑ = 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ-μₘₗₑ)²\n最优的 σ² 也是对样本方差求期望。 这个 σ²ₘₗₑ 是有偏估计，因为 σ²ₘₗₑ 的期望不等于 σ²：\n先对 σ²ₘₗₑ 做简化:\nσ²ₘₗₑ = 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ - μₘₗₑ)² = 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ² - 2xᵢμₘₗₑ + μₘₗₑ²) ， 展开括号的平方\n= 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - 1/N ⋅ ∑ᵢ₌₁ᴺ 2xᵢ⋅μₘₗₑ + 1/N ⋅ ∑ᵢ₌₁ᴺ μₘₗₑ²) ，∑带进去 = 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - 2μₘₗₑ² + μₘₗₑ² ，第2项里有个样本均值，第3项与i无关 = 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - μₘₗₑ²\n对 σ²ₘₗₑ 求期望：\nE[ σ²ₘₗₑ ] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ - μₘₗₑ)² ] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - μₘₗₑ² ] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² -μ² - (μₘₗₑ²-μ²) ] ，添加μ² 横等变换 = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² -μ²] - E [μₘₗₑ²-μ²] ，拆成2个期望 先看第 1 项： 把 μ² 写到 ∑ 里面： E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² -μ²] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ² -μ²)] ，μ²与i无关 = 1/N ⋅ ∑ᵢ₌₁ᴺ [E(xᵢ² -μ²)] ，里面是一个期望，把外面的期望展开 = 1/N ⋅ ∑ᵢ₌₁ᴺ [ E(xᵢ²) - E(μ²) ] ，μ²是常数 = 1/N ⋅ ∑ᵢ₌₁ᴺ [ E(xᵢ²) - μ² ] ，因为 μ 是随机变量 xᵢ 的期望：E(xᵢ)=μ = 1/N ⋅ ∑ᵢ₌₁ᴺ [ E(xᵢ²) - E(xᵢ)² ] ，这是xᵢ 的方差（定义）\n= 1/N ⋅ ∑ᵢ₌₁ᴺ σ² = σ²\n也就是说，第 1 项是 σ²，对于第 2 项： E [μₘₗₑ²-μ²] = E (μₘₗₑ²) - E(μ²) = E (μₘₗₑ²) - μ² = E (μₘₗₑ²) - E²(μₘₗₑ) = Var(μₘₗₑ) ，这是 μₘₗₑ 的方差 = Var( 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ ) ，把1/N 提出来，会变成平方 = 1/N² ⋅ ∑ᵢ₌₁ᴺ Var(xᵢ) = 1/N² ⋅ ∑ᵢ₌₁ᴺ σ² = 1/N ⋅ σ²\n所以 σ²ₘₗₑ 的期望等于上面两项相减： E[ σ²ₘₗₑ ] = σ² - 1/N ⋅ σ² = (N-1)/N ⋅ σ²\nσ² 的无偏估计是 σ² = 1/(N-1) ⋅ ∑ᵢ₌₁ᴺ (xᵢ-μₘₗₑ)²\n2. 高斯分布2-极大似然估计（无偏估计VS有偏估计） P2\n一个估计量 T(θ) 的期望 E(T(θ) 等于它的本身最初的值，这个参数的估计是无偏的，比如 E(μ^) = μ; E(σ^) = σ。如果不相等就是有偏的。\n用MLE估计出来的方差 σ²ₘₗₑ 的期望小于模型的真实方差。 因为计算方差时，计算的是样本到样本均值 的距离，而不是到真实均值的距离，因为真实均值要做无数次试验才能得到（除非μ=样本均值）。 如果用样本均值代替真实均值μ 的话：Var(x)= E [(x-μ)²] ➔ Var(x)= E [( x-x̄ )]²，除非 μ=样本均值 x⁻，Var(x) = E( x-x̄ )² \u0026lt; E(x-μ)²。 证明如下样本方差与总体方差 - 小时候挺菜 -博客园：\n(1/n)⋅∑ᵢ₌₁ᴺ(xᵢ-x⁻)² = (1/n)⋅∑ᵢ₌₁ᴺ [(xᵢ-μ)+ (μ-x⁻)]² = (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)² + (2/n)⋅∑ᵢ₌₁ᴺ(xᵢ-μ)⋅(μ-x⁻) + (1/n)⋅∑ᵢ₌₁ᴺ(μ-x⁻)² = (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)² + 2(x⁻-μ)(μ-x⁻) + (μ-x⁻)² = (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)² - (μ-x⁻)²\n如果 μ ≠ x⁻，则 (1/n)⋅∑ᵢ₌₁ᴺ(xᵢ-x⁻)² \u0026lt; (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)²\nMLE 是点估计，会造成偏差\n3. 高斯分布3-从概率密度函数角度观察 P3\n多维高斯分布的概率密度函数: 对于一个 p 维的随机向量 𝐱∈ℝᵖ 服从多维的高斯分布，其概率密度函数为：\n𝐱～N(𝛍,Σ)= 1/(2πᵖᐟ²⋅|Σ|¹ᐟ²) ⋅ exp(-½(𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍))， 其中μ 是期望, Σ是方差矩阵，exp里面-1/2后面是线代中的二次型\n对于一个样本（随机向量）p个维度: 𝐱 = (x1,\\ x2,\\ \u0026hellip;,\\ xp)\n𝛍 也是 p 维的向量：𝛍 = (μ1,\\ μ,\\ \u0026hellip;,\\ μp)\nΣ 就是 p×p 维的矩阵：\nΣ =(σ11, σ12, \u0026hellip;, σ1p σ21, σ22, \u0026hellip;, σ2p ⋮ ⋮ ⋮ ⋮ σp1, σp2, \u0026hellip;, σpp)\n通常，这个矩阵Σ是半正定的，而且是沿对角线对称的，比如 σ12 = σ21。 在本节中假设 Σ 是正定的，以便叙述。\n在 PDF 中，𝐱 是自变量，𝛍,Σ 是参数。式中与 𝐱 相关的只有 (𝐱-𝛍)ᵀ，其他部分认为是系数，所以集中看一下 exp 中的部分：\n(𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) 是一个标量，这个函数可以看作 马氏距离，两个向量：𝐱 和 𝛍 之间的距离\n马氏距离 Mahalanobis distance is used to measure multivariate distances between a point and a normal distribution with covariance 𝚺. janakiev-blog\n假设有两个二维的向量：𝐳1=(z11,\\\\ z12)，𝐳2=(z21,\\\\ z22)\n根据定义求两向量之间的距离： (𝐳1-𝐳2)ᵀ ⋅ Σ⁻¹ ⋅ (𝐳1-𝐳2) = (z11-z21, z12-z22)ᵀ ⋅ Σ⁻¹ ⋅ (z11-z21, \\\\ z12-z22)\n如果方差矩阵 Σ 是单位矩阵：Σ=I，马氏距离就是欧氏距离\n(z11-z21, z12-z22)ᵀ ⋅ I ⋅ (z11-z21, \\\\ z12-z22) = (z11-z21)² + (z12-z22)²\n方差矩阵 因为假设了 Σ 是正定的（每个特征值λᵢ都是大于0的，不能等于0），而且是对称的，所以对 Σ 做一个特征值分解：\nΣ = UΛUᵀ，其中 U 是正交矩阵：UUᵀ=UᵀU=I，U=(𝐮₁,𝐮₂,\u0026hellip;, 𝐮ₚ)，每个小 𝐮 是列向量，所以U是p×p的矩阵；Λ 是对角的：Λ=diag(λᵢ), i=1,\u0026hellip;p；\n把矩阵形式展开：\nΣ = UΛUᵀ = (𝐮₁,𝐮₂,\u0026hellip;, 𝐮ₚ) ⋅\n(λ₁, 0, 0, \u0026hellip; 0 0, λ₂, 0, \u0026hellip; 0 ⋮⋮⋮⋮ 0, 0, 0, \u0026hellip; λₚ) ⋅ (𝐮₁ᵀ,\\\\ 𝐮₂ᵀ,\\\\ \u0026hellip;,\\\\ 𝐮ₚᵀ) = (𝐮₁λ₁, 𝐮₂λ₂, \u0026hellip;, 𝐮ₚλₚ) ⋅ (𝐮₁ᵀ,\\\\ 𝐮₂ᵀ,\\\\ \u0026hellip;, 𝐮ₚᵀ) = ∑ᵢ₌₁ᵖ 𝐮ᵢλᵢ𝐮ᵢᵀ\nΣ⁻¹ = (UΛUᵀ)⁻¹ = (Uᵀ)⁻¹ Λ⁻¹ U⁻¹ = U Λ⁻¹ U⁻¹ ，（正交矩阵的转置等于它的逆），其中特征值矩阵 Λ⁻¹ = diag(1/ λᵢ), ᵢ=1,\u0026hellip;,p = ∑ᵢ₌₁ᵖ 𝐮ᵢ (1/λᵢ) 𝐮ᵢᵀ\n把 Σ⁻¹ 代入马氏距离：\n(𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) = (𝐱-𝛍)ᵀ ⋅ ∑ᵢ₌₁ᵖ [ 𝐮ᵢ (1/λᵢ) 𝐮ᵢᵀ ] ⋅ (𝐱-𝛍) ，把(𝐱-𝛍) 放到 Σ 里面 = ∑ᵢ₌₁ᵖ [ (𝐱-𝛍)ᵀ ⋅ 𝐮ᵢ (1/λᵢ) 𝐮ᵢᵀ ⋅ (𝐱-𝛍) ]\n定义一个向量 𝐲 = (y₁, y₂, \u0026hellip;, yₚ)，其中每一维是一个标量：yᵢ = (𝐱-𝛍)ᵀ ⋅ 𝐮ᵢ 所以上面的马氏距离可以写成： (𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) = ∑ᵢ₌₁ᵖ [ yᵢ (1/λᵢ) yᵢᵀ ] = ∑ᵢ₌₁ᵖ [ yᵢ² (1/λᵢ) ] ， yᵢ 是标量，就是平方\n假设 p=2，马氏距离= y₁²/λᵢ + y₂²/λ₂，而且令 马氏距离= 1: y₁²/λ₁ + y₂²/λ₂ = 1，就得到一个椭圆曲线\nx1 和 x2 是原来的坐标轴，𝐮₁, 𝐮₂ 是新的基向量。把 xᵢ 变换成 yᵢ 就是向量 𝐱 减去均值之后（去中心化），然后投影到向量 𝐮ᵢ 上得到的坐标。\n𝐱 和 𝛍 之间的马氏距离，在𝐮₁, 𝐮₂ 的坐标系下是一个椭圆曲线，不同的样本x对应曲线上不同的点。 如果 λ₁ \u0026gt; λ₂，则半长轴=√λ₁；半短轴=√λ₂\n𝐱 和 𝛍 之间的马氏距离可以是任意值，即椭圆方程不等于1，取不同的值 r：\ny₁²/λ₁ + y₂²/λ₂ = r\n因为马氏距离是概率密度中的一项，马氏距离不同对应的概率就不同，r-\u0026gt; p(r)\n当 r 取不同值的时候，就是一圈一圈的椭圆。对于一个两维的随机变量 𝐱=(x1,x2)，它的概率值y是第3维，所以在这个三维空间中，如果固定 y 值（即r值），就是对曲面水平横切了一刀，得到一条等高线，是在特征向量 𝐮 下的椭圆\n如果 Σ 分解出来的所有的特征值 λᵢ 都相同，曲线就变成圆了，在各个轴上的投影都相等。\n4. 高斯分布4-局限性 P4\n在高维高斯分布的概率密度函数中，只有 exp 里面的“二次型”与样本 x 相关，而且是𝐱 和 𝛍 之间的马氏距离， 对正定矩阵 Σ 做特征值分解，可以发现“二次型”对应于以特征向量 𝐮 为基底的坐标系下的标准椭圆曲线，\n用 Δ 表示二次型，则概率密度函数：p(x)=1/(2πᵖᐟ²⋅|Σ|¹ᐟ²) ⋅ exp(-Δ/2)\n在概率密度函数中，只有 x 是自变量，𝛍,Σ 都是模型的参数。 给定一个概率值 0 \u0026lt;= p(x) \u0026lt;= 1，比如 0.5，它会对应一个 Δ= r1，对应到椭圆方程的右侧，再把 r1 乘到左边就变成了标准的椭圆方程。 当 p=0.5, 则 Δ= r2，会得到另一个椭圆曲线。 如果把所有概率取一遍，就对应一个曲面。椭圆对应一条条等高线\n(1) 方差矩阵参数太多 Σₚₓₚ 有 p² 个参数，但又因为它是对称的，实际的参数个数= (p²-p)/2 + p = (p²+p)/2 = p(p+1)/2 = O(p²)\n时间复杂度是维数的平方，在高维问题中，p 会很大，参数太多，计算太复杂。\n可以简化方差矩阵：假设 Σ 是对角矩阵，只有对角线上有值 (λ₁ 0 0 \u0026hellip; 0 \\ 0 λ₂ 0 \u0026hellip; 0 \\ \u0026hellip; \\ 0 0 0 \u0026hellip; λₚ)\n因为它是对角矩阵（已经相互独立），就不需要对它做特征值分解了，Σ就用自己，没有引入新基向量 U=(𝐮₁,𝐮₂,\u0026hellip;, 𝐮ₚ)，U就是单位矩阵，所以基向量还是 (𝐱₁,𝐱₂,\u0026hellip;, 𝐱ₚ)，仍向𝐱投影 journeyc 的评论\nΔ = (𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) = ∑ᵢ₌₁ᵖ (𝐱ᵢ-𝛍ᵢ)² (1/λᵢ)\n在各个轴上的投影为 yᵢ = (𝐱-𝛍)ᵀ ⋅ 𝐱ᵢ\n所以这里的椭圆就是在 (𝐱₁,𝐱₂,\u0026hellip;, 𝐱ₚ) 下的，长短轴与各x轴平行，而没有旋转\n如果 ∑ 是对角阵，并且 λ₁ = λ₂ = \u0026hellip; = λₚ，曲线就变成“正的”圆形，称为各向同性的高斯分布\n通过简化方差矩阵，变成各向同性的，解决参数过多的问题。 比如在因子分析中，假设隐变量 z 是对角矩阵。概率PCA （P-PCA) 就是因子分析的特殊情况，假设 z 是各向同性的概率分布\n(2) 一个高斯表达力有限 仅用一个高斯分布，对模型的描述可能不准确。GMM 是多个高斯分布的混合。\n5. 高斯分布5-已知联合概率求边缘概率及条件概率 P5\n已知一个多维高斯分布，求它的边缘概率分布，和条件概率分布\n把 p 维的随机向量 𝐱 看作两组的联合：𝐱=(𝐱ₐ,\\ 𝐱b)，其中 xₐ∈ℝᵐ, xb∈ℝⁿ, m+n=p。 把 p 维的期望 𝛍 也分成两组: 𝛍 = (𝛍ₐ,\\ 𝛍b)； 把方差矩阵 ∑ 拆成 4 块：∑ = (∑ₐₐ , ∑ₐb \\ ∑bₐ , ∑bb)，这是对称矩阵，所以∑ₐb = ∑bₐ\n然后把随机向量 𝐱 看作是 (𝐱ₐ,𝐱b) 的联合概率分布，求边缘概率分布 P(𝐱ₐ)，以及条件概率分布 P(𝐱b|𝐱ₐ)。根据对称性，P(𝐱b) 和 P(𝐱ₐ|𝐱b) 也可求得\n求解方法：配方法（PRML中），这节会采用一种比配方法稍微简单一点的方法\n首先引入一个定理： 已知一个随机变量 x 服从高斯分布 x～N(μ,Σ)，有 y=Ax+B，则 y 也服从高斯分布： y～N(Aμ+B, AΣAᵀ)。 （x 是p维向量，y 是 q 维向量，则 A 是qxp 的矩阵，Σ是pxp的，则 AΣAᵀ 是qxq的）\n不严谨的解释：\nE[y] = E[Ax+B] = AE[x]+B = Aμ+B\nVar[y] = Var [ Ax+B ] = Var [Ax] + Var [B] , B是常数方差=0 = A Var [x] Aᵀ = AΣAᵀ\n比如对一维随机变量 x～N(μ, σ²)，y=ax+b，则 Var[y] = Var[ax+b] = a²Var[x] = a²⋅σ²。直观上看，一维是a²，多维应该是AAᵀ\n求边缘概率 P(𝐱ₐ) 的分布 构造一个矩阵： 𝐱ₐ = (Iₘ 0) (𝐱ₐ \\ 𝐱b)\n其中 (Iₘ 0) 对应 A，(𝐱ₐ \\ 𝐱b) 就是 x，B=0\n根据上述定理:\nE[𝐱ₐ] = E[A𝐱+B] = A𝛍+B = (Iₘ 0) (𝛍ₐ,\\ 𝛍b) = 𝛍ₐ\nVar[𝐱ₐ] = AΣAᵀ = (Iₘ 0) (∑ₐₐ , ∑ₐb \\ ∑bₐ , ∑bb) (Iₘ \\ 0) = (Iₘ ∑ₐₐ, Iₘ ∑ₐb) (Iₘ \\ 0) = ∑ₐₐ\n所以 𝐱ₐ～N(𝛍ₐ, ∑ₐₐ)\n求条件概率 P(𝐱b|𝐱ₐ) 的分布 （可用配方法）\n构造3个变量：\n先定义一个 x_{b⋅a} 的变量：x_{b⋅a} = x_b - ∑_bₐ ∑ₐₐ⁻¹ xₐ 。 如果 x_{b⋅a} 的分布知道了 x_{b⋅a}～N(𝛍^, ∑^)，那么 x_b = x_{b⋅a} + ∑_bₐ ∑ₐₐ⁻¹ xₐ，x_b 与 xₐ 之间的关系就找到了。 rationalizable 的评论：“如果能找到一个线性变换 Z = Xa+C⋅Xb，使得Z与Xb不相关 Cov(Z, Xb)=0，那么 Var(Xa|Xb) = Var(Z|Xb) = Var(Z)，E(Xa|Xb)=E(Z)-C⋅Xb，就都可以计算出来了。”\n与此对应，定义 𝛍_{b⋅a} = 𝛍_b - ∑_bₐ ∑ₐₐ⁻¹ 𝛍ₐ。\n因为 ∑ 是分块矩阵，把∑{bb⋅a} 定义为∑{aa} 的舒尔补Schur complement: ∑{bb⋅a} =∑{bb} - ∑{ba} ∑{aa}⁻¹ ∑_{ab}\n(1) 先求 x_{b⋅a} 的分布: x_{b⋅a} = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) (xₐ,\\\\ x_b) = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) 𝐱\n所以 (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) 就是 A\nE[x_{b⋅a}] = A𝛍+B = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ)⋅(𝛍ₐ,\\\\ 𝛍b) = 𝛍b - ∑_bₐ ∑ₐₐ⁻¹⋅𝛍ₐ = 𝛍_{b⋅a}\nVar[x_{b⋅a}] = AΣAᵀ = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) ⋅ (∑ₐₐ , ∑ₐb \\\\ ∑bₐ , ∑bb) ⋅ ((-∑_bₐ ∑ₐₐ⁻¹)ᵀ,\\\\ Iₙ) = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) ⋅ (∑ₐₐ , ∑ₐb \\\\ ∑bₐ , ∑bb) ⋅ (-∑ₐₐ⁻¹ ∑_bₐᵀ, \\\\ I) ，其中 ∑ₐₐ⁻¹ 是对称的，转置没变 = (- ∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐₐ + ∑bₐ, -∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐb + ∑bb) ⋅ (-∑ₐₐ⁻¹ ∑_bₐᵀ, \\\\ I)\n在第 1 项中，∑ₐₐ 是可逆，∑ₐₐ⁻¹ ⋅ ∑ₐₐ = I，然后 -∑_bₐ + ∑bₐ = 0：\n= (0, -∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐb + ∑bb) ⋅ (-∑ₐₐ⁻¹ ∑_bₐᵀ, \\\\ I)\n因为两个向量变量的协方差 Cov(X,Y) 与 Cov(Y,X) 互为转置矩阵，但它自身并不是对称矩阵，因此转置并不是它自己: ∑_bₐᵀ ≠ ∑_bₐ\n= ∑bb - ∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐb ，就是定义的 ∑_{bb⋅a}\n（弹幕：应该是根据舒尔补反向构造的 x_{b⋅a} 和 𝛍_{b⋅a}。\u0026ldquo;构造型证明\u0026rdquo;?）\n所以得出结论： x_{b⋅a} ～ N(𝛍_{b⋅a}, ∑_{bb⋅a})\n(2) 证明：x_{b⋅a} 与 xₐ 相互独立 专栏\n若 x 为服从高斯分布的随机变量 x～N(μ,Σ)，则相互独立的两变量就不相关： M⋅x ⟂ N⋅x ⇔ M∑N =0。 其中 M，N 均为矩阵，M⋅x，N⋅x 也服从高斯分布。\n证：因为 x～N(μ,Σ)，所以 M⋅x～N(Mμ, MΣMᵀ)，N⋅x～N(Nμ, NΣNᵀ)，计算二者的协方差矩阵：\nCov(M⋅x，N⋅x) = E[(M⋅x-M⋅μ) (N⋅x-N⋅μ)ᵀ] = E[ M ⋅ (x-μ)⋅(x-μ)ᵀ ⋅ N] = M⋅E[ (x-μ) (x-μ)ᵀ ]⋅N = MΣNᵀ\n因为 M⋅x ⟂ N⋅x 且均为高斯分布，所以 Cov(M⋅x，N⋅x) = MΣNᵀ = 0\n在之前的推导中，引入了 x_{b⋅a} = x_b - ∑_bₐ⋅∑ₐₐ⁻¹ ⋅ xₐ 可以改写为：\nx_{b⋅a} = (-∑_bₐ⋅∑ₐₐ⁻¹, I) (xₐ,\\\\ x_b)，这里 (-∑_bₐ⋅∑ₐₐ⁻¹, I) 对应 M，(xₐ,\\\\ x_b) 是 x\nxₐ = (I, 0) (xₐ,\\\\ x_b)，这里(I, 0)对应 N，(xₐ,\\\\ x_b) 是 x\nx_{b⋅a} 就是 M⋅x，xₐ 就是N⋅x， 所以 MΣNᵀ = (-∑_bₐ⋅∑ₐₐ⁻¹, I) (∑ₐₐ , ∑ₐb \\\\ ∑bₐ , ∑bb) (I,\\\\ 0)，其中∑是x的方差矩阵 = (0, ∑bb-∑_bₐ⋅∑ₐₐ⁻¹) (I,\\\\ 0) = 0\n所以 x_{b⋅a} 与 xₐ 相互独立可推出 x_{b⋅a} 与 xₐ 不相关： x_{b⋅a} ⟂ xₐ ⇒ x_{b⋅a} | xₐ = x_{b⋅a}\n注意：\n一般情况下两个随机变量之间独立一定不相关，不相关不一定独立（也就是独立的概念更“苛刻”一点，不相关的概率稍微“弱”一点） 如果两个随机变量均服从高斯分布，那么“不相关”等价于“独立” 随机变量独立是由分布函数定义的，而不相关只是用一阶矩（即数学期望）定义的。分布函数是比矩更高的概念，分布函数能决定矩，而矩未必能决定分布函数。 独立和互斥是什么关系？独立和不相关是什么关系？ - 武辰的文章 -知乎\n(3) 再求 xb|xₐ 的分布 知道了 x_{b⋅ₐ} 的分布后，可知 x_b：\nx_b = x_{b⋅a} + ∑_{ba} ∑ₐₐ⁻¹ xₐ\n因为 x_b 与 xₐ 是相互独立的服从高斯分布的随机变量，所以：\nx_b|xₐ = x_{b⋅a}|xₐ - ∑{ba} ∑ₐₐ⁻¹ xₐ | xₐ = x_b = x{b⋅a} - ∑_{ba} ∑ₐₐ⁻¹ xₐ\n还是同样套用Ax+B，把 x_{b⋅a} 看作 x, A=I, B=∑_bₐ ⋅ ∑ₐₐ⁻¹ ⋅ xₐ\nE [x_b|xₐ] = 𝛍_{b⋅a} + ∑_bₐ⋅∑ₐₐ⁻¹⋅xₐ\nVar [x_b|xa] = A⋅Var(x_{b⋅a})⋅Aᵀ = ∑_{bb⋅a}\n所以 xb|xₐ ～N ( 𝛍_{b⋅a} + ∑bₐ⋅∑ₐₐ⁻¹⋅xₐ, ∑{bb⋅a} )\n用同样的方法，可得 x_b～N(𝛍_b, ∑_{bb})。\nP(xₐ|x_b) 就是把 P(xb|xₐ) 中的 a,b 对换。\n6. 高斯分布6-已知边缘和条件概率求联合概率分布 P6\n已知（边缘概率分布）p(x) = N(x | μ,Λ⁻¹) 和（条件概率分布）p(y|x) = N(y | Ax+b, L⁻¹) （其中 Λ 是精度矩阵=协方差矩阵 ∑ 的逆，Λ⁻¹=∑）， 并且假设 y 的期望与 x 之间有线性关系：μ_y = Ax+b，但二者的方差之间没关系 求 p(y)，p(x|y)。（类似于贝叶斯定理：p(x|y) = p(y|x)p(x) / p(y)）\n这个问题经常在线性高斯模型中出现，比如在卡尔曼滤波中，隐状态（高斯分布）与观测变量之间有线性关系： z⁽ᵗ⁺¹⁾ = A z⁽ᵗ⁾ + B + ε，ε是噪声，ε～N(0,Q)，ε与z 相互独立， x⁽ᵗ⁾= Cz⁽ᵗ⁾+D+δ，δ也是高斯噪声 δ～N(0,R)\n还比如在概率pca中，把 p 维的 x 降到 q 维的 z 空间，满足线性关系 x = Wz+b+ε，ε 是服从各向同性的高斯分布 ε～N(0, σ²I)，z 的先验可以是标准高斯分布 z～N(0,I)，ε 与 z 相互独立。也是一种线性高斯模型。\n（在PRML 中仍使用配方法求解，而且比上一节的推导更复杂）\n(1) 求 p(y) 依据两个前提条件，可以把 y 定义为： y = Ax + b + ε，令 ε 是一个高斯噪声 ε～N(0, L⁻¹)，x、y、ε 都是随机变量，ε和x相互独立，A和b 都是系数。 （y 是在 x 给定的情况下发生的，固定了x则它的方差为0，所以方差L⁻¹中不含 x 的方差）\nE[y] = E[Ax + b + ε] = E [ Ax+b ] + E [ε] ，按照上一节引入的定理\n= Aμ+b ，ε的期望是0\nVar[y] = Var[Ax + b + ε] = Var[ Ax+b ] + Var [E] = A⋅Λ⁻¹⋅Aᵀ + L⁻¹\n所以就得到了 y 的分布： y～N(Aμ+b, A⋅Λ⁻¹⋅Aᵀ + L⁻¹)\n(2) 求 p(z) 构造变量 z 是 x 和 y 的联合：z = (x,\\ y)。 （“任意个有限维的高斯分布的联合分布均是高斯分布”，是说两个随机变量合起来的分布还是高斯，并不是GMM（对似然加权）正态分布随机变量的和还是正态分布吗？ - 江城雨-知乎。幽冥若炎的评论：不需要两个多维随机变量相互独立？x与y独立，因为x与ε独立）\n根据上一节的结论，边缘概率分布就相当于仅仅考虑一部分维度，所以z服从的分布的期望和方差就是 x 的μ,∑ 与 y 的μ,∑ 拼起来：\nE[z] = [μ,\\\\ Aμ+b]\nVar[z] = [Λ⁻¹, unknown,\\\\ unknown, A⋅Λ⁻¹⋅Aᵀ + L⁻¹]\n也就是： z = (x,\\ y) ～ N ( [μ,\\\\ Aμ+b], [Λ⁻¹, unknown,\\\\ unknown, A⋅Λ⁻¹⋅Aᵀ + L⁻¹] )\n因为方差矩阵本身应该是对称的，所以两个 unknown 是一样的，记为 Δ，它的最后结果里面不应该出现x 和 y。\nΔ = Cov(x,y) = E[ (x-E[x]) ⋅ (y-E[y])ᵀ ] = E [ (x-μ) ⋅ (y-(Aμ+b)ᵀ) ，把 y 的表达式代入 = E [ (x-μ) ⋅ (Ax + b + ε -Aμ -b)ᵀ) = E [ (x-μ) ⋅ (Ax - Aμ + ε)ᵀ) ，两项里有共同的(x-μ) = E [ (x-μ) ⋅ (Ax - Aμ)ᵀ + (x-μ) ⋅ εᵀ) = E [ (x-μ) ⋅ (Ax - Aμ)ᵀ ] + E [ (x-μ) ⋅ εᵀ ]\n因为 x 与 ε 独立：x⟂ε，所以 x-μ 与 ε 独立，所以第2个期望可拆开： E [ (x-μ) ⋅ εᵀ ] = E [ (x-μ) ] ⋅ E[ εᵀ ]，又因为 ε 的期望等于0，所以整项都=0，所以就剩第1项\nΔ = E [ (x-μ) ⋅ (Ax - Aμ)ᵀ ] = E [ (x-μ) ⋅ (x - μ)ᵀ ⋅ Aᵀ ] ，A不是随机变量 = E [ (x-μ) ⋅ (x - μ)ᵀ ] ⋅ Aᵀ，第1项是x的方差 = Var[x] ⋅ Aᵀ = Λ⁻¹ ⋅ Aᵀ\n所以 z 的方差矩阵： Var(z) = [Λ⁻¹, Λ⁻¹ ⋅ Aᵀ,\\\\ Λ⁻¹ ⋅ Aᵀ, A⋅Λ⁻¹⋅Aᵀ + L⁻¹]\n(3) 求 p(x|y) 根据上一节的结论：已知 x=(xₐ,\\ x_b)，则有 P(x_b | xₐ) = N(𝛍_{b⋅a} + ∑_bₐ⋅∑ₐₐ⁻¹⋅xₐ, ∑_{bb⋅a})。\nz 对应 x, xₐ,x_b 对应 x,y，z 的期望和方差已知，代入即可。\n7. 不等式1-Jensen不等式 P7\n比如在 EM 算法推导时，会用到此不等式\n假设 f(x) 是凸 convex function 则该函数的期望大于等于期望的函数： E(f(x)) ≥ f(E(x))\n(1) 证明 （以下是一个构造性证明）\n有一个凸函数 f(x)，在 x 轴上取 x 的期望值 E[x]，它对应的函数值是 f(E[x])，过这个函数值做这个凸函数的切线，将这条切线定义为 l(x) = ax+b。\nf(E[x]) = l(E[x]) = aE(x)+b\n因为 f(x) 是凸函数，所以对于任意的 x 都有 f(x)≥l(x)，对此式两边同时求期望： E[ f(x) ] ≥ E [ l(x) ] = E[ax+b] = E[ax] + E[b] = aE[x] +b = f(E[x])\n也就是 E[ f(x) ] ≥ f(E[x])\n死神之名111 的评论： “第七节：很不幸，这个推导是错误的，你先说了f=l，你其实只说明了线性函数的jensen不等式成立，真正严格证明你需要做两个积分差，再利用凸性，二阶导＞0，综合一下就是当前结果。”\n(2) 直观理解 通常的表述：在 x 轴上取两个点 a 和 b，然后在两点之间任意选取一个 c 点，选取时通常先在 [0,1] 上去一个 t 值，然后做“线性插值”：c=ta+(1-t)b。\nf(a) 和 f(b) 连线为函数 g，则 c 的函数值为 g(c)，可以看到 g(c) ≥ f(c)\n设线段ac 的长度是 t，线段 cb 的长度是 1-t（实际应该是ta 和(1-t)b），所以两梯形的斜边之比也是 t:1-t，\n然后分别过 g(c) 和 f(a) 做水平线，形成相似三角形，则 f(b)-g(c) 与 g(c)-f(a) 之比是 1-t : t。\n所以 g(c) = t ⋅ f(a) + (1-t) ⋅ f(b)\n代入 g(c) ≥ f(c) 就是： t ⋅ f(a) + (1-t) ⋅ f(b) ≥ f(t⋅a+(1-t)⋅b)\n","date":"2022-12-28T00:31:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/02_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","title":"watch: ML - 白板 02 | Mathematical Basis"},{"content":"Difference between plt and fig,ax object-oriented\nInverse the X-axis GeeksforGeeks\n1 2 3 import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.invert_xaxis() Add caption below the x-label SO\n1 2 3 fig, ax = plt.subplots() txt = \u0026#39;near and far are distance\u0026#39; fig.text(x=.5, y=.001, s=txt, ha=\u0026#39;center\u0026#39;) 1 2 3 fig, ax = plt.subplots() ax.set_xlabel(\u0026#39;\u0026#39;\u0026#39;z_cam near and far are distance\u0026#39;\u0026#39;\u0026#39;) 微分柱形图 SO\nSet xticks xticks is the (fixed) number on the x-axis, while xticklabels can be customized strings.\nFor plt (Matplotlib.pyplot.xticks()): Matplotlib Set_xticks - Detailed Tutorial - Python Guides\nMatplotlib.axes.Axes.set_xticklabels() Matplotlib Set_xticklabels -Python Guides\n1 2 3 fig, ax = plt.subplots() ax.set_xticks([0, np.pi, 2*np.pi, 3*np.pi]) ax.set_xticklabels(labels=[\u0026#39;0\u0026#39;, r\u0026#39;$\\pi$\u0026#39;, r\u0026#39;2$\\pi$\u0026#39;, r\u0026#39;3$\\pi$\u0026#39;], fontdict=None, fontsize=6, fontstyle=\u0026#39;italic\u0026#39;, color=\u0026#39;red\u0026#39;, verticalalignment=\u0026#39;top\u0026#39;, horizontalalignment=\u0026#39;left\u0026#39;, rotation=90, minor=True, Change the number and text of the showing Ticks GfG\nFor plt:\n1 2 plt.xticks(ticks=[1,2,3,4], labels=None, **kwargs) plt.yticks(ticks=[7,13,24,22], labels=None,) Use locator_param() to change the tightness and number of ticks.\nplt.locator_params(axis='both', nbins=4)\nUse xlim() to restrict the diplayed area, rather the whoel plot.\n1 2 plt.xlim(0,3) plt.locator_params(axis=\u0026#39;x\u0026#39;, nbins=3) Use Matplotlib.ticker.MaxNLocator class\nSet Number of Ticks in Matplotlib | Delft Stack\n1 2 3 4 5 6 7 8 from matplotlib.ticker import MaxNLocator fig, ax = plt.subplots(1,1) ax.yaxis.set_major_locator(MaxNLocator(5)) # hide specific ticks for i, tick in enumerate(ax.xaxis.get_ticklabels()): if i%2 !=0: tick.set_visible(False) Set y-axis view limits Matplotlib.axes.Axes.set_ylim() in Python - GeeksforGeeks\n1 2 3 4 5 6 7 fig, ax = plt.subplots(facecolor =\u0026#39;# A0F0CC\u0026#39;) x, y = 4*(np.random.rand(2, 100) - .5) ax.plot(x, y, \u0026#39;g\u0026#39;) ax.set_ylim(-3, 3) # 游标 cursor = Cursor(ax, useblit = True, color =\u0026#39;red\u0026#39;, linewidth = 2) Set the 1st x as 1 (not 0) Python MatplotLib plot x-axis with first x-axis value labeled as 1 (instead of 0)\nTicks Add extra ticks, labels Add extra ticks which must be number, not work for string SO\n1 2 3 fig, ax= plt.subplots() extraticks = [2.1, 3, 7.6] ax.set_xticks(list(ax.get_xticks()) + extraticks) Use ax.set_xticklabels() for string Create label list - SO\n1 2 3 4 5 near, far = 1, 15 ax.set_xticks(list(ax.get_xticks()) + [-near,-far]) # Their labels also have to be appended at the end. xticklabels= [\u0026#39;-16\u0026#39;, \u0026#39;-14\u0026#39;, \u0026#39;-12\u0026#39;, \u0026#39;-10\u0026#39;, \u0026#39;-8\u0026#39;, \u0026#39;-6\u0026#39;, \u0026#39;-4\u0026#39;, \u0026#39;-2\u0026#39;,\u0026#39;0\u0026#39;, \u0026#39;-near\u0026#39;, \u0026#39;-far\u0026#39;] ax.set_xticklabels(xticklabels, rotation=90) Example: Mark the yticks for the highest and lowest points:\n1 2 3 4 5 6 7 rmse = np.load(\u0026#34;./error_list.npz\u0026#34;)[\u0026#39;Train_RMSE\u0026#39;] idx = np.arange(len(rmse)) fig, ax = plt.subplots() ax.plot(idx, rmse) extraticks = [rmse.min(), rmse.max()] ax.set_yticks(list(ax.get_yticks()) + extraticks) Example: 把新 ticklabel 加入并重新排列\nReferences:\nGemini 2.5P - Forcing Matplotlib xticklabels Replace xticklabels\nReferences:\nGemini 2.5P - Supports:\n(2025-08-22T12:55)\nRebuild the label list\n1 2 3 4 5 6 7 8 9 10 11 # 3. Define the desired tick locations and get their labels # The x-values where you want to place ticks new_locations = [5, 15, 20, 25, 30] # Get the corresponding labels directly from the DataFrame using a list comprehension # This assumes the index of the DataFrame corresponds to the x-values of your plot new_labels = [df[df.columns[0]][loc] for loc in new_locations] # 4. Set the new ticks and labels on the plot ax1.set_xticks(new_locations) ax1.set_xticklabels(new_labels, rotation=90) # Use rotation to prevent labels from overlapping Set xticks as integer (Not sure) ax.xaxis.set_major_locator(MaxNLocator(integer=True))\nPython：使用f-string保留小数点位数 csdn\n用decimal 模块：w3cschool\n1 2 3 from decimal import Decimal a = 12.345 Decimal(a).quantize(Decimal(\u0026#34;0.00\u0026#34;)) # 使用默认的进位方式（同round）“0.00”表示保留小数点后两位 Hide ticks Remove ticks\nReferences:\nGfG - How to Hide Axis Text Ticks or Tick Labels in Matplotlib? Supports:\nRemove tick labels by setting them to an empty list r1-GfG\n1 2 fig, ax1 = plt.subplots(1,1) ax1.set_xticks([]) Set tick bold, color (2024-08-04)\nSet an individual xtick label to be bold:\n1 ax.get_xticklabels()[label_ls.index(\u0026#34;Input point cloud\u0026#34;)].set_weight(\u0026#39;bold\u0026#39;) Ref: How to set the matplotlib axes tick labels fontweight to bold? - SO (Searched by \u0026ldquo;python matlibplot.pyplot xtick label bold\u0026rdquo; in DDG) Set ticklabel color\nReferences:\nChange Single Matplotlib Xtick Color Supports:\n(2025-08-11T22:40)\nax.get_xticklabels()[2].set_color('red') Fontsize of yticks References:\nAsked ChatGPT Set the fontsize of numbers, not labels for yticks:\n1 2 3 4 x = [1, 2, 3, 4] y = [1e6, 2e6, 2.5e6, 3e6] fig, ax = plt.subplots() ax.tick_params(axis=\u0026#39;y\u0026#39;, labelsize=14) Set font size for the scale factor (offset text) in the scientific notation\n1 2 3 4 5 6 7 8 9 10 11 12 13 import matplotlib.pyplot as plt x = [1, 2, 3, 4] y = [1e6, 2e6, 2.5e6, 3e6] fig, ax = plt.subplots() ax.plot(x, y) # Enable scientific notation on the y-axis ax.ticklabel_format(axis=\u0026#39;y\u0026#39;, style=\u0026#39;sci\u0026#39;, scilimits=(-2, 2)) # Set font size for the scale factor (offset text) ax.yaxis.get_offset_text().set_fontsize(14) plt.show() N u m b e r s 3 2 2 2 2 1 1 1 1 . . . . . . . . . 0 7 5 2 0 7 5 2 0 0 5 0 5 0 5 0 5 0 1 e 6 Margin to leave more space around the figure to show the ticks of the boundary Geeksforgeeks\nOr prevent the markers get clipped by the axes GfG plt xticks\n1 2 fig, ax = plt.subplots() ax.margins(0.5) Legend References:\ngfg Docs How to add legend below subplots in matplotlib? - SO How to specify legend position in graph coordinates - SO Searched by python matplotlib ax.legend specify positional coordinates in DDG Notes:\nSet legend to ax:\n1 2 ax.plot([1, 2, 3], label=\u0026#39;Inline label\u0026#39;) ax.legend(loc=\u0026#39;best\u0026#39;, bbox_to_anchor=(0.5, 0., 0.5, 0.5)) Put it below the subplots r3-SO:\n1 2 ax.legend(loc=\u0026#34;upper center\u0026#34;, bbox_to_anchor=(0.5, -0.13), fancybox=False, shadow=False, ncol=5, fontsize=6) (2024/12/04)\nSpecify the accurate posistion of legend r4-SO:\n1 ax.legend(loc=(0.25, 0.67)) 1 is for the right-bottom corner of the legend at the top edge of the plot. Draw a horizontal line plt : gfg\n1 plt.axhline(y=0.5, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;) ax: SO\n1 ax.hlines(y=0.2, xmin=0.01, xmax=20, linewidth=2, color=\u0026#39;r\u0026#39;) Tight layout 1 plt.tight_layout() Or: plt.rcParams[\u0026quot;savefig.bbox\u0026quot;] = 'tight' (2024-07-23)\nOr:\n1 2 3 4 5 fig, ax = plt.subplots() fig.tight_layout() ax.set_title(\u0026#34;Tweaking points within triangle\u0026#34;, fontsize=16) ax.set(xlim =(0, 2.5), ylim =(-1.25, 0.5)) Save as png gfg\n1 2 3 4 ax.set_facecolor(\u0026#39;pink\u0026#39;) # inner background plt.savefig(\u0026#39;depth_buffer.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, dpi =100, facecolor=\u0026#34;white\u0026#34;) # bkg color plt.savefig(\u0026#39;output.jpg\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, dpi =40) #is smaller Draw markers on axes plt.plot(x,y, zorder=10, clip_on=False)\nplotting markers on top of axes\nArrow of axis matplotlib axis arrow tip\nChange fonts Plot in Ubuntu has ugly fonts (which is like \u0026lsquo;sans-serif\u0026rsquo;).\nplt.rcParams[\u0026quot;font.family\u0026quot;] = \u0026quot;cursive\u0026quot;, This will change to your computer\u0026rsquo;s default monospace font. How to change fonts in matplotlib (python)?\n散点图 plt.scatter(x,y, c='r', s=4), s can control marker size, docs\nmarker 是反着画的\nmatplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None, **kwargs) x,y组成了散点的坐标；s为散点的面积；c为散点的颜色（默认为蓝色\u0026rsquo;b\u0026rsquo;）；marker为散点的标记；alpha为散点的透明度（0与1之间的数，0为完全透明，1为完全不透明）;linewidths为散点边缘的线宽；如果marker为None，则使用verts的值构建散点标记；edgecolors为散点边缘颜色。 csdn\n拉长坐标间距 Python设置matplotlib.plot的坐标轴刻度间隔以及刻度范围\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import matplotlib.pyplot as plt from matplotlib.pyplot import MultipleLocator # 用于设置刻度间隔 x_vals = list(range(11)) y_vals = [x**2 for x in x_vals] plt.plot(x_vals, y_vals, c=\u0026#39;green\u0026#39;) plt.title(\u0026#39;Squares\u0026#39;, fontsize=24) plt.tick_params(axis=\u0026#39;both\u0026#39;,which=\u0026#39;major\u0026#39;,labelsize=14) plt.xlabel(\u0026#39;Numbers\u0026#39;,fontsize=14) plt.ylabel(\u0026#39;Squares\u0026#39;,fontsize=14) x_major_locator=MultipleLocator(1) #把x轴的刻度间隔设置为1，并存在变量里 y_major_locator=MultipleLocator(10) #把y轴的刻度间隔设置为10，并存在变量里 ax=plt.gca() #ax为两条坐标轴的实例 ax.xaxis.set_major_locator(x_major_locator) #把x轴的主刻度设置为1的倍数 ax.yaxis.set_major_locator(y_major_locator) #把y轴的主刻度设置为10的倍数 plt.xlim(-0.5,11) #把x轴的刻度范围设置为-0.5到11，因为0.5不满一个刻度间隔，所以数字不会显示出来，但是能看到一点空白 plt.ylim(-5,110) #把y轴的刻度范围设置为-5到110，同理，-5不会标出来，但是能看到一点空白 plt.show() 设置图片尺寸大小 如何指定matplotlib输出图片的尺寸？ - pythonic生物人的回答 - 知乎 https://www.zhihu.com/question/37221233/answer/2250419008\nplt.rcParams['figure.figsize'] = (12.0, 8.0)\nfig, ax = plt.subplots(figsize=(15,0.5),dpi=300) pool\n设置图片分辨率 plt.figure(figsize=(a, b), dpi=dpi)\nmatplotlib设置分辨率\n网格 plt.grid(visible=True,linestyle=\u0026quot;--\u0026quot;, color='gray', linewidth='1',)\n绘制CDF (221211) Repeat drawing How to change a matplotlib figure in a different cell in Jupyter Notebook?\n1 2 fig.gca().scatter(x3, y3, color=\u0026#39;g\u0026#39;, linewidth=5) fig (2022-12-13)\nGet the arguments name use lib inspect with two .f_back: callers_local_vars = inspect.currentframe().f_back.f_back.f_locals.items(). Getting the name of a variable as a string\nGet current color How to get color of most recent plotted line in Python\u0026rsquo;s plt plt.gca().lines[-1].get_color()\nuse ax Get default line colour cycle\n1 2 line = ax.plot(x,y) ax.plot(x, y+.3, color = line.get_color()) 多曲线 Combine two figures matplotlib: combine different figures and put them in a single subplot sharing a common legend Multiple subplots GfG\n1 2 3 4 5 6 7 fig, ax = plt.subplots(3, 3) # draw graph for i in ax: for j in i: j.plot(np.random.randint(0, 5, 5), np.random.randint(0, 5, 5)) plt.show() title of subplots: ax[0][0].set_title(\u0026quot;xxx\u0026quot;)\nhide x ticks for top subplots and y ticks for right plots: How to set a single, main title above all the subplots with Pyplot?\n1 2 plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False) plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False) Set xticks for subplots: How to set xticks in subplots\n1 2 3 4 5 6 7 8 9 10 11 12 fig, axes = plt.subplots(nrows=3, ncols=4) # Set the ticks and ticklabels for all axes plt.setp(axes, xticks=[0.1, 0.5, 0.9], xticklabels=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], yticks=[1, 2, 3]) # Use the pyplot interface to change just one subplot... plt.sca(axes[1, 1]) plt.xticks(range(3), [\u0026#39;A\u0026#39;, \u0026#39;Big\u0026#39;, \u0026#39;Cat\u0026#39;], color=\u0026#39;red\u0026#39;) fig.tight_layout() plt.show() Set the fontsize of numbers of xticks: plt.setp(ax.get_xticklabels(), fontsize=12, fontweight=\u0026quot;normal\u0026quot;, horizontalalignment=\u0026quot;left\u0026quot;, rotation=90) set_xticks() needs argument for \u0026lsquo;fontsize\u0026rsquo; #12318 set_xticklabels-Docs\n多曲线不同尺度 Three lines with different ranges\nReferences:\nGemini 2.5P - Plotting Three Curves With Different Scales Supports:\n(2025-08-11T14:56)\nax.twinx() r1-Gemini\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import matplotlib.pyplot as plt import numpy as np # Generate some sample data x = np.linspace(0, 10, 100) curve1 = 990 + 410 * np.sin(x) # Range: 990 - 1400 curve2 = 4 + 6 * np.cos(x) # Range: -2 - 10 curve3 = 9 + 11 * np.sin(x+1) # Range: -2 - 20 # Create a figure and a primary axes object fig, ax1 = plt.subplots(figsize=(10, 6)) # Plot the first curve on the primary y-axis color = \u0026#39;tab:red\u0026#39; ax1.set_xlabel(\u0026#39;X-axis\u0026#39;) ax1.set_ylabel(\u0026#39;Curve 1\u0026#39;, color=color) ax1.plot(x, curve1, color=color) ax1.tick_params(axis=\u0026#39;y\u0026#39;, labelcolor=color) # Create a second y-axis that shares the same x-axis ax2 = ax1.twinx() color = \u0026#39;tab:blue\u0026#39; ax2.set_ylabel(\u0026#39;Curve 2\u0026#39;, color=color) ax2.plot(x, curve2, color=color) ax2.tick_params(axis=\u0026#39;y\u0026#39;, labelcolor=color) # Create a third y-axis ax3 = ax1.twinx() color = \u0026#39;tab:green\u0026#39; ax3.set_ylabel(\u0026#39;Curve 3\u0026#39;, color=color) ax3.plot(x, curve3, color=color) ax3.tick_params(axis=\u0026#39;y\u0026#39;, labelcolor=color) # To prevent the third y-axis from overlapping with the second, we need to move it to the right. ax3.spines[\u0026#39;right\u0026#39;].set_position((\u0026#39;outward\u0026#39;, 60)) fig.tight_layout() # otherwise the right y-label is slightly clipped plt.savefig(\u0026#34;three_curves_plot.png\u0026#34;) plt.close() Scale y-axis Log scale\nReferences:\nDocs Supports:\nplt.yscale\n1 2 3 4 x = np.arange(len(Train_RMSE)) fig, ax = plt.subplots() plt.yscale(\u0026#39;log\u0026#39;,) ax.plot(x, Train_RMSE) Scale\nProblems:\nMake the line only take the middle 50% y-range of the canvas. References: {{{\nGemini 2.5P - Matplotlib Y-Axis Fraction Scaling }}} Supports:\n(2025-08-22T13:57)\nInput\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # === # === Scaling the Line to the Middle 50% # === import matplotlib.pyplot as plt import matplotlib.ticker as mticker from fractions import Fraction import numpy as np # --- 1. Original Data --- x = [1, 2, 3, 4, 5] y = np.array([0.65, 0.72, 0.78, 0.71, 0.68]) # --- 2. Define Canvas and Target Ranges --- # The full y-range you want to see on the plot canvas_min, canvas_max = -2, 6 # The middle 50% of the canvas range canvas_range = canvas_max - canvas_min target_min = canvas_min + 0.25 * canvas_range target_max = canvas_max - 0.25 * canvas_range # --- 3. Scale the Data --- data_min, data_max = y.min(), y.max() data_range = data_max - data_min target_range = target_max - target_min # Normalize data to a 0-1 scale, then scale to the target range y_scaled = (((y - data_min) / data_range) * target_range) + target_min # --- 4. Create an Inverse Formatter for Labels --- def inverse_scale_formatter(scaled_val, pos): \u0026#34;\u0026#34;\u0026#34; Takes a scaled value from the y-axis and converts it back to its original data value for display as a label. \u0026#34;\u0026#34;\u0026#34; # Inverse normalization from target range back to 0-1 normalized_val = (scaled_val - target_min) / target_range # Inverse scaling from 0-1 back to the original data range original_val = normalized_val * data_range + data_min # Format as a fraction return f\u0026#39;{Fraction(original_val).limit_denominator()}\u0026#39; # --- 5. Create and Format the Plot --- fig, ax = plt.subplots(figsize=(8, 6)) # Plot the SCALED data ax.plot(x, y_scaled, marker=\u0026#39;o\u0026#39;, color=\u0026#39;purple\u0026#39;) # Set the y-axis limits to the full canvas range ax.set_ylim(canvas_min, canvas_max) # Apply the special inverse formatter ax.yaxis.set_major_formatter(mticker.FuncFormatter(inverse_scale_formatter)) Composite 2 imgs (2023-12-20) Setting alpha:\n1 2 3 4 5 6 7 8 9 10 11 12 13 import cv2 import matplotlib.pyplot as plt img1 = cv2.imread(\u0026#39;rot1.png\u0026#39;) pts1 = [(24, 124), (49, 124), (98, 124), (104, 124), (120, 124), (18, 146), (37, 146), (65, 146), (102, 146), (133, 146)] for pt in pts1: cv2.circle(img1, pt, 0, (0, 0, 255)) plt.figure(figsize=(20,10)) plt.imshow(img1[:,:, ::-1]) img2 = cv2.imread(\u0026#39;rot2.png\u0026#39;) plt.imshow(img2[:,:,::-1], alpha=0.6) An example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 import matplotlib.pyplot as plt import numpy as np from matplotlib.ticker import MultipleLocator epoch = np.array( [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]) train_loss = np.array( [0.27195, 0.07782, 0.05188, 0.03321, 0.03054, 0.02322, 0.01962, 0.01537, 0.01347, 0.01266, 0.00895, 0.01107, 0.00755, 0.00633, 0.00915, 0.00872, 0.00431, 0.00733, 0.00584, 0.00477, 0.00502, 0.00384, 0.00282, 0.00506, 0.00307, 0.00325, 0.00588, 0.00277, 0.00445, 0.00318, 0.00181, 0.00532, 0.00218, 0.00379, 0.00190, 0.00201, 0.00242, 0.00164, 0.00215, 0.00176, 0.00374, 0.00149, 0.00145, 0.00170, 0.00481, 0.00136, 0.00184, 0.00165, 0.00162, 0.00265, 0.00285, 0.00172, 0.00241, 0.00139, 0.00181, 0.00161, 0.00135, 0.00264, 0.00165, 0.00158]) valid_loss = np.array( [0.04518, 0.02028, 0.01517, 0.01853, 0.00928, 0.01604, 0.01256, 0.01068, 0.01229, 0.01066, 0.01251, 0.00756, 0.00836, 0.00777, 0.00623, 0.00942, 0.00761, 0.00691, 0.00626, 0.00607, 0.00708, 0.00703, 0.01061, 0.00758, 0.00807, 0.00541, 0.00521, 0.00570, 0.00844, 0.00610, 0.00735, 0.00448, 0.00466, 0.00734, 0.00633, 0.00511, 0.00737, 0.00380, 0.00410, 0.00604, 0.00685, 0.00546, 0.00618, 0.00411, 0.00543, 0.00572, 0.00631, 0.00821, 0.00578, 0.00525, 0.00391, 0.00529, 0.00621, 0.00606, 0.00979, 0.00515, 0.00555, 0.00712, 0.00535, 0.00439]) # 解决曲线图里面中文显示乱码问题 # plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;Hiragino Sans GB\u0026#39;] # 用来正常显示中文标签 # # plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False plt.figure(figsize=(8,6)) # 增加曲线说明 A, = plt.plot(epoch, train_loss, lw=1, label=\u0026#39;train_loss\u0026#39;) B, = plt.plot(epoch, valid_loss, lw=1, label=\u0026#39;valid_loss\u0026#39;) font1 = {\u0026#39;family\u0026#39;: \u0026#39;Times New Roman\u0026#39;, \u0026#39;weight\u0026#39;: \u0026#39;normal\u0026#39;, \u0026#39;size\u0026#39;: 18, } # x轴label plt.xlabel(\u0026#34;epoch\u0026#34;, labelpad=2, fontsize=18,) # y轴label plt.ylabel(\u0026#34;loss\u0026#34;, labelpad=4, fontsize=18) # 设置标题 # plt.title(\u0026#34;change in loss\u0026#34;, x=0.5, y=-0.2, pad=0.3, fontsize=18) plt.title(\u0026#34;change in loss\u0026#34;, fontsize=18) fig=plt.gcf() fig.set_facecolor(\u0026#39;white\u0026#39;) # ------限制显示x,y轴最小-最大值范围（刻度不一定是多少） plt.xlim(0, 60) plt.ylim(0, 0.3) # ------设置x,y轴刻度 x_major_locator = MultipleLocator(10) y_major_locator = MultipleLocator(0.02) ax = plt.gca() # ax为两条坐标轴的实例 ax.xaxis.set_major_locator(x_major_locator) # ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) # 把x轴的主刻度设置为1的倍数 ax.yaxis.set_major_locator(y_major_locator) # 把y轴的主刻度设置为10的倍数 # ------显示网格 # plt.grid() plt.grid(True, linestyle=\u0026#34;--\u0026#34;, color=\u0026#39;gray\u0026#39;, linewidth=\u0026#39;1\u0026#39;, axis=\u0026#39;both\u0026#39;) plt.legend(handles=[A, B], prop=font1) plt.tick_params(labelsize=18) labels = ax.get_xticklabels() + ax.get_yticklabels() [label.set_fontname(\u0026#39;Time New Roman\u0026#39;) for label in labels] font2 = {\u0026#39;family\u0026#39;: \u0026#39;Time New Roman\u0026#39;, \u0026#39;weight\u0026#39;: \u0026#39;normal\u0026#39;, \u0026#39;size\u0026#39;: 18} plt.show() plt.savefig(\u0026#39;saved_figure.jpg\u0026#39;) 3D interactive plot (2024-03-23)\nMake 3D interactive Matplotlib plot in Jupyter Notebook - GfG\n3D Scatter plot:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # for creating a responsive plot %matplotlib widget # importing required libraries from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt # creating random dataset xs = [14, 24, 43, 47, 54, 66, 74, 89, 12, 44, 1, 2, 3, 4, 5, 9, 8, 7, 6, 5] ys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 3, 5, 2, 4, 1, 8, 7, 0, 5] zs = [9, 6, 3, 5, 2, 4, 1, 8, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0] # creating figure fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # creating the plot plot_geeks = ax.scatter(xs, ys, zs, color=\u0026#39;green\u0026#39;) # setting title and labels ax.set_title(\u0026#34;3D plot\u0026#34;) ax.set_xlabel(\u0026#39;x-axis\u0026#39;) ax.set_ylabel(\u0026#39;y-axis\u0026#39;) ax.set_zlabel(\u0026#39;z-axis\u0026#39;) # displaying the plot plt.show() 3D Bar plot:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %matplotlib widget from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt import numpy as np # creating random dataset xs = [2, 3, 4, 5, 1, 6, 2, 1, 7, 2] ys = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] zs = np.zeros(10) dx = np.ones(10) dy = np.ones(10) dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # creating figure figg = plt.figure() ax = figg.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # creating the plot plot_geeks = ax.bar3d(xs, ys, zs, dx, dy, dz, color=\u0026#39;green\u0026#39;) # setting title and labels ax.set_title(\u0026#34;3D bar plot\u0026#34;) ax.set_xlabel(\u0026#39;x-axis\u0026#39;) ax.set_ylabel(\u0026#39;y-axis\u0026#39;) ax.set_zlabel(\u0026#39;z-axis\u0026#39;) plt.show() Camera Poses (2024-03-23)\nGenerate by ChatGPT with prompt: \u0026ldquo;How to plot multiple camera poses in a single figure?\u0026rdquo;\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 %matplotlib widget import numpy as np import matplotlib.pyplot as plt def plot_camera_poses(poses, axis_length=0.1): # Create a new figure fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # Plot each camera pose for pose in poses: # Extract camera position and orientation cam_position = pose[\u0026#39;position\u0026#39;] cam_orientation = pose[\u0026#39;rotation\u0026#39;] # Plot camera position ax.scatter(cam_position[0], cam_position[1], cam_position[2], c=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;) # Plot camera axes axes_endpoints = cam_position + axis_length * cam_orientation ax.plot3D([cam_position[0], axes_endpoints[0,0]], [cam_position[1], axes_endpoints[0,1]], [cam_position[2], axes_endpoints[0,2]], \u0026#39;r\u0026#39;) ax.plot3D([cam_position[0], axes_endpoints[1,0]], [cam_position[1], axes_endpoints[1,1]], [cam_position[2], axes_endpoints[1,2]], \u0026#39;g\u0026#39;) ax.plot3D([cam_position[0], axes_endpoints[2,0]], [cam_position[1], axes_endpoints[2,1]], [cam_position[2], axes_endpoints[2,2]], \u0026#39;b\u0026#39;) # Set plot limits and labels ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) # Show plot plt.show() # Example camera poses (positions and rotation matrices) poses = [ { \u0026#39;position\u0026#39;: np.array([0, 0, 0]), \u0026#39;rotation\u0026#39;: np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) }, { \u0026#39;position\u0026#39;: np.array([1, 1, 1]), \u0026#39;rotation\u0026#39;: np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]]) } ] # Plot camera poses plot_camera_poses(poses) 3D arrow (2024-03-23)\nPutting arrowheads on vectors in a 3d plot - SO\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyArrowPatch from mpl_toolkits.mplot3d import proj3d class Arrow3D(FancyArrowPatch): def __init__(self, xs, ys, zs, *args, **kwargs): super().__init__((0,0), (0,0), *args, **kwargs) self._verts3d = xs, ys, zs def do_3d_projection(self, renderer=None): xs3d, ys3d, zs3d = self._verts3d xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M) self.set_positions((xs[0],ys[0]),(xs[1],ys[1])) return np.min(zs) arrow_prop_dict = dict(mutation_scale=10, arrowstyle=\u0026#39;-|\u0026gt;\u0026#39;, color=\u0026#39;k\u0026#39;, shrinkA=0, shrinkB=0) fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) a = Arrow3D([0, 1], [0, 0], [0, 0], **arrow_prop_dict) ax.add_artist(a) a = Arrow3D([0, 0], [0, 1], [0, 0], **arrow_prop_dict) ax.add_artist(a) a = Arrow3D([0, 0], [0, 0], [0, 1], **arrow_prop_dict) ax.add_artist(a) Convex polygon on 2D (2024-03-26)\nHow to fill an area within a polygon in Python using matplotlib? - SO\nExample from Scipy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Convex hull of a random set of points: from scipy.spatial import ConvexHull, convex_hull_plot_2d import numpy as np rng = np.random.default_rng() points = rng.random((30, 2)) # 30 random points in 2-D hull = ConvexHull(points) # Plot it: import matplotlib.pyplot as plt plt.plot(points[:,0], points[:,1], \u0026#39;o\u0026#39;) for simplex in hull.simplices: plt.plot(points[simplex, 0], points[simplex, 1], \u0026#39;k-\u0026#39;) plt.plot(points[hull.vertices,0], points[hull.vertices,1], \u0026#39;r--\u0026#39;, lw=2) plt.plot(points[hull.vertices[0],0], points[hull.vertices[0],1], \u0026#39;ro\u0026#39;) plt.show() Fill 3D Polygon (2024-03-26)\nax.fill is used for 2D, and doesn\u0026rsquo;t work for 3D. ax.add_collection3d cannot follow the counter-clockwise order of convex hull to fill the polygon. For example, when drawing a rectangle, it will fill 2 triangles:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) verts = np.array([[ [0,0,0], [1,0,0], [0,1,0], [1,1,0] ]]) # (1,4,3) rec = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, edgecolors=\u0026#39;k\u0026#39;) ax.add_collection3d(rec) The Poly3DCollection code is generated by ChatGPT with prompt: \u0026ldquo;How to fill a 3D triangle with matlabplot\u0026rdquo; So, I draw 2 triangles to form a quadrilateral:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) verts = np.array([[ [0,0,0], [1,0,0], [0,1,0],]]) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) verts = np.array([[ [1,0,0], [0,1,0], [1,1,0] ]]) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) Plotting 3D Polygons - SO\nProject 3D point onto plane (2024-03-26)\nHow to project a point onto a plane in 3D? - SO\nThe distance from 3D point to the plane: dot product for 3D point and plane normal vector.\n3D point minus the distance, resulting in the projection.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection R = np.array([[ 0.97026268, 0.00747991, 0.24193878], # cam x-axis [-0.01474287, 0.99949291, 0.02822342], # cam y-axis [-0.24160499, -0.030951 , 0.96988095]]) #cam z-axis fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # fill rectangle verts = np.array([[ [0,0,0], R[0]*2, R[1]*2, ]]) # (1,4,3) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, ) ax.add_collection3d(tri) verts = np.array([[ R[0]*2, R[1]*2, (R[0]+R[1])*2 ]]) # (1,4,3) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, ) ax.add_collection3d(tri) p = np.array([1,1,1]) ax.scatter(*p, c=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;) plane_normal = R[2] len2plane = R[2]@ p.T projection = p-len2plane * plane_normal ax.scatter(*projection, marker=\u0026#39;*\u0026#39;) ax.plot3D([p[0], projection[0]], [p[1], projection[1]], [p[2], projection[2]], \u0026#39;gray\u0026#39;) Set line color \u0026amp; marker (2024-07-23)\nSet the line to orange and triangle marker:\n1 edge_1, = axis.plot([0,0], [0,0.5], color=\u0026#39;orange\u0026#39;, marker=\u0026#39;\u0026gt;\u0026#39;) Set marker appear only at the endpoint:\n1 seg_2, = ax.plot([0,0], [0,0], color=\u0026#39;green\u0026#39;, marker=\u0026#39;\u0026gt;\u0026#39;, markersize=\u0026#39;10\u0026#39;, markevery=[-1]) Ref: matplotlib: apply marker only to start point or end point? - SO (Searched by \u0026ldquo;python matplotlib pyplot set line marker only for one end\u0026rdquo; in DDG) Set line end arrow\nUse quiver by setting starting point and direction: Arrow on a line plot (Searched by \u0026ldquo;python matplotlib line arrow\u0026rdquo; in DDG) Python 制作动图 (2024-07-23)\n用 Python 制作程序动画，并保存为 GIF\nAsk Claude3.5: \u0026ldquo;How to use Python to create a GIF image showing a point moving within in a triangle?\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import numpy as np import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from PIL import Image import io # Define the triangle vertices triangle = np.array([(0, 0), (1, 0), (0.5, np.sqrt(3)/2)]) # Set up the figure and axis fig, ax = plt.subplots() ax.set_xlim(-0.1, 1.1) ax.set_ylim(-0.1, 1) ax.set_aspect(\u0026#39;equal\u0026#39;) # Plot the triangle ax.plot(np.append(triangle[:, 0], triangle[0, 0]), np.append(triangle[:, 1], triangle[0, 1]), \u0026#39;k-\u0026#39;) # Initialize the point point, = ax.plot([], [], \u0026#39;ro\u0026#39;) # Animation function def animate(frame): t = frame / 100 # Adjust speed # Calculate point position (example: move in a circular path within the triangle) x = 0.5 + 0.3 * np.cos(2 * np.pi * t) y = np.sqrt(3)/4 + 0.2 * np.sin(2 * np.pi * t) point.set_data(x, y) return point, # Create the animation anim = FuncAnimation(fig, animate, frames=100, interval=50, blit=True) # Save the animation as a GIF (Don\u0026#39;t work) # frames = [] # for i in range(100): # img_buf = io.BytesIO() # plt.savefig(img_buf, format=\u0026#39;png\u0026#39;) # img_buf.seek(0) # frames.append(Image.open(img_buf)) # # frames[0].save(\u0026#39;moving_point.gif\u0026#39;, save_all=True, append_images=frames[1:], duration=50, loop=0) plt.close(fig) Saving animated plot as .gif with PillowWriter doesn\u0026rsquo;t work: MovieWriter ffmpeg unavailable; trying to use \u0026lt;class \u0026lsquo;matplotlib.animation.PillowWriter\u0026rsquo;\u0026gt; instead - SO (Search in DDG) FuncAnimation (2024-07-23)\nExample: Create Animations with FuncAnimation in Python - PythonAlgos (Searched by \u0026ldquo;python make func animation\u0026rdquo; in DDG)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation fig, ax = plt.subplots() #plot sine function _range = np.arange(0, 2*np.pi, 0.001) sine = np.sin(_range) line = plt.plot(_range, sine) ax = plt.axis([0, 2*np.pi, -1, 1]) # The comma is a must, for unpackaging the tuple dot, = plt.plot([0], [np.sin(0)], \u0026#39;ro\u0026#39;) def func(i): dot.set_data(i, np.sin(i)) return dot, _animation = FuncAnimation(fig, func, frames=np.arange(0, 2*np.pi, 0.1), interval=10) _animation.save(\u0026#39;sine.mp4\u0026#39;, fps=30, extra_args=[\u0026#39;-vcodec\u0026#39;, \u0026#39;libx264\u0026#39;]) plt.show() Saving .mp4 requires: sudo apt install ffmpeg\nOther Refs:\nMore examples: Animations Using Python: A Comprehensive Guide - Medium Example of sine wave: Matplotlib.animation.FuncAnimation class in Python - GfG Save animation with specifying format (mp4, gif) and dpi: matplotlib.animation.Animation.save - Docs (Found in DDG)\n1 2 3 4 5 # Save the animation as a mp4 video anim.save(\u0026#39;Tweaking_within_Triangle.mp4\u0026#39;, writer = \u0026#39;ffmpeg\u0026#39;, fps = 10) # Save the animation as a GIF anim.save(\u0026#39;Tweaking_within_Triangle.gif\u0026#39;, writer = \u0026#39;pillow\u0026#39;, fps = 10) Ref: Animations using Matplotlib - saving animations Omit frame and ticks (2024-07-23)\nUse transparent=True:\n1 2 plt.axis(\u0026#39;off\u0026#39;) plt.savefig(\u0026#39;imgName\u0026#39;,bbox_inches=\u0026#39;tight\u0026#39;,transparent=True, pad_inches=0) Ref: savefig without frames, axes, only content - SO Use fig.subplot_adjust():\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import matplotlib.pyplot as plt import numpy as np pixels = 512 px = 1/plt.rcParams[\u0026#39;figure.dpi\u0026#39;] # pixel in inches fig, ax = plt.subplots(frameon=False) # Transparent content background fig.set_size_inches(pixels*px, pixels*px) ax = plt.Axes(fig, [0., 0., 1., 1.]) # This is needed for fitting content to 512*512 figure frame otherwise output figure will be less than desired size ax.set_axis_off() fig.add_axes(ax) plt.plot(np.sin(np.arange(10)), c = \u0026#39;black\u0026#39;) fig.subplots_adjust(bottom = 0) fig.subplots_adjust(top = 0.00001) # This value should be very small so that marginal axes of subplot would not overlay on the main figure fig.subplots_adjust(right = 1) fig.subplots_adjust(left = 0) plt.savefig(\u0026#39;no_content_data.png\u0026#39;) plt.close() Ref: savefig without frames, axes, only content - SO Similar solution: Save an image (only content, without axes or anything else) to a file using Matloptlib - SO (Searched by \u0026ldquo;python matplotlib pyplot save figure without title or axis ticks\u0026rdquo; in DDG) Annotation Add Rectangle (2024-08-03)\nAsk Claude3.5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import matplotlib.pyplot as plt from matplotlib.patches import Rectangle, Patch # --- Add a box to enclose methods being compared from matplotlib.patches import Rectangle, Patch # Find the indices of the ticks we want to highlight highlight_methods = [\u0026#34;Method3\u0026#34;, \u0026#34;Method4\u0026#34;, \u0026#34;Method5\u0026#34;] highlight_indices = [methods.index(method) for method in highlight_methods] # [3,4,5] # Get all tick positions tick_positions = ax.get_xticks() # Get the x-coordinates of the first and last tick to be highlighted x_start = tick_positions[highlight_indices[0]] x_end = tick_positions[highlight_indices[-1]] # Adjust to extend slightly beyond the ticks x_start -= 0.24 x_end += 0.24 # Get the y-range of the plot y_min, y_max = ax.get_ylim() # Create a rectangle rect = Rectangle((x_start, y_min+0.05), x_end - x_start, y_max - y_min-0.18, fill=False, edgecolor=\u0026#39;magenta\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=2) # Add the rectangle to the plot ax.add_patch(rect) # Adjust the plot limits to ensure the rectangle is visible ax.set_xlim(tick_positions[0] - 0.5, tick_positions[-1] + 0.5) # Create a custom patch for the legend legend_patch = Patch(facecolor=\u0026#39;none\u0026#39;, edgecolor=\u0026#39;magenta\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=\u0026#39;Comparison methods\u0026#39;) # Get the current handles and labels handles, labels = ax.get_legend_handles_labels() # Add the new patch to the handles handles.append(legend_patch) # Create the legend with the updated handles ax.legend(handles=handles, loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1, 0.5), fontsize=14) Arrow Arrow and text\nReferences: {{{\npython - Arrow properties in matplotlib annotate - Stack Overflow Searched by python matplotlib annotation arrow in DDG matplotlib.pyplot.annotate — Matplotlib 3.10.5 documentation }}} Supports:\n(2025-08-22T13:09)\nax.annotate r1-SO\n1 2 3 4 5 6 7 8 9 ax.annotate(\u0026#39;test\u0026#39;, xy=(0.9, 0.9), xycoords=\u0026#39;data\u0026#39;, xytext=(0, 0), textcoords=\u0026#39;data\u0026#39;, arrowprops=dict(arrowstyle= \u0026#39;\u0026lt;|-|\u0026gt;\u0026#39;, color=\u0026#39;blue\u0026#39;, lw=3.5, ls=\u0026#39;--\u0026#39;) ) xycoords: coordinate system, default: data r2-Docs ","date":"2022-12-26T17:05:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/python_draw/","title":"Memo: Lang - Python | Plotting"},{"content":"args \u0026amp; kwargs (2022-08-05)\n*args：表示接受任意个数的 Positional arguments，然后存放入一个元组中；传参时 位置 要对应，例如\n1 2 3 4 5 def fun (*args) print(args) fun(\u0026#39;fruit\u0026#39;, \u0026#39;animal\u0026#39;, \u0026#39;human\u0026#39;) \u0026gt;\u0026gt;\u0026gt; \u0026#39;fruit\u0026#39;，\u0026#39;animal\u0026#39;，\u0026#39;human\u0026#39; **kwargs：表示接受任意长的 Keywords arguments，然后存放入一个字典中；传参时要对应 key，例如\n1 2 3 4 5 def fun(**kwargs): for key, value in kwargs.items(): print(\u0026#34;%s:%s\u0026#34; % (key,value) fun(a=1,b=2,c=3)会输出 a=1 b=2 c=3 Positional args and keywords args\n1 2 3 4 5 6 def foo(a, b, **kwargs): var1 = a var2 = b args = kwargs var3 = args.[\u0026#39;key1\u0026#39;] 注解 annotation bilibili\n类型不匹配警告\n1 2 3 4 from typing import * def func(a: int, b: List[int]): # 第一个参数是int，第二个参数是list，其中每个参数是int ... 1 2 3 4 5 6 def func(a: List[List[int]], # 二维int数组 b: Dict, c: Set[int], d: Tuple[int] ): ... 1 2 3 4 5 6 7 8 9 class TreeNode: def __init__(self): self.val = None self.left = None self.right = None def dfs(node: TreeNode): # 类型为类名，或\u0026#34;TreeNode\u0026#34; node.val = 1 ... 1 2 3 4 5 def func(x: float) -\u0026gt; str: # 返回值类型 return str(x) def func2(func1: Callable[[float], str]): ... 1 a: int = 15 # 变量注解 1 print(func.__annotations__) # 查看函数的注解类型 Class Variable \u0026amp; Instance Variable 1 2 3 4 5 class MyClass(): class_var = 1 # Class Variable def __init__(self): self.var = 123 # Instance Variable All instances of the class have access to class_var, as well as the class itself. If class_var is mutable type, like list, class_var will influence on all instances. Reference: Python Class Attributes: An Overly Thorough Guide\n@classmethod classmethod can only be called through the class name instead through an object instantiated from the class, because it gets passed the cls as the argument.\nSimilarly, self means a method belongs to an instance, and can only be called via instance. classmethod can access the class variables.\nclassmethod usually is used for instantiating an object with a set of specific arguments rather through __init__().\n1 2 3 4 5 6 7 8 9 10 11 12 13 class PositionalEncoding(torch.nn.Module): def __init__(self, ..): # instance method ... @classmethod def from_conf(cls, conf, d_in=3): # conf: \u0026#39;default.conf\u0026#39;.model.code{} # PyHocon construction return cls( conf.get_int(\u0026#34;num_freqs\u0026#34;, 6), d_in, conf.get_float(\u0026#34;freq_factor\u0026#34;, np.pi), conf.get_bool(\u0026#34;include_input\u0026#34;, True), ) Code snippest from pixel-nerf\nRef: An essential guide to Python class methods and when to use them\n@staticmethod @staticmethod in Python - AskPython\nNo need to pass an instance (self) or class (cls) to it as the first argument. So it can be called both by the class name and an instance. Can be overridden by children class. __name__ 当一个 py 文件被直接执行时，__name__ 被设置为 '__main__'，如果是被 impot，则它的 __name__ 被设置为它的文件名(不包括后缀)，并且被导入时会执行顶层的代码（不包括 function, class）\nfreeCodeCamp-Goran Aviani\n__init__ References:\n为什么要写 init.py ? | Python - Hucci写代码 Notes:\n(2025-03-26)\nTo differenciate two scenarios: import \u0026lt;a file\u0026gt; and import \u0026lt;a package\u0026gt; r1-Hucci __file__ (2024-02-28) Ref: GfG\nThe file path of the function\u0026rsquo;s definition\n1 2 3 # Hello.py def HelloWorld(): print(\u0026#34;This is Hello.py\u0026#34;) Call that module in another file:\n1 2 3 4 5 6 7 8 9 10 11 # GFK.py import Hello import os Hello.HelloWorld() print(Hello.__file__) print(f\u0026#39;GFK.py\\\u0026#39;s __file__: {__file__}\u0026#39;) print(os.path.join(os.path.dirname(os.path.abspath(__file__)), \u0026#34;third_party/glm/\u0026#34;)) Output of python GFK.py\n1 2 3 4 This is Hello.py /home/yi/Downloads/ipynb_test/test_file_/Hello.py GFK.py\u0026#39;s __file__: GFK.py /home/yi/Downloads/ipynb_test/test_file_/third_party/glm/ Built-in functions Python Docs\ndir(object) returns a list of the names of attributes and methods of any kind of object, e.g., module, class, instance. No values are stored.\nvars(object) returns a dict: object.__dict__ containing changeable attributes.\nvars() acts like locals(): a dict of local variables for read. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class person: name = \u0026#34;jack\u0026#34; dir(person) # A list, len = 27 person.__dir__ # A method vars(person) # same as person.__dict__, len=5 # instance student = person() setattr(student, \u0026#39;age\u0026#39;, 14) vars(student) # out: {\u0026#39;age\u0026#39;: 14} dir(student) # a list of attr and methods of the object set(student.__dir__()) - set(dir(person)) # out: {\u0026#39;age\u0026#39;} set(student.__dir__()) == set(dir(student)) # True set(dir(student)) == set(person.__dir__(student)) # True set(dir(student)) - set(person.__dir__(person)) # {\u0026#39;__weakref__\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;name\u0026#39;} set(person.__dir__(person)) - set(dir(student)) # {\u0026#39;__abstractmethods__\u0026#39;, \u0026#39;__base__\u0026#39;, \u0026#39;__bases__\u0026#39;, ... [::-1] flip image 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import matplotlib.pyplot as plt from PIL import Image # pillow import cv2 Image.open(\u0026#39;1.jpg\u0026#39;) # RGB cv2.imread(\u0026#39;1.jpg\u0026#39;) # nd array, BGR plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)) plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[:,:, ::-1]) # 最后的通道取反 plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[::-1,:, ::-1]) # 上下翻转 plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[::-1, ::-1, ::-1]) # 左右也翻转 表达式内赋值 PEP 572, Python 3.8+， 类似 n := xxx\n比如，有一个字符串数组，想去掉每个字符串里的空格和空字符串\n1 2 3 arr = [\u0026#39;a\u0026#39;, \u0026#39;bb \u0026#39;, \u0026#39; \u0026#39;] arr = [x.strip() for x in arr] # 删除字符串前面和后面的空格 arr = [x for x in arr if x] # 空字符串会当作false 两行写一行: arr = [ y for x in arr if (y := x.strip)) ]\n团队开发最好还是用易懂的写法。 1_00_00什么意思？x:=y？Python里面的几个有趣特性\nAssignment expression - RealPython Index value from dict Get value by giving index from a dictionary\nAccessing dictionary value by index in python\nPython Dictionaries are Ordered from python 3.7 6, so the index can be used like:\nvalue_at_index = list(dic.values())[index]\nPython Dictionaries are Ordered now, but how?…and why? Make one-hot matrix Convert a vector to one-hot matrix\nConvert array of indices to one-hot encoded array in NumPy - StackOvf\n1 2 3 values = [1, 0, 3] n_values = np.max(values) + 1 np.eye(n_values)[values] if/else in a list comprehension SO\nFlatten a list of list Python - Flatten a list of lists to a single list (Dead link now)\nConcatenate list of tensor: torch.stack() first, then reshape Redirect print content (2023-08-08)\nRedirect the standard output (print stream) to a file using the sys module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sys # Save the current stdout stream to a variable original_stdout = sys.stdout # Write to file with open(\u0026#39;output.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: sys.stdout = f print(\u0026#34;The content given to sys.stdout \\ will be redirected to file, \\ instead of being printed out\u0026#34;) # Restore the original stdout stream sys.stdout = original_stdout argparse (2023-08-28)\naction=\u0026quot;store_true\u0026quot;, this option is True when this argument appears in the command\nnargs=+, this argument at least be assigned with 1 value. Similarly, nargs=3, the argument must be assigned with 3 values.\nArgparse: how to handle variable number of arguments (nargs=\u0026rsquo;*\u0026rsquo;) - SO\nExample:\n1 parser.add_argument(\u0026#39;--img_wh\u0026#39;, nargs=\u0026#34;+\u0026#34;, type=int, default=[1152, 864]) The command line: python eval.py --img_wh 640 512. The args in vscode \u0026ldquo;launch.json\u0026rdquo;: \u0026quot;args\u0026quot;: [\u0026quot;--img_wh\u0026quot;, \u0026quot;1152\u0026quot;, \u0026quot;864\u0026quot;,]\nUpdate Dict attributes (packge: mmcv)\nmmcv Docs\nmmsegment配置参数说明（五）- CSDN\n1 2 3 4 5 6 7 from mmcv import DictAction parser = argparse.ArgumentParser(description=\u0026#39;Train a recognizer\u0026#39;) parser.add_argument(\u0026#39;--cfg-options\u0026#39;, nargs=\u0026#39;+\u0026#39;, action=DictAction, default={}, help=\u0026#39;override some settings in the used config, the key-value pair \u0026#39; \u0026#39;in xxx=yyy format will be merged into config file. For example, \u0026#39; \u0026#34;\u0026#39;--cfg-options model.backbone.depth=18 model.backbone.with_cp=True\u0026#39;\u0026#34;) Example in launch.json of adapt-image-models:\n1 2 3 4 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34;, ] Set choices:\n1 2 3 4 5 parser.add_argument( \u0026#39;--launcher\u0026#39;, choices=[\u0026#39;none\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;slurm\u0026#39;, \u0026#39;mpi\u0026#39;], default=\u0026#39;none\u0026#39;, help=\u0026#39;job launcher\u0026#39;) Accept a dict (2023-08-31)\nAccepting a dictionary as an argument with argparse and python (duplicate) -SO\nargparse don\u0026rsquo;t have a type of dict, there\u0026rsquo;re 2 workarounds:\nSet type=str, then use json.loads() to parse:\nSet type=json.loads:\n1 2 3 4 5 import json parser.add_argument(\u0026#39;-d\u0026#39;, \u0026#39;--my-dict\u0026#39;, type=json.loads) args = parse.parse_args() mydict = args.my_dict # Will return a dictionary The DictAction of mmcv will only parse the top-level as a dict (key-value), but if the value passed is a dict too, it\u0026rsquo;ll be parsed as a string.\n1 2 3 4 5 6 7 8 9 10 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, // \u0026#34;train_pipeline[1].clip_len=3\u0026#34;, // Cannot identify [1] // \u0026#34;dict(train_pipeline={\u0026#39;1\u0026#39;: dict(clip_len=3)})\u0026#34;, // Error: has to 2 params, but got 1. // \u0026#34;train_pipeline= {\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: \\\u0026#34;3\\\u0026#34;}}\u0026#34; // The value will be parsed as a string, which will override the whole previous dict. ] Access a dict myDict.update({'aKey': \u0026quot;newVal\u0026quot;}) or myDict.update(aKey='newVal')\nmyDict.get('aKey', retVal)\nmyDict.setdefault('aKey', 'defVal'): if 'aKey' doesn\u0026rsquo;t exist, it will be inserted with the default value defVal.\nmyDict.copy() or newDict = dir(myDict) are shallow copy: when change the no-primitive data (such as list, nested data), the orignal dict will also change, because it only duplicate the top-level data, while the low-level data is referenced to the original data. Copy a Python Dictionary: A Complete Guide • datagy\nmyDict.pop('aKey') will return and remove an item.\nDelete items from dictionary while iterating Have to iterate the dict twice, the 1st round collects the keys to be deleted, and the 2nd round del those items.\nlogging Real Python\n1 2 3 4 5 6 7 import logging logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logger=logging.getLogger(__name__) logger.setLevel(logging.INFO) logger.info(\u0026#34;This is an info message\u0026#34;) Output: INFO:__main__:This is an info message\nlogging.info doesn\u0026rsquo;t show up on console tqdm (2023-10-20)\nDDG with searching \u0026ldquo;avoid writting tqdm bar into the log file\u0026rdquo;\nUse tqdm to iterate the outer loop and print in the inner loop. How can I make the tqdm progress bar be printed less frequently in the log file?\n1 2 3 for outer in tqdm(range(0, 1e5, 1e4)): for inner in range(1e4): print(outer+inner) Output tqdm to a null device, and print/log the status bar after desired steps. python - Making tqdm write to log files - Stack Overflow\n1 2 3 4 tqdm_bar = tqdm(range(20), file=open(os.devnull, \u0026#39;w\u0026#39;)) for i in tqdm_bar: if tqdm_bar.n % 5 ==0: print(str(tqdm_bar)) Redirecting console logging to tqdm.write(), leaving other logging handlers (e.g. log files) unaffected. tqdm.contrib.logging - Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 import logging from tqdm import trange from tqdm.contrib.logging import logging_redirect_tqdm LOG = logging.getLogger(__name__) if __name__ == \u0026#39;__main__\u0026#39;: logging.basicConfig(level=logging.INFO) with logging_redirect_tqdm(): for i in trange(9): if i == 4: LOG.info(\u0026#34;console logging redirected to `tqdm.write()`\u0026#34;) # logging restored Discussion: Pass progress bar to logger · Issue #313 · tqdm/tqdm tqdm output to stderr, which is not in sync with stdout. So only redict the stdout (print) to file: python test.py \u0026gt; output.log\nHow to \u0026ldquo;flush\u0026rdquo; tqdm progress bar explicitly?\nFound in Google\nMonitor log file in real-time: tail -f output.log. reddit\nDisable tqdm bar\nPresetting argument. Inspired by the comment of this answer of Why is tqdm printing to a newline instead of updating the same line?; Searched by DDG with searching \u0026ldquo;tqdm leave\u0026rdquo;\n1 2 3 4 5 from functools import partial from tqdm import tqdm tqdm = partial(tqdm, disable=True) for i in tqdm(iterable): Disable tqdm bar by environment variable after version 4.66.0:\n1 2 # pip install tqdm --upgrade eport TQDM_DISABLE=1 Silence tqdm\u0026rsquo;s output while running tests or running the code via cron; Searched by DDG with searching \u0026ldquo;tqdm disable\u0026rdquo;\nEnv variable overrides supported by tqdm==4.66.0\nleave=False means the bar will disappear instead of leaving it left after finishing. How to remove progressbar in tqdm once the iteration is complete\nUse Progress bar as status bar showing current entry, by removing all field but the description field and specifying position.\n1 2 3 4 5 6 7 8 outer = tqdm.tqdm(total=len(files), desc=\u0026#34;Files\u0026#34;, position=0) cur_file = tqdm.tqdm(total=0, position=1, bar_format=\u0026#34;{desc}\u0026#34;) for name in files: video = imageio.mimread(name, memtest=False) cur_file.set_description_str(f\u0026#34;Current file: {name}\u0026#34;) outer.update(1) Progress bar and status logging in python with tqdm\nRegistration mechanism A registry is a centralized \u0026ldquo;object\u0026rdquo; for convenience of interacting between differen components.\n1 2 3 4 5 from ..builder import BACKBONES # registry @BACKBONES.register_module() # add a component class ViT_CLIP(nn.Module): # backbone model def __init__(): isinstance() Check if an object is one of types\ndataset is expected to be a list or a tuple:\n1 dataset = dataset if isinstance(dataset, (list, tuple)) else [dataset] Convert str to classname Use eval\n1 eval(\u0026#34;cls_name\u0026#34;) Convert string to Python class object?\nUse getattr\n1 2 optim_type = \u0026#39;AdamW\u0026#39; self.optim = getattr(torch.optim, optim_type)(optim_params, **optim_kwargs) Code from MatchNeRF.\nEven and odd compatible 1 eval_batch_size = (self.eval_batch_size - 1) // sb + 1 Code from pixelNeRF\u0026rsquo;s \u0026ldquo;nerf.py\u0026rdquo;.\nSave argparse to file (2023-09-25)\nSaving python argparse file - SO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import json import time from pathlib import Path timestamp = time.strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;, time.localtime()) log_file = Path(cfg.work_dir) / f\u0026#39;{timestamp}.log\u0026#39; with open(f\u0026#34;log_file\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump({\u0026#34;CommandLine_Args\u0026#34;: args.__dict__, \u0026#34;ConfigFile_Settings\u0026#34;: conf }, f, indent=2 ) # --- Read --- parser = ArgumentParser() args = parser.parse_args() with open(\u0026#39;commandline_args.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: args.__dict__ = json.load(f) json has better readabiity, interoperability, security, but slower than pickle. Pickle or json?\njson.dumps convert any python object to JSON formatted string, while json.dump() write a serialized object to file. PYnative\nWrite JSON File (2024-03-21)\njson.dump() convert a python object to a json string. GfG\n1 2 3 4 json_data = {\u0026#34;camera_angle_x\u0026#34;: fovx, \u0026#34;frames\u0026#34;: frams_ls} with open(f\u0026#34;{dir_json}/transforms_train.json\u0026#34;, \u0026#39;w\u0026#39;) as file: file.write(json.dumps(json_data, indent=4)) Write a json string into a file using pathlib to avoid dangling with open(): SO\n1 2 3 json_data = {\u0026#34;camera_angle_x\u0026#34;: fovx, \u0026#34;frames\u0026#34;: frams_ls} pathlib.Path(f\u0026#34;{dir_json}/transforms_train.json\u0026#34;).write_text(json.dumps(json_data, indent=4)) But, in this way, I don\u0026rsquo;t know how to realize 'w' that always creats a new file. Working With JSON Data in Python - Real Python Move a dict to GPU (2023-09-27)\nGiven a dict containing list, tuple, and torch.Tensor,\nRecursive\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def move_to_device(X, device): if isinstance(X, dict): for k, v in X.items(): X[k] = move_to_device(v, device) elif isinstance(X, list): for i, e in enumerate(X): X[i] = move_to_device(e, device) elif isinstance(X, tuple) and hasattr(X, \u0026#34;_fields\u0026#34;): # collections.namedtuple dd = X._asdict() dd = move_to_device(dd, device) return type(X)(**dd) elif isinstance(X, torch.Tensor): return X.to(device=device) return X Code from MatchNeRF.\nMake pairs from 2 list Two for in one line:\n1 index_lists = [(a, b) for a in range(n_views - 1) for b in range(a + 1, n_views)] # [(0,1), (0,2), (1,2)] Code from MatchNeRF.\nSlice() for a dim (2023-09-29)\n1 2 3 a = torch.arange(5) x = slice(0, -1, 1) # specify start index, end index, step size a[x] # [0,1,2,3] Convert class to dict (2023-10-05)\nIn python, how do I cast a class object to a dict - SO\ncollections.namedtuple vs typing.NamedTuple- RealPython\ntyping.NamedTuple - Docs\nWhat are \u0026ldquo;named tuples\u0026rdquo; in Python? - SO\n1 2 3 4 5 6 7 8 9 10 11 from typing import NamedTuple class myNT(NamedTuple): r\u0026#34;\u0026#34;\u0026#34;a doc string\u0026#34;\u0026#34;\u0026#34; foo: int bar: str baz: list aTupleObj = myNT(1, \u0026#39;bar\u0026#39;, []) aTupleObj._asdict() NamedTuple (2024-04-09)\nNamedTuple is not mutable, so the following error will occur when trying to re-assign its atttibutes: AttributeError: can\u0026rsquo;t set attribute in python - SO\n1 2 3 4 5 6 7 from typing import NamedTuple class BasicPointCloud(NamedTuple): points : np.array colors : np.array normals : np.array pcd.points = new_points Format digits Fix length filling with zeros: How do I format a number with a variable number of digits in Python? - SO\n'123'.zfill(7): 0000123\nf-string round a float: Fixed digits after decimal with f-strings\n1 2 a = 10.1234 f\u0026#39;{a:.2f}\u0026#39; (2024-01-22)\nFiles Open multiple files: Source code\n1 2 with open(scene_info.ply_path, \u0026#39;rb\u0026#39;) as src_file, open(os.path.join(self.model_path, \u0026#34;input.ply\u0026#34;) , \u0026#39;wb\u0026#39;) as dest_file: dest_file.write(src_file.read()) Pass by Assignment Python is Pass by Reference - RealPython\n(2024-02-08)\nIn Python, passing arguments to a function is assigning the incoming objects to new variable name (identifier). This can be verified through \u0026ldquo;reference counter\u0026rdquo; and namespace, which is a dictionary, including key-value pairs representing a binding between target variable name and object.\n1 x = 2 # object 2\u0026#39;s refcount+1, namespace add: \u0026#39;x\u0026#39;:\u0026#39;2\u0026#39; When an object (value) is assigned to a variable name (identifier), the reference counter of the object is incremented. If the variable name is reassigned to another value, the pre-bounded object\u0026rsquo;s reference counter is decremented.\nEach function has its own namespace, which can be checked by print(locals())\nIf an attribute of the object supports modification, and the object is passed to a function argument, assigning values to the object\u0026rsquo;s attribute is in-place modification. Ref\nMisc:\nUse id() to get the address of a variable\nWhen Python modifies a variable through a function, the variable is not modified in place, but reassigned with the returned value.\nPassing by reference in Python can be replicated with: object\u0026rsquo;s attributes, dictionary (mapping type), and list-like data (subscriptable and mutable)\nExample refers to 3DGS:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class GaussianModel: def __init__(self,): self._xyz = torch.empty(0) class Scene: def __init__(self, gaussian: GaussianModel, exTen, color, ls): self.gaussian = gaussian # a binding created in locals() # The attribute _xyz supports assignment as tensor is mutable # object\u0026#39;s attribute is modified in place: self.gaussian._xyz = torch.ones(2,3) self.myTen = exTen # exTen\u0026#39;s refcount+1, self.myTen = torch.ones(1,4) # update to new binding self.color = color # a binding between self.color and color self.color.view(2,2) # new binding self.ls = ls self.ls[0] = 88 # affect the original list gaussian = GaussianModel() exTensor = torch.empty(0) color=torch.arange(4) myObj = Scene(gaussian, exTensor, color,ls:=[1,2]) print(gaussian._xyz) # changed to ones print(exTensor) # remain empty print(color) # unchanged print(ls) # changed to [88,2] Mutalbe - RealPython\nperplexity\nre Find numbers (2024-03-17)\nHow to extract numbers from a string in Python? - SO\n1 2 3 4 5 6 import re regx_pattern = r\u0026#39;\\d+\\.\\d+|\\d+\u0026#39; txt = \u0026#34;The price is $12.99 and the quantity is 5 21321 dasdsa 123213.\u0026#34; matches = re.findall(regx_pattern, txt) print(matches) Output: ['12.99', '5', '21321', '123213']\nimport (2024-04-09)\nImport file in 2 levels up.\nValueError: attempted relative import beyond top-level package - SO\n1 2 3 4 5 6 7 8 9 10 Project _3DGS scene __init__.py utils # for _3DGS # there is no __init__.py (Don\u0026#39;t know if it matters) utils # for current project __init__.py # empty is ok file2.py I want to use a method (write_j) implemented in file2.py inside scene/__init__.py. Although, the above SO post stated that all folder and subfolders should have an __init__.py, I tried the following syntax works:\n1 from utils.fuse_neighbors import fuse_neighbors Previously, the 3 dots: from ...utils.fuse_neighbors import fuse_neighbors, will lead to an error:\n1 ValueError: attempted relative import beyond top-level package Module (2024-05-16)\nA .py file is a module, not a function.\nAnd a module cannot be called:\n\u0026ldquo;combine_pcs.py\u0026rdquo;:\n1 2 3 def combine_plys(dataset): print(dataset) return eval.py:\n1 2 3 from utils import combine_pcs combine_pcs(dataset) error:\n1 2 3 File \u0026#34;/home/zi/Downloads/CasMVSNet_pl-comments/eval_refine.py\u0026#34;, line 418, in \u0026lt;module\u0026gt; combine_pcs(dataset) TypeError: \u0026#39;module\u0026#39; object is not callable Rectify: from module import the function:\n1 from utils.combine_plys import combine_plys Debug IceCream References:\n使用IceCream搞定Python调试 - 微信- pythonic生物人 Original author: 作者：Kevin Meneses González Notes:\n(2024/11/22)\n同时输出语句和结果 ","date":"2022-12-25T23:37:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/python_misc/","title":"memo: Python | Misc"},{"content":"总结之前讲过的各种模型，引出之后要讲的生成模型\n1. 定义 P1 - 【机器学习】白板推导系列(三十) ～ 生成模型综述(Generative Model Introduction)\n生成模型除了生成数据还能做什么任务？\nGAN 用来生成数据。 GMM 用来做聚类任务的。 这2个都是无监督学习（无标签）。 逻辑回归虽然与概率相关，但它不是生成模型。P(Y=1 | X) = ?, P(Y=0 | X) = ? 只是对条件概率建模，而不关注样本数据X本身的分布，\n所以（概率）生成模型关注的是样本的概率分布本身，而与它要解决的任务没有必然的联系。 既可以解决监督学习的任务（对 P(X,Y) 建模），也可以解决无监督学习的任务，对于隐变量模型，可以构造 P(X,Z) 并建模；或者不用隐变量，比如自回归模型，直接对 P(X) 建模（把P(X) 拆成各个维度之积）。\n所以关注样本的概率分布的模型就是生成模型\n2. 模型分类 P2\n按照解决的任务：监督 vs. 非监督 （自监督未介绍） 对各种模型分类\n任务：分类，回归，标记，降维，聚类，特征学习，密度估计，生成数据\n把机器学习模型分为“概率”与“非概率”一般没太大必要，但是生成模型一定是与概率相关的，所以这里以概率为分类标准\n监督学习任务\n概率模型（建模与概率相关）\n判别模型\n对”条件概率“的分布建模：逻辑回归(LR)，最大熵(MEMM)，条件随机场(CRF)，（输出为概率分布的参数的）神经网络NN\n生成模型\n简单的神经网络只能是判别模型，不是生成模型。但是NN里分步式表示可以和概率图模型结合，就变成了（深度）生成模型 具体见下文 非概率模型（建模时未考虑概率分布），若要解决分类问题，则大概率是判别模型\n感知机PLA，（硬间隔）支持向量机SVM，KNN，（输出为类别的）神经网络NN，树模型 非监督学习任务\n概率模型：必然是生成模型，因为非监督里没有标签Y无法判别，只能描述样本X的概率分布。概率图模型中的大部分是生成模型\n非概率模型：PCA降维（SVD分解），潜语义分析LSA (pLSA, LDA)，K-means，（不带标签的NN）自编码器Auto-Encoder\nPCA 从概率角度看，它就是P-PCA的一种，就是因子分析Factor Analysis LSA 的概率模式是 pLSA, 再改造就是 LDA K-means 从概率角度看，是特殊的GMM Auto-Encoder 的概率模式就是 VAE 各种生成模型 生成模型分成监督，非监督不重要\nNaive Bayes\n朴素贝叶斯是最简单的生成模型，直接描述 x，假设简单：因为它是判别模型，所以它假设是在给定 y 的情况下，样本 x∈ℝᵖ 可以表示成各维度之积\nP(x|y) = ∏ᵢ₌₁ᵖ p(xᵢ|y)\nMixture model\nGMM 认为 x 由 z 生成，在给定 z 的情况下，x服从高斯分布。\nTime-series model\n从时间序列角度，从有限到无限，比如 HMM，卡尔曼滤波，粒子滤波\nNon-parametric\n在参数空间上，从有限到无限：高斯过程 Gaussian process/ Dirichlet process，是非参贝叶斯模型， 它的参数不再是固定的，未知的常数了。高斯分布是参数模型：通过learning把（mu,sigma）学习出来，\nMixed Membership Model\n也是混合模型，比GMM那中复杂些，变量多。LDA 隐含狄利克雷分布，用来做文档聚类\nFactorial Modeel\n因子模型：因子分析FA，概率PCA P-PCA, ICA, 稀疏编码sparse coding\n以上 6 种都是结构化的概率图模型，这些模型每一类都有固定的套路，思想比较底层，专家设计的处理特定问题的算法。与下面的“深度”生成模型对应，它们则是“浅层”生成模型。\n以下是与深度学习神经网络相结合的模型:\nEnergy-based Model\nBoltzmann Machine玻尔兹曼机，包括 sigmoid network，deep belif network， 都是无向图模型\nVAE\n自编码器与概率图结合，用变分的手段处理\nGAN\nAuto regressive model\n自回归网络\nFlow-based model\n3. 模型表示\u0026amp;推断\u0026amp;学习 P3\n从模型表示，推断，和学习的角度去认识一个生成模型\n模型表示 “形神”兼备\n形\n节点可以是离散的，有可以是连续的； 边可以是有向的，也可以是无向的。如果所有的边是有向的，则为有向图模型 以上写出来的11 种除了 玻尔兹曼机，其他都是有向图模型 含有隐变量节点就是隐变量模型，若不使用隐变量就是“fully observed model” 概率图的结构：从层次来看，shallow （前6种） 或者 deep（后5种）； 或者从连接数量来看，就对应 sparse 和 dense 两类， 比如玻尔兹曼机的层间的连接没有缺失,它是稠密的, 而HMM一个隐变量只有2-3条边 神（概率分布本身）\n对于样本的概率密度函数来讲，它既可以是参数化模型，即它的参数是固定的，未知的常量，也可以是非参数化模型，即非参贝叶斯。\n参数角度 parametric vs. Non-parametric models 显性与隐性密度函数 Implicit Density vs. Explicit Density。显性是直接对P(X) 建模，若是隐变量模型，就对P(X,Z)建模，若是fully observed model，就直接对P(X) 分解。 隐性不直接对 P(X) 建模，因为它的任务不是先估计出概率密度函数，再从中生成样本。只需要确保样本是从 P(X) 中生成的即可。上述的只有 GAN 是Implicit 的，其余10种都是显式的 推断 推断是否 tractable， (intractable)\n学习 Likelihood-based model （极大似然估计) vs. Likelihood-free model (GAN 不关心 P(X)，也就不管样本的似然是多少，它有自己的判别器和目标函数）\n4. 模型分类 P4\n主要关注：无监督的，有向图的，深层结构的，参数化的，模型\nLikelihood-based model\n概率密度函数是显式的 推断 Tractable\nFully-observed model 它的概率/似然可直接求出，比如自回归 Change of variable model 变量替换，比如 Flow-based 模型不直接求解复杂的P(X)， 而把x与一个服从简单分布的变量z，用一个连续可逆的复杂函数联系起来，以引入非线性转换，x=g(z)，然后去学习 g(z) ， 因为 z=g⁻¹(x)，所以 Pₓ(X) = Pz(g⁻¹(x) | ∂g⁻¹(x)/∂x) 推断 Intractable 就用近似推断Approximate inference\n基于变分推断，比如VAE 基于马尔科夫链，比如 Energy-based model Likelihood-free model\n概率密度函数是隐式的，不直接关心P(X)， GAN 直接用generator 生成样本，然后用 判别器 去评价样本好坏。它是直接生成样本，而不是先估计PDF，再从PDF中采样生成样本。 不直接生成样本，可以用MC 采样，比如“生成随机网络” GSN，有点类似“去噪自编码器” 5. 概率图 vs. 神经网络 P5\n他俩不是非此即彼的关系，不是互斥的，是独立的，两者都有发生的可能性\n神经网络里有“计算图”\n概率图是概率分布的表示，而(前馈)神经网络是函数逼近器，只不过函数可能比较复杂，如果不给神经网络加修正，它与概率没关系。\nx \u0026ndash;\u0026gt; NN \u0026ndash;\u0026gt; y，y可以是离散的类别，也可以是连续的数值\nNN 的作用只有一个：逼近函数。输入样本，得到目标函数的值，用此值对 NN 中的参数（权重、偏置）求梯度，然后做梯度下降\n概率图模型可笼统的分为：有向图模型（贝叶斯网络），无向图模型（比如Boltzmann machine)\n玻尔兹曼机既属于无向图模型，也属于神经网络，代表“广义连结主义”。\n神经网络也可分为：确定性神经网络（CNN，RNN），随机性神经网络（比如 Boltzmann machine，sigmoid belief network)\n所以在讨论概率图与神经网络的区别时，不考虑 Boltzmann machine， 只比较有向图模型（贝叶斯网络）与确定性神经网络\n从“表示”，“推断”，“学习” 3个角度对比:\n模型表示\n贝叶斯网络：结构化的，浅层的，稀疏的（高维问题有各种条件独立性假设），节点有概率意义，具有可解释性，每个节点在建模时被赋予意义，比如LDA和HMM\n神经网络：深层的，稠密的（无条件独立性假设），节点仅用于计算σ(∑wᵢxᵢ)，没有任何概率意义/物理意义。它的解释性未知也不重要，神经网络每层的意义在建模时并未赋予。\n推断\n贝叶斯网络：精确推断，近似推断，MC采样推断，变分推断，估计后验分布 神经网络：它的推断很容易（前向）但没有意义，参数的分布不重要，只关注输出 学习\n贝叶斯网络：Likelihood-based: EM 神经网络：梯度下降（反向传播：一种高效的求导方法，就是链式求导法则+动态规划（递归+Cache）） 用法\n概率图描述了模型，适合高级别的推理任务 high-level reasoning 神经网络（的计算图）只用来计算，适合低级别简单的推理 low-level reasoning：弱推理，只是分类图像，而没有像人类一样理解；还适合表示学习：声音、图像识别（现在的语言模型是两个的综合） 6. 重参数化技巧（随机后向传播） P6\n最基础的神经网络就是一个函数逼近器。用样本 (X,Y) 去逼近函数 y=f(x;θ)。基于 y 构建目标函数，用BP求神经网络的权重和偏置的梯度，通过随机梯度下降修正\n用神经网络逼近概率分布（概率图），结合到一起就叫随机后向传播 Stochastic Backpropagation 或者叫重参数化技巧 Reparameterization Trick\n假设 y 是一个概率分布，它的概率密度函数是 P(y)。\n假定 P(y) = N(μ,σ²)，当对它采样时，先对中间变量 z 采样，再由 z 得到 y，其中 z 服从标准正态分布 z～N(0,1)，那么 y 与 z 的关系就是 y = μ+σ⋅z。\n因为对标准正态分布是很容易采样的，先采一个 z⁽ᶦ⁾～N(0,1)，那么 y⁽ᶦ⁾=μ+σ⋅z⁽ᶦ⁾\n给定 z，则 y 也固定，把 μ,σ 看作是未知但确定的参数，所以 y 与 z 之间就是一个线性变换。\n可以将 y 看作一个函数 y = f(μ,σ,z)，其中 z 是随机变量，除它之外都是确定性变换，可以用神经网络去逼近这个线性函数\nz \u0026ndash;\u0026gt; NN \u0026ndash;\u0026gt; y，NN 逼近 f；因为 z 是个随机变量，所以 y 也是个随机变量\n以上假设了 P(y) 是正态分布，所以 y 与 z 之间是线性关系，所以神经网络的参数是 μ, σ，令 θ={μ,σ²}。\n可以构造关于 y 的目标函数: J(y) ，因为 y 是关于μ, σ 的函数，所以求梯度∇_θ J(y)时：\n∂J(y)/∂θ = ∂J(y) / ∂y ⋅ ∂y/∂θ\n如果 目标变量 是个条件概率分布: P(y|x) = N(x; μ,σ²)，然后 z 还是服从一个标准正态分布 z～N(0,1)，那么 y 与 z 的关系是：y = μ(x) + σ(x)⋅z 其中 x 是输入，所以 μ,σ 都是 x 的函数。\n仍然可用神经网络去逼近 y 与 z 之间的函数，神经网络的参数是θ：\n1 2 3 x ——\u0026gt; NN ——\u0026gt; y ▲ z ————┘ 更变量之间的关系画得更详细： μ, σ 都从神经网络中出来，它们就是 θ 的函数，则 y = μ_θ(x) + σ_θ(x)⋅z\n1 2 3 4 x ─\u0026gt; NN-θ ──\u0026gt; μ_θ(x) ──── + ──\u0026gt; y └───\u0026gt; σ_θ(x) ─┐ ▲ ▼ │ z ──\u0026gt; × ──┘ 也可以用两个NN 分别逼近 μ, σ : μ(x) = f(x;θ) σ(x) = g(x;θ)\n然后构造关于 y 的目标函数 J_θ(y) = ∑ᵢ₌₁ᴺ ||y-yⁱ||²，然后对 θ 求梯度：\n∂J(y)/∂θ = ∂J(y) / ∂y ⋅ ∂y/∂μ ⋅ ∂μ/∂θ + ∂J(y) / ∂y ⋅ ∂y/∂σ ⋅ ∂σ/∂θ\n所以无论想求一个普通的概率分布，还是要求一个条件概率分布，都可以用神经网络逼近那个概率分布\n在本节的例子中，都是假设 P(y) 是高斯分布，即它是连续的，可微的，而且要求 y 本身是一个连续的随机变量，因为需要 y 对 μ,σ 求偏导。 如果 y 是离散随机变量，就不能用这个方法。\n最后，可以把上面两种情况，合并起来描述： P(y|w)，如果只求 P(y)，则w 就是参数 θ；如果要求 P(y|x)，那 w 就代表 x 和 θ，w={x;θ}，x 无所谓，它是条件概率中的条件，是输入。\n神经网络的参数 w，用神经网络去逼近概率分布 P(y|w)\n","date":"2022-12-25T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/30-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/","title":"watch: ML - 白板 30 | Review of Generative models"},{"content":"1 背景 P1\n频率角度： 解一个优化问题，比如：\n线性回归模型，数据拟合，用数据估计直线的W：f(W) = WᵀX: 策略(loss func): 最小化所有样本点的误差之和(最小二乘估计，无约束的优化问题): L(w) = Σᵢᴺ(wᵀxᵢ-yᵢ)²，Dataset: {(xᵢ,yᵢ)}ᵢᴺ，xᵢ∈ ℝᵖ，yᵢ∈ ℝ。 最优 w^ = argmin L(w)。\n解法：\n解析解：损失函数对 w 求导=0，则W^ = (XᵀX)⁻¹XᵀY, where X is train set 数值解：（随机）梯度下降， SVM 模型，分类问题，估计符号函数：f(w) = sign(wᵀx+b)；\n策略(loss func): 类间大，类内小（有约束的凸优化问题）： L(w) = min 1/2 wᵀw, s.t. yᵢ(wᵀxᵢ+b)≥1, i=1,\u0026hellip;,N\n解法：\n调用 QP 套件 拉格朗日对偶 EM 模型，迭代优化模型参数θ，使似然值取得最大：θ^ = argmax log P(X|θ)\n策略：迭代公式： $θ⁽ᵗ⁺¹⁾ = argmax_θ ∫_Z log P(X,Z|θ) P(Z|X,θ⁽ᵗ⁾) dZ$ （输入数据和隐变量的完全数据分布按照Z的后验分布求期望（加权和，求积分）） 贝叶斯角度： 解一个积分问题\n贝叶斯定理：P(θ|X) = P(X|θ) P(θ) / P(X)，后验分布 = 似然值⋅先验分布/参数空间的积分∫P(X|θ)P(θ)dθ\n贝叶斯推断Inference：依据贝叶斯定理求后验分布P(θ|X)。（然后可求分布的期望，方差）\n贝叶斯决策Decision：借助后验分布P(θ|X)，用已有样本 X 预测新样本 x^ 发生的概率： $P(\\\\^x|X) = ∫ P(\\\\^x,θ|X) dθ = ∫_θ P(\\\\^x|θ,X) P(θ|X) dθ = E_{θ|X} [P(\\\\^x|θ)]$，即对似然P(x^|θ)按照θ的后验分布求期望\n如何求后验分布P(θ|X)：\n精确推断，问题简单（参数空间/隐变量的维度不高）可以直接计算出分母的积分\n近似推断，参数空间维度高，无法直接求出分母积分\n确定性近似\n变分推断 随机近似\nMCMC 马尔科夫链蒙特卡洛方法 MH, Metropolis-Hastings Gibbs, 吉布斯采样 2 公式推导 P2\nCreate on 2022-12-16\n变分推断的目的：找到一个分布 q(Z) 去逼近无法得到解析解(intractable)的后验分布P(Z|X,θ)\nz: latent variable z + parameters θ，把参数也看作随机变量 X: observed data 样本数据 大Z: 样本的隐变量 (X,Z): Complete data P(X,Z) = P(Z|X)P(X)，联合概率 P(X) = P(X,Z)/P(Z|X)，“似然” 省略了θ，重点不在θ 似然取对数，两概率相除变成相减（类似推导也出现在EM，不过下面省略了θ，因为包含在了Z中）：\nlog P(X) = log P(X,Z) - log P(Z|X)\n引入 Z 的分布 q(Z)：\nlog P(X) = log (P(X,Z)/q(Z)) - log (P(Z|X)/q(Z))\n两边同时按照大 Z 的概率密度函数 q(Z) 求似然的期望:\n$$ ∫_Z q(Z)⋅logP(X) dZ = \\\\\\ ∫_Z q(Z)⋅log (\\frac{P(X,Z)}{q(Z)}) dZ \\\\\\ - ∫_Z q(Z)⋅log (\\frac{P(Z|X)}{q(Z)}) dZ \\\\\\ log P(X) = ELBO + KL(q(Z)||P(Z|X)) $$因为 logP(X) 与 Z 无关，所以等号左边没变，而等号右边变成了下界 ELBO + KL(Z的分布||Z的后验)。 ELBO 是 q(Z) 的函数，记为 L(q(Z))，是q(Z)的一个变分。\n当X固定，logP(X) 是固定的，又因为 KL 散度≥0，所以 L(q(Z)) 最大为 logP(X)。 希望 KL 散度最小，让 q(Z) 与 P(Z|X) 最接近，也就是让 L(q(Z) 最大\n数学表达： $\\rm \\\\^q(Z) = arg max_{q(Z)} L(q(Z)) = arg min KL(q(Z)||P(Z|X))$\n求解：\nz 包含隐变量和参数，所以 q(z) 是一个很大的联合概率，假设 q(z) 可以划分成 M 个相互独立的组（统计物理中的平均场理论）： q(z) = ∏ᵢ₌₁ᴹ qᵢ(zᵢ)\n每次只求 1 组 qⱼ，同时固定其余的组{1,2,\u0026hellip;,j-1,j+1,\u0026hellip;M}，逐个求完后，把M组的 q 连乘起来就是整体的 q(z)\nL(q(Z)) = ∫z q(Z)⋅log P(X,Z) dZ - ∫z q(Z)⋅log q(Z) dZ ，把 q(Z) 代入 L = E1 + E2\n对于E1： ∫z q(Z)⋅log P(X,Z) dZ = ∫z log P(X,Z)⋅∏ᵢ₌₁ᴹ qᵢ(Zᵢ) dz₁,z₂,\u0026hellip;, zₘ\n. . . .\n3. 再回首 P3\n符号修正\n. . . .\n4. 随机梯度变分推断-SGVI-1 P4\n基于平均场理论的变分推断无法解决复杂隐变量 z 的情况，比如 z 是一个神经网络，平均场就失效了，z 可以是任意复杂度的。\nstateDiagram-v2 隐变量z --\u003e 观测变量x: Generative model,\\n 条件P(x|z),\\n Decoder 观测变量x --\u003e 隐变量z: Inference model,\\n 后验P(z|x),\\n Encoder 基于平均场理论的变分推断（classical VI）也是一种坐标上升法Coordinate Ascend：一次迭代需要逐个更新 Z 的每个维度。 与此相对的，可以用（随机）梯度上升法解决最大化问题，做变分推断就叫做SGVI / SGVB\n先看下关于 q(z) 的参数的梯度能否求出？\n基于梯度的参数更新公式：θ⁽ᵗ⁺¹⁾ = θ⁽ᵗ⁾ + λ⁽ᵗ⁾⋅∇θ⁽ᵗ⁾。 变分推断以最小化两分布（Z 的假设分布与 Z 的后验分布）的KL散度，或者最大化下界ELBO（ L(q(Z)) ）为目标函数： q^ = arg min_q KL( q(Z) || P(Z|X,θ) ) = arg max_q L(q)， 要更新 q，就需要求出 L 对 q 的梯度：∂L/∂q。\nq(z) 或 q(z|x) 是隐变量 z 的概率分布，x 是观测变量，可以简化掉。 假设 q(z) 是指数族分布，它就有一个参数形式，因此求 q(z) 就是求它的参数。 假设 z 的概率分布 q(z) 是以 ϕ 为参数，如果能求出最好的 ϕ，就相当于得到了 q(z)，所以目标变为去求 ϕ：\nL(ϕ) = ELBO = E_qᵩ(z) [ log (P(x⁽ⁱ⁾,z|θ) / qᵩ(z)) ] = E_qᵩ(z) [ log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z) ] = ∫_z qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) dz - log qᵩ(z)) dz 样本似然：log P(x⁽ⁱ⁾|θ) = L(ϕ) + KL(q||P) ≥ L(ϕ)\n目标函数：ϕ^ = arg maxᵩ L(ϕ)\n把期望写成对随机变量 z 的积分:\n∇ᵩL(ϕ) = ∇ᵩ∫_z qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) dz - log qᵩ(z)) dz ：求偏导 ∇ᵩ 与积分 ∫_z 可交换 = ∫_z [∇ᵩqᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z))] + [qᵩ(z) ⋅ ∇ᵩ(log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z))] dz ：对两项之积求导 = ∫_z ① dz + ∫_z ② dz ：拆为两项分析 在第 ② 项中的 log P(x⁽ⁱ⁾,z|θ) 与 ϕ 无关，对它求导=0，所以 ② 对应的积分为：\n∫z qᵩ(z) ⋅ ∇ᵩ(-log qᵩ(z)) dz = -∫z qᵩ(z) ⋅ 1/qᵩ(z) ⋅ ∇ᵩqᵩ(z) dz = -∫z ∇ᵩqᵩ(z) dz = -∇ᵩ∫z qᵩ(z) dz = - ∇ᵩ1 = 0 所以第2项就约去了，L对 ϕ 求梯度就等于第1项：\n∇ᵩL(ϕ) = ∫z ∇ᵩqᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) dz\n这个式子没办法直接写成期望的形式，如果可以的话，就可以利用蒙特卡罗采样，把未知分布的期望近似出来。\n技巧：利用 ∇ᵩlog qᵩ(z) = 1/qᵩ(z)⋅∇ᵩqᵩ(z) 做等价变换：∇ᵩqᵩ(z) = qᵩ(z) ⋅ ∇ᵩlog qᵩ(z)，然后就可以写成一个期望：\n∇ᵩL(ϕ) = ∫z qᵩ(z) ⋅ ∇ᵩlog qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) dz = E_qᵩ(z) [ ∇ᵩlog qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ]\n所以 L 对 ϕ 的梯度 ∇ᵩL(ϕ) 就等于那一坨关于随机变量 z 的函数按照 qᵩ(z) 求期望。可以用蒙特卡罗采样对其估计：\n假定第 l 个样本 z⁽ˡ⁾~ qᵩ(z), l=1,2,..,L，从分布 qᵩ(z) 中采样 L 个样本， 则 L 个样本的平均值就是近似期望： ≈ 1/L ⋅ ∑ₗ₌₁ᴸ [ ∇ᵩlog qᵩ(z⁽ˡ⁾) ⋅ (log P(x⁽ⁱ⁾,z⁽ˡ⁾|θ) - log qᵩ(z⁽ˡ⁾)) ]\n每一步计算这个期望就是梯度，然后可以用梯度上升法\n5. 随机梯度变分推断-SGVI-2 Source video: P5\n上面使用 MC 采样是有问题的：\n先对 z 采样，算出 qᵩ(z)，当 qᵩ(z) 很小时（靠近0），对应的 log 值（log qᵩ(z⁽ˡ⁾)）变化剧烈，所以梯度 ∇ᵩlog qᵩ(z⁽ˡ⁾)就非常大， 造成整个统计量（所求期望的量:∇ᵩlog qᵩ(Z) ⋅ (log P(X⁽ⁱ⁾,Z|θ) - log qᵩ(Z))）的方差会非常大， 意味着需要更多的样本才能得到比较好的近似，或者如果方差非常大，可能就无法采样，这种直接用MC采样的方法就行不通。\n而且即便用蒙特卡罗采样得到了近似的期望（方差较大，不够精确），它等于下界 L(ϕ) 的梯度，再用梯度上升求分布 qᵩ(Z) 的参数 ϕ^，这样一环扣一环，不精确的梯度再叠加上梯度上升时引入的误差，误差（方差）会越来越大，所以在实际中不可行。\n如何降低统计量的方差？Variance Reduction\n重参数化技巧 Reparameterization trick 最初，下界 L(ϕ) 的梯度 ∇ᵩL(ϕ) = ∇ᵩE_qᵩ(Z) [ log P(X⁽ⁱ⁾,Z|θ) - log qᵩ(Z) ]， 在这个期望中，被统计量（似然-Z后验）和“权重”（Z的分布qᵩ(Z)）都与 ϕ 有关，只能像上面那样先展开，比较复杂。\n如果假定 Z 的分布（概率密度函数）qᵩ(Z) 是已经确定的分布 p(ε)，与 ϕ 无关，然后梯度号就可以直接写到中括号里面，先对似然值求梯度，再按这个确定分布（常量）p(ε)求期望： E_p(ε) [ ∇ᵩ(log P(X⁽ⁱ⁾,Z|θ) - log qᵩ(Z)) ]\n换句话说，z 是分布 qᵩ(Z|X) 中的采样 z～qᵩ(Z|X)，z 是一个随机变量，考虑把 z 和 ϕ 之间的关系解耦，也就是把 z 的随机成分单拎出来。\n重参数化技巧：假定 z 与 ε 和 x 之间有函数关系：z = gᵩ(ε,x⁽ⁱ⁾)，而且 ε 服从一个给定的（简单的）分布 ε~P(ε)。 这样 z 是关于随机变量 ε 的函数，z 仍然是一个随机变量，但它的随机性转移到了 ε 上（当ε和x定了，z就定了）:\nZ～qᵩ(Z|X) ↓ “随机性通过函数关系 g 转移” ε～p(ε)\n因为 ∫z qᵩ(z|x⁽ⁱ⁾)⋅dz = 1，∫ p(ε)⋅dε =1，而且 z 是 ε 的一个（线性）变换， 因此“定性地”认为：|qᵩ(z|x⁽ⁱ⁾)⋅dz| = |p(ε)⋅dε|\n把 ∇ᵩL(ϕ) 中的 z 代换为变换 gᵩ(ε,x⁽ⁱ⁾):\n∇ᵩL(ϕ) = ∇ᵩE_qᵩ(z) [ log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z) ] ，写成对Z积分的形式\n= ∇ᵩ ∫z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ⋅ qᵩ(z|x⁽ⁱ⁾)⋅dz ，代换 = ∇ᵩ ∫z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ⋅ p(ε)⋅dε ，写成期望 = ∇ᵩ E_p(ε) [ log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z) ] ，p(ε)与ϕ无关,∇ᵩ写里面\n= E_p(ε) [∇ᵩ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ] ，先对z求∇，再对ϕ求∇ z 是 ϕ 的函数: z=gᵩ(ε,x⁽ⁱ⁾) = E_p(ε) [∇_z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z|x⁽ⁱ⁾)) ⋅∇ᵩz] ，链式法则 = E_p(ε) [∇_z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z|x⁽ⁱ⁾)) ⋅ ∇ᵩgᵩ(ε,x⁽ⁱ⁾)]\n其中 p(ε) 与 ϕ 无关，两个 log 是 z 的函数（θ是上一时刻的），z是ε的函数，然后是 g 对 ε 的梯度。 此时再用蒙特卡罗采样对 ε 采样：ε⁽ˡ⁾～p(ε), l=1,2,..,L，把 ε 带入 中括号里的那个函数算函数值，再求平均值就是近似的期望：\n∇ᵩL(ϕ) ≈ 1/L ∑ₗ₌₁ᴸ ∇z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z|x⁽ⁱ⁾)) ⋅ ∇ᵩgᵩ(ε⁽ˡ⁾,x⁽ⁱ⁾)]， 其中 z=gᵩ(ε⁽ˡ⁾,x⁽ⁱ⁾)\n然就可以把这个近似梯度带入梯度上升公式：ϕ⁽ᵗ⁺¹⁾ = ϕ⁽ᵗ⁾ + λ⁽ᵗ⁾⋅∇ᵩL(ϕ)， 每一步求个近似期望，得到梯度，再做梯度上升\n","date":"2022-12-20T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/12-%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/","title":"watch: ML - 白板 12 | Variational Inference"},{"content":"Gaussian Mixture Model 高斯混合模型\n1. 模型介绍 Source video: P1\n根据中心极限定理，假设数据服从高斯分布是合理的\n一维数据:\n横轴是数据点，服从两个高斯分布的叠加，纵轴是概率密度函数值(PDF)，点越密代表该区域出现样本的概率越大。\n可以认为这些样本点服从一个高斯分布，但并不合理，用两个高斯比较精确，红色曲线是2个分布的叠加（混合）。\n(1) 从几何角度来看 GMM 的概率密度函数是多个高斯分布的加权平均，并不是直接相加，否则概率密度函数的积分可能大于1。权重是取到各种高斯分布的概率\n每个高斯有自己的均值 μ 和方差 Σ （如果是多维的，就是协方差矩阵），它们的参数是要学习出来的。\nGMM 中一个样本 x 的概率密度函数是K个高斯分布的加权平均： p(x) = ∑ₖ₌₁ᴷ αₖ N(x|μₖ,Σₖ), where ∑ₖ₌₁ᴷ αₖ = 1，α 是权重\n(2) 从混合模型（或概率生成）角度来看 GMM 用离散的随机变量 z 代表样本 x 分别属于 K 个高斯分布 概率。N 个样本是重复 N 次生成过程：先从K个高斯分布中选一个，再在这个高斯分布中采样生成一个样本。\n两维数据的 PDF 是3维的（曲面）:\n小x 是观测变量observed variable，两维的: x=(x1,x2) 引入隐变量latent variable 小z，一个 z 表明了一个样本 x 可能属于哪一个高斯分布，所以 z 是一个离散型的随机变量， 一个样本 x 属于各个高斯分布的概率分别为： z C₁ C₂ \u0026hellip; Cₖ 概率密度 p₁ p₂ \u0026hellip; pₖ z 代表不同的类别categories（不同的高斯分布），其中概率密度求和等于1：∑ₖ₌₁ᴷ pₖ = 1\n一个样本既可属于C1，也可属于C2\u0026hellip;, 只不过概率不同，所以用一个随机变量表达。 用隐变量 z 代表一个样本 x 所属的高斯分布比较方便: x ～ z，z是一个离散分布，可以是 C₁，也可以是 C₂,C₃,\u0026hellip;Cₖ,\nGMM 是一个生成模型（混合模型一般都是生成模型），每个样本都是按照生成过程逐一生成的： 假设有一个骰子有 K 个面，而且重量分布不均匀，所以各面的概率不同（越重越大）\n掷一次得到了第 k 面(概率是pₖ)，对应于第 k 个高斯分布 在这第 k 个高斯分布中去采样，就生成了一个样本 GMM 是最简单的生成模型，它的概率图：\nN 表示生成 N 个样本，小 x 是观测变量，用阴影来表示，小 z 是随机变量，用空心的圈表示，它服从的分布是以 p 为参数: p=(p1,p2,\u0026hellip;,pₖ) 是一个向量，所以用实心点表示参数 p；而 X 服从高斯分布，其参数为 (μ,Σ)\n2. 极大似然 Source video: P2\n从混合模型角度，写出 GMM 的概率密度函数 p(x): 既然引入了隐变量 𝐳，也是一个随机变量，把它的概率积掉就行了，又因为 𝐳 是离散型的随机变量，所以是把各个可能的取值求和。\n一个样本 x 的概率密度函数：\nP(x) = ∑_z P(x,z) = ∑_ₖ₌₁ᴷ P(x,z=Cₖ)，分别从 K 个高斯分布中采出 x 的概率: P(x,zₖ) 累加 = ∑ₖ₌₁ᴷ P(z=Cₖ) P(x|z=Cₖ)，联合概率拆成两项积 = ∑ₖ₌₁ᴷ pₖ ⋅ N(x | μₖ,Σₖ)，第k个高斯分布出现的概率 乘以 在第k个高斯分布中采出x的概率\n所以混合模型的概率密度函数与加权平均是一样的，只不过“权重” aₖ 变成了这里的概率值 pₖ\n做 N 次随机试验，每次做试验时(扔骰子)，(假设)隐变量 𝐳 是不同的（即K个正态分布出现的概率不同）， 每次生成样本时，依据 𝐳 从 K 个高斯分布中选一个分布，再从被选中的高斯分布中采一个 x，所以每个 x 和一个 𝐳 对应: (xᵢ,𝐳ᵢ)。 要求 xᵢ 的似然，就把这次试验中的 𝐳ᵢ 从联合概率P(xᵢ,𝐳ᵢ)中积掉。\nN个样本就是 N 次试验同时发生，发生的概率（似然）就是多次试验的连乘 P(X,Z)=p(x₁,z₁)p(x₂,z₂)\u0026hellip;p(xN,zN))，所以求 P(X)=∫z P(X,Z) dZ 就是多重积分: 先对 z1 积，再对 z2 积，\u0026hellip;.\nx observed variable, 观测随机变量 z latent variable, 隐变量，服从离散分布: K个类别（高斯分布）C₁,C₂,\u0026hellip;,Cₖ 分别对应概率 p₁,p₂,\u0026hellip;,pK； X observed data, N个观测数据 x1,x2,\u0026hellip;xN, 样本之间相互独立 Z \u0026ldquo;latent data\u0026rdquo;, z1,z2,\u0026hellip;zN, N 个样本 x 对应的生成它的隐变量 (X,Z): complete data, “完整数据” (x1,z1),(x2,z2),\u0026hellip;(xN,zN)，每个样本 x 对应一个隐变量 为了叙述方便，用 θ 代表参数 = {p₁,p₂,\u0026hellip;,pK, μ₁,Σ₁, μ₂,Σ₂,\u0026hellip;, μK,ΣK}，隐变量中 K 个高斯分布出现的概率 + K 个高斯的均值和方差 用极大似然估计参数 θ: θ^ = arg max_θ log P(X)\nGMM 是没有解析解的，所以直接用 MLE 是做不出来的，只能用数值方法得到近似解。而且 GMM （混合模型）中含有隐变量，所以用 EM 算法（迭代）会更有效率。\nθ^ = arg max_θ log P(X) = arg max_θ log ∏ᵢ₌₁ᴺ P(xᵢ) ，N个样本iid = arg max_θ ∑ᵢ₌₁ᴺ log P(xᵢ) ，连乘变连加 = arg max_θ ∑ᵢ₌₁ᴺ log ( ∑ₖ₌₁ᴷ pₖ ⋅ N(xᵢ | μₖ,Σₖ) )，将P(xᵢ)代入\n要求极大值对应的参数，可以对上式中的参数（pk,μₖ,Σₖ）求偏导=0，但是因为 log 里面是多项连加，而且高斯分布可能是高维的（表达式很复杂），无法求出解析解\n对于单个高斯分布，它的μ,σ² 都可以直接用 MLE 求出来。因为单一个正态分布的表达式简单，log可以把两项之积拆开，也可以把exp去掉，简化之后，对似然求导，可得到解析解。\n3. EM求解-E-step Source video: P3\n用 EM 算法通过 MLE 求出 GMM 的参数 θ。\nEM 是要最大化似然的下界，即似然的期望：θ⁽ᵗ⁺¹⁾ = arg max_θ ∫_Z q(Z) ⋅ log P(X,Z|θ) dZ， 这个积分的本意是对（每个样本的）似然值 log P(x,z|θ) 按照隐变量 z 的分布 q(z) 求期望。 因为 z 的分布是离散的，包括 K 个（高斯分布出现的）概率值，所以 logP(x,z|θ) 和 q(z) 也都是离散的，二者相乘的积分就是加权和 ∑ q(z)⋅log P(x,z|θ)，就是一个样本的期望。\n因为大 Z 是多个小 z 的联合，是多个”事件“同时发生的联合概率，又因为样本之间独立同分布，大Z的概率就是单个 z 的概率连乘：q(Z)=q(z₁)⋅q(z₂)\u0026hellip;q(zN)， 所以对大 Z 的积分就是多重积分，即对每个小 z 积分: ∫z₁∫z₂\u0026hellip;∫zN； 而只做有限 N 次试验的话，就是把 N 次实验的加权和连乘起来：\nE = ∫_Z q(Z) ⋅ log P(X,Z|θ) dZ ，X和Z都是大写, N个样本集合. (|也可写成;: θ是随机变量vs固定值 ¹)\n= ∫_z₁ ∫_z₂\u0026hellip;∫_zN q(Z) ⋅ log P(X,Z|θ) dz₁ dz₂ dzN，把q(Z)写开\n= ∑_z₁ ∑_z₂\u0026hellip;∑_z$\\_N$ ∏ᵢ₌₁ᴺ q(zᵢ) ⋅ log ∏ᵢ₌₁ᴺ P(xᵢ,zᵢ|θ)，zᵢ都是离散分布\n= ∑_z₁ ∑_z₂\u0026hellip;∑_z$\\_N$ ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ log ∏ᵢ₌₁ᴺ P(xᵢ,zᵢ|θ) ，当 q(zᵢ) 等于Z的后验分布时，KL散度=0，\u0026ldquo;积分\u0026rdquo;（似然的期望）取到最大。 = ∑_z₁ ∑_z₂\u0026hellip;∑_z$\\_N$ [ ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ ∑ᵢ₌₁ᴺ log P(xᵢ,zᵢ|θ) ]，log连乘变连加\n把这个期望 E 记为 Q(θ,θ⁽ᵗ⁾)，是关于 θ 的一个函数（log 中的 θ 是变量，而θ⁽ᵗ⁾是常数）。\n先把 log 的连加展开：\nQ(θ,θ⁽ᵗ⁾) = ∑_z₁ ∑_z₂\u0026hellip;∑_z$\\_N$ ∏ᵢ₌₁ᴺP(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ [ log P(x₁,z₁|θ) + log P(x₂,z₂|θ) + \u0026hellip; + log P($x\\_N,z\\_N$|θ)]\n多重\u0026quot;积分\u0026quot;，先对 z₁ 积分：先只取出第 1 项 logP(x₁,z₁|θ)，并且只把 z₁ 从联合概率中取出来：\n∑_z₁ ∑_z₂\u0026hellip;∑_z$\\_N$ [ log P(x₁,z₁|θ) ⋅ ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ]\n= ∑_z₁ ∑_z₂\u0026hellip;∑_zN [ log P(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾) ⋅ ∏ᵢ₌₂ᴺP(zᵢ|xᵢ,θ⁽ᵗ⁾) ] ，把只与 z₁ 相关的项分出来 = ∑_z₁ logP(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾) ⋅ ∑_z₂\u0026hellip;∑_zN [ ∏ᵢ₌₂ᴺP(zᵢ|xᵢ,θ⁽ᵗ⁾) ]\n对于后半部分的从 2 到 N 的联合概率：\n∑_z₂\u0026hellip;∑_zN [ ∏ᵢ₌₂ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ] = ∑_z₂\u0026hellip;∑_zN [ P(z₂|x₂,θ⁽ᵗ⁾) ⋅ P(z₃|x₃,θ⁽ᵗ⁾) \u0026hellip; ⋅ P(zN|xN,θ⁽ᵗ⁾)] 因为每一个累加号都只与一个 z 相关，也就是把每个 z 对应的离散概率分布（p₂,\u0026hellip;,pK）求和，并且概率密度函数积分等于 1: = ∑_z₂ P(z₂|x₂,θ⁽ᵗ⁾) ∑_z₃ P(z₃|x₃,θ⁽ᵗ⁾) \u0026hellip; ∑_zN P(zN|xN,θ⁽ᵗ⁾) = 1⋅1⋅\u0026hellip;⋅1 = 1\n所以对 z₁ 积分的结果，除了只与 z1 相关的项: ∑_z₁ logP(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾)，其余都是1。同理之后对 z2, z3,\u0026hellip;zN积分时，也只保留与自己相关的项\nQ(θ,θ⁽ᵗ⁾) = ∑_z₁∑_z₂\u0026hellip;∑_zN ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ [logP(x1,z1|θ) + logP(x2,z2|θ) +\u0026hellip; + logP(x_N,zN|θ)]\n= ∑_z₁ log P(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾) + ∑_z₂ log P(x₂,z₂|θ) ⋅ P(z₂|x₂,θ⁽ᵗ⁾) + \u0026hellip; + ∑_zN log P(xN,zN|θ) ⋅ P(zN|xN,θ⁽ᵗ⁾) = ∑ᵢ₌₁ᴺ ∑_zᵢ log P(xᵢ,zᵢ|θ) ⋅ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ，这里xᵢ,zᵢ都是小写，单个样本\n综上，似然P(X)的期望等于 N 个样本的似然期望的和。\n其中，单个样本的联合概率（或者说似然）: P(x,z|θ) = P(z|θ) P(x|z,θ) = p_z ⋅ N(x | μ_z,Σ_z)，这里下标 z 表示该样本的隐变量。 因为 P(x) = ∑ₖ₌₁ᴷ pₖ ⋅ N(x | μₖ,Σₖ)， 所以单个样本的后验概率：P(z|x,θ) = P(x,z)/P(x) = p_z ⋅ N(x | μ_z,Σ_z) / ∑ₖ₌₁ᴷ pₖ ⋅ N(x | μₖ,Σₖ)\n把似然和后验代入 Q:\nQ(θ,θ⁽ᵗ⁾) = ∑ᵢ₌₁ᴺ ∑_zᵢ log (p_zᵢ ⋅ N(xᵢ | μ_zᵢ,Σ_zᵢ)) ⋅ [ p_zᵢ ⋅ N(xᵢ | μ_zᵢ,Σ_zᵢ) / ∑ₖ₌₁ᴷ pₖ ⋅ N(xᵢ | μₖ,Σₖ) ] ，zᵢ是1⋯K中的任意一个\n以上就是 EM 中的 E-step：把（N个样本的）似然的期望表示出来。M-step 要关于参数 θ 求 Q 的最大值。\n4. EM求解-M-step Source video: P4\n上面得到了 Q 的表达式，对每个样本的似然按照后验求期望，然后N个样本累加。 M-step 要解一个最优化问题。\n后验展开是：p_zᵢ ⋅ N(xᵢ | μ_zᵢ⁽ᵗ⁾,Σ_zᵢ⁽ᵗ⁾) / ∑ₖ₌₁ᴷ pₖ⁽ᵗ⁾ ⋅ N(xᵢ | μₖ⁽ᵗ⁾,Σₖ⁽ᵗ⁾)\n在后续推导中，仍然把一个样本z 的后验简记为： P(zᵢ|xᵢ,θ⁽ᵗ⁾)，因为 θ⁽ᵗ⁾是上一时刻的参数：θ⁽ᵗ⁾={p₁⁽ᵗ⁾,p₂⁽ᵗ⁾,\u0026hellip;,pK⁽ᵗ⁾, μ₁⁽ᵗ⁾,Σ₁⁽ᵗ⁾, μ₂⁽ᵗ⁾,Σ₂⁽ᵗ⁾,\u0026hellip;, μ_K⁽ᵗ⁾,Σ_K⁽ᵗ⁾}，是常数，所以就简单写。 而似然 logP(xᵢ,zᵢ|θ) 中的θ 是变量，所以似然的期望的累加就表示为：\nQ(θ,θ⁽ᵗ⁾) = ∑ᵢ₌₁ᴺ ∑_zᵢ log (p_zᵢ ⋅ N(xᵢ | μ_zᵢ, Σ_zᵢ)) ⋅ P(zᵢ|xᵢ, θ⁽ᵗ⁾)\n交换两个累加符号顺序：\nQ(θ,θ⁽ᵗ⁾) = ∑_zᵢ ∑ᵢ₌₁ᴺ log (p_zᵢ ⋅ N(xᵢ | μ_zᵢ, Σ_zᵢ)) ⋅ P(zᵢ|xᵢ, θ⁽ᵗ⁾)\nzᵢ 是一个样本上，K个高斯分布可能的概率，z 在最外面，可以把 zᵢ 替换成小 k（从1到K），∑_zᵢ 替换为 ∑ₖ₌₁ᴷ：\nQ(θ,θ⁽ᵗ⁾) = ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ log (pₖ ⋅ N(xᵢ | μₖ, Σₖ)) ⋅ P(zᵢ=Cₖ|xᵢ, θ⁽ᵗ⁾) = ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ [ log pₖ + log N(xᵢ | μₖ, Σₖ) ] ⋅ P(zᵢ=Cₖ|xᵢ, θ⁽ᵗ⁾)\n目标函数就为： θ⁽ᵗ⁺¹⁾ = arg max_θ Q(θ,θ⁽ᵗ⁾)\nθ⁽ᵗ⁺¹⁾ 包括 pₖ⁽ᵗ⁺¹⁾={p₁⁽ᵗ⁺¹⁾,p₂⁽ᵗ⁺¹⁾,\u0026hellip;,p_K⁽ᵗ⁺¹⁾}, μₖ⁽ᵗ⁺¹⁾={μ₁⁽ᵗ⁺¹⁾,μ₂⁽ᵗ⁺¹⁾,\u0026hellip;,μ_K⁽ᵗ⁺¹⁾}, Σₖ⁽ᵗ⁺¹⁾={Σ₁⁽ᵗ⁺¹⁾,Σ₂⁽ᵗ⁺¹⁾,Σ_K⁽ᵗ⁺¹⁾}，\n下面只介绍 pₖ 的求法：\n在 Q(θ,θ⁽ᵗ⁾) 中，只有 log pₖ 与 pₖ 相关：\np⁽ᵗ⁺¹⁾ = arg max_pₖ ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ log pₖ ⋅ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾), s.t. ∑ₖ₌₁ᴷpₖ=1\n求这个带约束的最优化问题，用拉格朗日乘子法求解:\n写出拉格朗日函数：\nL(pₖ,λ) = ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ log pₖ ⋅ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + λ(∑ₖ₌₁ᴷpₖ - 1)\n对 pₖ 求偏导时，后面的z的后验分布是常量，求导后是系数。pₖ 的 k 是从1到K的任一数，只对 pₖ 求导，所以带有 p₁,p₂,\u0026hellip;, pₖ₋₁ 的项都是0，即最外层的累加号没了。\n∂L/∂pₖ = ∑ᵢ₌₁ᴺ 1/pₖ ⋅ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + λ ≜ 0 ，令其等于0\n两边同时乘以 pₖ:\n∑ᵢ₌₁ᴺ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + pₖ ⋅ λ = 0\n为了利用约束条件，把 k=1,2,\u0026hellip;,K 的式子都加起来：\n∑ᵢ₌₁ᴺ ∑ₖ₌₁ᴷ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + ∑ₖ₌₁ᴷpₖ ⋅ λ = 0\n其中 ∑ₖ₌₁ᴷ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) 是对隐变量 zᵢ 的概率密度函数“积分”，为1，所以\n∑ᵢ₌₁ᴺ 1 + λ = 0 λ = -N\n把 λ=-N 代入上式：\n∑ᵢ₌₁ᴺ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + pₖ ⋅ (-N) = 0 pₖ⁽ᵗ⁺¹⁾ = 1/N ⋅ ∑ᵢ₌₁ᴺ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾)\n上式是通项，把 k=1,2,..K，代入就可得到每个 p。\n求 μₖ 和 Σₖ 的方法类似，只关心 log N(xᵢ | μₖ,Σₖ)，log 可以把高斯分布的概率密度函数简化（两项积拆成两项和，exp也可去掉），而且是无约束，直接（关于矩阵）求导，令其=0就行\nRef 概率论：p(x|theta)和p(x;theta)的区别 - -柚子皮- - CSDN ","date":"2022-12-19T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/11-gmm/","title":"watch: ML - 白板 11 | GMM"},{"content":"Source videos: 【机器学习】白板推导系列(三十二) ～ 变分自编码器(VAE)】\n1. 模型表示 P1\nVariation 来自概率图模型； Auto Encoder 来自神经网络\nVAE 也是一种 LVM (Latent Variable Model)，最简单的隐变量模型是 GMM，它的概率图模型表示为：\nflowchart TB latent((z)) --\u003e observed((x)) 小 x 是观测变量，小 z 是假想的服从某种分布的隐变量，先对 z 的分布 P(z) 中采样，再在 z 固定的情况下，从分布 P(x|z) 中采样 x。\nGMM K 个高斯分布的混合，样本数据 x 可能来自于这 K 个高斯分布的任一个，只是概率不同。 而 VAE 是无限个高斯分布的混合。\nGMM 假设隐变量 z 是服从一维的离散型概率分布：z～Categorical dist，分布列：\nz 1 2 \u0026hellip; K p p₁ p₂ \u0026hellip; pₖ z 代表不同的类别categories（不同的高斯分布），其中概率密度求和等于1：∑ₖ₌₁ᴷ pₖ = 1。x 在 z 选定的情况下，服从高斯分布：x|zₖ～N(x|μₖ,Σₖ)\nGMM 顶多可以被用来做聚类任务，但无法解决复杂的任务，比如目标检测，因为它的 z 太简单了，只有 1 维还是离散型的变量，只能对 1 个属性划分 K 类别，\n比如有一群人作为样本，想学习出一个z 用来表示一个样本。 GMM 假设了 z 只是一个 1 维K类的变量，就只能把这群人分成 K 类 （比如按“职业”属性分成：工人，农民，知识份子\u0026hellip;），所以只能表达出一个人是来自于哪类。 也就是说，GMM 对样本的表达非常肤浅，因为一个人有多个属性，必须用用多个维度表达： 比如 “性别”z₁={男,女}，“肤色”z₂={黄,白,黑}，“年龄”z₃={ℤ}，“身高”z₄连续的，这么多维度 GMM 无法表达出来。\n隐变量 z 应该是高维的，连续的随机变量，假设 z 服从高斯分布: z～N(μ=0,Σ=I)，满足高维连续。P(z) 是先验，它并不重要，只是辅助建模，我们最终关心的是 inference 过程：给一个 x 返回它的 z，也就是后验 P(z|x)\n如果样本 x 本身是连续的，可以假设它的条件概率也服从高斯分布：x|z～N(μ(z),Σ(z))。若是离散的，则仍然用 categorical distribution。\n条件概率 x|z 的均值和方差都是 z 的函数，也就是先给定了 z，然后求出 μ(z) 和 Σ(z)，相当于得到了 x。所以实际上要学 μ,Σ 与 z 之间函数关系。 可以用神经网络逼近出来这个函数，所以 μ,Σ 是神经网络（参数θ）的函数，用 μ_θ, Σ_θ 表示。\n而不直接通过算 Likelihood 求 x|z 的概率分布，是因为 z 的维度太高，不好把 z 积掉：P(x) = ∫_z P(x,z) dz =∫_z P(z) P(x|z) dz。 因为假设了 z 的维度很高，高到积分算不出来，则 P(x) 是 intractable 的， 又因为后验分布 P(z|x) = P(x|z)P(z) / P(x)，所以z的后验也算不出来。也就无法从 x 到 z 做inference。 只能用重参数化技巧和神经网络逼近后验分布。\n推断\u0026amp;学习 P2\nflowchart TB latent((z)) -- \"Pᶿ(x|z) \\n Decode \\n Generation\" --\u003e observed((x)) observed((x)) -. \"Pᶿ(z|x) or qᶲ(z|x)\\n Encode \\n Inference\" .-\u003e latent((z)) z 是隐变量假定服从高斯分布 P(z)=N(0,I)，x 是观测变量假定服从高斯分布 P_θ (x|z)=N(μ_θ (z), Σ_θ (z))。 假如参数 θ 已经训练好了，生成样本时，先从 z 的分布 P(z) 中采样一个 z⁽ⁱ⁾，然后就能从 P_θ( x|z⁽ⁱ⁾) 采样出一个 x⁽ⁱ⁾。\n因为后验分布 P(z|x) 无法通过贝叶斯公式算出，所以用 q_ϕ(z|x) 不断逼近后验分布P_θ(z|x)。\n用 EM 求解 GMM 时，是最大化似然，似然可以分成下界 ELBO 和 KL( q_ϕ(z|x) || P_θ(z|x))。\n最原始的 EM 的 E-step 要写出n个样本似然p(x)的（最大的）期望：当 q(z|x)=P(z|x) 时, KL=0, 似然的期望就是 ELBO。\nM-step 就是解最大化问题：θ = arg max_θ ELBO = arg max_θ E_P(z|x,θ⁽ᵗ⁺¹⁾) [ log P(x,z|θ⁽ᵗ⁾) ]. E-M 即“最大的期望 与 期望的最大” VAE 不能用基础的 EM 解决，因为需要 q(z|x) 能取到 P(z|x)，但是这里的后验 P(z|x) 是 intractable 的，所以只能让 KL 散度足够小，找出最好的 q，也就是找出它最好的参数ϕ，让 q_ϕ(z|x) 足够接近 P_θ(z|x)。\n在EM中，通过引入隐变量 z 和 Z 的分布 q(Z)，对数似然 logP(X) 被拆分成 ELBO + KL散度(q(Z)||P(Z|X))。 对应到这里的情况，KL 散度中的 q(Z) 应该是后验 qᵩ(z|x)，也就是对似然 P(x|z) 引入的是 qᵩ(z|x)，同样 ELBO 中也是按 q(z|x) 求期望。 ELBO 把里面的 log 展开，即可写成联合概率P(z,x) + 熵H[q(z|x)]；再把联合概率拆开，其中 z 的先验 P(z) 可与后验的熵 H[q(z|x)] 结合成一个KL散度：z的后验与z的先验要靠近。\n$\u003c\\\\^θ,\\\\^ϕ\u003e$ = arg min KL( q_ϕ(z|x) || P_θ(z|x) ) = arg max ELBO ，等价于最大化似然P(x|z)的下界 = arg max E_qᵩ(z|x) [ log (P_θ (z,x)/qᵩ(z|x)) ] ，以 z 的后验加权 = arg max E_qᵩ(z|x) [ log P_θ (z,x) - log qᵩ(z|x)) ] = arg max E_qᵩ(z|x) [ log P_θ(z,x) ] - E_qᵩ(z|x) [ log q(z|x) ] ，第2项是 q(z|x) 的熵 = arg max E_qᵩ(z|x) [ log P_θ(z,x) ] + H[qᵩ(z|x)] ，log里面的联合概率拆开 = arg max E_qᵩ(z|x) [ log P_θ(x|z) ] + E_qᵩ(z|x) [ log P(z) ] + H[qᵩ] ，P(z) 是先验分布不带参数\n= arg max E_qᵩ(z|x) [ log P_θ(x|z) ] + E_qᵩ(z|x) [ log P(z)-log q(z|x)] = arg max E_qᵩ(z|x) [ log (P_θ(x|z) ] + E_qᵩ(z|x) [ log (P(z) / q(z|x))] = arg max E_qᵩ(z|x) [ log P_θ(x|z) ] + ∫_z qᵩ(z|x)⋅log ( P(z) / q(z|x)) dz = arg max E_qᵩ(z|x) [ log P_θ(x|z) ] - KL( qᵩ(z|x) || P(z) )\n目标函数是一个期望减 KL 散度，期望要最大，而KL散度要最小。\n变分推断用梯度上升求最大化问题，首先求目标函数对 θ,ϕ 的梯度，然后更新\n采用 SGVI / SGVB / SVI / Armotized Inference （重参数化技巧+神经网络）求后验，解决 Inference 问题。\nSGVI 假设 z|x 服从高斯分布 z|x～N(μ_ϕ(x),Σ_ϕ(x))，并且与随机高斯噪声 ε～N(0,I) 之间有函数关系 z= μ_ϕ(x) + Σ_ϕ¹ᐟ²(x)⋅ε，协方差矩阵的指数是½，因为 Σ 里面是σ²。\nflowchart LR x(\"Input x\") --\u003e net(\"NN-ϕ\") --\u003e a(\"μ(x)\") --\u003e o((\"+\")) --\u003e z(\"latent z\") net --\u003e b(\"Σ(x)\") --\u003e m((\"×\")) --\u003e o ε --\u003e m 对于目标函数中的 E_qᵩ(z|x) [ log P_θ(x|z) ]，中括号里面是给定 z 生成 x，而期望的权重 q(z|x) 是给定 x 时，z 的后验概率。 整个训练过程就是，先给了样本 x 得到了后验 q_ϕ(z|x)，从中采样得一个 z⁽ⁱ⁾，然后用它算似然 logP(x|z⁽ⁱ⁾)，是一个环路。 所以在训练过程中，log 里的 z 不是先验 P(z)，而是后验 P(z|x)。 在训练好之后，得到了θ，生成样本时，就直接从 P(z) 中采样，再代入 P_θ(x|z) 得到 x。\nflowchart LR x --\u003e Decoder --\u003e z --\u003e Encoder --\u003e x' 目标函数后面的 KL 散度相当于 正则化项。在训练时，要让 q_ϕ(z|x) 尽量与先验 P(z) 靠近，避免坍缩到一个点上，否则第 1 项似然的期望很可能就过拟合了。 或者说让 q_ϕ 的熵 H[q_ϕ] 倾向于大。 熵意味着信息量，信息量大意味着有广泛的可能性，分布更平均，高斯分布越扁方差越大熵越大，钟形曲线越瘦高，说明只在期望那一个点上可能性最大，基本上是确定的，熵就很小，\nsumNote\n","date":"2022-12-19T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/32-vae/","title":"watch: ML - 白板 32 | VAE"},{"content":"先验概率 标签的直观分布\n后验概率： 在某一事件先成立的条件下，标签的分布。\n假设已知玩不玩英雄联盟这件事情 (𝐘) 上的概率分布（先验）为：\nP(Y=玩)=0.6；P(Y=不玩)=0.4\n另外，已知性别 (𝐗) 分布（似然/类条件概率）：玩LOL人群中:80%是男生,20%女生；不玩LOL的人中有:20%男生,80%女生，也就是：\nP(X=男性|Y=玩lol)=0.8，P(X=小姐姐|Y=玩lol)=0.2 P(X=男性|Y=不玩lol)=0.2，P(X=小姐姐|Y=不玩lol)=0.8\n求：一个男生他玩LOL的概率（后验，它是在先观察到性别X事件发生后得到的）\n根据贝叶斯定理 P(Y|X)=(P(X|Y)⋅P(Y))/P(X), P(Y=玩 | X=男性) = P(X=男性|Y=玩)⋅P(Y=玩) / (P(X=男性|Y=玩)⋅P(Y=玩) + P(X=男性|Y=不玩)⋅P(Y=不玩))\n知乎用户V6oo4r 的评论： 先验概率是以全事件为背景下，A事件发生的概率: P(A|Ω). 后验概率是以新事件B为背景下，A事件发生的概率: P(A|B).\n全事件一般是统计获得的，所以成为先验概率，没有做实验前的概率。\n新事件一般是实验，如试验B，此时的事件背景从全事件变成了B，该事件B可能对A的概率有影响，那么需要对A现在的概率进行一个修正，从P(A|Ω) 变成了 P(A|B)，所以成 P(A|B) 为后验概率，也就是试验（事件B发生）之后的概率。 P(A|B)= P(B|A)⋅P(A|Ω)/P(B|Ω)\n例子来源：如何理解先验概率与后验概率-昌硕-知乎专栏\n机器学习-白板推导系列(一)-开篇\n频率派 参数θ可能不仅是一个，也可能是一组数\n贝叶斯派 参数 θ 是随机变量，服从一种概率分布（先验分布）\n贝叶斯定理把参数的先验分布和后验分布通过似然值联系起来，参数的后验分布= 似然x先验分布/在参数空间对样本数据积分。\n参数估计的方法：MAP 最大后验估计\n","date":"2022-12-16T13:02:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/01_%E9%A2%91%E7%8E%87%E6%B4%BE-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE/","title":"watch: ML - 白板 01 | Frequentist vs Bayesian"},{"content":"贝叶斯解释“L1和L2正则化”，本质上是最大后验估计。如何深入理解贝叶斯公式？\n(贝叶斯公式中的概率可以是随机事件的概率，也可以是概率分布wiki)\n似然函数 L(θ|X)：X发生之后，关于θ的函数。 似然值：参数 θ 发生后，X 发生的概率，即 L(θ|X) = P(X|θ)。\n频率派认为训练数据是来自真实分布的采样，似然值（X发生概率）最大的 θ 就是最接近真实的，但并不能说这组参数就是最有可能的分布。 贝叶斯派想直接求参数 θ 的概率，认为真实θ分布发生的概率应该最大。\nMLE 在两个地方做了近似：寻找使 X 发生概率最大的 θ 并不等于 θ 发生的概率最大；θ 是一个分布，找到的是它的众数。 MAP 只有一个近似：寻找概率最大的 θ 分布是严谨的，只剩\u0026quot;θ 是一个分布，找到的是它的众数\u0026quot;。\n(221216): 概率模型的参数θ出现的概率是概率密度函数，但在概率密度函数上，任一个点（一个θ）的概率是0，因为它对应的面积为0，所以只有谈论\u0026quot;一段区间\u0026quot;出现的概率才有意义，比如θ落在0.2~0.3之间的概率为60%。\nMAP 等于 MLE 加上正则化 θ 在训练集 X 上发生的概率 P(θ|X) 可用贝叶斯公式展开：\nP(θ|X) = (P(X|θ)/P(X)) ⋅ P(θ)\n模型参数 θ 的概率 = X 在参数为 θ 时发生的概率 与 X 在所有可能的 θ 下发生的概率之和 的比值 × θ 的起始概率。 其中 P(X) =∫ P(X|θ)P(θ) dθ 。\nP(θ|X) 和 P(θ) 都是 θ 的概率，不过前者是有条件的（是X发生之后的，是更新后的），所以叫做“后验分布”（X是试验，即“试验之后”）；后者是在X发生之前，（不看X）心中已有的知识，是先验分布。\nP(X|θ) 和 P(X) 都是 X 的概率，不过前者是在参数为 θ 时，X 发生的概率，后者是各种参数得到 X 的概率之和，它们的比值是先验分布 P(θ) 的置信度。\n从先验分布P(θ)出发，算出实验结果 X 在参数为 θ 时发生的概率，然后除以 X 在各种θ时发生的概率之和，做为系数，对先验分布做修正。\nP(X) 是对所有可能的 θ 做积分（也称边缘概率），比如对于抛硬币实验，就要对正-反概率：从 (正:0, 反:1) 到 (正:1, 反:0) 之间的所有情况，求出X发生的概率再积分（是个常数）。无法直接求，可以用蒙特卡洛方法近似。 不过，想求概率最大时的 θ，并不必求出概率最大是多少。因为 P(X) 与 θ 无关，所以 P(θ|X) ∝ P(X|θ) P(θ)，只需求似然度乘以先验分布乘积最大时，对应的 θ 就行。\nMLE 认定：θ = arg max_θ P(X|θ)；\nMAP 认定：θ = arg max_θ (P(X|θ)⋅P(θ))\n(MLE和MAP的求解推导 极大似然估计/最大后验估计—通过抛硬币例子理解)\n代入神经网络中的符号：\nMLE: W = arg max_W P(X,Y|W) ∝ arg max_W log P(X,Y|W)，这就是最大似然估计损失函数（“是猫”的概率）\nMAP: W = arg max_W P(W|X,Y) ∝ arg max_W (log P(X,Y|W) + log P(W))，损失函数相比 MLE 多了先验分布 P(W)\n理论上先验分布可以任意选择，只要使用大量数据迭代无限次后，后验分布的最大概率对应的就是真实θ，但如果数据量有限，不同的先验分布会收敛到不同的θ。\n如果采用正态分布 wᵢ~N(0,σ²) 作为先验分布 P(W)，则其对应 L2 正则化项：λ||𝐖||₂+C：\n$$ log P(W) = log ∏_i \\frac{1}{σ\\sqrt{2π}} e^{-\\frac{(w_i-0)²}{2σ²}} = -\\frac{1}{2σ²}∑_i w_i² + C $$ 如果采用拉普拉斯分布 wᵢ~Laplace(0,b) 作为先验分布 P(W)，则其对应 L1 正则化项：λ||𝐖||₁+C：\n$$ log P(W) = log ∏_i \\frac{1}{2b} e^{-\\frac{|w_i-0|}{b}} = -\\frac{1}{b}∑_i |w_i| + C $$ 正态分布和拉普拉斯分布都是限定了高维空间中的向量 𝐖 的位置，L2正则化约束了向量模长（坐标平方和再开方）服从正态分布，L1正则化约束了两向量距离（坐标之差）服从拉普拉斯分布\n如果先验分布 P(W) 是平均分布，概率是常数，与W无关，MAP 就退化成了 MLE\nMAP 比 MLE 多了一个先验分布，先验分布就是正则化项。\n贝叶斯公式描述的是：用新的实验结果对先验分布做修正，先验分布就是优化的起点，不同的先验分布，对应的优化起点不同.\n最大后验估计像是一个损失函数的集合，选择不同的先验分布，相当于选择了不同的损失函数，包括MLE, L1正则化，L2正则化。\nMAP 比 MLE 更正确 有一个人，女性，27岁，985硕士毕业，单身，在上海生活，平时喜欢表达，幽默，有些理想主义，关心少数人群，经常在网上发表犀利言论，问这个人更有可能是一个脱口秀演员呢？ 还是更有可能是一个女性主义的脱口秀演员？\n直觉选择后者，但其实前者概率更大，因为“脱口秀演员”包含的范围更大。\n人的直觉运用了最大似然估计，选择“女性主义的脱口秀演员” 更可能出现描述的那些特质。 如果运用最大后验估计，按照概率乘法即可计算出哪个情况概率较大。\n给定特征集合 D={女性，27岁，985硕士毕业，\u0026hellip;, 发表犀利言论}， 结论集合 T = {t₁=p₁, t₂=p₁^p₂}，其中 p₁ 是脱口秀演员，p₂ 是女性主义脱口秀演员\n最大似然估计： T = arg max_T P(D|T)\n当 T = t₂ 时：P(D|T=t₂) = P(D|p₁^p₂) = P(p₁^p₂|D) P(D) / P(p₁^p₂) = P(p₁,p₂,D)/P(p₁,p₂) = P(p₂|p₁,D) P(D|p₁) P(p₁) / (P(p₁|p₂)P(p₁)) = P(p₂|p₁,D) P(D|p₁) / P(p₁|p₂) = (P(p₂|p₁,D)/P(p₁|p₂)) P(D|T=t₁)\nP(D|T=t₂) 与 P(D|T=t₁) 之间相差一个大于1的系数，所以当 T=t₂ 时似然值更大\n最大后验估计： T = arg max_T P(T|D)\n当 T = t₂ 时：P(T=t₂|D) = P(p₁^p₂|D) = P(D|p₁^p₂) P(p₁^p₂) / P(D) = P(p₁,p₂,D) / P(D) = P(p₂|p₁,D) P(p₁|D) P(D)/ P(D) = P(p₂|p₁,D) P(p₁|D) = P(p₂|p₁,D) P(T=t₁|D)\nP(T=t₂|D) 与 P(T=t₁|D) 之间相差一个小于1的系数，所以当 T=t₁ 时后验概率更大\n所以用贝叶斯方式思考更理性\n用贝叶斯理解概率 概率的两种理解：频率派和贝叶斯派\n频率派认为概率是某件事多次发生的频率，抛硬币可以多次重复，但不适合解释神经网络，(多分类)神经网络蕴含的模型与人脑中的模型的差异是损失函数，被看做为一个概率（“是猫的概率”）。这个概率是是softmax 对各类别上的 activation z 做归一化后得到的，是样本之间的差异，不好用频率解释。\n贝叶斯公式中用的都是概率密度函数，先验概率 P(θ) 理解成在没有数据可供参考时，对神经网络参数的相信程度，相信程度可以是正态分布；后验概率是已知数据X之后，经过修正的对网络参数的相信程度。θ分布修正后，似然度就发生变化，在配分函数中的占比就变化，从而继续修正分布，当某个分布的似然度在配分函数中占比最大时，它就是最优的 θ 分布，其后验概率最大，它发生的可能性最大，对它的相信程度最高。\n分母 P(X) 是边缘概率，也叫配分函数，将其按条件概率展开：\n$$ P(θ_i | X) = \\frac{P(X|θ_i) P(θ_i)}{∑ P(X|θ) P(θ)} = \\frac{P(X|θ_i) P(θ_i)}{\\int_θ P(X|θ) P(θ) dθ}\n= \\frac{P(X|θ_i) P(θ_i)}{P(X|θ_i) P(θ_i) + ∑_{k≠i} P(X|θ_k) P(θₖ) dθ} $$\n分子是具体的一个 θ，分母是把所有可能的 θ 全部取一遍并相加，如果 θ 是连续的，就是积分的过程。\n置信度是 [0,1] 之间的数，但不好求，似然值用梯度下降求，配分函数用蒙特卡洛方法求\n用贝叶斯理解梯度下降 找最优参数的过程拆解成一个序列： P(θ|X) = P(s₁ s₂ s₃ \u0026hellip; sₙ|X)； 给定数据集 X， 先得到 s₁，下降一次得到 s₂，再得到 s₃，下降n次到达 sₙ\n用贝叶斯公式展开：\nP(θ|X) = P(s₁ s₂ s₃ \u0026hellip; sₙ|X) = P(X|s₁ s₂ s₃ \u0026hellip; sₙ) P(s₁ s₂ s₃ \u0026hellip; sₙ) / P(X) = P(s₁ s₂ s₃ \u0026hellip; sₙ, X)/P(X) = P(sₙ | s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X) P(s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X)/P(X)\n= P(sₙ | s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X) P(sₙ₋₁| s₁ s₂ s₃ \u0026hellip; sₙ₋₂, X) \u0026hellip;. P(s₃|s₁,s₂,X) P(s₂|s₁,X) P(s₁|X) P(X)/P(X)\n= P(sₙ | s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X) P(sₙ₋₁| s₁ s₂ s₃ \u0026hellip; sₙ₋₂, X) \u0026hellip;. P(s₃|s₁,s₂,X) P(s₂|s₁,X) P(s₁|X)\n从右向左求。\n假设中间的每一步并不依赖前面的所有结果，只依赖于它前面一步的结果:\nP(θ|X) = P(sₙ | sₙ₋₁, X) P(sₙ₋₁| sₙ₋₂, X) \u0026hellip;. P(s₃|s₂,X) P(s₂|s₁,X) P(s₁|X)\n要求 P(θ|X) 的最大值，计算量还是太大，不是考虑整体乘积达到最大，只考虑局部每一小节，下一步等于上一步的最大值:\n$$s_{i+1} = arg max_{s_{i+1}} P(s_{i+1} | s_i, X)$$从第一项到最后一项依次都取最大，最后的结果不一定是整体最大值，但也能得到一个近似的结果。上式就是最大后验估计， 损失函数是：P(sᵢ₊₁ | sᵢ, X)， 则梯度下降为： sᵢ₊₁ = sᵢ + η ⋅ ∇ P(sᵢ₊₁ | sᵢ, X)\n","date":"2022-12-16T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/13_l1l2%E6%AD%A3%E5%88%99%E5%8C%963-%E8%B4%9D%E5%8F%B6%E6%96%AF/","title":"watch: DL - 王木头 13 | L1L2 Reg (3), Bayesian Probability"},{"content":"1. 算法收敛性证明 Source video: P1\nEM \u0026ldquo;期望最大\u0026rdquo; 用于解决具有隐变量的混合模型的参数估计，解决极大似然问题。 对于简单模型的参数估计问题，解析解可以直接求导得到，得不到解析解的用梯度下降或EM求数值解。\nX: random variable, observed data X={x₁,x₂,..xₙ} 各样本独立同分布iid θ: all parameters log P(X|θ): log-likelihood, 对数似然 用极大似然估计： θ_MLE = arg max_θ P(X|θ) ➔ arg max log P(X|θ)\n对于含有隐变量的混合模型，直接求解析解非常困难，比如在 GMM （高斯混合模型）中，就很难写\nEM迭代公式 Z: Latent variable 隐变量也是随机变量，不同值有不同的出现概率 (X,Z): Complete data 完整数据 θ⁽ᵗ⁺¹⁾: t+1 时刻的参数 P(Z|X,θ): Z出现的后验概率 P(X,Z): X 和 Z 同时发生的联合概率。 log P(X,Z|θ): 对数联合概率，对数完全数据 θ⁽ᵗ⁺¹⁾ = arg max_θ ∫_Z log P(X,Z | θ) ⋅ P(Z|X,θ⁽ᵗ⁾) dZ，（很像最大似然估计 log P(X|θ)，只是这里的数据除了X还有隐变量Z，不用梯度下降而用迭代更新参数）\n可以看作是按照 Z 的后验分布 (Z|X,θ⁽ᵗ⁾) 求对数联合概率的期望 （期望的定义-wiki）：\n第一步 Expectation：求对数完全数据 log P(X,Z|θ) 以后验 P(Z|X,θ⁽ᵗ⁾) 为概率密度函数的期望:\n$E_{Z|X,θ⁽ᵗ⁾}[log P(X,Z|θ)]$\n第二步 Maximization：找到期望最大时对应的参数作为下一时刻的参数: $\\rm θ⁽ᵗ⁺¹⁾ = arg\\ max_θ E\\_{P(Z|X,θ⁽ᵗ⁾)} [log P(X,Z|θ)]$\n收敛性证明 （非严格）只证明一步，从 θ⁽ᵗ⁾ ➔ θ⁽ᵗ⁺¹⁾，似然会增大：log P(X|θ⁽ᵗ⁾) ≤ log P(X|θ⁽ᵗ⁺¹⁾)，事件 X 在新一时刻的参数 θ⁽ᵗ⁺¹⁾ 所代表的概率模型下，发生的可能性变大了，不断迭代最后可取得最大 log-likelihood 对应的参数。\n完全数据的似然：P(X,Z|θ) = P(Z|X,θ) P(X|θ)，θ是起始值可以任意取\n取对数：log P(X|θ) = log P(X,Z|θ) - log P(Z|X,θ)\n两边关于 Z 的后验分布求积分（求对数似然的期望）：\n左边：\n∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(X|θ) dZ = log P(X|θ) ∫z P(Z|X,θ⁽ᵗ⁾) dZ （似然与Z无关提到积分外面）\n= log P(X|θ) （对Z的概率积分为1）\n右边：\n∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(X,Z|θ) dz - ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(Z|X,θ) dz = Q(θ, θ⁽ᵗ⁾) - H(θ,θ⁽ᵗ⁾)，分别分析两项\nQ 存在于EM迭代公式中，根据EM 的定义：θ⁽ᵗ⁺¹⁾ 对应最大的Q，所以： Q(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) ≥ Q(θ, θ⁽ᵗ⁾) 是成立的。 上式的θ是初始值，若令θ = θ⁽ᵗ⁾，则 Q(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) ≥ Q(θ⁽ᵗ⁾, θ⁽ᵗ⁾) 得证\n因为 H 前面有个负号，所以要证 H(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) ≤ H(θ⁽ᵗ⁾, θ⁽ᵗ⁾)\n后 - 前时刻的 H：\nH(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) - H(θ⁽ᵗ⁾, θ⁽ᵗ⁾) = ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(Z|X,θ⁽ᵗ⁺¹⁾) dz - ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(Z|X,θ⁽ᵗ⁾) dz = ∫z P(Z|X,θ⁽ᵗ⁾)⋅log (P(Z|X,θ⁽ᵗ⁺¹⁾ / P(Z|X,θ⁽ᵗ⁾)) dz （合并） = - KL( P(Z|X,θ⁽ᵗ⁾) || P(Z|X,θ⁽ᵗ⁺¹⁾)) （相对熵≥0） ≤ 0\n(另一种证法) 用Jensen不等式，而不使用相对熵。\n因为log是 concave （凹）函数：在曲线上任意取两个点，连成的直线小于log函数。concave函数的性质：a,b两点之间的一点c（两端点的线性组合）对应到直线函数上的值一定小于对应到log函数上的值\n上面的合并为 1个log 之后的式子，可以看作是先求Z的后验的对数，再求\u0026quot;对数值\u0026quot;的期望，一定小于先求（自变量\u0026quot;Z的后验\u0026quot;的）期望再求对数：（E[log x] ≤ log E[x]）\n∫z P(Z|X,θ⁽ᵗ⁾)⋅log (P(Z|X,θ⁽ᵗ⁺¹⁾ / P(Z|X,θ⁽ᵗ⁾)) dz ≤ log ∫z P(Z|X,θ⁽ᵗ⁾)⋅(P(Z|X,θ⁽ᵗ⁺¹⁾ / P(Z|X,θ⁽ᵗ⁾))) = log ∫z (P(Z|X,θ⁽ᵗ⁺¹⁾) dz = log 1 = 0\n2. 迭代公式的导出 Source video: P2\nEM 迭代公式来自于将对数似然 logP(X|θ) 分解为 ELBO + KL散度。\n引入变量Z，则 P(X) 就变成了联合概率：P(X,Z|θ) = P(Z|X,θ)⋅P(X|θ)， 所以对数似然就变为：\nlogP(X|θ) = log P(X,Z|θ) - log P(Z|X,θ)\n引入 Z 的概率分布 q(Z): logP(X|θ) = log P(X,Z|θ) - log q(Z) - log P(Z|X,θ) + log q(Z) = log (P(X,Z|θ) / q(Z)) - log (P(Z|X,θ) / q(Z))，q(Z)≠0\n技巧：对等式两边，按照分布 q(Z) 求似然的期望（积分）：\n左边：\n∫z q(Z)⋅log P(X|θ) dZ = log P(X|θ)⋅∫z q(Z) dZ = log P(X|θ)，q(Z)是概率密度函数积分为1，因此左边求完期望无变化，所以求似然 logP(X|θ) 就变成了求q(Z)。\n右边：\n∫z q(Z)⋅log (P(X,Z|θ)/q(Z)) dz - ∫z q(Z)⋅log (P(Z|X,θ)/q(Z)) dz = ELBO + KL(q(Z) || P(Z|X,θ)) 其中第1项：Evidence Lower Bound (ELBO,下界)，第2项是Z的概率密度函数q(Z)与Z的后验概率P(Z|X,θ)的相对熵。\n因为 KL散度恒≥0，所以样本似然 log P(X|θ) ≥ ELBO。只有当 Z 的概率分布 q(Z) 与 Z 的后验分布 P(Z|X,θ) 相等时，KL散度等于0，似然=ELBO。\nELBO 是 log (P(Z,X|θ)/q(Z)) 按照 q(Z) 求期望（加权和;求积分）。当对数似然=ELBO，即达到最大时，q(Z)=P(Z|X,θ⁽ᵗ⁾)，也就是先用上一时刻的θ⁽ᵗ⁾ 求出q(Z)，然后ELBO里就只有log里的θ是变量，滑动调整θ使ELBO最大，取对应的θ作为θ⁽ᵗ⁺¹⁾。\nEM 想让 ELBO 达到最大，先（通过KL散度=0）找到样本对数似然 log P(X|θ) 的最大值对应的参数θ，作为新的θ，然后再算它对应的期望并得到ELBO， 取到最大值时的形式，即 ELBO， 然后取ELBO最大时对应的参数θ； 不断提高下界从而让对数似然 log P(X|θ) 也逐渐变大，最终ELBO等于logP(X|θ)，KL散度=0，Z的概率分布与它的后验分布相等， 最优θ^ 是 ELBO 取最大时的θ\nθ^ = arg max_θ ELBO = arg max_θ ∫z q(Z) ⋅ log (P(X,Z|θ) / q(Z)) dz ，代换q(Z)\n= arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ log (P(X,Z|θ) / P(Z|X,θ⁽ᵗ⁾)) dZ\n这个式子与 EM 的迭代公式相比，log 里多了一个分母：P(Z|X,θ⁽ᵗ⁾)，展开log：\nθ^ = arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ [log P(X,Z|θ) - log P(Z|X,θ⁽ᵗ⁾)] dZ = arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ log P(X,Z|θ) dZ - P(Z|X,θ⁽ᵗ⁾) ⋅ log P(Z|X,θ⁽ᵗ⁾)] dZ\n其中第2项与θ无关，因为 θ⁽ᵗ⁾ 是上一时刻的参数，是个常数，不是变量，而 log 中的 θ 是变量，会变到ELBO 取最大时对应的参数。所以就得到了迭代公式：\nθ^ = arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ log P(X,Z|θ) dz\n3. 公式导出之ELBO+Jensen\u0026rsquo;s Inequality Source video: P3\nEM 迭代公式也可来自于将对数似然 logP(X|θ)分解为 ELBO + Jensen\u0026rsquo;s Inequality\nJensen\u0026rsquo;s Inequality 结论：对于一个凹concave 函数 f(x)，在定义域x上取两点：a,b 连线小于a,b之间的函数值。 任意在 a,b 之间取一点 c = t⋅a+(1-t)b, where t∈[0,1]，f(c)=f(t⋅a+(1-t)b) ≥ t⋅f(a) + (1-t)f(b)。\n比如当 t=½ 时，f(a/2+b/2) ≥ f(a)/2 + f(b)/2，两边都是期望（平均数,加权和），简记为:先求期望再求函数值 大于等于 先求函数值再求期望，f(E) ≥ E[f]。当 f(x) 是常函数时，等号成立。\nlog P(X|θ) = log ∫zP(X,Z|θ) dz，在似然中引入隐变量 Z，然后求X的边缘概率，即对Z求积分（“把最终结果中不需要的事件合并成其事件的全概率而消失” marginal probability-wiki）\n= log ∫z ( P(X,Z|θ) / q(Z)) * q(Z) dz，引入 Z 的分布 q(Z) = log E_q(z) [P(X,Z|θ)/q(Z)]，把积分看作求期望 ≥ E_q(z) [ log(P(X,Z|θ)/q(Z)) ]，Jensen不等式等号在 P(X,Z|θ) / q(Z)=C 成立, log C 是常函数\n这个期望 E_q(z) [ log(P(X,Z|θ)/q(Z)) ] 就是对数似然的下界，就是ELBO。\nq(Z) = P(X,Z|θ) / C ∫z q(Z) dZ = 1 = ∫z P(X,Z|θ) / C dZ = 1/C ∫z P(X,Z|θ) dZ = 1/C P(X|θ) （求边缘概率）\nC = P(X|θ) 把 C 代换：q(Z) = P(X,Z|θ) / P(X|θ) = P(Z|X,θ)\n所以当 Jensen 不等式取等号时，Z的分布 q(Z) 就是 Z 的后验分布P(Z|X,θ)。\n所以 EM 第一步先按照上一时刻的参数 θ⁽ᵗ⁾ 和数据 X 求出 Z 的后验分布 P(Z|X,θ⁽ᵗ⁾)， 再将对数似然 log(P(X,Z|θ)/P(Z|X,θ⁽ᵗ⁾)) 按照 Z 的后验分布求期望，得到对数似然的下界ELBO， 这个下界是关于 θ 的函数: ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(X,Z|θ) dZ，所以可找到这个下界最大时对应的θ，作为θ⁽ᵗ⁺¹⁾。不断迭代提高下界（期望），从而提高对数似然\n4. 再回首 Source video: P4\nEM 是解决优化问题的迭代算法（和梯度下降是一个level），而HMM，GMM 是模型\n从之前狭义的（理想的）EM 推广到广义的（一般的）EM 狭义的EM 是广义EM 的一个特例 EM 的变种 EM 主要用于概率生成模型，数据（随机变量）包括观测数据 X 及其对应的隐变量 Z，所以(X,Z) 叫做完全数据complete data，θ 是概率模型的参数。Z 是建模时引入的，Z生成了X，我们只能观测到X。 比如GMM 中，假设 z 是一个离散的分布，比如 K 个类别 z=1,2,\u0026hellip;,K，每个类别都有一定的概率：\n隐变量 z 1 2 \u0026hellip; K 概率密度 p₁ p₂ \u0026hellip; pₖ 然后在 z 给定的情况下，x 满足高斯分布：P(x|z) ~ Gaussian，因此对完整数据 P(X,Z) 建模\n还有 HMM 也可以从生成的角度来解释。比如 N 个隐变量: z₁, z₂, \u0026hellip;, zₙ 是马尔科夫链的结构:\n$$ s1 ➔ s2 ➔ ... ➔ sₙ \\\\\\ ↧ \\quad\\ ↧ \\quad ... \\quad ↧ \\\\\\ x1 \\quad x2 \\quad ... \\quad xₙ $$可以把 z₁, z₂, \u0026hellip;, zₙ 看成是统一的变量 Z，把 x₁, x₂, \u0026hellip;, xₙ 看成是一个X\n观测到了 X，假设它的概率模型的参数是θ，就可以用 EM 来估计参数 θ。\n使用MLE 来估计参数：θ^ = arg max P(X|θ) = arg max ∏ᵢ₌₀ᶰP(xᵢ|θ) ➔ arg max log P(X|θ)\n不能直接求解这个最大化问题的原因是，不知道P(X)，因为样本X是非常复杂的，所以“会引入自己的归纳偏置，假定它是服从某个模型的。”\n生成模型就是假设每个样本 x⁽ⁱ⁾ 都有一个隐变量 z⁽ⁱ⁾，x⁽ⁱ⁾ 是由 z⁽ⁱ⁾ 生成的，所以P(X) 就变成了联合分布 P(X,Z)（即把X分解处理），然后把 Z 积分掉就可以了: P(X) = ∫z P(X,Z) dZ\nnote-苏剑林-VAE\n5. 广义EM Source video: P5\nEM 用于解决参数估计问题，优化函数使用 MLE ，找到让对数似然 log P(X|θ) 达到最大的参数 θ。 样本X 的分布 P(X) 未知，所以假设每个 x 是由隐变量 z 生成的。比如 GMM 假设 z 服从离散的概率分布，HMM 假设 z 满足马尔科夫链。然后 P(X) 就等于联合概率分布P(X,Z) 比上Z的后验分布P(Z|X)，也就是未知X分布经过生成模型的假设，把问题具体化了。\n因为 P(X|θ) = P(Z,X|θ) / P(Z|X,θ)，所以（复杂的未知的）对数似然目标函数可分解为: 下界ELBO+Z的分布q(Z)与Z的后验分布P(Z|X,θ)的KL散度: log P(X|θ) = ELBO + KL(q||P)\nELBO 是一个期望，它和q(Z) 和 θ 有关，所以将其记为 L(q,θ); KL\u0026gt;=0, 当q=P时，KL=0，所以：log P(X|θ) ≥ L(q,θ)，而且最优参数 θ^ 是在 q^= P(Z|X,θ) 时取到。\n但是 q^ 并不一定能取到 P(Z|X,θ)，因为这个后验可能是 intractable，是算不出来的。这由生成模型的复杂度决定， 如果生成模型比较简单，比如GMM的 z 和 HMM 的 z，他们是离散的，结构化的，是tractable，是可以（用EM）计算出来的\n但是对于绝大多数的 z 是无法求出他的后验 P(z|x)。比如 VAE 的 z 是高维的，无法把它从 P(x,z) 中积掉，就得不到 P(x)，也就无法（用贝叶斯公式）得到后验 P(z|x)，所以最优的 q^(Z) 取不到，所以就需要变分（近似）推断：重参数化技巧+神经网络梯度下降用 q_ϕ(z|x) 逼近 P_θ(z|x)。\n数学表达： 当 θ 固定 的时候，对数似然 log P(X|θ) 就是固定的，然后当 q(Z) 越接近 Z 的后验分布 P(Z|X,θ)，KL散度就越小，同时 ELBO 就越大。所以求最优的分布 q(Z) 就变成一个优化问题： q^(Z) = arg min_q KL(q(Z) || P(Z|X,θ)) = arg max_q L(q,θ)\n当 q^固定 的时候，再做极大似然找 θ，也就是做“狭义”的EM，最优的 θ^= arg maxᶱ L(q^, θ)\n广义EM（两个最大化问题）:\nE-step: q⁽ᵗ⁺¹⁾ = arg max_q L(q,θ⁽ᵗ⁾)，固定θ求最优的q^ M-step: θ⁽ᵗ⁺¹⁾ = arg maxᶱ L(q⁽ᵗ⁺¹⁾, θ)，固定q^求最优的θ 对 ELBO 做变形（展开log）：\nL(q,θ) = E_q(z) [log (P(Z,X|θ)/q(Z))] = E_q(z) [ log P(Z,X|θ) - log q(Z) ]\n= E_q(z) [log P(Z,X|θ)] - E_q(z) [log q(Z)] = E_q(z) [log P(Z,X|θ)] + H[q(Z)]\n其中第2项 H[q(Z)] 是分布 q(Z) 的熵: ∫ q(Z)⋅log (1/q(z)) dz ， 所以 ELBO = 完全数据似然按照 q(Z) 求期望+ q(Z) 的熵\n之前的EM 是广义EM 的一个特殊情况。对于 E-step, 狭义EM 默认 q 直接就取到了后验 P(Z|X,θ⁽ᵗ⁾)，因为假定了后验能够求出。对于 M-step, 狭义EM 认为 q^ 已经找到了，那么广义EM 的M-step 中的熵就是确定值，要优化的只有似然。\n6. EM 的变种 Source video: P6\n（标准的，一般指的）广义的EM：对似然下界(ELBO联合概率按照z的后验求期望) L(q,θ) 求两次最大化，先固定θ⁽ᵗ⁾求q⁽ᵗ⁺¹⁾，然后固定q⁽ᵗ⁺¹⁾ 求 θ⁽ᵗ⁺¹⁾\n因为两步都是求 Maximum，所以 EM 也称 MM (Maximation Maximation)\n对于两个参数，先固定一个求另一个，再反过来，这种算法是坐标上升法，比如SMO。如果参数是多维的，固定其中某一个/两个，然后去求其他的。求参数的顺序没关系。\n坐标上升法 与 梯度上升法并列\n损失函数的等高线如下图，梯度上升法的参数路径是沿着梯度的，而坐标上升法类似曼哈顿距离，\n如果 E-step 中最优的 q^(Z)，也就是Z 的后验P(Z|X,θ) 无法求得，就可以用变分推断求近似最优，比如基于平均场理论的变分法近似后验分布，再做 M-step，称这个组合为VBEM 变分贝叶斯EM。 如果用蒙特卡罗采样去求近似后验分布，叫作MCEM，蒙特卡罗EM。\nVI（变分推断） 和 VB （变分贝叶斯）指的是同一个东西\n","date":"2022-12-16T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/10-em%E7%AE%97%E6%B3%95/","title":"watch: ML - 白板 10 | EM Algorithm"},{"content":"2连咸饼 ➔ P(袋子) 从同一袋子里连续取出了2个咸饼干，问最有可能是以下5个袋子中的哪个？\n饼干比例: 100%甜 75%甜25%咸 50%甜50%咸 25%甜75%咸 100%咸 MLE 0 0.0625 0.25 0.5625 1 设从每个袋子中取出咸饼干的概率是p，则MLE 只需对每个袋子求p²，则第5个袋子的似然最大。而MAP还要考虑各袋子出现的概率g，所以MAP函数为 p²×g，可得第4个袋子的后验最大\n先验g (袋子出现的概率) 0.1 0.2 0.4 0.2 0.1 袋中咸饼干比例 θ 0 0.25 0.5 0.75 1 事件 X:二连咸(MLE) 0 0.0625 0.25 0.5625 1 MAP 0 0.0125 0.1 0.1125 0.1 后验 0 3.85% 30.8% 34.6% 30.8% 因为这里的样本空间由两个随机变量组成：事件X和袋子概率g，所以P(X)就是在所有θ可能的情况下X发生的概率和(0.325)。因为P(X)是个常数，所以做 MAP 只看分子就行。\n男生 ➔ P(打lol) 一个男生，求他打lol的概率\n先验g 60% 40% 男女比例 θ 80%男20%女 20%男80%女 事件X:1男 (MLE) 0.8 0.2 事件X:1男 (MAP) 0.48 0.08 后验 85.7% 14.3% 9正1反 ➔ P(硬币) 抛10次硬币9正1反，求这枚硬币朝上的概率? 【机器学习我到底在学什么】哲学角度聊聊贝叶斯派和频率派，数学角度看看极大似然估计和最大后验估计\n频率派会说是0.9，因为这样对应\u0026quot;抛出9次朝上\u0026quot;的概率最大； 贝叶斯派依据贝叶斯公式，带入先验概率和实验结果，认为向上概率落在 0.5~0.9 之间的概率最大: 比如参数θ\u0026rsquo;=0.5 的先验概率是 P(θ\u0026rsquo;)=0.8, 把θ\u0026rsquo;=0.5 带入二项分布可算出事件9正1反发生的概率：P(X|θ\u0026rsquo;)=C₉¹⁰⋅(0.5)⁹⋅(0.5)¹=0.00976. 所以贝叶斯公式的分子=0.8×0.00976=0.00781。 而分母 P(X) 应为 θ 从0取到1时，各种0对应的事件X:9正1反发生的似然之和，但从0到1 是不可数的，无法得到解析解，可以做数值模拟。下面的例子只假设样本空间只有 3 中参数θ 的取值：0.5, 0.9, 0.4。\n先验 g 80% 1% 19% 参数 θ 50%上50%下 90%上10%下 40%上60%下 事件 X C₉¹⁰⋅(0.5)⁹⋅(0.5)¹ C₉¹⁰(0.9)⁹(0.1)¹ C₉¹⁰(0.4)⁹(0.9)¹ MLE 0.00976 0.387 0.00236 MAP 0.00781 0.00387 0.000448 后验 64.4% 31.9% 3.7% 上表中 MLE = P(X|θ), MAP = P(X|θ)P(θ), 而“事件X发生，参数取到θ的概率”: P(θ|X) = MAP/P(X)，其中 P(X) 是事件X在样本空间中发生的概率，应为当参数取各种 θ 时，事件X 发生的条件概率 之和，也就是各种θ 对应的 MAP 求和。\n感觉不太恰当 甲地下雨了，求乙地下雨概率 条件概率 例题\n特征 ➔ P(嫁) 数据挖掘实验-Naive Bayes\n","date":"2022-12-15T23:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/map_examples/","title":"MAP examples"},{"content":"1 Peak signal-to-noise Ratio 两幅大小为m×n的单色图，一幅是原图I，另一幅是近似图像K。 两者之间的平均平方误差Mean Squared Error是：\n$$ MSE = \\frac{1}{mn} \\sum_{i=1}^m \\sum_{j=1}^n\\ [I(i,j) - K(i,j)]^2 $$则峰值信噪比定义为： PSNR = 10 log₁₀ (MAXᵢ² / MSE)\nMAXᵢ 是原图中可能的最大像素值，当一个像素用8 bits表示时，MAXᵢ=255\n为什么是这样子？\n常用于衡量有损压缩编解码器的重建质量。 wikipedia\nMSE越小，PSNR越大。当两图没有误差时，PSNR趋于无穷\n2 Structural SIMilarity Index 用于预测数字图像的可感知质量 perceived quality SSIM ² is used to measure the distortion degree of an image, or measure the similarity between two images from 3 aspects: Luminance, Contrast, and Structure.\nRatio of luminance l(X,Y) = 2μₓμᵧ+C₁/(μₓ²+μᵧ²+C₁), where μₓ is the mean of the intensity of all pixels in image X: μₓ = 1/N ∑ᵢ₌₁ᴺ Xᵢ. And C₁ prevents the divisor from being 0.\nBased on AM-GM inequality: (√x - √y)²≥0, if and only if μₓ=μᵧ, l(X,Y)=1.\nRatio of contrast c(X,Y) = 2σₓσᵧ+C₂/(σₓ²+σᵧ²+C₂), where σₓ is the (non-bias) standard deviation of the intensity of all pixels in the image X: σₓ = (1/(N-1) ∑ᵢ₌₁ᴺ (Xᵢ-μₓ)²)¹ᐟ². If and only if σ₁=σ₂, c(X,Y)=0.\nStructure is reflected by correlation coefficient: s(X,Y) = σₓᵧ+C₃/(σₓσᵧ+C₃), where σₓᵧ is the covaraiance of the intensity of 2 images: σₓᵧ= 1/(N-1) ∑ᵢ₌₁ᴺ (Xᵢ-μₓ)(Yᵢ-μᵧ).\nThe intent of the paper may be to measure the information aside from Luminance and Contrast ⁵, so each pixel is reduced by the mean and devided by stddev: (Xᵢ-μₓ)/σₓ and (Yᵢ-μᵧ)/σᵧ, i.e., normalization, and then compute the distance between corresponding pixels in 2 images by inner product.\nThus, S(X,Y) is 1/(N-1) ∑ᵢ₌₁ᴺ [(Xᵢ-μₓ)/σₓ ⋅ (Yᵢ-μᵧ)/σᵧ ] = σₓᵧ/σₓσᵧ\nThe final expression is the product of the above 3 features with specified power (weights) α β γ:\nS(X,Y) = l(X,Y)ᵅ ⋅ c(X,Y)ᵝ ⋅ s(X,Y)ᵞ ∈ [-1,1]\nwhere l(X,Y), c(X,Y), s(X,Y) ∈ [-1,1], and since brightness ≥0, the actural range of l and c are (0,1]. If and only if the images X and Y are the same, the 3 items are equal to 1 at the same time.\nLet α, β, γ=1, the SSIM(X,Y) = $\\frac{ 2(μₓμᵧ+C₁) (2σₓᵧ + C₂) }{(μₓ²+μᵧ²+C₁) (σₓ²+σᵧ²+C₂)}$, where C₁ = (K₁L)², C₂ = (K₂L)², C₃=C₂/2, and L is the maximum for a pixel (L=2^b). Based on the rule of thumb, K₁ = 0.01, K₂=0.03.\nIn practice, SSIM is not performed on the entire image, but calculating the mean and stddev in a local sliding window (kernel, filter of 11x11 with stddev=1.5, sum=1), which represents a circular-symmetric Gaussian Weighting Function.\nHence, the local mean and std-dev are confined within the kernel:\nμₓ = ∑ᵢ wᵢ Xᵢ σₓ = ∑ᵢ wᵢ (Xᵢ-μₓ)²)¹ᐟ² σₓᵧ= ∑ᵢ wᵢ (Xᵢ-μₓ)(Yᵢ-μᵧ) where wᵢ is the parameters of the Gaussian kernel smoothing the image, such that fine-details are smeared and compare mainly the general features.\nAt the end, the average of all local SSIM is M-SSIM(X,Y) = 1/M . ∑ⱼ₌₁ᴹ SSIM(Xᵢ, Yⱼ)\nUsed as loss Since the SSIM measures the similarity between 2 images, it can be used in the supervised training. Thus, SSIM dissimilarity, SSIMD = (1-SSIM)/2 ∈ (0,] is a kind of loss function.\n有损压缩Lossy compression两种基本机制： wikipedia\n有损变换编解码：对图像/声音采样， 预测编解码： 3 LPIPS The motivation of Learned Perceptual Image Patch Similarity (LPIPS) ³ is that: the conclusion for the similarity between 2 images from deep neural networks are aligned with humans perception. But the structure-based metrics usually give opposite judgement on the too-smoothed images ⁵. (2023-02-18)\nLPIPS compares the intermediate convolutional feature vectors at different levels.\nlpips.LPIPS(net=\u0026quot;alex\u0026quot;) 把两幅图输入神经网络 (VGG, Alexnet) 进行多层级特征间的对比。 每层输出的 feature mapᴴˣᵂˣᶜ 激活后归一化，再相减，将各层差异按像素加权求和，并(除以像素个数)做spatial 平均，再把各层差异加起来 ⁴\nRef Image quality assessment: from error visibility to structural similarity The Unreasonable Effectiveness of Deep Features as a Perceptual Metric LPIPS图像相似性度量标准_Alocus_的博客-CSDN博客_lpips度量 NeRF常用评价指标都是什么意思？PSNR、SSIM、LPIPS详解 - 意の茗 - bilibili ","date":"2022-12-08T12:59:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/metrics/","title":"memo: Vis | Visual Metrics"},{"content":"Code | Arxiv (2103) | ProjPage\n(2023-08-16) Re-read\nExperiments Train model only on (MVSNet) DTU dataset, where the objects are partitioned the same as PixelNeRF (imgsize: 300x400).\nDatasets (Download):\nStage Data Contents Resolution N_views Size Train DTU 88 scenes 512x640 19G Test DTU 16 scenes 3/20 Test LLFF Test Blender Feature map: 32 channels Cost volume: 128 planes MLP: 6 layers Coarse-to-fine: One field and fine-tune Ray pts: 128 Device: 2080Ti Batch size: 1024 rays Optimizer: Adam, lr=5e-4 Old Notes Old notes on 2022-12-06 Abstract\n3 nearby input views plane-swept cost volumes geometry-aware scene reasoning generalize across scenes 1 Introduction\nTopic: Novel view synthesis\nRecent progress: neural rendering\nFormer solutino and drawbacks:\nNeRF and its following works can produce photo-realistic novel view synthesis results. But they need a long-time per-scene optimization Own Solution:\nGoal: use \u0026hellip; to \u0026hellip; 1 sentences introduce the name and functionality, properties. Analysis: generalizability -\u0026gt; avoid tedious per-scene optimization and regress directly novel viewpoints quantitative outcomes Tech stack\nMVSNet -\u0026gt; generalizable net of 3D reconstruction Cost volume is built by warping 2D img features of src views onto sweeping planes Regress geometry and appearance from a cost volume (per-voxel neural features) 3D cnn aggregates the cost volumes to a neural scene encoding volume (2023-08-16)\nPlay Environment inplace-abn needs to be installed from source, as issue#36. But I cannot install it:\n1 2 3 RuntimeError: The detected CUDA version (10.2) mismatches the version that was used to compile PyTorch (11.3). Please make sure to use the same CUDA versions. My cuda version is 11.3 as shown by:\n1 2 import torch print(torch.version.cuda) Then I specified the version the same as the pl-tutorial of AIkui:\n1 2 3 4 5 6 - pip - pip: - torch==1.11.0 - torchvision==0.12.0 - pytorch_lightning==1.6.0 - inplace_abn With that, inplace_abn has installed.\nBut the API has change since v1.5 (as issue) and resulted in: Exception has occurred: TypeError __init__() got an unexpected keyword argument 'distributed_backend'\nTry this setting:\n1 2 3 4 - torch==1.10.1 - torchvision==0.11.2 - pytorch_lightning==1.3.5 - inplace_abn Cannot import inplace_abn:\n1 2 Exception has occurred: ImportError /home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/inplace_abn/_backend.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c1015SmallVectorBaseIjE8grow_podEPvmm I checked my cuda version, which is 10.2:\n1 2 3 4 5 (mvsnerf) zichen@lambda-server:~/Downloads/mvsnerf-comments$ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Wed_Oct_23_19:24:38_PDT_2019 Cuda compilation tools, release 10.2, V10.2.89 According to Previous PyTorch Versions | PyTorch, I install the versions compiled with cuda10.2. Then print(torch.version.cuda) returned 10.2 instead.\n1 2 3 4 5 6 7 8 - pip - pip: - torch==1.10.1+cu102 - torchvision==0.11.2+cu102 - torchaudio==0.10.1 - -f https://download.pytorch.org/whl/cu102/torch_stable.html - pytorch_lightning==1.3.5 - inplace_abn Still cannot import inplace_abn with the same ImportError as above:\nThen Install from source, and inplace_abn can be installed:\nAnother Error:\n1 2 3 4 Exception has occurred: ImportError cannot import name \u0026#39;get_num_classes\u0026#39; from \u0026#39;torchmetrics.utilities.data\u0026#39; (/home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/torchmetrics/utilities/data.py) File \u0026#34;/home/zichen/Downloads/mvsnerf-comments/train_mvs_nerf_pl.py\u0026#34;, line 17, in \u0026lt;module\u0026gt; from pytorch_lightning.callbacks import ModelCheckpoint Change to specific versions, as summary:\n1 2 pip install torchmetrics==0.5.0 pip install setuptools==59.5.0 Numpy version incompatible issue#77\n1 ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part. File \u0026ldquo;/home/zichen/Downloads/mvsnerf-comments/data/dtu.py\u0026rdquo;, line 98, in build_proj_mats self.proj_mats, self.intrinsics = np.stack(proj_mats), np.stack(intrinsics)\n","date":"2022-12-06T14:19:00Z","image":"https://ar5iv.labs.arxiv.org/html/2103.15595/assets/x2.png","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-mvsnerf/","title":"read: NVS - NeRF | MVS-NeRF"},{"content":"(2024-07-01)\n因为 Homography 处理的是 平面，得到平面上各点的深度，所以 MVS 方法得到的结果是深度图。 (2024-05-16)\nHomography 是把一张照片变到另一个视角下\nex: 桌子上放着一张 照片，从不同视角去看，除了正视，从其他视角看，看到照片里的内容就变形了。\n注意：必须是照片，不要关注照片里的 3D 物体。 Homography 就是描述从不同角度看 照片，看到的照片的样子。 不论你从哪看，它就只是一个平面，所以斜着看照片，上面的内容就会拉伸收缩：比如上面的字放大缩小，一个正方形变成梯形。 变换视角时，不会出现物体间的遮挡，因为只是在分析一个 平面。 Homography warps two images to a common plane, or maps a pixel from a camera plane to another camera plane.\nA pixel can be mapped to another camera because the 2 pixels are the projections of a common 3D point.\nHomography warps the images captured from different angles, observing a common plane at a certain depth, to a unified viewpoint for stitching or other applications.\nA homography corresponds to only one depth. If the image doesn\u0026rsquo;t only capture a plane at a certain depth, the part with a deviated depth will be distorted in the warped image. Thus, solving homographies requires specifying depth.\nHomography relation is built based on the pose transformation between two cameras, and a depth embodied by a co-observed point or a co-observed plane. Or it can be sovled from 4 pairs of matched pixels.\nSpecifically, two cameras are connected via the 2 camera-space coordinates conversion for a depth-known co-observed 3D point.\nThe conversion can be represented by two camera\u0026rsquo;s poses in the world space, or the direct transformation from one camera to the other.\nThe depth of the co-observed 3D point can be given by specifying either the z-coordinate directly or a plane (𝐧,d) indirectly.\n(2023-12-19) Two scenarios for homography: stitching images based on a co-observed plane, and transferring viewpoint for a plane surface. They both rely on the observed plane.\nTwo Cameras The homography matrix is solved based on matching points in a pair of images.\n𝐲 ₁ c a W m o 𝐏 1 r ₁ l d s 𝐇 p a 𝐐 c e 𝐏 c ₂ a m 2 𝐲 ₂ P 𝐱 2 r o t p j o l e a c n t e s $$ 𝐲₁ = 𝐏₁ ⋅ 𝐐 \\\\\\ 𝐲₂ = 𝐏₂ ⋅ 𝐐 \\\\\\ 𝐲₂ = 𝐏₂ ⋅ 𝐏₁⁻¹ * 𝐲₁ $$ 𝐏 is the projecting operation that converts a world coordinates of a point 𝐱 to pixel coordinates 𝐲.\n$𝐏 = 𝐊[𝐑|𝐭]$ and $𝐏⁻¹ = [𝐑|𝐭]⁻¹𝐊⁻¹$\n𝐏₂ ⋅ 𝐏₁⁻¹ is the homography matrix.\nBesides 𝐊[𝐑|𝐭], the z-coordinate of covisible point 𝐐 (or the \u0026ldquo;distance from camera center to plane\u0026rdquo; d) is necessary.\np l a n e 𝐧 C c a e m n d e t : : : r e a r 𝐐 ( x , y , z ) Since the inverse intrisics $𝐊⁻¹ ⋅(u,v,1)ᵀ$ solely can\u0026rsquo;t determine a unique 3D point as z has been canceled when $(u,v,1)ᵀ=\\frac{1}{z}⋅𝐊⁻¹ ⋅ (x,y,z)ᵀ$, the z must be specified at first. 5\n$$ (x,y,z)ᵀ = 𝐊⁻¹ ⋅z ⋅(u,v,1)ᵀ $$If assuming camera\u0026rsquo;s z-axis is perpendicular to the observed plane, then z is indeed $d$. Thus, the z-retained coordinates of the pixel 𝐲₁ is (ud,vd,d), such that $𝐊⁻¹ ⋅d⋅(u,v,1)ᵀ$ is a unique 3D point in the camera space.\nRecall the NeRF\u0026rsquo;s sampling points (get_rays_np): meshgrid as pixel coordinates → divided by focal (with z=1) to be in camera space → @c2w to be in world space → z vals sampling\nThus $𝐊⁻¹ ⋅(u,v,1)ᵀ$ only determines the viewdir, not a 3D point.\nThen, roll back it to world space and project to the other camera to get the pixel 𝐲₂.\n$$ 𝐲₂ = 𝐊₂[^{𝐑₂ \\quad 𝐭₂}_{0 \\quad 1}] [^{𝐑₁ \\quad 𝐭₁}\\_{0 \\quad 1}]⁻¹𝐊₁⁻¹ ⋅d⋅𝐲₁ $$Overview: Plane1 → camera1 → world space → camera 2 → plane 2\nHowever, if either 𝐐\u0026rsquo;s $z$ (or the distance $d$), or poses of two cameras is unknown, the 3x3 homography matrix can be solved from 4 pairs of matching pixels.\n(2023-12-04)\nBasis Changes Refer to Planar homographies - Department of Computer Science, University of Toronto\nIn a \u0026ldquo;meta space\u0026rdquo; holding two cameras, for example the world space, a ray emitted from the world origin passed through 2 world points 𝐦, 𝐧 locating on 2 planes separately.\nc W o M a o r e m r i t e l g a r d i a n P c l a 𝐚 a m ₁ n e 𝐚 e r 𝐦 ₂ a o f 1 𝐚 ₀ 𝐛 𝐧 ₁ P c l a a m n e 𝐛 e r ₀ a o 𝐛 f 2 ₂ R a y $\\vec{a₀}$ and $\\vec{b₀}$ are the normal vector of two planes.\n$\\vec{a₁}$, $\\vec{a₂}$ are orthogonal, so they form a basis of the plane, same as $\\vec{b₁}$, $\\vec{b₂}$.\nThus, the world point 𝐦 on the plane can be represented by its planar basis as 𝐩 = (p₁,p₂,1) :\n$$ 𝐦 = 𝐚₁ p₁ + 𝐚₂ p₂ + 𝐚₀ = \\begin{pmatrix}𝐚₁ \u0026 𝐚₂ \u0026 𝐚₀\\end{pmatrix} \\begin{pmatrix} p₁ \\\\\\ p₂ \\\\\\ 1 \\end{pmatrix} = 𝐀 \\begin{pmatrix} p₁ \\\\\\ p₂ \\\\\\ 1 \\end{pmatrix} $$ Specifically, 𝐚₀, 𝐚₁, 𝐚₂ make up the camera space.\nSince each column in A is the camera (source) space basis axis seen from the world (original) space, 𝐀 contains c2w, and $𝐀 = [𝐑|𝐭]ₘ⁻¹𝐊ₘ⁻¹$\nSimilarly, the point 𝐧 represented by the basis of its plane is 𝐪 = (q₁,q₂,1):\n$$𝐧 = \\begin{pmatrix}𝐛₁ \u0026 𝐛₂ \u0026 𝐛₀\\end{pmatrix} \\begin{pmatrix} q₁ \\\\\\ q₂ \\\\\\ 1 \\end{pmatrix} = 𝐁 \\begin{pmatrix} q₁ \\\\\\ q₂ \\\\\\ 1 \\end{pmatrix} $$ where 𝐁 can be interpreted as $𝐁= [𝐑|𝐭]ₙ⁻¹𝐊ₙ⁻¹$ Because the two world points are on the common line, and they satisfy the perspective projection centered at the world origin, so the only difference between them is a scaling factor.\n$$𝐦 = α 𝐧$$ α is a scalar that depends on 𝐧. And eventually, on 𝐪. So α is a function of 𝐪, i.e., α(𝐪). Substituting their coordinates 𝐩 and 𝐪:\n$$ \\begin{aligned} 𝐀𝐩 = α(𝐪) ⋅𝐁⋅𝐪 \\\\\\ 𝐩 = α(𝐪) ⋅𝐀⁻¹⋅𝐁⋅𝐪 \\end{aligned} $$ A is invertible because its 3 rows (base vectors) are independent and nonzero as the plane doesn\u0026rsquo;t cross zero. If 𝐦 and 𝐧 are overlapped, i.e., the same point, which is just observed from different angles, the scale factor α = 1.\n(2023-12-05)\nTransfer Camera Source article: Pinhole Camera: Homography - 拾人牙慧 - 痞客邦\nHomography transfers a pixel on cam1 to another camera\u0026rsquo;s film.\nA homography can be solved if the rotation and translation from one camera to the other and the co-observed plane (𝐧,d) is defined under the original camera space.\nIn other words, the camera-1\u0026rsquo;s space is regraded as the world space. Thus, the extrinsics of cam1 is [𝐈|0], and cam2\u0026rsquo;s extrinsics is [𝐑|𝐭].\n𝐐 c 𝐩 a m 𝐑 1 , 𝐭 , ( 𝐧 c , a d 𝐪 m ) 2 𝐐 ' Note: In this setting, the co-observed 3D point 𝐐\u0026rsquo;s coordinates aren\u0026rsquo;t given, so the plane {𝐧,d} is necessary to derive the crucial z-coordinate of 𝐐.\nSupp: A plane in a 3D space can be defined by its normal vector 𝐧 (a,b,c) and a point 𝐐 (x,y,z) located on it as: $𝐧𝐐 = 0$ (12.5: Equations of Lines and Planes in Space).\n$$ 𝐧⋅𝐐 = (a,b,c) (x-x₀, y-y₀, z-z₀) \\\\\\ = ax-ax₀+by-by₀+cz-cz₀ = 0 $$ where (x₀,y₀,z₀) is the \u0026ldquo;origin\u0026rdquo;. If the plane is displaced from the origin along its normal vector by distance $d$, the plane is $𝐧𝐐-d=0$. (Plane Equation - Song Ho)\n$$ 𝐧⋅𝐐 = (a,b,c) (x-da-x₀, y-db-y₀, z-dc-z₀) \\\\\\ = ax-ax₀+by-by₀+cz-cz₀ -d = 0 $$Thus, 𝐐 can be determined by 𝐧 and d.\nThe normal vector 𝐧 uses coordinates in the camera-1, as it\u0026rsquo;s used to locate the point 𝐐 observed by camera-1.\n𝐂 d ₁ : : : : 𝐑 , 𝐭 𝐩 𝐊 ⁻ 𝐧 ¹ 𝐐 𝐊 𝐐 C ' o p m l 𝐪 m a o n 𝐂 n e ₂ Within the camera-1 space, the distance from the camera center 𝐂₁ to the plane can be expressed as the inner product of $𝐐$ (or $𝐂₁𝐐$) and $𝐧$\n$$ d = 𝐧ᵀ⋅𝐐 = 1⋅|𝐐|⋅cosθ $$ If 𝐧 points towards the side with the camera-1, then θ is larger than 90°, so d is negative. Thus, a minus sign will be added.\nIn MVSNet, all preset depth planes are parallel to the reference camera, so 𝐧 represented in the reference camera space is $(0,0,-1)ᵀ$, and $𝐧 𝐑_{ref}$ (i.e., fronto_direction) is the opposite z-axis of $𝐑_{ref}$ (extrinsics) in the world space.\nIf $d$ and $𝐧$ are known, the coordinate of 𝐐 under camera-1 space can be written as:\nIf $d$ and 𝐧 are known, although there is $𝐐 = \\frac{d}{𝐧ᵀ}$, the 𝐐=(Qx,Qy,Qz)ᵀ can\u0026rsquo;t be determined uniquely based on it alone. Because there are 3 unkonwns with only 1 constraint, there is infinitely many solutions.\nHowever, the following equation can be used:\n$$ 1 = - \\frac{𝐧ᵀ⋅𝐐}{d} $$Assume the conversion between two cameras is: $$𝐐' = 𝐑𝐐 + 𝐭$$ 𝐑 and 𝐭 transform the coordinates in camera-1 to coordinates in camera-2, i.e., the camera-1 is transformed to camera-2.\n𝐐\u0026rsquo; is the coordinates of the covisible point in the camera-2\u0026rsquo;s space. With it, the target pixel on camera-2 is: $𝐊'𝐐'$.\nBy substituting the above equation of 1, 𝐐\u0026rsquo; can be expressed with plane\u0026rsquo;s parameters:\n$$ 𝐐' = 𝐑𝐐 + 𝐭⋅1 = (𝐑 -𝐭 \\frac{𝐧ᵀ}{d})𝐐 $$ Essentially, the z-coordinate of 𝐐 is determined by the plane uniquely.\nThus, 𝐐 can be given by specifying z directly or a plane (𝐧,d) equivalently.\nAnd 𝐐\u0026rsquo; can be represented by two cameras\u0026rsquo; poses or the transformation from one camera to the other.\n(2024-05-30) The point 𝐐 is written as a form with a plane, because the object described by homography is a plane.\nSuppose two cameras have different intrinsics 𝐊 and 𝐊\u0026rsquo;, so their projection pixels are 𝐩 and 𝐪 respectively:\n$$ 𝐩 = 𝐊𝐐 \\quad \\text{and} \\quad 𝐪 = 𝐊' 𝐐' $$Substituting 𝐐\u0026rsquo;, the 𝐪 will be derived from 𝐩:\n$$ 𝐪 = 𝐊' (𝐑-𝐭 \\frac{𝐧ᵀ}{d})𝐐 = 𝐊' (𝐑-𝐭 \\frac{𝐧ᵀ}{d}) 𝐊⁻¹𝐩 $$Hence, the homography from 𝐩 to 𝐪 is: $$ 𝐇 = 𝐊' (𝐑-𝐭 \\frac{𝐧ᵀ}{d}) 𝐊⁻¹ $$(2024-05-10)\nThe way of introducing plane parameters in this homography formula is representing the coordinates of 𝐐 in camera1 by plane normal and d.\nAlthough $\\frac{d}{𝐧ᵀ}$ is not enough to determine the 𝐐, the transformation [𝐑|𝐭] serves as another constraint to ensure that the relationship between 𝐐\u0026rsquo; and 𝐐 holds.\n(2023-12-06)\n$𝐊⁻¹ ⋅(u,v,1)ᵀ$ can\u0026rsquo;t restore a 3D point uniquely because z has been absorbed into u,v. (i.e., z is unknown in $\\rm 𝐐 = 𝐊⁻¹⋅ z⋅ (u,v,1)ᵀ$) Instead, it represents a line of points with various z, corresponding to the epipolar line on the other camera film.5\nTherefore, the distance from a camera to the plane is used to specify the point on a certain plane, which will be transformed first to the other camera space and then projected perspectively onto camera film.\n$$ \\begin{aligned} 𝐐 \u0026= -\\frac{d}{𝐧ᵀ} \u0026 \\text{unique point} \\\\\\ 𝐐' \u0026= 𝐑 𝐐 + 𝐭 \u0026 \\text{Another camera space} \\\\\\ 𝐪 \u0026= 𝐊 𝐐' = 𝐊(𝐑 𝐐 +𝐭) \u0026 \\text{perspective project} \\\\\\ \u0026= 𝐊 (𝐑 -𝐭 \\frac{𝐧ᵀ}{d}) 𝐐 \\\\\\ \u0026= 𝐊 (𝐑 -𝐭 \\frac{𝐧ᵀ}{d}) 𝐊^{-1} 𝐩 \\end{aligned} $$In this way, the projection on a camera film is mapped to another camera\u0026rsquo;s film.\nNote: the pixel coordinates on the two films are not the same.\n(2023-12-07)\nGeneralize H Source article: 5. Multi-View Stereo中的平面扫描(plane sweep) - ewrfcas的文章 - 知乎\nThe generalized homography describes the pixel mapping from the reference plane 𝐂₁ to the plane of an arbitrary camera 𝐂ᵢ.\nW o o r 𝐑 𝐭 r i ₁ ₁ l g ⋅ d i n 𝐇 ᵢ 𝐑 ₁ ᵢ R , e 𝐭 f ᵢ 𝐩 1 S c o a 𝐐 u m 𝐪 r e 𝐐 c r ' e a i To apply the above derived expression 𝐇, the camera Ref-1 should always be the \u0026ldquo;world space\u0026rdquo;, and the target plane is various source camera-i.\nThus, the coordinates 𝐐\u0026rsquo; in a source camera-i space is transformed from 𝐐 in the Ref-1 camera space as:\n$$ 𝐐' =\\begin{bmatrix} 𝐑ᵢ \u0026 𝐭ᵢ \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 𝐑₁ \u0026 𝐭₁ \\\\\\ 0 \u0026 1 \\end{bmatrix}^{-1} 𝐐 $$The matrices product can be simplified as:\n$$ \\begin{bmatrix} 𝐑ᵢ \u0026 𝐭ᵢ \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 𝐑₁ \u0026 𝐭₁ \\\\\\ 0 \u0026 1 \\end{bmatrix}^{-1} = \\begin{bmatrix} 𝐑ᵢ \u0026 𝐭ᵢ \\\\\\ 0 \u0026 1 \\end{bmatrix} \\frac{1}{𝐑₁}\\begin{bmatrix} 1 \u0026 -𝐭₁ \\\\\\ 0 \u0026 𝐑₁ \\end{bmatrix} = \\\\\\ \\begin{bmatrix} 𝐑ᵢ \u0026 𝐭ᵢ \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix}𝐑₁⁻¹ \u0026 -𝐑₁⁻¹𝐭₁ \\\\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 𝐑ᵢ𝐑₁⁻¹ \u0026 -𝐑ᵢ𝐑₁⁻¹𝐭₁+𝐭ᵢ \\\\\\ 0 \u0026 1 \\end{bmatrix} \\\\\\ $$Therefore, the generalized relation between 𝐐\u0026rsquo; and 𝐐 is:\n$$ 𝐐' = \\tilde{𝐑}𝐐+ \\tilde{𝐭} = 𝐑ᵢ𝐑₁⁻¹ 𝐐 + (-𝐑ᵢ𝐑₁⁻¹𝐭₁+𝐭ᵢ)$$Substituting $\\tilde{𝐑}$ and $\\tilde{𝐭}$ into the planar homography:\n$$ 𝐪 =𝐊ᵢ (𝐑ᵢ𝐑₁⁻¹ - \\frac{(-𝐑ᵢ𝐑₁⁻¹𝐭₁+𝐭ᵢ) 𝐧ᵀ}{d} ) 𝐊₁⁻¹𝐩 $$Rearrange it to align with the MVSNet paper:\n$$ 𝐪 =𝐊ᵢ 𝐑ᵢ(𝐑₁⁻¹ - \\frac{(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ)𝐧ᵀ}{d}) 𝐊₁⁻¹𝐩 \\\\\\ = 𝐊ᵢ 𝐑ᵢ ( 𝐈 - \\frac{(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ) 𝐧ᵀ 𝐑₁}{d})𝐑₁⁻¹ 𝐊₁⁻¹𝐩 $$The eq. (1) in MVSNet was wrong: $𝐇_i(d) = 𝐊ᵢ 𝐑ᵢ ( 𝐈 - \\frac{(𝐭₁-𝐭ᵢ) 𝐧ᵀ}{d})𝐑₁ᵀ 𝐊₁ᵀ$, (issue #77)\n$𝐊₁⁻¹ \\neq 𝐊₁ᵀ$ since 𝐊 may not be an orthogonal matrix. 𝐑ᵢ⁻¹ and 𝐑₁ can\u0026rsquo;t be cancled. Corresponding to MVSNet-TF code, c_left is $-𝐑₁⁻¹𝐭₁$; c_right is $-𝐑ᵢ⁻¹𝐭ᵢ$; c_relative is $-(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ)$ fronto_direction is $-𝐧ᵀ 𝐑₁$, which is R_left[2,:]. Therefore, temp_vec = tf.matmul(c_relative, fronto_direction) is $(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ) 𝐧ᵀ 𝐑₁$.\nWarp An Image (2023-12-19) Homography warps a non-rectangle to a rectangle.\n(2023-02-08)\nHomography matrix warps an image to another plane. In the application of image stitching, multiple images are warped to a unified plane.\nThe images have to be observing the common plane.\nIt works for stitching planar panoramas because the scene is far from the camera.\n(2023-12-17) In other words, co-planar points remain co-planar after a homography transformation (matrix). For example, a point on a plane that in image-1 remains located on the plane viewed by image-2.\nTwo pairs of matched points will perform the same transformation: $$\\begin{cases}p_1 = H p_2 \\\\\\ q_1 = H q_2 \\end{cases}$$Put differently, \u0026ldquo;a plane remains a plane in another view\u0026rdquo;, while the portion that is not on the plane will be warped.\nR,T can be solved from H and K.\nDemo: Warp the image to simulate the scene seen from another displaced camera (Aerial View). Refer to: OTC4 Homography說明以及小實驗-AI葵\nSelect 10 points to form a plane, which remains a plane in another view after transformion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import cv2 import numpy as np import matplotlib.pyplot as plt %matplotlib inline img1 = cv2.imread(\u0026#34;rot1.png\u0026#34;) pts1 = [(24, 124), (49, 124), (98, 124), (104, 124), (120, 124), (18, 146), (37, 146), (65, 146), (102, 146), (133, 146)] for pt in pts1: cv2.circle(img1, pt, 0, (0, 0, 255)) plt.figure(figsize=(20,10)) plt.imshow(img1[:,:, ::-1]) img2 = cv2.imread(\u0026#34;rot2.png\u0026#34;) pts2 = [(45, 116), (64, 120), (110, 127), (118, 128), (135, 131), (31, 135), (46, 137), (69, 143), (105, 150), (140, 158)] src_points = np.array(pts1, dtype=np.float32).reshape(-1,1,2) dst_points = np.array(pts2, dtype=np.float32).reshape(-1,1,2) Homography, mask = cv2.findHomography(src_points, dst_points, 0, 0.0) # (3,3) # warp img1 to the view of img2 warped1 = cv2.warpPerspective(img1, Homography, (img1.shape[1], img1.shape[0])) plt.figure(figsize=(20,10)) plt.imshow(warped1[:,:,::-1]) # reverse plt.imshow(img2[:,:,::-1], alpha=0.6) # overlap Bird\u0026rsquo;s-Eye View:\n1 2 3 4 5 src_trapezoid = np.float32([[1, 128], [47, 91], [120, 91], [157, 128]]) dst_rectangle = np.float32([[64, 104], [64, 64], [104, 64], [104, 104]]) homoMatrix = cv2.getPerspectiveTransform(src_trapezoid, dst_rectangle) warped = cv2.warpPerspective(img, homoMatrix,(img.shape[1], img.shape[0])) The image got transformed by the homography to deform the trapezoidal road lane to a rectangle displaying on the image by changing the camera pose to looking down:\nBEV is a homography, but only make sense for the ground plane. 单应性Homograph估计：从传统算法到深度学习 - 白裳的文章 - 知乎 Multi-Depth Planes (2023-12-17)\nGiven camera parameters of a reference view and a source view in the world space respectively, and multiple depth planes, solve the homographies that transforming the source image into the reference camera space:\nR e 𝐊 𝐑 𝐭 f ᵣ ᵣ ᵣ v i 𝐊 𝐑 𝐭 e ₛ ₛ ₛ w S i 𝐳 r m n 𝐧 c g o r m 𝐇 a l G i d v ₁ e n d e p d t ₂ h p l a n d e ₃ s : d ₄ The formula used in MVSNet-tf code is: $𝐊ᵢ𝐑ᵢ ( 𝐈 - \\frac{-(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ) (-𝐧ᵀ 𝐑₁)}{d})𝐑₁⁻¹𝐊₁⁻¹$ because the $-𝐧ᵀ 𝐑₁$ is the z-axis of 𝐑₁ in the world space.\nA source images is warped to the reference camera space and watching a common depth plane. Therefore, the points on the depth plane will have the same x coordinates as the points in the reference image.\nWarp by Sampling (2023-12-18)\nSample the source image to be warped at the locations that is determined by the mapping of homography inversely from the target view to the source view. Refer to MVSNet-pytorch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch from torchvision import transforms h, w = ref.shape[:2] vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed uv1 = torch.cat([uv, torch.ones(len(uv), 1)], dim=-1) # (hw,3) src_KRT = torch.eye(4) src_KRT[:3] = torch.from_numpy(K_s @ RT_s[:3, :4]) ref_KRT = torch.eye(4) ref_KRT[:3] = torch.from_numpy(K_r @ RT_r[:3, :4]) proj = src_KRT @ torch.inverse(ref_KRT) rot, trans = torch.split(proj, [3,1], dim=-1) rot_uv1 = rot[:3] @ uv1.t() # (3, hw) d = 425.0 + 1.06*2.5 * torch.arange(192).view(1,-1,1) #(1, 192, 1) rot_uvd = rot_uv1.unsqueeze(1).expand(-1, 192,-1) * d #(3, 192, hw) pix_proj = rot_uvd + trans[:3].unsqueeze(1).expand(-1, 192, -1) # (3, 192, hw) u_src = 2*(pix_proj[0] / pix_proj[2])/(w-1)-1 v_src = 2*(pix_proj[1] / pix_proj[2])/(h-1)-1 uv_src = torch.stack([u_src, v_src], dim=-1) # (192, hw, 2) uv_src_d1 = uv_src[-1] # plane at the furthest depth sampled = torch.nn.functional.grid_sample( transforms.ToTensor()(src).unsqueeze(0), # (1,3,512,640) uv_src_d1.view(1, h, w, 2), mode=\u0026#39;nearest\u0026#39;) sampled_scaled = torch.tensor(sampled[0] * 255, dtype=int) # (3, 512, 640) fig, ax = plt.subplots(1,3, figsize=(30,15)) ax[0].imshow(src[:,:,::-1]) # (512, 640, 3) ax[0].set_title(\u0026#34;Source view\u0026#34;) ax[1].imshow(sampled_scaled.permute(1,2,0).numpy()[:,:,::-1]) ax[1].set_title(\u0026#34;Warped source\u0026#34;) ax[2].imshow(ref[:,:,::-1]) ax[2].set_title(\u0026#34;Reference view\u0026#34;) The result is almost the same as the one yielded by opencv:\n4 Co-planar Points (2022-12-03)\nHomography matrix builds the relatioship between 2 images to make the pixels corresponding to the same 3D points overlap ³ based on the projection from the common 3D point to different image planes.\nIn following example, two cameras look at a flat plane. And they both observe the points located on that flat plane.\nThen, the matching pixels on the two camera planes can be connected by the chain: img1 ➔ plane0 ➔ img2.\nWhen a 3D world point is projected to 2D camera plane, the coordinates transformation consists of two matrix: extrinsic matrix [R|T] transforming the coordinate system, and the intrinsic matrix [K] projecting the 3D point to a 2D pixel, that is:\n$$ \\begin{bmatrix} u\\\\\\ v\\\\\\ 1\\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f/dₓ \u0026 0 \u0026 cₓ \u0026 0 \\\\\\ 0 \u0026 f/d_y \u0026 c_y \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u00261 \\end{bmatrix} \\begin{bmatrix} r₁₁ \u0026 r₁₂ \u0026 r₁₃ \u0026 t₁ \\\\\\ r₂₁ \u0026 r₂₂ \u0026 r₂₃ \u0026 t₂ \\\\\\ r₃₁ \u0026 r₃₂ \u0026 r₃₃ \u0026 t₃ \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} X \\\\\\ Y \\\\\\ Z \\\\\\ W \\end{bmatrix} $$where the W is the homogeneous coordinate for accommodating the translation vector $[^{^{t₁}\\_{t₂}} \\_{^{t₃}\\_{1}}]$ in the extrinsic matrix.\nOmit z because all points on the plane have z=0.\nGiven $[^{\\_{x₁}}\\_{^{y₁}\\_{w₁}}]$ = H₁₀ $[^{\\_{x₀}}\\_{^{y₀}\\_{w₀}}]$, then there is $$\\rm [^{\\_{x₂}}\\_{^{y₂}\\_{w₂}}] = H₂₀ H₁₀⁻¹[^{\\_{x₁}}\\_{^{y₁}\\_{w₁}}]$$, where H₂₀ H₁₀⁻¹ is the homograhpy matrix H₃ₓ₃. It represents the mapping between the pair of points: (x₁,y₁,w₁) and (x₂,y₂,w₂).\n$$ \\begin{bmatrix} x₂\\\\\\ y₂\\\\\\ w₂ \\end{bmatrix} = \\begin{bmatrix} H₁₁ \u0026 H₁₂ \u0026 H₁₃ \\\\\\ H₂₁ \u0026 H₂₂ \u0026 H₂₃ \\\\\\ H₃₁ \u0026 H₃₂ \u0026 H₃₃ \\\\\\ \\end{bmatrix} \\begin{bmatrix} x₁\\\\\\ y₁\\\\\\ w₁ \\end{bmatrix} $$Expand the above equation, there are 2 equations (with considering x₂\u0026rsquo;=x₂/w₂, y₂\u0026rsquo;=y₂/w₂, such that w = 1):\n$$\\begin{cases} x₂' = \\frac{x₂}{w₂} = \\frac{H₁₁x₁ + H₁₂y₁ + H₁₃w₁}{H₃₁x₁ + H₃₂y₁ + H₃₃w₁} \\\\\\ y₂' = \\frac{y₂}{w₂} = \\frac{H₂₁x₁ + H₂₂y₁ + H₂₃w₁}{H₃₁x₁ + H₃₂y₁ + H₃₃w₁}\\end{cases} \\\\\\ ⇓ \\\\\\ \\rm\\\\{^{x₂'H₃₁x₁ + x₂'H₃₂y₁ + x₂'H₃₃w₁ - H₁₁x₁ - H₁₂y₁ - H₁₃w₁ = 0} \\_{y₂'H₃₁x₁ + y₂'H₃₂y₁ + y₂'H₃₃w₁ - H₂₁x₁ - H₂₂y₁ - H₂₃w₁ = 0} $$The last element H₃₃ is determined when other elements are set, so the degree of freedom for this matrix is 8.\nTherefore, the homography matrix can be solved from 4 pairs of matching points: (x₁,y₁)-(x₂,y₂); (x₃,y₃)-(x₄,y₄); (x₅,y₅)-(x₆,y₆); (x₇,y₇)-(x₈,y₈).\nAfter reformalizing the equation system to a matrix form, the sigular value decomposition can be applied.\n$$\\rm{ [^{^{^{-x₁\\\\; -y₁\\\\; -w₁\\\\; 0\\quad 0\\quad 0\\quad x₂'x₁\\quad x₂'y₁\\quad x₂'w₁} \\_{0\\quad 0\\quad 0\\quad -x₁\\\\; -y₁\\\\; -w₁\\\\; y₂'x₁\\quad y₂'y₁\\quad y₂'w₁}} \\_{^{-x₃\\\\; -y₃\\\\; -w₃\\\\; 0\\quad 0\\quad 0\\quad x₄'x₃\\quad x₄'y₃\\quad x₄'w₃} \\_{0\\quad 0\\quad 0\\quad -x₃\\\\; -y₃\\\\; -w₃\\\\; y₄'x₃\\quad y₄'y₃\\quad y₄'w₃}}} \\_{^{^{pair3-1}\\_{pair3-2}} \\_{^{pair4-1}\\_{pair4-2}}}] \\\\ [^{^{^{H₁₁}\\_{H₁₂}} \\_{^{H₁₃}\\_{H₂₁}}} \\_{^{^{H₂₂}\\_{H₂₃}} \\_{^{H₃₁}\\_{^{H₃₂}\\_{H₃₃}}}}] = 0 } $$Solution This is a homogeneous linear system 𝐀𝐡=0, so its solution has 2 cases:\nIf the number of non-zero rows of the coefficients matrix 𝐀 after Gaussian elimination is less than the number of unknowns of 𝐡, i.e., (rank(𝐀) \u0026lt; #h), there is only the zero solution;\nOr infinitely many non-zero solution, but only the solution satisfying constraints of magnitude is selected.\nIf 𝐀 is invertible (square matrix \u0026amp; rank(𝐀) = #col), then 𝐡=𝐀⁻¹⋅0 = 0 ???\nOtherwise, if 𝐀 is not invertible, there would be inifinetly many non-zero solutions. So the optimal solution should be approached by minimizing the error $$J = ½‖𝐀𝐡-0‖²$$ Therefore, the least-square solution is when the derivative\n$$ ∂J/∂𝐡 = 0 \\\\\\ ⇒ 𝐀ᵀ(𝐀𝐡-0) = 0\\\\\\ ⇒ 𝐀ᵀ𝐀𝐡-𝐀ᵀ⋅0 = 0 $$If 𝐀ᵀ𝐀 is invertible, the optimal 𝐡=(𝐀ᵀ𝐀)⁻¹𝐀ᵀ⋅0 = 0 ???, where the (𝐀ᵀ𝐀)⁻¹𝐀ᵀ is the pseudo-inverse matrix.\nUse all the matching pints to compute a robust H, even though 4 pairs are enough to solve the 8 unkowns (An element is 1 after scaling the matrix by it). Constrained Least Squares. Solve for 𝐡: A𝐡=0, such that ‖𝐡‖²=1. Define least squares problem: min_𝐡 ‖A𝐡‖², such that ‖𝐡‖²=1. We know that: ‖A𝐡‖² = (A𝐡)ᵀ(A𝐡) = 𝐡ᵀAᵀA𝐡, and ‖𝐡‖²= 𝐡ᵀ𝐡 =1. So the problem is: minₕ(𝐡ᵀAᵀA𝐡), such that 𝐡ᵀ𝐡 =1.\nDefine Loss function L(𝐡,λ): L(𝐡,λ)= 𝐡ᵀAᵀA𝐡 - λ(𝐡ᵀ𝐡 -1). And take derivatives of L(𝐡,λ) w.r.t. 𝐡: 2AᵀA𝐡 - 2λ𝐡 =0. It\u0026rsquo;s the classical eigenvalue problem: AᵀA𝐡 = λ𝐡.\nSVD. Find the eigenvalue and eigenvectors of the matrix AᵀA. 𝐡 is he eighenvector with the smallest eigenvalue λ.\nFirst Principles of CV-Dr.Shree;\nMisc (2023-12-07) Epipolar geometry requires translation between cameras, whereas homography doesn\u0026rsquo;t. homography - Carleton University Ref OpenCV4 - Docs - Basic concepts of the homography explained with code Homography in computer vision explained - Behnam Asadi 3D投影变换（含透视投影Perspective Projection）-子燕若水 -CSDN ","date":"2022-12-03T17:30:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/vis/b-note-planar_homography/","title":"Memo: Vis - 3D | Planar Homography"},{"content":"原视频：“L1和L2正则化”直观理解(之一)，从拉格朗日乘数法角度进行理解 - 王木头学科学 -bilibili\n(2023-01-30) 加上正则化项，就变为求拉格朗日函数的最小值，即求\u0026quot;损失函数+权重模长\u0026quot;整体的最小值。 λ 是个比值，λ不同对应的W的位置不同，即λ确定了W的位置。然后 𝐖 自己不断调整，使 J(𝐖) 与 λ‖𝐖‖ 二者的梯度等大反向(抵消)，从而使 $∇_𝐖 L(𝐖,λ)=0$。λ不同，则最优W就不同。_\n机器学习的两个核心议题：优化与正则化。优化找到最优参数；正则化减少模型的过拟合：对模型权重𝐖（不处理b）进行L1和L2正则化\n从3个角度理解 L1 和 L2 正则化：\n拉格朗日乘数法 权重衰减 贝叶斯概率 正则化 与“正则表达式”没有关系，正则表达式是编程中用来处理字符串的技术。 正则化常常指模型权重的L1和L2范数。Dropout 也是一种正则化，在训练时，随机的让一些神经元失效。\n花书：凡是可以减少泛化误差（过拟合），而不是减少训练误差的方法，都可以称作正则化方法。\nL1, L2 正则化项是加在损失函数上，约束模型参数𝐖的 L1, L2 范数的项。L1范数使W变得稀疏，L2范数使W变得小。\n拟合时一般希望保留更多特征，高次的函数表达能力更强，所以模型倾向于学出高次项的系数，使训练误差更小，但不能很好地预测新数据，没有找到基础规律，出现过拟合 3。\n如果特征（各属性）已知，可以把 polynomials 的高次项系数置为0；否则，需采用与\u0026quot;特征的定义\u0026quot;无关的约束，可以选择模长最小的，误差(mse)也最小的参数。 Problem is now well-posed for any degree。Even very high polynomials, simple function tend to be learned with regularization2 所以对细节特征的系数（θ₃²+θ₄²）增大惩罚力度，bias them not to be large （min ∑ᵢᵐ(y^(xᵢ)-yᵢ)² + 1000θ₃²+1000θ₄²）1\n范数：向量长度。网络权重 𝐖 是高维空间中的一个向量（也可认为W是高维空间一个点，范数即为这个点到原点的距离）。 若使用欧几里得距离（坐标差求平方和再开根号）度量它的模长，就是 L2 范数: ‖𝐖‖₂=√(|w₁|²+|w₂|²+\u0026hellip;+|wᵢ|²)； 若用曼哈顿距离计算（坐标差的绝对值求和），就是 L1 范数: ‖𝐖‖₁=|w₁|+|w₂|+\u0026hellip;+|wᵢ|。\n当 Lp 范数的 p 取值大于等于 1 时，有相同距离的点构成的集合是一个凸集； 当 p 取 0-1 之间，是非凸集。 当p小于等于1时，会带来稀疏性，所以 the L1 norm is the only norm which both induces some sparsity in the solution and remains convex for easy optimization. 如果问题对应的函数是一个凸函数，它的取值范围（可行域）是一个凸集，这就是一个凸优化问题，容易求解。L1和L2正则化在某种程度上，就是在利用 L1 和 L2 范数的凸集特性。\n出现过拟合的原因之一：W太大会把噪声放大 神经网络的输出层做了三件事：\n接收上一层的激活值 a⁽ˡ⁻¹⁾，乘以权重 Wˡᵀ，加上偏移bᵀ，得到线性叠加后的 zˡ： zˡ= Wˡᵀ⋅a⁽ˡ⁻¹⁾+bᵀ。\n再做（激活引入非线性/限制取值0-1，然后）softmax 把线性结果 zˡ 变成概率分布: aˡ = softmax (zˡ)。\n输出层的损失函数：J(aˡ) = MLE (aˡ)。\n极大似然 MLE 与 CrossEntropy 等价，前者最大化重复采样训练样本的概率，后者最小化网络给出的类别概率乘以真实概率对应的信息量。 （softmax 搭配MLE，等价于最大熵。）\n调整 𝐖,𝐛 使损失函数J(𝐖,𝐛)最小，但当损失值最小时，对应的 𝐖,𝐛 不是唯一的。比如:\n把前面所有隐藏层的权重和偏置都放大2倍（用ReLu激活），但同时输出层的权重和偏置缩小 1/2⁽ˡ⁻¹⁾倍，得到的线性结果不变，则损失值不变，所以最终优化得到的参数与初始值相关。\n如果输入放大 2 倍的同时输出层权重缩小 2 倍，线性结果不变，损失值也不变。所以最终优化得到的 𝐖,𝐛 与输入数据相关。\n训练神经网络时的目的是找到损失函数 J(W,b) 的最小值，但相同的损失值可能对应多组绝对值（模长）不同的(W,b)。\n绝对值不同的 𝐖,𝐛 可能在训练集上的损失值相同，但在测试集、unseen输入、\u0026ldquo;带噪声的输入\u0026quot;上的表现有差别，不同参数的泛化能力不同。 绝对值大的参数会把噪声放大，从而影响预测结果。\n正则化就是人为的设定参数𝐖 的取值范围（可行域），让W不能超出该范围，从而在可行域范围内，求损失函数的最小值。带条件的优化问题可以用拉格朗日乘数法求解。 我们只需要规定 𝐖 的范围（多项式各未知数的系数），因为 W 直接决定了模型曲线是什么样子，而 b 与最终拟合的曲线的形状（过拟合与否）无关，b只影响平移，因此只要𝐖 被约束好了，b在训练时会自动调整好，所以不需额外做约束。\nw 的可行域范围 在求损失函数最小值时，权重 𝐖 的模长不要超过C：\n若采用曼哈顿距离度量模长，即计算向量的L1范数，则优化问题为：\nmin J(𝐖 , 𝐛, 𝐗), s.t. ‖𝐖‖₁-C≤0\n若采用欧几里得距离计算模长（向量的L2范数），则优化问题为：\nmin J(𝐖 , 𝐛, 𝐗), s.t. ‖𝐖‖₂-C≤0\n把优化问题写成拉格朗日函数 = 目标函数 + 约束条件乘以拉格朗日乘子 λ： L(𝐖, λ) = J(𝐖) + λ(‖𝐖‖₁ - C), 然后求解：$\\rm min_𝐖 \\ max_λ L(𝐖,λ), s.t. λ≥0$ （不考虑b,X）\n上图坐标轴是 𝐖 的各个维度，椭圆是损失函数J(𝐖)的等高线，中心是损失函数的最小值对应的 𝐖，绿色框代表可行域范围（𝐖 的模长限制），蓝色点是在满足约束条件时，损失函数能取到的最小值，对应的𝐖 ；或者说在可行域范围内找损失函数梯度+𝐖可行域梯度=0的点。\n“当目标函数是一个凸函数或是一个凹函数，并且对应的约束条件是一个凸集，那么整个问题（目标函数+λ⋅约束条件）就是一个凸优化问题”。 约束条件采用L1或L2范数时，可行域是一个凸集。对应于凸集的约束条件并不会改变原来问题的性质， 即如果原来的问题本身是一个凸问题，加上这个约束条件后，仍然是凸问题，如果原来的问题是非凸问题，加上这个约束条件也不会让这个问题变得更糟糕。\n(2022-11-07) 损失函数与正则化项都画在二维平面上，是因为研究的只是“两个对象”：输入X和输出Y之间的关系？\n不，因为是要限制w的模长，二维平面只表示了二维w。加了正则化项，相当于修改了初始时的w，缩短它到那条虚线的距离，但不考虑它对应的损失值变大还是变小了，虚线总是沿着损失函数的梯度方向，而且连接着损失值最小的那个w，不过那条虚线上的w是等比的，等比例缩放的 w 能收敛到的最值是相同的\n(2023-01-30) 正则化项 λ‖𝐖‖ 的作用是持久的。在 λ‖𝐖‖ 的基础上，𝐖 做调整以使损失值最小，也就是使 J(𝐖) 与 λ(‖𝐖‖₁ - C) 两项的梯度抵消（等大反向）。\u0026ldquo;在原始最小二乘的结果上做了缩放\u0026rdquo;4。\nL1, L2 正则化项 把拉格朗日函数展开：\nL(𝐖,λ) = J(𝐖) + λ(‖𝐖‖₂-C) = J(𝐖) + λ‖𝐖‖₂ - λC\n但是常见的 “损失函数+ L2正则化项” 的表达式是 拉格朗日函数+λC：\nL\u0026rsquo;(𝐖,λ) = L(𝐖,λ) + λC = J(𝐖) + λ‖𝐖‖₂\n在求L\u0026rsquo; 和 L 这两个拉格朗日函数最小值时，虽然最小值可能不一样，但对应的 𝐖 是一样的 （因为只要求解 $∇_𝐖 L'(𝐖,λ) =∇_𝐖 L(𝐖,λ)=0$）：\n$arg_𝐖 (min_𝐖\\ max_λ L'(𝐖,λ), s.t.\\ λ≥0) = arg_𝐖 (min_𝐖\\ max_λ L(𝐖,λ), s.t.\\ λ≥0)$\n如何直观理解 L\u0026rsquo; 呢？ 或者说：为什么\u0026quot;常用的L2正则化表达式\u0026quot;比\u0026quot;损失函数加上 𝐖 约束条件写成的拉格朗日函数\u0026rdquo;，少一个 λC 呢？\nC 决定了 𝐖 的模长，不指定 C 是因为 λ 可以控制模长的范围。拉格朗日乘子λ的作用是调节\u0026quot;约束条件λ‖𝐖‖的梯度\u0026quot;的大小，使之与\u0026quot;目标函数J(𝐖)的梯度\u0026quot;等大反向抵消（求导=0就是求梯度=0的点）。 当λ=0时，相当于没有约束；而当λ=inf时，W会趋于0，失去拟合能力了。λ 可以用cross validation 来选择\n红色是损失函数的等高线，绿色是约束条件的等高线，箭头表示梯度\nλ 的绝对值等于\u0026quot;损失函数J(𝐖)的梯度大小\u0026quot;除以\u0026quot;约束条件对应函数λ‖𝐖‖的梯度大小\u0026quot;，所以空间中各点处的λ就能算出来，反过来不同的 λ 对应的点的位置不同。只有在恰当的位置J(W)与λ‖𝐖‖的梯度之和才等于0。 ~~不同的 λ 对应的梯度=0的点的位置不同，损失函数J(𝐖)最小值不同，但对应的（拉格朗日函数的最优）W是相同的??? ~~\nλ和C之中只有一个超参数：在拉格朗日函数中，C是超参数，人为指定模长范围C后，λ 也就跟着确定了；而在 L\u0026rsquo; 中，λ 是超参数，人为指定后，就能唯一确定梯度=0的点的位置。\nL2正则化（给定λ后）确定的最值点（最优𝐖）基本不会落在坐标轴上（可行域是圆形）， 而L1正则化找到的满足约束条件与目标函数的梯度之和=0的点（可行域范围与损失函数等高线相切的点）容易取在坐标轴上（可行域有尖角且落在坐标轴上，容易与损失函数相切）， 当最优 𝐖 落在某一坐标轴上时，只有那一维不是0，其他维度的坐标都是0。\n比如依据两个特征做判断时，最优 𝐖 不落在坐标轴上，则两个特征都有一定的决定作用， 而通过调整 λ 使最优W落在坐标轴上时，则只会关注该轴特征的有无，所以使用 L1 可以使决策变得简单，使 W 向量变得稀疏（只有某个“重要”维度(对结果贡献大)有值，其他维都是0），把特征之间的关系去耦合了，把模型复杂度降低了，从而减少过拟合。 但是L1 的解不太稳定，训练时各批次数据的损失函数不同，椭圆会变化，切点可能从一个轴换到另一个轴上，对应的W变化大。3\nL1 和 L2 都是对 W 进行约束，但效果不同，可以把二者结合起来一起用。L2正则化只是限制模长，L1 正则化还带来了稀疏性\n正则化带来的损失值误差？不重要啊 评论：上图绘制的椭圆对应的损失函数J(𝐖)的接收的输入是整个实数域，而在神经网络中，损失函数接收的是网络的输出值，网络不同的权重可能对应相同的输出值，损失值也就相同\n因为不同的初始参数 𝐖 最终收敛到的损失函数最小值可能是相等的，比如等比缩放参数时，min J(𝐖 ,𝐛) = min J(a𝐖 ,a𝐛)， (𝐖 ,𝐛) 与 (a𝐖 ,a𝐛) 共线（虚线），\n加约束条件λ‖𝐖‖与不加约束条件(λ=0)，通过最小化拉格朗日函数L(𝐖,λ)收敛到的 W 不同（W由λ决定），但是如果两个向量W是共线的，它们对应的 L(W,λ) 能够收敛到的最小值（where L(W,λ)的梯度=0）是相同的，只不过λ取的不好，还没走到最小值，还没平衡好 J(W) 和 ||W||之间的权重比例。\n同一虚线上的 W 最终能够收敛到的拉格朗日函数值是相同的。虚线上的点是本可能收敛到的最值\n如果不加约束条件，初始的较大的 W 本可以收敛到J(W)椭圆中心（损失最小值），但是因为指定了约束条件λ，模长被限制了，被拉向原点了，找到的最优W与椭圆中心离得很远，损失值J(W)变大了，看起来就带来了误差。 但是（最小化拉格朗日函数过程中的）误差并不是到椭圆中心的距离，而是到“虚线”的距离，虚线上的 W 已经调整到能使 J(W) 与 ||W||的梯度共线，因为 λ 是人为设置的，所以W还要继续调整，最终虚线上的W使 L(W,λ)的梯度=0。\n在虚线上的不同 W 对应的拉格朗日函数值L(W,λ)的梯度是相同的（在虚线上的点，方向共线已满足，只是λ不同）， 虽然可行域越小，对损失函数J(W)的最小值偏离的误差就越大，因为W初始值决定了最终能收敛到哪个J(W)，但关注的不是它J(W)。\nW调整到最后：J(W)的梯度方向与||W||梯度方向共线，并且对于超参数缩放因子λ，有L(W,λ)的梯度=0。\n所以真正关注的到最优解的偏差，是到虚线的距离，超参数λ选的好与坏，带来的偏差不大，所以能用正则化就用。\n- 例子：只有两个分量的W 与对应的损失值 绘制的图像为：\n同心椭圆是损失函数 J 的等高线，即每一圈椭圆上，损失值是相同的，中间的灰色图形的最外沿的一圈代表 W 的范数。正方形代表 L1 范数的图像，圆形代表 L2 范数的图像\n看下这篇：机器学习之正则化（Regularization）- Acjx -博客园\nDDG搜索：entropy regularization 熵正则化\n(2023-01-30) DDG search: \u0026ldquo;正则化项\u0026rdquo;\n正则化的作用 原文链接：正则化及正则化项的理解 - guuuuu - CSDN\n防止过拟合：𝐖 越小，拟合的函数曲线越简单光滑，越不容易过拟合；\n正则化项代表先验信息：试验之前的对‖𝐖‖的认知，λ 是对先验信息的相信程度。\n频率派直接对参数𝐖进行分析（而贝叶斯派是对参数出现的概率P(𝐖)进行分析）； 参数 𝐖 直接出现在损失函数中，所以频率派对损失函数做修正：加上了先验部分知识，即正则化项。\n对于模型：y = θ₀ + θ₁x₁ + \u0026hellip; + θⱼxⱼ + \u0026hellip; + θₙxₙ，解最优化问题：\n$arg\\ min_{θ₀,θ₁,...,θₙ}\\ J(θ) = 1/2m ⋅(∑ᵢ₌₁ᵐ(h_θ(xⁱ)-yⁱ)² + λ∑ⱼ₌₁ⁿ(θⱼ-\\\\^θⱼ)²)$， 其中 ^θⱼ 为先验解。 λ 不同大小的选择，体现了这个先验解 ^θⱼ 的可信程度。如果 λ 是一个很小的整数，那正则化项将不起什么作用，说明给的先验解有很大的不确定性，在一定程度上是不可信的；\n如果 λ 很大，则正则化项占支配地位，最后的解将靠近于 ^θⱼ。\n频率派 贝叶斯派 找最优𝐖 直接对参数 𝐖 本身下手 对参数出现的概率 P(𝐖) 下手 思路 最优 𝐖 使训练误差最小 最优 𝐖 出现的后验概率 P(𝐖 优化公式 损失函数 贝叶斯公式 先验 正则化项（对𝐖 的先验认知） 自带先验概率（P(𝐖)） 修正 加上一部分先验信息 对似然值做 \u0026ldquo;P(𝐖)/配分函数\u0026rdquo; 的缩放 超参数 λ; L1 or L2范数 先验概率的分布: Laplace, Gaussian 有助于处理条件数（condition number）不好的情况下，矩阵求逆困难的问题。\n概念：如果方阵 A 是非奇异的（A的行列式不等于0，正定矩阵一定是非奇异的），那么 A 的 condition number 定义为：𝜅(A) = ‖A‖ ‖A⁻¹‖ 可以看出：如果 A 是奇异的，那么 A 的条件数为无穷大。条件数越小，所获得的解越可靠，模型鲁棒性越好，抗干扰能力越强。 例如对于模型 AX=b，A 的条件数越小（A的行列式远不接近于0），那么 A，b 的稍微的变化对解 X 的影响越小，对 X 的求解对样本集（A，b）中引入的干扰的抵抗能力越强，即所求解 X 越可靠。\nDDG search: \u0026ldquo;正则化项 pytorch\u0026rdquo;\nPyTorch 12.正则化-科技猛兽-知乎 1 2 3 # optimizer w and w/o regularization optim_normal = torch.optim.SGD(net_normal.parameters(), lr=lr_init, momentum=0.9) optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-2) DDG search: \u0026ldquo;pytorch optimizer weight decay\u0026rdquo; SGD-pytorch docs\n(2023-02-26) DDG serach: \u0026ldquo;正则化项 可以完全避免过拟合吗\u0026rdquo;\n在机器学习中，L2正则化为什么能够缓过拟合？ - 知乎\nRef [知识梳理-03] Regularization 正则化-凩子白-bilibili Linear regression (6): Regularization (UCI cs273a) - Alexander Ihler 什么是 L1 L2 正规化 正则化 Regularization (深度学习 deep learning) ","date":"2022-11-07T11:52:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/11_l1l2%E6%AD%A3%E5%88%99%E5%8C%961-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/","title":"watch: DL - 王木头 11 | L1L2 Reg (1), Larange Multiplier"},{"content":"(2022-5-25) 损失函数是输入样本batch的函数，不同batch的误差函数不同，如果在一个batch上某 w 的导数为零，在下一个batch上该 w 的导数不为零，就可以继续修正，而不会停滞在鞍点。误差函数横轴是 w，纵轴是 error。0\n3 loss functions 损失函数是为了: 衡量两个概率模型间的差别， 三种思路：最小二乘法(MSE)，极大似然估计(MLE)，交叉熵(CE) ¹\nMSE 对于单分类问题（是or不是），也就是抛硬币（反面的概率已经蕴含在正面的概率之中了），那么：\n最小二乘就是：概率(sigmoid输出0~1)减标签。为了依据误差修正w，误差取平方使其可导。如果损失函数中代入的是预测的正(反)概率，那损失函数是个二次曲线 L = (prob - label)²，横坐标是概率，纵坐标是loss，当prob=target, 预测出来的值与离散的观察值最接近。\n对于输出是多维的，在各个维度上都是二次曲线，多元最小二乘（多元线性回归）: J(w) = (Xw-Y)ᵀ(Xw-Y)。解∇J(w) = 0，就是求 pseudoinverse matrix ⁸。\n如果这样能一次求出 w，为什么还要梯度下降呢？ 因为有时 X 不可逆（可以加正则化项解决 note）， 而且有的激活函数 g 也不可逆（通过手动做归一化解决，例如 B-ELM 中使用了 minmax 函数）。\n当MSE 用于回归问题，loss=∑ᵢ(yᵢ-wxᵢ-b)²是凸函数，直接求导等于零，即可求出解析解；但是用于分类问题，输出需要经过sigmoid/softmax变成概率，loss=∑ᵢ(yᵢ-1/(1+e⁻ʷˣⁱ))²是非凸的，不能直接求解析解，而且不宜优化 ³。\nMLE 极大似然估计：各个可能的假设模型产生训练样本标签的分布的概率是多少，目标就是找到概率最大时对应的模型（加个负号取最小）；∏ᵢ pᵢˣ (1-pᵢ)¹⁻ˣ\nCE 交叉熵：网络模型要与人脑中的模型足够接近，某一事件在网络模型中发生对应的信息量要接近在人脑中发生对应的信息量，多个事件要以他们在人脑中发生的概率加权。∑ᵢ humanᵢ(-log₂ netᵢ)\n最小二乘可以用于回归，即网络输出可以是任意的数值；而极大似然估计和交叉熵都是基于概率的，网络的输出是概率，位于0-1之间，所以采用MLE或CE损失函数时，输出层神经元的激活函数需要用sigmoid，把输出压缩到0-1之间; 而隐藏层都可以用ReLu。 多类别问题输出用 softmax 激活，得到各类别的概率分布。\n交叉熵认为各类别相互独立，每一维是一个二分类器，单个样本的概率（似然）是：P₁ʸ¹ ⋅ P₂ʸ² ⋅ \u0026hellip; ⋅ Pₖʸᵏ， 所以需要用 softmax 做一下归一化\nMSE 与 CE 区别 (Google search: \u0026ldquo;为什么不用mse做损失函数\u0026rdquo;)\nMSE 不适合分类问题² 工程角度：如果用MSE做分类，对 softmax 的输出使用 MSE，即正确类的概率越接近 1 越好，其他类的概率越小越好: minimize Loss = (prob_true-1)² + ∑(prob_other)²。 但是在 Loss 的梯度表达式中存在 prob_true 这个因子，可能在训练初期 prob_true 很小，梯度趋于0，无法更新。 而在用 CE 做Loss时，它的梯度中不含有单独的 prob_true 这一项（被消掉了），就不易发生梯度消失² 。\n(2022-11-06) 分类问题常使用 softmax，所以适合使用CE；而回归问题不常使用softmax，所以适合使用 MSE。\n理论角度：二者假设不同，MSE假设观察到的 y\u0026rsquo;=真实y+高斯噪声，所以通过极大似然法求解一组参数使得对应的高斯噪声最小的情况。所以MSE求解出来的值会更偏向于各个离散的观察值。而CE的假设应该是多分类情况下，拟合不同类别的概率分布。\u0026ldquo;多分类问题的分布符合多项式分布，CE是多项式分布的最大似然⁵\u0026rdquo;\n交叉熵不适用回归问题⁴ 在(多)分类问题中，交叉熵的损失函数只和分类正确的预测结果有关系，而MSE的损失函数还和错误的分类有关系，因此该\u0026quot;分类\u0026quot;函数除了让正确的分类尽量变大，还会让错误的分类变得平均，但实际在分类问题中，MSE 的这个调整是没有必要的 ⁶。\n把 多分类问题 中的 \u0026ldquo;类别\u0026rdquo; 对应到 多元回归问题 中的 \u0026ldquo;特征\u0026rdquo;。对于一个连续的输出量，应是由各个特征共同作用的，分别有不同的贡献，而不能只看重某一个特征，所以CE不适合回归问题。但也可以用 ⁷。\n损失函数的性质 (2023-02-17)\n可微分性 可导性 单调性 凸性 可分离性 可表示性 借助 pytorch 可视化损失函数的导数9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch import matplotlib.pyplot as plt def abs_func(x): return x.abs() x = torch.linspace(-2,2,100) x.requires_grad_(True) y = abs_func(x) plt.plot(x.detach().numpy(), y.detach().numpy()) plt.show() y_prime = torch.autograd.grad(y.sum(), x, create_graph=True)[0] plt.plot(x.detach().numpy(), y_prime.detach().numpy()) plt.show() Ref 【機器學習2021】類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)-李宏毅 “损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法” 机器学习面试之MSE与CE的区别？- 简书 交叉熵损失(Cross-entropy)和平方损失(MSE)究竟有何区别？ 为什么均方差（MSE）不适合分类问题？交叉熵（cross-entropy）不适合回归问题？ 简单的交叉熵，你真的懂了吗？ - 蔡杰的文章 - 知乎 分类模型中交叉熵比MSE更合适 - 郝曌骏- github 分类必然交叉熵，回归无脑MSE？未必 - 冷比特er的文章 - 知乎 机器学习基础学习-多元线性回归问题（数学解实现）- csdn 6.2 损失函数性质 - 梗直哥丶(质量不高，只看代码) ","date":"2022-11-06T22:23:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/dl-loss_functions/","title":"memo: DL | Loss Functions"},{"content":" iterable 是存储多个值的对象，并可以逐个返回它的值 generator defined by yield itertools.cycle(iterable) vs while True: 前者生成的 iterator 可以无限次重复遍历一个有限的 iterable，而while True 只会遍历一遍 iterable。 so\n1 2 3 4 5 6 7 8 9 10 def myGenerator():\t# define a iterable yield 1 yield 2 def loop_iterator(iterable=[]): for val in itertools.cycle(iterable): # turn to iterator yield val\t# this func is a generator gen = loop_iterator(iterable=myGenerator())\t# create a generator next(gen) Contrast:\n1 2 3 4 5 6 def loop_while(iterable): while True: for val in iterable: yield val\t# define a generator gen = loop_while(myGenerator()) next(gen) yield 关键字用于 generator function 中，返回一个值，但不终止函数执行，等待下一次调用再继续 GfG-yield。 Generator function 创建一个 generator object，也就是iterable，它可以用 __next__() 方法或 for 循环遍历。 定义 generator function 只需要 yield 关键字，比较方便，而定义 iterator 需要定义 __next__() 和 __iter__() 方法 GfG-gener\niterator has next iterator 是实现了__iter__() 和 __next__() 方法的对象，而 list, tuple, dict, set 都是 iterable 对象，它们都可以调用自己的 iter() 方法变成一个 iterator。 for 循环遍历 iterable 时其实创建了一个iterator，每次循环执行__next__()方法 w3school\n1 2 3 myList = [1,2,3] myIterator = iter(myList) # 或 ls.__iter__() print(next(myIterator)) # traverse the next value ","date":"2022-10-24T10:57:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/python_iter/","title":"memo: Python | iter"},{"content":"2022-8-21: Experiments were conducted on the code of PixeNerf. Trying to get the identical loss curves every time.\n在 train.py 中设置np.random.seed(0)和torch.manual_seed(0)，使每次训练时的图片和像素，以及验证时的object和视图是一样的；\n在 trainer.py 中设置 worker_fn\n在 nerf.py 中设置torch.manual_seed(2201)，每次取一样的随机数，loss曲线有的地方还是有0.1的差异。\ntrain_set 通过ColorJitterDataset 做了颜色增强，data.util.py中加np.random.seed(0)，然后E 0 的第1个batch 的psnr就相同了(10.55053)。\n设置 models.py 中的运算为 deterministic，pytorch=1.6.0 （Docs中）只有以下两个设置，但好像没效果。\n1 2 torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False pytorch=1.8 才能使用 torch.use_deterministic_algorithms()，于是:\n卸载conda uninstall pytorch，重装了1.12.1，报错:\npyparsing.exceptions.ParseException: Expected '}', found '='。\n又重装了 1.10.2 之后: 在from torch.utils.tensorboard import SummaryWriter处报错: AttributeError: module 'distutils' has no attribute 'version'\n降级:\n1 2 pip uninstall setuptools pip install setuptools==59.5.0 然后那个参数解析pyparsing还是报错，不知道怎么解决。 pytorch forum\n把环境删了，修改 environment.yml 中的版本：pytorch==1.11.0, torchvision==0.12.0 (版本号相差1)，重新创建环境：conda env create -f envxx.yml。然后在 torch.matmul() 处报错:\n1 2 3 4 5 6 7 RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA \u0026gt;= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility 在程序开头设置环境变量：export CUBLAS_WORKSPACE_CONFIG=:4096:8。\n又在 loss.backward() 中 autograd 时报错：\n1 2 3 4 5 6 RuntimeError: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set \u0026#39;torch.use_deterministic_algorithms(True)\u0026#39;. You can turn off determinism just for this operation, or you can use the \u0026#39;warn_only=True\u0026#39; option, if that\u0026#39;s acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.` 加上参数：torch.use_deterministic_algorithms(True, warn_only=False) 可以运行；再加上torch.backends.cudnn.benchmark = False，除batch 1外，两次实验结果仍不完全一致，而且性能下降很多，好像是pytorch版本导致的，放弃。又重装回原环境。 torch-reproducibility-doc; It\u0026rsquo;s introduced in 1.8\npython train/train.py --name dtu_origin --conf conf/exp/dtu.conf --datadir data/DTU_Dataset/rs_dtu_4 --nviews 3 --gpu_id='0 2' --epochs 400_000\n","date":"2022-10-24T10:57:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_reproducibility/","title":"memo: PyTorch | Reproducibility"},{"content":"Pytorch-Transformer Model Parallelism using Transformers and PyTorch\nLoading the data\nInstantiate a model\nCreate torch Dataset and DataLoader\n1 class myDataset(torch.utils.data.Dataset): Split the data into train and val sets:\n1 2 from sklearn.model_selection import train_test_split df_train, df_val = train_test_split(imdb_df, test_size=0.3, random_state=2021 create DataLoader for train set and val set:\nMake a wrapper for the model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MultiGPUClassifier(torch.nn.Module): def __init__(self, roberta_model): super(MultiGPUClassifier, self).__init__() # Embedding layer --\u0026gt; cuda:0 self.embedding = roberta_model.roberta.embeddings.to(\u0026#39;cuda:0\u0026#39;) # Encoder Layer --\u0026gt; cuda:1 self.encoder = roberta_model.roberta.encoder.to(\u0026#39;cuda:1\u0026#39;) # Classifier --\u0026gt; cuda:1 self.classifier = roberta_model.classifier.to(\u0026#39;cuda:1\u0026#39;) def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None): # Pass the input_ids to cuda:0 since embedding layer in cuda:0 emb_out = self.embedding(input_ids.to(\u0026#39;cuda:0\u0026#39;)) # Move the outputs of embedding layer to cuda:1 as input to encoder layer enc_out = self.encoder(emb_out.to(\u0026#39;cuda:1\u0026#39;)) classifier_out = self.classifier(enc_out[0]) return classifier_out # Initialize the model multi_gpu_roberta = MultiGPUClassifier(roberta_model) Upon constructing the model, the memory usage can be seen using nvidia-smi.\nCreate optimizer and loss function for the model:\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import get_linear_schedule_with_warmup, AdamW EPOCHS = 2 LR = 1e-5 optimizer = AdamW(multi_gpu_roberta.parameters(), lr=LR) total_steps = len(train_data_loader)*EPOCHS scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) loss_fn = torch.nn.CrossEntropyLoss().to(\u0026#39;cuda:1\u0026#39;) # match with the roberta.classifier layer Create a helper function for training the model and returning accuracy and losses:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def train_model (model, data_loader, loss_fn, optimizer, scheduler, n_examples): model = model.train() # losses = [] correct_predictions = 0 for d in data_loader: # take a batch input_ids = d[\u0026#39;input_ids\u0026#39;] attention_mask = d[\u0026#39;attention_mask\u0026#39;] # Reshaping attention mask for adapting the forward pass reshaped_attention_mask = attention_mask.reshape(d[\u0026#39;attention_mask\u0026#39;].shape[0],1,1,d[\u0026#39;attention_mask\u0026#39;].shape[1]) targets = d[\u0026#39;labels\u0026#39;] outputs = model(input_ids = input_ids, attention_mask = reshaped_attention_mask) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets.to(\u0026#39;cuda:1\u0026#39;)) # move targets to cuda:1 to calculate loss correct_prediction += torch.sum(preds == targets.to(\u0026#39;cuda:1\u0026#39;)) losses.append(loss.item()) loss.backward() # Clip the gradients of the model to prevent exploding gradients using clip_grad_norm torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() # gradient descent scheduler.step() # lr decay optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) Create a helper function for evaluating the model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def eval_model(model, data_loader, loss_fn, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for d in data_loader: input_ids = d[\u0026#39;input_ids\u0026#39;] attention_mask = d[\u0026#39;attention_mask\u0026#39;] reshaped_attention_mask = attention_mask.reshaped(d[\u0026#39;attention_mask\u0026#39;].shape[0],1,1,d[\u0026#39;attention_mask\u0026#39;].shape[1]) targets = d[\u0026#39;labels\u0026#39;] outputs = model(input_ids = input_ids, attention_mask=reshaped_attention_mask) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets.to(\u0026#39;cuda:1\u0026#39;)) correct_predictions += torch.sum(preds == targets.to(\u0026#39;cuda:1\u0026#39;)) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) Create the training loop and only store the best one:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from collections import defaultdict history = defaultdict(list) best_accuracy = 0 %%time for epoch in range(EPOCHS): print(f\u0026#39;Epoch {epoch+1}/{EPOCHS}) print(\u0026#39;-\u0026#39; * 10) train_acc, train_loss = train_model(multi_gpu_roberta, train_data_loader, loss_fn, optimizer, scheduler, len(df_train)) print(f\u0026#39;Train Loss:{train_loss}; Train Accuracy: {train_acc}\u0026#39;) val_acc, val_loss = eval_model(multi_gpu_roberta, val_data_loader, loss_fn, len(df_val)) print(f\u0026#39;Val Loss: {val_loss}; Val Accuracy: {val_acc}\u0026#39;) history[\u0026#39;train_acc\u0026#39;].append(train_acc) history[\u0026#39;train_loss\u0026#39;].append(train_loss) history[\u0026#39;val_acc\u0026#39;].append(val_acc) history[\u0026#39;val_loss\u0026#39;].append(val_loss) if val_acc \u0026gt; best_accuracy: torch.save(multi_gpu_roberta.state_dict(), \u0026#39;multi_gpu_roberta_best_model_state.bin\u0026#39;) best_accuracy = val_acc Visualizing model performance\nCombining DDP with Model Parallelism DDP tutoria\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class ToyMpModel(nn.Module): # model parallel, multi-gpu model def __init__(self, dev0, dev1): # use 2 gpu super(ToyMpModel, self).__init__() self.dev0 = dev0 self.dev1 = dev1 self.net1 = torch.nn.Linear(10,10).to(dev0) # move 0th layer to dev0 self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10,5).to(dev1) # move 1st layer to dev1 def forward(self, x): x = x.to(self.dev0) # move input to the dev0 same as 0th layer x = self.relu(self.net1(x)) x = x.to(self.dev1) # move output of 0th layer to dev1 return self.net2(x) DDP wraps a multi-GPU model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def demo_model_parallel(rank, world_size): # rank indicates the index of this process, world_size is the total numer of gpu will be used. print(f\u0026#34;Running DDP with model parallel example on rank {rank}.\u0026#34;) setup(rank, world_size) # set env vars, and initialize process group # set up multi-GPU model and devices for this process dev0 = (rank * 2) % world_size dev1 = (rank * 2 + 1) % world_size mp_model = ToyMpModel(dev0, dev1) ddp_mp_model = DDP(mp_model) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001) optimizer.zero_grad() # outputs will be on dev1 outputs = ddp_mp_model(torch.randn(20,10)) labels = torch.randn(20, 5).to(dev1) loss_fn(outputs, labels).backward() optimizer.step() cleanup() if __name__ == \u0026#34;__main__\u0026#34;: n_gpus = torch.cuda.device_count() assert n_gpus \u0026gt;=2, f\u0026#34;Requires at least 2 GPUs to run, but got {n_gpus}\u0026#34; world_size = n_gpus run_demo(demo_model_parallel, world_size) Apply Model Parallel to Existing Modules SINGLE-MACHINE MODEL PARALLEL BEST PRACTICES ResNet50\nnn.Sequential\nGNT model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class GNTModel(object): ├ __init__(self,args) │ ├─ self.net_coarse = GNT().to(device) │ │ └─ __init__() │ │ ├─ self.rgbfeat_fc = nn.Sequential(Liner, ReLU, Linear) │ │ ├─ for i in range(args.trans_depth): │ │ │ ├─ self.view_trans.append( Transformer2D() ) # ModuleList() │ │ │ │ ├─ self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate) │ │ │ │ │ ├─ self.fc1 = nn.Linear(dim, ff_hid_dim) │ │ │ │ │ ├─ self.fc2 = nn.Linear(ff_hid_dim, dim) │ │ │ │ │ ├─ self.dp = nn.Dropout(ff_dp_rate) │ │ │ │ │ └─ self.activ = nn.ReLU() │ │ │ │ │ │ │ │ │ └─ self.attn = Attention2D(dim,attn_dp_rate) │ │ │ │ ├─ self.q_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.k_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.v_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.pos_fc = nn.Sequential(Linear, ReLU, Linear) │ │ │ │ ├─ self.attn_fc = nn.Sequential(Linear, ReLU, Linear) │ │ │ │ ├─ self.out_fc = nn.Linear(dim, dim) │ │ │ │ └─ self.dp = nn.Dropout(dp_rate) │ │ │ │ │ │ │ ├─ self.ray_trans.append( Transformer() ) # nn.ModuleList() │ │ │ │ ├─ self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate) │ │ │ │ └─ self.attn = Attention(dim, n_heads, attn_dp_rate, attn_mode, pos_dim) │ │ │ │ ├─ if attn_mode in [\u0026#34;qk\u0026#34;,\u0026#34;gate\u0026#34;]: │ │ │ │ ├─ if attn_mode in [\u0026#34;pos\u0026#34;, \u0026#34;gate\u0026#34;]: │ │ │ │ ├─ if attn_mode == \u0026#34;gate\u0026#34;: │ │ │ │ ├─ self.v_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.out_fc = nn.Linear(dim, dim) │ │ │ │ ├─ self.dp = nn.Dropout(dp_rate) │ │ │ │ ├─ self.n_heads = n_heads │ │ │ │ └─ self.attn_mode = attn_mode │ │ │ │ │ │ │ └─ if i % 2 == 0: self.q_fc.append( nn.Sequential()) │ │ │ └─ else: self.q_fc.append(nn.Identity()) # nn.ModuleList() │ │ │ │ │ ├─ self.pos_enc = Embedder() # 21x3=63 funcs │ │ └─ self.view_enc = Embedder() # 21x3=63 funcs │ │ │ ├─ self.net_fine = GNT().to(device) │ ├─ self.feature_net = ResUNet(coarse_out_ch, fine_out_ch, signle_net).to(device) │ │ ├─ self.conv1 = nn.Conv2d() │ │ ├─ self.bn1 │ │ ├─ self.relu │ │ ├─ self.layer1 = self._make_layer(block,planes,blocks,stride,dilate) │ │ │ ├─ norm_layer │ │ │ ├─ previous_dilation │ │ │ ├─ if dilate: self.dilation *= stride │ │ │ ├─ if stride !=1 or inplanes differs from outchanl * expansion: │ │ │ │ └─ downsample = nn.Sequential(conv1x1(), norm_layer()) │ │ │ │ │ │ │ ├─ layers.append(BasicBlock(self.inplanes, planes, stride,downsample,)) # list │ │ │ │ ├─ norm_layer │ │ │ │ ├─ self.conv1 │ │ │ │ ├─ self.bn1 │ │ │ │ ├─ self.relu │ │ │ │ ├─ self.conv2 │ │ │ │ ├─ self.bn2 │ │ │ │ ├─ self.downsample │ │ │ │ └─ self.stride │ │ │ │ │ │ │ ├─ self.inplanes = planes * block.expansion │ │ │ └─ for _ in range(1,blocks): │ │ │ └─ layers.append(BasicBlock(self.inplanes,planes,)) │ │ │ ├─ norm_layer │ │ │ ├─ self.conv1 │ │ │ ├─ self.bn1 │ │ │ ├─ self.relu │ │ │ ├─ self.conv2 │ │ │ ├─ self.bn2 │ │ │ ├─ self.downsample │ │ │ └─ self.stride │ │ │ │ │ ├─ self.layer2 │ │ ├─ self.layer3 │ │ ├─ self.upconv3 │ │ ├─ self.iconv3 │ │ ├─ self.upconv2 │ │ ├─ self.iconv2 │ │ └─ self.out_conv │ │ │ ├─ learnable_params │ ├─ self.optimizer = torch.optim.Adam() │ ├─ self.scheduler │ ├─ self.start_step │ └─ if args.distributed: │ ├─ self.net_coarse = torch.nn.parallel.DDP │ ├─ self.feature_net = torch.nn.parallel.DDP │ └─ self.net_fine = torch.nn.parallel.DDP │ ├ switch_to_eval(self): │ └─ render_kwargs_train{network_query_fn, model, ...} ├ switch_to_train(self): ├ save_model(self, filename): ├ load_model(self, filename): └ load_from_ckpt(self, out_folder): │ () ├─ h In a new file: model_parallel.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class GNTModel(object): ├ __init__(self, args, load_opt=True, load_schedular=True): │ ├─ device = [\u0026#39;cuda:0\u0026#39;,\u0026#39;cuda:1\u0026#39;,\u0026#39;cuda:2\u0026#39;,\u0026#39;cuda:3\u0026#39;] │ ├─ self.net_coarse = GNT(..., device) # transformer_network_parallel.py │ ├─ __init__(self, args, in_feat_ch=...) │ ├─ super(GNT, self).__init__() │ ├─ self.rgbfeat_fc = nn.Sequential().to(device[0]) │ ├─ │ │ │ │ │ │ │ │ │ │ │ │ In train.py:\n1 from gnt.model_parallel import GNTModel Pytorch-DDP-RPC Invited Talk: PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li ytb\n(DDG search: tensorflow model split distributed parallel)\n","date":"2022-10-21T20:35:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_model_parallel/","title":"memo: PyTorch | Model Parallel"},{"content":"Elliot Waite Source video: PyTorch Autograd Explained - In-depth Tutorial-Elliot Waite\nTwo tensors are created with the values of 2 and 3 and assigned to variables a and b. Then the product of a and b is assigned to variable c.\nIn the following diagram, each struct is a tensor containing several attributes.\ndata holds the data of the tensor grad holds the calculated gradient value grad_fn, gradient function points to a node in the backwards graph is_leaf marks is this tensor a leaf of a graph requires_grad is False by default for all tensors that are being input (like a,b) into or output (like c) from an operation. Such that no backwards graph will be created. However, if the attribute requires_grad of a is set to True and a is passed into any operation (Mul), the output tensor (c) will also have requires_grad=True and be apart of the backwards graph, because c is no longer a leaf. And c\u0026rsquo;s grad_fn points to MulBackward, which calculates the gardient of its operation w.r.t. input tensor a: ∂c/∂a = ∂(a∗b)/∂a = b\nThe blue contents are the backwards graph behind the tensors and their operations.\nWhen the Mul function is called, the ctx context variable will save the values to be used in the backwards pass: MulBackward operation.\nSpecifically, the input tensor a is stored by the method ctx.save_for_backward(...) and referenced by the property ctx.saved_tensors in the backward pass.\nThe MulBackward has another attribute next_functions is a list of tuples, each one associated with an input tensor that were passed to the Mul operation.\n(AccumulatedGrad, 0) corresponds to the input tensor a meaning the gradient of a will be calculated continuously by AccumulatedGrad operation. (None, 0) is associated with input tensor b, no further calculation is needed for its gradient. The AccumulateGrad operation is used to sum the gradients (from multiple operations) for the input tensor a.\nWhen executing c.backward(), the backward pass of gradients starts. The initial gradient is 1.0 and then passed into MulBackward, where it times b getting 3.0. Then by looking at the next_functions, it needs to get into AccumulatedGrad to obtain the gradient w.r.t. tensor a. Finally, the attribute grad of a comes from AccumulateGrad.\nSimple example Two input tensors are both requires_grad=True.\nThey\u0026rsquo;re multiplied together to get c.\nIf here executing c.backward(), the initial gradient 1.0 will start the backward pass from its grad_fn.\nThen tensor d is created to multiply with c to get e.\nIf executing e.backward() here, to calculate the gradient of e w.r.t. the leaft nodes on the graph, the backward pass is as follows:\nThe initial gradient 1.0 is first passed into e.grad_fn, i.e., MulBackward, where it multiplies with gradient of the operation w.r.t. the leaf nodes: ∂(c∗d)/∂d (=c=6.0). (Since c is not a leaf, ∂e/∂c doesn\u0026rsquo;t need to compute.)\nThen by looking at property next_functions, the gradients go continuously into MulBackward and AccumulateGrad separately.\nIn MulBackward, to get ∂e/∂a, the incoming gradient ∂e/∂c multiplies with gradient of the Mul operation w.r.t. a: ∂(a∗b)/∂a, so the gradient of leaf a is ∂e/∂a = ∂e/∂c × ∂(a∗b)/∂a = 4×3=12. Also to get ∂e/∂b, the incoming gradient ∂e/∂c multiplies with ∂(a∗b)/∂b, ∂e/∂b = ∂e/∂c × ∂(a∗b)/∂b = 4×2 = 8 While ∂e/∂c gets into AccumulateGrad, no other operations needed to calculate the gradients w.r.t. d, the grad of leaf node d is 6.0. Avoid In-place operation When the MulBackward operation retrieve input tensors through ctx.saved_tensors, it is necessary to ensure that the correct values are referenced. Therefore, each tensor must maintains its attribute _version, which will be increamented (+1) when performing in-place operation (e.g., c+=1) each time.\nThus, if calling e.backward() after c+=1, the ctx.saved_tensors will get an error, beacuse the attribute _version of the input tensor c is not matched with the previously saved one. In this way, the input tensors to be used are ensured haven\u0026rsquo;t changed in the time since the operation was performed in the forward pass.\nHowever, the Add function doesn\u0026rsquo;t affect the graidents, so it doesn\u0026rsquo;t need to save any its input tensors for the backward pass. Hence, the ctx will not store its input tensors. And the initial gradient will directly looking at the next_functions after getting into AddBackward node. In this case, doing c+=1 before e.backward(), no errors will occur because the input tensor c is not retrieved by ctx.saved_tensors.\nUnbind operation A tensor is created with 1-D list holding 3 values and assigned to variable a.\nThen by executing b,c,d = a.unbind(), tensor a is split along the 1st dimension and tensors b, c, d are created. This operation will generate the graph as follows:\nAll of the grad_fn of b, c, d point to the same UnbindBackward function.\nIf b, c, d are multiplied together to get e, there will be two Mul operation in the forward pass and two MulBackward is the backward pass.\nThe property next_functions of those 2 MulBackward functions both have tuple (UnbindBackward, i ), but their indecies are different, where i is 0, 1, 2 corresponding to the 3 outputs from the Unbind function.\n(UnbindBackward, 0) means the current gradient is associated with the first input tensor b of the (1st) Mul operation, which is also the first output from the Unbind function. (UnbindBackward, 1) is saying this is the gradient for the second output fromt he Unbind function. (UnbindBackward, 2) indicates this gradient is for the third output of the Unbind function. The index value is used to inform the UnbindBackward which output tensor the gradient is calculated for. Such that the UnbindBackward can output a list of gradients.\nThe gradient of the leat node can be calculated by calling e.backward(). The backward pass is started off with the initial gradient of 1.\nComplicated example The following scalar tensor can be replaced with any vector or matrix or any n-dimension array.\nThe tensors are created with the same value of 2 and they both don\u0026rsquo;t require grad.\na and b are multiplied together to get c, which doesn\u0026rsquo;t require grad too.\nThen by executing c.requires_grad = True, the tensor c will be a port of the backwards graph. So that any future operations done using c as an input will start to build the backwards graph.\nThe full forward graph is build as:\nAnother tensor d is created with requires_grad=False. Multiply d with c to get e. Another leaf f is constructed with requires_grad=False (not on the graph). Multiply f with e to get g Another tensor h is created with requires_grad=True (a leaf on the graph). Devide g by h to get i; Add i and h together to get j. (h is leaf and fed into 2 operations, so its AccumulteGrad has two inputs from DivBackward and AddBackward. Multiply j and i together to get k. (i is passed into both Add and Mul, so its (grad_fn) DivBackward has 2 streams from AddBackward and MulBackward. Unlike h has its 2 backward streams converge at AccumulateGrad, the 2 backward streams of i instead converge at the DivBackward that corresponds to the operation Div generating the i. (Yellow means the leaf that doesn\u0026rsquo;t on the graph. Green means the leaf on the graph. Brown means the non-leaf)\nAt the end, by executing k.backward(), the backward pass will start with a gradient of 1.0, which will times the gradient of each operation w.r.t. local input tensors sequentially from bottom to top. If an operation\u0026rsquo;s input is a leaf, the next_functions will point to the AccumulateGrad for this leaf node. Once all gradients streams are accumulated, the sum is put into the attribute grad of the left node. Finally, ∂k/∂leaf will obtained.\nretain_grad() By default, the gradients will only calculated for the leaf nodes, and the grad of the intermediate nodes are kept None. But an intermediate tensor can retain their gradient by calling its retain_grad() method. For example, i.retain_grad(), which will set up a hook \u0026ldquo;that gets called in the backward pass that will basically tell the DivBackward function that any gradients passed into it should be saved on the grad attribute of tensor i.\ndetach() m = k.detach() will create a new tensor sharing the same underlying data as k, but m will no longer require gradients. m will be a leaft node but not on the graph because its grad_fn is None without pointing any node on the graph.\nUsually, the backwards graph is expected to get garbage collected after finishing the training loop. Once the k.backward() is executed, some values have no references (get freed) in the graph. Specifically, the references to the saved_tensors. But the actual graph still exists in memory. If the output tensor k is needed to kept for longer than the training loop, k can be detached from the graph.\nSimilar functions:\nk.numpy() : ndarray k.item() : python int or python float k.tolist() : original tensor that holds multiple values will be converted to python list (old summary on 2022-10-19)\nThose tensors whose attribute requires_grad=True will be nodes on a backwards graph. Any output tensor yielded from any operation is not leaf node.\ngrad_fn \u0026amp; grad The grad_fn of the \u0026rsquo;non-leaf\u0026rsquo; (intermediate) node points to a node (like MulBackward) which will multiply the incoming gradient by the gradient of this operation w.r.t. its inputs (leaf nodes with requires_grad=True), or pass the grad of the \u0026rsquo;non-leaf\u0026rsquo; node up to next gradient-computing node for other leaf nodes before. This\u0026rsquo;s like the divergence of ∂L/∂wᶦ and ∂L/∂wᶦ⁻¹.\nIf an \u0026ldquo;non-leaf\u0026rdquo; node is passed into multiple operations, its gradient equals the sum of the gradients coming from each operation. Its gradient will be fed into the \u0026ldquo;gradient-computing\u0026rdquo; node as the incoming gradient of the operation that generates it.\nWhile if a leaf node is used by multiple functions, its gradient equals to the sum of the gradients comeing from the \u0026ldquo;gradient-computeing\u0026rdquo; nodes of every operation. Hence, the grad of the leaf node is accumulated by AccumulateGrad function.\nctx ctx (context) stores the operators doing the operation for computing the gradient in backward graph. The gradients at different time are different, so the version of variables needs to be recorded. The in-place operation will increment the version of the variable, and then calling the backward method will cause an error due the mismatch of the _version value.\nHowever, if an operation doesn\u0026rsquo;t bring gradients (like Add), its operators are not necessary to store. The incoming gradient will not change when passing through its corresponding backwards function (AddBackward). In this case, modifying its operators before calling .backward() is okay, becaue they are not involved with gradient.\nGradient for a one-dim tensor is a list of partial derivative. The second value in the tuple is the index among the mutliple outputs of the operation, indicating to who the gradient belongs\noutput don\u0026rsquo;t have grad by default The \u0026ldquo;non-leaf\u0026rdquo; node will not store grad by default, unless set its .retain_grad(), from which a hook can be set up to make its grad_fn to save any gradient passed to it into grad of this node.\ndetach() m=k.detach() will create a leaf node m sharing the same data as k, but will no longer require gradient. Its grad_fn=None meaning it doesn\u0026rsquo;t have a reference to the backwards graph. This operation is used to store a output value separately, but free the backwards graph after a training loop (forward+backward). Alternatives for detaching from the graph:\nk.numpy(), k.item() convert one-element tensor to python scalar. k.tolist(). (2022-11-01)\ngradient is the upstream gradients until this calling tensor (from the intermediate node to the network ouput). Since the explicit expression of output is unknown, the derivative $\\rm\\frac{ d(output) }{ d(input) }$ cannot be calculated directly. But the derivative $\\rm\\frac{ d(output) }{ d(intermediate) }$ and $\\rm\\frac{ d(intermediate) }{ d(input) }$ are all known. Therefore, by passing the d(output)/d(intermediate), say 0.05, to gradient, the d(output)/d(input) is the returned value of intermeidate.backward(gradient=0.05,). The gradient argument in PyTorch backward - devpranjal Blog (2023-08-02)\nFreeze params module.requires_grad_(False) will change all its parameters. PyTorch Forum\nIf I explicitly add the learnable params one-by-one, like this tutorial (if p.requires_grad), only the ordinal of the layers in the state dict ckpt['optimizer]['state'] changed, while the total number of states remains the number of layers to be performing gradient descent.\n1 2 3 4 5 6 7 # Test in GNT with multiple encoders: self.optimizer.add_param_group({\u0026#34;params\u0026#34;: self.multiEncoders.parameters()}) # Saved `ckpt[\u0026#39;optimizer\u0026#39;][\u0026#39;state\u0026#39;]` has a max index: 1390, but total 262 layers. params = [p for p in self.multiEncoders.parameters() if p.requires_grad] self.optimizer.add_param_group({\u0026#34;params\u0026#34;: params}) #`ckpt[\u0026#39;optimizer\u0026#39;][\u0026#39;state\u0026#39;]` has a max index: 281, but total 262 layers too. with torch.no_grad(): will stop backward-propagation for a block of operations wrapped in its region (in a context). This is equivalent to module.requires_grad_(False). Demo-SO\nInference mode is similar with no_grad, but even faster. module.eval() = module.train(False) affects the settings of nn.Dropout and nn.BatchNorm2d. And it has nothing to do with grad. Docs - Autograd\nIn pixelNeRF, self.net.eval() and self.net.train() will jump to SwinTransformer2D_Adapter.train(mode=True) requires_grad_ vs requires_grad requires_grad_ can do for non-leaf node, while requires_grad will have error. PyTorch Forum\nTwice nn.Parameter (2024-04-11)\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from torch import nn from torchviz import make_dot x = torch.rand(2,3) w = torch.rand(3,2).requires_grad_(True) for i in range(2): f = nn.Parameter(w @ x) # shape (3,3) loss = f.sum() loss.backward() # dot = make_dot(loss, params={\u0026#39;w\u0026#39;: w, \u0026#39;x\u0026#39;: x}) # dot.render(\u0026#39;computational_graph\u0026#39;, format=\u0026#39;png\u0026#39;) As shown above, once \u0026ldquo;re-convert\u0026rdquo; the w@x as the nn.Parameter, the forward graph doesn\u0026rsquo;t include w and the operation Matrixmultiplication. The graph is starting from f instead.\nTherefore, even though w is a leaf node and require_grad is True, it won\u0026rsquo;t obtain .grad: w.grad is None.\nIn contrast, do not re-set the nn.Parameter won\u0026rsquo;t change the leaf tensors.\n1 2 3 4 5 6 7 import torch x = torch.rand(2,3) w = torch.rand(3,2).requires_grad_(True) for i in range(2): f = w @ x loss = f.sum() loss.backward() In this way, the leaf tensors are prepared outside the for loop (specifally forward()). And the leaf tensors (i.e., w) will be added onto the graph correctly during each iteration rebuilding.\nGraph in For Loop (2024-04-11)\nThe computational graph (of PyTorch 1.x) is build in each iteration, as the previous graph has been destroyed after executing .backward()\nTherefore, the tensors to be optimized, i.e., the leaf node of the graph, have to be present within the for loop as the starting point of the graph.\nIn the following code, w needs to be optimized.\n1 2 3 4 5 6 7 8 9 import torch x = torch.rand(2,3) w = torch.rand(2,3).requires_grad_(True) optimizer = torch.optim.Adam([w], lr=0.0001) for i in range(2): f = w * x loss = f.sum() loss.backward() optimizer.step() The forward graph can be plotted as:\nThe endpoint (green box) is the tensor loss The w has performed Multiplication and Sum to get loss The graph will be rebuilt in each iteration, with the w and x as the starting points. And the graph will be freed after loss.backward() optimizer.step() updates leaf tensors that require grad. Docs Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch from torchviz import make_dot x = torch.rand(2,3) w = torch.rand(2,3).requires_grad_(True) optimizer = torch.optim.Adam([w], lr=0.0001) for i in range(2): f = w * x loss = f.sum() loss.backward() optimizer.step() if i == 0: dot = make_dot(loss, params={\u0026#39;w\u0026#39;: w, \u0026#39;x\u0026#39;: x}) dot.render(\u0026#39;computational_graph\u0026#39;, format=\u0026#39;png\u0026#39;) However, if moving the operation f=w*x outside the for loop, the graph only contain an operation: f.sum()\n1 2 3 4 5 6 7 import torch x = torch.rand(2) # leaf w = torch.rand(2).requires_grad_(True) # leaf f = w * x # non-leaf tensor won\u0026#39;t have .grad for i in range(5): loss = f.sum() loss.backward() From the second run, the rebuilt graph only includes \u0026ldquo;Sum\u0026rdquo;, and no longer includes w (out of this graph). In other words, the w doesn\u0026rsquo;t belong to this newly rebuilt graph.\nTherefore, the .backward() method cannot be completed for w.\nw ∗ x = f r f e o b w u a i r f l d o d ( r ) t l h o e o p g r l a o p s h s e a c h i t e r In the second iteration, the graph doesn\u0026rsquo;t include w. So, when the gradient backpropagates, the gradient cannot reach w.\nOn the other hand, the w still points to the previous graph, without updating.\nError:\n1 2 3 4 5 6 7 File \u0026#34;/home/yi/Downloads/CppCudaExt_PT_Tut_AIkui/test_debug.py\u0026#34;, line 8, in \u0026lt;module\u0026gt; loss.backward() RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. (2024-06-07)\nQuestion description:\n对一个tensor的不同切片做可微运算，记循环中的可微运算为f1,f2,\u0026hellip;，但是在每次backward之后，计算图会套娃fi,例如f2(f1)\n把这个可微操作给包装进自定义的auto_func也会出现同样的问题\nMy answer:\n我理解是 for 的每次循环都会重新构建计算图，因为每次 .backward 之后，计算图就被销毁了， 所以 21 行计算 xyz 的矩阵乘是不是可以放到 for 循环里面？\nHe didn\u0026rsquo;t reply.\n","date":"2022-10-19T21:17:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-autograd/","title":"memo: PyTorch | Autograd"},{"content":"手写数字数据集有10个类别，\n如果对每一类别按照二分类问题（是/不是）计算概率的话，每种类别的概率互相独立，与真实情况中，各结果之间互相抑制的事实矛盾。\n样本的分类结果满足一个概率分布，这就要求属于各类的概率都要大于0，而且概率之和为1。\n二分类问题只需要计算一个概率（另一个是互补），所以十分类问题只需要计算9个概率，但是第10个分类的计算方式与前9个不统一，就导致需要构造一些额外的计算图处理特殊情况，无法最大化地实现并行计算，所以希望所有类别的概率运算处理都是一样的。\n所以除了最后一层，前面的层还是用sigmoid，最后一层用softmax激活函数，满足：\n$$ \\begin{cases} P(y=i) ≥ 0 \\\\ \\sumᵢ₌₀^9 P(y=i) =1 \\end{cases} $$假设 $Z^l \\in \\mathbb R^K$ 是最后一个线性层 $l$ 的输出，共有K个类别，则经过 Softmax函数，线性层的输出变成概率分布：\n$$ P(y=i) = \\frac{e^{Z_i}}{\\sum_{j=0}^{K-1} e^{Z_j}},\\ i\\in\\{0,\\cdots, K-1 \\} $$分子使用指数运算从而恒大于零，分母是各输出之和，实现归一化。\n对于二分类问题（样本标签Y=1,0）交叉熵：$-(1\\cdot log \\hat{Y} + 0\\cdot log(1-\\hat{Y}))$。\n对于三分类问题（样本标签Y=1,0,0），交叉熵：$-(1 \\cdot log \\hat{Y}₁+ 0 + 0)$\n不管有多少类，只有1项是非零的。零项对训练没有意义，所以损失函数直接写为：$Loss(\\hat{Y}, Y) = -Y log \\hat{Y}$\n例如最后一个线性层的输出为：\n$$ [^{\\_{0.2}} \\_{^{0.1} \\_{-0.1}}] \\overset{\\rm Exponent}{\\longrightarrow} [^{_{1.22}} _{^{1.11} _{0.90}}] \\overset{\\rm Divide\\ sum}{\\longrightarrow} [^{\\_{0.38}} \\_{^{0.34} \\_{0.28}}] \\overset{-Y log \\hat{Y}}{\\longrightarrow} Loss $$对预测值先求对数，再数乘以样本 label (-Y)，被称为Negative Log Likelihood Loss (NLLLoss)，用numpy实现此计算过程：\n1 2 3 4 5 6 import numpy as np y = np.array([1,0,0]) #样本标签 z = np.array([0.2, 0.1, -0.1]) #线性层的输出 y_pred = np.exp(z) / np.exp(z).sum() #预测值归一化 loss = (- y* np.log(y_pred)).sum() #取对数乘以-Y，就是NLLLoss print(loss) 如果把softmax函数也算到损失函数中，在pytorch中叫做交叉熵损失：Torch.nn.CrossEntropyLoss()。这样的话，神经网络的最后一个线性层不要做激活，直接传给交叉熵损失：\n1 2 3 4 5 6 import torch y = torch.LongTensor([0]) #长整型 (第0个类别) z = torch.Tensor([[0.2, 0.1, -0.1]]) #线性层输出 criterion = torch.nn.CrossEntropyLoss() #定义损失函数 loss = criterion(z,y) #计算损失 print(loss) Mini-Batch: batch_size=3。\n1 2 3 4 5 6 7 8 9 10 11 12 import torch criterion = torch.nn.CrossEntropyLoss() Y = torch.LongTensor([2,0,1]) #三条样本，分别属于第2类，第0类，第1类,用于索引真实类别对应的预测值 Y_pred1 = torch.Tensor( [0.1, 0.2, 0.9], #(2)classified [1.1, 0.1, 0.2], #(0)classified [0.2, 2.1, 0.1]) #(1)classified Y_pred2 = torch.Tensor( [0.8, 0.2, 0.3], #(0)misclassified [0.2, 0.3, 0.5], #(2)misclassified [0.2, 0.2, 0.5]) #(2)misclassified loss1 = criterion(Y_pred1, Y) #损失较小 0.4966 loss2 = criterion(Y_pred1, Y) # 1.2389 print(\u0026#34;Batch Loss1=\u0026#34;, loss1.data,\u0026#34;\\nBatch Loss2=\u0026#34;,loss2.data) MNIST Dataset 图像是28×28的矩阵。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## 引入包 import torch from torchvision import transforms #处理图像 from torchvision import datasets from torch.utils.data import DataLoader import torch.nn.functional as F #激活 import torch.optim as optim ## 准备数据 batch_size = 64 transform = transforms.Compose([ #把一系列对象组成一个pipeline transforms.ToTensor(), #把整数像素值0-255转变为图像张量：值0-1，维度：CxWxH (1x28x28)，方便卷积 transforms.Normalize((0.1307,), (0.3081,)) ]) #归一化，减去均值,除以标准差, 使所有的像素值满足0-1分布 train_dataset = datasets.MNIST(root=\u0026#39;../dataset/mnist/\u0026#39;, train=True, download=True, transform=transform) #读取数据时就做转变 train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size) test_dataset = dataset.MNIST(root=\u0026#39;../dataset/mnist/\u0026#39;, train=False, download=True, transform=transform) test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) #不打乱，每次测试顺序一样，方便对比结果 ## 设计模型 class Net(torch.nn.Module): def __init__(self): self.l1 = torch.nn.Linear(784, 512) #线性层把784维变成512维 self.l2 = torch.nn.Linear(512, 256) #将到256 self.l3 = torch.nn.Linear(256, 128) #将到128 self.l4 = torch.nn.Linear(128, 64) #将到64 self.l5 = torch.nn.Linear(64, 10) #将到10，输出(N,10)的矩阵 def forward(self, x): #向前计算输出 x = x.view(-1, 784) #改变张量的形状，把一张图像变成一个二阶的张量（矩阵）784列，-1表示自动计算行数N x = F.relu(self.l1(x)) #输入l1，对输出做激活 x = F.relu(self.l2(x)) x = F.relu(self.l3(x)) x = F.relu(self.l4(x)) return self.l5(x) #最后一个线性层不激活 model = Net() ## 构造损失和优化器 criterion = torch.nn.CrossEntropyLoss() #经过softmax，求对数，乘以-Y optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) #模型较大，用冲量 ## 训练和测试 def train(epoch): #一轮训练的运算 running_loss = 0.0 for batch_idx, data in enumerate(train_loader, 0): #取出训练样本 inputs, target = data #取出样本和标签 optimizer.zero_grad() #梯度清零 outputs = model(inputs) loss = criterion(outputs, target) #前馈:计算输出和损失 loss.backward() #反馈 optimizer.step() #更新一步权重 running_loss += loss.item() #累计损失 if batch_idx %300 == 299 #每300批（因为从0开始数）输出一次loss print(\u0026#39;[%d, %5d] loss: %.3f\u0026#39; % (epoch+1, batch_idx + 1, running_loss/300)) running_loss = 0.0 def test(): correct = 0 total = 0 with torch.no_grad(): #不需要反向传播，就不需要计算梯度 for data in test_loader: images, labels = data #取出测试样本及其标签 outputs = model(image) #计算预测值 Nx10 的矩阵 _, predicted = torch.max(outputs.data, dim=1) #找出每一行中最大值的下标, 即所属类别，和它的值。dim=1表示沿着行方向寻找（0是列方向） total += labels.size(0) #测试样本总数N correct += (predicted == labels).sum().item() #正确分类的个数 print(\u0026#34;Accuracy on test set: %d %%\u0026#34; % (100*correct /total)) if __name__ == \u0026#39;__main__\u0026#39;: for epoch in range(10): #训练10轮 train(epoch) if epoch %10 ==9: test() ","date":"2022-10-18T22:11:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/9_%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/","title":"watch: PyTorch - 刘二 09 | Multi-classification Task"},{"content":"Source video: “随机梯度下降、牛顿法、动量法、Nesterov、AdaGrad、RMSprop、Adam”，打包理解对梯度下降法的优化\n梯度下降法在实际应用中的优化：\n减小每次梯度下降的计算量：随机（分批次）梯度下降 减少迭代次数，即优化下降路径 随机梯度下降 对于一个凸问题，时间复杂度与误差量级的关系：\n对于一个强凸问题，能收敛得更快\n正常情况下，标准梯度下降法（1个batch）应该比随机梯度下降法快，但可以证明，不会快过 1/k\n一个 batch 上的损失函数，可能与整个数据集上的损失函数不同，各处对应的梯度也不同， 所以每次迭代时的梯度方向不一定是“全体数据的误差函数”上的最优，每一步的行进可能会偏离下降最快的最优路径，从而导致需要更多次的迭代，才能到达极值点。\n如下图，从 A 点 到 A\u0026rsquo; 点的最优路径是橙色线，如果分两步，先走到 B ，B 再沿着它的梯度方向走，就走偏了。\n减小步长，可以让下降路径更贴近最优下降路径，但是计算量太大。\n牛顿法 牛顿法是用来拟合曲线的，在梯度下降中，就是拟合损失函数表面上的最优下降路径对应的曲线。\n对于一个只有一维变量的问题，纵轴是各变量取值对应的误差，蓝色曲线即是损失函数。 要到达损失函数的最小值处，根据梯度下降法，先求出损失函数在当前点的梯度（各个方向分量，按向量加法相加），这里只有一个变量（一个方向），就是求损失函数的切线。 然后变量沿着梯度（切线）方向移动一点，看看此时的误差值。\n抛物线比直线更贴近损失函数，从而使下降路径与损失函数更贴合，而不是折线。 因为整个数据集上的损失函数未知，每下降一步，就在当前点的邻域范围内做泰勒展开，用一段高次函数对损失函数做近似代替； 又因为是找下降的方向，所以要保留到二次项，这样就能 拟合出在损失函数（表面）上的下降路径。 牛顿法的每一步是确定的：抛物线的顶点对应的横坐标就是这一步要走到的位置，所以牛顿法里没有步长。 下降的方向就不是梯度的方向了?\n如上图，灰色的直线是到极值点的最优路径，但是未知。 牛顿法希望每一步都走在损失函数上，即拟合出损失函数（表面）上的最优下降路径。\n泰勒展开保留二阶导，用二次函数近似表达损失函数上的每一点：\nf(x) = J(a₀) + J\u0026rsquo;(a₀)(x-a₀) + 1/2 J\u0026quot;(a₀)(x-a₀)²\n求 f(x) 的极值，就是求顶点所在位置，令 f\u0026rsquo;(x) = 0:\n$$ f'(x) = 0 + J'(a₀) + J\"(a₀)(x-a₀) = 0 \\\\\\ x = a₀ - J'(a₀) / J\"(a₀) $$然后让变量走到顶点的位置，对应于权重更新：W = W - J\u0026rsquo;(a₀) / J\u0026quot;(a₀)，没有学习率η。 用一阶导数与二阶导数的比值对（一元）变量W做更新。path-int\n直观理解：按固定步长做梯度下降法是橙色线，步长无穷小时，下降路径是灰色线，牛顿法的下降路径是绿色线，比梯度下降法更贴近最优路径的灰色线。 所以牛顿法是在拟合最优路径对应的曲线。\n对高维函数求二阶偏导，需要算 Hessian 矩阵。对于高维的损失函数 J(W)，参数更新公式为： 𝐖 = 𝐖 - 𝛁𝐉²(𝐖)⁻¹ ⋅ 𝛁𝐉(𝐖)，其中 𝛁𝐉²(𝐖) 就是 Hessian 方阵 𝐇(𝐖)\n$$ 𝐇(𝐖) = \\begin{bmatrix} \\frac{∂}{∂W₁}\\frac{∂f}{∂W₁} \u0026 \\frac{∂}{∂W₂}\\frac{∂f}{∂W₁} \u0026 ... \u0026 \\frac{∂}{∂Wₙ}\\frac{∂f}{∂W₁}\\\\\\ \\frac{∂}{∂W₁}\\frac{∂f}{∂W₂} \u0026 \\frac{∂}{∂W₂}\\frac{∂f}{∂W₂} \u0026 ... \u0026 \\frac{∂}{∂Wₙ}\\frac{∂f}{∂W₂}\\\\\\ ... \\\\\\ \\frac{∂}{∂W₁}\\frac{∂f}{∂Wₙ} \u0026 \\frac{∂}{∂W₂}\\frac{∂f}{∂Wₙ} \u0026 ... \u0026 \\frac{∂}{∂Wₙ}\\frac{∂f}{∂Wₙ}\\\\\\ \\end{bmatrix} $$列向量更新公式：𝐖ₙₓ₁ = 𝐖ₙₓ₁ - 𝐇(𝐖)⁻¹ₙₓₙ ⋅ 𝛁𝐉(𝐖)ₙₓ₁\n虽然迭代次数比梯度下降法少，但并不实用，缺点:path-int\n计算量太大，每一步都要计算n维向量的一阶差分 O(n)，Hessian矩阵 O(n²)，及其逆 不能保证目标函数值在迭代过程中一直下降，可能会先升高，再下降； 不能保证收敛：因为牛顿法使用二阶泰勒展开近似，需要初始点在极小点附近，每一步都满足近似条件（Hessian矩阵是正定的），效果才会比较好。 如果离得很远，获得的结果可能非常奇怪。一般应用其他方法先搜索到极小点附近，再用牛顿法（或拟牛顿法）来继续更高精度的搜索。 如果目标函数 f(x) 只是一阶可微，二阶不可微（Hessian矩阵不存在），牛顿法就不适用了。 如果二阶可微，理论上牛顿法收敛速度比梯度法要快。牛顿法收敛阶数至少是2，梯度法收敛阶数最差情况下是1。 如果是一个多元凸函数，但是不是处处可导，Taylor近似展开不能适应，牛顿法不可应用。 若 H 不可逆，需要用 Levenberg-Marquadt 修正：加常量阵 λ𝐈，即给对角线加上足够大的值，使所有的 eigenvalue 都大于0，意味着在任何方向上的一阶导数都大于0. path-int 动量法 牛顿法同时考虑损失函数的所有维度，找出最优下降路径。\n梯度是一个多维的向量，可以把各个维度拆开（向量的分解），单独分析每个方向上的变化\n加权是要把维度分开考虑\n考虑下降两步，抵消相反方向的梯度 惯性 mv，力作用的效果？把速度抵消，减少震荡\n前几步梯度大，直接加的话，占主导，而且不准，用一个系数控制前后两个部分的权重就用 beta 和 1-beta （exponentially weighted moving average），越是先前发生的状态，乘以的（1-beta）次数越多，占比越来越小，对当前的值影响越小\nNesterov 等高线坐标系下的点是权重W，Nesterov 把上一时刻的权重按照上一时刻的梯度方向走了一步，没有与当前时刻的梯度加权，把权重点移动到的新位置，这个位置是比如果采用加权时的权重超调一些的，把新位置的梯度带回去，也就是考虑了下一步位置的情况\n当前梯度方向d1，历史梯度方向d2，未来梯度方向d3，共同决定向哪个方向走。未来就是先按d2走一步梯度下降，走到一个点，计算那个点的梯度。不是按d1和d2的加权方向走，那样就是提前做了决策，还把决策的结果拿过来用了，应该用历史数据来决策\nAdaGrad 当前梯度除以累计历史梯度内积再开根号，梯度的各方向分量数量级不一致，所以除以累计历史梯度模长，梯度值大的那个方向，除以一个数（可能与该方向数值在同一数量级）就没那么大了，不至于在那个方向上走太多，而且分母越加越大，梯度越往后越小。 稀疏数据集：只要关心维度/特征的有无就能把类别分开，而不需关心特征是否明显（猴子和人）；非稀疏数据：需要关注同一特征上的差别，在同一特征上的数值，才能分类（长尾猫和短尾猫） 稀疏数据不同样本没有共同的特征，不同特征的绝对值差异大，数值相减后差值大，梯度大\n维度越多，越不需要区分在同一特征上的数值差异，只需关心特征的有无，单位球内的点分散到各个维度上了，不再挤在同一维度上了，各个轴上的长度就不用那么长了，所以体积就变小了\n王木头解释成学习率的衰减不太合适\n梯度按泰勒公式展开，牛顿法用了二次项，动量法修正0次项基础值，AdaGrad修正1次项（梯度变化量）\n","date":"2022-10-02T12:07:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/08_%E6%94%B9%E8%BF%9B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/","title":"watch: DL - 王木头 08 | Advanced Gradient Descent"},{"content":"10“拉格朗日对偶问题”如何直观理解？“KKT条件” “Slater条件” “凸优化”打包理解\n10.1 拉格朗日乘数法 一种寻找多元函数在其变量受到一个或多个约束条件时的极值的方法（自变量的取值范围有限制）\n将一个有n个变量与k个约束条件的最优化问题转换为一个解有 n+k 个变量的方程组的解的问题。\n对每个约束条件用拉格朗日乘子(待定系数) λᵢ 加权，加到目标函数后面，就是拉格朗日函数。 拉格朗日函数与目标函数的最值是一样的，所以求目标函数的最值就转化为求拉格朗日函数的最值。\n$$ \\begin{aligned} 目标函数求最小值：\u0026 min\\ f_0(𝐱),\\quad 𝐱 ∈ ℝⁿ \\\\\\ m个约束条件：\u0026 s.t. \\quad f_i(𝐱) ≤ 0,\\ 其中 i=1,2,3...m \\\\\\ 拉格朗日函数：\u0026 L(𝐱,\\pmb{λ}) = f_0(𝐱) + \\sum λ_i f_i(𝐱) \\end{aligned} $$ 目标函数被加上了约束条件，变量只能在规定的范围内取值，求导得到的极值点可能不在规定范围内，而在规定范围内可能也没有极值点。拉格朗日乘数法把带约束问题转化为无约束问题。\n每个约束条件用拉格朗日乘子加权\n10.2 用梯度理解Lagrange Multiplier 一个约束条件的情况：\n求 $f(x,y)$ 的最小值，并且有一个约束条件 $y=g(x)$。\n该问题的拉格朗日函数为：$L(x,y) = f(x,y) + λ(y-g(x))$；[约束条件=0 表示一条线]\n对拉格朗日函数求梯度，梯度等于0的点就是极值对应的点；\n把 $∇ L(x,y) = 0$，沿x,y两个方向展开，调整λ使两个方向上的偏导都为0，也就是梯度在两个方向上的分量都为0：\n$$ \\begin{cases} \\frac{∂f(x,y)}{∂x} + λ\\frac{∂(y-g(x))}{∂x} = 0 \\\\\\ \\frac{∂f(x,y)}{∂y} + λ\\frac{∂(y-g(x))}{∂y} = 0 \\end{cases} $$ 1 2 在图1中，坐标系是x-y，同心圆是目标函数 $f(x,y)$ 的等高线，圆心点对应的函数值最小，越往外值越大。红线是(x,y)约束条件。\n图2显示了两个函数的梯度，只有在相切的位置，目标函数的梯度方向与约束条件的梯度方向才是共线的，再通过拉格朗日乘子 λ 调整向量长短，使两个梯度相加才可能为零。除了相切点位置，都无法实现两梯度之和为零。\n多个约束条件：\n$$ \\begin{aligned} 目标函数：\u0026 min \\ f(𝐱), \\quad 𝐱∈ ℝⁿ \u0026 \\text{(𝐱 是个n维向量)}\\\\\\ m个约束条件：\u0026 s.t. \\quad g_i(𝐱) = \\pmb{a_i^T} ⋅ 𝐱 + b_i ≤ 0, \u0026\\text{(m个超平面围成的区域)}\\\\\\ \u0026 其中i=1,2,3...m, \\pmb{a_i} ∈ ℝⁿ, \\ b_i ∈ ℝⁿ \\\\\\ 拉格朗日函数：\u0026 L(𝐱, \\pmb λ) = f(𝐱) + \\sum λ_i g_i(𝐱) \\end{aligned} $$假设有5个一维的约束条件，就是5条直线，并且假设它们围成了一个五边形，还要假设五边形的内部是5个约束条件同时满足的区域，如下图1。\n(对于二维平面上的一条直线:$y=ax+b$，想知道 $y≤0$ 表示的是哪块区域，可以根据(x,0)这个点判断。 $y=ax+b ≤0 \\Rightarrow x≤\\frac{-b}{a}$， 所以在$\\frac{-b}{a}$左侧就是规定的区域。 如果a是负数，符号就会改变，从而多条直线能围出一个封闭的限制区域。 )\n1 2 对拉格朗日函数求梯度，令其等于零：\n$$ \\begin{aligned} \\pmb ∇ L(𝐱,\\pmb λ) = 0 \\Downarrow -\\pmb ∇ f(\\pmb λ) = ∑ λ_i ⋅ \\pmb∇ g_i(𝐱) \\end{aligned} $$目标函数梯度的反方向应等于所有约束条件的梯度加权和。\n由图2可知，真正起贡献的只有两个约束条件在，只需求两个约束条件的梯度和，五边形内部是 ≤ 0，则外部是\u0026gt;0，所以梯度方向指向外面，两个梯度通过 λ 调节成与目标函数梯度等大反向.\n两约束条件的交点 x\u0026rsquo; 既满足这两个约束条件，在这点的梯度=0：\n$$ \\begin{aligned} \u0026 g_α(x') = g_β(x') = 0 \\\\ \u0026 λ_α ∇ g_α(x') + λ_β ∇ g_β(x') = -∇ f(x) ⇒ λ_α, λ_β≠0 \\end{aligned} $$而该点 x\u0026rsquo; 也满足其他约束条件 gᵢ(x\u0026rsquo;)\u0026lt;0，但它们的 λᵢ 都等于0，不起作用: λᵢ∇ gᵢ(x)=0 ⇒ λᵢ=0 (i≠α,β) λ 不能小于 0，否则会把梯度方向取反，会参与梯度叠加。\n所有的 λᵢ 都≥0，如果λᵢ=0, 它对应的约束条件 gᵢ(x) 是松弛的；如果λᵢ\u0026gt;0, 它对应的约束条件 gᵢ(x) 是紧致的。约束条件像皮筋，把最优解从最值点拽到了极值点。 当目标函数的最小值落在可行域范围内，则所有的约束条件都是松弛的。\n拉格朗日乘数法本质还是求导=0，梯度等于0的点是极值点，但不一定是最值点\nKKT 条件中的互补松弛\n","date":"2022-10-01T14:14:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/10_%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/","title":"watch: DL - 王木头 10 | Method of Lagrange Multipliers"},{"content":"Source video: 5-“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”\n信息量 事件发生 概率 的负对数 -log₂ p\n若$f(p)$ 被定义为信息量，要让体系自洽则需满足：\n$$ \\begin{aligned} f(p) \u0026 \\coloneqq 信息量 \\\\\\ f(p_1 ⋅ p_2) \u0026 = f(p_1) + f(p_2) \\end{aligned} $$对于阿根廷从八强打到冠军这件事，可以拆成两件事：阿根廷进入决赛+阿根廷赢了决赛。\n这两种描述的信息量是一样的：$f$(阿根廷夺冠) = $f$(阿根廷进入决赛) + $f$(阿根廷赢了决赛) $\\Rightarrow f(\\frac{1}{8}) = f(\\frac{1}{4}) + f(\\frac{1}{2})$\n另外还要满足事件间的概率关系：P(阿根廷夺冠) = P(阿根廷进入决赛) ⋅ P(阿根廷赢了决赛)\n所以选择 log 函数，可以满足自洽；从直观来看，发生概率越小，所含信息量越大， 而 log 函数是递增的，所以系数取 -1；而底数可以选 e，也可选 2。\n$$ f(x) \\coloneqq -log_2 x $$底数选2，相当于用抛硬币事件来衡量信息量。某事件的发生概率是 1/8，相当于抛3个硬币全部朝上的概率。并且以 2 为底计算出的信息量的单位是比特。类似地，输入 16 比特的数据就是把 16 个 0/1 确定下来\n熵 系统中各事件信息量的 期望\n$$ \\begin{aligned} \u0026H(P) \\coloneqq E(P_f) \\\\\\ \u0026= ∑_{i=1}^m p_i ⋅ f(p_i) = ∑_{i=1}^m p_i(-log_2p_i) \\\\\\ \u0026= -∑_{i=1}^m p_i ⋅ log_2 p_i \\end{aligned} $$ 衡量整个系统中的所有事件的不确定性\nKL散度 (相对熵) 系统Q相对于系统P差多少：事件 i 在两系统中的 信息量之差，按照 i 在系统 P 中的概率加权求和。\n$$ D_{KL}(P\\\\|Q) = ∑_{i=1}^m p_i ⋅ (-log_2 q_i) - ∑_{i=1}^m p_i ⋅ (-log_2 p_i) $$ 无法直接对比两个不同种类模型之间的差异（无法公度），而且人脑中的概率模型不清楚，无法求熵，需要 相对熵\nQ系统P系统的概率分布\n对于某事件 $i$ 在系统 Q 中的信息量 $f_Q(q_i)$ 减去它对应到在系统 P 中的信息量 $f_P(p_i)$，再按照在系统 P 中的概率 $p$ 求期望：\n$$ \\begin{aligned} D_{KL} (P \\\\| Q) \u0026 \\coloneqq ∑_{i=1}^m p_i ⋅ \\left(f_Q(q_i) - f_P(p_i)\\right) \\\\\\ \u0026= ∑_{i=1}^m p_i ⋅ \\left((-log_2 q_i) - (-log_2 p_i)\\right) \\\\\\ \u0026= \\underbrace{∑_{i=1}^m p_i ⋅ (-log_2 q_i)}_{交叉熵H(P,Q)}- \\underbrace{∑ᵢ₌₁ᵐ pᵢ⋅ (-log₂pᵢ)}\\_{P的熵} \\end{aligned} $$如果事件 i 在两系统中的信息量相等，差值为0，说明两个系统完全相等。\n根据吉不斯不等式：\n如果有两个概率系统，$∑_{i=1}^n p_i = ∑_{i=1}^n q_i = 1$，且 $p_i, q_i \\in (0,1]$，则有：\n$$ \\- ∑_{i=1}^n p_i log_{p_i} ≤ -∑_{i=1}^n p_i log_{q_i} $$当且仅当 $p_i = q_i\\ ∀ i$ 时，等号成立。\n也就是说 KL 散度恒大于等于0（距离）。\n交叉熵 $$ H(P,Q) = ∑_{i=1}^m p_i ⋅ (- \\operatorname{log}_2 q_i) $$ 事件 i 在概率系统 Q 中的信息量，按照 i 在系统 P 中的概率加权求和\n为了使 KL 散度最小，即概率系统 Q 最接近系统P，就需要交叉熵最小（最接近系统P的熵）\n对于一张图片，人脑系统（目标系统 P）只有 2 个事件：是猫和不是猫 (\u0026ldquo;是猫\u0026quot;事件发生的概率为 x，则\u0026quot;非猫\u0026quot;事件发生的概率为 1-x)， 神经网络（系统Q）的结果（y）是像猫的概率，所以交叉熵为：\n$$ \\begin{aligned} H(P,Q) \u0026= ∑_{i=1}^2 p_i ⋅ (-log_2 q_i) \\\\\\ \u0026= x ⋅ log(y) + (1-x) ⋅ log(1-y) \\\\\\ \u0026= 1 ⋅ log(y) + 0 ⋅ log(1-y) \\end{aligned} $$ ","date":"2022-09-30T13:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/05_%E4%BA%A4%E5%8F%89%E7%86%B5/","title":"watch: DL - 王木头 05 | Info Quantity \u0026 Cross Entropy"},{"content":"4-“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”\n损失函数 定量衡量两个概率模型的差异 三种方法： 最小二乘法 极大似然 交叉熵 最小二乘法 直接比较判断结果。\nmin $\\sum_{i=1}^n (x_i - y_i)^2$\n人脑中的概率模型无法准确说出，而神经网络的概率模型蕴藏在参数里面，没有统一的表达？只能从结果入手。\n将每次人的判断结果 $x_i$ (1/0) 与 神经网络的判断结果 $y_i$ (45%:1, 55%:0) 的误差 $|x_i -y_i|$ 求和取最小，从而保证在结果上看是最接近的。\n$$ min \\sum_{i=1}^n |x_i - y_i| $$因绝对值在定义域上不是全程可导的，求平方不影响 x 和 y 的大小关系，而且全程可导，加1/2是为了求导方便。\n用最小二乘法作为损失函数，使用梯度下降法很麻烦\u0026hellip;\n极大似然 对于已发生的现实事件，有很多概率模型都能导致这个情况发生，取似然值最大的那个概率模型作为最“真实的”概率模型。\n由于噪声的存在，现实世界中的概率模型偏移了理想世界中的模型，由于理想世界与现实世界之间有次元壁，无法直接知道理想世界中真实的概率模型，所以只能从现实世界反推，估计出一个概率模型，它使该现实事件发生的可能性最大。\n比如掷10次硬币的结果是7次正面，3次反面。有3种概率模型：\n概率统计模型 θ 正面 反面 1 0.1 0.9 2 0.7 0.3 3 0.8 0.2 这三种概率模型都可以掷出7次正面，3次反面。不过第2种概率模型掷出7正3反的概率（似然值）最大，所以认为第2种是最接近“真实的”概率模型。\n$$ P(C₁, C₂, C₃, ..., C_10 | \\theta) = \\prod_{i=1}^{10} P(C_i | \\theta) $$ 似然值：用可能导致现实事件发生的概率模型，计算出来的这种情况发生的概率值.\n用已经标注好的 n 张图片（硬币正反）去训练神经网络，神经网络的概率模型 (𝐖, 𝐛) 产生出这n张图片的概率就是模型的似然值：\n$$ \\begin{aligned} \u0026P(x_1, x_2, x_3, ..., x_n | \\mathbf {W,b}) \\\\\\ \u0026= \\prod_{i=1}^n P(x_i | \\mathbf{W,b}) \u0026 \\text{$x_i \\in \\{0,1\\}$,表示\"是猫\",\"不是猫\"}\\\\\\ \u0026= \\prod_{i=1}^n P(x_i | y_i) \u0026 \\text{𝐖,𝐛决定了模型（给出的\"是猫的概率\"$y_i$}） \\\\\\ \u0026= \\prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-{x_i}} \u0026\\text{采样服从0-1分布: $P=\\begin{cases} y_i, \u0026 x=1是猫 \\\\\\\\ 1-y_i, \u0026 x=0不是猫\\end{cases}$} \\end{aligned} $$比如有7张是猫，3张不是猫，并假设神经网络在 𝐖,𝐛 的参数下，判断是猫的概率是 45%，不是猫的概率是 55%。那么此模型的似然值=$(0.45)^7 (0.55)^3$\n连乘变连加：\n$$ \\begin{aligned} \u0026log \\left( \\prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-x_i} \\right) \\\\\\ \u0026= \\sum_{i=1}^n log(y_i^{x_i} (1-y_i)^{1-x_i}) \\\\\\ \u0026= \\sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i)) \\end{aligned} $$当似然值达到最大的时候，就是最接近人脑的模型。\n极大似然估计：$max \\sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i))$\n习惯求极小：$min - \\sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i))$\nx 是训练数据（从真实分布中采样得到的），有自己的分布（目标分布），希望模型输出的标签与输入的标签分布一致，所以 \u0026ldquo;让学习到的分布产生输入数据的概率最大 ∑log P(输入数据 | learned分布)\u0026rdquo; 与 \u0026ldquo;最小化两分布差异 ∑ P(目标分布) log(P(learned分布))” 等价，也就是极大似然与交叉熵等价。\n","date":"2022-09-30T13:26:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/04_%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","title":"watch: DL - 王木头 04 | Loss Functions"},{"content":"类似分布式系统中的概念 1\ngroup: 当前process group (world)，一个job是一个组。一个组里面有多个主机（node） world_size: 参与job（整个网络中）的进程个数 (or gpu个数 or 数据集被切成world_size份） rank: 当前主机的编号，group内各进程的标志符是从 0 到 world_size 的连续整数 local_rank: 为当前主机内的一个进程分配GPU（每台主机可以开启多个进程（执行同一份代码）） DP vs DDP Comparison between Dataparallel and DistributedDataParallel 8\nDataParallel is single-process, multi-thread, and only works on single machine (with multiple card). While DistributedDataParallel is multi-process and works for both single- and multi- machine training. DDP works with model parallel torch.distributed 使用流程 Refer to 2\n创建进程组:\n1 torch.distributed.init_process_group(backend=\u0026#39;nccl\u0026#39;, init_method=\u0026#39;env://\u0026#39;) 如果需要 group 内集体通信，用new_group 创建子分组\n创建DDP对象：\n1 2 ddp_model = torch.nn.parallel.DistributedDataParallel( net, device_ids=[args.local_rank], output_device=args.local_rank) net位于local_rank指定的gpu上\n为数据集创建sampler:\n1 train_sampler = DistributedSampler(train_set, num_replicas=world_size, rank=rank) 确保每个进程的 dataloader 只会 load 到整个数据集的一个特定子集，而不重复 3\n在每个主机上用命令 torch.distributed.launch 启动进程（如果已开启的进程未达到world_size，则所有进程会一直等待），开始训练\n恢复训练时，给第一主机的命令加上--resume\n销毁进程组：destory_process_group()\ninit_method 指定了各进程(主机)向 rank=0 的进程发送信息的(url)地址。\nTCP 方式需要指定 rank 0 进程的ip地址 (init_method='tcp://10.1.1.20:23456')，并且需要手动指定各进程的rank。\n第2种是使用一个在同组内各进程共享的文件交换信息, url应以file://开头+文件地址，例如 init_method=file:///mnt/nfs/sharedfile，不会自动删除。\n第3种（默认）从环境变量中读取配置：MASTER_PORT, MASTER_ADDR, WORLD_SIZE, RANK。 init_method=env:// 7\n使用tcp初始化，使用3台主机，执行3次命令:\nMulti-node multi-gpu All you need to do is 4\nCreate a process group by RANK and WORLD_SIZE (auto set from the command arguments nproc_per_node,nnodes,and node_rank of torchrun) Wrap the model by torch.nn.parallel.DistributedDataParallel() move the model to gpu through LOCAL_RANK Wrap the dataset by DistributedSampler() resnet_ddp.py 5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def train(): # read hyper params from command ... parser.add_argument(\u0026#34;--local_rank\u0026#34;, type=int, help=\u0026#34;Local rank. Necessary for using the torch.distributed.launch utility.\u0026#34;) # specify gpu index # initialize process group torch.distributed.init_process_group(backend=\u0026#34;nccl\u0026#34;) # construct model model = torchvision.models.resnet18(pretrained=False) # Wrap the model on the GPU assigned to the current process device = torch.device(f\u0026#34;cuda:{local_rank}\u0026#34;) model = model.to(device) ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) # only save (and restore) the model on the gpu whose local_rank=0 if resume == True: map_location = {\u0026#34;cuda:0\u0026#34;: \u0026#34;cuda:{}\u0026#34;.format(local_rank)} ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location)) # prepare dataset train_set = torchvision.datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True,transform=transform) # a process will only use its own subset train_sampler = DistributedSampler(dataset=train_set) tarin_loader = DataLoader(dataset=train_set, batch_size=128, sampler=train_sampler, num_workers=8) # loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5) # training cycle for epoch in range(1000): # check accuracy of the model on local_rank=0 # train mode ddp_model.train() # iter all train_set for data in train_loader: inputs, labels = data[0].to(device), data[1].to(device) outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.zero_grad() optimizer.step() if __name__ == \u0026#34;__main__\u0026#34;: train() The training script resnet_ddp.py will run on two nodes, and each of nodes has 8 gpus and each gpu would launch one process.\nIn the terminal of the first node, excute the following command.\n1 2 3 4 python -m torch.distributed.launch \\ --nproc_per_node=8 --nnodes=2 --node_rank=0 \\ --master_addr=\u0026#34;192.168.0.1\u0026#34; \\ --master_port=1234 resnet_ddp.py Excute the same command but with different node_rank on the second node:\n1 2 3 4 python -m torch.distributed.launch \\ --nproc_per_node=8 --nnodes=2 --node_rank=1 \\ --master_addr=\u0026#34;192.168.0.1\u0026#34; \\ --master_port=1234 resnet_ddp.py Single-node multi-worker Refer to 6\n1 2 3 4 torchrun --standalone --nnodes=1 \\ --nproc_per_node=$NUM_TRAINERS \\ YOUR_TRAINING_SCRIPT.py \\ (--arg1 ... train script args ...) (2023-08-27)\nExample of launch.json For running AIM\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Python: Current File\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;torch.distributed.launch\u0026#34;, \u0026#34;console\u0026#34;: \u0026#34;internalConsole\u0026#34;, \u0026#34;justMyCode\u0026#34;: true, \u0026#34;env\u0026#34;: {\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;: \u0026#34;4\u0026#34;}, \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;1\u0026#34;, // GPUs \u0026#34;--master_port\u0026#34;, \u0026#34;29500\u0026#34;, \u0026#34;tools/train.py\u0026#34;, \u0026#34;configs/recognition/vit/vitclip_base_diving48.py\u0026#34;, \u0026#34;--launcher\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;--test-last\u0026#34;, \u0026#34;--validate\u0026#34;, \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34; ] } ] } Pass env to args Settings below in \u0026ldquo;launch.json\u0026rdquo; don\u0026rsquo;t work:\n1 2 3 4 5 6 7 8 \u0026#34;env\u0026#34;: { \u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;GPUS\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;PORT\u0026#34;: \u0026#34;29500\u0026#34; }, \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;${env:GPUS}\u0026#34;, \u0026#34;--master_port\u0026#34;, \u0026#34;${env:PORT}\u0026#34;, May refer to\nLaunch.json: how to reference an environment variable Resolve Environment Variables in Args defined in launch.json for Debugging #91053 Ref PyTorch 多进程分布式训练实战-murphypei-githubio Pytorch 分布式训练 - 会飞的闲鱼的文章 - 知乎 Pytorch多机多卡分布式训练 - 谜一样的男子的文章 - 知乎 Multi node PyTorch Distributed Training Guide For People In A Hurry-Lambda PyTorch Distributed Training - Lei Mao TORCHRUN (ELASTIC LAUNCH) DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED GETTING STARTED WITH DISTRIBUTED DATA PARALLEL 【pytorch distributed】nccl 集合通信（collective communication）\n(2024-06-02)\n【pytorch distributed】accelerate 基本用法（config，launch）数据并行 - 五道口纳什\n","date":"2022-09-22T19:49:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_distributed/","title":"memo: PyTorch | Data Parallel"},{"content":"(Feature image from: latex · GitHub Topics · GitHub)\nBasics 文档页面旋转90度 Rotate a page 180 degrees in LaTeX\nFor the whole document:\n1 \\documentclass[landscape]{article} 交叉引用替换为文字 LaTeX/Hyperlinks\n用法：\n1 2 3 \\usepackage{hyperref} ... \\hyperref[label_name]{\u0026#39;\u0026#39;link text\u0026#39;\u0026#39;} \\ref{label_name} 只是数字\nCross-references post\n\\pageref{key} command, which prints the number of the page where the \\label{key} was inserted.\nBasic usage (2021-01-15)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 \\documentclass{article}\t%文档类型：文章 \\documentclass{book}\t%书 \\documentclass{beamer}\t%幻灯片格式 \\documentclass[UTF8]{ctexart}\t%ctexart支持中英文混拍，指定文档编码类型 \\documentclas[UTF8]{ctexbook}\t%对书籍排版 \\title{文章标题} \\author{文章作者} \\date{文档的修改日期} \\date{\\today}\t%自动生成当天日期 \\begin{document}\t%下面是正文，上面是前言，\\begin和\\end之间是环境/作用域，位于同一个环境中的内容将会共享相同的文字格式 \\maketitle\t%在当前位置生成文档的标题（前言区设置的信息） \\textbf{加粗文字}\t%bold font \\textit{斜体}\t%italic \\underline{加下划线} %两个回车是换段，一个回车是一个空格 \\part{书籍的第一部分} \\chapter{书籍的第一章} \\section{这是地一个章节} 你好！ \\subsection{子章节} 二级章节下的内容 \\subsubsection{这是一个三级章节} 三级章节下的内容 \\section{这是第二个章节} 第二章节下的内容 %插入图片 \\usepackage{graphicx}\t%引用包，包含了若干绘制图片的指令 \\begin{figure}\t%把图片嵌套到figure环境中，可以指定标题 \\centering\t%将图片居中显示 \\includegraphics[width=0.5\\textwidth]{图片名字可省略后缀}\t%在当前位置添加一张图片,图片宽度等于0.5倍的当前文本区域的宽度 \\caption{这是图片的标题} \\end{figure} %列表 \\begin{itemize}\t%无序列表的创建：列表中的每一个元素都需要以\\item开头 \\item 列表项1 \\item 列表项2 \\item 列表项3 \\end{itemize} \\begin{enumrate}\t%有序列表:前面带编号 \\item 列表项1 \\item 列表项2 \\item 列表项3 \\end{enumerate} %数学公式 质能方程：$E=mc^2$ \\begin{equation} E=mc^2 \\end{equation} \\[ E=mc^2 \\] \\over 是几分之几 \\[ d={k \\varphi(n)+1} \\over e \\] codecogs 可以测试公式 %表格 \\begin{table}\t%table环境设置标题 \\caption{表格的标题} \\center\t%表格居中显示 \\begin{tabular}{|c| c| c|}\t%有三列，每列都居中（centering），| 代表竖直边框。 % {l c c} 则表示第一列左对齐(left) % {p{2cm} c c}\t设置列宽2cm \\hline\t#水平边框 单元格1 \u0026amp; 单元格2 \u0026amp; 单元格3 \\\\ \\hline\\hline\t%双横线 单元格4 \u0026amp; 单元格5 \u0026amp; 单元格6 \\\\ \\hline 单元格7 \u0026amp; 单元格8 \u0026amp; 单元格9 \\\\ \\hline \\end{tabular} \\end{table} 引用 条目后显示页码 (2024-06-22)\nUse package backref.\n(Ask chatGPT4o: \u0026ldquo;How to add the page number where a reference gets cited behind the entry item automatically in latex\u0026rdquo;)\n\\usepackage[backref=page]{hyperref}\n(Ref: Backref package for page reference Found in DDG)\n显示引用自某一页 (2024-06-22)\n1 2 3 4 5 6 7 In-text citation: (Moghaddam, 2018, p. 30) Reference: Moghaddam, F. M. (2018). Mutual radicalization: How groups and nations drive each other to extremes. https://doi.org/10.1037/0000089-000 Use package biblatex\nReferencing page number with only one reference (Found in DDG)\nWhen to Include Page Numbers in a Reference List Entry\nExample: biblatex - Formatting back references in bibliography - TeX\n公式 矩阵每行加 label 1 \\usepackage{blkarray} Example: overleaf DIP assign 3\nSuitable for R markdown Label rows and columns of a matrix in R Markdown with Latex and HTML rendering\nVector arrow (2024/06/21)\nSearch: \u0026quot;\u0026quot; in DDG\nUse xshlongvec\nExtensible \\vec instead of \\overrightarrow\nFonts Bold and italic\n(2024-06-23)\n\\bm Bold italic vectors Caligraphy: \\mathcal, where \\cal f will make the entire line to be caligraphy.\nTwo different calligraphic font styles in math mode\n公式加框 (ChatGPT: Using \\textbf{}, \\textit{}, or \\boxed{} for Emphasis)\n1 \\boxed{E = mc^2} 长公式换行 (2024-06-23)\n\\begin{aligned}\nHow to break a long equation?\n文字颜色 (2024/07/17)\nOverleaf Tutorial\nUse package xcolor:\n1 2 3 \\usepackage[dvipsnames]{xcolor} {\\color{ForestGreen} Sentences to be highlighted}. 论文新加内容用森林绿 ForestGreen，删除内容用砖红色 BrickRed\n1 2 3 {\\color{BrickRed} \\sout{Text to be deleted} } 两组公式并排 References:\nDeepSeek Notes:\n(2025/02/08)\nUse {array} environment\n$$ \\begin{array}{c|c} \\begin{aligned} x + y = 35 \\end{aligned} \u0026 \\begin{aligned} 2x + 4y = 94 \\end{aligned} \\end{array} $$ Use {align*} environment r1-DS:\n1 2 3 4 \\begin{align*} x + y \u0026amp;= 5 \u0026amp;\u0026amp; \\text{and} \u0026amp;\u0026amp; 2x - y \u0026amp;= 3 \\\\\\ a \u0026amp;= b + c \u0026amp;\u0026amp; \\text{or} \u0026amp;\u0026amp; d \u0026amp;= e - f \\end{align*} $$ \\begin{align*} x + y \u0026= 5 \u0026\u0026 \\text{and} \u0026\u0026 2x - y \u0026= 3 \\\\\\ a \u0026= b + c \u0026\u0026 \\text{or} \u0026\u0026 d \u0026= e - f \\end{align*} $$ 推导注释 References:\nDeepSeek Prompt: \u0026ldquo;What are the conventions for adding comments in formula derivations?\u0026rdquo; Notes:\n(2025/02/08)\nintertext doesn\u0026rsquo;t work in markdown\n1 2 3 4 5 \\begin{align} F \u0026amp;= ma \\\\ \u0026amp;= m \\cdot \\frac{dv}{dt} \\intertext{Since acceleration is the derivative of velocity} \u0026amp;= \\frac{d}{dt} (mv) \\end{align} $$ \\begin{align} F \u0026= ma \\\\ \u0026= m \\cdot \\frac{dv}{dt} \\intertext{Since acceleration is the derivative of velocity} \u0026= \\frac{d}{dt} (mv) \\end{align} $$ stackrel\n1 x \\stackrel{\\text{def}}{=} y + z $$ x \\stackrel{\\text{def}}{=} y + z $$ 删除线 Problems:\n\\cancel{} 是斜线，如何在文字上加 strikethrough References:\n元宝 | LaTeX 文字删除线实现方法 Notes:\n(2025-05-13T23:19)\nUse package ulem and command \\sout{}r1-元宝 代码 插入代码块 matlab Highlighting MATLAB Code in LaTeX with mcode\n高亮行内代码 (2024-06-20)\nUse package xparse\ninsert code keywords inline\nUse \\texttt{}: \\texttt{Under\\_scores\\_} need to be escaped.\nCode Snippet IN Text\nTable Table每行自动标号 Reference table row in LaTeX\n使用 counter\n1 2 3 4 5 6 7 8 9 10 11 12 13 \\usepackage{array} \\newcounter{rowcntr}[table] \\renewcommand{\\therowcntr}{\\arabic{rowcntr}} % A new columntype to apply automatic stepping \\newcolumntype{N}{\u0026gt;{\\refstepcounter{rowcntr}\\therowcntr}w{c}{0.1em}} % Reset the rowcntr counter at each new tabular % \\AtBeginEnvironment{tabular}{\\setcounter{rowcntr}{0}} ....... \\begin{tabular}{|N|c|c|c|} \\end{tabular} How does the \\newcolumntype command work?\nw{c}{0.1em} 是控制对齐和列宽\n长表格自动换页 latex long table automatic move to next page\n\\usepackage{longtable}\nMulti-page tables using \\longtable\n里面不能用 tabular 环境，不过据说它支持大部分tabular 特性\n1 2 3 4 \\begin{longtable}{|c|c|c|c|} Title \u0026amp; Nerf’s Problems \u0026amp; Solutions \u0026amp; Flaw \\endhead % 让表头出现在每页上 \\end{longtable} 表格内换行 cell 内部再用个 tabular 环境:\n\\begin{tabular}[t]{@{}p{\\linewidth}@{}}PlenOctrees for Real-time Rendering of Neural Radiance Fields \\\\ ICCV 2021 Oral \\end{tabular}\n\\newline ICCV 2021\n表格列宽 (2024/10/03)\nVertical: p, m,b, Horizontal: \\raggedright (left-align), \\centering, \\raggedleft (righ-align)\n1 2 3 4 5 6 7 8 9 10 11 \\usepackage{array} %----- \\begin{tabular}{p{0.2\\textwidth}\u0026gt;{\\centering}p{0.2\\textwidth}\u0026gt;{\\centering}p{0.2\\textwidth}\u0026gt;{\\centering\\arraybackslash}p{0.2\\textwidth}} \\hline \\multirow{2}{*}{Country}\u0026amp;\\multicolumn{2}{c}{Population}\u0026amp;\\multirow{2}{*}{Change (\\%)}\\\\\\cline{2-3} \u0026amp;2016\u0026amp;2017\u0026amp;\\\\ \\hline China\u0026amp;1,403,500,365\u0026amp;1,409,517,397\u0026amp;+0.4\\%\\\\ \\hline \\end{tabular} Ref: Control the width of table columns (tabular) in LaTeX - texblog (Searched by \u0026ldquo;latex table column width and align\u0026rdquo; in DDG) 1个cell内多行 (2024/10/03)\nUse package multirow\nRef: Control the width of table columns (tabular) in LaTeX - texblog (Searched by \u0026ldquo;latex table column width and align\u0026rdquo; in DDG) 缩放表格 References:\nIs there a way to slightly shrink a table, including font size, to fit within the column boundaries?\nMissing \\endgroup inserted. \\endgroup \\end{tabular}} when using \\scalebox - LaTeX Stack Exchange Searched by latex threeparttable Missing \\endgroup inserted. in DDG Use resizebox with math mode; Missing } inserted - TeX - LaTeX Stack Exchange Searched by latex resizebox Missing \\endgroup inserted. in DDG ChatGPT Prompted by I am using a double-column layout in Latex. How to resize a table to fix the width of a single column? Please do not use \\resizebox in Notes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \\usepackage{graphics} % ... \\begin{table} \\centering \\resizebox{\\columnwidth}{!}{% \\begin{tabular}{r|lll} \\multicolumn{1}{r}{} \u0026amp; \\multicolumn{1}{l}{Heading 1} \u0026amp; \\multicolumn{1}{l}{Heading 2} \u0026amp; \\multicolumn{1}{l}{Heading 3} \\\\ \\cline{2-4} Row 1 \u0026amp; Cell 1,1 \u0026amp; Cell 1,2 \u0026amp; Cell 1,3 \\\\ Row 2 \u0026amp; Cell 2,1 \u0026amp; Cell 2,2 \u0026amp; Cell 2,3 \\end{tabular}% } \\end{table} (2024/10/03)\nresizebox doesn\u0026rsquo;t compatible with Springer Nature template\nThe following snippet will run into errors:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \\begin{table}[htbp] \\centering \\caption{Number of Points} \\resizebox{\\textwidth}{!}{ \\begin{tabular}{llllllllllll} \\toprule Method \u0026amp; Scan1 \u0026amp; Scan4 \u0026amp; Scan9 \\\\ \\hline COLMAP \u0026amp; 388 \u0026amp; 449 \u0026amp; 91 \\\\ CasMVSNet \u0026amp; 0.63 M \u0026amp; 0.57 M \u0026amp; 0.42 M \\\\ \\hline Method \u0026amp; Scan32 \u0026amp; Scan33 \u0026amp; Scan34 \\\\ \\hline COLMAP \u0026amp; 322 \u0026amp; 110 \u0026amp; 332 \\\\ CasMVSNet \u0026amp; 0.45 M \u0026amp; 0.26 M \u0026amp; 0.61 M \\\\ \\bottomrule \\end{tabular} \\label{tab:n_points_dtu} } % curly brace of resizebox \\end{table} Error:\n1 2 3 4 Missing \\endgroup inserted. Missing } inserted. LaTeX Error: \\begin{threeparttable} on input line 440 ended by \\end{tabular}. Package graphics Error: Division by 0. Processing: I gave up using resizebox.\n(2024/11/13)\nDo not use \\resizebox r2-SE, r3-SE Use {tabularx} (Prompted by r4-Chat)\nDo not forget \\usepackage{tabluarx} 1 2 3 4 5 6 7 8 9 10 11 \\begin{table}[ht] \\centering \\begin{tabularx}{\\columnwidth}{XXX} \\hline Header 1 \u0026amp; Header 2 \u0026amp; Header 3 \\\\ \\hline Data 1 \u0026amp; Data 2 \u0026amp; Data 3 \\\\ \\hline Data 4 \u0026amp; Data 5 \u0026amp; Data 6 \\\\ \\hline \\end{tabularx} \\caption{Table caption} \\label{tab:example} \\end{table} label (2024-06-20)\nAdd label before \\end{table} labels with tabular\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \\documentclass{article} \\begin{document} Critical temperature for different Type I superconductors is given in Table~\\ref{Tab:Tcr} \\begin{table}[ht] \\caption{My Table} \\centering \\begin{tabular}{l l} material \u0026amp; T [K]\\\\ \\hline Sn \u0026amp; 3,7 \\\\ Pb \u0026amp; 7,2 \\\\ Al \u0026amp; 1,2\\\\ \\end{tabular} \\label{Tab:Tcr} \\end{table} \\end{document} Table Notes (2024-07-07)\nRef: Zeng\u0026rsquo;s thesis:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \\begin{table}[htbp] \\caption{Comparison with 3 MLS-based upsampling methods} \\resizebox{\\columnwidth}{!}{% \\begin{threeparttable} \\begin{tabular}{lccc} \\toprule \\textbf{Method} \u0026amp; \\textbf{Acc. (mm)} \u0026amp; \\textbf{Comp. (mm)} \u0026amp; \\textbf{Overall (mm)} \\\\ \\midrule 1 \u0026amp; \\textbf{0.373} \u0026amp; 2.454 \u0026amp; 1.414 \\\\ 2 \u0026amp; 0.391 \u0026amp; \\underline{2.436} \u0026amp; \\underline{1.414} \\\\ 3 \u0026amp; 0.389 \u0026amp; 2.459 \u0026amp; 1.424 \\\\ 4 \u0026amp; 0.390 \u0026amp; 2.448 \u0026amp; 1.419 \\\\ 5 \u0026amp; 0.455 \u0026amp; \\textbf{2.010} \u0026amp; \\textbf{1.232} \\\\ \\bottomrule \\end{tabular} \\begin{tablenotes} \\footnotesize \\item[$\\cdot$] Values in \\textbf{BOLDFACED} are the best results among all methods. \\item[$\\cdot$] Values in \\underline{UNDERLINED} are the second best results among all methods. \\end{tablenotes} \\end{threeparttable} } \\label{tab:comparison} \\end{table} Paper tips (2024-03-21)\nguanyingc/latex_paper_writing_tips\nPlots 画程序流程图 Code using algorithm2e package\n画坐标系 Coordinate system in LaTeX with TikZ\nPgfplots package overleaf\nOverfull \\hbox with pgfplots graph\n画夹角 How to draw a simple angle, two intersecting lines Tikz\n画圆柱体 generate simple cylindrical shape with text in latex (tikz)\nLearn How to Draw a Cylinder Shape in TikZ\n同步缩放 TikZ 与其中的 node 在 LaTeX 中同步缩放 TikZ 与其中的 node\nTikz Examples (2024-04-11)\nExample of Petar Veličković X\nInkscape (2024-04-23)\nHow I draw figures for my mathematical lecture notes using Inkscape - Gilles Castel 生成 latex 代码\nProblems:\n(2025-09-12T09:56)\n我想用 Quarto 写 slides，但是它不支持 GoAT。\n我喜欢 GoAT ascii art, 是因为我在终端里看文档的时候，也可以看到“图”。\n次于以上的方法是：我想把图和文字放在一起，而 不用单独保存图片文件， 就像 tikz 和 latex 那样，图片在 build 时转换成 svg， 但是我不会写 tikz\nInkScape 可以将矢量图形导出成 .tex 代码，那我应该就可以加入 Quarto 里面了? 但是把全部代码粘贴到文档中，可能太长了，还是另存为一个文件吧。\nTex 代码虽然不如 GoAT 那么直观，但是可以用于制作 Quarto slides， 而 slides 最好还是在浏览器上看吧，终端里看不了。\nReferences: {{{\n【InkScape+LaTeX】如何绘制矢量图并优雅地用在LaTeX文档中？ - bilibili - 豪底狄 Searched by inkscape科研绘图 at bilibili }}} Supports:\n(2025-09-12T09:55)\n在 GUI 中绘制，之后另存为 pdf，同时会生成 .pdf_tex 文件，然后将其加入 .tex r1-LaTex\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\documentclass{article} \\usepackage{graphicx} \\usepackage{xcolor} % 如果 svg 图片中含有数学公式，则需要使用数学库 \\usepackage{} \\begin{document} \\begin{figure} \\def\\svgwidth{\\columnwidth} \\input{output.pdf_tex} \\end{figure} \\end{document} Take Lecture Notes (2024-04-23)\nHow I\u0026rsquo;m able to take notes in mathematics lectures using LaTeX and Vim (Found Gilles Castel as he was followed by cxzhou35 on Github.)\nHow I manage my LaTeX lecture notes\nPseudoCode 长伪代码跨栏 Placing a single algorithm in two columns\n1 2 \\usepackage[linesnumbered,ruled,vlined]{algorithm2e} \\usepackage{multicol} 把算法 内容 用 \\begin{multicols}[2] ... \\end{multicols} 包起来，而不是在 \\begin{algorithm} ... \\end{algorithm}外面。\nComments (2024-06-01)\nCurious how to add comments like this:\n3DGS paper 里是一样的 pdf\nReferences:\nAlgorithms - Overleaf, Online LaTeX Editor Searched by latex pseudocode block algorithm comment in DDG algorithms - How do you add a comment to pseudocode in LaTeX? - TeX - SE\nNotes:\n(2024-10-24)\nOverleaf has the same style comments (Pcmt-Ovrl):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\documentclass{article} \\usepackage{algorithm} \\usepackage{algpseudocode} \\begin{document} \\begin{algorithm} \\caption{An algorithm with caption}\\label{alg:cap} \\begin{algorithmic} \\State $N \\gets \\frac{N}{2}$ \\Comment{This is a comment} \\end{algorithmic} \\end{algorithm} \\end{document} The \\COMMENT{} of the package algorithms doesn\u0026rsquo;t work in Overleaf. (Pcmt-SE)\n(2024-07-08)\nExamples for comments：How to Write Algorithm Pseudo Code in LaTeX · Blowfish - GitHub Pages (Found in DDG)\nSearch: \u0026ldquo;abbreviation for algorithm in academic paper for pseudocode\u0026rdquo; in DDG\nPseudocode 1 Guidelines for writing pseudocode - University of Waterloo (Found in pseudocode-s2)\nPseudocode Best Practices - University of British Columbia (Found in pseudocode-s2)\nLink Color (2024/06/15)\nhyperref options: overleaf Docs\n1 2 \\usepackage[hidelinks]{hyperref} \\hypersetup{colorlinks=true,allcolors=blue} % affect toc Set each option individually: \\hypersetup{colorlinks=true,linkcolor=blue, citecolor=blue}\nThe option linktoc=page may remove the links attached on titles texts in the table of contents: \\tableofcontents, and the page numbers get links.\nRef:\noptions for appearance of links in hyperref - SE How to change reference color to blue - SE Images 1 2 3 4 5 6 \\begin{figure} \\centering \\includegraphics[width=0.6\\textwidth]{sections/Images/240520.jpg} \\caption{} \\label{fig:BPA_o3d} \\end{figure} 多张图片并排 subfigure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \\usepackage{subfigure}%并排子图 共享标题 有子标题 \\begin{document} \\begin{figure}[H] \\centering \\subfigure[一次函数]{ \\label{fig:subfig:onefunction} \\includegraphics[scale=0.3]{figureDemo2}} \\hspace{0.5in} % 两图片之间的距离 \\subfigure[二次函数]{ \\label{fig:subfig:twofunction} \\includegraphics[scale=0.3]{figureDemo3}} \\caption{2个图片并排演示} \\label{fig:twopicture} \\end{figure} \\end{document} (2024-06-20)\nUse \\subcaptionbox, without using any new package. (Found when ask chatGPT to revise the above code.)\nsubfigure has been deprecated? 1 2 3 4 5 6 7 8 9 10 11 12 \\begin{figure}[H] \\centering \\subcaptionbox{pic1\\label{fig:subfig:onefunction}}{ \\includegraphics[scale=0.3]{sections/Images/240520.jpg} } \\hfill \\subcaptionbox{pic2\\label{fig:subfig:twofunction}}{ \\includegraphics[scale=0.3]{sections/Images/240520.jpg} } \\caption{2pics} \\label{fig:twopicture} \\end{figure} Effect:\n( a ) p i c 1 F i g . 1 : 2 p i c s ( b ) p i c 2 (2024-07-09)\n3x2 images array:\n( ( ( a c e ) ) ) p p p i i i c c c F 1 3 5 i g . 1 : 6 p ( ( ( i b d f c ) ) ) s p p p i i i c c c 2 4 6 Ask chatGPT-4o\nCode snippet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \\documentclass{article} \\usepackage{graphicx} \\usepackage{subcaption} \\begin{document} \\begin{figure}[htbp] \\centering \\begin{minipage}[b]{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{pic1} \\caption*{(a) pic1} \\end{minipage} \\begin{minipage}[b]{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{pic2} \\caption*{(b) pic2} \\end{minipage} \\vspace{0.5cm} % Space between rows \\begin{minipage}[b]{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{pic3} \\caption*{(c) pic3} \\end{minipage} \\begin{minipage}[b]{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{pic4} \\caption*{(d) pic4} \\end{minipage} \\vspace{0.5cm} % Space between rows \\begin{minipage}[b]{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{pic5} \\caption*{(e) pic5} \\end{minipage} \\begin{minipage}[b]{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{pic6} \\caption*{(f) pic6} \\end{minipage} \\caption{6pics} \\label{fig:six_pics} \\end{figure} \\end{document} minipage\n1 2 3 4 \\begin{figure}[htbp] \\centering \\end{figure} Insert svg (2024/06/16)\nRef: How to Use SVG Images in LaTeX | Baeldung\nUse package: \\usepackage[inkscapearea=page]{svg}\n1 2 3 4 5 6 \\begin{figure} \\centering \\includesvg[scale=0.5]{sections/Images/txt.svg} \\caption{MVSNet pipeline} \\label{fig:MVSNet} \\end{figure} Span 2 Cols References:\nPrompt for ChatGPT: \u0026ldquo;How to make a wide image displayed across two columns?\u0026rdquo; Notes:\nUse figure* environment: r1-Chat\n1 2 3 4 5 6 7 8 \\documentclass[iicol]{sn-jnl} \\begin{figure*}[ht] \\centering \\includegraphics[width=\\textwidth]{your_image_file.png} \\caption{Your caption here} \\label{fig:wide_image} \\end{figure*} Manage Version Control References:\nSearched by Should I use git to manage latex project? in DDG Use Git Notes:\n(2024/12/08)\nFesible r1-Reddit Caption Notes:\n(2024/12/16)\nCancel the : after Fig. 1 (MVA requirement)\n1 2 \\usepackage{caption} \\captionsetup[figure]{labelsep=space} Asked ChatGPT and Claude:\nIn latex, how to cancel the : after figure index, to satisfy the journal requirement:\nFigure captions begin with the term Fig. in bold type, followed by the figure number, also in bold type. No punctuation is to be included after the number, nor is any punctuation to be placed at the end of the caption.\n编号 Notes:\n(2024/12/16)\nAppendix 中的表格的编号 继续 正文的编号，而不是 A1\n1 2 3 4 5 \\begin{appendices} \\renewcommand{\\thetable}{\\arabic{table}} % Reset table numbering to Arabic \\setcounter{table}{2} \\section{Table of number of points}\\label{secA1} Answered by ChatGPT Editors Vim Problems:\nOverleaf\u0026rsquo;s editor is not good. References:\n我在neovim中的LaTeX编辑环境 - 帕特里柯基 Notes:\n(2025-05-03T19:47)\n用 nvim 插件实现编译快捷键r1-柯基 ","date":"2022-09-12T17:40:00Z","image":"https://raw.githubusercontent.com/github/explore/80688e429a7d4ef2fca1e82350fe8e3517d3494d/topics/latex/latex.png","permalink":"http://blog.zichen.uk/post/writenotes/lang/latex/","title":"Memo: Lang - Latex | Common Usages"},{"content":"Arxiv\nNotes UNet 常用于图像 Segmentation。\nCNN的目标是提取特征（减少冗余信息），经过多个pooling层（以及strides\u0026gt;1），最后的 feature map 的尺寸（分辨率）是最小的， 但图像分割任务需要为图片的每个像素判断 label，所以需要把 CNN 最后输出的 feature map 恢复至原来的尺寸， U-Net 通过逐级 upsampling (插值) 得到了与cnn对称的feature maps。 然后就可以把 CNN 中间过程产生的分辨率较高的 feature maps 与对应的 upsampled feature maps 结合起来，从而输出更精确的 segmentation map。 U-Net architecture Ronneberger 2015 左侧是 CNN \u0026ldquo;收缩\u0026quot;路径（encoder），右侧是\u0026quot;扩展\u0026quot;路径（decoder）。\n左侧 CNN 的每一级做两次 conv3x3 (unpadded)，然后ReLU激活并通过 2x2 max-pooling 做下采样， 每次下采样会把 通道数量加倍。\n右侧每次先对 feature map 做 2 倍上采样和两次 con3x3 把通道数减一半。\n灰色箭头是 concatenation（Resnet 的 skipconnect 是直接相加，不是拼接），把来自cnn的feature map 的边缘裁剪一下，拼接到右侧的feature map上。最后的feature map 做conv1x1 把64通道变换到所需的类别个数\n上采样不会增加(恢复)空间信息\nskip connection的原理是什么？为什么U-net中要用到skip connection?-akkaze-郑安坤的回答\n(2023-07-10)\nExample Tensorflow: CV2020 - 16 - Object Segmentation.ipynb\nPyTorch: U-Net: Training Image Segmentation Models in PyTorch\nSegmentation needs to give a label for each pixel, so the output should have the same size as the input image.\nThe hidden feature vector has lost spatial information along contracting. And up-sampling (interpolation) doesn\u0026rsquo;t restore the location of features, but just kind of \u0026ldquo;copy\u0026rdquo; the features to around pixel.\nThe feature on each pixel is concatenated with the feature before contracting which still locates in its original position. Then the convolution later on will \u0026ldquo;fuse\u0026rdquo; one pixel\u0026rsquo;s location \u0026ldquo;characteristic\u0026rdquo; into its feature vector.\nThe output feature map is an expansion of the compact hidden featuer map, but conditioned with spatial location.\nThis way the trained model can classify a pixel based on spatial position and surrouding RGB feature.\nU-Net Explained: Understanding its Image Segmentation Architecture - medium\n","date":"2022-09-08T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/misc/b-note-unet/","title":"read: UNet"},{"content":"Note GL_PROJECTION matrix: camera space point(xe,ye,-ze) -\u0026gt; perspective projection to near plane -\u0026gt; homo:(xp,yp,1) -\u0026gt; scale -\u0026gt; homo:(xn,yn,zn,1) -\u0026gt; (x_clip,y_clip,z_clip, w_clip) , where \u0026lsquo;xn,yn,zn\u0026rsquo; are variables raning from -1 to 1.\nRefer to OpenGL Projection Matrix - songho\nNormalized Device Coordinates (NDC) is used to determine whether a 3D point can appear on the computer monitor, which is a cube with length 1. The transformation from eye coordinates to NDC is mapping the truncated pyramid frustum to a cube.\nPerspective Projection - songho\nAccording to the perspective projection, the projection point of a world point (xₑ,yₑ,zₑ) on the near plane is\n$$ \\begin{cases} x_p = \\frac{n}{-z_e} x_e \\\\\\ y_p = \\frac{n}{-z_e} y_e \\end{cases} $$(Camera coordinates is right-hand system looking in the -z direction, while NDC is looking in the z direction under left-hand coordinates)\nIn order to use a matrix to represent NDC transformation, the homogeneous coordinates are used to enable division, so the transformation can be represented as:\n$$ \\begin{pmatrix} x_{clip} \\\\\\ y_{clip} \\\\\\ z_{clip} \\\\\\ w_{clip} \\end{pmatrix} = M_{projection} \\cdot \\begin{pmatrix} x_e \\\\\\ y_e \\\\\\ z_e \\\\\\ w_e \\end{pmatrix} $$ (2024-02-15) wₑ is the homogeneous coordinate for storing the original depth value of the camera point after the multiplication with the projection matrix, where the 4-th row is [0 0 -1 0], so wₑ = 1. And the depth will be divided at the very end step: the perspective division, so as to make the intermediate processes linear operations.\nComparing merely projecting a 3D point onto plane with a w2c, the projection matrix specifies specific behavior for the z-axis of the ND space (not the source camera space any more).\nTherefore, the NDC is:\n$$ \\begin{pmatrix} x_{ndc} \\\\\\ y_{ndc} \\\\\\ z_{ndc} \\end{pmatrix} = \\begin{pmatrix} x_{clip} / w_{clip} \\\\\\ y_{clip}/w_{clip} \\\\\\ z_{clip}/w_{clip} \\end{pmatrix} $$Because $w_{clip}$ is the denominator, it should equal to -zₑ; Hence, the forth row of matrix should be $[0\\ 0\\ -1\\ 0]$\nMapping [l, r] and [b, t] to [-1, 1] with linear realtionship: Two points (l,-1),(r,1) can be used to determine the line:\n$$ x_{NDC} = \\frac{1-(-1)}{r-l} \\cdot x_p + β $$and then substitute (r,1) for $(x_p,x_{NDC})$ to solve β = -(r+l)/(r-l).\nTherefore, $x_{NDC} = \\frac{2}{r-l}x_p - \\frac{r+l}{r-l}$.\nSimilarly, $y_{NDC} = \\frac{2}{t-b} y_p- \\frac{t+b}{t-b}$\nSubstitute xp, yp with the form of xₑ, yₑ:\n$$ x_{NDC} = (\\frac{2n}{r-l} \\cdot x_e + \\frac{r+l}{r-l} \\cdot z_e) / -z_e \\\\\\ y_{NDC} = (\\frac{2n}{t-b} \\cdot y_e + \\frac{t+b}{t-b} \\cdot z_e) / -z_e $$Therefore, the first two row elements of the matrix can be determined.\nSuppose the third row is $[0\\ 0\\ A\\ B]$ (z value is independent to x and y), so:\n$$ z_{NDC} = \\frac{A z_e + B w_e}{-z_e} $$Substitute the correspondence between (-n, -f) and (-1, 1) into the above equation:\n$$ \\begin{cases} \\frac{-A n + B}{n} = -1 \\newline \\frac{-A f + B}{f} = 1 \\end{cases} $$Therefore, A = -(f+n)/(f-n), and B = -2fn / (f-n)\nFinally, the matrix $M_{projection}$ is\n$$ \\begin{pmatrix} \\frac{2n}{r-l} \u0026 0 \u0026 \\frac{r+l}{r-l} \u0026 0 \\\\\\ 0 \u0026 \\frac{2n}{t-b} \u0026 \\frac{t+b}{t-b} \u0026 0 \\\\\\ 0 \u0026 0 \u0026 \\frac{-(f+n)}{f-n} \u0026 \\frac{-2fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 -1 \u0026 0 \\end{pmatrix} $$ Songho\u0026rsquo;s Idea References:\nkaolin.render.camera.PinholeIntrinsics — NVIDIA Kaolin Library documentation Mentioned in QQ chat by \u0026ldquo;Ligo、\u0026rdquo; at 1:48 AM, Dec 24, 2024 Notes:\n(2024-11-30)\nSongho\u0026rsquo;s derivation starts with the film coordinates. Specifically, x on the camera film rangers $[l, r]$.\nHe first scaled the film Coordinates to $[-1, 1]$, obtaining the final ND coordinates.\nHowever, the Projection matrix should transform the camera-space coordinates to the clipping-space coordinates.\nSo, he substituted the camera-space Coordinates that have performed perspective division, and multiplied the z values with the obtained ND coordinates to get clippling-space coordinates.\nFinally, he identified the coefficients in the matrix according to the corresponding items.\nIn my opinion, I believe the derivation of Projection matrix could be logical by splitting the transformation into 2 steps: perspective projection and range scaling.\nIn other words, regarding the Projection Matrix as a product of 2 matrix: intrinsics matrix and a scaling matrix.\nAnyway, the main idea for Projection Matrix (or even any projection operation) is range scaling. Specifically, Projection Matrix is used in the process of scaling the camera coordinates to [-1, 1].\nIllustration of scaling the x range on the camera film to [-1, 1]:\n⋱ - ( ⋱ 1 ↑ 0 , ⋱ 0 ┄ - ) c ┄ ⋱ x ┄ - ┄ - ⋱ ┄ - ┄ - ⋱ ⋰ C ⋰ a - m ( ⋰ 2 - c c ⋰ f e 1 ↑ x i n , ⋰ l t 0 m e ) ⋰ r (2024-12-29)\nKaolin r1-Docs applies the similar derivation by decomposing the ProjectMatrix to 3 steps:\n$\\rm FullProjectionMatrix = Ortho \\times Depth Scale \\times Perspective$\nCode (2023-10-02)\n3D world coords multiplied with Inverse intrinsics matrix, Scale the [near, far] to [0,1] Code credits MatchNeRF\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_coord_ref_ndc(extr_ref, intr_ref, pts_3D, inv_scale, near_far=None, lindisp=False): \u0026#39;\u0026#39;\u0026#39; Warp the provided position to the reference coordinate, and normalize to NDC coordinate. pts_3D [batch, N_rays N_sample 3] \u0026#39;\u0026#39;\u0026#39; bs, N_rays, N_samples, N_dim = pts_3D.shape # (bs=1, n_rays=1024, n_pts=128, n_dim=3) pts_3D = pts_3D.reshape(bs, -1, N_dim) # (bs, n_rays*n_pts, n_dim) near, far = torch.split(near_far, [1, 1], dim=-1) # (1,2) -\u0026gt; both are (1,1) # wrap to ref view if extr_ref is not None: # 3D pts in world space -\u0026gt; camera space of a src view pts_ref_world = world2cam(pts_3D, extr_ref) if intr_ref is not None: # using projection # pts in camera space -\u0026gt; image plane coords with z point_samples_pixel = pts_ref_world @ intr_ref.transpose(-1, -2) # normalize to 0~1 point_samples_pixel[..., :2] = (point_samples_pixel[..., :2] / point_samples_pixel[..., -1:] + 0.0) / inv_scale.reshape(bs, 1, 2) if not lindisp: point_samples_pixel[..., 2] = (point_samples_pixel[..., 2] - near) / (far - near) # normalize to 0~1 else: point_samples_pixel[..., 2] = (1.0/point_samples_pixel[..., 2]-1.0/near)/(1.0/far - 1.0/near) else: # using bounding box near, far = near.view(bs, 1, 3), far.view(bs, 1, 3) point_samples_pixel = (pts_ref_world - near) / (far - near) # normalize to 0~1 point_samples_pixel = point_samples_pixel.view(bs, N_rays, N_samples, 3) # (bs, n_rays*n_pts, 3) -\u0026gt; (bs, n_rays, n_pts, 3) return point_samples_pixel near-far = [0,1] (2024-01-16)\nProjection Matrix 详解 - 贰芍的文章 - 知乎\n","date":"2022-09-02T11:39:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/ndc/","title":"Memo: Vis | NDC"},{"content":"Stereographic Projection Source video: Visualizing quaternions (4d numbers) with stereographic projection\nLeveraging projection to reduce the dimensionality by 1. Such that quaternion in 4D space can be learned through the projection counterparts in 3D space.\n把 (i-实轴) 单位圆 做球极投影 stereographic projection 到 y 轴上:\n从 x 轴的 -1 点出发过圆上每个点做射线，与 y 轴的交点即是圆上点的投影，\n在这条投影线上，靠近 0 的点密集，越往外越稀疏，-i 和 i 是真实的圆上的点（代表着 x 为 0 的圆上点），(-i,i) 之间是右半圆的投影；\n左乘 i 就是圆逆时针旋转 90 度，就是 1 ➔ i ➔ -1 ➔ -i 做一次循环移位。\n把三维 (i-j-实轴) 单位球投影到水平面上:\n从实轴的 -1 点出发连接球面上各点，与 i-j 平面交点即为投影点，\n赤道是真实的球面上的点（代表着实部为 0 的三维复数），赤道之内是北半球的投影，之外是南半球的投影， 赤道两侧是对称的，极点附近的投影点密集；\n左乘 i 就是球在 i 轴方向顺时针旋转 90°，赤道和零度经线共同在 1-i 平面内完成了一次 1 ➔ i ➔ -1 ➔ -i 循环移位， 乘以 j 就是球在 j 轴方向上顺时针，在 1-j 平面内完成一次 1➔ j ➔ -1 ➔ -j 循环移位\n把四维 (i-j-k-实轴) 单位超球投影，\n从实轴的 -1 点出发连接超球面上各点，与某个平面的交点形成一个个球面，每个球面就是每个四元数的投影，\n在 i-j-k 空间的三维单位球是真实的超球面上的点（代表着所有实部为 0 的四元数）， 单位球内是实部为 0-1 之间的四元数的投影，负数实部的四元数被投影到了单位球外，实部=-1 的四元数在无穷远处(我们看不到)，\n我们能看到的投影点都有相同的模长（？不懂），四元数旋转时，就是三个维度共同移位，\n(2023-11-07) 如果绘制出来的三维球代表 i,j,k 三个虚轴，那实轴就画不出来了，只能先固定它再分析。 （把以上两种情况合起来:）i 是圆的球极投影，\u0026lsquo;j+k\u0026rsquo; 是球的球极投影， i 的移位顺序是 1,i,-1,-i，\u0026lsquo;jk\u0026rsquo; 的移位顺序是 j,k,-j,-k。 i 与 jk 满足右手定则：拇指指向 i，则四指卷曲方向就是 jk 旋转方向（拇指指向 j，k ➔ i ➔ -k ➔ -i 循环移位）\n四元数的可视化-3B1B + Krasjet评论\n(2023-11-07)\n3D Rotation Because projection has been performed, the real axis is collapsed.\nSo when \u0026ldquo;rotating\u0026rdquo;，the real axis is fixed and only imaginary axes rotate.\nTherefore, a 4-D quaternion can be used to describe the rotation in 3D space through its mutable 3 imaginary axes.\nCompared with Euler angles that define the hierarchical order of three angles, quaternion depicts the evolution of a single 4D vector. Convert a quaternion to a 3x3 rotation matrix\n","date":"2022-08-31T22:00:00Z","image":"https://img.youtube.com/vi/d4EgbgTm0Bg/maxresdefault.jpg","permalink":"http://blog.zichen.uk/post/writenotes/calc/quaternion/","title":"memo: Calc | Quaternion"},{"content":"Define multi layers by for loop 1 2 3 4 5 6 for i in range (n_layers): setattr(self, f\u0026#34;layer{i}\u0026#34;, nn.Linear(2, n_hidden_units), nn.ReLU(True)) # 取出多层： for i in range(n_layers): layer = getattr(self, f\u0026#34;layer{i}\u0026#34;) Access all weights of a model 1 for name, param in model.named_parameters: Refer:\npytorch教程之nn.Module类详解——使用Module类来自定义模型- CSDN Initialize weights of nn.Linear model.apply(fn) will apply function fn to every children submodule. Therefore, let fn be init_weights()\n1 2 3 4 5 6 7 8 9 10 @torch.no_grad() # this func won\u0026#39;t create graph def init_weights(m): print(m) if type(m) == nn.Linear: torch.nn.init.ones_(m.weight) m.bias.data.fill_(0.01) print(m.weight) model = Net() model.apply(init_weights) Refer:\nHow to initialize weights in PyTorch? - StackOverflow init_weights Example code ModuleList usage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026#39;\u0026#39;\u0026#39; ModuleList is like a iterator \u0026#39;\u0026#39;\u0026#39; class LinearNet(nn.Module): def __init__(self, in_features, out_features, num_layers, layer_size): super(LinearNet, self).__init__() self.linears = nn.ModuleList([nn.Linear(in_features, layer_size)]) # 1st layer self.linears.extend([nn.Linear(layer_size, layer_size) for i in range(1, num_layers-1)]) self.linears.append(nn.Linear(layer_size, out_features)) def forward(self, x): for idx, layer in enumerate(self.linears): x = layer(x) # x = self.linears[idx // 2](x) + layer(x) # pass x through each layer return x Refer:\nWhen should I use nn.ModuleList and when should I use nn.Sequential? torch.Tensor 包含单一数据类型元素的多维矩阵 有10种张量类型，torch.Tensor是默认张量类型torch.FloatTensor的别名 Note 张量变异方法都带有下划线后缀,它们直接原地修改原张量的属性，而不派生新张量。例如torch.FloatTensor.abs_()直接计算并修改原张量，而torch.FloatTensor.abs()在新张量中计算结果。\nGet value of a tensor a = torch.tensor([3])\na.data ⮕ tensor([3]) a.item() ⮕ 3 (\u0026ldquo;Convert one-element tensor to python scalar.\u0026rdquo;) a.numpy() ⮕ array([3]) b = (a/2).requires_grad_(True)\nb.detach().cpu().numpy() ⮕ array([1.5], dtype=float32) (2023-07-27)\nMemory allocated for a tensor 1 2 a = torch.arange(4) # torch.int64 Num_bytes = a.element_size() * a.nelement() # 8 * 4 How to know the memory allocated for a tensor on gpu?\nMove specified axis Move the specified axis to the second axis\nQuestion for ChatGPT:\nIf I was given a variable channel_dim, and it can be 3, that means the tensor is (B, H, W, channel_dim), so how can I transform the tensor to make the channel_dim the 2nd axis?\nI mean the channel_dim is a variable, it can be any axis.\nAnswer:\nApologies for the confusion. If the channel_dim is a variable and can be any axis, you can use the following code snippet to move the specified axis to the second axis of the tensor:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch # Assuming you have a tensor with size (B, H, W, C) and channel_dim can be any axis tensor = torch.randn(B, H, W, C) channel_dim = 2 # Replace this with the desired axis you want to move to the second position # Permute the tensor to move the specified axis (channel_dim) to the second axis # Generate the permutation order: (0, channel_dim, 1, 2, ..., channel_dim-1, channel_dim+1, ..., C-1) perm_order = list(range(len(tensor.shape))) perm_order.remove(channel_dim) perm_order = [0, channel_dim] + perm_order[1:] tensor_permuted = tensor.permute(*perm_order) # The resulting tensor will have size (B, C, H, W) if channel_dim was initially the third axis (2) # The resulting tensor will have size (B, W, C, H) if channel_dim was initially the second axis (1), and so on. print(tensor_permuted.size()) (2023-08-01)\nDynamically change Conv2d layer Question of chatGPT:\nI\u0026rsquo;m writing pytorch. I want to use nn.Conv2d, but the in_channels is the result generated in the forward method, how could I make the parameter of Conv2d optimized during training?\nMake the in_channels a instance variable, like self.in_chnls = in_channels.\nThen in the __init__(self, in_channels) method, a Conv2d layer can be construct: self.conv = nn.Conv2d(self.in_chnls, out_chnls, ...)\nThen update it in the forward before calling self.conv layer: self.in_chnls = x.shape[1]\n(DDG search: \u0026ldquo;use nn.Conv2d with dynamically determined in_channels during the forward pass\u0026rdquo;)\nSimilar question: Dynamically set Conv2d based on input channels - PyTorch Forum\nHow to create a custom layer that accepts the input during forward pass - PyTorch forum\nMake other class (nn.Module), in which the prefix model is called. But the in_channels is required when initializing Conv2d(), so the parameters of it can be defined. \u0026ldquo;Can we not define the filter size at runtime?\u0026rdquo; Why does nn.Conv2d require in_channels? - PyTorch Forum\nnn.LazyConv2d(out_channels, ...) doesn\u0026rsquo;t need in_channels. Docs torch.nn.modules.lazy.LazyModuleMixin Get device of a module 1 dev = next(model.parameters()).device How to get the device type of a pytorch module conveniently?\nModule.apply(fn) Docs Example - AIM Apply the function fn to each submodule and the Module itself. This can be used to initialize weights, like the code in AIM:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class ViT_CLIP(nn.Module): def __init__(self,): self.__init__() def init_weights(self,): # A class method def _init_weights(m): # Pass a submodule if isinstance(m, nn.Linear): trunc_normal_(m.weight, std=.02) if isinstance(m, nn.Linear) and m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.LayerNorm): nn.init.constant_(m.bias, 0) nn.init.constant_(m.weight, 1.0) # Call _init_weights self.apply(_init_weights) Customize Autograd Op (2024-01-23)\nThe .apply() method of a customized operation that subclasses autograd.Function requires forward and backward to be static methods. Docs\nExample - 3DGS\ntorch.roll Circular shift (\u0026ldquo;循环移位\u0026rdquo;) along some dimensions. Docs\nExample - AIM - swin2d Move 1 step along 1 dimension:\n1 2 x = torch.tensor([[1,2,3],[4,5,6]]) torch.roll(x, shifts=1, dims=0) It will shift 1 element in the dimension 0, i.e., [4,5,6] to the next position, so x will become: [[4,5,6],[1,2,3]]\nMove steps (2, 1) along 2 dimensions separately:\n1 torch.roll(x, shifts=(2,1), dims=(0,1)) The dimension 0 will shift twice, and the dimension 1 will shift once. So x becomes: [[3,1,2],[6,4,5]]\ntorch.max torch.max(x, dim) compares each atom element on the equal position according to the dimension dim\n1 2 3 4 5 6 7 8 a = torch.randint(8, (2,3,3) tensor([[[0, 3, 2], [6, 1, 5], [2, 7, 0]], [[4, 7, 4], [6, 1, 0], [0, 1, 6]]]) torch.max(a, 0) will compare: 0-4, 3-7,, 2-4; 6-6, 1-1, 5-0; 2-0, 7-1, 0-6, so the result is [[4,7,4] [6, 1, 5] [2, 7, 6].\ntorch.max(a,1) will compare: 0-6-2, 2-1-7, 2-5-0; 4-6-0, 7-1-1, 4-0-6; then the result is [[6,7,5], [6,7,6]]\ntorch.diff The back one minus the front one.\n1 2 3 4 5 6 a = torch.arange(12, (2,2,3)) tensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) torch.diff(a): the last dim subtract [ [ [1-0, 2-1], [4-3, 5-4]]; [7-6, 8-7], [10-9, 11-10] ] = [ [ [1,1], [1,1] ]; [1,1], [1,1] ] ]\ntorch.diff(a, n=2): do again for 1st-time result: [ [ [0], [0]; [[0],[0]]]\ntorch.diff(a, append=a), the append tensor needs to have the same dimensions of input.\nCount #params From Match-NeRF:\n1 2 3 4 5 for name, param in self.model.named_parameters(): log.info(f\u0026#39;{name}: {param.requires_grad}\u0026#39;) num_param = sum(p.numel() for p in self.model.parameters() if p.requires_grad) num_total_param = sum(p.numel() for p in self.model.parameters()) log.info(\u0026#39;Number of total parameters: {}, tunable parameters: {}\u0026#39;.format(num_total_param, num_param)) (2023-12-21) From MVSNet:\n1 sum([p.data.nelement() for p in model.parameters()]) param_group (2024-04-11)\nPer-parameter options: an iterable of parameter groups. Docs\nHave practiced in MatchNeRF exp1 before: different modules are trained with separate LRs. Example in 3DGS\ntorch.unbind Docs\n(2024-05-17)\nBreak one dimension apart.\n1 2 3 4 a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.unbind(a) # out a tuple: ([1,2,3], [4,5,6], [7,8,9]) torch.chunk can get the same effect, but it requires specifying the number of chunks:\n1 2 \u0026gt;\u0026gt;\u0026gt; torch.chunk(a, 3, dim=0) (tensor([[1, 2, 3]]), tensor([[4, 5, 6]]), tensor([[7, 8, 9]])) In contrast, torch.split requires specifying the number of entries contained in a chunk:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;\u0026gt;\u0026gt; torch.split(a, 3, dim=0) # return a tuple (tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),) \u0026gt;\u0026gt;\u0026gt; torch.split(a, 2, dim=0) (tensor([[1, 2, 3], [4, 5, 6]]), tensor([[7, 8, 9]])) \u0026gt;\u0026gt;\u0026gt; torch.split(a, 1, dim=0) (tensor([[1, 2, 3]]), tensor([[4, 5, 6]]), tensor([[7, 8, 9]])) ","date":"2022-08-26T20:25:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_misc/","title":"memo: PyTorch | Misc"},{"content":"(2022-12-28)\n白板推导: VAE Source video：【机器学习】白板推导系列(三十二) ～ 变分自编码器(VAE)】\nVAE 和 GMM 一样也是生成模型：隐变量 z 生成观测变量 x，从而学习样本数据 x 本身的分布。不过 VAE 的 z 不是1维的，而是多维的。\n用 EM 解 GMM 的最优参数 θ 时，假设 z 的后验分布 P(z|x;θ⁽ᵗ⁾) 能够取到，而且因为 z 是离散的，P(x) = ∑ₖ₌₁ᴷP(x,z=Cₖ;θ) 就能算出来， 所以后验 P(z|x)=P(x|z)P(z)/P(x) 也能算出来，其中分子两项是假设的分布。E步把目标函数: \u0026ldquo;最大的期望\u0026rdquo; $E\\_{P(z|x)} [ log P(x,z|θ) ]$ 写出来，M步对其求导找出（期望最大时的）最佳θ⁽ᵗ⁺¹⁾。\n而 VAE 的 z 是高维连续的，P(x) = $∫_z P(x,z) dz$ 积不出来，后验P(z|x) 就没法用贝叶斯公式导出，但可以用随机梯度下降变分推断(SGVI)近似后验。\n“推断”是从 x 到 z 的过程：用样本 x 修正 z 的先验 P(z) 得到 z 的后验 P(z|x)；而“生成”是从 z 的后验分布 P(z|x) 中采样出 z，再变换成样本的分布 P(x)。\nflowchart LR; observed((x)) --\u003e|Inference| latent((z)); latent --\u003e|Generation| observed 用 q(z|x;ϕ) 逼近后验 P(z|x;θ) 时，要最小化 $KL(q_{ϕ(z|x)} || P_{θ(z|x)})$，在 θ 固定，则似然 P(X) 也固定的情况下，等价于最大化 ELBO： arg max $E_{qᵩ(z|x)} [ log (P(z,x;θ)/qᵩ(z|x)) ]) ]$。类似广义 EM 的 E步，在 θ 固定的情况下，求后验的近似。\n通过把联合概率拆开，这个 ELBO 也可写成: $E_{qᵩ(z|x)} [ log P(x|z;θ) ] - KL(q(z|x;ϕ) ||P(z))$。 当 z 的先验分布 P(z) 是标准正态时，那么KL散度就是希望 qᵩ(z|x) 的方差保持为I，不要太小，分布坍缩到一点就是过拟合：x与z一一对应，就变成 AE了，只能对训练样本 x 推出正确的 z。\n求生成模型的最优参数 θ 同样是要使似然期望最大：arg max ELBO（x后验与KL散度同时优化），（没法直接求导）可采用梯度上升法，所以 VAE 是把 EM 的两步合起来了，既逼近后验 p(z|x) 的参数 ϕ，又逼近生成模型 p(x|z) 的参数θ。\n在计算 ELBO 对 ϕ 的梯度时，∇ᵩL(ϕ) 可以写成一个期望，直接对 z 采样求均值可能由于方差太大而失效（而且采样操作不可导，就无法对z求梯度）， 所以先对一个高斯噪声 ε 采样，再根据变换：z=μ_ϕ(x) + Σ_ϕ¹ᐟ²(x)⋅ε 得到 z (重参数化)。然后求均值(=梯度)，带入梯度上升公式，更新ϕ。\n训练NN时，输入一个 x，神经网络输出后验分布（编码）p(z|x) 以及采样出一个z。用这个z 通过另一个网络逼近 x 的后验分布（生成模型）p(x|z)， 也就是在学到的 z 成立的情况下，从 p(x|z) 中采到 x 的概率，目标函数就是希望这个概率越大越好，可以假设 x服从二项或正态，把参数代入公式即得概率 或者说，分布 p(x|z) 的\u0026quot;众数\u0026quot; x\u0026rsquo;要与输入 x 的距离越小越好，当方差很小时，众数就是期望，即每次采样都会采到期望 μ(z)，所以以 μ(z) 与 x 的距离作为目标函数。 一个x是多维的，它有自己（维度之间）的分布。\nflowchart LR subgraph Encoder x(\"Input x\") --\u003e net(\"NN-ϕ\") --\u003e a(\"μ(x)\") \u0026 b(\"Σ(x)\") end subgraph Decoder a --\u003e o((\"+\")) --\u003e z(\"sampled z \\n from N(μ(x),Σ(x))\") --\u003e net2(\"NN-θ\\n(μ(z))\") --\u003er(\"reconstructed\\n x'\") b --\u003e m((\"×\")) --\u003e o ε(\"ε～N(0,𝐈)\") --\u003e m end 完整笔记：shuhuai008-32； 参考：隐变量模型到EM到变分自编码器 - 我要给主播生猴子 -知乎\n(2022-12-29)\n苏剑林: VAE（一） Source article：变分自编码器（一）：原来是这么一回事-苏剑林\n通过隐变量 z 求样本 x 的分布：p(x) = ∫_z p(x|z) p(z) dz。 先学习 z 的后验分布 p(z|x)（ 编码 ），再由它 生成 $\\\\^x$，$\\\\^x$应与x一样。\nflowchart LR sample(x) --\u003e|\"N(μᶿ,Σᶿ)\"| latent(\"p(z|x)\") --\u003e|\"sampling z \\n P(x)=Σp(x|z)⋅p(z)？\"| recon(x^) sample -.-|=| recon 注意区分先验 p(z) 与后验 p(z|x)。因为假设了后验是正态分布的形式，所以是对 μᶿ,Σᶿ 做重参数化。而先验只出现在正则化项中，不参与后验的训练。\n一个样本点x⁽ⁱ⁾对应一个（独立的、多元的）后验分布 p(z|x⁽ⁱ⁾)，这样从中采出的 z 就一定对应这个样本点。所以每个样本有自己的正态分布。\nz 是后验分布的一个采样，采样就会有偏差（方差），导致重构误差不为0。如果不加正则化，为了减小误差，Σ会不断减小到0，退化成AE。 从这个角度看 vae 是 AE 加上噪声，并约束噪声的强度（方差）尽量为1.\nvae 希望所有的后验分布（”一般正态“）都与标准正态相似：μ=0，Σ=I，采样z时就保证了生成能力。因为各分量独立，所以是d维一元N的加和： KL( N(μ,Σ)|| N(0,I) ) = ½ Σ [(-logσ² + μ² + σ² -1) ]\n重参数化技巧把服从N(μ,Σ)的随机变量 z 的概率拆成一个服从标准正态的变量ε和一个参数变换μ+ε×σ，从而实现虽然采样操作不可导，但它不参与梯度的反向传播。\n条件VAE：样本属于不同的类别-期望不同 cvae代码：用2个线性层分别拟合μ和Σ，用重参数化技巧采样z，x与x\u0026rsquo;之间的重构损失用了交叉熵 model：\nflowchart LR x(x\\n original_dim) --\u003e|\"Dense\\n(intermediate_dim)\"| h --\u003e|\"Dense\\n(latent_dim)\"| μ(\"z_mean\") \u0026 Σ(\"z_log_var\"); y(y\\n num_classes) --\u003e|\"Dense\\n(latent_dim)\"| yh μ \u0026 Σ --\u003e rp{sampling} --\u003e|reparame\\n terization| z --\u003e|\"Dense\\n (intermediate_dim)\"| h_decoded --\u003e|\"Dense\\n(original_dim)\"| x_decoded_mean (2022-12-30)\n苏剑林: VAE（二） 原文：变分自编码器（二）：从贝叶斯观点出发\n期望的数值计算与采样计算不同：数值计算是先给一个数列 x（其中$x⁰ \u003c x¹ \u003c x²\u003c...xⁿ$），然后对里面的每个数 x⁽ⁱ⁾ 按它的概率加权求和：E[x]=∫ xp(x) dx。 但如果 x⁽ⁱ⁾ 是分布 p(x) 中的采样，概率大的会被多采几次，样本集合 x 中就包含了概率信息，不用再乘 p(x⁽ⁱ⁾)了：E[x]≈1/n⋅∑ᵢ₌₀ⁿ x⁽ⁱ⁾, x⁽ⁱ⁾∼p(x)\n推导目标函数时，先构造了 p(x,z)=p(z|x)⋅p^(x)，再构造 q(x,z)=q(x|z)q(z)，这两个构造毫无关系，希望它俩互相靠近，而不是为了逼近z的后验p(z|x)。notes; (vae三-josh00的评论)\n在生成阶段，若假设 p(x|z) 服从二项分布，则重构误差就是交叉熵；若假设 p(x|z) 服从正态分布，则重构误差就是MSE\n训练时，生成阶段只从 z 的后验分布中采样一个，因为 z 是专属于一个x。P(x)➔ μ_ϕ(x), Σ_ϕ¹ᐟ²(x) ➔ z ➔ μ_θ(z) ➔ x'\n(2022-12-31)\n苏剑林: VAE（三） 原文：变分自编码器（三）：这样做为什么能成？\nvae生成时，只采一个z：因为x与z一一对应（自编码器）方差为0，vae引入了先验q(z)=N(0,I)，方差也不会太大，也就是每次采样，结果都一样。如果直接做最大似然p(x|z)，就需要从z的先验p(z)中采多个样本先估计出每个x的似然，再求似然的期望最大化。但如过没采到 zₖ，它对应的 xₖ的似然就是0，ln0是-∞，导致训练失败。\nVAE 的重建生成相当于在AE上加了噪声（方差），所以可以生成与原始样本不同的数据。\n%%{ init: { 'flowchart': { 'curve': 'linear' } } }%% flowchart TB x[\"x\\n(样本)\"] --\u003e nn[\"隐变量的分布 μ(x), Σ(x)\"] --\u003e z --\u003e nn2[\"数据的分布 p(x|z)\\n 方差很小\"] --\u003e recon[x'\\n重建样本] nn --\u003e |\"通过采样，从 “多个” 到 “一个”，\n从 “无限” 到 “唯一”\"| recon IWAE 对p(x,z)=∫p(x|z)p(z)dz 做等价变换，从而可从后验p(z|x)中采样z。\n(2023-06-04)\nPCA 与 VAE They\u0026rsquo;re both learning the distribution of data.\nDDG search: \u0026ldquo;PCA 与 VAE 对比\u0026rdquo;\nUnderstanding Variational Autoencoders (VAEs) - Joseph Rocca\nVariational Autoencoders explained — with PyTorch Implementation - Sanna Persson\n1 import torch Instead of mapping the input into a fixed vector, we want to map it into a distribution. From Autoencoder to Beta-VAE - Lil\u0026rsquo;Log\n","date":"2022-08-26T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/vae/c-sum-vae/","title":"sum: VAE"},{"content":"Code | Arxiv\n一句中文：构建 point-alinged feature field, 然后用 attention 把特征融合成图像，特征的attn score 反映了深度\nInsights:\nNeRF is in a backward optimization fashion. The color is mapped to points along with the optimization. So the radience field is recovered backward. While Generalizable NeRFs assign feature onto points in the feed-forward process.\nProjecting the points onto feature maps exerts the inductive bias of epipolar constraints for injecting geometry prior.\nIt\u0026rsquo;s inferior to NLF\nSamples on a single ray cannot recover refraction and scattering, in which the ray will bend. So GNT managed this by its view transformer?\nOcclusion-aware is realized by giveing the most visible reference view the most weight. Depth-aware is endowed by the importance of each point to the pixel color. \u0026ldquo;importance=density\u0026rdquo;\nGNT doesn\u0026rsquo;t care the quality of 3D geometry reconstruction.\nOnly net_coarse is used and trained with 4096 rays, 192 N_samples in one iteration. They didn\u0026rsquo;t split the N_rand into chunks in the train.py, but they did when rendering a full imagein evaluation period. So I need to distribute the 4 blocks onto 4 crads for accomodating the 4096 rays in one batch, if I want to reproduce their expriment. In my previous training, the number of points fed into MLP is N_rand x N_samples = 2048x64 = 131072.\nAbstract Generalizable NeRF Transformer (GNT) two transformer-based stages view transformer: multi-view geometry, coordinate-aligned features, epipolar lines ray transformer: ray marching rendering, decode point features reconstruct single scene without rendering formula attention map 1 Introduction Topoic: novel view synthesis by NeRF (coordinate-based model + differentiable volumetric rendering)\nProblems: one network overfits one scene with long optimization.\nFormer solutions \u0026amp; drawbacks: Ibrnet, pixelNerf, NLF proved the coordinates are not necessary, but the novel view can be interpolated from other views\u0026rsquo; image features.\nTask:\nContributions \u0026amp; Reason\ncooridnate network and volumetric renderer are composed into a transformer architecture. use multi-view image features to infer coordinate-aligned features. Later these features are decoded to novel view directly without volume rendering. Results statement\n有泛化能力意思是，训练好的模型可以直接用输入图像重建 unseen 场景的新视图？\n2 Related Work Advances in Transformer\nNeural Radiance Fiels: NeRF, Mip-NeRF; surface representation, dynamic scenes, reflection modeling; Generalization nerf: PixelNeRF, IBRNet, MVSNeRF, NeuRay; accelerate nerf.\nTransformer Meets Radiance Fields: IBRNet, NerFormer, Generalizable neural radiance field, NLF, Vision transformer for nerf-based view synthesis; SRT\n3 Preliminaries 4 Method: Make Attention All NeRF needs 4.1 Epipolar Geometry Constrained Scene Representation Multi-View Feature Aggregation.\nVanilla NeRF use MLP to parameterize scene in a backward optimization fashion? (先渲染出图片，依据图片误差再返回去更新辐射场）\nIn contrast, generalizable NeRFs construct the radiance field in a feed-forward scheme\n用图像特征优化辐射场，训练好后，就可以对辐射场应用volume rendering来渲染新视图；而本文是从辐射场映射回图像特征，从而生成新视图\n(2023-12-16) NeRF 是从待渲染的像素出发，向空间发出射线，去查询颜色。而 Forward 是把空间点投影到像素上。\nRepresent the scene as a feature field, where each point in the space has a part of image feature.\nUse attention to fuse all pixel on ResUNet feature maps of source views is memory prohibitive\nOnly fuse the pixels locating in the paring epipolar lines of \u0026ldquo;neighboring views\u0026rdquo; （和 PixelNeRF 一样，不过人家没提对极几何这个词，就是把光线上的点投影到不同视图上）\nMemory-Efficient Cross-View Attention\nOnly use one read-out token in the query sequence to iteratively fuse features from neighbor views\nThe similarity is not computed by dot multiplication, but subtraction, so the attention score is calculated for every channel of the features.\nThe attention scores matrix and \u0026lsquo;V\u0026rsquo; are added by the relative directions between source views and the target view.\nDiscussion on Occlusion Awareness.\n4.2 Attention Driven Volumetric Rendering Different illumination effects and material scenarios need to apply specific handcrafted rendering formula. Data-driven renderer decode the image features into images realizing various phenomena in one way.\nRay Feature Aggregation:\nAnalogy the token features as the color in the volum rendering fomula. Therefore, do attention for coordinate-aligned features to aggregate the final color (rgb) for the novel pixel/ray. mean pooling to compress the feature patch into a pixel use dot-product attention to fully mix features of other points getting comprehensive contextual information. 关于 auto-regressive rendering 的延伸讨论？ Discussion on Depth Cuing\ndepth is the average of marching distance with attention score NLF 有相同的架构，区别在哪？ 5 Experiments single scene; generalization to unseen scenes\n5.1 Implementation Details Source and Target view sampling 在 Blender 数据集上的 PSNR 无明显提升\n(2023-08-16)\nCode train.py\n\\begin{algorithm} \\begin{algorithmic} \\PROCEDURE{train}{args} \\STATE data: [ rgb$_{(1,H,W,3)}$, $\\newline\\qquad$ camera$_{(1,34)}$, $\\newline\\qquad$ rgb\\_path$_{(str)}$,$\\newline\\qquad$ src\\_rgbs$_{(8,H,W,3)}$, $\\newline\\qquad$ src\\_camera$_{(8,34)}$, $\\newline\\qquad$ depth\\_range$_{(1,2)}$ ] \\STATE $\\newline$ \\STATE ray\\_sampler = RaySamplerSingleImage(data) \\STATE ray\\_batch = ray\\_sampler.random\\_sample(N\\_rand) \\STATE featmaps = model.feature\\_net(ray\\_batch[\"src\\_rgbs\"]) \\STATE ret = render\\_rays(ray\\_batch, model, projector, featmaps) \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} llff_test.py __init__() of a training dataset:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def LLFFDataset(Dataset): def __init__(self, args, mode, scenes=(), random_crop=True): scenes = os.listdir(folder_path) # Stuff all scenes into lists for i, sceneName in enumerate(scenes): rgb, poses, bds, render_poses, i_test, imgnames = load_llff_data(sceneName) near, far = np.min(bds), np.max(bds) intrinsics, c2w = batch_parse_llff_poses(poses) self.train_intrinsics.append(intrinsics[i_train]) self.train_poses.append(c2w[i_train]) self.train_rgb_files.append(imgnames[i_train]) self.render_rgb_files.extend(imgnames[i_train]) self.render_intrinsics.extend(intrinsics[i_train]) self.render_poses.extend(c2w[i_train]) self.render_depth_range.extend([[near,far]]*len(i_train)) self.render_train_set_ids.extend([i]*len(i_train)) ","date":"2022-08-23T00:00:00Z","image":"https://vita-group.github.io/GNT/assets/overview.png","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-gnt/","title":"Read: Render - NVS | GNT"},{"content":" torch.view() 只能处理 contiguous tensor，但 torch.reshape() 不限。\n我在改变形状时：latent = latent.view(SB, NS, self.latent_size, B).permute(0,3,1,2).view(-1, NS, self.latent_size)，\n报错： RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\ncontiguous means \u0026ldquo;Two adjacent cells in a row of tensor are neighbors on the memory as well\u0026rdquo;. What is the difference between contiguous and non-contiguous arrays?\n(2023-12-23) \u0026ldquo;数组存储顺序与按行展开的顺序一致\u0026rdquo;。 【代码精读】开山之作MVSNet PyTorch版本超详细分析 - doubleZ的文章 - 知乎\nview vs reshape: What\u0026rsquo;s the difference between reshape and view in pytorch?\nFind whether a tensor is copied or not after reshape: Check if the reshaped tensor will be affected by the original tensor.\nStride (2023-09-28) contiguous means the relative sequence (underlying 1D memory arrangement when they were first created) of elements in a tensor hasn\u0026rsquo;t been changed, although the shape can be mutated. (The data on memory blocks never reshape.)\nE.g., the layout of a = torch.arange(4) means 0,1,2,3 are next to each other in this order. a.stride() is (1,).\nb = a.reshape(2,2) $\\[^{[0,1]}_{[2,3]}]$ is still contiguous. Because only the data\u0026rsquo;s dimensional interpretation changed, and the read order of 0,1,2,3 didn\u0026rsquo;t change. b.stride() is (2,1).\ntorch.flip(b, [0]) $\\[^{[2,3]}_{[0,1]}]$ is contiguous, because they\u0026rsquo;re next to each other in terms of the tensor\u0026rsquo;s .stride() which remains (2,1).\nThe stride is responsible to the initial underlying layout, if the stride gets changed, the tensor won\u0026rsquo;t be contiguous anymore.\nThe operation transpose and permute will change stride, which isn\u0026rsquo;t row-contiguous any longer.\nso b.transpose(0,1) or b.permute(1,0): $\\[^{[0,2]}_{[1,3]}]$ is not contiguous any more.\n.contiguous() will change the stride to match the current shape. What does .contiguous() do in PyTorch?\n(2023-12-22) .contiguous() will copy the data to a new memory strip, which can be checked via .storage(). The new and the original tensors won\u0026rsquo;t affect each other. Pytorch - Contiguous vs Non-Contiguous Tensor / View — Understanding view(), reshape(), transpose() - Kathryn\n.permute() made .stride() decoupled with the tensor\u0026rsquo;s varying shapes mutating from the original shape.\nThat means when the tensor reverts to the original size, the .stride() won\u0026rsquo;t return to the structure matched with the tensor\u0026rsquo;s original shape.\n1 2 3 4 5 6 7 8 9 a = torch.arange(8).reshape(2,4) # contiguous b = a.transpose(0,1) # (4,2) not contiguous b.view(2,4) # Can\u0026#39;t work # because the stride don\u0026#39;t match the target shape b.contiguous().view(2,4) # Or use b.reshape(2,4) # will copy data if .view() don\u0026#39;t work. Therefore, .contiguous() is necessary before view() after transpose() or permute().\nRead-Write Head (2023-10-13) stride 是为了从一条内存上索引（\u0026ldquo;拼凑\u0026rdquo;） 取出 张量的某一个维度，读写头的步幅。 How does NumPy\u0026rsquo;s transpose() method permute the axes of an array?\n计算一个 tensor 的 stride：给定一个 contiguous 的tensor： x.shape = (2,2,4), 则 x.stride() = (8,4,1), 所以 一个维度上的 stride 等于它后面的维数累乘：8=2×4, 4=4×1, 1=1.\n【pytorch Tensor shape 变化 view 与 reshape（contiguous 的理解）】\ntranspose 转置某2个维度，即交换那两个维度上的 stride，不同的维度在内存上走不同的距离。\n.T 会倒序全部维度上的 stride, e.g., (2,2,4) -\u0026gt; (4,2,2)\n(2023-10-23) Examples:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026gt;\u0026gt;\u0026gt; a = torch.arange(6).reshape(2,3,1) # stride: (3,1,1) tensor([[[0], [1], [2]], [[3], [4], [5]]]) \u0026gt;\u0026gt;\u0026gt; b = a.transpose(0,1) # stride: (1,3,1), not contiguous tensor([[[0], [3]], [[1], [4]], [[2], [5]]]) \u0026gt;\u0026gt;\u0026gt; c = a.transpose(1,2) # stride: (3,1,1), contiguous tensor([[[0, 1, 2]], [[3, 4, 5]]]) \u0026gt;\u0026gt;\u0026gt; d = a.transpose(0,2) # stride: (1,1,3), not contiguous tensor([[[0, 3], [1, 4], [2, 5]]]) \u0026gt;\u0026gt; e = a.T # stride: (1,1,3) .transpose swaps strides, .T reverses strides, and permute reorders strides, while .view changes stride to match the shape while keeping elements \u0026ldquo;互相接壤的\u0026rdquo;。\ncontiguous \u0026ldquo;互相接壤的\u0026rdquo;： 矩阵的一行中相邻的 2 个元素，在内存上也相邻。 其中，矩阵一行的末尾与下一行的开头相邻。\n(2023-10-24) contiguous means that no matter how the shape changes, the movement of the read/write head acts like indexing a flattened tensor.\nRectangle\u0026rsquo;s Shape (2023-12-10) Changing the shape of a tensor is like stretching an area-fixed rectangle, although h and w are changed, the relative order of internal elements is not changed.\nImagine dragging the bottom right corner to change the height and width of the rectangle. Alternatively, one can imagine the data as sand grains being enclosed by a size-changble frame. And once the shape changed, the rows are filled up first:\nm e m o r y : R ⋅ ⋅ ⋅ W 3 ⋅ ⋅ ⋅ 1 ↑ h x e ⋅ ⋅ ⋅ 2 a 6 d ⋅ ⋅ ⋅ 3 ⋅ ⋅ ⋅ 4 ⋅ ⋅ ⋅ ⋯ D f r i H c a l , h g l a W n h i g e n e r t e o ⋅ ⋅ ⋅ ⋅ 2 ⋅ ⋅ x ⋅ ⋅ 9 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ 1 8 ⋅ ⋅ The reading order is unchanged, keeping the sequence from 1 to 18:\n1 7 1 3 2 8 1 4 3 9 1 5 4 1 1 0 6 5 1 1 1 7 6 1 1 2 8 1 1 0 2 1 1 3 1 2 4 1 3 5 1 4 6 1 5 7 1 6 8 1 7 9 1 8 (我忘了是不是有这么一种玩具：有一个长方形，框住了一些“棋子”。 当你拖动长方形的一个角的时候，因为它面积是固定的，所以里面的棋子会重新排列。对应到 .view() 就是每行先填满。)\nSince the view doesn\u0026rsquo;t changes the stride of the read/write header, the target shape requires to match the current stride. torch view - Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 xy= torch.cartesian_prod(torch.arange(3), torch.arange(2)) # tensor([0, 0], # [0, 1], # [1, 0], # [1, 1], # [2, 0], # [2, 1]]) # memory: 0 0 0 1 1 0 1 1 2 0 2 1 # shape: (2,6), stride: (2,1) print(xy.view(2,6)) # contiguous, stride: (4,1) # tensor([[0, 0, 0, 1, 1, 0], # [1, 1, 2, 0, 2, 1]]) # 一条龙串下来 xy.t().is_contiguous() # False, stride: (1,2) # tensor([[0, 0, 1, 1, 2, 2], # [0, 1, 0, 1, 0, 1]]) view is filling an empty box from the innermost dimension to outermost by consuming the 1D memory data.\n(2023-12-22) The tensors created by torch.meshgrid() are not contiguous.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 y, x = torch.meshgrid(torch.arange(3), torch.arange(2)) # y: vertical coordinate # tensor([[0, 0], # [1, 1], # [2, 2]]) # Memory is: 0 1 2 # Not contiguous. stride on each dim: (1,0) y.storage() # 0 # 1 # 2 # [torch.LongStorage of size 3] # x: horizontal coordinate # tensor([[0, 1], # [0, 1], # [0, 1]]) # Memory is 0 1 # Not contiguous. stride on each dim: (0,1) x.storage() # 0 # 1 # [torch.LongStorage of size 2] y.contiguous().stride() # stride: (2,1) When reading y and x, the read-write head has to go back or repeat some bytes, instead iterates the 1D memory sequence once, so they\u0026rsquo;re not contiguous.\nrepeat will copy the data, while expand won\u0026rsquo;t, with a stride of 0 on the singleton dimension. (singleton means the size of that dimension is 1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 y.unsqueeze(2).repeat(1,1,2) # shape: (3,2,2), contiguous # tensor([[[0, 0], # [0, 0]], # # [[1, 1], # [1, 1]], # # [[2, 2], # [2, 2]]]) # stride: (4,2,1) y.unsqueeze(2).expand(-1,-1,2) # shape: (3,2,2), Not contiguous # tensor([[[0, 0], # [0, 0]], # # [[1, 1], # [1, 1]], # # [[2, 2], # [2, 2]]]) # stride: (1,0,0) ","date":"2022-08-21T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-contiguity/","title":"memo: PyTorch | Contiguous \u0026 Stride"},{"content":"Background 全连接：线性层串行\n输入图像 in pytorch：(C, H, W) ToTensor doc\nCNN = Feature Extraction + Classification\nCCD 光敏电阻 + 透镜系统， 一个电阻只检测一个光锥区域的光线，根据电阻与光强的函数关系，得到灰度图\ninput channels: rgb\nConv Layer 每次取一个块：(C * kernel_h * kernel_w) 通过 Conv layer 得到 (C\u0026rsquo; * kernel_h\u0026rsquo; * kernel_w\u0026rsquo;)。\n做 Conv layer 时，input 的所有 channel 都做卷积操作（各通道使用的卷积核不同），然后(按不同权重)相加。 这两步做 n 次，那么 output的 channels 就是 n。所以后续的每一层 channel 都包含了input 的全部通道的信息。 CNN 的权重存在于卷积核中。\n(2023-10-22) 各输入通道 乘以的 kernel 不同，然后各通道直接 相加:\n1 2 3 4 5 6 7 8 9 10 a = torch.arange(18.).reshape(1, 2,3,3) # (bs, C, h,w) n = 1 # convolution repeat times conv_lyr = torch.nn.Conv2d(a.size(1), n, kernel_size=3, bias=False) print(conv_lyr.weight) # shape (1,2,3,3) conv_lyr.weight = torch.nn.Parameter( torch.stack([torch.ones(3,3), torch.zeros(3,3)]).unsqueeze(0)) print(conv_lyr.weight) out_a = conv_lyr(a) # (1,1,1,1), tensor([[[[36.]]]]) # i.e. 0+1+2+3+4+...+8 + 0+0+..0 = 36. (2023-10-22) Conv2d 与 FC 做的数学运算 相同：\np p p p i i i i F x x x x C e e e e l l l l L 1 2 3 4 a y c e h r n R G B R G B R G B R G B ' l - - - - - - - - - - - - s s d w i w ₂ w w m ₁ ₃ ₄ 1 d d d d i i i i m m m m 1 1 1 1 o o o o f f f f o o o o u u u u t t t t 1 2 3 4 1 s t A S O u c p u m h o t n r o l t = f i C o o a o f n l n l v 4 o f ⁺ w l p e a i o i y x x u g e e t h r l p t ' s u ⁺ e s t d w w ' c ₁ ₃ s i h n n 1 p l w w s u ₂ ₄ t t 1 c c h h n n l l s 如果是 1x1 的卷积 (stride=1)，即没有邻居像素相加，Conv 就会和 FC 等价: 把每个像素投影到另一个维度的空间。\nchnl1 × w\u0026rsquo; + chnl2 × w\u0026rsquo;\u0026rsquo; + chnl3 × w\u0026rsquo;\u0026rsquo;\u0026rsquo; = out-chnl1\ndim1 × w\u0026rsquo; + dim2 × w\u0026rsquo;\u0026rsquo; + dim3 × w\u0026rsquo;\u0026rsquo;\u0026rsquo; = out-dim1\n(2023-12-01) I already forgot the original meaning of the left picture because I didn\u0026rsquo;t write descriptions.\nFor a FC layer given by 4 samples of 3 dimensions, each dimension multiplied by a factor wₓ becomes a portion of the out dimension. This corresponds to the operations in Conv2d layer: every channel is multiplied by a kernel and then sum up all weighted channels to form one of out channels.\nThe difference is that in FC the 4 samples aren\u0026rsquo;t merged, but Conv2d merged 4 pixels to 1 pixel. Thus, the number of pixels in FC is consistent, whereas conv layer reduces pixels.\nEach channel in feature maps always shares a common 2-D kernel. A Conv2d layer stands for a 4-D kernel as each channel uses different kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch in_channels, out_channels = 5, 10 width, height = 100, 100 kernel_size = 3 batch_size = 1 input = torch.randn(batch_size, in_channels, width, height) # (1,5,100,100) conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size) output = conv_layer(input) # (1,10,98,98) print(conv_layer.weight.shape) # (10,5,3,3) Padding 保持输出图像尺寸不变。3x3核pad 1圈，5x5 pad 2圈\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) conv_layer = torch.nn.Conv2d(1,1, kernel_size=3, padding=1, bias=False) kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3) # (out_chnls, in_chnls, h, w) conv_layer.weight.data = kernel.data output = conv_layer(input) print(output) stride 卷积核移动步长，（减少步数）用于缩小feature map 的大小\nMax Pooling 下采样, 每 h x w 区块里面取最大值，kerenl_size=2 尺寸缩小为原来的一半。该操作无参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2) output = maxpooling_layer(input) print(output) \u0026gt;\u0026gt;\u0026gt; tensor([[[[4., 8.], \u0026gt;\u0026gt;\u0026gt; [9., 8.]]]]) 定义网络：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5) self.pooling = torch.nn.MaxPool2d(kernel_size=2) self.fc = torch.nn.Linear(320, 10) def forward(self, x): batch_size = x.size(0) x = F.relu(self.pooling(self.conv1(x))) x = F.relu(self.pooling(self.conv2(x))) x = x.review(batch_size, -1) # or flatten x = self.fc(x) return x # 使用交叉熵损失，所以不做激活 model = Net() device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_avaliable() else \u0026#34;cpu\u0026#34;) model.to(device) ","date":"2022-08-14T16:52:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/10_cnn%E5%9F%BA%E7%A1%80/","title":"watch: PyTorch - 刘二 10 | CNN Basics"},{"content":"【北京大学】Tensorflow2.0 - bilibili\n创建环境 1 2 3 4 conda create -n tf2.1 python=3.7 conda install -c conda-forge cudatoolkit=10.1 cudnn=7.6 pip install --upgrade pip pip install tensorflow==2.1 导入失败，报错：\n1 2 3 4 5 TypeError: Descriptors cannot not be created directly. If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc \u0026gt;= 3.19.0. If you cannot immediately regenerate your protos, some other possible workarounds are: 1. Downgrade the protobuf package to 3.20.x or lower. 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower). 查看protobuf 版本：pip show protobuf\n1 2 pip uninstall protobuf pip install protobuf==3.19.0 StackOverflow ans\n","date":"2022-08-14T15:15:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/0/","title":"watch: TF2 - PKU 00 | Environment Setup"},{"content":"编码器中是 self-attention，是自编码，q、k、v同源，计算出各单词(query)在整个句子(values、keys) 中的份量； 解码器中是 encoder-decoder attention，q、k(v)不同源，query是解码器已生成内容的词向量，values、keys 来自编码器； decoder 输出一个词向量，还要经过 linear 和 softmax 才能变成单词。\n17 Transformer 的解码器（Decoders）——我要生成一个又一个单词- 爱钓鱼的程序猿\n","date":"2022-08-06T17:20:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/transf-nickchen/17/","title":"watch: Transf - Nick 17 | Decoder"},{"content":"Reference:\nPL Viedo tutorial of AI葵 youtube\nkwea123/pytorch-lightning-tutorial\nPyTorch Lightning official Tutorial\nPL example mnist source code\npytorch-lighting 优点：\n方便在各种设备(cpu/gpu/tpu/多节点)上运行，只需关心算法实现 控制随机数，重现实验结果，固定划分各batch 工程文件夹目录：\nrequirements.txt : 所需依赖包 train.py : main opt.py : hyperparameter models 文件夹： 包含不同的模型 networks1.py datasets 文件夹： 包含不同数据集的 dataloader losses.py: 各种loss func .gitignore: ckpts/, logs/, MNIST/ requirements.txt\n1 2 3 torch==1.11.0 torchvision==0.12.0 pytorch_lightning==1.6.0 Versioning Policy opt.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import argparse def get_opts(): parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--root_dir\u0026#39;, type=str, default=\u0026#39;./data/\u0026#39;, help=\u0026#39;root directory of dataset\u0026#39;) parser.add_argument(\u0026#39;--hidden_dim\u0026#39;, type=int, default=128, help=\u0026#39;number of hidden dimensions\u0026#39;) parser.add_argument(\u0026#39;--val_len\u0026#39;, type=int, default=5000, help=\u0026#39;number of validation samples split from train set\u0026#39;) parser.add_argument(\u0026#39;--batch_size\u0026#39;, type=int, default=128, help=\u0026#39;number of training samples in one batch\u0026#39;) parser.add_argument(\u0026#39;--lr\u0026#39;, type=float, default=1e-4, help=\u0026#39;learning rate\u0026#39;) parser.add_argument(\u0026#39;--num_epochs\u0026#39;, type=int, default=1, help=\u0026#39;number of epochs\u0026#39;) parser.add_argument(\u0026#39;--num_gpus\u0026#39;, type=int, default=1, help=\u0026#39;number of gpus to be used\u0026#39;) parser.add_argument(\u0026#39;--expname\u0026#39;, type=str, default=\u0026#39;test\u0026#39;, help=\u0026#39;experiment name\u0026#39;) # return parser.parse_args() args, unknown = parser.parse_known_args() return args networks1.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torch import nn class myLinearModel(nn.Module): def __init__(self, hidden_dim): super().__init__() self.net = nn.Sequential( nn.Linear(28*28, hidden_dim), nn.ReLU(True), nn.Linear(hidden_dim, 10) ) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; x: (B, 1, 28, 28) channel=1 \u0026#39;\u0026#39;\u0026#39; x = x.flatten(start_dim=1) # (B, 28*28) return self.net(x) train.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 import torch from torch import nn from torch.nn import functional as F from torchvision import datasets, transforms from torch.utils.data import DataLoader, random_split from torch.optim.lr_scheduler import CosineAnnealingLR # from opt import get_opts # from models.networks1 import myLinearModel from pytorch_lightning import LightningModule, Trainer, seed_everything from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar from pytorch_lightning.loggers import TensorBoardLogger seed_everything(1234, workers=True) def get_learning_rate(optimizer): # for recording logs for param_group in optimizer.param_groups: return param_group[\u0026#39;lr\u0026#39;] class MNISTSystem(LightningModule): # LightningModule puts all parts together # Design model def __init__(self, hparams): \u0026#39;\u0026#39;\u0026#39; hparams: all hyper parameters \u0026#39;\u0026#39;\u0026#39; super().__init__() # self.hparams = hparams self.save_hyperparameters(hparams) # store exp conditions for reproduction and loss visualization # network components self.net = myLinearModel(self.hparams.hidden_dim) def forward(self, x): return self.net(x) # Prepare data def prepare_data(self): \u0026#39;\u0026#39;\u0026#39; Download train set and test set (execute only once) \u0026#39;\u0026#39;\u0026#39; datasets.MNIST(self.hparams.root_dir, train=True, download=True) datasets.MNIST(self.hparams.root_dir, train=False, download=True) def setup(self, stage=None): \u0026#39;\u0026#39;\u0026#39; Preprocessing Split the train and validation set (called by every device) \u0026#39;\u0026#39;\u0026#39; dataset = datasets.MNIST(self.hparams.root_dir, train=True, download=False, transform=transforms.ToTensor()) # load data (B, channel, H,W) train_length = len(dataset) self.train_set, self.val_set = random_split(dataset, \\ [train_length - self.hparams.val_len, self.hparams.val_len]) def train_dataloader(self): \u0026#39;\u0026#39;\u0026#39; Split the train set into multiple batches for one epoch \u0026#39;\u0026#39;\u0026#39; return DataLoader(self.train_set, shuffle=True, # every epoch is different num_workers=4, # cpu threads batch_size=self.hparams.batch_size, pin_memory=True) def val_dataloader(self): return DataLoader(self.val_set, shuffle=False, # comparing acc between epochs num_workers=4, batch_size=self.hparams.batch_size, pin_memory=True) # Construct optimizer and loss func def configure_optimizers(self): self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.hparams.lr) schedular = CosineAnnealingLR(self.optimizer, T_max=self.hparams.num_epochs, \\ eta_min=self.hparams.lr/1e2) return [self.optimizer], [schedular] # different models use different optimizers,schedulars # Training cycle def training_step(self, batch, batch_idx): \u0026#39;\u0026#39;\u0026#39; batch: come from iterable train_dataloader batch_idx: index of the current batch \u0026#39;\u0026#39;\u0026#39; imgs, labels = batch # images\u0026#39; pixels, labels logits = self(imgs) # call forward loss = F.cross_entropy(logits, labels) # including softmax # tensorboard self.log(\u0026#39;train/loss\u0026#39;, loss) self.log(\u0026#39;lr\u0026#39;,get_learning_rate(self.optimizer)) return loss def validation_step(self, batch, batch_idx): \u0026#39;\u0026#39;\u0026#39; Compute acc for every batch \u0026#39;\u0026#39;\u0026#39; imgs, labels = batch logits = self(imgs) loss = F.cross_entropy(logits, labels) acc = torch.sum(torch.eq(torch.argmax(logits, -1),labels).to(torch.float32))/len(labels) log = {\u0026#39;val_loss\u0026#39;:loss, \u0026#39;acc\u0026#39;: acc} return log def validation_epoch_end(self, batch_outputs) -\u0026gt; None: \u0026#39;\u0026#39;\u0026#39; Compute average loss/acc among all batches for one epoch \u0026#39;\u0026#39;\u0026#39; mean_loss = torch.stack([x[\u0026#39;val_loss\u0026#39;] for x in batch_outputs]).mean() mean_acc = torch.stack([x[\u0026#39;acc\u0026#39;] for x in batch_outputs]).mean() self.log(\u0026#39;val/loss\u0026#39;, mean_loss, prog_bar=True) # record loss of every step self.log(\u0026#39;val/acc\u0026#39;, mean_acc, prog_bar=True) # show on the progress bar if __name__ == \u0026#39;__main__\u0026#39;: hparams = get_opts() mnistsystem = MNISTSystem(hparams) # construct training system # save weights to files ckpt_cb = ModelCheckpoint(dirpath=f\u0026#39;ckpts/{hparams.expname}\u0026#39;, filename=\u0026#39;{epoch:d}\u0026#39;, # epoch=0,... monitor=\u0026#39;val/acc\u0026#39;, mode=\u0026#39;max\u0026#39;, save_top_k=5) # only store 5 max acc models\u0026#39; weights (-1 all) # progress bar pbar = TQDMProgressBar(refresh_rate=1) callbacks = [ckpt_cb, pbar] # tensorboard events logger = TensorBoardLogger(save_dir=\u0026#39;logs\u0026#39;, name=hparams.expname, default_hp_metric=False) # trainer = Trainer(max_epochs=hparams.num_epochs, callbacks=callbacks, logger = logger, enable_model_summary = True, # print model structure accelerator=\u0026#39;auto\u0026#39;, # devices type devices = hparams.num_gpus, num_sanity_val_steps = 1, # run once val before training to verfiy if it\u0026#39;s normal benchmark=True, # cudnn accelerate need each batch has same size profiler=\u0026#34;simple\u0026#34; if hparams.num_gpus==1 else None, # count time for every operation ) trainer.fit(mnistsystem) ","date":"2022-08-06T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pl_template-aikui/","title":"memo: PL | Template - AIkui"},{"content":"工具类：\nDataset 用于构造数据集，可以用索引取出数据\nDataLoader: 取出一个 mini-batch，做训练\n\u0026ldquo;Batch\u0026rdquo; 是把全部数据输入神经网络，计算预测值，这样可以充分发挥向量计算的并行性，速度很快，但是可能到达鞍点，无法继续训练。可以利用数据的噪声避免停留在鞍点上，从而达到全局最优，所以需要1个样本，1个样本地计算，叫作随机梯度下降SGD，性能较好但是训练时间太长。所以采用 \u0026ldquo;mini-batch\u0026rdquo; 来平衡速度和性能。（通常把mini-batch叫做batch）\nEpoch: 训练轮数，所有的样本都做过一次前馈和反馈\nBatch-size: 每次训练使用的样本量，经过一次前馈、反馈和更新\nIteration：迭代次数，分了几个batch\nDataLoader: 对支持索引，支持获取其长度的数据集进行加载，生成一个 iterable loader, 对数据分组，每次迭代给出一个batch的X和y，并自动转换成Tensor。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torch from torch.utils.data import Dataset #抽象类,不能实例化，只能被它的子类继承 from torch.utils.data import DataLoader #加载, 划分数据 class DiabetesDataset(Dataset): #自定义的类继承自Dataset def __init__(self): pass # def __getitem__(self, index): #使对象支持下标操作，根据索引返回数据 pass def __len__(self): #返回数据集的条数 pass dataset = DiabetesDataset() #实例化数据集，支持索引数据和获取长度 train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) #实例化加载器, 洗牌使每次epoch的数据都不一样，使用2个并行进程读取数据 加载数据有两种方式，如果数据量小，直接把所有的数据加载进内存，之后使用getitem读取；当数据集很大时，需要将其拆分成小文件，在__init__方法中初始化列表，把小文件名放入列表中；有时标签也很大，比如对图片的每个像素预测语义信息，y的维度与x相同，所以也需要拆分，文件名放入列表。在需要使用某数据的时候，用__getitem__方法读取。\npytorch 0.4中出现一个问题：由于windows 和 linux的多进程库不同，spawn与fork不同。Windows下需要把迭代的loader封装起来，不能直接“顶格”写\n1 2 3 4 5 6 train_loader = DataLoder(...) if __name__ == \u0026#39;__main__\u0026#39;: for epoch in range(100): for i, data in enumerate (train_loader, 0): ... 加载糖尿病数据集:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import torch from torch.utils.data import Dataset, DataLoader class DiabetesDataset(Dataset): def __init__(self, filepath): #需要文件名 xy = np.loadtxt(filepath, delimiter=\u0026#39;,\u0026#39;, dtype=np.float32) self.len = xy.shape[0] #样本条数：矩阵的第一个维度 self.x_data = torch.from_numpy(xy[:, :-1]) #样本, 直接放在内存中 self.y_data = torch.from_numpy(xy[:, [-1]]) #标签取最后一列 def __getitem__(self, index): return self.x_data[index], self.y_data[index] #返回一个元组 def __len__(): return self.len data = DiabetesDataset(\u0026#39;diabetes.csv.gz\u0026#39;) train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) for epoch in range(100): for i, data in enumerate(train_loader, 0): #迭代加载器，enumerate获得第几次迭代，x和y的元组放入data inputs, labels = data #取出样本和标签 y_pred = model(inputs) #计算预测值 loss = criterion(y_pred, labels) #计算损失 optimizer.zero_grad() #梯度清零 loss.backward() #反向传播 optimizer.step() #更新梯度 ","date":"2022-08-05T19:25:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/8_%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/","title":"watch: PyTorch - 刘二 08 | Load Datasets"},{"content":"注意力核心作用：特征融合。在输入的序列内做特征融合。\nbilibili\ncnblog\n从海量数据中找出重要的部分，设计一种运算让机器自动甄别数据，对于深度学习领域，为模型添加一个模块，只关心输入输出即可\n如何在深度学习模型上做注意力?\n一张图片的信息分布是不均匀的，人类会给蕴含更多、更重要信息的区域更多的注意力。\n参与“注意力”的两个主体：人脑（query），图片（value=key），人脑知道哪些地方重要，它会着重看图片中的那些地方，所以应该给那些区域更高的权重，也就是计算人脑中的“重要度分布”模版与图片的相似度。\nquery 与 key (=value) 可以做点乘，计算余弦相似度。 一个 query 与 keys 的各个部分做内积得到多个标量，就是各个部分与 query 的相似度，即注意力分数\n对这些分数做一次 softmax()，得到各个key 与 query的相似度的概率分布，就是各个部分的权重，有些部分权重趋于0，则它们被舍弃了，只把重点提取出来，把各个部分加权求和，就得到\u0026quot;被注意过后\u0026quot;的图片了，比原始图片多了每部分的重要度。对这个新图片再做卷积，重要的部分会被保留下来。\n(如果一张图片以像素级别做注意力，计算量太大，所以可以先用卷积提取一下特征，把尺寸降下来，如果卷积太多次了，与原始图片差异太大了，可以用跳连接补充原始信息)\n一般 keys = values，也可以不相等，但二者之间必有某种联系，这样才能依据 query 与 keys 的相似度找出 values 中重要的和不重要的部分。\n每一个 query 都与 keys 点积，则会产生 num_query x num_key 的注意力分数矩阵\n内积得到各个 key 的注意力分数之后，除以根号下方差dk (标准差)，做一个缩放是为了平滑各分数间的差距，如果差额太大，做了e指数，高分数概率会很大，而低分数的概率会很低，然后乘以 value 就是变成很小的值，导致在反向传播时，这些小数值没有足够的梯度（梯度消失），更新不了它对应的权重。或者说如果不除以标准差，各key注意力分数的分布的方差可能很大，大的很大，小的很小，做了softmax后，小数更小。所以除以标准差，让方差为1，削弱差距。(为什么不让方差再小呢？)\n","date":"2022-08-05T17:37:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/transf-nickchen/9_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","title":"watch: Transf - Nick 09 | Attention Mechanism"},{"content":"Code | Arxiv | ProjPage\n一句话：用图像特征训练NeRF，（所以有泛化能力,不局限于单场景，不像nerf只做优化）\n如图1，光线是随机从像素空间选择的（为了随机选一些(N个)空间点），空间点投影到不同视图的 feature maps (512 chnls) 上，得到 (N∗Nviews个) feature volume，送入 mlp 变换成 r,g,b,density，再做 volume rendering\nAbstract NeRF conditioned by few images costly independent optimizing use convolution to learn scene prior no explicit 3D supervision single image novel view synthesis task 1 Introduction Topic: Synthesis novel views for a scene from sparse views\nProblem\nFormer solution \u0026amp; drawbacks:\nDifferentiable neural rendering: represent the sence as a neural network which generates rendered images NeRF: encode the 3D position and view dirs to volume density and color. It needs too many images. Task: predicting NeRFs from one or several images.\nContributions: Utilize pixel-aligned spatial image features to learn scene priors (specifically) Input image → convolutional feature grid → 𝐱,𝐝,feature(residual) → NeRF (pool-based multi-view features fusion) → σ,rgb\nResults statement\nno 3D supervision: 3D shape or object masks not trained in a canonical coordinate system, but in each camera coordinate system \u0026ldquo;convolution preserves the spatial alignment between the image and the output 3D representation\u0026rdquo; flexibility on number of input views in test period (Experiments) ShapeNet; DTU\n2 Related Work Novel View Synthesis Learning-based 3D reconstruction Viewer-centric 3D reconstruction 3 Background NeRF (job) (input-output) x,d -\u0026gt; density, color (training manner) volume rendering (loss func) Limitation: only use geometric consistency, resulting in individual optimization for every view 4 Image-conditioned NeRF (improvement) input spatial image features into network. (Specifically) Two components: convolutional encoder + nerf mlp. Spatial query position is drawn from camera space.\n(paragraph order) From simple case to general case\n4.1 Single-Image pixelNeRF (main idea) The inputs are all from view space. (steps) Input image → feature volume W=E(I) → assign image feature to sample points by projecting the 3D position to 2D location → NeRF network (pipeline) (the role of query view direction) 4.2 Incorporating Multiple Views (main idea) extract information from multi views to resolve geometric ambiguities and able to accept multiple images in test time. No relation with world space\nWorld space can base on any view. (notion explain)\nProject the world query point into each input view space. The fisrt layer processes each view independently and the final layer aggregates all views.\n(model pipeline)\nEncode each input img into feature volume and retrieve the corresponding feature at the sample point\u0026rsquo;s projecting location. The features companied with (xyz,viewdirs) are input to the first layer and all the intermediate vectors are fused by average pooling before entering the final layers, which compress the vector into density and color. 5 Experiments Datasets:\nShapeNet (category-specific and category-agnostic view synthesis); ShapNet scenes with unseen categories and multiple objects; DTU rescaled to 1/4: 400x300 Baselines: SRN, DVR\nMetrics: PSNR, SSIM, LPIPS\nImplementation Details:\nImage encoder: ResNet34; Features: non-pooling, upsampled using bilinear interpolation, concatenated feature maps of every image. Combine the points\u0026rsquo; position and view direction in a ResNet manner (residual) 5.1 ShapeNet Benchmarks ShapeNet for category-specific and category-agnostic\nCategory-specific View Synthesis Benchmark\none-shot and two-shot view synthesis\nA model is trained for reconstructing one category and the used images contains 50 random views per object instance. Only 2 are encoded and fed into network.\nAblations. The benefit of local features\nCategory-agnostic Object Prior\nTrain a single model to the 13 largest categories of ShapeNet pick one view randomly from 24 fixed elevation view of each object instance. 5.2 Pushing the Boundaries of ShapeNet (tasks) less controlled capture scenarios in ShapeNet:\nunseen object categories, multiple-object scene, simulation-to-real transfer on car images. Generalization to novel categories: apply the model on unseen categories\nMultiple-object scenes: The geometric features need can be applied in any view direction (360°)\n5.3 Scene Prior on Real Images Code (2023-07-20)\ncalc_losses():\n\\begin{algorithm} \\caption{Main steps of clac$\\_$losses()} \\begin{algorithmic} \\PROCEDURE{clac-losses}{} \\STATE Sample 4 objects from 88 training scanned objects (folder) as a super batch \\STATE Each object samples randomly 1 or 3 from its total of 49 NV as target images for training \\STATE Sample a batch of rays (N$\\_$rand) \\STATE Extract latent feature maps `self.latent` \\STATE Sample z$\\_$coarse points on the rays \\STATE Transform points onto NS camera space \\STATE Perspective project xyz to 2D location uv on feature maps \\STATE Indexing latent vectors from feature maps by `F.grid$\\_$sample()` \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} (2024-06-06)\nNeuRay: PixelNeRF is an analogy to stereo matching. Intro\nMVSNet projects multiple hypotheses depth planes onto source images (feature maps) and then matches multi-view features with a UNet. Similarly, PixelNeRF projects multiple points on each ray at sampled depth onto the source images.\nMVSNet don\u0026rsquo;t have the problem of occulsion because all it deals with is a photo (plane) without 3D structure from begining to end.\nWhereas NeRF handles the entire 3D space, the occulusion could happen when viewpoint changes.\nNeuRay use image features to determine if a sample point on a ray is visble or not.\nDTU dataset (2023-08-17)\nget_split_dataset() (in \u0026ldquo;data/init.py\u0026rdquo;) will construct different Dataset objects for different datasets.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 flags[\u0026#34;list_prefix\u0026#34;] = \u0026#34;new_\u0026#34; if training: flags[\u0026#34;max_imgs\u0026#34;] = 49 flags[\u0026#34;sub_format\u0026#34;] = \u0026#34;dtu\u0026#34; flags[\u0026#34;scale_focal\u0026#34;] = False flags[\u0026#34;z_near\u0026#34;] = 0.1 flags[\u0026#34;z_far\u0026#34;] = 5.0 # Apply color jitter during train train_aug = ColorJitterDataset train_aug_flags = {\u0026#34;extra_inherit_attrs\u0026#34;: [\u0026#34;sub_format\u0026#34;]} train_set = DVRDataset(path=datadir, stage=\u0026#34;train\u0026#34;, **flags, **kwargs) if train_aug is not None: train_set = train_aug(train_set, **train_aug_flags) ","date":"2022-08-04T00:00:00Z","image":"https://ar5iv.labs.arxiv.org/html/2012.02190/assets/x2.png","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-pixelnerf/","title":"read: Render - NVS | PixelNeRF"},{"content":"Self-Attention(自注意力机制) - bilibili\nvalue 是一个 d 维的向量； 现分析 n 个 value:\nself-attention 的目的是为了使每个 value 带上所有 value 的信息，也就是所有 value 加权相加，相似部分的权重大些，无关部分的权重小些。\n权重即是各 value 与 value 之间的相似度，只需每个 value 都与所有 value 做内积就可得到，内积大说明两个value夹角小，含义相似；如果各value之间正交，比如one-hot编码，那么只有自己和自己乘结果是1，其余都是0。\n用矩阵运算表示就是“自己乘以自己的转置” (n,d)⋅(n,d)ᵀ→(n,n)，得到 nxn 的注意力分数方阵\n把这套机制放到Attention运算中，queries 和 keys 都等于 values。\n但是这些权重是不变的，因为每次 querys 都等于 keys，对协方差矩阵做完softmax 得到的分布总是一样的？ doubt: 实验：两个矩阵分别自己和自己做矩阵乘法，然后做softmax，观察两个结果\n如果仅凭 values 自己调整，要达到任务所需的向量表示，可能速度太慢，所以分别给 q,k,v 加了一层线性变换，反向传播时也调整这个线性层的权重，让 value 的新词向量更快的移动到准确的位置。\u0026ldquo;从而提升模型的拟合能力1\u0026quot;。\nSelf-attention 中每一个 input 都与所有 input 做内积，没有考虑到 input 的顺序，所以原始文本的顺序信息丢失了，所以需要位置编码 1\n例子：\n两个单词 thinking 与 machines，分别乘上 Wq, Wk, Wv 得到线性变换后的 queries (q1,q2), keys (k1,k2), values (v1,v2)\ns11 = q1 × k1, s12 = q1 × k2; 然后 s11, s12 做scale，再做softmax得到两个权值 s11\u0026rsquo; (0.88), s12\u0026rsquo; (0.12), 则 z1 = s11\u0026rsquo; × v1 + s12\u0026rsquo; × v2 就是 thinking 的新的向量表示。\n对于 thinking，初始的词向量（one-hot, Elmo）为 x1，这个x1 与其他词的向量正交，无关，不包含其他单词的任何信息，对 thinking machines 这两个词做完 self-attention 之后，thinking 新的词向量带上了 machines 的信息，带上了多少呢？带上了 machines 与 thinking 相似的部分。或者说：这个新词向量蕴含了 thinking machines 这句话对于 thinking 而言哪个词更重要的信息。\n新的词向量包含了整个句子所有单词的信息，重要的单词占比多一点。\nAttention 与 Self-attention 区别： QKV相乘就是注意力，只是一种运算，但并没有规定 QKV 是怎么来的。通过一个查询变量 Q 去找到 V 里面比较重要的东西：QK相乘求相似度 S，然后 SV 相乘得到 V 的新的向量表示（对于词向量，则它包含了句法/语义特征）。Q 可以是任何一个东西，V也是任何一个东西，K 往往是和V 同源的\n自注意力要求 QKV 同源，QKV 是对同一个 X 乘上了不同的权重矩阵，做了不同的线性变换，使它们在空间中岔开了，X 就是一个词向量，但它表达的信息可能没那么准确，通过反向传播调整它们的权重矩阵，把它们移动了合适的位置上，这个位置的词向量可以为我的任务准确地分配各部分的重要性。\n交叉注意力机制：Q 和 V 不同源，但 K 和 V 同源\nSelf-Attention 代码实现pytorch 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from math import sqrt import torch import torch.nn class SelfAttentionLayer(nn.Module): def __init__(self, input_dim, dim_k, dim_v): \u0026#39;\u0026#39;\u0026#39; Inputs: input_dim: dim of input feature vector dim_k: dim of key is same as query, because they do dot product dim_v: dim of value, suitable for task \u0026#39;\u0026#39;\u0026#39; super(SelfAttentionLayer, self).__init__() self.q = nn.Linear(input_dim, dim_k) self.k = nn.Linear(input_dim, dim_k) self.v = nn.Linear(input_dim, dim_v) self._norm_fact = 1/sqrt(dim_k) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; Input: x: (batch_size, seq_len, input_dim) \u0026#39;\u0026#39;\u0026#39; Q = self.q(x) # (batch_size, seq_len, dim_k) K = self.k(x) # (batch_size, seq_len, dim_k) V = self.v(x) # (batch_size, seq_len, dim_v) atten = nn.Softmax(dim=-1)(torch.bmm(Q,K.permute(0,2,1))) * self._norm_fact # (bs,seq_len,seq_len) output = torch.bmm(atten, V) # Q * K.T() * V, (bs, seq_len, dim_v) return output if __name__ == \u0026#39;__main__\u0026#39;: X = torch.randn(4,3,2) self_atten = SelfAttentionLayer(input_dim=2, dim_k=4, dim_v=5) res = self_atten(X) print(res.size()) Multi-head self-attention:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SelfAttentionMultiHead(nn.Module): def __init__(self, input_dim, dim_k, dim_v, num_heads): super(SelfAttentionMultiHead, self).__init__() assert dim_k % num_heads == 0 # dim_k is divided by num_heads assert dim_v % num_heads == 0 self.q = nn.Linear(input_dim, dim_k) self.k = nn.Linear(input_dim, dim_k) self.v = nn.Linear(input_dim, dim_v) self.num_heads = num_heads self.dim_k = dim_k self.dim_v = dim_v self._norm_fact = 1/sqrt(dim_k) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; Input: x: (batch_size, seq_len, input_dim) \u0026#39;\u0026#39;\u0026#39; # 过线性层之后，拆分成（num_heads, bs, seq_len, dim_k÷num_heads） Q = self.q(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.num_heads) K = self.k(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.num_heads) V = self.v(x).reshape(-1, x.shape[0], x.shape[1], self.dim_v // self.num_heads) print(x.shape) print(Q.size()) # Q * K.T() atten = nn.Softmax(dim=-1)(torch.matmul(Q, K.permute(0,1,3,2))) # (bs, seq_len, seq_len) # Q * k.T() * V output = torch.matmul(atten, V).reshape(x.shape[0], x.shape[1], -1) # (bs, seq_len, dim_v) return output Self-Attention - TF 2 1 2 3 4 5 6 7 8 9 10 11 12 def single_head_attention(z): # z: [None, n, dm] Q = tf.keras.layers.Dense(units = dk)(z) # Q: [None, n, dk] K = tf.keras.layers.Dense(units = dk)(z) V = tf.keras.layers.Dense(units = dv)(z) score = tf.matmul(Q, K, transpose_b = True)/ tf.sqrt(dk * 1.0) # score : [None, n, n] W = tf.nn.softmax(score, axis = -1) # W: [None, n, n] H = tf.matmul(W, V) # H: [None, n, dv] return H Multi-head self-attention 3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, dk, dv, num_heads): \u0026#39;\u0026#39;\u0026#39; Inputs: dk: dim of keys after linear layer num_heads: the dk is divided into num_heads parts \u0026#39;\u0026#39;\u0026#39; super(MultiHeadAttention, self).__init__() assert dk % num_heads == 0 assert dv % num_heads == 0 self.dk = dk self.dv = dv self.num_heads = num_heads self.wq = tf.keras.layers.Dense(dk) self.wk = tf.keras.layers.Dense(dk) self.wv = tf.keras.layers.Dense(dv) def call(self, x): \u0026#39;\u0026#39;\u0026#39; x: a batch of sequences that need to do self-attention (batch_size, seq_len, d_input) \u0026#39;\u0026#39;\u0026#39; seq_len = tf.shape(x)[1] q = self.wq(x) # (batch_size, seq_len, dk) k = self.wk(x) # (batch_size, seq_len, dk) v = self.wv(x) # (batch_size, seq_len, dv) # Split the last dimension and transpose to (bs, num_heads, seq_len_q, dk) Q = tf.transpose(tf.reshape(q, (-1, seq_len, self.num_heads, self.dk//self.num_heads)), perm=[0,2,1,3]) K = tf.transpose(tf.reshape(k, (-1, seq_len, self.num_heads, self.dk//self.num_heads)), perm=[0,2,1,3]) V = tf.transpose(tf.reshape(v, (-1, seq_len, self.num_heads, self.dv//self.num_heads)), perm=[0,2,1,3]) # Attention score = tf.matmul(Q, K, transpose_b=True)/ tf.sqrt(self.dk // self.num_heads * 1.0) # scale by the \u0026#34;dk\u0026#34; of one head attention_weights = tf.keras.activations.softmax(score) scaled_attention = tf.matmul(attention_weights, V) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, d_subspace) concat_attention = tf.reshape(scaled_attention, (-1, seq_len, self.dv)) # (batch_size, seq_len_q, dv) return concat_attention, attention_weights Refer:\n超详细图解Self-Attention - 伟大是熬出来的的文章 - 知乎 Transformer I - zaidalyafeai/AttentioNN - github TF tutorial transformer ","date":"2022-08-02T23:37:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/transf-nickchen/10_%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","title":"watch: Transf - Nick 10 | Self-attention Mechanism"},{"content":"bilibili\n每个单词都要与前后所有单词加权相加，每个单词包含了句子的全员信息，只考虑了相似性，而丢失了 Input sequence 信息 （也就是改变单词顺序，不会改变各单词的词向量）。所以需要先加上位置信息\n为每个 input 做一次位置编码（各 input 的全局顺序）\n\u0026hellip;.\n","date":"2022-07-29T17:28:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/transf-nickchen/14_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/","title":"watch: Transf - Nick 14 | Positional Encoding"},{"content":"bilibili\n多头是为了用不同的权重初始化线性变换\nSelf-attention 输出的新词向量比输入的词向量有更多的句法特征和语义特征\nMulti-Head self-attention 输出的新词向量 比 self-attention 得到的词向量包含更多的\n头的个数用 h 表示，一般 h=8 。并不是直接用输入 X 做Attention 计算，而是先把 X 分成 8 块（分别位于8个子空间），分别计算得到 8 个新词向量，然后把它们拼起来，再做一次线性变换（改变维度与 X 匹配）得到最终词向量\n机器学习的本质：非线性变换 y = activation(wx+b)，把一个向量从一个位置变换到另一个位置上\n多头的作用：把一个向量拆分成8个子向量，8个向量同时优化，可以更快的收敛到合适的位置。分太多也不好。\n词向量一般 512 维，分 8 块；若是视频向量 5120，可以分 80 块\n参数量更大，8套参数(Wq,Wk,Wv)\n","date":"2022-07-29T17:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/transf-nickchen/13_%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B/","title":"watch: Transf - Nick 13 | Multi-head Self-Attention Mechanism"},{"content":"import torch 就是导入pytorch。Python中的torch就是pytorch，所以这里不是import pytorch，而是import torch 也合理。\ntorch.Tensor 包含单一数据类型元素的多维矩阵 有10种张量类型，torch.Tensor是默认张量类型torch.FloatTensor的别名 Note 张量变异方法都带有下划线后缀,它们直接原地修改原张量的属性，而不派生新张量。例如torch.FloatTensor.abs_()直接计算并修改原张量，而torch.FloatTensor.abs()在新张量中计算结果。\nTensors Operation on Tensors 张量有100多种运算，这些运算在GPU上运行比在CPU上快。可使用张量的.to方法转移到GPU上，对大张量的移动需要花费很多时间和内存。\n张量的索引和切片与Numpy很像。\ntensor[...,-1] 和 tensor[:,-1] 都表示取张量的最后一列\nCreation Ops torch.tensor(data) 是一个构造器construtor 复制data, 构造一个张量 当data是一个张量x时，这种方法等效于x.clone().detach()：创建新的leaf 张量，并不在当前计算图中 如果仅希望改变requires_grad标志，使用requires_grad_()和detach()方法来避免复制数据。如果data是ndarry，使用torch.as_tensor()创建张量，不复制数据。 torch.tensor.requires_grad_() 把该张量的属性requires_grad 置为True 有的张量是从DataLoader中来的，需要做一些预处理，再开始让autograd开始记录这个张量上的操作 在原地修改，不需创建新变量，没有复制 torch.tensor.detach() 创建一个新张量，与原张量指向同一块内存，但不允许修改二者的size/stride/storage，否则报错 新张量从当前计算图中分离，不需计算梯度 没有复制 torch.as_tensor() 把data转换为tensor 与 torch.tensor 不同，这种创建方式尽量避免复制数据(指向同一块内存) 如果data是ndarry（或tensor），并且它的dtype和device都与目标输出对应一致，那么就不会复制数据，而是新张量和data共同指向那块内存，改变张量，原data也会改变。 当data是list, tuple, scalar 或其他array_like的数据，或者dtype不一致，或者device不一样，都会复制数据创建新tensor。 torch.sparse COO tensors 一种存储形式: tensor is stored as 2 tensor: indices and values indices are coordinate in tensor Reduce memory consumption Strided tensor stores each elements, while COO tensor only record non-zero numbers. torch.as_strided() 创建一个窗口view，对底层连续的(一维)数据重新排列(\u0026ldquo;裁剪\u0026rdquo;) 输入一个张量，指定输出张量的size和每个维度跳跃的步长stride。 对于 1 2 x = torch.randn(3,5,5) #三维张量，一共125个数据 t = torch.as_strided(x, (3,3,3), (5,3,1)) #输出一个(3,3,3)的张量，最低维度的起点从第一个数开始，跳跃步长为1，倒数第2维度的每个起点,跳跃步长为3，最高维度的每个起点间隔步长为5。 根据size和stride可以判断内存上是否连续。对pytorch中Tensor的剖析 torch.from_numpy(ndarray) 新张量与ndarry共享同一块内存，但此张量不能修改size Reverse a dim, e.g. from bottom to top, or from right to left.\n1 xy = torch.flip(xy, [1]) # to match the code ","date":"2022-07-28T15:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch_torch/","title":"memo: PyTorch | Tensor Ops"},{"content":"卷积就是特征提取器，就是CBAPD: convolution, batchnorm, activateion, pooling, dropout\ntf中定义卷积层，指定卷积核的个数，kernel核长\n1x1卷积核是为了减少计算量，减少特征图（通道）的个数\n","date":"2022-07-28T11:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/5_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"watch: TF2 - PKU 05 | CNN"},{"content":"统计语言模型：用概率估计下一个单词是哪个\nn 元语言模型：只分析 n 个单词\n神经网络语言模型：用词向量表示每个单词，词向量越紧凑越精炼越好（相对于one-hot编码）\n","date":"2022-07-26T22:41:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/transf-nickchen/4_%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","title":"watch: Transf - Nick 04 | 统计语言模型"},{"content":"4 网络八股扩展 4.1 自制数据集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 import tensorflow as tf from PIL import Image import numpy as np import os train_dir = \u0026#39;./mnist_image_label/mnist_train_jpg_60000/\u0026#39; train_txt = \u0026#39;./mnist_image_label/mnist_train_jpg_60000.txt\u0026#39; x_train_savepath = \u0026#39;./mnist_image_label/mnist_x_train.npy\u0026#39; y_train_savepath = \u0026#39;./mnist_image_label/mnist_y_train.npy\u0026#39; test_dir = \u0026#39;./mnist_image_label/mnist_test_jpg_10000/\u0026#39; test_txt = \u0026#39;./mnist_image_label/mnist_test_jpg.txt\u0026#39; x_test_savepath = \u0026#39;./mnist_image_label/mnist_x_test.npy\u0026#39; y_test_savepath = \u0026#39;./mnist_image_label/mnist_y_test.npy\u0026#39; def generateDS(dir, labels_path): f = open(labels_path, \u0026#39;r\u0026#39;)\t# 存有 文件名 及其 标签 contents = f.readlines()\t# 读取所有行 f.close()\t# 关闭 x, y_ = [], []\t# 每张图对应的灰度值数据和标签 for content in contents:\t# 逐行读出 value = content.split()\t# 以空格分开 img_path = dir + value[0]\t# 图片名 img = Image.open(img_path) img = np.array(img.convert(\u0026#39;L\u0026#39;)) # 8位灰度图像 img = img/255. x.append(img)\t# 灰度值np.array 放入列表 y_.append(value[1])\t# 标签 放入列表 print(\u0026#39;loading: \u0026#39;+content) x = np.array(x)\t# 列表变 np.array y_ = np.array(y_) y_ = y_.astype(np.int64)\t# 标签是64位整型 return x, y_ if os.path.exists(x_train_savepath) and os.path.exists(y_train_savepath) and os.path.exists(x_test_savepath) and os.path.exists(y_test_savepath): print(\u0026#39;---Loading dataset---\u0026#39;) x_train_save = np.load(x_train_savepath)\t# 读 y_train = np.load(y_train_savepath) x_test_save = np.load(x_test_savepath) y_test = np.load(y_test_savepath) x_train = np.reshape(x_train_save, (len(x_train_save),28,28))\t# 变形 x_test = np.reshape(x_test_save, (len(x_test_save),28,28)) else:\t#不存在，要制作数据集 print(\u0026#39;---Generating dataset---\u0026#39;) x_train, y_train = generate(train_dir, train_txt) x_test, y_test = generate(test_dir, test_txt) print(\u0026#39;---Saving dataset\u0026#39;)\t# 保存为以后使用 x_train_save = np.reshape(x_train, (len(x_train), -1)) x_test_save = np.reshape(x_test, (len(x_test), -1)) np.save(x_train_savepath, x_train_save) np.save(y_train_savepath, y_train) np.save(x_test_savepath, x_test_save) np.save(y_test_savepath, y_test) 4.2 数据增强 用于扩展数据集，对图像的增强就是对图像的简单形变，用来应对因拍照角度不同引起的图片变形\n1 2 3 4 5 6 7 8 9 image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator( rescale = 所有数据将乘以该数值， rotation_range = 随机旋转角度数范围， width_shift_range = 随机宽度偏移量， height_shift_range = 随机高度偏移量， horizontal_flip = 是否随机水平翻转， zoom_range = 随机缩放的范围[1-n, 1+n]) image_gen_train.fit(x_train) 其中 x_train 需要是四维，需要变形：x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)，把 (60000, 28, 28) -\u0026gt; (60000, 28, 28, 1)\n例如：\n1 2 3 4 5 6 7 8 9 image_gen_train = ImageDataGenerator( rescale = 1. / 1., # 若为图像，分母为255时，可归至0~1 rotation_range = 45, # 随机45度旋转 width_shift_range=.15, # 宽度偏移 height_shift_range=.15, # 高度偏移 horizontal_flip=False, # 水平翻转 zoom_range = 0.5 # 将图像随机缩放阈量50% ) image_gen_train.fit(x_train) 模型训练也要改，把 model.fit(x_train, y_train, batch_size=32, ...) 改为 model.fit(image_gen_train.flow(x_train, y_train, batch_size=32), ...)\n4.3 断点续训, 存取模型 读取模型：load_weights(文件路径)\n1 2 3 4 checkpoint_save_path = \u0026#39;./checkpoint/mnist.ckpt\u0026#39; # 定义文件路径 if os.path.exists(checkpoint_save_path + \u0026#39;.index\u0026#39;): # 存在索引表说明已经保存过参数了 print(\u0026#39;---- Loading the model ----\u0026#39;) model.load_weights(checkpoint_save_path) 保存模型：\n1 2 3 4 5 6 7 8 9 cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_save_path, # 保存路径 save_weights_only = True, # 是否只保留 weights save_best_only = True, # 是否只保留最优结果 # 在训练过程中保存，记录到history中 history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) 4.4 提取可训练参数 返回模型中可训练的参数 model.trainable_variables\n设置print输出格式：np.set_printoptions(threshold=超过多少省略显示) np.inf 表示无限大\n1 2 3 4 5 6 7 8 9 10 print(model.trainable_variables) # 把参数直接打印出来 file = open(\u0026#39;./weights.txt\u0026#39;, \u0026#39;w\u0026#39;) # 把参数存入文件 for v in model.trainable_variables: file.write(str(v.name) + \u0026#39;\\n\u0026#39;) file.write(str(v.shape) + \u0026#39;\\n\u0026#39;) file.write(str(v.numpy()) + \u0026#39;\\n\u0026#39;) file.close() 4.5 acc/locc 可视化 用于查看训练效果。history中记录了训练集的loss和sparse_categorical_accuracy；测试集上的val_loss 和 val_sparse_categorical_accuracy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 acc = history.history[\u0026#39;sparse_categorical_accuracy\u0026#39;] val_acc = history.history[\u0026#39;val_sparse_categorical_accuracy\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] plt.subplot(1,2,1) plt.plot(acc, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(val_acc, label=\u0026#39;Validation Accuracy\u0026#39;) plt.title(\u0026#39;Training and Validation Accuracy\u0026#39;) plt.legend() plt.subplot(1,2,2) plt.plot(loss, label=\u0026#39;Training Loss\u0026#39;) plt.plot(val_loss, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training and Validation Loss\u0026#39;) plt.legend() plt.show() 4.6 调用模型 返回前向传播计算结果：predict(输入特征，batch_size=整数)\n三步：复现模型，加载参数，预测\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from PIL import Image import numpy as np import tensorflow as tf model_save_path = \u0026#39;./checkpoint/mnist.ckpt\u0026#39; model = tf.keras.models.Sequential([ # 复现网络 tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)]) model.load_weights(model_save_path) # 加载网络 preNum = int(input(\u0026#34;Input the number of test pictures: \u0026#34;)) # 接收用户输入 for i in range(preNum): image_path = input(\u0026#34;the path of test picture: \u0026#34;) # 输入 1.png img = Image.open(image_path) # 读取图片 # 预处理输入图片的格式 与 训练数据一致 img = Img.resize((28, 28), Image.ANTIALIAS) # 变为 28x28 与训练图片尺寸相同 img_arr = np.array(img.convert(\u0026#39;L\u0026#39;)) # 变为灰度图 img_arr = 255 - img_arr # 黑白反转 # 预处理或者变为只有黑白像素的高对比度图片, 2种方法二选一 for i in range(28): for j in range(28): if img_arr[i][j] \u0026lt; 200: # 小于200 全变白. 可滤去背景噪声，当阈值选择合适识别效果更好 img_arr[i][j] = 255 else: img_arr[i][j] = 0 img_arr = img_arr / 255.0 # 归一化 x_predict = img_arr[tf.newaxis, ...] # (28, 28) -\u0026gt; (1, 28, 28) result = model.predict(x_predict) # 输入网络 pred = tf.argmax(result, axis=1) # 返回最大概率值的索引 print(\u0026#39;\\n\u0026#39;) tf.print(pred) ","date":"2022-07-24T16:41:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/4_%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1%E6%89%A9%E5%B1%95/","title":"watch: TF2 - PKU 04 | NN Framework Extension"},{"content":"3 keras搭建神经网络 3.1 神经网络搭建八股 import train data, test data model = tf.keras.models.Sequential model.compile 设置优化器，损失函数，评价指标 model.fit 设置训练过程，输入输出 epoch，batch model.summary 打印网络结构和参数统计 model = tf.keras.models.Sequential([网络结构]) # 描述各层网络\n网络结构举例：\n拉直层：tf.keras.layers.Flatten() 把输入特征变成一维数组\n全连接层：tf.keras.layers.Dense(神经元个数，activation=\u0026quot;激活函数\u0026quot;，kernel_regularizer=哪种正则化) 。激活函数可选：\u0026lsquo;relu\u0026rsquo;, \u0026lsquo;softmax\u0026rsquo;, \u0026lsquo;sigmoid\u0026rsquo;, \u0026rsquo;tanh\u0026rsquo;；kernel_regularizer可选：tf.keras.regularizers.l1()、tf.keras.regularizers.l2()\n卷积层：tf.keras.layers.Conv2D(filters=卷积核个数，kernel_size=卷积核尺寸，strides=卷积步长，padding=\u0026quot;valid\u0026quot; or \u0026quot;same\u0026quot;)\nLSTM层：tf.keras.layers.LSTM()\nmodel.compile(optimizer=优化器，loss=损失函数，metrics=[\u0026quot;准确率\u0026quot;])\noptimizer可选：\n\u0026lsquo;sgd\u0026rsquo; 或 tf.keras.optimizers.SGD(lr=学习率，momentum=动量参数) \u0026lsquo;adagrad\u0026rsquo; 或 tf.keras.optimizers.Adagrad(lr=学习率) \u0026lsquo;adadelta\u0026rsquo; 或 tf.keras.optimizers.Adadelta(lr=学习率) \u0026lsquo;adam\u0026rsquo; 或 tf.keras.optimizers.Adam(lr=学习率，beta_1=0.9, beta_2=0.999) loss 可选：\n\u0026lsquo;mse\u0026rsquo; or tf.keras.losses.MeanSquaredError() \u0026lsquo;sparse_categorical_crossentropy\u0026rsquo; or tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) , from_logits 为True表示没经过softmax，是网络的直接结果 Metrics 可选：\n\u0026lsquo;accuracy\u0026rsquo;：y_ 和 y 都是数值，类别是数字，如 y_=[1], y=[1] \u0026lsquo;categorical_accuracy\u0026rsquo;，target是独热码，pred是概率分布，如 y_=[0,1,0], y=[0.256, 0.695, 0.048] \u0026lsquo;sparse_categorical_accuracy\u0026rsquo;: target是数值，pred是独热码 (概率分布)，如 y_ = [1], y=[0.256, 0.695, 0.048] model.fit(训练集的输入特征，训练集的标签，batch_size= , epoches= , validation_data=(测试集的输入特征，测试集的标签)【或用 validation_split=从训练集划分多少比例给测试集】，validation_freq=多少次epoch测试一次)\n3.2 Iris 代码用keras重写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf from sklearn import datasets import numpy as np x_train = datasets.load_iris().data y_train = datasets.load_iris().target np.random.seed(116) np.random.shuffle(x_train) np.random.seed(116) np.random.shuffle(y_train) tf.random.set_seed(116) # 设计模型 model = tf.keras.models.Sequential([ tf.keras.layers.Dense(3, activation=\u0026#39;softmax\u0026#39;, kernel_regularize=tf.keras.regularizers.l2()) ]) # 配置训练方法 model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.1), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) # 训练过程 model.fit(x_train, y_train, batch_size=32, epoches=500, validataion_split=0.2, validation_freq=20) # 训练集的20%做测试集 model.summary() Sequential 搭建上层输出就是下层输入的顺序网络结构，但无法写出带有跳连接的非顺序网络结构，\n继承Model类 自定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from tensorflow.keras.layers import Dense from tensorflow.keras import Model class MyModel(Model): def __init__(self): super(MyModel, self).__init__() #定义网络结构块 self.d1 = Dense(3, activation=\u0026#39;sigmoid\u0026#39;) def call(self, x): #调用网络结构块，实现前向传播 y = self.d1(x) return y model = MyModel() MNIST 数据集 总共7万张图片，6万的训练集，1万作为测试集\n使用Sequential\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import tensorflow as tf from matplotlib import pyplot as plt # 读取MNIST数据集 mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train/255.0, x_test/255.0\t# 输入数值变小更适合神经网络 model = tf.keras.models.Sequential([ # 将输入样本拉直为一维向量，784个像素点的灰度值 tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) model.fit(x_train, y_train, batch_size=32, epochs=5, validation=(x_test, y_test), validation_freq=1) model.summary() 使用 Model类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras import Model mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class MyModel(Model): def __init__(self): super(MyModel, self).__init__() self.flatten = Flatten() self.d1 = Dense(128, activation=\u0026#39;relu\u0026#39;) self.d2 = Dense(10, activation=\u0026#39;softmax\u0026#39;) def call(self, x): x = self.flatten(x) x = self.d1(x) y = self.d2(x) return y model = MyModel() model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) model.fit(x_train, y_train, batch_size=32, epochs=5, validation=(x_test, y_test), validation_freq=1) model.summary() FASHION 数据集\n1 2 fashion = tf.keras.datasets.fashion_mnist (x_train, y_train), (x_test,y_test) = fashion.load_data() ","date":"2022-07-24T14:01:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/3_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%85%AB%E8%82%A1/","title":"watch: TF2 - PKU 03 | NN Building Steps"},{"content":"2 神经网络优化 2.1 常用函数 在每个元素上执行条件语句，为真则返回A，为假则返回B：tf.where(条件语句，真返回A，假返回B)\n1 2 3 4 a = tf.constant([1, 2, 3, 1, 1]) b = tf.constant([0, 1, 3, 4, 5]) c = tf.where(tf.greater(a,b), a, b)\t# 若a\u0026gt;b, 返回a对应位置的元素，否则返回b对应位置的元素 print(c)\t# tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32) 返回一个 [0,1) 之间的随机数：np.random.RandomState.rand(维度) 维度为空时返回标量\n1 2 3 4 5 rdm = np.random.RandomState(seed=1)\t# 设置随机种子每次生成的随机数相同 a = rdm.rand() b = rdm.rand(2,3) print(a) # 0.4170220047 print(b) # [[7.20324493e-01 1.14374817e-04 3.02332573e-01] [1.46755891e-01 9.23385948e-02 1.86260211e-01]] 将两个数组按垂直方向叠加 np.vstack(数组1，数组2)\n1 2 3 4 a = np.array([1, 2, 3])\t# shape=(3,) b = np.array([4, 5, 6]) c = np.vstack((a,b)) print(c)\t# [[1 2 3] [4 5 6]] shape=(2,3) 生成网格坐标\nnp.mgrid[起始值：结束值：步长，起始值：结束值：步长，...] 生成若干维度的等差数组，不包括结束值\nx.ravel() 将多维数组x变为一维数组，“把变量拉直”\nnp.c_[数组1，数组2，...] 数组对应位置元素配对\n1 2 3 4 5 6 7 import numpy as np x, y = np.mgrid[1:3:1, 2:4:0.5] # x坐标+1递增，y坐标+0.5递增 grid = np.c_[x.ravel(), y.ravel()] # 两个(8,) array配对 print(x) # [[1. 1. 1. 1.] [2. 2. 2. 2.]] shape=(2,4) print(y) # [[2. 2.5 3. 3.5] [2. 2.5 3. 3.5]] print(grid) # [[1. 2.] [1. 2.5] [1. 3.] [1. 3.5] [2. 2.] [2. 2.5] [2. 3.] [2. 3.5]] shape=(8,2) 神经网络复杂度\n多用网络层数和网络参数的个数表示:\n空间复杂度：层数=隐藏层的层数+1个输出层；总参数=总w+总b\n时间复杂度：乘加运算次数（多少次wx+b）\n2.2 指数衰减学习率 先用较大的学习率，快速得到较优解，然后逐步减小学习率，使模型在训练后期稳定\n指数衰减学习率 = 初始学习率 x 学习率衰减率 ^ (当前epoch(或batch) / 多少epoch(或batch)衰减一次)\n1 2 3 4 5 6 7 8 9 10 11 12 13 epoch = 40 LR_BASE = 0.2 LR_DECAY = 0.99 LR_STEP = 1 for epoch in range(epoch): lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP) with tf.GradientTape() as tape: loss = tf.square(w+1) grads = tape.gradient(loss, w) w.assign_sub(lr * grads) print(f\u0026#34;After {epoch} epoch, w is {w.numpy()}, loss is {loss}, lr is {lr}\u0026#34;) 2.3 激活函数 sigmoid 函数 作用：把无限变有限，把无穷归一，引入非线性，全域可导（导数在0-0.25之间）；缺点：应用链式法则容易造成梯度消失；输入特征最好使均值为0的小数，但经过sigmoid后变为正数，导致收敛慢；其中有幂运算，计算时间长\ntf.math.tanh(x) 激活后的输出的均值是0，导数在0-1之间，仍易造成梯度消失，两次幂运算，计算时间 更长\ntf.nn.relu(x) f(x)=max(x,0) 优点：导数不是0就是1，在正区间内解决了梯度消失问题；只需判断输入是否大于0，计算速度快；收敛速度远快于sigmoid 和 tanh。缺点：输出不是以0为均值，收敛慢；Dead Relu问题，当输入特征是负数时，激活函数输出为0，反向传播时，梯度为0，导致相应的参数无法更新。可以改进随机初始化，避免过多的负数特征输入Relu函数；可以通过减小学习率，减小参数分布的巨大变化，避免训练中产生过多的负数特征\ntf.nn.leaky_relu(x) f(x) = max(ax, x) 是为了解决Relu在负区间的导数为0 引起神经元死亡问题而设计的，它在负区间引入了固定的斜率a。理论上来讲，Leaky Relu 有 Relu 的所有优点，外加不会有 Dead Relu问题，但是在实际操作中，并没有完全证明 Leaky Relu 总好于Relu\n对于初学者的建议：\n首选relu激活函数 学习率设置较小值 输入特征标准化，即让输入特征满足以0为均值，以1为标准差的正态分布 初始参数中心化，即让随机生成的参数满足以0为均值，根号下当前层输入特征个数分之2 为标准差的正态分布。 2.4 损失函数 预测值y 与 已知答案 y_ 的差距\nNN优化目标：loss最小，主流三种计算方法：mse，交叉熵，自定义\nloss_mse = tf.reduce_mean(tf.square(y_ - y))\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import tensorflow as tf import numpy as np SEED = 23455 rdm = np.random.RandomState(seed=SEED) x = rdm.rand(32, 2)\t# 真实y = x1+x2 y_ = [[x1+x2 + (rdm.rand()/10.0 - 0.05)] for (x1, x2) in x] # 加上噪声[0,1)/10 = [0,0.1); [-0.05, 0.05) x = tf.cast(x, dtype=tf.float32) w1 = tf.Variable(tf.random.normal([2,1], stddev=1, seed=1)) # 初始化网络参数 2x1 epoch = 15000 lr = 0.002 for epoch in range(epoch): with tf.GradientTape() as tape: y = tf.matmul(x, w1)\t# 前向 loss_mse = tf.reduce_mean(tf.square(y_ - y)) grads = tape.gradient(loss_mse, w1)\t# 损失函数对参数求偏导 w1.assign_sub(lr * grads)\t# 更新参数 if epoch % 500 == 0:\t# 每迭代500轮，打印参数 print(f\u0026#34;After {epoch} training setps, w1 is {w1.numpy()}\u0026#34;) print(f\u0026#34;final w1 is {w1.numpy()}\u0026#34;) # [[1.0009] [0.9977]] 。。。。\n2.6 优化器 参数更新：下一时刻的参数 等于 当前时刻的参数 减去 η （梯度下降）。η等于学习率乘上（一阶动量除以根号下二阶动量）。\nSGD中无动量，一阶动量等于loss对参数的偏导数，二阶动量=1，所以 η 是沿梯度方向改变的步长。\n单层网络 w*x+b 应用 SGD：\n1 2 w1.assign_sub(lr * grads[0]) b1.assign_sub(lr * grads[1]) SGDM 在SGD基础上增加了一阶动量，一阶动量等于上一时刻的一阶动量与当前时刻的梯度共同作用：mₜ = β⋅mₜ₋₁ + (1-β)⋅gₜ ， β是接近1的系数（经验值0.9），所以上一时刻的动量占主导；二阶动量=1。 一阶动量是“指数滑动平均值”？过去一段时间的平均值？\n1 2 3 4 5 6 7 8 m_w, m_b = 0, 0 beta = 0.9 # sgd-momentum m_w = beta * m_w + (1-beta) * grads[0]\t# grads是loss对各参数的偏导数 m_b = beta * m_b + (1-beta) * grads[1] w1.assign_sub(lr * m_w) b1.assign_sub(lr * m_b) Adagrad 在SGD 基础上增加了二阶动量，可以对模型中的每个参数分配自适应学习率了。一阶动量与SGD设置相同，等于loss的梯度，二阶动量是从开始时刻到现在，梯度平方的累计和 $V_t = \\sum_{τ=1}^t g_τ^2$\n1 2 3 4 5 6 7 v_w, v_b = 0, 0\t# 二阶动量零时刻初值为0 # 在 for 循环里，每batch更新一次： v_w += tf.square(grads[0])\t# 梯度平方的累计和 v_b += tf.square(grads[1]) w1.assign_sub(lr * grads[0] / tf.sqrt(v_w)) b1.assign_sub(lr * grads[1] / tf.sqrt(v_b)) RMSProp 在SGD基础上增加了二阶动量，一阶动量仍等于当前时刻loss的梯度，二阶动量 $V_t = β⋅V_{t-1} + (1-β)⋅g_t^2$\n1 2 3 4 5 6 7 8 v_w, v_b = 0, 0\t# 二阶动量零时刻初值为0 beta = 0.9 # 在 for 循环里，每batch更新一次： v_w += beta*v_w + (1-beta) * tf.square(grads[0])\t# 梯度平方 与 上一时刻的二阶梯度的加权和 v_b += beta*v_b + (1-beta) * tf.square(grads[1]) w1.assign_sub(lr * grads[0] / tf.sqrt(v_w)) b1.assign_sub(lr * grads[1] / tf.sqrt(v_b)) Adam 同时结合了SGDM的一阶动量和RMSProp的二阶动量，并分别增加修正系数：mₜ = β₁⋅mₜ₋₁ + (1-β₁)⋅gₜ ，$\\hat{m_t} = \\frac{m_t}{1-β_1^t}$ ，Vₜ = β₂⋅Vₜ₋₁ + (1-β₂)⋅gₜ²，$\\hat{V_t}=\\frac{V_t}{1-β_2^t}$ , t 是从开始到现在经历的所有batch数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 m_w, m_b = 0, 0 v_w, v_b = 0, 0 beta1, beta2 = 0.9, 0.999 delta_w, delta_b = 0, 0 global_step = 0 m_w = beta1 * m_w + (1 - beta1) * grads[0]\t# 一阶动量=上一时刻与此刻梯度加权和 m_b = beta1 * m_b + (1 - beta1) * grads[1] v_w = beta2 * v_w + (1-beta2) * tf.square(grads[0])\t# 二阶动量=上一时刻与此刻梯度平方的加权和 v_b = beta2 * v_b + (1-beta2) * tf.square(grads[1]) # 修正 m_w_correction = m_w / (1-tf.pow(beta1, int(global_step))) m_b_correction = m_b / (1-tf.pow(beta1, int(global_step))) v_w_correction = v_w / (1-tf.pow(beta2, int(global_step))) v_b_correction = v_b / (1-tf.pow(beta2, int(global_step))) w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction)) b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction)) 不同优化器训练速度不同\n","date":"2022-07-24T13:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/2_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/","title":"watch: TF2 - PKU 02 | NN Optimization Methods"},{"content":"创建张量 tf.constant(张量内容, dtype=数据类型(可选))\n1 2 3 4 5 import tensorflow as tf a = tf.constant([1,5], dtype=tf.int64) # 创建一阶张量, 2个元素 print(a) # tf.Tensor([1,5], shape=(2,), dtype=int64). tf1.x不会显示元素 print(a.dtype) # \u0026lt;dtype: \u0026#39;int64\u0026#39;\u0026gt; print(a.shape) # (2,) 将numpy的数据类型转换为 Tensor 数据类型:\ntf.convert_to_tensor(数据名，dtype=数据类型（可选）)\n1 2 3 4 5 6 import tensorflow as tf import numpy as np a = np.arange(0, 5) b = tf.convert_to_tensor(a, dtype=tf.int64) print(a) # [0 1 2 3 4] print(b) # tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64) 生成全0，全1，全指定值的张量：\n1 2 3 4 5 tf.zeros([2,3]) #tf.Tensor([[0. 0. 0.] [0. 0. 0 .]],shape=(2,3), dtype=float32) tf.ones(4) #tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32) tf.fill([2,2], 9) # tf.Tensor([[9 9] [9 9]], shape=(2,2),dtype=int32) 生成正态分布的随机数（默认均值为0，标准差为1），常用于初始化参数:\ntf.random.normal(维度，means=均值，stddev=标准差)\n生成截断式正态分布的随机数，分布更集中在均值附近，随机数取值在正负2个标准差之内 (mu-2sigma, mu+2sigma)，如果落在外面则重新生成：\ntf.random.truncated_normal(维度， mean=均值，stddev=标准差)\n1 2 d = tf.random.normal([2,2], mean=0.5, stddev=1) e = tf.random.truncated_normal([2,2], mean=0.5, stddev=1) 生成均匀分布随机数 [min, max) 左闭右开：\ntf.random.uniform(维度，minval=最小值，maxval=最大值)\n常用函数 强制 tensor 转换为指定类型 tf.cast(张量名，dtype=数据类型) 计算张量维度上元素的最小值 tf.reduce_min(张量名) 找到张量中的最大元素：tf.reduce_max(张量名) 1 2 3 4 5 6 7 x1 = tf.constant([1., 2., 3.], dtype=tf.float64) print(x1) # tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64) x2 = tf.cast(x1, tf.int32) print(x2) # tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64) print(tf.reduce_min(x2), tf.reduce_max(x2)) # tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(1, shape=(), dtype=int32) axis指定操作方向，对于二维张量，axis=0表示对第0维操作；若不指定axis，则所有元素参与计算 1 2 3 4 5 x = tf.constant([[1,2,3], [2,2,3]]) print(tf.reduce_mean(x)) # 所有元素的平均值 tf.Tensor(2, shape=(), dtype=int32) print(tf.reduce_mean(x, axis=0)) # tf.Tensor([1 2 3], shape=(3,) dtype=int32) print(tf.reduce_sum(x, axis=1)) # 对第1维求和 tf.Tensor([6 7], shape=(2,) dtype=int32) tf.Variable() 将变量标记为“可训练”，被标记的变量会在反向传播中记录梯度信息。在神经网络训练中，常用该函数标记待训练参数。 w = tf.Variable(tf.random.normal([2,2], mean=0, stddev=1)), 把生成的随机数标记为可训练\n常用数学运算：tf.add, tf.subtract, tf.multiply, tf.divide, tf.square, tf.pow, tf.sqrt, tf.matmul。 1 2 3 4 tf.add(张量1，张量2) tf.subtract(张量1，张量2) tf.multiply(张量1，张量2) tf.divide(张量1，张量2) 只有维度相同的张量才可以做四则运算\n1 2 3 4 5 6 a = tf.ones([1,3]) b = tf.fill([1,3], 3.] print(tf.add(a,b)) print(tf.subtract(a,b)) # tf.Tensor([[-2. -2. -2.]], shape=(1,3), dtype=float32) print(tf.multiply(a,b)) print(tf.divide(b,a)) # tf.Tensor([[3. 3. 3.]], shape=(1,3), dtype=float32) 两矩阵相乘：\n1 2 3 a = tf.ones([3,2]) b = tf.fill([2,3], 3.) print(tf.matmul(a,b)) # tf.Tensor([[6. 6. 6.] [6. 6. 6.] [6. 6. 6.], shape=(3,3), dtype=float32) 把特征和标签配对 tf.data.Dataset.from_tensor_slices((输入特征，标签))，Numpy和Tensor格式都适用 1 2 3 4 5 6 features = tf.constant([12, 23, 10, 17]) # 一个数是一个样本 labels = tf.constant([0, 1, 1, 0]) dataset = tf.data.Dataset.from_tensor_slices((features, labels)) print(dataset) for element in dataset: print(element) 运行结果：\n1 2 3 4 5 \u0026lt;TensorSliceDataset shapes: ((),()), types: (tf.int32, tf.int32)\u0026gt; （特征，标签）对 (\u0026lt;tf.Tensor: id=9, shape=(), dtype=int32, numpy=12\u0026gt;, \u0026lt;tf.Tensor: id=10, shape=(), dtype=int32, numpy=0\u0026gt;) (\u0026lt;tf.Tensor: id=11, shape=(), dtype=int32, numpy=23\u0026gt;, \u0026lt;tf.Tensor: id=12, shape=(), dtype=int32, numpy=1\u0026gt;) (\u0026lt;tf.Tensor: id=13, shape=(), dtype=int32, numpy=10\u0026gt;, \u0026lt;tf.Tensor: id=14, shape=(), dtype=int32, numpy=1\u0026gt;) (\u0026lt;tf.Tensor: id=15, shape=(), dtype=int32, numpy=17\u0026gt;, \u0026lt;tf.Tensor: id=16, shape=(), dtype=int32, numpy=0\u0026gt;) 实现某函数对指定参数的求导运算，用 with 结构记录计算过程：\n1 2 3 with tf.GradientTape() as tape: 若干个计算过程 grad = tape.gradient(函数，对谁求导) 例如：\n1 2 3 4 5 with tf.GradientTape() as tape: w = tf.Variable(tf.constant(3.0))\t# 初值为3，可以求导 loss = tf.pow(w, 2) grad = tape.gradient(loss, w)\t# loss 对 w 求导，2w=6 print(grad)\t# tf.Tensor(6.0, shape=(), dtype=float32) 在遍历时返回索引号 enumerate(iterable)\n1 2 3 seq = [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;] for count, value in enumerate(seq): print(count, value) 分类问题中，用独热码表示标签，tf.one_hot(待转换数据, depth=几分类) 将标签列表转换为 one-hot 形式的数据\n1 2 3 4 classes = 3 labels = tf.constant([1,0,2])\t# 输入的元素值最小为0，最大为2 output = tf.one_hot(labels, depth=classes) print(output)\t# [[0. 1. 0.] [1. 0. 0.] [0. 0. 1.]],shape=(3,3), dtype=float32) 先做大小排序？ 使网络输出符合概率分布 tf.nn.softmax(x)\n1 2 3 y = tf.constant([1.01, 2.01, -0.66]) y_prob = tf.nn.softmax(y) print(y_prob)\t# tf.Tensor([0.25598174 0.69583046 0.0481878], shape=(3,), dtype=float32) 参数自更新 (自减) assign_sub()，参数要用tf.Variable定义为”可训练“\n1 2 3 w = tf.Variable(4) w.assign_sub(1)\t# w -=1, 即 w=w-1 print(w)\t# \u0026lt;tf.Variable \u0026#39;Variable:0\u0026#39; shape=() dtype=int32,numpy=3\u0026gt; 返回指定维度的最大值的索引 tf.argmax(张量名，axis=操作轴)\n1 2 3 test = np.array([[1, 2, 3] [2,3,4] [5,4,3] [8,7,2]])\t# shape=(4,3) print(tf.argmax(test, axis=0))\t# tf.Tensor([3 3 1], shape=(3,), dtype=int64) print(tf.argmax(test, axis=1))\t# tf.Tensor([2 2 0 0 ],shape=(4,),dtype=int64) 鸢尾花分类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # 1. Prepare data # read from sklearn.datasets import datasets x_data = datasets.load_iris().data\t# features y_data = datasets.load_iris().target\t# labels # mess up np.random.seed(116) np.random.shuffle(x_data) np.random.seed(116)\t# 使用相同的seed，特征与标签保持对应 np.random.shuffle(y_data) tf.random.set_seed(116) # separate 永不相见的训练集和测试集 x_train = x_data[:-30]\t# 前120 y_train = y_data[:-30] x_test = x_data[-30:] y_test = y_data[-30:] # pair， 每次输入一个batch train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) # 定义网络中所有可训练参数 w1 = tf.Variable(tf.random.truncated_normal([4,3], stddev=0.1, seed=1)) # 一层网络指的使那个密密麻麻交织的网络啊，重点不在节点，而在网！ b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1)) lr=0.1 train_loss_results = []\t# 存储各epoch的loss test_acc = []\t# 存储各epoch后在测试集上的准确率 epoch = 500 loss_all = 0\t# 各batch的loss求和 # 嵌套循环迭代，with结构更新参数，显示当前loss for epoch in range(epoch):\t# 数据集级别迭代 for step, (x_train, y_train) in enumerate(train_db): # batch 级别迭代 with tf.GradientTape() as tape:\t# 记录梯度信息 y = tf.matmul(x_train, w1) + b1\t# 线性 y = tf.nn.softmax(y)\t# 变成概率 y_ = tf.one_hot(y_train, depth=3)\t# 标签变为独热码，方便分类 loss = tf.reduce_mean(tf.square(y_ - y))\t# 均方误差损失 loss_all += loss.numpy()\t# 各batch的loss累加 grads = tape.gradients(loss, [w1, b1])\t# 1个batch的loss 对 w1,b1 求偏导 w1.assign_sub(lr*grads[0])\t# 参数自更新 b1.assign_sub(lr*grads[1]) print(\u0026#34;Epoch {}, loss: {}\u0026#34;.format(epoch, loss_all/4))\t# 4个batch的loss平均一下 train_loss_results.append(loss_all / 4) loss_all = 0 # 每个epoch之后，在test集上的表现 total_correct, total_number = 0,0 for x_test, y_test in test_db: y = tf.matmul(x_test,w1) + b1\t# y y = tf.nn.softmax(y) pred = tf.argmax(y, axis=1)\t# 提取类别 pred = tf.cast(pred, dtype=y_test.dtype)\t# 转换到与y_test相同的数据类型 correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32) # 把bool结果转换为int correct = tf.reduce_sum(correct)\t# 各batch的正确数加起来 total_correct += int(correct)\t# 当前正确率 total_number += x_test.shape[0]\t# 当前已测试过的样本数目 acc = total_correct / total_number\t# 一个epoch后，在测试集上的准确率 test_acc.append(acc) print(\u0026#34;Test_acc:\u0026#34;, acc) print(\u0026#34;----------------\u0026#34;) # 绘制loss曲线 plt.title(\u0026#39;Loss Function Curve\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.plot(train_loss_results, label=\u0026#34;$Loss$\u0026#34;) plt.legend() plt.show() plt.title(\u0026#34;ACC Curve\u0026#34;) plt.xlabel(\u0026#34;Epoch\u0026#34;) plt.ylabel(\u0026#39;Acc\u0026#39;) plt.plot(test_acc, label=\u0026#34;$Accuracy$\u0026#34;) plt.legend() plt.show() ","date":"2022-07-24T12:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B/","title":"watch: TF2 - PKU 01 | NN Computation Process"},{"content":"F.grid_sample F.grid_sample(tensor,p,mode) 把 tensor 插补成与网格 p 有相同大小的 tensor。 网格p指定了在 input 上的采样点坐标，在采样点附近插值形成一个新像素，各 channel 上采样点相同。 grid 中的 (x,y) 已缩放到 [-1,1]，在函数内变换到 [0,W],[0,H] 索引像素 PyTorch中grid_sample的使用方法-csdn\nmode = \u0026lsquo;bilinear\u0026rsquo; 是三线性插值: 双线性插值（bilinear）的3D形式(卿卿小徐的评论); c++源码\n4D input 对应一个 batch 的图片 (B,C,H,W)。\nalign_corners = True 认为input以像素为单位，各像素由其中心点代表，从而做双线性插值的时候，就用像素的四个角点； (2023-10-08: 角点？邻点？)\nFalse 是把 input 当作各像素角点的集合，此时 input 的边界 (W-1,H-1) 小于原来的图片边界 (W,H)，则grid采样点的坐标可能超出 input边界（比如采样点落在图片边缘，而双线性插值需要用周围 4 个点），所以需要在input外围padding，再与各neighbor点做插值。\ntorch.nn.functional.grid_sample() 函数实例-csdn\nAs shown below, black points are input datapoints. In the left figure, the datapoints fit the pixels corners of the input image to be scaled, whereas in the right figure, the datapoints forms an independent image to perform interpolation.\na = l i T g r n u _ e c o r n e r s a = l i F g a n l _ s c e o r n e r s A numerical example of difference: Docs - nn.Upsample\n(2024-03-09) In the left figure, the corners of the grid formed by pixels and sampling points are aligned. Whereas, the right figure isn\u0026rsquo;t.\n像素中心点的位置与一个像素的大小有关，所以对于相同维度的输入，采样点坐标可能不同。 所以使用像素角点做为基准 (align_corners = False)，与像素尺寸无关，采样点位置是相对的。 Docs\n线性插值是一维长度的加权平均，双线性插值是二维面积（两个方向）的加权平均：每个顶点的权重是其对角位置上的矩阵面积占比；三线性插值是三维体积的加权和。\n【三维重建和NeRF常用的三线性插值，原理讲解+代码实现】-意の茗\n双线性插值：在x方向和y方向上做线性回归并预测在目标点上的数值。所插值是周围4个neighbor的加权和，权重是两方向上neighbor到目标点距离与两点间距之比的乘积\ngrid_sample()函数及双线性采样 - 180天后再改名的文章 - 知乎\n(2023-12-18) F.grid_sample() cannot sample images?\n1 2 3 import cv2 src = cv2.imread(\u0026#34;dtu/Rectified/scan11_train/rect_011_3_r5000.png\u0026#34;) # ndarray, (h,w,3) F.grid_sample(torch.from_numpy(src).unsqueeze(0), uv_src.view(1, 192*h, w, 2)) And it will lead to: RuntimeError: grid_sampler_2d_cpu not implemented for Byte\nTherefore, the image requires to be normalized to [0,1]:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import PIL import torch.nn.functional as F from torchvision import transforms from matplotlib import pyplot as plt h, w = 128, 160 y,x = torch.meshgrid(torch.arange(h), torch.arange(w)) normalized_y, normalized_x = y/h, x/w normalized_xy = torch.stack([normalized_x, normalized_y], dim=-1) src = PIL.Image.open(\u0026#34;dtu/Rectified/scan11_train/rect_009_6_r5000.png\u0026#34;) src = transforms.ToTensor()(src) samp= F.grid_sample(src.unsqueeze(0), normalized_xy.unsqueeze(0)) # revert to an image samp_scaled = torch.tensor(samp[0] * 255, dtype=int).permute(1,2,0) plt.imshow(samp_scaled) Example in GNT train_imgs is a 4D tesnor.\nfeatmaps is a 5D tensor (N, Chanl, D, H, W,), the first dimension also vary determined by the indexing tensor\n1 2 3 4 5 6 7 8 9 10 11 12 # compute the projection of the query points to each reference image pixel_locations, mask_in_front = self.compute_projections(xyz, train_cameras) # pixel coords: (n_views, n_rays, n_samples, 2), (n_views, n_rays, n_samples) normalized_pixel_locations = self.normalize( # pixel coords range: ([0,h],[0,w])-\u0026gt; [-1,1] for F.grid_sample pixel_locations, h, w ) # [n_views, n_rays, n_samples, 2] # rgb sampling rgbs_sampled = F.grid_sample(input=train_imgs, grid=normalized_pixel_locations, align_corners=True) # (n_views, 3, n_rays, n_samples) rgb_sampled = rgbs_sampled.permute(2, 3, 0, 1) # [n_rays, n_samples, n_views, 3] # deep feature sampling # sample n_view feature maps for each 3D point. All chanls on the location in a feature map will be taken. feat_sampled = F.grid_sample(featmaps, normalized_pixel_locations, align_corners=True) # (n_views, out_chnls, H, W)-\u0026gt;(n_views, out_chnls, n_rays,n_samples) F.interpolate F.interpolate(input, size, [scale_factor,] mode, ) 把 input 缩放到 size，或者各维度缩放 factor 倍。 插值算法为mode。可以处理3D,4D,5D input。4D input的维度：(B, chnls, H, W) Docs\n1 2 3 4 5 6 7 8 import torch depth_ref = np.load(\u0026#34;mvsnet_depth.npy\u0026#34;).reshape(480,640) depth_data = torch.tensor(depth_ref) print(depth_data.shape) rsz = torch.nn.functional.interpolate(depth_data[None,None], size=(60, 80), mode=\u0026#34;bicubic\u0026#34;, align_corners=False,).squeeze() print(rsz.shape) # (480,640) plt.imshow(rsz) plt.savefig(\u0026#34;PT_F_intrpl_exmpl.jpg\u0026#34;,bbox_inches=\u0026#39;tight\u0026#39;, dpi =100) scipy zoom (2024-05-06)\n1 2 3 4 from scipy.ndimage import zoom scale_f_depth = 1/8 depth_rsz = zoom(depth_ref, (scale_f_depth, scale_f_depth), order=3) # (8,10) plt.imshow(depth_rsz) If mode=\u0026quot;constant\u0026quot;, the bottom row is 0 (cval).\n(2023-10-10)\nnn.Upsample This \u0026ldquo;layer\u0026rdquo; has no learnable parameters:\n1 2 3 4 upsampler = torch.nn.Upsample(size=(3,4), mode=\u0026#34;bilinear\u0026#34;) a = torch.arange(4).view(1,1, 2,2).float() dict(upsampler.named_parameters()).items() # out: dict_items([]) So it may be equal to torch.nn.functional.interpolate, which can be used as a layer in a model: Which function is better for upsampling: upsampling or interpolate?\ntorch.nn.functional layers require passing the learnable parameters from outside, since they don\u0026rsquo;t contain nn.Parameter like nn.Module does. Are torch.nn.Functional layers learnable?\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.weight = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) # Corresponding to a nn.module: self.linear = nn.Linear(in_features, out_features) def forward(self, input): return F.linear(input, weight, bias) # return self.linear(input) (2023-10-22)\nDownsample Conv layer can perform evenly downsampling, referring to Context Cluster Reducing points by fusing neighbors covered by kernels:\n1 2 proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=padding) Pooling can also be used for placing centers, referring to Context Cluster\n1 centers_proposal = nn.AdaptiveAvgPool2d((proposal_w, proposal_h)) F.interpolate\nTrilinear Interp (2024-02-28)\nPerform 2-point interpolation in 3 directions sequentially:\nEach point has 3 weights: u, v, w (or (1-u), (1-v), (1-w))\n","date":"2022-06-23T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-sample_interpolate/","title":"memo: PyTorch | Sample \u0026 Interpolation"},{"content":"最浅显易懂的 KMP 算法讲解-奇乐编程学院-bilibili\nKMP的next数组中保存了: 主串中最长的已匹配的字符。线性时间复杂度：主串指针不回退，子串指针回退到已经匹配的部分的后面（跳过已匹配的部分)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def kmp_search(string, patt): # string主串，patt子串 next = build_next(patt): i = 0 # 主串中的指针 j = 0 # 子串中的指针 while i \u0026lt; len(string): if string[i] == patt[j]: # 字符匹配，指针后移 i += 1 j += 1 elif j \u0026gt; 0: # 字符失配，根据 next 数组跳过子串前面的一些字符 j = next[j - 1] else: # 子串第一个字符就失配 i += 1 if j == len(patt): # 匹配成功 return i - j next 数组统计了各字符（包括它在内）之前已重复部分的长度。构造时有两个指针，前方指针指向前缀长度的后面一个字（前缀长度就是该指针之前的(不包括指向的)字符个数）；后方的指针指向当前字符（后缀）。后方指针一直后移，而前方指针要回退查看后缀是否存在与前缀中。 若两指针指向的字符相同，则后方指针对应的next数组元素为: 前缀长度数值+1，然后两个指针都后移一位； 如果它们指向的字符不相同，就看看后缀是否是前缀的一个子集（前缀中是否包含以后缀为结尾的短串，所以就要回到它前面的另一次重复看是否与后缀相接），则后方指针不动，前方指针回退到它左侧字符第一次出现时的下一个字符上（也就是左侧字符对应的前缀长度的下一个字符上，也就是跳过比较左侧字符对应的前缀长度个字符），因为再往前就是与此左侧字符的前缀重复的部分，比较这两个指针指向的字符，相同的话，也就是能连上这个后缀，则后方指针指向的next数组元素就是前方指针的前缀长度+1，然后两指针前移；不同的话，前方指针再回退，再与后缀比较，移动到第一个字符(前缀长度=0)还是与后缀不同的话就置0，后方指针前移，前方指针停在第一字符上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def build_next(patt): \u0026#34;\u0026#34;\u0026#34; 构造next数组 \u0026#34;\u0026#34;\u0026#34; next = [0] # next数组 初值元素一个0 prefix_len = 0 # 当前前缀长度 i = 1 # 后方指针 while i \u0026lt; len(patt): # 一次遍历 if patt[prefix_len] == patt[i]: # 前方指针字符 = 后方指针指向字符 prefix_len += 1 # 前缀长度+1 next.append(prefix_len) i += 1 # 后方指针后移 else: if prefix_len == 0: # 前方指针回到了第一个字符 next.append(0) # next[i]=0 i += 1 else: prefix_len = next[prefix_len - 1] # 前方指针回退到左侧字符的前缀长度的下一个字上，也就是前缀长度变成左侧字符的前缀长度 return next ","date":"2022-06-19T22:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/algo/kmp/","title":"KMP algorithm"},{"content":"(Feature image from:\nWhat is a Pinhole Camera, How Does a Pinhole Camera Work? - smartclass4kids.com (Searched by Camera tree in DDG image Clipart) Pinhole camera - Wikipedia (Searched by pinhole camera in DDG) )) 相机旋转 (2022-05-22)\n坐标系旋转是 point 旋转的逆（点逆时针旋转θ 等效于坐标系顺时针旋转θ）\n点P在初始坐标系下的坐标为 Pₒ，在目标坐标系下的坐标为 Pₜ， 从 Pₒ 到 Pₜ 中间是 target 坐标系在 origin 坐标系下的表示（方向向量）[𝐫ₓ 𝐫ᵧ]ᵀ:\n$$ \\begin{bmatrix} xₜ \\\\\\ yₜ \\end{bmatrix} = \\begin{matrix} rₓ: \\\\\\ r_y: \\end{matrix} \\begin{bmatrix} a₁ \u0026 b₁ \\\\\\ a₂ \u0026 b₂ \\end{bmatrix} \\begin{bmatrix} xₒ \\\\\\ yₒ \\end{bmatrix} $$ 横着看：行向量 (a₁,b₁) 是 target 系的 x 轴在 origin 系下的方向向量， (a₂,b₂) 是 target 系的 y 轴在 origin 系下的方向向量。 例如，下图中 origin 系是 world，target 系是 camera:\nOriginal 坐标$[^x_y]$做线性组合，变换到了 Target 坐标系$[^{x_c}_{y_c}]$。 竖着看：列向量 (a₁,a₂) 是 origin 系的 x 轴在 target 系下的方向向量， (b₁,b₂) 是 origin 系的 y 轴在 target 系下的方向向量。\n综上，旋转矩阵 R (in w2c) 横着看就是 camera (target) 系在 world 系下的表示，竖着看就是 world (original) 系在 camera 系下的表示。\n(2023-11-05) 所以通过转置就可以把 R in w2c 切换成 R in c2w。 但是要把 w2c 变成 c2w，需要对 [R|t] 整体求逆。 欧几里得变换 [R T]：旋转矩阵 R 加平移向量 T，把点在 origin 系的坐标变成在 target 系下的坐标，或者说把 origin 系变换成 target 系。\nFor example, w2c as below.\n先旋转后平移：点在 origin (world) 系下的坐标先经过 target (camera) 系在 origin 系下的方向向量 R 的线性组合，即 投影（做内积）到了一个与 target 系坐标轴都平行的新坐标系（虚线）下， 再加上一段平移向量 T，从而使新坐标是以 target 系的原点开始，所以 T 就是 origin 系的原点在 target 系视角下的坐标。\nThis process is expressed by [R|t]. Conversely, [R|t] denotes rotation first then translation.\n(2024-01-09) 𝐭 is coordinate measured in the target space, because 𝐭 is simply added onto the target coordiantes without \u0026ldquo;recombination\u0026rdquo; of the elementary vectors in a basis. Therefore, 𝐭 is the original center seen from the target space.\n或者先平移后旋转：origin (world) 系下的坐标先减去 target (camera) 系原点在 origin 系下的坐标 C， 变到了一个新坐标系下（其原点与 target 系原点重合）， 再旋转到与 target 系各轴重合；所以 C 就是 camera 光心在 world 系下的坐标。\nIn this case, R and T can\u0026rsquo;t be written as an augmented matrix, but separate matrices. If the given point\u0026rsquo;s coords are world coords, then apply [R|t]. While if point is already in camera space, only apply [R]. 可借助 vanishing points (待定系数)来求旋转矩阵（This point is at infinite but finite in image.） Camera Projection Matrix-UofMinnesota\n透视投影 (2022-05-08)\n透视投影（内参矩阵K）把 camera space 下的坐标投影到焦平面上，X 除以 Z 乘以 f（即以 f/z 为系数对 x,y 做一个缩放）。 如果焦距(焦平面距离) f 是常数，那就直接是与 z 成反比。\n(2024-01-31) 所以 \u0026ldquo;z\u0026rdquo; 代表的是 \u0026ldquo;Zoom\u0026rdquo; 缩放：近大远小. Ray Marching for Dummies! - Ytb - The Art of Code （这里坐标都是绝对值，不考虑坐标系的选取）\nB a c k p l a y n e f - / 0 p - i - n Z - h - o Y l e F r o n t 因为景物是 倒置 的，所以像素坐标系的 y 轴是向下的？ 若采用齐次坐标（用矩阵表达除法）, 对 [X,Y,Z] 做透视投影得到的是 [fX, fY, Z]，则 [u, v, 1] = [fX/Z, fY/Z, 1]。 再以像素尺寸 dx,dy 缩放并加上(+)光心坐标 cx,cy，把原点从光心移到左上角（像素系的v轴是朝下的，所以还需要加负号？），就变到了像素坐标系下：\n$$ \\begin{bmatrix} \\frac{f}{dx} \u0026 0 \u0026 cₓ \\\\\\ 0 \u0026 \\frac{f}{dy} \u0026 c_y \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} X \\\\\\ Y \\\\\\ Z \\end{bmatrix} = \\begin{bmatrix} fₓX + cₓZ \\\\\\ f_yY + c_yZ \\\\\\ Z \\end{bmatrix} = \\begin{bmatrix} \\frac{fₓX}{Z}+cₓ \\\\\\ \\frac{f_yY}{Z}+c_y \\\\\\ 1 \\end{bmatrix} $$ 投影变换 投影变换 GL_PROJECTION 是把 相机空间 下的点 (xₑ,yₑ,zₑ) 变换到 屏幕空间 的 clip 坐标 (xc,yc,zc,wc)：\n先透视投影 (1/Z缩放) 到相机的 near plane (焦距为-n)。\n透视除法需要除以 -zₑ，所以齐次坐标的 wₑ = -zₑ 再把 x,y 的取值范围：[top,bottom],[left,right] 线性变换到[-1,1]。\n令相机空间下的 [near, far] 的 NDC 坐标等于 [-1, 1]\n因为是从三维到三维，要想用矩阵表达透视缩放和平移，就需要使用齐次坐标。\nclip 坐标是 NDC 的齐次形式，所以这个矩阵（Projection Matrix） 完成了 frustum culling 和 NDC 变换。\n$$ \\begin{bmatrix} \\frac{2}{r-l}⋅n \u0026 0 \u0026 \\frac{r+l}{r-l} \u0026 0 \\\\\\ 0 \u0026 \\frac{2}{t-b}⋅n \u0026 \\frac{t+b}{t-b} \u0026 0 \\\\\\ 0 \u0026 0 \u0026 -\\frac{f+n}{f-n} \u0026 -\\frac{2fn}{f-n} \\\\\\ 0 \u0026 0 \u0026 -1 \u0026 0 \\end{bmatrix} \\begin{bmatrix} xₑ \\\\\\ yₑ \\\\\\ zₑ \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{r-l}⋅nxₑ + \\frac{r+l}{r-l}⋅zₑ \\\\\\ \\frac{2}{t-b}⋅nyₑ + \\frac{t+b}{t-b}⋅zₑ \\\\\\ -\\frac{f+n}{f-n}⋅zₑ -\\frac{2fn}{f-n} \\\\\\ -zₑ \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{r-l}⋅n\\frac{xₑ}{-zₑ} + \\frac{r+l}{r-l} \\\\\\ \\frac{2}{t-b}⋅n\\frac{yₑ}{-zₑ} + \\frac{t+b}{t-b} \\\\\\ \\frac{f+n}{f-n} +\\frac{2fn}{(f-n)zₑ} \\\\ 1 \\end{bmatrix} $$viewing\u0026amp;project-utexas\nNDC 空间 屏幕显示的世界深度范围是 near,far 两个焦平面之间的区域。 openGL 中两焦平面间的 z interval 被映射到 [-1,1]，即 Normalized Device Coordinates， 变换到 NDC 后就可以根据z_ndc把超出范围外的物体裁剪掉。\n屏幕窗口显示的是近焦平面。可以用 fov 控制近焦平面的边长，可以把屏幕的 t,b,l,r 映射到近焦平面的边长 [-1,1]， 这样屏幕显示的空间就是一个 立方体。 探秘三维透视投影-齐次坐标的妙用 -奇乐bili\n(2023-11-30)\nNDC = Perspective projection to the near plane with depths kept + Scaling. (2024-06-20)\nNDC is a cube. (2024-07-18)\nProjection Matrix converts the camera space to the clip space:\nCamera space Clip space After Clipping and Perspective division Frustum Still a frustum Cube Images credit: Quick Understanding of Homogeneous Coordinates for Computer Graphics - Miolith 中文翻译: 计算机图形学快速理解：齐次坐标 - Miolith) LearnOpenGL -Coordinate Systems(Searched by: \u0026ldquo;Normalized device coordinates ndc\u0026rdquo; in DDG24/07/21) Metal by Tutorials, Chapter 2: 3D Models | raywenderlich.com. Found in DDG image Clip space is still a frustum: X,Y after Perspective projection (Times focal and add c) and scaling to $(\\frac{2(f_x X + c_x Z)}{W}-Z, \\frac{2(f_y Y + c_y Z)}{H} - Z)$ (No perspec div yet); Z [near, far] are Scaled (\u0026ldquo;AZ+B\u0026rdquo;) to $\\left[\\frac{far+near}{far-near} near - \\frac{2far⋅near}{far-near}, \\frac{far+near}{far-near} far - \\frac{2far⋅near}{far-near}\\right]$ (No perspec div yet)\nAlthough the \u0026ldquo;perspective projection\u0026rdquo; is actually NOT performed by the Projection Matrix, the Projection Matrix is built aiming to obtain a cube after clipping and perspective projection.\n(2024-07-21)\nThe Projection Matrix transforms coordinates from camera space to clip space. Clip space is not a cube until after the clipping operaiont and the perspective division are performed (which are not encompassed in the projection matrix).\nAfter the clipping operation, only points located whose z is smaller than its x and y remain, thereby, resulting in a cube, containing points to be rendered, which is often referred to as the \u0026ldquo;normalized device coordinate (NDC) space.\u0026rdquo;\nIf not performing clipping operation and only perspective division for the clip space, the result space is not a cube, but a space where the range of x-y plane is [-inf, inf], and the z-axis ranges from [-1,1].\nA point (X,Y,Z) in camera space performed Projection Matrix and Perspective division becomes: $(\\frac{ 2 ( \\frac{f_x X}{Z} + c_x) - W}{W}, \\frac{2 ( \\frac{f_y Y}{Z} + c_y) - H}{H}, z_{NDC})$\nBecause the X projected onto the image space $(\\frac{f_x X}{Z}+c_x)$ can be [-inf, inf] considering all points in the camera space, the range of the scaled image-space coord x also is [-inf, inf]. However, the field of view of a camera is limited.\nTherefore, the clipping operation filters out the points that are out of the FOV!\nIf a point whose ratio between X and Z is larger than FOV, it won\u0026rsquo;t be rendered.\n❌ ( X , C Y a , m Z e ) X r c a ( ₓ 0 s , f p 0 Z ₓ a ) c e ❌ ( 0 ₚ I ᵢ … m ₓ a , … c g 0 ₓ e ₚ … ᵢ ⋮ ∙ ⋮ s ₓ … p ) - a … c c ₓ e … The fov of camera is $\\frac{c_x}{f_x}$. The similar triangle is $\\frac{c_x}{f_x} = \\frac{X}{Z}$\nIf a point has $\\frac{c_x}{f_x} \u003c \\frac{X}{Z}$, it won\u0026rsquo;t be seen by the camera.\nThe boundary is $\\frac{c_x}{f_x} = \\frac{X}{Z}$. At this time, the range of the deviation from the camera optical axis $\\frac{f_x X}{Z}$ is [-cₓ,cₓ]. If $cₓ$ equals a half of width of the image $cₓ = \\frac{W}{2}$, then the range of $x_{NDC}$ (i.e., $\\frac{ 2 ( \\frac{f_x X}{Z} + c_x) - W}{W}$) is [-1,1].\nMy previous understanding: the point whose X is larger than Z will be deleted is not accurate, as the FOV can be large:\n( A X c , u Y t , e Z ) X t c r ₓ i a f n Z ₓ g l e ` ` O X b ` t - u c - s ( ₓ - ` e 0 - , · t 0 Z r ) i ` a n ` g l e ` ` 相机变换矩阵 推导相机变换矩阵-csdn-潘宏\n(不同基底间的) 坐标转换公式: 𝐯=𝐐 𝐯\u0026rsquo;=𝐑 𝐯\u0026rsquo;\u0026rsquo; ⇒ 𝐯\u0026rsquo;\u0026rsquo;= 𝐑⁻¹𝐐 𝐯\u0026rsquo;，其中𝐐,𝐑 是不同的正交矩阵，代表坐标系，因为正交矩阵的逆等于转置，所以可以写为：𝐯\u0026rsquo;\u0026rsquo;= 𝐑ᵀ𝐐 𝐯'\nUVN相机模型用向量定义相机朝向：N 是相机观察方向的反方向，U 由辅助向量up与N叉乘确定，辅助向量用于让相机产生偏转（不歪头一般取(0,1,0)）；V=N×U，V 落在 up 与 N 形成的平面上。\n例如nerf的函数viewmatrix() 用于构建平均相机位姿 poses_avg 的UVN相机坐标系 [X|Y|Z]（世界系只有一个，而相机系有多个，取平均相机系作为\u0026rsquo;新世界系\u0026rsquo;）。\nView transformation: 把物体坐标从世界系变换到相机系下，也就是做一次相机运动的逆变换。变换过程：初始时相机系与世界系重合，(在世界系下)相机做旋转、再平移接近物体，然后相机与物体一起做逆平移、逆旋转，相机又回到初始位置，物体就变到了相机系下。\n逆平移易求(取反)，逆旋转不易求(求逆的顺序)；但是做完逆平移后，相机系与世界系的原点重合了，只是基底不同，利用坐标转换公式就可以求出在相机系下的坐标 𝐯\u0026rsquo;\u0026rsquo;= 𝐑ᵀ𝐐 𝐯\u0026rsquo;，其中𝐑是UVN系统，𝐐 是世界系(对角阵)，𝐯\u0026rsquo;是逆平移后的向量𝐓⁻¹𝐯，故最终的坐标变换矩阵(w2c外参矩阵Extrinsic matrix)：𝐑ᵀ𝐐 𝐓⁻¹\ndoubt: 旋转矩阵求逆 DDG\n3个矩阵 外参矩阵把点的 world space 坐标 Xw 变换到相机系下：Xc=R⋅Xw+T；\n内参矩阵把点的 camera space 坐标 Xc 变换到焦平面(原点在图片中央) （加上缩放因子fx,fy和光心坐标(cx,cy)可以变换到像素坐标系u,v， 原点在图片左上角,v轴朝下）上：P=K⋅Xc；\n相机投影矩阵 Camera projection matrix：把世界点直接变换到图像平面上（内参矩阵K₃ₓ₃ ∗ 外参矩阵[R T]₃ₓ₄ = P₃ₓ₄）。\nRef:\n11.1 Camera matrix-CMU SLAM入门之视觉里程计(2)：相机模型（内参数，外参数） Camera Calibration and 3D Reconstruction-openCV UVN 模型 从外参矩阵Extrinsic matrix₄ₓ₄ 提取出相机的位置和朝向: (最后一行是齐次坐标）最后一列是世界系中心在相机系中的位置， 左上3x3是相机在世界系下旋转运动R的转置（列向量是世界系，行向量是相机系）。如果再知道相机的观察方向，借助一个辅助向量up，就能确定UVN系统。 StackExchange\ndoubt: UVN 相机模型 Google Search\n(2024-04-02)\nEstablishing w2c from camera position and looking-at vector. Placing a Camera: the LookAt Function - Scratchapixel The \u0026ldquo;forward\u0026rdquo; direction is defined as From - To, because He marked the \u0026ldquo;out\u0026rdquo; of the screen as \u0026ldquo;forward\u0026rdquo;\nAfter determining the \u0026ldquo;forward\u0026rdquo; vector, specify a temporary \u0026ldquo;up\u0026rdquo; vector (usually (0,1,0)), which is not necessary to be perpendicular to the \u0026ldquo;forward\u0026rdquo;, to produce the \u0026ldquo;right\u0026rdquo; vector, accroding to \u0026ldquo;forward\u0026rdquo;× temporary \u0026ldquo;up\u0026rdquo;.\nOnce the \u0026ldquo;right\u0026rdquo; vector is obtained, re-construct the accurate \u0026ldquo;up\u0026rdquo; vector by \u0026ldquo;forward\u0026rdquo; cross \u0026ldquo;right\u0026rdquo;. Note: He define the Backward as \u0026ldquo;forward\u0026rdquo;.\nThe c2w in this post is row-major format. So, he writes each row is a direction.\nz = -1 is the camera-space coordinate of the ray direction vector.\nThe limitation of the looking-at method is the case that the \u0026ldquo;forward\u0026rdquo; direction is aligned with the temporary \u0026ldquo;up\u0026rdquo; (0,1,0), as the cross product of 2 parallel vectors is zero $\\vec{0}$. 10.4: The Cross Product - Mathematics LibreTexts\nA solution is using quaternion.\n(2024-04-04)\nNeRF builds RUB matrix (i.e., the averaged c2w)：average up × back (z-axis) = right (x-axis) Code Cam Coords System (2024-03-21)\n确定了相机坐标系与世界坐标系的相对关系，才能正确地把一个 3D 点的世界坐标变成相机坐标系下的坐标。\n假定世界坐标系是右手坐标系，相机坐标系可能不与世界系重合。\n相机坐标系有 3 个轴：左（右），上（下），前（后），方向不同则：点在该方向上的坐标相差 1 个负号。而且 3 个轴的排列次序也不统一。\nNeRF代码解读-相机参数与坐标系变换 - 陈冠英 - 知乎\nOpen3D\u0026rsquo;s camera coord. sys. is RDF. camera coordinate system of visualization #1347 确定了相机在 世界 坐标系中的朝向和位置: Up vector, viewing direction and position， 才可以操纵相机 (Camera Manipulation)：Changing roll, yaw, pitch, dollying (Slides - Hong Qin). An interactive example in LearnWebGL。 Songho also explain camera manipulation OpenGL Camera.\n所以坐标变换的顺序是：世界系下的坐标 ➡ 相机系下的坐标 ➡ 相机做 6 DoF 运动（等价于物体的相机系坐标做 inverse 运动）➡ 在相机运动完成后，再把 3D 点在相机系下的坐标投影到相机平面。\n对于矩阵 w2c，前 3 列的每一列是世界坐标系的每个 axis 在相机坐标系下的坐标。\n(2024-03-25)\nw2c 是把一个 点 的世界坐标转换成相机坐标。一个 3D 点的坐标等于 一个数组乘以坐标系。 所以 w2c 等于 rotation matrix 乘以世界坐标系：\n$$ \\begin{bmatrix} c_{x_1} \u0026 c_{y_1} \u0026 c_{z_1} \\\\\\ c_{x_2} \u0026 c_{y_2} \u0026 c_{z_2} \\\\\\ c_{x_3} \u0026 c_{y_3} \u0026 c_{z_3} \\end{bmatrix} = \\begin{bmatrix} r₁₁ \u0026 r₁₂ \u0026 r₁₃ \\\\\\ r₂₁ \u0026 r₂₂ \u0026 r₂₃ \\\\\\\\ r₃₁ \u0026 r₃₂ \u0026 r₃₃ \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$ 因为 rotation matrix 的特性，它的一行就是目标坐标系的一个轴在 源系下的坐标。它的一列就是源系的一个轴在目标系下的坐标。\n(2024-06-20)\nIs this because that a point (x,y,z) can be regarded as a inner product of a coordinate tuple and the source basis. Once the source basis times the rotation matrix, it becomes the target basis.\nAnd since the source bais is a all-1 matrix, the rotation is the target basis: each column is an axis of the target basis. And because the coordinates tuple is measured in the source space, the target basis is also being viewed in the source space. And as the rotation matrix is at left, based on the rule of matmul, each row is supposed to be an axis of the target space. After the matrix multipliation, the coordinates are transformed into the targe space. $$ \\begin{bmatrix} x_{tar} \\\\\\ y_{tar} \\\\\\ z_{tar} \\end{bmatrix} = \\begin{bmatrix} \\text{tar axis1 in src} \\\\\\ \\text{tar axis2 in src} \\\\\\ \\text{tar axis3 in src} \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} x_{src} \\\\\\ y_{src} \\\\\\ z_{src} \\end{bmatrix} $$ Each row in rotation matrix for w2c is an axis of target camera coordinate system. This can be verified by the example below:\nPlotting script: Test_rotation_matrix.ipynb\n同样，对于矩阵 c2w，前 3 列的每 一列 是相机坐标系的每个 axis 在世界坐标系下的坐标。 所以要 调换 相机坐标系在世界坐标系下的朝向，对 c2w 的 rot 的 某一列 乘上一个负号即可。\n相机坐标系的定义影响的是 3D 点在相机坐标系下的坐标：改变相机朝向，最终体现在 3D 点在相机系下的坐标的正负。 具体来说，对于 w2c 中的旋转矩阵，要 调换 相机系的一个轴的方向，应对 rotation matrix 中对应的 一行 添加负号。\n因为 NeRF 使用的是 c2w，它的旋转矩阵的 每一列 是一个相机系的轴，而且次序是 DRB 所以代码中交换第 0 列(Down) 和第 1 列 (Right)，然后对 y 方向乘上 -1 变成 Up。最终变到了 OpenGL 的 RUB。 Code\n(2024-03-24)\n\u0026ldquo;The view matrix transforms all world coordinates to camera-space coordinates.\u0026rdquo; \u0026ndash; LearnOpenGL - Camera\nTherefore, the view matrix (extrinsics) transforms the X,Y,Z axes of the world coordinates system to X,Y,Z axes of the camera coordinates system.\nLet the world X-Y-Z axes be the 3 unit column vectors, as shown in the below right matrix, they\u0026rsquo;re transformed to camera axes by a w2c:\n$$ \\begin{bmatrix} r₁₁ \u0026 r₁₂ \u0026 r₁₃ \u0026 t₁ \\\\\\ r₂₁ \u0026 r₂₂ \u0026 r₂₃ \u0026 t₂ \\\\\\\\ r₃₁ \u0026 r₃₂ \u0026 r₃₃ \u0026 t₃ \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\\\ 0 \u0026 0 \u0026 1 \\\\\\ 1 \u0026 1 \u0026 1 \\end{bmatrix} $$ The extrinsics matrix transforms a world coordinates into camera-space coordinates. Next, the point will be applied with the projection matrix (scaling axes to preserve points whose $z_{clip}$ is larger than its $x_{clip},\\ y_{clip},\\ z_{clip}$, and performing intrinsics) for frustum clipping. The projection matrix requires a 4D homogeneous coordinates: $[x_{cam},y_{cam},z_{cam},1]^T$. So, the above extrinsic matrix has a 4-th row, that results in an additional $1$, which is reserved for storing the depth z value, for the final perspective division. After multiplied with the extrinsics (w2c), the X,Y,Z axes of the world system are still 3 columns, but values become their coordinates under the camera coordinate system. And the 3 rows in the R of w2c are the camera coordinate system.\nHowever, the camera coordinate system has many different matrix formats in different 3D applications. For example, OpenGL (Blender) uses RUB order.\nUsually, the world space is RUB as well. So, transforming world-space coordinates to OpenGL camera-space coordiantes doesn\u0026rsquo;t need to reverse axes.\nWhereas, OpenCV uses RDF camera coord. sys. Thus, the sign of y and z coordinates require flips in the camera space.\nOne of the differences between OpenCV and OpenGL is that in OpenGL, the z-axis has near and far boundary. Refer to Amy Tabb.\nThat post was found with searching \u0026ldquo;Converting camera poses from OpenCV to OpenGL can be easy\u0026rdquo; (DDG), that is a medium blog, which is found when searching \u0026ldquo;camera coordinates right front up\u0026rdquo; (DDG)\n(2024-03-25)\nExample with a 3D point p:\nZ Y O D o w n p F o r w a X r , d R i g h t The above figure shows the right-hand world coordinate system (X-Y-Z) and a OpenCV camera coordinate system (Right-Down-Forward).\nThose 2 coordinate systems have a common origin $O$. And the camera has no rotation.\nThe coordinates of a point p under the world space is $(2, 2, 1)$. However, the coordinates in the camera space is $(2, -2, -1)$.\nThis shows that when converting the world-space coordinate to OpenCV camera-space coordinates, there is a \u0026ldquo;sign matrix\u0026rdquo;:\n$$ \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 -1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\\\ 2 \\\\\\ 1 \\end{bmatrix} $$When the camera shifts from the world origin by rotation and translation, i.e., the extrinsics matrix, which transforms the axes of world system. So, the result coordinates is measured in a transformed world coordinate system.\nThus, the \u0026ldquo;sign matrix\u0026rdquo; is required to convert the \u0026ldquo;transformed world\u0026rdquo; system to OpenCV (or other) camera coordinate system.\n$$ \\begin{bmatrix} x_{cam} \\\\\\ y_{cam} \\\\\\ z_{cam} \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 -1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} r₁₁ \u0026 r₁₂ \u0026 r₁₃ \u0026 t₁ \\\\\\ r₂₁ \u0026 r₂₂ \u0026 r₂₃ \u0026 t₂ \\\\\\\\ r₃₁ \u0026 r₃₂ \u0026 r₃₃ \u0026 t₃ \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} x_{world} \\\\\\ y_{world} \\\\\\ z_{world} \\\\\\ 1 \\end{bmatrix} $$Therefore, the matrix transforming world coordinates to OpenCV camera coordiantes is:\n$$ \\begin{bmatrix} r₁₁ \u0026 r₁₂ \u0026 r₁₃ \u0026 t₁ \\\\\\ -r₂₁ \u0026 -r₂₂ \u0026 -r₂₃ \u0026 -t₂ \\\\\\\\ -r₃₁ \u0026 -r₃₂ \u0026 -r₃₃ \u0026 -t₃ \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} $$ Since w2c transforms world axes to camera-space coordinates, the coordinates performed by w2c must be a world coordinates of a point, instead of a camera coordinates.\nSo, the \u0026ldquo;sign matrix\u0026rdquo; must be applied after w2c. Otherwise, the world coordinate becomes a camera coordinate immediately, which doesn\u0026rsquo;t match w2c. Specifically, the below order is incorrect:\n$$ w2c \\begin{bmatrix} 1 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 -1 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 -1 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} x_{world} \\\\\\ y_{world} \\\\\\ z_{world} \\\\\\ 1 \\end{bmatrix} $$In other words, the \u0026ldquo;sign matrix\u0026rdquo; should be applied on a camera-space coordinates.\nIn NeRF, the provided matrix is c2w, where each column of the rot is a camera axis, and the columns order is DRB. So, the first 2 columns need to switch, thus, becoming RDB. Then, to align the camera coordinate system of Blender: RUB, the second row (the U axis) needs to be multiplied with -1. Code\n(2024-03-26)\nNote: the rotation matrix in c2w and w2c are different. For the rot in c2w, each column is an axis of camera, so reversing the direction of a camera axis requires multiplying a column with -1.\nWhereas, for the rot in w2c, each row is an axis of a camera. Thus, to reverse a camera axis, a row needs to be negated.\n(2024-03-26)\nTest reversing a row and a column in a rotation matrix for w2c:\nOriginal Rot in w2c Flip 0th row Flip 0th column $$\\begin{bmatrix} 0.970 \u0026 0.00747 \u0026 0.241 \\\\\\ -0.0147 \u0026 0.999 \u0026 0.028 \\\\\\ -0.241 \u0026 -0.0309 \u0026 0.969 \\end{bmatrix}$$ $$\\begin{bmatrix} -0.970 \u0026 -0.00747 \u0026 -0.241 \\\\\\ -0.0147 \u0026 0.999 \u0026 0.0282 \\\\\\ -0.241 \u0026 -0.0309 \u0026 0.969 \\end{bmatrix}$$ $$\\begin{bmatrix} -0.970 \u0026 0.00747 \u0026 0.241 \\\\\\ 0.0147 \u0026 0.999 \u0026 0.0282 \\\\\\ 0.241 \u0026 -0.0309 \u0026 0.969 \\end{bmatrix}$$ $p_{cam}=[2.18994, 0.99823, 0.45571]$ $p_{cam}=[-2.18994, 0.99823, 0.45571]$ $p_{cam}=[-1.69110, 1.05720, 1.42213]$ A row of the rotation matrix in w2c is the coordinates of a camera axis in the world space.\nA column is the coordinates of a world axis in the camera space.\nThe original rotation matrix transforms the world axes to a tilted coordinate system.\nFlip the 0th row of the rotation matrix: only the X axis entirely turns to the oppsite direction.\nFlip the 0th row of the rotation matrix: the x component of all the X-Y-Z axes are affected. Apparently, this is not desired result. When flipping a single axis, the other axes should be unchanged.\nFigure plotting script: Test_reverse_cam_axis.ipynb\nIdentify Cam Axes 只有旋转矩阵 R，但不知道相机在世界坐标系中的朝向（也不知道各轴的次序）。\n@will 在 23-11-10 8:25 AM 说：画出来看看正不正对场景。 @什么办 在 22-11-21 8:50 PM 展示过他用 plt 画的相机位姿。 (2024-03-27)\nThe pose1 does not face towards the object (MVSNet_testing/dtu/scan1/cams/00000000_cam.txt):\nR for w2c rect_001_0 (full) mod signs I have dragged the figure to make the z axis upside down. The camera position is $[-191.02, 3.28832, 22.5401 ]$ And I feel the pose should be $[3, 191, 22]$\n(2024-03-30) Camera position was wrong, but it\u0026rsquo;s not due to signs. The 4-th column in w2c is not the camera position in world.\nDragging z-axis to upside down is equivalent to negating the z coordinate and switching the x and y of points coordinates. Specifically, given a point (x,y,z), dragging z to flip equals: x=y; y=x; z=-z\nDrag manually Flip z and switch x,y Code for modifying axes for scan23 1 2 3 4 5 6 7 8 9 10 11 12 13 14 %matplotlib widget import numpy as np import matplotlib.pyplot as plt import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/mnt/data2_z/SampleSet/MVS Data/Points/stl/stl023_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vs = np.asarray(pcd.points) samples = vs[np.random.choice(vs.shape[0],1000)] x = samples[:,1] y = samples[:,0] z = -samples[:,2] fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(x, y, z, color=\u0026#39;black\u0026#39;) (204-03-28)\nI know DTU matches the setup of OpenCV because the function cv2.decomposeProjectionMatrix is used. But, I\u0026rsquo;m still confused about the scene visualization with matplotlib. (2024-03-30)\nThe 4-th column of the w2c is NOT the camera center position in world space!! (The 4-th column of the w2c is the world center in the camera space: $\\rm WorldCen_{cam} = R_{w2c} 0_{world} + T_{w2c}$.) The camera position in world should be the 4-th column of c2w, i.e., $-R_{w2c}^T t_{w2c}$.\nThe t returned by cv2.decomposeProjectionMatrix is the camera position as well. After correcting the camera center position, the camera geometry is correct:\nPlotting script: gist\nIdentifying the axes directions should be independent of camera geometry.\nOnly drawing one camera may not easily indicate if it\u0026rsquo;s facing the scene.\n(2024-03-31)\nOpen3D can set the window (visualizer) to be the specified camera pose.\nChange set_front to different row in the rotation mat:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([0, 0, 0]) view_ctl = vis.get_view_control() vis.add_geometry(pcd) view_ctl.set_front(w2c[0][:3]) vis.run() vis.destroy_window() set_front(w2c[2][:3]) set_front(w2c[0][:3]) Set the extrinsic to simulate a camera pose: Determining the Proper Camera Position #2338\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import open3d as o3d import numpy as np pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([0, 0, 0]) vis.add_geometry(pcd) view_ctl = vis.get_view_control() w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) cam = view_ctl.convert_to_pinhole_camera_parameters() cam.extrinsic = w2c view_ctl.convert_from_pinhole_camera_parameters(cam, True) current_param = view_ctl.convert_to_pinhole_camera_parameters() print(current_param.extrinsic) vis.run() vis.destroy_window() The argument allow_arbitrary=True is required (using 0.18.0), reminded by: How to you position camera and look at certain location in Open3D? #1483\nThis argument is added to free the limitation on pinhole camera models. ConvertFromPinholeCameraParameters() failed #834\nSimilar issues:\nconvert_from_pinhole_camera_parameters does not work #1343\nconvert_from_pinhole_camera_parameters allow_arbitrary=True modifies intrinsic matrix #5816\n\u0026ldquo;view matrix\u0026rdquo; means w2c. While \u0026ldquo;world transformation matrix\u0026rdquo; is c2w. 3D GEP\nEach column in c2w is a camera axis coordinates in world space. So, 3 columns represent directions, such as RDF, and the 4-th colmun is the camera center in world space.\nHe gave a code demo to show the matrix format in column-major memory accessing.\nOpenCV➡OpenGL (2024-03-29)\nJust negate the 2nd and 3rd rows in the rotation matrix that transforms word coords to cam coords. Such that, RDF camera system becomes RUB. (And note OpenGL reads matrix by columns.) OpenCV to OpenGL coordinate system transform - SO\nThis process can be done with a \u0026ldquo;sign matrix\u0026rdquo;: [[1,0,0,0], [0,-1,0,0], [0,0,-1,0], [0,0,0,1]].\nThis \u0026ldquo;sign matrix\u0026rdquo; also appears in PixelNeRF to process DTU. Yu called it as \u0026ldquo;similarity transform\u0026rdquo;. (issue#2)\nSame as the function T_opencv_to_opengl() in camtools\n(2024-04-02)\nMapping a coordinate system to another has two transformations: rotation+translation [R|t] and sign matrix. Converting camera poses from OpenCV to OpenGL can be easy - readmedium (Found when searching \u0026ldquo;opencv to opengl transformation\u0026rdquo; DDG)\nSpecifically, change the source basis first, and then flip axes of the source basis (world) to target basis (camera).\nz y W o r l d x R | t R y o t a t e d z w o x r l d f s l i i g p n z ᶜ C a m e r x a ᶜ y ᶜ I realized that the terminologies: w2c and c2w are siutable for the transition between world and the OpenGL camera coordinate syste, beacuse their axes are aligned, i.e., both RUB.\nOtherwise, for example, between world and OpenCV camera system, the world system is not directly becoming the target camera system after rotation and translation [R|t].\nThat post also takes the column-major memory access into account.\nDTU dataset Original (2024-02-21)\nDTU Homepage Each object scan is taken from 49 fixed camera positions.\nFor the SampleSet, the images dimensions are 1600x1200:\n1 2 yi@yi:~/Downloads/DTU_SampleSet$ identify MVS\\ Data/Rectified/scan1/rect_001_0_r5000.png MVS Data/Rectified/scan1/rect_001_0_r5000.png PNG 1600x1200 1600x1200+0+0 8-bit sRGB 2.85068MiB 0.000u 0:00.000 The camera projection matrix 𝐏₃ₓ₄ from world to image, i.e. K@[R|t] (Casmvsnet and SO):\n1 2 3 4 yi@yi:~/Downloads/DTU_SampleSet$ cat MVS\\ Data/Calibration/cal18/pos_001.txt 2607.429996 -3.844898 1498.178098 -533936.661373 -192.076910 2862.552532 681.798177 23434.686572 -0.241605 -0.030951 0.969881 22.540121 (2024-03-27) The P is estimated by Matlab, and Matlab regards camera coordinates system as RDF (mentioned in Multi-cameras on same coordinate system - Dima found by Perplexity), which is the same as OpenCV camera model.\nThe P can be decomposed to K,R,t by decomposeProjectionMatrix(): 1 2 3 4 5 P_orig=np.array( [[2607.429996, -3.844898, 1498.178098, -533936.661373], [-192.076910, 2862.552532, 681.798177, 23434.686572], [-0.241605, -0.030951, 0.969881, 22.540121]]) K,R,t = cv2.decomposeProjectionMatrix(P_orig)[:3] The original intrinsic matrix K (performed K/K[2][2]) is:\n1 2 3 array([[ 2.89233051e+03, -2.48063349e-04, 8.23205273e+02], [ 0.00000000e+00, 2.88317528e+03, 6.19070918e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]) It\u0026rsquo;s aligned with the intrinsics in mvs_training (not the cams in train/ folder):\nmvs_training/dtu/Cameras/00000000_cam.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 z@homepc:~/Downloads/Datasets_life/mvs_training/dtu/Cameras$ cat 00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 2892.33 0 823.205 0 2883.18 619.071 0 0 1 425 2.5 The table lists focal length and image resolution correspondence:\nScale Resolusion f_x c_x 1 1200x1600 2892.3 823.205 1/2 600x800 1446.1 411.603 1/2 (crop) 512x640 1446.1 331.603 1/4 300x400 723.08 1/8 150x200 361.5 (2024-03-27)\nIn Dima\u0026rsquo;s answer, he described RDF as the world space. That means the extrinsics has been applied by the \u0026ldquo;sign matrix\u0026rdquo;, which changes the world axes to camera axes. So, the R decomposed from P essentially corresponds to the RDF coordinates system.\nIn other words, the camera coordinate system is used as the world coord. sys.\nWhereas, the world system during visualization is usually RUB (Y-axis is Up), like OpenGL. So, the object is upside down when plotting the point cloud with matplotlib.\nAnd the ccs in Open3D also is RDF (relative to world space RUB), so its initial w2c has reverse the y-axis and z-axis of the world space:\n1 2 3 4 [[ 1. 0. 0. -0.] [-0. -1. -0. 0.] [-0. -0. -1. 0.] [ 0. 0. 0. 1.]] Code: Print current cam pose 1 2 3 4 5 6 7 8 import open3d as o3d vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() view_ctl = vis.get_view_control() current_param = view_ctl.convert_to_pinhole_camera_parameters() print(current_param.extrinsic) vis.run() vis.destroy_window() MVSNet (2024-02-22)\nTraining set: dtu_training.rar (19G) (\u0026ldquo;mvs_training/dtu/\u0026rdquo;)\nAs mentioned in the section 4.1 of the MVSNet paper, the training images are 1/2 * (1200,1600) = (600,800), which then cropped to (512,640).\n8 0 ( 4 1 1 , 3 0 9 ) ( 8 2 3 , 6 1 9 ) c r o p ( 3 3 P 1 a , t 2 c 6 h 5 ) C N N f m 1 e a / a p 4 t In addition, because the camera is looking at feature maps, the focal lengths should be scaled with the ratio of the size of feature map to the input image size.\nAs feature map size (128,160) is 1/4 input image (512,640) mentioned in paper section 3.1, the focal_x should be: 2892.33 * 1/2 * 1/4 = 361.541. Issue And the c_x also need to be scaled by 1/4, i.e., 331.603 * 1/4 = 82.901.\nNote: The already calculated trianing camera params are placed in \u0026ldquo;mvs_training/dtu/Cameras/train\u0026rdquo;. Code While the cameras displayed outside the \u0026ldquo;train/\u0026rdquo; are params corresponding to the original DTU images (1200,1600).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 z@lambda:~/Downloads/mvs_training/dtu$ identify Rectified/scan1_train/rect_001_0_r5000.png Rectified/scan1_train/rect_001_0_r5000.png PNG 640x512 640x512+0+0 8-bit sRGB 626KB 0.000u 0:00.000 z@lambda:~/Downloads/mvs_training/dtu$ cat Cameras/train/00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 361.54125 0.0 82.900625 0.0 360.3975 66.383875 0.0 0.0 1.0 425.0 2.5 Testing set (dtu.zip) has the full-size images:\nTesting images are\u0026rsquo;t downsized twice or cropped, so the focal lengths only times 1/4. Code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 z@lambda:~/Downloads/data2/MVSNet_testing/dtu$ identify scan1/images/00000000.jpg scan1/images/00000000.jpg JPEG 1600x1200 1600x1200+0+0 8-bit sRGB 705KB 0.000u 0:00.000 z@lambda:~/Downloads/data2/MVSNet_testing/dtu$ cat scan1/cams/00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 2892.33 0 823.205 0 2883.18 619.071 0 0 1 425 2.5 The factor adaptive_scaling is used for the requirement that image size must be evenly divisible by 32 (e.g., 864x1152) and reducing images for limited VRAM. So, this step will also change resolution, focals, and principle points: Code\nMVSNet-PyTorch Training cam: 1/8 focal of the original DTU Code\nTesting cam: 1/4 focal of the original DTU Code\nPixelNeRF (2023-08-17)\n\u0026ldquo;rs_dtu_4\u0026rdquo; follows the DVR format. Each object has 6 matrices. Take the object 0 as an example:\n1 2 3 4 5 6 [\u0026#39;scale_mat_0\u0026#39;, \u0026#39;scale_mat_inv_0\u0026#39;, \u0026#39;world_mat_0\u0026#39;, # Projection Matrix, 4x4 \u0026#39;world_mat_inv_0\u0026#39;, # Inverse Projection matrix, 4x4 \u0026#39;camera_mat_0\u0026#39;, # ??? \u0026#39;camera_mat_inv_0\u0026#39;] Use cv2.decomposeProjectionMatrix(P) to solve 𝐊,𝐑,𝐭 from 𝐏₃ₓ₄. Code in PixelNeRF:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 P = all_cam[\u0026#34;world_mat_0\u0026#34;] P = P[:3] # (3,4), projection: Intrinsics * Extrinsics * 3Dpoint K, R, t = cv2.decomposeProjectionMatrix(P)[:3] K = K / K[2, 2] # Not the camera_mat, # Xc = extrinsics*Xw extrinsics = np.eye(4, dtype=np.float32) extrinsics[:3,:3] = R # The 4th column is the rotated transVec extrinsics[:3,3] = -(R @ (t[:3]/t[3]))[:,0] print(extrinsics) # c2w equals inverse extrinsics c2w = np.eye(4, dtype=np.float32) c2w[:3, :3] = R.transpose() # The 4th column is the normalized t decomposed by cv2. c2w[:3, 3] = (t[:3] / t[3])[:, 0] c2w == np.linalg.inv(extrinsics) The K (focal) in pixelNeRF is about twice as large as the intrinsics of \u0026ldquo;dtu_training\u0026rdquo;, because the image size of pixelNeRF (300x400) is twice as small as MVSNet (or MVSNeRF) in each dimension, where (512x640) is cropped from (600x800). It\u0026rsquo;s like when you observe the scene from far away, the image captured gets smaller.\nSince the projection matrix computed from K@(R|Rt) is different, the decomposed intrinsics will be different.\n(2024-02-22) The above statement may be wrong. Take the 1st camera as an example:\n1 2 3 4 cams = np.load(\u0026#34;pixel-nerf/data/DTU_Dataset/rs_dtu_4/DTU/scan1/cameras.npz\u0026#34;) P = cams[\u0026#39;world_mat_0\u0026#39;][:3] K, R, t = cv2.decomposeProjectionMatrix(P)[:3] K / K[2, 2] The K equals 1/4 the intrinsics of the original DTU dataset:\n1 2 3 [[ 7.23082629e+02, -6.20158374e-05, 2.05801318e+02], [ 0.00000000e+00, 7.20793819e+02, 1.54767729e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]] Because camera is used to project 3D points ont feature maps, the focals should be scaled based on the ratio of the feat map to the original image (1600,1200).\nc2w (3x4) is not the Inverse Projection Matrix (4x4).\nInverse Projection Matrix = np.linalg.inv(Projection Matrix)\nProjection matrix converts a 3D world coords to 2D pixel coords;\nExtrinsics ( w2c = [R|t] = (R|Rt) ) converts 3D world coords to 3D camera coords.\nInverse Extrinsics ( c2w ) converts 3D camera coords to 3D world coords.\nGive Extrinsics and Intrinsics (of dataset \u0026ldquo;dtu_training\u0026rdquo; from MVSNet), the Projection matrix can be restored as implemented in MVSNeRF def build_proj_mats()\nThe translation vector also needs rotation. OpenCV Decompose projection matrix\nProjection matrix = Intrinsics@Extrinsics = K@[R|t] = K@(R|Rt) = (KR|KRt)\nDecomposed t needs normalization, and to be negated sometimes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 K = np.array([[631, 0, 384], [ 0, 631, 288], [ 0, 0, 1]]) R = np.array([[-0.30164902, 0.68282439, -0.66540117], [-0.63417301, 0.37743435, 0.67480953], [ 0.71192167, 0.6255351 , 0.3191761 ]]) t = np.array([ 3.75082481, -1.18089565, 1.06138781]) P = np.eye(4) P[:3, :3] = K @ R P[:3, 3] = K @ R @ t K1, R1, t1 = cv2.decomposeProjectionMatrix(P[:3, :])[:3] t == -(t1[:3]/t1[3]) The original t can be obtained directly from projection matrix: np.linalg.inv(P[:3,:3]) @ P[:3,3], i.e., use the inverse rotation to rotate the transVec back.\n(2024-03-29)\nThe t returned by cv2.decomposeProjectionMatrix is the position of a camera in the world space. (Docs) So, it\u0026rsquo;s actually the translation vector in c2w: $t_{c2w}$.\nBecause t (denoted as $t_{cv2}$ for later) is the camera center, its corresponding camera-space coordinates is 0. Thus, this is the relationship:\n$$t_{cv2} = R_{c2w} 0 + t_{c2w} \\\\\\ t_{cv2} = t_{c2w}$$To get the $t_{w2c}$, i.e., the 4-th column in the w2c (extrinsics), the conversion formula is $t_{w2c} = - R_{w2c} t_{c2w}$.\nThis relationship can be derived from the transformation between camera-space coordinate X and world coordinate P:\n$$ P = R_{c2w} X + t_{c2w} \\\\\\ X = \\underbrace{R_{c2w}^T}\\_{R_{w2c}} P \\underbrace{- R_{c2w}^T t_{c2w}}\\_{t_{w2c}} $$Considering the above example, t is not the 4-th column in extrinsics, but the -R @ (t/t[3])[:3] is.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;\u0026gt;\u0026gt; t array([[-0.99198397], [ 0.00603084], [-0.12611273], [-0.00519817]]) \u0026gt;\u0026gt;\u0026gt; t/t[3] array([[190.83346195], [ -1.16018638], [ 24.26100588], [ 1. ]]) \u0026gt;\u0026gt;\u0026gt; -R @ (t/t[3])[:3] array([[-191.01958721], [ 3.28830259], [ 22.54011993]]) Therefore, in PixelNeRF directly used t as the 4-th column of the c2w (named as pose)\nI was reminded by:\nOne question about cv2.decomposeProjectionMatrix #10\nQuestions on how to use PyTorch3D\nHow to find camera position and rotation from a 4x4 matrix? - SE (surfaced by \u0026ldquo;given camera extrinsics, how to determine right, up, front\u0026rdquo; DDG)\n$$0=RC+T \\\\\\ C=−R^T T$$ In the folllwing posts, they all mentioned the 4-th column in Extrinsics is not the camera center, but $-R_{c2w}^T C$, where C is the camera center in world space:\nDissecting the Camera Matrix, Part 2: The Extrinsic Matrix - ksimek\nHe derived w2c from c2w, ie. w2c = (c2w)⁻¹, and provided an interactive demo for visualizing camera intrinsic, extrinsic.\nCamera Pose \u0026amp; Pose Estimation - MiaoDX refered by Camera extrinsic matrix from camera location and rotation\nHow to plot the camera and image positions from camera calibration data? - SO\nThe OP cited wikipedia $C = -R^{-1} T = -R^T T$\nPlot Camera Trajectory - SO Understanding COLMAP\u0026rsquo;s Camera Poses and Depth Data #1476\ninv([R|t]) = [R'|-R'*t] Camera position in world coordinate from cv::solvePnP - SO\n","date":"2022-05-22T00:00:00Z","image":"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Pinhole-camera.svg/600px-Pinhole-camera.svg.png?20190523023446","permalink":"http://blog.zichen.uk/post/writenotes/vis/b-note-camera_matrices/","title":"Memo: Vis - 3D | Camera Matrices"},{"content":"极坐标系下的二维拉普拉斯方程 - 知乎 Let the two-dimensional function u(x,y) be expressed as u(r,φ) in polar coordinate system. The Laplace equation: $\\nabla^2 u = 0$\nAccording to the chain rule, the derivative of u(r,φ) with respect to x equals the derivative of u(r,φ) with respect to r and φ first then multiplying their derivative with respect to x:\n$$ \\begin{cases} \\frac{∂u(r,φ)}{∂x} = \\frac{∂u}{∂r} \\frac{∂r}{∂x} + \\frac{∂u}{∂φ} \\frac{∂φ}{∂x} \\\n\\frac{∂u(r,φ)}{∂y} = \\frac{∂u}{∂r} \\frac{∂r}{∂y} + \\frac{∂u}{∂φ} \\frac{∂φ}{∂y} \\end{cases} $$\nSo the second-order partial derivative formula is:\n$$ \\begin{cases} \\frac{∂^2u}{∂ x^2} = \\frac{∂}{∂x} (\\frac{∂u}{∂x}) = \\frac{∂}{∂x} \\left( \\frac{∂u}{∂r} \\frac{∂r}{∂x} + \\frac{∂u}{∂φ} \\frac{∂φ}{∂x}\\right) \\\n\\frac{∂^2u}{∂ y^2} = \\frac{∂}{∂y} (\\frac{∂u}{∂y}) = \\frac{∂}{∂y} \\left( \\frac{∂u}{∂r} \\frac{∂r}{∂y} + \\frac{∂u}{∂φ} \\frac{∂φ}{∂y}\\right) \\ \\end{cases} $$\nAccording to the derivative product rule:\n$$ \\begin{cases} \\frac{∂}{∂x} \\left( \\frac{∂u}{∂r} \\frac{∂r}{∂x} + \\frac{∂u}{∂φ} \\frac{∂φ}{∂x}\\right) = \\underline{\\frac{∂}{∂x}(\\frac{∂u}{∂r})} \\frac{∂r}{∂x} + \\frac{∂}{∂x} (\\frac{∂r}{∂x}) \\frac{∂u}{∂r}\n\\underline{\\frac{∂}{∂x}(\\frac{∂u}{∂φ})} \\frac{∂φ}{∂x} + \\frac{∂}{∂x} (\\frac{∂φ}{∂x}) \\frac{∂u}{∂φ}\\ \\frac{∂}{∂y} \\left( \\frac{∂u}{∂r} \\frac{∂r}{∂y} + \\frac{∂u}{∂φ} \\frac{∂φ}{∂y}\\right) = \\underline{\\frac{∂}{∂y}(\\frac{∂u}{∂r})} \\frac{∂r}{∂y} + \\frac{∂}{∂y} (\\frac{∂r}{∂y}) \\frac{∂u}{∂r}\n\\underline{\\frac{∂}{∂y}(\\frac{∂u}{∂φ})} \\frac{∂φ}{∂y} + \\frac{∂}{∂y} (\\frac{∂φ}{∂y}) \\frac{∂u}{∂φ}\\ \\end{cases} $$ And\n$$ \\begin{aligned} \\begin{cases} \\frac{∂}{∂x} (\\frac{∂u}{∂r}) = \\frac{∂}{∂r}(\\frac{∂u}{∂r}) \\frac{∂r}{∂x} + \\frac{∂}{∂φ}(\\frac{∂u}{∂r}) \\frac{∂φ}{∂x} \\\n\\frac{∂}{∂x} (\\frac{∂u}{∂φ}) = \\frac{∂}{∂r}(\\frac{∂u}{∂φ}) \\frac{∂r}{∂x} + \\frac{∂}{∂φ}(\\frac{∂u}{∂φ}) \\frac{∂φ}{∂x} \\ \\end{cases}\n\\quad\n\\begin{cases} \\frac{∂}{∂y} (\\frac{∂u}{∂r}) = \\frac{∂}{∂r}(\\frac{∂u}{∂r}) \\frac{∂r}{∂y} + \\frac{∂}{∂φ}(\\frac{∂u}{∂r}) \\frac{∂φ}{∂y} \\\n\\frac{∂}{∂y} (\\frac{∂u}{∂φ}) = \\frac{∂}{∂r}(\\frac{∂u}{∂φ}) \\frac{∂r}{∂y} + \\frac{∂}{∂φ}(\\frac{∂u}{∂φ}) \\frac{∂φ}{∂y} \\ \\end{cases} \\end{aligned} $$\nSo\n$$ \\begin{cases} \\frac{∂^2 u}{∂x^2} = \\left(\\frac{∂}{∂r}(\\frac{∂u}{∂r}) \\frac{∂r}{∂x} + \\frac{∂}{∂φ}(\\frac{∂u}{∂r}) \\frac{∂φ}{∂x} \\right) \\frac{∂r}{∂x}\n\\frac{∂}{∂x} (\\frac{∂r}{∂x}) \\frac{∂u}{∂r} \\left(\\frac{∂}{∂r}(\\frac{∂u}{∂φ}) \\frac{∂r}{∂x} + \\frac{∂}{∂φ}(\\frac{∂u}{∂φ}) \\frac{∂φ}{∂x} \\right) \\frac{∂φ}{∂x} \\frac{∂}{∂x} (\\frac{∂φ}{∂x}) \\frac{∂u}{∂φ} \\ \\frac{∂^2 u}{∂y^2} = \\left(\\frac{∂}{∂r}(\\frac{∂u}{∂r}) \\frac{∂r}{∂y} + \\frac{∂}{∂φ}(\\frac{∂u}{∂r}) \\frac{∂φ}{∂y} \\right) \\frac{∂r}{∂y}\n\\frac{∂}{∂y} (\\frac{∂r}{∂y}) \\frac{∂u}{∂r} \\left(\\frac{∂}{∂r}(\\frac{∂u}{∂φ}) \\frac{∂r}{∂y} + \\frac{∂}{∂φ}(\\frac{∂u}{∂φ}) \\frac{∂φ}{∂y} \\right) \\frac{∂φ}{∂y} \\frac{∂}{∂y} (\\frac{∂φ}{∂y}) \\frac{∂u}{∂φ} \\end{cases} $$ Simplified:\n$$ \\begin{aligned}\n\\begin{cases} \\frac{∂^2 u}{∂x^2} = \\frac{∂^2 u}{∂r^2} \\frac{∂r}{∂x}\\frac{∂r}{∂x} + \\frac{∂^2 u}{∂φ∂r} \\frac{∂φ}{∂x} \\frac{∂r}{∂x} + \\frac{∂^2r}{∂x^2} \\frac{∂u}{∂r}\n\\frac{∂^2 u}{∂r∂φ} \\frac{∂r}{∂x} \\frac{∂φ}{∂x} + \\frac{∂^2 u}{∂φ^2} \\frac{∂φ}{∂x} \\frac{∂φ}{∂x} + \\frac{∂^2φ}{∂x^2} \\frac{∂u}{∂φ} \\ \\frac{∂^2 u}{∂y^2} = \\frac{∂^2 u}{∂r^2} \\frac{∂r}{∂y} \\frac{∂r}{∂y} + \\frac{∂^2 u}{∂φ∂r} \\frac{∂φ}{∂y} \\frac{∂r}{∂y} + \\frac{∂^2r}{∂y^2} \\frac{∂u}{∂r}\n\\frac{∂^2 u}{∂r∂φ} \\frac{∂r}{∂y} \\frac{∂φ}{∂y} + \\frac{∂^2 u}{∂φ^2} \\frac{∂φ}{∂y} \\frac{∂φ}{∂y} + \\frac{∂^2φ}{∂y^2} \\frac{∂u}{∂φ} \\ \\end{cases} \\ \\Rightarrow\n\\begin{cases} \\frac{∂^2 u}{∂x^2} = \\frac{∂^2 u}{∂r^2} \\frac{∂r}{∂x} \\frac{∂r}{∂x}+ 2\\frac{∂^2 u}{∂φ∂r} \\frac{∂φ}{∂x} \\frac{∂r}{∂x} + \\frac{∂^2r}{∂x^2} \\frac{∂u}{∂r}\n\\frac{∂^2 u}{∂φ^2}\\frac{∂φ}{∂x} \\frac{∂φ}{∂x}+ \\frac{∂^2φ}{∂x^2} \\frac{∂u}{∂φ} \\ \\frac{∂^2 u}{∂y^2} = \\frac{∂^2 u}{∂r^2} \\frac{∂r}{∂y} \\frac{∂r}{∂y} + 2\\frac{∂^2 u}{∂φ∂r} \\frac{∂φ}{∂y} \\frac{∂r}{∂y} + \\frac{∂^2r}{∂y^2} \\frac{∂u}{∂r}\n\\frac{∂^2 u}{∂φ^2} \\frac{∂φ}{∂y} \\frac{∂φ}{∂y} + \\frac{∂^2φ}{∂y^2} \\frac{∂u}{∂φ} \\ \\end{cases} \\end{aligned} $$\nIn polar coordinate system:\n$$ \\begin{aligned} \\frac{∂r}{∂x} \u0026= (\\sqrt{x^2+y^2})' = \\frac{1}{2}(x^2+y^2)^{-\\frac{1}{2}} 2x = \\frac{x}{r} = cosφ \\\\ \\frac{∂φ}{∂x} \u0026= (arctan(\\frac{y}{x}))' = \\frac{1}{1+(\\frac{y}{x})^2} \\cdot (\\frac{y}{x})' = -\\frac{y}{r^2} = \\frac{-sinφ}{r} \\\\ \\frac{∂r}{∂y} \u0026= (\\sqrt{x^2+y^2})' = \\frac{y}{r} = sinφ \\\\ \\frac{∂φ}{∂y} \u0026= (arctan(\\frac{y}{x}))' = \\frac{x}{r^2} = \\frac{cosφ}{r} \\end{aligned} $$And also:\n$$ \\begin{aligned} \\frac{∂^2 r}{∂x^2} \u0026= (\\frac{x}{r})' = \\frac{1}{r} - x^2 r^{-3} = \\frac{y^2}{r^3} \\\\ \\frac{∂^2 r}{∂y^2} \u0026= (\\frac{y}{r})' = \\frac{1}{r} - y^2 r^{-3} = \\frac{x^2}{r^3} \\\\ \\frac{∂^2 φ}{∂^2 x} \u0026= (\\frac{-y}{r^2})' = \\frac{2xy}{r^4} \\\\ \\frac{∂^2 φ}{∂^2 y} \u0026= (\\frac{x}{r^2})' = \\frac{-2xy}{r^4} \\end{aligned} $$So:\n$$ \\begin{aligned} \\frac{∂^2 r}{∂x^2} + \\frac{∂^2 r}{∂y^2} = \\frac{1}{r} \\\\ \\frac{∂^2 φ}{∂^2 x} + \\frac{∂^2 φ}{∂^2 y} = 0 \\end{aligned} $$The Laplacian equation becomes:\n$$ \\begin{aligned} \\frac{∂^2 u}{∂x^2} + \\frac{∂^2 u}{∂y^2} = \u0026amp; \\frac{∂^2 u}{∂r^2} \\frac{∂r}{∂x} \\frac{∂r}{∂x} + 2\\frac{∂^2 u}{∂φ∂r} \\frac{∂φ}{∂x} \\frac{∂r}{∂x} + \\frac{∂r^2}{∂x^2} \\frac{∂u}{∂r} + \\frac{∂^2 u}{∂φ^2} \\frac{∂φ}{∂x} \\frac{∂φ}{∂x} + \\frac{∂^2φ}{∂x^2} \\frac{∂u}{∂φ} \\ \u0026amp; +\\frac{∂^2 u}{∂r^2} \\frac{∂r}{∂y} \\frac{∂r}{∂y} + 2\\frac{∂^2 u}{∂φ∂r} \\frac{∂φ}{∂y} \\frac{∂r}{∂y} + \\frac{∂r^2}{∂y^2} \\frac{∂u}{∂r} + \\frac{∂^2 u}{∂φ^2} \\frac{∂φ}{∂y} \\frac{∂φ}{∂y} + \\frac{∂^2φ}{∂y^2} \\frac{∂u}{∂φ} \\\n=\u0026amp; (\\frac{∂r}{∂x} \\frac{∂r}{∂x}+ \\frac{∂r}{∂y} \\frac{∂r}{∂y}) \\frac{∂^2 u}{∂r^2} + 2(\\frac{∂φ}{∂x} \\frac{∂r}{∂x} + \\frac{∂φ}{∂y} \\frac{∂r}{∂y}) \\frac{∂^2 u}{∂φ∂r} + (\\frac{∂r^2}{∂x^2}+\\frac{∂r^2}{∂y^2}) \\frac{∂u}{∂r} \\\\ \u0026amp; + \\left(\\frac{∂φ}{∂x} \\frac{∂φ}{∂x} + \\frac{∂φ}{∂y} \\frac{∂φ}{∂y} \\right) \\frac{∂^2u}{∂φ^2} + (\\frac{∂^2φ}{∂x^2} +\\frac{∂^2φ}{∂y^2})\\frac{∂u}{∂φ} \\\\ =\u0026amp; \\frac{∂^2 u}{∂r^2} + 2(\\frac{-sinφ cosφ}{r} +\\frac{cosφ sinφ}{r}) \\frac{∂^2 u}{∂φ∂r} + \\frac{1}{r} \\frac{∂u}{∂r} + \\frac{1}{r^2} \\frac{\\partial^2 u}{\\partial φ^2} + 0 \\\\ =\u0026amp; \\frac{∂^2 u}{∂r^2} + \\frac{1}{r} \\frac{∂u}{∂r} + \\frac{1}{r^2} \\frac{\\partial^2 u}{\\partial φ^2} \\end{aligned} $$\nThe form of Laplace operator in polar coordinate system:\n$$ \\nabla^2 = \\frac{∂^2}{∂r^2} + \\frac{1}{r} \\frac{∂}{∂r} + \\frac{1}{r^2} \\frac{\\partial^2}{\\partial φ^2} $$参考: 拉普拉斯算子的极坐标、柱坐标和球坐标表示\n球坐标系下拉普拉斯方程 从直角坐标到球坐标的变换：\n$$ \\begin{cases} r = \\sqrt{x^2 + y^2 + z^2} \\\\ θ = arctan (\\frac{\\sqrt{x^2 + y^2}}{z}) \\\\ φ = arctan \\frac{y}{x} \\end{cases} $$拉普拉斯算符：$\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} + \\frac{\\partial^2}{\\partial z^2}$\n将拉普拉斯算符 $\\nabla^2$ 作用到球坐标系下的函数 $f(r, \\theta, \\phi)$ 上:\n$$ \\nabla^2 f(r, \\theta, \\phi) = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} + \\frac{\\partial^2 f}{\\partial z^2} $$根据链式法则：\n$$ \\begin{cases} \\frac{\\partial f}{\\partial x} = \\frac{∂f}{∂r} \\frac{∂r}{∂x} + \\frac{∂f}{∂\\theta} \\frac{∂\\theta}{∂x} + \\frac{∂f}{∂\\phi} \\frac{∂\\phi}{∂x}\\\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial r} \\frac{\\partial r}{\\partial y} + \\frac{\\partial f}{\\partial \\theta} \\frac{\\partial \\theta}{\\partial y} + \\frac{\\partial f}{\\partial \\phi} \\frac{\\partial \\phi}{\\partial y}\\\n\\frac{\\partial f}{\\partial z} = \\frac{\\partial f}{\\partial r} \\frac{\\partial r}{\\partial z} + \\frac{\\partial f}{\\partial \\theta} \\frac{\\partial \\theta}{\\partial z} + \\frac{\\partial f}{\\partial \\phi} \\frac{\\partial \\phi}{\\partial z}\\ \\end{cases} $$\n二阶导（运用乘法法则）：\n$$ \\begin{cases} \\frac{∂^2f}{∂x^2} \u0026amp;= \\frac{∂}{∂x}(\\frac{∂f}{∂x}) \\ \u0026amp;= \\frac{∂}{∂x}(\\frac{∂f}{∂r} \\frac{∂r}{∂x} + \\frac{∂f}{∂\\theta} \\frac{∂\\theta}{∂x} + \\frac{∂f}{∂\\phi} \\frac{∂\\phi}{∂x})\\\n\u0026amp;= \\frac{∂}{∂x}(\\frac{∂f}{∂r}) \\frac{∂r}{∂x} + \\frac{∂}{∂x}(\\frac{∂r}{∂x})\\frac{∂f}{∂r} +\\frac{∂}{∂x}(\\frac{∂f}{∂θ}) \\frac{∂θ}{∂x} + \\frac{∂}{∂x}(\\frac{∂θ}{∂x})\\frac{∂f}{∂θ} +\\frac{∂}{∂x}(\\frac{∂f}{∂φ}) \\frac{∂φ}{∂x} + \\frac{∂}{∂x}(\\frac{∂φ}{∂x})\\frac{∂f}{∂φ} \\\n\\frac{∂^2f}{∂y^2} \u0026amp;= \\frac{∂}{∂y}(\\frac{∂f}{∂y}) \\ \u0026amp;= \\frac{∂}{∂y}(\\frac{∂f}{∂r} \\frac{∂r}{∂y} + \\frac{∂f}{∂\\theta} \\frac{∂\\theta}{∂y} + \\frac{∂f}{∂\\phi} \\frac{∂\\phi}{∂y})\\\n\u0026amp;= \\frac{∂}{∂y}(\\frac{∂f}{∂r}) \\frac{∂r}{∂y} + \\frac{∂}{∂y}(\\frac{∂r}{∂y})\\frac{∂f}{∂r} +\\frac{∂}{∂y}(\\frac{∂f}{∂θ}) \\frac{∂θ}{∂y} + \\frac{∂}{∂y}(\\frac{∂θ}{∂y})\\frac{∂f}{∂θ} +\\frac{∂}{∂y}(\\frac{∂f}{∂φ}) \\frac{∂φ}{∂y} + \\frac{∂}{∂y}(\\frac{∂φ}{∂y})\\frac{∂f}{∂φ} \\\n\\frac{∂^2f}{∂z^2} \u0026amp;= \\frac{∂}{∂z}(\\frac{∂f}{∂z}) \\ \u0026amp;= \\frac{∂}{∂z}(\\frac{∂f}{∂r} \\frac{∂r}{∂z} + \\frac{∂f}{∂\\theta} \\frac{∂\\theta}{∂z} + \\frac{∂f}{∂\\phi} \\frac{∂\\phi}{∂z})\\\n\u0026amp;= \\frac{∂}{∂z}(\\frac{∂f}{∂r}) \\frac{∂r}{∂z} + \\frac{∂}{∂z}(\\frac{∂r}{∂z})\\frac{∂f}{∂r} +\\frac{∂}{∂z}(\\frac{∂f}{∂θ}) \\frac{∂θ}{∂z} + \\frac{∂}{∂z}(\\frac{∂θ}{∂z})\\frac{∂f}{∂θ} +\\frac{∂}{∂z}(\\frac{∂f}{∂φ}) \\frac{∂φ}{∂z} + \\frac{∂}{∂z}(\\frac{∂φ}{∂z})\\frac{∂f}{∂φ} \\ \\end{cases} $$\n因为（再使用链式法则）:\n$$ \\begin{aligned} \\begin{cases} \\frac{∂}{∂x} (\\frac{∂f}{∂r}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂r}) \\frac{∂r}{∂x} + \\frac{∂}{∂θ}(\\frac{∂f}{∂r}) \\frac{∂θ}{∂x} + \\frac{∂}{∂φ}(\\frac{∂f}{∂r}) \\frac{∂φ}{∂x} \\ \u0026amp;= \\frac{∂^2f}{∂r^2} \\frac{∂r}{∂x}+ \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂x}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂x} \\\n\\frac{∂}{∂x} (\\frac{∂f}{∂θ}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂θ}) \\frac{∂r}{∂x} + \\frac{∂}{∂θ}(\\frac{∂f}{∂θ}) \\frac{∂θ}{∂x} + \\frac{∂}{∂φ}(\\frac{∂f}{∂θ}) \\frac{∂φ}{∂x} \\ \u0026amp;= \\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂x}+ \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂x}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂x} \\\n\\frac{∂}{∂x} (\\frac{∂f}{∂φ}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂φ}) \\frac{∂r}{∂x} + \\frac{∂}{∂θ}(\\frac{∂f}{∂φ}) \\frac{∂θ}{∂x} + \\frac{∂}{∂φ}(\\frac{∂f}{∂φ}) \\frac{∂φ}{∂x} \\ \u0026amp;= \\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂x}+ \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂x}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂x} \\ \\end{cases} \\\n\\begin{cases} \\frac{∂}{∂y} (\\frac{∂f}{∂r}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂r}) \\frac{∂r}{∂y} + \\frac{∂}{∂θ}(\\frac{∂f}{∂r}) \\frac{∂θ}{∂y} + \\frac{∂}{∂φ}(\\frac{∂f}{∂r}) \\frac{∂φ}{∂y} \\ \u0026amp;= \\frac{∂^2f}{∂r^2} \\frac{∂r}{∂y}+ \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂y}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂y} \\\n\\frac{∂}{∂y} (\\frac{∂f}{∂θ}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂θ}) \\frac{∂r}{∂y} + \\frac{∂}{∂θ}(\\frac{∂f}{∂θ}) \\frac{∂θ}{∂y} + \\frac{∂}{∂φ}(\\frac{∂f}{∂θ}) \\frac{∂φ}{∂y} \\ \u0026amp;= \\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂y}+ \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂y}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂y} \\\n\\frac{∂}{∂y} (\\frac{∂f}{∂φ}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂φ}) \\frac{∂r}{∂y} + \\frac{∂}{∂θ}(\\frac{∂f}{∂φ}) \\frac{∂θ}{∂y} + \\frac{∂}{∂φ}(\\frac{∂f}{∂φ}) \\frac{∂φ}{∂y} \\ \u0026amp;= \\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂y}+ \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂y}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂y} \\ \\end{cases} \\\n\\begin{cases} \\frac{∂}{∂z} (\\frac{∂f}{∂r}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂r}) \\frac{∂r}{∂z} + \\frac{∂}{∂θ}(\\frac{∂f}{∂r}) \\frac{∂θ}{∂z} + \\frac{∂}{∂φ}(\\frac{∂f}{∂r}) \\frac{∂φ}{∂z} \\ \u0026amp;= \\frac{∂^2f}{∂r^2} \\frac{∂r}{∂z}+ \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂z}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂z} \\\n\\frac{∂}{∂z} (\\frac{∂f}{∂θ}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂θ}) \\frac{∂r}{∂z} + \\frac{∂}{∂θ}(\\frac{∂f}{∂θ}) \\frac{∂θ}{∂z} + \\frac{∂}{∂φ}(\\frac{∂f}{∂θ}) \\frac{∂φ}{∂z} \\ \u0026amp;= \\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂z}+ \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂z}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂z} \\\n\\frac{∂}{∂z} (\\frac{∂f}{∂φ}) \u0026amp;= \\frac{∂}{∂r}(\\frac{∂f}{∂φ}) \\frac{∂r}{∂z} + \\frac{∂}{∂θ}(\\frac{∂f}{∂φ}) \\frac{∂θ}{∂z} + \\frac{∂}{∂φ}(\\frac{∂f}{∂φ}) \\frac{∂φ}{∂z} \\ \u0026amp;= \\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂z}+ \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂z}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂z} \\ \\end{cases} \\end{aligned} $$\n所以：\n$$ \\begin{cases} \\frac{∂^2f}{∂x^2} \u0026amp;= (\\frac{∂^2f}{∂r^2} \\frac{∂r}{∂x}+ \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂x}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂x}) \\frac{∂r}{∂x} + \\frac{∂}{∂x}(\\frac{∂r}{∂x})\\frac{∂f}{∂r} \\ \u0026amp;+ (\\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂x}+ \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂x}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂x}) \\frac{∂θ}{∂x} + \\frac{∂}{∂x}(\\frac{∂θ}{∂x})\\frac{∂f}{∂θ} \\ \u0026amp;+ (\\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂x}+ \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂x}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂x}) \\frac{∂φ}{∂x} + \\frac{∂}{∂x}(\\frac{∂φ}{∂x})\\frac{∂f}{∂φ} \\\n\\frac{∂^2f}{∂y^2} \u0026amp;= (\\frac{∂^2f}{∂r^2} \\frac{∂r}{∂y}+ \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂y}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂y}) \\frac{∂r}{∂y} + \\frac{∂}{∂y}(\\frac{∂r}{∂y})\\frac{∂f}{∂r} \\ \u0026amp;+ (\\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂y}+ \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂y}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂y}) \\frac{∂θ}{∂y} + \\frac{∂}{∂y}(\\frac{∂θ}{∂y})\\frac{∂f}{∂θ} \\ \u0026amp;+ (\\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂y}+ \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂y}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂y}) \\frac{∂φ}{∂y} + \\frac{∂}{∂y}(\\frac{∂φ}{∂y})\\frac{∂f}{∂φ} \\\n\\frac{∂^2f}{∂z^2} \u0026amp;= (\\frac{∂^2f}{∂r^2} \\frac{∂r}{∂z}+ \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂z}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂z}) \\frac{∂r}{∂z} + \\frac{∂}{∂z}(\\frac{∂r}{∂z})\\frac{∂f}{∂r} \\ \u0026amp;+ (\\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂z}+ \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂z}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂z}) \\frac{∂θ}{∂z} + \\frac{∂}{∂z}(\\frac{∂θ}{∂z})\\frac{∂f}{∂θ} \\ \u0026amp;+ (\\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂z}+ \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂z}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂z}) \\frac{∂φ}{∂z} + \\frac{∂}{∂z}(\\frac{∂φ}{∂z})\\frac{∂f}{∂φ} \\end{cases} $$\n展开得：\n$$ \\begin{cases} \\frac{∂^2f}{∂x^2} \u0026amp;= \\frac{∂^2f}{∂r^2} \\frac{∂r}{∂x} \\frac{∂r}{∂x} + \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂x}\\frac{∂r}{∂x}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂x} \\frac{∂r}{∂x} + \\frac{∂^2r}{∂x^2}\\frac{∂f}{∂r} \\ \u0026amp;+ \\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂x}\\frac{∂θ}{∂x} + \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂x} \\frac{∂θ}{∂x}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂x} \\frac{∂θ}{∂x} + \\frac{∂^2θ}{∂x^2} \\frac{∂f}{∂θ} \\ \u0026amp;+ \\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂x} \\frac{∂φ}{∂x}+ \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂x} \\frac{∂φ}{∂x}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂x} \\frac{∂φ}{∂x} + \\frac{∂^2φ}{∂x^2}\\frac{∂f}{∂φ} \\\n\\frac{∂^2f}{∂y^2} \u0026amp;= \\frac{∂^2f}{∂r^2} \\frac{∂r}{∂y} \\frac{∂r}{∂y} + \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂y} \\frac{∂r}{∂y}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂y} \\frac{∂r}{∂y} + \\frac{∂^2r}{∂y^2}\\frac{∂f}{∂r} \\ \u0026amp;+ \\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂y} \\frac{∂θ}{∂y} + \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂y} \\frac{∂θ}{∂y}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂y} \\frac{∂θ}{∂y} + \\frac{∂^2θ}{∂y^2} \\frac{∂f}{∂θ} \\ \u0026amp;+ \\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂y} \\frac{∂φ}{∂y} + \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂y} \\frac{∂φ}{∂y}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂y} \\frac{∂φ}{∂y} + \\frac{∂^2φ}{∂y^2}\\frac{∂f}{∂φ} \\\n\\frac{∂^2f}{∂z^2} \u0026amp;= \\frac{∂^2f}{∂r^2} \\frac{∂r}{∂z} \\frac{∂r}{∂z} + \\frac{∂^2f}{∂θ∂r} \\frac{∂θ}{∂z} \\frac{∂r}{∂z}+ \\frac{∂^2f}{∂φ∂r}\\frac{∂φ}{∂z} \\frac{∂r}{∂z} + \\frac{∂^2r}{∂z^2}\\frac{∂f}{∂r} \\ \u0026amp;+ \\frac{∂^2f}{∂θ∂r} \\frac{∂r}{∂z} \\frac{∂θ}{∂z} + \\frac{∂^2f}{∂θ^2} \\frac{∂θ}{∂z} \\frac{∂θ}{∂z}+ \\frac{∂^2f}{∂φ∂θ}\\frac{∂φ}{∂z} \\frac{∂θ}{∂z} + \\frac{∂^2θ}{∂z^2} \\frac{∂f}{∂θ} \\ \u0026amp;+ \\frac{∂^2f}{∂r∂φ} \\frac{∂r}{∂z} \\frac{∂φ}{∂z} + \\frac{∂^2f}{∂θ∂φ} \\frac{∂θ}{∂z} \\frac{∂φ}{∂z}+ \\frac{∂^2f}{∂φ^2}\\frac{∂φ}{∂z} \\frac{∂φ}{∂z} + \\frac{∂^2φ}{∂z^2}\\frac{∂f}{∂φ} \\ \\end{cases} $$\nbecause:\n$$ \\begin{aligned} \\begin{cases} \\frac{∂r}{∂x} \u0026amp;= \\frac{x}{r} \\ \\frac{∂r}{∂y} \u0026amp;= \\frac{y}{r} \\ \\frac{∂r}{∂z} \u0026amp;= \\frac{z}{r} \\end{cases} \\quad\n\\begin{cases} \\frac{∂θ}{∂x} \u0026amp;= \\frac{z}{r^2} \\frac{x}{\\sqrt{x^2 + y^2}} \\ \\frac{∂θ}{∂y} \u0026amp;= \\frac{z}{r^2} \\frac{y}{\\sqrt{x^2 + y^2}} \\ \\frac{∂θ}{∂z} \u0026amp;= \\frac{-\\sqrt{x^2 + y^2}}{r^2} \\end{cases} \\quad\n\\begin{cases} \\frac{∂φ}{∂x} \u0026amp;= \\frac{-y}{x^2 + y^2} \\ \\frac{∂φ}{∂y} \u0026amp;= \\frac{x}{x^2 + y^2} \\ \\frac{∂φ}{∂z} \u0026amp;= 0 \\end{cases}\n\\\n\\begin{cases} \\frac{∂^2r}{∂x^2} \u0026amp;= \\frac{r^2-x^2}{r^3} \\ \\frac{∂^2r}{∂y^2} \u0026amp;= \\frac{r^2-y^2}{r^3} \\ \\frac{∂^2r}{∂z^2} \u0026amp;= \\frac{r^2-z^2}{r^3} \\end{cases} \\quad\n\\begin{cases} \\frac{∂θ}{∂x} \u0026amp;= \\frac{zr^2(x^2+y^2) - zx^2 [2(x^2+y^2)+1]}{r^4 (x^2+y^2)^{\\frac{3}{2}}} \\ \\frac{∂θ}{∂y} \u0026amp;= \\ \\frac{∂θ}{∂z} \u0026amp;= \\end{cases} \\quad\n\\end{aligned} $$\n","date":"2022-05-17T13:18:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/%E7%90%83%E8%B0%90%E5%87%BD%E6%95%B0/","title":"球谐函数"},{"content":"References:\nNeRF源码解读-dezeming | dezeming.top Searched by 杀生丸学ai in DDG(250410) (2022-07-20)\nHyperparameters N_rand 或 batch_rays 是一个 epoch/iteration 要训练的光线, no_batching 是每 epoch 只从一张图中选像素训练，若为 False 则打乱所有训练图像的像素（都与test阶段渲染全图无关）。\nllff 使用了batching，每一个 epoch 使用的(4096)光线来自所有训练图像； 而 blender 不做 batching，每 epoch 只从一张图中取光线，所以 N_rand 也较小(1024)， 有人说是因为 blender 包含 360 度的图片，可能会采到完全对立的两条光线（一个从正面看，一个从反面看），可能影响训练（影响density分布？）。 NeRF源码解析-什度学习\nN_samples 决定每条光线上的采样点数量和NeRF的渲染质量;\nchunk 是 model 一次计算的 rays, netchunk 是实际每次输入 model 的 3D points 量（模型的 train 和 test 都与它们有关）。 chunk 和 netchunk 是两层循环。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Select N_rand rays for training. # Or input all rays in an image (H,W) for rendering def render(rays): for each ray-chunk in rays: ret = render_rays(ray-chunk) def render_rays(): # Sampling 3D points and do PE for each point-netchunk in all points of this ray-chunk: raw.append(model(pts-netchunk)) return tf.concat(raw) # Collect ret for each ray-chunk ret_list.append[ret] # All rays are composed: all_ret = tf.concat(ret_list) # for N_rand or (H,W) rays chunk（光线）和netchunk（点）可依显存大小调整。当 N_rand 相同，而 netchunk 不同，曲线几乎重叠(~0.03)，只影响训练速度。\n虽预设 chunk =1024x32，但 N_rand (batch_rays) 预设只有1024， 所以实际输入 model 的光线是 1024，则输入 coarse model 的数据点有1024x64个（真正的\u0026rsquo;batch_size\u0026rsquo;）， 当通过attention layer时，要创建一个 (65536,128,128) 张量要用1.07G （不，tf 先创建计算图，PE 函数占了8个G，其他的显存应该是返回值(tensor)占的）\nndc 在 render() 中默认为 True， 只有当数据集不是 llff 类型，或指定不使用 ndc（--no_ndc）时，参数字典会添加一项ndc=False。\nlindisp = False 是对深度（世界空间:[near,far] 或者 ndc 空间:[0,1]）线性采样； 为 True 则对 1/depth 线性采样\ntestskip 不影响llff, 也不影响 train 集（逐张训练），跑 val 集和 test 集会跳过图片。\nfactor是 llff 数据集在 load_llff_data() 时的 shrink 倍数， half_res 影响 blender(和deepVoxel)数据集； render_factor 是 test 阶段(render_path())的渲染分辨率\nrandom_seed 在 test 阶段不起作用(perturb=False, raw_noise_std=0)。 在train 时，(use_batching) 会 打乱 所有像素， (no_batching)会 随机 选图片和像素, 在 density 上加随机 noise，在光线采样时加入随机 perturb。\nperturb 和 raw_noise_std 在 test (--render_only, render_path()) 渲染阶段都是0。\nOOM 3 similar variables:\nN_rand is the #rays used for 1 \u0026ldquo;epoch\u0026rdquo; to train\nchunk is the #rays to do render_rays() in a for loop, where points will be sampled to do positional embedding (fine stage has double pts to do PE.),\nThe embedded_fn() in coarse stage takes 2G meomery; The embedded_dirs() in coarse stage takes 2G; And the fine stage takes another 4G. NeRF model takes 357 MB.\nDict all_ret contains all the returend values from model. In the 12th iteration of render_rays() after coarse stage, VRAM becomes 10763 MB from 8465 MB and then OOM, where I returned 3 additional tensors: pts, viewdirs, z_vals. So the return value seems to occupy memory as well.\nBut if keeping the original settings where fewer tensors are returned, it becomes 10763 MB when finishing the ray-chunk for loop (32768 ray x 23 chunks) in batchify_rays().\nMemory change: 485 MB ⮕ 2277 MB ⮕ 4369 MB ⮕ 8465 MB\nnetchunk is the #pts fed into model.\nretraw is one of the reasons:\nWith setting --render_only and --factor=4, if retraw=True, OOM occurs at the line all_ret = {k: tf.concat(all_ret[k], 0) for k in all_ret} in batchify_ray(). 1 2 rgb, disp, acc, extras = render( H, W, focal, chunk=chunk, c2w=c2w[:3, :4], retraw=True, **render_kwargs) This shows that the returned values indeed take some memory. (2022-12-26)\nInverse transform sampling sample_pdf(z_vals_mid, color_weights[...,1:-1],N_importance,) code\nPDF is the proportion of the color-weight of a point to the sum of all weights on a ray. Each point has a ratio indicating how much of its color contributes to the pixel color. (They exclueded the start and end pts, and may only focus on the distribution of the middle part.)\nCDF tells how much the color has been rendered up to the current point. The input of CDF is the color-weight of a point, and its output is the cumulative weight from ro to that point. The weights will be accumulated to 1. So coarse net learns the weights CDF roughly. And fine sampling is based on the CDF increment.\n(When testing) N_importance points u are selected evenly from [0,1] and they will fall into buckets spaced apart by CDFs of coarse points. The percentage of the position of u between the coarse point below and above it determines the distance marching from its corresponding left-neighbor z_vals_mid. The marching distance of a fine pts z is proportional to the increment of cdf.\nWhen the CDF rises faster, there are more samples because the steep slope makes many us fall together.\nIn the above figure, the vertical coordiantes are the uniformly sampled u. Horizontal x-ticks are coarse samples on a ray. Green markers | are midpoints of coarse samples\u0026rsquo; z. Red points are fine samples. Red points are all start from a midpoint and march a distance which is proportional to the cdf increment of its corresponding u.\ntf.searchsorted(seq, values, side) returns the bin-index for each value. seq defines a series of bins, and values are assigned to bin-indices based on the edges listed in seq. side indicates whether the index of a bin is marked by its left or right edge. For example: edges = [-1, 3.3, 9.1, 10.0]; values = [0.0, 4.1, 12.0], return array([1,2,4]) more\n1 2 3 0 1 2 3 4 ————|———●—————|————●———————————|————|————●———— -1 0 3.3 4.1 9.1 10 12 (2022-06-15)\nPE has no π (pi in positional embedding? #12)\n世界坐标系下的 z（通过 projectmatrix 的第3行）从 (-near,-∞) 的范围缩放到了ndc坐标的[-1,1]，x,y并没有参与z的缩放，x,y是依据屏幕大小缩放的，如果 scene 不能被一个屏幕装下（一幅image没有把scene拍全），屏幕外的点的坐标是在[-1,1]范围外的：比如 scene 的范围是[-1.5, 1.5]，然后在embed时乘上π，则最低频时的定义域为 [-1.5π, 1.5π]，因为周期性，多个x可能对应同一个y，导致重复嵌入，所以再缩放一个π，定义域变为[-1.5,1.5]，在这段区间上是单调的，不同的x被编码后的值是不同的。 nerf代码分享-爱睡觉的人人-bili\n(2022-06-15)\nUse t to sample points 使用中间量 t 是为了统一 LLFF 场景和 blender 场景对光线采样的代码（llff 的光线长度z∈[0,1]，blender的光线长度z∈[near,far]）。\nblender场景边界有限可以直接对光线长度采样，但是 llff 场景光线无限长（直接采点效果不好？）， 需要先将场景的世界系变换为 NDC 系（只对 llff 场景有效），左乘 projection matrix 使世界系的 z 方向边界坐标 [-near,-far] 变为 [-1,1]。\n则世界系中的rays表达式: 𝐫=𝐨+t𝐝 变成了 𝐫=𝐨\u0026rsquo;+t\u0026rsquo;𝐝\u0026rsquo; (起点𝐨\u0026rsquo;和方向𝐝\u0026rsquo;都是near,far的函数)； 令𝐨\u0026rsquo;=𝐨，则(世界)光线长度t∈[0,∞] 变成(NDC)光线长度 t\u0026rsquo;∈[0,1] 附录公式15。 后面就可以通过对 t\u0026rsquo;∈[0,1] 采样，来对世界系中的各条光线采样（不是对世界空间的z轴采样，而是在各条光线上采样）。\n在对世界光线做 NDC 变换前，把各条光线的起点 𝐨 转移到其与近平面 (z_world=-near) 的交点处，这样变换到NDC后，光线长度t\u0026rsquo;=0 就对应着 -near，只会对各条光线落在z_world∈[-near,-far]的部分采样。在从世界系变到NDC中光线(𝐨\u0026rsquo;和𝐝\u0026rsquo;)时，认为near平面在z_world=1处(函数ndc_rays()的参数), far平面在 z_world=∞ 处 (公式23,24)。 (作者在issue34解释说:) 因为NDC的z轴是按 1/depth 线性变化的，所以世界系-far=∞没关系（变到ndc中就是far=1），只会稍微降低采样效率。\n确定好ndc光线后，把near,far重新赋值为光线长度的取值范围（llff 的NDC中光线长度的取值范围只能是[0,1]，而blender场景的光线长度范围就是世界系的[near,far]），用于在各光线上采样，再根据光线公式𝐨\u0026rsquo;+z⋅𝐝\u0026rsquo;得到采样点的3D坐标。（near,far不是depth深度范围，而是射线长度。） nerf code\nnear,far本来只是长度，在nerf中，世界系(avg_camera)与相机系z轴重合，scene落在-z方向，所以近远边界的 世界坐标 为-near,-far，而ndc的z轴方向是相机观测方向，近远平面的ndc坐标=-1,1。\n(2024-07-15)\n我上面说的“世界坐标系”，应该是指那个“平均相机坐标系”，因为 Projection Matrix 是把相机系坐标变换到 clipping space。 (2023-05-25)\nt or z sampling When sampling points:\n1 pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3] will: 这里rays_d要是没有归一化，直接就是 K逆*(u,v,1)，那这里z_vals 就是到成像平面距离；如果归一化了，那就是到光心距离. (QQ chat 2023-05-25T04:09:00) 哈哈: 基于数据集给定的深度直接映射三维点，远端会膨胀，投影到z轴上几何就正常了，所以NeRF出来的深度是视点到三维点的距离 (2022-06-15)\nRays in NeRF 对光线的处理：(camera ➔ world (avg camera) ➔ ndc)\n构造（\u0026ldquo;世界系\u0026quot;中的）光线：rays=get_rays_np(c2w) -\u0026gt; (ro+rd, 3)\n像素系(u,v) ➔ 成像平面系(u-cx, v-cy) ➔ unproj2相机系(x=u-cx/f, y=-(v-cy)/f, z=-1) ➔ ∗c2w（\u0026ldquo;世界系\u0026rdquo;），这里取平均相机系作为“世界系”。只有把光线变换到一个共同的\u0026quot;世界系\u0026quot;下才能用不同视图对同一 3D point 做优化。 传入的c2w=poses[:,:3,:4] 是从 load_llff_data()返回，其中函数recenter_poses()/spherify_poses()调整原poses，让它们能够把点变换到\u0026quot;新世界系\u0026quot;下。（NeRF代码中的poses全是c2w变换） avgc_2w=poses_avg(poses)，平均相机系➔世界系 poses=np.linalg.inv(avgc_2w)@poses，相机系➔世界系➔平均相机系(\u0026lsquo;新世界系\u0026rsquo;) 渲染（世界系中的）光线：rgb=render(batch_rays)\nro, rd = ndc_rays(near, ro, rd)，光线的世界坐标变NDC all_ret = batchify_rays(rays,chunk)；在render_rays()中，从NDC的 [near,far] 采样z 不直接使用 colmap 生成的c2w，因为 colmap (sfm) 中的世界系是由输入序列的第一帧确定的，但第一帧的方向是随机的，不一定正对scene，场景就重建不全，后面帧的 poses (列向量)都是相机在世界系中的表示 爱睡觉人人16:00。\n从像素平面构建出的 rays 是沿着相机系的-z轴的（观测方向，即景物在相机z轴的背面），也就是NDC的Z轴方向(ndc假设)；把光线变到平均相机系(世界系)下后，再变换到NDC，则scene就落在NDC的正半z轴上（之后都使用ndc坐标）。 因为 NDC 假设相机是沿 -z 方向观察，所以取各相机z轴反方向的平均，作为NDC的z轴方向。 因为光线先被变换到世界系下，再把世界系变为NDC（而OpenGL是把相机系变换到ndc），所以世界系的z轴与NDC的z轴一样看向scene。 nerf-issue#34 LLFF data preprocessing\n求平均相机系(在世界系下的表示) recenter_poses： 相机光心的坐标直接对第4列取平均得到；旋转矩阵R分为3个列向量：各相机z轴取平均，再使用右手定则确定x,y。 把4个列向量x,y,z轴和光心 concat 起来就是平均相机系在世界系中的表示，就是c2w。c2w 求逆得 w2c。 原poses 把相机系中点变换到世界系下，再由w2c变换到平均相机系(‘新世界系’)下。\n(矩阵的列名是源系, 行名是目标系, 变换顺序为从\u0026rsquo;列\u0026rsquo;到\u0026rsquo;行\u0026rsquo;, 每列的意义是\u0026rsquo;源\u0026rsquo; seen from \u0026lsquo;目标系\u0026rsquo;)\n对poses $\\begin{bmatrix} R \u0026 t \\\\\\ 0 \u0026 1 \\end{bmatrix}$ 求逆时，R转置，t 会有一点变化，最后一行还是0001。\n(2024-03-29)\n对 c2w 求逆后，w2c 中的 t 等于 $-R_{c2w}^T t_{c2w}$. For example, given a 3D point p, suppose its world-space coordinates is P. And its camera-space coordinates is X. So, there is:\n$$ \\begin{bmatrix} R_{c2w} \u0026 t_{c2w} \\\\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} X \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} P \\\\\\ 1 \\end{bmatrix} \\\\\\ R_{c2w} X + t_{c2w} = P \\\\\\ X = R_{c2w}^{-1} (P - t_{c2w}) \\\\\\ X = \\underbrace{R_{c2w}^T}\\_{R_{w2c}} P \\underbrace{- R_{c2w}^T t_{c2w}}\\_{t_{w2c}} $$ doubt: rays_d = dir@c2w.T 是怎么推导的? (c2w ∗ dir)ᵀ = dirᵀ ∗ c2wᵀ nerf-pl\n(2022-05-22)\ndoubt: C2W 是 K[R|T] 的逆，colmap直接求出了？对\nc2w 中的 [R|T] 为： $$ \\begin{array}{ccc} \\qquad \\rm Xcam \\quad\\quad Ycam \\quad\\quad Zcam \\quad cam\\ center\\\\\\ \\begin{bmatrix} 0.989 \u0026 -0.0224 \u0026 -0.142 \u0026 -3.67 \\\\\\ -0.0272 \u0026 -0.999 \u0026 -0.0318 \u0026 -1.603 \\\\\\ -0.141 \u0026 -0.0354 \u0026 -0.989 \u0026 -0.276 \\end{bmatrix} \\end{array} $$ 构造 world 系中的光线即确定 ro 和 rd：ro就是 cam center 在world中的坐标，就等于c2w第4列 ro=c2w[:3,-1]，rd 是用像素(u,v)对应的相机系坐标(u,v,1)变换到world系下的坐标(xw,yw,zw)表示 rd=c2w[:3,:3]@pixels_cam_coord bds.min 在load数据时，缩放场景下界bds.min()和平移变量T（至1./bd_factor，R不需缩放）是为了以防场景太大，变换到 NDC 时超出 near 平面；虽然这里缩放后的边界不等于 near，但之后在ndc_rays()中会被缩放回 -near (-1)。issue#34 (2022-06-03)\nget_rays() get_rays()从像素平面构建ray的方向向量dir： 首先把像素坐标 (u,v) 反投影到相机系 (RUB) 下: (xₚ=(u-cx)/f, yₚ=-(v-cy)/f, zₚ=-1)，v轴与y轴反向，取这些点深度值=-1（如下图）。 再乘以 c2w 变换到 world 坐标系下。从而一条光线在世界系中表示为：𝐫=𝐨+t𝐝）dir 与球坐标 φθ 没有关系。 nerf-issue#24: the direction in get_rays； 爱睡觉人人04:07\nColmap/OpenCV中的相机坐标系是 [right | down | forwards]， 经过LLFF存储poses为 [down|right|backwards]， NeRF 在_load_data()读取后立即变换为 (OpenGL中的相机系) [right | up | backwards]。\nposes_bounds.npy文件中是 camera-to-world！(而不是外参矩阵!) 每一行17个值， 前15个值是3x5的pose matrix，最后2个值是near,far场景边界。 一个 pose matrix 由5个列向量组成：cam down-axis, cam right-axis, cam back-axis, cam center, hwf(cam intrinsics)， 前3列是世界系下的相机系。\n_load_data() 返回的 poses 是 reshape 后的(3,5,20), 索引 poses[:,1:2] 对应到matrix中的第2列。 所以在 #250行 重新调整poses的\u0026quot;行\u0026quot;顺序，使R的列顺序(相机系)为：[right,up,back]。又在#251行把poses变回(20,3,5) Using your own poses without running COLMAP-LLFF\n在后续工作中，把nerf使用的poses（经load_llff_data()处理，作用为cam-to-avg_cam），又变回OpenCV下的相机系（比如IBRNet,GNT）。 如下图(from Zirui Wang-Twitter)\n(2022-11-14)\nThe direction of the constructed world rays rays_d should not be normalized to 1 when used for sampling points (where in nerf-pl is wrong confess). Original nerf didn\u0026rsquo;t normalize in get_rays(). Because the points are sampled based on z, so they locate on parallel planes. If the directions are normalized, the plane will become a sphere.\nBut the viewdirs is normalized when it used as the model input (See run_nerf.py line#306). viewdir is the normalized rd.\n(2024-07-15)\n把网络输出的 volume density 转换成 alpha 时，把采样点的 z 坐标间距 dists 转换成了光线段的 Euclidian 长度: z 间距乘以 rays_d 的模长.\nAlpha (self-occluaion自遮挡) 与 光线段长度 成正比.\ndist dists are intervals between [near,far] and used for transforming the density σ (activation) to opacity α of each interval on the ray: α=(1-e⁻ʳᵉˡᵘ⁽⁶ ᕁ ᵈⁱˢᵗ⁾), because the opacity is proportional to distance (\u0026ldquo;thickness\u0026rdquo;). The dists can be equal when no perturb, so if density is high like around the surface, the opacity is close to 1. And the color coefficient: w=(1-α) ∏ₜ₌₁ᵗ⁻¹ αₜ is monotonically decreasing, so the further a point is, the smaller its color coeff will be until 0.\nThe derivative of color weight at the surface point t∗ is not zero: $dw(t^∗)/dt \u003c 0$ (see Neus proof). Hence, the minia is not on the surface. Errors (2023-05-13)\nmogrify error A new environment is created on Ubuntu 22.04. The package imagemagick is installed in the env nerf by checking conda list. But when I debug the code in vscode, the shell /bin/sh cannot found the command mogrify:\n1 2 3 4 Fixing random seed 2201 Minifying 64 ./data/nerf_llff_data/fern mogrify -resize 1.5625% -format png *.JPG /bin/sh: 1: mogrify: not found Testing in a terminal, if the env is (nerf), executing the command: /bin/sh -c mogrify is okay and it will prompt the optional arguments.\nBut if the env is (base), executing /bin/sh -c mogrify, the same error occurs: /bin/sh: 1: mogrify: not found.\nI found a Chinese post for the same problem. He installed imagemagick again: sudo apt install imagemagick 【NeRF】在yenchenlin/nerf-pytorch上运行新的数据集\nFor this CalledProcessError \u0026quot;/bin/sh: 1: MY_COMMAND: not found\u0026quot; problem, some people suggest to create a link to /usr/bin, e.g., linux.org and ms-community\nmogrify doesn\u0026rsquo;t exist in /opt or /usr as the command find /opt /usr -name magick didn\u0026rsquo;t return anything. mogrify command not found (homebrew)\nmogrify only exits in:\n1 2 3 (nerf) w@homepc:~$ find ~/anaconda3 -name mogrify /home/w/anaconda3/pkgs/imagemagick-7.1.1_5-pl5321h211c493_1/bin/mogrify /home/w/anaconda3/envs/nerf/bin/mogrify Therefore, maybe the debugger of vscode will only search in the /usr/bin (without searching the newly created virtrual envs) ? But the interperter of VScode is indeed shown as Python 3.7.12 ('nerf') ~/anaconda3/envs/nerf/bin/python. I don\u0026rsquo;t know \u0026hellip;\nSolutions:\nCreat a symbolic link sudo ln -s ~/anaconda3/envs/nerf/bin/mogrify /usr/bin/mogrify, then that error disappeared. apt install imagemagick works too because it creates a binary in /usr/bin, which will be found by \u0026ldquo;/bin/sh\u0026rdquo;. (Didn\u0026rsquo;t try) Add the ~/anaconda3/envs/nerf/bin into PATH by adding the line: export PATH=$PATH:/home/zichen/anaconda3/envs/nerf/bin in ~/.bashrc imread_v2() error The version of imageio in this env is 2.28.1. And it\u0026rsquo;s 2.19.0 in my lab server. When reading images:\n1 2 3 def imread(f): if f.endswith(\u0026#39;png\u0026#39;): return imageio.imread(f, ignoregamma=True) 1 read() got an unexpected keyword argument \u0026#39;ignoregamma\u0026#39; Maybe I can remove that argument. But I chose to downgraded the package: conda install imageio=2.19.0.\n(2023-05-14)\nBlas GEMM launch failed Matrices cannot be multiplied at the 1st layer, but the input data and the coarse model both are correct.\n1 2 def ret(inputs): # inputs:(N_rays*N_samples, 90); ret: (n_inputs, 4) return tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0) Error message:\n1 tensorflow.python.eager.core._NotOkStatusException: InternalError: Blas GEMM launch failed : a.shape=(65536, 63), b.shape=(63, 256), m=65536, n=256, k=63 [Op:MatMul] Reboot doesn\u0026rsquo;t work. (Verified: nothing to with imagemagick.)\nnvidia-smi doesn\u0026rsquo;t show another process (notebooks, pycharm) using GPU besides Xorg and gnome-shell.\nNvidia suggested 30 series card to use CUDA 11.2 or newer. Error Internal: Blas GEMM launch failed 30系显卡不兼容?-知乎; Problems Training on RTX3080 - DeepSpeech - Mozilla Discourse\nSolution is using tf1.15 maintained by Nvidia Dr.Donald-2020-12-09 ,referenced by this post Solved: Error with Tensorflow \u0026amp; GPU - Dataiku community And this package requires Python 3.8, but nerf is using 3.7, so there is an error: error: subprocess-exited-with-error, when installing pip install --user nvidia-tensorflow[horovod] issue#15 However, if I directly downgrade conda install python=3.8, there will be too many conflict error. Also modifying environment.yml and creating based on that doesn\u0026rsquo;t work neither.\nBase on the answer on SO, I create a new env:\n1 2 3 4 5 6 conda create -n nerf-nvtf python=3.8 conda activate nerf-nvtf pip install --user nvidia-pyindex # conda install -c conda-forge openmpi # for multi-GPU # export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/anaconda3/envs/nerf/lib/ pip install --user nvidia-tensorflow[horovod] Then I tried import tensorflow works. But once I installed other packages:\n1 conda install numpy matplotlib imageio imageio-ffmpeg configargparse imagemagick Then import tensorflow cannot find the module.\nIs this problem caused by the following modificaitons?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 The following packages will be REMOVED: libgomp-11.2.0-h1234567_1 The following packages will be UPDATED: libgcc-ng anaconda/pkgs/main::libgcc-ng-11.2.0-~ --\u0026gt; anaconda/cloud/conda-forge::libgcc-ng-12.2.0-h65d4601_19 libstdcxx-ng anaconda/pkgs/main::libstdcxx-ng-11.2~ --\u0026gt; anaconda/cloud/conda-forge::libstdcxx-ng-12.2.0-h46fd767_19 zlib anaconda/pkgs/main::zlib-1.2.13-h5eee~ --\u0026gt; anaconda/cloud/conda-forge::zlib-1.2.13-h166bdaf_4 The following packages will be SUPERSEDED by a higher-priority channel: _libgcc_mutex anaconda/pkgs/main::_libgcc_mutex-0.1~ --\u0026gt; anaconda/cloud/conda-forge::_libgcc_mutex-0.1-conda_forge _openmp_mutex anaconda/pkgs/main::_openmp_mutex-5.1~ --\u0026gt; anaconda/cloud/conda-forge::_openmp_mutex-4.5-2_kmp_llvm python anaconda/pkgs/main::python-3.8.16-h7a~ --\u0026gt; anaconda/cloud/conda-forge::python-3.8.16-0_73_pypy I tried to remove those packages, but there\u0026rsquo;s a long waitting.\nSo I create a new environment:\n1 2 3 4 5 conda create -n nerf-nvtf-1.15 python=3.8 conda activate nerf-nvtf-1.15 pip install --user nvidia-pyindex pip install --user nvidia-tensorflow[horovod] pip install \u0026#39;imageio==2.19.0\u0026#39; configargparse Then the code can run normally.\nLego ply (2024-04-03)\nThe points3d.ply opened in Meshlab is a cube.\n","date":"2022-05-17T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-nerf_code_notes/","title":"Read: NVS - NeRF | Code Understanding"},{"content":"(2022-05-08)\nCondense math expression Homogeneous coord 可以把 常数项 和 除法 引入矩阵运算，是为了把平移（加常数）和透视投影（除深度）写到一个矩阵中。 矩阵乘法就是先乘再加，当齐次坐标为1，用于添加常数项； 当齐次坐标不为1，可以作为系数被除掉，得到归一的xyz。\nRefer to 探秘三维透视投影-齐次坐标的妙用 -奇乐编程学院 bilibili (2023-12-20)\n使用齐次坐标，则非线性的透视投影可以写成线性的运算。\nA linear operation T() satisfies additivity $T(a+b) = T(a) + T(b)$ and homogeneity $T(ca) = c T(a)$, where c is a scalar.\nHowever, perspective projection 𝐏 for representing a 3D scene on a 2D plane requires x, y divided by depth: x/d, y/d. Intuitively, use scaling to indicate depth. Perspective projection will alter the shape, such as parallel lines are no longer parallel:\nParallel lines aren\u0026rsquo;t parallel after projection.\nSimilarly, the 2D projection of a 3D ellipsoid may not an ellipse with both ends of equal size, but a tapered oval, resembling the outline of an egg.\nEllipse vs oval\nPerspective projection of a 3D Gaussian does not result in a 2D Gaussian. \u0026ndash; Mathematical Supplement for the gsplat Library\nThus, 𝐏 is not linear.\nBut if we bypass the division by considering x, y are already divided by d, forming 2D plane coordiantes (x/d, y/d), and then multiplying by d again, the 2D coordinates (x/d, y/d) need to append an additional dimension, i.e., (x/d, y/d, 1), to record the multiplier d.\nSo, the 2D plane coordinates (x/d, y/d, 1) becomes (x,y,d) after multiplying with d.\n$$(u,v) = (u,v,1) = (x/d, y/d, 1) = (x, y, d)$$Note: Appending 1 is not trying to revert a 2D pixel to 3D space, but used to represent the one more operation for the 2D coordinates. So it still represents a 2D pixel after appending 1.\nThat means, during the intermediate computation, there is no need to calculate (x/d, v/d) to obtain (u,v) , but use (x, y, d) to refer a 2D plane coordiantes.\nIn summary, the division is skipped, and 𝐏 becomes a linear operation represented with a 3x3 matrix.\nWhen moving in 3D space, the 3D coordiantes (x,y,z) needs an extra dimension as (x,y,z,1) to combine rotation and translation together. And [R|t] is a linear operation.\n(2024-02-15) The projection matrix also applies to a 4D camera point (x,y,z,1), where the homogeneous coordinate 1 will store the z, after the camera point got multiplied by the projection matrix, where the 4-th row is [0 0 -1 0].\nAs the resulting clip coordinates are not the final transformation yet in the pipeline, the original depth $z$ in camera space requires to be recorded for the perspective division that is supposed to be the final step.\nAfter frustum clipping and the clip coordinates perform perspective division, ND coordinates are obtained and utilized in the ND space for image formation.\n(2024-01-01) Summarize again\nIn the perspective projection, homogeneous coordinates use 3D coordinates to represent a 2D pixel for temporarily storing the depth value, which will be divided at the very end to keep the intermediate computations as linear operations.\nMoreover, in the view transformation, homogeneous coordinates uses 4D coordinates to represent a 3D point for holding the translation vector.\n(2024-07-18)\n齐次坐标的作用：\nImage from Quick Understanding of Homogeneous Coordinates for Computer Graphics - Miolith 中文翻译: 计算机图形学快速理解：齐次坐标 - Miolith)\n(2024-07-21)\n透视除法的根源是相似三角形：$\\frac{x_{film}}{focal} = \\frac{X_{cam}}{Z_{cam}}$。 (2022-08-16)\nDistinguish point and vector 齐次坐标用于区分 向量 和 点。\u0026lsquo;向量\u0026rsquo;只需基向量的线性组合，而\u0026rsquo;点\u0026rsquo;需要加上原点，把\u0026rsquo;点\u0026rsquo;表示为从原点出发的向量。 给定一组基向量𝐱,𝐲,𝐳，则一个向量𝐯 = a𝐱+b𝐲+c𝐳；而一个点 𝐩 = 𝐨+a𝐱+b𝐲+c𝐳，其中𝐨是原点。所以(a,b,c,0)是向量𝐯的坐标，而(a,b,c,1) 是点𝐩的坐标。\n平移变换需要使用齐次坐标，是因为只有‘点’需要平移，要想表示点就得用齐次坐标。而向量没有位置的概念，只有大小和方向\nRefer to 深入探索透视投影变换 - popy007 -CSDN (2024-02-17) Tutorial 3 : Matrices - opengl-tutorial.org\n(x,y,z,w=1) is a position, while (x,y,z,w=0) is a direction.\n(2023-02-13)\nShow 3D world on a plane When the homogeneous coordinate w=1 is appended behind the Cartesian coordinates (u,v), the result (u,v,w=1) becomes the 3D point (x,y,depth) because u=x/depth, v=y/depth.\n2D world on plane\n3D world on plane\nFor example, the railroad tracks are parallel on the 2D ground plane. But when they\u0026rsquo;re observed in a (higher-dimension 3D) projective space (human eyes, camera, convex lens), the parallel lines would converge. Otherwise, if our eyes are plane mirror, we will never find the world is 3D.\nThis effect can be interpreted as that the coordinates (x,y) scale down as 3D points get further away. Hence, drawing a railroad onto canvas should follow th relationship: (x/depth, y/depth), where x,y are constants and the depth increases.\n2D plane can only represent 2 directions, so if we want to display 3D world on a 2D plane, the additional dimension (depth) has to be engaged implicitly.\nTherefore, the meaning of pixel (u,v) on plane is (x/depth, y/depth), which corresponds to the 3D point (x,y,depth), such that the picture mimics the scene looked at by human: x,y are inversely proportional to depth (Big near, small far: perspective).\nThe homogeneous coordinate w=1 is used to accommodate the depth: (u = x/depth, v = y/depth, w=1) ⇒ (x, y, depth).\nThe w is not specified arbitrarily. If the given 2D coordinates are (u,v), then the w should =1, waiting for the depth split from u,v.\nTherefore, an extra dimension is supplimented to adapt the observation from higher-dimension space.\nThen, the homogenous coordinates of a pixel (u,v) on the plane is (u,v,w). When analyzing it in 2D space, its coordinates are (u/w, v/w).\nFor example, two pixels represented by homogeneous coordinates are (1,2,1) and (1,2,0)\nThat is, the projection pixel (u,v) of a 3D point, when the point goes far away, the coordinates (u,v) are not constant but inversely changing with depth. This effect can be represented by an extra dimension to reflect the depth change.\n(2023-02-12)\nCompensate for Cartesian coord The homogeneous coordiante w is supplemented to adapting Cartesian coordiantes to represent projective space Homogeneous Coordinates - songho. (Cannot convince me)\nThe parallel lines should never intersect at infinity in Cartesian space (plane), but they have to converge in projective space (human eye/camera).\nTo use 2D planes to represent perspective, the homogeneous coordinate w is appended behind the Cartesian coordinates (x,y) of each point to adapt to the projective observation.\nThus, each point in projective space has 3 coordinates (x,y,w). Then, the 2D coordinates of each point are obtained by normalizing the 3rd dimension: (x/w, y/w, 1), such that two parallel lines would converge. In other words, it\u0026rsquo;s easier to analyze multiple points by scaling their w to 1.\nHomogeneous coordinate w is the auxiliary for the Cartesian space. Thus, the effect of depth can be represented on a plane (like projection).\nIf the point (1,2) from Cartesian space is combined with different w to make up the homogeneous coordinates (1,2,w), the corresponding 2D coordinates (1/w, 2/w) will form a line.\nIf the 3 coordinates change propotionally, like (1,2,3), (2,4,6), \u0026hellip; (n,2n,3n), these homogeneous coordinates corresponds to a common 2D coordinates (1/3, 2/3) on the plane. This means the homogeneous coordinates are scale invariant. Or inversely, a pixel on the plane corresponds to a line in the 3D space (homogeneous coordinates system).\nw is an attribute for each point in perspective space, where every point has 3 coordinates (x,y,w), while the points in Euclidean space don\u0026rsquo;t have this property.\nWhat we human perceived on our retina or captured on the camera plane are the projection: (x/w, y/w, 1).\nBecause each 3D point has differnt w, their projections are located on different position on the image plane. Thus,\nTherefore, given an image, the homogeneous coordinates for each pixel are (u/w, v/w, 1). If the w is known, then the homogeneous coordinates can be wrriten as (u, v, w).\nThe point in 3D space has the coordinates: (x,y,w) is divided by the w. homogeneous coordinates (x,y,w)\nA picture showing projective effect actually is a stack of different planes with different depth.\n(x/w, y/w), where the x,y are already divided by the dpeth w, so if we want to get the Cartisan coordiantes back, the w has to be separated: (x, y, w), then the first 2 number are 2D Cartesian coordinates.\nThat means the homogeneous coordiantes of 2D point (x,y) is just appending a w at the end, like (x,y,w).\nThe points with propotional homogeneous coordinates corresponds to the same 2D Cartesian point. For example, (1,2,3) and (2,4,6) With the extra dimenstion, the coordinates for a 2D pixel\nHomogeneous coordinates convert the non-homogeneous linear system to a homogeneous system.\nIf the homogeneous coordinate w added behind Cartesian coordinates is to represent the depth (x/w,y/w,1)ᵀ, then the homogeneous coordinate w=1 added behind a 3D points is to accommodate the translation (x/w, y/w, z/w, 1)ᵀ. Ref 深入探索透视投影变换(续) - popy007 -CSDN Homogeneous coordinates - Wikipedia (Back to top)\n","date":"2022-05-08T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/b-note-homogeneous_coord/","title":"Memo: Vis - 3D | Homogeneous Coordinates"},{"content":"P15\nIrradiance The power per (perpendicular/projected) unit area incident on a surface point. $E(x)≡\\frac{dΦ}{dA} cos\\theta$ 单位面积上的能量(垂直与表面的分量) 光线在单位球面的能量与距离呈平方反比， ","date":"2022-05-02T09:24:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/games-101_cg/15-light_transport_global_illumination/","title":"watch: CG - 闫令琪 15 | Ray-Training-3"},{"content":"P14\nSpatial Partitioning 对空间做划分 tree 八叉树Oct-Tree 把一个3维的包围盒切成8份(2^3) 高维空间就是2^n 叉树， KD-Tree 每次划分只在1个维度上划分(二叉树) n个维度依次循环被划分，保证空间上是均匀的 问题：包围盒是否包围三角面需要对包围盒与三角面求交，不好写; 一个物体与多个叶子节点相交，所以每个叶子节点中都有存储这个物体（冗余？），最好是一个物体只存在于一个格子中（基于物体划分）。 BSP-Tree 每次划分方向并不与坐标轴平行 计算量大 KD-Tree Preprocessing 对场景建立KD-Tree Traversing a KD-Tree 从最大的包围盒开始，判断光线是否与之有交点，如果有交点，分别对它的两个子节点判断是否与光线有交点，如果没交点就不判断其子节点。走到叶子节点，就对包围盒中的所有物体求交。 Object Partitions \u0026amp; Bounding Volume Hierarchy (BVH) 把三角形分成两组，再分别重新求它们的包围盒，知道一个盒子中最多含有5个三角形就停止划分\n一个物体只会存在于一个包围盒中,(各盒子会有重叠)\nHow to subdivide a node?\nChoose a dimension to split Heurstic #1: Always choose the longest axis in node (沿最长轴划分) Heurstic #2: Split node at location of median object (以中间物体的位置划分，两侧物体数量相等，平衡意味着最大深度小，搜索次数少) Data Structure for BVHs\nInternal nodes store: Bounding box and Pointers to its child nodes Leaf nodes store: Bounding box and Objects Nodes represent subset of primitives(基础元素) in scene. All objects in subtree 遍历方式与KD-Tree 相同：\n1 2 3 4 5 6 7 8 9 10 11 Intersect(Ray ray, BVH node) { if (ray misses node.bbox) return; if (node is a leaf node) test intersection with all objs; return closest intersection; hit1 = Intersect(ray, node.child1); hit2 = Intersect(ray, node.child2); return the closer of hit1, hit2; Radiometry Termiology Radiant energy: 电磁辐射的能量 Q Radiant flux: 单位时间的能量 Φ=dQ/dt [Watt]（功率：单位时间射出多少光子） Radiant intensity: 从光源发出的光 Irradiance: 表面接收的光 Radiance: 沿光线传播的光 Radiant intensity The radiant (luminous) intensity I is the power Φ per unit solid angle ω emitted by a point light source. $I(w) ≡ \\frac{dΦ}{dω}$ [candela]; $Φ = \\int_S² I dw = 4πI$ 某一方向上的能量 ","date":"2022-05-01T18:28:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/games-101_cg/14-ray_tracing2/","title":"watch: CG - 闫令琪 14 | Ray-Tracing-2"},{"content":"P13\n判断点是否在多边形内部：以该点为起点沿任意方向做一条射线，与多边形的交点个数是奇数（相切情况认为是两个相同的实根？）可以推广至三维。\n夹板\n","date":"2022-05-01T14:24:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/games-101_cg/13/","title":"watch: CG - 闫令琪 13"},{"content":"Multi-Layer Perceptron MLP consists of many layers of perceptrons. MLP 由多层感知机构成。 Perceptrons 只能解决线性二分类问题，是因为它采用的阶跃激活函数： $e(t)=\\\\{^{0,t\u003c0}_{1,t\u003e0}$，只能确定多维输入空间中的一个超平面_。\n比如在二维空间中，它就无法解决异或问题。要产生非线性（斜率不固定）的决策边界可以换用非线性的激活函数，sigmoid function, tanh 或者 ReLu等等。它们使得感知机可以表达曲线，也就是即使输入超过阈值一些，输出没有对应的变化，依然为0。此时输入输出不再是线性关系，而不像阶跃函数，超过阈值就输出1。引入非线性之后，多层感知机才可以逼近任意非线性的决策边界。否则，MLP就相当于多条直线加权相加，最终得到的还是一条直线4，只能给出0-1分类。(这里的多条直线并不是联立求方程组，可以限定一块区域，而是被感知机加权，最后还是一条直线: wn(\u0026hellip;(w2(w1+b1)+b2+\u0026hellip;+bn) ）。Example with cubic function.6\n另一方面非线性激活函数可以帮助自动调整权重。多层感知机会带来大量需要调节的权重 w 1，手动调节是不可能实现的。阶跃函数的纵轴是0或1，横轴是wx（激活值），w的微小变化会导致激活值发生变化，如果输出层采用阶跃函数，output可能会跳变（在零点处不可微），就无法直接求导找到输出与输入之间的关系2。为了量化微小变化Δw带给输出的偏差，需要把台阶铺开，也就是变成sigmoid函数。从此目标就是调整 w 使MLP输出与目标值之间的误差e不断降低。而w的变化方向应该是使误差下降最快的方向，也就是梯度方向，就需要e对w求导数（e 先对 sigmoid 求导，sigmoid 再对 w求导），再乘以步长就是 w 每次需要修正的量。\n同时，网络中其余的感知机也都要使用sigmoid函数，才能求得网络中所有w的导数。 因为误差e是前一层感知机输出Z的函数，比如采用最小二乘法，则 e = (σ(Z)-target)²，根据链式法则，需要对左侧一层的激活函数求导才能求得之前 w 的导数。如果它们仍采用阶跃激活函数，导数在非零处为零，但在零点处不可导3，就无法修正之前的w，也就无法完成误差的反向传播。\nMSE 对 σ 求导：((σ(z)-target)²)\u0026rsquo; = 2σ(z)， σ 对 z 求导：σ(z)\u0026rsquo; = σ(z)(1-σ(z))， z 对 w 求导：x（来自上一层的输出，或是网络Input）， 所以输出层的 w 的导数为：2σ(z)∗σ(z)(1-σ(z))∗x（简化为 δ∗x）\n要求左侧一层的 w₋₁ 的导数，根据 chain rule: 这层的z对 x (也就是 σ₋₁(z₋₁)) 的导数: w， x 对 z₋₁ 的导数：σ(z₋₁)(1-σ(z₋₁))， z₋₁ 对 w₋₁ 的导数：x₋₁， 所以e对左侧一层的 w₋₁ 的导数为：δ∗w∗σ(z₋₁)(1-σ(z₋₁))∗x₋₁（简化为 δ₋₁∗x₋₁）\n再往左的话，即 δ₋₂∗x₋₂ , \u0026hellip;\n对于输出层有多个神经元，误差是多个神经元输出之和，所以e求导的时候要兵分多路，e分别对每一路求导数，然后加起来。4\n(2022-12-25) 比如下图中，有两个输出神经元 y₁^, y₂^，求 e 对 w₁ 的梯度∂e/∂w₁，就是两个输出神经元分别对 w₁ 求梯度，再加起来。\n刘二那个反向传播的图，分两路：一路对x求导，一路对w求导。\n每个局部计算的导数，是在forward 构建计算图时就算好的，函数的梯度存储在各input节点上，而不是存在output节点上。\nsigmoid function Perceptron accepts several input signals and give one output, including 0 and 1. Each input times its corresponding weight and is added up. If the weighted summation is larger than threshold, the output is 1. Otherwise, output is 0.\n深度学习一定要有偏置项Bias吗？外加一段简单的神经网络反向传播BP算法手写推导～ 一般要有 bias，因为线性函数 ax+b，b让模型在竖轴上移动，它在线性模型中是需要的。但在深度学习神经网络中，并不是每个地方都要设置 bias。 在清华大学的 cpm 项目中，认为在一些地方不设置 bias，使得模型训练稳定性会更强，计算速度以及显存消耗有优势\n参考 1. Lecture 10 - Neural Networks -Caltech (Learning from data)-ytb 2. NNDL Sigmoid Neurons 3. PyTorch: Introduction to Neural Network — Feedforward / MLP 4. 神经网络激活函数的作用和原理？有没有形象解释？ - 颜沁睿的回答 - 知乎 5. How Does Backpropagation Work? - kasperfred Why Neural Networks can learn (almost) anything - Emergent Garden - ytb ","date":"2022-02-14T23:10:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/dl-mlp_nonlinearity/","title":"memo: MLP Nonlinearity"},{"content":"一级标题 二级标题 正文\nbold text （设置快捷健 ,b\nitalic text (斜体快捷键 ,i)\nbold italic\nwasted text (删除线 ,s)\n3个` 括起来是代码块(1左边那个键)\n加上python会显示对应的高亮\n1 print(\u0026#34;hello\u0026#34;) [Named Link](http://www.baidu.com/\"Named link title\u0026quot;)\nhttp://www.baidu.com/\nhttp://example.com/\nheading-1 (转到一级标题处）\nFirst Header second Header Content Cell Content Cell content Cell content cell 一行代码 用` `括起来\ncode\nBullet List Item1 (*号和文字之间有空格） Nested bullet Sub_nested bullet etc Sub_nested sub_nested Bullet List item 2 1.A number list 1. A nested numbered list 2, which is numbered 2.which is numberd\n[] An uncompleted task ( - 和 [ 之间有空格） A complete task [] A subtask 引用块\n子引用块\n\u0026mdash; 水平线\nTitle 1 折叠内容Content 1 Content 1 Content 1 Content 1 Content 1\nF\n多行公式独立编号：(来源：Markdown下LaTeX公式、编号、对齐) $$ \\begin{eqnarray*} x^n+y^n \u0026=\u0026 z^n \\tag{1.4} \\\\ x+y \u0026=\u0026 z \\tag{1.5} \\end{eqnarray*} $$单个公式换行:\n单个公式很长的时候需要换行，但仅允许生成一个编号时，可以用split标签包围公式代码，在需要转行的地方使用\\\\，每行需要使用1个\u0026amp;来标识对齐的位置，结束后可使用\\tag{...}标签编号。 $$ \\begin{split}a \u0026= b \\\\c \u0026= d \\\\e \u0026= f \\end{split}\\tag{1.3} $$ 字母头上 ^ 1 $\\^θ$ 用 vim 插件 MarkdownPreview 在浏览器中可以正确渲染这个公式， 但是 Hugo 的KaTex 需要多打一个back slash：\\\\^θ 才能正确渲染，但这样做 vim 插件里就渲染不对了。\n单行写分段函数 For katex, there is one more \\\n1 \\\\{^{x=0}\\_{x=1} (从知乎看到的，忘了记录是哪个帖子)\n交换图表 MathJax基础（10）：Commutative diagrams\n$$ $\\require{AMScd}$ \\begin{CD} A @\u003ea\u003e\u003e B\\\\ @V b V V= @VV c V\\\\ C @\u003e\u003ed\u003e D \\end{CD} $$ mermaid style %% node style classDef lyrs fill:#ff9 class lyr11,act1,lyr12,lyr21,act2,lyr22 lyrs; %% link linkStyle 9,10,11,12,16,17,18,19 stroke:#0af,stroke-width:3px %% fontsize classDef node font-size:20px; Anchor Uppercase -\u0026gt; lowercase; Spaces -\u0026gt; -\nGithub Markdown anchor only linking to top of the page\n图片 Images side by side table 1 2 3 4 5 \u0026lt;table\u0026gt;\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/DOG.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px;\u0026#34;\u0026gt;是因为边界没有极值？所以只用中间层\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/DOG_levels.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px\u0026#34;\u0026gt;图2.DOG金字塔的实际显示效果\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/SIFT_Features.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px\u0026#34;\u0026gt;图3.DOG图像归一化结果\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/table\u0026gt; figure Problems:\nDisplay multiple image horizontally without using .css file Supports:\n\u0026lt;figure style=\u0026quot;\u0026quot;\u0026gt;\n1 2 3 4 5 \u0026lt;figure style=\u0026#34;display: flex; justify-content: space-between; align-items: center;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;path/to/image1.jpg\u0026#34; alt=\u0026#34;Description of image 1\u0026#34; style=\u0026#34;width: 48%;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;path/to/image2.jpg\u0026#34; alt=\u0026#34;Description of image 2\u0026#34; style=\u0026#34;width: 48%;\u0026#34;\u0026gt; \u0026lt;/figure\u0026gt; \u0026lt;figcaption style=\u0026#34;text-align: center; margin-top: 8px;\u0026#34;\u0026gt;Fig.1 - Two images side-by-side using inline styles.\u0026lt;/figcaption\u0026gt; ::: aside\nGemini 2.5P - Show Images Side-by-Side with CSS ::: 图片 caption Use \u0026lt;figcaption\u0026gt;\nReferences: {{{\nGemini 2.5P - Adding Captions to HTML Images }}} Supports:\n(2025-09-13T19:47)\nWarp the \u0026lt;img\u0026gt; tag in a \u0026lt;figure\u0026gt; tag:\n1 2 3 4 \u0026lt;figure\u0026gt; \u0026lt;img src=\u0026#34;path/to/img.png\u0026#34; alt=\u0026#34;Alternative text\u0026#34; width=\u0026#34;280\u0026#34;\u0026gt;\u0026lt;/img\u0026gt; \u0026lt;figcaption\u0026gt;Description\u0026lt;/figcaption\u0026gt; \u0026lt;/figure\u0026gt; 特殊字符 Common Unicode U+2016\t‖\tDOUBLE VERTICAL LINE\nWhite square: □ U+25A1 White Square Unicode Character - Compart\nThere is no superscript comma in unicode. Superscripting any Letter/Number for comma\nleft ceil, left floor, right ceil, right floor:\n⌈x₀⌉ |x₁| ⌊x₂⌋\ncircle:\n⦿\nClipboard of symbl.cc\nεφμf𝐱⋱⋮⋯⌊⌋σ√↓↑↓▼⬇λ˙⋅□↑⇔γβ∈α∑×‖⑊ ⊠∂Λθ∭∞°Σ➔©→✅⬯⬭τδ⊗∫𝚺𝛍𝐔𝐈𝐮∈𝐱𝐌𝐖𝚫ℤω℮∏‖⊙⊛ηρ𝐝𝐜𝐕𝒢𝐭𝐉𝐦𝛈≈❙❘⇤↳⋮⋆⭑✦★•𝑳𝐿𝐲𝐏𝐩𝐪𝐚𝐛𝐦𝐧𝐊𝐑𝐭𝐀𝐁𝐐𝐂𝐇𝐅𝐕|⏐▭✚▱𝐘𝐗𝐳𝐃𝐄⬯˚∘Δ😊≤▱■⌷⍨⍩⍤⟨〈⟩⬮↘∇𝛁 ▦\nLess than \u0026lt; will be mis-parsed as a html syntax by vim-markdown, use ❮ (Heavy Left-Pointing Angle Quotation Mark Ornament, brackets), or math $\u003c$\n(2025/02/11)\nUse html code: \u0026amp;lt;, \u0026amp;gt; (Found when copying text out from a .docx in Gmail) ${}$ won\u0026rsquo;t render the curly braces { }, use ｛ ｝ (Fullwidth Curly Brackets)\nOr $\\\\{ \\\\}$\n画斜线角度不够，可以用 ellipsis ⋱⋰ (Up Right Diagonal Ellipsis)\n可以在终端里显示的图形符号：Dingbats 集，和 Miscellaneous Symbols 里有一些。\n括号：⦗，因为 goat 里 ( 显不出来，也可以用中文括号（，不过显示出来偏右：和右边的字符有点重叠。\nList of all Unicode\u0026rsquo;s open/close brackets - SO\nNumbers in Dingbats: ➀ ➁ ➂ ➃ ➄ ➅ ➆ ➇ ➈ ➉ 改变 md 属性 Ctrl + S 字符 (^S) 会改变 Markdown 文件的文件类型\nSupports:\n(2025-08-05T09:36)\n一般 md 文件，在 yazi 里按回车会在终端里直接打开，但是如果文件中出现了 Ctrl+S 字符， 打开方式就变成用 GUI 的软件打开\n表格列宽 Cvt 2 Excel References:\nOnline MD to XLS converter | Free GroupDocs Apps Searched by convert markdown table to excel sheet in DDG Notes:\n(2024/12/20)\nUpload .md then download .xlsx r1-GroupDocs 整体缩放表格 用 css 样式\nReferences: {{{\nGemini 2.5P - Scaling Markdown Tables with HTML/CSS }}} Supports:\n(2025-08-28T15:08)\nDefine a sytle in the file r1-Gemini\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026lt;style\u0026gt; /* Styles for the screen */ .my-table-wrapper { overflow-x: auto; /* Allow scrollbar on screen if needed */ } /* Styles ONLY for printing */ @media print { body { font-size: 10pt; /* Set base font size for the whole document */ } .my-table-wrapper { overflow-x: visible; /* Ensure no scrollbar is printed */ } .my-table-wrapper table { width: 100%; table-layout: fixed; } .my-table-wrapper th, .my-table-wrapper td { word-break: break-all; font-size: 9pt; /* Further reduce font in table if needed */ } } \u0026lt;/style\u0026gt; \u0026lt;div class=\u0026#34;my-table-wrapper\u0026#34;\u0026gt; | Header 1 | Header 2 | A_very_very_long_and_unbreakable_header_string_that_causes_issues | |----------|----------|-------------------------------------------------------------------| | Data 1 | Data 2 | Some content that needs to fit neatly on the printed page. | \u0026lt;/div\u0026gt; Vim 中单格换行 Problems:\n用 vim 编辑 Markdown，一个 cell 的内容太多，让一行太长，不美观，不利于在 vim 中阅读\nGoAT (diagon) 可以固定 table 的布局，但是数学公式无法渲染，也不可使用 html 语法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \\`\\`\\`goat ┌──────────────┬─────────────────────────┐ │ │ Ray │ ├──────────────┼─────────────────────────┤ │Function │ $y=x$ │ ├──────────────┼─────────────────────────┤ │Parametric │ (Origin, Direction, Len)│ │equations in │ │ │Vector form │ │ ├──────────────┼─────────────────────────┤ │Parametric │ $x = o_x + d_x t$ │ │equations in │ $y = o_y + d_y t$ │ │component form│ │ └──────────────┴─────────────────────────┘ \\`\\`\\` Supports:\nQuarto supports multiline “grid” tables r1-GPT5\n1 2 3 4 5 6 7 8 9 10 11 12 13 +----------------------+------------------------------+ | | Ray | +======================+==============================+ | Function | $y = x$ | +----------------------+------------------------------+ | Parametric | (Origin, Direction, Len) | | equations in | | | Vector form | | +----------------------+------------------------------+ | Parametric | $x = o_x + d_x t$ | | equations in | $y = o_y + d_y t$ | | component form | | +----------------------+------------------------------+ ::: aside\nReferences {{{ 1. [GPT5 - Markdown table cells multiline](https://chatgpt.com/c/6910f49b-62f0-832d-9867-d53846628f19) }}} ::: 分栏 Problems:\n两个代码块并排显示，图片并排显示 VSCode extension: markdown-fence References:\nalislin/markdown-fence: Markdown Fence vscode 扩展，支持分栏格式 Introduced in markdown 轻松支持并列分栏 - 废除的文章 - 知乎 Searched by markdown 如何实现两个代码块并排 in DDG Supports:\n(2025-07-01T11:35)\nSpecial syntax:\n1 2 3 4 5 \u0026lt;!-- fence:start --\u0026gt; ![img1]() \u0026lt;!-- fence --\u0026gt; ![img2]() \u0026lt;!-- fence:end --\u0026gt; Use HTML Table Problems:\nVSCode extension requires VSCode to generate html code, which is cumbersome. Supports:\nUse HTML table r1-Gemini:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th style=\u0026#34;text-align:left; padding:8px; border-bottom:1px solid #ddd;\u0026#34;\u0026gt;Left title\u0026lt;/th\u0026gt; \u0026lt;th style=\u0026#34;text-align:left; padding:8px; border-bottom:1px solid #ddd;\u0026#34;\u0026gt;Right title\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td style=\u0026#34;vertical-align: top; width: 50%; padding-right: 1rem;\u0026#34;\u0026gt; \u0026lt;pre style=\u0026#34;margin:0;\u0026#34;\u0026gt;\u0026lt;code class=\u0026#34;language-c\u0026#34;\u0026gt; R_IIC00_Master_Send(0XC0, data_buf_tx, 1); \u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td style=\u0026#34;vertical-align: top; width: 50%;\u0026#34;\u0026gt; \u0026lt;pre style=\u0026#34;margin:0;\u0026#34;\u0026gt;\u0026lt;code class=\u0026#34;language-c\u0026#34;\u0026gt; R_IIC00_Master_Send(0XC0, data_buf_tx, 1); \u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; Empty lines are not allowed. ::: aside\nReferences {{{ 1. [Gemini 2.5P - Markdown Two-Column Layout Methods](https://gemini.google.com/share/ed42f08f0ca3) 2. [Gemini 2.5P - 2-column layout in Markdown](https://chatgpt.com/c/690e4c7e-2084-8325-9023-0df26622653e) }}} ::: 打印分页 使用 html tag\nReferences:\nGemini 2.5P - Markdown 分页符实现方法 Supports:\n(2025-07-22T11:03)\n\u0026lt;div style=\u0026quot;page-break-after: always;\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; 公式 箭头指示 DDG search: \u0026ldquo;latex math equation block where a symbol is pointed by an upward arrow?\u0026rdquo;\nArrow to an equal symbol in a equation to justify it - SE\n$$ a+b\\underset{\\substack{\\color{red}\\uparrow \\\\\\ % or: \\rotatebox{90}{$\\longrightarrow$} \\mathclap{\\textup{\\tiny commutative}} \\\\\\ \\mathclap{\\textup{\\tiny property}}}}{=}b+a $$ 符号加方框 1 \\boxed{x} $$ \\boxed{x} $$ multi-line (2024/06/17)\n\\substack\n$$ \\sum_{\\substack{1\\le i\\le n \\\\\\ i\\ne j}} $$\\atop vs. \\substack for multiple lines under a sum ( First seen in mathpix)\n\\overset and underset\n$$ \\overset{N}{=} \\quad \\underset{i=1}{=} $$Using underset and overset together\nMathjax (2025-05-15T23:28)\n不支持使用 \\bm 加粗，例如：[^{\\bm \\mu}_1]:\n$$ 𝐭' = 𝐏⋅ 𝐓_{w2c}⋅ [^{\\bm \\mu}_1] \\\\\\ $$ Small text (2024-05-24)\nUse the attribute font-size of tag \u0026lt;p\u0026gt;, e.g., \u0026lt;p style=\u0026quot;font-size:10px\u0026quot;\u0026gt;text\u0026lt;/p\u0026gt;: text\nKeep the text inline without going to a new line: \u0026lt;span style=\u0026quot;font-size:10px; color:red;\u0026quot;\u0026gt;text\u0026lt;/span\u0026gt;: text\n\u0026lt;small\u0026gt;text\u0026lt;/small\u0026gt;: text\nDeprecated in HTML5: \u0026lt;font size=\u0026quot;3\u0026quot; color=\u0026quot;red\u0026quot;\u0026gt;text\u0026lt;/font\u0026gt;: text\niframe (2024-01-21)\nThe size of the content can be set as below:\n1 2 3 4 5 \u0026lt;iframe width=\u0026#34;560\u0026#34; height=\u0026#34;315\u0026#34; src=\u0026#34;https://www.youtube.com/embed/-P28LKWTzrI?si=szYmMkdL7Ixw-6_4\u0026#34; title=\u0026#34;YouTube video player\u0026#34; frameborder=\u0026#34;0\u0026#34; allow=\u0026#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\u0026#34; allowfullscreen\u0026gt;\u0026lt;/iframe\u0026gt; Diagrams Mermaid mermaid-cli References:\nmermaid-js/mermaid-cli Could not find expected browser chrome locally - Stack Overflow Searched by mermaid Error: Could not find Chrome (ver. 131.0.6778.85). This can occur if either in DDG Notes:\n(2024-07-25)\nSteps\nInstall\n1 npm install -g @mermaid-js/mermaid-cli Edit input.mmd:\n1 2 3 4 5 classDiagram-v2 class Pancake { } class Waffle { } Export to svg: usage\n1 mmdc -i input.txt -o output.svg -b transparent Either .mmd or .txt works. Export to png:\n1 mmdc -i input.mmd -o out.png The content of the input.mmd is pure mermaid syntax:\n1 2 flowchart TD Start((\u0026#34;Start\u0026#34;)) --\u0026gt; Initialize[\u0026#34;Initialize\u0026#34;] --\u0026gt; End((\u0026#34;End\u0026#34;)) No chrome found?\n1 2 3 4 5 6 7 8 9 10 11 12 13 zichen@House:~/OneDrive/Exercises/Experiments/Embed/03_05-Guide-Hongsheng-LIN_Program$ mmdc -i test.txt -o output.svg -b transparent Generating single mermaid chart Error: Could not find Chrome (ver. 131.0.6778.85). This can occur if either 1. you did not perform an installation before running the script (e.g. `npx puppeteer browsers install chrome-headless-shell`) or 2. your cache path is incorrectly configured (which is: /home/zichen/.cache/puppeteer). For (2), check out our guide on configuring puppeteer at https://pptr.dev/guides/configuration. at ChromeLauncher.resolveExecutablePath (file:///usr/local/lib/node_modules/@mermaid-js/mermaid-cli/node_modules/puppeteer-core/lib/esm/puppeteer/node/BrowserLauncher.js:266:27) at ChromeLauncher.executablePath (file:///usr/local/lib/node_modules/@mermaid-js/mermaid-cli/node_modules/puppeteer-core/lib/esm/puppeteer/node/ChromeLauncher.js:202:25) at ChromeLauncher.computeLaunchArguments (file:///usr/local/lib/node_modules/@mermaid-js/mermaid-cli/node_modules/puppeteer-core/lib/esm/puppeteer/node/ChromeLauncher.js:83:37) at async ChromeLauncher.launch (file:///usr/local/lib/node_modules/@mermaid-js/mermaid-cli/node_modules/puppeteer-core/lib/esm/puppeteer/node/BrowserLauncher.js:45:28) at async run (file:///usr/local/lib/node_modules/@mermaid-js/mermaid-cli/src/index.js:500:17) at async cli (file:///usr/local/lib/node_modules/@mermaid-js/mermaid-cli/src/index.js:202:3) This issue is solved by installing the packages showed in the prompts r2-SO:\n1 2 3 4 5 6 zichen@House:~/OneDrive/Exercises/Experiments/Embed/03_05-Guide-Hongsheng-LIN_Program$ npx puppeteer browsers install chrome-headless-shell Need to install the following packages: puppeteer@23.9.0 Ok to proceed? (y) y chrome-headless-shell@131.0.6778.85 /home/zichen/.cache/puppeteer/chrome-headless-shell/linux-131.0.6778.85/chrome-headless-shell-linux64/chrome-headless-shell Mermaid CDN Problems:\n(2025-09-16T09:04)\nQuarto internal mermaid doesn\u0026rsquo;t perform well. References:\nUsage | Mermaid - JS.ORG Supports:\n(2025-09-16T09:30)\nThe following snippetr1-Docs written in an markdown file won\u0026rsquo;t be rendered by MarkdownPreviewEnhanced\n1 2 3 4 5 6 7 8 9 \u0026lt;pre class=\u0026#34;mermaid\u0026#34;\u0026gt; graph LR A --- B B--\u0026gt;C[fa:fa-ban forbidden] B--\u0026gt;D(fa:fa-spinner); \u0026lt;/pre\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34;\u0026gt; import mermaid from \u0026#39;https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs\u0026#39;; \u0026lt;/script\u0026gt; It may need extra command to trigger rendering. The index.md generated by executing quarto render index.qmd seems to contain javascript syntax for mermaid diagram. An html containing a mermaid diagram can be displayed with iframe. However, Hugo has difficulty to locate the html file.\nCircuits (2024-10-15)\nI wanted to include analog circuits in my blog post. The PlantUML seems not to support circuits diagrams after reading its docs.\nSearch: \u0026ldquo;can plantuml draw analog circuits?\u0026rdquo; in DDG\nMaking Drawing Circuits in Markdown a Cinch!\nInsert \u0026lsquo;schemdraw\u0026rsquo; svg in markdown\nschemdraw-markdown: an extention for Python-Markdown\nBTW, his blog (maybe: Pelican-blogsite) is really cool with various interactive presentation objects, such as PPT slides and \u0026lsquo;immich\u0026rsquo; photo album.\nAdd Timestamp Problems:\n(2025-02-21)\nI want to add the current time as version label. References:\njavascript - Auto insert date and time in form input field? - Stack Overflow Searched by javascript How to add the current data and time automatically in the markdown file in DDG DeepSeek Notes:\nOne approach is to use an input field, where the current time is displayed by default r1-SO. However, in the VSCode extension: \u0026ldquo;Markdown-Preview-Enhanced\u0026rdquo;, the input field appears blank. And the time is missing after exporting the document to a PDF. The script works in a browser.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ### Method 1 \u0026lt;input type=\u0026#34;date\u0026#34; id=\u0026#34;myDate\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;time\u0026#34; id=\u0026#34;myTime\u0026#34; /\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; function SetDateTime() { var date = new Date(); // Set the date var day = date.getDate(); var month = date.getMonth() + 1; var year = date.getFullYear(); if (month \u0026lt; 10) month = \u0026#34;0\u0026#34; + month; if (day \u0026lt; 10) day = \u0026#34;0\u0026#34; + day; var today = year + \u0026#34;-\u0026#34; + month + \u0026#34;-\u0026#34; + day; document.getElementById(\u0026#39;myDate\u0026#39;).value = today; // Set the time var hours = date.getHours(); var minutes = date.getMinutes(); var seconds = date.getSeconds(); if (hours \u0026lt; 10) hours = \u0026#34;0\u0026#34; + hours; if (minutes \u0026lt; 10) minutes = \u0026#34;0\u0026#34; + minutes; if (seconds \u0026lt; 10) seconds = \u0026#34;0\u0026#34; + seconds; var currentTime = hours + \u0026#34;:\u0026#34; + minutes + \u0026#34;:\u0026#34; + seconds; document.getElementById(\u0026#39;myTime\u0026#39;).value = currentTime; } \u0026lt;/script\u0026gt; \u0026lt;body onload=\u0026#34;SetDateTime();\u0026#34;\u0026gt; Other contents... Based on the above script, date and time can be obtained as plain text r2-DS:\nThis method can display the time after exporting PDF via Chrome -\u0026gt; PDF in the extension \u0026ldquo;Markdown-Preview-Enhanced\u0026rdquo;, although the time is not there in the preview window.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 ### Method 2 \u0026lt;div id=\u0026#34;markdown-content\u0026#34;\u0026gt; # My Markdown File **Date and Time:** \u0026lt;span id=\u0026#34;currentDateTime\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; This is some example Markdown content. \u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; function insertDateTime() { var date = new Date(); // Format the date var day = date.getDate(); var month = date.getMonth() + 1; var year = date.getFullYear(); if (month \u0026lt; 10) month = \u0026#34;0\u0026#34; + month; if (day \u0026lt; 10) day = \u0026#34;0\u0026#34; + day; var formattedDate = year + \u0026#34;-\u0026#34; + month + \u0026#34;-\u0026#34; + day; // Format the time var hours = date.getHours(); var minutes = date.getMinutes(); var seconds = date.getSeconds(); if (hours \u0026lt; 10) hours = \u0026#34;0\u0026#34; + hours; if (minutes \u0026lt; 10) minutes = \u0026#34;0\u0026#34; + minutes; if (seconds \u0026lt; 10) seconds = \u0026#34;0\u0026#34; + seconds; var formattedTime = hours + \u0026#34;:\u0026#34; + minutes + \u0026#34;:\u0026#34; + seconds; // Combine date and time var formattedDateTime = formattedDate + \u0026#34; \u0026#34; + formattedTime; // Insert the formatted date and time into the Markdown content document.getElementById(\u0026#39;currentDateTime\u0026#39;).textContent = formattedDateTime; } // Call the function to insert the date and time when the page loads insertDateTime(); \u0026lt;/script\u0026gt; Other content ... Code Call Graph Problems:\nI want to place elements at exact positions in a diagram. PlantUML for Call Stack References:\nJianpingCAI/call-stack-to-plantuml - GitHub Searched by plantuml for call stack graph in DDG Other tools are mentioned in Perplexity 6. Layout — The Hitchhiker\u0026rsquo;s Guide to PlantUML documentation Found in s1 Notes:\n(2025/03/12)\nUse Activity diagram to illustrate a code r1-GitHub\nThe positions of elements are determined by arrows r2-Hitchhiker\n代码块自定义行号 Problems:\nMarkdown 中的 Code block 的行号是从 1 开始，但是我想使用源文件中的行号 Issues:\nUse js library Notes:\n(2025-05-21T14:24)\nHighlight.js\nReferences:\nGemini 2.5P - Markdown Code Block Line Numbers 注释 使用 html comment tag \u0026lt;!-- --\u0026gt; 中间不能有空行\nSupports:\n(2025-08-25T11:07)\n在 VSCode 插件 Markdown-preview-Enhanced 中， 如果注释语法错误，后文有的内容不会显示\n下面的多行注释不会成功，因为有空行：\n1 2 3 4 5 6 \u0026lt;!-- * **References**: 1. \u0026lt;a id=\u0026#34;\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; [Article 1]() --\u0026gt; 必须为连续，可能是因为 markdown 的连续几行会被解释成一行，如果有空行，就是两段了\n1 2 3 4 5 \u0026lt;!-- * **References**: 1. \u0026lt;a id=\u0026#34;\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; [Article 1]() --\u0026gt; 必须是非空行，即使是有一个空格，也行。（这就是空格的意义）\n1 2 3 4 5 6 \u0026lt;!-- * **References**: 1. \u0026lt;a id=\u0026#34;\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; [Article 1]() --\u0026gt; 图文混排 Problems:\n我想给文字加边框/下划线，并且在特定文字下面加箭头指示下一步推理。\nMarkdown 不能指定文本之间的相对位置\nSupports:\n使用 cssr1-GPT5， 但是复杂几何图形的绘制代码不好写 ::: aside\nReferences {{{ 1. [GPT5 - Markdown 布局限制](https://chatgpt.com/s/t_690c1e913c94819180556b452600fc19) }}} ::: Integrate Excalidraw Problems:\nExcalidraw 能否用程序绘制，并嵌入 Markdown？ Supports:\n@baptiste 的 minicali 可以用命令生成简单的元素 r1- ::: aside\nReferences {{{ 1. [baptiste/minixcali](https://github.com/baptiste/minixcali) Mentioned in [blag/ - maths in excalidraw](https://blag.bapt.xyz/posts/maths-excalidraw/) 2. [Integration | Excalidraw developer docs](https://docs.excalidraw.com/docs/@excalidraw/excalidraw/integration) }}} ::: Actions:\nExcalidraw Render Math Export Math from qmd to Excalidraw Problems:\n@baptiste has already written many math equations in his qmd notes, so he wants to inserting those math blocks into Excalidraw. ::: aside\nReferences {{{ 1. [Quarto (maths) to Excalidraw #4144 · quarto-dev quarto-cli - GitHub](https://github.com/orgs/quarto-dev/discussions/4144) Searched by `quarto qmd integrate excalidraw` at [DDG](https://duckduckgo.com/?q=quarto+qmd+integrate+excalidraw\u0026ia=web) }}} Supports:\nQuarto is based on pandocr1-Gemini. By creating Pandoc custom writer, and defining filter rules, math blocks can be converted into svg.\nBranch: math.preview.excalidraw.com r1-Gemini\n::: aside\nReferences {{{ 1. [Gemini 2.5P - Quarto-Excalidraw Interoperability Discussion](https://gemini.google.com/u/1/app/75dd152969d32da9?pageId=none) 2. [Gemini 2.5P - Automating Excalidraw Math Import](https://gemini.google.com/u/1/app/015bc0595a89d342?pageId=none) }}} ::: Convert to Markdown MarkItDown Problems:\nConvert various files to markdown for feeding LLM. Supports:\nComparison MarkItDown with Pandoc r1-RealPython ::: aside\nReferences {{{ 1. [Python MarkItDown: Convert Documents Into LLM-Ready Markdown - Real Python](https://realpython.com/python-markitdown/) }}} ::: ","date":"2022-02-12T20:33:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/markdown/","title":"Memo: Lang - Markdown | Syntax \u0026 Misc"},{"content":"Arxiv\nSummary (2024-08-01)\nNovel view synthesis is a solution for image-based rendering task.\nNeRF leverages volumetric representation and volume rendering for novel view synthesis. While LLFF uses light field representation and MPI for NVS.\nAs Ben quoted Kajiya: \u0026ldquo;\u0026hellip; in 10 years, all rendering will be volume rendering.\u0026rdquo; on ECCV2022Tutorial\n(2022-01-08)\nThe input and output are the same as a volumetric model, but the loss function is based on the renderer error. A volumetric model maps something to the attributes of every volumetric grid. The output of a model is determined by the loss function. In NeRF, the loss is the rendering error between ground truth RGB image and the rendered image that is generated according to density (alpha channel) and RGB color at each point on the path of ray 1. Therefore, the output of the network should be a filed of density and color. Overview diagram\nOptimize Tricks Positional encoding: mapping the inputs to a higher dimensional space so that MLP could converge to a high-frequency function.\n(2023-10-27) PE is Fourier transform and then MLP re-composes Fourier coefficicent to the final value.\nwill. - QQ group:\n傅里叶级数是对三角函数项的线性加权；神经网络的线性层是对输入的线性加权。 density 网络就是用傅里叶级数拟合了 sigma 在三维空间中的分布 用优化器优化了傅里叶级数，然后通过体渲染约束体密度的性质。\n赤子泛舟：\n我理解也是等效做了一次傅里叶变换和反变换，mlp其实在拟合频域特征。 虽然声称没用卷积，但其实整个过程相当于做了个大的卷积。\n群除我佬：就是一个更吊的3d输入。\nHierarchical volume sampling: allocating more samples to where it is expected to have a significant contribution on the final rendering.\nviewdir is represented by Cartesian (x,y,z), rather than spherical coordinates (φ,θ,r)\nσ is output of linear layer, so it lies in (-∞,+∞); but volume density is [0~∞), so σ(x) is activated by ReLU. Then it converts to alpha (the probability of terminating) by $(1-e^{-σ⋅δ})$, that is when σ=0, alpha=0 meaning no volume no block, while if σ→∞, the alpha→1 meaning this volume block light entirely. Therefore, the transmittance is (1-alpha)=$e^{-σδ}$, so the decay coefficient (cumulative transimitance) of a RGB color is $Π^{i-1}e^{-σδ}$\nMath derivation 2.3 Integration along Ray Segmetns\nSecond sampling is based on \u0026ldquo;color decay coeff (weights)\u0026rdquo;, rather than volume density. The bigger color weights means more volume here. σ(x) is proportional to alpha and weights\n(2024-07-20)\nBen has an derivation in his ECCV2022 report\nOpacity is alpha. If so, the opacity and alpha in 3DGS may should be understood as: opacity is an attribute of the scene elements, while alpha is an attribute of the filter contributing to pixels.\nDensity is used in the continuous volume render function, whereas Alpha is used the discrete alpha compositing.\nChallenge: Indirect illumination is even more computationally-expensive than rendering with direct lighting. Angjoo\n(2024-07-22)\nNeRF 是在各射线上采样数据点，用MLP拟合 整个三维空间的从坐标到 RGBA 的函数； MLS 是用多项式拟合各点 局部的曲面函数，然后在各点的 local plane 采样以实现 resampling。 Code ImportError: No module named \u0026lsquo;_pywrap_tensorflow_internal\u0026rsquo;\nCreate environment: conda env create -f environment.yml, and this error occured when executing import tensorflow.\nSolution: I didn\u0026rsquo;t creat the envrionment from the yml file but directly specify packages and version. conda create -n nerf python=3.7 cudatoolkit=10.0 tensorflow-gpu=1.15 -c conda-forge\nReference 1. Ray tracing volume densities ","date":"2022-01-08T00:00:00Z","image":"https://github.com/bmild/nerf/raw/master/imgs/pipeline.jpg","permalink":"http://blog.zichen.uk/post/writenotes/model/nerfs/b-note-nerf_bmild/","title":"read: Render - NVS | NeRF"},{"content":"title: \u0026ldquo;3D Content Creation and Stylization with AI\u0026rdquo;\nSource link: GAMES Webinar 215-视觉专题：智能三维内容生成\u0026ndash;重建与创造-Kangxue Yin\n3D内容创作： 游戏，电影特效，动画，VR/AR，专业化程度高（建模软件）\n动画电影创作流程:\n剧本 原画 Layout: 物体相机摆放，位置关系 特效技术研发 2D 建模为3D 纹理 绑定骨骼，皮肤，相对运动关系 人物运动（运动捕捉） VFX 特效模拟（爆炸） 打光 渲染(物理仿真) 梦工厂pipeline：https://www.youtube.com/watch?v=ru0tQRJ4qKs\n3D 内容成本高：2009 阿凡达（3D） $ 2.37亿 ，而 2010年的2D电影 让子弹飞 只有 $ 0.18亿\n元宇宙需要大量3D 内容\n降低3D内容制作成本：让普通玩家参与创作（房子，汽车\u0026hellip;）NFT，未经训练，细节不足，风格不够多样，用人工智能辅助。\nResearch works Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape\n输入 voxel 模型，网络合成细节。\n3D represention 选择\nDiscrete Repre\nImplicit Fields:\n生成网络细节不足，转换为mesh有问题\ndifferentiable iso-surfacing 把隐式方程转化为一个mesh，然后用 mesh 和 ground truth 之间的差（loss）来优化网络，降低 iso-surfacing 离散化带来的误差，其中使用的不是 Marching cube 而是 Marching Tetrahedra\n3DStyleNet: Creating\n3D 物体的风格迁移\n","date":"2022-01-07T22:22:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/shapes/215-%E8%A7%86%E8%A7%89%E4%B8%93%E9%A2%98%E6%99%BA%E8%83%BD%E4%B8%89%E7%BB%B4%E5%86%85%E5%AE%B9%E7%94%9F%E6%88%90--%E9%87%8D%E5%BB%BA%E4%B8%8E%E5%88%9B%E9%80%A0-kangxue-yin/","title":"Watch: 215-智能三维内容生成"},{"content":"6-如何理解“梯度下降法”？什么是“反向传播”？通过一个视频，一步一步全部搞明白\n正向传播 输入数据沿着神经网络正向传递 输入数据与各个感知机的w和b点乘，将结果代入激活函数，给出判断结果 反向传播 把判断结果的偏差传递给各个w和b，根据参数对偏差贡献的大小，成比例的调整 未训练好的神经网络的$\\mathbf w,b$不准确，导致判断结果有偏差。如果某 $\\mathbf w/b$ 对最终的判断结果有重大影响，则该参数对于偏差也是有重大影响的。所以在减小误差的过程中，应优先调整d对偏差有重大影响的参数。 脑补过程：\n神经网络输出层的输出值是 $a^{[3]}$:\n$$ \\mathbf a^{[3]} = \\sigma(\\mathbf w^{[3]} \\cdot \\mathbf a^{[2]} + b^{[3]}) $$其中 $\\sigma$ 是激活函数，$\\mathbf w^{[3]}$输出层权重，$\\mathbf a^{[2]}$上一层的输出值，$b^{[3]}$是输出层偏置系数。\n可以计算损失函数（交叉熵），得到偏差J：\n$$ J = \\frac{1}{n} \\left( -∑_{i=1}^n (y^{(i)}) ⋅ log_2 a^{\\[3\\](i)} + (1-y^{(i)} ⋅ log_2(1-a^{\\[3\\](i)}) )\\right) $$其中 $y_i$ 是理想系统中输出y的概率；$log₂ a^{\\[3\\](i)}$是判断结果的信息量。\n偏差直接来自输出层的感知机:\n偏差来自三部分：当前层的 w 和 b，以及上一层的输出 $a^{[2]}$。其中 w 和 b 可以根据占比直接调整，而 $a^{[2]}$ 的偏差来自于上一层，按照贡献大小分配偏差：\n然后前一层的感知机占有的偏差又可分成3部分，调整 $w, b$ 和 上一层的输出.\n第一隐藏层的感知机的偏差与第二隐藏层所有感知机相关：\n由此，整个神经网络中的每个 $w$ 和 b 都能分配到偏差占比。\n以上利用的是数值的加法分配偏差，还可使用向量的加法来分配偏差，不过首先要确定向量的方向。\n梯度的反方向就是要找的向量方向：数值减小最快的方向。 梯度可以在i方向和j方向上分解，对于点(x,y)沿变化率最大的方向变化就是在i和j方向上同步变化。\n对(输出层)损失函数 J 求梯度：\n$$ \\begin{aligned} \u0026\\nabla J (\\mathbf w^{[3]}, a^{[2]}, b^{[3]}) \\\\\\ \u0026 = (\\alpha, \\beta, \\gamma) \u0026 \\text{三个分量的系数，简略表示} \\\\\\ \u0026 = \\alpha \\cdot \\mathbf i + \\beta \\cdot \\mathbf j + \\gamma \\cdot \\mathbf k \\end{aligned} $$输出层的输出沿梯度方向变化$\\eta$步长，向目标值靠近：\n$$ \\begin{aligned} \\mathbf w^{[3]}_{(\\rm target)} \u0026= \\mathbf w^{[3]} - \\eta \\cdot \\alpha \\\\ b^{[3]}_{(\\rm target)} \u0026= b^{[3]} - \\eta \\cdot \\gamma \\\\ \\\\ a^{[2]}_{(\\rm target)} \u0026= a^{[2]} - \\eta \\cdot \\beta \u0026 需要反向传递分配到第2隐藏层上 \\\\ \\cancel{\\eta \\cdot}\\ \\beta \u0026= a^{[2]}_{(\\rm target)} - a^{[2]}_{(\\rm now)} \\end{aligned} $$感知机输出值a的中间传递不考虑 $\\eta$，偏差最终分配在输入层的w和b上，其中含有$\\eta$。从而可以看出隐藏层与输出层类似，也是目标与输出之间的差值。因此对于第2隐藏层的\u0026quot;损失函数\u0026quot;： $J^{[2]} = a^{[2]}_{(\\rm target)} - a^{[2]}_{(\\rm now)}$，$J^{[2]} (\\mathbf w^{[3]}, a^{[2]}, b^{[3]})$ 开启下一轮：求梯度,求损失函数\n各层的运算形式相同（损失函数），就可以迭代分配偏差。J沿梯度方向变化就可以最快的减小偏差，因此使用了向量的加法做偏差分配。\n梯度 求偏导就是求（固定另一个维度）曲线的切线，两个偏导组合就是两条切线组合，两条切线确定了切面。所以对曲面求偏导，就是在求切面 把两个切线合成一个向量，就是梯度 $\\alpha, \\beta,\\gamma$ 的具体表示（求偏导）：\n$$ \\begin{aligned} \\alpha \u0026= \\frac{\\partial J}{\\partial \\mathbf w^{[3]}} \\\\ \\gamma \u0026= \\frac{\\partial J}{\\partial b^{[3]}} \\\\ \\beta \u0026= \\frac{\\partial J}{\\partial \\mathbf a^{[2]}} \\end{aligned} $$分配偏差：\n$$ \\begin{aligned} \\mathbf w^{[3]} \u0026= \\mathbf w^{[3]} - \\eta \\cdot \\frac{\\partial J}{\\partial \\mathbf w^{[3]}} \\\\ b^{[3]} \u0026= b^{[3]} - \\eta \\cdot \\frac{\\partial J}{\\partial b^{[3]}} \\\\ \\frac{\\partial J}{\\partial \\mathbf a^{[2]}} \u0026= a^{[2]}_{(\\rm target)} - a^{[2]}_{(\\rm now)} \u0026 \\text{“新损失函数”} \\end{aligned} $$令$J^{[2]}=a^{[2]}_{(\\rm target)} - a^{[2]}_{(\\rm now)}$ 作为下一轮的损失函数，对损失函数$J^{[2]}(\\mathbf w^{[2]}, \\mathbf a^{[1]}, b^{[2]}$)分配：\n$$ \\begin{aligned} \\mathbf w^{[2]} \u0026amp;= \\mathbf w^{[2]} - \\eta \\cdot \\frac{\\partial J^{[2]}}{\\partial \\mathbf w^{[2]}} \\ b^{[2]} \u0026amp;= b^{[2]} - \\eta \\cdot \\frac{\\partial J^{[2]}}{\\partial b^{[2]}} \\\n\\frac{1}{n} \\sum_{i=1}{n} \\frac{\\partial J^{[2]}}{\\partial \\mathbf a^{[1]}} \u0026amp;= a^{[1]}{(\\rm target)} - a^{[1]}{(\\rm now)} \u0026amp; \\text{“$a^{[1]}$的偏差是4个$a^{[2]}$的偏差之平均”} \\end{aligned} $$\n令 𝑱⁽¹⁾=a₍ₜₐᵣᵨₑₜ₎⁽¹⁾ - a₍ₙₒᵥ₎⁽¹⁾ 作为下一轮的损失函数，对损失函数$J^{[1]}(\\mathbf w^{[1]}, \\mathbf a^{[0]}, b^{[1]}$)分配，其中 $a^{[0]}$ 是输入，无法修改，只调整 w 和 b。\n对于第 $l$ 层的第 $i$ 个感知机，接受上一层所有神经元的输出 $a^{[l-1]}$，与它的 $\\mathbf w_i^{[l]}$ 和 $b_i^{[l]}$ 做线性运算得到 $z_{i}^{[l]}$，把 z 送入激活函数得到这个感知机的输出：$\\sigma(z_i^{[l]})=a_i^{[l]}$\n$$ \\begin{aligned} z_i^{[l]} \u0026= \\mathbf w_i^{[l]} \\cdot \\mathbf a^{[l-1]} + b_i^{[l]} \\\\ z_i^{[l]} \u0026= \\begin{bmatrix} \\mathbf w_{i,1}^{[l]} \\\\ \\vdots \\\\\\mathbf w_{i,n}^{[l]} \\end{bmatrix}^T \\cdot \\begin{bmatrix} a_{1}^{[l-1]} \\\\ \\vdots \\\\ a_{n}^{[l-1]} \\end{bmatrix} + b_i^{[l]} \\end{aligned} $$对一层感知机进行表述:\n没有下标 $i$ 表示整层，第$l$层各感知机的输出：\n$$ \\begin{aligned} \\mathbf z^{l} \u0026amp;= \\mathbf w^{l} \\cdot \\mathbf a^{l-1} + b^{[l]} \\ \\mathbf z^{l} \u0026amp;= \\begin{bmatrix} z_1^{[l]} \\ \\vdots \\z_i^{[l]} \\end{bmatrix} = \\begin{bmatrix} \\mathbf w_{1,1}^{[l]} \u0026amp; \\cdot \u0026amp; \\mathbf w_{1,j}^{[l]} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\cdots \\ \\mathbf w_{i,1}^{[l]} \u0026amp; \\cdot \u0026amp; \\mathbf w_{i,j}^{[l]} \\ \\end{bmatrix} \\cdot \\begin{bmatrix} a_1^{[l-1]} \\ \\vdots \\ a_j^{[l-1]} \\end{bmatrix} + \\begin{bmatrix} b_1^{[l]} \\ \\vdots \\ b_i^{[l]} \\end{bmatrix} \\\na^{[l]} \u0026amp;=\\sigma\\left(z^{[l]}\\right)=\\sigma\\left(\\left[\\begin{array}{c} z_{1}^{[l]} \\ \\vdots \\ z_{i}^{[l]} \\end{array}\\right]\\right)=\\left[\\begin{array}{c} \\sigma\\left(z_{1}^{[l]}\\right) \\ \\vdots \\ \\sigma\\left(z_{i}^{[l]}\\right) \\end{array}\\right]=\\left[\\begin{array}{c} a_{1}^{[l]} \\ \\vdots \\ a_{i}^{[l]} \\end{array}\\right]\n\\end{aligned} $$\n输出层的损失函数：$J(y,a^{[l]})$，y是label，(k)是第几个样本：\n每个样本$\\mathbf x$都有 j 个属性，对应第0层的输出 $a^{[0]}$：\n$$ a^{[l]}=\\sigma\\left(z^{[l]}\\right)=\\sigma\\left(\\left[\\begin{array}{c} z_{1}^{[l]} \\\\ \\vdots \\\\ z_{i}^{[l]} \\end{array}\\right]\\right)=\\left[\\begin{array}{c} \\sigma\\left(z_{1}^{[l]}\\right) \\\\ \\vdots \\\\ \\sigma\\left(z_{i}^{[l]}\\right) \\end{array}\\right]=\\left[\\begin{array}{c} a_{1}^{[l]} \\\\ \\vdots \\\\ a_{i}^{[l]} \\end{array}\\right] $$多分类问题，有多个输出 $aᵢ^{[l](k)}$，此时的损失函数是把所有输出节点都考虑进来。不考虑常量: 样本x和标签y，损失函数的输出层感知机的函数：\n反向传播：对J求梯度，给各变量分配偏差(偏导)，走$eta$步长\n把对输出层各感知机的偏导看作是第 $l+1$ 层，第 l+1 层只对 第l层的一个感知机有作用，$J_1^{[l+1]}$ 是 $\\mathbf w_1^{[l]},\\ a^{l-1},\\ b_1^{l}$ 的函数：\n最后$\\nabla J$的各项都从输出层 $a$ 开始求导（链式求导）：\n$$ \\nabla J_{1}^{[l+1]} = \\left( \\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial W_{1}^{[l]}}, \\quad \\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial a^{[l-1]}}, \\quad \\sqrt{\\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma}} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial b_{1}^{[l]}} \\right) $$其中 $\\mathbf w$ 是向量，对其求偏导要对它的每个分量求偏导：\n$$ \\begin{array}{l} \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1,1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1,2}^{[l]}}, \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1, j}^{[l]}}\\right) \\\\ \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{1}^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{2}^{[l-1]}}, \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{j}^{[l-1]}}\\right) \\end{array} $$每层的 $\\mathbf w$ 和 $b$ 求偏导之后可直接修改：\n$$ \\begin{array}{c} W_{1}^{[l]}=W_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}} \\quad b_{1}^{[l]}=b_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}} \\\\ \\vdots \\\\ W_{i}^{[l]}=W_{i}^{[l]}-\\eta \\cdot \\frac{\\partial J_{i}^{[l+1]}}{\\partial W_{i}^{[l]}} \\quad b_{i}^{[l]}=b_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{i}^{[l+1]}}{\\partial b_{i}^{[l]}} \\end{array} $$对 $a^{l-1}$ 求偏导得到的是第 $a^{[l-1]}$ 层的变化量，作为损失函数：\n$$ J_{1}^{[l]}=\\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}} \\quad \\cdots \\quad J_{i}^{[l]}=\\frac{\\partial Jᵢ^{[l+1]}}{\\partial a^{[l-1]}} $$3个窗口反向移动，做下一轮:\n以第2隐藏层为中心，进行分析：\n把 a 展开（对a的各分量求偏导）：\n$$ \\begin{array}{c} \\nabla J_{1}^{[l+1]}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\left.\\partial J_{1}^{[l+1]}\\right]}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\\\ \\vdots \\\\ \\nabla J_{k}^{[l+1]}=\\left(\\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\end{array} $$第l层的每个感知机的偏差由第l+1 层的所有偏差值赋予：\n然后第l层各参数的变化量：\n$$ \\left(\\Delta W_{1}^{[l]}, \\Delta a^{[l-1]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, \\Delta a^{[l-1]}, \\Delta b_{i}^{[l]}\\right) $$$\\delta a$ 作为下一轮的损失函数：\n$$ \\left(\\Delta W_{1}^{[l]}, J_{1}^{[l]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, J_{i}^{[l]} \\quad, \\Delta b_{i}^{[l]}\\right) $$第2次迭代：\n$a^{[0]}$ 是常量输入，求导为零，此时的$J^{l-1}$只与 $\\mathbf w$ 和 $b$ 有关:\n$$ \\begin{array}{c} \\nabla J_{1}^{[l+1]}= \\left( \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}},\\ \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\vdots \\\n\\nabla J_{k}^{[l+1]}= \\left( \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\ \\left( \\Delta W_{1}^{[l]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, \\Delta b_{i}^{[l]}\\right) \\end{array} $$\n","date":"2022-01-04T23:29:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/06_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/","title":"watch: DL - 王木头 06 | Gradient Descent"},{"content":"3 线性判别分析 (Fisher判别分析) - 模型定义 Source Video-P3\nData:\n$$ \\begin{aligned} \\text{Samples:}\\ \u0026amp; \\mathbf X = (\\mathbf x_1, \\mathbf x_2, \\cdots, \\mathbf x_N)^T_{N\\times p} = \\begin{pmatrix} \\mathbf x_1^T \\ \\mathbf x_2^T \\ \\vdots \\ \\mathbf x_N^T \\end{pmatrix}_{N \\times p} \\\\\n\\text{Samples:} \\ \u0026amp; \\mathbf Y = \\begin{pmatrix} y_1 \\ y_2 \\ \\vdots \\ y_N \\end{pmatrix}_{N \\times 1} \\\\\n\\text{Abbreviations:}\\ \u0026amp; \\left{ (\\mathbf x_i, y_i) \\right}_{i=1}^N, \\ \\mathbf x_i \\in \\R^p, \\ \\underset{(\\text{2class: } C_1,C_2)}{y_i \\in {+1, -1}}\\\\\n\\text{2 Group:}\\ \u0026amp;\\mathbf x_{C_1} = { \\mathbf x_i | y_i=+1} ; \\quad \\mathbf x_{C_2} = { \\mathbf x_i | y_i=-1} \\\\\n\\text{Number:}\\ \u0026amp; |\\mathbf x_{C_1}| = N_1, \\ |\\mathbf x_{C_2}|=N_2, \\ N_1 + N_2 = N \\end{aligned} $$\nFisher 类内小，类间大\n把p维降到1维，投影到某一个轴上再做分类。\n在投影方向($\\mathbf w$)上，类内点的坐标方差足够小，类间距离要大\n投影方向就是分类超平面（$\\mathbf w^T \\mathbf x$）的法向量。\n记号\n限定 $\\| \\mathbf w \\| =1$\n样本点$\\mathbf x_i$ 在投影轴$\\mathbf w$上的投影长度: $z_i = \\mathbf w^T \\mathbf x_i$\n$$ \\begin{aligned}\n均值: \\overline{z} =\u0026amp; \\frac{1}{N} \\sum_{i=1}^N z_i = \\frac{1}{N} \\sum_{i=1}^N \\mathbf w^T \\mathbf x_i \\\n协方差矩阵: S_{z} =\u0026amp; \\frac{1}{N} \\sum_{i=1}^N (z-\\overline z)(z-\\overline z)^T \\\n=\u0026amp; \\frac{1}{N} \\sum_{i=1}^N (\\mathbf w^T \\mathbf x_i-\\overline z)(\\mathbf w^T \\mathbf x_i-\\overline z)^T \\\n类C_1:\\ \\overline{z_1} =\u0026amp; \\frac{1}{N_1} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\\nS_{z_1} =\u0026amp;\\frac{1}{N_1} \\sum_{i=1}^{N_1} (\\mathbf w^T \\mathbf x_i-\\overline{z_1})(\\mathbf w^T \\mathbf x_i-\\overline{z_1})^T \\\\ 类C_2:\\ \\overline{z_2} =\u0026amp; \\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\\nS_{z_2} =\u0026amp; \\frac{1}{N_2} \\sum_{i=1}^{N_2} (\\mathbf w^T \\mathbf x_i-\\overline{z_2})(\\mathbf w^T \\mathbf x_i-\\overline{z_2})^T \\\\ 类内距离: \u0026amp; S_{z_1} + S_{z_2} \\ 类间距离: \u0026amp; (\\overline{z_1} - \\overline{z_2})^2 \\\n目标函数: \u0026amp; J(\\mathbf w) = \\frac{(\\overline{z_1} - \\overline{z_2})^2}{S_{z_1} + S_{z_2}} \\\n分子：\u0026amp; \\left[ \\frac{1}{N_1} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i-\\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\right]^2 \\\n=\u0026amp; \\left[ \\mathbf w^T \\left( \\frac{1}{N_1} \\sum_{i=1}^{N_1}\\mathbf x_i-\\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf x_i \\right) \\right]^2\\\\ =\u0026amp; \\left[ \\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) \\right]^2\\\\ =\u0026amp; \\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}})^T \\mathbf w^T \\\\ S_{z_1}=\u0026amp; \\frac{1}{N_1} \\sum_{i=1}^{N_1} (\\mathbf w^T \\mathbf x_i - \\frac{1}{N_1} \\sum_{j=1}^{N_1} \\mathbf w^T \\mathbf x_j)(\\mathbf w^T \\mathbf x_i - \\frac{1}{N_1} \\sum_{j=1}^{N_1} \\mathbf w^T \\mathbf x_j)^T \\\n=\u0026amp; \\frac{1}{N_1}\\sum_{i=1}^{N_1} \\mathbf w^T (\\mathbf x_i - \\overline{\\mathbf x_{C_1}}) (\\mathbf x_i - \\overline{\\mathbf x_{C_1}})^T \\mathbf w\\\\ =\u0026amp; \\mathbf w^T \\left[ \\frac{1}{N}\\sum_{i=1}^{N_1} (\\mathbf x_i - \\overline{\\mathbf x_{C_1}}) (\\mathbf x_i - \\overline{\\mathbf x_{C_1}})^T \\right] \\mathbf w \\\\ =\u0026amp; \\mathbf w^T S_{C_1} \\mathbf w \\\\ 分母：\u0026amp; \\mathbf w^T S_{C_1} \\mathbf w + \\mathbf w^T S_{C_2} \\mathbf w\\ =\u0026amp; \\mathbf w^T (S_{C_1} + S_{C_2}) \\mathbf w \\\nJ(\\mathbf w) =\u0026amp; \\frac{\\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}})^T \\mathbf w^T} {\\mathbf w^T (S_{C_1} + S_{C_2}) \\mathbf w} \\ =\u0026amp; \\frac{\\mathbf w^T S_b \\mathbf w}{\\mathbf w^T S_{w} \\mathbf w} \\quad \\text{($S_b$: 类间方差; $S_w$: 类内方差)} \\ =\u0026amp; \\mathbf w^T S_b \\mathbf w \\cdot (\\mathbf w^T S_{w} \\mathbf w)^{-1}\n\\end{aligned} $$\n求: $\\hat{\\mathbf w} = \\underset{\\mathbf w}{\\operatorname{arg\\ max}}\\ J(\\mathbf w)$\n4 线性判别分析 (Fisher判别分析) - 模型求解 Video-P4\n$$ \\begin{aligned} \\frac{\\partial J(\\mathbf w)}{\\partial \\mathbf w} \u0026amp;= 0 \\\n2 S_b \\mathbf w (\\mathbf w^T S_w \\mathbf w)^{-1} - \\mathbf w^T S_b \\mathbf w (\\mathbf w^T S_w \\mathbf w)^{-2} \\cdot 2S_w \\mathbf w \u0026amp;= 0 \\\nS_b \\mathbf w (\\mathbf w^T S_w \\mathbf w) - \\mathbf w^T S_b \\mathbf w S_w \\mathbf w \u0026amp;= 0 \\\n\\underbrace{\\mathbf w^T}{1\\times p} \\underset{\\in \\R} {\\underbrace{S_b}{p\\times p} \\underbrace{\\mathbf w}{p\\times 1}} S_w \\mathbf w \u0026amp;= S_b \\mathbf w \\underset{\\in \\R} {(\\underbrace{\\mathbf w^T}{1\\times p} \\underbrace{S_w}{p\\times p} \\underbrace{\\mathbf w}{p\\times 1})} \\\nS_w \\mathbf w \u0026amp;= \\frac{\\mathbf w^T S_w \\mathbf w}{\\mathbf w^T S_b \\mathbf w} S_b \\mathbf w \\\n\\mathbf w \u0026amp;= \\frac{\\mathbf w^T S_w \\mathbf w}{\\mathbf w^T S_b \\mathbf w} S_w^{-1} \\cdot S_b \\cdot \\mathbf w \\\n只关心方向：\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot S_b \\cdot \\mathbf w \\\n\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}}) \\underbrace{ (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})^T \\mathbf w }_{1\\times 1 \\in \\R} \\\n\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})\n\\end{aligned} $$\n如果$S_w^-1$ 是对角矩阵，而且是各向同性，则 $S_w^{-1} \\propto 单位矩阵I$，所以 $\\mathbf w \\propto (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})$\n5 逻辑回归 (Logistic Regression) Video-P5\n硬输出：0 和 1；或者 +-1\n软输出：概率\n9 朴素贝叶斯分类器 (Naive Bayes Classifer) Video-P9\n核心思想：朴素贝叶斯假设，又叫条件独立性假设，最简单的概率有向图模型\n在给定类别的情况下，属性（维度）之间是相互独立的\n随机变量 $y$ 是随机变量，对应 p 维自变量\n从概率图角度来看，在给定 y 的情况下，从 $x_1$ 到 $x_2$ 的路径被 y 阻断了，所以 $x_1$ 和 $x_2$ 独立。\n概率表达式：\n$$ P(\\mathbf x | y) = \\prod_{j=1}^p P(x_i | y) $$假设的动机：为了简化运算， 对于 $\\mathbf x = (x_1, x_2, \\cdots x_p)^T$，忽略了 $x_i$ 与 $x_j$ 之间的关系，如果p非常大，导致计算困难。\n$$ P(y|x) = \\frac{P(x,y)}{P(x)} = \\frac{P(y)\\cdot P(x|y)}{P(x)} \\propto P(y) \\cdot P(x | y) $$分类数据：$\\{(x_i, y_i)\\}_{i=1}^N, \\ x_i \\in \\R^p, \\ y_i \\in {0,1}$，对于一个给定的 x，对它分类：\n$$ \\begin{aligned} \\hat y =\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ \\underset{后验}{\\underline{P{(y|x)}}} \\\n=\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ \\frac{P(x,y)}{P(x)}\\\\ =\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ P(y) \\cdot P(x|y) \\end{aligned} $$\n","date":"2021-12-24T13:58:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/04_%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/","title":"watch: ML - 白板 04 | Linear Classification"},{"content":"Convolution 对像素重新计数，并计算新的“像素值”的过程\n卷积核从左上角开始，每次向左或向下滑动，并与其重叠的部分做内积（对应项相乘再求和）\n提取特征\n不做填充（valid padding），卷积后的输出尺寸为 $\\lfloor\\frac{n-k}{s}\\rfloor+1$\n图像尺寸：n×n\n卷积核尺寸：k×k\n步长：s\n卷积核从左上角开始，每次向左滑动一列，最后停靠在右边缘，这时卷积核左侧的像素数再加上1（当前次），就是输出的尺寸 n-k+1。\n比如下图一行有5个像素，k=2，卷积核前面有3个再加上最后1个: 3+1 =4。\n如果步长s=2，不能正好滑到最后，可以丢掉多余的部分或者填充像素。2\n如果步长s=3，计算式应为：$\\frac{n-k}{s}+1 = \\frac{5-2}{3}+1 =2$\n如果步长s=4，计算式应为：$\\lfloor\\frac{n-k}{s}\\rfloor+1 = \\lfloor\\frac{5-2}{4}\\rfloor+1 =1$\n对于 same padding, 输出尺寸：$\\lfloor \\frac{(n+2\\times p-k)}{s} \\rfloor+1$\n就是先对原始图像补充 p 圈像素，再做卷积。\nPadding 在图像外围填充一圈或几圈像素，像素值通常为0 保证输出与输入的尺寸一致。1 常见两种padding： valid padding: 不填充，只使用原始图像 same padding: 填充边缘，使卷积结果与输入尺寸一致。\n为了使输出尺寸仍等于n，即：$\\frac{n-k+2*p}{s}+1 = n$，解得：$p=\\frac{(n-1)*s+k-n}{2}$；如果s=1，则 $p=\\frac{k-1}{2}$。 Stride 卷积核滑动的步长 s stride=1，则卷积核每次向左滑动一列或者向下滑动一行 压缩信息：成比例缩小输出的尺寸，stride=2，则输出为输入的1/2。1 Pooling 保留特征，并减少计算量 max-pooling: 近视眼，只能看到最大的;\naverage-pooling\n(2023-12-12)\nF.avg_pool3d Number of channels doesn\u0026rsquo;t change, and D, H, W shrink. Docs\n1 2 inp = torch.ones(1, 3, 7, 9, 13) F.avg_pool3d(inp, (4, 2, 3), stride=4, padding=1) # (1,3,2,3,4) C h D n = l 4 1 ⋮ ⋱ ⋱ ⋱ ⋮ ⋮ ⋮ ' C h D n = l 4 2 ⋮ ⋱ ⋱ ⋱ ⋮ ⋮ ⋮ ' C h D n = l 4 3 ⋮ ⋱ ⋱ ⋱ ⋮ ⋮ ⋮ MVSNet uses AvgPool3d to compute the sum of every 4 depth-probability planes:\n1 2 3 4 5 avg4 = F.avg_pool3d( F.pad(prob_volume.unsqueeze(1),# (bs,C=1,D=192,H=128,W=160) pad=(0, 0, 0, 0, 1, 2)), # (bs,C=1,D=195,H=128,W=160) (4, 1, 1), stride=1, padding=0)# (bs,C=1,D=192,H=128,W=160) prob_volume_sum4 = 4 * avg4 Deconvolution Complexity of CNN 3\nConvTranspose2d() 1 torch.nn.ConvTranspose2d() Deconvolution visualization\n(2023-07-19)\ntorchvision.models.resnet34 ResNet - PyTorch | Source code\nlayers: [3,4,6,3] means that layer1 has 3 BasicBlock (resnet50 is Bottleneck) convolution blocks, and layer2 has 4 blocks, and layer3 has 6 blocks, and layer4 has 3 blocks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def _forward_impl(self, img) # img.shape: (3, 300, 400) x = self.conv1(img) # nn.Conv2d(), channels↑ and size↓, (1, 64, 150, 200) x = self.bn1(x) # batch norm, shape doesn\u0026#39;t change, (1, 64, 150, 200) x = self.relu(x) # act, shape keeps the same, (1, 64, 150, 200) x = self.maxpool(x) # max-pooling, size↓, (1, 64, 75, 100) # 3 identical \u0026#39;BasicBlock\u0026#39; x = self.layer1(x) # stride in the 1st block is 1, so shape doesn\u0026#39;t change, (1, 64, 75, 100) # 4 identical \u0026#39;BasicBlock\u0026#39; x = self.layer2(x) # stride in the 1st block is 2, so size↓, (1, 128, 38, 50) # 6 identical \u0026#39;BasicBlock\u0026#39; x = self.layer3(x) # stride in the 1st block is 2, so size↓, (1, 256, 19, 25) # 3 identical \u0026#39;BasicBlock x = self.layer4(x) # stride in the 1st block is 2, so size↓, (1, 512, 10, 13) x = self.avgpool(x) # target output is (1,1), so (1, 512, 1, 1) x = torch.flatten(x,1) x = self.fc(x) # 512 -\u0026gt; num_classes (2023-09-12)\nF.pad Padding an image along width, or height, or depth directions. Docs\nThe order of dimensions should be arranged according to Width, Height, Depth, e.g., padding the last 3 dimensions: F.pad(x, (padding_left, padding_right, padding_top, padding_bottom, padding_front, padding_back) )\nSo the order of (l,r,t,b,f,b) is reverse against an image tensor: (Depth, H, W)\nnn.Conv3d Input: (B, Ch_in, D, H, W); Output: (B, Ch_out, D_out, H_out, W_out)\nFor example, a tensor with shape of (2, 3, 4, 224, 224) is 2 video clips with 3 frames and each frame is a 4-channel image with size 224x224.\nAfter convolution with a kernel of size (2, 4, 4), it can be transformed to (B=2, Ch_out=128, D=2, H=56, W=56)\nc o O a n u l v t l o ( l 2 c 4 v ( , h e 2 4 1 c , , = h 2 4 4 n , ) s l f 4 u s r ) o m a o f m f e c c h e h 1 a c 2 h s t e p f r a m e 3 Iterate each channel for D frames to convolve with a unique 3D kernel. Once every channel has multiplied by a kernel, all the 4 weighted channels are summed directly to form one of output channels.\nDepthwise Convolution (2023-07-25)\nSeparate a convolution into two steps:\nShrink the size of the feature maps using 1-channel plane-wise kernel (Depthwise Conv);\nExpand the number of channels using 1x1 kernel (Pointwise Conv).\nFLOPs reduced, but the IO access increased resulting in slower inference. Depth-wise Convolution - 沈景兵的文章 - 知乎\nExpanding channels process costs the equal amount of FLOPs in normal convolution and pointwise convolution. For example, when expanding 3 channels to 256 channels, each pixel performs multiplication 256 times.\nHowever, the depthwise convolution doesn\u0026rsquo;t multiply a kernel by each channel and sum them together, but only multiply a kernel by only one channel. A Basic Introduction to Separable Convolutions - Medium\nFewer parameters: 3x3x253 kernels are replaced with 1x1x256 kernels for every pixel on the resultant feature map.\n(2024-06-22)\nA video expalnation: Nik_Li - 15分钟看懂depthwise convolution\n第一阶段的 depth-wise convlution 是一直不变的，#out channels 是由第二阶段的 point-wise convlution 做多次决定。\ndepth-wise 对比 传统卷积类比于：一个 Encoder（第一阶段固定） 和 多个 Decoder（多个输出通道）； 而传统卷积层是：每个输出通道的“输入量”是不一样的（第一阶段不固定），每一次卷积就是一个 encoder-decoder 组合。\n如果只有一个输出通道，两种卷积是等价的，因为表达式可以写成一样的。 如果是有多个输出通道，那么就需要求 Encoder 需要有很好的能力，支持被不同的 “decoder” 解释成不同的输出，以对应不同的信息\nReference CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride) - G-kdom的文章 - 知乎 (accessed Dec. 22, 2021). 卷积神经网络参数计算及卷积层输出尺寸计算 - 凡夫俗子的文章 - 知乎 卷积神经网络的复杂度分析 - Michael Yuan的文章 - 知乎 ","date":"2021-12-23T17:29:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/dl-conv_layers/","title":"memo: DL | Convolution Layers"},{"content":"从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变\n卷积 $\\int_{-\\infin}^{+\\infin} f(\\tau) g(x-\\tau) d\\tau$\n一个人一边进食一边消化，进食函数 f 显示了各时刻的进食量：\n消化函数 g 表示肚子里剩余食物的比例随时间的变化，它与吃多少无关：\n比如要求一个人在下午2点，肚子里还剩多少食物，就是之前吃的每一顿饭经过了“独立”的消化后剩余的部分再求和：\n一般情况，求t时刻的胃中还剩多少食物：x时刻吃的食物f(x)，经过了(t-x)时间，还剩下的比例是$g(t-x)$，乘以总量就是x时刻吃的食物到t时刻还剩多少$f(x)\\cdot g(t-x)$，对t之前每一时刻吃的东西到了t时刻还剩多少，加起来：$\\int_0^t f(x) g(t-x) dx$\n其中两函数的自变量相加后，x就被消掉了，这是卷积的一个标志。\nx 与 (t-x) 在图像上的对应：\n一个系统，输入(f)不稳定，输出(g)稳定，就可以用卷积来求这个系统的存量\n另一种理解：之前发生的事件对当前事件的影响，每件事的影响力随时间变化是g，所以导致 t 时刻事件的发生是之前各时刻事件的影响在t时刻的和。\n如果影响是随距离变化的函数，那么在某位置的事件就是之前各位置在此位置产生的影响之和。\n始皇既没，余威震于殊俗。 \u0026ndash; 贾谊《过秦论》\n图像的卷积操作 周围像素点对当前像素点的影响\n3x3卷积核规定周围一圈像素对当前像素点的影响，5x5就是用了周围2圈。平滑卷积操作就是把当前像素值替换为与周围像素的平均值。\n卷积核与图片的数学运算：$f(x,y)\\star g(m,n) = \\sum f(x,y) \\cdot g(m-x, n-y)$。（x与m-x相加只剩m，y与n-y相加只剩n，两个维度上都是卷积）\n如果只考虑点(x,y) 周围相邻1个像素对它的影响，并且每个相邻像素对当前像素的影响由卷积核g规定。比如要求像素f(x-1,y-1)对f(x,y)的影响，就是它本身乘以对应的比例g。 类比吃饭的例子，f(x,y)是t时刻，f(x-1,y-1)是x时刻，x时刻吃的食物到了t时刻还剩下百分之$g(t-x)$，所以 $f(x-1,y-1)$ 对应的比例为 $g(x-(x-1), y-(y-1)) = g(1,1)$，图片像素位置f与卷积核中g的位置并不一一对应，而是要旋转180度。\ng函数不等于卷积核：\n图像的卷积操作省略了g函数的旋转，直接与卷积核相乘再相加\n卷积层 卷积核是局部特征的模板 不考虑某位置就将其卷积核对应位置设置成零，而要重点考虑某位置，就设置得高一些 保留局部特征，得到feature map 垂直边界滤波器和水平边界滤波器 卷积神经网络可视化-github\n","date":"2021-12-23T07:46:00-05:00","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/01_%E5%8D%B7%E7%A7%AF/","title":"watch: DL - 王木头 01 | Convolution"},{"content":"什么是“感知机”，它的缺陷为什么让“神经网络”陷入低潮\n感知机 分类工具\n只有两个输出\n为线性二分类问题(定义域元素无穷多，值域只有两个值)提供了模板答案\n一个线性函数再加一个激活函数\n线性函数是对标准模型的描述，激活函数是判断输入数据是否符合标准模型\n感知机缺陷 异或运算无法实现 异或没办法线性可分：无法只用一条线把0和1区分开，必须要画一个圈才能将0和1区分开 增加感知机数量：通过中间层，合并两个相同的状态 核方法升维 3-“神经网络”是什么？如何直观理解它的能力极限？它是如何无限逼近真理的？\n神经网络 多层感知机\n输入层，隐藏层，输出层\n全连接网络：每个节点都和下一层节点全部相连 前馈神经网络：数据的传递方向是单向向前传播 普遍逼近定理：只要神经网络有隐藏层，它就可以任意逼近一个连续函数\n不像感知机那样，用两侧的数据（是否）把分界线夹逼出来\n对于多个类别，因为无法明确的定义每一个类别(猫的定义是什么？)，所以不同类别之间的界限不是黑白分明，没有明确的是非，只能说最有可能是哪一类。激活函数采用sigmoid函数，而不是感知机的0-1阶跃函数，把是非问题转化为好坏问题。\n人和神经网络都用自己的认知(标准)去判断是非，神经网络把自己的结果与人的结果进行比对，调整自己的模型，使两个模型之间的差异(损失函数)最小\n","date":"2021-12-22T14:39:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/02-3/","title":"watch: DL - 王木头 02 | Perceptron"},{"content":"1. 最小二乘法及其几何意义 Source video: P1\n2. 最小二乘法-概率视角-高斯噪声-MLE 3. 正则化-岭回归 4. 正则化-岭回归-概率角度-高斯噪声高斯先验-MAP ","date":"2021-12-21T23:02:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/03-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"watch: ML - 白板 03 | Linear Regression"},{"content":"1-背景\n抑制过拟合 增加样本数据 正则化：增加约束限制参数空间 降维 维度灾难 References:\nDeepSeek | 纠正高维特征向量距离的说法 Notes:\n数学角度： 比如每增加一个二值属性，要想完全cover样本空间，所需样本数会以2的指数增长\n几何意义： 在高维空间中，立方体的内切球的体积趋近于零，也就是说把立方体的四个角削掉，只剩下内切球，基本就一点不剩了知乎:机器学习中的维度灾难，四个角所占比例不高，却拥有几乎全部的体积。 所以如果在高维空间中取一超立方体，其中存在样本的概率很低，因为样本只存在于四个角中，这就是数据的稀疏性，并且分布不均匀。很难做分类。\n维度 超立方体体积 超内切球体积 2 1 π (0.5)² 3 1 4/3 π (0.5)³ D 1 K(0.5)ᴰ; 当 D→∞, V(超球体)→0 几何意义2: 两个同心圆的半径相差 $\\varepsilon \\ (0\u003c\\varepsilon\u003c1)$，内圆的半径为 $1-\\varepsilon$，外超球体的体积为：$V_外=K \\cdot 1^D = K$；环形带的体积：$V_{环形带} = V_外-V_内 = K - K(1-\\varepsilon)^D$。\n两体积之比：$\\frac{V_环}{V_外} = \\frac{K- K(1-\\varepsilon)^D}{K} = 1-(1-\\varepsilon)^D$。 不论$\\varepsilon$取多小，当维度趋于无穷，$\\underset{D\\rightarrow \\infin}{lim} (1-\\varepsilon)^D = 0$，也就是比值为1，环形带(壳)体积等于外球的体积 球内的样本只存在与球壳上\n维度灾难会导致过拟合\n需要降维\n(2025-04-13T21:19:49)\n评价一个人的维度很多，多到让每个人只是空间中的一个点，一个独一无二的点，每个人都是不一样的。\nSupports:\n从一个维度比较两个人，比的是长度，从两个维度比较，比的是面积，从三个维度比较，比的是体积，这都是指数级的增长哦， 因为两项技能在一个人身上，确实像是乘法一般将人的能力放大。 维度？唯独！ 特征向量维度那么高，所以两个特征向量之间的距离很大吧？ 维度数量与距离之间无因果关系r1-DS 降维 避免过拟合，减小泛化误差 直接降维/特征选择: 只保留重要的维度; LASSO带来系数的系数性，使某些属性对应的系数等于0。 线性降维: PCA, MDS 非线性降维: 流形（ISOmap, LLE） 2-样本均值\u0026amp;样本方差矩阵\nData:\n$$ \\mathbf X_{p\\times 1} = (\\mathbf x_1, \\mathbf x_2, \\cdots, \\mathbf x_N)^T_{N\\times p} = \\begin{pmatrix} \\mathbf x_1^T \\ \\mathbf x_2^T \\ \\vdots \\ \\mathbf x_N^T \\end{pmatrix}_{N \\times p},\\quad\n\\mathbf x_i \\in \\R^p,\\ i=1, 2, \\cdots, N $$\nSample Mean:\n$$ \\begin{aligned} \\bar{\\mathbf X} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\mathbf x_i \\ \u0026amp; = \\frac{1}{N} (\\mathbf x_1, \\mathbf x_2,\\cdots, \\mathbf x_N) \\begin{pmatrix} 1 \\ 1 \\ \\vdots \\ 1 \\end{pmatrix}_{N\\times 1} \\ \u0026amp; = \\frac{1}{N} \\ \\mathbf X^T \\ \\mathbf 1_N\n\\end{aligned} $$\nSample Covariance:\n$$ S = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X})^2 \\quad (一维样本) $$$$ \\begin{aligned} S_{p\\times p} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X}) (\\mathbf x_i - \\bar{\\mathbf X})^T \\quad (p维样本) \\\n\u0026amp; = \\frac{1}{N} (\\mathbf x_1 - \\bar{\\mathbf X} \\quad \\mathbf x_2 - \\bar{\\mathbf X}\\ \\cdots \\ \\mathbf x_N - \\bar{\\mathbf X}) \\begin{pmatrix} (\\mathbf x_1 - \\bar{\\mathbf X})^T \\ (\\mathbf x_2 - \\bar{\\mathbf X})^T \\ \\vdots \\ (\\mathbf x_N - \\bar{\\mathbf X})^T \\end{pmatrix} \\\n\u0026amp; = \\frac{1}{N} \\left[(\\mathbf x_1 \\ \\mathbf x_2 \\cdots \\mathbf x_N) - \\mathbf{\\bar{X}} \\ (1 \\ 1 \\cdots 1)\\right] (\\mathbf x_i - \\bar{\\mathbf X})^T \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T_{p\\times N} - \\frac{1}{N} \\mathbf{X}^T \\mathbf 1_N \\ \\mathbf 1_N^T \\ ]\\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) ]\\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\quad \\text{($I_N$是NxN方阵)} \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T \\underline{(I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T)} ] \\cdot [ \\underline{(I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T)^T} \\mathbf X] \\\n\u0026amp; = \\frac{1}{N} \\mathbf X^T H \\cdot H^T \\mathbf X \\ \u0026amp; = \\frac{1}{N} \\mathbf X^T H \\mathbf X\n\\end{aligned} $$\nH 是中心矩阵，把数据的均值移动到原点(中心化).\n$$ \\begin{aligned} H \u0026amp;= (\\mathbf I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) \\ H^T \u0026amp;= (\\mathbf I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) =H \u0026amp; (对称性)\\ H^2 \u0026amp;= H \\cdot H \u0026amp; (幂等性)\\ \u0026amp;= (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) \\ \u0026amp;= I_N - \\frac{2}{N} \\mathbf 1_N \\mathbf 1_N^T + \\frac{1}{N^2} 1_N \\mathbf 1_N^T 1_N \\mathbf 1_N^T \\\n\u0026amp;= I_N - \\frac{2}{N} \\begin{pmatrix} 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{pmatrix} + \\frac{1}{N^2} \\begin{pmatrix} N \u0026amp; N \u0026amp; \\cdots \u0026amp; N \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ N \u0026amp; N \u0026amp; \\cdots \u0026amp; N \\\\ \\end{pmatrix} \\\\ \u0026amp;= I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T \\\\ \u0026amp;= H \\\\ H^n \u0026amp;= H \\end{aligned} $$\n3 PCA-最大投影方差\n经典PCA 一个中心，两个基本点\n核心：将一组可能线性相关的变量，通过正交变换，变换成一组线性无关的变量/基/投影方向（对原始特征空间的重构）\n基本点：最大投影方差；最小重构距离（两种角度,效果相同）\n最大投影方差 最能体现原来样本的分布 Steps: 中心化：把样本均值移动到原点 ($\\mathbf x_i - \\bar{\\mathbf X}$)\n样本点在 $\\mathbf u_1$ 方向上的投影，也是在$\\mathbf u_1$方向上的坐标：\n$$ (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 $$其中 $\\| \\mathbf u_1\\| = 1$ (或$\\mathbf u_1^T \\mathbf u_1 = 1$)，所以内积等于投影。 两个向量的内积写成一个向量的转置乘以另一个向量，$\\mathbf a_{p\\times 1} \\cdot \\mathbf b_{p \\times 1} = \\mathbf a^T_{1\\times p} \\ \\mathbf b_{p \\times 1} = n_{1\\times 1}$\n投影方差：因为均值已经为0，投影直接平方\n$$ \\begin{aligned} J \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\left( (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 \\right)^2 \\\n\u0026amp;= \\sum_{i=1}^N \\ \\frac{1}{N} \\ \\mathbf u_1^T (\\mathbf x_i - \\bar{\\mathbf X}) \\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 \\\n\u0026amp;= \\mathbf u_1^T \\left(\\frac{1}{N} \\ \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X}) \\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\right) \\mathbf u_1 \\\n\u0026amp;= \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 \\end{aligned} $$\n找到使 J 最大的方向 $\\mathbf u_1$\n$$ \\begin{cases} \\hat \\mathbf u_1 = \\operatorname{arg\\ max}\\ \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 \\\\ s.t. \\quad \\mathbf u_1^T \\mathbf u_1 = 1 \\end{cases} $$带约束的优化问题，用拉格朗日乘子法，写出拉格朗日函数：\n$$ L(\\mathbf u_1, \\lambda) = \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 + \\lambda (1 - \\mathbf u_1^T \\mathbf u_1) $$求导：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf u_1} = 2 S \\cdot \\mathbf u_1 \u0026- \\lambda \\cdot 2 \\mathbf u_1 = 0 \\\\ S \\underbrace{\\mathbf u_1}_{\\text{Eigen vector}} \u0026= \\underbrace{\\lambda}_{\\text{Eigen value}} \\mathbf u_1 \\end{aligned} $$ 4-PCA-最小重构代价\n最小代价重构 从重构空间恢复到原始空间，代价最小 Steps: 向量$\\mathbf x_i$在新的特征空间中的表示：\n$$ \\begin{aligned} \\mathbf x_i \u0026= ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1)\\cdot \\mathbf u_1 + ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_2)\\cdot \\mathbf u_2 + \\cdots + ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_p)\\cdot \\mathbf u_p \\\\ \u0026= \\sum_{k=1}^p ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k \\end{aligned} $$ 降维：根据特征值，取前q个最大的特征向量(方向)。\n$$ \\hat{\\mathbf x}_i = \\sum_{k=1}^q ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k $$ 重构代价: $\\| \\mathbf x_i - \\hat{\\mathbf x}_i \\|^2$\nN个样本的重构代价最小：\n$$ \\begin{aligned} J \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N | \\mathbf x_i - \\hat{\\mathbf x}i |^2 \\ \u0026amp;= \\frac{1}{N} \\sum{i=1}^N | \\sum_{k=q+1}^p ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k |^2 \\\n\u0026amp;= \\frac{1}{N} \\sum_{i=1}^n \\sum_{k=q+1}^p \\left( (\\mathbf x_i - \\bar{\\mathbf x})^t \\mathbf u_k \\right)^2 \\quad \\text{(向量的模等于坐标的平方和)} \\\n\u0026amp;= \\sum_{k=q+1}^p \\underline{ \\sum_{i=1}^n \\frac{1}{N} \\left( (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k \\right)^2 } \\\n\u0026amp;= \\sum_{k=q+1}^p \\mathbf u_k^T \\cdot S \\cdot \\mathbf u_k \\qquad\\ \\rm s.t.\\ \\mathbf u_k^T \\mathbf u_k = 1 \\\n\\end{aligned} $$\n最优化问题：\n$$ \\begin{cases} \\mathbf u_k = \\operatorname{arg\\ min} \\sum_{k=q+1}^p \\mathbf u_k^T \\cdot S \\cdot \\mathbf u_k \\\\ s.t. \\quad \\mathbf u_k^T \\mathbf u_k = 1 \\end{cases} $$因为各特征向量互不相关，所以可以一个一个解，也就是求剩余的每个特征向量的最小重构代价对应的特征值$\\lambda$\n$$ \\begin{cases} \\operatorname{arg\\ min} \\mathbf u_{q+1} \\cdot S \\cdot \\mathbf u_{q+1}\\\\ s.t. \\quad \\mathbf u_{q+1}^T \\ \\mathbf u_{q+1} = 1 \\end{cases} $$$$ J = \\sum_{i=q+1}^p \\lambda_i $$当J最小时，对应的就是最小的几个特征值\n5-SVD角度看PCA和PCoA\nPCA 找最大的投影方向(特征向量)，就是主成分\n求解主成分：对方差矩阵做特征值分解：$S = G K G^T$（因为S是对称矩阵，所以它的奇异值分解就是特征值分解）， 其中特征向量是正交的: $G^T G = I$；K是对角矩阵，元素都是特征值，其排列满足： $k_1 \u003e k_2 \u003e \\cdots \u003e k_p$。降到q维，就取前 q 个值，作为G的q个列向量。\n$$ K= \\begin{pmatrix} k_1 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 k_2 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 \\ddots \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 k_p \\end{pmatrix} $$探索一下：\n对中心化之后的原始数据做SVD奇异值分解：\n$$ H X = U \\Sigma V^T \\rightarrow SVD: \\begin{cases} U^T U = I \u0026 \\text{(列正交)} \\\\ V^T V = V V^T = I \u0026 \\text{(正交)} \\\\ \\Sigma \u0026 \\text{(对角)} \\end{cases} $$然后代入协方差矩阵（推导省略常数$\\frac{1}{N}$）：\n$$ \\begin{aligned} S_{p\\times p} \u0026= X^T H X \\\\ \u0026= X^T H^T H X \u0026 \\text{(等幂性)} \\\\ \u0026= V \\Sigma \\underline{U^T \\cdot U} \\Sigma V^T \\\\ \u0026= V \\Sigma I \\Sigma V^T \u0026 \\text{(U列正交)}\\\\ \u0026= V \\Sigma^2 V^T \u0026 \\text{($\\Sigma$对角阵)} \\end{aligned} $$就相当于对 S 做了奇异值分解了，对应于上面 S 的特征值分解：\n$$ 特征向量G = V, 特征值K = \\Sigma^2 $$所以，不用直接对 S做特征值分解，直接对数据做完中心化之后，做奇异值分解，就可以得到特征向量V。\n定义一个矩阵 T（S反过来，对数据内积分解）:\n$$ \\begin{aligned} T_{N\\times N} \u0026= H X X^T H^T \\\\ \u0026= U \\Sigma V^T \\cdot V \\Sigma U^T \\\\ \u0026= U \\Sigma^2 U^T \\end{aligned} $$T 和 S 有相同的特征值(eigen value): $\\Sigma^2$。\nPCA：先对 S 做特征值分解，找到了主成分（特征向量/投影方向）；然后样本点 $HX$ 乘以方向向量$V$（投影），得到各方向上的坐标。 坐标矩阵：$HX \\cdot V = U \\Sigma \\underline{V^T \\cdot V} = U \\Sigma$\n而对T做特征分解，可以直接得到坐标，这叫主坐标分析（PCoA）\n对T两边左乘$U\\Sigma$（做一个缩放）： $$ \\begin{aligned} T \u0026= U \\Sigma^2 U^T \\\\ T U \\Sigma \u0026= U \\Sigma^2 \\underline{U^T U} \\Sigma \\\\ \u0026= U \\Sigma^3 \\\\ T \\underbrace{U \\Sigma}_{特征向量} \u0026= U \\Sigma \\underbrace{\\Sigma^2}_{特征值} \\end{aligned} $$也就是说，对T做SVD奇异值分解后，直接得到的特征向量就是坐标。\n如果数据的维度太高，$S_{p\\times p}$ 不好计算，可以对$T_{N\\times N}$分解。\n6-Probablistic PCA\n","date":"2021-12-16T13:33:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/05_%E9%99%8D%E7%BB%B4/","title":"watch: ML - 白板 05 | Dimensionality Reduction"},{"content":"Video 7 2021-10-04\n[toc]\nDimensionality Reduction Avoids the curse of Dimensionality October 6, 2021\nCurse of Dimensionality When dimensionality increases, data becomes increasingly sparse Concepts become less meaningful: density and distance Subspace combinations grow very fast Dimentionality Reduction Eliminate irrelevant features and reduces noise\n$X$ is a set of $N$ features: $X=\\{X_1, X_2, \\cdots X_N\\}$，a reduced set $X'$ is a transformation of $X$ and consists of $d$ features so that $d","date":"2021-12-14T14:47:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/aml_feature-selection/","title":"watch: AML | Feature Selection"},{"content":"Video 17 Validation 2021-12-08\nOutline:\nThe validation set Model selection Cross validation Review of Lec12 (期末不涉及12)\nRegularization: add a overfit or complexity penalty term，与模型复杂度有关，使用这个\u0026quot;惩罚项\u0026quot;估计out-of-sample error\n两种正则化方法：\nconstrained regularization: select some type of hypotheses unconstrained regularization: 不是最小化$E_{out}$，而是最小化 $E_{\\rm augment}(\\mathbf w) = E_{in}(\\mathbf w) + \\underbrace{\\frac{\\lambda}{N} \\mathbf w^T \\bf w}_{\\text{penalty term}}$ 选择一个 regularizer 去估计penalty项: $E_{\\rm augment}(\\mathbf w) = E_{in}(\\mathbf w) + \\frac{\\lambda}{N} \\Omega(h)$\n其中 $\\Omega(h)$ 是regularizer，$\\lambda$ 是正则化参数(regularization parameter)\n$\\Omega(h)$: 启发式地选择 heuristic，通常使用weight decay，找到一个 smooth, simple $h$\n$\\lambda$ 决定了正则化被引入的程度。如果选了正确的$\\lambda$，可以很好的估计未知目标函数。Validation 也要找到一个合适的$\\lambda$\nValidation vs Regularization 在learning 过程中，$E_{out}(h)$ 未知（因为目标函数未知），但是它等于 $E_{in}(h)+$ overfit penalty，Ein 是已知的（预测值与训练样本真实值的误差），还需要知道overfit penalty。 所以为了计算 Eout 有两种方法：Regularization 是先估计出 overfit penalty。而Validation 是直接估计 Eout。 $$ \\begin{aligned} \\rm Regularization: E_{out}(h) = E_{in}(h) + \\underbrace{\\text{overfit penalty}}_{\\mathclap{\\text{regularization estimates this quantity}}} \\\\ \\\\ \\rm Validation: \\underbrace{E_{out}(h)}_{\\mathclap{\\text{validation estimates this quantity}}} = E_{in}(h) + \\text{overfit penalty} \\end{aligned} $$Analyzing the estimate Out-of-sample point 是没有在训练阶段中使用的点，\n在一个out-of-sample 点 $(\\mathbf X,y)$ 上的误差是 $\\mathbf e(h(\\mathbf x),y)$。根据要解决问题的不同，误差函数有不同的形式：\n$$ \\begin{aligned} \\text{回归, Squared error:} \u0026 (h(\\mathbf x)-y)^2 \\\\ \\text{分类, Binary error:} \u0026 [\\![ h(\\mathbf x)\\neq y]\\!] \\end{aligned} $$ $h$ 在 out-of-sample分布 上的误差的期望是$E_{out}(h)$： $\\mathbb E[\\mathbf e(h(\\mathbf x),y)] = E_{out}(h)$\n$h$ 在 out-of-sample分布 上的误差的方差是$\\sigma^2$： $\\operatorname{var}[\\mathbf e(h(\\mathbf x),y)] = \\sigma^2$\n从1个点到1组点： 从training set 中独立地选出K个点组成一个验证集(validation set) $(\\mathbf x_1,y_1), \\cdots, (\\mathbf x_K, y_K)$，验证集上的误差是 $E_{\\text{val}}(h) = \\frac{1}{K} \\sum_{k=1}^K \\mathbf e(h(\\mathbf x_k), y_k)$\n不同验证集误差的期望：$\\mathbb E[E_{\\text{val}}(h)] = \\frac{1}{K} \\sum_{k=1}^K \\mathbb E[\\mathbf e(h(\\mathbf x_k), y_k)] = E_{out}(h)$ (期望放里面，就是$E_{out}$)\n不同验证集误差的方差：$\\operatorname{var} [E_{\\text{val}}(h)] = \\frac{1}{K^2} \\sum_{k=1}^K \\operatorname{var}[\\mathbf e(h(\\mathbf x_k), y_k)] = \\frac{\\sigma^2}{K}$ (因为各点互相独立，所以协方差矩阵除了对角线其他位置都是零)\n验证集的误差等于Eout 加一个 $\\frac{1}{\\sqrt{K}}$ 阶（标准差）的偏置项：\n$$ E_{\\text{val}}(h) = E_{\\text{out}}(h) \\pm O(\\frac{1}{\\sqrt{K}}) $$如果增加验证集样本数量 K，偏置项变小，验证集误差就越接近Eout。\n对于数据集 $\\mathcal D = (\\mathbf x_1, y_1), \\cdots, (\\mathbf x_N, y_N)$\n选K个点作为验证集：$\\mathcal D_{\\rm val}$\n剩下 N-K 个点是训练集：$\\mathcal D_{\\rm train}$\n对于偏置项：$O(\\frac{1}{\\sqrt{K}})$，小K让Eval 与 Eout 差的远，而大K让Ein 与 Eout 差得远。所以K需要tradeoff\n以前通常用全部的数据集来训练，得到g，现在只用了一部分数据 (reduced dataset) 来训练，得到$g^-$，所以它的 Ein和Eout 都比g大。然后计算 $g^-$ 在验证集上的误差 $E_{val}(g^-)$，作为Eout 的近似，如果K很大，近似效果会差。经验法则：$K= \\frac{N}{5}$\nValidation set 不是 test set\n$E_{val}(g^-)$ 也不是 $E_{out}$。测试集与训练无关 (unbiased)，而验证集会在训练阶段帮助我们选择超参数，从而影响了学习过程 (optimistic bias)。\n比如，有两个假设 $h_1$ 和 $h_2$，其实它们真正的Eout都是0.5：$E_{out}(h_1) = E_{out}(h_2) = 0.5$ ，但是未知。它们分别在验证集上的误差为 $\\mathbf e_1,\\ \\mathbf e_2$，然后我们会选择留下误差小的那个：$\\mathbf e = min(\\mathbf{e_1,e_2})$, 它的 Eout $\\mathbb E(\\mathbf e)$ 要小于真实值0.5，因为它用的训练数据少于全部数据集，所以validataion 给出的误差是偏向“乐观的”\nModel selection 比如要解决一个分类问题，有M个假设空间：$\\mathcal H_1,\\cdots, \\mathcal H_M$ （比如svm的核可以为linear, polynomial, rbf，选哪种好呢？）。\n根据 (有缩减的reduced) 训练集，从每个假设空间选出“最佳假设”(finalists model 决赛选手)。然后分别在验证集上计算Eval。根据这 M 个Eval，选出最佳 $E_{val}$ 和最佳假设空间 $\\mathcal H_{m^*}$。然后再使用整个数据集在最佳假设空间中找出最佳假设 $g_{m^*}$\n使用$\\mathcal D_{\\rm val}$ 和 $E_{\\rm val}(g_{m^*}^-)$ 选择的最佳假设空间 $\\mathcal H_{m^*}$ 是 $E_{out}(g_{m^*}^-)$ 的一个 biased estimate，因为没有使用全部的数据集，所以叫biased。\n不同容量的验证集与预期偏差的关系如下图：\n验证集中数据 K 越多，用于训练的样本越少，Eout越差，但是同时 $O(\\frac{1}{\\sqrt{K}})$ 减小，$E_{\\rm val}$ 会越接近 $E_{\\rm out}$。\nHow much bias 对于 M 个假设空间：$\\mathcal H_1, \\cdots ,\\mathcal H_M$，从中选出了 M 个 finalists model $H_{\\rm val} = \\{ g_1^-, g_2^-,\\cdots, g_M^- \\}$，然后用验证集 $\\mathcal D_{\\rm val}$ 去“训练”它们，也就是再找出它们中的最佳 minus 假设 $g_{m^\\star}^-$（$E_{\\rm val}$最小）。\n对于一个\u0026quot;训练\u0026quot;过程，对于假设 $g_{m^\\star}^-$ 有Hoeffding不等式成立：\n$$ E_{out} (g_{m^\\star}^-) \\leq E_{val}(g_{m^\\star}^-) + O \\left( \\sqrt{\\frac{ln M}{K}} \\right) $$如果有无穷多个假设集（无穷多个正则化参数，$\\lambda$ 是连续值），所以 $O \\left( \\sqrt{\\frac{ln M}{K}} \\right)$ 就变得不再有效\n为了约束 M，就像之前那样，引入 VC 维。比如，我们不关心正则化参数 $\\lambda$ 能取多少值，而是关心我们有几个参数（自由度），我们只有1个参数 $\\lambda$，所以VC维是1。\nData contamination 在训练阶段用了多少数据样本 $E_{in}，E_{out}(E_{test})，E_{\\rm val}$ Contamination: Optimistic (deceptive) bias in estimating Eout Training set: totally contaminated Validation set: slightly contaminated (起到了“测试”的效果，但也被用于训练了) Test set: totally \u0026lsquo;clean\u0026rsquo; (完全用于测试) Cross validation 把train set 分成n折，每次取n-1折做训练，计算在剩下那折上的准确率，n个准确率求平均就是该组超参数的表现。\n不使用test set，却可以估计在test set上的表现。\n目的是选最佳的超参数；不能根据在train set上的准确率判断好坏。\n选用不同超参数时，CV准确率的变化趋势与在test set上的变化趋势近似一致。\nK 进退两难: $g^-$是用 reduced训练集找出的最佳，K越小，用于训练的数据越多，越接近真实的Eout，而根据Hoeffding不等式，$E_{\\rm val}(g^-)$需要很大的K，才能近似$E_{out}(g^-)$\n$$ E_{\\rm out}(g) \\underset{\\mathclap{\\substack{\\\\ \\text{小K才近似}}}}{\\approx} E_{\\rm out}(g^-) \\underset{\\mathclap{\\substack{\\\\ \\text{大K才近似}}}}{\\approx} E_{\\rm val}(g^-) $$$E_{out}$ 是最终目标，但是只知道验证误差 $E_{\\rm val}(g^-)$\nhave K both small and large\n两种交叉验证方法：\nLeave One Out\nK=1，每次迭代选1个样本做验证，剩下N-1个样本做训练。去除第n个样本的训练集$\\mathcal D_n:$\n$$ \\mathcal D_n = (\\mathbf x_1,y_1),\\cdots,(\\mathbf x_{n-1},y_{n-1}),\\sout{(\\mathbf x_n, y_n)},(\\mathbf x_{n+1},y_{n+1}),\\cdots,(\\mathbf x_N, y_N) $$从 $\\mathcal D_n$ 中学到的假设是 $g_n^-$，验证误差 $\\mathbf e_n = E_{\\rm val}(g_n^-) = \\mathbf e(g_n^- (\\mathbf x_n),y_n)$\n对每个留出的样本点，计算验证误差，然后取平均，就是交叉验证误差 (cross validation error): $$ E_{CV} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf e_n $$ 对于3个点，每次取出一个做验证集，剩下两个做训练集，线性回归问题，对于两个样本，误差最小的Linear假设，就是过两点的一条直线。\n对于 Constant 假设：\n对比 $E_{CV}$，constant 模型的交叉验证误差较小，所以最终选择constant模型\nN个样本的数据集要迭代 N 次，每次在 N-1 个样本上训练，如果有1千个样本就要迭代1千次，计算复杂度太高。\nLeave More Out\n把数据集划分成多份，划分成10份的话：$K = \\frac{N}{10}$，只需迭代10 ($\\frac{N}{K}$)次，每次在N-K个点上训练。\nCross validation in action 数字分类任务，把2个特征（symmetry和Average intensity）非线性变换到20维空间，最高幂次为5的多项式\n$$ \\left(1, x_{1}, x_{2}\\right) \\rightarrow\\left(1, x_{1}, x_{2}, x_{1}^{2}, x_{1} x_{2}, x_{2}^{2}, x_{1}^{3}, x_{1}^{2} x_{2}, \\ldots, x_{1}^{5}, x_{1}^{4} x_{2}, x_{1}^{3} x_{2}^{2}, x_{1}^{2} x_{2}^{3}, x_{1} x_{2}^{4}, x_2^{5}\\right) $$ 使用特征数量越多，模型越复杂，$E_{in}$ 越小（迭代了很多次），$E_{out}$先减小后增大，出现Overfitting，而$E_{CV}$的趋势与$E_{out}$相同，因为$E_{out}$未知，$E_{CV}$是 $E_{out}$ 的近似，所以可以根据 $E_{CV}$ 来决定该选用几个特征。Ecv 的最小值出现在5 和7，所以可以选用6个特征的模型。\n没用validation时，直接使用20个特征的模型很复杂，而且过拟合（噪音），Ein为零；使用validation后，决定只用6个特征，模型相对简单，Eout较小。\n例题 Given three two-dimensional data examples $x_1 = (-1,1)，x_2=(0,2)$, and $x_3=(1,1)$, perform the leave-one-out cross validation for a linear fit using these data examples. What is $E_{CV}$?\n$$ E_{CV} = \\frac{1}{N} \\sum_{n=1}^N \\varepsilon_n $$where $\\varepsilon_n = (y_n - g(x_n))^2$\nNote: The line passing through two-dimensional data points $(x_1, y_1)$ and $(x_2,y_2)$ can be obtained as follows: $y-y_1 = \\frac{y_2 - y_1}{x_2-x_1} \\times (x-x_1)$\nGA answer:\nKeep $x_1$ as for the validation, while $x_2, x_3$ as for training:\n$g:\\ y-2 = \\frac{1-2}{1-0}(x-0) \\Rightarrow y=-x+2$\n$\\varepsilon_1 = (1-g(-1))^2 = (1-3)^2 = 4$\nKeep $x_2$ as for the validation:\n$g:\\ y-1 = \\frac{1-1}{1+1}(x+1) \\Rightarrow y=1$\n$\\varepsilon_2 = (2-g(0))^2 = (2-1)^2 = 1$\nKeep $x_3$ as for the validation:\n$g:\\ y-1 = \\frac{2-1}{0+1}(x+1) \\Rightarrow y=x+2$\n$\\varepsilon_3 = (1-g(1))^2 = (1-3)^2 = 4$\n$E_{CV} = \\frac{1}{3}(4+1+4) = 3$\n","date":"2021-12-14T01:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec13_validation/","title":"watch: AML 13 | Validation"},{"content":"Video 13 Neural Network 2021-11-10\nOutline:\nStochastic gradient descent Neural network model Backpropagation algorithm Gradient Descent 沿着误差函数 $\\mathbf e$ 的负梯度方向，一步一步最小化 in-sample error。\nEin 是（线性/非线性）模型的权重 $\\mathbf w$ 的函数：\n$$ E_{in}(\\mathbf w) = \\frac{1}{N} \\sum_{n=1}^N \\mathbf e(h(\\mathbf x_n), y_n) $$$\\mathbf e$ 是误差函数，计算假设值与样本真实值之间的误差。复杂的误差函数越难优化。有时不能向 linear regression 那样 one-shot 求出最佳w。可以用梯度下降，一步一步地使误差下降。移动的方向是负梯度方向$-\\nabla$，每次移动的大小与 $-\\nabla E_{in}$ (Gradient error) 成比例（$\\eta$是学习率）：\n$$ \\Delta \\mathbf w = - \\eta \\nabla E_{in}(\\mathbf w) $$这里$\\nabla$ Ein 是基于所有的样本点($\\mathbf x_n, y_n$)，叫做\u0026quot;batch GD\u0026quot;，也就是使用所有点做了一次梯度下降。\nStochastic gradient descent 一次随机选一个点做梯度下降。\nPick one ($\\mathbf x_n, y_n$) at a time. Apply GD to $\\mathbf e(h(\\mathbf x_n),y_n)$\nN 次梯度下降的“平均方向”(Average direction)还是等于$-\\nabla E_{in}$:\n$$ \\mathbb E_n [-\\nabla \\mathbf e(h(\\mathbf x_n), y_n)] = \\frac{1}{N} \\sum_{n=1}^N -\\nabla \\mathbf e(h(\\mathbf x_n),y_{n}) = -\\nabla E_{in} $$随机梯度下降是梯度下降的 randomized version\nSGD 的好处：\n简化计算 (Cheaper computation): 每次只看一个样本点 随机化 (Randomization): 避免陷入局部最小或鞍点，无法继续优化。如果使用“batch GD”，那么初始位置最关键，因为只走一步，所以容易陷入附近的局部最优。 简单 (Simple) Rule of thumb (经验法则): $\\eta = 0.1$ works\n例子：电影评分\nUser $u_i$ 的喜好有K个属性，Movie $v_j$ 的也有对应的K个属性。根据这个用户他之前评价过的电影 $r_{ij}$ (rating)，调整用户的各属性权重，最小化误差。\n$$ \\mathbf e_{ij} = \\left( \\underbrace{ r_{ij}}_{\\text{actual}} - \\underbrace{\\sum_{k=1}^K u_{ik}v_{jk}}_{\\text{predict}} \\right)^2 $$反过来，把用户属性输入模型就可以估计某电影的评分\n\u0026hellip;\n2D perceptron 的break point=4，也就是感知机无法解决异或问题。\n\u0026hellip;\nNeural Network model 对于从神经元 $i$ 出发，指向第 $l$ 层的神经元 $j$ 的权重 $w_{ij}^{(l)}$\n$$ \\begin{cases} 1 \\leq l \\leq L \u0026 \\text{隐藏层/输出层序号, 输入层是0} \\\\ 0 \\leq i \\leq d^{(l-1)} \u0026 \\text{w出发的神经元: 0代表从bias出发}\\\\ 1 \\leq j \\leq d^{(l)} \u0026 \\text{w指向的神经元: 至少有一个,最多有$d^{l}$}\\\\ \\end{cases} $$第 $l$ 层的某神经元，接受了来自上一层所有神经元的输入（内积）：\n$$ x_j^{(l)} = \\theta(s_j^{(l)}) = \\theta \\left( \\sum_{i=0}^{d^{(l-1)}} w_{ij}^{(l)} x_i^{(l-1)} \\right) $$从bias term 开始加到第 $d^{(l-1)}$ 个，把信号 $s_j^{(i)}$ 传入 $\\theta$ 非线性激活函数\n一个样本 $\\mathbf x$ 有$d^{(0)}$ 个维度，所以输入层对应有：$x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)}$，经过一层一层传递，直到最终输出一个值：$x_1^{(L)} = h(\\mathbf x)$\nBackpropagation algorithm 应用随机梯度下降，调节神经网络的权重，使误差函数最小\n网络全部的权重 $\\mathbf w = {w_{ij}^{(l)}}$ 决定了一个假设 $h(\\mathbf x)$ (输入到输出的映射)\n对于一个样本 $(\\mathbf x_n,\\ y_n)$ 上的误差：$\\mathbf e(h(\\mathbf X_n),\\ y_n) = \\mathbf e(\\mathbf w)$，使用SGD，调整权重，减小误差\n误差函数对每个权重求梯度：\n$$ \\nabla \\mathbf e(\\mathbf w): \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}, \\quad \\text{for all } i,j,l $$ 计算误差函数 $\\mathbf e$ 对各权重 w 的梯度 $\\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}$\n误差 $\\mathbf e$ 是实际输出 $\\theta(s)$ 减去真实值 $y$，所以误差函数首先是 $s_{j}^{(l)}$ 的函数，第 $l$ 层的第 $j$ 个神经元的输入信号$s_{j}^{(l)}$ 是来自上一层所有神经元的输出贡献之和，所以 $s_{j}^{(l)}$ 是 $w_{ij}^{(l)}$ 的函数，根据链式求导法则：\n$$ \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}} = \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_{j}^{(l)}} \\times \\frac{\\partial s_{j}^{(l)}}{\\partial w_{ij}^{(l)}} $$其中：$\\frac{\\partial s_{j}^{(l)}}{\\partial w_{ij}^{(l)}} = x_i^{(l-1)}$，也就是前一层的神经元的输出。 把误差对输入信号的导数称为：$\\delta_j^{(l)} = \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_{j}^{(l)}}$。\n所以（某神经元上的）误差对各权重 w 的梯度等于：$\\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}= \\delta_j^{(l)} x_i^{(l-1)}$。\n计算$\\delta$:\n从最后一层（输出层$l=L,\\ j=1$）的 $\\delta_1^{(L)}$ 开始计算，输出神经元的输入信号是 $s_1^{(L)}$:\n$$ \\begin{aligned} \\delta_1^{(L)} \u0026= \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_1^{(L)}}\\\\ \\mathbf e(\\mathbf w) \u0026= \\left ( x_1^{(L)}- y_n \\right)^2 \u0026 \\text{预测值-实际值} \\\\ x_1^{(L)} \u0026= \\theta(s_1^{(L)}) \u0026 \\text{神经元的输出是$\\theta$的输出} \\\\ \\theta'(s) \u0026= 1 - \\theta^2(s) \u0026 \\text{for the tanh} \\end{aligned} $$之前层 ($l-1$层) 神经元上的误差对其输入信号的导数：\n因为第 $l-1$ 层的某个神经元会对第 $l$ 层的全部神经元都有贡献，所以它的误差来自第 $l$ 层的全部神经元 $\\delta_j^{(l)}$，所以需要求和。根据链式法则，误差$\\mathbf e$ 从上一层 ($l$层) 过来，所以首先是 $s^{(l)}$ 的函数，然后$s^{(l)}$ 是第 $(l-1)$ 层神经元 $x^{(l-1)}$ 的函数，最后 $x^{(l-1)}$ 才是它输入信号 $s^{(l-1)}$ 的函数：\n$$ \\begin{aligned} \\delta_i^{(l-1)} \u0026amp;= \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_i^{(l-1)}} \\ \u0026amp;=\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathbf{e}(\\mathbf{w})}{\\partial s_{j}^{(l)}} \\times \\frac{\\partial s_{j}^{(l)}}{\\partial x_{i}^{(l-1)}} \\times \\frac{\\partial x_{i}^{(l-1)}}{\\partial s_{i}^{(l-1)}} \u0026amp; \\text{第$l$层所有神经元误差求和}\\\n\u0026amp;=\\sum_{j=1}^{d^{(l)}} \\delta_{j}^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_{i}^{(l-1)}\\right) \\\n\\delta_{i}^{(l-1)} \u0026amp;=\\left(1-\\left(x_{i}^{(l-1)}\\right)^{2}\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)} \u0026amp; \\text{$\\theta$与j无关,求导放前面; $x_i^{l-1}$也就是$\\theta(s)$} \\end{aligned} $$\n所以最后一层之前层的神经元的 $\\delta$ 等于 1 减去这个神经元输出的平方，再乘上从它出发的各权重与下一层的$\\delta$ 的内积之和。最后一层的$\\delta^{(L)}$算出来了，才能算倒数第2层的$\\delta^{(L-1)}$，从而可以反向地一层一层求出误差$\\mathbf e$ 对各个权重 w 的梯度。\nBackpropagation algorithm:\nInitialize all weights $w_{ij}^{(l)}$ at random for t=0,1,2, \u0026hellip;, do //循环 Pick $n \\in \\{ 1,2,\\cdots,N \\}$ //从N个样本中挑一个 Forward: Compute all $x_j^{(l)}$ //计算每个神经元的输出，从而得出预测值 Backward: Compute all $\\delta_j^{l}$ //计算每个神经元的误差(贡献) Update the weights: $w_{ij}^{(i)} \\leftarrow w_{ij}^{l} - \\eta x_i^{(l-1)} \\delta_j^{(l)}$ //迭代直到收敛 Iterate to the next step until it is time to stop Return the final weights $w_{ij}^{l}$ Final remark: hidden layers\n隐藏层是在“模仿”非线性变换：把高维样本（线性不可分）变换到新的维度空间，叫做“learned nonlinear transform”。隐藏层的每个神经元是 \u0026ldquo;learned feature\u0026rdquo;。\n神经元数量越多，自由度越多（有效参数越多），VC维越高，模型复杂度越高，需要更多的样本，才能保证可以从 $E_{in}$ 泛化到Eout.\nExample： Back Propagation（梯度反向传播）实例讲解\n令 $x_1=1, x_2=0.5$ ，然后我们令 $w_1, w_2, w_3, w_4$ 的真实值分别是 1,2,3,4 ，令 $w_5, w_6$ 的真实值是 $0.5, 0.6$ 。这样我们可以算出 $y$ 的真实目标值是 $t=4$ 。\n那么为了模拟一个Back Propagation的过程，我们假设我们只知道 $x_1=1, x_2=0.5$ ，以及对应的目标 $t=4$ 。我们不知道 $w_1,w_2,w_3,w_4,w_5,w_6$ 的真实值，现在我们需要随机为他们初始化值，假设我们的随机化结果是 $w_1=0.5, w_2=1.5, w_3=2.3, w_4=3, w_5=1, w_6=1$。\nForward: 计算 $h_1, h_2, y$ 的预测值和误差项 E，其中 $E=\\frac{1}{2}(t-y)^2$\n$$ \\begin{aligned} h_1 \u0026= w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0.5 \\cdot 1 + 1.5 \\cdot 0.5 = 1.25 \\\\ h_2 \u0026= w_3 \\cdot x_1 + w_4 \\cdot x_2 = 2.3 \\cdot 1 + 3 \\cdot 0.5 = 3.8 \\\\ y \u0026= w_5 \\cdot h_1 + w_6 \\cdot h_2 = 1 \\cdot 1.25 + 1 \\cdot 3.8 = 5.05 \\\\ E \u0026= \\frac{1}{2} (y-t)^2 = 0.55125 \\end{aligned} $$ Backward\nupdata $w_5$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_5} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_5} = (y-t)\\cdot h_1 = 1.05 \\cdot 1.25 =1.3125 \\\n\\frac{\\partial E}{\\partial y} \u0026amp;= (t-y)\\cdot -1 = y-t \\ \\frac{\\partial y}{\\partial w_5} \u0026amp;= \\frac{\\partial (w_5 h_1 + w_6 h_2)}{\\partial w_5} = h_1 \\ w_5^+ \u0026amp;= w_5 - \\eta \\cdot \\frac{\\partial E}{\\partial w_5} = 1-0.1\\cdot 1.3125 = 0.86875 \\end{aligned} $$\nupdata $w_6$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_6} \u0026= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_6} = (y-t)\\cdot h_2 = 1.05 \\cdot 3.8 = 3.99 \\\\ w_6^+ \u0026= w_6 -\\eta \\cdot \\frac{\\partial E}{\\partial w_6} = 1-0.1\\cdot 3.99 = 0.601 \\end{aligned} $$下面我们再来看 $w_1, w_2, w_3, w_4$ ，由于这四个参数在同一层，所以求梯度的方法是相同的\nupdata $w_1$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_1} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_1} = (y-t) \\cdot w_5 \\cdot x_1 = 1.05 \\cdot 1 \\cdot 1 = 1.05 \\\nw_1^+ \u0026amp;= w_1 - \\eta \\cdot \\frac{\\partial E}{\\partial w_1} = 0.5 - 0.1 \\cdot 1.05 = 0 .395 \\end{aligned} $$\nupdata $w_2$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_2} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_2} = (y-t) \\cdot w_5 \\cdot x_2 = 1.05 \\cdot 1 \\cdot 0.5 = 0.525 \\\nw_2^+ \u0026amp;= w_2 - \\eta \\cdot \\frac{\\partial E}{\\partial w_2} = 1.5 - 0.1 \\cdot 0.525 = 1.4475 \\end{aligned} $$\nupdata $w_3$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_3} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_3} = (y-t) \\cdot w_6 \\cdot x_1 = 1.05 \\cdot 1 \\cdot 1 = 1.05 \\\nw_3^+ \u0026amp;= w_3 - \\eta \\cdot \\frac{\\partial E}{\\partial w_3} = 2.3 - 0.1 \\cdot 1.05 = 2.195 \\end{aligned} $$\nupdata $w_4$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_4} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_4} = (y-t) \\cdot w_6 \\cdot x_2 = 1.05 \\cdot 1 \\cdot 0.5 = 0.525 \\\nw_4^+ \u0026amp;= w_4 - \\eta \\cdot \\frac{\\partial E}{\\partial w_4} = 3 - 0.1 \\cdot 0.525 = 2.9475 \\end{aligned} $$\nForward:\n$$ \\begin{aligned} h_1 \u0026= w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0.395 \\cdot 1 + 1.4475 \\cdot 0.5 = 1.11875 \\\\ h_2 \u0026= w_3 \\cdot x_1 + w_4 \\cdot x_2 = 2.195 \\cdot 1 + 2.9475 \\cdot 0.5 = 3.66875 \\\\ y \u0026= w_5 \\cdot h_1 + w_6 \\cdot h_2 = 0.97191 + 2.204918 = 3.17683 \\\\ E \u0026= \\frac{1}{2} (y-t)^2 = 0.338802 \\end{aligned} $$ ","date":"2021-12-14T00:53:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec10_neural_networks/","title":"watch: AML 10 | Neural Networks"},{"content":"Video 12 Bias-Variance Tradeoff 11-01-2021\nOutline:\nBias and Variance Learning Curves Review of Lec 7\n$d_{VC}(\\mathcal H)$: the most number of points $\\mathcal H$ can shatter $d_{VC}$是有限值，让 g 近似 $f$ 成为可能。VC维仅由假设集决定。 为了降低到某一概率，$d_{VC}$越大，所需样本点数N越多。$N\\geq 10 d_{VC}$ $E_{out} \\leq E_{in}+\\Omega$ Generalization bound $\\Omega(N,\\mathcal H, \\delta))$: Bias and Variance Approximation-generalization tradeoff 近似与泛化的权衡\n小的Eout 意味着g在 out-of-sample 上也是f的一个好的近似（样本外误差也很小）。 越复杂的假设集 $\\mathcal H$（M越大），有更好的机会近似 $f$（更可能包含最佳假设g） 越简单的假设集 $\\mathcal H$，有更好的机会在out-of-sample上泛化。 最理想情况：假设集中只包含一个正在寻找的“未知的目标函数” $\\mathcal H=\\{f\\}$，g 也就是f。 Quantifying the tradeoff 之前的 VC analysis 是一种评估方法：$E_{out} \\leq E_{in}+\\Omega$ 与之相似，Bias-variance analysis 是另一种评估方法：把 Eout 分解成两项： 假设集$\\mathcal H$ 能有多近似 $f$ （Bias） 能在多大程度上确定$\\mathcal H$中的好的假设 （Variance） 这里分析的目标函数是实值的 real-valued, 并且使用平方误差 squared error Start with $E_{out}$ Eout 是假设集中的最佳假设 $g$ 与 未知目标函数 $f$ 在输入空间 $\\mathcal X$ 的各个点上的差距的期望；g 是样本集 $\\mathcal D$ 的函数，样本集不同，选出来的最佳假设 g 也不同：\n$$ E_{out}(g^{(D)}) = \\mathbb E_{\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] $$因为每次从输入空间抽出的样本集 $\\mathcal D$ 不一样，g就不一样，Eout 也就不一样，所以把各个Eout求个期望，作为最终的Eout：\n$$ \\begin{aligned} \\mathbb E_{\\mathcal D} \\left[ E_{out} \\left( g^{(\\mathcal D)} \\right) \\right] = \\mathbb E_{\\mathcal D} \\left[ \\mathbb E_{\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \\\n= \\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \u0026amp; \\text{(交换位置)} \\ \\end{aligned} $$\n只关注其中 \u0026ldquo;1个点上的平均误差期望\u0026rdquo;：$\\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$\n定义平均假设 (average hypothesis)：$\\bar{g}(\\mathbf x) = \\mathbb E_{\\mathcal D} \\left[ g^{(\\mathcal D)}(\\mathbf x) \\right]$ (the best thing you can do)，从各不同训练集上得出的最佳假设的平均值。\n比如有 K 个训练集：$\\mathcal D_1, \\mathcal D_2, \\cdots, \\mathcal D_K$，那么平均假设就是：$\\bar{g}(\\mathbf x) \\approx \\frac{1}{K} \\sum_{k=1}^K g^{\\mathcal D_k}(\\mathbf x)$\n把平均假设代入\u0026quot;1个点上的平均误差期望\u0026quot;：\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\ \u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) -\\bar{g}(\\mathbf x) + \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\quad \\text{(减一个加一个)} \\\n\u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 + \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 + 2 \\left( g^{(D)}(\\mathbf x)-\\bar{g}(\\mathbf x) \\right) \\left( \\bar{g}(\\mathbf x) -f(\\mathbf x) \\right) \\right] \\quad \\text{(代入括号)} \\\n\u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] + \\underbrace{\\mathbb E_{\\mathcal D} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]}{与D无关,期望还是自己} + 2 \\left( \\underbrace{ \\mathbb E{\\mathcal D} \\left[ g^{(D)}(\\mathbf x) \\right] -\\bar{g}(\\mathbf x) }_{相等, =0} \\right) \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right) \\\n\u0026amp; = \\underbrace{ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] }{\\rm var(\\mathbf x)} + \\underbrace{ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 }{\\rm bias(\\mathbf x)} \\end{aligned} $$\n$\\bar{g}(\\mathbf x)$ 是\u0026quot;最佳假设\u0026quot;，它与目标未知函数的差是常数 bias (与D无关)，而 $g^{\\mathcal D}(\\mathbf x)$ 随训练集不同，会上下波动，与\u0026quot;平均值\u0026quot;的差的平方，再取平均就是方差。 各个最佳假设与目标未知函数的平方误差的期望，被拆成了两部分：各最佳假设与平均假设的方差，加上平均假设与目标未知函数的平方误差。\n所以 Eout 等于：\n$$ \\begin{aligned} \u0026 \\mathbb E_{\\mathcal D} \\left[ E_{out} (g^{(\\mathcal D)}) \\right] \\\\ \u0026 = \\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \\\\ \u0026 = \\mathbb E_{\\mathbf x} \\left[ \\rm bias(\\mathbf x) + var(\\mathbf x) \\right] \\\\ \u0026 = \\rm bias + var \\end{aligned} $$The trade off between bias and var bias = $\\mathbb E_{\\mathbf x} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$\nvariance = $\\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] \\right]$\n如果假设集中只有一个假设 h，它与 $f$ 的距离就是 bias（方差为0）。如果是一个复杂的假设集，其中包含很多假设，更有可能囊括了 f，在它附近的都是“最佳假设”（红色），因为平均假设$\\bar g(\\mathbf x)$就在 f 附近所以bias较小，而方差比较大（很多小值加起来也会大）。\nExample: sine target 近似正弦曲线，未知目标函数 $f(x) = sin(\\pi x)$，把 f 从输入空间从 [-1,1] 扩展到实数域 $f:[-1,1] \\rightarrow \\mathbb R$。只有两个样本点 N=2。\n有两个假设集，自由度不同：\n$$ \\begin{aligned} \u0026 \\mathcal H_0 : h(x) = b \u0026{\\text{各假设只有一个参数b (常数)}} \\\\ \u0026 \\mathcal H_1 : h(x) = ax+b \u0026{\\text{各假设有两个参数a,b (直线)}} \\end{aligned} $$Approximation: 上帝视角可以看出两个假设集中的最佳假设(bias最小)分别应该为：\n黄色区域是 bias (或者说就是 Eout, 因为单个假设的方差为0)。所以在近似[-1,1]区间上的正弦函数时，$\\mathcal H_1$ 比 $\\mathcal H_0$ 的 bias 更小。\nLearning: 两个样本点的位置是随机的\n如果一开始两个样本点位置如图：\n为了使 bias 最小，$\\mathcal H_0$中的“最佳假设”应该在两样本点中间，$\\mathcal H_1$ 中的“最佳假设”应该穿过两个样本点。\n两个样本点（训练集）每次从输入空间中取的都不一样，对应的“最佳假设”也有很多种可能：\n对于假设集 $\\mathcal H_0$，最佳假设的分布如图：\n对各个“最佳假设”取平均，平均假设位于”平衡位置“，灰色区域是 variation.\n对于假设集 $\\mathcal H_1$，最佳假设（过两样本点的直线）的分布如图：\n平均假设是红色直线，灰色区域是variation。\n对比两个假设集：\n只有一个参数的，最简单的（常数）假设集 $\\mathcal H_0$ 的(平均假设) bias大，方差小。而比较复杂的（直线）假设集 $\\mathcal H_1$ 的 bias 小，方差大。\n最终，$\\mathcal H_0$ 的 Eout= 0.50+0.25 = 0.75，$\\mathcal H_1$ 的 Eout=0.21+1.69 = 1.9。根据 Eout，简单的假设集 $\\mathcal H_0$ 好于复杂的假设集 $\\mathcal H_1$。因为我们是从 Ein \u0026ldquo;泛化\u0026rdquo; 到 out，如果Eout 太大，Generalization bound 太大，Ein 与 Eout 相差太大，二者不follow，就不能通过Ein 学习到 Eout.\n复杂的（自由度多的）假设集有很好近似能力，但是学习能力很差，因为方差太大，不一定能学到最佳假设。\nLesson learned: 模型的复杂度应该匹配 数据（样本决定了最终找出的假设），而不应该匹配目标函数的复杂度。比如只有15个样本，假设已知目标函数是10阶的。你可以选1阶，2阶的模型，它们对应的参数有2个，3个，按照经验法则 $N\u003e10 d_{VC}$，它们至少需要20个，30个样本。但现在只有15个，如果你觉得1阶（直线）不太可能的话，可以用2阶（二次函数）。如果用10阶模型，就出现过拟合了。\nLearning Curves Ein 与 Eout 的曲线。\n对于任意有N个(训练)样本数据集 $\\mathcal D$。\nExpected Eout = $\\mathbb E_{\\mathcal D} \\left[ E_{out} \\left( g^{(\\mathcal D)} \\right) \\right]$ (不同测试集上的最佳假设 g 的平均)\nExpected Ein = $\\mathbb E_{\\mathcal D} \\left[ E_{in} \\left( g^{(\\mathcal D)} \\right) \\right]$ (不同(训练)数据集上的最佳假设 g 的平均)\n它们随 N 如何变化？(How do they vary with N?)\n对于Simple Model（的假设集），随着样本数不断增加，Eout越来越小，越来越近似 f，而 Ein越来越大，因为每个样本都有误差，样本越多加起来越大。N越大，Ein与Eout越接近，Generalized bound越小，收敛于 “平均假设” (黑色水平线)。\n对于Complex Model，“平均假设”更靠近 f，bias较小，所以黑色水平线它更低，同样随着N增大，Eout与Ein 不断趋近于 “平均假设”。但是当N很小的时候，Eout很大（方差很大）。Eout 与Ein 差距很大，复杂模型相较于简单模型的 Generalization bound 更大。复杂模型的 Ein 在样本很少的时候是零，因为在样本数小于VC维或者假设集的effective参数自由度时，模型可以把所有点全部分开 (shatter all the points)。在超过VC维之后，Ein开始增加。\nVC vs Bias-variance 在 VC 分析中，$E_{out} = E_{in} + \\text{Generalization error}$\n在 Bias-variance 中，不再关注$E_{in}$，因为 $E_{out}=\\text{var + bias} = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] +\\mathbb E_{\\mathcal D} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$，Eout 等于“粗黑线”(average hypothesis, bias)加上variance。bias取决与假设集而与N无关，所以bias是直线。\n随着N增大，它们都趋近于平均假设。所以都需要tradeoff：简单的模型近似能力差，但它 Generalization error小；复杂的模型近似能力强，但它需要更多的样本，才能减小 Generalization bound。对于复杂的模型，样本数越少，Eout越大。样本少的时候，简单模型的Eout 可能比复杂模型的 Eout 还要小。所以样本少选简单模型，样本多选复杂模型。\nLinear Regression case Noisy target $y = \\mathbf w^{*T} \\mathbf x$ + noise （用线性模型, 从有噪声的样本中，学习一个线性目标函数）\nData set $\\mathcal D = \\{ (\\mathbf x_1, y_1), \\cdots, (\\mathbf x_N, y_N) \\}$\nLinear regression solution: $\\mathbf w = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf y$\nIn-sample error vector = $\\bf X w - y$\n\u0026ldquo;Out-of-sample\u0026rdquo; error vector = $\\bf X w - y'$ （使用相同的x, noise不同，得到测试集）\n有了上面非常特殊的情况，才能得到下面的公式：\n$\\sigma^2$ 是 energy of the noise。一个 zero-mean noise的energy 与variance 成正比，\n","date":"2021-12-13T15:53:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec8_bias_variance/","title":"watch: AML 08 | Bias-Variance Tradeoff"},{"content":"Video 14 Overfitting 2021-11-17\nOutline:\nWhat is overfitting? The role of noise Deterministic noise Dealing with overfitting An Illustration of Overfitting on a Simple Example\n蓝色曲线是 unknowen target function。从这未知函数中生成5个样本点，因为有噪声，所以它们偏离了曲线。 根据这5个点去近似未知函数。因为噪声的存在，在用4阶的多项式（有5个参数）拟合这生成的5个点时，出现了过拟合：$E_{in}=0;\\ E_{out} \\gg 0$。（如果没有噪声，有可能使用相同阶数的模型就能完美拟合。为了拟合带噪声的数据，就使用了超过样本所表达信息的高阶模型，造成Eout 很大）\nWhat is Overfitting? 随着VC 维的增加，$E_{in}$ 不断下降，$E_{out}$反而上升。\n拟合数据的程度超过了样本数据应有(支持)的程度 (Fitting the data more than is warranted)\n随着 VC 维的增加，假设集$\\mathcal H$中的模型复杂度增加，对未知函数的近似能力增强，虽然generalization error在增加，但一开始，$E_{in}$ 和 $E_{out}$ 都在下降。超过某一点后，$E_{in}$继续减小，$E_{out}$反而上升，Generalization error 变更大:\n实验两种情况：一个10阶的函数有噪声采样15个样本，一个50阶的函数无噪声采样15个样本：\n分别用2阶和10阶的多项式去拟合：\n从2阶模型转到10阶模型，穿过了更多的样本点，Ein更小了，但Eout变大了，说明出现了过拟合。简单模型甚至好于无噪声的复杂目标函数。noise 的阶数可能很高，所以简单模型的Ein 比较差。但是没有噪声的高阶目标函数样本也存在某种“噪声”：determinstic noise.\n假设集$\\mathcal H$ should match to the quantity and quality of the data, than the complexity of unknown target function.\n两个学习器：O (overfitting) 和 R (restricted)\n\u0026hellip;\u0026hellip;.\n结果:\n左图，固定目比函数的复杂度为20th 阶多项式，随着增加噪声能量，样本的数量越来越不足以表达出目标函数的复杂度，这时希望不断减小Ein，非要拟合出超出数据表达范围的部分来减小Eout 会事与愿违，适得其反，南辕北辙，导致错误的形式，Eout反而越来越大，所以过拟合越来越严重。增加样本数量，可以减小过拟合。\n右图，无噪声的高阶目标函数样本，固定噪声能量 $\\sigma^2=0.1$。随着增加目标函数的复杂度，过拟合也越来越严重。所以这里存在另一种不同于左图随机噪声的“确定性噪音”。\nDeterministic noise 是假设集$\\mathcal H$ 无法捕捉到的 f 的部分。(The part of f that H cannot capture)\n因为模型复杂度有限，所以假设集中的最佳假设无法拟合目标函数的某些部分，$h^\\star$ 与目标函数 f 之间的面积就是确定性噪音: $f(\\mathbf x) - h^\\star(\\mathbf x)$。假设集太简单，无法表达出目标函数的复杂度，它无法理解的部分会给它带来困扰，当它尝试拟合这些能力范围之外的东西，就会导致错误的形式。那些无法理解的东西对它来说就是噪音。当使用更复杂的假设集，确定性噪声就会减小，因为它可以捕捉到更多\n确定性噪声与随机噪声的主要区别:\n确定性噪音取决于假设集$\\mathcal H$，而随机噪声对所有的假设集都一样，Nothing can capture it, therefore it\u0026rsquo;s noise. 对于一个给定的样本$\\mathbf x$，确定性噪声的量是固定的，就是 $f(\\mathbf x) - h^\\star(\\mathbf x)$。而随机噪声，两个相同的x，产生的噪声大小是随机的。 但是它们两者对机器学习造成的影响是相同的，因为数据集是给定的（一次使用），假设集一旦确定它的确定性噪声也固定了。So in a given learning situation, they behave the same. Impact on overfitting 确定性噪声造成的过拟合在10阶以上的target complexity 才出现，因为这里的假设集是10阶的，超出10阶的部分它才无法近似。 噪声造成过拟合是因为有限的样本，让你误以为你可以完美拟合。但其实随机噪声是捕捉不到的，而且当目标函数复杂度高于假设集的时候，确定性噪声也是捕捉（学）不到的。你以为你学到了，但其实造成了过拟合。 Noise and bias-variance $$ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] = \\underbrace{ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] }_{\\rm var(\\mathbf x)} + \\underbrace{ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 }_{\\rm bias(\\mathbf x)} $$Eout 被分成了两部分，其中的 f 是没有噪声的，如果给样本数据加入噪声，上式会如何变化？\n给定actual output：$y = f(\\mathbf x) + \\varepsilon(\\mathbf x)$，假设噪声期望是0：$\\mathbb E[\\varepsilon(\\mathbf x)] = 0$\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - y \\right)^2 \\right] \u0026amp; \\text{$\\varepsilon$影响了y，也要对ε求期望}\\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) -\\varepsilon(\\mathbf x) \\right)^2 \\right] \u0026amp; \\text{加一个减一个} \\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) +\\bar g(\\mathbf x) - f(\\mathbf x) -\\varepsilon(\\mathbf x) \\right)^2 \\right] \\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) \\right)^2\n\\left( \\bar g(\\mathbf x) - f(\\mathbf x) \\right)^2 \\left( \\varepsilon(\\mathbf x) \\right)^2 + \\text{cross term} \\right] \\end{aligned} $$ 求期望之后 cross term 都变成0了，因为它们包含ε，ε 的期望为0。前面两项只与数据集有关，而与训练样本的噪声ε无关，所以原来（没加噪声）等于0的项现在还等于0。Eout 只剩三项：\n$$ E_{out} = \\underbrace{ \\mathbb E_{\\mathcal D,\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) \\right)^2 \\right] }_{var}\n\\underbrace{ \\mathbb E_{\\mathbf x} \\left[ \\left( \\bar g(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] }_{\\substack{bias \\↑ \\ \\text{deterministic noise}}} \\underbrace{ \\mathbb E_{ε, \\mathbf x} \\left[ \\left( \\varepsilon(\\mathbf x) \\right)^2 \\right] }_{\\substack{\\sigma^2 \\ ↑ \\ \\text{stochastic noise}}} $$ 第三项是 target 与 actual output之间的差距，就是随机噪声，第二项是\u0026quot;平均假设\u0026quot;（最佳假设the best thing you can do）与target 之间的差距，它无法拟合的部分就是确定性噪声。\n两个噪声是平等的，因为增加样本数量，方差缩小，但两个噪声不可避免的（假设集给定，数据集给定，整体的近似就确定了）。当假设尝试拟合噪声时，就产生了方差（和过拟合）。\nTwo cures Regularization: putting the brakes 踩刹车（增加一点限制; 提前停止）\nValidation: Checking the bottom line 找到底线（找到比Ein 更好的反映拟合质量的误差$E_{CV}$）\n","date":"2021-12-13T13:55:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec11_overfitting/","title":"watch: AML 11 | Overfitting"},{"content":"Video 10 Training and testing 10-20-2021\nOutline\nFrom training to testing Illustrative examples Break point Puzzle Train multiple model / hypotheses: $$ \\mathbb P[|E_{in} - E_{out}| \u003e \\varepsilon] \\leq \\underbrace{2 \\ M \\ e^{-2\\varepsilon^2 N}}_{\\text{union bound}} $$Test one model / hypothesis: (Hoeffding Inequality) $$ \\mathbb P[|E_{in} - E_{out}| \u003e \\varepsilon] \\leq 2 e^{-2\\varepsilon^2 N} $$M: 假设集中所有可能假设的个数, 可以是无穷，所以概率可以是无穷，就不能保证 $E_{out}\\approx E_{in}$\nLearning is to find the best hypothesis g which make the probability of \u0026quot;\u0026quot; ($\\mathcal B$ad event) minimum.\n\u0026ldquo;Learning\u0026rdquo; 是找到假设集中的最佳假设 g（让 $\\mathcal B$ad event: $|E_{in}(h_m) - E_{out}(h_m)|\u003e\\varepsilon$ 发生的概率最小），g 可以是假设1，或者假设2，\u0026hellip;，或者假设M，就对应概率相加，就是union bound： $$ \\mathbb P [\\rm \\mathcal B_1 or \\mathcal B_2 or \\cdots \\mathcal B_M] \\leq \\underbrace{ \\mathbb P [\\mathcal B_1] + \\mathbb P [\\mathcal B_2] + \\cdots +\\mathbb P [\\mathcal B_M]}_{\\text{no overlaps: M terms}} $$“等于” 发生在它们互不重叠的情况，如果各假设间有重叠，它们的和就变小。\n事实上，Bad Events are very overlapping.\n白色与灰色分界线是 unknown target function。 蓝线和绿线是两个 hypothesis。 假设 h 与真实界线所差的面积（红色阴影）是$E_{out}$，黄色面积是 $\\Delta E_{out}$。$E_{in}$未在图中体现，需要对整个面积空间采样，样本点落在 $E_{out}$ 中的个数是$E_{in}$，样本点落在黄色区域内的个数是$\\Delta E_{in}$。 $h_1$ 与 $h_2$ 有很大的重叠，$|E_{in}(h_1) - E_{out}(h_1)| \\approx |E_{in}(h_2)-E_{out}(h_2)|$\n改变h，$E_{out}$ 有显著变化，但$E_{in}$不会有显著变化，因为训练集样本点的个数很有限。所以很多假设的分类效果是一样的。\nReplace M with the number of dichotomies (把样本点成功二分类的斜线的条数).\nA hypothesis $\\rightarrow$ A dichotomy A dichotomy is a mini hypothesis\n对比:\nInput space: A hypothesis, 全输入空间都是输入 $h: \\mathcal X \\rightarrow\\{-1, +1\\}$ A dichotomy, 只有 N 个训练点 $h: \\{\\mathbf{x_1,x_2,\\cdots,x_N}\\}\\rightarrow \\{-1, +1\\}$ Number of hypotheses $|\\mathcal H|$ = M: 可以是无穷个 $|\\mathcal H(\\mathbf{x_1,x_2,\\cdots,x_N}\\})|$ = $m_{\\mathcal H}(N)$ 最多 $2^N$ 个 （N个$\\pm 1$组合) 用 growth function $m_{\\mathcal H}(N)$ 替代 M\n$m_{\\mathcal H}(N)$ 必须是polynomial in N，这样e的负指数可以起作用抵消 $m_{\\mathcal H}(N)$，让union bound变得小。\nGrowth function $m_{\\mathcal H} (N)$ 由 N 个样本点组成任意的，能成功被假设集 $\\mathcal H$ 二分的 dichotomies 最多的个数 (不限位置): $m_{\\mathcal H}(N) = \\underset{x_1,\\cdots,x_N \\in \\mathcal X}{max}\\ |\\mathcal H(x_1, \\cdots, x_N)|$. （\u0026ldquo;most dichotomies on any N points\u0026rdquo;） $$ m_{\\mathcal H}(N) \\leq 2^N $$ 有些 config 的 dichotomies 达不到 $2^N$ 种：\n对于 2D perceptron hypothesis / dichotomies：\n3个点，最多能分8种（若限定点的位置可能更少）\n4个点，最多能分14种（有2种情况 (异或)，2D perceptron分不开）\n(2行2列摆放最多）按照4个o，3个o (1个x)，2个o，1个o，0个o 排列组合。\n如果4点共线，只能分8种。\n如果3点共线，余1个单独在一行，只能分12种。\n对于 Positive rays hypothesis：\n位置摆放是固定的：共线。对于N个点，正射线只能处理 N+1 种配置，也就是圆圈全在右侧，可以是0个圈，1个圈，2个圈，\u0026hellip;，N个圈。\n对于 Positive Intervals hypothesis\n正区间只能处理中间有一段是+1，两侧是-1的配置。需要用两个边界 (threshold) 来确定一个区间：一共有N个点，形成N+1个空，在这N+1个空里选两个，就形成了一个区间，所以是 $C_{N+1}^2$，再加上两个边界在同一个空的情况。\n对于 Convex sets hypothesis\n凸集可以把任意颜色配置的N个点完全分开，凸集里面是+1，外面是-1。所以它的growth function 是 $2^N$，凸集模型没有break point。\nBreak point k 无法用假设集 $\\mathcal H$ 完全把两类点分开的点的个数 k。也就是开始出现 $m_{\\mathcal H}(k) \u003c 2^k$ 这种情况时的点数。\n例如没有任何一种 4 个点的配置，能用2D perceptron 将其完全分开，所以 2D perceptron 的 break point 是 4：$m_{\\text{2D percep}}(4) = 14 \u003c 2^4$。大于4个的点集也不能被完全分开。\n如果没有 break point, growth function is $2^N$.\n如果存在 break point, growth function is a polynomial function in N : $\\sum_{i=0}^{k-1} C_N^i$, whose maximum power is $N^{k-1}$.\n","date":"2021-12-12T23:23:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec5_traning_vs_testing/","title":"watch: AML 05 | Training vs Testing"},{"content":"Video 11 Theory of Generalization 10-25-2021\nOutline\nProof that the growth function is polynomial Proof that $m_{\\mathcal H}(N)$ can replace M VC dimension Examples of polynomial $m_{\\mathcal H}(N)$ Proof see the book\nKey quantity $B(N,k)$: Maximum number of dichotomies on N points, with break point $k$.\nTheorem: $B(N,k) \\leq \\sum_{i=0}^{k-1} \\left( \\begin{aligned} N\\\\ i \\end{aligned} \\right)$\nFor a given $\\mathcal H$, the break point $k$ is fixed:\n$$ m_{\\mathcal H}(N) \\leq \\underbrace{ \\sum_{i=0}^{k-1} \\left( \\begin{aligned} N\\\\ i \\end{aligned} \\right) }_{\\text{maximum power is} N^{k-1}} $$ Positive ray (break point k=2):\n$$ m_{\\mathcal H}(N) = C_{N+1}^1 \\leq (=) \\sum_{i=0}^{k-1} C_N^i = N+1 $$ Positive intervals (break point k=3):\n$$ m_{\\mathcal H}(N) = C_{N+1}^2 + 1 \\leq (=) \\sum_{i=0}^{k-1} C_{N}^i = \\frac{1}{2}N^2 + \\frac{1}{2}N + 1 $$ 2D perceptrons (break point k=4): $$ m_{\\mathcal H}(N)=? \\leq \\sum_{i=0}^{k-1} C_{N}^i = \\frac{1}{6}N^3 + \\frac{5}{6}N + 1 $$ 如果存在 break point, growth function is polynomial function in N : $\\sum_{i=0}^{k-1} C_N^i$, whose maximum power is $N^{k-1}$ ($d_{VC}$).\n如果没有break point, growth function is $2^N$，就不能保证它是关于N的多项式（最高次幂是 $d_{VC}$，能被e的负幂次抵消），也就不能保证 Bad events 的概率上界足够小，那样就不能通过 Ein 学到Eout。\nProof that $m_{\\mathcal H}(N)$ can replace $M$ Replace $M$ with $m_{\\mathcal H}(N)$ to narrow the upper bound. If M hypotheses are not overlapping, the probability upper bound is very lose.\n$$ \\begin{aligned} \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026 \u003e \\varepsilon] \\leq 2\\ M\\ e^{-2\\varepsilon^2 N} \\quad \\text{(Union bound)} \\\\ \u0026 \\Downarrow \\text{(Not quite)} \\\\ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026 \u003e \\varepsilon] \\leq 2\\ m_{\\mathcal H}(N)\\ e^{-2\\varepsilon^2 N} \\\\ \\end{aligned} $$ 长方形中的每个点代表一个样本数据集。 The \u0026ldquo;flower\u0026rdquo; represents \u0026ldquo;bad events\u0026rdquo; probability (Ein 和 Eout 相差超过$\\varepsilon$ 的概率).\nFig (a) 表示的是一个假设的“验证”。While Learning needs to pick the best hypothesis from M hypotheses based on \u0026ldquo;bad events\u0026rdquo; probability.\nIn Fig (b), every hypothesis is not overlapping each other. So their bad events probabilities are taking the whole space, resulting the upper bound maybe very large. 所以不可能通过 $E_{in}$ 学到 $E_{out}$，因为大部分情况下，坏事件发生，它们相差超过了$\\varepsilon$。\nIn Fig （c）, many hypotheses overlap together in training data. VC bound 比较小，Ein与Eout 相差超过 $\\varepsilon$ 的概率不大。\nWhat to do about $E_{out}$?\nTake another set of samples, called $E_{in}'(h)$. $E_{in}$ 与 $E_{in}'$ 独立同分布，所以 Ein and Ein\u0026rsquo; are both following Eout, Ein and Ein\u0026rsquo; are following each other.\n$$ \\begin{aligned} \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026 \u003e \\varepsilon] \\leq 2\\ m_{\\mathcal H}(N)\\ e^{-2\\varepsilon^2 N} \\\\ \u0026 \\Downarrow \\text{(but rather)} \\\\ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026 \u003e \\varepsilon] \\leq 4\\ m_{\\mathcal H}(2N)\\ e^{-\\frac{1}{8} \\varepsilon^2 N} \\quad \\text{(VC bound)}\\\\ \\end{aligned} $$(Lec7)\nThe Vapnik-Chervonenkis Inequality: $$ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u003e \\varepsilon] \\leq 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} $$ Vapnik-Chervonenkis Inequality 对任何有break point的 hypothesis 都成立。\n相较于直接用$m_{\\mathcal H}(N)$替换M，VC维其实 make the bound worser, larger: $2 m_{\\mathcal H}(N) e^{-2\\varepsilon^2 N} \u003c 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N}$\nVC dimension VC维 $d_{VC}$ 是某假设集 $\\mathcal H$ 最多能完全（二分）分开的点的个数。\n$$ d_{VC}(\\mathcal H) = k-1 $$完全分开意味着，假设集$\\mathcal H$能分割开由N个点组成的任意的颜色配置：$m_{\\mathcal H}(N) = 2^N$\n有了 $d_{VC}$ 之后：\n$N \\leq d_{VC}(\\mathcal H) \\Rightarrow \\mathcal H$ can shatter N points\n$k \\geq d_{VC}(\\mathcal H) \\Rightarrow k$ is a break point for $\\mathcal H$\n$m_{\\mathcal H}(N)$ 的最高维度是 $d_{VC}$\nGrowth function 用 break point k 表示：$m_{\\mathcal H}(N) \\leq \\sum_{i=0}^{k-1} \\begin{pmatrix} N \\\\ i \\end{pmatrix}$\nGrowth function 用 VC dimension $d_{VC}$ 表示：$m_{\\mathcal H}(N) \\leq \\underbrace{ \\sum_{i=0}^{d_{VC}} \\begin{pmatrix} N \\\\ i \\end{pmatrix} }_{\\text{minimum power is} N^{d_{VC}}}$\n对于不同类型的 hypothesis set:\n$\\mathcal H$ is positive rays: $d_{VC}=1$ $\\mathcal H$ is 2D perceptrons: $d_{VC}=3$ $\\mathcal H$ is convex sets: $d_{VC}=\\infin$ VC dimension and learning 因为 $d_{VC}(\\mathcal H)$ 是有限的, 就可以从假设集 $\\mathcal H$ 中学习到足够接近未知目标函数 $f$ 的假设g。（保证了Ein与Eout相差很多的概率不大） $d_{VC}$ 独立于 learing algorithm, input distribution, target function. 不管它们是什么样，$d_{VC}$只由假设集决定。 VC dimension of perceptrons $d_{VC} = d+1$, d is the working dimension of perceptrons (证明见书) 例如：2D perceptron 位于 2D plane，所以$d_{VC}=3$; 而 3D perceptron位于3D space，所以$d_{VC}=4$ $d_{VC} \\ (d+1)$ 是权重参数的个数 ($w_0, w_1, \\cdots, w_d$)。1 代表bias项，d代表每个样本点的维度数。 Interpreting the VC dimension VC维是自由度 (Degrees of freedom)\n不同的weights就是不同的 hypothesis。Every dimension of $d_{VC}$ is a tunable parameter, and parameters create degrees of freedom.\nNumber of parameters analog degrees of freedom; $d_{VC}$ equivalent \u0026ldquo;binary\u0026rdquo; degrees of freedom.\nThe usual suspects\npositive ray: $d_{VC}=1$，一个参数对应一个边界 positive interval: $d_{VC}=2$，两个参数对应两个边界，确定一个hypothesis Not just parameters\nParameters 在实际情况中可能不是自由度。 每个 perceptron 有 2 个 parameters ($w_0, w_1$ 对应 $b$ 和$x$). So 4 perceptron have 8 parameters those have an impact on a hypothesis, but the $d_{VC}$ maybe not 8. $d_{VC}$ measures the effective number of parameters Number of data points needed 经验法则： $N \\geq 10 d_{VC}$\nVC维的两个量化参数: $\\varepsilon, \\delta$:\n$$ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u003e \\varepsilon] \\leq \\underbrace{ 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} }_\\delta $$$\\delta$是 N 的函数：$N^{d_{VC}} e^{-N}$ （$m_{\\mathcal H}(2N)$是N的多项式）\n横坐标是 N，纵坐标是概率上界 $\\delta$，每条曲线代表不同的VC维:\n$d_{VC}$越大，概率会先上升得越高，之后负指数take the control，突然急剧减小。因为这是概率，所以只考虑小于1 ($10^0$ 以下)的部分，概率越小越好。\n在同一概率下，增加VC维，N的值也呈线性增加。更大的VC维（要调节更多的参数）需要更多的样本点。为了达到很小的 Bad events概率，就需要更多的样本点。\nRule of thumb:(经验) $N \\geq 10 d_{VC}$\nRearranging things For VC inequality，把 Bad events 发生的概率称为 $\\delta$\n$$ \\mathbb P [ |E_{out}(g) - E_{in}(g)| \u003e \\varepsilon] \\leq \\underbrace{ 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} }_\\delta $$从 $\\delta$ 中推出 ε :\n$$ \\varepsilon = \\underbrace{ \\sqrt{\\frac{8}{N} ln \\frac{4 m_{\\mathcal H} (2N)}{\\delta}} }_\\Omega $$$\\Omega$是$N, \\mathcal H, \\delta$ 的函数。\nGood events 表示为：$|E_{out} - E_{in}|\\leq \\Omega(N,\\mathcal H, \\delta)$，它的概率是 $P(good) \\geq 1-\\delta$。\nGeneralization bound 通常 Eout 比 Ein 大，所以可以去掉绝对值，那么 Good events 就是: $E_{out}-E_{in} \\leq \\Omega(N,\\mathcal H, \\delta)$，称为 Generalization error。\n然后移项得到 $E_{out}$:\n$$ E_{out} \\leq \\underbrace{ E_{in} + \\underbrace{\\Omega(N,\\mathcal H, \\delta)}_{\\text{Generalization error}} }_{\\text{Generalization bound}} $$$E_{in}$ 从训练样本中得知，再根据以上关系，就可得知Eout。\n","date":"2021-12-12T20:01:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec6_7_generalization_dvc/","title":"watch: AML 06 | Genearalization-dVC"},{"content":"Video 6 Linear Models 9-29-2021\nOutline\nInput representation Linear Classificatin Linear Regression Nonlinear Transformation Review of Lecture 2\nLearning is feasible in a probabilistic sense.\nRed marble frequence $\\nu$ in the bin is unknow → $E_{out}$ is unknown → $E_{in}$: red marble frequence in the sample $\\mu$ → 最佳假设 g 使 $E_{in}$ 和 $E_{out}$ 最接近，这时Bad events 的概率为: Ein 和 Eout 相差超过 $\\epsilon$ 的概率 $|E_{in}(g) - E_{out}(g)| \u003e \\epsilon$\n因为 g 肯定是假设集 H 中 M 个假设的其中之一，每个都有可能:\n$$ |E_{in}(h_2) - E_{out}(h_2)| \u003e \\epsilon\\ \\textbf{ or } |E_{in}(h_2) - E_{out}(h_2)| \u003e \\epsilon\\ \\textbf{ or }\\ \\cdots |E_{in}(h_M) - E_{out}(h_M)| \u003e \\epsilon $$也就是 M 个假设对应的Bad events发生概率之和（各假设之间无overlapping，也就是最坏的情况，但通常各假设间有相关性），把M加到Hoeffding Inequality右侧，称为Union Bound：\n$$ P[ |E_{in}(g)-E_{out}(g)|\u003e \\epsilon ] \\leq 2M e^{-2 \\epsilon^2 N} $$如果样本数量N足够大，Bad event 的概率就会变小.\nLinear model Weights are linear related.\nDigits pictures:\nraw input: $\\mathbf{x} = (x_0, x_1, \\cdots, x_{256})$ (x0是bias)\nlinear model: $(w_0, w_1, \\cdots, w_{256})$ (inputs constribution,w0=1)\nFeatures: Extracted useful information （257维太高,线性模型不太行）\nIntensity and symmetry $\\mathbf x=(x_0, x_1, x_2)$， $x_0=$bias\nlinear model: $(w_0, w_1, w_2)$, $w_0=1$\nPLA is not so smart. It focus on one misclassified point and update the weight, then the in-sample error maybe better or worse. There are a lot of flactration during iterations. 错过了最小误差，就回不去了。\n$E_{out}$ 是验证集上的误差，error of out-of-sample follows $E_{in}$\nPocket: Keep the best weights (in pocket). Replace it when finding better in the future.\nLinear Classification Outputs of a linear model are binary\n$$ h(\\mathbf x) = \\operatorname{sign} \\left( \\sum_{i=0}^d w_i x_i \\right) : \\{ +1,-1\\} $$ +1 or -1 (Approve or Deny)\nLinear Regression Outputs of a linear model are real-valued\n$$ h(\\mathbf x) = \\sum_{i=0}^d w_i x_i = \\mathbf{w}^T \\mathbf{x} \\quad \\text{(w0 for bias x0)} $$ 不再传给sign函数，判断$\\pm 1$分类\ndata set: $\\rm (\\pmb x_1, y_1), (\\pmb x_2, y_2),\\cdots (\\pmb x_N, y_N)$\n用线性回归去复制 data set，然后预测未来x的y。\n用假设 $h(\\mathbf x) = \\mathbf w^T \\mathbf x$ 近似未知目标函数 $f(\\mathbf x)$:\n$$ \\left(h(\\mathbf x) - f(\\mathbf x) \\right)^2 $$(Square error will make solving linear regression problem easily one-shot.)\nIn-sample error:\n一个特征是点到直线距离，两个特征是点到超平面距离。\n$$ E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^N (h(\\mathbf x_n) - y_n)^2 $$In-sample error 是权重 $\\mathbf w$ 的函数 （$\\mathbf x$和y都是固定的训练样本,只有$\\mathbf w$是变量），线性回归的目标是找到使 In-sample error 最小的$\\mathbf w$:\n$$ \\begin{aligned} E_{in}(\\mathbf w) \u0026= \\frac{1}{N} \\sum_{n=1}^N (\\mathbf w^T \\mathbf x_n - y_n)^2 \\\\\\ \u0026= \\frac{1}{N} \\| \\mathbf{Xw} - \\mathbf y \\|^2 \\end{aligned} $$（把求和变成矩阵，方便求导找最值）其中：\n$$ \\mathbf X= \\begin{bmatrix} \\mathbf x_1^T \\\\\\ \\mathbf x_2^T \\\\\\ \\vdots \\\\\\ \\mathbf x_N^T \\end{bmatrix}, \\mathbf y= \\begin{bmatrix} y_1^T \\\\\\ y_2^T \\\\\\ \\vdots \\\\\\ y_N^T \\end{bmatrix} $$ 求 $E_{in}$ 的最小值：对 $\\mathbf w$ 求导，并令其等于0:\n$$ \\begin{aligned} E_{in}'(\\mathbf w) \u0026= 0 \\\\\\ \\frac{2}{N} \\mathbf X^T (\\mathbf {Xw} -y) \u0026= 0 \\\\\\ \\mathbf X^T \\mathbf {Xw} \u0026= \\mathbf X^T y \\\\\\ \\mathbf w \u0026= \\mathbf X^{\\dagger} y \u0026 \\text{where } \\mathbf X^\\dagger = (\\bf X^T X)^{-1} X^T \\end{aligned} $$Perceptron is more similar to the learning process that you\u0026rsquo;re just trying to learn something from one iteration to the other iteration. Here it\u0026rsquo;s not iterative. Linear regression is a kind of one-shot learner that learns one iteration.\n$\\mathbf X^\\dagger$ is the pseudo-inverse of $\\mathbf X$:\n$$ \\underbrace{ \\begin{pmatrix} \\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{10} \u0026amp; \\cdots x_{N0} \\ x_{01} \u0026amp; x_{11} \u0026amp; \\cdots x_{N1} \\ \\vdots \\ x_{0d} \u0026amp; x_{1d} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }_{(d+1)\\times N}\n\\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{01} \u0026amp; \\cdots x_{0d} \\ x_{10} \u0026amp; x_{11} \u0026amp; \\cdots x_{1d} \\ \\vdots \\ x_{N0} \u0026amp; x_{N1} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }_{N\\times (d+1)}\n\\end{pmatrix}^{-1}\n\\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{10} \u0026amp; \\cdots x_{N0} \\ x_{01} \u0026amp; x_{11} \u0026amp; \\cdots x_{N1} \\ \\vdots \\ x_{0d} \u0026amp; x_{1d} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }{(d+1)\\times N} }{(d+1)\\times N} $$\n$\\mathbf w = \\mathbf X^\\dagger y = \\underbrace{[w_0\\ w_1\\ \\cdots w_{d}]}_{(d+1)\\times 1}$\nLinear regression algorithm 构建 the data matrix $\\mathbf X$ and the vector y from the data set $(\\mathbf X_1, y_1), \\cdots, (\\mathbf X_N, y_N)$\n$$ \\mathbf X = \\begin{bmatrix} \\cdots \\mathbf x_1^T \\cdots \\\\ \\cdots \\mathbf x_2^T \\cdots \\\\ \\vdots \\\\ \\cdots \\mathbf x_N^T \\cdots \\\\ \\end{bmatrix} , y= \\begin{bmatrix} y_1^T \\\\ y_2^T \\\\ \\vdots \\\\ y_N^T \\end{bmatrix} $$ 计算伪逆矩阵 $\\mathbf X^\\dagger = (\\bf X^T X)^{-1} X^T$\n返回 $\\mathbf w = \\mathbf X^\\dagger y$\nLinear regression for classification 利用线性回归一次性解出 $\\mathbf w$，将其作为perception的初值，再迭代。 Linear regression 学习一个实值函数 $y=f(x)$ 二分类函数的 $\\pm 1$ 也是实数 使用linear regression “训练”（学习）到最佳$\\mathbf w$（使$E_{in}$(平方误差)最小）$\\mathbf w^T \\mathbf x_n \\approx y_n = \\pm 1$ 将这个$\\mathbf w$ 作为perceptron的初始值开始训练，随机初始化w可能迭代很多次也不会收敛。 Linear regression boundary 线性回归 one-shot 解出的w 对应一条直线. 回归是为了使整体的 $E_{in}$ (点到超平面的距离) 最小，当两类数据分布不均匀时，超平面会偏移“分类边界”。in-smaple Error (平方误差)不是Classification error。再用 perception 优化分类结果。 Video 9\nNonlinear transformation Use $\\Phi$ to transform the non-linear input space $\\mathcal X$ to a linear space $\\mathcal Z$ (where there is linear relation between $\\mathbf w$s)\nAny point $\\mathbf x \\overset{\\Phi}{\\rightarrow} \\mathbf z$ preserves the linearity, so that points are linearly separable.\n$g(\\mathbf x) = \\tilde g(\\Phi(\\mathbf x)) = \\rm sign(\\tilde \\mathbf{w}^T \\Phi(\\mathbf x))$\nTransformation:\n$$ \\begin{aligned} \\mathbf x = (x_0, x_1, \\cdots, x_d)\\ \u0026amp;\\overset{\\Phi}{\\rightarrow} \\mathbf z = (z_0, z_1, \\cdots, z_{\\tilde d}) \u0026amp; \\text{维度可以不同,$x_0$是bias} \\\n\\mathbf{x_1, x_2, \\cdots, x_N}\\ \u0026amp;\\overset{\\Phi}{\\rightarrow} \\mathbf{z_1, z_2, \\cdots, z_N} \u0026amp; \\text{n个点都做变换}\\\ny_1, y_2, \\cdots, y_N \\ \u0026amp;\\overset{\\Phi}{\\rightarrow} y_1, y_2, \\cdots, y_N \u0026amp; \\text{标签不变} \\\n\\text{No weights in } \\mathcal X \u0026amp; \\qquad \\widetilde \\mathbf w =(w_0, w_1,\\cdots, w_{\\tilde d}) \u0026amp; \\text{z空间中建立线性模型} \\\ng(\\mathbf x) \u0026amp;= \\rm sign (\\tilde \\mathbf w^T \\Phi(\\mathbf x)) \\end{aligned} $$\n","date":"2021-12-12T20:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec3_linear_model/","title":"watch: AML 03 | Linear Model"},{"content":"Video 16 Classification 2021-12-06\nsupervised learning practical diagram\nK-means 对三螺线数据训练，分类结果不好，因为它是用于聚类的\nFuzzy C Means 是另一种改进的聚类算法，分类效果也不好\nDecision Tree 分类器对三螺线的训练样本分类效果很好\nK nearest neighbor 是非常强大的 lazy learner\nMultilayer neural network 强大\nPerformance evaluation 分类器预测类别标签有多准确？选择哪个分类器（模型）更合适？ Remember: The data have to be used both for training and testing. More training data → better generalization. More test data → better estimation for the classification error probability. Do not evaluate performance on training data → the conclusion would be optimistically biased. (否则会偏向训练集) Methods for estimating a classifier\u0026rsquo;s accuracy: Holdout method (reserve 2/3 for training and 1/3 for testing) random subsampling (iterative holdout) Cross-validation (partition the data into k folders Stratified oversampling and undersampling (保持类别比例) Bootstrap (sampling with replacement) Comparing classifiers: Confidence intervals Cost-benefit analysis and ROC Curves Once evaluation is finished, all the available data can be used to train the final classifier. (知道了最佳参数后，再用全部的数据训练最佳假设) Hold out method Given data is randomly partitioned into two independent sets 比如：2/3 作为Training set 去构建模型，1/3作为Test set 去估计准确率 Random sampling: It is a variation of holdout method.\nRepeat the method k times, accuracy is estimated as average of obtained accuracies. Confusion matrix Represents the number of correct and incorrect predictions made by the classification model in comparison with the real outcomes (actual class).\nTP or True positive: # of tuples in class positive that were labeled by the classifier as class positive. FN or False negative: # of tuples in class positive that were labeled by the classifier as class negative FP or False positive: # of tuples in class negative that were labeled by the classifier as class positive. TN or True negative # of tuples in class negative that were labeled by the classifier as class negative. Evaluation measures\nMeasure Formula Accuracy, recognition rate (TP+TN)/all Error rate, misclassification rate (FP+FN)/all Sensitivity, true positive rate, recall TP/(TP+FN) Specificity, true negative rate TN/(TN+FP) Precision TP/(TP+FP) F, F1,F-score, Harmonic mean of precision and recall $\\frac{2 \\times \\rm Precision \\times recall}{\\rm Precision + recall}$ $F_\\beta$ where $\\beta$ is a none negative real number $\\frac{(1+\\beta)^2 \\times \\rm Precision \\times recall}{\\beta^2 \\times \\rm Precision +recall}$ Accuracy/recognition rate: the proportion of the total number of predictions that were correct. Error rate: 1- accuracy Precision: what % of tuples that the classifier labeled as positive are actually positive (查准率) Recall: what % of positive tuples did the classifier label as positive? (查全率) 当数据几乎是均匀分布时：准确性可以成为一个很好的评估指标\n比如100个人，99个没患癌，1个是癌症患者，但是模型结果是100个人都是健康，准确率99%，但它并不是可靠的模型。\nImbalanced data: There is an important class which is rare. e.g. cancerous patient Classifier may ignore the small class! Accuracy is not a good measurement as it does not consider FN rate that is so important in imbalanced data. In this case, classifier evaluation measures such sensitivity (or recall), specificity, precision, F-measure are better suited. Evaluation 也可以关注其他的指标：\nSpeed, Robustness, Scalability, Interpretability\nReceiving Operating Characteristic (ROC) Represent a relation between sensitivity and specificity for a given classifier.\nThe area under the curve is the measure of the accuracy of the classifier. The perfect accuracy is equal to one. The closer to red line, the less accurate model 如果模型的准确率显著低于红线，是不能接受的(有错)。ROC 曲线上升越快，越接近1，越好 (Learner 1最好)\nIt can be used for visual comparison of classification models.\nROC space: Two dimensional: FP rate on X axis → FPR=FP/(TN+FP) TP rate on Y axis → TPR=TP/(TP+FN) （灵敏度） FPR=1-SPC （= 1- 特异度） Model Selection Criteria Model selection criteria is always based on a compromise between the complexity of the model and its prediction accuracy on the training data\nGiven a dataset, basically we are looking for the simplest model that attains highest accuracy.\nModel 1 Model 2 Model 3 Complexity ✓✓ ✓ x (overfit) Training error xx ✓ ✓✓ Overall - ✓ - Ensemble system - Strategies \u0026amp; components 合奏系统\n每次抽取不同的样本(子集)，训练多个模型，然后聚合（aggregation）起来，误差可能更小\n也可以训练不同种类的分类器：感知机，DT,kNN,SVM\u0026hellip;\nEnsemble 系统有两Key Component: 分类算法（注意训练集样本的多样性）和融合方法（简单：多数票）\nEnsemble 适合用于很大容量数据，也可以用于很小容量数据。\nLarge volume data:\nSmall size data:\n数据很少，用复杂的模型可能导致过拟合，所以第一次使用比较弱的感知机，有3个点分错了，增加它们的权重，使它们更可能被抽取到作为下一次的训练样本。第二次分类后，再强调分错的2个蓝点。第三次分类，就只有1个红点分错了。Ensemble 使得模型不复杂，更准确\n其他优势：处理复杂的决策边界，非线性情况，实时\u0026hellip;\n","date":"2021-12-12T19:58:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/aml_classification/","title":"watch: AML | Classification"},{"content":"Outline\nExample of machine Learning Components of Learning Perceptron Types of learning Puzzle Machine Learning Essence A pattern exists We cannot pin it down mathematically we have data on it. Perceptron $h(\\mathbf x) = \\operatorname{sign} \\left( \\sum_{i=0}^d w_i x_i \\right) = \\operatorname{sign} (\\mathbf w^T \\mathbf x)$ PLA steps: Given the training set: $(\\mathbf x_1, \\mathbf y_1),(\\mathbf x_2, \\mathbf y_2), \\cdots, (\\mathbf x_N, \\mathbf y_N)$ pick a misclassified point: $sign (\\mathbf w^T \\mathbf x_n) \\neq y$ update the weight vector: $\\mathbf w \\leftarrow \\mathbf w + y_n \\mathbf x_n$ Types of learning Supervised learning: input \u0026ldquo;correct output\u0026rdquo;. Unsupervised learning: no \u0026ldquo;correct output\u0026rdquo; input. Reinforcement learning: introduce the grade of output. 这门课在证明一件事：通过fit数据，就可以”learn“到未知目标函数。\n","date":"2021-12-12T19:30:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec1_learning_problem/","title":"watch: AML 01 | Learning Problem"},{"content":"Video 15 Clustering 2021-11-22\nOutline:\nClustering K-means K nearest neighbor Cluster Cluster is a set of data objects that similar to one another within the same group dissimilar to the objects in other groups High quality clusters: High intra-class similarity; Low inter-class similarity Cluster analysis an unsupervised learning (unlabeled) 给定一组数据对象 找到数据对象之间的相似性 把相似的数据对象归到 clusters 典型应用:\nAs a stand-alone tool to get insight into data distribution As a preprocessing step for other algorithms (classifier, regressor) 聚类方法的质量的相关因素：\nthe similarity measure used by the method its implementation its ability to discover some or all of the hidden patterns 聚类分析的考虑因素\nPartitioning criteria Single level vs. hierarchical partitioning (often, multi-level hierarchical partitioning is desirable) Separation of clusters Exclusive (e.g., one customer belongs to only one region) vs. non-exclusive (e.g., one document may belong to more than one class) 是否专属于1类 Similarity measure Distance-based (e.g., Euclidian, road network, vector) vs. connectivity-based (e.g., density or contiguity) Clustering space Full space (often when low dimensional) vs. subspaces (often in high-dimensional clustering) 聚类的挑战和要求\nQuality Ability to deal with different type of attributes (不同属性) Discovery of clusters with arbitrary shape (任意形状) Ability to deal with noisy data (噪声) Interpretability and usability Constraint based clustering Scalability Constraint based clustering High dimensionality Incremental clustering and insensitivity to input order Similarity measure 对于两个样本点的第i个维度: $x_i$和$y_i$，两者的相似性可以用一个距离函数表达：$\\rm d(x_i, y_i)$ Similarity measure are usually different based on type of data: interval-scaled, boolean, categorical, ordinal ratio, and vector variables. 距离种类： Euclidean: $\\sqrt{\\sum_{i=1}^{k}\\left(x_{i}-y_{i}\\right)^{2}}$ 两点所有属性间的距离 Manhattan: $\\sum_{i=1}^{k}\\left|x_{i}-y_{i}\\right|$ Minkowski: $\\left(\\sum_{i=1}^{k}\\left(\\left|x_{i}-y_i\\right|\\right)^{2}\\right)^{1 / q}$ Major approaches Partitioning approaches (分区): They create various partitions and then evaluate them by some criterion e.g., minimizing the sum of square errors typical methods: k-means, k-medoids, CLARANS\nHierarchical approaches (分层): They create a hierarchical decomposition of the set of data (or objects) using some criterion typical methods: Diana, Agnes, BIRCH, CHAMELEON\nDensity-based approaches: They are based on connectivity and density functions typical methods: DBSACN, OPTICS, DenClue\nGrid-based approaches: They are based on a multiple-level granularity structure typical methods: STING, WaveCluster, CLIQUE\nModel-based approaches: A model is hypothesized for each of the clusters and then aim to find the best fit of that model to each other typical methods: EM, SOM, COBWEB\nFrequent pattern-based: They are based on the analysis of frequent patterns typical methods: p-Cluster\netc\u0026hellip;\nPartitioning method K-means 可视化：Visualizing K-Means Clustering\nK nearest neighbors supervised, efficient, clustering, classification and regression learning algorithm ","date":"2021-12-11T14:44:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/aml_clustering/","title":"watch: AML | Clustering"},{"content":"Video 9 - Error and noise 10-18-2021\nOutline\nError measures Noisy targets Preamble to the theory Review of Lec 3\nLinear Models\nUsing “signal” to classify and regress\nsignal:\n$$ \\sum_{i=0}^d w_i x_i = \\mathbf{w^T x} $$ Linear Classification: $h(\\mathbf x) = \\rm sign(\\mathbf{w^T x})$ (把信号传入threshold, PLA, Pocket)\nLinear Regression: $h(\\mathbf x) = \\mathbf{w^T x}$ (不把信号传入threshold, one-shot learning)\n$\\mathbf w = \\mathbf{(x^T x)^{-1} x^T} y$\nError measures Quantify the dissimilarity between the output of hypothesis $h$ and the output of the unknown target function $f$.\nAlmost all error measures are pointwise\nCompute $h$ and $f$ on individual points $\\mathbf x$ using a pointwise error $e(h(\\mathbf x), f(\\mathbf x))$:\nBinary error: $e(h(\\mathbf x), f(\\mathbf x))= [\\![ h(\\mathbf x) \\neq f(\\mathbf x) ]\\!]$ （不相等error=1, 相等error=0） (Classification)\nSquared error: $e(h(\\mathbf x), f(\\mathbf x)) = (h(\\mathbf x) - f(\\mathbf x))^2$ (真实距离) (Regression)\nIn-sample error: $h(x)$ 与 $f(x)$ 在各样本点上的差异\n$$ E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^N e(h(\\mathbf x_n), f(\\mathbf x_n)) $$Out-of-sample error: $h(x)$ 与 $f(x)$ 在空间所有点上的偏差的期望\n$$ E_{out}(h) = \\mathbb E_x [e(h(\\mathbf x), f(\\mathbf x))] $$ How to choose the error measure\nFalse accept and False reject\nconfusion matrix (混淆矩阵):\n$$ \\begin{array}{c|lcr} \u0026 \\qquad f (\\text{unknown}) \u0026 \\\\ h\u0026 +1 \u0026 -1 \\\\ \\hline +1 \u0026 \\text{no error} \u0026 \\text{false accept} \\\\ -1 \u0026 \\text{false reject} \u0026 \\text{no error} \\\\ \\end{array} $$The error measure is pretty much related to the kind of application with different penalty.\nNoisy targets 确定的目标分布 $f(\\mathbf x) = \\mathbb E(y|\\mathbf x)$ + 噪声 $y-f(\\mathbf x)$\n有时相同的输入对应不同的标签，所以潜在关系不是一个\u0026quot;函数\u0026quot; $y=f(\\mathbf x)$，而是一个分布 $P(y|\\mathbf x)$\n$\\mathbf x$ 按照某种未知的分布 $P(\\mathbf x)$ 从空间$\\mathcal X$ 中抽取出来。标签 $y$ 服从分布 $P(y|\\mathbf x)$。所以输入 $(\\mathbf x,y)$ 是由联合分布 $P(\\mathbf x) P(y|\\mathbf x) = P(\\mathbf x,y)$ 产生。\nDetermistic target 是当 P(y|x)=0 的特殊的noisy target, 那时噪声=0，也就是 $y=f(\\mathbf x)$\nPreamble to the theory\nLearning is feasible in a probabilitstic sence: $E_{out}(g) \\approx E_{in}(g)$ We need $g\\approx f$, which means $E_{out}(g) \\approx 0$ $E_{out}(g) \\approx E_{in}(g)$ (Hoeffding Inequality) $E_{in}(g) \\approx 0$ (PLA, Pocket, Linear classification/regression) ","date":"2021-12-05T19:19:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec4_error_and_noise/","title":"watch: AML 04 | Error and Noise"},{"content":"Selecting dimensionality reduction with Pipeline and GridSearchCV (在sklearn网站搜索\u0026quot;gridsearchcv\u0026quot;发现的)\nPCA降维实例(GridSearchCV求最优参)\n若不进行降维，速度慢，准确度低(非常低不能接受)\n","date":"2021-11-22T03:24:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/projecttips/%E9%99%8D%E7%BB%B4gridsearch/","title":"DR-GridSearch"},{"content":"Pytorch 保存模型与加载模型 保存和加载模型\n保存模型与加载\n使用的三个核心功能：\ntorch.save 将序列化对象保存到磁盘。此函数调用pickle模块把（模型、tensor、字典等）对象序列化。 torch.load 函数调用pickle的unpickling功能把文件反序列化到内存。 torch.nn.Module.load_state_dict 调用反序列化函数state_dict加载模型的参数字典。 1. 状态字典 state_dict 模型的state_dict包括：各层网络的可学习参数（权重和偏置），优化器的state_dict包括优化器的状态和超参数。 模型torch.nn.Module的可学习参数，用model.parameters()访问 2. 两种方式： 只保存网络中的参数（速度快，占空间少，推荐）\n1 2 3 4 5 6 7 #保存参数 torch.save(net1.state_dict(), \u0026#39;net_params.pt\u0026#39;) #或\u0026#39;.pth\u0026#39;,或\u0026#39;pkl\u0026#39; #加载 model = ModelClass(*args, **kwargs) #先实例化一个模型对象 model = model.load_state_dict(torch.load(PATH)) #把文件反序列化成字典对象，把参数传给模型 model.eval() #设置dropuout 和 batch normalization层为评估模式，否则可能导致模型推断结果不一致。 保存整个网络的结构和参数与加载：\n1 2 3 4 5 6 #保存 torch.save(net1, \u0026#39;net.pkl\u0026#39;) #加载 newmodel = torch.load(PATH) #不需重构模型，直接load newmodel.eval() 3. 保存和加载Checkpoint 用于推理/继续训练 保存训练状态：\n1 2 3 4 5 6 torch.save({\u0026#39;epoch\u0026#39;: epoch+1, #保存当前的迭代次数 \u0026#39;model_state_dict\u0026#39;: model.state_dict(), #保存模型参数 \u0026#39;optimizer_state_dict\u0026#39;: optimizer.state_dict(), #保存优化器参数 \u0026#39;loss\u0026#39;: loss, #其余一些想保持的参数都可以添加进来 ..., }, PATH) #后缀可以用 \u0026#39;.pth.tar\u0026#39;或 \u0026#39;.pth\u0026#39; 加载：\n1 2 3 4 5 6 7 8 9 10 11 12 model = ModelClass(*args, **kwargs) optimizer = OptimizerClass(*args, **kwargs) #** checkpoint = torch.load(PATH) model.load_state_dict(checkpoint[\u0026#39;model_state_dict\u0026#39;]) optimizer.load_state_dict(checkpoint[\u0026#39;optimizer_state_dict\u0026#39;]) epoch = checkpoint[\u0026#39;epoch\u0026#39;] loss = checkpoint[\u0026#39;loss\u0026#39;] model.eval() # - or - model.train() 。。。。。部分内容需公众号验证\n4. 在一个文件中保存多个模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 保存 torch.save({ \u0026#39;modelA_state_dict\u0026#39;: modelA.state_dict(), \u0026#39;modelB_state_dict\u0026#39;: modelB.state_dict(), \u0026#39;optimizerA_state_dict\u0026#39;: optimizerA.state_dict(), \u0026#39;optimizerB_state_dict\u0026#39;: optimizerB.state_dict(), ... }, PATH) # 加载 modelA = TheModelAClass(*args, **kwargs) modelB = TheModelBClass(*args, **kwargs) optimizerA = TheOptimizerAClass(*args, **kwargs) optimizerB = TheOptimizerBClass(*args, **kwargs) checkpoint = torch.load(PATH) modelA.load_state_dict(checkpoint[\u0026#39;modelA_state_dict\u0026#39;]) modelB.load_state_dict(checkpoint[\u0026#39;modelB_state_dict\u0026#39;]) optimizerA.load_state_dict(checkpoint[\u0026#39;optimizerA_state_dict\u0026#39;]) optimizerB.load_state_dict(checkpoint[\u0026#39;optimizerB_state_dict\u0026#39;]) modelA.eval() modelB.eval() # - or - modelA.train() modelB.train() 当保存一个模型由多个torch.nn.Modules组成时，例如GAN(对抗生成网络)、sequence-to-sequence (序列到序列模型), 或者是多个模 型融合, 可以采用与保存常规检查点相同的方法。换句话说，保存每个模型的 state_dict 的字典和相对应的优化器。如前所述，可以通 过简单地将它们附加到字典的方式来保存任何其他项目，这样有助于恢复训练。\nPyTorch 中常见的保存 checkpoint 是使用 .tar 文件扩展名。\n要加载项目，首先需要初始化模型和优化器，然后使用torch.load()来加载本地字典。这里，你可以非常容易的通过简单查询字典来访问你所保存的项目。\n请记住在运行推理之前，务必调用model.eval()去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。 如果你想要恢复训练，请调用model.train()以确保这些层处于训练模式。\n5. 使用在不同模型参数下的热启动模式 1 2 3 4 5 保存 torch.save(modelA.state_dict(), PATH) 加载 modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False) 在迁移学习或训练新的复杂模型时，部分加载模型或加载部分模型是常见的情况。利用训练好的参数，有助于热启动训练过程，并希望帮助你的模型比从头开始训练能够更快地收敛。\n无论是从缺少某些键的 state_dict 加载还是从键的数目多于加载模型的 state_dict , 都可以通过在load_state_dict()函数中将strict参数设置为 False 来忽略非匹配键的函数。\n如果要将参数从一个层加载到另一个层，但是某些键不匹配，主要修改正在加载的 state_dict 中的参数键的名称以匹配要在加载到模型中的键即可。\n6.通过设备保存/加载模型 6.1 保存到 CPU、加载到 CPU 1 2 3 4 5 6 保存 torch.save(model.state_dict(), PATH) 加载 device = torch.device(\u0026#39;cpu\u0026#39;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=device)) 当从CPU上加载模型在GPU上训练时, 将torch.device(\u0026lsquo;cpu\u0026rsquo;)传递给torch.load()函数中的map_location参数.在这种情况下，使用 map_location参数将张量下的存储器动态的重新映射到CPU设备。\n6.2 保存到 GPU、加载到 GPU 1 2 3 4 5 6 7 8 保存 torch.save(model.state_dict(), PATH) 加载 device = torch.device(\u0026#34;cuda\u0026#34;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.to(device) # 确保在你提供给模型的任何输入张量上调用input = input.to(device) 当在GPU上训练并把模型保存在GPU，只需要使用model.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))，将初始化的 model 转换为 CUDA 优化模型。另外，请 务必在所有模型输入上使用.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))函数来为模型准备数据。请注意，调用my_tensor.to(device)会在GPU上返回my_tensor的副本。 因此，请记住手动覆盖张量：my_tensor= my_tensor.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))。\n6.3 保存到 CPU，加载到 GPU 1 2 3 4 5 6 7 8 保存 torch.save(model.state_dict(), PATH) 加载 device = torch.device(\u0026#34;cuda\u0026#34;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=\u0026#34;cuda:0\u0026#34;)) # Choose whatever GPU device number you want model.to(device) # 确保在你提供给模型的任何输入张量上调用input = input.to(device) 在CPU上训练好并保存的模型加载到GPU时，将torch.load()函数中的map_location参数设置为cuda:device_id。这会将模型加载到 指定的GPU设备。接下来，请务必调用model.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))将模型的参数张量转换为 CUDA 张量。最后，确保在所有模型输入上使用 .to(torch.device(\u0026lsquo;cuda\u0026rsquo;))函数来为CUDA优化模型。请注意，调用my_tensor.to(device)会在GPU上返回my_tensor的新副本。它不会覆盖my_tensor。 因此， 请手动覆盖张量my_tensor = my_tensor.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))。\n6.4 保存 torch.nn.DataParallel 模型 1 2 3 4 保存 torch.save(model.module.state_dict(), PATH) 加载 # 加载任何你想要的设备 torch.nn.DataParallel是一个模型封装，支持并行GPU使用。要普通保存 DataParallel 模型, 请保存model.module.state_dict()。 这样，你就可以非常灵活地以任何方式加载模型到你想要的设备中。\n","date":"2021-11-22T02:56:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/projecttips/%E4%BF%9D%E5%AD%98%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"memo: PyTorch | Save Model"},{"content":"F1 score: F1 = 2* (precision * recall )/(precision+recall)\nsklearn.metrics.f1_score\nsklearn中 F1-micro 与 F1-macro区别和计算原理\n","date":"2021-11-22T02:16:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/projecttips/%E8%AF%84%E4%BB%B7metrics/","title":"Metrics-sklearn"},{"content":"决策树调参\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import numpy as np import matplotlib.pyplot as plt ## 加载iris数据集 from sklearn.datasets import load_iris iris = load_iris() X,y = iris.data[:,:2], iris.target #只取前2个特征 ## 分割测试集与训练集 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify = y, random_state= 42) ## 适配决策树，并计算准确率 from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score dtc = DecisionTreeClassifier() dtc.fit(X_train, y_train) y_pred = dtc.predict(X_test) accuracy_score(y_test, y_pred) ## 画出决策树示意图 from sklearn.tree import export_graphviz from io import StringIO from IPython.display import Image import pydot def create_png(clf): dot_iris = StringIO() export_graphviz(clf.out_file = dot_iris, feature_name = iris.feature_names[:2], filled = True) graphs = pydot.graph_from_dot_data(dot_iris.getvalue()) return graphs[0].create_png() Image(create_png(dtc)) ## 用GridSearchCV搜索最佳参数 from sklearn.model_selection import GridSearchCV dtc = DecisionTreeClassifier() grid = {\u0026#39;criterion\u0026#39;: [\u0026#39;gini\u0026#39;,\u0026#39;entropy\u0026#39;], \u0026#39;max_depth\u0026#39;: [3,5,7,9,20] } gs = GridSearchCV(dtc, param_grid=grid, cv=5) gs.fit(X_train, y_train) ## 查看网格搜索得到的模型的准确率 accuracy_score(y_test, gs.predict(X_test)) ## 查看搜索结果 gs.cv_results_ gs.best_estimator_ ## 查看网格搜索得到的决策树示意图 Image(create_png(gs.best_estimator_)) ## 探索最大深度对决策树性能的影响 grid=[\u0026#39;max_depth\u0026#39;:range(3,50)] gs2 = GridSearchCV(dtc, param_grid=grid, cv=5) gs2.fit(X_train, y_train) gs2.cv_results_[\u0026#39;mean_test_score\u0026#39;] plt.plot(range(3,50), gs2.cv_results_[\u0026#39;mean_test_score\u0026#39;]) 随机森林-【机器学习】【sklearn】网格搜索GridSearchCV 1. 加载数据 from sklearn.datasets import load_wine\nwine = load_wine() X = wine.data y = wine.target\nfrom sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier()\n2. 网格搜索找出最优参数 param_grid = {\u0026ldquo;n_estimator\u0026rdquo;:np.arange(10,201,10), \u0026ldquo;max_features\u0026rdquo;:np.arange(0.1, 1, 0.1), \u0026ldquo;max_depth\u0026rdquo;: np.arange(3,13), \u0026ldquo;bootstrap\u0026rdquo;: [True, False] } #定义字典，设置参数的可取值\nfrom sklearn.model_selection import GridSearchCV\nmyGrid = GridSearchCV(rfc, param_grid=param_grid, cv=5) #构造网格搜索，内置k折交叉验证\nmyGrid.fit(X,y) #训练\nprint( myGrid.best_params_, #最优参数组合 myGrid.best_score_, myGrid.best_estimator_, #最优模型 myGrid.best_index_ )\n输出最优参数：\n1 2 3 4 5 6 7 8 9 10 11 from sklearn import metrics best_parameters = dict() best_parameters = grid_search.best_estimator_.get_params() for param_name in sorted(parameters.keys()): print \u0026#34;\\t%s: %r\u0026#34; % (param_name, best_parameters[param_name]) pipeline.set_params(clf__alpha = 1e-05, tfidf__use_idf = True, vect__max_df = 0.5, vect__max_features = None) pipeline.fit(X_train, y_train) pred = pipeline.predict(X_test) 3. 使用最优模型做分类 \u0026hellip;.\n","date":"2021-11-21T21:44:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/projecttips/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9gridsearchcv/","title":"GridSearchCV-sklearn"},{"content":"每天一点sklearn之KFold(9.8)\nKFod把数据集划分成K份，返回一个索引生成器，可以用循环遍历它。\nclass sklearn.model_selection.KFold(n_splits='warm', shuffle=False, random_state=None)\nn_splits: K，把数据集划分成k份 shuffle: 打乱顺序再划分 random_state: 相当于随机种子，一般都要和shuffle搭配使用，只有当shuffle=True的时候，才有意义，每次打乱的结果是一样的 1 2 3 4 5 6 from sklearn.model_selection import KFold kf1 = KFold(n_splits=3, shuffle=True) #把数据集划分成3份 for train_index, test_index in kf1.split(xtrain[:20]): print(\u0026#39;In KFold,test_index is:{}\u0026#39;.format(test_index)) #第一份做验证集，剩下两份做训练集 print(\u0026#39;In KFold,train_index is:{}\u0026#39;.format(train_index)) 分层抽样，需要传入label：\n1 2 3 4 5 6 7 8 9 from sklearn.model_selection import StratifiedKFold skf = StratifiedKFold(n_splits=3,shuffle=True, random_state=1) for train_index, test_index in skf.split(xtrain[:20],ytrain[:20]): print(\u0026#39;In StratifiedFold,test_index is:{}\u0026#39;.format(test_index)) #第一份做验证集，剩下两份做训练集 print(ytest[test_index].value_counts()) #各类个数1:1 print(\u0026#39;In StratifiedFold,train_index is:{}\u0026#39;.format(train_index)) print(ytrain[test_index].value_counts()) ","date":"2021-11-21T15:28:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/projecttips/kfold%E5%87%BD%E6%95%B0/","title":"KFold函数-sklearn"},{"content":"RNN Cell 一个线性单元 把输入x的维度（inputSize）变换到 hiddenSize 处理带有时间序列的数据 x2 与 h1 共同决定 h2 t时刻的输入 $x_t \\in \\mathbb R^3$ 经过 RNN cell 转换，变成了一个隐状态变量 $h_t \\in \\mathbb R^5$， 因此RNN cell是一个线性层（矩阵运算）映射到另外一个维度空间，不过这个线性层是共享的。\n$h_t$ 会参与下次 RNN cell 计算 $h_{t+1}$，把之前的信息合并。 在计算 $h_1$ 时，需要输入先验 $h_0$，如果没有先验，就输入与 $h_1, h_2...$ 同维度的零向量。\n遍历数据集 $x_t \\in \\mathbb R^{\\rm input\\_size}$，做线性变换\n$$ W^{\\rm hidden\\ size \\times input\\ size}_{hi} xₜ + b\\_{hi} $$变换到一个 hidden_size×1 的向量；\n上一层的隐变量 $h_{t-1}$ 也进行线性变换 $W_{hh} h_{t-1} + b_{hh}$ 得到一个 hidden_size×1的向量， 它与 $x_t$ 的线性变换输出相加，得到的向量仍为 hidden_size × 1， 再做激活 tanh，算出隐向量 $h_t \\in \\mathbb R^{\\rm hidden\\_size}$\n两个线性运算可以合一起：\n$$ \\begin{aligned} \u0026 W_{hh} h_{t-1} + W_{hi} x_{t} \\\\\\ \u0026 = [W_{hh} \\ \\ W_{hi}]^{\\rm h\\ size \\times (h\\ size + i\\ size)} \\begin{bmatrix} h_{t-1} \\\\\\ x_t \\end{bmatrix}^{\\rm (h\\ size+i\\ size) \\times 1} \\\\\\ \u0026 = h_t^{\\rm h\\ size \\times 1} \\end{aligned} $$$h_t = \\rm tanh(W_{hi} x_t + b_{hi} + W_{hh}h_{t-1}+b_{hh})$\n在Pytorch中，可以自己构造 RNN cell，并处理序列的循环；也可以直接使用RNN。\n自己创建RNN Cell: cell = torch.nn.RNNCell (input_size=i_size, hidden_size = h_size)\n用 cell 计算下一时刻的隐变量：h_1 = cell (x_1, h_0)。\n这两个输入的 shape：x(batch_size, input_size)，h(batch_size, hidden_size)， h_1的维度与h_0一样。batch是一批的样本条数。\n数据集可以表达成一个张量：dataset.shape = (seqLen, batch_size, input_size)\n使用RNNCell：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch batch_size = 1 seq_len = 3 #有3个样本:x1,x2,x3 input_size = 4 #x都是4x1的向量 hidden_size =2 #h都是2×1的向量 cell = torch.nn.RNNCell( input_size=input_size, hidden_size=hidden_size ) dataset = torch.randn(seq_len, batch_size, input_size) #初始化h0，全零 hidden = torch.zeros(batch_size, hidden_size) for idx, input in enumerate(dataset): hidden = cell(input, hidden) RNN class 实例化RNN: cell = torch.nn.RNN(input_size = input_size, hidden_size=hidden_size, num_layer=num_layers)\nRNN的输出有两个张量，调用：out, hidden = cell(inputs, hidden)。\ninputs 是整个输入序列，输入的 hidden 是 $h_0$，out 是所有的隐层输出 $h_1 \\cdots h_N$，输出的 hidden 是最后一个 cell 的输出$h_N$。\ninputs 的形状是 (seqLen, batch, input_size)，hidden 的形状是 (numLayers, batch, hidden_size)，numLayers 是RNN的层数，每层最终都会输出一个隐变量。 output的形状为(seqLen, batch, hidden_size)。输出hidden与输入hidden的形状一样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch batch_size = 1 seq_len = 3 input_size = 4 hidden_size = 2 num_layers = 1 cell = torch.nn.RNN(input_size = input_size, hidden_size= hidden_size, num_layers = num_layers) inputs = torch.randn(seq_len, batch_size, input_size) hidden = torch.zeros(num_layers, batch_size, hidden_size) out, hidden = cell(inputs, hidden) Example: Seq ➔ Seq 例子： 序列 ➔ 序列，把“hello\u0026quot;转换到“ohlol”\n把输入“hello”变成由数字构成的向量：\n字符级别，为出现过的字符构造一个词典； 词级别，为出现过的单词构造词典。 这里为每个元素分配索引，用索引代替字母。再把每个索引变成一个向量（独热码），向量的长度与词典的条数一样\n$$ \\begin{aligned} \\begin{bmatrix} h \\\\\\ e \\\\\\ l \\\\\\ l \\\\\\ o \\end{bmatrix} \\underset{\\longrightarrow}{词典有4条索引} \\quad \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 2 \\\\\\ 2 \\\\\\ 3 \\end{bmatrix} \\rightarrow \\begin{matrix} [0 \u0026 1 \u0026 0 \u0026 0] \\\\\\ [1 \u0026 0 \u0026 0 \u0026 0] \\\\\\ [0 \u0026 0 \u0026 1 \u0026 0] \\\\\\ [0 \u0026 0 \u0026 1 \u0026 0] \\\\\\ [0 \u0026 0 \u0026 0 \u0026 1] \\end{matrix} \\end{aligned} $$ 依次把 5 个独热向量输入 RNN，向量长度（input_size） = 4， output 是 5 个 RNN Cell的输出，每个输出对应于 4 个字母（h, e, l, o）中的一个，因此是一个多分类问题。\n所以设置每个 hidden 是一个长度为 4 的向量，这 4 个线性输出分别对应（h,e,l,o）。 把这个向量传入 softmax就变成了一个概率分布，表示 4 个字母分别可能的概率。\n再与真实标签的独热向量计算损失，即可用反向传播+梯度下降优化网络参数。\n自己设计RNN Cell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 import torch ## Step-1 准备数据 input_size = 4 # 每个样本的独热向量的长度为4 hidden_size = 4 # 隐变量的长度为4，对应4个输出类别 batch_size = 1 # number of tokens idx2char = [\u0026#39;e\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;] # 方便打印结果 x_data = [1, 0, 2, 2, 3] # 样本 \u0026#34;hello\u0026#34;（索引） y_data = [3, 1, 2, 3, 2] # 输出 \u0026#34;ohlol\u0026#34; 的标签(真实类别), o是第4类，h是第2类 one_hot_lookup =[[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]] # 把样本索引变成独热向量，其形状为 (seqSize, input_size) = (5, 4) x_one_hot = torch.Tensor([one_hot_lookup[x] for x in x_data]) # 全部样本分成几个 batch，-1表示 batch 的个数 num_batch 自动计算 inputs = x_one_hot.view(-1, batch_size, input_size) # (5,1,4) # label vector (seqSize, 1)。因为 batch_size 是 1，所以第 1 维度是 1 labels = torch.LongTensor(y_data).view(-1,1) # (5,1) ## Step-2 设计模型 class Model(torch.nn.Module): #继承自Module def __init__(self, input_size, hidden_size, batch_size): super(Model, self).__init__() self.batch_size = batch_size self.input_size = input_size self.hidden_size = hidden_size self.rnncell = torch.nn.RNNCell( input_size=self.input_size, hidden_size=self.hidden_size ) # 实例化 RNN Cell def forward(self, input, hidden): # Module 里面的 __call__函数调用了forward方法 # 用 input 和 hidden state 计算下一个 hidden state hidden = self.rnncell(input, hidden) # input: (batchSize, inputSize) return hidden # (batchSize, hiddenSize) def init_hidden(self): #生成默认的h0,全零矩阵 return torch.zeros(self.batch_size, self.hidden_size) # 实例化模型对象,batch_size 只有在构造默认 h0 时被用到 net = Model(input_size, hidden_size, batch_size) ## Step-3 损失和优化器 criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.1) #基于SGD的改进优化器 ## Step-4 训练循环 n_iters = 15 for epoch in range(n_iters): loss = 0 optimizer.zero_grad() #梯度归零 hidden = net.init_hidden() #生成 h0 print(\u0026#39;Predicted string: \u0026#39;, end=\u0026#39;\u0026#39;) # 遍历每个 batch，每个 batch 里有 batch_size 个 token。 # inputs:（num_batch, batchSize, inputSize） for input, label in zip(inputs, labels): # input: (batchSize, inputSize), label: (batchSize,) # zip 是沿着矩阵的第 0 维度拼接，也就是沿着 num_batch 方向拼接，所以一个矩阵对应一个label。 # 计算下一时刻的 hidden，维度为 (batchSize, hiddenSize) hidden = net(input, hidden) # 累加各 batch 的损失（需要构造计算图,不可用item()） loss += criterion(hidden, label) # 按维度 1 找 hidden 里的最大值的下标,就属于那类 _, idx = hidden.max(dim=1) # 看一下输出了什么字符，虽然字符对了，但它的概率不是1，所以还有损失。 print(idx2char[idx.item()], end=\u0026#39;\u0026#39;) loss.backward() # 计算梯度 optimizer.step() # 更新一步 print(f\u0026#39;, Epoch [{epoch+1}/{n_iters}], loss ={loss.item():.4f}\u0026#39;) #打印损失 调用pytorch的RNN类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 import torch ## 准备数据 input_size = 4 #共有helo 4种字符，需要用4元素的独热向量来表示每种字符 hidden_size = 4 #输出分别属于4类的概率 num_layers = 1 batch_size = 1 #每个batch是1个字符 seq_len = 5 #全部样本分成5个seq/batch：\u0026#34;h\u0026#34;;\u0026#34;e\u0026#34;;\u0026#34;l\u0026#34;;\u0026#34;l\u0026#34;;\u0026#34;o\u0026#34; idx2char = [\u0026#39;e\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;] x_data = [1,0,2,2,3] y_data = [3,1,2,3,2] #各样本的真实类别，用于索引真实类别对应的预测值 one_hot_lookup =[[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]] x_one_hot = [one_hot_lookup[x] for x in x_data] #把样本变成独热向量，其形状为seq×input_size inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size) labels = torch.LongTensor(y_data) #维度(seqSize×batchSize,1)：torch.Size([5])，所有样本的标签(类别) ## 设计模型 class Model(torch.nn.Module): def __init__(self, input_size, hidden_size, batch_size, num_layers=1): super(Model, self).__init__() self.num_layers = num_layers self.batch_size = batch_size #用于创建默认h0 self.input_size = input_size self.hidden_size = hidden_size self.rnn = torch.nn.RNN(input_size= self.input_size, hidden_size = self.hidden_size, num_layers=num_layers) def forward(self, input): #Module调用此方法，自动迭代数据集，给出最终输出 hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size) #如果在外面定义，这就不用写 out, _ = self.rnn(input, hidden) return out.view(-1, self.hidden_size) #输出变成两维的（seqLen×batchSize, hiddenSize），交叉熵损失只能接受二维的Tensor和一维的labels：一条样本的向量对应一个label net = Model(input_size, hidden_size, batch_size, num_layers) ## 损失和优化器 criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.05) #lr 0.1比较好 ## 训练循环 for epoch in range(15): optimizer.zero_grad() outputs = net(inputs) #inputs的维度：SeqLen*BatchSize*InputSize，outputs的维度：SeqLen*BatchSize*HiddenSize loss = criterion(outputs, labels) #labels维度是SeqLen×BatchSize×1，也就是（5,1） loss.backward() optimizer.step() _, idx = outputs.max(dim=1) idx = idx.data.numpy() print(\u0026#39;Predicted:\u0026#39;, \u0026#39;\u0026#39;.join([idx2char[x] for x in idx]), end=\u0026#39;\u0026#39;) #把4个字符拼成一个单词 print(\u0026#39;,Epoch [%d/15] loss =%.3f\u0026#39; % (epoch+1, loss.item())) #打印损失 独热向量在编码词/字符时：\n独热向量维度太高，字符级别需128维，单词级别维度太高 向量太稀疏 是硬编码的 希望吧 单词/字符 联系到一个低维、稠密、从数据中学习到的向量，流行又强大的方法是嵌入层Embedding。把高维稀疏样本映射到低维稠密的空间(数据降维)。\n独热向量$x_n$通过嵌入层 Embed 变成稠密的表示，经过RNN线性变换后，再用一个线性层，让最后的隐变量输出与分类的数量一致\n4维转5维，构建一个查找表：\n查找表做转置，再乘以输入的独热向量，就可取出“对应行”\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 num_class = 4 #类别 input_size = 4 #输入样本是4维的 hidden_size = 8 #隐变量输出是8维 embedding_size = 10 #把输入从4维嵌入到10维空间 num_layers = 2 #2层RNN batch_size = 1 seq_len = 5 idx2char = [\u0026#39;e\u0026#39;,\u0026#39;h\u0026#39;,\u0026#39;l\u0026#39;,\u0026#39;o\u0026#39;] x_data = [[1,0,2,2,3]] y_data = [3,1,2,3,2] inputs = torch.LongTensor(x_data) labels = torch.LongTensor(y_data) class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.emb = torch.nn.Embedding(input_size, embedding_size) #嵌入层查找表的大小：inputSize×embeddingSize，对输入数据做维度转换 self.rnn = torch.nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #RNN输入:(batchSize, seqLen, embeddingSize)，RNN输出:(batchSize, seqLen, hiddenSize) self.fc = torch.nn.Linear(hidden_size, num_class) #维度变换: 从hidden_size到类别数量 def forward(self, x): hidden = torch.zeros(num_layers, x.size(0), hidden_size) x = self.emb(x) #(batch,seqLen, embeddingSize) 把长整形的张量转变成嵌入层稠密的向量 x, _ = self.rnn(x, hidden) x = self.fc(x) return x.view(-1,num_class) #维度等于类别数 net = Model() #后面都一样 criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.05) for epoch in range(15): optimizer.zero_grad() outputs = net(inputs) #inputs的维度：SeqLen*BatchSize*InputSize，outputs的维度：SeqLen*BatchSize*HiddenSize loss = criterion(outputs, labels) #labels维度是SeqLen×BatchSize×1，也就是（5,1） loss.backward() optimizer.step() _, idx = outputs.max(dim=1) #取出值最大的维度的索引，也就是所属的第几类。 idx = idx.data.numpy() #索引从0开始 print(\u0026#39;Predicted:\u0026#39;, \u0026#39;\u0026#39;.join([idx2char[x] for x in idx]), end=\u0026#39;\u0026#39;) #把4个字符拼成一个单词 print(\u0026#39;,Epoch [%d/15] loss =%.3f\u0026#39; % (epoch+1, loss.item())) #打印损失 用RNN对MNIST图片分类 感觉里面的流程挺规范的。\n初始化h0 为一个对角矩阵； 用了3层RNN，只用10个epoch就达到96%的accuracy了。 大概原理 把MNIST中每张图片看成一个序列，这个序列含有28个$x$（对应每一行像素），x的维度是28。每行像素都变换到一个h，这个h会参与计算下一行的h，最终得到第28行的h，将此h输入线性全连接层，变换到10维，每一维对应着0-9的每一类。然后这个10维的向量通过softmax，就是属于每类的概率了。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 import torch import torch.nn as nn import torch.nn.functionall as F import torch.optim as optim import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt # 1. 加载数据集 MNIST batch_size = 128 train_dataset = torchvision.datasets.MNIST(root=\u0026#39;./\u0026#39;, train=True, transform=transforms.ToTensor(), download=True) #加载训练集 test_dataset = torchvision.datasets.MNIST(root=\u0026#39;./\u0026#39;, train=False, transform=transforms.ToTensor()) #加载测试集 train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) #构造数据加载器，每次迭代从数据集中取出batchSize个样本，每个epoch都不同 test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # 查看一下数据 def imshow(img): npimg = img.numpy() plt.imshow(np.transpose(npimg,(1,2,0))) dataiter = iter(train_loader) #取出一份128张图片 images, labels = dataiter.next() imshow(torchvision.utils.make_grid(images,nrow=15)) #一排15张图 # 2. 定义模型 N_STEPS = 28 # 1张图片是1个序列，所以1个序列包含28个样本 N_INPUTS = 28 # 输入数据的维度(1行有28个像素) N_NEURONS = 150 # RNN中间的特征的大小(hiddenSize) N_OUTPUT = 10 # 输出数据的维度(分类的个数) N_EPHOCS = 10 # epoch的大小(训练10轮) N_LAYERS = 3 # 3层RNN class ImageRNN(nn.Module): #继承自nn.Module def __init__(self, batch_size, n_inputs, n_neurons, n_outputs, n_layers): super(ImageRNN, self).__init__() self.batch_size = batch_size #每次输入batchSize张图片 self.n_inputs = n_inputs # 输入的维度 self.n_outputs = n_outputs # 分类的大小 self.n_neurons = n_neurons # RNN中输出的维度 self.n_layers = n_layers # RNN中的层数 self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons, num_layers=self.n_layers) #实例化RNN对象，inputSize,hiddenSize self.FC = nn.Linear(self.n_neurons, self.n_outputs) def init_hidden(self): #生成h0是一个对角矩阵 # (num_layers, batch_size, n_neurons) # initialize hidden weights with zero values # 这个是net的memory, 初始化memory为0 return (torch.zeros(self.n_layers, self.batch_size, self.n_neurons).to(device)) def forward(self,x): #计算RNN Cell的输出 # transforms x to dimensions : n_step × batch_size × n_inputs x = x.permute(1,0,2) # 需要把n_step放在第一个维度 self.batch_size = x.size(1) # 每次需要重新计算batch_size, 因为可能会出现不够一个batch的情况 self.hidden = self.init_hidden() # 初始化hidden state rnn_out, self.hidden = self.basic_rnn(x,self.hidden) # 前向传播 out = self.FC(rnn_out[-1]) # 求出每一类的概率 return out.view(-1,self.n_outputs) # 最终输出大小 : batch_size × n_output(10)。然后接softmax找出所属类别，计算损失。 # 测试一下 device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) #方便之后使用gpu运算 model = ImageRNN(batch_size, N_INPUTS, N_NEURONS, N_OUTPUT, N_LAYERS).to(device) #构造RNN模型 model.basic_rnn.weight_hh_l0.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) # 初始化模型的weight为对角矩阵 model.basic_rnn.weight_hh_l1.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) model.basic_rnn.weight_hh_l2.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) dataiter = iter(train_loader) # 取1份数据 images, labels = dataiter.next() model.hidden = model.init_hidden() #初始化h0 logits = model(images.view(-1,28,28).to(device)) #输入模型 print(logits[0:2]) #打印前2张图的hidden输出:10维向量 # 3. 定义损失函数和优化器 criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(),lr=0.001) def get_accuracy(logit, target, batch_size): #用来最后计算模型的准确率 corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum() accuracy = 100.0 * corrects/batch_size return accuracy.item() # 4. 训练 for epoch in range(N_EPHOCS): train_running_loss = 0.0 train_acc = 0.0 model.train() # trainging round for i, data in enumerate(train_loader): #每次取出一个batch optimizer.zero_grad() #梯度清零 # reset hidden states model.hidden = model.init_hidden() #初始化h0 # get inputs inputs, labels = data #输入一个batch的样本和标签 inputs = inputs.view(-1,28,28).to(device) labels = labels.to(device) # forward+backward+optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_running_loss = train_running_loss + loss.detach().item() train_acc = train_acc + get_accuracy(outputs, labels, batch_size) model.eval() print(\u0026#39;Epoch : {:0\u0026gt;2d} | Loss : {:\u0026lt;6.4f} | Train Accuracy : {:\u0026lt;6.2f}%\u0026#39;.format(epoch, train_running_loss/i, train_acc/i)) # 5. 评价模型：计算测试集准确率 est_acc = 0.0 for i,data in enumerate(test_loader,0): inputs, labels = data labels = labels.to(device) inputs = inputs.view(-1,28,28).to(device) outputs = model(inputs) thisBatchAcc = get_accuracy(outputs, labels, batch_size) print(\u0026#34;Batch:{:0\u0026gt;2d}, Accuracy : {:\u0026lt;6.4f}%\u0026#34;.format(i,thisBatchAcc)) test_acc = test_acc + thisBatchAcc print(\u0026#39;============平均准确率===========\u0026#39;) print(\u0026#39;Test Accuracy : {:\u0026lt;6.4f}%\u0026#39;.format(test_acc/i)) #96% # 6. 定义hook, 查看模型中间过程 k折交叉验证 《动手学深度学习》的pytorch版\n返回第i折训练和验证数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def get_k_fold_data(k,i,X,y): #我不想把标签分出来,Dataset对象可以分 assert k\u0026gt;1 fold_size = X.shape[0] // k #先做除法，再向下取整(不大于) X_train, y_train = None, None for j in range(k): idx = slice(j*fold_size, (j+1)*fold_size) #切片函数slice(start, end, step) X_part, y_part = X[idx,:], y[idx] if j == i: X_valid, y_valid = X_part, y_part elif X_train is None: X_train, y_train = X_part, y_part else: X_train = torch.cat((X_train, X_part), dim=0) #增加行数 y_train = torch.cat((y_train, y_part), dim=0) return X_train, y_train, X_valid, y_valid 训练k次\n1 2 3 4 5 def k_fold(k, X_train, num_epochs, learning_rate, weight_decay, batch_size): train_ls_sum, valid_ls_sum = 0, 0 for i in range(k): data = get_k_fold_data(k, i, X_train) .... ","date":"2021-11-21T10:26:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/12_rnn%E5%9F%BA%E7%A1%80/","title":"watch: PyTorch - 刘二 12 | RNN Basics"},{"content":"用pytorch 提供的工具构建线性模型\nforward 前馈：求一组样本的损失 backward 反向：求损失关于各w的梯度 update 梯度下降更新w 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import torch ## 1. 准备数据 x_data = torch.Tensor([[1.0],[2.0],[3.0]]) #3条数据作为一个batch，把3个样本一次性求出一个损失，定义成矩阵以便利用numpy的广播机制 y_data = torch.Tensor([[2.0],[4.0],[6.0]]) #y与x一样也是3x1的矩阵 ## 2. 设计模型: 计算 y_pred = x*w +b class LinearModel(torch.nn.Module): #把模型定义为一个类，所有的神经网络模型都继承自nn.Module（包含了很多训练方法(i.e.反向传播)） def __init__(self): #必须实现构造函数,初始化对象 super(LinearModel, self).__init__() #super调用父类的构造函数 self.linear = torch.nn.Linear(1,1) #构造线性单元对象：输入样本和输出样本的特征（维度）都是1列，样本条数是一个batch（3条） def forward(self, x): #必须实现计算预测值方法 y_pred = self.linear(x) #linear也是callable的对象，加上括号()会调用nn.Linear的 call 方法，其中包含forward方法 wx+b return y_pred model = LinearModel() #实例化模型对象 ## 3. 构造损失函数和优化器 criterion = torch.nn.MSELoss(size_average = False) #损失函数MSE, 得到标量损失值，这个过程会构建计算图，所以也应该继承自nn.Moudle optimizer = torch.optim.SGD(model.parameter(), lr=0.01) #实例化优化器对象，它不会构建计算图, 传入需要优化的参数，learning rate固定 ## 4. 训练周期 for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred, y_data)#前馈：计算预测值和损失 print(epoch, loss) #loss是对象，打印时会自动调用__str__();不会产生计算图 optimizer.zero_grad() #所有参数的梯度归零 loss.backward() #反向传播 optimizer.step() #进行一次更新，根据所有参数的梯度和步长做更新 ## 输出w和b print(\u0026#39;w=\u0026#39;, model.linear.weight.item()) print(\u0026#39;b=\u0026#39;, model.linear.bias.item()) ## 测试模型 x_test = torch.Tensor([[4.0]]) y_test = model(x_test) print(\u0026#39;y_pred =\u0026#39;, y_test.data) 输入数据的格式 用矩阵一次性计算出一个batch的y_pred 或 loss\nnumpy的广播机制：做运算的两个数组维度不同，把小矩阵(重复)扩充到与大矩阵相同的大小\n运算：\n$$ \\begin{aligned} \\begin{bmatrix} y\\_{pred}^{(1)} \\\\\\ y\\_{pred}^{(2)} \\\\\\ y\\_{pred}^{(3)} \\end{bmatrix}\\_{3\\times 1} = w \\cdot \\begin{bmatrix} x^{(1)} \\\\\\ x^{(2)} \\\\\\ x^{(3)} \\end{bmatrix}\\_{3\\times 1} + b \\end{aligned} $$其中 w 和 b 会触发广播机制，变成 $[w\\ w\\ w]^T_{3\\times 1},\\ [b\\ b\\ b]^T_{3\\times 1}$\n计算损失：\n$$ \\begin{bmatrix} loss\\_1 \\\\\\ loss\\_2 \\\\\\ loss\\_3 \\end{bmatrix} = \\begin{pmatrix} \\begin{bmatrix} \\hat{y\\_1} \\\\\\ \\hat{y\\_2} \\\\\\ \\hat{y\\_3} \\end{bmatrix} - \\begin{bmatrix} y\\_1 \\\\\\ y\\_2 \\\\\\ y\\_3 \\end{bmatrix} \\end{pmatrix}^2 $$loss 需要是一个标量，所以需求和：$loss = (\\frac{1}{N}) \\sum_{i}^N loss_i$。向量没法backward\nnn.Linear类 class torch.nn.Linear(in_features, out_features, bias=True) Docs-Linear\n对数据数据应用一个线性变换：$y_{1\\times n} = w^T_{1\\times 3} X_{3\\times n}+b$，n条样本，x有3个特征，y有一个特征\n传入参数:\nin_feature: 每个输入样本的维度（列数,特征） out_feature: 输出样本的列数 bias: 是否需要b，默认为true 线性单元包括两个成员Tensors(weight, bias)可以完成w*x+b的运算（同样继承自module，可以反向传播）\nmagic method __call__() callable 对象是一个可以被调用执行的对象 如果为一个类编写了__call__()方法，那么在该类的对象后面加括号，就会调用执行 __call__() 方法 1 2 3 4 5 class Foobar: def __init__(self): pass def __call__(self, *args, **kwargs): #实现此方法，让对象可调用 *args 代表没有固定数量和变量名的输入参数，是一个元组；**kwargs表示带变量名的输入参数，是一个字典：\n1 2 def fun(*args, **kwargs): pass func(1,2,3, x=3, y=5)，*args为(1,2,3)，**kwargs为{'x':3, 'y':5}，就可以遍历各个参数了\nnn.MSELoss类 class torch.nn.MSELoss(size_average=True, reduce=True)\n继承自nn.Module\n$\\hat{𝐲} - 𝐲$，减完之后求平方，求和\n参数：\nsize_average=True 是否最后的损失要求均值，$\\frac{1}{N}$没什么用，求导与w无关 reduce=True 是否降维 SGD类 class torch.optim.SGD(params.lr=\u0026lt;object, object\u0026gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)\nparams 是需要优化的参数。使用model.parameters()，这个成员函数会检查model中的所有成员，有哪些参数需要用梯度下降更新，加入需要训练的参数集合中。搜索model中的linear成员时，会调用linear.params()，linear有2个参数，\n","date":"2021-11-20T16:03:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/5_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92pytorch/","title":"watch: PyTorch - 刘二 05 | Linear Regression"},{"content":"每次迭代，loss的波动很大的解决办法\n每次迭代，loss的波动很大，有如下几方面：\n学习率选取较大； 在loss不再下降的时候降低学习率； 每个epoch训练之前，对数据进行重新打乱，如果你的 batch 的内容和顺序都是固定的，可能造成模型 overfit 这个顺序； 各个 batch 的 loss 有不同是正常的，但如果波动太大，可能说明你的各个 batch 不是 homogeneous 的（即内容差别太大），不能代表整体数据。可以试试加大 batch size。 总结就是： 当loss不下降时，降低学习率，一般降低到原来的0.1，在每个epoch开始之前，均打乱一次数据，适当增大batch_size的大小。\n————————————————\nPytorch使用shuffle打乱数据\n","date":"2021-11-20T11:43:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/projecttips/%E6%AF%8F%E6%AC%A1%E8%BF%AD%E4%BB%A3loss%E6%B3%A2%E5%8A%A8%E5%BE%88%E5%A4%A7/","title":"loss波动大"},{"content":"RBF核函數參數自動挑選法Python實作\n$x,z$ 是原空间两个点的坐标，$\\gamma$ 是参数：\n$$ \\kappa (x,z,\\gamma) = \\operatorname{exp} (-\\gamma \\| x-z \\|^2) $$rbf核的特殊性质：\n在特征空间中，自己和自己的内积为1（范数），所以$\\phi(x)$ 是在一个超球面上\n两向量夹角等于内积：\n$$ cos(\\theta) = \\frac{\\kappa(x,z,\\gamma)}{\\sqrt{\\kappa(x,x,\\gamma)} \\sqrt{\\kappa(z,z,\\gamma)}} = \\frac{\\kappa(x,z,\\gamma)}{1} = \\kappa(x,z,\\gamma) $$ 当 $x \\neq z$ 时：\n$\\gamma \\rightarrow 0 \\ \\Rightarrow cos(\\theta)=\\kappa(x,z,\\gamma) \\rightarrow 1 \\ \\Rightarrow \\theta \\rightarrow 0^\\circ \\ \\Rightarrow \\phi(x)与\\phi(z)越相似$ $\\gamma \\rightarrow \\infin \\ \\Rightarrow cos(\\theta)=\\kappa(x,z,\\gamma) \\rightarrow 1 \\ \\Rightarrow \\theta \\rightarrow 90^\\circ \\ \\Rightarrow \\phi(x)与\\phi(z)越不相似$ 调参 varying the parameter 在原空间中的距离越大向量，当$\\gamma$变大时，角度拉大的速度越快。 当$\\gamma$ 很小的时候，各向量夹角很小，点很集中，不好分类； 当$\\gamma$很大时，各向量互相垂直，每个点都变一类；所以要取不大不小的$\\gamma$，同类向量夹角小，不同类向量的夹角已经拉到很开。\n目标： 调整gamma，同类的内积越接近1越好，不同类内积越接近0越好\ncode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # 生成样本数据及其标签 from sklearn import datasets Data,labels = datasets.make_circles(n_samples=400, noise=0.1, factor=0.1) #Data是400x2的矩阵，labels是1x400的列表 # 画三维散点图 import plotly.express as px fig = px.scatter(x=Data[:,0], y=Data[:,1], color=labels) fig.update_layout(yaxis=dict(scalenchor=\u0026#39;x\u0026#39;)) #y轴的刻度与x轴一样 from sklearn.metrics.pairwise import rbf_kernel import numpy as np def AutoRBF(gv, Data, labels): #计算gv对应的指标J # 计算Kernel matrix K = rbf_kernel(Data, gamma=gv) #400x400 # 组内：对角线上的block n=[] #统计各类样本的个数 w=0 #同组内积矩阵和累计 for i in range(0, max(labels)+1): #遍历每一类 idx = labels==i #把labels列表中等于某类的元素变成:True/False，形成列表idx #Within the Kernel matrix，取出同类矩阵 KW = K[:,idx] #先取列号等于idx列表中元素为True的index的列 KW = KW[idx,:] #再取行号=idx列表中True元素的index的行 n.append(sum(idx)) #统计各类元素个数，这里是[200,200] w = w + np.sum(KW) #第一类内积矩阵求和加上第二类内积矩阵求和 # 组间（不同类样本内积的block） b = np.sum(K) - w #全部元素之和减去同类内积矩阵之和w就是不同类内积矩阵之和b nw = sum(np.power(n,2)) #每类200个，共200x200 + 200x200=4万个值 nb = sum(n)**2 - nw w=w/nw #取平均 b=b/nb #取平均 J=(1-w)+b #w越接近1越好，b越接近0越好 return J # 第一种方法找最佳gamma：grid grid = np.linspace(0,50,1000) #0-50切成1000份 J=[] #存储各gv对应的J值 for gv in grid: J.append(AutoRBF(gv,Data,labels)) px.line(x=grid, y=J) #把J画成曲线 # 第二种方法：用 minimize 函数找最低点（比grid快很多） from scipy.optimize import minimize gv0=1/Data.shape[1] #初始值：1/维度数 sol = minimize(AutoRBF,gv0,args=(Data,labels)) #solution是使目标函数AutoRBF最小的gv（初值为gv0），args放函数的其余参数 bestgv = sol.x #最低点的x坐标，最好的gv # 用kpca降维后，能不能分得好 from sklearn.decomposition import KernelPCA myKPCA = KernelPCA(n_components=2, kernel=\u0026#39;rbf\u0026#39;, gamma=bestgv) myKPCA.fit(Data) #训练 reduced_Data = myKPCA.transform(Data) #投影到低维 fig = px.scatter(x=reduced_Data[:,0], y=reduced_Data[:,1],color=labels) fig.update_layout(yaxis=dict(scaleanchor=\u0026#39;x\u0026#39;)) ","date":"2021-11-19T17:14:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/rbf%E6%A0%B8%E5%8F%82%E6%95%B0%E8%87%AA%E5%8A%A8%E6%8C%91%E9%80%89/","title":"RBF-Kernel-Param-Select(Python)"},{"content":"最基本的数据类型:Tensor，存储所有的数值，标量，向量，矩阵，高阶tensor\ntensor有两个成员： data（权重数值本身w）， grad（也是tensor，损失值（标量）对权重的导数 ∂loss/∂w）\n构建计算图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import torch # 准备数据集 x_data = [1.0, 2.0, 3.0] y_data = [2.0, 4.0, 6.0] w = torch.Tensor([1.0]) # 创建权重初始值，tensor变量中只有一个值 w.requires_grad = True # 该变量需要计算梯度，默认的tensor不需要计算梯度 # 设计模型 def forward(x): return x* w # w是个tensor，乘法*被重载了: tensor 与tensor之间的数乘，x被自动转换成tensor，所以乘法结果也是tensor，并且也会需要计算梯度。 # 计算损失 def loss(x,y): y_pred = forward(x) # 计算预测值 return (y_pred - y) ** 2 # 每调用一次loss函数，计算图被动态地构建出来 # 训练过程 print(\u0026#34;predict (before training)\u0026#34;, 4, forward(4).item()) for epoch in range(100): #训练100轮 for x,y in zip(x_data, y_data): # 对应组合拼起来 # 前馈：计算每个样本的损失(随机梯度下降)，是一个(标量)tensor，含有1个值， # 如果是一个向量没法backward l = loss(x,y) # 反向传播：调用张量 l 的成员函数，把图上的所有梯度求出来，存到 w 中，然后计算图就被释放了， # 下一次loss计算会创建新的计算图（因为每次计算图可能不一样） l.backward() print(\u0026#39;\\t grad:\u0026#39;, x,y, w.grad.item()) #item把grad变成int/float,直接拿出它的数值(防止产生计算图) w.data = w.data - 0.01 * w.grad.data #更新数值，成员grad也是一个tensor，tensor的乘法会建立计算图，所以要取其data再做乘法，就不会建立计算图，只是修改w的数值（并不是说以后还要对这个运算求梯度） w.grad.data.zero_() #权重的梯度(导数)的数值清零，否则各次由.backward()计算出的梯度值会累加（有的时候需要梯度累加） print(\u0026#34;progress\u0026#34;,epoch, 1.item()) #打印每轮训练的loss print(\u0026#34;predict (after training)\u0026#34;, 4, forward(4).item()) 如果要在程序中对损失 l 求和取平均，注意要取出数值（使用int/float运算）： sum + = l.item()，否则因为 l 是tensor，一直加，计算图一直延长，导致内存泄漏。 （这就是为什么要避免 in-place operation? 1)\nRef AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD ","date":"2021-11-17T21:47:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/4_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","title":"watch: PyTorch - 刘二 04 | Backpropagation"},{"content":"用于做分类\n线性回归： $y\\in \\mathbb R$是连续的\n$$ Affine Model(Linear unit): \\hat{y} = x*w + b \\\\ Loss function: loss = (\\hat{y} - y)^2 = (x\\cdot w - y)^2 $$分类：y的取值是一个离散值集合：MINIST 0-9\n计算属于每一类的概率，取最大值做判别\n","date":"2021-11-15T10:39:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/6_logistic%E5%9B%9E%E5%BD%92/","title":"watch: PyTorch - 刘二 06 | Logistic Regression"},{"content":"Source video: 《PyTorch深度学习实践》完结合集-02\n刘洪普 Blog\n监督学习过程：\n准备数据集 模型设计 训练 inferring 数据集：\nx(hours) y(grades) 1 2 2 4 3 6 4 ? 1-3 training, 4 testing 预测\n监督学习：输出值已知\n数据分成 Training set 和 Testing set 两部分(不能偷看测试集的label)\nTraining set 接近 数据的真实联合分布 D(x,y)，根据大数定律需要大量数据\nTraining Set 中分出一部分做开发集，验证模型性能\n模型设计：\n模型：y = f(x)\n先用线性模型是否有效，再换其他的模型。\n线性模型：$\\hat{y}=f(x) = x*w +b$\n预测值 $\\hat{y} = x * w$ (先不考虑b)\n找到最优的权重，先随机数，计算与数据集的误差（平方和最小），\n损失函数,针对一个样本：$loss = (\\hat(y)-y)^2 = (x*w-y)^2$，必须是一个标量，才能让他变得更小，不断优化。\nx(Hours) y(grades) y_predict(w=3) Loss(w=3) 1 2 3 1 2 4 6 4 3 6 9 9 mean = 14/3 选取w，让平均损失降到最低\nCost function, 对于整个training set, 平均平方误差(Mean Square Error): $cost = \\frac{1}{N} \\sum_{n=1}{N}(\\hat{y}_n - y_n)^2$\nx(Hours) Loss(w=0) Loss(w=1) Loss(w=2) Loss(w=3) Loss(w=4) 1 4 3 0 1 4 2 16 6 0 4 16 3 36 9 0 9 36 MSE 18.7 4.7 0 4.7 18.7 不保证能找到0，在真实最小值附近穷举。经过测试最优w存在于0-4之间，对之间所有可能取值（对实数域采样）都计算一下损失，绘制曲线，找最低点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np import matplotlib.pyplot as plt x_data = [1.0, 2.0, 3.0] #数据用两个列表表示 y_data = [2.0, 4.0, 6.0] # 定义模型 def forward(x): #前馈 return x*w def loss(x,y): y_pred = forward(x) return (y_pred - y) * (y_pred - y) w_list = [] mse_list = [] for w in np.arange(0.0, 4.1, 0.1): #间隔0.1 print(\u0026#39;w=\u0026#39;, w) l_sum = 0 for x_val, y_val in zip(x_data, y_data): #拼成training pair y_pred_val = forward(x_val) loss_val = loss(x_val, y_val) #每个样本的loss l_sum += loss_val #cost function是loss function 的平均 print(\u0026#39;\\t\u0026#39;,x_val, y_val, y_pred_val, loss_val) print(\u0026#39;MSE=\u0026#39;, l_sum/3) w_list.append(w) mse_list.append(l_sum/3) plt.plot(w_list, mse_list) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.xlabel(\u0026#39;w\u0026#39;) plt.show() 用训练的轮数 epoch 做横坐标，检查超参数，判别是否收敛.\n打印日志输出，实时画图，Visdom\n要存盘，避免崩溃，计算白费\n模型：$\\hat{y} = x*w +b$ 有两个参数，损失函数是曲面,找最低点\n3d图绘制使用 np.meshgrid()\n","date":"2021-11-14T23:32:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/2_%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","title":"watch: PyTorch - 刘二 02 | Linear Model"},{"content":"训练模型的最常用算法\nwhat is best model for the data?\nlinear model, 随机猜测一个斜率（权重）,找让误差最小的最优权重。\n通常损失函数有多个参数，穷举某一区间中所有点，搜索最优参数，计算量太大（1个参数搜索100个点，2个参数就是100^2）\n分治：先分大块(4x4)搜索，确定一个小区域，再在小区域中分（4x4）搜索。但是如果cost function 很粗糙，。\n寻找使cost funciton 最小的权重，是一个优化问题：\n$$ Mean Square Error: cost(w) = \\frac{1}{N} \\sum_{n=1}^N (\\hat{y_n}- y_n)^2 \\\\ w^* = \\underset{w}{argmin} cost(w) $$梯度下降：\n初始权重，确定滚动方向，到达最低点\n$w= w- \\alpha \\frac{\\partial cost}{\\partial w}$ (a 是学习率）\n梯度下降只能找到局部最优。实际上，深度神经网络的损失函数并没有很多局部最优，但是存在鞍点，它的梯度为零，到达了按点没办法继续迭代。\n随机\n","date":"2021-11-14T22:36:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/3_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","title":"watch: PyTorch - 刘二 03 | Gradient Descent"},{"content":"Automatic RBF Kernel Parameter Selection Method-李政轩\nKernel Method 使用kernel mapping $\\phi$ 把数据送到 feature space。但通常 $\\phi$ 未知，所以使用 kernel function $\\kappa$ 做变换。\n最常用 RBF kernel:\n$$ \\kappa (x,z,\\sigma) = \\operatorname{exp} \\left( -\\frac{\\| x-z \\|^2}{2\\sigma^2} \\right) $$表示内积 $\\phi(x)^T \\phi(z)$\n性质较好：\n在特征空间中，每个样本的范数是 +1； 把样本投影到球面上（而且是在正的卦限） $$ $$ ","date":"2021-11-11T16:05:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/automatic_rbf_kernel_para_select/","title":"RBF-Kernel-Param-Select"},{"content":"P1-背景介绍\nKernel Method 把低维空间的非线性问题，转化到高维空间的线性问题求解 Kernel trick 使用核函数减少计算量，避免计算高维特征空间的内积（计算角度） Kernel Function 把输入空间𝓧 映射（任意形式$ϕ(x)$）到高维特征空间𝓩\n$$ K(x,x')=\\phi(x)^T \\phi(x') $$ 为什么是：\n非线性带来高维转换:\n线性分类最完美的情况:二分类问题严格线性可分，即存在一个或多个线性超平面可以把两类正确分开，不同的初值最终收敛的结果不同。对于非线性输入无法收敛。\n对于不同的输入数据，采用不同的算法：\n线性可分 线性不可分 存在一点点非线性 严格非线性 PLA Pocket Algorithm 多层感知机(隐藏层数≥1,逼近任一连续函数);\n非线性转换(Cover Therom: 高维比低维更易线性可分) Hard-margin SVM Soft-margin SVM Kernel SVM(先做非线性转换，再做SVM) 对偶表示带来内积:\nSVM的思想是最大间隔分类，是一个（凸）优化问题。然后根据拉格朗日对偶性，转化为求解原问题的对偶问题:\n$$ \\begin{array}{c} \\begin{array}{cc} 原问题\\ \\begin{cases} \\underset{\\mathbf w,b}{\\operatorname{min}}\\ \\frac{1}{2} \\mathbf w^T \\mathbf w \\ s.t. \\quad y_i(\\mathbf w^T x_i + b) \\geq 1 \\end{cases} \\end{array}\n\\Longrightarrow \\begin{array}{cc} 对偶问题\\ \\begin{cases} \\underset{\\lambda}{\\operatorname{min}}\\ \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j \\mathbf x_i^T \\mathbf x_j - \\sum_{i=1}^N \\lambda_i \\ s.t. \\quad \\lambda_i \\geq 0, \\quad \\forall i=1,\u0026hellip;,N \\ \\ \\qquad \\sum_{i=1}^N \\lambda_i y_i =0 \\end{cases} \\end{array}\n\\end{array}\n$$\nDual Problem 包含内积: $\\mathbf x_i^T \\mathbf x_j$，即需要求解任意两个数据之间的内积\n而对于非线性可分问题，做了非线性转换之后，内积变为：$\\phi(x_i)^T \\phi(x_j)$，但是对于高维空间的 $\\phi(x)$，由于维度太高很难求。 所以希望找到一个函数直接求内积: $K(\\mathbf{x,x'})$，而避免求单个特征的 $\\phi(x)$\n数学表示：\n$$ \\forall \\mathbf{x,x'} \\in \\mathcal X,\\quad \\exist \\phi: \\mathcal X \\rightarrow \\mathcal Z s.t. \\quad K(\\mathbf{x,x'}) = \\phi(\\mathbf x)^T \\phi(\\mathbf x') = \u003c\\phi(\\mathbf x) \\phi(\\mathbf x')\u003e 则称：K(\\mathbf{x,x'}) 是一个核函数 $$将(输入空间的)样本代入核函数即可算出(高维空间的)内积，减少了计算量\n","date":"2021-11-10T13:36:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/07_%E6%A0%B8%E6%96%B9%E6%B3%95/","title":"watch: ML - 白板 07 | Kernel Method"},{"content":"向量投影到哪个方向，就用该方向的方向向量的转置乘以向量\n把$[-3,4]^T$ 投影到 $[0,1]^T$ 方向上，就是：$[0, 1] \\begin{bmatrix} -3 \\\\ 4 \\end{bmatrix} = 4$\n把 $[-4, 1]^T$ 投影到 $[1,2]^T$ 方向上，先对投影向量的模长归一: $[\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}]^T$， 然后 $[\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}] \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix} =\\frac{-2}{\\sqrt{5}}$\n三维空间，$\\mathbf x$ 在$(\\mathbf x_1, \\mathbf x_2)$ 的投影变换到以 $(\\mathbf e_1, \\mathbf e_2)$ 为基（相互垂直）的坐标系下。向量没变，坐标变了。$\\mathbf x$ 分别在$\\mathbf{e_1,e_2}$ 方向上投影，得到新坐标：\n$$ \\begin{aligned} \\mathbf e_1^T \\mathbf x \u0026amp;= \\left[\\frac{1}{\\sqrt{2}}\\ \\frac{1}{\\sqrt{2}}\\ 0 \\right] \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\\n\\mathbf e_2^T \\mathbf x \u0026amp;= \\left[\\frac{1}{\\sqrt{2}}\\ -\\frac{1}{\\sqrt{2}}\\ 0\\right] \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\end{aligned}\n\\Longrightarrow\n\\underbrace{ \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; 0 \\\\ \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{2}} \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} }_{三维到二维，降维}\n\\Rightarrow\n\\begin{bmatrix} \\mathbf e_1^T \\ \\mathbf e_2^T \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} $$\n投影矩阵\n$$ A \\coloneqq [\\mathbf e_1 \\ \\mathbf e_2] = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\\\ 0 \u0026 0 \\end{bmatrix} $$所以坐标变换，相当于左乘投影矩阵的转置 $A^T$\n$$ \\begin{bmatrix} \\mathbf e_1^T \\\\ \\mathbf e_2^T \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} = A^T \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} $$主成分之间是相互独立的，互相垂直\n如上图，选 $\\mathbf e_1$ 为拉得最开的方向，是为了方便找 $\\mathbf e_2$ 方向上的最大最小值 ","date":"2021-11-04T09:55:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/pca/","title":"PCA"},{"content":"Maximum Variance Unfolding\n它找到一个好的核函数，使 \u0026ldquo;所有转换后的数据点之间的距离之和 \u0026ldquo;在 \u0026ldquo;邻域图中的距离 \u0026ldquo;不变的约束下达到最大。直观地说，当一个流形被适当地展开时，各点的方差是最大的。因此，目标函数旨在展开变换空间中的数据点（由核函数诱导），同时，约束条件保证在展开时满足局部属性。\n给定 n 个点 $X={x_1, \\cdots, x_n}$，MVU 首先构建一个 neighborhood graph G，其中每个点 $x_i$ 都和离它最近的 k 个点有连接。\n找一个图形 $Y={y1, \\cdots,y_n}$，其满足：$\\| y_i - y_j \\|^2 = \\|x_i - x_j \\|^2,\\ for \\forall (i,j) \\in G$，然后对Y中各点间的距离之和取最大：\n$$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} \\sum_{ij} | y_i -y_j |^2 \\\n\u0026amp; \\text{subject to}\\ |y_i - y_j |^2 = | x_i - x_j |^2 ,\\ \\text{for } \\forall (i,j) \\in G \\end{aligned} $$\n可以通过引入一个核函数($k: X \\times X \\rightarrow R$)找到一个合理而可行的Y的近似。\n因此需要找到一个好的核函数，在变换后的空间中，所有变换后的数据之间的距离之和是最大，满足3个限制条件：\n核函数有效; 变换后数据点的均值为零;(便于写目标函数) $\\|\\phi(x_i) - \\phi(x_j) \\|^2 = \\| x_i - x_j \\|^2 ,\\ for \\forall (i,j) \\in G$ $$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} \\sum_{ij} |\\phi(x_i) - \\phi(x_j) |^2 \\\n\u0026amp; \\text{subject to} \\begin{cases} K \\text{ is positive semidefinite} \u0026amp; \\text{有效}\\ | \\sum_i \\phi(x_i) |^2 = 0 \u0026amp; \\text{零均值}\\ |\\phi(x_i) - \\phi(x_j) |^2 = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G \\end{cases} \\end{aligned} $$\n首先改写约束条件2：\n$$ \\| \\sum_i \\phi(x_i) \\|^2 = \\left( \\sum_i \\phi(x_i) \\right)^T \\left( \\sum_j \\phi(x_j) \\right) = \\sum_{ij} \\phi(x_i)^T \\phi(x_j) = \\sum_{ij} k_{ij} = 0 $$约束条件3的左边部分：\n$$ \\|\\phi(x_i) - \\phi(x_j) \\|^2 = \\left( \\phi(x_i) - \\phi(x_j) \\right)^T \\left( \\phi(x_i) - \\phi(x_j) \\right) = k_{ii} + k_{jj} - 2k_{ij} $$改写目标函数：\n$$ \\sum_{ij} \\| \\phi(x_i) - \\phi(x_j) \\|^2 = \\sum_{ij} (k_{ii} + k_{jj} - 2 k_{ij} ) = \\sum_{ij} k_{ii} + \\sum_{ij} k_{jj} -2\\sum_{ij} k_{ij} = 2n \\sum_i k_{ii} - 2 \\cdot 0 = 2n \\cdot tr(K) $$最终，优化问题是一个半正定规划问题(SDP)：\n$$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} tr(K) \\\n\u0026amp; \\text{subject to} \\begin{cases} K \\text{ is positive semidefinite} \u0026amp; \\text{有效}\\ \\sum_{ij} k_{ij} = 0 \u0026amp; \\text{零均值} \\ k_{ii} + k_{jj} - 2k_{ij} = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G \\end{cases} \\end{aligned} $$\nSDP 的解K 是用于输入Kernel PCA的核矩阵。\nstat946f10-uwaterloo\nMVU is a variation of Kernel PCA\n","date":"2021-11-03T12:37:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/projecttips/mvu_notes/","title":"MVU notes"},{"content":"3 component of using Learning Pattern exists Pin down mathematically have data (most important) Learning set up Unkown target function Dataset: Learing algorithm picks $g\\appriox f from hypothesis set H Perceptron Learning Algorithm Feasibility of learning. In order to establish that learning setup and modifier the data, and in order to answer if that learning is feasible. We said that we gonna restart with specific __ and we went an example of bin. I just quickly review that.\nWe suppose to have a bin. In bin there are red marbles and green marbles as we see. And probility that if you pick a red marble is called $\\mu$, and a probability that you pick a\nBIN Model Bin with red and blue marbles; Pick a sample of N marble independently\n$\\mu$: probability to pick a red marbles from the bin (blue: 1-$\\mu$) $\\nu$: fraction of red marbles in the sample In a large sample (large N), ν is probably close to μ (within tolerance ε)\nHoeffding\u0026rsquo;s Inequality:\n$$ \\begin{aligned} P[Bad] \u0026= P[|ν-μ|\u003eε]≤ 2e^{-2ε^2N}, \u0026 \\text{for any ε\u003e0} \\\\ P[Good] \u0026= P[|ν-μ|≤ε]\u003e 1-2e^{-2ε^2N},\u0026 \\text{for any ε\u003e0} \\end{aligned} $$ N is large, ε is large, the P[Bad] becomes small, ν and μ are very close to each other.\nN=1000, ε=0.05, the probability of ν-ε ≤ μ ≤ ν+ε is 0.986 N=1000, ε=0.1, the probability of ν-ε ≤ μ ≤ ν+ε is 0.999 μ∈[ν-ε, μ+ε], error bar is ±ε learn from ν and reach outside the data (μ). There still is a probability of getting wrong sample, but not often.\nμ≈ν is probably approximately correct (PAC-learning)\n\u0026ldquo;probably\u0026rdquo; : probabilty 2exp(-2 ε^2 N) \u0026ldquo;approximately\u0026rdquo; : tolerance ε\n","date":"2021-10-22T16:03:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/lec2_learning-feasible/","title":"watch: AML 02 | Learning Feasibility"},{"content":"SVM SVM本质上是一个判别模型，解决二分类问题，与概率无关 超平面: 𝐰ᵀ𝐱+b=0 分类模型：f(𝐰) = sign(𝐰ᵀ𝐱+b) SVM有3宝，间隔对偶核技巧 三大分类算法： Hard-margin SVM （硬间隔） Soft-margin SVM Kernel SVM 1 硬间隔SVM-模型定义（最大间隔分类器） Video-P1\nHard-margin SVM 最大间隔分类器（max margin()）\n几何意义：对于平面上点的分类问题， 如果分界线两侧的点到线的距离很小，会对噪声很敏感，如果有一点噪声可能就被分到另一侧去了，泛化误差大。所以最好的分界线满足对所有点的距离都足够大。\n从无限条可以正确分类的直线（超平面）中，选择最好的\n数学表述：\n使间隔margin函数最大，实现对N个p维的样本点 ${(x_i, y_i)}_{i=1}^{N}, \\quad x_i \\in \\R^p, y\\in\\{-1,1\\}$正确分类\n$$ \\begin{aligned} \u0026amp; \\rm max , margin(\\mathbf w, b) \\\n\u0026amp; s.t. \\begin{cases} \\mathbf w^T x_i + b \u0026gt; 0, \u0026amp; y_i = 1 \\ \\mathbf w^T x_i + b \u0026lt; 0, \u0026amp; y_i = -1 \\end{cases}\n\\Rightarrow y_i(\\mathbf w^Tx_i +b )\u0026gt;0, \u0026amp; \\text{for $\\forall$ i=1,\u0026hellip;,N}（同号） \\end{aligned} $$\n每个点到直线的距离: $\\rm distance(w,b,x_i) = \\frac{1}{\\| w \\|} |w^T x_i +b|$\n$$ \\begin{aligned} \\rm margin(\\mathbf w,b) \u0026amp; = \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, distance(\\mathbf w,b,x_i) \\ \u0026amp; = \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, \\frac{1}{| \\mathbf w |} |\\mathbf w^T x_i +b|\n\\end{aligned} $$\n因为限制条件 $y_i(\\mathbf w^T x_i + b)\u003e0$，而且$y_i=\\{-1,1\\}$，所以可以替换上式中的绝对值。\n所以最大间隔分类器可写为：\n$$ \\rm \\underset{\\mathbf w,b}{max} \\frac{1}{\\| \\mathbf w\\|}\\; \\underset{x_i, i=1,...,N}{min} \\; |\\mathbf w^T x_i +b| $$(因为min只与$x_i$有关，而$\\frac{1}{\\| \\mathbf w\\|}$与$x$无关，所以移到了前面)\n对于限制条件 $y_i(\\mathbf w^T x_i + b)\u003e0$，说明存在最小值：\n$$ \\exist \\, r\u003e0, \\quad s.t.\\; \\rm \\underset{\\mathbf w,b,x_i,i=1,...,N}{min}\\, y_i (\\mathbf w^T x_i +b) = r $$所以可以继续简化：\n$$ \\rm \\underset{\\mathbf w,b}{max} \\frac{1}{\\| \\mathbf w\\|} r $$因为超平面可以同比例缩放（$\\mathbf{w^T x} + b = 2\\mathbf{w^T x} + 2b$），所以可以设置最小值r为1，相当于把超平面缩放到1，系数乘在前面的$\\frac{1}{\\| \\mathbf w\\|}$里\n所以问题最终转化为一个凸优化问题：\n$$ \\begin{cases} \\rm \\underset{w,b}{max} \\frac{1}{| \\mathbf w |} \\ s.t. ; \\operatorname{min} y_i (\\mathbf w^T x_i + b) = 1 \\end{cases}\n\\Rightarrow\n\\begin{cases} \\rm \\underset{w,b}{min} \\frac{1}{2} \\mathbf{w^T w} \u0026amp; \\text{(目标函数是二次)}\\ s.t. ; y_i (\\mathbf w^T x_i + b) ≥ 1, ; i=1,\u0026hellip;,N \u0026amp; \\text{(N个线性约束)} \\end{cases} $$\n求解QP问题 (凸二次规划Quadratic programming问题)\n在维度不高，样本个数不多的情况下，比较好求解。 但是对于维度很高，样本很多，或者对数据$x$做$\\phi(x)$的特征转换到了新的特征空间$Z$中，而Z的维度比原数据的维度高很多，就没办法直接求解，计算量太大。 借助拉格朗日乘子，引出它的对偶问题，求解相对容易的对偶问题。\n把目标函数写成拉格朗日函数 $L$，把带约束问题化成无约束问题：拉格朗日函数只有对 λ 的约束，没有对 $𝐰$ 和 $b$ 的约束\n$$ L(\\mathbf w,b,λ)= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} \\; \\underbrace{λ_i}_{≥0} \\; \\underbrace{(1-y_i(𝐰^T x_i + b))}_{≤0} $$问题转化为：\n$$ \\begin{cases} \\rm \\underset{\\mathbf w,b}{min}\\ \\underset{λ}{max}\\ L(\\mathbf w,b,λ)\\\\ s.t. \\; λ_i ≥0 \\end{cases} $$ 原问题的对偶问题：先对𝐰,b求L的最小值，再对 λ 求 L 的最大值\n$$ \\begin{cases} \\rm \\underset{λ}{max}\\ \\underset{\\mathbf w,b}{min}\\ L(\\mathbf w,b,λ)\\\\ s.t. \\; λ_i ≥0 \\end{cases} $$从直观上看，从最大值里面选的最小值一定是大于从最小值里面的最大值（省略证明）,也就是弱对偶关系：\n$$ \\rm min\\, max\\, L ≥ max \\, min\\, L $$我们还想要强对偶关系，即 $\\rm min\\, max\\, L = max \\, min\\, L$。对于凸二次优化问题，天生满足强对偶关系（证明略），原问题和它的对偶问题是同解的。所以直接求解对偶问题即可。\n先固定 $λ$，对 $𝐰,b$ 求L的最小值，在此对偶问题中没有对𝐰,b的约束条件，所以是一个无约束的优化问题，直接求导：\n先对b求导：\n$$ \\begin{aligned} \\frac{∂L}{∂b} \u0026= \\frac{∂}{∂b} \\left[ \\cancel{\\sum_{i=1}^{N} λ_i} - \\sum_{i=1}^{N} λ_i y_i(\\cancel{\\mathbf w^T x_i} + b)\\right] \\\\ \u0026= \\frac{∂}{∂b} \\left[ -\\sum_{i=1}^{N} λ_i y_i b \\right] \\\\ \u0026= -\\sum_{i=1}^{N} λ_i y_i =0 \\end{aligned} $$把这个结果带入$L$\n$$ \\begin{aligned} L(\\mathbf w,b,λ) \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} ; λ_i ; (1-y_i(\\mathbf w^T x_i + b))\\ \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} \\ λ_i - \\sum_{i=1}^{N} ; λ_i y_i(\\mathbf w^T x_i + b)\\ \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} \\ λ_i - \\sum_{i=1}^{N} ; λ_i y_i\\mathbf w^T x_i\n\\cancel{\\sum_{i=1}^{N} ; λ_i y_i b} \\ \\end{aligned} $$\n再对$𝐰$求导，定义导数为等于0\n$$ \\begin{aligned} \u0026 \\frac{∂L}{∂𝐰} = \\frac{1}{2} 2𝐰 - \\sum_{i=1}^N λ_i y_i x_i ≔ 0 \\\\ \u0026 \\Rightarrow 𝐰^* = \\sum_{i=1}^{N} λ_i y_i x_i \\end{aligned} $$把这个最优$𝐰^*$的表达式再带入$L$，就是L的最小值：\n$$ \\begin{aligned} L(\\mathbf w,b,λ) \u0026amp; = \\frac{1}{2} \\underbrace{ \\left(\\sum_{i=1}^{N} λ_i y_i 𝐱_i \\right)^T}{\\sum{i=1}^N λ_i y_i 𝐱_i^T } \\sum_{j=1}^N λ_j y_j 𝐱_j\n\\sum_{i=1}^N \\ λ_i y_i \\left(\\sum_{j=1}^{N} λ_j y_j 𝐱_j \\right)^T 𝐱_i \\sum_{i=1}^N λ_i \\ \u0026amp; = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N λ_i λ_j y_i y_j \\underline{𝐱_i^T 𝐱_j} \\sum_{i=1}^N λ_i y_i \\sum_{j=1}^N λ_j y_j \\underline{𝐱_j^T 𝐱_i} \\sum_{i=1}^N λ_i \\ \u0026amp; = -\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N λ_i λ_j y_i y_j 𝐱_i^T 𝐱_j \\sum_{i=1}^N λ_i \\ \\end{aligned} $$ 所以对偶问题又化为：\n$$ \\begin{cases} \\underset{λ}{\\operatorname{min}} ; \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N λ_i λ_j y_i y_j 𝐱_i^T 𝐱_j\n\\sum_{i=1}^N λ_i \\ s.t. \\ λ_i ≥0; \\ \\sum_{i=1}^N λ_i y_i =0 \\end{cases} $$ 固定 $\\mathbf w,b$, 对于向量$λ$求$L$的最小值\n此时拉格朗日函数只与 $\\lambda$ 有关\n求出 $\\lambda$，根据 $\\mathbf w = \\sum_{i=1}^{m} \\lambda_i y^{(i)} x^{(i)}$ 就可求出 $\\mathbf w$，再根据 $b^* = -\\frac{\\rm max\\ x_{i:y^{(i)}=-1} \\mathbf w^{*T} x^{(i)} + min_{i:y^{(i)}} \\mathbf w^{*T} x^{(i)}}{2}$，就可求出 $b$，最终得到分离超平面和分类决策函数。\nKKT条件 原问题与对偶问题具有强对偶关系的充要条件就是满足KKT条件\n$$ \\begin{cases} \\frac{∂L}{∂𝐰} =0,\\frac{∂L}{∂b} =0,\\frac{∂L}{∂λ} =0 \u0026\\text{拉格朗日函数对w，对b，对λ 求偏导都等于0}\\\\ λ_i (1-y_i(𝐰^T x_i + b)) = 0 \u0026 \\text{松弛互补条件slackness complementary} \\\\ λ_i ≥ 0 \\\\ 1-y_i(𝐰^T x_i + b)≤0 \\end{cases} $$ 由这些条件就可以求出最优解的$w^*$和$b^*$\n由 ∂L/∂𝐰 可求出𝐰∗=∑ᵢ₌₁ᴺ λᵢ yᵢ xᵢ\n4 软间隔SVM-模型定义 Video-P4\nSoft-margin SVM 允许一点点错误 loss\n$$ min \\frac{1}{2} w^T w + loss $$ loss函数:\n违反约束条件的点的个数：$loss = \\sum_{i=1}^N I \\left\\{ \\underbrace{y_i (w^T x_i + b)\u003c1}_{关于w不连续} \\right\\}$ （I是指示函数）\n不连续性： 令$z=y(\\mathbf w^T \\mathbf x+b)$，则 $loss_{0/1} = \\begin{cases} 0, \u0026\\text{z\u003c1} \\\\\\\\ 0, \u0026otherwise \\end{cases}$\n在z=1处有跳跃，不连续导致求导有问题。\n用距离\n超平面 能把 n 维欧式空间分成两部分的 n-1 维子空间。\nn 维空间 $\\R^n$ 的超平面是由方程：$𝐰^T 𝐱 + b = 0$ 定义的子集。（𝐰 和 𝐱 都是n维向量）\n法向量与超平面内任一向量垂直。假设在三维空间中，水平面内的一个向量 $𝐱-𝐱'$ 与法向量 $\\mathbf w$ 垂直，如下图(源自如何理解超平面？)：\n满足：\n$$ \\begin{aligned} (𝐱-𝐱')\\mathbf w \u0026= 0 \\\\ (x_1 - x_1', x_2 - x_2', x_3 - x_3') \\cdot (w_1, w_2, w_3) \u0026= 0 \\\\ x_1 w_1 + x_2 w_2 + x_3 w_3 \u0026= w_1 x_1' + w_2 x_2' + w_3 x_3' \\\\ \\mathbf w^T \\mathbf x \u0026= \\mathbf w^T \\mathbf x' \\end{aligned} $$由于 $\\mathbf w^T \\mathbf x'$ 是常数项，$-\\mathbf w^T \\mathbf x' ≔ b$， 所以超平面的公式可写为：\n$$ \\mathbf w^T \\mathbf x + b = 0 $$ 点到超平面距离 点到超平面的函数距离，除以法向量的范数\n求平面外一点 $\\mathbf x$ 到平面的距离 d。\n根据三角函数：$cos \\theta = \\frac{d}{\\| \\mathbf x-\\mathbf x' \\|}$ (空间中一点向超平面作垂线，$\\theta$只能是锐角，不必担心正负)\n$\\mathbf x-\\mathbf x'$ 与 $\\mathbf w$ 的内积为： $|(\\mathbf x-\\mathbf x') \\cdot \\mathbf w | = \\|\\mathbf x-\\mathbf x' \\| \\cdot \\| \\mathbf w \\| \\cdot cos \\theta$ （因为法向量可能反向，所以给等式左边加上绝对值）\n联立可得：\n$$ d = \\dfrac{|(𝐱 - 𝐱') 𝐰 |}{\\| 𝐰 \\|} = \\dfrac{|𝐰𝐱 - 𝐰𝐱'|}{\\| 𝐰 \\|} $$因为 $𝐱'$在超平面内，$𝐰𝐱' = -b$，于是最后得到的任意点到超平面的距离公式：\n$$ d = \\frac{|𝐰𝐱+b|}{\\| 𝐰 \\|} $$ 几何距离与函数距离 几何距离：点到直线（超平面）距离 函数距离：$Δy$，直线（超平面）上的点y=0，所以不在直线上的点到直线的函数距离就是点的y值。 ","date":"2021-10-13T18:59:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/06_svm/","title":"watch: ML - 白板 06 | SVM"},{"content":"P4\n3D Transformations 3D Scale $$ \\mathbf S(s_x, s_y, s_z) = \\begin{pmatrix} s_x \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 s_y \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 s_z \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{pmatrix} $$3D Translation $$ \\mathbf T(t_x,t_y,t_z) = \\begin{pmatrix} 1 \u0026 0 \u0026 0 \u0026 t_x \\\\\\ 0 \u0026 1 \u0026 0 \u0026 t_y \\\\\\ 0 \u0026 0 \u0026 1 \u0026 t_z \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{pmatrix} $$Rotaion around axises 绕x轴旋转：\nx方向坐标不变，所以旋转矩阵各列向量的x分量都不贡献，所以第一行是1 0 0；\n为了保证逆时针旋转是正向旋转，其余两轴的顺序如图：\ny-z平面旋转，用待定系数法或展开三角函数，可得旋转关系为： $\\begin{cases} x'=x \\\\\\\\ y'=ycosθ-zsinθ \\\\\\\\ z'=ysinθ+zcosθ \\end{cases}$，\n则旋转矩阵为 $\\begin{pmatrix} x' \\\\\\\\ y' \\\\\\\\ z' \\end{pmatrix} = \\begin{pmatrix} 1\u00260\u00260\\\\\\\\ 0 \u0026 cosθ \u0026 -sinθ \\\\\\\\0 \u0026 sinθ \u0026 cosθ \\end{pmatrix} \\begin{pmatrix} x\\\\\\\\y\\\\\\\\z\\end{pmatrix}$；\n写成齐次坐标，不考虑平移，所以仿射变换矩阵为：\n$$ R_x(α)= \\begin{pmatrix} 1 \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 cosθ \u0026 -sinθ \u0026 0 \\\\\\ 0 \u0026 sinθ \u0026 cosθ \u0026 0 \\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\\\\\ \\end{pmatrix} $$ 绕y轴旋转：\ny方向不变，所以矩阵第2行为：0 1 0；\nx-z平面旋转，用待定系数法，可得旋转关系： $\\begin{pmatrix}z'\\\\\\\\x' \\end{pmatrix} = \\begin{pmatrix}cosθ\u0026 -sinθ\\\\\\\\sinθ \u0026 cosθ\\end{pmatrix} \\begin{pmatrix} z\\\\\\\\x \\end{pmatrix}$ (或者展开三角函数，可得旋转关系为： $\\begin{cases} x'=zsinθ+xcosθ \\\\\\\\ z'=zcosθ-xsinθ\\end{cases}$) 写成旋转矩阵为：\n$$ \\begin{pmatrix} x' \\\\\\ y' \\\\\\ z' \\end{pmatrix} = \\begin{pmatrix} cosθ \u0026 0 \u0026 sinθ \\\\\\ 0 \u0026 1 \u0026 0 \\\\\\ -sinθ \u0026 0 \u0026 cosθ \\end{pmatrix} \\begin{pmatrix} x \\\\\\ y \\\\\\ z \\end{pmatrix} $$三个轴循环对称：$x\\boxed{yzx}y\\boxed{zxy}z\\boxed{xyz}$，即给定任意两个可得后面一个，所以Y是由$Z×X$的到的 $\\begin{pmatrix}Z\\\\\\\\ X\\\\\\\\ Y\\end{pmatrix}$，这与通常书写 矩阵顺序$\\begin{pmatrix}X\\\\\\\\ Y\\\\\\\\ Z\\end{pmatrix}$不同，导致系数易位，$-sinθ$不在左上角，看起来与绕其他两轴的旋转矩阵不一致。\n绕z轴旋转：\nz方向不变，所以矩阵第3行为：0 0 1；\nx-y平面旋转，有旋转关系 $\\begin{cases}x'=rcos(α+θ)\\\\\\\\y'=rcos(α+θ)\\end{cases}$ 以及 $\\begin{cases}x=rcosα\\\\\\\\y=rsinα\\end{cases}$ 可得：$\\begin{cases}x'=xcosθ-ysinθ\\\\\\\\y'=ycosθ+xsinθ\\end{cases}$ 写成旋转矩阵为：\n$$ \\begin{pmatrix} x' \\\\\\ y' \\\\\\z' \\end{pmatrix} = \\begin{pmatrix} cosθ \u0026 -sinθ \u0026 0 \\\\\\ sinθ \u0026 cosθ \u0026 0 \\\\\\ 0 \u0026 0 \u0026 1 \\\\\\ \\end{pmatrix} \\begin{pmatrix} x \\\\\\ y \\\\\\z \\end{pmatrix} $$ 任意三维旋转 渲染 给三维物体拍照片 摆好模型 Model transformation 摆好照相机 View transformation 投影到相片 Projection transformation View Transformation 通过旋转和平移，把世界坐标系中的相机调整到标准姿态\n相机的姿态包括：位置、朝向和旋转\n标准姿态：\n光心在原点； 朝着-z方向看； 向上是y轴 从标准姿态变换到当前姿态，再取逆，就是视图变换\n正交投影 平行光 把空间中的长方体映射到一个边长为2的立方体（左右下上远近的边界都是1） 中心平移到原点: $\\begin{pmatrix} 1 \u0026 0 \u0026 0 \u0026 -\\frac{r+l}{2} \\\\\\\\ 0 \u0026 1 \u0026 0 \u0026 -\\frac{t+b}{2} \\\\\\\\ 0 \u0026 0 \u0026 1 \u0026 -\\frac{n+f}{2} \\\\\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{pmatrix}$ 缩放到2: $\\begin{pmatrix}\\frac{2}{r-l} \u0026 0 \u0026 0 \u0026 0 \\\\\\\\ 0 \u0026 \\frac{2}{t-b} \u0026 0 \u0026 0 \\\\\\\\ 0 \u0026 0 \u0026 \\frac{2}{n-f} \u0026 0 \\\\\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\end{pmatrix}$ 透视投影 视锥 近大远小 把棱台挤压成长方体，长方体再做正交投影 远平面缩放到与近平面相同大小\n根据相似，确定不同深度的缩放系数\n$$ \\begin{cases} y'=\\frac{n}{z}y \\\\\\ x'=\\frac{n}{z}x \\\\\\ z'=unknow \\end{cases} $$远平面缩放之后各点坐标：\n$$ \\begin{pmatrix} \\frac{nx}{z} \\\\\\ \\frac{ny}{z} \\\\\\ unknown \\\\\\ 1 \\end{pmatrix} \\overset{乘以深度z}{==} \\begin{pmatrix} nx \\\\\\ ny \\\\\\ unkown \\\\\\ z \\end{pmatrix} $$ 确定$M\\_{persp→ortho}$\n$$ M\\_{persp→ortho}^{(4×4)} \\begin{pmatrix} x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{pmatrix}= \\begin{pmatrix} nx \\\\\\ ny \\\\\\ unknown \\\\\\ z \\end{pmatrix} $$$$ M_{persp→ortho}^{(4×4)} = \\begin{pmatrix} n \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 n \u0026 0 \u0026 0 \\\\\\ ? \u0026 ? \u0026 ? \u0026 ? \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ \\end{pmatrix} $$近平面上的点作用之后不变：\n$$ \\begin{pmatrix} n \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 n \u0026 0 \u0026 0 \\\\\\ ? \u0026 ? \u0026 ? \u0026 ? \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ \\end{pmatrix} \\begin{pmatrix} x \\\\\\ y \\\\\\ n \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} x \\\\\\ y \\\\\\ n \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} nx \\\\\\ ny \\\\\\ n^2 \\\\\\ n \\end{pmatrix} $$因为$n^2$与x,y无关，所以第三行为：(0 0 A B), $An+B=n^2$\n远平面的中心点也没变，满足：\n$$ \\begin{pmatrix} n \u0026 0 \u0026 0 \u0026 0 \\\\\\ 0 \u0026 n \u0026 0 \u0026 0 \\\\\\ 0 \u0026 0 \u0026 A \u0026 B \\\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\\\ \\end{pmatrix} \\begin{pmatrix} 0 \\\\\\ 0 \\\\\\ f \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ 0 \\\\\\ f \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\\\ 0 \\\\\\ f^2 \\\\\\ f \\end{pmatrix} $$所以$Af+B=f^2$\n联立可解得A,B:\n$$ \\begin{cases} An+B=n^2 \\\\\\ Af+B=f^2 \\end{cases} ⇒ \\begin{cases} A=n+f \\\\\\ B=-nf \\end{cases} $$ 透视投影矩阵：$M_{persp} = M_{ortho} M_{persp→ortho}$\n","date":"2021-09-21T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/games-101_cg/04-transform-cont/","title":"watch: CG - 闫令琪 04 | Transformation Cont."},{"content":"P5\n光栅化Rasterize 把物体画在屏幕上 把$[-1,1]^3$标准立方体(canonical cube)投影到屏幕上 viewing frustum 摄像机的视野\n图4 视体 三维世界中屏幕上可见的区域\n作用：确定哪些物体会被屏幕显示\n视体可用近平面的宽高比($\\frac{width}{height}$)和垂直可视角度定义\n屏幕 二维数组 每个元素是一个像素 ","date":"2021-09-20T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/games-101_cg/05-rasterization-1triangles/","title":"watch: CG - 闫令琪 05 | Rasterization-1(Triangles)"},{"content":"P3 Transformation\n旋转矩阵 乘以一个向量时，改变向量方向的矩阵\n表示了不同维度坐标的线性变换\n作用：把向量(默认绕原点、逆时针)的旋转用矩阵乘法表示\n对于一个二维向量，旋转关系用矩阵乘法形式为：\n$$ \\begin{pmatrix} x' \\\\\\ y' \\end{pmatrix} = M_{2×2} \\begin{pmatrix} x \\\\\\ y \\end{pmatrix} $$用待定系数法可以确定各元素\n对于(a,0)点：\n$$ \\begin{pmatrix} acosθ \\\\ asinθ \\end{pmatrix} = \\begin{pmatrix} A \u0026amp; B \\\\ C \u0026amp; D \\end{pmatrix}\n\\begin{pmatrix} a \\\\ 0 \\end{pmatrix} $$\n解得：$A=cosθ, C=sinθ$； 同理代入(0,b)点，可解得：$B=-sinθ, D=cosθ$，所以旋转矩阵为：\n$$ \\begin{pmatrix} cosθ \u0026 -sinθ \\\\\\ sinθ \u0026 cosθ \\end{pmatrix} $$ 旋转矩阵的逆 等于它的转置（$R_θ^{-1}=R_θ^T$）\n因为旋转矩阵是一个正交矩阵\n逆操作就是顺时针旋转相同的角度，也就是正向旋转$-θ$，代入得：\n$$ \\begin{pmatrix} cosθ \u0026 sinθ \\\\\\ -sinθ \u0026 cosθ \\end{pmatrix} $$即为旋转矩阵的转置\n齐次坐标 向量最后加个0，点最后加个1\n3D vector: $(x,y,z,0)^T$ 3D point: $(x,y,z,1)^T$\n作用：平移变换也可写成一个矩阵\n对于平移关系：\n$$ \\begin{cases} x' = x + t_x \\\\\\ y' = y + t_y \\end{cases} $$x,y方向都没有旋转，所以旋转矩阵为：\n$$ \\begin{pmatrix} 1 \u0026 0 \\\\\\ 0 \u0026 1 \\end{pmatrix} $$写成齐次坐标：\n$$ \\begin{pmatrix} 1 \u0026 0 \\\\\\ 0 \u0026 1 \\\\\\ 0 \u0026 0 \\end{pmatrix} $$只有平移，附加到后面：\n$$ \\begin{pmatrix} x' \\\\\\ y' \\\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026 0 \u0026 t_x \\\\\\ 0 \u0026 1 \u0026 t_y \\\\\\ 0 \u0026 0 \u0026 1 \\end{pmatrix} \\begin{pmatrix} x \\\\\\ y \\\\\\ 0 \\end{pmatrix} $$ 同一点的表示不唯一：\n$$ \\begin{pmatrix} x \\\\\\ y \\\\\\ z \\\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} kx \\\\\\ ky \\\\\\ kz \\\\\\ k \\end{pmatrix} $$ 仿射变换 旋转变换和平移变换拼成一个矩阵 ","date":"2021-09-19T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/games-101_cg/03-transformation/","title":"watch: CG - 闫令琪 03 | Transformation"},{"content":"画竖直和水平的分割线 1 2 3 plt.vlines(x, ymin, ymax) plt.hlines(y, xmin, xmax) 例：plt.vlines(0, 0, 0.5, colors = \u0026quot;r\u0026quot;, linestyles = \u0026quot;dashed\u0026quot;)\n贯穿 frame 的线 纵线从 frame 的 top to bottom: axvline\nReferences:\nGemini 2.5P - Adding Vertical Lines in Matplotlib Supports:\n(2025-08-12T05:55)\nax.axvline(x=5) r1-Gemini\n1 2 fig, ax = plt.subplots(1,1) ax.axvline(x=5, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;, linewidth=2, label=\u0026#39;x=5\u0026#39;) 纵坐标刻度变文字 参考Matplotlib：设置坐标轴范围，刻度，位置，自定义刻度名称，添加数据标签\n1 2 3 # 把纵坐标-2变为文字“really bad\u0026#34; plt.yticks([-2, -1.8, -1, 1.22, 3],[r\u0026#39;$really\\ bad$\u0026#39;, r\u0026#39;$bad$\u0026#39;, r\u0026#39;$normal$\u0026#39;, r\u0026#39;$good$\u0026#39;, r\u0026#39;$really\\ good$\u0026#39;]) 坐标显示范围 1 2 3 #设置坐标轴范围 plt.xlim((-5, 5)) plt.ylim((-2, 2)) 坐标轴设置 1 2 3 4 5 6 7 8 9 10 11 ax = plt.gca() # 设置上边和右边无边框 ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) # 设置x坐标刻度数字或名称的位置 ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) # 设置边框位置 ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;, 0)) 带箭头的x-y坐标系 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import matplotlib.pyplot as plt import numpy as np import math # 引入axisartist工具 import mpl_toolkits.axisartist as axisartist # 创建画布 # fig = plt.figure(figsize=(8,8)) fig = plt.figure(dpi = 100) # 使用axisartist.Subplot 方法创建一个绘图区对象 ax ax = axisartist.Subplot(fig, 111) # 将绘图区对象添加到画布中 fig.add_axes(ax) # 设置绘图区原来所有坐标轴隐藏 ax.axis[:].set_visible(False) # 添加新的坐标轴 ax.axis[\u0026#34;x\u0026#34;] = ax.new_floating_axis(0,0) #第一个0代表水平直线，第二个0代表直线经过0点 ax.axis[\u0026#34;y\u0026#34;] = ax.new_floating_axis(1,0) #1代表竖直直线，0代表直线经过0点 # 给 x轴,y轴 加上箭头 ax.axis[\u0026#34;x\u0026#34;].set_axisline_style(\u0026#34;-\u0026gt;\u0026#34;,size =1.0)#空心箭头 ax.axis[\u0026#34;y\u0026#34;].set_axisline_style(\u0026#34;-|\u0026gt;\u0026#34;,size=1.0)#实心箭头 # 设置刻度显示方向 ax.axis[\u0026#34;x\u0026#34;].set_axis_direction(\u0026#34;top\u0026#34;) ax.axis[\u0026#34;y\u0026#34;].set_axis_direction(\u0026#34;right\u0026#34;) # 绘制曲线： 两点连线 1 plt.plot([0,6],[0,0.5]) 箭头 1 plt.arrow(0,0,6,0.5,head_width=0.1,head_length=0.1,overhang=1,ec=\u0026#34;deepskyblue\u0026#34;,linestyle=\u0026#34;:\u0026#34;) 用注释画箭头 1 2 3 ax=plt.gca() ax.annotate(\u0026#34;隐逝波\u0026#34;, xy=(0,0), xytext=(6,0.5), arrowprops=dict(arrowstyle=\u0026#34;\u0026lt;-\u0026#34;, color=\u0026#34;deepskyblue\u0026#34;, linestyle=\u0026#34;:\u0026#34;) ) 隐藏坐标轴 1 plt.xticks([]), plt.yticks([]) # 隐藏x和y轴 坐标轴标签 函数原型及参数 matplotlib.pyplot.xlabel(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)\nxlabel：类型为字符串，即标签的文本。 labelpad：类型为浮点数，默认值为None，即标签与坐标轴的距离。 loc：取值范围为{‘left’, ‘center’, ‘right’}，默认值为rcParams[“xaxis.labellocation”]（‘center’），即标签的位置。 **kwargs：Text 对象关键字属性，用于控制文本的外观属性，如字体、文本颜色等。 ","date":"2021-06-30T10:57:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/python/python_matplot/","title":"memo: Python | Matplotlib"},{"content":"\n本系列笔记刊载的所有内容，包括文字、图片等，均来自云南大学丁怀义老师的课程《电磁场理论与计算》以及他的讲义，版权归丁怀义老师（dinghy@ynu.edu.cn）所有。\n[toc]\n第五章 研究有源（电荷和电流）体系中电磁场的产生。\n标势矢势 标势：电场的势函数；矢势：磁场的势函数。\n磁场散度等于零，任意一个矢量场旋度的散度都等于零，所以可以用一个矢势$\\pmb A$来表示磁场。\n动电场的旋度不为零(不是保守场)，而是等于 $\\pmb B$ 的时间微分加负号，把磁场用矢量场 $\\pmb A$ 表示，即可发现一个新的保守场：$\\pmb E + \\frac{∂\\pmb A}{∂t}$，这个保守场可以写成标量场 $φ$ 的负梯度，所以可用这个标势 $φ$ 来表示电场。\n真空中没有自由电荷、自由电流，所以麦克斯韦方程组为：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb E = 0 \\\\ \\pmb ∇ \\pmb × \\pmb E = -\\frac{∂ \\pmb B}{∂t} \\\\ \\pmb ∇ \\cdot \\pmb B = 0 \\\\ \\pmb ∇ \\pmb × \\pmb B = μ_0 ε_0 \\frac{∂ \\pmb E}{∂t} \\end{cases} \\tag{5.1.1} $$在介质内部也有电荷和电流对电磁场的贡献，但是做了线性近似之后，可以将其贡献写在介电常数ε和磁导率μ上，其他的电荷和电流称为自由电荷和自由电流，其为零时，得到的方程依然类似于真空中的电磁场。\n对于有随时间变化的电流$\\pmb J(\\pmb x,t)$和电荷$\\pmb ρ(\\pmb x, t)$的情况，麦克斯韦方程组为：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb E = \\frac{\\pmb ρ(\\pmb x, t)}{ε_0}\\\\ \\pmb ∇ \\pmb × \\pmb E = -\\frac{∂ \\pmb B}{∂t} \\\\ \\pmb ∇ \\cdot \\pmb B = 0 \\\\ \\pmb ∇ \\pmb × \\pmb B = μ_0 \\pmb J(\\pmb x,t) + μ_0 ε_0 \\frac{∂ \\pmb E}{∂t} \\end{cases} \\tag{5.1.2} $$在解静电磁场方程组5.1.1时，将两个旋度方程再取旋度，等号右侧就出现了对方，也就出现了自己，就可以写成波动方程的形式。将此过程应用于动电磁场方程组5.1.2：\n$$ \\begin{array}{c} \\begin{cases} \\begin{aligned} \u0026 \\pmb ∇ × (\\pmb ∇ × \\pmb E) = -\\frac{∂}{∂t} (\\pmb ∇ × \\pmb B) = -μ_0 \\frac{∂ \\pmb J(\\pmb x,t)}{∂t} - μ_0 ε_0 \\frac{∂^2 \\pmb E}{∂t^2} \\quad (交替时空微分)\\\\ \u0026 \\pmb ∇ × (\\pmb ∇ × \\pmb B) = μ_0 \\pmb ∇ × \\pmb J(\\pmb x,t) + μ_0 ε_0 \\frac{∂ \\pmb ∇ × \\pmb E}{∂t} = μ_0 \\pmb ∇ × \\pmb J(\\pmb x,t) - μ_0 ε_0 \\frac{∂^2 \\pmb B}{∂t^2} \\end{aligned} \\end{cases} \\\\ \\begin{cases} \\pmb ∇ × (\\pmb ∇ × \\pmb E) = \\pmb ∇ (\\pmb ∇ \\cdot \\pmb E) - \\pmb ∇^2 \\pmb E = \\frac{1}{ε_0} \\pmb ∇ ρ(\\pmb x,t) - \\pmb ∇^2 \\pmb E \\\\ \\pmb ∇ × (\\pmb ∇ × \\pmb B) = \\pmb ∇ (\\pmb ∇ \\cdot \\pmb B) - \\pmb ∇^2 \\pmb B = - \\pmb ∇^2 \\pmb B \\end{cases} \\end{array} $$从而可得:\n$$ \\large \\begin{cases} \\pmb ∇^2 \\pmb E - μ_0 ε_0 \\frac{∂^2 \\pmb E}{∂ \\pmb t^2} = \\frac{1}{ε_0} \\pmb ∇ ρ(\\pmb x,t) + μ_0 \\frac{∂ \\pmb J(\\pmb x,t)}{∂ t} \\\\ \\pmb ∇^2 \\pmb B - μ_0 ε_0 \\frac{∂^2 \\pmb B}{∂ t^2} = - μ_0 \\pmb ∇ × \\pmb J(\\pmb x,t) \\end{cases} $$可以看出有源场的情况下，会出现电流和电荷的旋度和梯度，使得求解并非简单，所以\u0026hellip;\n静电场和静磁场中标势和矢势的定义\n由： $ \\begin{cases} \\pmb ∇ \\cdot \\pmb B = 0 \u0026\\text{(任一矢量场的旋度的散度等于零)}\\\\ \\pmb ∇ × \\pmb E = 0 \u0026\\text{(保守场可以写成标量场梯度加负号)} \\end{cases} $\n定义了矢势 $\\pmb A$ 和标势 $φ$，用来表示磁场和电场：\n$$ \\begin{cases} \\pmb B = \\pmb ∇ × \\pmb A \\\\ \\pmb E = -\\pmb ∇ φ \u0026\\text{} \\end{cases} $$矢势 $\\pmb A$ 和标势 $φ$ 并非唯一：\n$$ \\begin{cases} \\pmb{A'} = \\pmb A + \\pmb ∇ Λ \u0026\\text{(任意标量场$Λ$梯度(保守场)的旋度为零)} \\\\ φ' = φ + C \u0026\\text{(常数C的梯度为零)} \\end{cases} $$对任意的标量场$Λ$ 和常数C，做此变换后，所得到的静电场和静磁场不变。 （对于标势而言，因为是梯度，所以相对量重要，绝对量不重要）\n在随时间变化的动电磁场中：\n因为还未发现磁荷，所以 $\\pmb ∇ \\cdot \\pmb B = 0$ 仍然成立，因而仍可以定义矢势 $\\pmb A$ 使得：\n$$ \\pmb B = \\pmb ∇ × \\pmb A $$但是 $\\pmb ∇ × \\pmb E = 0$ 不再成立，因为 $\\pmb ∇ × \\pmb E = - \\frac{∂ \\pmb B}{∂t}$，将 $\\pmb B = \\pmb ∇ × \\pmb A$ 带入可得：\n$$ \\pmb ∇ × \\pmb E = - \\frac{∂(\\pmb ∇ × \\pmb A)}{∂t} $$交替时空微分，再移项可得：\n$$ \\pmb ∇ × \\left(\\pmb E + \\frac{∂ \\pmb A}{∂t} \\right) = 0 $$因此说明 $\\pmb E + \\frac{∂ \\pmb A}{∂t}$ 是一个保守场，所以可以写成一个标量场的负梯度形式：\n$$ \\pmb E + \\frac{∂ \\pmb A}{∂t} = - \\pmb ∇φ $$从而电磁场的表达式可写为：\n$$ \\begin{cases} \\pmb B = \\pmb ∇ × \\pmb A \\\\ \\pmb E = -\\pmb ∇ φ - \\frac{∂ \\pmb A}{∂t} \\end{cases} \\tag{5.1.3} $$当 $\\pmb A$ 不随时间变化时，就演变成了静电场和静磁场的表达式。同样，这里的矢势 $\\pmb A$ 和标势 $φ$ 也不是唯一的，做如下变换：\n$$ \\begin{cases} \\pmb{A'} = \\pmb A + \\pmb ∇ Λ \\\\ φ' = φ + \\frac{∂ \\pmb{A'}}{∂t} \\end{cases} $$后所得的电磁场不变：\n$$ \\begin{cases} \\pmb B = \\pmb ∇ × \\pmb{A'} = \\pmb ∇ × \\pmb A + \\pmb ∇ × \\pmb ∇ Λ \\\\ \\pmb E = -\\pmb ∇ φ' = - \\pmb ∇ φ + \\pmb ∇ \\frac{∂ Λ}{∂t} - \\frac{∂ \\pmb A}{∂t} - \\frac{∂ \\pmb ∇ Λ}{∂t} = - \\pmb ∇ φ - \\frac{∂ \\pmb A}{∂t} \\end{cases} $$ 5.1 规范 针对特定的问题，采取某种规范，使方程变得简洁。\n麦克斯韦方程组用矢势 $\\pmb A$ 和标势 $φ$ 表示：\n因为是使用麦克斯韦方程组的第2和第3个方程： $ \\begin{cases} \\pmb ∇ \\cdot \\pmb B = 0 \\\\ \\pmb ∇ × \\pmb E = - \\frac{∂ \\pmb B}{∂t} \\end{cases} $ 推出的矢势 $\\pmb A$ 和标势 $φ$，所以这两个方程自动满足。将矢势 $\\pmb A$ 和标势 $φ$ 带入麦克斯韦方程组的第1和第4个方程，此二方程涉及到电荷和电流的源：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb E = - \\pmb ∇^2 φ - \\frac{∂ (\\pmb ∇ \\cdot \\pmb A)}{∂t} = \\frac{ρ(\\pmb x,t)}{ε_0} \\\\ \\pmb ∇ × \\pmb B = \\pmb ∇ ×(\\pmb ∇ × \\pmb A) = \\pmb ∇ (\\pmb ∇ \\cdot \\pmb A) - \\pmb ∇^2 \\pmb A = μ_0 \\pmb J(\\pmb x,t) - μ_0 ε_0 \\frac{∂ \\left(\\pmb ∇ φ + \\frac{∂ \\pmb A}{∂ t} \\right)}{∂ t} \\end{cases} $$可化简为：\n$$ \\begin{cases} \\pmb ∇^2 φ + \\frac{∂ (∇ \\cdot \\pmb A)}{∂t} = - \\frac{ρ(\\pmb x,t)}{ε_0} \\ \\pmb ∇^2 \\pmb A - \\frac{1}{c^2} \\frac{∂^2 \\pmb A}{∂ t^2}\n\\pmb ∇ \\underline{ \\left( \\pmb ∇ \\cdot \\pmb A + \\frac{1}{c^2} \\frac{∂φ}{∂t} \\right) } = - μ_0 \\pmb J(\\pmb x,t) \\end{cases} \\tag{5.1.4} $$ 此方程较为复杂，因为 $\\pmb A$ 和 $φ$ 不唯一，所以可以针对具体问题选取某种特殊的 $\\pmb A$ 和 $φ$，使得方程简化，这种选择叫做规范。\n洛伦兹规范 矢势 $\\pmb A$ 的空间微分加上标势 $φ$ 时间微分的 $1/c^2$ 倍等于0。\n$$ \\pmb ∇ \\cdot \\pmb A + \\frac{1}{c^2} \\frac{∂φ}{∂t} = 0 \\tag{5.1.5} $$则 $\\pmb ∇ \\cdot \\pmb A = - \\frac{1}{c^2} \\frac{∂φ}{∂t}$\n方程组5.1.4可变为：\n$$ \\begin{cases} \\pmb ∇^2 φ - \\frac{1}{c^2} \\frac{∂^2 φ}{∂ t^2} = -\\frac{ρ(\\pmb x,t)}{ε_0} \\\\ \\pmb ∇^2 \\pmb A - \\frac{1}{c^2} \\frac{∂^2 \\pmb A}{∂ t^2} = - μ_0 \\pmb J(\\pmb x,t) \\end{cases} \\tag{5.1.6} $$四个量（$φ、A_x、A_y、A_z$）是独立的有源波动方程，或者称作达朗贝尔方程（☐是d\u0026rsquo;Alembert operator），形如：\n$$ ☐ Ψ = f(\\pmb x,t) $$其中拉普拉斯算符$\\pmb ∇^2$作用于矢势 $\\pmb A$ 等于分别作用于分量（大小），并保留矢量特性：\n$$ \\pmb ∇^2 \\pmb A = \\pmb ∇^2 (A_x \\pmb i + A_y \\pmb j + A_z \\pmb k) = \\underbrace{\\pmb ∇^2 A_x}_{大小标量} \\pmb i + \\pmb ∇^2 A_x \\pmb i + \\pmb ∇^2 A_x \\pmb i $$如果方程右边 $f(\\pmb x,t)$ 为0，就是波动方程。\n洛伦兹规范的存在性\n证明：对于任意$\\pmb E,\\ \\pmb B$，是否总能找到满足洛伦兹规范的$\\pmb A$ 和 $φ$ ?\n如果$\\pmb E,\\ \\pmb B$ 起初选择了 $\\pmb{A'}$ 和 $φ'$，它们不满足洛伦兹规范：\n$$ \\pmb ∇ \\cdot \\pmb{A'} + \\frac{1}{c^2} \\frac{∂φ'}{∂t} ≠ 0 $$但是可以通过寻找标量场 $Λ$ 做变换：\n$$ \\begin{cases} \\pmb A = \\pmb{A'} + \\pmb ∇ Λ \\\\ φ = φ' - \\frac{∂Λ}{∂t} \\end{cases} $$使得$\\pmb A$和$φ$满足洛伦兹规范：\n$$ \\pmb ∇ \\cdot \\pmb A + \\frac{1}{c^2} \\frac{∂φ}{∂t} = \\pmb ∇ \\cdot \\pmb{A\u0026rsquo;} + \\frac{1}{c^2} \\frac{∂φ\u0026rsquo;}{∂t}\n\\pmb ∇^2 Λ \\frac{1}{c^2} \\frac{∂^2 Λ}{∂ t^2} = 0 $$ 则有：\n$$ \\pmb ∇^2 Λ - \\frac{1}{c^2} \\frac{∂^2 Λ}{∂ t^2} = - \\left( \\pmb ∇ \\cdot \\pmb{A'} + \\frac{1}{c^2} \\frac{∂φ'}{∂t} \\right) $$将 $\\left( \\pmb ∇ \\cdot \\pmb{A'} + \\frac{1}{c^2} \\frac{∂φ'}{∂t} \\right)$ 看作源，这个方程就是达朗贝尔方程，从而可解得标量场$Λ$，就找到了满足洛伦兹规范的$\\pmb A$ 和 $φ$\n同时也可以看出，即使在洛伦兹规范下，$\\pmb A$ 和 $φ$ 也不唯一。如果$\\pmb{A'}$ 和 $φ'$ 本身已经满足洛伦兹规范了，所以 $\\pmb ∇^2 Λ - \\frac{1}{c^2} \\frac{∂^2 Λ}{∂ t^2} = 0$，只要是满足这个方程的$Λ$场，叠加到 $\\pmb{A'}$ 和 $φ'$ 上仍然满足洛伦兹规范。\n库伦规范 $ \\pmb ∇ \\cdot \\pmb A = 0 $\n在这种规范下，方程组5.1.4可变为：\n$$ \\begin{cases} \\pmb ∇^2 φ = - \\frac{ρ(\\pmb x,t)}{ε_0} \\\\ \\pmb ∇^2 \\pmb A - \\frac{1}{c^2} \\frac{∂^2 \\pmb A}{∂ t^2} - \\frac{1}{c^2} \\frac{∂ \\pmb ∇ φ}{∂ t} = - μ_0 \\pmb J(\\pmb x,t) \\end{cases} $$这个方程组的好处是电势的解很容易得到，第一个方程就是静电场方程（泊松方程），解得？\n$$ φ(\\pmb x,t) = \\frac{1}{4πε_0} ∫ \\frac{ρ(\\pmb{x'},t)}{|\\pmb x - \\pmb{x'}|} dV' $$将其带入第二个方程后为：\n$$ \\pmb ∇^2 \\pmb A - \\frac{1}{c^2} = -μ_0 \\pmb J(\\pmb x,t) - \\frac{1}{c^2} \\frac{∂ \\pmb ∇φ}{∂t} $$这也是达朗贝尔方程。但坏处是因为 $\\pmb A$ 和 $φ$ 有关联，不能独立求解； 另外还有一个问题就是其不满足洛伦兹协变，这是狭义相对论中的问题，简单来看， $ φ(\\pmb x,t) = \\frac{1}{4πε_0} ∫ \\frac{ρ(\\pmb{x'},t)}{|\\pmb x - \\pmb{x'}|} dV' $ ，电荷 $\\pmb{x'}$ 是瞬间在空间任何一个位置 $\\pmb x$ 产生了电势，而且电荷一发生变化，在远场另一位置的电势会同时变化，超越了光速，这不符合狭义相对论。\n但是这并不意味这该方程错误，对于时空变换，库伦规范下的的 $\\pmb A$ 和 $φ$ 应使用其它的变换，而不是洛伦兹变换，对于瞬时产生的电势也并不存在理论问题，因为电场是由 $\\pmb A$ 和 $φ$ 共同决定的，而 $\\pmb A$ 是以光速 c 传播的量。$φ$ 不包含信息，而$\\pmb A$ 包含信息，所以它俩一起才有意义，还是以光速传播。\n而洛伦兹规范满足洛伦兹协变，$ρ$ 和 $\\pmb J$ 的变化需要时间 $\\Delta t = \\frac{|\\pmb r|}{c}$ 才能传过去。\n库伦规范的存在性\n对于任意 $\\pmb E,\\ \\pmb B$，总是可以找到$\\pmb A$ 和 $φ$ 满足库伦规范。\n如果起初 $\\pmb B$ 的矢势不满足库伦规范：\n$$ \\pmb ∇ \\cdot \\pmb{A'} ≠ 0 $$则可以找到标量场 $Λ$，将 $\\pmb{A'},φ'$ 变换为 $\\pmb A$ 和 $φ$：\n$$ \\begin{cases} \\pmb A = \\pmb{A'} + \\pmb ∇ Λ \\\\ φ = φ' - \\frac{∂Λ}{∂t} \\end{cases} $$使得：\n$$ \\pmb ∇ \\cdot \\pmb A = 0 $$则标量场$Λ$满足：\n$$ \\pmb ∇^2 Λ = - \\pmb ∇ \\cdot \\pmb{A'} $$把 $\\pmb ∇ \\cdot \\pmb{A'}$ 当作库伦场中的场源，解泊松方程就可以解出标量场$Λ$。\n与洛伦兹规范一样，库伦规范下 $\\pmb A$ 和 $φ$ 也并非完全确定，如果 $\\pmb A$ 本来就满足库伦规范：\n$$ \\begin{aligned} \\pmb{A_1} \u0026amp;= \\pmb A + \\pmb ∇ Λ \\ \\pmb ∇ × \\pmb{A_1} \u0026amp;= \\underbrace{ \\cancel{\\pmb ∇ \\cdot \\pmb A} }_0\n\\underbrace{\\pmb ∇^2 Λ = 0}_{拉普拉斯方程,解不唯一} \\end{aligned} $$ 满足拉普拉斯方程 $\\pmb ∇^2 Λ = 0$ 的 $Λ$ 解不唯一，这些 $Λ$ 解代入： $ \\begin{cases} \\pmb{A_1} = \\pmb A + \\pmb ∇ Λ \\\\ φ_1 = φ - \\frac{∂Λ}{∂t} \\end{cases} $ ，也满足库伦规范。\n库伦规范的另一个好处是可以将电场分为横场和纵场。\u0026hellip;\n5.2 推迟势 场点 $\\pmb x$ 处的势是 $\\frac{|\\pmb x-\\pmb{x'}|}{c}$ 时间之前的场源产生的势场。\n解达朗贝尔方程 方程具有线性性质 + 已知一个点源产生的场 -\u0026gt; (积分得) 连续场\n在洛伦兹规范 $\\pmb ∇ \\cdot \\pmb A + \\frac{1}{c^2} \\frac{∂φ}{∂t} = 0$ 下，对应的麦克斯韦方程组为：\n$$ \\begin{cases} \\pmb ∇^2 φ - \\frac{1}{c^2} \\frac{∂^2 φ}{∂ t^2} = - \\frac{ρ(\\pmb x, t)}{ε_0} \\\\ \\pmb ∇^2 \\pmb A - \\frac{1}{c^2} \\frac{∂^2 \\pmb A}{∂ t^2} = -μ_0 \\pmb J(\\pmb x,t) \\end{cases} $$这可以分为四个形式一样的达朗贝尔方程，一般形式为：\n$$ \\pmb ∇^2 Ψ (x,t) - \\frac{1}{c^2} \\frac{∂^2 Ψ(x,t)}{∂ t^2} = - F(\\pmb x,t) $$这是一个线性方程，其解法与库伦定律类似。\n假定场源 $F_1$ 对应的场方程为 $Ψ_1$，场源 $F_2$ 对应的场方程为 $Ψ_2$，则 $F_1 + F_2$ 对应的场方程的解为 $Ψ_1 + Ψ_2$。\n在解静电场中泊松方程：\n$$ \\pmb ∇^2 φ = - \\frac{ρ}{ε_0} $$的时候，由点电荷 $ρ(\\pmb x) = Q δ(\\pmb x-\\pmb{x'})$ 的解： $ φ(\\pmb x-\\pmb{x'}) = - \\frac{Q}{4πε_0 |\\pmb x-\\pmb{x'}|} $ ，利用线性方程的叠加性，可得泊松方程的解为：\n$$ φ(\\pmb x) = ∫ \\frac{ρ(\\pmb x-\\pmb{x'})}{4 π ε_0 |\\pmb x-\\pmb{x'}|} dV' $$同样，对于达朗贝尔方程，也首先考虑随时间变化的电源所对应场的解。\n设在原点处有场源为 $F(t) δ(\\pmb x)$，其满足达朗贝尔方程：\n$$ \\pmb ∇^2 Ψ (\\pmb x,t) - \\frac{1}{c^2} \\frac{∂^2 Ψ(\\pmb x,t)}{∂ t^2} = -F(t) δ(\\pmb x) $$而除原点外没有源，外场的方程为波动方程：\n$$ \\pmb ∇^2 Ψ(\\pmb x,t) - \\frac{1}{c^2} \\frac{∂^2 Ψ(\\pmb x,t)}{∂ t^2} = 0 $$在上一章讲过一维波动方程：$\\frac{∂^2 Ψ(x,t)}{∂ x^2} - \\frac{1}{v^2} \\frac{∂^2 Ψ(x,t)}{∂ t^2} = 0 $ 的解为：\n$$ Ψ(x,t) = g_1(x-vt) + g_2(x+vt) $$平面波在真空中匀速传输，波形不变。\n三维波动方程的解也具有类似的形式，不过不是平面波而是球面波（电荷产生的场）。由于方程具有球对称性，故采用球坐标：$Ψ(r,t)$ 只是位矢 $r$ 的函数，与 $θ$ 和 $\\phi$ 无关。三维波动方程变为：\n$$ \\frac{1}{r^2} \\frac{∂}{∂ r} \\left( r^2 \\frac{∂ Ψ(r,t)}{∂ r} \\right)\n\\frac{1}{c^2} \\frac{∂^2 Ψ(r,t)}{∂ t^2} =0 \\tag{5.2.1} $$ 可以将 $Ψ(r,t)$ 写成任何形式，不过为了让$u(r,t)=rΨ(r,t)$，所以：\n$$ Ψ(r,t) = \\frac{u(r,t)}{r} $$把$Ψ(r,t)$代入波动方程5.2.1得：\n$$ \\frac{∂^2 u(r,t)}{∂ x^2} - \\frac{1}{c^2} \\frac{∂^2 u(r,t)}{∂ t^2} = 0 $$这是一个$u(r,t)$的波动方程，它的解在讲平面波时分析过：\n$$ u(r,t) = g_1(r-ct) + g_2(r+ct) $$则波函数 $Ψ(r,t)$ 的解为：\n$$ Ψ(r,t) = \\frac{g_1(r-ct)}{r} + \\frac{g_2(r+ct)}{r} $$说明了以速度 $c$ 传播，以$\\frac{1}{r}$衰减。\n画示意图\n第一项表示从原点向外发射出去的波，第二项表示进入原点的汇聚波。 现在讨论的是发射波，所以将第二项舍去，则波动方程5.2.1的解（场方程）可以写为如下形式：\n$$ Ψ(r,t) = \\frac{g(r-ct)}{r} \\tag{5.2.2} $$所以对于非原点区域，达朗贝尔方程解就是5.2.2形式，所以只要证明该解在原点处依然成立。而 $g(r-vt)$ 是由F(t)决定的，“猜测”它们之间的关系为：\n$$ g(r-vt) = F(t- \\frac{r}{v}) \\frac{1}{4π} $$$\\frac{r}{v}$ 是变化传播时间，$\\frac{1}{4π}$ 是类比库伦定律。\n把此关系再带回在原点处有场源为 $F(t) δ(\\pmb x)$ 的达朗贝尔方程：\n$$ \\begin{aligned} \\pmb ∇^2 Ψ(r,t) - \\frac{1}{c^2} \\frac{∂^2 Ψ(r,t)}{∂ t^2} = - F(t) δ(\\pmb x) \\\\ \\left( \\pmb ∇^2 - \\frac{1}{c^2} \\frac{∂^2}{∂ t^2} \\right) \\frac{F(t-\\frac{r}{v})}{4πr} = - F(t) δ(\\pmb x) \\end{aligned} $$证明该方程成立即可。\n方程左右两边在原点处都有奇点，可以用积分来证明其正确性。这里选择半径为$η \\to 0$ 的小球积分。则方程右边：\n$$ ∫ - F(t) δ(\\pmb x) dV = - F(t) \\quad \\text{(δ函数的性质)} $$方程左边：\n$$ ∫ \\left( \\pmb ∇^2 - \\frac{1}{c^2} \\frac{∂^2}{∂ t^2} \\right) \\frac{F(t-\\frac{r}{v})}{4πr} dV = ∫_0^η 4π r^2 \\left( \\pmb ∇^2 - \\frac{1}{c^2} \\frac{∂^2}{∂ t^2} \\right) \\frac{F(t-\\frac{r}{v})}{4πr} dr $$将$F(t-\\frac{r}{v})$ 做泰勒展开：$F(t-\\frac{r}{v}) = F(t) + O(r)$，$O(r)$ 是r的高阶项，与t无关。所以上述积分可变为：\n$$ ∫_0^η 4π r^2 \\left( \\pmb ∇^2 - \\frac{1}{c^2} \\frac{∂^2}{∂ t^2} \\right) \\frac{F(t)}{4πr} dr\n∫_0^η 4π r^2 \\left( \\pmb ∇^2 - \\frac{1}{c^2} \\frac{∂^2}{∂ t^2} \\right) \\frac{O(r)}{4πr} dr $$ 在这两项中都有对时间的二阶导，不过因为是在无穷小的区域体积分，所以这部分积分为零。\n当 $η \\to 0$时，dV无穷小，对于 r\u0026gt;0次（V\u0026gt;3次）的无穷小积分都是0，只有我们定义的 $δ$ 函数（$\\pmb ∇^2 \\frac{1}{r}$）在零点附近积分不为零。 在第二项中$O(r)$包括了$r^1,\\ r^2,\\ r^3 ...$等高阶项，再与分母约掉一个r，也就是只包含0次及以上的？所以它们的无穷小区域体积分都是零（$4πr^2$不能抵消？(r\u0026lt;R)）。但是在第一项中包含$δ$函数，所以只有第一项（零阶项）不为零：\n$$ ∫_0^η \\pmb ∇^2 \\frac{F(t)}{4πr} dV = \\frac{F(t)}{4π} ∫_0^η \\pmb∇^2 \\frac{1}{r} dV = -F(t) $$因而，在原点也成立。\n所以原点处场源 $F(t) δ(\\pmb x)$ 所产生的场为：（有$\\frac{r}{v}$的时间延迟，以$\\frac{1}{4πr}$衰减）\n$$ Ψ(\\pmb x,t) = \\frac{F(t-\\frac{r}{v})}{4πr} $$对于具有空间分布的连续源，源密度为 $f(\\pmb x,t)$ 的达朗贝尔方程为：\n$$ \\pmb ∇^2 Ψ(\\pmb x,t) - \\frac{1}{c^2} \\frac{∂^2 Ψ(\\pmb x,t)}{∂ t^2} = - f(\\pmb x,t) $$其解为：\n$$ Ψ(\\pmb x,t) = ∫ \\frac{f(\\pmb{x'},t-\\frac{|\\pmb x-\\pmb{x'}|}{c})}{4π |\\pmb x-\\pmb{x'}|} dV' $$也就是对于连续的源，就是把点源的解进行积分。由该解看出，不同点源的位置 $\\pmb{x'}$ 不同，传输时间$\\frac{|\\pmb x-\\pmb{x'}|}{c}$不同，所以在远点$\\pmb x$的势是各点源到 $\\pmb x$ 的贡献之和。\n所以，对于满足洛伦兹规范的$\\pmb A$ 和 $φ$，其达朗贝尔方程的解为：\n$$ \\large \\begin{cases} \\pmb A(\\pmb x,t) = ∫ \\frac{μ_0 \\pmb J(\\pmb{x'},\\ t-\\frac{|\\pmb x - \\pmb{x'}|}{c})}{4π |\\pmb x - \\pmb{x'}|} dV' \\\\ φ(\\pmb x,t) = ∫ \\frac{ρ (\\pmb{x'},\\ t-\\frac{|\\pmb x - \\pmb{x'}|}{c})}{4π ε_0 |\\pmb x - \\pmb{x'}|} dV' \\end{cases} \\tag{5.2.3} $$这表示 $\\pmb x$ 处的场是由 $\\pmb{x'}$ 处的场在 $t - \\frac{|\\pmb x- \\pmb{x'}|}{c}$ 时刻的源产生的场，也就是说场以光速传播。\n证明：洛伦兹规范是电荷守恒的要求\n令：\n$$ t' = t - \\frac{|\\pmb x - \\pmb{x'}|}{c} \\quad \\text{(平移)} $$则：\n$$ \\begin{aligned} \\pmb ∇ \\cdot \\pmb A(\\pmb x,t) \u0026amp;= ∫ \\pmb ∇ \\cdot \\frac{μ_0 \\pmb J \\left( \\pmb{x\u0026rsquo;},\\ t- \\frac{|\\pmb x - \\pmb{x\u0026rsquo;}|}{c} \\right)}{4π |\\pmb x - \\pmb{x\u0026rsquo;}|} dV\u0026rsquo; \\ \u0026amp;= \\underbrace{ ∫ - \\pmb ∇\u0026rsquo; \\cdot \\frac{μ_0 \\pmb J(\\pmb{x\u0026rsquo;},t\u0026rsquo;)}{4π|\\pmb x-\\pmb{x\u0026rsquo;}|} dV\u0026rsquo; }_{=0} + ∫ \\frac{μ_0 \\pmb ∇\u0026rsquo; \\cdot \\pmb J(\\pmb{x\u0026rsquo;},t\u0026rsquo;)}{4π |\\pmb x-\\pmb{x\u0026rsquo;}|} dV\u0026rsquo; \u0026amp;\\text{(分部积分)}\\ \u0026amp;= ∫ \\frac{μ_0 \\pmb ∇\u0026rsquo; \\cdot (\\pmb{x\u0026rsquo;},\\ t\u0026rsquo;)}{4π|\\pmb x-\\pmb{x\u0026rsquo;}|} dV\u0026rsquo; \\\n\\frac{1}{c^2} \\frac{∂ φ}{∂ t} \u0026amp;= μ_0 ε_0 \\frac{∂}{∂ t} ∫ \\frac{ρ(\\pmb{x\u0026rsquo;},t\u0026rsquo;)}{4π ε_0 |\\pmb x - \\pmb{x\u0026rsquo;}|} dV' \\end{aligned} $$\n($\\pmb{∇\\cdot A}$的第一项中$\\pmb J$是有限量，肯定会衰减到0，边界上都是零，应用散度定理，散度体积分等于边界的面积分，就是0)\n从而可得：\n$$ \\pmb ∇ \\cdot \\pmb A(\\pmb x,t) + \\frac{1}{c^2} \\frac{∂φ}{∂t} = \\frac{μ_0}{4π} ∫ \\frac{\\pmb ∇' \\cdot \\pmb J (\\pmb{x'},t') + \\frac{∂}{∂t} ρ(\\pmb{x'},t')}{|\\pmb x- \\pmb{x'}|} dV' $$根据电荷守恒：（$\\pmb J(\\pmb x,t)$和$φ(\\pmb x,t)$不独立）\n$$ \\pmb ∇' \\cdot \\pmb J(\\pmb{x'},t') + \\frac{∂}{∂t} ρ(\\pmb{x'},t') = 0 $$因而有洛伦兹规范：（$\\pmb A(\\pmb x,t)$和$φ(\\pmb x,t)$不独立）\n$$ \\pmb ∇ \\cdot \\pmb A(\\pmb x,t) + \\frac{1}{c^2} \\frac{∂φ}{∂t} = 0 $$所以洛伦兹规范是受电荷守恒限定的，也就是说如果我们以电荷守恒方程来限定 $\\pmb A(\\pmb x,t)$ 和 $φ(\\pmb x,t)$ 的关系，就不需要再考虑洛伦兹规范了。\n$$ $\\require{AMScd}$ \\begin{CD} \\pmb J(\\pmb x,t)\t@\u003e\u003e\u003e\t\\pmb A \\\\ @V 电荷守恒 V V @VV洛伦兹规范 V\t\\\\ ρ(\\pmb x,t) @\u003e\u003e\u003e\tφ(\\pmb x,t) \\end{CD} $$ 5.3 偶极辐射 如果已知电流源和电荷源的空间和时间分布，由推迟势即可得 $\\pmb A(\\pmb x,t)$ 和 $φ(\\pmb x,t)$ 的时空解。\n事实上，由于洛伦兹规范：\n$$ \\pmb ∇ \\cdot \\pmb A(\\pmb x,t) + \\frac{1}{c^2} \\frac{∂φ}{∂t} = 0 $$或者电荷守恒：\n$$ \\pmb ∇ \\cdot \\pmb J(\\pmb x,t) + \\frac{∂}{∂t} ρ(\\pmb x,t) = 0 $$有两种途径来求出矢势 $\\pmb A$ 和标势 $φ$ ： 只需要电流的时空分布 $\\pmb J(\\pmb x,t)$ 解出 $\\pmb A(\\pmb x,t)$，电荷时空分布 $ρ(\\pmb x,t)$ 由电荷守恒确定，然后再由电荷时空分布得到 $φ(\\pmb x,t)$，或者由已解出的 $\\pmb A(\\pmb x,t)$ 用洛伦兹规范算出$φ(\\pmb x,t)$。 当然这里会差一个常量，不过我们这里只讨论随时间变化的场，静止部分不考虑，所以，只需解出 $\\pmb A(\\pmb x,t)$ 即可。\n从第四章知道，随时间变化的场在一般谐振$ω(t)$的情况下容易解，尤其是用复数表示，相位差可直接变为复相位系数。而我们又知道，一个随时间变化的量可以以简谐振动为基矢做傅里叶变换（三角函数是正交完备的），这样在线性方程中，只要解出任意一个频率ω下的场方程，然后按照傅里叶变换的系数叠加不同频率的解就可以得到最终解。\n具体步骤如下：\n对于达朗贝尔方程：\n$$ \\pmb∇^2 Ψ(\\pmb x,t) - \\frac{1}{c^2} \\frac{∂^2 Ψ(\\pmb x,t)}{∂t^2} = - f(\\pmb x,t) $$将源 $f(\\pmb x,t)$ 做傅里叶变换成频率求和：\n$$ f(\\pmb x,t) = \\frac{1}{\\sqrt{2π}} ∫^{+∞}_{-∞} f(\\pmb x,ω) e^{-iωt}dω $$对于某一频率的谐振源 $f(\\pmb x,ω)e^{-iωt}$，对应的解（也是谐帧的）为：\n$$ Ψ(\\pmb x,ω) e^{-iωt} $$则整个源 $f(\\pmb x,t)$ 对应的解为各个频率的解的叠加：\n$$ Ψ(x,t) = \\frac{1}{\\sqrt{2π}} ∫^{+∞}_{-∞} Ψ(\\pmb x,ω) e^{-iωt}dω $$而频域下的解 $f(x,ω)$ 和 $Ψ(x,ω)$ 可由傅里叶变换得到：\n$$ \\begin{aligned} f(x,ω) = \\frac{1}{\\sqrt{2π}} ∫_{-∞}^{+∞} f(x,t) e^{iωt}dω \\\\ Ψ(x,ω) = \\frac{1}{\\sqrt{2π}} ∫_{-∞}^{+∞} Ψ(x,t) e^{iωt}dω \\end{aligned} $$所以，首先讨论随时间谐振变化的单一频率场的解，并以复数的形式表达。\n现有空间电流分布为：\n$$ \\pmb J(\\pmb x,t) = \\pmb J(\\pmb x) e^{iωt} $$其中 $\\pmb J(\\pmb x)$ 是个复数，它的模表示电流的大小，相位表示空间电流的相位分布。根据式5.2.3，其矢势 $\\pmb A(\\pmb x,t)$ 为：\n$$ \\begin{aligned} \\pmb A(\\pmb x,t) \u0026= ∫ \\frac{μ_0 \\pmb J(\\pmb{x'}, t-|\\pmb x - \\pmb{x'}|/c)}{4π|\\pmb x - \\pmb{x'}|} dV' \u0026\\text{(时间的推迟)} \\\\ \u0026= ∫ \\frac{μ_0 \\pmb J(\\pmb{x'}) e^{-iω(t-|\\pmb x - \\pmb{x'}|/c)}}{4π|\\pmb x - \\pmb{x'}|} dV' \u0026\\text{(相位的推迟)} \\\\ \u0026= \\frac{μ_0}{4π} e^{-iωt} ∫ \\frac{\\pmb J(\\pmb{x'}) e^{iω|\\pmb x - \\pmb{x'}|/c}}{|\\pmb x - \\pmb{x'}|} dV' \u0026\\text{(把共同的振动提出)} \\end{aligned} $$不同位置的源，相位有不同的推迟，把所有位置的源在场点$\\pmb x$ 处的贡献积分就是该点处的势。\n令 $k=\\frac{ω}{c}$（单位距离内延迟多少相位），则：\n$$ \\pmb A(\\pmb x,t) = \\frac{μ_0}{4π} e^{-iωt} ∫ \\frac{\\pmb J(\\pmb{x'}) e^{ik|\\pmb x-\\pmb{x'}|}}{|\\pmb x-\\pmb{x'}|} dV' $$可以看出，全空间中矢势 $\\pmb A(\\pmb x,t)$ 也是以频率 ω 谐振，空间各点 $\\pmb{x'}$ 的电流 $\\pmb J(\\pmb{x'})$ 在 $\\pmb x$ 的贡献与 $\\pmb{x'}$ 处电流成正比，与距离成反比，且有一个相位差 $k|\\pmb x-\\pmb{x'}|$，这是延迟贡献的相位差。\n标势 $φ(\\pmb x,t)$ 可以通过洛伦兹规范求得：\n$$ \\frac{∂ φ(\\pmb x,t)}{∂ t} = -c^2 \\pmb∇ \\cdot \\pmb A(\\pmb x,t) = -\\frac{1}{4πε_0} e^{-iωt} \\pmb∇ \\cdot ∫ \\frac{\\pmb J(\\pmb{x'}) e^{ik|\\pmb x-\\pmb{x'}|}}{|\\pmb x-\\pmb{x'}|} dV' $$则：\n$$ φ(\\pmb x,t) = \\frac{1}{4πε_0ωi} e^{-iωt} \\pmb∇ \\cdot ∫ \\frac{\\pmb J(\\pmb{x'}) e^{ik|\\pmb x-\\pmb{x'}|}}{|\\pmb x-\\pmb{x'}|} dV' $$也可以从电荷分布 $ρ(\\pmb x,t)$ 的角度来解标势 $φ(\\pmb x,t)$：\n$$ \\frac{∂}{∂ t} ρ(\\pmb x,t) = -\\pmb∇ \\cdot \\pmb J(\\pmb x,t) = -e^{-iωt} \\pmb∇ \\cdot \\pmb J(\\pmb x) $$则：\n$$ ρ(\\pmb x,t) = \\frac{1}{iω} e^{-iωt} \\pmb ∇ \\cdot \\pmb J(\\pmb x) $$则：\n$$ φ(\\pmb x,t) = \\frac{1}{4πε_0ωi} e^{-iωt} ∫ \\frac{[\\pmb∇' \\cdot \\pmb J(\\pmb{x'})]e^{ik|\\pmb x-\\pmb{x'}|}}{|\\pmb x-\\pmb{x'}|} dV' $$然后利用：\n$$ \\begin{cases} \\pmb B = \\pmb∇×\\pmb A \\\\ \\pmb E = -\\pmb∇φ - \\frac{∂\\pmb A}{∂t} \\end{cases} $$就可得到电磁波的空间分布，可以看出，空间中任何一点的电磁场也都是以频率ω振荡，这是能量守恒的体现。\n原则上，以上结论已可以求解任何时变电流场下的电磁波。下面考虑几种简单且典型的情况，并给出解析解和图像，分析其物理性质，第一种情况就是偶极辐射。\n偶极辐射 电流区尺寸远小于电流区到场点的距离 $L≪ R$，并且远小于一个波长 $L≪\\frac{2π}{k}$ 时，电流区发射电磁波是偶极辐射。\n在局域空间内有电流分布，有两个尺度需要考虑：一个是所感兴趣场的位置与源区的距离$|\\pmb x -\\pmb{x'}|$，一个是电流区的尺寸$L$\n首先分析所感兴趣场的位置$\\pmb x$ 到场源的距离$\\pmb x-\\pmb{x'}$，做泰勒展开：\n$$ |\\pmb x-\\pmb{x'}| = R-\\pmb n \\cdot \\pmb{x'} + ... $$零级近似（所感兴趣位置到电流区离得远时，大致取中心）距离就是R；\n图 由上图看出，3个点源$\\pmb{x_1'},\\ \\pmb{x_2'},\\ \\pmb{x_3'}$ 在在同一圆上，距电流区中心偏移量一样大，但是模长不同：$|\\pmb x-\\pmb{x_1'}| \u003c |\\pmb x-\\pmb{x_2'}| \u003c |\\pmb x-\\pmb{x_3'}|$，由此可知，模长与方向有关。 所以一级近似是：$- \\pmb n \\cdot \\pmb{x'}$（$\\pmb n$ 是R的方向矢量，因为同向时要相减，所以有负号；因为垂直的时候，距离相等，也就是一级近似量为0，所以是点乘。\n电流区尺寸$L$：\n如果场源间分布比较紧凑 $|\\pmb{x'}|≪ R$，则 $\\frac{1}{|\\pmb x- \\pmb{x'}|}$ 变化不大，就可以用 $\\frac{1}{R}$ 代替， 但是 $e^{i k |\\pmb x-\\pmb{x'}|}$ 中的 $|\\pmb x-\\pmb{x'}|$ 不可轻易得用 R 来代替。相位差距大不大不能看绝对大小，因为 $cosθ$ 是周期函数，虽然0和1差不多，但是 cos0 和 cos1 差得就很大，已经差了周期的$\\frac{1}{6}$了，所以相位差要和周期做比较，r方向空间频率是k，所以相位的变化周期就是波长$λ=\\frac{2π}{k}$。\n因为这里是相位，需要考虑点源互相之间的距离产生的相位差 $k|\\pmb x-\\pmb{x'}|$ 与2π之间的大小关系，即 $k\\pmb n \\cdot \\pmb{x'}$ 与2π之间的关系，也即 $k|\\pmb{x'}|$ 与 2π 之间的大小关系，即 $|\\pmb{x'}|$ 与$λ$ 之间的关系。\n如果电流区内各点源间距较小：$r≪λ$，即 $k\\pmb n\\cdot \\pmb{x'}≪1$，则不同位置的 $\\pmb{x'}$ 电流对空间某一位置 $\\pmb x$ 的矢势的贡献的相位是一致的：$e^{ik|\\pmb x-\\pmb{x'}|} \\approx e^{ikR}$，这种情况称为偶极辐射。\n如果电流区域尺寸较大，即 $|\\pmb{x'}|\\simλ$ 或者 $|\\pmb{x'}|\u003eλ$，则 $e^{ik|\\pmb x-\\pmb{x'}|}$ 中的相位要考虑 $|\\pmb x-\\pmb{x'}| = R - \\pmb n\\cdot \\pmb{x'} + \\cdots$ 的高阶项，例如 $\\pmb n \\cdot \\pmb{x'}$ 项的贡献称为磁偶极辐射和电四极辐射。\n设电流区的尺寸为$L$，当 $L≪ R$，$L≪λ\\ (=\\frac{2π}{k})$，就是说把$\\frac{1}{|\\pmb x-\\pmb{x'}|}$ 近似成 $\\frac{1}{R}$，去除点源间（推迟）相位差异。则矢势 $\\pmb A(\\pmb x,t)$ 可简化为：\n$$ \\pmb A(\\pmb x,t) = \\frac{μ_0}{4π} \\frac{e^{i(kR-ωt)}}{R} ∫ \\pmb J(\\pmb{x'}) dV' $$由该式看出：$e^{ikR}$ 是相位推迟，$\\frac{1}{R}$ 是衰减，因为各点源在远场的贡献一样，所以把各场源全部积分，其中$\\pmb J(\\pmb{x'})$ 是复数，表示各点互相之间也有相位差。因为这里不是横稳电流，不满足： $ \\begin{cases} \\frac{∂\\pmb J}{∂t} = 0\\\\ \\pmb∇ \\cdot \\pmb J = 0 \\end{cases} $ 所以对电流的积分不为零。\n其中 $∫ \\pmb J(\\pmb{x'})dV'$ 的物理意义：\n$$ ∫ \\pmb J(\\pmb{x'}) dV' = ∫ ρ(\\pmb{x'}) v(\\pmb{x'}) dV' = \\frac{d}{dt} ∫ ρ(\\pmb{x'}) \\pmb{x'} dV' = \\dot{\\pmb p} $$电流密度$\\pmb J(\\pmb{x'})$等于电荷密度$\\rho{(\\pmb x')}$乘以速度$v(\\pmb{x'})$， 速度等于位置$\\pmb{x'}$的一阶导，电荷密度乘以位置就是偶极 $\\pmb p$， 所以 $∫ \\pmb J(\\pmb{x'}) dV'$ 的物理意义就是偶极的一阶导，也就是偶极的振荡速率，所以矢势 $\\pmb A$ 可写为偶极的振荡速率，再加上相位差和衰减：\n$$ \\pmb A(\\pmb x,t) = \\frac{μ_0}{4π} \\frac{e^{i(kR-ωt)}}{R} \\dot{\\pmb p} $$可以看出，矢势 $\\pmb A(\\pmb x,t)$ 与偶极振荡速率$\\dot{\\pmb p}$成正比，与距离 R 成反比，另外还有一个因为距离推迟的相位因子 $e^{i(kR-ωt)}$。 因为 $\\pmb p$ 也是随时间以 ω 频率振荡，因此 $\\dot{\\pmb p} = -iω\\pmb p$，所以矢势 $\\pmb A(\\pmb x,t)$ 的表达式可写为偶极$\\pmb p$乘以系数：\n$$ \\pmb A(\\pmb x,t) = -\\frac{iωμ_0}{4π} \\frac{e^{i(kR-ωt)}}{R} \\pmb p $$说明矢势 $\\pmb A(\\pmb x,t)$ 与频率 ω 的一次方成正比。\n由此就可以计算电场和磁场的分布：（偶极取z方向）\n$$ \\begin{aligned} \\pmb B \u0026= \\pmb∇×\\pmb A \\\\ \u0026= \\pmb∇×\\left[ -\\frac{iωμ_0}{4π} \\frac{e^{i(kR-ωt)}}{R} \\pmb p \\right]\\\\ \u0026= \\pmb∇×\\left[ -\\frac{iωμ_0}{4π} \\frac{e^{i(kR-ωt)}}{R} pcosθ\\pmb{e_r} + \\frac{iωμ_0}{4π} \\frac{e^{i(kR-ωt)}}{R} psinθ\\pmb{e_θ} \\right] \\quad \\text{($\\pmb{e_r}$和$\\pmb{e_θ}$是方向矢量)}\\\\ \u0026= \\frac{1}{R} \\left[ \\frac{∂}{∂R} \\left( \\frac{iωμ_0}{4π} e^{i(kR-ωt)} psinθ \\right) + \\frac{∂}{∂θ} \\left( \\frac{iωμ_0}{4π} \\frac{e^{i(kR-ωt)}}{R} pcosθ \\right) \\right] \\pmb{e_\\phi} \\\\ \u0026= \\frac{iωμ_0}{4πR} \\left[ ike^{i(kR-ωt)} psinθ - \\frac{e^{i(kR-ωt)}}{R} psinθ \\right] \\pmb{e_\\phi} \\\\ \u0026= -\\left( \\frac{ω^2}{4πε_0c^3R} + \\frac{iω}{4πε_0c^2R^2} \\right) e^{i(kR-ωt)} psinθ \\pmb{e_\\phi} \\end{aligned} $$关于电场E\n","date":"2021-06-30T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/2021-06-30-ch5_%E7%94%B5%E7%A3%81%E6%B3%A2%E8%BE%90%E5%B0%84/","title":"ch5:电磁波辐射"},{"content":"\n本系列笔记刊载的所有内容，包括文字、图片等，均来自云南大学丁怀义老师的课程《电磁场理论与计算》以及他的讲义，版权归丁怀义老师（dinghy@ynu.edu.cn）所有。\n第四章 研究真空中电磁场的传播。\n[toc]\n4.1 真空平面电磁波 真空中没有电荷、电流（也没有介质，介质也是一种电荷）；平面波的波阵面（在任何时刻，波相位相等的每一点所形成的曲面）是相互平行的平面。平面波的传播方向垂直于波前（最前方的曲面）。\n比如某电磁波沿 x 轴方向传播，在同一 y-z 平面上的点的值相同（颜色相同）。\n图4.1.1 平面波 目的是简化电磁波的一般形式。根据麦克斯韦方程组：\n$$ \\left\\{ \\begin{aligned} \\pmb ∇ \\cdot \\pmb E \u0026= \\frac{ρ_f}{ε_0} - \\frac{\\pmb ∇ \\cdot \\pmb P(\\pmb x, t)}{ε_0} \\\\ \\pmb ∇ × \\pmb E \u0026= -\\frac{∂ \\pmb B}{∂ t} \\\\ \\pmb ∇ \\cdot \\pmb B \u0026= 0 \\\\ \\pmb ∇ × \\pmb B \u0026= μ_0 \\pmb J_f + μ_0 \\pmb ∇ × \\pmb M(\\pmb x, t) + μ_0 \\frac{∂ \\pmb P}{∂ t} + μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} \\end{aligned} \\right. \\tag{4.1.1} $$可简化为：（麦克斯韦方程组可推出真空电磁波，但真空电磁波推不出麦克斯韦方程组）\n$$ \\left\\{ \\begin{aligned} \\pmb ∇ \\cdot \\pmb E \u0026= 0 \u0026\\text{(真空无电荷)} \\\\ \\pmb ∇ × \\pmb E \u0026= -\\frac{∂ \\pmb B}{∂ t} \\\\ \\pmb ∇ \\cdot \\pmb B \u0026= 0 \\\\ \\pmb ∇ × \\pmb B \u0026= μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} \u0026\\text{(无电流,无介质)} \\end{aligned} \\right. \\tag{4.1.2} $$可以看到其中电场和磁场的形式不太对称：系数不同是由于单位选择，方向不同是左手系、右手系选择。\n解波动方程 消元\n电场、磁场的旋度再取旋度，于是等号右边就出现了对方：\n$$ \\left\\{ \\begin{aligned} \u0026\\pmb ∇ × (\\pmb ∇ × \\pmb E) = - \\frac{∂}{∂ t}(\\pmb ∇ × \\pmb B) = -μ_0 ε_0 \\frac{∂^2 \\pmb E}{∂ t^2} \\quad \\text{(时间微分$\\frac{∂}{∂ t}$与空间微分$\\pmb ∇ ×$交换了位置)} \\\\ \u0026\\pmb ∇ × (\\pmb ∇ × \\pmb B) = μ_0 ε_0 \\frac{∂}{∂ t}(\\pmb ∇ × \\pmb E) = -μ_0 ε_0 \\frac{∂^2 \\pmb B}{∂ t^2} \\end{aligned} \\right. $$这时电场与磁场形式就对称了。\n再根据公式： $\\pmb ∇ × (\\pmb ∇ × \\pmb f) = \\pmb ∇ (\\pmb ∇ \\cdot \\pmb f) - \\pmb ∇^2 \\pmb f$ ，可以把电场、磁场写成拉普拉斯算符的形式：\n$$ \\left\\{ \\begin{aligned} \u0026\\pmb ∇ × (\\pmb ∇ × \\pmb E) = \\pmb ∇ (\\pmb ∇ \\cdot \\pmb E) - \\pmb ∇^2 \\pmb E = 0 -\\pmb ∇^2 \\pmb E \u0026 \\text{(真空无电荷，电场散度为0)}\\\\ \u0026\\pmb ∇ × (\\pmb ∇ × \\pmb B) = \\pmb ∇ (\\pmb ∇ \\cdot \\pmb B) - \\pmb ∇^2 \\pmb B = 0 -\\pmb ∇^2 \\pmb B \\end{aligned} \\right. $$从而可得电场和磁场的波动方程：\n$$ \\left\\{ \\begin{aligned} \u0026 \\pmb ∇^2 \\pmb E - \\frac{1}{c^2} \\frac{∂^2 \\pmb E}{∂ t^2} = 0 \\\\ \u0026 \\pmb ∇^2 \\pmb B - \\frac{1}{c^2} \\frac{∂^2 \\pmb B}{∂ t^2} = 0 \\\\ \\end{aligned} \\right. \\tag{4.1.3} $$其中，$c=\\frac{1}{\\sqrt{μ_0 ε_0}}$ 是光速。 上面两个方程包含六个未知量：$E_x, E_y, E_z, B_x, B_y, B_z$ ，展开写为：\n$$ \\left\\{ \\begin{aligned} \\pmb ∇^2 E_x - \\frac{1}{c^2} \\frac{∂^2 E_x}{∂ t^2} = 0 \\\\ \\pmb ∇^2 E_y - \\frac{1}{c^2} \\frac{∂^2 E_y}{∂ t^2} = 0 \\\\ \\pmb ∇^2 E_z - \\frac{1}{c^2} \\frac{∂^2 E_z}{∂ t^2} = 0 \\\\ \\pmb ∇^2 B_x - \\frac{1}{c^2} \\frac{∂^2 B_x}{∂ t^2} = 0 \\\\ \\pmb ∇^2 B_y - \\frac{1}{c^2} \\frac{∂^2 B_y}{∂ t^2} = 0 \\\\ \\pmb ∇^2 B_z - \\frac{1}{c^2} \\frac{∂^2 B_z}{∂ t^2} = 0 \\\\ \\end{aligned} \\right. $$它们都有同一种形式：\n$$ \\begin{aligned} \\pmb ∇^2 ψ - \\frac{1}{c^2} \\frac{∂^2 ψ}{∂ t^2} = 0 \\\\ \\frac{∂^2 ψ}{∂ x^2} + \\frac{∂^2 ψ}{∂ y^2} + \\frac{∂^2 ψ}{∂ z^2} - \\frac{1}{c^2} \\frac{∂^2 ψ}{∂ t^2} = 0 \\end{aligned} $$ （空间和时间的系数不一样，$c^2$ 可以并到 $t$ 里，时间的度规是 -1，空间的度规是: $x^2+y^2+z^2$ 。）\n这个波动方程是一个二阶三维微分方程，先简化为一维形式：波函数 $ψ$ 仅是 $x$ 和 $t$ 的函数，在任何一个 y-z 平面内$ψ$ 值为常量，就像一个平面在 x 方向传播，也就是平面波：\n$$ \\frac{∂^2 ψ}{∂ x^2} - \\frac{1}{c^2} \\frac{∂^2 ψ}{∂ t^2} =0 $$它的解的形式为：\n$$ ψ(x,t) = f(x-ct) + g(x+ct) $$可以看到有两个波包，$f(x-ct)$是以速度为 $c$ 的波包向右传播，$g(x+ct)$是速度为 $-c$ 的波包向左传播，且波包的形状 $f(x), g(x)$ 不变。\n图4.1.2 $f(x)$ 和 $g(x)$ 的具体形式由初始条件和边界条件决定。\n例如给出边界条件：\n$$ \\left \\{ \\begin{aligned} ψ(x,0) \u0026= u(x) \u0026 \\text{(初始时刻任意位置场的大小)}\\\\\\\\ \\frac{∂ ψ}{∂ t} |_{t=0} \u0026= v(x) \u0026 \\text{(初始时刻场的变化率)} \\end{aligned} \\right. $$对于二阶微分方程，仅知道初始时刻场的大小是不能知道以后各时刻的演化的。就像对于牛二定律，只知道加速度，但不知道初始速度，也无法确定各时刻的情况。所以初始条件还需要给出初始时刻场的变化率。\n把两个初始条件带入平面波解的形式，可得两个方程：\n$$ \\left \\{ \\begin{aligned} \u0026 f(x) + g(x) = u(x) \\\\ \u0026 -c f'(x) + c g'(x) = v(x) \\\\ \\end{aligned} \\right. $$两个方程可以解出两个未知量$f(x),g(x)$。第2式中有一阶导，对两边积分（积分上下限随意，因为积分结果是一个数，后面有常量m可以做补充）可得：\n$$ f(x)-g(x) = - \\frac{1}{c} \\int _0 ^x v(ξ)dξ +m $$从而可解出两个波包：\n$$ \\left \\{ \\begin{aligned} f(x) = \\frac{1}{2} \\left[ u(x)-\\frac{1}{c} \\int_0^x v(ξ)dξ + m\\right] \\\\ g(x) = \\frac{1}{2} \\left[ u(x)+\\frac{1}{c} \\int_0^x v(ξ)dξ -m\\right] \\end{aligned} \\right. $$所以波函数的解为：\n$$ ψ(x,t) = \\frac{1}{2} \\left[ u(x-ct) + u(x+ct) + \\frac{1}{c} \\int_{x-ct}^{x+ct} v(ξ) dξ \\right] $$也就是说如果给定了初始条件：初始时刻任意位置场的大小和场的变化率，就可得到场在未来任意时刻的分布。这个解的物理意义是：给电磁场一个初始变化率，电磁场会演化分成两个波包，一个向左一个向右传播。\n图4.1.3 例1 一分二 初始条件：初始时刻场的变化率为零，即：\n$$ v(ξ) = \\frac{∂ ψ}{∂ t} |_ {t=0} = 0 $$对变化率积分，也就是波函数解的第三项：$\\frac{1}{c} \\int_{x-ct}^{x+ct} v(ξ) dξ = 0$ ，所以波函数的解：\n$$ ψ(x,t) = \\frac{1}{2} \\left[u(x-ct) + u(x+ct) \\right] $$ 所研究的 $\\pmb E$ 和 $\\pmb B$ 是时间的函数。如果没有初始变化率，则初始时刻的波包有一半向右传播，一半向左传播，两个波包的形状与初始时刻的波包形状相同。\n图4.1.4 例2 只向左传播 初始时刻场的时间变化率为：空间变化率乘c：\n$$ v(ξ) = \\frac{∂ ψ}{∂ t} \\bigg| _{t=0} = c u'(ξ) $$变化率积分得：\n$$ \\frac{1}{c} \\int_{x-ct}^{x+ct} v(ξ) dξ = u(x+ct) - u(x-ct) $$从而可写出波函数：\n$$ \\begin{aligned} ψ(x,t) \u0026 = \\frac{1}{2} \\left[ u(x-ct) + u(x+ct) + \\frac{1}{c} \\int_{x-ct}^{x+ct} v(ξ) dξ \\right] \\\\ \u0026 = \\frac{1}{2}\\left[ u(x-ct) + u(x+ct) + u(x+ct) - u(x-ct) \\right] \\\\ \u0026 = u(x+ct) \\end{aligned} $$ 根据变化率，下一时刻，x1的值会增加 $u(x_1)-u(x_0)$ ，所以波包是在向左移动\n图4.1.5 例3 只向右传播 初始时刻场的时间变化率为：空间变化率乘 c 的负数：\n$$ v(ξ) = \\frac{∂ ψ}{∂ t} |_{t=0} = -c u'(ξ) $$则变化率积分后：\n$$ \\frac{1}{c} \\int_{x-ct}^{x+ct} v(ξ) dξ = u(x-ct) - u(x+ct) $$从而波函数的解为：\n$$ \\begin{aligned} ψ(x,t) \u0026 = \\frac{1}{2} \\left[ u(x-ct) + u(x+ct) + \\frac{1}{c} \\int_{x-ct}^{x+ct} v(ξ) dξ \\right] \\\\ \u0026 = \\frac{1}{2}\\left[ u(x-ct) + u(x+ct) - u(x+ct) + u(x-ct) \\right] \\\\ \u0026 = u(x-ct) \\end{aligned} $$ 根据变化率，下一时刻，x1的值会减少 $u(x_1)-u(x_0)$ 这么多，也就是变成了x0的值，就是向右移动了。\n图4.1.6 三维平面波 电磁场在 y-z 平面内无变化，仅在 x 方向有空间变化和传输。\n根据一维平面波的解可知其解的形式为两个波包的组合：\n$$ \\left\\{ \\begin{aligned} \u0026 \\pmb E = \\pmb{E_1}(x-ct) + \\pmb{E_2}(x+ct) \\\\ \u0026 \\pmb B = \\pmb{B_1}(x-ct) + \\pmb{B_2}(x+ct) \\end{aligned} \\right. $$这六个波并非是各自独立的，还受到麦克斯韦方程组的约束。\n对于$E_x$ ：\n由$\\pmb ∇ \\cdot \\pmb E =0$，可得：$\\frac{∂ E_x}{∂ x} + \\frac{∂ E_y}{∂ y} + \\frac{∂ E_z}{∂ z} =0$ ;\n又因为 $\\pmb E$ 在 y-z 平面内无变化：$\\frac{∂ E_y}{∂ y}=0, \\frac{∂ E_z}{∂ z}=0$，从而 $\\frac{∂ E_x}{∂ x}=0$ 也应当为零，即$E_x$不随空间位置变化。\n由 $\\pmb ∇ × \\pmb B = μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t}$ 可得：\n$$ \\begin{aligned} \\frac{∂ E_x}{∂ t} \u0026= \\frac{1}{μ_0 ε_0} \\left( \\frac{∂ B_y}{∂ z} - \\frac{∂ B_z}{∂ y} \\right) \\\\ \u0026= \\frac{1}{μ_0 ε_0}(0-0) =0 \\end{aligned} $$说明 $E_x$ 也不随时间变化。踪上所述，$E_x$ 是一个恒定均匀电场，因为研究的是随时间变化的场，所以这部分不在考虑范围内，就取$E_x = 0$。\n对于 $B_x$：\n同样，由$\\pmb ∇ \\cdot \\pmb B=0,\\ \\pmb ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂ t}$，可知 $B_x$ 也是一个不随空间和时间变化的场，取$B_x = 0$。\n这样真空电磁波就是横波：在传播方向（x方向）上没有分布，只有在 y-z 方向上有分量，与传播方向垂直。\n对于 $E_z$ 和 $B_y$：\n根据电场旋度等于磁场时间微分的相反数$\\pmb ∇ × \\pmb E = - \\frac{∂ \\pmb B}{∂ t}$，可推出 $\\frac{∂ B_y}{∂ t}$ 与 $\\frac{∂ E_z}{∂ x}$ 的关系：\n$$ \\begin{aligned} \\pmb ∇ × \\pmb E \u0026 = - \\frac{∂ \\pmb B}{∂ t}\\\\ \\begin{vmatrix} \\pmb i \u0026 \\pmb j \u0026 \\pmb k \\\\ \\frac{∂}{∂ x} \u0026 \\frac{∂}{∂ y} \u0026 \\frac{∂}{∂ z} \\\\ E_x \u0026 E_y \u0026 E_z \\end{vmatrix} \u0026=- \\frac{∂ \\pmb B}{∂ t} \\\\ (\\frac{∂ E_z}{∂ y} - \\frac{∂ E_y}{∂ z}) \\pmb i -(\\frac{∂ E_z}{∂ x} - \\frac{∂ E_x}{∂ z}) \\pmb j +(\\frac{∂ E_y}{∂ x} - \\frac{∂ E_x}{∂ y}) \\pmb k \u0026= - (\\frac{∂ B_x}{∂ t} \\pmb i + \\frac{∂ B_y}{∂ t} \\pmb j + \\frac{∂ B_z}{∂ t} \\pmb k)\\\\ (\\frac{∂ E_z}{∂ y} - \\frac{∂ E_y}{∂ z}) \\pmb i - (\\frac{∂ E_z}{∂ x} - 0) \\pmb j + (\\frac{∂ E_y}{∂ x} - 0) \\pmb k = \u0026= - (0 + \\frac{∂ B_y}{∂ t} \\pmb j + \\frac{∂ B_z}{∂ t} \\pmb k ) \\end{aligned} $$根据对应项相等，可得：\n$$ \\frac{∂ B_y}{∂ t} = \\frac{∂ E_z}{∂ x}-\\frac{∂ E_x}{∂ z} = \\frac{∂ E_z}{∂ x} - 0 = \\frac{∂ E_z}{∂ x} $$也就是$B_y$的时间微分等于$E_z$对x求导，根据三维平面波解的形式，可知以下关系：\n$$ -c B'_{y1}(x-ct) + cB'_{y2}(x+ct) = E_{z1}'(x-ct) + E_{z2}'(x+ct) \\tag{4.1.6} $$ 由磁场的旋度等于电场的时间微分$\\pmb ∇ × \\pmb B = μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t}$ ，可得$\\frac{∂ E_z}{∂ t}$与$\\frac{∂ B_y}{∂ x}$的关系：\n$$ \\begin{aligned} \\pmb ∇ × \\pmb B \u0026= μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} \\\\ \\begin{vmatrix} \\pmb i \u0026 \\pmb j \u0026 \\pmb k \\\\ \\frac{∂}{∂ x} \u0026 \\frac{∂}{∂ y} \u0026 \\frac{∂}{∂ z} \\\\ B_x \u0026 B_y \u0026 B_z \\\\ \\end{vmatrix} \u0026 = μ_0 ε_0 \\left( \\frac{∂ E_x}{∂ t} \\pmb i + \\frac{∂ E_y}{∂ t} \\pmb j + \\frac{∂ E_z}{∂ t} \\pmb k \\right) \\\\ \\left( \\frac{∂ B_z}{∂ y} - \\frac{∂ B_y}{∂ z} \\right) \\pmb i - \\left( \\frac{∂ B_z}{∂ x} - \\frac{∂ B_x}{∂ z} \\right) \\pmb j + \\left( \\frac{∂ B_y}{∂ x} - \\frac{∂ B_x}{∂ y} \\right) \\pmb k \u0026 = μ_0 ε_0 \\left( \\frac{∂ E_x}{∂ t} \\pmb i + \\frac{∂ E_y}{∂ t} \\pmb j + \\frac{∂ E_z}{∂ t} \\pmb k \\right) \\\\ ()\\pmb i - (\\frac{∂ B_z}{∂ x} - 0) \\pmb j + (\\frac{∂ B_y}{∂ x} - 0) \\pmb k \u0026 = μ_0 ε_0 (0 + \\frac{∂ E_y}{∂ t} \\pmb j + \\frac{∂ E_z}{∂ t} \\pmb k) \\end{aligned} $$根据对应项相等，可得：\n$$ \\frac{1}{c^2} \\frac{∂ E_z}{∂ t} = \\frac{∂ B_y}{∂ x} - 0 = \\frac{∂ B_y}{∂ x} $$也就是$E_z$的时间微分等于$c^2$乘以$B_y$对x求导。把三维平面波的解代入其中，得到以下关系：\n$$ -E_{z1}'(x-ct)+E_{z2}'(x+ct) = cB_{y1}'(x-ct) + cB_{y2}'(x+ct) \\tag{4.1.7} $$由 4.1.6 和 4.1.7 两式，可得：\n$$ \\left\\{ \\begin{aligned} c B_{y1}'(x-ct) \u0026= -E_{z1}'(x-ct) \\\\ c B_{y2}'(x+ct) \u0026= E_{z2}'(x+ct) \\\\ \\end{aligned} \\right. $$两边积分后，忽略直流项可得：\n$$ \\left\\{ \\begin{aligned} c B_{y1}(x-ct) \u0026= -E_{z1}(x-ct) \\\\ c B_{y2}(x+ct) \u0026= E_{z2}(x+ct) \\\\ \\end{aligned} \\right. $$ 这个关系表明 $B_y$ 与 $E_z$ 形状相同，大小相差c倍，当 $E_z$ 沿正方向传播时，$B_y$ 在负半轴；当 $E_z$ 沿负方向传播时，$B_y$ 在正半轴。\n图4.1.7 对于 $E_y$和$B_z$ ：\n根据 $\\pmb ∇ × \\pmb E = - \\frac{∂ \\pmb B}{∂ t}$ 和 $\\pmb ∇ × \\pmb B = μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t}$ 可推出$E_y$ 和 $B_z$的关系，然后把三维平面波的解的形式代入其中，可得：\n$$ \\left\\{ \\begin{aligned} c B_{z1}(x-ct) = E_{y1}(x-ct) \\\\ c B_{z2}(x+ct) = -E_{y2}(x+ct) \\end{aligned} \\right. $$则$\\pmb B$ 的 z 方向分量与 $\\pmb E$ 的 y 方向分量的关系：\n图4.1.8 $B_z$与$E_y$形状相同，大小相差c倍；当$B_z$沿正方向传播时，$E_y$在正半轴；当$B_z$沿负方向传播时，$E_y$在负半轴。\n将传输方向把4个波分组得：\n$$ 向右： \\left \\{ \\begin{aligned} c B_{y1}(x-ct) = -E_{z1}(x-ct) \\\\ c B_{z1}(x-ct) = E_{y1}(x-ct) \\end{aligned} \\right. $$$$ 向左： \\left \\{ \\begin{aligned} c B_{y2}(x+ct) = E_{z2}(x+ct) \\\\ c B_{z2}(x+ct) = -E_{y2}(x+ct) \\end{aligned} \\right . $$可以看出，电场与磁场垂直，且电磁波传播方向为$\\pmb E × \\pmb B$ （右手定则）。\n由于$\\pmb E$ 和 $\\pmb B$ 之间的微分关系（方向锁定和大小比例相差c倍），只要给出 $\\pmb E$ 和 $\\pmb B$ 的初始时刻场分布，无须知道初始随时间变化率，即可得到任意时刻 $\\pmb E$ 和 $\\pmb B$ 的场分布。\n例如，给出初始时刻的$E_y(x)$ 和 $B_z(x)$ :\n$$ \\left\\{ \\begin{aligned} \u0026 E_{y1}(x) + E_{y2}(x) = E_y(x) \u0026\\text{两个波包组合} \\\\ \u0026 B_{z1}(x) + B_{z2}(x) = B_z(x) \\\\ \u0026 E_{y1}(x) = c B_{z1}(x) \u0026\\text{向右传播}\\\\ \u0026 E_{y2}(x) = -c B_{z2}(x) \u0026\\text{向左传播} \\end{aligned} \\right. $$就可解得：\n$$ \\left \\{ \\begin{aligned} E_{y1}(x) = \\frac{1}{2} \\left[ E_y(x) + c B_z(x) \\right] \\\\ B_{z1}(x) = \\frac{1}{2c} \\left[ E_y(x) + c B_z(x) \\right] \\\\ E_{y2}(x) = \\frac{1}{2} \\left[ E_y(x) - c B_z(x) \\right] \\\\ B_{z2}(x) = - \\frac{1}{2c} \\left[ E_y(x) - c B_z(x) \\right] \\\\ \\end{aligned} \\right. $$再加入时间 t：\n$$ \\left \\{ \\begin{aligned} E_{y1}(x,t) \u0026= \\frac{1}{2} \\left[ E_y(x-ct) + cB_z(x-ct) \\right] \u0026 \\text{向右传播}\\\\ B_{z1}(x,t) \u0026= \\frac{1}{2c} \\left[ E_y(x-ct) + cB_z(x-ct) \\right] \u0026 \\text{向右传播}\\\\ E_{y2}(x,t) \u0026= \\frac{1}{2} \\left[ E_y(x+ct) - cB_z(x+ct) \\right] \u0026 \\text{向左传播} \\\\ B_{z2}(x,t) \u0026= -\\frac{1}{2c} \\left[ E_y(x+ct) - cB_z(x+ct) \\right] \u0026 \\text{向左传播} \\\\ \\end{aligned} \\right. $$ 如果初始时刻 $E_y(x)=cB_z(x)$，那么解为：\n$$ \\left\\{ \\begin{aligned} \u0026 E_{y1}(x,t) = c E_y(x-ct) \\\\ \u0026 B_{z1}(x,t) = B_z(x-ct) \\\\ \u0026 E_{y2}(x,t) = 0 \\\\ \u0026 B_{z2}(x,t) = 0 \\end{aligned} \\right. $$说明仅有沿正方向传输的波包：\n图4.1.9 如果初始时刻 $E_y(x)=-c B_z(x)$ ，则解为：\n$$ \\left\\{ \\begin{aligned} \u0026 E_{y1}(x,t) = 0 \\\\ \u0026 B_{z1}(x,t) = 0 \\\\ \u0026 E_{y2}(x,t) = c E_y(x+ct) \\\\ \u0026 B_{z2}(x,t) = B_z(x+ct) \\end{aligned} \\right. $$ 图4.1.10 若初始时刻 $B_z(x)=0$ ，则解为：\n$$ \\left\\{ \\begin{aligned} \u0026 E_{y1}(x,t) = \\frac{1}{2} E_y(x-ct) \\\\ \u0026 B_{z1}(x,t) = \\frac{1}{2c} B_y(x-ct) \\\\ \u0026 E_{y2}(x,t) = \\frac{1}{2} B_y(x+ct) \\\\ \u0026 B_{z2}(x,t) = -\\frac{1}{2c} B_y(x+ct) \\end{aligned} \\right. $$有一半波包沿正向传输，另一半向负方向传输，所以在初始时刻两次磁场抵消了。\n图4.1.11 总结： 真空平面电磁波有以下特点:\n电场和磁场都是横波，即电场和磁场矢量与波的传输方向垂直。 某一特定方向传输的电磁波其电场和磁场相互垂直。 某一特定方向传输的电磁波的波包可以是任意形状，且传输过程形状不变。 某一特定方向传输的电场和磁场波包形状相同，比例为$|\\pmb E| = c |\\pmb B|$。 某一特定方向传输的电磁波的传输方向由电场和磁场方向锁定，传输方向为$\\pmb E × \\pmb B$ 的方向 4.2 真空平面时谐电磁波 时谐：随时间谐振。\n电磁场以固定频率 $ω$ 震荡，沿着 $x$ 轴正方向传播：\n$$ \\left \\{ \\begin{aligned} \\pmb E(x,t) = \\pmb E_0 \\mathrm{cos}[k(x-ct)+θ] \\\\ \\pmb B(x,t) = \\pmb B_0 \\mathrm{cos}[k(x-ct)+θ] \\end{aligned} \\right. $$ 角频率$ω=ck$；\n时间周期：$T=\\frac{2π}{ω}$；\n空间周期(波长)：$\\lambda=\\frac{2π}{k}$（所以$ω$和$k$是对应的）；\n电场磁场同相位$θ$；\n电场$\\pmb E_0$与磁场$\\pmb B_0$相互垂直，且均在 y-z 平面内，大小关系为 $\\left|\\pmb{E_0} \\right| = c \\left|\\pmb{B_0} \\right|$；\n传播方向$\\pmb{E_0} × \\pmb{B_0}$沿着x 轴正方向。\n不考虑共同相位，则时谐电磁波可写成：\n$$ \\left \\{ \\begin{aligned} \\pmb E(x,t) = \\pmb{E_0} cos(kx-ωt) \\\\ \\pmb B(x,t) = \\pmb{B_0} cos(kx-ωt) \\end{aligned} \\right. $$这个形式也适用沿着 x 轴负方向传播，不过$ω=-ck$。\n沿任意方向 沿任意 $\\pmb x(x,y,z)$ 方向传播的时谐电磁波：\n画图\nk 的矢量特性 波矢$\\pmb k$ 是空间频率，因为空间有三个维度，所以要表明是在哪个方向上的空间频率。 分离变量法 认为4个自变量独立，即平面时谐电磁波的6个分量都是4个独立函数的乘积：\n$$ E_i(\\pmb r,t) = f_1(x) f_2(y) f_3(z) g(t) $$ 步骤：\n代入二阶微分方程：\n电磁波的6个分量（$E_x,E_y,E_z,B_x,B_y,B_z$）都满足二阶微分方程： $$ \\pmb ∇^2 ψ - \\frac{1}{c^2} \\frac{∂^2 ψ}{∂ t^2}=0 $$将$E_i(\\pmb r,t)$ 代入此方程：\n$$ \\begin{aligned} f_2(y) f_3(z) g(t) \\frac{∂^2 f_1(x)}{∂ x^2} \u0026 + f_1(y) f_3(z) g(t) \\frac{∂^2 f_2(y)}{∂ y^2} + \\\\ f_1(y) f_2(z) g(t) \\frac{∂^2 f_3(z)}{∂ z^2} \u0026 - \\frac{1}{c^2} f_1(x) f_2(y) f_3(z) \\frac{∂^2 g(t)}{∂ t^2}=0 \\end{aligned} $$ 等号两边都除以 $E_i$ :\n$$ \\frac{ \\frac{∂^2 f_2(y)}{∂ y^2} }{f_1(x)}\n\\frac{ \\frac{∂^2 f_2(y)}{∂ y^2} }{f_2(y)} \\frac{ \\frac{∂^2 f_3(z)}{∂ z^2} }{f_3(z)} \\frac{1}{c^2} \\frac{ \\frac{∂^2 g(t)}{∂ t^2} }{g(t)} =0 $$ 令这四项分别等于：$-k_x^2,\\ -k_y^2,\\ -k_z^2,\\ \\ k_x^2+k_y^2+k_z^2$ ，则：\n$$ \\begin{aligned} \\frac{∂^2 f_2(y)}{∂ y^2} + k_x^2 f_1(x) = 0 \\\\ \\frac{∂^2 f_2(y)}{∂ y^2} + k_y^2 f_2(y) = 0 \\\\ \\frac{∂^2 f_3(z)}{∂ z^2} + k_z^2 f_3(z) = 0 \\\\ \\frac{∂^2 g(t)}{∂ t^2} + ω^2g(t) = 0 \\\\ \\end{aligned} $$其中 $ω^2=c^2(k_x^2 + k_y^2 + k_z^2)$ 。从实数域变到复数域的好处是不用分正弦和指数2种情况讨论，解都用指数形式表示。 在真空场中，满足拉普拉斯方程：\n$$ \\pmb ∇^2 \\varphi = 0 \\implies k_x^2 + k_y^2 + k_z^2 = 0 $$3个k里面至少有一个$k_i$是虚数；对于四维时空，满足：\n$$ \\pmb ∇^2 ψ - \\frac{1}{c^2} \\frac{∂^2 ψ}{∂ t^2} = 0 \\implies k_x^2 + k_y^2 + k_z^2 - \\frac{ω^2}{c^2}=0 $$因为有时间项的存在，所以可以在 xyz 三个方向上全是实数，即传播不会衰减，但是现实不接受无穷大，要用边界限制；\n也可以有2个方向远大于$\\frac{ω^2}{c^2}$，比如$k_x^2 + k_y^2 \\gg \\frac{ω^2}{c^2}$, 则 $k_z^2$ 是虚数，也就是x,y 方向上震荡，z方向衰减。\n解得？：\n$$E_i(x,t) = E_{i0} e^{i(k_x x + k_y y + k_z z-ωt)} = E_{i0} e^{i(\\pmb k \\cdot \\pmb t - ω t)}$$所以真空平面时谐电磁波的解为：\n$$ \\left\\{ \\begin{aligned} \\pmb E = \\pmb {E_0} e^{i(\\pmb k\\cdot \\pmb r- ω t)}\\\\ \\pmb B = \\pmb {B_0} e^{i(\\pmb k \\cdot \\pmb r- ω t)} \\end{aligned} \\right. $$此解同样受麦克斯韦方程组约束：\n根据：$\\left \\{ \\begin{aligned} \\pmb ∇ \\cdot \\pmb E =0 \\\\ \\pmb ∇ \\cdot \\pmb B =0 \\end{aligned} \\right.$ ， 可得：$\\left\\{ \\begin{aligned} \\pmb E_0 \\cdot \\pmb k =0 \\\\ \\pmb B_0 \\cdot \\pmb k=0 \\end{aligned} \\right.$， 说明：$\\pmb{E_0}$$与$$\\pmb k$垂直，$\\pmb{B_0}$与$\\pmb k$垂直。\n根据：$\\left\\{ \\begin{aligned} \\pmb ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂ t}\\\\ \\pmb ∇ × \\pmb B=μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} \\end{aligned} \\right.$， 可得：$\\left\\{ \\begin{aligned} i \\pmb k × \\pmb{E_0} = -iω \\pmb{B_0} \\\\ i \\pmb k × \\pmb{B_0} = iμ_0 ε_0 ω \\pmb{E_0} \\end{aligned} \\right.$ ， 说明：$\\pmb{E_0}, \\pmb{B_0},\\pmb k$ 三个向量相互垂直，遵循右手定则，且电场和磁场同相位。也就是说真空平面时谐电磁波是真空平面电磁波的特例：\n图4.2.1 偏振 相位差 $e^{iθ}$ ，就是两个振动的比值。\n对于两个同频率的运动：$\\left\\{ \\begin{aligned} a(t) \u0026= A cos\\, ωt \\\\ b(t) \u0026= B cos (ωt+θ) \\end{aligned} \\right.$\n用复数表示：$\\left\\{ \\begin{aligned} \\widetilde{a(t)} \u0026= A e^{iωt} \\\\ \\widetilde{b(t)} \u0026= B e^{iωt+θ} \\end{aligned} \\right.$\n两个运动的比值就是复振幅：$Arg \\left( \\frac{ \\widetilde{b(t)} }{\\widetilde{a(t)}} \\right) = \\frac{B}{A} e^{iθ}$ ，$ωt$消掉了，只包含相位差和两个振幅的比值。\n4.3 介电常数 就是电位移矢量 $\\widetilde{\\pmb D(ω)}$ 与电场 $\\widetilde{\\pmb E(ω)}$ 之间的相位差： $\\frac{\\widetilde{\\pmb D(ω)}}{\\widetilde{\\pmb E(ω)}}=e^{i θ}$\n.\n在真空中，$μ_0 ε_0$ 不随时间变化，所以介电常数对电磁波的传输没有影响；但对于时谐电场，介质极化可能跟不上电场的节奏（$\\pmb P(t) = ε_0 \\chi_e \\pmb E(t)$），使得电极化率 $\\chi_e$ 也是时间的函数，则介电常数 $ε = ε_0 (1+\\chi_e)$ 也随时间变化，导致解方程组很麻烦。如果介电常数不随时间变化，就可以直接套用真空电磁波的解；而在频域下，介电常数表达式中没有时间$t$。\n偶极就是电子的偏移量乘以电荷：\n$$ \\pmb p = -e \\pmb{x} $$电子偏移量按经典力学分析求解（见下小节），所以一个电荷产生的偶极为：\n$$ \\pmb p = -e \\pmb{x_0} e^{-i ω t} = \\frac{e^2 \\pmb{E_0} e^{-i ω t}}{m \\left[ω_0^2 - ω^2 - i ω γ \\right]} $$偶极密度$\\pmb P$（单位体积内偶极的数量为 n）：\n$$ \\widetilde{\\pmb P(t)} = \\frac{n e^2 \\pmb{E_0} e^{-i ω t}}{m \\left[ω_0^2 - ω^2 - i ω γ \\right]} $$所以电极化率 $\\chi_e$ 为:\n$$ \\chi_e = \\frac{ \\widetilde{\\pmb P(t)}}{ ε_0 \\widetilde{\\pmb E(t)}} = \\frac{n e^2}{ε_0 m \\left[ω_0^2 - ω^2 - i ω γ \\right]} $$所以介电常数表达式为：\n$$ ε(ω) = ε_0(1+ \\chi_e) = ε_0 + \\frac{n e^2}{ m \\left[ω_0^2 - ω^2 - i ω γ \\right]} $$ 介电常数$ε(ω)$ 是个实数：结果与真空情况相同\n介电常数$ε(ω)$ 是个复数：k也可能是复数，虽与真空形式相同，但物理意义不同。\n电子动力学 按经典观点分析：电子是质点，受3个力：外电场力、回复力、阻力:\n$$ \\begin{aligned} 电场力：\u0026 -e \\pmb E(\\pmb x,t) \u0026 \\\\ 回复力：\u0026 \\pmb F = -k \\pmb x \u0026 \\text{(微观单元中电子位移后产生)}\\\\ 阻力：\u0026 \\pmb f = -a \\dot{\\pmb x} \u0026 \\text{(近似正比于速度)}\\\\ \\end{aligned} $$根据牛顿第二定律，电子的运动微分方程为：\n$$ m \\left[\\ddot{\\pmb x} + γ \\dot{\\pmb x} + ω_0^2 \\pmb x \\right] = -e \\pmb E(\\pmb x,t) $$其中：$γ = \\frac{\\alpha}{m},\\ ω_0^2 = \\frac{k}{m}$\n这个二阶微分方程是受迫振动模型，方程的解与初始时刻电子位置 $\\pmb x$ ，速度 $\\dot{\\pmb x}$，电场随时间的变化率$\\pmb E(\\pmb x, t)$ 都有关系。\n当电场为谐振形式时：\n$$ \\pmb E(\\pmb x,t) = \\pmb{E_0} cos(ω t) $$电子位移的解也应当具有角频率为 $ω$ 的振荡，即为 $sin(ω t)$ 和 $cos(ω t)$ 的线性组合形式。采用复数的表达形式，则电场写为：$\\pmb E = \\pmb{E_0} e^{-i ω t}$ ，电子位移写成 $\\pmb x = \\pmb{x_0} e^{-i ω t}$，其中 $\\pmb{x_0}$ 也是复数，被称为复振幅，包含了与电场振动的相位差。\n原牛二方程可写为：\n$$ \\begin{aligned} m \\left[ -ω^2 - iω γ + ω_0^2 \\right] \\pmb{x_0} e^{-i ω t} = -e \\pmb{E_0} e^{-i ω t} \\\\ \\pmb{x_0} = \\frac{-e \\pmb{E_0}}{m \\left[ω_0^2 - ω^2 - i ω γ \\right]} \\end{aligned} $$ 复相位 同时包含了振幅和相位角\n$$ \\begin{aligned} \\pmb{x_0} \u0026= - \\frac{e\\pmb{E_0}}{m} \\frac{1}{ω_0^2 - ω^2 - iω γ} \\\\ \u0026 = -\\frac{e \\pmb{E_0}}{m} (ω_0^2 - ω^2 - iω γ)^{-1} \\\\ \\end{aligned} $$ 振幅：$\\sqrt{(ω_0^2 - ω^2)^2+(-ω γ)^2}$\n幅角：$\\left\\{ \\begin{aligned} sin θ \u0026= \\frac{-ω γ}{\\sqrt{(ω_0^2 - ω^2)^2+(-ω γ)^2}} \\\\ cos θ \u0026= \\frac{ω_0^2-ω^2}{\\sqrt{(ω_0^2 - ω^2)^2+(-ω γ)^2}} \\\\ tan θ \u0026= \\frac{-ω γ}{ω_0^2 - ω^2} \\end{aligned} \\right.$\n色散关系 在分离变量解四维时空的“拉普拉斯方程”时，需要满足：\n$$ \\begin{aligned} ω^2 \u0026= \\nu^2(k_x^2 + k_y^2 + k_z^2) \\\\ ω^2 \u0026= \\frac{1}{μ_0 ε(ω)} (k_x^2 + k_y^2 + k_z^2) \\end{aligned} $$ 介电常数$ε(ω)$ 决定了色散关系，$ω$ 是 $k$ 的函数。\n当$ε(ω) = ε_0$ 时，$ω = c k$，$ω$ 与 $k$ 是线性关系，也就是真空平面波，随时间推波形不变。\n$ε_r(ω)$\u0026gt;1介质 $$ ε(ω) \u003e ε_0(ω) $$ $$ ε(ω) = ε_0 + \\frac{n e^2}{m} \\frac{1}{ω_0^2 - ω^2 + i ω γ} $$当$ω \\ll ω_0$ 且 $γ \\ll ω_0$ 时，介电常数$ε(ω)$ (近似) 为实数，而且相位差很小：\n$$ tan θ = \\frac{ω γ}{ω_0^2-ω^2} $$可以看作 $\\pmb D(ω)$ 与 $\\pmb E(ω)$ 是同相位，$\\pmb E(ω)$ 发生了变化，$\\pmb D(ω)$ 立刻跟上。\n在此种介质中，某一频率的平面光的波矢$\\pmb k(ω)$ 大于该频率在真空中的波矢$\\pmb{k_0}(ω)$，波长小于真空中的波长： $$ \\begin{aligned} k(ω) = \\sqrt{μ_0 ε(ω)} ω = \\sqrt{μ_0 ε_0 ε_r(ω)} ω = \\sqrt{ε_r(ω)} k_0(ω) \u003e k_0(ω) \\\\\\\\ \\lambda(ω) = \\frac{2π}{k(ω)} = \\frac{2π}{\\sqrt{\\epsilon_r(ω)}k_0(ω)} = \\frac{\\lambda_0}{\\sqrt{ε_r(ω)}}\u003c \\lambda_0 \\end{aligned} $$ 在此种介质中，某一频率的平面光的相速度小于真空光速：\n$$ \\nu_{ph}(ω) = \\frac{ω}{k(ω)} = \\frac{1}{\\sqrt{μ_0 ε(ω)}} = \\frac{1}{\\sqrt{μ_0 ε_0 ε_r(ω)}} = \\frac{c}{\\sqrt{\\epsilon_r(ω)}} \u003c c $$ 不同频率的电磁波的相速度不同，由$ε(ω)$决定。（相速度就是某一频率波的传播速度）\n傅里叶展开 群速度 波包中心的传输速度：$\\nu_g = \\frac{d ω}{d k}$\n任意平面电磁波，在初始时刻，可以用傅里叶变换分解成一系列不同频率$ω$ 的时谐平面电磁波的叠加，因为频率不同，传输速度不同，在传播一段时间之后，将各个波包叠加起来后与初始时刻形状不同。因此很难精确描述波包的传输速度，需要使用群速度来大概描述波包中心的传输速度。\n证明：对于一维函数$u(x,t)$ 可做傅里叶展开为一系列波矢 k 的波的积分：\n$$ u(x,t) = \\frac{1}{\\sqrt{2π}} \\int A(k) e^{ikx-i ω t} dk $$其中：\n$$ A(k) = \\frac{1}{\\sqrt{2π}} \\int u(x,0) e^{-kx} dx $$为波包初始时刻在 $k$ 空间展开的系数，每一个 $k$ 对应着一个振动频率 $ω = ω(k)$ ，之后各波包以 $e^{ikx-iω t}$ 的形式传输。\n因为 $ω(k)$ 形式很复杂，取 $k_0$ 为这个波包 $A(k)$ 的波包中心波矢，$ω(k)$ 在 $k_0$ 处的泰勒展开为：\n$$ ω(k) = ω(k_0) + \\left. \\frac{d ω}{d k} \\right|_{k_0} (k-k_0) + \\frac{1}{2} \\left. \\frac{d^2 ω}{d k^2} \\right|_{k_0} (k-k_0)^2 + \\cdots $$取一级线性近似，令 $ω_0 = ω(k_0)$，把$ω(k)$代入波包表达式$u(x,t)$：\n$$ \\begin{aligned} u(x,t) \u0026\\approx \\frac{1}{\\sqrt{2π}} \\int A(k)\\ exp \\left[ ikx - i \\left( ω(k_0) + \\frac{d ω(k)}{d k} |_{k_0}(k-k_0) \\right) \\right] dk \\\\ \u0026 = \\frac{exp \\left[ i \\left( \\frac{d ω(k)}{d k} |_{k_0} k_0 - ω_0 \\right) t \\right] }{\\sqrt{2π}} \\int A(k) exp \\left[ ikx-i \\frac{d ω}{dk} |_{k_0} kt \\right] dk \u0026 \\text{(把与 k 无关的部分提到积分号外面)} \\\\ \u0026 = \\underbrace{ exp \\left[ i \\left( \\frac{d ω (k)}{d k} \\bigg|_{k_0} - ω_0 \\right) t \\right] } _{随时间振荡因子，振幅为1} \\cdot \\underbrace{ u \\left(x-\\frac{d ω}{d k} \\bigg|_{k_0}t,\\ 0 \\right) } _{波包以速度为\\frac{dω}{dk} 传播} \\end{aligned} $$ 当 $ω(k) = v_g(k-k_0)$ 时，振荡因子等于0，整个波包的传输以$\\nu_g$ 稳定传输，且群速度等于相速度；\n当 $ω(k) = \\nu _g k + \\alpha$ 线性函数时，振荡因子不等于0，有细节振动；\n当 $ω(k) = ω_0 + \\nu_g(k-k_0)$ 时，上式$u(x)$ 约等号变为等号，波包的传输速度就等于$\\frac{d ω}{d k}$ ，且波包形状不随时间变化。但是相位因子: $\\left( \\left. \\frac{d ω}{d k} \\right|_{k_0}k_0 - ω_0 \\right) t$ 不为零，所以传输过程还有个整体的振荡。\n一个频率的速度是 相速度：$\\nu_{ph}= \\frac{ω}{k}$\n多个频率叠加的速度是 群速度：$\\nu_g = \\frac{\\rm d ω}{d k}$\n4.6 介质界面处 在两种不同介电常数的材料交界面处，有三束波：\n$$ \\begin{array}{c} 入射波i： \\begin{array}{cc} \\left{ \\begin{array}{c} \\pmb{E_i} = \\pmb{E_{i0}} e^{i(\\pmb k_i \\cdot x - ω_i t)} \\ \\pmb{B_i} = \\pmb{B_{i0}} e^{i(\\pmb k_i \\cdot x - ω_i t)} \\end{array} \\right. \\end{array}\\\n反射波 r： \\begin{array}{cc} \\left\\{ \\begin{array}{c} \\pmb{E_r} = \\pmb{E_{r0}} e^{i(\\pmb k_r \\cdot x - ω_r t)} \\\\ \\pmb{B_r} = \\pmb{B_{r0}} e^{i(\\pmb k_r \\cdot x - ω_r t)} \\end{array} \\right. \\end{array}\\\n透射波 t： \\begin{array}{cc} \\left\\{ \\begin{array}{c} \\pmb{E_t} = \\pmb{E_{t0}} e^{i(\\pmb k_t \\cdot x - ω_t t)} \\\\ \\pmb{B_t} = \\pmb{B_{t0}} e^{i(\\pmb k_t \\cdot x - ω_t t)} \\end{array} \\right. \\end{array} \\end{array} $$\n界面连续条件 参考电磁场的边界条件-维基百科\n电位移矢量$\\pmb D$ 的法向分量就是分界面上的表面电荷。如果分界面上没有表面电荷，那么 $\\pmb D$ 的法向分量连续； $\\pmb E$ 的切向分量在界面两侧是连续的； $\\pmb B$ 的法向分量在界面两侧是连续的； $\\pmb H$ 的切向分量是两种介质间的表面电流密度。因此如果分界面上没有表面电流，$\\pmb H$ 的切向分量在界面两侧是连续的。 因为每束波都包含 $\\pmb{E_0}(x,y,z), \\pmb{B_0}(x,y,z), \\pmb k(x,y,z), ω$ 共10个未知量，三束波就是30个未知量；不过它们并非独立，还受麦克斯韦方程约束，即界面两侧的连续性条件：\n$$ \\left\\{ \\begin{aligned} \\underbrace{ \\pmb ∇ \\cdot \\pmb D = ρ_f}_{真空无自由电荷=0} \u0026\\Rightarrow \\pmb{n_{12}} \\cdot (\\pmb{D_1} - \\pmb{D_2}) = 0 \u0026\\text{$\\pmb D$法向连续} \\\\ \\underbrace{ \\pmb ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂ t} }_{\\frac{∂ \\pmb B}{∂ t}是有限值,\\ 取非常小面积积分=0} \u0026\\Rightarrow \\pmb n × (\\pmb{E_1}-\\pmb{E_2}) = 0 \u0026\\text{$\\pmb E$切向连续} \\\\ \\pmb ∇ \\cdot \\pmb B = 0 \u0026\\Rightarrow \\pmb n \\cdot (\\pmb{B_1} - \\pmb{B_2}) = 0 \u0026 \\text{$\\pmb B$法向连续} \\\\ \\underbrace{ \\pmb ∇ × \\pmb H = -\\frac{∂ \\pmb D}{∂ t}+\\pmb{J_f} }_{\\frac{∂ \\pmb D}{∂ t} 是有限量，取非常小面积分=0; 无J_f} \u0026\\Rightarrow \\pmb n × (\\pmb{H_1} - \\pmb{H_2}) = 0 \u0026 \\text{$\\pmb H$切向连续} \\\\ \\end{aligned} \\right. $$折射率$\\pmb{n_{12}}$ 是从介质1 到 介质2 的法向单位矢量，某矢量乘上它代表该矢量的法向分量；某矢量叉乘它代表该矢量的切向分量。\n根据这些约束条件，减少自由度。\n$ω$相等,$k_x$守恒 三束波的角频率 ω 一定相等，而且3个波矢的水平分量$k_x$也相等。\n因为连续条件要求任意时刻都连续，如果有一束波的频率不等于ω，即使在某一时刻相等了，后面也不连续，所以任一时刻三束波的频率都相等；(能量守恒；时间平移不变性)\n$$ ω_i = ω_r = ω_t = ω $$因为连续条件要求任一位置都连续，如果有一束波的波矢（空间角频率）在 x-y 平面上的分量不等于 $\\pmb k_x$ ，即使在某一位置相等了，其他位置也不连续，所以任一位置三束波的 x-y 平面分量 都相等。（x方向空间平移不变性，z方向不满足）\n$$ \\pmb{k_i}_{//} = \\pmb{k_r}_{//} = \\pmb{k_t}_{//} $$为了不失一般性，将这个方向定义为 x 方向，则 y 方向的分量就为0：\n$$ \\left\\{ \\begin{aligned} k_{ix} = k_{rx} = k_{tx} = k_x \\\\ k_{iy} = k_{ry} = k_{ty} = 0 \\end{aligned} \\right. $$ 仅用入射波的角频率 ω 和（入射方向）波矢 $k_x$ 两个变量就能完全决定三束波的角频率和波矢（12个变量变2个）。\n根据色散关系：\n$$ \\left{ \\begin{aligned} \u0026amp; k_{ix}^2 + k_{iy}^2 + k_{iz}^2 = μ_0 ε_1(ω) ω^2 = k_1^2 \\\n\u0026amp; k_{rx}^2 + k_{ry}^2 + k_{rz}^2 = μ_0 ε_1(ω) ω^2 = k_1^2 \u0026amp; \\text{(入射反射波在同一介质中)}\\\n\u0026amp; k_{tx}^2 + k_{ty}^2 + k_{tz}^2 = μ_0 ε_2(ω) ω^2 = k_2^2 \\ \\end{aligned} \\right. $$\n因为 3束波的$k_x$都相等，3个$k_y$ 等于0，$k_1, k_2$ 的大小由波的频率$ω$ 决定，所以 $k_z$ 可用 $k_x$ 表示：\n$$ \\left\\{ \\begin{aligned} \u0026 k_{iz} = -k_{rz} = \\sqrt{k_1^2 - k_x^2} \u0026\\text{(方向相反)} \\\\ \u0026 k_{tz} = \\sqrt{k_2^2 -k_x^2} \\end{aligned} \\right. $$所以，仅用$k_x$ 即可表示出三个波的波矢：\n$$ \\left\\{ \\begin{aligned} \\pmb{k_i} = \\left( k_x, 0, \\sqrt{k_1^2 - k_x^2} \\right) \\\\ \\pmb{k_r} = \\left( k_x, 0, -\\sqrt{k_1^2 - k_x^2} \\right) \\\\ \\pmb{k_t} = \\left( k_x, 0, \\sqrt{k_2^2 - k_x^2} \\right) \\\\ \\end{aligned} \\right. $$其中：$\\left\\{ \\begin{aligned} k_1=\\sqrt{μ_0 ε_1(ω)}ω \\\\ k_2=\\sqrt{μ_0 ε_2(ω)}ω \\end{aligned} \\right.$\ns波p波约束 s波：电场振动方向垂直于入射面(x-z 面)，包含三个电磁场分量 $E_y,H_x,H_z$；\np波：电场振动方向在入射面(x-z 面)内，包含三个电磁场分量 $H_y, E_x, E_z$。\n根据4.2节由麦克斯韦方程组推出的真空平面电磁波的约束条件：\n$$ \\left\\{ \\begin{aligned} \u0026 \\pmb{E_0} \\cdot \\pmb k = 0 \u0026 \\text{(传播方向$\\pmb k$定了,$\\pmb E_0$只能在2个方向内)} \\\\ \u0026 i \\pmb{k} × \\pmb{E_0} = -i ω \\pmb{B_0} \u0026 \\text{($\\pmb k$和$\\pmb{E_0}$都定了,$\\pmb{B_0}$就唯1确定了)} \\end{aligned} \\right. $$所以每束波的$\\pmb{E_0}$ 和 $\\pmb{B_0}$ 6个分量（$E_{0x},E_{0y},E_{0z},B_{0x},B_{0y},B_{0z}$）中，只有两个未知量，可以是$\\pmb E_0$的两个方向分量，也可以是从6个中任选2个。这里选择 $E_y$ 和 $B_y$ 作为两个独立的未知量分析它们如何决定一束波。选它们的好处在于它们代表了两种相互正交的偏振状态，如下图4.6.1所示：\n图4.6.1 $E_y$ 和 $B_y$ 是独立的，分别可以推出$H_x,H_z$，和$E_x,E_z$，麦克斯韦方程组被分成了两个独立的方程组。\ns 波：\n电场仅有 $E_y$ 分量（没有$E_x,E_z$分量），则代表了电场偏振方向垂直于入射面（x-z 面），称为 s 波（如红色），则磁场方向在入射面（x-z面）内； 因为$k_y=0$，所以 s 电磁波的电场分量$E_y$为：\n$$ E_y(\\pmb r,t) = \\widetilde{A_s} e^{i(k_x x + k_z z - ω t)} $$根据：$i \\pmb{k} × \\pmb{E_0} = - i ω \\pmb{B_0}$，展开可得：\n$$ (-k_z \\cdot E_y)\\pmb i + 0 + (k_x \\cdot E_y)\\pmb k = i ω(B_x, 0, B_z) $$对应项相等？可得 s 波的磁场的两个分量：\n$$ \\begin{cases} B_x(\\pmb r, t) = - \\widetilde{A_s} \\frac{k_z}{ω} e^{i(k_x x + k_z z - ω t)} \\\\ B_z(\\pmb r, t) = \\widetilde{A_s} \\frac{k_x}{ω} e^{i(k_x x + k_z z - ω t)} \\end{cases} $$ p 波：\n磁场仅有$B_y$分量（没有$B_y,B_z$分量），则代表了电场偏振在入射面（x-z面）内，称为 p 波（如绿色）。 因$k_y=0$，所以 p 电磁波的磁场分量$B_y$为：\n$$ B_y(\\pmb r, t) = \\widetilde{A_p} e^{i(k_x x + k_z z - ω t)} $$再根据：$\\left \\{ \\begin{aligned} \u0026 \\pmb{E_0} \\cdot \\pmb{k} =0 \\\\ \u0026 i \\pmb{k} × \\pmb{E_0} = - i ω \\pmb{B_0} \\end{aligned} \\right.$ 分别得到：\n$$ \\begin{aligned} \u0026 \\pmb{E_0} \\cdot \\pmb k =0 \\\\ \u0026 E_{0x} \\cdot k_x + \\cancel{E_{0y} \\cdot k_y} + E_{0z} \\cdot k_z = 0 \\\\ \u0026 E_z = - \\frac{E_x k_x}{k_z} \\end{aligned} $$$$ \\begin{aligned} i \\begin{vmatrix} \\pmb i \u0026amp; \\pmb j \u0026amp; \\pmb k \\ k_x \u0026amp; k_y \u0026amp; k_z \\ E_x \u0026amp; E_y \u0026amp; E_z \\ \\end{vmatrix}\n= - i ω (0, B_y, 0) \\\\ (\\cancel{ \\underline{k_y} \\cdot E_z} - \\cancel{k_z \\cdot \\underline{E_y}}) \\pmb i - (k_x \\cdot E_z - k_z \\cdot E_x) \\pmb j + (\\cancel{k_x \\cdot \\underline{E_y} } - \\cancel{ \\underline{k_y} \\cdot E_x)} \\pmb k = -ω(0, B_y, 0) \u0026amp;\n\\end{aligned} $$\n对应项相等，并把Ez 用 Ex表示，可解得 p 波电场的 Ex 分量：\n$$ \\begin{aligned} k_x \\cdot (-\\frac{E_x k_x}{k_z}) - k_z \\cdot E_x \u0026amp;= ω \\cdot B_y \\\nE_x k_x^2 -k_z^2 E_x \u0026amp;= ω k_z \\widetilde{A_p} e^{i(k_x x + k_y y - ω t)} \\ -E_x(k_x^2+k_z^2+k_y^2) \u0026amp;= ω k_z \\widetilde{A_p} e^{i(k_x x + k_y y - ω t)} \\\n-E_x \\frac{ω^2}{\\nu^2} \u0026amp;= ω k_z \\widetilde{A_p} e^{i(k_x x + k_y y - ω t)} \\\nE_x \u0026amp;= - \\widetilde{A_p} \\frac{k_z}{μ_0 ε(ω) ω} e^{i(k_x x + k_y y - ω t)} \\ \\end{aligned} $$\n也就可以得到 p 波电场的 Ez 分量：\n$$ \\left \\{ \\begin{aligned} E_x(\\pmb r, t) \u0026= - \\widetilde{A_p} \\frac{k_z}{μ_0 ε(ω) ω} e^{i(k_x x + k_y y - ω t)} \\\\ E_z(\\pmb r, t) \u0026= \\widetilde{A_p} \\frac{k_x}{μ_0 ε(ω) ω} e^{i(k_x x + k_y y - ω t)} \\end{aligned} \\right. $$ 通过 s 波和p 波，每一束电磁波的两个自由度分别转化成了$A_s$和$A_p$。任意一束单色入射波都可以分解为两束互相垂直的线偏振波——s波和p波。\n这两种偏振光斜入射\ns波反射折射系数 反射系数：反射波与入射波在界面处复振幅之比。\n折射系数：透射波与入射波在界面处复振幅之比。\n界面处三束s波：\n$$ \\begin{array}{c} \\begin{array}{cc} 入射波：\\\\ \\left\\{ \\begin{array}{c} E_{iy}(\\pmb r, t) = A_{is} e^{i(k_x x + k_{iz}z - ωt)} \\\\ B_{ix}(\\pmb r, t) = -A_{is} \\frac{k_{iz}}{ω} e^{i(k_x x+k_{iz}z - ωt)} \\\\ B_{iz}(\\pmb r, t) = A_{is} \\frac{k_x}{ω} e^{i(k_x x+k_{iz}z - ωt)} \\end{array} \\right. \\end{array} \u0026 \\begin{array}{cc} 反射波：\\\\ \\left\\{ \\begin{array}{c} E_{ry}(\\pmb r, t) = A_{rs} e^{i(k_x x + k_{rz}z - ωt)} \\\\ B_{rx}(\\pmb r, t) = -A_{rs} \\frac{k_{rz}}{ω} e^{i(k_x x+k_{rz}z - ωt)} \\\\ B_{rz}(\\pmb r, t) = A_{rs} \\frac{k_x}{ω} e^{i(k_x x+k_{rz}z - ωt)} \\end{array} \\right. \\end{array} \u0026 \\begin{array}{cc} 透射波：\\\\ \\left\\{ \\begin{array}{c} E_{ty}(\\pmb r, t) = A_{ts} e^{i(k_x x + k_{tz}z - ωt)} \\\\ B_{tx}(\\pmb r, t) = -A_{ts} \\frac{k_{tz}}{ω} e^{i(k_x x+k_{tz}z - ωt)} \\\\ B_{tz}(\\pmb r, t) = A_{ts} \\frac{k_x}{ω} e^{i(k_x x+k_{tz}z - ωt)} \\end{array} \\right. \\end{array} \\end{array} $$ 在界面（z=0）两侧，电场的切向分量连续（$\\pmb n × (\\pmb{E_1} - \\pmb{E_2}) = 0$），也就是说界面上侧电场的切向分量，与界面下侧电场的切向分量相等，还要注意界面下侧的电场是两束波的叠加：\n$$ E_{iy} + E_{ry} = E_{ty} \\implies A_{is} + A_{rs} = A_{ts} \\tag{1} $$ 因为对于介电介质（非磁性无损耗介质）的磁导率是$μ_0$，没有$μ$，所以$B$ 与 $H$ 一样（$B=μ H$ ，$\\pmb n × (\\pmb{H_1} - \\pmb{H_2}) = 0$），所以B或H 的切向分量也连续：\n$$ B_{ix} + B_{rx} = B_{tx} \\implies -A_{is} \\frac{k_{iz}}{ω} - A_{is} \\frac{k_{rz}}{ω} = -A_{ts} \\frac{k_{tz}}{ω} \\tag{2} $$ 在界面（z=0）两侧，磁感应强度法向分量连续（$\\pmb n \\cdot (\\pmb{B_1} - \\pmb{B_2}) = 0$）：\n$$ B_{iz} + B_{rz} = B_{tz} \\implies A_{is} \\frac{k_x}{ω} + A_{rs} \\frac{k_x}{ω} = A_{ts} \\frac{k_x}{ω} \\tag{3} $$ 方程(3) 与 方程(1) 相同，说明不能一下子把3个未知量都确定下来，还留一个自由度，这也与实际实验情况相符，在实验中，以入射光为自由度，调节其强度、频率和入射角度，它决定了反射波和透射波。\n只取前两个方程，两边同除以入射波的复振幅，并将$k_{iz} = - k_{rz}$ 代入，可得s波的透射波与入射波、反射波与入射波的振幅之比：\n$$ \\Large \\begin{cases} \\frac{A_{ts}}{A_{is}} = \\frac{2 k_{iz}}{k_{iz} + k_{tz}}\\\\ \\frac{A_{rs}}{A_{is}} = \\frac{k_{iz} - k_{tz}}{k_{iz} + k_{tz}} \\end{cases} $$由此看出，s 波的三束波的电磁场仅有一个自由度的变量（入射波）。\np波反射折射系数 p波在界面处的三束波：\n$$ \\begin{array}{c} \\begin{array}{cc} 入射波：\\\\ \\begin{cases} B_{iy}(\\pmb r, t) = A_{ip} e^{i(k_x x + k_{iz} z - ωt)} \\\\ E_{ix}(\\pmb r, t) = A_{ip} \\frac{k_{iz}}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{iz} z - ωt)} \\\\ E_{iz}(\\pmb r, t) = -A_{ip} \\frac{k_x}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{iz} z - ωt)} \\end{cases} \\end{array} \u0026 \\begin{array}{cc} 反射波：\\\\ \\begin{cases} B_{ry}(\\pmb r, t) = A_{rp} e^{i(k_x x + k_{rz} z - ωt)} \\\\ E_{rx}(\\pmb r, t) = A_{rp} \\frac{k_{rz}}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{rz} z - ωt)} \\\\ E_{rz}(\\pmb r, t) = -A_{rp} \\frac{k_x}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{rz} z - ωt)} \\end{cases} \\end{array} \u0026 \\begin{array}{cc} 透射波：\\\\ \\begin{cases}{c} B_{ty}(\\pmb r, t) = A_{tp} e^{i(k_x x + k_{tz} z - ωt)} \\\\ E_{tx}(\\pmb r, t) = A_{tp} \\frac{k_{tz}}{μ_0 ε_2(ω) ω} e^{i(k_x x + k_{tz} z - ωt)} \\\\ E_{tz}(\\pmb r, t) = -A_{tp} \\frac{k_x}{μ_0 ε_2(ω) ω} e^{i(k_x x + k_{tz} z - ωt)} \\end{cases} \\end{array} \\end{array} $$在界面两侧，磁感应强度$\\pmb B$ /磁场强度$\\pmb H$ 的切向分量连续：$B_{iy} + B_{ry} = B_{ty}$；\n在界面两侧，电场$\\pmb E$ 的切向分量连续：$E_{ix} + E_{rx} = E_{tx}$；\n在界面两侧，电位移矢量$\\pmb D$ 的法向分量连续：$ε_1(ω) \\left[ E_{iz} + E_{rz} \\right] = ε_2(ω) E_{tz}$。\n也就是：\n$$ \\begin{cases} A_{ip} + A_{rp} = A_{tp} \u0026amp; (1) \\\nA_{ip} \\frac{k_{iz}}{μ_0 ε_1(ω) ω} + A_{rp} \\frac{k_{rz}}{μ_0 ε_1(ω) ω} = A_{tp} \\frac{k_{tz}}{μ_0 ε_2(ω) ω} \u0026amp; (2)\\\nε_1(ω) \\left[ A_{ip} \\frac{k_x}{μ_0 ε_1(ω) ω} A_{rp} \\frac{k_{x}}{μ_0 ε_1(ω) ω} \\right] = - ε_2(ω) A_{tp} \\frac{k_{x}}{μ_0 ε_2(ω) ω} \u0026amp; (3) \\end{cases} $$ 方程（3）与方程（1）相同，所以只取前两个，并将 $\\large k_{iz} = - k_{rz}$ 代入以上方程可得:\n$$ \\begin{cases} A_{ip} + A_{rp} \u0026 = A_{tp} \u0026 (1) \\\\ A_{ip} \\frac{k_{iz}}{μ_0 ε_1(ω) ω} - A_{rp} \\frac{k_{iz}}{μ_0 ε_1(ω) ω} \u0026 = A_{tp} \\frac{k_{tz}}{μ_0 ε_2(ω) ω} \u0026 (2) \\end{cases} $$方程(1) 代入方程(2) 可得p波的透射波与入射波、反射波与入射波的振幅之比：\n$$ \\begin{cases} A_{ip} \\frac{k_{iz}}{μ_0 ε_1(ω) ω}\n(A_{tp} - A_{ip}) \\frac{k_{iz}}{μ_0 ε_1(ω) ω} = A_{tp} \\frac{k_{tz}}{μ_0 ε_2(ω) ω} \u0026amp; \\text{(可以解得$\\frac{A_{tp}}{A_{ip}})$} \\ A_{ip} \\frac{k_{iz}}{μ_0 \\epsilon_1(ω) ω} A_{rp} \\frac{k_{iz}}{μ_0 ε_1(ω) ω} = (A_{ip}+ A_{rp}) \\frac{k_{tz}}{μ_0 ε_2(ω) ω} \u0026amp; \\text{(可以解得$\\frac{A_{rp}}{A_{ip}})$} \\end{cases} $$ $$ \\left { \\begin{aligned} \\frac{A_{tp}}{A_{ip}} = \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz} + ε_1(ω) k_{tz}} \\\n\\frac{A_{rp}}{A_{ip}} = \\frac{ε_2(ω) k_{iz} - ε_1(ω) k_{tz}}{ε_2(ω) k_{iz} + ε_1(ω) k_{tz}} \\end{aligned} \\right. $$\n由此可看出，$p$ 波的三束波的电磁场仅有一个自由度的变量。\n4.7 介电常数\u0026gt;1的介质界面 菲涅尔公式 根据色散关系：$k_1^2= k_x^2 + k_y^2 + k_z^2 = μ_0 ε(ω) ω^2$， 只要$ω$ 定了，$k^2$大小是定值（方向未定），再给定$k_x$，因$k_y=0$，$k_z$就定了。\n根据界面连续条件，可得反射定律和折射定律：\n图4.7.1 由色散关系，三束波的波矢的每个分量都可取为实数，则波矢分量可写成角度形式（实数才能写角度）：\n$$ \\begin{cases} sin θ_1 = \\bigg| \\frac{k_x}{k_1} \\bigg| \\\\ sin θ_r = \\bigg| \\frac{k_x}{k_1} \\bigg| \\\\ sin θ_2 = \\bigg| \\frac{k_x}{k_2} \\bigg| \\\\ \\end{cases} $$可得反射角和入射角之间的关系：$θ_r = θ_1$ （反射定律）\n折射角和入射角之间的关系：$\\frac{sin θ_1}{sin θ_2} = \\frac{k_2}{k_1} = \\sqrt{\\frac{ε_2(ω)}{ε_1(ω)} }$\n介质折射率：$n(ω) = \\sqrt{\\frac{ε(ω)}{ε_0}}$\n则：$\\frac{sin θ_1}{sin θ_2} = \\frac{n_2}{n_1}$ （折射定律）\n反射定律和折射定律的本质原因是$k_x$ 守恒，有时又称为动量守恒。而$k_x$守恒又是由于表面平整，即平移对称性。如果界面上不够平整，比如有灰尘，则在界面上有其它波矢产生，最终在非反射和折射方向出现光，即为散射。\n电场透射系数(角度) 把电场透射波与入射波振幅之比换成角度表示。\ns 波：\ns 波是用电场定义的，所以 s 波的振幅之比就是电场的振幅之比。\n$$ \\begin{aligned} \\frac{A_{ts}}{A_{is}} \u0026= \\frac{2 k_{iz}}{k_{iz} + k_{tz}} \\\\ \u0026 = \\frac{2 \\frac{k_{iz}}{k_x}}{\\frac{k_{iz}}{k_x} + \\frac{k_{tz}}{k_x}} \u0026 \\text{(两边同除kx,变成角度)} \\\\ \u0026 = \\frac{2 \\rm cot θ_1}{\\rm cot θ_1 + cot θ_2} \u0026 \\text{(上下同乘 $sin θ_1 \\cdot sin θ_2$)} \\\\ \u0026 = \\frac{2 \\rm cos θ_1 sin θ_2}{\\rm cos θ_1 sin θ_2 + cos θ_2 sin θ_1} \u0026 \\text{(二角和差公式)} \\\\ \u0026 = \\frac{2 \\rm cos θ_1 sin θ_2}{\\rm sin(θ_1 + θ_2)} \\end{aligned} $$ p 波：\n为什么乘上$\\sqrt{\\frac{ε_1}{ε_2}}$？：因为 p 波是用磁场定义的，$\\frac{A_{tp}}{A_{ip}}$是磁场振幅之比，所以需要把磁场振幅换成电场振幅：磁场的大小是电场大小的$\\frac{1}{c}$，也就是$\\frac{1}{\\sqrt{μ_0 ε(ω)}}$，所以各自要乘上在各自介质中系数，才是电场的振幅。\n$$ \\begin{aligned} \\sqrt{ \\frac{ε_1}{ε_2} } \\frac{A_{tp}}{A_{ip}} \u0026amp;= \\sqrt{\\frac{ε_1}{ε_2}} \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz} + ε_1(ω) k_{tz}} \\\n\u0026amp;= \\frac{\\rm sin θ_2}{\\rm sin θ_1} \\frac{2 \\frac{ε_2(ω)}{ε_1(ω)} \\frac{k_{iz}}{k_x} }{ \\frac{ε_2(ω)}{ε_1(ω)} \\frac{k_{iz}}{k_x} + \\frac{k_{tz}}{k_x}} \u0026amp; \\text{(上下同除$ε_1(ω) k_x$变成角度)} \\\n\u0026amp;= \\frac{\\rm sin θ_2}{\\rm sin θ_1} \\frac{2 \\frac{\\rm sin^2(θ_1)}{\\rm sin^2(θ_2)} \\rm cot θ_1}{ \\frac{\\rm sin^2(θ_1)}{\\rm sin^2 (θ_2)} \\rm cot θ_1 + \\rm cot θ_2} \u0026amp; \\text{(上下同乘$\\rm sin^2(θ_2)$)} \\\n\u0026amp;= \\frac{\\rm sin θ_2}{\\rm sin θ_1} \\frac{2 sin^2(θ_1) \\rm cot θ_1 }{ sin^2(θ_1) \\rm cot θ_1+ sin^2(θ_2) \\rm cot θ_2} \\\n\u0026amp;= \\frac{\\rm sin θ_2}{\\rm sin θ_1} \\frac{2 sin^2(θ_1) \\frac{\\rm cos θ_1}{\\rm sin θ_1} }{ sin^2(θ_1) \\frac{\\rm cos θ_1}{\\rm sin θ_1} + sin^2(θ_2)\\frac{\\rm cos θ_2}{\\rm sin θ_2}} \u0026amp; \\text{(上下同除$\\rm sin θ_1$)} \\\n\u0026amp; = \\frac{2 \\rm sin θ_2 cos θ_1}{\\rm sin θ_1 cos θ_1 + sin θ_2 cos θ_2 } \\\n\u0026amp; = \\frac{4 \\rm sin θ_2 cos θ_1}{\\rm sin(2 θ_1) + sin(2 θ_2)} \\\n\u0026amp; = \\frac{2 \\rm sin θ_2 cos θ_1}{\\rm sin(θ_1 + θ_2) sin(θ_1 - θ_2)} \u0026amp;\\text{==怎么化简?==}\\ \\end{aligned} $$\n注意：这里不是透过率，透过率是透射波与入射波的能量之比，电磁波的能量表达式是：$\\frac{1}{2} ε E_0^2$（$E_0$是振幅）， 而且注意对于p波的透射波和入射波在不同的介质里，所以求能量时需要乘上各自的介电常数。\n电场反射系数(角度) 电场反射波与入射波的振幅之比，换成角度表示。\n对于 s 波的反射系数：\n$$ \\begin{aligned} \\frac{A_{rs}}{A_{is}} \u0026amp; = \\frac{k_{iz} - k_{tz}}{k_{iz} + k_{tz}} \\\n\u0026amp; = \\frac{ \\frac{k_{iz}}{k_x} - \\frac{k_{tz}}{k_x} } { \\frac{k_{iz}}{k_x} + \\frac{k_{tz}}{k_x} } \u0026amp;{(上下同除kx)} \\\n\u0026amp; = \\frac{\\rm cot θ_1 - cot θ_2 }{\\rm cot θ_1 + cot θ_2 }\t\u0026amp; \\text{(换成角度)} \\\n\u0026amp; = \\frac{\\rm cos θ_1 sin θ_2 - cosθ_2 sin θ_1}{\\rm cos θ_1 sin θ_2 + cosθ_2 sin θ_1} \u0026amp; \\text{(换成 sin 和 cos)} \\\n\u0026amp; = \\frac{\\rm sin(θ_2 - θ_1)}{\\rm sin(θ_2 + θ_1)} \\end{aligned} $$\n对于 p 波的反射系数：有没有负号都可以？\n$$ \\begin{aligned} -\\frac{A_{rp}}{A_{ip}} \u0026amp; = -\\frac{ε_2(ω) k_{iz} - ε_1(ω) k_{tz}}{ε_2(ω) k_{iz} + ε_1(ω) k_{tz}} \\ \u0026amp; = -\\frac{ \\frac{ε_2(ω)}{ε_1(ω)} k_{iz} - k_{tz} }{ \\frac{ε_2(ω)}{ε_1(ω)} k_{iz} + k_{tz} } \\\n\u0026amp; = -\\frac{ \\frac{\\rm sin^2 θ_1}{\\rm sin^2 θ_2} \\frac{k_{iz}}{k_x} - \\frac{k_{tz}}{k_x} }{\\frac{\\rm sin^2 θ_1}{\\rm sin^2 θ_2} \\frac{k_{iz}}{k_x} + \\frac{k_{tz}}{k_x} } \\\n\u0026amp; = -\\frac{\\rm sin^2 θ_1 cot θ_1 - sin^2 θ_2 cot θ_2 } {\\rm sin^2 θ_1 cot θ_1 + sin^2 θ_2 cot θ_2 } \\ \u0026amp; = -\\frac{\\rm sin θ_1 cos θ_1 - sin θ_2 cos θ_2}{\\rm sin θ_1 cos θ_1 + sin θ_2 cos θ_2} \\ \u0026amp; = -\\frac{\\rm sin 2 θ_1 - sin 2 θ_2}{\\rm sin 2 θ_1 + sin 2 θ_2} \u0026amp;{二倍角公式}\\ \u0026amp; = -\\frac{\\rm sin(θ_1 - θ_2) cos(θ_1 + θ_2)}{\\rm cos(θ_1 - θ_2) sin(θ_1 + θ_2)} \u0026amp;{和差化积公式}\\ \u0026amp; = -\\frac{\\rm tan(θ_1 - θ_2)}{\\rm tan(θ_1 + θ_2)} \u0026amp; \\text{wolfram:True} \\end{aligned} $$\n半波损失 光从光疏介质射向光密介质时，反射波在离开反射点时的振动方向相对于入射波到达入射点时的振动相反，或者说，反射波相对于入射波相位突变π，这种现象叫做半波损失。\n对于 s 波电场的反射系数：$\\frac{A_{rs}}{A_{is}} = \\frac{\\rm sin(θ_2 - θ_1)}{\\rm sin(θ_2 + θ_1)}$ ：\n当 $θ_1 \u003e θ_2$ 时，也就是从光疏到光密，反射系数是负值，说明振幅符号相反，多了个-1，也就是多了$e^{iπ}=\\rm cosπ + i\\ sinπ = -1$，相差π相位，像是损失了一半波长；\n而当 $θ_1\u003cθ_2$ 时，反射系数是正值，振幅符号相同，相差0相位，没有半波损失。\n对于 p 波：\n为什么会发生半波损失？ - 带带大法师的回答 - 知乎\n如果反射系数能写成角度形式，而且两个正弦的比是一个负值，说明有半波损失。如果不能写成角度形式，那只有在入射波和反射波相位差（$2 arctan(\\frac{β}{k_z})$）等于$π$ 时才有半波损失。\n布儒斯特角 当 入射角$θ_1$(/反射角)+折射角$θ_2$=90度时，p波的反射系数趋近于0，仅有s波的反射，这时的入射角是布儒斯特角。\n图4.7.5 对于p波的反射系数：\n$$ \\begin{aligned} \\frac{A_{rp}}{A_{ip}} \u0026= \\frac{\\rm tan(θ_1 - θ_2)}{\\rm tan(θ_1 + θ_2)} \\\\ \u0026=\\frac{tan(θ_1 - θ_2)}{\\infin} \\to0 \\end{aligned} $$当$θ_1 + θ_2 = 90度$，分母无穷大，反射系数趋近于零。\n求解布儒斯特角：\n$$ \\left\\{ \\begin{aligned} θ_1 + θ_2 = \\frac{π}{2} \\\\ \\frac{\\rm sin θ_1}{\\rm sin θ_2} = \\frac{n_2}{n_1} \\end{aligned} \\right. $$可得：$θ_1 = \\rm arctan(\\frac{n_2}{n_1})$\n全反射 当光线从光密介质（较高折射率的介质）进入到光疏介质（较低折射率的介质），入射角大于全反射临界角时，折射光会消失，只有反射波。\n介质 1 的色散关系：$k_1^2 = k_x^2 + k_{iz}^2 = μ_0 ε_1(ω) ω^2$ ；\n介质 2 的色散关系：$k_2^2 = k_x^2 + k_{tz}^2 = μ_0 ε_2(ω) ω$ ；\n假如$ε_1(ω)\u003eε_2(ω)$，根据色散关系可得：$k_1^2 \u003e k_2^2$，通过变换入射光角度，可以找到一个 $k_x$，使得：\n$$ \\large k_2 \u003c k_x \u003c k_1 $$这时在介质1中：$k_{iz} = -k_{rz} = \\sqrt{k_1^2 - k_x^2}$ 是一个实数；而在介质2中：$k_{tz}=\\sqrt{k_2^2 - k_x^2}$ 是一个纯虚数，也就是：$k_{tz} = i \\sqrt{k_x^2 - k_2^2} = i β$ ，说明此时在介质2中，电磁波沿着 z 方向的传播为指数衰减的形式：\n$$ \\large e^{i k_{tz} z} = e^{i \\sqrt{k_x^2 - k_2^2}z} = e^{i\\cdot i \\sqrt{k_x^2 - k_2^2} z} = e^{- \\sqrt{k_x^2 - k_2^2} z} $$波矢分量$k_x$ 越大，衰减越快，离开界面足够远的时候 z 方向电磁波衰减为零；而x方向还是振荡传播，也就是说没有光从界面射出。\n穿透深度（趋肤深度）：$\\delta = \\frac{1}{β} = \\frac{1}{\\sqrt{k_x^2 - k_2^2}}$。因为隐式波其实不可能衰减到0，只是说到了某一位置，降低到某一数值以下就认为到0了，这个深度叫穿透深度。\n根据全反射条件：$k_2 \u003c k_x \\frac{k_2}{k_1} = \\sqrt{\\frac{ε_2}{ε_1}} = \\frac{n_2}{n_1} $$所以全反射临界角为：$θ_c = \\rm arcsin(\\frac{n_2}{n_1})$\n入射波与反射波的相位差一般不是$π$，没有半波损失。\n$$ \\begin{aligned} \u0026 s波：\\frac{A_{rs}}{A_{is}} = \\frac{k_{iz} - iβ}{k_{iz}+i β} \\\\ \u0026 p波：\\frac{A_{rp}}{A_{ip}} = \\frac{ε_2(ω) k_{iz} - ε_1(ω) i β}{ε_2(ω) k_{iz} + ε_1(ω) i β} \\end{aligned} $$s 波的相位差：$2 \\rm arctan(\\frac{β}{k_{iz}}) = 2 arctan( \\frac{ \\sqrt{k_x^2 - k_2^2} }{ \\sqrt{k_1^2 -k_x^2} }) = 2 arctan( \\sqrt{\\frac{k_x^2 - μ_0 ε_2(ω) ω^2}{ μ_0 ε_1(ω) ω^2 - k_x^2}} )$，因为$k_1$与$k_2$相差不大，折射率范围在1～3，\np 波的相位差：$2 \\rm arctan(\\frac{ε_1(ω) β}{ε_2(ω) k_{iz}})$\ns波全反射 介质界面处平面电磁波的形式不变，只是把 $k_{tz}$ 换成 $i β$ $\\left( = i \\sqrt{k_x^2 - k_2^2} \\right)$，透过系数和反射系数为：\n$$ \\Large \\begin{cases} \\frac{A_{ts}}{A_{is}} = \\frac{2 k_{iz}}{k_{iz} + i β}\\\\ \\frac{A_{rs}}{A_{is}} = \\frac{k_{iz} - iβ}{k_{iz} + i β} \\end{cases} $$ 反射系数 $\\frac{A_{ts}}{A_{is}} = \\frac{k_{iz} - iβ}{k_{iz} + i β}$ （虚数不能写成角度形式）的模为1，也就是说反射波与入射波的振幅一样，所以称之为全反射。反射波与入射波的相位相差 $2 \\rm arctan(\\frac{β}{k_{iz}})$，不是0，也不是$π$，是一个可观的角度，不是半波损失。当相位差等于$π$ 时，才有半波损失。\n将$A_{rs} = A_{is} \\frac{k_{iz} - iβ}{k_{iz} + i β}$ 和 $A_{ts} = A_{is} \\frac{2 k_{iz}}{k_{iz} + i β}$代入反射波和透射波的方程为：\n$$ 反射波： \\left{ \\begin{aligned} \u0026amp; E_{ry}(\\pmb r,t) = A_{is} \\frac{k_{iz} - i β}{k_{iz} +i β } e^{i(k_x x +k_{rz} z - ω t)} \\ \u0026amp; B_{rx}(\\pmb r,t) = A_{is} \\frac{k_{iz} - i β}{k_{iz} +i β } \\frac{k_{iz}}{ω} e^{i(k_x x +k_{rz} z - ω t)} \\ \u0026amp; B_{rz}(\\pmb r,t) = A_{is} \\frac{k_{iz} - i β}{k_{iz} +i β } \\frac{k_{x}}{ω} e^{i(k_x x +k_{rz} z - ω t)} \\ \\end{aligned} \\right.\n透射波： \\left{ \\begin{aligned} \u0026amp; E_{ty}(\\pmb r,t) = A_{is} \\frac{2 k_{iz}}{k_{iz}+i β} e^{i(k_x x + k_{tz} z - ω t)} \\ \u0026amp; B_{tx}(\\pmb r,t) = -A_{is} \\frac{2 k_{iz}}{k_{iz}+iβ} \\frac{i β}{ω} e^{i (k_x x + k_{tz} z - ω t)} \\ \u0026amp; B_{tz}(\\pmb r,t) = A_{is} \\frac{2 k_{iz}}{k_{iz}+iβ} \\frac{k_x}{ω} e^{i (k_x x + k_{tz} z - ω t)} \\end{aligned} \\right. $$\n透射波的 x 方向磁场比 z 方向磁场相差$\\frac{π}{2}$ 相位：\n$$ \\large \\begin{aligned} \\frac{-A_{is} \\frac{2k_{iz}}{k_{iz} + i β} \\frac{i β}{ω} } {A_{is} \\frac{2 k_{iz}}{k_{iz} + i β} \\frac{k_x}{ω} } \u0026= - \\frac{i β}{k_x} = - i \\frac{\\sqrt{k_x^2 - k_2^2}}{k_x} = e^{i π} e^{i \\frac{π}{2}} \\sqrt{1 - \\frac{k_2^2}{k_x^2}}\\\\ \u0026= e^{i \\frac{3 π}{2}} \\sqrt{1 - \\frac{k_2^2}{k_x^2}} \\end{aligned} $$相差$\\frac{π}{2}$ 相位是椭偏，也就是说：$B_{tx}$ 和 $B_{tz}$ 合成后的磁场，随着波向x正方向传播，它的方向一直在旋转，旋转方向由 $k_x$ 的方向决定。\np波全反射 介质界面处平面电磁波的形式不变，只是把 $k_{tz}$ 换成 $i β$ $\\left( = i \\sqrt{k_x^2 - k_2^2} \\right)$，透过系数和反射系数为：\n$$ \\Large \\begin{cases} \\frac{A_{tp}}{A_{ip}} = \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz} + ε_1(ω) k_{tz}} = \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz} + ε_1(ω) i β} \\\n\\frac{A_{rp}}{A_{ip}} = \\frac{ε_2(ω) k_{iz} - ε_1(ω) k_{tz}}{ε_2(ω) k_{iz} + ε_1(ω) k_{tz}} = \\frac{ε_2(ω) k_{iz} - ε_1(ω) i β}{ε_2(ω) k_{iz} + ε_1(ω) i β} \\\\ \\end{cases} $$\np波(磁场)反射系数 $\\frac{ε_2(ω) k_{iz} - ε_1(ω) k_{tz}}{ε_2(ω) k_{iz} + ε_1(ω) i β}$ 的模为1，也就是说反射波与入射波的振幅一样，所以称为全反射。反射波与入射波的相位差：$2 \\rm arctan \\left( \\frac{ε_1(ω) β}{ε_2(ω) k_{iz}} \\right)$ ，当相位差等于$π$ 时，才有半波损失。\n把 $A_{rp} = A_{ip} \\frac{ε_2(ω) k_{iz} - ε_1(ω) i β}{ε_2(ω) k_{iz}+ ε_1(ω) i β}$ 和 $A_{tp} = A_{ip} \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz}+ ε_1(ω) i β}$ 代入 p 波的反射波 和透射波方程：\n$$ 反射波： \\large \\begin{cases} B_{ry}(\\pmb r, t) = A_{ip} \\frac{ε_2(ω) k_{iz} - ε_1(ω) i β} {ε_2(ω) k_{iz} + ε_1(ω) i β} e^{i(k_x x + k_{rz} z - ωt)} \\\\ E_{rx}(\\pmb r, t) = - A_{ip} \\frac{ε_2(ω) k_{iz} - ε_1(ω) i β} {ε_2(ω) k_{iz} +ε_1(ω) i β} \\frac{k_{iz}}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{rz} z - ωt)} \u0026\\text{$(k_{iz}=-k_{rz}$)}\\\\ E_{rz}(\\pmb r, t) = - A_{ip} \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz} ε_1(ω) i β} \\frac{k_x}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{rz} z - ωt)} \\end{cases} $$$$ 透射波： \\large \\begin{cases} B_{ty}(\\pmb r, t) = A_{ip} \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz}+ ε_1(ω) i β} e^{i(k_x x + k_{tz} z - ωt)} \\\\ E_{tx}(\\pmb r, t) = A_{ip} \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz}+ ε_1(ω) i β} \\frac{i β}{μ_0 ε_2(ω) ω} e^{i(k_x x + k_{tz} z - ωt)} \\\\ E_{tz}(\\pmb r, t) = - A_{ip} \\frac{2 ε_2(ω) k_{iz}}{ε_2(ω) k_{iz}+ ε_1(ω) i β} \\frac{k_x}{μ_0 ε_2(ω) ω} e^{i(k_x x + k_{tz} z - ωt)} \\end{cases} $$透射波的 x 方向电场 $E_{tx}$ 比 z 方向电场 $E_{tz}$ 差 $\\frac{π}{2}$ 相位，也是椭偏，合成后的电场方向在旋转，旋转方向与 $k_x$ 方向决定。\n4.8 介电常数为虚数介质界面 波矢的 z 方向分量 $k_z$ 有虚部，沿 z 方向传播振幅指数衰减。\n根据受迫振动模型：\n$$ \\begin{aligned} ε(ω) =\u0026amp; ε_0 + \\underbrace{\\frac{n e^2}{m} \\frac{1}{ω_0^2 - ω^2 + i ω γ} }_{χ (虚数)} \\\n\\mathrm{Re} (ε(ω)) =\u0026amp; ε_0 + \\frac{n e^2}{m} \\frac{ω_0^2 - ω^2}{(ω_0^2 - ω^2)^2 + (ω γ)^2} \\\n\\mathrm{Im} (ε(ω)) =\u0026amp; \\frac{n e^2}{m} \\frac{ω γ}{(ω_0^2 - ω^2)^2 + (ω γ)^2} \\end{aligned} $$\n真空介电常数 $ε_0$ 再加上电极化率 $χ$ 部分，就是介电常数 $ε(ω)$；n 是单位体积内电子数目，$ω_0$ 是共振频率，$ω$ 是外场的频率，$γ$ 是阻力系数（阻力与速度成正比）。\n如果$ω = ω_0$，则电极化率部分 $χ(ω)$ 的实部变为零，则介电常数变为：\n$$ ε(ω) = ε_0 + i \\frac{n e^2}{m γ ω} $$这还不是纯虚数，要满足虚部远大于实部：$\\frac{n e^2}{m γ ω} \\gg ε_0$，才能近似看作是纯虚数.\n$\\large ε$与电导关系 电导反映了电子的运动，为电极化矢量$\\pmb D$与外加电场$\\pmb E$之间带来了$\\frac{π}{2}$ 的相位差。\n阻力系数$γ$ 表明阻力与速度成正比：$m γ v$ 。这个速度是阻力与外加电场力平衡后的速度，因为电子的初速度为0，需要加速一段时间才能平衡，如果电场变化不是很快（即低频下），电场驱动力与阻力实时保持平衡：$m γ v = e \\pmb E$，可见$v \\propto \\pmb E$ 。\n电流密度与电子速度成正比：\n$$ \\pmb J =n e v = \\frac{n e^2 \\pmb E}{m γ} = σ \\pmb E $$ 由此可得电导率的表达式： $$ σ = \\frac{n e^2}{m γ} $$ 把电导率代入共振频率时的介电常数，可得： $$ ε(ω) = ε_0 + i \\frac{σ}{ω} $$与静电场不同的是，交变电场下电导部分也可以看作是介电常数的一部分，只是贡献的是介电常数的虚部。介电常数代表电位移矢量与外加电场之间的相位差，电导在虚部说明它带来了$\\frac{π}{2}$ 的相位差（$e^{i \\frac{π}{2}} = i$）。\n电位移矢量$\\pmb D$是偶极子密度，偶极就是位置乘上电荷量（$p = \\pmb r \\cdot e$），位置$r$与速度$v$相位差$\\frac{π}{2}$，速度正比于电流密度$\\pmb J$（$=nsv$），电流密度正比于电场$\\pmb E$（$J=σ E$），所以电位移矢量$\\pmb D$与电场$\\pmb E$ 相位相差$\\frac{π}{2}$.\n从麦克斯韦方程组出发，将电导率写入介电常数：\n在角频率为$ω$ 的时谐场中：\n$$ ∇ × \\pmb H = \\pmb J_f + \\frac{∂ \\pmb D}{∂ t} $$假如$\\pmb D = ε_0 \\pmb E = ε_0 E_0 e^{-i ω t}$，并把$\\pmb J = σ \\pmb E$ 代入上式：\n$$ \\pmb ∇ × \\pmb H = σ \\pmb E - i ω ε_0 \\pmb E = -i ω \\left( ε_0 + i \\frac{σ}{ω} \\right) \\pmb E $$这其中已经出现了 $\\left( ε_0 + i \\frac{σ}{ω} \\right)$ ，令其等于$ε(ω)$，这样就有：\n$$ \\pmb ∇ × \\pmb H = ε(ω) \\frac{\\pmb E}{∂ t} = \\frac{∂ \\pmb D}{∂ t} $$这样就把电流项写到了电位移矢量中。\n电导率 表示物质传输电流能力强弱的一种测量值。\n当施加电压于导体的两端时，其电荷载子会呈现朝某方向流动的行为，因而产生电流。\n根据欧姆定律，定义为电流密度$\\pmb J$ 和 电场强度 $\\pmb E$ 的比值。\n$$ \\pmb J = σ \\pmb E $$ 与 $I = \\frac{U}{R}$ 等价。\n$ε(ω)$纯虚数 $$ε(ω) = i \\frac{σ}{ω}$$ 在交变电场达到共振频率时，介电常数等于：$ε(ω) = ε_0 + i \\frac{σ}{ω}$；如果虚部与实部的比值非常大：\n$$ \\begin{aligned} \\frac{ \\frac{σ}{ω} }{ε_0} \\gg 1 \\\\ \\frac{σ}{ε_0} \\gg ω \\end{aligned} $$即电导率足够大时，导体的介电常数近似为纯虚数：$i \\frac{σ}{ω}$ ，称为理想导体。\n此时色散关系为：\n$$ k^2 = μ_0 ε(ω) ω^2 =i μ_0 σ ω $$ 一般金属的电导率$σ$ 在 $10^3 \\sim 10^6 \\rm S$ ，则要求 $ω \\ll 10^{14}$ (可见光波段)。\n垂直入射金属 真空中一束光垂直入射金属表面，s波反射系数近似等于-1，p波反射系数近似等于1。\n由于光垂直入射界面，所以没有 $k_x$ 分量，所以在真空中： $k_{iz} = \\sqrt{μ_0 ε_0 ω^2 - k_x^2}= \\sqrt{μ_0 ε_0}ω$；\n在金属中的透射波波矢 z 方向分量：\n$$ \\begin{aligned} k_{tz} \u0026= \\sqrt{i μ_0 σ ω} \\\\ \u0026 = \\left[μ_0 σ ω e^{i \\frac{π}{2}} \\right]^\\frac{1}{2} \\\\ \u0026 = \\sqrt{μ_0 σ ω}\\ e^{i \\frac{π}{4}} \\\\ \u0026 = \\sqrt{μ_0 σ ω}\\ \\left( cos(\\frac{π}{4}) + i sin(\\frac{π}{4}) \\right) \\\\ \u0026 = \\sqrt{\\frac{μ_0 σ ω}{2}}\\ + i\\sqrt{\\frac{μ_0 σ ω}{2}} \\\\ \\end{aligned} $$根据 s波的菲涅尔公式：\n$$ \\Large \\begin{cases} \\frac{A_{ts}}{A_{is}} = \\frac{2 k_{iz}}{k_{iz} + k_{tz} } \\\\ \\frac{A_{rs}}{A_{is}} = \\frac{k_{iz} - k_{tz}}{k_{iz} + k_{tz} } \\end{cases} $$代入$k_{iz}、k_{tz}$，得到光垂直入射金属表面是，（能量）反射率为：\n$$ R = \\left| \\frac{A_r}{A_i} \\right|^2 = \\left| \\frac{ \\sqrt{μ_0 ε_0} ω - \\left( \\sqrt{\\frac{μ_0 σ ω}{2}}\\ + i\\sqrt{\\frac{μ_0 σ ω}{2}} \\right)} {\\sqrt{μ_0 ε_0} ω + \\left( \\sqrt{\\frac{μ_0 σ ω}{2}}\\ + i\\sqrt{\\frac{μ_0 σ ω}{2}} \\right)} \\right|^2 ≈ 1 - 2 \\sqrt{\\frac{2 ω ε_0}{σ}} $$（能量(功率)等于$\\frac{1}{2} ε E_0^2$，所以上面要取平方。）\n对于反射系数：\n$$ \\frac{A_r}{A_i} = \\frac{k_{iz} - k_{tz}}{k_{iz} + k_{tz}} = \\frac{\\sqrt{μ_0 ε_0} ω - \\sqrt{μ_0 ε(ω)} ω}{ \\sqrt{μ_0 ε_0} ω + \\sqrt{μ_0 ε(ω)} ω} ≈ -1 $$因为 $ε(ω) = ε_0 + i \\frac{σ}{ω}$ 被近似为纯虚数，需要满足前提条件：$\\frac{σ}{ω} ≫ ε_0$， 所以$ε(ω) ≫ ε_0$，所以在$R_s$中主要部分是$\\frac{- \\sqrt{μ_0 ε(ω)}}{+\\sqrt{μ_0 ε(ω)}}$，比值为 -1，即认为理想金属 s波 的反射系数$R_s$为 -1。\n在导体内部的传输深度$\\delta$（趋肤深度）等于$β$ 的倒数： $$ δ = \\frac{1}{β} = \\frac{1}{k_{tz}} = \\frac{1}{\\sqrt{i μ_0 σ ω}} $$取 $σ$ 在$10^4$ 量级，$ω$ 在GHz波段（手机通讯频率），$ε_0 = 8.854 × 10^{-12} F/m$，则反射系数 $R \\sim 1-10^{-3} ≈ 1$，穿透深度$δ \\sim 10^{-3}$m。相对于其波长（$\\frac{c}{ω}=\\frac{3 × 10^8}{10^9}=30$cm），只能穿透1毫米，所以穿透效果很弱。\n根据 p波的菲涅尔公式：\n$$ \\Large \\begin{cases} \\frac{A_{tp}}{A_{ip}} = \\frac{2 ε(ω) k_{iz}}{ε(ω) k_{iz} + ε_0 k_{tz}} \\\n\\frac{A_{rp}}{A_{ip}} = \\frac{ε(ω) k_{iz} - ε_0 k_{tz}}{ε(ω) k_{iz} + ε_0 k_{tz}} \\end{cases} $$\n对于其反射系数：\n$$ \\begin{aligned} \\frac{A_{rp}}{A_{ip}} \u0026= \\frac{ε(ω) k_{iz} - ε_0 k_{tz}}{ε(ω) k_{iz} + ε_0 k_{tz}} = \\frac{ε(ω) \\sqrt{μ_0 ε_0}ω - ε_0 \\sqrt{μ_0 ε(ω)}ω }{ε(ω) \\sqrt{μ_0 ε_0}ω + ε_0 \\sqrt{μ_0 ε(ω)}ω} = \\frac{ε(ω) \\sqrt{ε_0} - ε_0 \\sqrt{ε(ω)}}{ε(ω) \\sqrt{ε_0} + ε_0 \\sqrt{ε(ω)}} \\\\ \u0026 \\approx \\frac{i \\mathrm 10^{-5} × (\\frac{\\sqrt{2}}{2} + i \\frac{\\sqrt{2}}{2}) × 3 × 10^{-6} - 8.854 × 10^{-12} × (\\frac{\\sqrt{2}}{2} + i \\frac{\\sqrt{2}}{2}) × 10^{-2} } { i \\mathrm 10^{-5} × (\\frac{\\sqrt{2}}{2} + i \\frac{\\sqrt{2}}{2}) × 3 × 10^{-6} + 8.854 × 10^{-12} × (\\frac{\\sqrt{2}}{2} + i \\frac{\\sqrt{2}}{2}) × 10^{-2} } \\\\ \u0026 \\approx \\frac{10^{-11} - 10^{-14}}{10^{-11} + 10^{-14}} \\\\ \u0026 \\approx \\frac{1 - 0.001}{1 + 0.001} \\approx 1 \\end{aligned} $$所以 p波的反射系数 $\\frac{A_{rp}}{A_{ip}}$ 的主要部分是 $\\frac{ε(ω) k_{iz}}{ε(ω) k_{iz}}$，比值近似为1。\n倾斜入射金属 与垂直入射结论相同，s波反射系数为-1，对p波反射系数为1。\n光波倾斜入射金属表面：波矢x方向分量 $k_x$ 不为零， 则在真空中的入射波波矢z方向分量：$k_{iz} = \\sqrt{μ_0 ε_0 ω^2 - k_x^2}$； 在金属中的透射波波矢z方向分量：$k_{tz} = \\sqrt{μ_0 ε(ω) ω^2 - k_x^2}$ 。\ns 波反射系数： $$ \\frac{A_{rs}}{A_{is}} = \\frac{k_{iz} - k_{tz}}{k_{iz} + k_{tz}} = \\frac{\\sqrt{μ_0 ε_0 ω^2 - k_x^2} - \\sqrt{μ_0 ε(ω) ω^2 - k_x^2} }{ \\sqrt{μ_0 ε_0 ω^2 - k_x^2} + \\sqrt{μ_0 ε(ω) ω^2 - k_x^2} } $$同样，该分数的主要部分是$\\frac{- \\sqrt{μ_0 ε(ω) ω^2 - k_x^2} }{+ \\sqrt{μ_0 ε(ω) ω^2 - k_x^2} }$， 因为 $k_x \u003c k_0 \\ll k_1$，比较模长： $μ_0 ε(ω) ω^2 \\gg k_x^2$， 所以减一个$k_x$就相当于在虚数末尾再加一个小向量，结果就是带来一个微扰角度，所以s波倾斜入射时理想金属反射系数还是 -1。\n画图\np 波反射系数同理，$\\frac{A_{rp}}{A_{ip}}$ 的主要部分是 $\\frac{ε(ω) \\sqrt{μ_0 ε_0 ω^2 - k_x^2}}{ε(ω) \\sqrt{μ_0 ε_0 ω^2 - k_x^2}}$ ， 因为 $k_0 \u003e k_x$，所以 $k_x$也是微扰，所以p波倾斜入射时理想金属反射系数还是 1。\n波节波腹 反射波的$E_x、E_y、B_Z$ 的相位几乎与入射波差$π$，所以在界面处形成波节；而反射波的 $B_x、B_y、E_z$ 的相位几乎与入射波相同，所以在界面处形成波腹。\n对于理想金属，s 波的反射系数 $\\frac{A_{rs}}{A_{is}}=-1$ 和 p 波的反射系数 $\\frac{A_{rp}}{A_{ip}} = 1$，也就是：$Ars = - A_{is},\\ A_{rp} = A_{ip}$，将此关系代入 s 波和 p 波的入射波和反射波的表达式：\n$$ \\begin{array}{c} s 波：\\ \\begin{array}{c} 入射波： \\left{ \\begin{array}{cc} E_{iy}(\\pmb r,t) = A_{is} e^{i(k_x x + k_{iz}z - ω t)}\\ B_{ix}(\\pmb r,t) = -A_{is} \\frac{k_{iz}}{ω} e^{i(k_x x + k_{iz}z - ω t)}\\ B_{iz}(\\pmb r,t) = A_{is} \\frac{k_x}{ω} e^{i(k_x x + k_{iz}z - ω t)}\\ \\end{array} \\right. \u0026amp; 反射波： \\left{ \\begin{array}{cc} E_{ry}(\\pmb r,t) = A_{rs} e^{i(k_x x + k_{rz}z - ω t)}\\ B_{rx}(\\pmb r,t) = -A_{rs} \\frac{k_{rz}}{ω} e^{i(k_x x + k_{rz}z - ω t)}\\ B_{rz}(\\pmb r,t) = A_{rs} \\frac{k_x}{ω} e^{i(k_x x + k_{rz}z - ω t)} \\end{array} \\right. \\end{array} \\ \\ p 波：\\ \\begin{array}{c} 入射波： \\left{ \\begin{array}{c} B_{iy}(\\pmb r,t) = A_{ip} e^{i(k_x x + k_{iz} z - ω t)}\\ E_{ix}(\\pmb r,t) = A_{ip} \\frac{k_{iz}}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{iz} z - ω t)}\\ E_{iz}(\\pmb r,t) = -A_{ip} \\frac{k_{x}}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{iz} z - ω t)} \\end{array} \\right. \u0026amp; 反射波： \\left{ \\begin{array}{c} B_{ry}(\\pmb r,t) = A_{rp} e^{i(k_x x + k_{rz} z - ω t)}\\ E_{rx}(\\pmb r,t) = A_{rp} \\frac{k_{rz}}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{rz} z - ω t)}\\ E_{rz}(\\pmb r,t) = -A_{rp} \\frac{k_{x}}{μ_0 ε_1(ω) ω} e^{i(k_x x + k_{rz} z - ω t)} \\end{array} \\right. \\end{array}\n\\end{array} $$\n因为$\\frac{A_{rs}}{A_{is}} = -1$，所以 s波的 $E_{iy}$ 和 $E_{ry}$ 相差$π$ 相位；因为$k_{iz}=-k_{rz}$，所以 $B_{ix}$ 和$B_{rx}$、$E_{ix}$ 和 $E_{rx}$ 也相差$π$相位，一个波峰一个波谷就抵消了。\n对于 s 波的 $B_{ix}$ 和 $Brx$、p 波的 $B_{iy}$ 和 $B_{ry}$、$E_{iz}$ 和 $E_{rz}$ ，相位几乎相同（$k_{iz}$和$k_{rz}$是一样的），所以在界面处形成波腹。\n总结对比 理想金属的反射 和 介电介质全反射的对比：\n两者反射率均为1，在另一侧的透射波 $k_z$ 均有虚部，指数衰减；\n介电介质只有在大于全反射角入射的情况下，才会有全反射，而理想金属表面在任意角度反射率都为1；\n大多数介电介质反射波与入射波的相位差既非$π$，也非0，s 波是 $2 \\rm arctan(\\frac{β}{k_{iz}})$，p 波是$2 \\rm arctan \\left( \\frac{ε_1(ω) β}{ε_2(ω) k_{iz}} \\right)$；而理想金属的入射波与反射波的相位差为$π$ 或 0：\n分量 方向 半波损失 波节/波腹 $E_x, E_y$ 界面的切向方向 都有半波损失 波节 $E_z$ 法向方向 无相位差 波腹 $B_x,B_y$ 界面的切向方向 无相位差 波腹 $E_z$ 界面的法向方向 $π$相位差, 半波损失 波节 波导 将光空间受限，实现光定向传输的光学器件。 在真空中，或者无限大的均匀介质中，光波收到不确定原理的限定，束斑越小，发散角度越大，所以如果要想定向传播，就需要无限大的平面波，因而无法实现固定束斑尺寸的定向传输。 理想的波导是由反射率为1的反射面围成，常用的为理想金属、介电介质的全反射或者光子晶体。 4.9 金属二维波导 使用理想金属，在两个维度约束光，使其定向传播。\n4.9.1 金属二维波导 考虑光在 z 方向受限的波导，即在 0\u0026lt;z\u0026lt;a 区间为真空，其他区间为理想金属，a 是二维波导的厚度。 上下两个界面的反射率近似为 1，光可以在这里来回反射。 并不是所有的模式都可以在里面稳定传输，只有波导的解为稳定解的电磁波，才能在里面稳定传输。\n由于在 x-y 面内是各向同性，为了不矢一般性，规定传播方向为 x 方向，则沿着 y 方向的波矢为零。\n光在波导中传播，波导中的任一点都是两束波（入射波和反射波）的叠加，两束波的相位满足一定关系：对于 s 波的反射系数为 -1（相位差π)，对于 p 波反射系数为 1（相位相同）。因此分成两种情况来讨论：TE波（s波）和TM波（p波）。\n画图2\n因为金属的反射系数为 -1 和 1，较为简单，而介电介质的全反射情况的反射系数是一个带幅角的复数，较为复杂，所以先从金属二维波导讲起。\nTE波（s波）模式：只有电场是横波\n电磁场有 $E_y、B_x、B_z$ 三个分量。考虑两束波的 z 方向波矢分别为 $k_z$ 和 $-k_z$，x 方向波矢为 $k_x$，则波导中的电磁波可写成两束波叠加的形式：\n$$ \\left\\{ \\begin{aligned} E_y (\\pmb r) \u0026 = A_+ e^{i (k_x x + k_z z)} + A_- e^{i (k_x x - k_z z)} \\\\ B_x (\\pmb r) \u0026 = -A_+ \\frac{k_z}{ω} e^{i (k_x x + k_z z)} + A_- \\frac{k_z}{ω} e^{i (k_x x - k_z z)} \\\\ B_z (\\pmb r) \u0026 = A_+ \\frac{k_x}{ω} e^{i (k_x x + k_z z)} + A_- \\frac{k_x}{ω} e^{i (k_x x - k_z z)} \\\\ \\end{aligned} \\right. $$（$A_+$是正方向波的振幅，$A_-$是负方向波的振幅）\n根据理想金属界面的反射相位关系：$\\frac{A_r}{A_i}= -1$，可得：\n$$ \\left\\{ \\begin{aligned} \\frac{A_+}{A_-} \u0026= -1 \u0026 (z=0)\\\\ \\frac{A_- e^{- i k_z a}}{A_+ e^{i k_z a}} \u0026= -1 \u0026 (z=a) \\end{aligned} \\right. $$可解得反射波与入射波在 z=a 处的相位差为 nπ：（在 z=a 处振幅抵消为0）\n$$ \\begin{aligned} e^{-2 i k_z a} = 1 \u0026= e^{2 i k_z a} = e^{i 2n π} \\\\ k_z a \u0026= n π \\end{aligned} $$波矢的z方向分量，也就是 z 方向的空间频率为：$k_z = \\frac{n π}{a}$（n为正整数），说明z方向是驻波； 从而 z 方向的波长为：$\\lambda_z = \\frac{2 π}{k_z} = \\frac{2 a}{n}$，说明二维波导厚度的两倍必须是光 z 方向波长的整数倍。\n图4.9.2 驻波 把 $k_z$ 表达式代入波导中电磁波的解：（两互共轭相减只剩 sin，两互共轭相加只剩 cos）\n$$ \\left { \\begin{aligned} E_y (\\pmb r, t) = E_0 sin \\left( \\frac{n π}{a} z \\right) e^{i (k_x x - ω t)} \\ B_x (\\pmb r, t) = i E_0 \\frac{k_z}{ω} cos \\left( \\frac{n π}{a} z \\right) e^{i (k_x x - ω t)} \\ B_z (\\pmb r, t) = E_0 \\frac{k_x}{ω} sin \\left( \\frac{n π}{a} z \\right) e^{i (k_x x - ω t)}\n\\end{aligned} \\right. $$\n其中：\n$$ k_x^2 = k_0^2 - \\left( \\frac{n π}{a} \\right)^2 = μ_0 ε_0 ω^2 - \\left( \\frac{n π}{a} \\right)^2 $$n 为正整数（1,2,3,\u0026hellip;），而且需要 $k_x^2 \u003e 0$ 才能在 x 方向振荡传输（否则\u0026lt;0 是指数衰减，传着传着就没了），所以波导和波长(频率)有约束条件：\n$$ \\begin{aligned} k_x^2 = k_0^2 - \\left( \\frac{n π}{a} \\right)^2 \u003e 0 \\\\ \\frac{n π}{a} \u003c k_0 \\end{aligned} $$所以对于某一频率的波，n 最大取：\n$$ n \u003c \\frac{k_0 a}{π} = \\frac{2 a}{\\lambda} $$可以看出，如果 n 要有正整数的解，就要求 $\\lambda \\le 2 a$，所以 $2a$ 被称为截止波长 $λ_c$（如果波长λ大于2a，就会衰减），对应截止角频率：$ω_c = \\frac{2π}{T} = \\frac{2π}{\\frac{λ_c}{c}} = 2π \\frac{c}{λ_c}$。\n如果 n 是小数，就不满足：$e^{-2 i k_z a} = 1$，也就是说光在波导之间走一个来回的相位差不为2π，每走一个来回会多一个相位差，随着时间传播，各点都有2π范围内所有方向的光，结果就是各个波互相抵消了（衰减为零了）。\n图4.9.2 抵消了 所以必须满足在 z=a 处，两波相位差等于$n π$，等再回到 z=0 处就是相位差2nπ（0），如果0相位时值为0，那么在整个传输过程中，只有在 z=0 和 z=a 处值为0，而不会在各个点处值都为零。\n所以二维波导厚度的2倍至少大于一个波长。凡是真空波长大于 $\\lambda_c$ 的电磁波（或者频率小于 $ω_c$ 的电磁波，均不能在此波导中传输。\nTM波（p波）模式：只有磁场是横波\n电磁场有 $B_y、E_x、E_z$ 三个分量。考虑两束波波矢的 z 方向分量分别为：$k_z$ 和 $-k_z$，波矢x方向分量为：$k_x$，则电磁波的解为以下形式：\n$$ \\left\\{ \\begin{aligned} B_y(\\pmb r) \u0026= A_+ e^{i (k_x x + k_z z)} + A_- e^{i (k_x x - k_z z)} \\\\ E_x(\\pmb r) \u0026= A_+ \\frac{k_z}{ω μ_0 ε(ω)} e^{i (k_x x + k_z z)} - A_- \\frac{k_z}{ω μ_0 ε(ω)} e^{i (k_x x - k_z z)} \\\\ E_z(\\pmb r) \u0026= -A_+ \\frac{k_x}{ω μ_0 ε(ω)} e^{i (k_x x + k_z z)} - A_- \\frac{k_x}{ω μ_0 ε(ω)} e^{i (k_x x - k_z z)} \\end{aligned} \\right. $$p 波在理想金属界面的反射系数为1，在 z=0 和 z=a 处满足：\n$$ \\left \\{ \\begin{aligned} \u0026 \\frac{A_+}{A_-} = 1 \u0026 \\text{(z=0)}\\\\ \u0026 \\frac{A_- e^{- i k_z a}}{A_+ e^{i k_z a}} = 1 \u0026 \\text{(z=a)} \\end{aligned} \\right. $$和TE波一样，根据相位差得到光程需满足：$k_z a = n π$。\n则波导中的电磁波为：\n$$ \\left\\{ \\begin{aligned} \u0026 B_y(\\pmb r, t) = B_0 cos \\left( \\frac{n π}{a} z \\right) e^{i(k_x x - ω t)} \\\\ \u0026 E_x(\\pmb r, t) = i B_0 \\frac{k_z}{ω μ_0 ε(ω)} sin \\left( \\frac{n π}{a} z \\right) e^{i(k_x x - ω t)} \\\\ \u0026 E_z(\\pmb r, t) = B_0 \\frac{k_x}{ω μ_0 ε(ω)} cos \\left( \\frac{n π}{a} z \\right) e^{i(k_x x - ω t)} \\end{aligned} \\right. $$其中：$k_x^2 = k_0^2 - \\left( \\frac{n π}{a} \\right)^2 = μ_0 ε_0 ω^2 - \\left( \\frac{n π}{a} \\right)^2$\n因n是正整数，且 $k_x^2 \u003e 0$，即：\n$$ \\frac{n π}{a} \u003c k_0 $$所以对于某一频率的电磁波，n最大取：\n$$ n \u003c \\left[ \\frac{a}{π} k_0 \\right] = \\left[ \\frac{2a}{λ} \\right] $$因n是正整数，所以要求 $λ\u003c2a$ 。则 $2a$ 被称为截止波长 $λ_c$，对应的截止频率为：$ω_c = 2 π \\frac{c}{λ}$。\n凡是真空波长大于截止波长 $λ_c$ 的电磁波（或者频率小于$ω_c$）的电磁波均不能在此波导中传输。（此结论与TE波相似）\n介质二维波导 利用介质全反射实现光在介质中定向传输。\n三层结构：两侧是真空，中间是一层介质，其相对介电常数是大于1的实数，记其介电常数为ε。\n对于固定频率为 ω，x 方向波矢为 $k_x$ 的波，在三种介质中 ω 和 $k_x$ 守恒， 其中：$k_0 \u003c k_x \u003c k$，以保证在介质中 $k_z$ 是实数，而真空中 $k_{z0}$ 是纯虚数。\n（$k$ 和 $k_0$ 分别为介质和真空中的波矢，$k_z$ 和 $k_{z0}$ 分别为介质和真空中 z 方向的波矢）\n$$ \\begin{aligned} k_0^2 \u0026= μ_0 ε_0 ω^2 \\\\ k^2 \u0026= μ_0 ε ω^2 \\\\ k_{z0} \u0026= ± i \\sqrt{k_x^2 - μ_0 ε_0 ω^2} = ± i β \\\\ k_z \u0026= ± \\sqrt{μ_0 ε ω^2 - k_x^2} \\end{aligned} $$$k_{z0}$ 在上层介质中取正，在下层真空中取负，在介质中$k_z$正负均取，是两束波的叠加，下层界面为z=0 平面，上层界面为z=a 平面，a 为介质厚度。\n图4.9.3 对于TE模式（s波）\n介质中电磁场方程为：\n$$ \\left\\{ \\begin{aligned} E_y(\\pmb r) \u0026= A_+ e^{i (k_x x + k_z z)} + A_- e^{i(k_x x - k_z)} \\\\ B_x(\\pmb r) \u0026= -A_+ \\frac{k_z}{ω} e^{i (k_x x + k_z z)} + A_- \\frac{k_z}{ω} e^{i (k_x x - k_z z)} \\\\ B_z(\\pmb r) \u0026= A_+ \\frac{k_x}{ω} e^{i (k_x x + k_z z)} + A_- \\frac{k_x}{ω} e^{i (k_x x - k_z z)} \\end{aligned} \\right. $$在 z=a 和 z=0 处，使用由菲涅尔反射定律推出的s波全反射公式（$\\frac{A_i}{A_r} = \\frac{k_{iz} - k_{tz}}{k_{iz} + k_{tz}}$）可得：\n$$ \\left\\{ \\begin{aligned} \\frac{A_+}{A_-} \u0026= \\frac{-k_z + iβ}{- k_z - iβ} \u0026\\text{(z=0)} \\\\ \\frac{A_- e^{-i k_z a}}{A_+ e^{i k_z a}} \u0026= \\frac{k_z - iβ}{k_z + iβ} \u0026\\text{(z=a)} \\end{aligned} \\right. $$由此可得：\n$$ \\begin{aligned} \\left( \\frac{k_z - iβ}{k_z + iβ} \\right)^2 \u0026= e^{-i 2 k_z a} \\\\ \\left( \\frac{ \\sqrt{k_z^2 + β^2} e^{i\\ arctan(\\frac{β}{k_z})}} {\\sqrt{k_z^2 + β^2} e^{- i\\ arctan(\\frac{β}{k_z})}} \\right)^2 \u0026= e^{-i 2 k_z a} \\\\ e^{2 i\\ \\mathrm{arctan}(\\frac{β}{k_z}) ×2} \u0026= e^{-i 2 k_z a} \\\\ 2 \\mathrm{arctan}(\\frac{β}{k_z}) ×2 + 2 nπ \u0026= -2 k_z a \\\\ \\end{aligned} $$解得：\n$$ k_z a = 2 \\mathrm{arctan}(\\frac{β}{k_z}) + nπ \\tag{4.9.1} $$其中要求：\n$$ k_z \u003c \\sqrt{k^2 - k_0^2} = \\sqrt{\\mu_0 (\\varepsilon - \\varepsilon_0)} ω $$4.9.1是一个超越方程，一般只有数值解。\nn 为整数，是波导的阶数，记作$TE_n$。\n注意到：$-2 \\mathrm{arctan}(\\frac{β}{k_z})$ 是反射波与入射波的相位差，所以公式4.9.1改写为：\n$$ 2 k_z a - 4 \\mathrm{arctan}(\\frac{β}{k_z}) = 2 nπ $$方程左边表示光在两个镜面行走一圈的相位变化（光程相位变化和反射相位变化之和），应当等于2π的整数倍。\n对于TM模式（p波）\n三层中的电磁场方程为：\n$$ \\left\\{ \\right. $$根据p波的全反射公式（$\\frac{}{} = \\frac{}{}$），可得z=0 和 z=a 处的振幅之比：\n$$ \\left\\{ \\right. $$由此可得：\n$$ \\left( \\frac{ε_0 k_z - iεβ}{ε_0 k_z + iεβ} \\right)^2 = e^{-i 2 k_z a} $$化简得到幅角的超越方程：\n$$ k_z a = 2 \\mathrm{arctan} \\left( \\frac{εβ}{ε_0 k_z} \\right) + nπ \\tag{4.9.2} $$其中还要求：$k_z \u003c \\sqrt{k^2 - k_0^2} = \\sqrt{μ_0 (ε-ε_0)}$ β\n这也是个超越方程，一般只有数值解。n 为波导的阶数，记作$TM_n$。\n因为$-2 \\mathrm{arctan} \\left( \\frac{εβ}{ε_0 k_z} \\right)$正好是p波反射波与入射波的相位差，所以4.9.2改写为：\n$$ 2 k_z a - 4 \\mathrm{arctan} \\left( \\frac{εβ}{ε_0 k_z} \\right)= 2nπ $$左边则表示光在两个镜面间行走一圈的相位变化（光程相位变化和反射相位变化之和），其应当等于2π整数倍。\n因$\\frac{ε}{ε_0}\u003e1$，所以对于同一级的n，TM波的$k_z$的解要大于TE波的$k_z$的解，它俩的$k_z$都大于金属波导的$k_z$解。 所以当介质波导厚度a 逐渐增加的时候，对于某一频率的电磁波，出现的第一个模式是TE模式，然后是TM模式。 而金属波导第一个模式是TE和TM模式同时出现。\n4.11 多层膜电磁波传输 如果已知每层膜的厚度、介电常数、磁导率，根据 $ω$ 和 $k_x$ 守恒，就可利用界面两侧电磁场的连续性建立每一层薄膜之间的电磁波振幅之间的关系。\n图4.11.1 对于线性介质，因为由麦克斯韦方程组得到的界面连续条件要求在任一时刻都满足，所以$ω$任意时刻都相同，否则如果有一时刻不连续，后面就都不连续了。\n对于平整的界面有平移不变性，所以 $k_x$ 在任一位置都相同；否则如果在某一位置不连续，后面位置也不会连续了。\n所以只要在同一层面上，各点的电磁波就是相同的；而在z方向上传输会产生相位差。 介质中的各点处都是两束波（入射波和反射波）的叠加。\n由此，对于第 m 层薄膜内的两束波可以用各自 z 方向波矢来代表：$k_{zm}$ 和 $- k_{zm}$ （在同一介质内，$ε(ω)$相同，$k^2$相同，$k_x$相同，所以两束波的$k_z$大小相同,方向相反），其中：\n$$ k_{zm} = \\sqrt{μ_m ε_m(ω) ω^2 - k_x^2} $$根据界面连续条件，可以建立界面两侧电磁波的联系，从而确定出电磁波在各层介质中的传播，最后关注到光从多层薄膜器件穿出的透射波得到情况。\n传输矩阵 $$ \\begin{aligned} \u0026 界面1下表面电场磁场(E_{y1}、H_{x1}) \\stackrel{分解}{\\longrightarrow} 两束波的振幅(E_{1+}、E_{1-}) \\stackrel{界面连续条件}{\\longrightarrow} \\\\ \u0026 界面1上表面两束波振幅(E_{2+}、E_{2-}) \\stackrel{介质2中传输}{\\longrightarrow} 振幅相位变化(e^{i k_{z2} d_2}) \\stackrel{合成}{\\longrightarrow} 界面2电场磁场(E_{y2}、H_{x2}) \\end{aligned} $$（想用交换图表表现，但是markdown-preview插件好像不支持？）\n描述了入射波（反射波）到透射波的中间转化过程。\n把一个层面上的电场和磁场与相邻的另一个层面上的电场和磁场联系起来，如此可以将其外推到整个光子空间。如果知道了最初层面上入射场的分布，就可以利用传输矩阵法计算出最后层面上的透射场的分布，从而计算出光子晶体的透射系数和反射系数。（《用传输矩阵法(TMM)研究光子晶体的传输特性》梅洛勤）\n对于 TE模式（s波）：\n第1层介质中 z 方向波矢为 $k_{z0}$ 的 s 波可写为：为什么不用 B 了？（$\\pmb H = \\frac{\\pmb B}{μ_0} - \\pmb M$）\n$$ \\left\\{ \\begin{aligned} E_y(\\pmb r, t) \u0026= E_{1+} e^{i (k_x x + k_{z1} z - ω t)} \\\\ H_x(\\pmb r, t) \u0026= - E_{1+} \\frac{k_{z1}}{μ_1 ω} e^{i (k_x x + k_{z1} z - ω t)} \\\\ H_z(\\pmb r, t) \u0026= E_{1+} \\frac{k_x}{μ_1 ω} e^{i (k_x x + k_{z1} z - ω t)} \\end{aligned} \\right. $$（$E_{1+}$ 是“入射波”的振幅，$E_{1-}$是“反射波”的振幅） 第1层介质中 z 方向波矢为 $-k_{z0}$ 的波（反射波）可写为：（换成$-k_{z1}$） $$ \\left\\{ \\begin{aligned} E_y(\\pmb r, t) \u0026= E_{1-} e^{i (k_x x -k_{z1}z - ω t)} \\\\ H_x(\\pmb r, t) \u0026= E_{1-} \\frac{k_{z1}}{μ_1 ω} e^{i (k_x x -k_{z1}z - ω t)}\\\\ H_z(\\pmb r, t) \u0026= E_{1-} \\frac{k_x}{μ_1 ω} e^{i (k_x x -k_{z1}z - ω t)} \\end{aligned} \\right. $$因为“入射波”和“反射波”位于同一介质，所以不考虑相位$e^{i(k_x x + k_{z1} z - ω t)}$。 由此可将界面 1 **下表面**电场、磁场的振幅分解为两束波的叠加： $$ \\left\\{ \\begin{aligned} E_{y1} \u0026= E_{1+} + E_{1-} \u0026 \\text{(电场切向分量相加)} \\\\ H_{x1} \u0026= - \\frac{k_{z1}}{μ_1 ω} E_{1+} + \\frac{k_{z1}}{μ_1 ω} E_{1-} \u0026 \\text{(磁场强度切向分量相加)} \\\\ \\end{aligned} \\right. $$对于$H_{z1}$ （因为是$\\pmb B$的法向分量连续，需要把$H_z$变换为$B_z$，不过由4.6节可知：边界连续性条件的第三个方程与第一个方程相同，所以上面只取前两个方程：$E_y$和$H_x$） 把第1层介质中 $E_{y1},\\ H_{x1}$ 的振幅写成矩阵方程的形式： $$ \\begin{aligned} \\begin{pmatrix} E_{y1} \\\\ H_{x1} \\end{pmatrix} = \\begin{pmatrix} 1 \u0026 1 \\\\ -\\frac{k_{z1}}{μ_1 ω} \u0026 \\frac{k_{z1}}{μ_1 ω} \\end{pmatrix} \\begin{pmatrix} E_{1+} \\\\ E_{1-} \\end{pmatrix} \\end{aligned} $$根据界面连续条件: $\\left\\{ \\begin{aligned} \u0026amp; \\pmb E的切向分量连续，\\\\ \u0026amp; \\pmb D的法向分量在界面间没有表面电荷时连续，\\\\ \u0026amp; \\pmb B的法向分量连续，\\\\ \u0026amp; \\pmb H的切向分量在没有表面电流时连续 \\end{aligned} \\right.$ 因为$E_{y1}$和$H_{x1}$切向连续，所以 $E_{y1},\\ H_{x1}$ 既是界面**下表面**电磁场的振幅，也是界面**上表面**电磁场的振幅，不过它是由第2层介质中的两束波叠加而成： $$ \\begin{aligned} \\begin{pmatrix} E_{y1} \\\\ H_{x1} \\end{pmatrix} = \\begin{pmatrix} 1 \u0026 1 \\\\ -\\frac{k_{z2}}{μ_2 ω} \u0026 \\frac{k_{z2}}{μ_2 ω} \\end{pmatrix} \\begin{pmatrix} E_{2+} \\\\ E_{2-} \\end{pmatrix} \\end{aligned} $$反解后可得界面1上表面两束波的振幅表达式： $$ \\begin{aligned} \\begin{pmatrix} E_{2+} \\\\ H_{2-} \\end{pmatrix} = \\begin{pmatrix} 1 \u0026 1 \\\\ -\\frac{k_{z2}}{μ_2 ω} \u0026 \\frac{k_{z2}}{μ_2 ω} \\end{pmatrix}^{-1} \\begin{pmatrix} E_{y1} \\\\ H_{x1} \\end{pmatrix} \\end{aligned} $$这两束波沿 z 方向在介质2中传输，从界面1传播到界面2，相位发生了变化，所以界面2处各点的复振幅为： $$ \\begin{pmatrix} e^{i k_{z2} d_2} \u0026 0 \\\\ 0 \u0026 e^{-i k_{z2} d_2} \\\\ \\end{pmatrix} \\begin{pmatrix} E_{2+} \\\\ E_{2-} \\end{pmatrix} $$然后可合成得到界面2处的电场和磁场强度： $$ \\begin{aligned} \\begin{pmatrix} E_{y2} \\ H_{x2} \\end{pmatrix}\n\u0026amp; = \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{-k_{z2}}{μ_2 ω} \u0026amp; \\frac{k_{z2}}{μ_2 ω} \\end{pmatrix} \\begin{pmatrix} e^{i k_{z2} d_2} \u0026amp; 0 \\\\ 0 \u0026amp; e^{-i k_{z2} d_2} \\end{pmatrix} \\begin{pmatrix} E_{2+} \\\\ E_{2-} \\end{pmatrix} \\\\ \u0026amp; = \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{-k_{z2}}{μ_2 ω} \u0026amp; \\frac{k_{z2}}{μ_2 ω} \\end{pmatrix} \\begin{pmatrix} e^{i k_{z2} d_2} \u0026amp; 0 \\\\ 0 \u0026amp; e^{-i k_{z2} d_2} \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{-k_{z2}}{μ_1 ω} \u0026amp; \\frac{k_{z2}}{μ_1 ω} \\end{pmatrix}^{-1} \\begin{pmatrix} E_{y1} \\\\ H_{x1} \\end{pmatrix} \\end{aligned} $$\n依此类推，可以得到**相邻**两个界面之间 $E_y$ 和 $H_x$ 的关系： $$ \\begin{aligned} \\begin{pmatrix} E_{yj} \\ H_{xj} \\end{pmatrix}\n\u0026amp; = \\underbrace{ \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{-k_{zj}}{μ_j ω} \u0026amp; \\frac{k_{zj}}{μ_j ω} \\end{pmatrix} }_{界面j处合成} \\underbrace{ \\begin{pmatrix} e^{i k_{zj} d_j} \u0026amp; 0 \\\\ 0 \u0026amp; e^{-i k_{zj} d_j} \\end{pmatrix} }_{介质j中传输} \\underbrace{ \\begin{pmatrix} E_{j+} \\\\ E_{j-} \\end{pmatrix} }_{界面j-1上表面} \\\\ \u0026amp; = \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{-k_{zj}}{μ_j ω} \u0026amp; \\frac{k_{zj}}{μ_j ω} \\end{pmatrix} \\begin{pmatrix} e^{i k_{zj} d_j} \u0026amp; 0 \\\\ 0 \u0026amp; e^{-i k_{zj} d_j} \\end{pmatrix} \\underbrace{ \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{-k_{zj}}{μ_j ω} \u0026amp; \\frac{k_{zj}}{μ_j ω} \\end{pmatrix}^{-1} \\begin{pmatrix} E_{yj-1} \\\\ H_{xj-1} \\end{pmatrix} }_{界面j-1下表面分解} \\end{aligned} $$\n其中： $$ \\begin{aligned} T_j \u0026amp;=\n\\underbrace{ \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{k_{zj}}{μ_j ω} \u0026amp; \\frac{k_{zj}}{μ_j ω} \\end{pmatrix} }_{合成} \\underbrace{ \\begin{pmatrix} e^{i k_{zj} d_j} \u0026amp; 0 \\\\ 0 \u0026amp; e^{-i k_{zj} d_j} \\end{pmatrix} }_{传输} \\underbrace{ \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{k_{zj}}{μ_j ω} \u0026amp; \\frac{k_{zj}}{μ_j ω} \\end{pmatrix}^{-1} }_{分解} \\\\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \\left( e^{i k_{zj} d_j} + e^{-i k_{zj} d_j} \\right) \u0026amp; - \\frac{μ_j ω}{2 k_{zj}} \\left( e^{i k_{zj} d_j} - e^{-i k_{zj} d_j} \\right) \\\\ - \\frac{k_{zj}}{2 μ_j ω} \\left( e^{i k_{zj} d_j} - e^{-i k_{zj} d_j} \\right) \u0026amp; \\frac{1}{2} \\left( e^{i k_{zj} d_j} + e^{-i k_{zj} d_j} \\right) \\end{pmatrix} \\end{aligned} \\tag{4.11.1} $$\n为相邻两层之间的传输矩阵，从而可得多层薄膜整体的入射波（$E_{1+}$），反射波（$E_{1-}$）和透射波（$E_{n+}$）（振幅）之间的关系： $$ \\begin{aligned} \\begin{pmatrix} E_{n+} \\ E_{n-} \\end{pmatrix} \u0026amp; = \\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_{zn}}{μ_n ω} \u0026amp; \\frac{k_{zn}}{μ_n ω} \\end{pmatrix}^{-1}\n\\begin{pmatrix} \\prod_{j=2}^{n-1} T_j \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{k_{z1}}{μ_1 ω} \u0026amp; \\frac{k_{z1}}{μ_1 ω} \\end{pmatrix} \\begin{pmatrix} E_{1+} \\\\ E_{1-} \\end{pmatrix}\\\\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_n ω}{2 k_{zn}} \\\\ \\frac{1}{2} \u0026amp; \\frac{μ_n ω}{2 k_{zn}} \\end{pmatrix} \\begin{pmatrix} \\prod_{j=2}^{n-1} T_j \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\\\ -\\frac{k_{z1}}{μ_1 ω} \u0026amp; \\frac{k_{z1}}{μ_1 ω} \\end{pmatrix} \\begin{pmatrix} E_{1+} \\\\ E_{1-} \\end{pmatrix}\\\\ \\end{aligned} \\tag{4.11.2} $$\n对于TM模式（p波）\n与 TE 波类似：\n例如第 1 层介质中，z 方向波矢为 $k_{z0}$ 的波可写为：\n$$ \\left \\{ \\begin{aligned} H_y(\\pmb r, t) \u0026= H_{1+} e^{i (k_x x + k_{z1} z - ω t)} \\\\ E_x(\\pmb r, t) \u0026= H_{1+} \\frac{k_{z1}}{ε_1 ω} e^{i (k_x x + k_{z1}z - ω t)} \\\\ E_z(\\pmb r, t) \u0026= - H_{1+} \\frac{k_x}{ε_1 ω} e^{i (k_x x + k_{z1}z - ω t)} \\end{aligned} \\right. $$z 方向波矢为 $-k_{z0}$ 的波可写为：\n$$ \\left \\{ \\begin{aligned} H_y(\\pmb r, t) \u0026= H_{1-} e^{i (k_x x - k_{z1} z - ω t)} \\\\ E_x(\\pmb r, t) \u0026= - H_{1-} \\frac{k_{z1}}{ε_1 ω} e^{i (k_x x - k_{z1}z - ω t)} \\\\ E_z(\\pmb r, t) \u0026= - H_{1-} \\frac{k_x}{ε_1 ω} e^{i (k_x x - k_{z1}z - ω t)} \\end{aligned} \\right. $$则在界面1下表面的电磁场的振幅为（由 4.6 节可知，边界连续性条件的第三个方程跟第一个相同，所以我们只取前两个，$H_y$ 和 $E_x$）：\n$$ \\left\\{ \\begin{aligned} H_{y1} \u0026= H_{1+} + H_{1-} \\\\ E_{x1} \u0026= H_{1+} \\frac{k_{z1}}{ε_1 ω} - H_{1-} \\frac{k_{z1}}{ε_1 ω} \\end{aligned} \\right. $$写成矩阵方程的形式即为：\n$$ \\begin{pmatrix} H_{y1} \\ E_{x1} \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{z1}}{ε_1 ω} \u0026amp; -\\frac{k_{z1}}{ε_1 ω} \\end{pmatrix}\n\\begin{pmatrix} H_{1+} \\ H_{1-} \\end{pmatrix} $$\n由界面连续性条件可知，界面1上表面电场、磁场的振幅也为 $H_{y1}$ 和 $E_{x1}$，不过它是由第2层介质中的两束波的振幅$H_{2+},\\ H_{2-}$叠加而成：\n$$ \\begin{pmatrix} H_{y1} \\ E_{x1} \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{z2}}{ε_2 ω} \u0026amp; -\\frac{k_{z2}}{ε_2 ω} \\end{pmatrix}\n\\begin{pmatrix} H_{2+} \\ H_{2-} \\end{pmatrix} $$\n反解后可得：界面1上表面两束波振幅$H_{2+},\\ H_{2-}$\n$$ \\begin{pmatrix} H_{2+} \\ H_{2-} \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{z2}}{ε_2 ω} \u0026amp; -\\frac{k_{z2}}{ε_2 ω} \\end{pmatrix}^{-1}\n\\begin{pmatrix} H_{y1} \\ E_{x1} \\end{pmatrix} $$\n当这束波传输到界面2时，两束波的振幅变为：\n$$ \\begin{pmatrix} e^{i k_{z2} d_2} \u0026amp; 0 \\ 0 \u0026amp; e^{i k_{z2} d_2} \\end{pmatrix}\n\\begin{pmatrix} H_{2+} \\ H_{2-} \\end{pmatrix} $$\n然后合成界面2 处的磁场振幅$H_{y2}$、电场振幅$E_{x2}$：\n$$ \\begin{aligned} \\begin{pmatrix} H_{y2} \\ E_{x2} \\end{pmatrix} \u0026amp;= \\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{z2}}{ε_2 ω} \u0026amp; - \\frac{k_{z2}}{ε_2 ω} \\end{pmatrix}\n\\begin{pmatrix} e^{i k_{z2} d_2} \u0026amp; 0 \\ 0 \u0026amp; e^{- i k_{z2} d_2} \\end{pmatrix}\n\\begin{pmatrix} H_{2+} \\ H_{2-} \\end{pmatrix}\n\\ \u0026amp;= \\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{z2}}{ε_2 ω} \u0026amp; - \\frac{k_{z2}}{ε_2 ω} \\end{pmatrix}\n\\begin{pmatrix} e^{i k_{z2} d_2} \u0026amp; 0 \\ 0 \u0026amp; e^{- i k_{z2} d_2} \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{z2}}{ε_2 ω} \u0026amp; - \\frac{k_{z2}}{ε_2 ω} \\end{pmatrix}^{-1}\n\\begin{pmatrix} H_{y1} \\ E_{x1} \\end{pmatrix}\n\\end{aligned} $$\n依此类推，可以得到相邻两个界面之间的 $H_y$ 和 $E_x$ 的关系：\n$$ \\begin{pmatrix} H_{yj} \\ E_{xj} \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{zj}}{ε_j ω} \u0026amp; - \\frac{k_{zj}}{ε_j ω} \\end{pmatrix}\n\\begin{pmatrix} e^{i k_{zj} d_j} \u0026amp; 0 \\ 0 \u0026amp; e^{- i k_{zj} d_j} \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{zj}}{ε_j ω} \u0026amp; - \\frac{k_{zj}}{ε_j ω} \\end{pmatrix}^{-1}\n\\begin{pmatrix} H_{yj-1} \\ E_{xj-1} \\end{pmatrix} $$\n其中\n$$ \\begin{aligned} T_j\n\u0026amp;=\n\\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{zj}}{ε_j ω} \u0026amp; - \\frac{k_{zj}}{ε_j ω} \\end{pmatrix}\n\\begin{pmatrix} e^{i k_{zj} d_j} \u0026amp; 0 \\ 0 \u0026amp; e^{- i k_{zj} d_j} \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ \\frac{k_{zj}}{ε_j ω} \u0026amp; - \\frac{k_{zj}}{ε_j ω} \\end{pmatrix}^{-1} \\ \\ \u0026amp;=\n\\begin{pmatrix} \\frac{1}{2} (e^{i k_{zj} d_j} + e^{-i k_{zj} d_j}) \u0026amp; -\\frac{ε_j ω}{2 k_{zj}} (e^{i k_{zj} d_j} - e^{-i k_{zj} d_j}) \\ -\\frac{k_{zj}}{2 ε_j ω} (e^{i k_{zj} d_j} - e^{-i k_{zj} d_j}) \u0026amp; \\frac{1}{2} (e^{i k_{zj} d_j} + e^{-i k_{zj} d_j})\n\\end{pmatrix} \\end{aligned} \\tag{4.11.3} $$\n为两层之间的传输矩阵，从而可得多层薄膜整体入射波$H_{1+}$、反射波$H_{1-}$和透射波$H_{n+}$之间的关系：\n$$ \\begin{aligned} \\begin{pmatrix} H_{n+} \\ H_{n-} \\end{pmatrix} \u0026amp; = \\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_{zn}}{ε_n ω} \u0026amp; \\frac{k_{zn}}{ε_n ω} \\end{pmatrix}\n\\begin{pmatrix} \\prod_{j=2}^{n-1} T_j \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_{z1}}{ε_1 ω} \u0026amp; \\frac{k_{z1}}{ε_1 ω} \\end{pmatrix}\n\\begin{pmatrix} H_{1+} \\ H_{1-} \\end{pmatrix}\n\\ \u0026amp; =\n\\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{ε_n ω}{2 k_{zn}} \\ \\frac{1}{2} \u0026amp; \\frac{ε_n ω}{2 k_{zn}} \\ \\end{pmatrix}\n\\begin{pmatrix} \\prod_{j=2}^{n-1} T_j \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_{z1}}{ε_1 ω} \u0026amp; \\frac{k_{z1}}{ε_1 ω} \\end{pmatrix}\n\\begin{pmatrix} H_{1+} \\ H_{1-} \\end{pmatrix}\n\\end{aligned} \\tag{4.11.4} $$\n得到了在特定$ω$ 和 $k_x$ 条件下 $H_{1+}，H_{1-}，H_{n+}$和$H_{n-}$ 之间的关系。\nTE模式 和 TM模式的传输矩阵都有四个未知量，两个方程，显然自由度过高。不过现实中并不需要这么多量。\n例如，如果要研究 TE 波的透射和反射，那么 $E_{n-} = 0$，则只剩先三个变量：$E_{1+},\\ E_{1-},\\ E_{n+}$，分别对应着入射波、反射波和透射波。\n如果要研究波导，最下和最上侧都要求是指数衰减的隐逝波，则$E_{n+}=0,\\ E_{1-}=0$，这样就剩下两个变量：$E_{1+},\\ E_{n-}$。如果要有解，就需要对矩阵中的系数有所限定，也就是在波导中求得的波导色散关系。\n下面举几个例子加以说明。\n单个界面的反射和透射\n对于单个界面，则无中间的传输矩阵 $T_j$。\n对于 TE波，由公式4.11.2可得：\n$$ \\begin{aligned} \\begin{pmatrix} E_{2+} \\ E_{2-} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \u0026amp; - \\frac{μ_2 ω}{2 k_{z2}} \\ \\frac{1}{2} \u0026amp; \\frac{μ_2 ω}{2 k_{z2}} \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_{z1}}{μ_1 ω} \u0026amp; \\frac{k_{z1}}{μ_1 ω} \\end{pmatrix}\n\\begin{pmatrix} E_{1+} \\ E_{1-} \\end{pmatrix} \\end{aligned} $$\n对于电介质，磁导率$μ_2 = μ_1 = μ_0$。对于反射和透射，$E_{2-}=0$，则：\n$$ \\begin{pmatrix} E_{2+} \\ 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} + \\frac{k_{z1}}{2 k_{z2}} \u0026amp; \\frac{1}{2} - \\frac{k_{z1}}{2 k_{z2}} \\ \\frac{1}{2} - \\frac{k_{z1}}{2 k_{z2}} \u0026amp; \\frac{1}{2} + \\frac{k_{z1}}{2 k_{z2}} \\ \\end{pmatrix}\n\\begin{pmatrix} E_{1+} \\ E_{1-} \\end{pmatrix} $$\n三个未知量，两个方程，所以要留一个自由度，也就是能解得反射波、透射波是入射波的多少倍：\n$$ \\Large \\begin{cases} \\frac{E_{1-}}{E_{1+}} = \\frac{k_{z1} - k_{z2}}{k_{z1} + k_{z2}} \\\\ \\frac{E_{2+}}{E_{1+}} = \\frac{2 k_{z1}}{k_{z1} + k_{z2}} \\\\ \\end{cases} $$这就是4.6节得到的s波的反射系数。\n对于 TM 波，则为：\n$$ \\begin{aligned} \\begin{pmatrix} H_{2+} \\ 0 \\end{pmatrix} \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{ε_2 ω}{2 k_{z2}} \\ \\frac{1}{2} \u0026amp; \\frac{ε_2 ω}{2 k_{z2}} \\ \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_{z1}}{ε_1 ω} \u0026amp; \\frac{k_{z1}}{ε_1 ω} \\end{pmatrix}\n\\begin{pmatrix} H_{1+} \\ H_{1-} \\end{pmatrix}\n\\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} + \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \u0026amp; \\frac{1}{2} - \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \\ \\frac{1}{2} - \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \u0026amp; \\frac{1}{2} + \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \\end{pmatrix}\n\\begin{pmatrix} H_{1+} \\ H_{1-} \\end{pmatrix}\n\\end{aligned} $$\n解得：\n$$ \\left\\{ \\begin{aligned} \\frac{H_{1-}}{H_{1+}} = \\frac{ε_2 k_{z1} - ε_1 k_{z2}}{ε_2 k_{z1} + ε_1 k_{z2}} \\\\ \\frac{H_{2+}}{H_{1+}} = \\frac{2 ε_2 k_{z1}}{ε_2 k_{z1} + ε_1 k_{z2}} \\end{aligned} \\right. $$ 表面等离激元 只有一个界面，两侧都是隐逝波。（surface plasma polarition (SPP)）\n在两侧介质中的$k_z$都是纯虚数，所以波在两侧的介质中都是指数衰减。\n$$ \\begin{aligned} k_{z1} = i β_1 \\\\ k_{z2} = i β_2 \\end{aligned} $$其中： $ \\begin{aligned} β_1 = \\sqrt{k_x^2 - μ_1 ε_1 ω^2} \\\\ β_2 = \\sqrt{k_x^2 - μ_2 ε_2 ω^2} \\end{aligned} $ ，且： $ \\begin{aligned} k_x^2 \u003e μ_1 ε_1 ω^2 \\\\ k_x^2 \u003e μ_2 ε_2 ω^2 \\end{aligned} $\n对于TE模式\n此时界面两侧分别只有$E_{1-}$和$E_{2+}$，而不存在$E_{1+}$和$E_{2-}$。因为“反射波” $E_{1-} e^{i k_{z1} z} = E_{1-} e^{-\\sqrt{k_x^2 - μ_1 ε_1 ω^2} z}$ 是指数衰减，那么“入射波” $E_{1+} e^{i (-k_{z1}) z} = E_{1+} e^{\\sqrt{k_x^2 - μ_1 ε_1 ω^2} z}$ 就是指数增加，这是不允许的；$E_{2-}$同理，不能存在指数增加。\n界面两束波的关系用传输矩阵表示为：\n$$ \\begin{pmatrix} E_{2+} \\ 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} + \\frac{\\mu_2 k_{z1}}{2 \\mu_1 k_{z2}} \u0026amp; \\frac{1}{2} - \\frac{\\mu_2 k_{z1}}{2 \\mu_1 k_{z2}} \\ \\frac{1}{2} - \\frac{\\mu_2 k_{z1}}{2 \\mu_1 k_{z2}} \u0026amp; \\frac{1}{2} + \\frac{\\mu_2 k_{z1}}{2 \\mu_1 k_{z2}} \\end{pmatrix}\n\\begin{pmatrix} 0 \\ E_{1-} \\end{pmatrix} $$\n则：\n$$ \\large \\begin{cases} E_{2+} = \\left( \\frac{1}{2} - \\frac{\\mu_2 k_{z1}}{2 \\mu_1 k_{z2}}\\right) E_{1-} \\\\ \\frac{1}{2} + \\frac{\\mu_2 k_{z1}}{2 \\mu_1 k_{z2}} = 0 \\end{cases} $$可得：\n$$ \\large \\begin{cases} \\frac{\\beta_1}{\\beta_2} = - \\frac{\\mu_1}{\\mu_2} \\\\ \\frac{E_{2+}}{E_{1-}} = 1 \\end{cases} $$这说明如果界面一侧磁导率为正，另一侧为负，则可以实现两侧均为指数衰减的，束缚在二维界面上的传输波，此成为磁SPP。\n自然界中磁导率为负的材料较为少见，在人造的超材料中可能存在。\n对于TM模式\n界面两侧磁场振幅的关系用传输矩阵方法表示：\n$$ \\begin{pmatrix} H_{2+} \\ 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} + \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \u0026amp; \\frac{1}{2} - \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \\ \\frac{1}{2} - \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \u0026amp; \\frac{1}{2} + \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \\end{pmatrix}\n\\begin{pmatrix} 0 \\ H_{1-} \\end{pmatrix} $$\n则\n$$ \\begin{cases} H_{2+} = \\left( \\frac{1}{2} - \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} \\right) H_{1-} \\\\ \\frac{1}{2} + \\frac{ε_2 k_{z1}}{2 ε_1 k_{z2}} = 0 \\end{cases} $$可得：\n$$ \\large \\begin{cases} \\frac{\\beta_1}{\\beta_2} = - \\frac{ε_1}{ε_2} \\\\ \\frac{H_{2+}}{H_{1+}} = 1 \\end{cases} $$这表明：如果界面一侧介电常数为正，另一侧为负，则可以实现两侧均为指数衰减的，束缚在二维界面上的传输波，此称电SPP。\n自然界中存在介电常数为负的材料，银或者铝在可见光波段介电常数为负，与真空的界面处会形成电SPP。\n在界面二维波导上传播的（行）波，不是用入射波打出来的（所以没有入射波），那样$k_x$不够大；应该在界面边缘入射，傅立叶变换后会存在$k_x$很大的分量，从而产生隐逝波。 或者把光打到针尖上散射，因为针尖很细，所以傅立叶变换后，在很宽频带内都有分布，就产生了足够大的$k_x$分量，用来产生隐逝波。\n介质波导 第一层和第三层是真空，第二层是介电物质，光在第二层介质中全反射传播。\n图4.11.2 介质二维波导 对于这个模型： $$ \\begin{aligned} \u0026 ε_1 = ε_3= ε_0,\\ ε_2 = ε \\\\ \u0026 μ_1 = μ_2 = μ_3 = μ_0 \\end{aligned} $$给定 $k_0 \u003c k_x \u003c k$，如果形成波导，对于TE波（s波），只有$E_{1+}$和$E_{3-}$；对于TM波（p波）只有$H_{1+}$和$H_{3-}$。\n对于TE波\n传输矩阵为：\n$$ \\begin{aligned} \u0026amp; \\begin{pmatrix} E_{3+} \\ 0 \\end{pmatrix} \\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 i β} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 i β} \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{2} (e^{i k_z a} + e^{-i k_z a}) \u0026amp;\n-\\frac{μ_0 ω}{2 k_{zj}}(e^{i k_z a} - e^{-i k_z a}) \\ -\\frac{k_z}{2 μ_0 ω} (e^{i k_z a} - e^{-i k_z a}) \u0026amp; \\frac{1}{2} (e^{i k_z a} + e^{-i k_z a}) \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{i β}{μ_0 ω} \u0026amp; \\frac{i β}{μ_0 ω} \\end{pmatrix}\n\\begin{pmatrix} 0 \\ E_{1-} \\end{pmatrix} \\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 β i} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 β i} \\ \\end{pmatrix}\n\\begin{pmatrix} cos k_z a \u0026amp; -i \\frac{μ_0 ω}{k_z} sin k_z a \\ -i \\frac{k_z}{μ_0 ω} sin k_z a \u0026amp; cos k_z a \\ \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{β i}{μ_0 ω} \u0026amp; \\frac{β i}{μ_0 ω} \\end{pmatrix}\n\\begin{pmatrix} 0 \u0026amp; E_{1-} \\end{pmatrix} \\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 β i} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 β i} \\ \\end{pmatrix}\n\\begin{pmatrix} cos k_z a - \\frac{β}{k_z} sin k_z a \u0026amp; cos k_z a + \\frac{β}{k_z} sin k_z a \\ -i \\frac{k_z}{μ_0 ω} sin k_z a - i \\frac{β}{μ_0 ω} cos k_z a \u0026amp; -i \\frac{k_z}{μ_0 ω} sin k_z a + i \\frac{β}{μ_0 ω} cos k_z a \\end{pmatrix}\n\\begin{pmatrix} 0 \u0026amp; E_{1-} \\end{pmatrix} \\ \u0026amp;= \\begin{pmatrix} cos k_z a + \\frac{1}{2} (\\frac{k_z}{β} - \\frac{β}{k_z}) sin k_z a \u0026amp; \\frac{1}{2} (\\frac{k_z}{β} - \\frac{β}{k_z}) sin k_z a \\ -\\frac{1}{2} (\\frac{k_z}{β} + \\frac{β}{k_z} sin k_z a \u0026amp; cos k_z a - \\frac{1}{2} (\\frac{k_z}{β} - \\frac{β}{k_z}) sin k_z a \\end{pmatrix}\n\\begin{pmatrix} 0 \\ E_{1-} \\end{pmatrix} \\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} (\\frac{k_z}{β} - \\frac{β}{k_z}) sin k_z a E_{1-} \\ [cos k_z a - \\frac{1}{2} (\\frac{k_z}{β} - \\frac{β}{k_z}) sin k_z a] E_{1-} \\end{pmatrix} \\end{aligned} $$\n可得：\n$$ \\left\\{ \\begin{aligned} \u0026 cos k_z a - \\frac{1}{2} (\\frac{k_z}{β} - \\frac{β}{k_z} ) sin k_z a = 0 \\\\ \u0026 E_{3+} = \\left[ \\frac{1}{2} (\\frac{k_z}{β} + \\frac{β}{k_z}) sin k_z a \\right] E_{1-} \\end{aligned} \\right. $$解得：\n$$ \\left\\{ \\begin{aligned} k_z a \u0026= 2 \\mathrm{arctan} (\\frac{β}{k_z}) + n π \\\\ E_{3+} \u0026= E_{1-} \\end{aligned} \\right. $$这与4.9.1的解一致：介质全反射的相位差再加二维波导的离散条件。\n对于TM波：\n隐逝波的隧穿 足够靠近界面，就能看到隐逝波。\n从光密介质到光疏介质，全反射并不是说没有一点电磁波透射过去，而是一个随距离指数衰减的波。如果另一个介质靠近，就可以将这部分电磁波耦合出去。\n图4.11.3 隐逝波的隧穿 如图 4.11.2 所示结构，介质 1 和介质 3 为介电常数为 $ε$ 的电介质（光密），介质 2 为真空（光疏），介质 2 的厚度为 $a$。从介质 1 入射的电磁波的入射角大于临界角，即$k_0 \u003c k_x \u003c k$，则光在真空传播是指数衰减，隐逝波在真空中传输一段距离 a 后，进入介质 3 。\n对于TE波，只有 $E_{1+}$、$E_{1-}$和$E_{3+}$ 存在。\n不考虑$E_{3-}$?：我们从来没有把4个量全部考虑的，因为只有两个方程，要留一个自由度，最多就是3个量：入射、反射、透射。 因为上下两个介质是半无限大的区域，所以不会有反射波；而且即使上半介质有限，E3+打到界面上，在上侧还是只有透射波，没有反射波。 如果只考虑两个量，那就是界面波导。\n$$ \\begin{aligned} \u0026amp; \\begin{pmatrix} E_{3+} \\ 0 \\end{pmatrix} \\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 k_z} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 k_z} \\ \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{2} (e^{-β a} + e^{β a}) \u0026amp; -\\frac{μ_0 ω}{2 i β} (e^{-β a} - e^{β a}) \\ -\\frac{i β}{2 μ_0 ω} (e^{-β a} - e^{β a}) \u0026amp; \\frac{1}{2} (e^{-β a} + e^{β a}) \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_z}{μ_0 ω} \u0026amp; \\frac{k_z}{μ_0 ω} \\end{pmatrix}\n\\begin{pmatrix} E_{1+} \\ E_{1-} \\end{pmatrix} \\ \\\n\u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 k_z} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 k_z} \\ \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{2} (e^{- β a} + e^{β a}) - i \\frac{k_z}{2 β} (e^{-β a} - e^{β a}) \u0026amp; \\frac{1}{2} (e^{- β a} + e^{β a}) - i \\frac{k_z}{2 β} (e^{-β a} - e^{β a}) \\\n\\frac{i β}{2 μ_0 ω} (e^{-β a} - e^{β a}) - \\frac{1}{2} \\frac{k_z}{μ_0 ω} (e^{- β a} + e^{β a}) \u0026amp; \\frac{i β}{2 μ_0 ω} (e^{-β a} - e^{β a}) + \\frac{1}{2} \\frac{k_z}{μ_0 ω} (e^{- β a} + e^{β a}) \\end{pmatrix} \\begin{pmatrix} E_{1+} \\ E_{1-} \\end{pmatrix} \\ \\\n\u0026amp;=\n\\begin{pmatrix} \\frac{1}{2} (e^{-β a} + e^{β a}) - \\frac{i}{4} (\\frac{k_z}{β} - \\frac{β}{k_z})(e^{-β a} - e^{β a}) \u0026amp; \\frac{1}{2} (\\frac{k_z}{β} + \\frac{β}{k_z}) (e^{-β a} - e^{β a})\\\n-\\frac{i}{4} (\\frac{k_z}{β} + \\frac{β}{k_z}) (e^{-β a} - e^{β a}) \u0026amp; \\frac{1}{2} (e^{-β a} + e^{β a}) + \\frac{i}{4} (\\frac{k_z}{β} - \\frac{β}{k_z})(e^{-β a} - e^{β a}) \\end{pmatrix}\n\\begin{pmatrix} E_{1+} \\ E_{1-} \\end{pmatrix} \\ \\\n\u0026amp;= \\begin{pmatrix} \u0026amp; \\left[ \\frac{1}{2} (e^{-β a} + e^{β a}) - \\frac{i}{4} (\\frac{k_z}{β} - \\frac{β}{k_z})(e^{-β a} - e^{β a}) \\right] E_{1+} + \\left[ \\frac{1}{2} (\\frac{k_z}{β} + \\frac{β}{k_z}) (e^{-β a} - e^{β a}) \\right] E_{1-} \\ \u0026amp; \\left[ -\\frac{i}{4} (\\frac{k_z}{β} + \\frac{β}{k_z}) (e^{-β a} - e^{β a}) \\right] E_{1+} + \\left[ \\frac{1}{2} (e^{-β a} + e^{β a}) + \\frac{i}{4} (\\frac{k_z}{β} - \\frac{β}{k_z})(e^{-β a} - e^{β a}) \\right] E_{1-}\n\\end{pmatrix}\n\\end{aligned} $$\n解得：\n$$ \\Large \\begin{cases} \\frac{E_{3+}}{E_{1+}} = \\frac{1} {\\frac{1}{2} (e^{-β a} + e^{β a}) + \\frac{i}{4} (\\frac{k_z}{β} - \\frac{β}{k_z})(e^{-β a} - e^{β a})}\n\\\n\\frac{E_{1-}}{E_{1+}} = \\frac{\\frac{i}{4} (\\frac{k_z}{β} + \\frac{β}{k_z}) (e^{-β a} - e^{β a})} {\\frac{1}{2} (e^{-β a} + e^{β a}) + \\frac{i}{4} (\\frac{k_z}{β} - \\frac{β}{k_z})(e^{-β a} - e^{β a})} \\end{cases} $$\n可以看出，当$a \\to + \\infin$时，其解与4.7.5相同?\n假设：$ε_0\u003cε=10×10^{-12},\\ \\omega=10^9$，$k_x$ 比 $k$ 小一点，两个 $k_z$ 大概是：\n$$ \\begin{aligned} k_{z} \u0026amp;= k_{z3} = k_z = \\sqrt{μ_0 ε ω^2 - k_x^2} \u0026amp;\\text{(实数)} \\ \u0026amp;= \\sqrt{ (4π×10^{−7}) × (10 × 10^{-12}) × 10^{18} - 10}\\ \u0026amp;= \\sqrt{4π-10} \\\nk_{z0} \u0026amp;= i β = i \\sqrt{k_x^2 - μ_0 ε_0 ω^2} \u0026amp;\\text{(纯虚数)}\\ \u0026amp;=i \\sqrt{10 - 4 π} \\\n\\frac{k_{z}}{β} \u0026amp;= \\sqrt{\\frac{4π-10}{10 - 4 π}} = 1 \\end{aligned} $$\n所以也就是离得足够远时，透射波与入射波的振幅之比趋近于零，看不到透射波了；反射波与入射波振幅之比的模长为1。(反射系数近似等于-i？它能化简成$\\frac{k_{iz} - iβ}{k_{iz} + iβ}$的形式？)\n对于TM波，只有 $H_{1+}$、$H_{1-}$ 和 $H_{3+}$ 存在\n使用传输矩阵：\n解得：\n分布式布拉格反射镜 两种透明的介质，每一层厚度为$\\frac{1}{4}$波长，交替组合成多层薄膜，足够厚就变成了一面镜子。\n介质1的介电常数为$ε_1$，其厚度为$d_1 = \\frac{\\lambda}{4} \\sqrt{ \\frac{ε_1}{ε_0} }$; 介质2的介电常数为$ε_2$，其厚度为$d_2 = \\frac{\\lambda}{4} \\sqrt{ \\frac{ε_2}{ε_0} }$。\n介质1内的传输矩阵\n$$ \\begin{pmatrix} \\frac{1}{2} (e^{i k_{z1} d_1} + e^{-i k_{z1} d_1}) \u0026amp; -\\frac{μ_0 ω}{2 k_{z1}} (e^{i k_{z1} d_1} - e^{i k_{z1} d_1}) \\ -\\frac{k_{z1}}{2 μ_0 ω} (e^{i k_{z1} d_1} - e^{-i k_{z1} d_1}) \u0026amp; \\frac{1}{2} (e^{i k_{z1} d_1} + e^{-i k_{z1} d_1} ) \\end{pmatrix} \\begin{pmatrix} 0 \u0026amp; -i \\sqrt{ \\frac{μ_0}{ε_1}} \\ -i \\sqrt{ \\frac{ε_1}{μ_0}} \u0026amp; 0 \\end{pmatrix} $$\n电磁波垂直入射（即$k_x = 0$），对于每一层来说，$k_z d = \\frac{π}{2}$（由于垂直入射，s波和p波等同，这里只用s波形式）\n介质2内的传输矩阵：\n$$ \\begin{pmatrix} \\frac{1}{2} (e^{i k_{z2} d_2} + e^{-i k_{z2} d_2}) \u0026amp; -\\frac{μ_0 ω}{2 k_{z2}} (e^{i k_{z2} d_2} - e^{i k_{z2} d_2}) \\ -\\frac{k_{z2}}{2 μ_0 ω} (e^{i k_{z2} d_2} - e^{-i k_{z2} d_2}) \u0026amp; \\frac{1}{2} (e^{i k_{z2} d_2} + e^{-i k_{z2} d_2} ) \\end{pmatrix} \\begin{pmatrix} 0 \u0026amp; -i \\sqrt{ \\frac{μ_0}{ε_2}} \\ -i \\sqrt{ \\frac{ε_2}{μ_0}} \u0026amp; 0 \\end{pmatrix} $$\nn 对介质的传输介质：\n$$ \\begin{aligned} \u0026amp; \\begin{pmatrix} E_t \\ 0 \\end{pmatrix} \\ \u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 k_0} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 k_0} \\end{pmatrix}\n\\left[ \\begin{pmatrix} 0 \u0026amp; -i \\sqrt{\\frac{μ_0}{ε_2}} \\ -i \\sqrt{\\frac{ε_2}{μ_0}} \u0026amp; 0 \\end{pmatrix}\n\\begin{pmatrix} 0 \u0026amp; -i \\sqrt{\\frac{μ_0}{ε_1}} \\ -i \\sqrt{\\frac{ε_1}{μ_0}} \u0026amp; 0 \\end{pmatrix} \\right]^n\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_0}{μ_0 ω} \u0026amp; \\frac{k_0}{μ_0 ω} \\ \\end{pmatrix}\n\\begin{pmatrix} E_i \\ E_r \\end{pmatrix} \\\n\u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 k_0} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 k_0} \\end{pmatrix}\n\\begin{pmatrix} -\\sqrt{\\frac{ε_1}{ε_2}} \u0026amp; 0 \\ 0 \u0026amp; -\\sqrt{\\frac{ε_2}{ε_1}} \\ \\end{pmatrix}^n\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_0}{μ_0 ω} \u0026amp; \\frac{k_0}{μ_0 ω} \\ \\end{pmatrix}\n\\begin{pmatrix} E_i \\ E_r \\end{pmatrix} \\\n\u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 k_0} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 k_0} \\end{pmatrix}\n\\begin{pmatrix} (-\\frac{n_1}{n_2})^n \u0026amp; 0 \\ 0 \u0026amp; (-\\frac{n_2}{n_1})^n \\end{pmatrix}\n\\begin{pmatrix} 1 \u0026amp; 1 \\ -\\frac{k_0}{μ_0 ω} \u0026amp; \\frac{k_0}{μ_0 ω} \\ \\end{pmatrix}\n\\begin{pmatrix} E_i \\ E_r \\end{pmatrix} \\\n\u0026amp;= \\begin{pmatrix} \\frac{1}{2} \u0026amp; -\\frac{μ_0 ω}{2 k_0} \\ \\frac{1}{2} \u0026amp; \\frac{μ_0 ω}{2 k_0} \\end{pmatrix}\n\\begin{pmatrix} (-\\frac{n_1}{n_2})^n \u0026amp; -\\frac{n_1}{n_2})^n \\ -\\frac{k_0}{μ_0 ω} (-\\frac{n_2}{n_1})^n \u0026amp; -\\frac{k_0}{μ_0 ω} (-\\frac{n_2}{n_1})^n \\end{pmatrix}\n\\begin{pmatrix} E_i \\ E_r \\end{pmatrix} \\\n\u0026amp;= \\begin{pmatrix} \\frac{1}{2} (-\\frac{n_1}{n_2})^n + \\frac{1}{2} (-\\frac{n_2}{n_1})^n \u0026amp; \\frac{1}{2} (-\\frac{n_1}{n_2})^n - \\frac{1}{2} (-\\frac{n_2}{n_1})^n \\ \\frac{1}{2} (-\\frac{n_1}{n_2})^n - \\frac{1}{2} (-\\frac{n_2}{n_1})^n \u0026amp; \\frac{1}{2} (-\\frac{n_1}{n_2})^n + \\frac{1}{2} (-\\frac{n_2}{n_1})^n \\end{pmatrix}\n\\begin{pmatrix} E_i \\ E_r \\end{pmatrix} \\\n\\end{aligned} $$\n可解得：\n$$ \\left\\{ \\begin{aligned} \\begin{aligned} \\frac{E_r}{E_i} \u0026= \\frac{(-\\frac{n_1}{n_2})^n - (-\\frac{n_2}{n_1})^n} {(-\\frac{n_1}{n_2})^n + (-\\frac{n_2}{n_1})^n} \u0026= \\frac{(\\frac{n_2}{n_1})^{2n} - 1}{(\\frac{n_2}{n_1})^{2n} + 1} \\end{aligned} \\\\ \\begin{aligned} \\frac{E_t}{E_i} \u0026= \\frac{2} {(-\\frac{n_1}{n_2})^n + (-\\frac{n_2}{n_1})^n} \u0026= \\frac{2 (-\\frac{n_2}{n_1})^n}{(-\\frac{n_2}{n_1})^{2n} + 1} \\end{aligned} \\end{aligned} \\right. $$可以看出，只要$n_1 \\neq n_2$，当层数 n 足够大时，反射波振幅$E_r$与入射波振幅$E_i$之比为1。\n总结：\n除了以上的例题，还有很多中薄膜光学器件的问题都可以用传输矩阵的方法解决，我们这里就不再一一列举了。\n还需要强调的一点是用传输矩阵计算反射和透射后，还可以将结果代入原传输矩阵，令其在薄膜中间任意一点传输停止，并且将两束波合成电磁场，即可得到薄膜中任意一点电磁场强度。\n","date":"2021-06-20T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/2021-06-24-ch4_%E7%94%B5%E7%A3%81%E6%B3%A2%E4%BC%A0%E6%92%AD/","title":"ch4:电磁波"},{"content":"Books 频率派 统计机器学习\n《统计学习方法》李航\n12章：1绪论，12总结，中间10个常用算法：感K朴决逻 支提E隐条\n《机器学习》“西瓜书” 周志华\n很多学习方法，全面但不深入，基本推导和原理，\nThe Elements of Statistical Learning（ESL）\n贝叶斯派 概率图模型\nPattern Recognition And Machine Learning (PRML)\n12章算法：回分神核析 图混近采连 顺组\nMachine Learning-A Probabilistic Perspective (MLAPP)\n百科全书，包罗万象\n深度学习 《DeepLearning》“圣经” 张志华译\nVideos shuhuai007 github\n《机器学习基石》——台大 林轩田\n基本理论：VC Theroy，正则化，基础线性模型\n《机器学习技法》——台大 林轩田\n各种模型：SVM(好)，决策树，随机森林，神经网络，深度学习（前向网络）\n《机器学习导论》——张志华\n频率派角度，推导较多\n《统计机器学习》——张志华\n一些统计理论，贝叶斯角度：如概率不等式，偏数学，推导较多\n《斯坦福课堂 CS 229》——吴恩达\n大量数学推导，好像有2017年的新版添加deepLearning内容\n概率模型一系列视频——2015 徐亦达\n深度较深，EM，MCMC，HMM，滤波算法。在github上有notes：概率模型，DeepLearning\n《ML》(机器学习)——2017 台大 李宏毅\nCNN，RNN，LSTM\n《MLDS》——2018 台大 李宏毅\n深度学习里的优化，正则化，实践方法，NLP模型\n李宏毅2020机器学习深度学习（附完整课件和源码）[b站]\n","date":"2021-06-10T06:52:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/","title":"watch: ML - 白板 00 | Learning Materials"},{"content":"这个第一项中括号里面感觉不是δ函数啊？\n当 x=x\u0026rsquo; 时，中括号里是无穷大，这个体积分为何会等于 4πJ(x) ？\n而且当 x ≠ x\u0026rsquo; ，x 与 x\u0026rsquo; 距离小于 1 时, 中括号里也可能是有限值或无穷大；x 与 x\u0026rsquo; 距离大于 1 时，中括号里也不一定是 0。所以我感觉这不是一个δ函数，还是不太明白公式（3）为何最后得 µ0J(x)？我看安培定则中用到的静磁场旋度是 µ0J(x\u0026rsquo;)。\n然后下面第二项，我推导出来的还剩两项，感觉它们不等于0\n","date":"2021-05-17T21:44:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/question/","title":"question"},{"content":"\n本系列笔记刊载的所有内容，包括文字、图片等，均来自云南大学丁怀义老师的课程《电磁场理论与计算》以及他的讲义，版权归丁怀义老师（dinghy@ynu.edu.cn）所有。\n[toc]\n1. 静电磁场 库伦定律解决了静电场的一切问题:\n$$ \\pmb E(\\pmb x) = \\iiint_V \\frac{ρ(\\pmb{x\u0026rsquo;})}{4πε_0 |\\pmb x - \\pmb{x\u0026rsquo;}|^3} (\\pmb x - \\pmb{x\u0026rsquo;}) d \\pmb{V'}\n\\Longleftrightarrow\n\\begin{cases} \\pmb ∇ \\cdot \\pmb E = \\frac{ρ}{ε_0} \\ \\pmb ∇ × \\pmb E = 0 \\end{cases} $$\n库伦定律可以推出高斯定律（电场散度等于电荷密度除以$ε_0$），但是高斯定律推不出库伦定律，还需要电场旋度=0（因为泊松方程使用的是电势，而电势场的旋度为0，所以由泊松方程 $\\pmb ∇^2 φ = \\frac{ρ(\\pmb{x'})}{ε_0}$ 可以推出库伦定律）。 毕奥-萨伐尔定律解决了静磁场的一切问题：\n$$ \\left. \\begin{aligned} \\pmb B(\\pmb x) = \\frac{μ_0}{4π} \\int \\frac{\\pmb J( \\pmb{x\u0026rsquo;}) × (\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x\\ -\\pmb{x\u0026rsquo;}|^3} dV\u0026rsquo; \\\\ \\pmb ∇ \\cdot \\pmb J = 0 \\end{aligned} \\right}\n\\Longleftrightarrow \\begin{cases} ∇ \\cdot \\pmb B = 0 \\ ∇ × \\pmb B = μ_0 \\pmb J(\\pmb x) \\end{cases} $$\n$\\pmb B$ 的散度=0 是自然满足的，$\\pmb B$ 的旋度是使用毕奥-萨伐尔定律和恒稳电流散度为0（$\\pmb ∇ \\cdot \\pmb J = 0$）推出来的。 所以静电磁场核心可总结为以下四个微分方程：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb E = \\frac{ρ}{ε_0} \\\\ ∇ × \\pmb E = 0 \\\\ ∇ \\cdot \\pmb B = 0 \\\\ ∇ × \\pmb B = μ_0 \\pmb J \\end{cases} $$外加一个约束方程：$\\pmb ∇ \\cdot \\pmb J = 0$\n2. 介质中 在电中性介质中（去除了净电荷），只剩极化电荷是若干个偶极子。电偶极子的一级近似为0（正负电荷相等，从远处看总电荷量为0）；（极化）恒稳电流产生的磁场的一级近似也等于0，所以都只保留到偶极项。把极化电荷和极化电流对电势和磁矢势的贡献纳入到电位移矢量 $\\pmb D$ 和磁场强度 $\\pmb H$ 中：\n$$ \\pmb D = ε_0 \\pmb E + \\pmb P $$$\\pmb E$ 是外电场，$\\pmb P$ 是电极化强度（电偶极密度），对电位移矢量 $\\pmb D$ 求散度：\n$$ \\pmb ∇ \\cdot \\pmb D = ρ - ρ_p = ρ_f $$$ρ$ 是空间中全部电荷，$ρ_p$是极化电荷，两者一减就是自由电荷$ρ_f$ 。\n$$ \\pmb H = \\frac{\\pmb B}{μ_0} - \\pmb M $$$\\pmb B$ 是空间磁感应强度（磁感线），$\\pmb M$ 是磁化强度（磁偶极密度），对磁场强度$\\pmb H$求旋度：\n$$ \\pmb ∇ × \\pmb H = \\pmb J - \\pmb J_m = \\pmb J_f $$$\\pmb J$是空间全部电流，$\\pmb J_f$是极化电流。\n于是，介质中麦克斯韦方程组可写成：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb D = ρ_f \\\\ \\pmb ∇ × \\pmb E = 0 \\\\ \\pmb ∇ \\cdot \\pmb B = 0 \\\\ \\pmb ∇ × \\pmb H = \\pmb J_f(\\pmb x) \\end{cases} $$3. 变化的电磁场 在静电场和静磁场的方程中电场和磁场是分离的。但是：法拉第发现了磁生电：（磁铁穿过载流螺线管）变化的磁场能产生感生电动势；楞次确定了电动势的方向，总结为：\n任何封闭电路中感应电动势的大小，等于穿过这一电路磁通量的变化率。\n$$ U = \\oint \\pmb E \\cdot d \\pmb l = -\\frac{∂}{∂t} \\int \\pmb B \\cdot d \\pmb s $$ $\\pmb B$ 是磁感线密度？，乘上面积 s 就是磁通量。 根据斯托克斯定理：矢量场环路积分等于矢量场旋度的面积分\n$$ \\oint \\pmb E \\cdot d \\pmb l = \\int \\pmb ∇ × \\pmb E\\ d\\pmb s $$所以有：\n$$ \\pmb ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂t} $$在静电场中，E是保守场，环路积分一圈等于0，旋度为0；但是变化的电场的旋度与磁场有关。受此启发，麦克斯韦临门一脚，做了最后一个补充：电生磁。如果 $\\pmb J$ 并非恒稳电流，即 $\\pmb ∇ \\cdot \\pmb J \\neq 0$，就不能推出 $\\pmb ∇ × \\pmb B = μ_0 \\pmb J$ 了。对第四个方程求散度：\n$$ \\pmb ∇ \\cdot (\\pmb ∇ × \\pmb B) = μ_0 \\pmb ∇ \\cdot \\pmb J $$方程左边自动等于0（旋度场的散度等于零），再根据电荷守恒：\n$$ \\begin{aligned} -\\frac{∂ ρ}{∂ t} \u0026= ∇ \\cdot \\pmb J \u0026 \\text{散度就是净流出} \\\\ \\pmb ∇ \\cdot \\pmb J + \\frac{∂ ρ}{∂ t} \u0026= 0 \u0026 \\text{电荷守恒(流出+流入=0)} \\end{aligned} $$因为$\\pmb ∇ \\cdot \\pmb E = \\frac{ρ}{ε_0}$ 这个式子没改，把 $ρ$ 用 $\\pmb E$ 来表示。\n$$ 0 = \\pmb ∇ \\cdot \\pmb J + \\frac{∂ ρ}{∂ t} = \\pmb ∇ \\cdot \\left( \\pmb J + ε_0 \\frac{∂ \\pmb E}{∂ t} \\right) $$所以磁场的旋度改写为：\n$$ \\pmb ∇ × \\pmb B = μ_0 \\pmb J + μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} $$所以麦克斯韦方程组写为：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb E = \\frac{ρ}{ε_0} \\\\ ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂ t} \\\\ ∇ \\cdot \\pmb B = 0 \\\\ ∇ × \\pmb B = μ_0 \\pmb J + μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} \\end{cases} $$$ρ$ 和 $\\pmb J$ 说明电荷决定了电磁场，再加上电荷在电磁场中的受力方程：\n$$ \\pmb F = q (\\pmb E + \\pmb v + \\pmb B) $$就解决了经典电磁学的一切动力学问题。\n4. 介质中 如果在介质中，还是使用偶极近似，不过电偶极密度和磁偶极密度是时间的函数 $\\pmb P(\\pmb x ,t)$，$\\pmb M( \\pmb x ,t)$。极化电荷密度和极化电流密度的概念仍然成立：\n$$\n\\pmb ∇ \\cdot \\pmb P(\\pmb x,t) = ρ_P \\ \\pmb ∇ × \\pmb M(\\pmb x,t) = \\pmb J_M $$ 电偶极子的正负电荷，在拉伸收缩时会有电流效应（电子移动）：\n$$ \\pmb p(t) = \\sum q_i x_i(t) \\quad \\text{电偶极} \\\n\\frac{d \\pmb p}{d t} = \\sum q_i v_i \\quad \\text{qv就是电流} $$\n严谨写为：\n$$ \\pmb p = \\int \\pmb x ρ(\\pmb x) dV \\\\ \\frac{d \\pmb p}{d t} = \\frac{d}{dt} \\int \\pmb x ρ(\\pmb x) dV = \\int \\frac{d \\pmb x}{d t} ρ(\\pmb x) dV = \\int \\pmb J_p dV $$因而：\n$$ \\frac{d \\pmb P}{d t} =\\frac{d \\frac{\\sum_i \\pmb p_i}{\\Delta V}}{d t} = \\pmb J_p $$$\\pmb J_p$ 被称为“电极化电流”。将极化电荷 $ρ_P$ 极化电流 $\\pmb J_M$ 和电极化电流 $\\pmb J_p$ 代入方程组：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb E = \\frac{ρ_f + ρ_p}{ε_0} \\\\ \\pmb ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂ t} \\\\ \\pmb ∇ \\cdot \\pmb B = 0 \\\\ \\pmb ∇ × \\pmb B = μ_0 \\pmb J_f + μ_0 \\pmb J_M + μ_0 \\pmb J_P + μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} \\end{cases} $$把 $ρ_P,\\ ρ_M,\\ \\pmb J_M$ 的表达式代入麦克斯韦方程组：（考试要考）\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb E = \\frac{ρ_f}{ε_0} - \\frac{\\pmb ∇ \\cdot \\pmb P(\\pmb x, t)}{ε_0} \\\\ \\pmb ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂ t} \\\\ \\pmb ∇ \\cdot \\pmb B = 0 \\\\ \\pmb ∇ × \\pmb B = μ_0 \\pmb J_f + μ_0 \\pmb ∇ × \\pmb M(\\pmb x, t) + μ_0 \\frac{∂ \\pmb P}{∂ t} + μ_0 ε_0 \\frac{∂ \\pmb E}{∂ t} \\end{cases} $$仿照静电磁场中对电位移矢量和磁场强度的定义：\n$$ \\pmb D = ε_0 \\pmb E + \\pmb P \\\\ \\pmb H = \\frac{\\pmb B}{μ_0} - \\pmb M $$则：\n$$ \\begin{aligned} \\pmb ∇ \\cdot \\pmb D \u0026= ε_0 \\pmb ∇ \\cdot \\pmb E + \\pmb ∇ \\cdot \\pmb P \\\\ \u0026 = ρ_f + ρ_p - ρ_p \\\\ \u0026 = ρ_f \\end{aligned} $$$$ \\begin{aligned} \\pmb ∇ × \\pmb H \u0026= \\frac{\\pmb ∇ × \\pmb B}{μ_0}-\\pmb ∇ × \\pmb M(\\pmb x, t) \\\\ \u0026 = \\pmb J_f + \\pmb ∇ × \\pmb M(\\pmb x, t) + \\frac{∂ \\pmb P}{∂ t} + ε_0 \\frac{∂ \\pmb E}{∂ t} - \\pmb ∇ × \\pmb M(\\pmb x, t) \\\\ \u0026 = \\pmb J_f +\\frac{∂ \\pmb P}{∂ t} + ε_0 \\frac{∂ \\pmb E}{∂ t} \\\\ \u0026 = \\pmb J_f + \\frac{∂ \\pmb D}{∂ t} \\end{aligned} $$所以介质中麦克斯韦方程组可写成：\n$$ \\begin{cases} \\pmb ∇ \\cdot \\pmb D = ρ_f \\\\ \\pmb ∇ × \\pmb E = -\\frac{∂ \\pmb B}{∂ t} \\\\ \\pmb ∇ \\cdot \\pmb B = 0 \\\\ \\pmb ∇ × \\pmb H = \\pmb J_f + \\frac{∂ \\pmb D}{∂ t} \\end{cases} $$由于 $\\frac{∂ \\pmb D}{∂ t}$ 在这里跟电流密度 $\\pmb J$ 具有等同地位，所以 $\\frac{∂ \\pmb D}{∂ t}$ 也称作 “位移电流”，变化的电场产生磁场。\n","date":"2021-05-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/2021-05-01-ch3_%E9%BA%A6%E5%85%8B%E6%96%AF%E9%9F%A6%E6%96%B9%E7%A8%8B%E7%BB%84/","title":"ch3:麦克斯韦方程组"},{"content":"姓名：王子陈 学号：20171050008 专业：电子科学与技术 一、一维势阱电势分布 势阱底面电势为1,其余三面电势为0，求势阱内电势分布。\n因为阱内无电荷，所以求解拉普拉斯方程：\n$$ \\nabla^2\\phi=0 \\\\ \\frac{\\phi(x+\\Delta x,y)+\\phi(x-\\Delta x,y)-2\\phi(x,y)}{\\Delta x^2}+ \\frac{\\phi(x,y+\\Delta y)+\\phi(x,y-\\Delta y)-2\\phi(x,y)}{\\Delta y^2}=0 \\\\ [\\phi(x+\\Delta x,y)+\\phi(x-\\Delta x,y)-2\\phi(x,y)]\\Delta y^2= [-\\phi(x,y+\\Delta y)-\\phi(x,y-\\Delta y)+2\\phi(x,y)]\\Delta x^2 \\\\ \\phi(x,y)= \\frac{[\\phi(x+\\Delta x,y)+\\phi(x-\\Delta x,y)]\\Delta y^2+[\\phi(x,y+\\Delta y)+\\phi(x,y-\\Delta y)]\\Delta x^2} {2(\\Delta x^2+\\Delta y^2)} $$源代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import matplotlib.pyplot as plt xlength=1e-3 #x方向长度 1mm （单位 m） ylength=1e-3 #y方向长度 0.1mm n = 10 #100行 m = 10 #100列 phiArray=np.zeros((n,m)) phiArray[:,0]=1 #矩阵第一列置1 for i in range(10000): #左+右 phiArray[1:-1,1:-1]=((phiArray[1:-1,2:]+phiArray[1:-1,0:-2])*((xlength/n)**2) \\ +(phiArray[2:,1:-1]+phiArray[0:-2,1:-1])*((ylength/m)**2)) \\ /(2*((xlength/n)**2+(ylength/m)**2)) zeroArray=np.zeros((n,1)) for i in range(n): if( list(phiArray[:,i])==list(zeroArray[:,0])): print(i) break print(phiArray[:,m-2]) plt.pcolor(phiArray.T) #pcolor绘制的顺序是反的 plt.colorbar() plt.xlabel(\u0026#34;x\u0026#34;,fontsize=24) plt.ylabel(\u0026#34;y\u0026#34;,fontsize=24) plt.savefig(\u0026#39;./1x1mm_10x10_1wTimes\u0026#39;) plt.show() 改变 x 方向和 y 方向的长度以及 dx 和 dy (也就是 n 和 m)的值,计算、做图并分析这些参数对最终电势分布的影响。\n以下图像都迭代了一万次。\n1. 参照组 x长度=1mm, y长度=1mm, n=100格, m=100格\ndx=10$\\mu$m, dy=10$\\mu$m\n观察尺度比较小，x,y的观察范围都是1mm，每一格表示长度为10微米。\n2. y方向长度增加 x长度=1mm, y长度=1cm, n=100格, m=100格\ndx=10$\\mu$m, dy=0.1mm。\ny方向一格代表0.1毫米，跨度比较大，超过1毫米就看不出色彩变化了，其实还有很微弱的数值：第98行(1cm处)的数值量级在$10^{-25}-10^{-23}$\n3. y方向长度减小 x长度=1mm, y长度=0.1mm, n=100格， m=100格\ndx=10$\\mu$m，dy=100$\\mu$m\ny方向观测范围为0-0.1mm，在此范围内电势比较大\n4. 改变x方向长度变化不大 在x方向上，电势是均匀的，只是拉长了\n下图是x长度为1厘米的图像：\n5. m(行数)减少 x长度=1mm, y长度=1mm, n=100格, m=10格\ndx=10$\\mu$m，dy=0.1mm\ny方向上1格长度为0.1毫米，看起来没那么细腻，y方向相邻行有明显突变\n6. m(行数)增多 x长度=1mm, y长度=1mm, n=100格, m=1000格\ndx=10$\\mu$m, dy=1$\\mu$m\ny方向1格长度为1微米，没有颗粒感，更细腻\n7. n(列数)减少 x长度=1mm, y长度=1mm, n=10格, m=100格\nx方向1格长度100$\\mu$m，x方向有明显突变\n8. m, n都减少 x长度=1mm, y长度=1mm, n=10格, m=10格\nx方向和y方向1格都是距离0.1毫米，颗粒感比较强\n二、针尖放电 Point Discharge 盒子底面放一个针尖等势体，三面电势为0。（底边和红色区域电势为固定值）\n针尖：两条直线拼起来\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg #%matplotlib notebook xLength=1e-3 #x方向长度 1mm （单位 m） yLength=1e-3 #y方向长度 0.1mm n = 100 #矩阵100行 m = 100 #矩阵100列 dx = xLength/n #x方向一格长度 dy = yLength/m #y方向一格长度 phiArray=np.zeros((n,m)) volt = 0.1 #电势 height = yLength*0.5 #针尖高度 width = xLength*0.1 #1/2针尖宽度 slop_1 = height/width #直线1斜率 slop_2 = -slop_1 x1 = xLength/2-width #直线1截距 x1到原点距离 x2 = xLength/2+width #直线2截距 x2到原点距离 x1Num = x1/dx #截距x1所在网格序号 x2Num = x2/dx #截距x2所在网格序号 # 初始化等势体电势 def initialize_point(phiArray, slop_1, slop_2, x1Num, x2Num, volt): phiArray[:,0]=volt for i in range(n): for j in range(m): if ((j*dy/dx)\u0026lt;=slop_1*(i-x1Num)) \\ and ((j*dy/dx)\u0026lt;=slop_2*(i-x2Num)): phiArray[i][j]=volt # 迭代 initialize_point(phiArray, slop_1, slop_2, x1Num, x2Num, volt) for i in range(100): phiArray[1:-1,1:-1]=((phiArray[1:-1,2:]+phiArray[1:-1,:-2])*((xLength/n)**2) \\ +(phiArray[2:,1:-1]+phiArray[:-2,1:-1])*((yLength/m)**2)) \\ /(2*((xLength/n)**2+(yLength/m)**2)) #yLength\u0026lt;1时，yLength/m和xLength/n要换下位置 initialize_point(phiArray, slop_1, slop_2, x1Num, x2Num, volt) #等势体的电势每次迭代不能变 # 计算电场 Ex = -(phiArray[2:,1:-1]-phiArray[:-2,1:-1])/(2*dx) #电势在x方向梯度的相反数 Ey = -(phiArray[1:-1,2:]-phiArray[1:-1,:-2])/(2*dy) #电势在y方向梯度的相反数 plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (15.0, 5.0) plt.subplot(1,2,1) plt.pcolor(phiArray.T) plt.colorbar() plt.xlabel(\u0026#34;x\u0026#34;, fontsize = 25) plt.ylabel(\u0026#34;y\u0026#34;, fontsize = 25) plt.title(\u0026#34;电势分布\u0026#34;, fontsize = 25) plt.subplot(1,2,2) plt.pcolor((Ex**2+Ey**2)[round(0.2*n):round(0.7*n),:].T) plt.colorbar() plt.xlabel(\u0026#34;x\u0026#34;, fontsize = 25) plt.ylabel(\u0026#34;y\u0026#34;, fontsize = 25) plt.title(\u0026#34;电场强度\u0026#34;, fontsize = 25) plt.savefig(\u0026#34;./p1x1_100x100_0.5x0.2_v=0.1.png\u0026#34;) #命名的数字单位是mm print(\u0026#34;max E=\u0026#34;,np.max((Ex**2+Ey**2)[round(0.2*n):round(0.8*n),:])) 改变 x 方向和 y 方向的长度、dx 和 dy(也就是 n 和 m)的值、针尖的高度和宽度、电势大小等数据,计算、做图并分析这些参数对最终电势分布的影响。\n1. 参照组 x长度=1mm, y长度=1mm, n=100, m=100, height=0.5mm, width=0.2mm\n容器x方向长1毫米，y方向高1毫米，针高度为容器高度的一半：0.5毫米，针的1/2宽度为容器宽的0.1倍：0.1毫米，所以针宽0.2毫米\nx方向一个网格长dx=10$\\mu$m, y方向一个网格长dy=10$\\mu$m。电势分布及电场强度分布如下，针尖处具有最大场强为：\n$max E= 6.139\\times 10^{8}$ 2. 容器y方向加长到10毫米 x方向长度=1mm， y方向长度=10mm， n=100, m=100， dx=0.01mm， dy=0.1mm， height=5mm， width=0.2mm\n因为场强等于电势的微分： $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x})^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y\\ \\uparrow})^2\\downarrow $$ dy变大，场强变小。虽然分子上的变化率也变大了，但分母(把$\\phi$也差分)是三次的增速更快，整体变小。\n针高远大于针宽，像一条竖线，可能针尖端y方向的变化太小而忽略，类似于截顶\n$max E=1.144\\times 10^{8}$ 3. 容器y方向缩短到0.1毫米 xLength=1mm， yLength=0.1mm，n=100， m=100， dx=0.01mm， dy=0.001 m， height=0.05mm， width=0.2mm $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x})^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y\\ \\downarrow})^2\\uparrow $$ dy变小，场强变大。\n$max\\ E=1.065\\times 10^{10}$ 4. 容器x方向加长到10毫米 xLength=10mm， yLength=1mm， n=100， m=100， dx=0.1mm， dy=0.01mm， height=0.5mm， width=2mm. $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x\\ \\uparrow})^2\\downarrow+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y})^2 $$ dx变大，场强变小。\n$max\\ E=1.065\\times 10^{8}$\n5. 容器x方向减小到0.1毫米 xLength=0.1mm， yLength=1mm，n=100， m=100， dx=0.001mm， dy=0.01m， height=0.5mm， width=0.02mm $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x\\ \\downarrow})^2\\uparrow+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y})^2 $$ dx变小，场强变大。\n$max\\ E=1.144\\times 10^{10}$\n6. 增加针尖相对宽度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.5mm， width=1mm\n$$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x}\\downarrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y}\\downarrow)^2 $$ 分母没变，看分子：针尖附近x方向,y方向电势变化缓慢，所以针尖尖端场强变小。\n针尖的最大场强=$max E= 4.033\\times 10^{8}$\n7. 减小针尖相对宽度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.5mm， width=0.1mm\n容器x方向长度=1毫米，1/2针宽是容器宽度的0.05倍，所以针宽0.1毫米。\n针尖高度不变，宽度减小，针尖附近x方向，y方向电势变化加剧，所以针尖尖端强度变大。\n$max E=6.331\\times 10^{8}$\n8. 增加针尖相对高度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.7mm， width=0.1mm\n容器y方向高度=1毫米，针尖高度等于容器高度的0.7倍,即0.7毫米。\n针尖宽度不变，高度加长，相较于参照组针尖变尖锐了，针尖两侧电势变化加剧，针尖场强变大。\n$max E=6.264\\times 10^{8}$\n9. 减小针尖相对高度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.7mm， width=0.1m\n容器y方向高度=1毫米，针尖高度等于容器高度的0.3倍,即0.3毫米，针尖宽度还是0.2毫米。\n针尖宽度不变，高度缩短，相较于参照组针尖变钝了，针尖两侧电势变化缓慢，针尖场强变小。\n$max E = 5.772\\times 10^{8}$\n10. 增加n(列)=200 xLength=1mm， yLength=1mm， n=200， m=100， dx=0.005mm， dy=0.01mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\uparrow} \\downarrow}\\uparrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m}})^2 $$n 增大，dx减小，Ex增大，场强变大：$max\\ E=1.219\\times 10^9$\n11. 减小n(列)=10 xLength=1mm， yLength=1mm， n=10， m=100， dx=0.1mm， dy=0.01mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\downarrow} \\uparrow}\\downarrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m}})^2 $$n 减小，dx增大，Ex减小，场强变小：$max\\ E=1.410\\times 10^8$\n12. 增加m(行)=200 xLength=1mm， yLength=1mm， n=100， m=200， dx=0.01mm， dy=0.005mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n} })^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\uparrow}\\downarrow}\\uparrow)^2 $$m 增大，dy减小，Ey增大，场强变大： $max\\ E=1.705\\times 10^9$\n13. 减小m(行)=10 xLength=1mm， yLength=1mm， n=100， m=10， dx=0.01mm， dy=0.1mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n} })^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\downarrow}\\uparrow}\\downarrow)^2 $$m 减小，dy增大，Ey减小，场强变小： $max\\ E=1.037\\times 10^8$\n14. 网格10x10 xLength=1mm， yLength=1mm， n=10， m=10， dx=0.1mm， dy=0.1mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\downarrow}\\uparrow}\\downarrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\downarrow}\\uparrow}\\downarrow)^2 $$n,m 减小，dx,dy增大，Ex,Ey减小，场强变小： $max\\ E=2.479\\times 10^7$\n15. 网格200x200 x长度=1mm， y长度=1mm， n=200， m=200，dx=0.005， dy=0.005， height=0.5mm， width=0.2mm $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\uparrow}\\downarrow}\\uparrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\uparrow}\\downarrow}\\uparrow)^2 $$ n，m 增大，dx,dy减小，Ex,Ey增大，场强变大：\n电场最强在针尖处，最大值为 $2.456\\times 10^{9}$\n16. 增大电势 等势体电势为10V，空间电势变大，针尖电势增大：$max\\ E=6.139\\times 10^{10}$（增大2个数量级）\n17. 减小电势 等势体电势为0.1V，空间电势变小，针尖电势减小：$max\\ E=6.139\\times 10^{6}$（减小2个数量级）\n","date":"2021-04-13T01:44:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/%E6%95%B0%E5%80%BC%E4%BB%BF%E7%9C%9F/1_%E5%8A%BF%E9%98%B1%E9%92%88%E5%B0%96%E7%94%B5%E5%8A%BF%E5%88%86%E5%B8%83/%E4%BB%BF%E7%9C%9F1_%E7%94%B5%E5%8A%BF%E5%88%86%E5%B8%83/","title":"数值仿真"},{"content":"入门：C/C++中的段错误（Segmentation fault）【转】\nGDB调试 1 2 gcc -g test.c -o test #把调试信息加入二进制代码中 gdb test #启动gdb调试 一、暂停/恢复程序运行 1. 启动gdb 1 2 3 gdb \u0026lt;program\u0026gt;\t#当前目录下编译好的二进制代码 gdb \u0026lt;program\u0026gt; core\t#同时调试程序和core文件（core dump后产生的文件） gdb \u0026lt;program\u0026gt; \u0026lt;PID\u0026gt;\t#如果是服务程序，可以指定进程id，gdb会自动attach上去调试它，program应在PATH环境变量中搜得到 2. 设置断点 用break命令设置断点：\nbreak \u0026lt;function\u0026gt; ：在进入指定函数时停住\n1 2 break class::function break function(type, type) break \u0026lt;linenum\u0026gt; ：在指定行号停住\nbreak +offset / -offset ： 在当前行的前 / 后的offset行停住\nbreak filename:linenum ：在源文件filename的linenum行停住\nbreak filename::function ：在源文件filename的function函数入口停住\nbreak *address ：在程序运行的内存地址处停住\nbreak ：没有加参数时，表示在下一条指令处停住\nbreak \u0026hellip; if \u0026lt;condition\u0026gt; ：\u0026hellip;可以是上述参数，condition表示条件，在条件成立时停住 （条件断点）\n1 break if i=100\t#当i=100时，停住程序 查看断点，可使用 info 命令：\ninfo breakpoints [n] （ n 表示断点号） info break [n] 3. 设置观察点 watchpoint 一般用来观察某个表达式（变量也是一种表达式）的值是否有变化了，如果有变化，马上停住程序。我们有下面的几种方法来设置观察点:\nwatch \u0026lt;expr\u0026gt; ：为表达式（变量）expr 设置一个观察点。 rwatch \u0026lt;expr\u0026gt; ：当表达式（变量）expr 被读时，停住程序。 awatch \u0026lt;expr\u0026gt; ：当表达式（变量）的值被读或写时，停住程序 info watchpoints ：列出当前设置的所有观察点 4. 设置捕捉点 catchpoint 用来捕捉程序运行时的一些事件。如:载入共享库(动态链接库)或是 C++ 的异常。设置捕捉点的格式为:\ncatch \u0026lt;event\u0026gt; ：当event发生时，停住程序\n1 2 catch throw\t#一个C++抛出的异常 catch catch\t#一个C++捕捉到的异常 tcatch \u0026lt;event\u0026gt; ：只设置一次捕捉点，当程序停住后，点被自动删除\n5. 维护断点 GDB 中的断点也就是上述的三类。在 GDB 中,如果你觉得已定义好的停止点没有用了,你可以使用 delete 、 clear 、 disable 、 enable 这几个命令来进行维护。\nclear ：清除所有的已定义的停止点 clear \u0026lt;function\u0026gt; 或 clear \u0026lt;filename:function\u0026gt; ：清除所有设置在函数中的停止点 clear \u0026lt;linenum\u0026gt; 或 clear \u0026lt;filename:linenum\u0026gt; ：清除所有设置在指定行上的停止点 delete [breakpoints] [range\u0026hellip;] ：删除指定的断点（breakpoints是断点号，若不指定断点号，表示删除所有的断点。range表示断点号的范围（如：3-7）。其简写命令为 d disable [breakpoints] [range\u0026hellip;] ：断点不会被删除，当再次需要时，enable即可。如果不指定断点，会disable所有断点 enable [breakpoints] [range\u0026hellip;] enable [breakpoints] once range\u0026hellip; ：使能指定的断点一次，当程序停住，该断点立刻被GDB禁用disable enable [breakpoints] delete range\u0026hellip; ：使能指定的断点一次，当程序停止后，该断点立刻被删除 6. 维护停止条件 一般来说,为断点设置一个条件,我们使用 if 关键词,后面跟其断点条件。并且,条件设置好后,我们可以用 condition 命令来修改断点的条件。 (只有 break 和 watch 命令支持 if, catch 目前暂不支持 if )\nconditon \u0026lt;bnum\u0026gt; \u0026lt;expression\u0026gt; ：修改断点号为 bnum 的停止条件为 expression condition \u0026lt;bnum\u0026gt; ：清除断点号 bnum 的停止条件 ignore \u0026lt;bnum\u0026gt; \u0026lt;count\u0026gt; ：指定程序运行时，忽略断点号为 bnum 的停止条件 count 次 7. 为断点设定运行命令 可以使用 GDB 提供的 command 命令来设置停止点的运行命令。 也就是说,当运行的程序在被停止住时,我们可以让其自动运行一些别的命令,这很有利行自动化调试。对基于 GDB 的自动化调试是一个强大的支持。\ncommands [bnum] \u0026hellip; command-list \u0026hellip; end\n为断点号 bnum 写一个命令列表，当程序被该断点停住时，gdb 会依次运行命令列表中的命令。例如：\n1 2 3 4 5 break foo if x\u0026gt;0\t#断点设置在函数foo中，断点条件是x\u0026gt;0 commands printf \u0026#34;x is %d/n\u0026#34;,x\t#打印 x 的值 continue\t#然后继续运行程序 end 如果要清除断点上的命令序列,那么只要简单的执行一下 commands 命令,并直接在输个 end 就行了。\n8. 断点菜单 在 C++ 中，可能会重复出现同一个名字的函数若干次（函数重载），在这种情况下, break \u0026lt;function\u0026gt; 不能告诉 GDB 要停在哪个函数的入口。 当然，你可以使用 break \u0026lt;function(type)\u0026gt; 也就是把函数的参数类型告诉 GDB，以指定一个函数。 否则的话，GDB 会给你列出一个断点菜单供你选择你所需要的断点。你只要输入你菜单列表中的编号就可以了。如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (gdb) b String::after [0] cancel [1] all [2] file:String.cc; line number:867 [3] file:String.cc; line number:860 [4] file:String.cc; line number:875 [5] file:String.cc; line number:853 [6] file:String.cc; line number:846 [7] file:String.cc; line number:735 \u0026gt; 2 4 6 Breakpoint 1 at 0xb26c: file String.cc, line 867. Breakpoint 2 at 0xb344: file String.cc, line 875. Breakpoint 3 at 0xafcc: file String.cc, line 846. Multiple breakpoints were set. Use the \u0026#34;delete\u0026#34; command to delete unwanted breakpoints. (gdb) 可见，GDB 列出了所有 after 的重载函数,你可以选一下列表编号就行了。 0 表示放弃设置断点, 1 表示所有函数都设置断点。\n9. 恢复程序运行和单步调试 当程序被停住了，可以用continue命令恢复程序的运行直到程序结束，或到下一个断点处。也可以使用step 或者 next 单步执行\ncontinue [ignore-count] 或者 c [ignore-count] 或者 fg [ignore-count] ：恢复程序运行直到 Basic:\n1 2 3 gcc -ggdb3 hello.c #gdb3会生成更丰富的调试信息，可以和gdb更好的完成内联功能 gdb a.out (gdb) start #主要的临时断点 Output 1 2 3 4 5 Temporary breakpoint 1 at 0x1169: file hello.c, line 3. Starting program: /home/jack/backup/opencvTest/a.out Temporary breakpoint 1, main () at hello.c:3 3 { 显示代码片段：\n1 (gdb) list\t#显示源代码 Output 1 2 3 4 5 6 7 8 9 10 1 #include\u0026lt;stdio.h\u0026gt; 2 int main(void) 3 { 4 int i = 0; 5 printf(\u0026#34;Hello, world\\n\u0026#34;); 6 printf(\u0026#34;i is %d\\n\u0026#34;, i); 7 i++; 8 printf(\u0026#34;i is now %d\\n\u0026#34;, i); 9 return 0; 10 } 无法显示中文 GDB Ref:\nCppCon 2016: Greg Law “GDB - A Lot More Than You Knew\u0026quot;\n\u0026lsquo;Become a GDB Power User\u0026rsquo; - Greg Law [ ACCU 2016 ] (2024-01-19) 两个视频差不多又有区别，第一个画质好内容少一些，第二个画质差但内容多一些 冬眠贝尔熊 的评论\n1. More than you knew 按下 Ctrl+x+a，从1979进入八十年代！类似图形界面的TUI，再按一遍回到命令行\nctrl + l : 刷新屏幕\nctrl + p / ctrl+n : prev / next command\nctrl + x + 2 : 第2个窗口，cycle though\n或者输入：tui enable，或者 layout src，或者 layout asm（显示汇编代码），进入 TUI\n2. GDB has Python Full Pyton interpreter with access to standard modules (unless your gdb installaion is messed up!)\nThe gdb python module gives most access to gdb\n1 2 3 (gdb) python gdb.execute()\t#执行gdb命令 (gdb) python gdb.parse_and_eval()\t#to get data from inferior (gdb) python help(\u0026#39;gdb\u0026#39;)\t#to see online help Python Pretty Printers\n1 2 3 4 5 class MyPrinter(object) def __init__(self,val): self.val = val; def to_string(self): return (self.val[\u0026#39;member\u0026#39;]) 1 2 3 4 import gdb.printing pp = gdb.printing.RegexpCollectionPrettyPrinter(\u0026#39;mystruct\u0026#39;) pp.add_printer(\u0026#39;mystruct\u0026#39;, \u0026#39;^mystruct$\u0026#39;, MyPrinter) gdb.printing.register_pretty_printer( gdb.current_objfile(),pp) 3. In-built pretty printers for STL GDB will ( try to ) pretty-print most STL container classes ( std : : vector , std : string , etc ) , e.g.\n1 2 3 4 5 6 10\tvec.push_back(5); (gdb) next 12\treturn 0; (gdb) print vec $6 = std::vector of length 3, capacity 4 = {3, 4, 5} (gdb) Note that this relies on Python pretty printers installed on the target system\nCompiling / linking with a different version of libstdc + + ( e . g . executable built on a different host than the one beingused to debug ) , then pretty printing might give strange results.\nThere are many ( list with info pretty-printers ) , includingstd : string , std : bitset , std : list , std : multimap , std : queue , std : set , std : shared _ ptr ,std : stack , std : tuple , std : unique _ ptr , std : vector , std : weak _ ptr , and iterators.\n4. .gdbinit 5. GDB is built on ptrace and signals GDB是建立在ptrace之上的。当一个正在被跟踪的程序收到一个信号，它会暂停并且tracer会通过waitpid 注意到，所以当 the inferior收到信号，它就会停止然后gbd获得控制。通常gdb会回到prompt，但是具体会做什么取决于信号和设置。\n1 (gdb) info signals 有两个信号很特别：\nSIGINT 当在按下 Crtl+c 会产生 SIGTRAP 当 the inferior 遇到断点或者单步调试时会产生。(改变代码，0xCC操作码生成陷阱) 1 2 3 4 5 6 (gdb) handle SIGINT stop pirnt pass Signal\tStop\tPrint\tPass to program\tDescription SIGINT\tYes\tYes\tYes\tInterrupt (gdb) handle SIGINT stop print nopass Signal\tStop\tPrint\tPass to program\tDescription SIGINT\tYes\tYes\tNo\tInterrupt 6. Breakpoints \u0026amp; watchpionts 1 2 3 4 5 watch foo #stop when foo is modified watch -l foo\t#watch location rwatch foo\t#stop when foo is read watch foo thread 3\t#stop when thread 3 modifies foo watch foo if foo \u0026gt; 10\t#stop when foo is\u0026gt;10 foo 是一个局部变量\n7. thread apply 1 2 3 thread apply 1-4 print $sp thread apply all backtrace thread apply all backtrace full 8. Dynamic Printf 不需改变代码，用printf的话需要考虑加在哪里，然后重新编译运行，查看输出，没查到结果，又要重复一遍\n9. Calling inferior functions call foo will call foo in your inferior\n10. Catchpoints 像断点\n11. Remote debugging 通过serial/sockets 调试远程服务器。\ngdbserver localhost:2000 ./a.out\n12. Multiprocess Debugging Modern Source video: 【GDB调试教程】如何设置条件断点？如何动态修改变量？Python和C++混合怎么调试？如何附加到进程？- 双笙子佯谬 - bilibili\n(2024-01-19)\n调试 Python 程序\nDocs: DebuggingWithGdb\n用 gdb 调试 python，进入之后把要调试的 test.py 文件作为参数传入. vid\n1 2 3 4 5 6 7 gdb test.py # This trivial way won\u0026#39;t work. gdb python # debug python r test.py # i.e., shell cmd: `python test.py` # the appended arguments will be put into argv # Or, with `-ex` gdb python -ex \u0026#39;r test.py\u0026#39; 这样，GDB 就可以捕获 python 的异常。\nTODO: Debugging Python C extensions with GDB - Redhat Developers\nTemporary breakpoint only break once.\n1 tb func ","date":"2021-04-08T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lang/gdb_cpp/","title":"memo: C++ | Debug with GDB"},{"content":"感觉半导体物理与电磁场理论联系很强，特来学习。听丁老师讲课非常爽。\n丁：抛弃经典粒子学说（质点，点电荷），不要从粒子基础上搭建。直接从波动性开始认识：单体就是一个复数场。\n复数场：空间任一点上是一个复数：$\\phi(r,t)=\\psi(r)e^{i\\theta(r,t)}$ (没抄下来，可能不对)。模平方就是单体在各点上出现的概率密度\n薛定谔方程 $$ i\\hbar\\frac{\\partial}{\\partial t}\\Psi(\\vec{r},t)=(-\\frac{\\hbar^2}{2\\mu}\\vec{\\nabla}^2+V(\\vec{r},t))\\Psi(\\vec{r},t) $$ 左边是波函数$\\Psi(\\vec{r},t)$随时间的变化率，右边是空间的微分（$\\mu$也可写成m, 对应经典学说里粒子的质量，实际上只是方程中的一个参数；V(r)是个势函数）\n知道初始状态，根据变化率，就可以知道以后各时刻情况。\n解薛定谔方程 分离变量法：将时间和空间分开，令 $$ \\Psi(\\vec{r},t)=\\psi(\\vec{r})\\varphi(t) $$ 方程变为： $$ {\\displaystyle i\\hbar {\\frac {1}{\\varphi (t)}}{\\frac {d\\varphi (t)}{dt}}=-{\\frac {\\hbar ^{2}}{2mu}}{\\frac {1}{\\psi (\\mathbf {r} )}}\\nabla ^{2}\\psi (\\mathbf {r} )+V(\\mathbf {r} )} $$ 左边是时间的函数，右边是位置的函数，令两边都等于常数$E_n$（也可能是常数的组合）： $$ {\\displaystyle i\\hbar {\\frac {1}{\\varphi(t) }}{\\frac {d\\varphi(t) }{dt}}=-{\\frac {\\hbar ^{2}}{2\\mu}}{\\frac {1}{\\psi(\\vec{r}) }}\\nabla ^{2}\\psi(\\vec{r}) +V(\\vec{r})=E_n} $$ n 表示第n个。En有时可以随便取，有时会受V(r)的限制。\n左边的方程： $$ \\begin{align} \\displaystyle i\\hbar {\\frac {1}{\\varphi_n(t) }}{\\frac {d\\varphi_n(t) }{dt}}\u0026=E_n \\\\ \\frac{d\\varphi_n(t)}{dt}\u0026=\\frac{E_n}{i\\hbar}\\varphi_n(t) \\end{align} $$ 函数的一阶导还有自己，这个函数应该是指数函数，所以解的形式为： $$ \\phi_n(r,t)=\\sum_n a_n \\psi_n(r) exp(\\frac{E_n}{i\\hbar}t) $$ $\\psi(r)$由V(r)决定。$\\psi(r)$是一组正交完备的函数，它们的线性组合就是唯一解，对应一组唯一的系数an。\n对于初始态零时刻，各位置的情况： $$ \\phi(\\vec{t},0)=\\sum_n a_n \\psi_n(\\vec{r}) $$ 求an：点乘$\\psi_n(x)$再对v积分（复数的归一性是乘上自己的共轭等于1）\n右边的方程： $$ -{\\frac {\\hbar ^{2}}{2\\mu}}{\\frac {1}{\\psi(\\vec{r}) }}\\nabla ^{2}\\psi(\\vec{r}) +V(\\vec{r})=E_n $$ V(r)已知，也是可以求出来的。\n已知0时刻的各定态，再依据薛定谔方程，就可知道各定态随时间的演化，再叠加\nV(r)=0 势函数=0，自由场，没有边界。这时薛定谔方程： $$ i\\hbar {\\frac {1}{\\varphi(t) }}{\\frac {d\\varphi(t) }{dt}} =-{\\frac {\\hbar ^{2}}{2\\mu}}{\\frac {1}{\\psi(\\vec{r}) }}\\nabla ^{2}\\psi(\\vec{r}) =E_n $$ 右边的方程： $$ \\begin{align} -{\\frac {\\hbar ^{2}}{2\\mu}}{\\frac {1}{\\psi(\\vec{r}) }}\\nabla ^{2}\\psi(\\vec{r})\u0026=E_n\\\\ \\nabla ^{2}\\psi(\\vec{r})\u0026=-\\frac{2\\mu E_n}{\\hbar^2}\\psi(\\vec{r}) \\end{align} $$ 类似拉普拉斯方程，不过不等于0。而是$\\nabla^2\\psi=\\alpha\\psi$\n拉普拉斯方程的解满足：$k_x^2+k_y^2+k_z^2=0$ 三个系数里面必须有实数有虚数；对于亥姆霍兹方程的解满足：$k_x^2+k_y^2+k_z^2=-\\alpha=\\frac{2\\mu E_n}{\\hbar^2}$，因为自由场无边界，如果有虚数，就会出现指数增加或衰减，都会出现无穷大的情况(e^-x在负无穷是无穷大)，不行。所以只能是3个实数，解的形式：$Ae^{i(k_xx+k_yy+k_zz)}$\n令$\\frac{E_n}{\\hbar}=w$，所以$k^2=\\frac{2\\mu w}{\\hbar}$，则$w=\\frac{\\hbar}{2\\mu}k^2$ ，是一个抛物线，在正k半轴，各点w与k成正比，也正是因为各点群相位不同才有涨宽和收缩\n在k空间里，不同的3个$k_x,k_y,k_z$可以构成相同的$w$\n平面波相速度 保持相位一致的点的速度 对于x方向平面波：$e^{i(kx-w_kt)}$，它的相位：$\\phi=k_x-w_kt$，相速度：$\\frac{\\Delta x}{\\Delta t}=\\frac{w}{k}$ 两个平面波相加 $$ sin(k_1x)+sin(k_2x) = 2sin\\frac{k_1+k_2}{2}x \\cdot cos\\frac{k_1-k_2}{2}x $$ 群速度（对应平面波相速度） $$ \\begin{align} \u0026\\frac{dw}{dk}=\\frac{\\hbar}{\\mu}k \\end{align} $$ “波的动量”(对应粒子：mv) “只是远远看上去有这么个关系‘ $$ \\frac{h}{\\mu}k \\cdot\\mu=hk=p $$ \u0026ldquo;波的动能\u0026rdquo;(1/2 mv^2) $$ \\begin{align} \u0026\\frac{1}{2} (\\frac{\\hbar}{\\mu}k)^2 \\mu \\\\ =\u0026\\frac{1}{2} (\\frac{\\hbar}{\\mu})^2 \\frac{2\\mu w}{\\hbar} \\mu \\\\ =\u0026\\hbar w \\end{align} $$ “波主轴$k_0$质量” $$ \\mu = \\hbar(\\frac{d^2w}{dk^2})^{-1} $$ 频率与k的二阶导的倒数在乘上约化普朗克常数。\n三维波 不确定性原理 $$ \\Delta x \\Delta p \\ge \\frac{\\hbar}{2} $$ 其实是： $$ \\Delta x \\Delta k \\ge \\frac{1}{2} $$ $\\Delta x$ 是波包宽度，$\\Delta k$ 是傅里叶变换后有多少个k（$\\frac{2\\mu w}{\\hbar}$）。\n$\\Delta k=0$ 说明k 是确定值，表示全空间里都是这个k0，所以空间越大，说是k0越靠谱，空间越小，说是k0越不靠谱，会弥散\n","date":"2021-04-06T08:42:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/%E9%87%8F%E5%AD%90%E5%8A%9B%E5%AD%A6/","title":"量子力学"},{"content":"\n本系列笔记刊载的所有内容，包括文字、图片等，均来自云南大学丁怀义老师的课程《电磁场理论与计算》以及他的讲义，版权归丁怀义老师（dinghy@ynu.edu.cn）所有。\ndinghy@ynu.edu.cn 丁怀义（理论）：重要的是理解图像\nyzhao@ynu.edu.cn 赵勇彪（计算）\n[toc]\n一、静电学 电荷不动 为什么是？不随时间变化，形式比较简单？ 目的是描述静止电荷产生的电场 如果电荷静止不动，距离电荷的远近就可以度量，距离与电场之间的关系用库仑定律来描述 1.1 库伦定律 一个电荷产生的电场，在空间各点的电场强度与电荷量成正比，与距离的平方成反比， $E=\\frac{Q}{4π ε_0 r^3}\\pmb{r}$\n由油滴实验测定\n目的是计算空间任意点的电场\n如何实现：如果已知全空间所有位置上的电荷分布，利用库伦定律的线性叠加性\n对于离散的多个点电荷，在空间产生的电场：\n对于连续的电荷密度，产生的电场：\n如果将两种形式统一？：点电荷的电荷密度如何表示？需要满足几个要求：\n两个要求\n$$ ρ = \\begin{cases} +∞ \u0026\\pmb{x}=\\pmb{x_1} \u0026\\text{点电荷体积无限小密度无穷大} \\\\\\ 0 \u0026 \\pmb{x} \\neq \\pmb{x_1} \\end{cases} $$ 应该是一个狄拉克delta函数，其数学表示为：\n$$ \\begin{array}{c} δ(x)=0,(x\\neq0). \u0026\\text{除零以外的点函数值都等于0} \\\\\\ \\int^{∞}_{-∞} δ(x)dx=1 \u0026 \\text{在整个定义域上的积分等于1} \\end{array} $$ 可以将形式统一\n要与Q成正比\n所以将其猜想为：$ρ()=Qδ(x-x_1)$\n图1.1 很多时候不可能得到全空间电荷密度，可以将库仑定律转换成微分形式，从而可得局部空间的电场性质，再适配与边界条件，即可求出有限区域内的电场分布。\n库仑定律可以计算空间整体的电场（已知屋子四壁的所有电荷，就可以求屋子内各点场强），要想计算局部关系就需要高斯公式（微分形式）\n1.2 高斯定律—积分形式 穿出任意闭合曲面 S 的电场强度 E 的净通量 = 闭合曲面包围住的净电荷量比上真空介电常数 $ε_0$\n$$ ∯_S \\pmb{E(x)} \\cdot d\\pmb{s}=\\iiint_V\\frac{ρ(x)}{ε_0}dV $$ 一个电荷激发的电场的通量表示着电荷对电场作用的基本数量关系\n对于一个 球面 包围着一个点电荷，则球面上电场的强度均为：(方向沿着半径方向)\n$$ \\pmb{E}=\\frac{q}{4πε_0 \\pmb{r}^3}\\pmb{r} $$电场与面积的乘积等于电荷量除以介电常数：\n$$ S\\cdot E=\\frac{q}{ε_0} $$ 对于 任意 连续封闭曲面包围着一个点电荷，并将电场E和曲面S看作矢量，则：\n$$ \\pmb{E}\\cdot d\\pmb{s}=\\frac{q}{4πε_0r^2}cosφ ds $$其中 $cosφ$ 是 $\\pmb{E}$ 和 $d\\pmb{s}$ 的夹角， $\\pmb{E}$ 是沿着半径方向，$d\\pmb{s}$ 乘上$cosφ$ 相当于把曲面投影到球面上， 根据立体角的定义，物体的投影面积等于半径的平方乘上对应的 立体角，则 $cosφ ds=r^2dΩ$ 。 对 $\\pmb{E}\\cdot d\\pmb{s}$ 做封闭曲面积分:\n$$ ∯_S \\pmb{E}\\cdot d\\pmb{s} = \\oiint_S \\frac{q}{4πε_0r^2}r^2dΩ=\\frac{q}{ε_0} $$与球面中心电荷情况一致\n再推广到 n 个点电荷的情况，第 i 个电荷带电荷 $q_i$，产生的电场为 $\\pmb{E_i}$，由电场的可叠加性得总电场为 $\\pmb{E}=\\sum_{i=1}^m \\pmb{E_i}$，则\n$$ \\oiint_S\\pmb{E}\\cdot d\\pmb{s} =\\oiint_S \\sum_{i=1}^m E_i\\cdot d\\pmb{s}=\\sum_{i=1}^m\\oiint_SE_i\\cdot d\\pmb{s}=\\sum_{i=1}^m\\frac{q_i}{ε_0}=\\frac{Q}{ε_0} $$将闭合曲面包围区域变成连续的电荷分布，其电荷密度为 $ρ{(x)}$，则电场分布为 $\\pmb{E(x)} = ∭_V \\frac{ρ(x')}{4πε_0|x-x'|^3}(x-x')dV'$ , 则：\n$$ \\begin{aligned} \\oiint_S \\pmb{E}\\cdot d\\pmb{s} \u0026= \\oiint_S \\iiint_V \\frac{ρ(x')}{4πε_0|x-x'|^3}(x-x')dV'\\cdot d\\pmb{s} \\\\ \u0026=\\iiint_V\\oiint_S\\frac{ρ(x')}{4πε_0|x-x'|^3}(x-x')\\cdot d\\pmb{s}dV' \\\\ \u0026=\\iiint_V \\frac{ρ(x')}{ε_0}dV'=\\frac{Q}{ε_0} \\end{aligned} $$ 说明了由表面的电场分布可以直接得到体内的电荷电量\n(+)球坐标 三个分量：到球心的距离r，天顶角$θ$，方位角$φ$\n目的是：\n与三维直角坐标系的转换：\n$$ \\begin{cases} x\u0026=\u0026rsinθ\\cosφ \\\\ y\u0026=\u0026rsinθ\\sinφ \\\\ z\u0026=\u0026rcosφ \\\\ \\end{cases} $$ (+)弧度制 弧度是平面角的一种单位 弧长$l∝$ 半径r，系数就是弧度：$dθ=\\frac{dl}{r}$ 目的是推出的公式比角度简洁 (+)立体角(球面度) 球面积与半径的平方之比：$dΩ=\\frac{ds}{r^2}$。\n图 与形状没有关系，只要面积一样，半径一样，立体角就一样。\n目的是描述站在某一点的观察者看到的物体大小的尺度。 例如，在某观察点看到的远处的大物体可能与近处的小物体有相同的面积\n以观测点为球心，构造一个单位球面；任意物体投影到该单位球面上的投影面积，就是该物体相对于该观测点的立体角。只要投影面积一样，半径一样，立体角就相等\n面积的计算方式：\n几何法：对于一个微元(曲面)矩形的面积=长×宽\n宽=半径×天顶角=$r*dθ$\n对于赤道上弧长=半径×方位角=$r*dφ$，但是不在赤道上的纬线对应的不是方位角（而且越往两极越小），可以将纬线段投影到赤道面上，再利用弧长公式：$r\\cdot sinθ \\cdot dφ$\n所以，微元面积 $ds=r^2 sinθ \\ dθ \\ dφ$\n立体角 $dΩ=\\frac{ds}{r^2}=sinθ\\ dθ\\ dφ$\n雅可比行列式\n坐标系变换的尺度缩放系数 坐标系就是网格，坐标就是“等x线”与“等y线”的交点， $$ \\begin{aligned} {\\rm dV} \u0026= \\mathrm{dx\\, dy\\, dz} = \\left| \\frac{∂(x,y,z)}{∂(r,θ,φ)} \\right| \\mathrm{dr\\, dθ\\, dφ} \\\\ \u0026= r^2\\ \\mathrm{sinθ\\, dr\\, dθ\\, dφ} \\end{aligned} $$体积就是面积乘以厚度，所以\n$$ \\begin{array}{c} {\\rm ds} = \\frac{{\\rm dV}}{{\\rm dr}} = r^2\\ \\mathrm{sinθ\\, dθ\\, dφ} \\\\ {\\rm dΩ} = \\rm \\frac{ds}{r^2} = sinθ\\ dθ\\ dφ \\end{array} $$ 立体角的积分\n整个球的立体角：做闭曲面积分\n$$ \\begin{aligned} \\oiint_S \\mathrm{d}Ω \u0026= \\int_0^{2π}\\int_0^π \\mathrm{sinθ\\, dθ\\, dφ}\\\\ \u0026= \\int_0^{2π} 2 \\mathrm{d}φ\\ \\\\ \u0026= 4π \\end{aligned} $$ ","date":"2021-04-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/2021-03-09-ch1_%E9%9D%99%E7%94%B5%E5%9C%BA/","title":"ch1:静电场"},{"content":"\n本系列笔记刊载的所有内容，包括文字、图片等，均来自云南大学丁怀义老师的课程《电磁场理论与计算》以及他的讲义，版权归丁怀义老师（dinghy@ynu.edu.cn）所有。\n[toc]\n电流 单位时间内通过的电荷$\\frac{ΔQ}{Δt}$。 电流密度 单位时间，单位面积上通过的电荷$\\frac{ΔQ}{Δt Δs}$ 恒稳电流 电流密度不随时间和空间变化。 （对时间的要求）不能是1个电荷，流过去这个地方就没有电流了，必须是大量的电荷稳定的电荷流动 $\\frac{∂ \\pmb{J}(\\pmb{x},t)}{∂ t}$ ； （对空间分布的要求）散度就是左边一个点减去右边一个点，就是一个物理量在两点间（局部）的变化量。电荷是随时间流动的，各点电荷密度不随时间变化说明电荷密度$\\pmbρ$分布不变，也就是散度为零。$\\frac{∂ ρ}{∂ t}=\\pmb∇⋅\\pmb J=0$ 。即使电荷在流动，保证流入和流出的量相等，此处电流密度就不变，否则变化的电流会产生变化的电场，变化的电场会产生变化的磁场，就不是静磁场了。 静磁场 由恒稳电流产生的磁场。 2.1 毕奥萨伐尔定律 $$ \\begin{aligned} \\pmb B(\\pmb x) \u0026=\\frac{μ_0}{4π}∫\\frac{Id\\pmb l'×(\\pmb{x-x'})}{|\\pmb x-\\pmb{x'}|^3} \\\\ \u0026=\\frac{μ_0}{4π}∫\\frac{Qd\\pmb l'×(\\pmb{x-x'})}{Δ tΔ s|\\pmb x-\\pmb{x'}|^3} Δ s\\\\ \u0026=\\frac{μ_0}{4π}∫\\frac{\\pmb J(\\pmb{x'})× (\\pmb{x-x'})}{|\\pmb x-\\pmb{x'}|^3}dV' \\end{aligned} $$$\\pmb{x'}$ 是场源的位矢，$\\pmb x$是远点位矢，$\\pmb J(\\pmb{x'})$是源点电流密度，$μ_0$是真空磁导率。\n作用：由恒稳电流在远点处产生的静磁场是所有恒稳电流元产生的静磁场的积分。\n但它不能给出准确的单独的电流元产生的磁场，只是近似，因为只有一个$\\pmb J(\\pmb x,t)$ 不是恒稳电流。电流微元对应的磁场贡献为： $$ d\\pmb B(\\pmb x) =\\frac{μ_0}{4π}\\frac{Id\\pmb l'×(\\pmb{x-x'})}{|\\pmb x-\\pmb{x'}|^3} =\\frac{μ_0}{4π}\\frac{\\pmb J(\\pmb{x'})× (\\pmb{x-x'})}{|\\pmb x-\\pmb{x'}|^3}dV' $$ 则一个以速度 $\\pmb v$ 运动的点电荷其产生的磁场为： $$ \\pmb B=\\frac{μ_0}{4π}\\frac{q\\pmb v×\\pmb r}{r^3} $$ 这个 $\\pmb v$ 和历史有关，就是说电荷是匀速直线过来的，还是拐着弯过来的，产生的磁场不一样，因为场的作用传递需要时间，传递的速度是光速。而且用微元产生的磁场计算出来的安培力（洛伦兹力）不满足牛顿第三定律，但是把全空间所有电流源积分产生的力是满足牛顿第三定律的：\n根据洛伦兹力表达式：$\\pmb F=q\\pmb v × \\pmb B$，可以得到两个导线电流微元之间的作用力：\n$$ \\begin{aligned} d\\pmb{F_{12}} \u0026= q\\pmb v × \\pmb B \\\\ \u0026= I_1 d\\pmb{l_1} × \\left(\\frac{μ_0}{4π} \\frac{I_2d\\pmb{l_2} × \\pmb{x_{12}}}{|\\pmb{x_{12}}|^3} \\right) \\\\ \u0026= \\frac{μ_0}{4π}I_1I_2 d\\pmb{l_1}× \\left( \\frac{d\\pmb{l_2} × \\pmb{x_{12}}}{|\\pmb{x_{12}}|^3} \\right)\\\\ \u0026= -\\frac{μ_0}{4π}I_1I_2 \\left[(d\\pmb{l_1}⋅ d\\pmb{l_2}) \\frac{\\pmb{x_{12}}}{|\\pmb{x_{12}}|^3} + d\\pmb{l_2}(\\frac{d\\pmb{l_1}⋅ \\pmb{x_{12}}}{|\\pmb{x_{12}}|^3}) \\right] \\end{aligned} $$$d\\pmb F_{12}$不等于$d\\pmb F_{21}$，（第二项的$d\\pmb{l_1}$和$d\\pmb{l_2}$交换位置）差个负号。不是一对等大反向的力，不满足牛三。不过在全空间积分之后，第二项为0，而第一项是满足牛顿第三定律的，全空间积分形式如下： $$ \\pmb{F_{12}}=-\\frac{μ_0}{4π}I_1I_2\\oint\\oint(d\\pmb{l_1} ⋅ d\\pmb{l_2}) \\frac{\\pmb{x_{12}}}{|\\pmb{x_{12}}|^3} $$由此表达式可以得到任意两个带有恒稳电流导线之间的受力，比如两个无限长电流分别为$I_1$,$I_2$ 平行导线单位长度的相互作用力大小为： $$ \\begin{aligned} F_{12} \u0026amp;= \\frac{μ_0}{4π} I_1 I_2\\ l^2\\ \\frac{1}{d^2} \\ dF \u0026amp;= \\frac{μ_0}{4π} I_1 I_2\\ 2 dl\\ \\frac{1}{d^2} \\ \\frac{dF}{dl} \u0026amp;= \\frac{μ_0}{2π} \\frac{I_1 I_2}{d^2}\n\\end{aligned} $$\n毕奥-萨伐尔定律可以类比库伦定律记忆，相似处：平方反比；区别有三：场源电流密度$\\pmb J(\\pmb{x'})$是矢量；场源叉乘位矢；方向右手定则。\n(e) 无限长导线磁场 例如：一根通有电流 $\\bf I$ 无穷长的导线，其产生的磁场为：\n$$ \\begin{aligned} l \u0026= r⋅cos(π-θ)=-rcosθ =-\\frac{a}{sin(π-θ)}cosθ=-\\frac{a\\ cosθ}{sinθ} \\\\ dl \u0026=\\frac{a}{sin^2θ}dθ \\\\ \\end{aligned} $$$$ \\begin{aligned} |\\pmb B| \u0026=\\frac{μ_0}{4π} ∫\\frac{Id\\pmb{l'} ×(\\pmb{x-x'})}{|\\pmb x-\\pmb{x'}|^3} \\\\ \u0026=\\frac{μ_0 I}{4π} ∫_{0}^{π} \\frac{\\frac{a}{sin^2θ}dθ × \\pmb r}{r^3} \\\\ \u0026=\\frac{μ_0 I}{4π} ∫_{0}^{π} \\frac{\\frac{a}{sin^2θ}dθ\\, sinθ}{r^2} \\\\ \u0026=\\frac{μ_0 I}{4π} ∫_{0}^{π} \\frac{sinθ}{a}dθ\\\\ \u0026=\\frac{μ_0 I}{4π a} (-cosθ)|_{0}^{π} \\\\ \u0026=\\frac{μ_0 I}{2π a} \\end{aligned} $$磁场的散度 不论静、动，磁场散度都为0\n$$ \\begin{aligned} \u0026amp; \\pmb ∇ ⋅ \\pmb B(\\pmb x) \\ \u0026amp;= \\pmb ∇ ⋅ \\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x\u0026rsquo;}) × (\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\ \u0026amp;= \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) ⋅\\pmb ∇ × \\frac{(\\pmb x-\\pmb{x\u0026rsquo;})}{|\\pmb x-\\pmb{x\u0026rsquo;}|^3} \\quad \\text{(对x求导, J(x\u0026rsquo;)相当于常数)} \\ \u0026amp;= \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\begin{vmatrix} \\pmb i \u0026amp; \\pmb j \u0026amp; \\pmb k \\ \\frac{∂}{∂ x} \u0026amp; \\frac{∂}{∂ y}\u0026amp; \\frac{∂}{∂ z} \\ \\frac{x-x\u0026rsquo;}{|x-x\u0026rsquo;|^3} \u0026amp; \\frac{y-y\u0026rsquo;}{|y-y\u0026rsquo;|^3} \u0026amp; \\frac{z-z\u0026rsquo;}{|z-z\u0026rsquo;|^3} \\end{vmatrix} \\ \u0026amp;= \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\left[ \\left( \\frac{∂}{∂y} \\frac{z-z\u0026rsquo;}{|z-z\u0026rsquo;|^3} - \\frac{∂}{∂z} \\frac{y-y\u0026rsquo;}{|y-y\u0026rsquo;|^3} \\right) \\pmb i - \\left( \\frac{∂}{∂x} \\frac{z-z\u0026rsquo;}{|z-z\u0026rsquo;|^3} - \\frac{∂}{∂z} \\frac{x-x\u0026rsquo;}{|x-x\u0026rsquo;|^3} \\right) \\pmb j \\right. \\ \u0026amp; \\qquad\\qquad\\qquad\\qquad\n\\left. \\left( \\frac{∂}{∂x} \\frac{y-y\u0026rsquo;}{|y-y\u0026rsquo;|^3} - \\frac{∂}{∂y} \\frac{x-x\u0026rsquo;}{|x-x\u0026rsquo;|^3} \\right) \\pmb z \\right] \\ \u0026amp;= \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ 0 \\ \u0026amp;=0 \\end{aligned} $$\n磁场的旋度 等于真空磁导率$μ_0$乘上恒稳电流的电流密度：$μ_0 \\pmb J(\\pmb{x'})$\n$$ \\begin{aligned}\n\u0026amp; \\pmb ∇ × \\pmb B(\\pmb x) \\ \u0026amp;= \\pmb ∇ × \\frac{μ_0}{4} ∫ \\frac{\\pmb J(\\pmb{x\u0026rsquo;}) × (\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\\n\u0026amp;= \\frac{μ_0}{4} ∫ \\pmb ∇ × \\frac{\\pmb J(\\pmb{x\u0026rsquo;}) × (\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\quad \\text{(两个矢量场叉乘的旋度有四项，只有两项不为0)} \\\n\u0026amp;= -\\frac{μ_0}{4} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) \\left[ \\pmb ∇ ⋅ \\frac{(\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right]dV' + \\frac{μ_0}{4π} ∫ \\left( \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\pmb ∇ \\right) \\frac{(\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} dV\u0026rsquo; \\quad \\text{(变成对x\u0026rsquo;求导)} \\\n\u0026amp; = \\frac{μ_0}{4} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) \\left[ \\pmb ∇\u0026rsquo; ⋅ \\frac{(\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right]dV' - \\frac{μ_0}{4π} ∫ \\left( \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\pmb ∇\u0026rsquo; \\right) \\frac{(\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} dV'\n\\end{aligned} $$\n其中第一项：\n$$ \\begin{aligned} \u0026amp; \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) \\left[ \\pmb ∇\u0026rsquo; ⋅ \\frac{(\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right] dV' \\quad \\text{(中括号是一个$δ$函数，积分为4πJ(x\u0026rsquo;))} \\quad {(1)} \\\n\u0026amp; \\text{(下面计算有错误)}\\\n\u0026amp;= \\cancel{ \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) \\left[ \\frac{∂}{∂ x\u0026rsquo;}\\frac{x-x\u0026rsquo;}{|x-x\u0026rsquo;|^3}\\pmb i + \\frac{∂}{∂ y\u0026rsquo;}\\frac{y-y\u0026rsquo;}{|y-y\u0026rsquo;|^3}\\pmb j + \\frac{∂}{∂ z\u0026rsquo;}\\frac{z-z\u0026rsquo;}{|z-z\u0026rsquo;|^3}\\pmb k \\right] dV\u0026rsquo; \\quad {(2)} } \\\n\u0026amp;= \\cancel{ \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) \\left[ \\frac{2}{(x-x\u0026rsquo;)^3} \\pmb i + \\frac{2}{(y-y\u0026rsquo;)^3} \\pmb j + \\frac{2}{(z-z\u0026rsquo;)^3} \\pmb k \\right] dV\u0026rsquo; \\quad {(3)} } \\\n\u0026amp;= \\cancel{ \\frac{μ_0}{4π} \\pmb J(\\pmb{x\u0026rsquo;}) ∫_x \\left[ \\frac{2}{(x-x\u0026rsquo;)^3} \\pmb i + \\frac{2}{(y-y\u0026rsquo;)^3} \\pmb j + \\frac{2}{(z-z\u0026rsquo;)^3} \\pmb k \\right] dV\u0026rsquo; \\quad {(4)} } \\\n\u0026amp;= \\cancel{ \\begin{cases} 0, \u0026amp; \\text{x=x\u0026rsquo;} \\ \\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;})⋅ (积分有限)无穷大？⋅ dV\u0026rsquo; \u0026amp; \\text{x $\\neq$ x\u0026rsquo;} \\end{cases} } \\\n\u0026amp;= \u0026hellip; \\ \u0026amp;= \\mu_0 \\pmb J(\\pmb{x})\n\\end{aligned} $$\n上面$\\frac{(\\pmb x - \\pmb{x'})}{|\\pmb x - \\pmb{x'}|^3}$是一个矢量场，需要根据两个矢量场相乘的散度运算规则计算，不能化简成$\\frac{1}{x^2}$，这是标量场，标量场没有散度)\n其中第二项：\n$$ \\begin{aligned} \u0026amp; -\\frac{μ_0}{4π} ∫ (\\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\pmb ∇\u0026rsquo;) \\frac{\\pmb x-\\pmb{x\u0026rsquo;}}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} dV\u0026rsquo; \\\n\u0026amp;= -\\sum_i \\frac{μ_0}{4π} ∫ (\\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\pmb ∇\u0026rsquo;) \\left\\langle \\frac{\\pmb x-\\pmb{x\u0026rsquo;}}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right\\rangle_i dV\u0026rsquo; \\quad \\text{(分部积分)} \\\n\u0026amp;= \\sum_i \\frac{μ_0}{4π} ∫( \\underbrace{\\pmb ∇\u0026rsquo; ⋅ \\pmb J(\\pmb{x\u0026rsquo;}) }_{=0} ) \\left\\langle \\frac{\\pmb x-\\pmb{x\u0026rsquo;}}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right\\rangle_i dV\u0026rsquo; -\\sum_i \\frac{μ_0}{4π} ∫ \\pmb ∇\u0026rsquo; ⋅ \\left( \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\left\\langle \\frac{\\pmb x-\\pmb{x\u0026rsquo;}}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right \\rangle_i \\right) dV'\n\\\\\u0026amp;\\qquad \\text{(第一项恒稳电流散度=0，第二项按照$\\pmb ∇ (\\pmb f ⋅ \\pmb g)$展开)} \\\\ \u0026amp;= 0 - \\sum_i \\frac{μ_0}{4π} ∫ \\left[ \\pmb J(\\pmb{x\u0026rsquo;}) × \\underbrace{ \\left( \\pmb ∇ × \\left \\langle \\frac{\\pmb x-\\pmb{x\u0026rsquo;}}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right \\rangle_i \\right) }_{=0}\n+ \\left( \\pmb J(\\pmb{x'}) ⋅ \\pmb ∇ \\right) \\left\\langle \\frac{\\pmb x-\\pmb{x'}}{|\\pmb x - \\pmb{x'}|^3} \\right \\rangle_i \\right. \\\\ \u0026amp; \\qquad\\qquad\\qquad + \\left. \\left\\langle \\frac{\\pmb x-\\pmb{x'}}{|\\pmb x - \\pmb{x'}|^3} \\right \\rangle_i × \\underbrace{ \\left( \\pmb ∇ × \\pmb J(\\pmb{x'}) \\right) }_{=0} + \\left( \\left\\langle \\frac{\\pmb x-\\pmb{x'}}{|\\pmb x - \\pmb{x'}|^3} \\right\\rangle_i ⋅ \\pmb ∇ \\right) \\pmb J(\\pmb{x'}) \\right] \\\\ \u0026amp;= 0-\\sum_i \\frac{μ_0}{4π} ∫ \\left[ 0 + \\left( \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\pmb ∇ \\right) \\left\\langle \\frac{\\pmb x-\\pmb{x\u0026rsquo;}}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right \\rangle_i + 0 + \\left( \\left\\langle \\frac{\\pmb x-\\pmb{x\u0026rsquo;}}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} \\right\\rangle_i ⋅ \\pmb ∇ \\right) \\pmb J(\\pmb{x\u0026rsquo;})\t\\right] \\\n\u0026amp;= 0-\\sum_i \\frac{μ_0}{4π} ∫ \\left[ 0 + \\cancel{ \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\left\\langle \\frac{-2}{(x-x\u0026rsquo;)^3} + \\frac{-2}{(y-y\u0026rsquo;)^3} + \\frac{-2}{(z-z\u0026rsquo;)^3} \\right\\rangle_i } \\right. \\\n\u0026amp; \\qquad\\qquad\\qquad\\qquad + \\left. 0 + \\cancel{ \\left\\langle \\frac{\\pmb x-\\pmb{x'}}{|\\pmb x - \\pmb{x'}|^3} \\right\\rangle_i ⋅ \\pmb J(\\pmb{x'}) } \\right] \\\\ \u0026amp; \\qquad \\text{(上面是同样的错误，要用$\\pmb ∇(\\pmb f⋅\\pmb g$ 来计算)} \\\n\u0026amp;= \u0026hellip; \\ \u0026amp;= 0 \\end{aligned} $$\n从而可以得到：\n$$ \\pmb ∇ \\times \\pmb B(\\pmb x) = \\mu_0 \\pmb J(\\pmb x) $$ 由恒稳电流产生的静磁场是无源有旋场。散度为零，说明无源，所有的磁感应线应当闭合。由高斯定理可得磁感应强度在闭合曲面的积分为零：\n$$ \\int \\pmb B(\\pmb x) ⋅ d \\pmb S = \\int \\pmb∇ ⋅ \\pmb B(\\pmb x) dV = 0 $$ 由斯托克斯定理可得磁场的环路积分为曲面的电流通量，此即为安培定则：\n$$ \\oint \\pmb B(\\pmb x) \\cdot d \\pmb l = \\int \\pmb \\nabla \\times \\pmb B(\\pmb x) \\cdot d \\pmb S = \\int \\mu_0 \\pmb J(\\pmb{x}) \\cdot d \\pmb S = \\mu_0 \\pmb I $$ 2.2 磁矢势 其旋度为磁感应强度$\\pmb B(\\pmb x)$的一个矢量场：\n$$ \\pmb B(\\pmb x) = \\pmb ∇ × \\pmb A(\\pmb x) \\tag{2.2.1} $$ 因为磁场的散度等于0（$\\pmb ∇ ⋅ \\pmb B(\\pmb x) = 0$），而任意矢量场旋度的散度都等于0，所以可以用磁矢势$\\pmb A(\\pmb x)$来描述磁场。\n证明：任一矢量场的旋度的散度等于0。\n有一矢量场 $\\pmb A(\\pmb x) = P \\pmb i + Q \\pmb j + R \\pmb k$\n$$ \\begin{aligned} \u0026 \\pmb ∇ ⋅ (\\pmb ∇ × \\pmb A) \\\\ \u0026 = \\pmb ∇ ⋅ \\left[ \\left( \\frac{∂R}{∂y} - \\frac{∂ Q}{∂ z} \\right) \\pmb i + \\left( \\frac{∂P}{∂z} - \\frac{∂ R}{∂ x} \\right) \\pmb j + \\left( \\frac{∂Q}{∂x} - \\frac{∂ P}{∂ y} \\right) \\pmb k \\right] \\\\ \u0026 = \\frac{∂^2 R}{∂x ∂y} - \\frac{∂^2 Q}{∂x ∂z} +\\frac{∂^2 P}{∂y ∂z} - \\frac{∂^2 R}{∂y ∂x} + \\frac{∂^2 Q}{∂z ∂x} - \\frac{∂^2 P}{∂z ∂y} \\\\ \u0026=0 \\end{aligned} $$ 磁矢势不唯一：\n因为任何一个保守场的旋度都为零，所以可以加上任意一个保守场（$-\\pmb ∇φ(\\pmb x)$），即：\n$$ \\pmb{A'}(\\pmb{x}) = \\pmb{A}(\\pmb{x}) + \\pmb ∇φ(\\pmb x) $$这里的标量场是任意一个光滑连续的标量场，得到的磁场 $\\pmb B(\\pmb x)$ 不变。\n用毕奥-萨伐尔定律推出由电流密度写出磁矢势表达式：\n(类比静电场中，用库伦定律由电荷密度写出电势的表达式)\n$$ \\begin{aligned} \\pmb B(\\pmb x) \u0026amp;= \\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x\u0026rsquo;})× (\\pmb x - \\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|^3} dV\u0026rsquo; \\\n\u0026amp;= -\\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) × \\pmb ∇ \\frac{1}{|\\pmb x-\\pmb x\u0026rsquo;|}dV\u0026rsquo; \\\n\u0026amp;= \\frac{μ_0}{4π} \\pmb ∇ × ∫ \\frac{\\pmb J(\\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|} dV\u0026rsquo; \\ \\end{aligned} $$\n由上式看出 $\\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x'})}{|\\pmb x - \\pmb{x'}|} dV'$ 是 $\\pmb A(\\pmb x)$ 的一种表达式，即：\n$$ \\pmb A(\\pmb x) = \\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x'})}{|\\pmb x - \\pmb{x'}|} dV' \\tag{2.2.2} $$写成一般形式为：（保守场的旋度都为零，可以加上任意一个保守场 $∇ \\phi(x)$ ，任一旋度为0的场可以写成标量场的梯度) $$ \\pmb A(\\pmb x) = \\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x'})}{|\\pmb x - \\pmb{x'}|} dV' + \\pmb ∇ \\phi $$ 安培环路定理(磁矢势) （总能找到一个磁场的磁矢势）磁矢势的二阶导等于真空磁导率乘以电流密度的相反数：\n$$ \\pmb ∇^2 \\pmb A(\\pmb x) = -μ_0 \\pmb J(\\pmb x) $$ 类比静电场中，由高斯定律得到电势的微分方程（泊松方程）；这里用安培环路定理分析磁矢势的微分方程：\n$$ \\begin{aligned} \\pmb ∇ × \\pmb B \u0026= μ_0 \\pmb J(\\pmb x) \\\\ (\\pmb ∇ × (\\pmb ∇ × \\pmb A(\\pmb x))) \u0026= μ_0 \\pmb J(\\pmb x) \\\\ \\pmb ∇(\\pmb ∇ ⋅ \\pmb A(\\pmb x)) - \\pmb ∇^2 \\pmb A(\\pmb x) \u0026= μ_0 \\pmb J(\\pmb x) \\end{aligned} $$通过选取某一规范下的$\\pmb A(\\pmb x)$，使得 $\\pmb ∇ ⋅ \\pmb A(\\pmb x) = 0$ (称为库伦规范)，则微分方程简化为：\n$$ \\pmb ∇^2 \\pmb A(\\pmb x) = -μ_0 \\pmb J(\\pmb x) \\tag{2.2.3} $$ 解上面的微分方程：\n如果 $\\pmb \\nabla \\cdot \\pmb A(\\pmb x) \\neq 0$，可以通过规范变换找出一个标量场 $\\varphi(\\pmb x)$，写出另一个满足库伦规范的磁矢势场：\n$$ \\begin{aligned} \\pmb{A'} (\\pmb x) \u0026= \\pmb A(\\pmb x) + \\pmb \\nabla \\varphi(\\pmb x) \\\\ \\pmb \\nabla \\cdot \\pmb{A'}(\\pmb x) \u0026= \\pmb \\nabla \\cdot \\pmb A(\\pmb x) + \\pmb \\nabla^2 \\varphi(\\pmb x) = 0 \\end{aligned} $$使得：\n$$ \\pmb \\nabla^2 \\varphi(\\pmb x) = - \\pmb \\nabla \\cdot \\pmb A(\\pmb x) $$这是一个泊松方程，可以解出$\\varphi$，通过这个$\\varphi$变换，就可以得到一个散度为零的磁矢势场：\n$$ \\pmb \\nabla \\cdot \\pmb{A'} (\\pmb x) =0 $$所以库伦规范是总能满足的。\n所以只要解方程2.2.3即可得到磁矢势。方程2.3的每一个分量都是泊松方程：\n$$ \\begin{cases} \\pmb \\nabla^2 A_x(\\pmb x) = - \\mu_0 J_x \\\\ \\pmb \\nabla^2 A_y(\\pmb x) = - \\mu_0 J_y \\\\ \\pmb \\nabla^2 A_z(\\pmb x) = - \\mu_0 J_z \\end{cases} $$类比于静电场泊松方程的解，可得：\n$$ \\pmb A(\\pmb x) = \\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x'})}{|\\pmb x-\\pmb{x'}|} dV'+\\pmb C \\quad \\text{(C是个常矢量场)} $$忽略常矢量场后，即为 2.2.2。\n证明：上面方程的解 $\\pmb A(\\pmb x)$ 满足 $\\pmb ∇ ⋅ \\pmb A(\\pmb x)=0$\n$$ \\begin{aligned} \u0026amp; \\pmb ∇ ⋅ \\pmb A(\\pmb x) \\ \u0026amp;= \\pmb ∇ ⋅ \\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x\u0026rsquo;})}{|\\pmb x-\\pmb{x\u0026rsquo;}|} dV\u0026rsquo; \\\n\u0026amp;= \\frac{μ_0}{4π} ∫ \\pmb ∇ ⋅ \\frac{\\pmb J(\\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|}dV\u0026rsquo; \\\n\u0026amp;= -\\frac{μ_0}{4π} ∫ \\pmb J(\\pmb{x\u0026rsquo;}) ⋅ \\pmb{∇\u0026rsquo;} \\frac{1}{|\\pmb x - \\pmb{x\u0026rsquo;}|}dV\u0026rsquo; \\quad \\text{（变成对 x\u0026rsquo; 求导）} \\\n\u0026amp;= -\\frac{μ_0}{4π} ∫ \\pmb ∇\u0026rsquo; ⋅ \\frac{\\pmb J(\\pmb{x\u0026rsquo;})}{|\\pmb x - \\pmb{x\u0026rsquo;}|}dV\u0026rsquo; + \\frac{μ_0}{4π} ∫ [\\pmb ∇\u0026rsquo; ⋅ \\pmb J(\\pmb{x\u0026rsquo;})] \\frac{1}{|\\pmb x - \\pmb{x\u0026rsquo;}|}dV\u0026rsquo; \\quad \\text{（分部积分）} \\\n\u0026amp; \\text{（第一项是整体的外围空间积分（高斯定理），第二项是对恒稳电流的积分）} \\\n\u0026amp; \\text{（第一项和第二项的$J(x')$形式一样，只是按$\\frac{1}{r}$衰减，平移了一段距离，所以它的散度也为0）} \\ \u0026amp;=0+0=0\n\\end{aligned} $$\n虽然$\\pmb A$的表达式不唯一，但是选择库伦规范后解比较简洁，在静磁场的问题处理中一般都选用此种规范。\n2.3 磁矢势多极展开 描述恒稳电流在远点产生的磁矢势。\n对$\\frac{1}{|\\pmb x - \\pmb{x'}|}$做泰勒展开：\n$$ \\frac{1}{|\\pmb x - \\pmb{x'}|} = \\frac{1}{|\\pmb x|} + \\frac{\\pmb x ⋅ \\pmb{x'}}{|\\pmb x|^3} + ... $$ 带入磁矢势$\\pmb A(\\pmb x) = \\frac{μ_0}{4π} ∫ \\frac{\\pmb J(\\pmb{x'})}{|\\pmb x-\\pmb{x'}|}dV'$ 得： $$ A_i(x)=\\frac{μ_0}{4π} \\left[ \\frac{1}{|\\pmb x|} ∫ J_i(\\pmb{x'})dx'^3 + \\frac{\\pmb x}{|\\pmb x|^3} ∫ J_i(\\pmb{x'})\\pmb{x'}d x'^3 +... \\right] \\quad \\text{(对x'积分)} $$ 磁单极项为0：\n因为 恒稳电流密度的散度为0，构造一个函数，并对其进行推导：\n$$ \\begin{aligned} ∫ \\pmb ∇ ⋅ \\left[ x_i \\pmb J(\\pmb{x'}) \\right] dx'^3 \u0026= 0 \\quad \\text{(分部积分)} \\\\ ∫ \\pmb ∇ x_i ⋅ [ \\pmb J(\\pmb{x'}) ] dx'^3 + ∫ x_i \\underbrace{ [ \\pmb ∇ ⋅ \\pmb J(\\pmb{x'}) ] }_{散度=0} dx'^3 \u0026 = 0 \\\\ ∫ (1,1,1) ⋅ \\pmb J(\\pmb{x'}) dx'^3 + 0 \u0026= 0 \\\\ ∫ \\pmb J(\\pmb{x'}) dx'^3 = 0 \\end{aligned} $$($\\pmb \\nabla \\cdot x_i$：对每个分量求导都为1，x方向(1,0,0)，y方向(0,1,0)，z方向(0,0,1))\n就是说：任何一个$J_x,J_y,J_z$ 都闭合，走一圈积分为零。\n可得磁单极项为0：\n$$ \\pmb A^{(0)}(\\pmb x) = \\frac{μ_0}{4 π} \\frac{∫ \\pmb J(\\pmb{x'}) dx'^3}{|\\pmb x|} = 0 $$ 磁偶极 磁偶极 $\\pmb m$ ：位矢与源电流密度的叉乘的体积分的一半，其大小等于某一平面上的一圈环路电流乘以面积\n$$ \\begin{aligned} \\pmb m \u0026= \\frac{1}{2} ∫ \\pmb{x'} × \\pmb J(\\pmb{x'}) dx'^3 \\\\ \u0026= \\frac{1}{2} ∫ \\pmb{x'} × \\pmb J(\\pmb{x'})\\ ds \\ dl \\\\ \u0026= \\frac{1}{2} ∫ \\pmb{x'} × I \\ d \\pmb l \\quad\\text{(方向给了$l$)}\\\\ \u0026\\text{($\\pmb x' × d \\pmb l$是平行四边形面积,$\\frac{1}{2}$就是三角形面积)} \\\\ \u0026= \\rm I ⋅ S \\end{aligned} $$叉乘就是1/2的平行四边形面积，沿环路走一圈，就是环围住的面积。\n磁偶极项为：\n$$ \\pmb A^{(1)}(\\pmb x) = \\frac{μ_0}{4π} \\frac{\\pmb x}{|\\pmb{x}|^3} ∫ J_i(\\pmb{x'}) \\pmb{x'} dx'^3 $$对于分子 $\\pmb x ∫ J_i(\\pmb{x'})\\pmb{x'}dx'^3$ ，构造一个函数：\n$$ \\begin{aligned} 0 \u0026= ∫ \\pmb ∇ ⋅ \\left[ x_i x_j \\pmb J(\\pmb{x'}) \\right] dx'^3 (\\pmb{x'})dx'^3 \\\\ 0 \u0026= ∫ \\pmb ∇ x_i ⋅ [x_j \\pmb J(\\pmb{x'})]dx'^3 (x')dx'^3 + ∫ \\pmb ∇ x_j ⋅ [x_i \\pmb J(\\pmb{x'})]dx'^3 + ∫ x_i x_j [\\pmb ∇ ⋅ J(\\pmb{x'})] dx'^3 \\\\ 0 \u0026= ∫ [x_i J_j(\\pmb{x'}) + x_j J_i(\\pmb{x'})] dx'^3 \\end{aligned} $$又因为：\n$$ \\begin{aligned} \\because\\ \u0026 A+B=0 \\\\ \u0026 B-A = B-(-A) = 2B \\\\ \\therefore\\ \u0026 B = \\frac{1}{2}(B-A) = -\\frac{1}{2}(A-B) \\end{aligned} $$所以，分子化简为：\n$$ \\begin{aligned} \u0026 \\pmb x ∫ J_i(\\pmb{x'})\\pmb{x'}dx'^3 \\\\ \u0026 = \\sum_j x_j ∫ x_j' J_i dx'^3 \\\\ \u0026 = -\\frac{1}{2} \\sum_j x_j ∫[ x_i' J_i - x_j' J_i ] dx'^3 \\\\ \u0026 = -\\frac{1}{2} \\sum_j ε_{ijk} \\ x_j \\ ∫ (\\pmb{x'} × \\pmb J)_k \\ dx'^3 \\end{aligned} $$代入磁偶极项为：\n$$ A^{(1)}(\\pmb x) = \\frac{μ_0}{4π} \\frac{ \\left[ \\frac{1}{2} ∫ \\pmb{x'} × \\pmb J dx'^3 \\right] × \\pmb x}{|\\pmb x|^3} = \\frac{μ_0}{4π} \\frac{\\pmb m × \\pmb x}{|\\pmb x|^3} $$其中的 $\\pmb m$ 就是磁偶极。\n2.4 磁偶极与电偶极对比 由电势的梯度求电场（ 电偶极 $\\pmb p=∫_V ρ(\\pmb {r'}) \\pmb{r'} dV'$ ） $$ \\begin{aligned} \\text{球内：} \u0026amp; -\\pmb ∇ \\left( \\frac{1}{4 π ε_0} \\frac{\\pmb p ⋅ \\pmb r}{R^3} \\right) \\ \u0026amp; = -\\frac{1}{4πε_0} \\frac{1}{R^3}[\\pmb p ×(\\pmb ∇ × \\pmb r) + (\\pmb p ⋅ \\pmb ∇)\\pmb r + \\pmb r ×(\\pmb ∇ × \\pmb p) + (\\pmb r ⋅ \\pmb ∇)\\pmb p] \\ \u0026amp; = -\\frac{1}{4πε_0} \\frac{1}{R^3} [\\pmb p × (\\pmb ∇ × \\pmb r) + (\\pmb p × \\pmb ∇) \\pmb r] \\quad \\text{($\\pmb p$均匀分布，它的微分都是0)} \\ \u0026amp; = -\\frac{1}{4πε_0} \\frac{1}{R^3} [0+\\pmb p] \\quad \\text{(叉乘r得0, 点乘r是基矢)} \\ \u0026amp; = -\\frac{1}{4 π ε_0} \\frac{\\pmb p}{R^3} \\\n\\text{球外：} \u0026amp; - \\end{aligned} $$\nReview 电极化 介质对空间电势的贡献可以看作是各分子或者晶胞所产生的电偶极子的贡献之和： $$ \\varphi^{(1)}(\\pmb r) = \\sum_i \\frac{\\pmb{p_i} ⋅ (\\pmb r - \\pmb{r_i})}{4 π ε_0 |\\pmb r - \\pmb{r_i}|^3} $$ 在电中性介质中虽然局部的分子或者晶胞电荷分布很复杂，比如原子核与外层电子不均匀分布；如果局部内的正负电荷等量，从远处看，零级近似就是0（没有电荷），但是产生的电势不为0，就可以分析其偶极项。将介质分成若干个单元，每个单元不大不小（如果太大就不能反映局部细节，如果太小只看一个原子，那就不能实现简化的目的）可以取100个分子。每个单元有一个电偶极。求介质产生的电势，就是各单元电偶极产生的电势之和。\n为了简化计算，希望把求和变成积分，积分就可以得到解析解。所以需要一个“偶极密度”（再对其做体积分）。从宏观上看，介质中的电偶极子排布很紧密，可以近似看作是连续分布，就可以定义 “电极化强度$\\pmb P$” 来表示密度（单位体积内有多少个电偶极子）： $$ \\pmb P(\\pmb r) = \\frac{\\sum_i \\pmb p_i}{Δ V} $$ 这样，全空间电偶极贡献求和变成积分形式： $$ \\varphi^{(1)}(\\pmb r) = ∫ \\frac{\\pmb P(\\pmb{r'}) ⋅ (\\pmb r - \\pmb{r'})}{4π ε_0 |\\pmb r- \\pmb{r'}|^3} dV' $$ 再通过分部积分化简（整体微分减只对偶极密度的微分）： $$ \\begin{aligned} \\varphi^{(1)}(\\pmb r) \u0026 = \\frac{1}{4π ε_0} ∫ \\pmb P(\\pmb{r'}) \\pmb ∇' \\frac{1}{|\\pmb r-\\pmb{r'}|}dV'\\\\ \u0026 = \\frac{1}{4π ε_0} ∫ \\left[ \\pmb ∇' ⋅ \\frac{\\pmb P(\\pmb{r'})}{|\\pmb r - \\pmb{r'}|} - \\frac{\\pmb{∇'} \\pmb P(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|}dV' \\right] \\end{aligned} $$ 根据高斯定理，对于局域分布（不是全空间都有）的偶极子，总能找到一个高斯面，包围住的电偶极子总和为0，所以第一项： $$ ∫ \\pmb ∇' ⋅ \\frac{\\pmb P(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|}dV' = ∫ \\frac{\\pmb P(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|} d\\pmb S' = 0 $$ 从而只剩第二项（只有表面上有极化电荷），则： $$ \\varphi^{(1)}(\\pmb r) = -\\frac{1}{4πε_0} ∫ \\frac{\\pmb ∇' ⋅ \\pmb P(\\pmb{r'})}{|\\pmb r - \\pmb{r'}|} dV' $$ 对比库伦定律 $\\left( \\varphi= ∫ \\frac{ρ(\\pmb{x'})}{4π ε_0} dV' \\right)$，分子可以定义为 “极化电荷” $$ ρ_P(r) = - \\pmb ∇' ⋅ \\pmb P(\\pmb r') $$ 则电偶极对电势的贡献可以写成极化电荷密度的形式： $$ \\varphi^{(1)}(\\pmb r) = -\\frac{1}{4πε_0} ∫ \\frac{ρ_P(r)}{|\\pmb r - \\pmb{r'}|}dV' $$ 静电场的高斯定理改写为： $$ \\begin{aligned} \\pmb ∇ ⋅ \\pmb E \u0026amp;= \\frac{ρ_f(\\pmb r)-\\pmb ∇\u0026rsquo; ⋅ \\pmb P(\\pmb r)}{ε_0} \\ \u0026amp;= \\frac{ρ_f(\\pmb r)+ρ_P(\\pmb r)}{ε_0} \\ \u0026amp;= \\frac{ρ(\\pmb r)}{ε_0}\n\\end{aligned} $$ 有时只知道自由电荷密度$ρ_f$，不知道极化电荷密度，所以定义一个中间辅助量 电位移矢量$\\pmb D$: $$ \\pmb D = ε_0 \\pmb E + \\pmb P $$ 对$D$求散度，结果只有自由电荷密度： $$ \\begin{aligned}\n\\pmb ∇ ⋅ \\pmb D \u0026amp;= \\pmb ∇ ⋅ ε_0 \\pmb E + \\pmb ∇ ⋅ \\pmb P \\ \u0026amp;= ρ_f + 0 \\quad \\text{（P是均匀分布，它微分为0）} \\end{aligned} $$ 进而可以求出极化电荷密度。\n一般来说，电偶极密度$\\pmb P$与外加电场$\\pmb E$ ，根据这个关系可以由空间自由电荷分布 $ρ_f$ 解出空间电厂或者电势分布。在一级近似下，$\\pmb P$与$\\pmb E$ 取线性关系： $$ \\pmb P = ε_0 \\chi_e \\pmb E $$ $\\chi_e$是电极化率。可得： $$ \\pmb D = ε_0 \\pmb E + \\pmb P = ε_0(1+\\chi_e) \\pmb E = ε \\pmb E $$ $ε$ 为介电常数，$ε_r = 1+ \\chi_e =\\frac{ε}{ε_0}$ 为相对介电常数。\n2.5 介质磁极化的宏观描述 空间磁矢势是局域所有磁偶极子的贡献之和： $$ \\pmb A^{(1)}(\\pmb r) = \\sum_i \\frac{μ_0}{4π} \\frac{\\pmb m × (\\pmb r- \\pmb{r'})}{|\\pmb r - \\pmb{r_i}|^3} $$ 在无净电流介质中，单个原子、分子或者晶胞中尺度范围内电子并非静止，稳态下可以认为是小范围内的局域恒稳电流，虽然每个原子、分子或者晶胞的局域电流很复杂，但是考虑远场近似，保留至偶极项，就是一个个磁偶极子。（强调：这种电流是一种经典的想象，实际在原子尺寸范围内电子运动涉及到相对论量子力学，其磁偶极并非可以简单的看作是电流，其中还包括了电子、质子和中子的自旋等，这里我们考虑的是宏观描述，所以将原子、分子或者晶胞的内部所有电子复杂行为近似等效为磁偶极）\n从宏观上来看，介质中的磁偶极子排布很紧密，可以近似看作是连续分布，定义一个表示磁偶极密度的量——磁化强度$\\pmb M$： $$ \\pmb M(\\pmb r) = \\frac{\\sum_i \\pmb{M_i}}{Δ V} $$ 磁偶极贡献求和的公式可变成积分形式： $$ \\pmb A^{(1)}(\\pmb r) = ∫ \\frac{μ_0}{4π} \\frac{\\pmb M(\\pmb{r'}) × (\\pmb r-\\pmb{r'})}{|\\pmb r-\\pmb{r'}|^3} dV' $$ 通过分部积分化简： $$ \\begin{aligned} \\pmb A^{(1)}(\\pmb r) \u0026 = \\frac{μ_0}{4π} ∫ \\pmb M(\\pmb{r'}) × \\pmb{∇'} \\frac{1}{|\\pmb r-\\pmb{r'}|} dV' \\\\ \u0026 = \\frac{μ_0}{4π} ∫ \\left[ -\\pmb ∇' × \\frac{\\pmb M(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|} + \\frac{\\pmb ∇' × \\pmb M(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|} \\right]dV' \\end{aligned} $$ 根据高斯定理，对于局域分布的磁偶极子，总能找到一个封闭曲面，使得包围的磁偶极子总量为0，所以第一项： $$ ∫ - \\pmb ∇' × \\frac{\\pmb M(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|}dV' = ∫ \\frac{\\pmb M(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|}× d \\pmb{S'}=0 $$ 从而只剩第二项 $$ \\pmb A^{(1)}(\\pmb r) = \\frac{μ_0}{4π} ∫ \\frac{\\pmb ∇' × \\pmb M(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|} dV' $$ 对比毕奥-萨伐尔定律，可以定义分子为 极化电流密度： $$ \\pmb J_M(\\pmb r) = \\pmb ∇ × \\pmb M(\\pmb r) $$ 将磁偶极子贡献的磁矢势积分写成： $$ \\pmb A^{(1)}(\\pmb r) = \\frac{μ_0}{4π} ∫ \\frac{\\pmb J_M(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|}dV' $$ 静电场的安培定律可改写为： $$ \\pmb ∇ × \\pmb B = μ_0 [\\pmb J_f(\\pmb r)+ \\pmb J_M(\\pmb r)] = μ_0 \\pmb J(\\pmb r) $$ 为了方便使用，定义磁场强度： $$ \\pmb H = \\frac{\\pmb B}{μ_0}-\\pmb M $$ 安培定律可变为： $$ \\pmb ∇ × \\pmb H = \\pmb J_f $$ 一般来说，$\\pmb M$ 与 $\\pmb B$ 有一定的函数关系，根据这个函数关系可以有空间自由电流分布 $\\pmb J_f$ 解出空间磁场或者磁矢势分布。在一级近似下，$\\pmb M$与$\\pmb B$取线性关系： $$ \\pmb M = μ_0 \\chi_M \\pmb H $$ $\\chi_M$ 为磁化率。可得： $$ \\pmb B = μ_0 \\pmb H + \\pmb M = μ_0(1+ \\chi_M)\\pmb H = μ \\pmb H $$ $μ$为磁导率，$μ_r=1+\\chi_e = \\frac{μ}{μ_0}$ 为相对磁导率。\n在无自由电流时，$\\pmb ∇ × \\pmb H = 0$，（类似电场强度E）则可以定义一个磁标势场 $\\phi_m$，使得： $$ \\pmb H = -\\pmb ∇ \\phi_m $$ 因为 $$ \\pmb ∇ ⋅ \\pmb H = \\pmb ∇ ⋅ \\frac{\\pmb B}{μ_0} - \\pmb ∇ ⋅ \\pmb M $$ （对电场强度E求散度得电荷密度）这里可以定义一个量：磁荷(密度) $$ ρ_M = - \\pmb ∇ ⋅ \\pmb M $$ （类比泊松方程）可得： $$ \\pmb ∇^2 \\phi_m = ρ_M $$ 根据这些，可以用解静电场的方法来求解静磁场。\n2.6 介质球体中的静电磁场问题 (e)已经电极化 例一：半径为 R ，均匀极化，极化强度（偶极密度）为$\\pmb P$的介质球，求空间电场分布。\n这里的 $\\pmb P$ 与 $\\pmb E$ 无关，极化完之后，把外加电场撤掉了。\n全空间无自由电荷，因而所有电场可看作都是由极化电荷贡献的。可以使用空间全积分的形式 $\\left( -\\frac{1}{4πε_0} ∫ \\frac{\\pmb P(\\pmb{r})|\\pmb r - \\pmb{r'}|}{|\\pmb r - \\pmb{r'}|^3} dV' \\right)$ ，也可以用化简后的： $$ \\varphi(\\pmb r) = -\\frac{1}{4πε_0} ∫ \\frac{\\pmb ∇ ⋅ \\pmb P(\\pmb r)}{|\\pmb r-\\pmb{r'}|} dV' $$ 介质球内部，偶极密度$\\pmb P$是均匀的，所以$\\pmb ∇ ⋅ \\pmb P =0$；在球外部，$\\pmb P$本身等于0；所以只有边界（球壳）上的极化电荷对电场有贡献。求$\\pmb P$的散度就是$\\pmb P$在球面的法向方向上的差，取极化方向为 $z$方向，则$\\pmb ∇ ⋅ P = 0- P ⋅ cosθ$，则极化电荷面密度 $ρ_P(\\pmb r)=Pcosθ$。这是纯正电偶极的面电荷分布，在2.4节已计算过，其电场分布为： $$\n$$ 根据$\\pmb D=ε_0 \\pmb E + \\pmb P$，可得电位移矢量$\\pmb D$的分布： $$$$\n(e)已经磁极化 例2：半径为 R 磁化强度为 $\\pmb M$ 的介质球壳，求空间磁场分布。\n解法一：全空间无自由电流，因而所有磁场可看作都由极化电流贡献，根据磁感应强度等于全空间各磁偶极子贡献的积分：\n$$ \\pmb A(\\pmb r) = \\frac{μ_0}{4π} ∫ \\frac{\\pmb ∇' × \\pmb M(\\pmb{r'})}{|\\pmb r-\\pmb{r'}|} dV' $$介质球内部，偶极密度$\\pmb M(\\pmb{r'})$是均匀的，所以其微分$\\pmb ∇' × \\pmb M(\\pmb{r'}) = 0$；球外部，M 本身等于0；所以只有边界上的磁化电流对磁矢势有贡献。叉乘是看切向分量的差，所以$\\pmb ∇' × \\pmb M(\\pmb{r'}) = 0-\\pmb M ⋅ sinθ$，所以磁化电流：$\\pmb J_M(\\pmb r) = \\pmb ∇ × \\pmb M(\\pmb r) = Msinθ \\pmb e_\\phi$ 。M是赤道上的磁化强度。已经在2.4节解过。\n解法二：H与E形式结构一样，B与D形式结构一样\n这里无自由电流，可以用磁荷，直接带入电极化的解。\n","date":"2021-04-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/2021-04-01-ch2_%E9%9D%99%E7%A3%81%E5%9C%BA/","title":"ch2:静磁场"},{"content":"(Feature figure from g++ → make → cmake - 二圈妹的文章 - 知乎)\n《CMake实践》\n三、cmake工程 建立工程目录 t1\n1 2 3 mkdir t1 cd t1 touch main.c CMakeLists.txt 编辑源文件 main.c\n1 2 3 4 5 6 # include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;Hello World from t1 Main!\\n\u0026#34;); return 0; } 编辑CMakeLists.txt (原书最后一行有误)\n1 2 3 4 5 PROJECT(HELLO)\t#定义工程名称 SET(SRC_LIST main.c)\t#定义变量SRC_LIST, 值为main.c源文件 MESSAGE(STATUS \u0026#34;This is BINARY dir\u0026#34; ${HELLO_BINARY_DIR})\t#向终端输出用户定义的信息 MESSAGE(STATUS \u0026#34;This is SOURCE dir\u0026#34; ${HELLO_SOURCE_DIR}) ADD_EXECUTABLE(hello ${SRC_LIST})\t#编译源文件生成可执行文件hello PROJECT(projectname [CXX] [C] [JAVA])\n定义工程名称projectname，可指定工程支持的语言(支持的语言列表可缺省)，默认支持所有语言。这条projecct指令隐式地定义了2个cmake变量：\n1 2 \u0026lt;projectname\u0026gt;_BINARY_DIR\t#此例为：HELLO_BINARY_DIR 二进制文件目录 \u0026lt;projectname\u0026gt;_SOURCE_DIR\t#源文件目录 因为采用的是内部编译，2个变量目前指的都是工程所在路径 /backup/cmake/t1\n同时 cmake 系统也帮我们预定义了2变量：\n1 2 PROJECT_BINARY_DIR PROJECT_SOURCE_DIR 他们的值分别跟 HELLO_BINARY_DIR 与 HELLO_SOURCE_DIR 一致。 为了统一起见,建议以后直接使用 PROJECT_BINARY_DIR，PROJECT_SOURCE_DIR，即使修改了工程名称,也不会影响这两个变量。 如果使用了**\u0026lt;projectname\u0026gt;_SOURCE_DIR**，修改工程名称后,需要同时修改这些变量。 SET (VAR [VALUE] [CACHE TYPE DOCSTRING [FORCE]])\nset 指令可以用来显式的定义变量。\n1 2 SET(SRC_LIST main.c) SET(SRC_LIST main.c;t1.c;t2.c)\t#源文件列表也可以是多个文件 MESSAG ([SEND_ERROR | STATUS | FATAL_ERROR] \u0026ldquo;message to display\u0026rdquo;\u0026hellip;)\n向终端输出用户定义的信息，包含了三种类型：\nSEND_ERROR 产生错误，生成过程被跳过 STATUS 输出前缀为 - 的信息 FATAL_ERROR 立即终止所有 cmake 过程 ADD_EXECUTABLE(hello ${SRC_LIST})\n定义了这个工程会生成一个文件名为 hello 的可执行文件，相关的源文件是 SRC_LIST中定义的源文件列表。用 ${ } 来引用变量\n开始构建\n在工程目录下：\n1 cmake .\t#构建当前目录，生成了Makefile 根据Makefile编译源代码，连接，生成目标文件、可执行文件(hello)\n1 2 3 make #或者 make VERBOSE=1\t#可以看到make构建的详细过程,以便排查错误 运行可执行文件(hello)\n1 ./hello cmake基本语法 变量使用 ${ } 方式取值，但是在IF控制语句中是直接使用变量名\n指令(参数1 参数2 \u0026hellip;)\n参数用括号括住，参数之间用空格或分号隔开：\n1 2 ADD_EXECUTABLE(hello main.c func.c) ADD_EXECUTABLE(hello main.c;func.c) 指令是大小写无关的，参数和变量是大小写敏感的。（推荐全部大写指令）\n工程名HELLO 和 可执行文件名hello 是没有任何关系的\n如果文件名中有空格，使用双引号括住：\n1 SET(SRC_LIST \u0026#34;func.c\u0026#34;) 清理工程：\n清除上次的make命令所产生的目标(object)文件（后缀为“.o”的文件）及可执行文件。\n1 make clean make disclean 对cmake无效，所以需要用外部构建(out-of-source)，来使工程目录整洁。\n安装\n将编译成功的可执行文件安装到系统目录中，一般为/usr/local/bin 目录中\n1 make install 生成发行版软件包\n将可执行文件及相关文件打包成一个tar.gz压缩的文件用来作为发布软件的软件包。\n1 make dist 它会在当前目录下生成一个名字类似“PACKAGE-VERSION.tar.gz”的文件。PACKAGE和VERSION，是我们在configure.in中定义的AM_INIT_AUTOMAKE(PACKAGE, VERSION)。\n检查发行软件包\n生成发布软件包并对其进行测试检查，以确定发布包的正确性。\n1 make distcheck 这个操作将自动把压缩包文件解开，然后执行configure命令，并且执行make，来确认编译不出现错误，最后提示你软件包已经准备好，可以发布了。\n外部构建： 编译会生成一些无法自动删除的中间文件，所以在工程目录下建立build目录，用于存放中间文件，然后 cmake .. 对上层目录编译，在build目录中生成了make需要的Makefile和其他的中间文件。运行make编译，就会在build目录下获得目标文件 hello.o\nPROJECT_SOURCE_DIR 仍指代工程目录，即 /backup/cmake/t1 PROJECT_BINARY_DIR 则指代编译目录，即 /backup/cmake/t1/build 四、规范的工程 规范要求 为工程添加一个子目录 src， 用来放置工程源代码 添加一个子目录 doc，用来放置工程的文档 hello.txt； 在工程目录添加文本文件 COPYRIGHT，README； 在工程目录添加一个 runhello.sh 脚本，用来调用可执行文件 hello 将构建后的目标(object)文件放入构建目录的 bin 子目录 最终安装这些文件：将可执行文件 hello 与 runhello.sh 安装至 /usr/bin, 将doc 目录的内容以及 COPYRIGHT/README 安装到 /usr/share/doc/cmake/t2, 编译 准备工作\n在 /backup/cmake 目录下建立 /t2 目录，将 /t1 工程的 main.c 和 CMakeLists.txt 拷贝到 /t2 目录下。\n构建工程\n添加子目录 src：\n1 2 mkdir src mv main.c src\t#源代码放入 src 目录 进入 /t2/src，编写 CMakeLists.txt （需要为任何子目录建立一个 CMakeLists.txt）\n1 ADD_EXECUTABLE(hello main.c)\t#把mainc.c源码编译成一个名为hello的可执行文件 修改 /t2 工程目录下的CMakeLists.txt ，指定源代码文件夹和编译输出(包括中间结果：每个子文件夹下都有CMakeLists.txt，都会产生编译结果)文件夹\n1 2 PROJECT(HELLO) ADD_SUBDIRECTORY(src bin)\t#指定源码目录/t2/src，指定make编译结果放入/build/bin， ADD_SUBDIRECTORY(source_dir [binary_dir] [EXCLUDE_FORM_ALL])\n此指令用于向当前工程添加存放源文件的子目录(source_dir)，并可以指定编译输出存放的位置(binary_dir)。[EXCLUDE_FROM_ALL] 参数的含义是将这个目录从编译过程中排除，比如，工程的example，可能就需要工程编译完成后，再进入example 目录单独进行构建。\n如果不指定 bin 目录，编译结果（包括中间结果）都将存放在 build/src 目录下，指定 bin 目录后，相当于在编译时将 /build/src 重命名为 /bin。\n换个地方保存可执行文件和库文件\n不论是 SUBDIRS 还是 ADD_SUBDIRECTORY 指令(不论是否指定编译输出目录),我们都可以通过 SET 指令重新定义 EXECUTABLE_OUTPUT_PATH和 LIBRARY_OUTPUT_PATH 变量，来指定最终的目标二进制的位置（即最终生成的可执行文件 hello 或者最终的共享库,不包含编译生成的中间文件)\n1 2 SET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\t#即为/build/bin SET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)\t#即为/build/lib 在ADD_EXECUTABLE或ADD_LIBRARY后面，写这条指令。\n建立build，进入build目录，进行外部编译\n1 2 3 cd build cmake .. make 构建完成后，你会发现生成的可执行文件 hello 位于 build/bin 目录中。\nINSTALL指令 有两种安装方法，一种是从代码编译后直接 make install 安装，一种是打包时的指定目录安装。可以通过：\n1 make install\t#将 hello 直接安装到 /usr/bin 目录 或者：\n1 make install DESTDIR=/tmp/test\t#安装在/tmp/test/usr/bin 目录，打包时这个方式经常被使用。 稍微复杂一点的是还需要定义 PREFIX，一般autotools工程，会运行这样的指令：\n./configure -prefix=/usr 或者 **./configure --prefix=/usr/local**来指定 PREFIX。\n对于cmake来说，使用：\nINSTALL 指令：\n用于定义安装规则，安装的内容可以包括二进制、动态库、静态库以及文件、目录、脚本等：\n1 2 3 4 5 6 INSTALL(TARGETS \u0026lt;target\u0026gt;... [...])\t#安装二进制 INSTALL({FILES | PROGRAMS} \u0026lt;file\u0026gt;... [...]) INSTALL(DIRECTORY \u0026lt;dir\u0026gt;... [...]) INSTALL(SCRIPT \u0026lt;file\u0026gt; [...]) INSTALL(CODE \u0026lt;code\u0026gt; [...]) INSTALL(EXPORT \u0026lt;export-name\u0026gt; [...]) 有时候，也会用到一个非常有用的变量**CMAKE_INSTALL_PREFIX**，用于指定cmake install时的相对地址前缀。用法如：\n1 cmake -DCMAKE_INSTALL_PREFIX=/usr .. CMAKE_INSTALL_PREFIX变量类似与configure 脚本的 -prefix。\nINSTALL 指令的各种安装类型：\n1. 目标文件的安装： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INSTALL(TARGETS targets ...\t#各种目标文件 [EXPORT \u0026lt;export-name\u0026gt;] [[ARCHIVE|LIBRARY|RUNTIME|OBJECTS|FRAMEWORK|BUNDLE| PRIVATE_HEADER|PUBLIC_HEADER|RESOURCE] #目标文件类型 [DESTINATION \u0026lt;dir\u0026gt;]\t#指定各文件的安装目录\u0026lt;dir\u0026gt; [PERMISSIONS permissions...]\t#文件的权限 [CONFIGURATIONS [Debug|Release|...]]\t#指定安装规则适用的构建配置列表(DEBUG或RELEASE等) [COMPONENT \u0026lt;component\u0026gt;] [NAMELINK_COMPONENT \u0026lt;component\u0026gt;] [OPTIONAL] #如果要安装的文件不存在，则指定不是错误。 [EXCLUDE_FROM_ALL]\t#指定该文件从完整安装中排除，仅作为特定于组件的安装的一部分进行安装； [NAMELINK_ONLY|NAMELINK_SKIP] ] [...] [INCLUDES DESTINATION [\u0026lt;dir\u0026gt; ...]] ) 参数中的TARGET可以是很多种目标文件，最常见的是通过ADD_EXECUTABLE或者ADD_LIBRARY定义的目标文件，即可执行二进制、动态库、静态库：以下是默认的安装路径\n目标文件 内容 安装目录变量 默认安装文件夹 ARCHIVE 静态库 ${CMAKE_INSTALL_LIBDIR} lib LIBRARY 动态库 ${CMAKE_INSTALL_LIBDIR} lib RUNTIME 可执行二进制文件 ${CMAKE_INSTALL_BINDIR} bin PUBLIC_HEADER 与库关联的PUBLIC头文件 ${CMAKE_INSTALL_INCLUDEDIR} include PRIVATE_HEADER 与库关联的PRIVATE头文件 ${CMAKE_INSTALL_INCLUDEDIR} include 为了符合一般的默认安装路径，如果设置了DESTINATION参数，推荐配置在安装目录变量下的文件夹。\n例如：\n1 2 3 4 5 INSTALL(TARGETS myrun mylib mystaticlib RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} ) 上面的例子会将：可执行二进制myrun安装到${CMAKE_INSTALL_BINDIR}目录，动态库libmylib.so安装到${CMAKE_INSTALL_LIBDIR}目录，静态库libmystaticlib.a安装到${CMAKE_INSTALL_LIBDIR}目录。\nINSTALL命令的其他一些参数的含义：\nDESTINATION：指定磁盘上要安装文件的目录； PERMISSIONS：指定安装文件的权限。有效权限是OWNER_READ，OWNER_WRITE，OWNER_EXECUTE，GROUP_READ，GROUP_WRITE，GROUP_EXECUTE，WORLD_READ，WORLD_WRITE，WORLD_EXECUTE，SETUID和SETGID；（11种权限） CONFIGURATIONS：指定安装规则适用的构建配置列表(DEBUG或RELEASE等)； EXCLUDE_FROM_ALL：指定该文件从完整安装中排除，仅作为特定于组件的安装的一部分进行安装； OPTIONAL：如果要安装的文件不存在，则指定不是错误。 注意一下CONFIGURATIONS参数，此选项指定的值仅适用于此选项之后列出的选项：例如，要为调试和发布配置设置单独的安装路径，请执行以下操作：\n1 2 3 4 5 6 INSTALL(TARGETS target CONFIGURATIONS Debug RUNTIME DESTINATION Debug/bin) INSTALL(TARGETS target CONFIGURATIONS Release RUNTIME DESTINATION Release/bin) 也就是说，DEBUG和RELEASE版本的DESTINATION安装路径不同，那么DESTINATION必须在CONFIGUATIONS后面。\n2. 普通文件的安装 1 2 3 4 5 6 INSTALL(\u0026lt;FILES|PROGRAMS\u0026gt; files... TYPE \u0026lt;type\u0026gt; | DESTINATION \u0026lt;dir\u0026gt; [PERMISSIONS permissions...] [CONFIGURATIONS [Debug|Release|...]] [COMPONENT \u0026lt;component\u0026gt;] [RENAME \u0026lt;name\u0026gt;] [OPTIONAL] [EXCLUDE_FROM_ALL]) FILES|PROGRAMS若为相对路径给出的文件名，将相对于当前源目录进行解释。其中，FILES为普通的文本文件，PROGRAMS指的是非目标文件的可执行程序(如脚本文件)。\n如果未提供PERMISSIONS参数，默认情况下，普通的文本文件将具有OWNER_WRITE，OWNER_READ，GROUP_READ和WORLD_READ权限，即644权限；而非目标文件的可执行程序将具有OWNER_EXECUTE, GROUP_EXECUTE,和WORLD_EXECUTE，即755权限。\n其中，不同的TYPE，cmake也提供了默认的安装路径，如下表：\nTYPE类型 安装目录变量 默认安装文件夹 BIN ${CMAKE_INSTALL_BINDIR} bin SBIN ${CMAKE_INSTALL_SBINDIR} sbin LIB ${CMAKE_INSTALL_LIBDIR} lib INCLUDE ${CMAKE_INSTALL_INCLUDEDIR} include SYSCONF ${CMAKE_INSTALL_SYSCONFDIR} etc SHAREDSTATE ${CMAKE_INSTALL_SHARESTATEDIR} com LOCALSTATE ${CMAKE_INSTALL_LOCALSTATEDIR} var RUNSTATE ${CMAKE_INSTALL_RUNSTATEDIR} /run DATA ${CMAKE_INSTALL_DATADIR} INFO ${CMAKE_INSTALL_INFODIR} /info LOCALE ${CMAKE_INSTALL_LOCALEDIR} /locale MAN ${CMAKE_INSTALL_MANDIR} /man DOC ${CMAKE_INSTALL_DOCDIR} /doc 请注意，某些类型的内置默认值使用DATAROOT目录作为前缀，以CMAKE_INSTALL_DATAROOTDIR变量值为内容。\n该命令的其他一些参数的含义：\nDESTINATION：指定磁盘上要安装文件的目录； PERMISSIONS：指定安装文件的权限。有效权限是OWNER_READ，OWNER_WRITE，OWNER_EXECUTE，GROUP_READ，GROUP_WRITE，GROUP_EXECUTE，WORLD_READ，WORLD_WRITE，WORLD_EXECUTE，SETUID和SETGID； CONFIGURATIONS：指定安装规则适用的构建配置列表(DEBUG或RELEASE等)； EXCLUDE_FROM_ALL：指定该文件从完整安装中排除，仅作为特定于组件的安装的一部分进行安装； OPTIONAL：如果要安装的文件不存在，则指定不是错误； RENAME：指定已安装文件的名称，该名称可能与原始文件不同。仅当命令安装了单个文件时，才允许重命名。 3. 目录的安装 1 2 3 4 5 6 7 8 9 10 INSTALL(DIRECTORY dirs... TYPE \u0026lt;type\u0026gt; | DESTINATION \u0026lt;dir\u0026gt; [FILE_PERMISSIONS permissions...] [DIRECTORY_PERMISSIONS permissions...] [USE_SOURCE_PERMISSIONS] [OPTIONAL] [MESSAGE_NEVER] [CONFIGURATIONS [Debug|Release|...]] [COMPONENT \u0026lt;component\u0026gt;] [EXCLUDE_FROM_ALL] [FILES_MATCHING] [[PATTERN \u0026lt;pattern\u0026gt; | REGEX \u0026lt;regex\u0026gt;] [EXCLUDE] [PERMISSIONS permissions...]] [...]) 该命令将一个或多个目录的内容安装到指定的目的地，目录结构被逐个复制到目标位置。每个目录名称的最后一个组成部分都附加到目标目录中，但是可以使用后跟斜杠来避免这种情况，因为它将最后一个组成部分留空。这是什么意思呢？\n比如，DIRECTORY后面如果是abc意味着abc这个目录会安装在目标路径下，abc/意味着abc这个目录的内容会被安装在目标路径下，而abc目录本身却不会被安装。即，如果目录名不以\u0026quot;/\u0026ldquo;结尾，那么这个目录将被安装为目标路径下的abc，如果目录名以/结尾，代表将这个目录中的内容安装到目标路径，但不包括这个目录本身。\nFILE_PERMISSIONS和DIRECTORY_PERMISSIONS选项指定对目标中文件和目录的权限。如果指定了USE_SOURCE_PERMISSIONS而未指定FILE_PERMISSIONS，则将从源目录结构中复制文件权限。如果未指定权限，则将为文件提供在命令的FILES形式中指定的默认权限(644权限)，而目录将被赋予在命令的PROGRAMS形式中指定的默认权限(755权限)。\n可以使用PATTERN或REGEX选项以精细的粒度控制目录的安装，可以指定一个通配模式或正则表达式以匹配输入目录中遇到的目录或文件。PATTERN仅匹配完整的文件名，而REGEX将匹配文件名的任何部分，但它可以使用/和$模拟PATTERN行为。\n某些跟随PATTERN或REGEX表达式后的参数，仅应用于满足表达式的文件或目录。如：EXCLUDE选项将跳过匹配的文件或目录。PERMISSIONS选项将覆盖匹配文件或目录的权限设置。\n例如：\n1 2 3 4 5 6 INSTALL(DIRECTORY icons scripts/ DESTINATION share/myproj PATTERN \u0026#34;CVS\u0026#34; EXCLUDE PATTERN \u0026#34;scripts/*\u0026#34; PERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ) 这条命令的执行结果是：将icons目录安装到share/myproj，将scripts/中的内容安装到share/myproj，两个目录均不包含目录名为CVS的子目录，对于scripts/*的文件指定权限为OWNER_EXECUTE，OWNER_WRITE，OWNER_READ，GROUP_EXECUTE，GROUP_READ。\n4. 安装时脚本的运行 有时候需要在install的过程中打印一些语句，或者执行一些cmake指令：\n1 2 INSTALL([[SCRIPT \u0026lt;file\u0026gt;] [CODE \u0026lt;code\u0026gt;]] [COMPONENT \u0026lt;component\u0026gt;] [EXCLUDE_FROM_ALL] [...]) SCRIPT参数将在安装过程中调用给定的CMake脚本文件(即.cmake脚本文件)，如果脚本文件名是相对路径，则将相对于当前源目录进行解释。CODE参数将在安装过程中调用给定的CMake代码。将代码指定为双引号字符串内的单个参数。\n例如：\n1 INSTALL(CODE \u0026#34;MESSAGE(\\\u0026#34;Sample install message.\\\u0026#34;)\u0026#34;) 这条命令将会在install的过程中执行cmake代码，打印语句。\n安装 就是把文件复制到到指定目录下\n添加 doc 目录及文件：\n1 2 3 cd /backup/cmake/t2 mkdir doc\t#存储工程文档 touch doc/hello.txt **添加脚本：**在工程目录添加 runhello.sh，内容为：\n1 2 cd /home/jack/backup/cmake/t2/build/bin ./hello\t#调用可执行文件 **添加文件：**工程目录中的 COPYRIGHT 和 README\n1 touch COPYRIGHT README 修改 CMakeLists.txt ，使之可以支持各种文件的安装\n安装文档，修改工程目录下的 CMakeLists.txt\n1 2 3 4 5 # 安装 COPYRIGHT/README 到 /\u0026lt;prefix\u0026gt;/share/doc/cmake/t2 INSTALL(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2) # 安装 runhello.sh 到 /\u0026lt;prefix\u0026gt;/bin INSTALL(PROGRAMS runhello.sh DESTINATION bin) 安装 doc 中的 hello.txt , 两种方式：\n通过 doc 目录建立 CMakeLists.txt 并将 doc 目录通过 ADD_SUBDIRECTORY 加入工程来完成。\n直接在工程目录通过 INSTALL(DIRECTORY 来完成)：\n因为 hello.txt 要安装到 /\u0026lt;prefix\u0026gt;/share/doc/cmake/t2,所以我们不能直接安装整个 doc 目录，这里采用的方式是安装 doc 目录中的内容，也就是使用 \u0026quot; doc/ \u0026ldquo;。 在工程目录下的CMakeLists.txt 中添加：\n1 INSTALL(DIRECTORY doc/ DESTINATION share/doc/cmake/t2) 编译并安装\n进入build 目录进行外部编译，注意使用 CMAKE_INSTALL_PREFIX 参数，这里将它安装到了 /tmp/t2 目录:\n1 2 3 cmake -DCMAKE_INSTALL_PREFIX=/tmp/t2/usr .. make make install cd 进入 /tmp/t2 目录看以下安装结果：\n1 2 3 4 5 6 7 8 9 10 11 ./usr ./usr/share ./usr/share/doc ./usr/share/doc/cmake ./usr/share/doc/cmake/t2 ./usr/share/doc/cmake/t2/hello.txt ./usr/share/doc/cmake/t2/README ./usr/share/doc/cmake/t2/COPYRIGHT ./usr/bin ./usr/bin/hello ./usr/bin/runhello.sh 如果要直接安装到系统，可以使用如下指令：\n1 cmake -DCMAKE_INSTALL_PREFIX=/usr .. 如果没有额外定义，CMAKE_INSTALL_PREFIX 的默认定义是 /usr/local\n五、静态库与动态库构建 动态库、静态库与可执行文件的区别：\n动态链接库（Dynamic Link Library，缩写为DLL）是在程序运行时动态调用的，可以被其它应用程序共享的程序模块，其中封装了一些可以被共享的例程和资源。动态链接库文件的扩展名一般是dll，也有可能是drv、sys和fon，它和可执行文件（exe）非常类似，区别在于DLL中虽然包含了可执行代码却不能单独执行，而应由Windows应用程序直接或间接调用。\nLib称为静态链接库(static link library)，是在编译的链接期间使用的，他里面其实就是源文件的函数实现。Lib只是Dll的附带品，是DLL导出的函数列表文件而已。\nDll其实和Exe是几乎完全一样的，唯一的不一样就是Exe的入口函数式WinMain函数（console程序是main函数），而Dll是DllMain函数，其他完全是一样的。所以有人也戏称Dll是不能自己运行的Exe。\n静态链接是指把要调用的函数或者过程链接到可执行文件中，成为可执行文件的一部分(拷贝函数) 动态链接所调用的函数代码并没有被拷贝到应用程序的可执行文件中去，而是仅仅在其中加入了所调用函数的描述信息（往往是一些重定位信息）。仅当应用程序被装入内存开始运行时，在操作系统的管理下，才在应用程序与相应的DLL之间建立链接关系。当要执行所调用DLL中的函数时，根据链接产生的重定位信息，操作系统才转去执行DLL中相应的函数代码。 可执行文件和动态库之间的区别：可执行文件中有main函数，动态库中没有main函数，可执行文件可以被程序执行，动态库需要依赖程序调用者。\n本节任务：\n建立一个静态库和动态库，提供 HelloFunc 函数供其他程序编程使用，HelloFunc 向终端输出 Hello World 字符串。 安装头文件与共享库。 1. 准备工作 在 /back/cmake 目录下建立 t3 目录，用于存放本节工程\n1 2 3 4 5 6 cd /backup/cmake/t3 touch CMakeLists.txt\t#建立工程文件 mkdir lib\t#建立库文件夹 cd lib touch hello.c hello.h\t#在lib目录下建立两个源文件 touch CMakeLists.txt 2. 建立共享库 编辑工程目录下的 CMakeLists.txt 文件内容为：\n1 2 PROJECT(HELLOLIB)\t#定义工程名 ADD_SUBDIRECTORY(lib)\t#指定 库 的文件夹 编辑 lib文件夹下的 hello.h 内容如下：( #if、#ifdef、#ifndef 区别 )\n1 2 3 4 5 #ifndef HELLO_H\t//如果当前的宏未被定义，HELLO_H是宏名 #define HELLO_H\t//定义宏 #include \u0026lt;stdio.h\u0026gt;\t//引入头文件 void HelloFunc();\t//声明函数 #endif 编辑 lib文件夹下的 hello.c 内容为：\n1 2 3 4 5 #include \u0026#34;hello.h\u0026#34;\t//\u0026#34;\u0026#34;括住表示: 预处理程序先到当前目录下寻找文件，再到预定义的缺省路径(通常由INCLUDE环境变量指定)下寻找文件 void HelloFunc() { printf(\u0026#34;Hello world\\n\u0026#34;); } 编辑 /lib/CMakeLists.txt :\n1 2 SET(LIBHELLO_SRC hello.c)\t#定义变量 ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})\t#将指定的源文件编译成库 采用外部编译，在工程目录下建立一个 build 目录，\n1 2 3 4 mkdir build cd build cmake .. make 这时，在 /build/lib 目录下得到一个 libhello.so，这就是我们期望的共享库。\n如果要指定 libhello.so 生成的位置，可以通过在主工程文件 CMakeLists.txt 中修改 ADD_SUBDIRECTORY(lib) 指令来指定一个编译输出位置或者在 lib/CMakeLists.txt 中添加：\n1 SET(LIBRARY_OUTPUT_PATH \u0026lt;路径\u0026gt;)\t#指定新的位置 ADD_LIBRARY 主要作用就是将指定的源文件生成链接文件，然后添加到工程中去。\n1 2 3 4 ADD_LIBRARY(libname #生成的库文件的名字 [SHARED|STATIC|MODULE]\t#库文件类型 [EXCLUDE_FROM_ALL]\t#指定这个库不会被默认构建,除非有其他的组件依赖或者手动构建 source1 source2 ... sourceN)\t#各个源文件 SHARED 库：会被动态链接（动态链接库），在运行时会被加载。 STATIC 库：是目标文件的归档文件，在链接其它目标的时候使用。 MODULE 库：是一种不会被链接到其它目标中的插件，但是可能会在运行时使用dlopen-系列的函数。 3. 添加静态库 同样使用上面的指令，我们在支持动态库的基础上再为工程添加一个静态库，按照一般的习 惯，静态库名字跟动态库名字应该是一致的，只不过后缀是.a 罢了。\n添加静态库的指令：\n1 ADD_LIBRARY(hello STATIC ${LIBHELLO_SRC}) 然后再在 build 目录进行外部编译，我们会发现，静态库根本没有被构建，仍然只生成了 一个动态库。因为 hello 作为一个 target 是不能重名的，所以，静态库构建指令无效。\n如果我们把上面的 hello 修改为 hello_static:\n1 ADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) 就可以构建一个 libhello_static.a 的静态库了。\n但是这种结果显示不是我们想要的,我们需要的是名字相同的静态库和动态库,因为 target 名 称是唯一的,所以,我们肯定不能通过 ADD_LIBRARY 指令来实现了。这时候我们需要用到 另外一个指令:\n1 2 3 4 SET_TARGET_PROPERTITES(target1 target2 ... PROPERTIES prop1 value1 prop2 value2 ...) 这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。\n在本例中,我们需要作的是向 lib/CMakeLists.txt 中添加一条:\n1 SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \u0026#34;hello\u0026#34;) 这样，我们就可以同时得到 libhello.so 和 libhello.a 两个库了。\n与SET_TARGET_PROPERTIES对应的指令是：\n1 GET_TARGET_PROPERTY(VAR target property)\t#得到属性值 具体用法如下：我们向 lib/CMakeLists.txt 中添加：\n1 2 GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME) MESSAGE(STATUS \u0026#34;This is the hello_static OUTPUT_NAME:\u0026#34;${OUTPUT_VALUE}) 如果没有这个属性定义，则返回 NOTFOUND。\n4. 动态库版本号 按照规则，动态库是应该包含一个版本号的，我们可以看一下系统的动态库，一般情况是：\n1 2 3 libhello.so.1.2 libhello.so -\u0026gt;libhello.so.1 libhello.so.1-\u0026gt;libhello.so.1.2 为了实现动态库版本号,我们仍然需要使用 SET_TARGET_PROPERTIES 指令。 具体使用方法如下：\n1 SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1) VERSION ：指代动态库版本， SOVERSION：指代API 版本 将上述指令加入 lib/CMakeLists.txt 中,重新构建看看结果。 在 build/lib 目录会生成: libhello.so.1.2 libhello.so.1-\u0026gt;libhello.so.1.2 libhello.so -\u0026gt;libhello.so.1\n5. 安装共享库和头文件 以上面的例子，我们需要将 libhello.a， libhello.so.x 以及 hello.h 安装到系统目录，才能真正让其他人开发使用，在本例中我们将 hello 的共享库安装到 \u0026lt;prefix\u0026gt;/lib 目录，将 hello.h 安装到**\u0026lt;prefix\u0026gt;/include/hello** 目录。\n利用上一节了解到的 INSTALL 指令，我们向 lib/CMakeLists.txt 中添加如下指令:\n1 2 3 4 5 INSTALL(TARGETS hello hello_static LIBRARY DESTINATION lib ARCHIVE DESTINATION lib)\t#注意,静态库要使用 ARCHIVE 关键字 INSTALL(FILES hello.h DESTINATION include/hello) 终端输入：\n1 2 3 cmake -DCMAKE_INSTALL_PREFIX=/usr .. make make install\t#Permission denied 我们就可以将头文件和共享库安装到系统目录/usr/lib 和/usr/include/hello 中了。\n完整代码： lib/CMakeLists.txt\n1 2 3 4 5 6 7 8 9 10 11 12 SET(LIBHELLO_SRC hello.c) ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})\t#将源码编译成动态库 ADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC})\t#编译成静态库 SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \u0026#34;hello\u0026#34;) #重命名 GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME) #读取名字输出显示 MESSAGE(STATUS \u0026#34;This is the hello_static OUTPUT_NAME:\u0026#34;${OUTPUT_VALUE}) SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1) #版本号 INSTALL(TARGETS hello hello_static\t#安装动态库和静态库 LIBRARY DESTINATION lib ARCHIVE DESTINATION lib)\t#注意,静态库要使用 ARCHIVE 关键字 INSTALL(FILES hello.h\t#安装头文件 DESTINATION include/hello) 六、如何使用外部共享库和头文件 上一节我们已经完成了 libhello 动态库的构建以及安装，本节我们的任务很简单： 编写一个程序使用我们上一节构建的共享库。\n1. 准备工作 建立工程文件夹：在/backup/cmake 目录建立 t4 目录,本节所有资源将存储在 t4 目录。\n1 2 3 4 5 cd t4 touch CMakeLists.txt mkdir src cd src touch main.c CMakeList.txt 2. 编写源文件 建立 src 目录，编写源文件 main.c ：\n1 2 3 4 5 #include \u0026lt;hello.h\u0026gt; int main() { HelloFunc(); } 编辑工程目录下的 CMakeLists.txt：\n1 2 PROJECT(NEWHELLO) ADD_SUBDIRECTORY(src) 编辑 /src/CMakeLists.txt：\n1 ADD_EXECUTABLE(main main.c) 如果直接编译会出错：/backup/cmake/t4/src/main.c:1:19: error: hello.h: 没有那个文件或目录\n3. 引入头文件搜索路径 hello.h 位于 /usr/include/hello 目录中，并没有位于系统标准的头文件路径，为了让我们的工程能够找到 hello.h 头文件，我们需要引入一个新的指令：INCLUDE_DIRECTORIES，其完整的语法是：\n1 2 3 INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...) 这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的后面，你可以通过两种方式来进行控制搜索路径添加的方式:\nCMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以 将添加的头文件搜索路径放在已有路径的前面。 通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。 现在我们在 src/CMakeLists.txt 中添加一个头文件搜索路径，方式很简单，加入:\n1 INCLUDE_DIRECTORIES(/usr/include/hello) 进入 build 目录，重新进行构建，这是找不到 hello.h 的错误已经消失，但是出现了一个新的错误： main.c:(.text+0x12): undefined reference to HelloFunc\u0026rsquo;`\n这是因为我们并没有 link 到共享库 libhello 上。\n4. 为 target 链接共享库 我们现在需要完成的任务是将目标文件(target)链接到 libhello，这里我们需要引入两个新的指令**LINK_DIRECTORIES** 和 TARGET_LINK_LIBRARIES\n1 LINK_DIRECTORIES(directory1 directory2 ...) 这个指令非常简单，添加非标准的共享库搜索路径，比如，在工程内部同时存在共享库和可执行二进制,在编译时就需要指定一下这些共享库的路径。这个例子中我们没有用到这个指令。\n1 2 3 TARGET_LINK_LIBRARIES(target library1 \u0026lt;debug | optimized\u0026gt; library2 ...) 这个指令可以用来为 target 添加需要链接的共享库，本例中是一个可执行文件，但是同样可以用于为自己编写的共享库添加共享库链接。\n为了解决我们前面遇到的 HelloFunc 未定义错误,我们需要作的是向 src/CMakeLists.txt 中添加如下指令：\n1 2 3 TARGET_LINK_LIBRARIES(main hello)\t#hello 是共享库 libhello.so # 也可以写成 TARGET_LINK_LIBRARIES(main libhello.so) 进入 build 目录重新进行构建，就得到了一个链接到 libhello 的可执行程序 main，位于 build/src 目录，运行 main 的结果是输出：Hello world\n让我们来检查一下 main 的链接情况:\n1 ldd src/main\t#可以用來尋找此執行檔鏈接了哪一些函式庫 1 2 3 4 linux-gate.so.1 =\u0026gt; (0xb7ee7000) libhello.so.1 =\u0026gt; /usr/lib/libhello.so.1 (0xb7ece000) libc.so.6 =\u0026gt; /lib/libc.so.6 (0xb7d77000) /lib/ld-linux.so.2 (0xb7ee8000) 可以清楚的看到 main 确实链接了共享库 libhello，而且链接的是动态库 libhello.so.1\n那如何链接到静态库呢? 方法很简单: 将 TARGET_LINK_LIBRRARIES 指令修改为:\n1 TARGET_LINK_LIBRARIES(main libhello.a) 重新构建后再来看一下 main 的链接情况：\n1 ldd src/main 1 2 3 linux-gate.so.1 =\u0026gt; (0xb7fa8000) libc.so.6 =\u0026gt; /lib/libc.so.6 (0xb7e3a000) /lib/ld-linux.so.2 (0xb7fa9000) 说明,main 确实链接到了静态库 libhello.a\n特殊的环境变量: CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 务必注意,这两个是环境变量而不是 cmake 变量。 使用方法是要在 bash 中用 export 或者在 csh 中使用 set 命令设置或者 CMAKE_INCLUDE_PATH=/home/include cmake ..等方式。\n这两个变量主要是用来解决以前 autotools 工程中 --extra-include-dir 等参数的支持的。也就是,如果头文件没有存放在常规路径 (/usr/include, /usr/local/include 等)，则可以通过这些变量就行弥补。\n我们以本例中的 hello.h 为例，它存放在**/usr/include/hello** 目录,所以直接查找肯定是找不到的。前面我们直接使用了**绝对路径****INCLUDE_DIRECTORIES(/usr/include/hello)**告诉工程这个头文件目录。\n为了将程序更智能一点,我们可以使用 **CMAKE_INCLUDE_PATH**来进行，使用 bash 的方法如下：\n1 export CMAKE_INCLUDE_PATH=/usr/include/hello 然后在头文件中将 **INCLUDE_DIRECTORIES(/usr/include/hello)**替换为：\n1 2 3 4 FIND_PATH(myHeader hello.h) IF(myHeader) INCLUDE_DIRECTORIES(${myHeader}) ENDIF(myHeader) 上述的一些指令我们在后面会介绍。这里简单说明一下，FIND_PATH 用来在指定路径中搜索文件名，比如: FIND_PATH(myHeader NAMES hello.h PATHS /usr/include /usr/include/hello)\n这里我们没有指定路径，但是，cmake 仍然可以帮我们找到 hello.h 存放的路径，就是因为我们设置了环境变量 CMAKE_INCLUDE_PATH。\n如果你不使用 FIND_PATH，**CMAKE_INCLUDE_PATH**变量的设置是没有作用的，你不能指望它会直接为编译器命令添加参数 -I\u0026lt;CMAKE_INCLUDE_PATH\u0026gt;。\n以此为例，CMAKE_LIBRARY_PATH 可以用在 FIND_LIBRARY 中。\n同样,因为这些变量直接为 FIND_指令所使用,所以所有使用 FIND_指令的 cmake 模块都会受益。\n","date":"2021-01-06T00:00:00Z","image":"https://picx.zhimg.com/v2-42db546bd0f2ffdd462636ddd87ecdc3_1440w.jpg?source=172ae18b","permalink":"http://blog.zichen.uk/post/writenotes/lang/cmake_practice/","title":"memo: CMake实践"},{"content":"Frequency folding\nThe sampled signal\u0026rsquo;s frequency spectrum is mirrored by the sample frequency. 1-Steve\nRef Shannon Nyquist Sampling Theorem ","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aliasing/","title":""},{"content":"姓名：王子陈 学号：20171050008 专业：电子科学与技术 一、一维势阱电势分布 势阱底面电势为1,其余三面电势为0，求势阱内电势分布。\n因为阱内无电荷，所以求解拉普拉斯方程：\n$$ \\nabla^2\\phi=0 \\\\ \\frac{\\phi(x+\\Delta x,y)+\\phi(x-\\Delta x,y)-2\\phi(x,y)}{\\Delta x^2}+ \\frac{\\phi(x,y+\\Delta y)+\\phi(x,y-\\Delta y)-2\\phi(x,y)}{\\Delta y^2}=0 \\\\ [\\phi(x+\\Delta x,y)+\\phi(x-\\Delta x,y)-2\\phi(x,y)]\\Delta y^2= [-\\phi(x,y+\\Delta y)-\\phi(x,y-\\Delta y)+2\\phi(x,y)]\\Delta x^2 \\\\ \\phi(x,y)= \\frac{[\\phi(x+\\Delta x,y)+\\phi(x-\\Delta x,y)]\\Delta y^2+[\\phi(x,y+\\Delta y)+\\phi(x,y-\\Delta y)]\\Delta x^2} {2(\\Delta x^2+\\Delta y^2)} $$源代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import matplotlib.pyplot as plt xlength=1e-3 #x方向长度 1mm （单位 m） ylength=1e-3 #y方向长度 0.1mm n = 10 #100行 m = 10 #100列 phiArray=np.zeros((n,m)) phiArray[:,0]=1 #矩阵第一列置1 for i in range(10000): #左+右 phiArray[1:-1,1:-1]=((phiArray[1:-1,2:]+phiArray[1:-1,0:-2])*((xlength/n)**2) \\ +(phiArray[2:,1:-1]+phiArray[0:-2,1:-1])*((ylength/m)**2)) \\ /(2*((xlength/n)**2+(ylength/m)**2)) zeroArray=np.zeros((n,1)) for i in range(n): if( list(phiArray[:,i])==list(zeroArray[:,0])): print(i) break print(phiArray[:,m-2]) plt.pcolor(phiArray.T) #pcolor绘制的顺序是反的 plt.colorbar() plt.xlabel(\u0026#34;x\u0026#34;,fontsize=24) plt.ylabel(\u0026#34;y\u0026#34;,fontsize=24) plt.savefig(\u0026#39;./1x1mm_10x10_1wTimes\u0026#39;) plt.show() 改变 x 方向和 y 方向的长度以及 dx 和 dy (也就是 n 和 m)的值,计算、做图并分析这些参数对最终电势分布的影响。\n以下图像都迭代了一万次。\n1. 参照组 x长度=1mm, y长度=1mm, n=100格, m=100格\ndx=10$\\mu$m, dy=10$\\mu$m\n观察尺度比较小，x,y的观察范围都是1mm，每一格表示长度为10微米。\n2. y方向长度增加 x长度=1mm, y长度=1cm, n=100格, m=100格\ndx=10$\\mu$m, dy=0.1mm。\ny方向一格代表0.1毫米，跨度比较大，超过1毫米就看不出色彩变化了，其实还有很微弱的数值：第98行(1cm处)的数值量级在$10^{-25}-10^{-23}$\n3. y方向长度减小 x长度=1mm, y长度=0.1mm, n=100格， m=100格\ndx=10$\\mu$m，dy=100$\\mu$m\ny方向观测范围为0-0.1mm，在此范围内电势比较大\n4. 改变x方向长度变化不大 在x方向上，电势是均匀的，只是拉长了\n下图是x长度为1厘米的图像：\n5. m(行数)减少 x长度=1mm, y长度=1mm, n=100格, m=10格\ndx=10$\\mu$m，dy=0.1mm\ny方向上1格长度为0.1毫米，看起来没那么细腻，y方向相邻行有明显突变\n6. m(行数)增多 x长度=1mm, y长度=1mm, n=100格, m=1000格\ndx=10$\\mu$m, dy=1$\\mu$m\ny方向1格长度为1微米，没有颗粒感，更细腻\n7. n(列数)减少 x长度=1mm, y长度=1mm, n=10格, m=100格\nx方向1格长度100$\\mu$m，x方向有明显突变\n8. m, n都减少 x长度=1mm, y长度=1mm, n=10格, m=10格\nx方向和y方向1格都是距离0.1毫米，颗粒感比较强\n二、针尖放电 Point Discharge 盒子底面放一个针尖等势体，三面电势为0。（底边和红色区域电势为固定值）\n针尖：两条直线拼起来\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg #%matplotlib notebook xLength=1e-3 #x方向长度 1mm （单位 m） yLength=1e-3 #y方向长度 0.1mm n = 100 #矩阵100行 m = 100 #矩阵100列 dx = xLength/n #x方向一格长度 dy = yLength/m #y方向一格长度 phiArray=np.zeros((n,m)) volt = 0.1 #电势 height = yLength*0.5 #针尖高度 width = xLength*0.1 #1/2针尖宽度 slop_1 = height/width #直线1斜率 slop_2 = -slop_1 x1 = xLength/2-width #直线1截距 x1到原点距离 x2 = xLength/2+width #直线2截距 x2到原点距离 x1Num = x1/dx #截距x1所在网格序号 x2Num = x2/dx #截距x2所在网格序号 # 初始化等势体电势 def initialize_point(phiArray, slop_1, slop_2, x1Num, x2Num, volt): phiArray[:,0]=volt for i in range(n): for j in range(m): if ((j*dy/dx)\u0026lt;=slop_1*(i-x1Num)) \\ and ((j*dy/dx)\u0026lt;=slop_2*(i-x2Num)): phiArray[i][j]=volt # 迭代 initialize_point(phiArray, slop_1, slop_2, x1Num, x2Num, volt) for i in range(100): phiArray[1:-1,1:-1]=((phiArray[1:-1,2:]+phiArray[1:-1,:-2])*((xLength/n)**2) \\ +(phiArray[2:,1:-1]+phiArray[:-2,1:-1])*((yLength/m)**2)) \\ /(2*((xLength/n)**2+(yLength/m)**2)) #yLength\u0026lt;1时，yLength/m和xLength/n要换下位置 initialize_point(phiArray, slop_1, slop_2, x1Num, x2Num, volt) #等势体的电势每次迭代不能变 # 计算电场 Ex = -(phiArray[2:,1:-1]-phiArray[:-2,1:-1])/(2*dx) #电势在x方向梯度的相反数 Ey = -(phiArray[1:-1,2:]-phiArray[1:-1,:-2])/(2*dy) #电势在y方向梯度的相反数 plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (15.0, 5.0) plt.subplot(1,2,1) plt.pcolor(phiArray.T) plt.colorbar() plt.xlabel(\u0026#34;x\u0026#34;, fontsize = 25) plt.ylabel(\u0026#34;y\u0026#34;, fontsize = 25) plt.title(\u0026#34;电势分布\u0026#34;, fontsize = 25) plt.subplot(1,2,2) plt.pcolor((Ex**2+Ey**2)[round(0.2*n):round(0.7*n),:].T) plt.colorbar() plt.xlabel(\u0026#34;x\u0026#34;, fontsize = 25) plt.ylabel(\u0026#34;y\u0026#34;, fontsize = 25) plt.title(\u0026#34;电场强度\u0026#34;, fontsize = 25) plt.savefig(\u0026#34;./p1x1_100x100_0.5x0.2_v=0.1.png\u0026#34;) #命名的数字单位是mm print(\u0026#34;max E=\u0026#34;,np.max((Ex**2+Ey**2)[round(0.2*n):round(0.8*n),:])) 改变 x 方向和 y 方向的长度、dx 和 dy(也就是 n 和 m)的值、针尖的高度和宽度、电势大小等数据,计算、做图并分析这些参数对最终电势分布的影响。\n1. 参照组 x长度=1mm, y长度=1mm, n=100, m=100, height=0.5mm, width=0.2mm\n容器x方向长1毫米，y方向高1毫米，针高度为容器高度的一半：0.5毫米，针的1/2宽度为容器宽的0.1倍：0.1毫米，所以针宽0.2毫米\nx方向一个网格长dx=10$\\mu$m, y方向一个网格长dy=10$\\mu$m。电势分布及电场强度分布如下，针尖处具有最大场强为：\n$max E= 6.139\\times 10^{8}$ 2. 容器y方向加长到10毫米 x方向长度=1mm， y方向长度=10mm， n=100, m=100， dx=0.01mm， dy=0.1mm， height=5mm， width=0.2mm\n因为场强等于电势的微分： $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x})^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y\\ \\uparrow})^2\\downarrow $$ dy变大，场强变小。虽然分子上的变化率也变大了，但分母(把$\\phi$也差分)是三次的增速更快，整体变小。\n针高远大于针宽，像一条竖线，可能针尖端y方向的变化太小而忽略，类似于截顶\n$max E=1.144\\times 10^{8}$ 3. 容器y方向缩短到0.1毫米 xLength=1mm， yLength=0.1mm，n=100， m=100， dx=0.01mm， dy=0.001 m， height=0.05mm， width=0.2mm $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x})^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y\\ \\downarrow})^2\\uparrow $$ dy变小，场强变大。\n$max\\ E=1.065\\times 10^{10}$ 4. 容器x方向加长到10毫米 xLength=10mm， yLength=1mm， n=100， m=100， dx=0.1mm， dy=0.01mm， height=0.5mm， width=2mm. $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x\\ \\uparrow})^2\\downarrow+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y})^2 $$ dx变大，场强变小。\n$max\\ E=1.065\\times 10^{8}$\n5. 容器x方向减小到0.1毫米 xLength=0.1mm， yLength=1mm，n=100， m=100， dx=0.001mm， dy=0.01m， height=0.5mm， width=0.02mm $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x\\ \\downarrow})^2\\uparrow+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y})^2 $$ dx变小，场强变大。\n$max\\ E=1.144\\times 10^{10}$\n6. 增加针尖相对宽度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.5mm， width=1mm\n$$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\Delta x}\\downarrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\Delta y}\\downarrow)^2 $$ 分母没变，看分子：针尖附近x方向,y方向电势变化缓慢，所以针尖尖端场强变小。\n针尖的最大场强=$max E= 4.033\\times 10^{8}$\n7. 减小针尖相对宽度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.5mm， width=0.1mm\n容器x方向长度=1毫米，1/2针宽是容器宽度的0.05倍，所以针宽0.1毫米。\n针尖高度不变，宽度减小，针尖附近x方向，y方向电势变化加剧，所以针尖尖端强度变大。\n$max E=6.331\\times 10^{8}$\n8. 增加针尖相对高度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.7mm， width=0.1mm\n容器y方向高度=1毫米，针尖高度等于容器高度的0.7倍,即0.7毫米。\n针尖宽度不变，高度加长，相较于参照组针尖变尖锐了，针尖两侧电势变化加剧，针尖场强变大。\n$max E=6.264\\times 10^{8}$\n9. 减小针尖相对高度 xLength=1mm， yLength=1mm， n=100， m=100， dx=0.01mm， dy=0.01mm， height=0.7mm， width=0.1m\n容器y方向高度=1毫米，针尖高度等于容器高度的0.3倍,即0.3毫米，针尖宽度还是0.2毫米。\n针尖宽度不变，高度缩短，相较于参照组针尖变钝了，针尖两侧电势变化缓慢，针尖场强变小。\n$max E = 5.772\\times 10^{8}$\n10. 增加n(列)=200 xLength=1mm， yLength=1mm， n=200， m=100， dx=0.005mm， dy=0.01mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\uparrow} \\downarrow}\\uparrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m}})^2 $$n 增大，dx减小，Ex增大，场强变大：$max\\ E=1.219\\times 10^9$\n11. 减小n(列)=10 xLength=1mm， yLength=1mm， n=10， m=100， dx=0.1mm， dy=0.01mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\downarrow} \\uparrow}\\downarrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m}})^2 $$n 减小，dx增大，Ex减小，场强变小：$max\\ E=1.410\\times 10^8$\n12. 增加m(行)=200 xLength=1mm， yLength=1mm， n=100， m=200， dx=0.01mm， dy=0.005mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n} })^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\uparrow}\\downarrow}\\uparrow)^2 $$m 增大，dy减小，Ey增大，场强变大： $max\\ E=1.705\\times 10^9$\n13. 减小m(行)=10 xLength=1mm， yLength=1mm， n=100， m=10， dx=0.01mm， dy=0.1mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n} })^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\downarrow}\\uparrow}\\downarrow)^2 $$m 减小，dy增大，Ey减小，场强变小： $max\\ E=1.037\\times 10^8$\n14. 网格10x10 xLength=1mm， yLength=1mm， n=10， m=10， dx=0.1mm， dy=0.1mm， height=0.5mm， width=0.2m $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\downarrow}\\uparrow}\\downarrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\downarrow}\\uparrow}\\downarrow)^2 $$n,m 减小，dx,dy增大，Ex,Ey减小，场强变小： $max\\ E=2.479\\times 10^7$\n15. 网格200x200 x长度=1mm， y长度=1mm， n=200， m=200，dx=0.005， dy=0.005， height=0.5mm， width=0.2mm $$ E=(\\frac{\\phi(x+\\Delta x)-\\phi(x-\\Delta x)}{2\\frac{xLength}{n\\ \\uparrow}\\downarrow}\\uparrow)^2+(\\frac{\\phi(y+\\Delta y)-\\phi(y-\\Delta y)}{2\\frac{yLength}{m\\ \\uparrow}\\downarrow}\\uparrow)^2 $$ n，m 增大，dx,dy减小，Ex,Ey增大，场强变大：\n电场最强在针尖处，最大值为 $2.456\\times 10^{9}$\n16. 增大电势 等势体电势为10V，空间电势变大，针尖电势增大：$max\\ E=6.139\\times 10^{10}$（增大2个数量级）\n17. 减小电势 等势体电势为0.1V，空间电势变小，针尖电势减小：$max\\ E=6.139\\times 10^{6}$（减小2个数量级）\n","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/cem-dhy/%E6%95%B0%E5%80%BC%E4%BB%BF%E7%9C%9F/1_%E5%8A%BF%E9%98%B1%E9%92%88%E5%B0%96%E7%94%B5%E5%8A%BF%E5%88%86%E5%B8%83/%E4%BB%BF%E7%9C%9F1_%E7%94%B5%E5%8A%BF%E5%88%86%E5%B8%83-yi-alienware-aurora-r8/","title":""},{"content":"Reflect Mirror Reflect Metal Hot Metal References:\n烧到红热的镜子还能反射出镜像吗？ - bilibili - 自家学习用 Notes:\n(2024/12/06)\nThe heated metal can still perseve mirror reflecting r1-自家学习用 Diffuse 透射 Reference:\n最速降线问题(约翰·伯努利如何用光学解答) - bilibili - 马同学图解数学 这道简单的数学题彻底改变了物理学 - Veritasium真理元素 Pushed by bilibili Notes:\n(2024-12-07)\n约翰伯努利用光的性质来证明最速降线 r1-马同学 (2024-12-28)\n最小作用量 r2-真理（Maxwell 电磁通论的绪论中也写了对动量 mv 的描述）\n描述能量是除了描述受力的另一种描述方法 ","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/waveoptics/d-vid-misc/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/invrender/b-note-do/","title":""},{"content":"AI Quiz for recalling EWA Splatting\n(2025-03-23)\nMy Prompt:\nPlease generate some questions based on the attached notes to help me recall my understanding.\n/home/zichen/OneDrive/Exercises/WriteNotes/Model/Splat/B-note-EWA_Splatting.md Model: qwq-32B | LM Studio; Frontend: Cherry Studio\nOkay, I need to generate some questions based on the provided notes about EWA Splatting. Let me start by reading through the notes carefully to understand the key concepts and details.\nFirst, the title mentions \u0026ldquo;Points | EWA Splatting,\u0026rdquo; so the focus is on rendering point cloud data using Gaussian splats. The notes cover several sections like TL;DR, Previous Work, Splatting, Resampling, Rendering, EWA Splats, etc.\nLooking at the TL;DR section: It mentions that splatting uses a linear approximation of 3D Gaussians for projection and that there\u0026rsquo;s a comparison between NeRF and volume rendering approaches. The key points here are understanding how splatting differs from traditional methods like ray casting or surface rendering.\nUnder Splatting, it explains that point clouds are treated as reconstruction kernels (Gaussians) and aggregated to form the scene function. Important equations include the mixture model of weights using Gaussian kernels. The process involves projecting these Gaussians onto a screen space after applying transformations (viewing and projective). Footprint functions integrate along the line-of-sight, precomputing contributions for each kernel.\nResampling section discusses anti-aliasing with low-pass filters to avoid aliasing during sampling. It mentions Nyquist frequency and how splatting uses Gaussian filters for this purpose. The rendering equations involve alpha compositing with opacity terms derived from footprint functions. There are approximations made, like assuming constant color within local support and using Taylor expansion on the exponential term.\nEWA Splats specifically use 3D Gaussians transformed via affine mappings (viewing and projective transformations) to become 2D elliptical splats in screen space. The integration step turns them into footprint functions by integrating along depth, then convolving with a Gaussian filter for anti-aliasing.\nNow, I\u0026rsquo;ll try to think of questions that test understanding of these concepts. Questions should cover definitions, processes, comparisons between methods (like NeRF vs Splatting), key equations, and the approximations used in the splatting equation.\nPossible question areas:\nDefinitions \u0026amp; Key Concepts:\nWhat is EWA Splatting? How does it differ from traditional ray casting? Define \u0026ldquo;footprint function\u0026rdquo; and its role in rendering. Explain what a reconstruction kernel is and how it contributes to the scene function. Process Steps:\nOutline the steps involved in projecting a 3D Gaussian into screen space for EWA Splats. Describe how footprint functions are computed from 3D kernels during splatting. What transformations (viewing, projective) are applied to kernels before integrating? Mathematical Details:\nWrite the equation for the mixture model of reconstruction kernels in object space and explain each term. Derive or explain how the footprint function qₖ(𝐱) is obtained from rₖ'. What approximation allows replacing e^{-x} with 1-x in the splatting equation? Anti-Aliasing \u0026amp; Filtering:\nHow does Gaussian filtering help avoid aliasing in splatting? Explain the role of low-pass filters and how they interact with footprint functions. Why is the local support assumption important for computational efficiency? Comparisons:\nCompare NeRF\u0026rsquo;s rendering approach (ray marching) with EWA Splatting\u0026rsquo;s precomputed splats. Discuss advantages of using Gaussian kernels in splatting over other primitives. Approximations and Assumptions:\nWhat are the four key approximations made to derive the final splatting equation? How does assuming constant color within kernel support affect the result? Technical Details of Transformations:\nExplain how the affine approximation (Taylor expansion) handles non-linear perspective projection. Derive the covariance matrix of a transformed Gaussian after viewing and projective transformations. Implementation Considerations:\nWhy is precomputing footprint tables beneficial? How does screen space organization affect rendering efficiency in splatting? Now, phrasing these into specific questions that can be answered based on the notes:\nWhat are reconstruction kernels in EWA Splatting and how do they contribute to forming a continuous scene function? Explain the process of projecting a 3D Gaussian kernel from object space through viewing and projective transformations to screen space. What is the footprint function qₖ(𝐱) and why is it important for rendering? How is it computed? Describe how anti-aliasing is achieved in EWA Splatting using low-pass filters. What role does the Gaussian filter play here? Compare the alpha compositing approach in NeRF with that in splatting, highlighting key differences. Explain the four approximations made to simplify the volume rendering equation into the splatting equation. How is the perspective projection handled computationally when transforming kernels to screen space? What linear approximation is used? Why are Gaussian distributions preferred for reconstruction kernels in EWA Splats over other kernel shapes? What is the significance of precomputing footprint tables, and how does it improve rendering performance? Describe the steps involved in computing an anti-aliased splat (ρₖ(𝐱)) from a 3D Gaussian kernel. Here are some questions based on the provided notes to test your understanding of EWA Splatting:\nConceptual Understanding What is EWA Splats, and how does it differ from traditional volume rendering techniques like ray casting?\nHighlight key aspects such as precomputed footprints, Gaussian kernels, and anti-aliasing. Explain the role of \u0026ldquo;reconstruction kernels\u0026rdquo; in splatting. What properties must they satisfy to form a valid scene function?\nDiscuss their use in aggregating contributions across points and ensuring coverage (e.g., mixture weights). What is a footprint function \\( q_k(\\mathbf{x}) \\)? How does it relate to rendering opacity for a kernel?\nDefine its computation as an integral over the line-of-sight and its purpose in screen space. Mathematical Details \u0026amp; Equations Write the equation for the scene function \\( f_c(\\mathbf{u}) \\) using reconstruction kernels, and explain each component.\nHighlight the weight mixture \\( w_k r_k(\\mathbf{u}) \\), kernel functions \\( r_k \\), and their role in forming continuous attributes. Derive the expression for footprint function \\( q_k(\\mathbf{x}) \\). What integral does it represent, and why is it precomputed?\nShow how it integrates over depth (x₂) along the line-of-sight to summarize kernel contributions. What approximation allows replacing \\( e^{-w_j q_j} \\) with \\( 1 - w_j q_j \\) in the splatting equation? Why is this used?\nDiscuss Taylor expansion and simplification for computational efficiency. Transformations \u0026amp; Projections Describe how a 3D Gaussian kernel is transformed into screen space via viewing (\\( \\phi \\)) and projective (\\( \\psi \\)) transformations.\nMention affine approximation of perspective projection (e.g., Jacobian \\( J_k \\)), and resulting covariance matrices. What non-linear operation is approximated using a first-order Taylor expansion in projective transformations? Why is this necessary?\nFocus on handling perspective division (\\( t_0/t_2, t_1/t_2 \\)) and distance calculation for \\( x_2 \\). Anti-Aliasing \u0026amp; Filters How does Gaussian filtering help mitigate aliasing in splatting? What is the \u0026ldquo;Nyquist frequency\u0026rdquo; in this context?\nLink to pre-filtered footprint functions and low-pass filters. Explain how anti-aliased splats (\\( \\rho_k(\\mathbf{x}) \\)) are derived from kernel projections and filtering.\nCombine footprint integration, convolution with Gaussian \\( h \\), and color/transmittance terms. Comparisons \u0026amp; Practical Aspects How does EWA Splatting differ from NeRF in its rendering approach? What trade-offs exist between them?\nCompare alpha compositing (integral vs summation of precomputed footprints) and scene representation. Why are Gaussian kernels preferred over other distributions for reconstruction? How do their properties simplify calculations?\nDiscuss analytical solutions, affine transformations preserving shape, and integral simplifications. Implementation \u0026amp; Assumptions What are the four key approximations made in deriving the final splatting equation \\( g_c(\\mathbf{x}) \\)?\nList assumptions like local support, transmittance ordering, constant color within kernels, etc. How do precomputed footprint tables accelerate rendering? What optimizations do they enable?\nFocus on avoiding repeated integrals and allowing efficient lookup during compositing. Critical Thinking What limitations arise from assuming \u0026ldquo;local support\u0026rdquo; of kernels in EWA Splats? How might this lead to artifacts?\nDiscuss edge aliasing or missing contributions from distant overlapping kernels. If a 3D kernel’s Gaussian is very large, how would its footprint function \\( q_k \\) change? What rendering consequences follow?\nAddress dilated footprints, increased opacity contribution, and potential blurring/occlusion issues. Visual \u0026amp; Diagrammatic Understanding Sketch or describe the transformation of a 3D Gaussian kernel into an elliptical splat in screen space after affine projection.\nInclude mean shift, covariance ellipse orientation, and footprint integration direction along depth (x₂). How does the \u0026ldquo;line-of-sight\u0026rdquo; integral for \\( q_k(\\mathbf{x}) \\) differ from NeRF’s ray-marching sampling of points?\nContrast volume integration in splats vs discrete point evaluation. These questions cover key concepts, equations, transformations, and practical considerations outlined in your notes. They aim to test both theoretical understanding and the ability to connect mathematical steps with rendering outcomes!\n","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/q-recall-ewa_splatting/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/splats/img/","title":"image"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ala-nathan/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/aml/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/depth/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/diffusion/imgs/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/imagen/vae/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/subnetwork/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/transformers/transf-nickchen/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/games-101_cg/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/vis/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"http://blog.zichen.uk/post/writenotes/model/simu/img/","title":"img"}]
