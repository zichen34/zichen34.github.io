[{"content":"PDF | SemaSch\nNotes (2024-06-02)\nAbstract\nTask: Display point set surface.\nSolution:\nDefine a smooth manifold surface based on local maps and MLS.\nlocal map from differential geometry: \u0026ldquo;local\u0026rdquo;: neighbors Upsampling and downsampling to fit the points spacing with the screen space resolution.\nThe approximation error is bounded.\nAdvantage: Applicable for any point set\nIntroduction\nTask: Use point set as the shape representation\nWhy matter?\nPoint sets data are getting popular.\nRepresenting a highly detailed surface with primitives requires substantial small unit that\u0026rsquo;s smaller than a single pixel.\nProblem:\nAlthough point set is an economical representation, point sets contain noisy and redundant points.\nPoint cloud is discrete without explicit surface.\nSolution:\nAdjust the density of points to reconstruct a smooth surface.\nUse polynomials to approximate surface with MLS\nPolynomials: What are basis functions are used to fit the surface function? What does the surface function look like? Projection: weighted sum / linear combination of x,y coordinate\n$$ \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix} [1 \\quad x \\quad x^2] [^x_y] $$\nMLS: The partial derivative of error w.r.t. coefficient matrix ùêÄ equals 0.\nExplain idea:\nGenerating new points is a sampling process on the hidden surface.\nCan generation model be applied? as this is a sampling process. The error has a limited bound. Rendering a point set is achieved by up-sampling and down-sampling the point set.\nThe sampling error is bounded. How to prove? Reduce the input point set $P=\\{ùê©·µ¢\\}$ to \u0026ldquo;representation points\u0026rdquo; $R=\\{ùê´·µ¢\\}$ defining an MLS surface $S_R$, which approximates the original surface $S_P$.\nSounds like Variational Inference: Let an assumed posterior distribution of z $q(z|x)$ with learnable params (Œº,œÉ) to approximate the true intractable posterior distribution of z $P(z|x)$, through minimizing the KL-divergence.\nOn the other hand, if analyzing ELBO, the goal of maximizing the log-likelihood of x is transformed into maximizing the variational lower bound: ELBO.\nFiguratively, use a proxy object to approximate the true object.\nRelated Work\n(2024-06-07)\nConsolidation\nA practical point cloud includes multiple scans to capture the complete geometry for a nontrivial object.\nWeighted sum based on implicit function\nWeighted sum based on points directly\nTrianglulation Physically-based\nMLS + Projection + Iterations Point Sample Rendering\ntime-critical rendering\nGlobal illumination effects -\u0026gt; ray tracing technique\nResampling the surface in object space is better than interpolating in the screen space.\nHybrid triangle-point approaches\nMethod Define the Surface\nBasic assumption: The input point set represent a surface implicitly.\nMain idea:\nThe Projection Procedure: 2 steps\nStep ‚Ö†: Approximate a Reference domain (plane) for a point ùê´:\n$H =\\{ùê± | ‚ü®ùêß,ùê±‚ü©-D = 0, ùê±‚àà ‚Ñù¬≥\\}, ùêß‚àà ‚Ñù¬≥,‚Äñùêß‚Äñ=1$\nThe plane H is determined by MLS, i.e., minimizing a weighted sum of squared errors (distances) from each neighboring point ùê©·µ¢ of the point ùê´ to the approximate plane H.\nThe weight is a function Œ∏ of the distance between the projection ùê™ of ùê´ on the plane to each of its neighboring point ùê©·µ¢.\n$$ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ \\underset{error}{(‚ü®ùêß,ùê©·µ¢‚ü©-D)¬≤} ‚ãÖ \\underset{weights}{Œ∏(‚Äñùê©·µ¢-ùê™‚Äñ)} $$\nH ùê´ ùêß ùê™ ‚ã∞ ‚ã∞ w ‚ã∞ D ‚àô ùê© ·µ¢ ‚ã∞ O P o f r l f r i a f o g n s m i e e n t s N is the number of neighbors of ùê´.\nA plane in 3D is defined with its normal vector ùêß and a point ùê± within the plane, that deviates from the origin by a distance D along the direction of normal: $‚ü®ùêß,ùê±‚ü©=D$\nùêß p ùê± l a n e D ‚àô O r i g i n Further, represent the projection ùê™ with a parameter t:\n$$ùê™ = ùê´ + tùêß$$\nThe loss function: encouraging neighboring points ùê©·µ¢ close to the H, becomes:\n$$‚àë·µ¢‚Çå‚ÇÅ·¥∫ ‚ü®ùêß,ùê©·µ¢ - ùê´ - tùêß‚ü©^2 ‚ãÖ Œ∏(\\| ùê©·µ¢ - ùê´ - tùêß \\|)$$\nH t ùê´ ùêß ùêß ùê™ ‚ã∞ ‚ã∞ w ‚ã∞ ùê© ·µ¢ ‚ã∞ As the projection ùê™ is on the surface, the distance from ùê©·µ¢ to the surface is written as the inner product of ùêß and ùê©·µ¢. Denote the target plane H with the set of projections:\n$$Q(t) = ùê™ = ùê´ + tùêß$$\nThe t reminds me the sampling in NeRF: $ùê® + tùêù$. After rescaling the range of [near,far] to [-1,1], the t ranges from [0,1] to sample points starting from the near plane.\nt is not the depth z, but steps that are compatible for LLFF scene (infinite boundary) and Blender scene (bounded).\nSimilarly, Point-MVSNet used steps to refine the depth of the point along a ray.\nThe plane H will serve as a \u0026ldquo;ground\u0026rdquo; for measuring the height of each ùê©·µ¢, using the coordinates (x,y) on the plane H for indexing.\nStep ‚Ö°: approximate surface with Local map:\nThe projeciton ùìü of ùê´ onto the original surface $S_P$ is a sum of the projection of ùê´ on the plane H and the \u0026ldquo;residual\u0026rdquo; approximed with a bivariate polynomial: g(x,y),\n$$ \\begin{aligned} \\cal{P}(ùê´) \u0026amp;= ùê™ + g(0,0)ùêß \\\\ \u0026amp;= ùê´ + (t+g(0,0))ùêß \\end{aligned} $$\nSimilarly, PointFlow in Point-MVSNet also predicts the depth residual to tweak a point. The g(x,y) is optimized through minimizing the weighted sum of neigboring pionts ùê©·µ¢ height error:\n$$‚àë·µ¢‚Çå‚ÇÅ·¥∫ (g(x·µ¢,y·µ¢) - f·µ¢)¬≤ ‚ãÖ Œ∏(\\| ùê©·µ¢ - ùê™ \\|)$$\nl p f o l o c a r a n S l e ùê´ p H g ùê´ ( ( 0 0 ‚ãÆ ‚ãÆ ùê™ , , e 0 0 ‚ã∞ ) ) ‚ã∞ w ‚ã∞ g ùê© ‚ã∞ ( ( ·µ¢ x x ‚ãÆ ùê™ ·µ¢ ·µ¢ ·µ¢ , , f y y ·µ¢ ·µ¢ ·µ¢ ) ) f·µ¢ is the height of the point ùê©·µ¢ w.r.t. the plane H:\n$$f·µ¢= ùêß‚ãÖ(ùê©·µ¢ - ùê™)$$\nThe projection ùê™ of ùê´ is the origin of the plane-H coordinate system.\nThe projection of ùê´ on the original surface can be represented as: $ùê™+g(0,0)‚ãÖùêß$\nProperties of the Projection Procedure\n(2024-06-09)\nThe equation holds, when the plane H is the original surface $S_P$:\n$$\\cal{P}(\\cal{P}(ùê´)) = \\cal{P}(ùê´)$$\nl p f o l o c a r a n l e ùê´ g H ( 0 , 0 ùê´ ( ) 0 = ùê™ , 0 0 ‚ã∞ ) ‚ã∞ w ‚ã∞ g ùê© ‚ã∞ ( ( ·µ¢ x x ùê™ ·µ¢ ·µ¢ ·µ¢ , , f y y ·µ¢ ·µ¢ ·µ¢ ) ) O s = r u 0 i r g f i a n c a e l S p Computing the Projection\nData Structures and Tradeoffs\nResults\nPlay ","date":"2024-06-02T10:47:00Z","image":"https://i.ibb.co/2jd5S0V/Paper-computing-and-rendering-point-set-surface-7-Figure8-1.png","permalink":"https://zichen34.github.io/writenotes/model/shapes/b-note-voronoimls/","title":"read: Points - Surface | Voronoi + MLS"},{"content":"camtools (2024-03-28)\nyxlao/camtools - Github\nPlot the 49 camera poses in DTU:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import numpy as np import os def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min import camtools as ct import open3d as o3d Ks, Ts= [], [] for i in range(49): intrinsics, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan1\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) Ks.append(intrinsics) Ts.append(extrinsics) cameras = ct.camera.create_camera_frames(Ks, Ts) o3d.visualization.draw_geometries([cameras]) Result Demo in README Don\u0026rsquo;t know how to add camera frames like his. 1 2 3 4 5 6 0 1 2 3 4 10 9 8 7 6 5 11 12 13 14 15 16 17 18 27 26 25 24 23 22 21 20 19 28 29 30 31 32 33 34 35 36 37 48 47 46 45 44 43 42 41 40 39 38 CameraViewer (2024-03-28)\nxt4d/CameraViewer found by DDG when searching \u0026ldquo;how to visualize camera pose\u0026rdquo;\nIt uses plotly to visualize cameras internally.\n(2024-03-28)\nTest the poses in DTU.\nThe extrinsics (w2c) don\u0026rsquo;t appear on the canvas if keeping the translation vectors.\nI set the translation (camera pose), i.e., 4th column in extrinsics to all 0, then the rotations are shown.\n1 2 3 4 5 # /mnt/data2_z/MVSNet_testing/dtu/scan1/cams/{0:08d}_cam.txt array([[ 0.970263 , 0.00747983, 0.241939 , 0. ], [-0.0147429 , 0.999493 , 0.0282234 , 0. ], [-0.241605 , -0.030951 , 0.969881 , 0. ]], dtype=float32) The reason could be that the translation vector is too large: [-191.02, 3.28832, 22.5401].\nBy reducing it by 1/100 times (extrinsics[:,3] = extrinsics[:,3]/100): [-1.9102, 0.0328832, 0.225401], the camera appears.\nThe w2c (extrinsics) can be prepared as npy files:\n1 2 3 4 5 for i in range(49): intrinsics, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan1\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) extrinsics = extrinsics[:3] extrinsics[:,3] = extrinsics[:,3]/100 np.save(f\u0026#39;/mnt/data2_z/Poses_CamViewer/obj/poses/{i:03d}\u0026#39;, extrinsics) BTW, writing json file manually is a time black hole.\nAs long as the filenames of poses and images are the same, it\u0026rsquo;s okay. The indexing doesn\u0026rsquo;t matter.\n1 ~/Downloads/CameraViewer$ python app.py --root /mnt/data2_z/Poses_CamViewer/obj/ --type w2c --image_size 128 If omitting the argument --type, the program will use poses.json. Otherwise, the program will read directories: poses/ and images/. 49 cameras for scan1\nThe above figure shows the original poses. And the principal axis is facing away from the object. pytransform3d (2024-03-29)\nPlot mesh and cameras:\nVisualizing camera trajectory in Open3D #148 (Found when searching \u0026ldquo;open3d visualize camera poses\u0026rdquo; DDG)\nThe mesh in the image is produced by Meshroom. And then use Figure.plot_camera()\npytransform3d.camera.plot_camera() Example\nCode for plotting pose 1 of DTU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import matplotlib.pyplot as plt import pytransform3d.camera as pc import pytransform3d.transformations as pt w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) c2w = np.linalg.inv(w2c) intrinsics = np.array([ [ 2.89233051e+03, -2.48063349e-04, 8.23205273e+02], [ 0.00000000e+00, 2.88317528e+03, 6.19070918e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]) sensor_size = np.array([1600, 1200]) # image size virtual_image_distance = 1 ax = pt.plot_transform(A2B=c2w, s=0.2) ax.set_xlim(186, 188) ax.set_ylim(3, 5) ax.set_zlim(20, 22) pc.plot_camera( ax, cam2world=c2w, M=intrinsics, sensor_size=sensor_size, virtual_image_distance=virtual_image_distance) plt.show() pytransform3d matplotlib In the matplotlib code, I have corrected the camera position to -extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1]. The 2 results are the same. Plot basis and camera plane:\nCamera Extrinsic Matrix with Example in Python - Part2\n1 2 3 4 5 6 7 # plot the global basis and the transformed camera basis ax = pr.plot_basis(ax) ax = pr.plot_basis(ax, R, offset) # plot the original and transformed image plane ax.plot_surface(xx, yy, Z, alpha=0.75) ax.plot_surface(xxt, yyt, Zt, alpha=0.75) Matplotlib (2024-03-30)\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 %matplotlib widget import os import numpy as np def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min poses_list = [] for i in range(49): _, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan23\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) poses_list.append({ # \u0026#34;position\u0026#34;: extrinsics[:,-1][:-1], # Wrong \u0026#34;position\u0026#34;: - extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1], \u0026#34;rotation\u0026#34;: extrinsics[:-1][:,:-1], }) import matplotlib.pyplot as plt from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection def plot_camera_poses(poses, axis_length=0.1): fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # Plot each camera pose for pose in poses: # Extract camera position and orientation cam_position = pose[\u0026#39;position\u0026#39;] cam_orientation = pose[\u0026#39;rotation\u0026#39;] # Plot camera position ax.scatter(cam_position[0], cam_position[1], cam_position[2], c=\u0026#39;r\u0026#39;, marker=\u0026#39;.\u0026#39;) axes_endpoints = cam_position + axis_length * cam_orientation # Plot camera canvas determined by 4 corners (forming 2 triangles) corner_back_1 = axes_endpoints[2] + 0.3*axis_length * cam_orientation[1] + 0.3*axis_length * cam_orientation[0] corner_back_2 = axes_endpoints[2] - 0.3*axis_length * cam_orientation[1] + 0.3*axis_length * cam_orientation[0] corner_back_3 = axes_endpoints[2] + 0.3*axis_length * cam_orientation[1] - 0.3*axis_length * cam_orientation[0] corner_back_4 = axes_endpoints[2] - 0.3*axis_length * cam_orientation[1] - 0.3*axis_length * cam_orientation[0] verts = np.array([[ corner_back_1, corner_back_2, corner_back_4]]) tri = Poly3DCollection(verts, alpha=0.3, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) verts = np.array([[ corner_back_1, corner_back_3, corner_back_4]]) tri = Poly3DCollection(verts, alpha=0.3, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) # Hull of the camera ax.plot3D([cam_position[0], corner_back_1[0]], [cam_position[1], corner_back_1[1]], [cam_position[2], corner_back_1[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_2[0]], [cam_position[1], corner_back_2[1]], [cam_position[2], corner_back_2[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_3[0]], [cam_position[1], corner_back_3[1]], [cam_position[2], corner_back_3[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_4[0]], [cam_position[1], corner_back_4[1]], [cam_position[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) # Camera plane edges ax.plot3D([corner_back_1[0], corner_back_2[0]], [corner_back_1[1], corner_back_2[1]], [corner_back_1[2], corner_back_2[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_3[0], corner_back_1[0]], [corner_back_3[1], corner_back_1[1]], [corner_back_3[2], corner_back_1[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_3[0], corner_back_4[0]], [corner_back_3[1], corner_back_4[1]], [corner_back_3[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_2[0], corner_back_4[0]], [corner_back_2[1], corner_back_4[1]], [corner_back_2[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) lines = [] for idx in range(len(poses)-1): lines.append([poses[idx][\u0026#39;position\u0026#39;], poses[idx+1][\u0026#39;position\u0026#39;]]) # Create a line collection lc = Line3DCollection(lines, colors=\u0026#39;b\u0026#39;, linewidths=1, label=\u0026#39;Camera Trajectory\u0026#39;) ax.add_collection3d(lc) # Set plot limits and labels ax.set_box_aspect([1, 1, 1]) ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) plt.show() return ax # Plot camera poses ax_prev = plot_camera_poses(poses_list, axis_length=60) # Plot point cloud import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/mnt/data2_z/SampleSet/MVS Data/Points/stl/stl023_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vs = np.asarray(pcd.points) samples = vs[np.random.choice(vs.shape[0],100)] x = samples[:,0] y = samples[:,1] z = samples[:,2] ax_prev.scatter(x, y, z, color=\u0026#39;gray\u0026#39;, alpha=0.4) The Line3DCollection usage is seen from an chatGPT-generated anwser: How to visualize colmap export that Camera parameters -SO open3d camera hull (2024-03-29)\ncreate_camera_visualization()\nSample code: Is there a way to draw a camera in a visualizer? #3876 (Found when searching \u0026ldquo;open3d draw cameras\u0026rdquo; DDG)\ncamera moves (2024-03-31)\nSetting the extrinsic matrix for ViewControl #2121 - Open3D (surfaced by DDG searching \u0026ldquo;open3d camera extrinsic set_extrinsic\u0026rdquo;)\nIterate multiple camera poses (extrinsics):\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np import os def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min poses_list = [] for i in range(49): _, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan23\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) poses_list.append({ \u0026#34;position\u0026#34;: - extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1], \u0026#34;rotation\u0026#34;: extrinsics[:-1][:,:-1], \u0026#34;extrinsics\u0026#34;: extrinsics, }) import time import itertools import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window(window_name=\u0026#34;Playback\u0026#34;, visible=True) vis.get_render_option().background_color = np.asarray([0, 0, 0]) vis.add_geometry(pcd) ctr = vis.get_view_control() cam = ctr.convert_to_pinhole_camera_parameters() for view_idx, camParams in zip(itertools.count(), poses_list): cam.extrinsic = camParams[\u0026#39;extrinsics\u0026#39;] ctr.convert_from_pinhole_camera_parameters(cam, True) vis.poll_events() vis.update_renderer() time.sleep(0.1) vis.destroy_window() Note: The argument allow_arbitrary=True is required in convert_from_pinhole_camera_parameters(cam, True) (using 0.18.0), Custom Animation Customized visualization - Open3D Docs\nCode from View Control No Effect in 0.17 #6098\n1 2 3 4 5 6 7 8 9 import open3d as o3d def rotate(vis): ctr = vis.get_view_control() ctr.rotate(5, 0) return False frame = o3d.geometry.TriangleMesh.create_coordinate_frame() o3d.visualization.draw_geometries_with_animation_callback([frame], rotate) Others demul/extrinsic2pyramid\nOpenCV has example code. How to plot the camera and image positions from camera calibration data? pytorch3d/docs/tutorials/utils/camera_visualization.\nsxyu/nerfvis: NeRF visualization library under construction\nBlender add-on: Photogrammetry-Importer How to visualize colmap export \u0026lsquo;images.txt\u0026rsquo; in blender? -SO\nkaolin.render.camera ‚Äî Kaolin documentation - Read the Docs\nWebGL Visualizing the Camera\nOpenCV (2024-04-01)\nOpenCV: cv::viz::WCameraPosition Class Reference\nDraw coordinates axes\nHow to draw 3D Coordinate Axes with OpenCV for face pose estimation? - SO\n1 2 scale = 0.1 img = cv2.drawFrameAxes(img, K, distortion, rotation_vec, translation_vec, scale) ","date":"2024-03-28T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/camera_plots/","title":"memo: Vis | Camera Poses Visualization"},{"content":"Awesome (2024-05-10)\nzishun/awesome-geometry-processing\n(Found by Perplexity: \u0026ldquo;Compare 3 libraries: CGAL (Computational Geometry Algorithms Library), Open3D, and PointCloudLibrary (PCL)\u0026rdquo;)\nplyfile (2024-03-27)\nRead ply file (Polygon File Format): Docs\n1 2 3 4 5 6 7 import numpy from plyfile import PlyData, PlyElement with open(\u0026#39;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#39;, \u0026#39;rb\u0026#39;) as f: plydata = PlyData.read(f) np.array(plydata.elements[0].data)[0] Output:\n1 (49.720848, -54.11675, 672.04956, 0.9649841, -0.08213623, -0.24911714, 102, 70, 44) The returned tuple is a single data, whose datatype has 9 members. I want to only take the first 3 values: x,y,z. But it\u0026rsquo;s not allowed to use syntax like [:3] to slice it.\nWrite a ply file\nExample in 3DGS:\nOpen3D Homepage\n(2024-03-27)\nCompare open3d, plyfile, pyntcloud, and meshio\nopen3d: has good docs. How To Read and Write ply File in Python - Jdhao plyfile: lightweight. Could be slow when reading a large ply file. Python plyfile vs pymesh - SO open3d read .ply file Docs\n1 2 3 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) Convert pcd to np.array and visualize it: Docs\n1 2 3 4 import numpy as np xyz_load = np.asarray(pcd.points) # (2880879, 3) print(f\u0026#39;xyz_load:\\n {xyz_load}\u0026#39;) o3d.visualization.draw_geometries([pcd]) Setting camera directions:\n1 2 3 4 lookat = np.array([[500.],[500.], [500.]]) up = np.array([[0.853452],[-0.447425], [0.267266]]) front = np.array([[0.417749],[0.893913],[0.162499]]) o3d.visualization.draw_geometries([pcd], width=500, height=500, lookat=lookat, up=up, front=front, zoom=1.0) open3d.visualization.draw_geometries\nlookat is the window center. set_lookat(), set_front(), set_up() usage of VisualControl #2139\nVisualize point cloud from a specified camera pose:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([1,1,1]) vis.add_geometry(pcd) view_ctl = vis.get_view_control() w2c = np.array([[-0.636298, -0.727666, 0.25618, -143.534], [0.0315712, 0.307237, 0.951109, -579.42], [-0.770797, 0.613276, -0.172521, 759.831], [ 0.0, 0.0, 0.0, 1.0]]) # cam 38 cam = view_ctl.convert_to_pinhole_camera_parameters() cam.extrinsic = w2c view_ctl.convert_from_pinhole_camera_parameters(cam, True) vis.run() vis.destroy_window() Triangulation open3d.geometry.TriangleMesh - Docs\nBall-Pivoting Ref: 5-Step Guide to generate 3D meshes from point clouds with Python - Medium - Florent Poux, Ph.D\nBasic code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import open3d as o3d import numpy as np pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/image_ref/scan1/points3d.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) distances = pcd.compute_nearest_neighbor_distance() avg_dist = np.mean(distances) # The radius of the ball should be larger than the avg distance between points radius = 3 * avg_dist # Ball-Pivoting Algorithm bpa_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(pcd,o3d.utility.DoubleVector([radius, radius * 2])) dec_mesh = bpa_mesh.simplify_quadric_decimation(100000) dec_mesh.remove_degenerate_triangles() dec_mesh.remove_duplicated_triangles() dec_mesh.remove_duplicated_vertices() dec_mesh.remove_non_manifold_edges() o3d.io.write_triangle_mesh(\u0026#34;bpa_mesh.ply\u0026#34;, dec_mesh) Open the .ply file with ParaView:\nThe ball-pivoting algorithm result is better than possion on this point cloud. Retrieve triangles vertices:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 print(np.asarray(dec_mesh.triangles).shape) print(dec_mesh.triangles[0]) print(np.asarray(dec_mesh.vertices).shape) print(dec_mesh.vertices[100567]) # print(np.asarray(dec_mesh.triangle_uvs).shape) print(dec_mesh) vertices = np.asarray(dec_mesh.vertices) # (220_583, 3) vertices = torch.from_numpy(vertices) triangles = np.asarray(dec_mesh.triangles) # (100_000, 3) triangles = torch.from_numpy(triangles) K = 3 # number of vertices # Coordinates of 3 vertices of each triangle nnCoords = vertices.gather(dim=0, index=triangles.reshape(-1,1).expand(-1,3).type(torch.int64)).view(-1,K,3) # (100_000,K,3) Poisson Basic Code:\n1 2 3 4 5 6 poisson_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=8, width=0, scale=1.1, linear_fit=False)[0] bbox = pcd.get_axis_aligned_bounding_box() p_mesh_crop = poisson_mesh.crop(bbox) o3d.io.write_triangle_mesh(\u0026#34;p_mesh_c.ply\u0026#34;, p_mesh_crop) Visualization in ParaView\nDon\u0026rsquo;t know if the normals of the input .ply caused the Poisson reconsturction bad. It seems like a sheet is blown up. Delaunay (2024-05-13)\nSearch: \u0026ldquo;pointcloudlibrary triangulation\u0026rdquo; in DDG\n\u0026ldquo;how to create mesh in delaunay triangulation using pcl library\u0026rdquo; IntelRealSense-github An alternative is using CloudCompare. Palissy ware project Other Libs pyminiply claims that it\u0026rsquo;s faster than open3d.\nply-parser\nStitch Clouds (2024-04-01)\nStitching point clouds from multiple cameras - camcalib\nWis3D (2024-04-07)\nzju3dv/Wis3D\nPCL PCL Tutorial(2014) Jeff Delmerico He have projects on volumetric reconstruction Compile Library CPU ‚úÖ (2024-05-11)\nReference: Docs\nSystem:\nUbuntu 22.04.4 LTS x86_64, Kernel: 6.5.0-28-generic, gcc (Ubuntu 9.5.0-1ubuntu1~22.04) 9.5.0 Cudatoolkit: cuda_11.6.r11.6/compiler.30794723_0 Nvidia Driver: 545.23.08 GPU: 3090Ti CPU: AMD Ryzen 7 5700X (16) @ 3.400GHz Board: X570 AORUS PRO WIFI -CF. Memory: 16GB. Download source.tar.gz (pcl-1.14.1)\nUncompress: tar xvf source.tar.gz\nBuild:\n1 2 cd pcl cmake -B ./build Compile and install:\n1 2 make -j2 sudo make -j2 install The library is installed in /usr/lib and /usr/include/\nInstall paths 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 -- Installing: /usr/local/lib/libpcl_kdtree.so.1.14.1 -- Installing: /usr/local/lib/libpcl_kdtree.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_kdtree.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_kdtree.so -- Installing: /usr/local/lib/pkgconfig/pcl_kdtree.pc -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/kdtree.h -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/io.h -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/kdtree_flann.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/impl/io.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/kdtree/impl/kdtree_flann.hpp -- Installing: /usr/local/lib/libpcl_octree.so.1.14.1 -- Installing: /usr/local/lib/libpcl_octree.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_octree.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_octree.so -- Installing: /usr/local/lib/pkgconfig/pcl_octree.pc -- Installing: /usr/local/include/pcl-1.14/pcl/octree/octree_base.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/octree/impl/octree_base.hpp ... -- Installing: /usr/local/lib/libpcl_search.so.1.14.1 -- Installing: /usr/local/lib/libpcl_search.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_search.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_search.so -- Installing: /usr/local/lib/pkgconfig/pcl_search.pc -- Installing: /usr/local/include/pcl-1.14/pcl/search/search.h -- Installing: /usr/local/include/pcl-1.14/pcl/search/kdtree.h ... -- Installing: /usr/local/lib/libpcl_sample_consensus.so.1.14.1 -- Installing: /usr/local/lib/libpcl_sample_consensus.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_sample_consensus.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_sample_consensus.so -- Installing: /usr/local/lib/pkgconfig/pcl_sample_consensus.pc -- Installing: /usr/local/include/pcl-1.14/pcl/sample_consensus/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/sample_consensus/impl/mlesac.hpp -- Installing: /usr/local/lib/libpcl_filters.so.1.14.1 -- Installing: /usr/local/lib/libpcl_filters.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_filters.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_filters.so -- Installing: /usr/local/lib/pkgconfig/pcl_filters.pc -- Installing: /usr/local/include/pcl-1.14/pcl/filters/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/filters/impl/farthest_point_sampling.hpp -- Installing: /usr/local/lib/pkgconfig/pcl_2d.pc -- Installing: /usr/local/include/pcl-1.14/pcl/2d/convolution.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/2d/impl/convolution.hpp ... -- Installing: /usr/local/lib/pkgconfig/pcl_geometry.pc -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/eigen.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/triangle_mesh.h -- Installing: /usr/local/include/pcl-1.14/pcl/geometry/impl/polygon_operations.hpp -- Installing: /usr/local/lib/libpcl_io_ply.so.1.14.1 -- Installing: /usr/local/lib/libpcl_io_ply.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_io_ply.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_io_ply.so -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/byte_order.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/io_operators.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/ply.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/ply/ply_parser.h -- Installing: /usr/local/lib/libpcl_io.so.1.14.1 -- Installing: /usr/local/lib/libpcl_io.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_io.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_io.so -- Installing: /usr/local/lib/pkgconfig/pcl_io.pc -- Installing: /usr/local/include/pcl-1.14/pcl/io/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/io/eigen.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/compression/octree_pointcloud_compression.h -- Installing: /usr/local/include/pcl-1.14/pcl/compression/color_coding.h -- Installing: /usr/local/include/pcl-1.14/pcl/compression/compression_profiles.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/io/impl/ascii_io.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/io/impl/pcd_io.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/io/impl/octree_pointcloud_compression.hpp -- Installing: /usr/local/lib/libpcl_features.so.1.14.1 -- Installing: /usr/local/lib/libpcl_features.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_features.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_features.so -- Installing: /usr/local/lib/pkgconfig/pcl_features.pc -- Installing: /usr/local/include/pcl-1.14/pcl/features/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/features/boundary.h -- Installing: /usr/local/include/pcl-1.14/pcl/features/range_image_border_extractor.h -- Installing: /usr/local/include/pcl-1.14/pcl/features/impl/board.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/features/impl/boundary.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/features/impl/range_image_border_extractor.hpp -- Installing: /usr/local/lib/libpcl_ml.so.1.14.1 -- Installing: /usr/local/lib/libpcl_ml.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_ml.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_ml.so -- Installing: /usr/local/lib/pkgconfig/pcl_ml.pc -- Installing: /usr/local/include/pcl-1.14/pcl/ml/feature_handler.h -- Installing: /usr/local/include/pcl-1.14/pcl/ml/multi_channel_2d_comparison_feature.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/dt/decision_forest.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/ferns/fern.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/impl/dt/decision_forest_evaluator.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/ml/impl/svm/svm_wrapper.hpp -- Installing: /usr/local/lib/libpcl_segmentation.so.1.14.1 -- Installing: /usr/local/lib/libpcl_segmentation.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_segmentation.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_segmentation.so -- Installing: /usr/local/lib/pkgconfig/pcl_segmentation.pc -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/extract_clusters.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/impl/extract_clusters.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/segmentation/impl/cpc_segmentation.hpp -- Installing: /usr/local/lib/libpcl_surface.so.1.14.1 -- Installing: /usr/local/lib/libpcl_surface.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_surface.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_surface.so -- Installing: /usr/local/lib/pkgconfig/pcl_surface.pc -- Installing: /usr/local/include/pcl-1.14/pcl/surface/boost.h -- Installing: /usr/local/include/pcl-1.14/pcl/surface/eigen.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/surface/poisson.h -- Installing: /usr/local/include/pcl-1.14/pcl/surface/3rdparty/poisson4/allocator.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/surface/impl/gp3.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/surface/impl/grid_projection.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/surface/impl/marching_cubes.hpp ... -- Installing: /usr/local/lib/libpcl_registration.so.1.14.1 -- Installing: /usr/local/lib/libpcl_registration.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_registration.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_registration.so -- Installing: /usr/local/lib/pkgconfig/pcl_registration.pc -- Installing: /usr/local/include/pcl-1.14/pcl/registration/eigen.h -- Installing: /usr/local/include/pcl-1.14/pcl/registration/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/transforms.h -- Installing: /usr/local/include/pcl-1.14/pcl/registration/transformation_estimation.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/gicp.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/impl/default_convergence_criteria.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/registration/impl/correspondence_estimation.hpp ... -- Installing: /usr/local/include/pcl-1.14/pcl/registration/impl/transformation_estimation_3point.hpp -- Installing: /usr/local/lib/libpcl_keypoints.so.1.14.1 -- Installing: /usr/local/lib/libpcl_keypoints.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_keypoints.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_keypoints.so -- Installing: /usr/local/lib/pkgconfig/pcl_keypoints.pc -- Installing: /usr/local/include/pcl-1.14/pcl/keypoints/keypoint.h -- Installing: /usr/local/include/pcl-1.14/pcl/keypoints/narf_keypoint.h -- Installing: /usr/local/include/pcl-1.14/pcl/keypoints/sift_keypoint.h ... -- Installing: /usr/local/lib/libpcl_tracking.so.1.14.1 -- Installing: /usr/local/lib/libpcl_tracking.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_tracking.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_tracking.so -- Installing: /usr/local/lib/pkgconfig/pcl_tracking.pc -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/tracking.h -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/tracker.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/impl/tracking.hpp -- Installing: /usr/local/include/pcl-1.14/pcl/tracking/impl/tracker.hpp ... -- Installing: /usr/local/lib/libpcl_recognition.so.1.14.1 -- Installing: /usr/local/lib/libpcl_recognition.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_recognition.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_recognition.so -- Installing: /usr/local/lib/pkgconfig/pcl_recognition.pc -- Installing: /usr/local/include/pcl-1.14/pcl/recognition/boost.h ... -- Installing: /usr/local/include/pcl-1.14/pcl/recognition/ransac_based/auxiliary.h ... -- Installing: /usr/local/lib/libpcl_stereo.so.1.14.1 -- Installing: /usr/local/lib/libpcl_stereo.so.1.14 -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/lib/libpcl_stereo.so.1.14.1\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/lib/libpcl_stereo.so -- Installing: /usr/local/lib/pkgconfig/pcl_stereo.pc -- Installing: /usr/local/include/pcl-1.14/pcl/stereo/stereo_grabber.h -- Installing: /usr/local/include/pcl-1.14/pcl/stereo/stereo_matching.h ... -- Installing: /usr/local/bin/pcl_pcd_convert_NaN_nan -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd_convert_NaN_nan\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd2ply -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd2ply\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_xyz2pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_xyz2pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pclzf2pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pclzf2pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd2vtk -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd2vtk\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_add_gaussian_noise -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_add_gaussian_noise\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd_change_viewpoint -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd_change_viewpoint\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_concatenate_points_pcd -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_concatenate_points_pcd\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_demean_cloud -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_demean_cloud\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_generate -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_generate\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_convert_pcd_ascii_binary -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_convert_pcd_ascii_binary\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_pcd_introduce_nan -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_pcd_introduce_nan\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_hdl_grabber -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_hdl_grabber\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2obj -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2obj\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2ply -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2ply\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ply2raw -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ply2raw\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_plyheader -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_plyheader\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_plane_projection -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_plane_projection\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_compute_hausdorff -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_compute_hausdorff\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_compute_cloud_error -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_compute_cloud_error\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_voxel_grid -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_voxel_grid\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_passthrough_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_passthrough_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_radius_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_radius_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_outlier_removal -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_outlier_removal\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_fast_bilateral_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_fast_bilateral_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_morph -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_morph\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_local_max -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_local_max\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_grid_min -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_grid_min\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_cluster_extraction -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_cluster_extraction\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_progressive_morphological_filter -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_progressive_morphological_filter\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_mls_smoothing -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_mls_smoothing\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_uniform_sampling -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_uniform_sampling\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_sac_segmentation_plane -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_sac_segmentation_plane\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_train_unary_classifier -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_train_unary_classifier\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_unary_classifier_segment -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_unary_classifier_segment\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_crf_segmentation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_crf_segmentation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_train_linemod_template -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_train_linemod_template\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_normal_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_normal_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_boundary_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_boundary_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_fpfh_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_fpfh_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_vfh_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_vfh_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_spin_estimation -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_spin_estimation\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_extract_feature -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_extract_feature\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_marching_cubes_reconstruction -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_marching_cubes_reconstruction\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_gp3_surface -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_gp3_surface\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_poisson_reconstruction -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_poisson_reconstruction\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_icp -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_icp\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_icp2d -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_icp2d\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_elch -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_elch\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_lum -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_lum\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ndt2d -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ndt2d\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_ndt3d -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_ndt3d\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_transform_point_cloud -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_transform_point_cloud\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_transform_from_viewpoint -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_transform_from_viewpoint\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_match_linemod_template -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_match_linemod_template\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; -- Installing: /usr/local/bin/pcl_linemod_detection -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/pcl_linemod_detection\u0026#34; to \u0026#34;/usr/local/lib\u0026#34; GPU ‚úÖ Compile PCL for leveraging Nvidia GPU: Tutorials\nI\u0026rsquo;m concerning GPU version is not good for debugging.\n(2024-05-13)\nSystem info:\nOS: Ubuntu 20.04.6 LTS x86_64, Kernel: 5.15.0-105-generic CPU: Intel i7-9700 (8) @ 4.700GHz GPU: NVIDIA GeForce GTX 1050 Ti; GPU: Intel UHD Graphics 630 Memory: 5010MiB / 15809MiB gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 Cudatoolkit: Cuda compilation tools, release 11.6, V11.6.55. Build cuda_11.6.r11.6/compiler.30794723_0 1 2 3 4 5 sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt install g++-7 -y sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 60 --slave /usr/bin/g++ g++ /usr/bin/g++-7 sudo update-alternatives --config gcc Open GPU options in ccmake, which is an interactive interface How do I install ccmake? - SE\n1 2 3 4 5 # install ccmake sudo apt-get install cmake-curses-gui mkdir build; cd build ccmake .. I turned ON 2 options: BUILD_CUDA and BUILD_GPU\nPress c again to finish configurations and then press g to generate makefiles.\n1 2 3 # cd build make sudo make install Triangulation Greedy Projection Code: Docs\nChange the path to .pcd file: pcl::io::loadPCDFile (\u0026quot;../../pcl/test/bun0.pcd\u0026quot;, cloud_blob);\nSave point cloud as .vtk file: pcl::io::saveVTKFile (\u0026quot;mesh.vtk\u0026quot;, triangles);\nBuild and compile:\n1 2 cmake -B ./build -DCMAKE_BUILD_TYPE=Debug make -C ./build Execute: ./greedy_projection will generate a mesh.vtk\nUse ParaView to open .vtk files.\nUncompress: tar xvf ParaView-5.12.0-MPI-Linux-Python3.10-x86_64.tar.gz\nExecute: ./home/jack/Programs/ParaView-5.12.0-MPI-Linux-Python3.10-x86_64/bin/paraview\nOpen the vtk file and open the \u0026ldquo;eye\u0026rdquo;.\nConvert ply pcd (2024-05-11)\nRef: Convertion of .ply format to .pcd format - SO\n1 2 3 import open3d as o3d ply = o3d.io.read_point_cloud(\u0026#34;source_pointcloud.ply\u0026#34;) o3d.io.write_point_cloud(\u0026#34;sink_pointcloud.pcd\u0026#34;, ply) Then use loadPCDFile to a PCLPointCloud2 template point cloud.\n1 2 pcl::PCLPointCloud2 cloud_blob; pcl::io::loadPCDFile (\u0026#34;../pcl/test/bun0.pcd\u0026#34;, cloud_blob); In this way, however, there is no triangle formed, as shown in the last line of the file \u0026ldquo;mesh.vtk\u0026rdquo;: POLYGONS 0 0. VTK file formats\nSimilarly, the following point cloud only produced 9 triangles. GreedyProjectionTriangulation ¬∑ Issue #4123 (Search: \u0026ldquo;PointCloudLibrary triangulation cannot form polygons\u0026rdquo; in DDG) (2024-05-13)\nThe greedy projection is not good.\nhow to create mesh in delaunay triangulation using pcl library #6225 (Surfaced by searching \u0026ldquo;pointcloudlibrary triangulation\u0026rdquo;)\nRead Ply file (2024-05-12)\nDocs\n1 2 #include \u0026lt;pcl/io/ply_io.h\u0026gt; pcl::io::loadPLYFile (\u0026#34;../CasMVSNet_pl-comments/results/dtu/image_ref/scan1/points3d.ply\u0026#34;, cloud_blob); Debug Repo-Github\n(2024-05-12)\nThe C/C++ extension in VSCode can\u0026rsquo;t cannot open source file \u0026quot;pcl/point_types.h\u0026quot;C/C++(1696)\nSet IncludePath referring to Visual Studio Code cannot open source file \u0026ldquo;iostream\u0026rdquo; - SO\nBTW, To find the path to C++: gcc -v -E -x c++ -\nCreate c_cpp_properties.json by pressing Ctrl+Shift+p and select C/C++: Edit Configurations (JSON)\nAppend includePath\n1 2 3 4 5 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/include/pcl-1.14\u0026#34;, \u0026#34;/home/zichen/.local/include/eigen3\u0026#34; ] Eigen must be installed: cannot open source file \u0026quot;Eigen/StdVector\u0026quot; (dependency of \u0026quot;pcl/io/pcd_io.h\u0026quot;)C/C++(1696)\nInstallation Guide: Eigen - GitHub Pages\nIt's installed in .local/include/eigen/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 -- Configured Eigen 3.3.7 -- -- Some things you can do now: -- --------------+-------------------------------------------------------------- -- Command | Description -- --------------+-------------------------------------------------------------- -- make install | Install Eigen. Headers will be installed to: -- | \u0026lt;CMAKE_INSTALL_PREFIX\u0026gt;/\u0026lt;INCLUDE_INSTALL_DIR\u0026gt; -- | Using the following values: -- | CMAKE_INSTALL_PREFIX: /home/zichen/.local -- | INCLUDE_INSTALL_DIR: include/eigen3 -- | Change the install location of Eigen headers using: -- | cmake . -DCMAKE_INSTALL_PREFIX=yourprefix -- | Or: -- | cmake . -DINCLUDE_INSTALL_DIR=yourdir -- make doc | Generate the API documentation, requires Doxygen \u0026amp; LaTeX -- make check | Build and run the unit-tests. Read this page: -- | http://eigen.tuxfamily.org/index.php?title=Tests -- make blas | Build BLAS library (not the same thing as Eigen) -- make uninstall| Removes files installed by make install -- --------------+-------------------------------------------------------------- -- -- Configuring done (3.0s) -- Generating done (1.1s) -- Build files have been written to: /tmp/eigen/build + make install Install the project... -- Install configuration: \u0026#34;Release\u0026#34; -- Installing: /home/zichen/.local/include/eigen3/signature_of_eigen3_matrix_library -- Installing: /home/zichen/.local/share/pkgconfig/eigen3.pc -- Installing: /home/zichen/.local/share/eigen3/cmake/Eigen3Targets.cmake -- Installing: /home/zichen/.local/share/eigen3/cmake/UseEigen3.cmake -- Installing: /home/zichen/.local/share/eigen3/cmake/Eigen3Config.cmake -- Installing: /home/zichen/.local/share/eigen3/cmake/Eigen3ConfigVersion.cmake -- Installing: /home/zichen/.local/include/eigen3/Eigen/Cholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/CholmodSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/Core -- Installing: /home/zichen/.local/include/eigen3/Eigen/Dense -- Installing: /home/zichen/.local/include/eigen3/Eigen/Eigen -- Installing: /home/zichen/.local/include/eigen3/Eigen/Eigenvalues -- Installing: /home/zichen/.local/include/eigen3/Eigen/Geometry -- Installing: /home/zichen/.local/include/eigen3/Eigen/Householder -- Installing: /home/zichen/.local/include/eigen3/Eigen/IterativeLinearSolvers -- Installing: /home/zichen/.local/include/eigen3/Eigen/Jacobi -- Installing: /home/zichen/.local/include/eigen3/Eigen/LU -- Installing: /home/zichen/.local/include/eigen3/Eigen/MetisSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/OrderingMethods -- Installing: /home/zichen/.local/include/eigen3/Eigen/PaStiXSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/PardisoSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/QR -- Installing: /home/zichen/.local/include/eigen3/Eigen/QtAlignedMalloc -- Installing: /home/zichen/.local/include/eigen3/Eigen/SPQRSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/SVD -- Installing: /home/zichen/.local/include/eigen3/Eigen/Sparse -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseCholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseCore -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseLU -- Installing: /home/zichen/.local/include/eigen3/Eigen/SparseQR -- Installing: /home/zichen/.local/include/eigen3/Eigen/StdDeque -- Installing: /home/zichen/.local/include/eigen3/Eigen/StdList -- Installing: /home/zichen/.local/include/eigen3/Eigen/StdVector -- Installing: /home/zichen/.local/include/eigen3/Eigen/SuperLUSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/UmfPackSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Householder ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/DenseBase.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/functors/StlFunctors.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/CwiseTernaryOp.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/products -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/products/TriangularMatrixVector_BLAS.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/Transpositions.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/NoAlias.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/util -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/util/BlasUtil.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/DenseStorage.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/ZVector -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/ZVector/Complex.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/Default -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/Default/Settings.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/Default/ConjHelper.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX/Complex.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/SSE -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/SSE/Complex.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/SSE/MathFunctions.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/CUDA -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/CUDA/Complex.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/CUDA/MathFunctions.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AltiVec -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AltiVec/Complex.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX512 -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX512/MathFunctions.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/AVX512/PacketMath.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON/Complex.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON/MathFunctions.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Core/arch/NEON/PacketMath.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PardisoSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PardisoSupport/PardisoSupport.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/CholmodSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/CholmodSupport/CholmodSupport.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Jacobi -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Jacobi/Jacobi.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Eigenvalues -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Eigenvalues/Tridiagonalization.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/misc -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/misc/Image.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SuperLUSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SuperLUSupport/SuperLUSupport.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/OrderingMethods -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/OrderingMethods/Ordering.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCore -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCore/SparseDot.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/IterativeLinearSolvers -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/IterativeLinearSolvers/IncompleteCholesky.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/QR -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/QR/CompleteOrthogonalDecomposition.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/StlSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/StlSupport/StdList.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU/Determinant.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU/arch -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/LU/arch/Inverse_SSE.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PaStiXSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/PaStiXSupport/PaStiXSupport.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SVD -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SVD/JacobiSVD.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Geometry -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Geometry/Umeyama.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCholesky/SimplicialCholesky.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseCholesky/SimplicialCholesky_impl.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/plugins -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/plugins/BlockMethods.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/MetisSupport -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/MetisSupport/MetisSupport.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky/LLT_LAPACKE.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky/LDLT.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/Cholesky/LLT.h -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseLU -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseLU/SparseLU_column_dfs.h ... -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseQR -- Installing: /home/zichen/.local/include/eigen3/Eigen/src/SparseQR/SparseQR.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/AdolcForward ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/LevenbergMarquardt ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/FFT ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/EulerAngles -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/EulerAngles/EulerAngles.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/EulerAngles/EulerSystem.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MatrixFunctions -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Eigenvalues -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Eigenvalues/ArpackSelfAdjointEigenSolver.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff/AutoDiffScalar.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff/AutoDiffJacobian.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/AutoDiff/AutoDiffVector.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/BVH -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/BVH/BVAlgorithms.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/BVH/KdBVH.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/NumericalDiff -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/NumericalDiff/NumericalDiff.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Splines ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/SpecialFunctions ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/SparseExtra ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Polynomials ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MoreVectorization -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/MoreVectorization/MathFunctions.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/KroneckerProduct -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/KroneckerProduct/KroneckerTensorProduct.h -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/Skyline ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/NonLinearOptimization ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/src/IterativeSolvers ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/Tensor -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/TensorSymmetry -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/ThreadPool -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/src -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/src/Tensor ... -- Installing: /home/zichen/.local/include/eigen3/unsupported/Eigen/CXX11/src/util ... Resampling (2024-05-28)\nInterpolate point cloud base on Moving Least Squares. Tutorial\n1 2 3 4 mkdir -p pcl_resampling \u0026amp;\u0026amp; cd $_ nvim resampling.cpp cmake -B ./build -DCMAKE_BUILD_TYPE=Debug make -C ./build Python version for PCL MLS: syedjameel/MovingLeastSquares (Found when search: \u0026ldquo;code example for PCL MovingLeastSquares UpsamplingMethod\u0026rdquo; in DDG) This repo: dtcMLOps/upsamplingCloudPCL enable specify options through input arguments. CloudCompare Homepage | Download page\nInstall:\n1 2 3 4 5 6 7 8 9 10 11 sudo apt install flatpak # Add the Flathub repository flatpak remote-add --if-not-exists flathub https://dl.flathub.org/repo/flathub.flatpakrepo # Install CloudCompare flatpak install flathub org.cloudcompare.CloudCompare # Run the app flatpak run org.cloudcompare.CloudCompare # (base) yi@Alienware:~$ flatpak run org.cloudcompare.CloudCompare It pops to warn about Python 3.11 requirement. But it\u0026rsquo;s okay to run in my conda env with Python 3.8. Open a .vtk file:\nThe mesh.vtk result of greedy_projection.cpp for dtu scan1 point cloud (.ply) indeed doesn\u0026rsquo;t have mesh:\n1 2 [19:15:38] [VTK] vtk output [19:15:39] An error occurred while loading \u0026#39;mesh\u0026#39;: nothing to load MeshLab Remeshing (2024-05-13)\nVersion: MeshLab 64bit dp v2023.12d built on Dec 12 2023 with GCC 9.4.0 and Qt 5.15.2.\nTutorial: MeshLab: Point Cloud to Mesh - Design Support - Greenwich Blogs (Search: \u0026ldquo;meshlab convert point cloud to mesh\u0026rdquo;)\nFilters -\u0026gt; Remeshing, Simplification and Reconstruction -\u0026gt; Surface Reconstruction: Screened Poisson (or Ball Pivoting)\nPoisson (defalut) Ball Pivoting (default) Recon depth: 8 Clustering radius: 20% The .ply file is generated by casmvsnet_pl ","date":"2024-03-27T18:14:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pointcloud_libs/","title":"memo: Lib - Points | Libraries Manipulating Point Cloud"},{"content":"NeRF Synthetic The dataset nerf/data/nerf_synthetic/lego composes of 3 things:\nRGB images (train/),\npoint cloud (points3d.ply),\nassociated camera poses (transforms_train.json), where each frame contains 3 attributes: file_path, rotation, and c2w transform_matrix\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \u0026#34;camera_angle_x\u0026#34;: 0.6911112070083618, \u0026#34;frames\u0026#34;: [ { \u0026#34;file_path\u0026#34;: \u0026#34;./train/r_0\u0026#34;, \u0026#34;rotation\u0026#34;: 0.012566370614359171, \u0026#34;transform_matrix\u0026#34;: [ [ -0.9999021887779236, 0.004192245192825794, -0.013345719315111637, -0.05379832163453102 ], [ -0.013988681137561798, -0.2996590733528137, 0.95394366979599, 3.845470428466797 ], [ -4.656612873077393e-10, 0.9540371894836426, 0.29968830943107605, 1.2080823183059692 ], [ 0.0, 0.0, 0.0, 1.0 ] ] }, Colmap Sparse (2024-03-14)\nBlender Add-On (2024-03-20)\nmaximeraafat/BlenderNeRF - Github Found by DDG searching \u0026ldquo;how to make nerf blender\u0026rdquo; AI RENDERING from Blender to NeRF | BlenderNeRF Tutorial - Youtube NeRFStudio Nerfstudio-Docs Creating VFX with NeRFs - Nerfstudio Blender Add-On Tutorial - Youtube DTU (2024-03-21)\nAdapt the DTU dataset to the NeRF synthetic style dataset format. Concretely, the extrinsics (w2c) in DTU (aligned with OpenCV) are required to be modified to c2w of OpenGL.\nDTU dataset already provides point cloud (ply) and multi-view images.\n(2024-04-02)\nThe extrinsics in DTU is w2c. So, c2w = np.linalg.inv(extrinsics)\nAnd because the camera coordinate system in OpenCV (used by DTU) is RDF, the 4 columns in the c2w are [Right | Down | Front | CamCenter]\nHowever, in OpenGL, the c2w matrix should be: [Right | Up | Back | CamCenter]\n1 2 3 4 5 6 7 8 9 Trsfm = np.array([[ 9.70263e-01, 7.47983e-03, 2.41939e-01, -1.91020e+02], [-1.47429e-02, 9.99493e-01, 2.82234e-02, 3.28832e+00], [-2.41605e-01,-3.09510e-02, 9.69881e-01, 2.25401e+01], [ 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00]]) c2w_opencv = np.linalg.inv(Trsfm) print(c2w_opencv) c2w_opengl = c2w_opencv c2w_opengl[:,1] = - c2w_opencv[:,1] c2w_opengl[:,2] = - c2w_opencv[:,2] The other way is adjusting w2c: Each row in the rotation matrix of w2c is a camera axes. And the 4-th column is $-R_{c2w}^{-1} t_{c2w} = - R_{w2c} C$, where the C is the camera center.\nTo align with the OpenGL camera coord. system (from RDF to RUB), the 2nd and 3rd rows in w2c need flips:\n1 2 3 4 5 w2c_opencv = Trsfm w2c_opencv[1] *=-1 w2c_opencv[2] *=-1 c2w_opengl_ = np.linalg.inv(w2c_opencv) print((c2w_opengl_ ==c2w_opengl).all()) The c2w_opengl_ equals to the c2w_opengl above.\n","date":"2024-03-20T12:31:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-test-make_dataset/","title":"test: NeRF | Make Datasets"},{"content":"A sparse reconstruction model consists of 3 .txt files: cameras.txt, images.txt, points3D.txt Docs\nCode for writing data: colmap / scripts / python / read_write_model.py\ncameras.txt\nEach line is the intrinsic parameters of a camera.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def write_cameras_text(cameras, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::WriteCamerasText(const std::string\u0026amp; path) void Reconstruction::ReadCamerasText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; HEADER = ( \u0026#34;# Camera list with one line of data per camera:\\n\u0026#34; + \u0026#34;# CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\\n\u0026#34; + \u0026#34;# Number of cameras: {}\\n\u0026#34;.format(len(cameras)) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, cam in cameras.items(): to_write = [cam.id, cam.model, cam.width, cam.height, *cam.params] line = \u0026#34; \u0026#34;.join([str(elem) for elem in to_write]) fid.write(line + \u0026#34;\\n\u0026#34;) images.txt\nOne camera has two lines. The 1-st line includes extrinsic parameters. The 2-nd line lists the coordinates of 2D keypoints.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def write_images_text(images, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::ReadImagesText(const std::string\u0026amp; path) void Reconstruction::WriteImagesText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; if len(images) == 0: mean_observations = 0 else: mean_observations = sum( (len(img.point3D_ids) for _, img in images.items()) ) / len(images) HEADER = ( \u0026#34;# Image list with two lines of data per image:\\n\u0026#34; + \u0026#34;# IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\\n\u0026#34; + \u0026#34;# POINTS2D[] as (X, Y, POINT3D_ID)\\n\u0026#34; + \u0026#34;# Number of images: {}, mean observations per image: {}\\n\u0026#34;.format( len(images), mean_observations ) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, img in images.items(): image_header = [ img.id, *img.qvec, *img.tvec, img.camera_id, img.name, ] first_line = \u0026#34; \u0026#34;.join(map(str, image_header)) fid.write(first_line + \u0026#34;\\n\u0026#34;) points_strings = [] for xy, point3D_id in zip(img.xys, img.point3D_ids): points_strings.append(\u0026#34; \u0026#34;.join(map(str, [*xy, point3D_id]))) fid.write(\u0026#34; \u0026#34;.join(points_strings) + \u0026#34;\\n\u0026#34;) points3D.txt\nEach line is a 3D point.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def write_points3D_text(points3D, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::ReadPoints3DText(const std::string\u0026amp; path) void Reconstruction::WritePoints3DText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; if len(points3D) == 0: mean_track_length = 0 else: mean_track_length = sum( (len(pt.image_ids) for _, pt in points3D.items()) ) / len(points3D) HEADER = ( \u0026#34;# 3D point list with one line of data per point:\\n\u0026#34; + \u0026#34;# POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\\n\u0026#34; + \u0026#34;# Number of points: {}, mean track length: {}\\n\u0026#34;.format( len(points3D), mean_track_length ) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, pt in points3D.items(): point_header = [pt.id, *pt.xyz, *pt.rgb, pt.error] fid.write(\u0026#34; \u0026#34;.join(map(str, point_header)) + \u0026#34; \u0026#34;) track_strings = [] for image_id, point2D in zip(pt.image_ids, pt.point2D_idxs): track_strings.append(\u0026#34; \u0026#34;.join(map(str, [image_id, point2D]))) fid.write(\u0026#34; \u0026#34;.join(track_strings) + \u0026#34;\\n\u0026#34;) Problems I didn\u0026rsquo;t manage to realize the method as the following problems:\nEach image entry requires specify 2D keypoints.\nNot sure how to obtain them yet.\nAre the X-Y-Z axes of DTU camera aligned with right-bottom-front in COLMAP ?\nThis may require visulization to verify.\nInstall Ubuntu 22.04 ‚úÖ (2024-04-17)\nOS: Ubuntu 22.04.4 LTS x86_64, Kernel: 6.5.0-27-generic\nInstall dependecies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 sudo apt-get install \\ git \\ cmake \\ ninja-build \\ build-essential \\ libboost-program-options-dev \\ libboost-filesystem-dev \\ libboost-graph-dev \\ libboost-system-dev \\ libeigen3-dev \\ libflann-dev \\ libfreeimage-dev \\ libmetis-dev \\ libgoogle-glog-dev \\ libgtest-dev \\ libsqlite3-dev \\ libglew-dev \\ qtbase5-dev \\ libqt5opengl5-dev \\ libcgal-dev \\ libceres-dev Error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 The following packages have unmet dependencies: libqt5opengl5 : Depends: qtbase-abi-5-15-3 qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) E: Unable to correct problems, you have held broken packages. Some posts suggests that the libqt5opengl5 has already been included into qt\nReference:\ncannot install qtbase-abi-5-5-1 on ubuntu 17.10 -SO\n1 2 (base) zichen@homepc:~$ apt-cache search qtbase-abi libqt5core5a - Qt 5 core module ubuntu22.04ÂÆâË£ÖËΩØ‰ª∂Âá∫Áé∞qtbaseÈîôËØØ_qtbase-abi-5-15-3-CSDNÂçöÂÆ¢\nËøô‰æùËµñÊçû‰∏çÁùÄÂïä- Community - Deepin Technology\nI want to skip those 2 packages and compile directly. However, got error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 (base) zichen@homepc:~/Downloads/colmap/build$ cmake .. -GNinja -- The C compiler identification is GNU 9.5.0 -- The CXX compiler identification is GNU 9.5.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found version \u0026#34;1.74.0\u0026#34;) found components: filesystem graph program_options system CMake Error at cmake/FindFLANN.cmake:89 (message): Could not find FLANN Call Stack (most recent call first): cmake/FindDependencies.cmake:17 (find_package) CMakeLists.txt:96 (include) -- Configuring incomplete, errors occurred! Found this issue: Colmap cmake .. error #1451\nI indeed have not installed the libmetis-dev and other packages except for the above 2 missing packages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sudo apt-get install \\ git \\ cmake \\ ninja-build \\ build-essential \\ libboost-program-options-dev \\ libboost-filesystem-dev \\ libboost-graph-dev \\ libboost-system-dev \\ libeigen3-dev \\ libflann-dev \\ libfreeimage-dev \\ libmetis-dev \\ libgoogle-glog-dev \\ libgtest-dev \\ libsqlite3-dev \\ libglew-dev \\ libcgal-dev \\ libceres-dev Linking failed:\n1 2 3 4 5 6 7 8 9 /home/zichen/anaconda3/lib/libQt5OpenGL.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Widgets.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Gui.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2 -lcudadevrt -lcudart_static -lrt -lpthread -ldl \u0026amp;\u0026amp; : /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldTag@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldName@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldReadCount@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldPassCount@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldDataType@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `_TIFFDataSize@LIBTIFF_4.0\u0026#39; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. Uninstall libtiff\nAccording to this issue: /usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/libfreeimage.so: undefined reference to `_TIFFDataSize@LIBTIFF_4.0\u0026rsquo; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. #1803\n1 2 3 4 ninja clean conda uninstall libtiff cmake .. -GNinja ninja I have libtiff:\n1 2 3 4 5 (base) zichen@homepc:~/Downloads/colmap/build$ conda list libtiff # packages in environment at /home/zichen/anaconda3: # # Name Version Build Channel libtiff 4.5.0 h6a678d5_2 https://repo.anaconda.com/pkgs/main Another error about libQt5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 [213/213] Linking CXX executable src/colmap/exe/colmap FAILED: src/colmap/exe/colmap : \u0026amp;\u0026amp; /usr/bin/c++ -Wno-maybe-uninitialized -Wall -O3 -DNDEBUG src/colmap/exe/CMakeFiles/colmap_main.dir/feature.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/sfm.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/colmap.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/database.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/gui.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/image.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/model.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/mvs.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/vocab_tree.cc.o -o src/colmap/exe/colmap -L/usr/local/cuda-11.6/targets/x86_64-linux/lib/stubs -L/usr/local/cuda-11.6/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-11.6/targets/x86_64-linux/lib:/home/zichen/anaconda3/lib: src/colmap/controllers/libcolmap_controllers.a src/colmap/retrieval/libcolmap_retrieval.a src/colmap/scene/libcolmap_scene.a src/colmap/sfm/libcolmap_sfm.a src/colmap/util/libcolmap_util.a src/colmap/util/libcolmap_util_cuda.a src/colmap/mvs/libcolmap_mvs_cuda.a src/colmap/ui/libcolmap_ui.a src/colmap/util/libcolmap_util_cuda.a src/colmap/controllers/libcolmap_controllers.a src/colmap/retrieval/libcolmap_retrieval.a src/colmap/sfm/libcolmap_sfm.a src/colmap/mvs/libcolmap_mvs.a src/thirdparty/PoissonRecon/libcolmap_poisson_recon.a /usr/lib/x86_64-linux-gnu/libgmpxx.so /usr/lib/x86_64-linux-gnu/libmpfr.so /usr/lib/x86_64-linux-gnu/libgmp.so src/colmap/estimators/libcolmap_estimators.a src/colmap/feature/libcolmap_feature.a /usr/lib/x86_64-linux-gnu/libflann.so /usr/lib/x86_64-linux-gnu/liblz4.so src/thirdparty/SiftGPU/libcolmap_sift_gpu.a /usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-11.6/targets/x86_64-linux/lib/libcurand.so /usr/lib/x86_64-linux-gnu/libGLEW.so src/colmap/optim/libcolmap_optim.a /usr/lib/x86_64-linux-gnu/libboost_program_options.so.1.74.0 src/colmap/image/libcolmap_image.a src/colmap/scene/libcolmap_scene.a src/colmap/feature/libcolmap_feature_types.a src/colmap/geometry/libcolmap_geometry.a src/colmap/math/libcolmap_math.a /usr/lib/x86_64-linux-gnu/libmetis.so /usr/lib/x86_64-linux-gnu/libboost_graph.so.1.74.0 /usr/lib/x86_64-linux-gnu/libboost_regex.so.1.74.0 src/colmap/sensor/libcolmap_sensor.a src/colmap/util/libcolmap_util.a /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.74.0 /usr/lib/x86_64-linux-gnu/libsqlite3.so /usr/lib/x86_64-linux-gnu/libGLX.so /usr/lib/x86_64-linux-gnu/libOpenGL.so /usr/lib/libceres.so.2.0.0 /usr/lib/x86_64-linux-gnu/libglog.so.0.4.0 /usr/lib/x86_64-linux-gnu/libunwind.so /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.2 -lpthread src/thirdparty/VLFeat/libcolmap_vlfeat.a /usr/lib/gcc/x86_64-linux-gnu/9/libgomp.so /usr/lib/x86_64-linux-gnu/libpthread.a /usr/lib/x86_64-linux-gnu/libfreeimage.so src/thirdparty/LSD/libcolmap_lsd.a /home/zichen/anaconda3/lib/libQt5OpenGL.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Widgets.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Gui.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2 -lcudadevrt -lcudart_static -lrt -lpthread -ldl \u0026amp;\u0026amp; : /usr/bin/ld: warning: libicui18n.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libicuuc.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libicudata.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_errorName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_setMillis_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getAlias_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_inDaylightTime_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_strToLower_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getStandardName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_strToUpper_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_setSubstChars_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getMaxCharSize_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getTimeZoneDisplayName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_fromUnicode_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getDefaultName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getDefaultTimeZone_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_clone_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getDSTSavings_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_strcoll_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_countAvailable_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openCountryTimeZones_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_compareNames_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getAvailableName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openTimeZoneIDEnumeration_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_setAttribute_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openTimeZones_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `uenum_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_countAliases_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_getSortKey_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_get_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `uenum_next_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_toUnicode_58\u0026#39; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. Reference:\nLinkage against libQt5Core - SO\nBuild from source error libQt5Core.so.5.15.2: undefined reference to #3829\nI already have libicu-dev installed.\nInstall qtbase5-dev\nI noticed the error said: \u0026ldquo;warning: libicudata.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found\u0026rdquo;.\nAnd the binary couldn\u0026rsquo;t find symbols: \u0026ldquo;/home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to u_errorName_58'\u0026rdquo;\nI didn\u0026rsquo;t have Qt? Is this why there is the suggestion?\n1 sudo aptitude install qtbase5-dev Someone used aptitude to install the above 2 packages. Ubuntu20.04ÂÆâË£Öcolmap‰ªéÈõ∂ÂºÄÂßãÂÖ®ËøáÁ®ãËÆ∞ÂΩïÔºàÂåÖÊã¨CUDA/CUDNN/ceres/anacondaÔºâ - CSDN I forgot to check Qt before installing qtbase5: How to find Version of Qt? - SO\n1 2 3 (base) zichen@homepc:~/Downloads/colmap$ QT_SELECT=5 qmake -v QMake version 3.1 Using Qt version 5.15.2 in /home/zichen/anaconda3/lib (2024-04-19) I accepted the first solution provided by the aptitude. But, I didn\u0026rsquo;t notice that the 1st solution is to keep everything unchanged and give up installing qtbase5. Qt Pkgs Matter (2024-04-19)\nThe \u0026ldquo;undefined reference\u0026rdquo; errors persist:\n1 /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_errorName_58\u0026#39; I uninstalled anaconda (removed ~/anaconda3 to /mnt), based on this answer: OpenCV undefined references for libQt5Core.so.5 - raggot (Found by search the above error).\nI tested later that rename the ~/anaconda3 works as well. „ÄêË∂ÖËØ¶ÁªÜ„ÄëÂÆâË£Ö‰∫ÜanacondaÂêéÔºåUbuntu18+COLMAPÈÖçÁΩÆÁñØÁãÇË∏©ÂùëË∏©Ëá≥È≠îÊÄîÁöÑËÆ∞ÂΩï - CSDN Then cmake .. -GNinja. It could not find Qt5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CMake Error at cmake/FindDependencies.cmake:150 (find_package): By not providing \u0026#34;FindQt5.cmake\u0026#34; in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \u0026#34;Qt5\u0026#34;, but CMake did not find one. Could not find a package configuration file provided by \u0026#34;Qt5\u0026#34; (requested version 5.4) with any of the following names: Qt5Config.cmake qt5-config.cmake Add the installation prefix of \u0026#34;Qt5\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;Qt5_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;Qt5\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): CMakeLists.txt:96 (include) Qt5 has been existed on the system:\n1 2 3 zichen@homepc:~/Downloads/colmap$ QT_SELECT=5 qmake -v QMake version 3.1 Using Qt version 5.15.3 in /usr/lib/x86_64-linux-gnu Specifing path to Qt5 doesn\u0026rsquo;t work:\n1 2 3 cmake .. -DQt5_DIR=/usr/lib/x86_64-linux-gnu/ -GNinja cmake .. -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/ -GNinja cmake -B ./build -DCMAKE_PREFIX_PATH=\u0026#34;/usr/lib/x86_64-linux-gnu/qt5;/usr/lib/qt5;/usr/share/qt5\u0026#34; -GNinja I really don\u0026rsquo;t have Qt?\n\u0026ldquo;ubuntu check qt version\u0026rdquo; DDG\n1 2 zichen@homepc:~/Downloads/colmap$ qmake -v qmake: could not find a Qt installation of \u0026#39;\u0026#39; The following packages already existed on the system: (qt4 - How to find Version of Qt? - SO)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 zichen@homepc:~/Downloads/colmap$ dpkg -l | grep qt ri libfcitx-qt5-1:amd64 1.2.7-1.2build1 amd64 Free Chinese Input Toy of X - D-Bus client libraries for Qt5 ri libfcitx-qt5-data 1.2.7-1.2build1 all Free Chinese Input Toy of X - data files for Qt5 integration ii libgsettings-qt1:amd64 0.2-4 amd64 library to access GSettings from Qt (shared libraries) ii libqt5core5a:amd64 5.15.8.1-1+dde amd64 Qt 5 core module ii libqt5core5a:i386 5.15.8.1-1+dde i386 Qt 5 core module ii libqt5dbus5:amd64 5.15.8.1-1+dde amd64 Qt 5 D-Bus module ii libqt5dbus5:i386 5.15.8.1-1+dde i386 Qt 5 D-Bus module ii libqt5gui5:amd64 5.15.8.1-1+dde amd64 Qt 5 GUI module ii libqt5network5:amd64 5.15.8.1-1+dde amd64 Qt 5 network module ri libqt5positioning5:amd64 5.15.8-1+dde amd64 Qt Positioning module ri libqt5printsupport5:amd64 5.15.8.1-1+dde amd64 Qt 5 print support module ii libqt5qml5:amd64 5.15.8.1-1+dde amd64 Qt 5 QML module ii libqt5qmlmodels5:amd64 5.15.8.1-1+dde amd64 Qt 5 QML Models library ii libqt5quick5:amd64 5.15.8.1-1+dde amd64 Qt 5 Quick library ii libqt5quickwidgets5:amd64 5.15.8.1-1+dde amd64 Qt 5 Quick Widgets library ri libqt5webchannel5:amd64 5.15.8-1+dde amd64 Web communication library for Qt ii libqt5widgets5:amd64 5.15.8.1-1+dde amd64 Qt 5 widgets module ri libqt5x11extras5:amd64 5.15.8-1+dde amd64 Qt 5 X11 extras ii qt5-qmake:amd64 5.15.3+dfsg-2ubuntu0.2 amd64 Qt 5 qmake Makefile generator tool ii qt5-qmake-bin 5.15.3+dfsg-2ubuntu0.2 amd64 Qt 5 qmake Makefile generator tool ‚Äî binary file ii qtchooser 66-2build1 amd64 Wrapper to select between Qt development binary versions ii qttranslations5-l10n 5.15.3-1 all translations for Qt 5 zichen@homepc:~/Downloads/colmap$ How to make sure that Qt5.4.2 is installed properly\n1 2 zichen@homepc:~/Downloads/colmap$ whereis qt5 qt5: /usr/lib/x86_64-linux-gnu/qt5 /usr/lib/qt5 /usr/share/qt5 I have qmake:\n1 2 3 4 zichen@homepc:~/Downloads/colmap$ export QT_SELECT=qt5-x86_64-linux-gnu zichen@homepc:~/Downloads/colmap$ qmake -v QMake version 3.1 Using Qt version 5.15.3 in /usr/lib/x86_64-linux-gnu Only 1 binary qmake exists:\n1 2 3 4 5 6 zichen@homepc:~/Downloads/colmap$ tree /usr/lib/qt5 /usr/lib/qt5 ‚îî‚îÄ‚îÄ bin ‚îî‚îÄ‚îÄ qmake 1 directory, 1 file There should be other libraries or binaries for development. apt can find qtbase5-dev, but I can\u0026rsquo;t install: How do you install qt on ubuntu22.04\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 zichen@homepc:~/Downloads/colmap$ apt-cache search qt zichen@homepc:~/Downloads/colmap$ sudo apt-get install qtbase5-dev Reading package lists... Done Building dependency tree... Done Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) Recommends: libqt5opengl5-dev (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed E: Unable to correct problems, you have held broken packages. qt5-default can\u0026rsquo;t be installed neither:\n1 2 3 4 5 6 7 8 9 10 zichen@homepc:~/Downloads/colmap$ sudo apt-get install qt5-default [sudo] password for zichen: Reading package lists... Done Building dependency tree... Done Reading state information... Done Package qt5-default is not available, but is referred to by another package. This may mean that the package is missing, has been obsoleted, or is only available from another source E: Package \u0026#39;qt5-default\u0026#39; has no installation candidate Others packages related qtbase5:\n1 2 3 4 5 zichen@homepc:~/Downloads/colmap$ apt-cache search qtbase5-dev qtbase5-dev - Qt 5 base development files qtbase5-dev-tools - Qt 5 base development programs qtbase5-gles-dev - Qt 5 base development files ‚Äî OpenGL ES variant qtbase5-private-gles-dev - Qt 5 base private development files ‚Äî OpenGL ES variant Some of them requires qtbase-abi-5-15-3\nBut, I can\u0026rsquo;t install it:\n1 E: Package \u0026#39;qtbase-abi-5-15-3\u0026#39; has no installation candidate Select the second solution offered by aptitude:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 zichen@homepc:~/Downloads/colmap$ sudo aptitude install qtbase5-dev The following NEW packages will be installed: qtbase5-dev{b} 0 packages upgraded, 1 newly installed, 0 to remove and 15 not upgraded. Need to get 1,135 kB of archives. After unpacking 15.7 MB will be used. The following packages have unmet dependencies: qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable The following actions will resolve these dependencies: Keep the following packages at their current version: 1) qtbase5-dev [Not Installed] Accept this solution? [Y/n/q/?] . The following actions will resolve these dependencies: Remove the following packages: 1) libqt5core5a:i386 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now)] 2) libqt5dbus5:i386 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now)] 3) libqt5positioning5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] 4) libqt5webchannel5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] 5) libqt5x11extras5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] Install the following packages: 6) libqt5sql5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 7) libqt5sql5-odbc [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 8) libqt5test5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 9) libqt5xml5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 10) qtbase5-dev-tools [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] Downgrade the following packages: 11) libqt5concurrent5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 12) libqt5core5a [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 13) libqt5dbus5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 14) libqt5gui5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 15) libqt5network5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 16) libqt5printsupport5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 17) libqt5qml5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 18) libqt5qmlmodels5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 19) libqt5quick5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 20) libqt5quickwidgets5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 21) libqt5widgets5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] Accept this solution? [Y/n/q/?] Y The following packages will be DOWNGRADED: libqt5concurrent5 libqt5core5a libqt5dbus5 libqt5gui5 libqt5network5 libqt5printsupport5 libqt5qml5 libqt5qmlmodels5 libqt5quick5 libqt5quickwidgets5 libqt5widgets5 The following NEW packages will be installed: libqt5sql5{a} libqt5sql5-odbc{a} libqt5test5{a} libqt5xml5{a} qtbase5-dev qtbase5-dev-tools{a} The following packages will be REMOVED: libqt5core5a:i386{a} libqt5dbus5:i386{a} libqt5positioning5{a} libqt5webchannel5{a} libqt5x11extras5{a} The following packages are RECOMMENDED but will NOT be installed: libqt5svg5 qt5-gtk-platformtheme 0 packages upgraded, 6 newly installed, 11 downgraded, 5 to remove and 15 not upgraded. Need to get 15.1 MB of archives. After unpacking 6,725 kB will be used. Do you want to continue? [Y/n/?] After installation, there are more binary files:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 zichen@homepc:~/Downloads/colmap$ tree /usr/lib/qt5/bin/ /usr/lib/qt5/bin/ ‚îú‚îÄ‚îÄ fixqt4headers.pl ‚îú‚îÄ‚îÄ moc ‚îú‚îÄ‚îÄ qdbuscpp2xml ‚îú‚îÄ‚îÄ qdbusxml2cpp ‚îú‚îÄ‚îÄ qlalr ‚îú‚îÄ‚îÄ qmake ‚îú‚îÄ‚îÄ qvkgen ‚îú‚îÄ‚îÄ rcc ‚îú‚îÄ‚îÄ syncqt.pl ‚îú‚îÄ‚îÄ tracegen ‚îî‚îÄ‚îÄ uic 0 directories, 11 files The Qt5OpenGL is required as well:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CMake Error at /usr/lib/x86_64-linux-gnu/cmake/Qt5/Qt5Config.cmake:28 (find_package): Could not find a package configuration file provided by \u0026#34;Qt5OpenGL\u0026#34; with any of the following names: Qt5OpenGLConfig.cmake qt5opengl-config.cmake Add the installation prefix of \u0026#34;Qt5OpenGL\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;Qt5OpenGL_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;Qt5OpenGL\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): cmake/FindDependencies.cmake:150 (find_package) CMakeLists.txt:96 (include) Similarly, use aptitude to install it:\n1 2 3 4 5 6 zichen@homepc:~/Downloads/colmap$ sudo aptitude install libqt5opengl5-dev The following NEW packages will be installed: libqt5opengl5{a} libqt5opengl5-dev 0 packages upgraded, 2 newly installed, 0 to remove and 27 not upgraded. Need to get 195 kB of archives. After unpacking 927 kB will be used. Do you want to continue? [Y/n/?] Y Finally, compilation succeeded:\n1 2 3 zichen@homepc:~/Downloads/colmap$ cmake -B ./build -GNinja cd build ninja (2024-04-18)\nCUDA requires GCC setup under Ubuntu 22.04. Docs\n1 2 3 4 # sudo apt-get install gcc-10 g++-10 export CC=/usr/bin/gcc-10 export CXX=/usr/bin/g++-10 export CUDAHOSTCXX=/usr/bin/g++-10 Installing nvidia-cuda-toolkit failed via apt install. I don\u0026rsquo;t install it since I already have CUDA-11.6 installed by downloading package manually.\nVerified Practice (2024-04-19)\nRename anaconda3 folder, e.g., anaconda3_1. Otherwise, the qt5 called by header and cmake mismatch. issue-sofa-fredroy\nUse aptitude to install unavailable dependecies, as apt install cannot install qtbase5-dev and libqt5opengl5-dev on Ubuntu 22.04.\nSelect an alternative solutions provided by aptitude.\nSpecify gcc and g++ versions for Ubuntu 22.04:\n1 2 3 4 # sudo apt-get install gcc-10 g++-10 export CC=/usr/bin/gcc-10 export CXX=/usr/bin/g++-10 export CUDAHOSTCXX=/usr/bin/g++-10 Compile colmap:\n1 2 3 4 5 # cd colmap cmake -B ./build -GNinja cd build ninja sudo ninja install The colmap is installed in /usr/local\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 [0/1] Install the project... -- Install configuration: \u0026#34;Release\u0026#34; -- Installing: /usr/local/share/applications/COLMAP.desktop -- Installing: /usr/local/lib/libcolmap_controllers.a -- Installing: /usr/local/lib/libcolmap_estimators.a -- Installing: /usr/local/lib/libcolmap_exe.a -- Installing: /usr/local/lib/libcolmap_feature_types.a -- Installing: /usr/local/lib/libcolmap_feature.a -- Installing: /usr/local/lib/libcolmap_geometry.a -- Installing: /usr/local/lib/libcolmap_image.a -- Installing: /usr/local/lib/libcolmap_math.a -- Installing: /usr/local/lib/libcolmap_mvs.a -- Installing: /usr/local/lib/libcolmap_optim.a -- Installing: /usr/local/lib/libcolmap_retrieval.a -- Installing: /usr/local/lib/libcolmap_scene.a -- Installing: /usr/local/lib/libcolmap_sensor.a -- Installing: /usr/local/lib/libcolmap_sfm.a -- Installing: /usr/local/lib/libcolmap_util.a -- Installing: /usr/local/lib/libcolmap_lsd.a -- Installing: /usr/local/lib/libcolmap_poisson_recon.a -- Installing: /usr/local/lib/libcolmap_vlfeat.a -- Installing: /usr/local/lib/libcolmap_ui.a -- Installing: /usr/local/lib/libcolmap_util_cuda.a -- Installing: /usr/local/lib/libcolmap_mvs_cuda.a -- Installing: /usr/local/lib/libcolmap_sift_gpu.a -- Installing: /usr/local/share/colmap/colmap-config.cmake -- Installing: /usr/local/share/colmap/colmap-config-version.cmake -- Installing: /usr/local/share/colmap/colmap-targets.cmake -- Installing: /usr/local/share/colmap/colmap-targets-release.cmake -- Installing: /usr/local/include/colmap -- Installing: /usr/local/include/colmap/estimators -- Installing: /usr/local/include/colmap/estimators/utils.h -- Installing: /usr/local/include/colmap/estimators/pose.h -- Installing: /usr/local/include/colmap/estimators/coordinate_frame.h -- Installing: /usr/local/include/colmap/estimators/two_view_geometry.h -- Installing: /usr/local/include/colmap/estimators/generalized_absolute_pose.h -- Installing: /usr/local/include/colmap/estimators/bundle_adjustment.h -- Installing: /usr/local/include/colmap/estimators/affine_transform.h -- Installing: /usr/local/include/colmap/estimators/generalized_absolute_pose_coeffs.h -- Installing: /usr/local/include/colmap/estimators/fundamental_matrix.h -- Installing: /usr/local/include/colmap/estimators/euclidean_transform.h -- Installing: /usr/local/include/colmap/estimators/alignment.h -- Installing: /usr/local/include/colmap/estimators/homography_matrix.h -- Installing: /usr/local/include/colmap/estimators/absolute_pose.h -- Installing: /usr/local/include/colmap/estimators/similarity_transform.h -- Installing: /usr/local/include/colmap/estimators/triangulation.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix_poly.h -- Installing: /usr/local/include/colmap/estimators/cost_functions.h -- Installing: /usr/local/include/colmap/estimators/generalized_relative_pose.h -- Installing: /usr/local/include/colmap/estimators/generalized_pose.h -- Installing: /usr/local/include/colmap/estimators/translation_transform.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix_coeffs.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix.h -- Installing: /usr/local/include/colmap/controllers -- Installing: /usr/local/include/colmap/controllers/feature_matching_utils.h -- Installing: /usr/local/include/colmap/controllers/automatic_reconstruction.h -- Installing: /usr/local/include/colmap/controllers/bundle_adjustment.h -- Installing: /usr/local/include/colmap/controllers/hierarchical_mapper.h -- Installing: /usr/local/include/colmap/controllers/incremental_mapper.h -- Installing: /usr/local/include/colmap/controllers/image_reader.h -- Installing: /usr/local/include/colmap/controllers/feature_extraction.h -- Installing: /usr/local/include/colmap/controllers/feature_matching.h -- Installing: /usr/local/include/colmap/controllers/option_manager.h -- Installing: /usr/local/include/colmap/math -- Installing: /usr/local/include/colmap/math/math.h -- Installing: /usr/local/include/colmap/math/matrix.h -- Installing: /usr/local/include/colmap/math/graph_cut.h -- Installing: /usr/local/include/colmap/math/polynomial.h -- Installing: /usr/local/include/colmap/math/random.h -- Installing: /usr/local/include/colmap/exe -- Installing: /usr/local/include/colmap/exe/gui.h -- Installing: /usr/local/include/colmap/exe/image.h -- Installing: /usr/local/include/colmap/exe/database.h -- Installing: /usr/local/include/colmap/exe/feature.h -- Installing: /usr/local/include/colmap/exe/model.h -- Installing: /usr/local/include/colmap/exe/mvs.h -- Installing: /usr/local/include/colmap/exe/vocab_tree.h -- Installing: /usr/local/include/colmap/exe/sfm.h -- Installing: /usr/local/include/colmap/optim -- Installing: /usr/local/include/colmap/optim/sampler.h -- Installing: /usr/local/include/colmap/optim/random_sampler.h -- Installing: /usr/local/include/colmap/optim/least_absolute_deviations.h -- Installing: /usr/local/include/colmap/optim/support_measurement.h -- Installing: /usr/local/include/colmap/optim/combination_sampler.h -- Installing: /usr/local/include/colmap/optim/ransac.h -- Installing: /usr/local/include/colmap/optim/progressive_sampler.h -- Installing: /usr/local/include/colmap/optim/sprt.h -- Installing: /usr/local/include/colmap/optim/loransac.h -- Installing: /usr/local/include/colmap/sensor -- Installing: /usr/local/include/colmap/sensor/models.h -- Installing: /usr/local/include/colmap/sensor/database.h -- Installing: /usr/local/include/colmap/sensor/bitmap.h -- Installing: /usr/local/include/colmap/sensor/specs.h -- Installing: /usr/local/include/colmap/util -- Installing: /usr/local/include/colmap/util/types.h -- Installing: /usr/local/include/colmap/util/ply.h -- Installing: /usr/local/include/colmap/util/string.h -- Installing: /usr/local/include/colmap/util/threading.h -- Installing: /usr/local/include/colmap/util/sqlite3_utils.h -- Installing: /usr/local/include/colmap/util/base_controller.h -- Installing: /usr/local/include/colmap/util/endian.h -- Installing: /usr/local/include/colmap/util/eigen_alignment.h -- Installing: /usr/local/include/colmap/util/logging.h -- Installing: /usr/local/include/colmap/util/version.h -- Installing: /usr/local/include/colmap/util/timer.h -- Installing: /usr/local/include/colmap/util/cuda.h -- Installing: /usr/local/include/colmap/util/controller_thread.h -- Installing: /usr/local/include/colmap/util/cache.h -- Installing: /usr/local/include/colmap/util/testing.h -- Installing: /usr/local/include/colmap/util/opengl_utils.h -- Installing: /usr/local/include/colmap/util/misc.h -- Installing: /usr/local/include/colmap/util/cudacc.h -- Installing: /usr/local/include/colmap/mvs -- Installing: /usr/local/include/colmap/mvs/mat.h -- Installing: /usr/local/include/colmap/mvs/meshing.h -- Installing: /usr/local/include/colmap/mvs/cuda_rotate.h -- Installing: /usr/local/include/colmap/mvs/cuda_texture.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat_ref_image.h -- Installing: /usr/local/include/colmap/mvs/image.h -- Installing: /usr/local/include/colmap/mvs/cuda_flip.h -- Installing: /usr/local/include/colmap/mvs/patch_match_cuda.h -- Installing: /usr/local/include/colmap/mvs/cuda_transpose.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat_prng.h -- Installing: /usr/local/include/colmap/mvs/depth_map.h -- Installing: /usr/local/include/colmap/mvs/normal_map.h -- Installing: /usr/local/include/colmap/mvs/workspace.h -- Installing: /usr/local/include/colmap/mvs/fusion.h -- Installing: /usr/local/include/colmap/mvs/patch_match.h -- Installing: /usr/local/include/colmap/mvs/model.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat.h -- Installing: /usr/local/include/colmap/mvs/consistency_graph.h -- Installing: /usr/local/include/colmap/image -- Installing: /usr/local/include/colmap/image/undistortion.h -- Installing: /usr/local/include/colmap/image/warp.h -- Installing: /usr/local/include/colmap/image/line.h -- Installing: /usr/local/include/colmap/sfm -- Installing: /usr/local/include/colmap/sfm/incremental_mapper.h -- Installing: /usr/local/include/colmap/sfm/incremental_triangulator.h -- Installing: /usr/local/include/colmap/retrieval -- Installing: /usr/local/include/colmap/retrieval/utils.h -- Installing: /usr/local/include/colmap/retrieval/vote_and_verify.h -- Installing: /usr/local/include/colmap/retrieval/inverted_file_entry.h -- Installing: /usr/local/include/colmap/retrieval/inverted_file.h -- Installing: /usr/local/include/colmap/retrieval/geometry.h -- Installing: /usr/local/include/colmap/retrieval/inverted_index.h -- Installing: /usr/local/include/colmap/retrieval/visual_index.h -- Installing: /usr/local/include/colmap/scene -- Installing: /usr/local/include/colmap/scene/synthetic.h -- Installing: /usr/local/include/colmap/scene/point2d.h -- Installing: /usr/local/include/colmap/scene/two_view_geometry.h -- Installing: /usr/local/include/colmap/scene/reconstruction_manager.h -- Installing: /usr/local/include/colmap/scene/reconstruction_io.h -- Installing: /usr/local/include/colmap/scene/image.h -- Installing: /usr/local/include/colmap/scene/scene_clustering.h -- Installing: /usr/local/include/colmap/scene/database.h -- Installing: /usr/local/include/colmap/scene/database_cache.h -- Installing: /usr/local/include/colmap/scene/camera.h -- Installing: /usr/local/include/colmap/scene/track.h -- Installing: /usr/local/include/colmap/scene/reconstruction.h -- Installing: /usr/local/include/colmap/scene/correspondence_graph.h -- Installing: /usr/local/include/colmap/scene/projection.h -- Installing: /usr/local/include/colmap/scene/visibility_pyramid.h -- Installing: /usr/local/include/colmap/scene/camera_rig.h -- Installing: /usr/local/include/colmap/scene/point3d.h -- Installing: /usr/local/include/colmap/tools -- Installing: /usr/local/include/colmap/feature -- Installing: /usr/local/include/colmap/feature/utils.h -- Installing: /usr/local/include/colmap/feature/types.h -- Installing: /usr/local/include/colmap/feature/matcher.h -- Installing: /usr/local/include/colmap/feature/sift.h -- Installing: /usr/local/include/colmap/feature/extractor.h -- Installing: /usr/local/include/colmap/geometry -- Installing: /usr/local/include/colmap/geometry/pose.h -- Installing: /usr/local/include/colmap/geometry/gps.h -- Installing: /usr/local/include/colmap/geometry/sim3.h -- Installing: /usr/local/include/colmap/geometry/rigid3.h -- Installing: /usr/local/include/colmap/geometry/homography_matrix.h -- Installing: /usr/local/include/colmap/geometry/triangulation.h -- Installing: /usr/local/include/colmap/geometry/essential_matrix.h -- Installing: /usr/local/include/colmap/ui -- Installing: /usr/local/include/colmap/ui/qt_utils.h -- Installing: /usr/local/include/colmap/ui/image_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/colormaps.h -- Installing: /usr/local/include/colmap/ui/bundle_adjustment_widget.h -- Installing: /usr/local/include/colmap/ui/feature_extraction_widget.h -- Installing: /usr/local/include/colmap/ui/model_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/thread_control_widget.h -- Installing: /usr/local/include/colmap/ui/point_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/project_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_options_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_manager_widget.h -- Installing: /usr/local/include/colmap/ui/license_widget.h -- Installing: /usr/local/include/colmap/ui/point_painter.h -- Installing: /usr/local/include/colmap/ui/media -- Installing: /usr/local/include/colmap/ui/undistortion_widget.h -- Installing: /usr/local/include/colmap/ui/render_options.h -- Installing: /usr/local/include/colmap/ui/movie_grabber_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_stats_widget.h -- Installing: /usr/local/include/colmap/ui/shaders -- Installing: /usr/local/include/colmap/ui/match_matrix_widget.h -- Installing: /usr/local/include/colmap/ui/feature_matching_widget.h -- Installing: /usr/local/include/colmap/ui/options_widget.h -- Installing: /usr/local/include/colmap/ui/main_window.h -- Installing: /usr/local/include/colmap/ui/log_widget.h -- Installing: /usr/local/include/colmap/ui/database_management_widget.h -- Installing: /usr/local/include/colmap/ui/triangle_painter.h -- Installing: /usr/local/include/colmap/ui/automatic_reconstruction_widget.h -- Installing: /usr/local/include/colmap/ui/line_painter.h -- Installing: /usr/local/include/colmap/ui/dense_reconstruction_widget.h -- Installing: /usr/local/include/colmap/ui/render_options_widget.h -- Installing: /usr/local/include/colmap/thirdparty -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/LiteWindow.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/GlobalUtil.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidGL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidCL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/CuTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftPyramid.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/GLTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCG.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftMatchCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftMatch.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftGPU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/CLTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/FrameBufferObject.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ShaderMan.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramGLSL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramGPU.h -- Installing: /usr/local/include/colmap/thirdparty/LSD -- Installing: /usr/local/include/colmap/thirdparty/LSD/lsd.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/fisher.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/qsort-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/generic.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/sift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop_sse2.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/heap-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/gmm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/ikmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/slic.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/stringop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/quickshift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/hog.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/lbp.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/float.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/kmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop_avx.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/host.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/dsift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/homkermap.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/array.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/pgm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/svmdataset.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/shuffle-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/svm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mser.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/liop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/scalespace.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/imopv_sse2.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/rodrigues.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/vlad.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/covdet.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/imopv.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/aib.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/hikmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/getopt_long.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/random.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/kdtree.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SurfaceTrimmer.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Polynomial.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BinaryNode.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.IsoSurface.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Geometry.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BSplineData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/CmdLineParser.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Allocator.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.Evaluation.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Factor.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BSplineData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Polynomial.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PPolynomial.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SparseMatrix.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/FunctionData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MarchingCubes.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SparseMatrix.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Array.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/FunctionData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Array.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MAT.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PoissonRecon.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MAT.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Geometry.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Octree.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.SortedTreeNodes.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MemoryUsage.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.WeightedSamples.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Hash.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PPolynomial.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Octree.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PointStream.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/CmdLineParser.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.System.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Ply.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MyTime.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PointStream.h -- Installing: /usr/local/share/colmap/cmake -- Installing: /usr/local/share/colmap/cmake/FindLZ4.cmake -- Installing: /usr/local/share/colmap/cmake/FindMetis.cmake -- Installing: /usr/local/share/colmap/cmake/FindFLANN.cmake -- Installing: /usr/local/share/colmap/cmake/FindGlew.cmake -- Installing: /usr/local/share/colmap/cmake/FindFreeImage.cmake -- Installing: /usr/local/share/colmap/cmake/FindGlog.cmake -- Installing: /usr/local/share/colmap/cmake/FindDependencies.cmake -- Installing: /usr/local/bin/colmap -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/colmap\u0026#34; to \u0026#34;\u0026#34; Ubuntu 20.04 ‚ùå (2024-04-18)\nSystem info:\nOS: Ubuntu 20.04.6 LTS x86_64; Kernel: 5.15.0-101-generic Host: Alienware Aurora R8 1.0.6 CPU: Intel i7-9700 (8) @ 4.700GHz, GPU: Intel UHD Graphics 630 GPU: NVIDIA GeForce GTX 1050 Ti Memory: 16GB gcc version 9.4.0 Cuda compilation tools, release 11.6, V11.6.55; Build cuda_11.6.r11.6/compiler.30794723_0 (nvcc -V) Nvidia Driver Version: 545.23.06 Dependecies: Docs\nThe installation has no error reported:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 (base) yi@yi-Alienware-Aurora-R8:~/Downloads$ sudo apt-get install \\ \u0026gt; git \\ \u0026gt; cmake \\ \u0026gt; ninja-build \\ \u0026gt; build-essential \\ \u0026gt; libboost-program-options-dev \\ \u0026gt; libboost-filesystem-dev \\ \u0026gt; libboost-graph-dev \\ \u0026gt; libboost-system-dev \\ \u0026gt; libeigen3-dev \\ \u0026gt; libflann-dev \\ \u0026gt; libfreeimage-dev \\ \u0026gt; libmetis-dev \\ \u0026gt; libgoogle-glog-dev \\ \u0026gt; libgtest-dev \\ \u0026gt; libsqlite3-dev \\ \u0026gt; libglew-dev \\ \u0026gt; qtbase5-dev \\ \u0026gt; libqt5opengl5-dev \\ \u0026gt; libcgal-dev \\ \u0026gt; libceres-dev [sudo] password for yi: Reading package lists... Done Building dependency tree Reading state information... Done libboost-filesystem-dev is already the newest version (1.71.0.0ubuntu2). libboost-filesystem-dev set to manually installed. libboost-program-options-dev is already the newest version (1.71.0.0ubuntu2). libboost-program-options-dev set to manually installed. libboost-system-dev is already the newest version (1.71.0.0ubuntu2). libboost-system-dev set to manually installed. libboost-graph-dev is already the newest version (1.71.0.0ubuntu2). libboost-graph-dev set to manually installed. libeigen3-dev is already the newest version (3.3.7-2). libglew-dev is already the newest version (2.1.0-4). build-essential is already the newest version (12.8ubuntu1.1). build-essential set to manually installed. git is already the newest version (1:2.25.1-1ubuntu3.11). The following additional packages will be installed: cmake-data googletest libaec-dev libatlas3-base libblas-dev libbtf1 libceres1 libcxsparse3 libflann1.9 libfreeimage3 libgflags-dev libgflags2.2 libgmp-dev libgmpxx4ldbl libgoogle-glog0v5 libgraphblas3 libhdf5-mpi-dev libhdf5-openmpi-dev libjxr0 libklu1 liblapack-dev libldl2 liblz4-dev libmongoose2 libmpfr-dev libqt5opengl5 librbio2 librhash0 libspqr2 libsuitesparse-dev qt5-qmake qt5-qmake-bin qtbase5-dev-tools qtchooser Suggested packages: cmake-doc liblapack-doc libmpfi-dev libntl-dev gmp-doc libgmp10-doc libhdf5-doc libmpfr-doc sqlite3-doc default-libmysqlclient-dev firebird-dev libpq-dev unixodbc-dev The following NEW packages will be installed: cmake cmake-data googletest libaec-dev libatlas3-base libblas-dev libbtf1 libceres-dev libceres1 libcgal-dev libcxsparse3 libflann-dev libflann1.9 libfreeimage-dev libfreeimage3 libgflags-dev libgflags2.2 libgmp-dev libgmpxx4ldbl libgoogle-glog-dev libgoogle-glog0v5 libgraphblas3 libgtest-dev libhdf5-mpi-dev libhdf5-openmpi-dev libjxr0 libklu1 liblapack-dev libldl2 liblz4-dev libmetis-dev libmongoose2 libmpfr-dev libqt5opengl5 libqt5opengl5-dev librbio2 librhash0 libspqr2 libsqlite3-dev libsuitesparse-dev ninja-build qt5-qmake qt5-qmake-bin qtbase5-dev qtbase5-dev-tools qtchooser 0 upgraded, 46 newly installed, 0 to remove and 47 not upgraded. Need to get 39.1 MB of archives. After this operation, 323 MB of additional disk space will be used. Do you want to continue? [Y/n] Compile\nSame error pertain to libtiff as above.\n1 2 3 4 5 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/colmap/build$ conda list libtiff # packages in environment at /home/yi/anaconda3: # # Name Version Build Channel libtiff 4.5.1 h6a678d5_0 Uninstall libtiff\nHowever, conda uninstall libtiff hangs forever.\n","date":"2024-03-14T23:40:00Z","permalink":"https://zichen34.github.io/writenotes/lib/colmap_notes/","title":"memo: COLMAP | Install and Usages"},{"content":"Code | Arxiv\nNotes The editing feature is based on Embedded deformation for shape manipulation\nThe transformation of each Gaussian in the entire point cloud is an expaction of the transformations of the K nearest control points.\nPlay Environment (2024-03-11)\nUbuntu 20.04, cuda-11.6\n1 2 3 4 5 conda create -n SC-GS python=3.10 conda activate SC-GS pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 # I also change the torch version to cu116 in requirements.txt pip install -r requirements.txt If I directly run pip install -r requirements.txt, the following error about pip compiling occurs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 (SC-GS) yi@yi:~/Downloads/SC-GS-comments$ pip install -r requirements.txt ... Collecting git+https://github.com/facebookresearch/pytorch3d.git (from -r requirements.txt (line 14)) Cloning https://github.com/facebookresearch/pytorch3d.git to /tmp/pip-req-build-2ndb6zwl Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-2ndb6zwl Resolved https://github.com/facebookresearch/pytorch3d.git to commit 7566530669203769783c94024c25a39e1744e4ed Preparing metadata (setup.py) ... error error: subprocess-exited-with-error √ó python setup.py egg_info did not run successfully. ‚îÇ exit code: 1 ‚ï∞‚îÄ\u0026gt; [6 lines of output] Traceback (most recent call last): File \u0026#34;\u0026lt;string\u0026gt;\u0026#34;, line 2, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;pip-setuptools-caller\u0026gt;\u0026#34;, line 34, in \u0026lt;module\u0026gt; File \u0026#34;/tmp/pip-req-build-2ndb6zwl/setup.py\u0026#34;, line 15, in \u0026lt;module\u0026gt; import torch ModuleNotFoundError: No module named \u0026#39;torch\u0026#39; [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed √ó Encountered error while generating package metadata. ‚ï∞‚îÄ\u0026gt; See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Refer issue 10 and issue 15\nCompilin Pillow requires: sudo apt-get install libjpeg-dev. Otherwise, error occus:\n1 2 3 4 5 6 7 8 9 10 11 The headers or library files could not be found for jpeg, a required dependency when compiling Pillow from source. Please see the install instructions at: https://pillow.readthedocs.io/en/latest/installation.html note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for Pillow Running setup.py clean for Pillow Failed to build Pillow ERROR: Could not build wheels for Pillow, which is required to install pyproject.toml-based projects PIL error: no attribute 'ANTIALIAS'\n1 2 3 File \u0026#34;/home/yi/anaconda3/envs/SC-GS/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py\u0026#34;, line 486, in make_image image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS) AttributeError: module \u0026#39;PIL.Image\u0026#39; has no attribute \u0026#39;ANTIALIAS\u0026#39; ChatGPT: The api has changed, the function should be called like:\nDidn't try 1 2 3 from PIL import Image, ImageFilter # ... image = image.resize((scaled_width, scaled_height), ImageFilter.ANTIALIAS) This error is solved by reinstalling the conda environment with python=3.8, pip will download the packages compatible (cp) with python 3.8. And Pillow cp38 still have the same api.\nTrain Train on 1050Ti (4GB)\n1 2 3 4 5 6 # Train with terminal only (for the resolution of 400*400 with best PSNR) CUDA_VISIBLE_DEVICES=0 python train_gui.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512 \\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 80000 iterations cost 1:47:24.\nDeformation Once the model has been trained, launch GUI with the output dir (just add --gui):\n1 2 3 4 5 CUDA_VISIBLE_DEVICES=0 python train_gui.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512\\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 --gui Unlike LBS, the model can be teared intio pieces.\nAnd the deformation may be anti-physical.\nAfter adjusting the poses, clik play to watch animation.\nTo debug the deformation code, once selected keypoints (A+Left Clik), Pause the debugger first, and then step by step inspect.\nPress D + Right-click drag will trigger callback_keypoint_drag\nClick Init Graph will tigger callback_animation_initialize to assign the attribute animate_tool by instantiating the class LapDeform\n1 dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Right, callback=callback_keypoint_drag) The command for eval only change train_gui.py to render.py:\n1 2 3 4 5 CUDA_VISIBLE_DEVICES=0 python render.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512 \\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 ","date":"2024-03-11T00:00:00Z","image":"https://arxiv.org/html/2312.14937v2/x2.png","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-sc-gs/","title":"read: SC-GS"},{"content":"remap Docs - OpenCV: Remapping\nUse cv2.remap to do image warpping:\nHomography mapping determines the projection location on the source image for a reference image.\nr e ( ( ( f 0 1 2 , , , p 0 0 0 i ) ) ) x e ( ( ( l 0 1 2 , , , c 1 1 1 o ) ) ) o r ( ( ( d 0 2 2 s , , , 2 2 2 ) ) ) p o ( ( ( i x x x n y y y t z z z s ) ) ) w ( ( ( o x x x r y y y l z z z d ) ) ) c ( ( ( o x x x o y y y r z z z d ) ) ) s ( ( ( p x x x r , , , o y y y j ) ) ) s ( ( ( o x x x n , , , y y y s ) ) ) r c ( ( ( x x x i , , , m y y y g ) ) ) By fetching the pixels of the source image to the grid, according to the projection coordinates, a warpped source image is obtained.\nTherefore, remap requires the (x,y) \u0026ldquo;new arrange pattern\u0026rdquo; of the source image as input, along with the source image.\nExample in CasMVSNet-pl:\n1 2 3 4 5 6 7 8 9 xy_ref = np.mgrid[:img_wh[1],:img_wh[0]][::-1].astype(np.float32) xy_src = xy_ref2src(xy_ref, depth_ref, P_world2ref, depth_src, P_world2src, img_wh) # Sample the depth of xy_src using bilinear interpolation depth_src2ref = cv2.remap(depth_src, xy_src[0].astype(np.float32), xy_src[1].astype(np.float32), interpolation=cv2.INTER_LINEAR) resize (2024-03-14)\n1 2 3 4 5 6 7 8 9 10 img_ref = cv2.imread(os.path.join(root_dir, # channel is BGR f\u0026#39;Rectified/{scan}/rect_{vid+1:03d}_3_r5000.png\u0026#39;)) # scale to specified size img_ref = cv2.resize(img_ref, tuple(args.img_wh), interpolation=cv2.INTER_LINEAR)[:,:,::-1] # to RGB # scaling with factor, h and w both increase 4 times proba_ref = cv2.resize(proba_ref, None, fx=4, fy=4, interpolation=cv2.INTER_LINEAR) ","date":"2024-03-07T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/opencv/","title":"memo: OpenCV"},{"content":"Parallel programming model - Wikipedia\nReduction ÂΩíÁ∫¶ (2024-03-01)\nSource video: CUDAÁºñÁ®ãÊ®°ÂûãÁ≥ªÂàóÂÖ´(ÂéüÂ≠êÊìç‰Ωú / ËßÑÁ∫¶ / ÂêëÈáèÂÖÉÁ¥†Ê±ÇÂíå) - Ken He Code The parallism design depends on the operations to be performed. For example, given a task N-number summation, the operation executed on each thread in parallel is addition.\nThreads reduce by half every time.\nAs the \u0026ldquo;plus\u0026rdquo; operation computes 2 numbers, the data sequence is bisected.\n1 2 3 4 5 6 7 8 9 10 11 source[8]: 0 1 2 3 4 5 6 7 step 1: thread 0: source[0] + source[4] -\u0026gt; source[0] thread 1: source[1] + source[5] -\u0026gt; source[1] thread 2: source[2] + source[6] -\u0026gt; source[2] thread 3: source[3] + source[7] -\u0026gt; source[3] step 2: thread 0: source[0] + source[2] -\u0026gt; source[0] thread 1: source[1] + source[3] -\u0026gt; source[1] step 3: thread 0: source[0] + source[1] As shown above, the number of inital threads allocated is a half of the total data items. And in the following steps, the number of launched threads is a half of the number of threads used last time.\nSpecificaly, in the 1st round, 4 threads for 8 items, and the 2nd round only uses 2 threads for 4 results of the last step, and the final round only uses 1 thread.\n8 4 4 2 2 1 r e i t i t i t s t h t h t h u e r e r e r l m e m e m e t s a s a s a : : d : d : d s s : : : 0 0 0 0 1 1 1 2 2 3 3 4 5 6 7 If using CPU, there will be 7 plus operation, however, on GPU, there are only 3 steps.\nWhen the total number of operations is larger than the allocated threads, the Grid stride loop trick can be used.\nFor example, there are 32 operations need to be executed, but only 8 threads are allocated. Therefore, each thread has to be reused 4 times.\nBased on this fact, accumulate the 4 loops at first and then perform summation within a block (8 threads).\nOnly consider the behavior of one thread: what values will it use?\nFor a thread in a block, the sum of values assigned to it during 4 loops is computed as:\n3 A 8 r 2 c e c t s e h u l l r l e o e t : o d p : 0 0 0 l 1 1 o o 2 2 p ‚ãØ ‚ãØ 1 7 7 8 l 9 o o 1 p 0 2 ‚ãØ 1 5 1 6 l 1 o 7 o p 1 8 3 ‚ãØ 2 3 2 4 2 l 5 o o 2 p 6 4 ‚ãØ 3 1 In this way, multiple steps are compressed into a single block (8 threads).\nShared memory is very fast, so it can be used for those memory that is frequently accessed.\nAs the accumulated sums of 4 loops for each thread requires summation across the BLOCK_SIZE at the end, they can be stored in shared memory for later frequent reading.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // grid loop: accumulate loops first // allocate the same size as a block __shared__ int acc_tmp[BLOCK_SIZE]; int shared_tmp = 0; // Necessary to get correct result // Each thread adds the thread after n_thrd_cur for(int ele_id=blockDim.x * blockIdx.x + threadIdx.x; ele_id\u0026lt;num_items; ele_id+=blockDim.x*gridDim.x){ shared_tmp += d_in[ele_id]; } // __syncthreads(); // Sometimes lead to wrong result acc_tmp[threadIdx.x] = shared_tmp; // assign shared mem __syncthreads(); // Necessary Note:\nIf directly using the shared memory to do accumulation like: acc_tmp[threadIdx.x] += d_in[ele_id];, the result could be wrong because it\u0026rsquo;s in a loop, where mutliple thread may access the same memory at the same time, due to shared memory is accessible for all threads in a block.\nHowever, the local variable (shared_tmp) reside in register is private for a thread, and other threads can\u0026rsquo;t access it. So modifying the shared_tmp is safe and necessary.\nThe last __syncthreads(); cannot be put right after the for loop, and must be after shared memory assignment (on 1050Ti). Otherwise, the result could be wrong. (1080Ti is ok.)\nSo far, only the \u0026ldquo;block\u0026rdquo; of shared memory acc_tmp needs to compute the sum.\nThe threads reduction is performed through a for loop to adjust the number of threads step-by-step:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Sum numbers in the shared memory with size of BLOCK_SIZE // Threads reduce by half each step // Initial number of threads is a half of the total data for (n_thrd_cur=BLOCK_SIZE/2; n_thrd_cur\u0026gt;=1; n_thrd_cur/=2){ // Let a thread to do an operations: plus // Temporary variable is necessary for memory safety: int sum_tmp = 0; // Only use threads required if (threadIdx.x \u0026lt; n_thrd_cur){ sum_tmp = acc_tmp[threadIdx.x] + acc_tmp[threadIdx.x + n_thrd_cur]; } __syncthreads(); // Necessary // as write after read for the same memory // Can\u0026#39;t reside in if or other brach syntax // Write result back to memory if (threadIdx.x \u0026lt; n_thrd_cur){ acc_tmp[threadIdx.x] = sum_tmp; } __syncthreads(); // Necessary for 1050Ti, 1080Ti } Finally, the sum of a block (shared memory) is stored in acc_tmp[0].\n__syncthreads() is used when a memory is read followed by writing/modification to avoid data Hazard-wiki (Race condition-wiki, Memory safety-wiki)\n__syncthreads() can\u0026rsquo;t reside in if because it\u0026rsquo;s a branch. Otherwise, when multiple threads run in parallel, threads may go different branches, consequently, leading to errors.\natomicAdd guarantees the read/write to an address won\u0026rsquo;t be disrupted by other threads.\nWhen adding the summation of each block acc_tmp[0] (shared memory) upto d_out (global memory), multiple threads access the same global memory d_out, so atomicAdd is applied:\n1 2 3 4 5 6 7 8 // Accumulate all blocks in the grid // The sum of each block was stored in acc_tmp[0] // Each block uses 1 thread to add its sum to the total sum of all blocks if (blockIdx.x * blockDim.x \u0026lt; num_items){ if (threadIdx.x == 0){ atomicAdd(d_out, acc_tmp[0]); } } Full code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 #include \u0026lt;stdio.h\u0026gt; #define N 10000000 // number of data. No = and column ; #define BLOCK_SIZE 256 // blockDim.x #define GRID_SIZE 32 // gridDim.x, the total threads allocated: 8192 __managed__ int source[N]; // allocate memory for input data __managed__ int result_gpu[1] = {0}; // output the sum of N items // Remeber to declare types for args __global__ void sum_gpu(int* d_in, int num_items, int* d_out){ // grid loop __shared__ int acc_tmp[BLOCK_SIZE]; int shared_tmp = 0; for (int ele_id=blockIdx.x * blockDim.x + threadIdx.x; ele_id \u0026lt; num_items; ele_id+=BLOCK_SIZE*GRID_SIZE){ shared_tmp += d_in[ele_id]; } acc_tmp[threadIdx.x] = shared_tmp; __syncthreads(); // threads reduction int sum_tmp = 0; for (int thd_cur=BLOCK_SIZE/2; thd_cur\u0026gt;=1; thd_cur/=2){ if (threadIdx.x \u0026lt; thd_cur){ sum_tmp = acc_tmp[threadIdx.x] + acc_tmp[threadIdx.x + thd_cur]; } __syncthreads(); if (threadIdx.x \u0026lt; thd_cur){ acc_tmp[threadIdx.x] = sum_tmp; } __syncthreads(); } // accumulate all blocks if (blockIdx.x * blockDim.x \u0026lt; num_items){ if (threadIdx.x == 0){ // d_out[0] += acc_tmp[0]; atomicAdd(d_out, acc_tmp[0]); } } } int main(){ // Initialize source data // Can\u0026#39;t use: for(int\u0026amp; i :sourace) for (int i=0; i\u0026lt;N; i++) source[i] = rand()%10; // record time cudaEvent_t start, stop_gpu, stop_cpu; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop_gpu); cudaEventCreate(\u0026amp;stop_cpu); cudaEventRecord(start); cudaEventSynchronize(start); for (int i=0; i\u0026lt;20; i++){ // avg time for 20 rounds result_gpu[0] = 0; // clear value sum_gpu\u0026lt;\u0026lt;\u0026lt;GRID_SIZE, BLOCK_SIZE\u0026gt;\u0026gt;\u0026gt;(source, N, result_gpu); cudaDeviceSynchronize(); // wait gpu } cudaEventRecord(stop_gpu); cudaEventSynchronize(stop_gpu); // cpu execution int result_cpu = 0; for (int i=0; i\u0026lt;N; i++) result_cpu += source[i]; cudaEventRecord(stop_cpu); cudaEventSynchronize(stop_cpu); float time_gpu, time_cpu; cudaEventElapsedTime(\u0026amp;time_gpu, start, stop_gpu); cudaEventElapsedTime(\u0026amp;time_cpu, stop_gpu, stop_cpu); cudaEventDestroy(start); cudaEventDestroy(stop_gpu); cudaEventDestroy(stop_cpu); printf(\u0026#34;Time on gpu: %.2f, Time on cpu: %.2f\\n\u0026#34;, time_gpu/20, time_cpu); printf(\u0026#34;%s\\n\u0026#34;, (result_gpu[0] == result_cpu) ? \u0026#34;Equal\u0026#34; : \u0026#34;Error\u0026#34;); printf(\u0026#34;Sum on gpu: %d, Sum on cpu: %d\\n\u0026#34;, *result_gpu, result_cpu); return 0; } Output:\n1 2 3 4 5 (base) yi@yi:~/Downloads/CUDA_Study$ nvcc Tut_KenHe/8_reduction.cu (base) yi@yi:~/Downloads/CUDA_Study$ ./a.out Time on gpu: 1.15, Time on cpu: 26.45 Equal Sum on gpu: 45011704, Sum on cpu: 45011704 ","date":"2024-03-01T18:20:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/tut_%E4%BD%95%E7%90%A8/","title":"watch: CUDA - ‰ΩïÁê® | CUDA Programming Model"},{"content":"Open3D (2024-05-13)\nJust found Open3D also can find nearest neighbors: 5-Step Guide to generate 3D meshes from point clouds with Python - Medium\n1 2 3 4 5 6 7 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/CasMVSNet_pl-comments/results/dtu/image_ref/scan1/points3d.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) distances = pcd.compute_nearest_neighbor_distance() avg_dist = np.mean(distances) radius = 3 * avg_dist CUDA Impl (2024-03-05)\nExample from simple-knn:\nUse thread block to divide the sorted points\nGiven 100K points, using only 1D grid and 1D block which length is 1024 (SO), the number of blocks is $\\frac{100K + 1024-1}{1024} = 97$\nHow many threads can a Nvidia GPU launch?, found by DDG Therefore, the kernel launch configuration parameters are \u0026lt;\u0026lt;\u0026lt;97, 1024\u0026gt;\u0026gt;\u0026gt;\nConvert 3D cartesian coordinates to Morton code\nMake up key-value pairs, where Morton code is key and points indices are values.\nSuch that, the points (indices) are sorted and similar points will gather together.\n1 2 3 4 5 6 7 8 thrust::device_vector\u0026lt;uint32_t\u0026gt; indices(P);\t// indices for points thrust::sequence(indices.begin(), indices.end());\t// indices: 0, 1, 2, 3. ... P thrust::device_vector\u0026lt;uint32_t\u0026gt; indices_sorted(P);\t// the sorted indices cub::DeviceRadixSort::SortPairs(nullptr, temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); temp_storage.resize(temp_storage_bytes); cub::DeviceRadixSort::SortPairs(temp_storage.data().get(), temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); Once the points are spread onto blocks, points within a block are similar to each other.\nFind the min and max coordinates combinations in each block\n1 boxMinMax \u0026lt;\u0026lt; \u0026lt;num_boxes, BOX_SIZE \u0026gt;\u0026gt; \u0026gt; (P, points, indices_sorted.data().get(), boxes.data().get()); Each thread has a point index. The indices indices_sorted of points are already sorted based on the Morton (z-order) code. So points within a block are similar to each other, and their distances are supposed to be small.\nG r a i i B n d l p d o o e c i x k n 1 t 0 0 2 m 4 i n t , h r m e a ‚ãØ a x d s B l o c k 1 1 0 2 m 4 i n t , h r m e a ‚ãØ a x d s ‚ãØ B l o m c i k n , 9 7 m a x The vector boxes records each block\u0026rsquo;s min and max coordinates:\nb 2 l x k y 0 z b 2 l x k y 1 z b 2 l x k y 2 z b 2 l x k y n z Based on the min and max coordinates, a point can quickly determine whether a block contains potential nearest neighbors.\nSpecifically, if the distance of a point to the min or max of a block is larger than reject, the block won\u0026rsquo;t be searched for possible nearest neighbors.\n1 2 3 4 5 for (int b = 0; b \u0026lt; (P + BOX_SIZE - 1) / BOX_SIZE; b++){ MinMax box = boxes[b]; float dist = distBoxPoint(box, point); if (dist \u0026gt; reject || dist \u0026gt; best[2]) continue; The reject value is the distance of the 3-rd nearest neighbor based on the initially sorted points sequence.\n1 2 3 4 5 6 7 8 9 int idx = cg::this_grid().thread_rank(); float3 point = points[indices[idx]]; float best[3] = { FLT_MAX, FLT_MAX, FLT_MAX }; for (int i = max(0, idx - 3); i \u0026lt;= min(P - 1, idx + 3); i++){ if (i == idx) continue; updateKBest\u0026lt;3\u0026gt;(point, points[indices[i]], best); } In this way, quiet amount of points are filtered out to save computation.\nIterate every points in the potential block\n1 2 3 4 5 6 7 8 9 for (int b = 0; b \u0026lt; (P + BOX_SIZE - 1) / BOX_SIZE; b++){ ... for (int i = b * BOX_SIZE; i \u0026lt; min(P, (b + 1) * BOX_SIZE); i++){ if (i == idx) continue; updateKBest\u0026lt;3\u0026gt;(point, points[indices[i]], best); } } dists[indices[idx]] = (best[0] + best[1] + best[2]) / 3.0f; Compute distance every two points:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 __device__ void updateKBest(const float3\u0026amp; ref, const float3\u0026amp; point, float* knn) { float3 d = { point.x - ref.x, point.y - ref.y, point.z - ref.z };\t// diff float dist = d.x * d.x + d.y * d.y + d.z * d.z;\t// sequare error for (int j = 0; j \u0026lt; K; j++)\t// K-nearest { if (knn[j] \u0026gt; dist)\t// closer than j { float t = knn[j];\t// tmp for previous j knn[j] = dist;\t// replace the previous j dist = t;\t// the previous j is compared with remaining neighbors } } } Test (2024-03-06)\nUbuntu 20.04, 1050Ti, cuda-11.6 (nvcc -V)\nClone: git clone https://gitlab.inria.fr/bkerbl/simple-knn.git\nEnvironment: conda env create -f environment.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 name: PCRefine channels: - pytorch - conda-forge - defaults dependencies: - python=3.10 - pip - pip: - torch==1.12.1+cu116 - torchvision==0.13.1+cu116 - --extra-index-url https://download.pytorch.org/whl/cu116 - ipykernel - plyfile Compile based on the setup.py: pip install .\nCall from python:\n1 2 3 4 5 6 7 import numpy as np import torch from simple_knn._C import distCUDA2 from utils import fetchPly pcd = fetchPly(\u0026#34;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/points3d.ply\u0026#34;) meanDist = distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()) Output the mean distance from neighbors to each point:\n1 2 (PCRefine) yi@yi:~/Downloads/simple-knn-comments$ python test_knn.py tensor([0.0013, 0.0020, 0.0008, ..., 0.0023, 0.0017, 0.0007], device=\u0026#39;cuda:0\u0026#39;) Indices (2024-03-08)\nModify the original code to return the neighbors\u0026rsquo; indices:\n1 2 3 4 5 6 7 8 (casmvsnet_pl) yi@yi:~/Downloads/CasMVSNet_pl-comments/submodules/simple-knn-comments$ python test_knn.py tensor([[41057, 23085, 16403], [21674, 59181, 31901], [18699, 99481, 15716], ..., [17352, 51604, 48154], [90929, 45350, 94932], [ 6797, 62182, 78410]], device=\u0026#39;cuda:0\u0026#39;, dtype=torch.int32) pytorch3d (2024-03-12)\nSet up environment to compile pytorch3d:\n1 2 3 4 5 conda create -n casmvsnet_pl python=3.8 pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 pip install git+https://github.com/facebookresearch/pytorch3d.git pip install -r requirements.txt Example from SC-GS\n1 nn_dist, nn_idxs, _ = pytorch3d.ops.knn_points(init_pcl[None], init_pcl[None], None, None, K=K+1) Use pytorch3d to find KNN in the lego point cloud:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np from plyfile import PlyData from typing import NamedTuple import torch import pytorch3d.ops class BasicPointCloud(NamedTuple): points : np.array colors : np.array normals : np.array # Code from: https://github.com/graphdeco-inria/gaussian-splatting def fetchPly(path): plydata = PlyData.read(path) vertices = plydata[\u0026#39;vertex\u0026#39;] # coordinates (x,y,z), normal (nx,ny,nz), r,g,b of 100K points positions = np.vstack([vertices[\u0026#39;x\u0026#39;], vertices[\u0026#39;y\u0026#39;], vertices[\u0026#39;z\u0026#39;]]).T # xyz: (3, 100K) -\u0026gt; (100K,3) colors = np.vstack([vertices[\u0026#39;red\u0026#39;], vertices[\u0026#39;green\u0026#39;], vertices[\u0026#39;blue\u0026#39;]]).T / 255.0 # rgb: (3, 100K) -\u0026gt; (100K, 3) normals = np.vstack([vertices[\u0026#39;nx\u0026#39;], vertices[\u0026#39;ny\u0026#39;], vertices[\u0026#39;nz\u0026#39;]]).T # normal xyz: (100K,3) return BasicPointCloud(points=positions, colors=colors, normals=normals) # NamedTuple pcd = fetchPly(\u0026#34;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/points3d.ply\u0026#34;) init_pcl = torch.from_numpy(np.asarray(pcd.points)).float().cuda() K=3 nn_dist, nn_idxs, _ = pytorch3d.ops.knn_points(init_pcl[None], init_pcl[None], None, None, K=K+1) # Both are (1,100k,4) print(nn_dist[0][:3]) print(nn_idxs[0][:3]) pytorch3d\u0026rsquo;s function can return nearest neighbors distance and indices at the same time. And the results are the same as the above cuda implementation:\n1 2 3 4 5 6 tensor([[0.0000, 0.0008, 0.0016, 0.0017], [0.0000, 0.0012, 0.0021, 0.0026], [0.0000, 0.0006, 0.0008, 0.0009]], device=\u0026#39;cuda:0\u0026#39;) tensor([[ 0, 41057, 23085, 16403], [ 1, 21674, 59181, 31901], [ 2, 18699, 99481, 15716]], device=\u0026#39;cuda:0\u0026#39;) I also tested the point cloud predicted by casmvsnet_pl, but this time the pytorch3d is so much slower than the cuda code (1h). Dont\u0026rsquo;t know why.\nResults are the same: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 scan1 contains 9.03 M points # cuda result nn indices: tensor([[2654648, 181848, 2654649], [1825888, 2654649, 182085], [1825889, 2025037, 182085], ..., [1788118, 5669911, 1996297], [2195795, 2814578, 1568896], [2814579, 2597015, 2195796]], device=\u0026#39;cuda:0\u0026#39;, dtype=torch.int32) # pytorch3d result nn indices: (1, 9030200, 4) tensor([[[ 0, 2654648, 181848, 2654649], [ 1, 1825888, 2654649, 182085], [ 2, 1825889, 2025037, 182085], ..., [9030197, 1788118, 5669911, 1996297], [9030198, 2195795, 2814578, 1568896], [9030199, 2814579, 2597015, 2195796]]], device=\u0026#39;cuda:0\u0026#39;) print(nn_dist) tensor([[[0.0000, 0.0137, 0.0167, 0.3456], [0.0000, 0.2516, 0.3126, 0.3652], [0.0000, 0.0262, 0.0991, 0.2446], ..., [0.0000, 0.0775, 0.0991, 0.1228], [0.0000, 0.0577, 0.0749, 0.0777], [0.0000, 0.0449, 0.1124, 0.1350]]], device=\u0026#39;cuda:0\u0026#39;) In simple-knn, the distance dist is a square error:\n1 2 float3 d = { point.x - ref.x, point.y - ref.y, point.z - ref.z }; float dist = d.x * d.x + d.y * d.y + d.z * d.z; ","date":"2024-02-29T11:50:00Z","permalink":"https://zichen34.github.io/writenotes/algo/knn/","title":"memo: algo | KNN"},{"content":"(Feature image is from: CasMVSNet_pl)\nRestoring point cloud: Fuse depth maps of multi-views, and then unproject the depth map to 3D points.\nMVSNet-PyTorch Source code: MVSNet-PyTorch\nDownload pretrained model checkpoint\n1 gdown 1j2I_LNKb9JeCl6wdA7hh8z1WgVQZfLU9 -O ./checkpoints/pretrained/MVSNet-Pytorch_model_000014.ckpt Download preprocessed testing data of MVSNet\n1 2 gdown 135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ -O /data2/zichen/MVSNet_testing_dtu.zip unzip /data2/zichen/MVSNet_testing_dtu.zip -d /data2/zichen/MVSNet_testing Modify data paths in eval.sh, and uncomment the line#302: save_depth() in \u0026ldquo;eval.py\u0026rdquo; to generate depth map first. And then run the script: ./eval.sh.\nThis program requires 15GB VRAM. so 1080Ti can\u0026rsquo;t run it.\nThe resolution of the test images (3x1184x1600; Feat map: 32x296x400) is too high to be processed by 1080Ti with 11 GB VRAM.\nWhen processing the 2nd source view, OOM occurs at:\n1 2 3 warped_src_fea = F.grid_sample( src_fea, grid.view(batch, num_depth * height, width, 2), mode=\u0026#39;bilinear\u0026#39;, padding_mode=\u0026#39;zeros\u0026#39;) (2024-02-23) If executing the eval.py with the training images, the Dataset data_yao_eval may be mismatched with the training data folder.\nVisualization: \u0026ldquo;outputs/mvsnet001_l3.ply\u0026rdquo;\nThere are noise point around the round edge of the bowl.\nUse Matlab to compute the quantitative Metrics:\nThe DTU dataset (\u0026ldquo;DTU_SampleSet.zip\u0026rdquo;) provides the \u0026ldquo;Matlab evaluation code/\u0026rdquo; to assess the quality of point clouds.\nBased on the SampleSet directory structure:\n1 2 3 4 5 6 7 8 9 10 11 (base) yi@yi:~$ tree /mnt/data2_zichen/SampleSet/ -d -L 2 /mnt/data2_zichen/SampleSet/ ‚îú‚îÄ‚îÄ Matlab evaluation code ‚îÇ¬†‚îî‚îÄ‚îÄ MeshSupSamp_web ‚îî‚îÄ‚îÄ MVS Data ‚îú‚îÄ‚îÄ Calibration ‚îú‚îÄ‚îÄ Cleaned ‚îú‚îÄ‚îÄ ObsMask # 3D parts used for evaluation ‚îú‚îÄ‚îÄ Points # ground truth point clouds for each scan ‚îú‚îÄ‚îÄ Rectified ‚îî‚îÄ‚îÄ Surfaces # Poisson reconstruction To evaluate point clouds of all scans, the folder \u0026ldquo;Points\u0026rdquo; needs to be replaced with the full version \u0026ldquo;Points/\u0026rdquo;.\nCreate a symbolic link Points in SampleSet/MVS Data for the folder Points/:\n1 2 ln -s Points/ SampleSet/MVS\\ Data/Points # Or: ln -s Points/ SampleSet/MVS\\ Data/ Modify the paths and specify the scan to be evaluated in the matlab code:\n1 2 3 4 5 6 7 8 % ground-truth data: dataPath=\u0026#39;/mnt/data2_zichen/SampleSet/MVS Data\u0026#39;; % the dir storing testing .ply files: plyPath=\u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained\u0026#39;; % .mat fill will be saved here: resultsPath=\u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained\u0026#39;; - - - UsedSets=[1] Run BaseEvalMain_web in matlab:\nOutput 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt;\u0026gt;\u0026gt; BaseEvalMain_web cSet = 1 DataInName = \u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained/mvsnet001_l3.ply\u0026#39; EvalName = \u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrainedmvsnet_Eval_1.mat\u0026#39; /mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained/mvsnet001_l3.ply ans = 19 14 Elapsed time is 58.298360 seconds. downsample factor: 1.6901 Elapsed time is 18.930595 seconds. Computing Data 2 Stl distances Elapsed time is 83.304838 seconds. Computing Stl 2 Data distances Distances computed Elapsed time is 42.517009 seconds. Saving results Elapsed time is 42.864935 seconds. Elapsed time is 52.741538 seconds. ans = 19 18 mean/median Data (acc.) 0.254528/0.180807 mean/median Stl (comp.) 0.254594/0.218734 The samller the values, the better. CasMVSNet-pl (2024-02-23)\nSource code: CasMVSNet_pl - AIkui\nEnvironment Depth_raw:\nThe MVSNeRF inherited some codes of this repo, thus, in mvsnerf\u0026rsquo;s training dataset, the directory \u0026ldquo;Depths/\u0026rdquo; under \u0026ldquo;mvs_training/dtu/\u0026rdquo; is also replaced with the folder \u0026ldquo;Depths/\u0026rdquo; unzipped from \u0026ldquo;Depth_raw.zip\u0026rdquo;\nCreate environment:\n(2024-04-04)\n1 2 3 4 conda create -n casmvsnet_pl python=3.8 conda activate casmvsnet_pl pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 pip install -r requirements.txt Python version: Environment casmvsnet_pl requires python 3.7 for pytorch 1.4.0, and the packages listed in requirements.txt. If using python 3.10, the available pytorch version is higher than 1.11. (2024-04-07)\nEnv on Ubuntu 22.04 ctk-11.6 in China:\n1 2 3 4 conda create -n casmvsnet_pl python=3.8 # pip always breaks when downloading torch. conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.6 -c pytorch -c conda-forge pip install -r requirements.txt # comment torch entries I cannot compile inplace-abn via pip with erros:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/utils/cpp_extension.py:820: UserWarning: There are no g++ version bounds defined for CUDA version 11.6 warnings.warn(f\u0026#39;There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}\u0026#39;) building \u0026#39;inplace_abn._backend\u0026#39; extension gcc -pthread -B /home/zichen/anaconda3/envs/casmvsnet_pl/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn.cpp -o build/temp.linux-x86_64-cpython-38/src/inplace_abn.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14 cc1plus: warning: command-line option ‚Äò-Wstrict-prototypes‚Äô is valid for C/ObjC but not for C++ gcc -pthread -B /home/zichen/anaconda3/envs/casmvsnet_pl/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn_cpu.cpp -o build/temp.linux-x86_64-cpython-38/src/inplace_abn_cpu.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14 cc1plus: warning: command-line option ‚Äò-Wstrict-prototypes‚Äô is valid for C/ObjC but not for C++ /usr/local/cuda-11.6/bin/nvcc -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn_cuda.cu -o build/temp.linux-x86_64-cpython-38/src/inplace_abn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -std=c++14 /home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/c10/core/SymInt.h(84): warning #68-D: integer conversion resulted in a change of sign /usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with ‚Äò...‚Äô: 435 | function(_Functor\u0026amp;\u0026amp; __f) | ^ /usr/include/c++/11/bits/std_function.h:435:145: note: ‚Äò_ArgTypes‚Äô /usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‚Äò...‚Äô: 530 | operator=(_Functor\u0026amp;\u0026amp; __f) | ^ /usr/include/c++/11/bits/std_function.h:530:146: note: ‚Äò_ArgTypes‚Äô error: command \u0026#39;/usr/local/cuda-11.6/bin/nvcc\u0026#39; failed with exit code 1 Then, I tried to install it through compiling the source code. Unfortunately, the same error ocurred.\nI compiled the source code on Ubuntu 20.04 and ctk-11.6 (I had created a same conda env using the above 2 lines. When using the pip install manner, it will find the previous cache of the inplace-abn package on the machine.). Superisingly, the compilation suceeded.\nThe crucial difference between the 2 trials could be the gcc version gcc --version. Their return values are:\n1 2 3 gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 After I changed to gcc-9 on Ubuntu 22.04, the compilation for inplace-abn succeeded.\nI was reminded to check gcc due to ChatGPT\u0026rsquo;s response (prompted with the above error), and the issuse CUDA version (12.1) #232 and this issue asked gcc: failed install #143\nThere is another solution: inplace_abnÂÆâË£ÖÊä•ÈîôÔºüÊù•ÁúãÁúãËøôÁØáÈÅøÂùëÊåáÂçóÂêßÔºÅ But I didn\u0026rsquo;t try it.\nHow to change gcc version:\nRef: How to switch between multiple GCC and G++ compiler versions on Ubuntu 22.04 LTS Jammy Jellyfish\nThere are multiple gcc versions under /usr/bin/.\nBut, they didn\u0026rsquo;t show up:\n1 2 (casmvsnet_pl_) z@homepc:~/Downloads/inplace_abn$ update-alternatives --list gcc update-alternatives: error: no alternatives for gcc Install GCC 9: sudo apt-get install gcc-9 g++-9.\nIt\u0026rsquo;s said the 2 lines below are creating list for multiple gcc and g++:\n1 2 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 9 But once I exectuted them, the gcc --version has changed to gcc 9.5.0\nThe gcc and g++ will revert to 11.4 by executing:\n1 2 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 11 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 11 (2024-05-13)\nInstall gcc-7 I want to use gcc-7 to compile PCL. I installed g++-7 as below, and specify it as default:\n1 2 3 sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update \u0026amp;\u0026amp; apt install g++-7 -y sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 60 --slave /usr/bin/g++ g++ /usr/bin/g++-7 However, an error prompted when install g++-9, because it can't be master\n1 2 3 (base) yi@Alien:~$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 (base) yi@Alien:~$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 9 update-alternatives: error: alternative g++ can\u0026#39;t be master: it is a slave of gcc So, I use a one-line command similar to the above gcc-7:\n1 (base) yi@Alien:~$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 --slave /usr/bin/g++ g++ /usr/bin/g++-9 (Ref: Update gcc alternatives in Ubuntu 18.04 - g++ cannot be slave of gcc)\nLater, the versions can be chosen from a list:\nsudo update-alternatives --config gcc\nNot found GPU:\nAlthough the Pytorch 1.4.0 (installed with pip) was compiled with cuda 10.1 (download), the cudatoolkit (nvcc -V) on the PATH is not neccessary to match the cuda version.\nFor example, with using the same conda environemnt, Alien-PC (Ubuntu 20.04) using cuda-11.6 and the lambda server (Ubuntu 18.04) using cuda-10.2 both can detect GPU and run.\nThe error: torch.cuda.is_available() returns Falsehere is not due to cuda, but my debug settings!!! Fuck!! the environment variable CUDA_VISIBLE_DEVICES in launch.json was set to 6 for server, which should be 0 on my PC.\n(2024-04-13)\nCreate an environment again on Lambda server: Ubuntu 18.04, cudatoolkit 11.6 (nvcc -V), and gcc, g++ both are 7.5.0 (gcc -v)\n1 2 3 4 5 6 7 8 9 10 (casmvsnet_pl_py38) z@lambda-server:~/Downloads/CasMVSNet_pl-comments$ g++ -v Using built-in specs. COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper OFFLOAD_TARGET_NAMES=nvptx-none OFFLOAD_TARGET_DEFAULT=1 Target: x86_64-linux-gnu Configured with: ../src/configure -v --with-pkgversion=\u0026#39;Ubuntu 7.5.0-3ubuntu1~18.04\u0026#39; --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu Thread model: posix gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04) The pytorch is not install separately, just using the requirements.txt as below. Git commit\n1 2 3 conda create -n casmvsnet_pl_py38 python=3.8 conda activate casmvsnet_pl_py38 pip install -r requirements.txt Evaluation Evaluation and Depth Map Fusion: evalutions/\nDownload pretrained model checkpoint\n1 2 3 4 mkdir -p ckpts wget https://github.com/kwea123/CasMVSNet_pl/releases/download/v1.0/exp2.zip -P ckpts/ unzip ckpts/exp2.zip -d ckpts/ wget https://github.com/kwea123/CasMVSNet_pl/releases/download/v1.0/_ckpt_epoch_10.ckpt -P ckpts/exp2 Specify testing data\nI only left scan9 in \u0026ldquo;lists/test.txt\u0026rdquo; for a quick peek.\nThe code has some path templates that don\u0026rsquo;t match the DTU testing data directory structure.\nCode Modifications File: datasets/dtu.py, build_metas() (line #40-42):\n1 2 3 pair_file = \u0026#34;pair.txt\u0026#34; # Make 49 * 22 testing pairs for scan in self.scans: # add `scan` into path with open(os.path.join(self.root_dir, scan, pair_file)) as f: File: datasets/dtu.py, build_proj_mats (Line #58-59):\n1 2 3 # all scans have the same cams, so hardcard \u0026#39;scan1\u0026#39; here: proj_mat_filename = os.path.join(self.root_dir, \u0026#39;scan1\u0026#39;, f\u0026#39;cams/{vid:08d}_cam.txt\u0026#39;) File: datasets/dtu.py, __getitem__() (Line #165-166):\n1 2 3 # Add `scan` into path: img_filename = os.path.join(self.root_dir, scan, f\u0026#39;images/{vid:08d}.jpg\u0026#39;) File: eval.py, read_image() (line #87):\n1 2 if dataset_name == \u0026#39;dtu\u0026#39;: return cv2.imread(os.path.join(root_dir, scan, f\u0026#39;images/{vid:08d}.jpg\u0026#39;)) File: eval.py, (Line #352): set name format to align the matlab code (of the MVSNet_pytorch).\n1 PlyData([el]).write(f\u0026#39;{point_dir}/scan{int(scan[4:]):03d}_l3.ply\u0026#39;) Execute eval.py. (1080Ti is okay.)\n1 CUDA_VISIBLE_DEVICES=5 python eval.py --root_dir /data2/zichen/MVSNet_testing/dtu Visualization:\nImport the result point cloud \u0026ldquo;results/dtu/points/scan9.ply\u0026rdquo; into MeshLab:\ncasmvsnet mvsnet Some floaters will be exposed when looking at the point cloud from novel views.\nCasMVSNet-pl has less noise points than MVSNet-pytorch.\nModify the paths in \u0026ldquo;BaseEvalMain_web.m\u0026rdquo; (of the MVSNet_pytorch):\n1 2 3 4 5 6 dataPath=\u0026#39;/mnt/data2_zichen/SampleSet/MVS Data\u0026#39;; % GT plyPath=\u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points\u0026#39;; % pred resultsPath=\u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results\u0026#39;; % store .mat method_string=\u0026#39;scan\u0026#39;; - - - UsedSets=[9] Output for scan9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cSet = 9 DataInName = \u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points/scan009_l3.ply\u0026#39; EvalName = \u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/resultsscan_Eval_9.mat\u0026#39; /mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points/scan009_l3.ply ans = 0 35 Elapsed time is 412.622045 seconds. downsample factor: 5.5621 Elapsed time is 285.285496 seconds. Computing Data 2 Stl distances Elapsed time is 101.764399 seconds. Computing Stl 2 Data distances Distances computed Elapsed time is 106.225475 seconds. Saving results Elapsed time is 106.982512 seconds. Elapsed time is 126.089258 seconds. ans = 0 52 mean/median Data (acc.) 0.357586/0.229884 mean/median Stl (comp.) 0.304593/0.177679 Evaluate point clouds of all testing scans with matlab and save outputs into write.txt for calculating the average performance:\n1 2 3 4 5 6 7 8 9 10 11 12 import re import numpy as np means_acc = [] means_comp = [] with open(\u0026#34;/mnt/Server/Downloads/MVSNet_pytorch-comments/evaluations/dtu/write.txt\u0026#34;, \u0026#34;r\u0026#34;) as file: for line in file.readlines(): if \u0026#39;acc.\u0026#39; in line: means_acc.append( eval(re.findall(\u0026#39;\\d+\\.\\d+|\\d+\u0026#39;,line)[0]) ) elif \u0026#39;comp.\u0026#39; in line: means_comp.append( eval(re.findall(\u0026#39;\\d+\\.\\d+|\\d+\u0026#39;,line)[0]) ) print(np.mean(means_acc), np.mean(means_comp)) Output: 0.36399536, 0.36997940\nResults of every scan. scans accuracy Completness num pts 1 0.235012/0.177113 0.219228/0.176128 29.40M 4 0.267118/0.198705 0.376820/0.192314 23.06M 9 0.312725/0.196551 0.204539/0.169905 25.38M 10 23.63M 11 19.57M 12 21.47M 13 22.35M 15 26.06M 23 29.97M 24 24.95M 29 19.67M 32 22.06M 33 16.67M 34 28.18M 48 15.82M 49 19.90M 62 22.97M 75 18.60M 77 7.46 M 110 27.34M 114 0.223317/0.169992 0.249666/0.178679 31.52M 118 0.243320/0.183991 0.403758/0.195397 30.18M avg 22 (2024-04-03)\nVisualization for scan1:\nThe aliasing: Moir√© pattern (Êë©Â∞îÁ∫π) resulted from insufficient sampling frequency is wave interference (Ê≥¢ÁöÑÂπ≤Ê∂â). Eval LLFF Data (2024-04-15)\nConvert LLFF dataset to the format of MVSNet.\nUse Colmap to solve the LLFF images: Colmap Tutorial; Ytb (The camera poses in NeRF are also recovered by Colmap with imgs2poses), and then use the script colmap2mvsnet.py provided by MVSNet to obtain matched format.\nEavl T\u0026amp;T (2024-05-25)\nDataset: Tanks and Temples\nThe F-score of the testing sets (without GT published) should be evaluated by submitting the point cloud (.ply) to their webpage. (Github)\nFrom the leaderboard, the results of MVSNet and casmvsnet_pl can be found there.\n3DGS shows the PSNR, rather than F-score.\nThe training sets have GT provided.\nMVSNet Sec5.2 evaluates its f-score on the intermediate set (Family, Francis, Horse, Lighthouse, M60, Panther, Playground, Train) of T\u0026amp;T.\nThe intermediate.zip only contains .mp4 videos.\nN = 5, W = 1920, H = 1056 and D = 256\nMVSNet 418\t43.48\tN.A.\t55.99\t28.55\t25.07\t50.79\t53.96\t50.86\t47.9\t34.69\nCasMVSNet_pl is tested on T\u0026amp;T with the default parameter in eval.py.\nCasMVSNet_pl 296.25\t55.09\tN.A.\t76.4\t52.83\t49.08\t49.72\t56.24\t51.99\t53.87\t50.63 Point-MVSNet 390.50 48.27 N.A. 61.79 41.15 34.20 50.79 51.97 50.85 52.38 43.06 didn\u0026rsquo;t exhibit the T\u0026amp;T results in their paper.\nDepth map fusion (2024-03-06)\nCode-CasMVSNet_pl\n\\begin{algorithm} \\caption{Depth map fusion} \\begin{algorithmic} \\FOR{view=1 \\TO 49} \\STATE Read reference image \\FOR{src=1 \\TO 10} \\STATE Warp src depth map to be seen from the ref view \\STATE Reproject the warped dMap\\_src onto dMap\\_ref \\IF {Pixel diff \u003c 1 pix \\AND depth error \u003c 0.01} \\STATE mask\\_geo = 1 \\ENDIF \\ENDFOR \\STATE Average reprojected ref depth maps and images \\STATE Unproject the mean ref depth map to point cloud in world space \\ENDFOR \\STATE Output 49 point clouds \\end{algorithmic} \\end{algorithm} Procedure description refer to paper MVSNet sec 4.2 (2024-05-16)\nAs long the predicted depth is accurate, warping the source view based on the homography will result in the same scale as the ref view.\nTherefore, the same pixel coordinate on the warped src image and the ref image correspond to a same 3D point. In other words, the warped src view and the ref view can overlap.\nWarping is done by sampling pixels from the source depth map according to the homography from the ref view to a src view.\nThe fused depth map of a ref view is the average of refined depth maps after applying the photometric and geometric constraints on each pair of the ref and a src view\nApply the final mask on the depth_est_averaged, then unproject pixels on dpeth map to world space.\nBecause there are 49 views for a scan, the output .ply file is a combination of 49 point clouds.\nCasMVSNet depth map fusion process: Plotting code snippet 1 2 3 4 5 6 7 import numpy as np import matplotlib.pyplot as plt fig, axes = plt.subplots(2,3,figsize=(9,5),dpi=200) plt.tight_layout() image_ref = np.load(\u0026#34;./image_ref.npy\u0026#34;) axes[0][0].imshow(image_ref) axes[0][0].set_title(\u0026#34;image_ref: scan_1 view0\u0026#34;) With substituing the predicted depth into homography, the warped source image aligns with the reference image pretty much.\nSeen from the valid-depth mask, some highlight spots are missed.\nCasMVSNet-pl averaged the color of 10 source views that have performed homography transformation. (desc)\n1 2 image_refined_ = \\ np.sum(image_src2refs, 0)/np.expand_dims((mask_geo_sum+1), -1) Whereas, MVSNet-pytorch only casts the color of the refernce view.\n1 color = ref_img[1:-16:4, 1::4, :][valid_points] # hardcoded for DTU dataset ","date":"2024-02-21T09:00:00Z","image":"https://github.com/kwea123/CasMVSNet_pl/raw/master/assets/demo.png","permalink":"https://zichen34.github.io/writenotes/model/mvs/b-test-mvs_ply/","title":"test: Points - MVSNets | Restore Point Cloud from Depth Map"},{"content":"Create Matrix mat3 (2024-02-01)\nType: typedef mat3x3 mat3 API - GLM: Types\nPassing a single scalar will initialize the diagonal. OpenGL Mathematics (GLM)\nOpenGLÁü©ÈòµËøêÁÆó‚Äî‚ÄîGLMÂ∫ìÁöÑ‰ΩøÁî® - CSDN\nExample: 3DGS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;glm/glm.hpp\u0026gt; #include\u0026lt;glm/gtx/string_cast.hpp\u0026gt; #include \u0026lt;iostream\u0026gt; int main(){ glm::mat3 S = glm::mat3(1.0f); // identity matrix // Make scaling matrix S[0][0] = 2; S[1][1] = 3; S[2][2] = 4; std::cout \u0026lt;\u0026lt; glm::to_string(S) \u0026lt;\u0026lt; std::endl; return 0; } Output 1 2 3 (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ g++ test_glm.cpp (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ ./a.out mat3x3((2.000000, 0.000000, 0.000000), (0.000000, 3.000000, 0.000000), (0.000000, 0.000000, 4.000000)) Refer to How do I print vector values of type glm::vec3 that have been passed by reference? - SO\nUse glm::to_string(). Episode 18 - OpenGL Math - Introduction to the GLM Library - Modern OpenGL - Mike Shah\n","date":"2024-02-01T14:50:00Z","permalink":"https://zichen34.github.io/writenotes/lib/glm/","title":"memo: glm | Basics"},{"content":"Introducing CUDA UnBound (CUB) - Microway\nInclusiveSum Docs\n(2024-01-27)\n1 2 d_in: [8 , 6 , 7 , 5 , 3 , 0 , 9 ] d_out: [8 , 14 , 21 , 26 , 29 , 29 , 38 ] Example from 3DGS\n1 2 3 4 5 6 cub::DeviceScan::InclusiveSum( nullptr, // No memory is allocated geom.scan_size, // num of bytes needed to reserve geom.tiles_touched, // input sequence geom.tiles_touched, // becomes prefix sums P) // number of items in the sequence, 100K points Given a block of P bytes of memory pointed to by geom.tiles_touched, obtain geom.scan_size indicating the number of bytes for temporay storage required to reserve for executing the function: SortPairs Docs\n(2024-02-03)\nSort key-value pairs according to keys\nExample in diff-rast-gass\n1 cub::DeviceRadixSort::SortPairs (2024-03-05)\nExample in simple-KNN of 3DGS\nClustering points (indices) based on morton codes morton:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Determine the number of bytes for temp_storage cub::DeviceRadixSort::SortPairs( nullptr, temp_storage_bytes, morton.data().get(), // in: keys morton_sorted.data().get(), // out: sorted keys indices.data().get(), // in: values indices_sorted.data().get(), // out: sorted values P); // Allocate memory for temp_storage temp_storage.resize(temp_storage_bytes); // Sort the pairs based on keys cub::DeviceRadixSort::SortPairs( temp_storage.data().get(), temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); Reduce min (2024-02-28)\nTo execute this function, temp_storage_bytes is required to be reserved. Therefore, this function needs to run twice:\nGet the number of required temporary bytes, with specifying d_temp_storage as nullptr;\nExecute the function again given the correct address d_temp_storage\nA simplest code: \u0026ldquo;cub_reduce.cu\u0026rdquo;\nFull code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include \u0026lt;cub/cub.cuh\u0026gt; // CustomMin functor struct CustomMin { template \u0026lt;typename T\u0026gt; __device__ __forceinline__ T operator()(const T \u0026amp;a, const T \u0026amp;b) const { return (b \u0026lt; a) ? b : a; } }; // Declare, allocate, and initialize device-accessible pointers for // input and output int N = 7; // num of items int d_in[7] = {8, 6, 7, 5, 3, 1, 9}; int d_out[1] = {0}; // the min value CustomMin min_op; int init = INT_MAX; // To be compared initially int main(){ int *in, *out; cudaMalloc(\u0026amp;in, sizeof(int)*N); cudaMalloc(\u0026amp;out, sizeof(int)); cudaMemcpy(in, d_in, sizeof(int)*N, cudaMemcpyHostToDevice); // Determine temporary device storage requirements void *d_temp_storage = NULL; size_t temp_storage_bytes = 0; cub::DeviceReduce::Reduce( d_temp_storage, temp_storage_bytes, in, out, N, min_op, init); cudaDeviceSynchronize(); // Allocate temporary storage cudaMalloc(\u0026amp;d_temp_storage, temp_storage_bytes); // Run reduction cub::DeviceReduce::Reduce( d_temp_storage, temp_storage_bytes, in, out, N, min_op, init); cudaDeviceSynchronize(); int min; cudaMemcpy(\u0026amp;min, out, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;The min is: %d\\n\u0026#34;, min); // 1 // Free allocated memory cudaFree(d_temp_storage); return 0; } Notes:\nIf the suffix is .cpp and when compiling with nvcc cub_reduce.cpp, there will be errors about Debug:\n1 2 3 4 5 6 /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/block/specializations/../../block/../util_debug.cuh:101:43: error: ‚ÄòDebug‚Äô is not a member of ‚Äòcub‚Äô 101 | #define CubDebug(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__) | ^~~~~ /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/util_allocator.cuh:695:17: note: in expansion of macro ‚ÄòCubDebug‚Äô 695 | if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error; | ^~~~~~~~ __host__ __forceline function cannot be called, need __device__\n1 2 3 4 5 6 /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/device/dispatch/dispatch_reduce.cuh(145): error: calling a __host__ function(\u0026#34;T1 CustomMin::operator ()\u0026lt;int\u0026gt; (const T1 \u0026amp;, const T1 \u0026amp;) const\u0026#34;) from a __global__ function( \u0026#34;cub::DeviceReduceSingleTileKernel\u0026lt; ::cub::DeviceReducePolicy\u0026lt;int, int, int, ::CustomMin\u0026gt; ::Policy600, int *, int *, int, ::CustomMin, int\u0026gt; \u0026#34;) is not allowed /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/device/dispatch/dispatch_reduce.cuh(145): error: identifier \u0026#34;CustomMin::operator ()\u0026lt;int\u0026gt; const\u0026#34; is undefined in device code The customize min_op (or CustomMin()) can be changed to cub::Sum() to solve the sum of input data.\nRef:\ncub::DeviceReduce ‚Äî CUB 104.0 documentation\nCUDAÈ´òÊÄßËÉΩËÆ°ÁÆóÁªèÂÖ∏ÈóÆÈ¢òÔºà‰∏ÄÔºâ‚Äî‚Äî ÂΩíÁ∫¶ÔºàReductionÔºâ - Will ZhangÁöÑÊñáÁ´† - Áü•‰πé\nCUDAÁºñÁ®ãÊ®°ÂûãÁ≥ªÂàóÂÖ´(ÂéüÂ≠êÊìç‰Ωú / ËßÑÁ∫¶ / ÂêëÈáèÂÖÉÁ¥†Ê±ÇÂíå) - Êâ´Âú∞ÁöÑÂ∞è‰ΩïÂ∞ö (‰ΩïÁê®) Found by DDG searching \u0026ldquo;cuda cub ‰æãÂ≠ê\u0026rdquo;\nsum (2024-03-04)\nMove the input data onto GPU first! Otherwise, it won\u0026rsquo;t be processed and the output variable won\u0026rsquo;t update and remains the initialized value, e.g., 0.\nMove the output data back to host for printing! Otherwise, the gpu memory is not allowed to access and return segment fault.\nFull code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #include \u0026lt;cub/cub.cuh\u0026gt; int N=7; // num of items int d_in[7] = {8, 6, 7, 5, 3, 0, 9}; int d_out[1]={0}; int main(){ int *in, *out; cudaMalloc(\u0026amp;in, sizeof(int)*N); // allocate memory cudaMalloc(\u0026amp;out, sizeof(int)); cudaMemcpy(in, d_in, sizeof(int)*N, cudaMemcpyHostToDevice); // Determine temporary device storage requirements void *d_temp_storage = NULL; size_t temp_storage_bytes = 0; cub::DeviceReduce::Sum( d_temp_storage, temp_storage_bytes, in, out, N); // Allocate temporary storage cudaMalloc(\u0026amp;d_temp_storage, temp_storage_bytes); // Run sum-reduction cub::DeviceReduce::Sum( d_temp_storage, temp_storage_bytes, in, out, N); int sum=0; cudaMemcpy(\u0026amp;sum, out, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;%d\\n\u0026#34;, sum); } Ref:\nDocs-Snippest\nExample Code Reference by this SO question: Sum reduction with CUB surfaced by DDG with searching: \u0026ldquo;cub::DeviceReduce::Reduce example tutorial\u0026rdquo;\n","date":"2024-01-27T15:35:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/cub/","title":"memo: CUDA | CUB"},{"content":"White background Code from 3DGS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pathlib import Path from PIL import Image image_path = Path(path) / cam_name # \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/./train/r_0.png\u0026#39; image_name = Path(cam_name).stem # r_0 image = Image.open(image_path) # \u0026lt;class \u0026#39;PIL.PngImagePlugin.PngImageFile\u0026#39;\u0026gt; im_data = np.array(image.convert(\u0026#34;RGBA\u0026#34;)) # (800,800,4) bg = np.array([1,1,1]) if white_background else np.array([0, 0, 0]) # Normalize to [0,1] norm_data = im_data / 255.0 # Re-composite img_arr = norm_data[...,:3] * norm_data[...,-1:] + bg * (1-norm_data[...,-1:]) # Convert to image img_rgb = Image.fromarray(np.array(img_arr * 255.0, dtype=np.byte), \u0026#39;RGB\u0026#39;) Scaling image Zooming image requires changing the focal lengths together, while cropping image doesn\u0026rsquo;t need.\nDownsize h and w, the focal also downscales, see NeRF:\n1 2 3 4 5 6 factor 4 # shrink raw image to 1/4 args = \u0026#39; \u0026#39;.join([\u0026#39;mogrify\u0026#39;, \u0026#39;-resize\u0026#39;, f\u0026#39;{100./factor}%\u0026#39;, \u0026#39;-format\u0026#39;, \u0026#39;png\u0026#39;, \u0026#39;*.{}\u0026#39;.format(ext)]) ... # the 5th column is hwf poses[:2, 4, :] = np.array(sh[:2]).reshape([2, 1]) # hw poses[2, 4, :] = poses[2, 4, :] * 1./factor # focal A b l e t f o s e e 4 b l k s . C l o s e r : o ¬Ω n f l y 1 t i l e f i l l s i n e y e s Example: CasMVSNet has 3 levels of feature maps, so the first 2 rows of the camera intrinsics are scaled up along with the image size increases:\n1 2 for l in reversed(range(self.levels)): intrinsics[:2] *= 2 # 1/4-\u0026gt;1/2-\u0026gt;1 Crop a patch doesn\u0026rsquo;t affect focals referring to GNT\nToTensor 1 from torchvision import transforms fov Code from 3DGS\ni m g f p l a n e l e f t z n _ e n a e r a r p l a n e r i g h t Field of view: fovX = $2* arctan(\\frac{width}{2f})$ Near plane\u0026rsquo;s right boundary: $z_{near} * tan(fovX)$ Convert fov to focal\n1 2 def fov2focal(fovX, width): # 1111.11103, 800 return width / (2 * math.tan(fov / 2)) Near plane computed from fov\n1 2 3 4 5 6 7 tanHalfFovY = math.tan((fovY / 2)) tanHalfFovX = math.tan((fovX / 2)) top = tanHalfFovY * znear bottom = -top right = tanHalfFovX * znear left = -right Pixel Coords (2024-03-14)\nnp.mgrid. Example from casmvsnet_pl\n1 2 3 xy_ref = np.mgrid[:args.img_wh[1],:args.img_wh[0]][::-1] # (2, args.img_h, args.img_w) # restore depth for (x,y): xyz_ref = np.vstack((xy_ref, np.ones_like(xy_ref[:1]))) * depth_refined[ref_vid] # (3:xyz, h,w) np.meshgrid. Example form MVSNet_pytorch\n1 2 3 4 5 xx, yy = np.meshgrid(np.arange(0, width), np.arange(0, height)) print(\u0026#34;yy\u0026#34;, yy.max(), yy.min()) yy = yy.reshape([-1]) xx = xx.reshape([-1]) X = np.vstack((xx, yy, np.ones_like(xx))) torch.meshgrid. Example form MVSNet_pytorch\n1 2 3 4 5 6 y, x = torch.meshgrid([torch.arange(0, height), torch.arange(0, width)]) y, x = y.contiguous(), x.contiguous() y, x = y.view(height * width), x.view(height * width) xyz = torch.stack((x, y, torch.ones_like(x))) # [3, H*W] xyz = torch.unsqueeze(xyz, 0).repeat(batch, 1, 1) # [B, 3, H*W] torch.cartesian_prod referred by Docs\n1 2 3 h, w = ref.shape[:2] vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed Write Image (2024-04-02)\nExample of cv2 in casmvsnet_pl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from PIL import Image # pillow import cv2 import numpy as np root_dir = \u0026#34;/mnt/data2_z/MVSNet_testing/dtu\u0026#34; img_path = f\u0026#39;{root_dir}/scan1/images/00000000.jpg\u0026#39; img = np.array(Image.open(img_path)) # RGB fig, ax = plt.subplots(1,2) ax[0].imshow(img) cv2.imwrite(f\u0026#39;1.png\u0026#39;, img[:,:,::-1]) # save ax[1].imshow(cv2.imread(img_path)) # nd array, BGR img_read = cv2.imread(img_path)[:,:, ::-1] # RGB print((img_read == img).all()) ","date":"2024-01-22T17:25:00Z","permalink":"https://zichen34.github.io/writenotes/vis/image_rw/","title":"memo: Vis | Image Read/Write"},{"content":"(Feature image credits: GS big picture, please say thanks to GS.#419 - yuedajiong)\ngaussian-splatting python Source code\nDefine Params (2024-01-22)\nCreate PyTorch tensors for Gaussians\u0026rsquo; parameters at GaussianModel() by __init__ them with torch.empty(0):\n_xyz (Gaussian center), _features_dc and _features_rest (SH coeffs), _scaling (vec3), _rotation(quaternion), sh_degree (color), _opacity (unweighted alpha)\nEach covariance matrix ùö∫ is built from a scaling vector and a quaternion:\ns q c u a a l t i e n r g n i v o e n c S R t o r t e a c t h i i o n n g m m a a t t r r i i x x ùêë ùêí ‚Üó ‚Üò ùö∫ = ùêë ùêí ùêí ·µÄ ùêë ·µÄ t l r o i w a e n r g ? l e ` s y m m ` symm is [Œ£‚ÇÄ‚ÇÄ, Œ£‚ÇÄ‚ÇÅ, Œ£‚ÇÄ‚ÇÇ, Œ£‚ÇÅ‚ÇÅ, Œ£‚ÇÅ‚ÇÇ, Œ£‚ÇÇ‚ÇÇ]\nRead point cloud and cameras at Scene() by calling __init__\nSceneInfo:\nBasic point cloud: points(x,y,z), normals (nx,ny,nz), colors (r,g,b)\nCameraInfo: extrinsics (R,T), fov (FovX,FovY), gt images (image, image_name, image_path, width, height)\nnerf_normalization (avg_cam_center (translate), max displacement from the avg cam (radius)),\nCameras list train_cameras of 300 Camera() is made up by repeatly loadCam()\nResize GT image for each camera (i.e., 300 views).\nViewing transform (world‚û°camera): self.world_view_transform\nProjection matrix (camera‚û°clip): self.projection_matrix\n$$ ùêè = \\begin{bmatrix} \\frac{2n}{r-l} \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f}{f-n} \u0026amp; \\frac{-fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $$\nBy denoting sight width as $W$, the above $r=\\frac{W}{2},\\ l=-\\frac{W}{2}$, so $(r-l)=W$. The sight center (principle center) (cx,cy) is (r,l), i.e., (W/2, H/2) The clip coordinate of $x$ produced by multiplying ùêè will result in $x_{NDC} ‚àà [-1,1]$\nBy using this ùêè, the final $z$ axis of ND coordinate ranges in [0,1], unlike the usual NDC cube of [-1,1]\n$$ near_{NDC} = \\frac{ \\frac{f * near}{f-n} + \\frac{-fn}{f-n} }{n} = 0 \\\\ far_{NDC} = \\frac{ \\frac{f*far}{f-n} + \\frac{-fn}{f-n} }{f} = 1 $$\nTherefore, this ùêè leads to a cuboid NDC space, rather than a cube NDC space. Figuratively, in the clip space, the points satisfying $0\u0026lt;z_{clip} \u0026lt; w_{clip}$ and $-w_{clip} \u0026lt; x_{clip}, y_{clip} \u0026lt; w_{clip}$ will be excluded from rendering. After clipping those point, the remaining space is the NDC space.\nThe transformation from world to clip space (P@w2c)·µÄ for each camera: self.full_proj_transform\nInitialize Gaussians\u0026rsquo; parameters create_from_pcd\n_xyz is set as the basic piont cloud _features_dc and _features_rest are converted from the color of the basic point cloud. The 0th-order SH coeffs is the rgb. And the rest of coeffs are initialized as 0. spatial_lr_scale is set to the max displacement from the avg cam center. _scaling is initialized with simple_KNN. _rotation is preset with 0 and 1 _opacity is initilized as the inverse of sigmoid(0.1) Training optimization settings, gaussians.training_setup()\nlr for each parameter Render (2024-01-23)\nrender()\nscreenspace_points: Gaussian centers on the 2D screen are initialized as 0. diff_gauss_raster (2023-11-18)\nCode | Explaination: kwea123/gaussian_splatting_notes\nPython Invokes The Python program gaussian_renderer/__init__.py calls GaussianRasterizer.forward() to render an image tile-by-tile.\nInternally, program goes to the submodule\u0026rsquo;s _RasterizeGaussians.forward() in __ini__.py, analogous to the \u0026ldquo;test.py\u0026rdquo; that calls the cuda-extension package\u0026rsquo;s methods, which are wrapped in PyTorch\u0026rsquo;s forward and backward.\n_C is a class of the diff_gaussian_rasterization package, as set in setup.py\nThe names of package\u0026rsquo;s methods are set in PYBIND11_MODULE()\nforward Steps RasterizeGaussiansCUDA() in rasterize_points.cu\nDefine geomBuffer, binningBuffer, and imgBuffer for storing data of point cloud, tiles, and the rendered image. Their memory will be allocated through a Lambda function resizeFunctional(), where tensor\u0026rsquo;s resize_ method is called. ‚ûî CudaRasterizer::Rasterizer::forward() in cuda_rasterizer/rasterizer_impl.cu\n‚ûî FORWARD::preprocess() in cuda_rasterizer/forward.cu\n‚ûî preprocessCUDA() in forward.cu\n\\begin{algorithm} \\begin{algorithmic} \\STATE RasterizeGaussiansCUDA \\STATE $\\quad$ CudaRasterizer::Rasterizer::forward \\STATE $\\qquad$ geomState, imgState \\COMMENT{Allocate memory} \\STATE $\\qquad$ FORWARD::preprocess \\STATE $\\qquad$ cub::DeviceScan::InclusiveSum \\COMMENT{Count tiles} \\STATE $\\qquad$ binningState \\STATE $\\qquad$ duplicateWithKeys \\STATE $\\qquad$ getHigherMsb \\STATE $\\qquad$ cub::DeviceRadixSort::SortPairs \\STATE $\\qquad$ identifyTileRanges \\STATE $\\qquad$ FORWARD::render \\end{algorithmic} \\end{algorithm} preprocessCUDA Frustum culling\nDelete points whose w (equal to depth) is larger than x, y, z in clip space. However, in_frustum() is based on coordinates in the camera space.\nConstruct 3D covariance matrix. Code\nGiven a quaternion $q = [r, \\ x, \\ y, \\ z]$, the rotation matrix is: wiki\n$$ ùêë = \\begin{bmatrix} 1 - 2(y¬≤ + z¬≤) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 2(xy + rz) \u0026amp; 1 - 2(x¬≤ + z¬≤) \u0026amp; 2(yz - rx) \\\\ 2(xz - ry) \u0026amp; 2(yz + rx) \u0026amp; 1 - 2(x¬≤+y¬≤) \\end{bmatrix} $$\nThe stretching matrix is diagonal and represented as a 3D vector $ùêí = [x, \\ y, \\ z]$\nCovariance matrix (3x3) of 3D Gaussian cov3D: ùö∫ = ùêëùêíùêí·µÄùêë·µÄ\nCovariance matrix (3x3) is projected onto 2D screen cov: ùö∫\u0026rsquo; = ùêâ ùêñ ùö∫ ùêñ·µÄùêâ·µÄ. Code\nTake the inverse of ùö∫\u0026rsquo; (ùö∫\u0026rsquo;‚Åª¬π), conic, to evaluate the 2D Gaussian PDF. Code\nThe extent of a point (Gaussian center) is a bounding square, where the \u0026ldquo;radius\u0026rdquo; from each side to the projected pixel is my_radius: 3œÉ. Code\nThe radius of the circumscribed circle for a 2D Gaussian is the max eigenvalue of the 2D covariance matrix $ùêÄ = [^{a \\ b}_{b \\ c}]$. The eigenvalues can be solved from:\n$$ det (ùêÄ - Œª‚ãÖùêà) = 0 \\\\ \\begin{vmatrix} a-Œª \u0026amp; b \\\\ b \u0026amp; c-Œª\\end{vmatrix} = 0 \\\\ (a-Œª)(c-Œª) - b^2 = 0 \\\\ Œª^2 - (a+c)Œª + ac-b^2 =0 $$\nTwo roots: $Œª‚ÇÅ,\\ Œª‚ÇÇ = \\frac{(a+c)}{2} ¬± \\sqrt{\\frac{(a+c)¬≤}{4}-(ac-b¬≤)}$\n1 2 3 4 5 float det = (cov.x * cov.z - cov.y * cov.y); // determinant - - - float mid = 0.5f * (cov.x + cov.z); float lambda1 = mid + sqrt(max(0.1f, mid * mid - det)); float lambda2 = mid - sqrt(max(0.1f, mid * mid - det)); Pixel coordinates: Scale the ND coordinates of x,y ‚àà [-1,1] to image-grid coordiantes point_image [0,W] through viewport transformation: $x = \\frac{W‚ãÖ(x-1)}{2} -0.5,\\ y=\\frac{H‚ãÖ(y-1)}{2} -0.5$\nPrepare Tiles (2024-02-02)\nObtain the total sum of touched tiles for all Gaussian centers by inclusive sum\n1 2 3 4 5 6 cub::DeviceScan::InclusiveSum(geomState.scanning_space, geomState.scan_size, geomState.tiles_touched, geomState.point_offsets, P) - - - int num_rendered; // total number CHECK_CUDA(cudaMemcpy(\u0026amp;num_rendered, geomState.point_offsets + P - 1, sizeof(int), cudaMemcpyDeviceToHost), debug); InclusiveSum produces a sequence of prefix sum for tiles_touched (the number of touched tiles) of 100k points.\nThe pointer point_offsets points to the first byte of the 100k-byte sequence.\nn t p s u i r u m l e m e f : o s i f : x p p o 1 9 9 i n t p 1 1 _ 2 0 o f p 2 3 f 3 5 5 s e ‚ãØ 1 5 t 6 1 s 3 8 6 7 4 9 1 4 1 9 4 0 ‚ãØ ‚ãØ ‚ãØ ‚ãØ 8 1 6 4 p p 4 2 o 1 . i 0 5 n 0 e t K 6 _ o f f s e t s + 9 9 9 9 9 P = 100k bytes (char) is the number of points (Gaussian center).\nSo, point_offsets + P - 1 is the last element in the prefix-sum sequence. Specifically, it\u0026rsquo;s the total number of touched tiles for all 100k points.\nEvaluate each Gaussian to pair every touched tile\u0026rsquo;s index and Gaussian\u0026rsquo;s depth: Code\n1 duplicateWithKeys \u0026lt;\u0026lt; \u0026lt;(P + 255) / 256, 256 \u0026gt;\u0026gt; \u0026gt; ( Iterate each tile that a Gaussian overlaps, and combine the tile index and the Gaussian\u0026rsquo;s depth to form a key\n1 2 3 4 5 for (int y = rect_min.y; y \u0026lt; rect_max.y; y++) { for (int x = rect_min.x; x \u0026lt; rect_max.x; x++) { gaussian_keys_unsorted[off] = key; 100x100 image are divided to tiles with size of 16x16, so the thread gridDim: (7,7,1)\n0 1 2 3 4 5 6 0 ‚ñ¶ 1 2 r e c t 3 _ m i n 4 5 6 r e c t 3 _ œÉ m a x For example, the above small Gaussian covers 9 tiles at: (0,1), (1,1), (2,1), (0,2), (1,2), (2,2), (0,3), (1,3), (2,3)\nThe tile index is computed as y*grid.x + x. Therefore, the 9 tiles\u0026rsquo; indices are: 8,9,10, 15,16,17, 22,23,24\nSuppose the tile id is 24 and the Gaussian\u0026rsquo;s depth is 15.7 (cast to int 15), the corresponding pair\u0026rsquo;s key is:\n1 2 3 tile id 24: 0000_0000_0000_0000_0000_0000_0001_1000 Shift left 32 bits: 0000_0000_0000_0000_0000_0000_0001_1000_0000_0000_0000_0000_0000_0000_0000_0000 combine depth (15): 0000_0000_0000_0000_0000_0000_0001_1000_0000_0000_0000_0000_0000_0000_0000_1111 Because each Gaussian is processed by a thread in parallel, the prefix sum point_offsets is referenced to locate the starting cell for each Gaussian to store keys into corresponding area of the array gaussian_keys_unsorted:\nP t r s k h e u o e r f m f y e i s f s a x : : : d : 0 0 1 2 G a 3 u 1 s s 4 5 6 7 8 G a 9 9 u 2 s s 1 1 0 0 G a 3 - u ‚ãØ s s ‚ãØ ‚ãØ ‚ãØ ‚ãØ G 1 a 0 u 0 s k s 2 2 . . 5 5 e e 6 6 The value is the Gaussian\u0026rsquo;s index ranging in [0, 100k-1], i.e., the thread index idx:\n1 2 3 auto idx = cg::this_grid().thread_rank(); --- gaussian_values_unsorted[off] = idx With performing SortPairs, the tile-Gaussian pairs sequence (binningState.point_list_keys) is arranged based on the tile ID, and for keys with identical tile id, the depths are then sorted.\n1 2 3 4 5 CHECK_CUDA(cub::DeviceRadixSort::SortPairs( ... binningState.point_list_keys_unsorted, binningState.point_list_keys, binningState.point_list_unsorted, binningState.point_list, ... K G e a y u i s s d : s : 8 9 4 7 0 t i 3 8 l 0 5 3 e 7 5 0 6 0 9 3 6 5 1 9 4 7 2 6 7 2 0 7 8 0 t i 3 5 l 9 6 8 e 6 8 1 5 5 3 3 1 8 8 1 8 2 9 4 2 3 4 5 2 4 - 1 5 ‚ãØ 6 0 9 ‚ãØ t i 8 5 l 2 1 3 e 1 0 4 1 4 3 8 2 0 2 3 5 Evaluate each pair to identify the boundaries ranges of each tile for rendering. Code\n1 identifyTileRanges \u0026lt;\u0026lt; \u0026lt;(num_rendered + 255) / 256, 256 \u0026gt;\u0026gt; \u0026gt; ( B t ( o K h p u e r a r n y e i a d s a r n s : d s g : : ) e | s [ t 0 i ] l . e x 0 r r a a n n g g e e s s [ [ 1 0 ] ] t . . i x y l e 1 r r a a n n g g e e s s [ [ - 2 1 ‚ãØ ] ] . . x y ‚ãØ t i l e r a 4 n 8 g e s [ 4 8 ] . y Count the number of tile-Gaussian pairs for each tile, as the threadIdx is the counter.\n1 auto idx = cg::this_grid().thread_rank(); Identify the tile\u0026rsquo;s boundary by detecting the change of tiles index in the list of the sorted tile-Gaussian keys: 1 if (currtile != prevtile) Render Pixel (2024-02-05)\nA block for a tile; A thread for a Gaussian; A pixel is an iterative composition of multiple Gaussian. Ultimately, in the end, a thread in a block corresponds to a pixel.\nA thread syntheses a single pixel by performing alpha compositing Gaussian-by-Gaussian.\n1 2 renderCUDA\u0026lt;NUM_CHANNELS\u0026gt; \u0026lt;\u0026lt; \u0026lt;grid, block \u0026gt;\u0026gt; \u0026gt; ( // grid: (7,7,1), block: (16,16,1), 49*256 threads for 100x100 pix A pixel\u0026rsquo;s color is rendered within the tile encompassing it, based on the obtained sorted sequence of Gaussians\u0026rsquo; indices.\nAll pixels in a tile correspond to the same Gaussians, but with different weights, as the weights depend on distance from the pixel coordinates to the 2D Gaussian center: $exp(-¬Ω (ùê±-\\bm Œº)^T \\bm Œ£_{(2D)}^{-1} (ùê±-\\bm Œº))$.\n1 6 A 1 6 ‚ñ† t i ‚ñ† l e G a u a ‚ñ® ‚ñ® s s G a u b ‚ñ® ‚ñ® s s G a u c ‚ñ® ‚ñ® s s ‚ãØ ‚ãØ G a u j ‚ñ® ‚ñ® s s ‚ãØ ‚ãØ G r a a u n ‚ñ® ‚ñ® s g s e 1 2 3 4 5 6 7 for (int j = 0; !done \u0026amp;\u0026amp; j \u0026lt; min(BLOCK_SIZE, toDo); j++) { ... float2 xy = collected_xy[j]; // pixel coords float2 d = { xy.x - pixf.x, xy.y - pixf.y }; // dist float4 con_o = collected_conic_opacity[j]; // cov float power = -0.5f * (con_o.x * d.x * d.x + con_o.z * d.y * d.y) - con_o.y * d.x * d.y; The coordinates of the pixel to be rendered by a thread is obtained as follows:\n1 p 6 i x p e i l x s _ A A m i p t n t i h i x r ‚ñ° l e e e l a : d 1 s d ( : 6 h b ( a m l p ( p r e o i b i e m c x l x k _ o e ) m c l i k s p ( n . i 2 . t x , x h _ 1 + r m ) t e a h a x r d G G G e _ a a a a i u u u d n s s s - d s s s i e i i i n x a a a d ( n n n - ) x . i x c , x n y o , d v p e i b x m x l a _ o t m c i k a n . n . t d y h + r o t e p h a a r d c e _ i a i t d n y - d i e n x d ( - ) y . ) y ) pix_min = (2 * BLOCK_X, 1 * BLOCK_Y) = (32, 16) pix_max = (max(32+BLOCK_X, W), max(16+BLOCK_Y, H)) = (48, 32) pix = (32+block.thread_index().x, 16+block.thread_index().y) pix_id = 100*pix.y + pix.x tile id = 1*7 + 2 = 9 range = ranges[9], number of Gaussians for this tile. After the pixel is fully rendered as the accumulated transmittance T is close to 0.0, this thread is assigned to transfer data (Gaussian properties) from the global memory to shared memory.\n(2023-02-06) A 16√ó16 thread block works repeatedly for one tile.\nIn NeRF, a rendering batch comprises multiple rays emitted from pixels, while in 3DGS, a rendering batch is a tile (16x16) of pixels.\nDifferent tile has different number of tile-Gaussian pairs (range). Thus, each tile requires varying computations.\nEach pixel in a tile results from multiple (range) tile-Gaussian pairs. Each tile-Gaussian pair is a \u0026ldquo;filter composition\u0026rdquo; and requires a thread to execute once. As only a single 16x16 thread block is assigned to render the tile. To fulfill all the tile-Gaussian pairs composition, the equal amount (range) of thread executions are needed. Thus, the computation for a tile has to re-use the 16x16 thread block several times repeatedly, i.e., rounds times.\n1 2 3 uint2 range = ranges[block.group_index().y * horizontal_blocks + block.group_index().x]; // given tile index, read (starting pair, ending tile-Gaussian pair) const int rounds = ((range.y - range.x + BLOCK_SIZE - 1) / BLOCK_SIZE);\t// the number of 16x16 blocks are allocated based on the number of tile-Gaussian pairs int toDo = range.y - range.x;\t// number of tile-Gaussian pairs to render the tile For example, if the tile 0 corresponds to 158 tile-Gaussian pairs in the sorted list: point_list_keys, the 16x16 thread block runs only once to render pixels in tile 0. And if tile 1\u0026rsquo;s range is 768 tile-Gaussian pairs, 3-round executions are needed.\n1 ‚ãÆ ‚ãÆ H 0 5 0 b l C t 1 k o i ( m l r 0 p e o , u u 0 t 0 n ) e d 1 5 1 6 b l C t 3 k o i ( m l r 1 p e o , u u 0 t 1 n ) e d 3 s 1 3 2 b l C t k o i ( m l 2 p e , u 0 t 2 ) e 4 7 4 8 b l k ( 3 , 0 ) 6 4 ‚ãØ ‚ãØ 9 6 b l k W ( 7 , 1 0 1 ) 2 Every Gaussian gets added onto every pixel (color += color * alpha * T) until a pixel gets fully rendered, i.e., T is close to 0:\n1 2 3 4 5 6 7 8 9 10 11 12 // Iterate tiles for (int i = 0; i \u0026lt; rounds; i++, toDo -= BLOCK_SIZE) { // if pixels in the tile are all rendered, break ... // Iterate Gaussians for (int j = 0; !done \u0026amp;\u0026amp; j \u0026lt; min(BLOCK_SIZE, toDo); j++) { ... // Add a Gaussian to every pixel with specific weight for (int ch = 0; ch \u0026lt; CHANNELS; ch++) C[ch] += features[collected_id[j] * CHANNELS + ch] * alpha * T; The alpha compositing can be calculated in parallel, as the cumulative summation is definite and independent of the adding sequence.\n(2024-04-27)\nA tile (16x16 pixels) is rendered from many tile-Gaussian pairs, which are composited batch-by-batch iteratively.\n1 6 A 1 6 ‚ñ† t i l e O i u t t e G e r a r u a ‚ñ® 1 s l s o f o o G p r a : u b ‚ñ® 2 s 5 s 6 G G a a u c ‚ñ® u s s s s i a ‚ãØ n s G a u j ‚ñ® s s T ‚ãØ h e l a G r s a a t u n ‚ñ® s g i s e t e r Each iteration processes 16x16 (256) tile-Gaussian pairs, because each tile is allocated with 16x16 threads.\nThe shared memory is leverage to speed up the inner for loop, which is for compositing the contributing Gaussians for the pixel processed by the thread.\nRender Depth (2024-04-28)\nRefer to code ashawkey/diff-gaussian-rasterization\nThe depth of a pixel is composited behind the alpha compositing of colors. Code.\n1 D += depths[collected_id[j]] * alpha * T; depths is the z coordinate of 3D Gaussian centers in the camera space. collected_id is the Gaussians index stored in shared memory. j is the index of a Gaussian in a round. alpha is the portion of Gaussian\u0026rsquo;s color that will be used. T is the accumulated decay factor for the current Gaussian color passing through filter before it. Similar to NeRF, where Œ± is the probability of ray terminating and T is the prob of ray passing, the depth map is an expectation of Gaussians (\u0026ldquo;probability * depth\u0026rdquo;), where the weights (\u0026ldquo;probability\u0026rdquo;) are alpha * T.\n(2024-05-05)\nSimilar to MVSNet, by regarding the Œ±T as the probability of depth, the \u0026ldquo;pre-defined hypothetical\u0026rdquo; depth values are aggregated by probabilities to form a predicted pixel depth.\nZiyi Yang believes: Using z-coordinate in camera space to represent depth is acceptable, because the depth essentially represents relative spatial relations, similar to z coordinate.\nHowever, robot0321 rebuttuled that regarding the z-value expectation as depth may cause error. And the depth map can be accquired by locating the frontmost point if there\u0026rsquo;s only one surface. conversation Other papers also applied this form: FSGS, SparseGS\n(2024-05-02)\nThe depths of Gaussians in the camera space are already included in the geometryBuffer.\nThe \u0026ldquo;buffer\u0026rdquo; memory allocated on GPU is for input data to kernel functions (depths, radii, points_xy_image, \u0026hellip;) Code.\nThe forward pass only performs depth rendering and return it, by mofifying files: cuda_rasterizer/forward.cu\nAnd change the function signatures in cuda_rasterizer/rasterizer_impl.cu.\nBackward Looking up definitions: backward.cu ‚¨Ö rasterizer_impl.cu ‚¨Ö rasterize_points.cu ‚¨Ö __init__.py\nbw render pixel color (2024-02-09) Calling entry\nThe rendered pixel is derived from color (R,G,B) and opacity of each Gaussian.\nDerivative flows backward:\nm e a n 2 D o w p ùö∫ e a ' i c ‚Åª g i ¬π h t t y o G G a c G u o a a s l u l s o s p i r s h a i a n a ‚Çñ n ‚Çñ ‚àë c Œ± T P c i o x l e o l r L o s s The partial derivatives of Loss w.r.t. each Gaussian can be computed in parallel similar to forward rendering, as the contribution is a summation?\nIncoming upstream gradient: dL_dpixels Accumulated transmittance over all Gaussians is known, so accumulated transmittance of each Gaussian can be accquired with: $T_{curr} = \\frac{T_{curr+1}}{(1-Œ±_{curr})} $ (2024-02-10)\nFor pixel color scalar (R,G,B) in each channel k:\nPartial derivative of pixel color w.r.t. each Gaussian\u0026rsquo;s color: Œ±‚ãÖT\n$$\\rm \\frac{‚àÇC_{pixel}}{‚àÇC_{k}} = Œ±_{curr}‚ãÖ T_{curr}$$\ndL_dcolors is the sum of 3 channels of all Gaussians that contribute to this pixel.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Batchify Gaussians responsible for the pixel for (int i = 0; i \u0026lt; rounds; i++, toDo -= BLOCK_SIZE){ ... // Each round calculate a BLOCK_SIZE Gaussians for (int j = 0; !done \u0026amp;\u0026amp; j \u0026lt; min(BLOCK_SIZE, toDo); j++){ ... T = T / (1.f - alpha); const float dchannel_dcolor = alpha * T; for (int ch = 0; ch \u0026lt; C; ch++){ // C=3 channels ... const float dL_dchannel = dL_dpixel[ch]; // add to previous dL_dcolors atomicAdd(\u0026amp;(dL_dcolors[global_id * C + ch]), dchannel_dcolor * dL_dchannel); } } } Partial derivative of (pixel color) Loss w.r.t. each Gaussian\u0026rsquo;s alpha: dL_dalpha\nl a s t - 1 ‚úö f l o a r s w t a r ‚úö d c c o u m r p r o s ‚úö i t ‚ãØ i n g = p i x $$ \\begin{aligned} \\rm C_{pix} \u0026amp;= \\rm C_{last-1} Œ±_{last-1} T_{curr}(1-Œ±_{curr})(1-Œ±_{last}) + C_{last}Œ±_{last}T_{curr}(1-Œ±_{curr}) + C_{curr} Œ±_{curr} T_{curr} + ‚ãØ \\\\ \\rm \\frac{‚àÇC_{pix}}{‚àÇŒ±_{curr}} \u0026amp;= \\rm - C_{last-1} Œ±_{last-1} T_{curr}(1-Œ±_{last}) - C_{last}Œ±_{last}T_{curr} + C_{curr} T_{curr} \\\\ \u0026amp;= \\rm (C_{curr} - \\underbrace{C_{last-1} Œ±_{last-1}}_{\\text{accum rec}} (1-Œ±_{last}) - C_{last}Œ±_{last}) ‚ãÖ T_{curr} \\\\ \\rm \\frac{‚àÇL}{‚àÇŒ±_{curr}} \u0026amp;= \\rm \\frac{‚àÇL}{‚àÇC_{pix}}‚ãÖ \\frac{‚àÇC_{pix}}{‚àÇŒ±_{curr}} \\end{aligned} $$\n1 2 3 4 5 6 7 8 9 10 11 for (int ch = 0; ch \u0026lt; C; ch++) // C=3 chnls { const float c = collected_colors[ch * BLOCK_SIZE + j]; accum_rec[ch] = last_alpha * last_color[ch] + (1.f - last_alpha) * accum_rec[ch]; last_color[ch] = c; const float dL_dchannel = dL_dpixel[ch]; dL_dalpha += (c - accum_rec[ch]) * dL_dchannel; ... } dL_dalpha *= T; Old notes $$ \\rm \\frac{‚àÇC_{pixel}}{‚àÇ Œ±_{curr}} = C_{k} T_{curr} - \\frac{‚àë_{m\u0026gt;curr}c‚Çò‚ãÖŒ±‚Çò‚ãÖ T‚Çò}{1-Œ±_{curr}} $$\nA summation of subsequent Gaussians which are based on the current Gaussian.\nAs the solving process starts from the rare Gaussian that depends on all previous Gaussians to, when calculating the current Gaussian, the summation of Gaussians affected by it is ready to use.\nConsider the contribution of background color to the pixel color finally: Code\n$$ \\begin{aligned} \\rm C_{pix} \u0026amp;= \\rm C_{gs} + T_{final}C_{bg} \\\\ \u0026amp;= \\rm Œ±_{gs} c_{gs} + (1-Œ±_{gs}) T_{final-1} C_{bg} \\\\ \\\\ \\rm \\frac{‚àÇ L}{‚àÇ Œ±_{gs}} \u0026amp;= \\rm \\frac{‚àÇ L}{‚àÇ C_{pix}} ‚ãÖ ( \\frac{‚àÇC_{pix}}{‚àÇŒ±_{gs}} + \\frac{‚àÇC_{pix}}{‚àÇT_{final}} ‚ãÖ \\frac{‚àÇT_{final}}{‚àÇŒ±_{gs}} ) \\\\ \u0026amp;= \\rm \\frac{‚àÇL}{‚àÇŒ±_{gs}} + \\frac{‚àÇ L}{‚àÇ C_{pix}} ‚ãÖ C_{bg} ‚ãÖ (-\\frac{T_{final}}{(1-Œ±_{gs})}) \\end{aligned} $$\n1 2 3 4 float bg_dot_dpixel = 0; for (int i = 0; i \u0026lt; C; i++) bg_dot_dpixel += bg_color[i] * dL_dpixel[i]; dL_dalpha += (-T_final / (1.f - alpha)) * bg_dot_dpixel; Partial derivative of Loss w.r.t. 2D means:\n$$ \\begin{aligned} G \u0026amp;= e^{-¬Ω (\\bm Œº-ùê±)·µÄ \\bm Œ£‚Åª¬π (\\bm Œº-ùê±)} \\\\ \u0026amp;= exp(-¬Ω ‚ãÖ [Œº‚Çì-x \\ Œº_y-y] ‚ãÖ [^{a \\ b}_{d \\ c}] ‚ãÖ [^{Œº‚Çì-x} _{Œº_y-y}])\\\\ \u0026amp;= exp(-¬Ω‚ãÖ (a‚ãÖŒî‚Çì¬≤ + (b+d)‚ãÖŒî‚ÇìŒî_y + c‚ãÖŒî_y¬≤)) \\\\ Œ± \u0026amp;= o ‚ãÖ G \\\\ \\frac{‚àÇŒ±}{‚àÇŒî‚Çì} \u0026amp;= o‚ãÖG‚ãÖ (-aŒî‚Çì - bŒî_y) \\\\ \\frac{‚àÇL}{‚àÇŒº_{x(flim)}} \u0026amp;= \\frac{‚àÇL}{‚àÇŒ±}‚ãÖ \\frac{‚àÇŒ±}{‚àÇŒî‚Çì}‚ãÖ \\frac{‚àÇŒî‚Çì}{‚àÇŒº‚Çì} ‚ãÖ \\frac{‚àÇŒº‚Çì}{‚àÇŒº_{x (film)}} \\\\ \u0026amp;= \\frac{‚àÇL}{‚àÇŒ±} ‚ãÖo‚ãÖG‚ãÖ (-aŒî‚Çì - bŒî_y) ‚ãÖ 1 ‚ãÖ \\frac{W}{2} \\end{aligned} $$\nAlthough b=d, to be consistent with the derivative ‚àÇL/‚àÇb in the Jacobian matrix, the b and d keep separated instead of using b+d = 2b. 1 2 3 4 5 6 7 8 9 10 11 const float ddelx_dx = 0.5 * W; // viewport transform const float ddely_dy = 0.5 * H; const float dL_dG = con_o.w * dL_dalpha; const float gdx = G * d.x; const float gdy = G * d.y; const float dG_ddelx = -gdx * con_o.x - gdy * con_o.y; const float dG_ddely = -gdy * con_o.z - gdx * con_o.y; // Update gradients w.r.t. 2D mean position of the Gaussian atomicAdd(\u0026amp;dL_dmean2D[global_id].x, dL_dG * dG_ddelx * ddelx_dx); atomicAdd(\u0026amp;dL_dmean2D[global_id].y, dL_dG * dG_ddely * ddely_dy); Partial derivative of Loss w.r.t. 3 elements a,b,c in the inverse of 2D covariance matrix, based on $Œ± = o‚ãÖG = o‚ãÖexp(-\\frac{1}{2}Œî‚Çì·µÄ[^{a\\ b}_{d\\ c}]Œî‚Çì)$:\nNote: Here $[^{a\\ b}_{d\\ c}]$ represents the inverse cov ùö∫‚Åª¬π, not the original cov2D ùö∫. $$ \\frac{‚àÇL}{‚àÇa} = \\frac{‚àÇL}{‚àÇŒ±}‚ãÖ \\frac{‚àÇŒ±}{‚àÇa} = \\frac{‚àÇL}{‚àÇŒ±}‚ãÖ o ‚ãÖ G ‚ãÖ (-¬ΩŒî‚Çì¬≤) $$\n1 2 3 atomicAdd(\u0026amp;dL_dconic2D[global_id].x, -0.5f * gdx * d.x * dL_dG); atomicAdd(\u0026amp;dL_dconic2D[global_id].y, -0.5f * gdx * d.y * dL_dG); atomicAdd(\u0026amp;dL_dconic2D[global_id].w, -0.5f * gdy * d.y * dL_dG); Partial derivative of Loss w.r.t. opacity based on $Œ±=oG$\n$$ \\frac{‚àÇL}{‚àÇo} = \\frac{‚àÇL}{‚àÇŒ±} ‚ãÖ \\frac{‚àÇŒ±}{‚àÇo} = \\frac{‚àÇL}{‚àÇŒ±}‚ãÖ G $$\nbw render pixel depth (2024-05-04)\nZiyi Yang has a PR only for depth forward and backward. 3DGS-PR#5 (2024-04-29)\nThe forward pass of ashawkey/diff-gaussian-rasterization also rendered alpha in addition to the depth map:\nm p e r a e n p 3 r D o c p e r s o s j d c m o e o e p p v a a t 2 n c h D 2 i D t y e w r ^ e e ( i n p g d o h e w t r e r ) o G a c l o p l h o a r ‚àë ‚àë ‚àë d Œ± c Œ± T Œ± T T p d p a p c i e i l i o x p x p x l e t e h e o l h l a l r d a e L l l R L p o p o G o t s h s B s h s a s s The weight of each Gaussian\u0026rsquo;s opacity is the exponential function, where the power depends on the distance from pixel (processed by the thread) to a Gaussian\u0026rsquo;s mean.\n$$ \\begin{aligned} Œ± \u0026amp;= \\text{opacity} ‚ãÖ e^{-¬Ω\\bm Œî·µÄ \\bm Œ£_{(2D)}^{-1} \\bm Œî} \\\\ \u0026amp;= o ‚ãÖ exp(-¬Ω \\begin{bmatrix} Œî_x \u0026amp; Œî_y \\end{bmatrix} \\begin{bmatrix} a \u0026amp; b \\\\ d \u0026amp; c \\end{bmatrix} \\begin{bmatrix} Œî_x \\\\ Œî_y \\end{bmatrix} ) \\\\ \u0026amp;= o ‚ãÖ exp(-0.5 ‚ãÖ(a ‚ãÖ Œî_x^2 + c ‚ãÖ Œî_y^2) - b ‚ãÖ Œî_x Œî_y) \\end{aligned} $$\nDepth loss:\n$$ \\begin{aligned} L_d \u0026amp;= ¬Ω(d_{render} - d_{target})¬≤ \\\\ \u0026amp;= ¬Ω(‚àë_{n}d‚Çô Œ±‚Çô T‚Çô - d_{target})¬≤ \\end{aligned} $$\nwhere n is the index of a Gaussian. The partial derivative of pixel depth w.r.t. a Gaussian\u0026rsquo;s depth: Code\n$$ \\frac{‚àÇd_{render}}{‚àÇd‚Çô} = Œ±‚Çô T‚Çô $$\n1 const float dpixel_depth_ddepth = alpha * T; The partial derivative of Loss of depth w.r.t. a Gaussian\u0026rsquo;s alpha:\nSimilar to rendering the color for a pixel:\n‚ñ° N + ‚ãØ ‚ãØ l a ‚ñ° s t - 1 ‚úö l ‚ñ° a s t ‚úö c ‚ñ° u r r + S ‚ãØ u m ‚ãØ u ‚ñ° p = ‚ñ° d e p t h $$ \\begin{aligned} L_d \u0026amp;= ¬Ω(‚àë_{n}d‚Çô Œ±‚Çô T‚Çô - d_{target})¬≤ \\\\ \u0026amp;= ¬Ω(d_{curr} Œ±_{curr} T_{curr} + d_{last} ‚ãÖŒ±_{last}‚ãÖ (1-Œ±_{curr}) T_{curr} \\\\ \u0026amp;\\quad + d_{last-1}‚ãÖ Œ±_{last-1}‚ãÖ (1-Œ±_{last})(1-Œ±_{curr}) T_{curr} \\\\ \u0026amp;\\quad + ‚ãØ - d_{target})¬≤ \\end{aligned} $$\nThe partial derivative of Loss of depth w.r.t. alpha of the current Gaussian:\n$$ \\begin{aligned} \\frac{‚àÇL_d}{‚àÇŒ±_{curr}} \u0026amp;= d_{curr}‚ãÖT_{curr} - d_{last} ‚ãÖŒ±_{last}‚ãÖT_{curr} \\\\ \u0026amp;\\quad - d_{last-1}‚ãÖ Œ±_{last-1}‚ãÖ (1-Œ±_{last})T_{curr} \\\\ \u0026amp;\\quad - d_{last-2}‚ãÖ Œ±_{last-2} ‚ãÖ (1-Œ±_{last}) (1-Œ±_{last-1})T_{curr} \\\\ \u0026amp;\\quad + \u0026hellip; \\\\ \u0026amp;= d_{curr}‚ãÖT_{curr} - d_{last} ‚ãÖŒ±_{last}‚ãÖ\\frac{T_{last}}{1-Œ±_{curr}} \\\\ \u0026amp;\\quad - d_{last-1}‚ãÖ Œ±_{last-1}‚ãÖ\\frac{T_{last-1}}{1-Œ±_{curr}} \\\\ \u0026amp;\\quad - d_{last-2}‚ãÖ Œ±_{last-2} ‚ãÖ\\frac{T_{last-2}}{1-Œ±_{curr}} \\\\ \u0026amp;\\quad + \u0026hellip; \\\\ \\end{aligned} $$\nTherefore, the general term for each Gaussian is:\n$$ \\frac{‚àÇL_d}{‚àÇŒ±_{curr}} = d_{curr}‚ãÖT_{curr} - ‚àë_{m\u0026lt;curr} \\frac{d_m Œ±_m T_m}{1-Œ±_{curr}} $$\nwhere the computation of the cumulative sum can be simplified using a recurrence relation (refer to will\u0026rsquo;s derivation):\n$$ \\begin{aligned} \u0026amp; ‚àë_{m\u0026lt;curr} \\frac{d_m Œ±_m T_m}{1-Œ±_{curr}} \\\\ \u0026amp;= ‚àë_{m\u0026lt;curr} \\frac{d_m Œ±_m T_m}{(1-Œ±_{curr})T_{curr}}T_{curr} \\\\ \u0026amp;= ‚àë_{m\u0026lt;curr} \\frac{d_m Œ±_m T_m}{T_{last}}T_{curr} \\\\ \u0026amp;= \\frac{d_{last} Œ±_{last} \\cancel{T_{last} }}{ \\cancel{ T_{last} } }T_{curr} + ‚àë_{m\u0026lt;last} \\frac{d_m Œ±_m T_m}{T_{last}}T_{curr} \\\\ \u0026amp;= d_{last} Œ±_{last}T_{curr} + (1-Œ±_{last}) ‚àë_{m\u0026lt;last} \\frac{d_m Œ±_m T_m}{T_{last-1}}T_{curr} \\\\ \\end{aligned} $$\nTherefore, the summation $‚àë_{m\u0026lt;curr}$ got split into 2 parts: the summation of terms before the previous 2 Gaussians (last-1), plus the previous one (last):\nd f e i r r i s v t a ‚Üì ‚¨Ø t s i o v l ‚àë e v ‚Ä¶ ‚Çò e \u0026lt; d ‚Çó ‚Ä¶ ‚Çê ‚Çõ + ‚Çú l a s S t ‚¨Ø u - m 1 u + p l a ‚¨Ø s t + c u ‚¨Ø r r The graph illustrates that the preceding result can be reused to compute the derivatives of the current Gaussian.\nThe reuseable term, i.e., accum_depth_rec, is: $‚àë_{m\u0026lt;curr} \\frac{d_m Œ±_m T_m}{T_{last}}T_{curr}$\n1 accum_depth_rec = last_alpha * last_depth + (1.f - last_alpha) * accum_depth_rec; (2024-05-03)\nSince depth map (‚àëzŒ±T) and RGB image (‚àëcŒ±T) have similar generation process, just with different number of channels. the code of back-propagation for Gaussians\u0026rsquo; depths can mimic the code of Gaussians\u0026rsquo; colors. Code\nThe ashawkey\u0026rsquo;s code also renders each pixel\u0026rsquo;s alpha besides depth (‚àëŒ±T) during forward, and produces a loss for pixel alpha $L_{alpha}$.\nThus, the loss $L_{alpha}$ will affect the derivative of each pixel\u0026rsquo;s alpha dL_dalphas, and futhur each Gaussian\u0026rsquo;s alpha.\nTo compute the derivative of the pixel alpha w.r.t. a Gaussian\u0026rsquo;s alpha, only Gaussians behind the current Gaussian will be considered.\n$$ \\begin{aligned} Œ±_{pixel} \u0026amp;= \u0026hellip;+ Œ±_{curr}T_{curr} + Œ±_{last}(1-Œ±_{curr})T_{curr} + Œ±_{last-1}(1-Œ±_{last})(1-Œ±_{curr})T_{curr} + \u0026hellip; \\\\ \\frac{‚àÇŒ±_{pixel}}{‚àÇŒ±_{curr}} \u0026amp;= [1 - Œ±_{last} -Œ±_{last-1}(1-Œ±_{last}) -Œ±_{last-2}(1-Œ±_{last-1})(1-Œ±_{last})+\u0026hellip;] T_{curr}\\\\ \u0026amp;= [1 - Œ±_{last} - Œ±_{last-1} \\frac{T_{last-1}}{T_{last}} -Œ±_{last-2}\\frac{T_{last-2}}{T_{last}} + \u0026hellip; ] T_{curr} \\\\ \u0026amp;= \\cancel{[1 - Œ±_{last} - ‚àë_{m\u0026lt;last}\\frac{Œ±_mT_m}{T_{last}}] T_{curr}} \\quad \\text{Can\u0026rsquo;t be recursive or explain the final $(1-Œ±_{last})$}\\\\ \u0026amp;= [1 - ‚àë_{m\u0026lt;curr}\\frac{Œ±_mT_m}{T_{last}}] T_{curr} \\\\ \u0026amp;= [1 - Œ±_{last} - (1-Œ±_{last}) ‚àë_{m\u0026lt;last}\\frac{Œ±_mT_m}{T_{last-1}}] T_{curr} \\end{aligned} $$\nMultiplying by $(1-Œ±_{last})$ is to use the sum of previously computed Gaussian (2024-05-04)\nThe recursive term should be a single term, instead of two-term combination. As shown above, if the sum $‚àë_{m\u0026lt;curr}$ doesn\u0026rsquo;t include $Œ±_{last}$, the $‚àë_{m\u0026lt;last}$ indeed can be written as the previous result by multiplying with ($(1-Œ±_{last})$), however when reversing back, the $- Œ±_{last} - (1-Œ±_{last}) ‚àë_{m\u0026lt;last}\\frac{Œ±_mT_m}{T_{last-1}}$ is not the $- ‚àë_{m\u0026lt;last}\\frac{Œ±_mT_m}{T_{last}}$, whereas it should equal to the 2-term sum: $- Œ±_{last} - ‚àë_{m\u0026lt;last}\\frac{Œ±_mT_m}{T_{last}}$. 1 2 accum_alpha_rec = last_alpha + (1.f - last_alpha) * accum_alpha_rec; dL_dopa += (1 - accum_alpha_rec) * dL_dalpha; The partial derivative of depth loss w.r.t. each Gaussian\u0026rsquo;s opacity: Code\nBased on the relationship: $Œ±=oG$\n$$ \\frac{‚àÇŒ±}{‚àÇo} = G, \\quad \\frac{‚àÇL}{‚àÇo} = \\frac{‚àÇL}{‚àÇŒ±} G $$\n1 atomicAdd(\u0026amp;(dL_dopacity[global_id]), G * dL_dopa); I don\u0026rsquo;t know why he changed dL_dalpha to dL_dopa? Subsequently, the partial derivative of alpha w.r.t. mean2D (or $\\bm Œî$) and cov2D (a,b,c): Code\nBased on the formula: $Œ± = o ‚ãÖ exp(-0.5 ‚ãÖ(a ‚ãÖ Œî_x^2 + c ‚ãÖ Œî_y^2) - b ‚ãÖ Œî_x Œî_y)$\n$$ \\begin{aligned} \\frac{‚àÇŒ±}{‚àÇŒî_x} \u0026amp;= o ‚ãÖ e^{-¬Ω\\bm Œî·µÄ \\bm Œ£_{(2D)}^{-1} \\bm Œî} ‚ãÖ (-aŒî_x - bŒî_y) \\\\ \\frac{‚àÇŒ±}{‚àÇŒî_y} \u0026amp;= o ‚ãÖ e^{-¬Ω\\bm Œî·µÄ \\bm Œ£_{(2D)}^{-1} \\bm Œî} ‚ãÖ (-cŒî_y - bŒî_x) \\\\ \\frac{‚àÇŒ±}{‚àÇa} \u0026amp;= o ‚ãÖ e^{-¬Ω\\bm Œî·µÄ \\bm Œ£_{(2D)}^{-1} \\bm Œî}‚ãÖ(-0.5Œî_x^2) \\\\ \\frac{‚àÇŒ±}{‚àÇb} \u0026amp;= o ‚ãÖ e^{-¬Ω\\bm Œî·µÄ \\bm Œ£_{(2D)}^{-1} \\bm Œî}‚ãÖ(-Œî_x Œî_y) \\\\ \\frac{‚àÇŒ±}{‚àÇc} \u0026amp;= o ‚ãÖ e^{-¬Ω\\bm Œî·µÄ \\bm Œ£_{(2D)}^{-1} \\bm Œî}‚ãÖ(-0.5Œî_y^2) \\\\ \\end{aligned} $$\nsnippet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 const float ddelx_dx = 0.5 * W; const float ddely_dy = 0.5 * H; const float dL_dG = con_o.w * dL_dopa; //I think it\u0026#39;s dL_dalpha const float gdx = G * d.x; const float gdy = G * d.y; const float dG_ddelx = -gdx * con_o.x - gdy * con_o.y; const float dG_ddely = -gdy * con_o.z - gdx * con_o.y; // Update gradients w.r.t. 2D mean position of the Gaussian atomicAdd(\u0026amp;dL_dmean2D[global_id].x, dL_dG * dG_ddelx * ddelx_dx); atomicAdd(\u0026amp;dL_dmean2D[global_id].y, dL_dG * dG_ddely * ddely_dy); // Update gradients w.r.t. 2D covariance (2x2 matrix, symmetric) atomicAdd(\u0026amp;dL_dconic2D[global_id].x, -0.5f * gdx * d.x * dL_dG); atomicAdd(\u0026amp;dL_dconic2D[global_id].y, -0.5f * gdx * d.y * dL_dG); atomicAdd(\u0026amp;dL_dconic2D[global_id].w, -0.5f * gdy * d.y * dL_dG); The partial derivative of mean2D to mean3D: Code\n(2024-05-04)\nGiven transformation $mean3D_{cam} = W2C ‚ãÖ mean3D_{world}$, only consider the z and the homogeneous coordinates.\n$$ \\begin{bmatrix} \\\\ \\\\ mul3 \\\\ homo \\end{bmatrix} = \\begin{bmatrix} \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \u0026amp; \\\\ 2 \u0026amp; 6 \u0026amp; 10 \u0026amp; 14 \\\\ 3 \u0026amp; 7 \u0026amp; 11 \u0026amp; 15 \\end{bmatrix} \\begin{bmatrix} m.x \\\\ m.y \\\\ m.z \\\\ 1 \\end{bmatrix} $$\n$$ \\begin{aligned} depth_{cam} \u0026amp;= \\frac{mul3}{homo} \\\\ \\frac{‚àÇdepth_{cam}}{‚àÇm.x} \u0026amp;= \\frac{w2c[2] ‚ãÖhomo - mul3‚ãÖ w2c[3]}{homo^2} \\end{aligned} $$\nI don\u0026rsquo;t know why his code omitted the homo:\n1 2 float mul3 = view[2] * m.x + view[6] * m.y + view[10] * m.z + view[14]; dL_dmean2.x = (view[2] - view[3] * mul3) * dL_ddepth[idx]; bw cov2D The cov2D $\\bm Œ£\u0026rsquo;_{ray}$ is originated from the 3D covariance matrix ùö∫ and the mean vector ùê± (ie, ùõç) in world space. So the $\\bm Œ£\u0026rsquo;_{ray}$ is the starting point of the derivative of Loss w.r.t. the ùö∫ and ùê± of each Gaussian.\nTo align notations in previous posts, the covariance matrix in ray space is dentoed as ùö∫\u0026rsquo;, and the cov matrix in world space is ùö∫.\nw o ùê± r l d ùêë c ùê≠ a m ùêâ ùêâ ùêë c w o o ùö∫ v r ùêì 3 l D d ùêì ùö∫ ùêì ·µÄ r a ùö∫ 2 y ' D s p c c ùö∫ o ' v ‚Åª 2 ¬π D Note:\nThe clip coordinates aren\u0026rsquo;t used in the conversion for the 3D covariance matrix from world space to ray space. This is aligned with the EWA splatting. As the Gaussian center\u0026rsquo;s coordiantes are used in the Jacobian ùêâ, which is an approximation of perspective division, where the camera-space coordinates are supposed to be used, the ùê≠ in ùêâ is coordinates of Gaussian center in camera space. So, only the viewmatrix \u0026ldquo;w2c\u0026rdquo; ùêë is applied to the world coordinates ùê±.\nHowever, the 2D Gaussian center on the screen indeed comes from clip coordinates, because points outside the frustrum don\u0026rsquo;t need to be rendered. Therefore, the projection matrix projmatrix is applied to the world coordinates ùê±.\nThe above two branches both are derived from the world coordinates ùê±, so the derivative of Loss w.r.t. ùê± ($\\frac{‚àÇL}{‚àÇùê±}$) comprises of two portions.\ndŒ£\u0026rsquo;‚Åª¬π/dŒ£' (2024-02-11)\nSo far, the derivative of loss w.r.t. the inverse (conics, $(\\bm Œ£\u0026rsquo;)‚Åª¬π$) of a 2D-plane covariance matrix $\\bm Œ£\u0026rsquo;$ (i.e., the 3D cov in ray space with 3rd row and column omitted) has been obtained. So, the derivative of loss w.r.t. the original 2D cov matrix ùö∫\u0026rsquo; is $\\frac{‚àÇL}{‚àÇ\\bm Œ£\u0026rsquo;‚Åª¬π}$ appended with the derivative of ùö∫\u0026rsquo;‚Åª¬π w.r.t. ùö∫'.\nConsider 4 variables a, b, c, d form the matrix $\\bm Œ£\u0026rsquo; = \\begin{bmatrix}a \u0026amp; b \\\\ d \u0026amp; c \\end{bmatrix}$, its inverse matrix is: $\\bm Œ£\u0026rsquo;‚Åª¬π = \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix}$, where b=d.\nThe partial derivatives of the inverse matrix w.r.t. each variable: a,b,c,d are:\n$$ \\begin{aligned} \\frac{‚àÇ \\bm Œ£\u0026rsquo;‚Åª¬π}{‚àÇa} \u0026amp;= \\frac{-c¬≤ + bc + dc - bd}{(ac-bd)¬≤}, \u0026amp; \\frac{‚àÇ \\bm Œ£\u0026rsquo;‚Åª¬π}{‚àÇb} \u0026amp;= \\frac{cd-ac-d¬≤+ad}{(ac-bd)¬≤} \\\\ \\frac{‚àÇ \\bm Œ£\u0026rsquo;‚Åª¬π}{‚àÇc} \u0026amp;= \\frac{-bd+ba+da-a¬≤}{(ac-bd)¬≤}, \u0026amp; \\frac{‚àÇ \\bm Œ£\u0026rsquo;‚Åª¬π}{‚àÇd} \u0026amp;= \\frac{cb-b¬≤-ac+ab}{(ac-bd)¬≤} \\\\ \\end{aligned} $$\n1 2 3 4 5 6 7 float denom = a * c - b * b; // detminant float dL_da = 0, dL_db = 0, dL_dc = 0; float denom2inv = 1.0f / ((denom * denom) + 0.0000001f); ... dL_da = denom2inv * (-c * c * dL_dconic.x + 2 * b * c * dL_dconic.y + (denom - a * c) * dL_dconic.z); dL_dc = denom2inv * (-a * a * dL_dconic.z + 2 * a * b * dL_dconic.y + (denom - a * c) * dL_dconic.x); dL_db = denom2inv * 2 * (b * c * dL_dconic.x - (denom + 2 * b * b) * dL_dconic.y + a * b * dL_dconic.z); dŒ£\u0026rsquo;/dŒ£ (2024-02-12)\nDerivative of 3D covariance matrix from ray space to camera space:\nAs the above 2D covariance matrix on plane is the 3D cov matrix ùö∫\u0026rsquo; in the ray space with the 3rd row and column omitted. Thus, the next step to be calculated is the conversion from ùö∫\u0026rsquo; in the 3D ray space to ùö∫ in the world space.\nThe 3D covariance matrix ùö∫\u0026rsquo; in the ray space is obtained after performing viewing transform and projective transform:\n$$ \\begin{aligned} \\bm Œ£\u0026rsquo;_{ray} \u0026amp;= ùêâ‚ãÖ ùêë_{w2c} ‚ãÖ\\bm Œ£_{world}‚ãÖ ùêë·µÄ_{w2c} ‚ãÖùêâ·µÄ \\\\ \\begin{bmatrix} a \u0026amp; b \u0026amp; ‚àò \\\\ b \u0026amp; c \u0026amp; ‚àò \\\\ ‚àò \u0026amp; ‚àò \u0026amp; ‚àò \\end{bmatrix} \u0026amp;= \\underbrace{ \\begin{bmatrix} \\frac{f_x}{t_z} \u0026amp; 0 \u0026amp; -\\frac{f_x t_x}{t_z^2} \\\\ 0 \u0026amp; \\frac{f_y}{t_z} \u0026amp; -\\frac{f_y t_y}{t_z^2} \\\\ \\frac{t_x}{‚Äñùê≠‚Äñ} \u0026amp; \\frac{t_y}{‚Äñùê≠‚Äñ} \u0026amp; \\frac{t_z}{‚Äñùê≠‚Äñ} \\end{bmatrix} \\begin{bmatrix} R_{11} \u0026amp; R_{12} \u0026amp; R_{13} \\\\ R_{21} \u0026amp; R_{22} \u0026amp; R_{23} \\\\ R_{31} \u0026amp; R_{32} \u0026amp; R_{33} \\end{bmatrix} }_{T} \\begin{bmatrix} v_0 \u0026amp; v_1 \u0026amp; v_2 \\\\ v_1 \u0026amp; v_3 \u0026amp; v_4 \\\\ v_2 \u0026amp; v_4 \u0026amp; v_5 \\\\ \\end{bmatrix} \\underbrace{ \\begin{bmatrix} R_{11} \u0026amp; R_{21} \u0026amp; R_{31} \\\\ R_{12} \u0026amp; R_{22} \u0026amp; R_{32} \\\\ R_{13} \u0026amp; R_{23} \u0026amp; R_{33} \\end{bmatrix} \\begin{bmatrix} \\frac{f_x}{t_z} \u0026amp; 0 \u0026amp; \\frac{t_x}{‚Äñùê≠‚Äñ} \\\\ 0 \u0026amp; \\frac{f_y}{t_z} \u0026amp; \\frac{t_y}{‚Äñùê≠‚Äñ} \\\\ -\\frac{f_x t_x}{t_z^2} \u0026amp; -\\frac{f_y t_y}{t_z^2} \u0026amp; \\frac{t_z}{‚Äñùê≠‚Äñ} \\end{bmatrix} }_{T·µÄ} \\\\ \u0026amp;= \\begin{bmatrix} T_{00} \u0026amp; T_{01} \u0026amp; T_{02} \\\\ T_{10} \u0026amp; T_{11} \u0026amp; T_{12} \\\\ T_{20} \u0026amp; T_{21} \u0026amp; T_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} v_0 \u0026amp; v_1 \u0026amp; v_2 \\\\ v_1 \u0026amp; v_3 \u0026amp; v_4 \\\\ v_2 \u0026amp; v_4 \u0026amp; v_5 \\\\ \\end{bmatrix} \\begin{bmatrix} T_{00} \u0026amp; T_{10} \u0026amp; T_{20} \\\\ T_{01} \u0026amp; T_{11} \u0026amp; T_{21} \\\\ T_{02} \u0026amp; T_{12} \u0026amp; T_{22} \\\\ \\end{bmatrix} \\end{aligned} $$\nThe derivatives of ùö∫\u0026rsquo; w.r.t. each element in the cov matrix $\\bm Œ£_{world}$ in world are:\n$$ \\begin{aligned} \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇv_0} = \\begin{bmatrix} T_{00}T_{00} \u0026amp; T_{00}T_{10} \\\\ T_{10}T_{00} \u0026amp; T_{10}T_{10} \\end{bmatrix} \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇv_1} = \\begin{bmatrix} T_{00}T_{01} \u0026amp; T_{00}T_{11} \\\\ T_{10}T_{01} \u0026amp; T_{10}T_{11} \\end{bmatrix} \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇv_2} = \\begin{bmatrix} T_{00}T_{02} \u0026amp; T_{00}T_{12} \\\\ T_{10}T_{02} \u0026amp; T_{10}T_{12} \\end{bmatrix} \\\\ \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇv_3} = \\begin{bmatrix} T_{01}T_{01} \u0026amp; T_{01}T_{11} \\\\ T_{11}T_{01} \u0026amp; T_{11}T_{11} \\end{bmatrix} \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇv_4} = \\begin{bmatrix} T_{01}T_{02} \u0026amp; T_{01}T_{12} \\\\ T_{11}T_{02} \u0026amp; T_{11}T_{12} \\end{bmatrix} \\\\ \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇv_5} = \\begin{bmatrix} T_{02}T_{02} \u0026amp; T_{02}T_{12} \\\\ T_{12}T_{02} \u0026amp; T_{12}T_{12} \\end{bmatrix} \\end{aligned} $$\nTrick: Just find out the terms involving $v_0, v_1, v_2, v_3, v_4, v_5$\nFor example, in the first matmul, the coefficients applied on v‚ÇÄ are $T_{00},\\ T_{10},\\ T_{20}$ (a row). Then, in the outer matmul, the coefficients that multiplis with $v_0$ are $T_{00},\\ T_{10},\\ T_{20}$ (a column).\nSo, the row and the column form a matrix:\n$$ \\begin{bmatrix} T_{00} \\\\ T_{10} \\\\ T_{20} \\end{bmatrix} \\begin{bmatrix} T_{00} \u0026amp; T_{10} \u0026amp; T_{20} \\end{bmatrix} ‚Üí \\begin{bmatrix} T_{00}T_{00} \u0026amp; T_{00}T_{10} \u0026amp; T_{00}T_{20} \\\\ T_{10}T_{00} \u0026amp; T_{10}T_{10} \u0026amp; T_{10}T_{20} \\\\ T_{20}T_{00} \u0026amp; T_{20}T_{10} \u0026amp; T_{20}T_{20} \\end{bmatrix} $$\nThe 3-rd row and column are omitted to yield the 2D covariance matrix on plane.\nSince the upstream derivative of loss w.r.t. $\\bm Œ£\u0026rsquo;_{ray}$ is a matrix as well, they need to do element-wise multiplication:\n$$ \\begin{aligned} \\frac{‚àÇL}{‚àÇv_0} \u0026amp;= \\begin{bmatrix} \\frac{‚àÇL}{‚àÇa} \u0026amp; \\frac{‚àÇL}{‚àÇb} \\\\ \\frac{‚àÇL}{‚àÇb} \u0026amp; \\frac{‚àÇL}{‚àÇc} \\end{bmatrix} ‚äô \\begin{bmatrix} T_{00}T_{00} \u0026amp; T_{00}T_{10} \\\\ T_{10}T_{00} \u0026amp; T_{10}T_{10} \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{‚àÇL}{‚àÇa} T_{00}T_{00} \u0026amp; \\frac{‚àÇL}{‚àÇb} T_{00}T_{10} \\\\ \\frac{‚àÇL}{‚àÇb} T_{10}T_{00} \u0026amp; \\frac{‚àÇL}{‚àÇc} T_{10}T_{10} \\end{bmatrix} \\end{aligned} $$\nAs loss is a scalar, the derivative of the loss w.r.t. $v_0$ should be the summation of all elements in the matrix (Need reference‚ùó):\n$$ \\frac{‚àÇL}{‚àÇv_0} = \\frac{‚àÇL}{‚àÇa} T_{00}T_{00} + 2√ó \\frac{‚àÇL}{‚àÇb}T_{00}T_{10} + \\frac{‚àÇL}{‚àÇc} T_{10}T_{10} $$\nDetails for v‚ÇÑ Similarly, the coefficients related to $v_4$ are the 3rd row of the right matrix ùêì·µÄ, and the 2nd column of the left matrix ùêì.\n$$ \\begin{bmatrix} T_{01} \\\\ T_{11} \\\\ T_{21} \\end{bmatrix} \\begin{bmatrix} T_{02} \u0026amp; T_{12} \u0026amp; T_{22} \\end{bmatrix} ‚Üí \\begin{bmatrix} T_{01}T_{02} \u0026amp; T_{01}T_{12} \u0026amp; T_{01}T_{22} \\\\ T_{11}T_{02} \u0026amp; T_{11}T_{12} \u0026amp; T_{11}T_{22} \\\\ T_{21}T_{02} \u0026amp; T_{21}T_{12} \u0026amp; T_{21}T_{22} \\\\ \\end{bmatrix} $$\nThe derivative of loss w.r.t. $v_1$\n$$ \\begin{aligned} \\frac{‚àÇL}{‚àÇv_1} \u0026amp;= \\begin{bmatrix} \\frac{‚àÇL}{‚àÇa} \u0026amp; \\frac{‚àÇL}{‚àÇb} \\\\ \\frac{‚àÇL}{‚àÇb} \u0026amp; \\frac{‚àÇL}{‚àÇc} \\end{bmatrix} ‚äô \\begin{bmatrix} T_{01}T_{02} \u0026amp; T_{01}T_{12} \\\\ T_{11}T_{02} \u0026amp; T_{11}T_{12} \\end{bmatrix} \\\\ \u0026amp;= T_{01}T_{02} \\frac{‚àÇL}{‚àÇa} + (T_{01}T_{12} + T_{11}T_{02})\\frac{‚àÇL}{‚àÇb} + T_{11}T_{12}\\frac{‚àÇL}{‚àÇc} \\end{aligned} $$\nBecause the covariance matrix is symmatric (cov[1][2]=cov[2][1]), the effect attributed to the change of $v_1$ should double.\n$$ \\frac{‚àÇL}{‚àÇv_4} = 2√óT_{01}T_{02} \\frac{‚àÇL}{‚àÇa} + 2√ó(T_{01}T_{12} + T_{11}T_{02})\\frac{‚àÇL}{‚àÇb} + 2√óT_{11}T_{12}\\frac{‚àÇL}{‚àÇc} $$\nIn the 3DGS code, dL_db had doubled.\n1 2 3 4 5 6 7 8 glm::mat3 T = W * J; ... dL_dcov[6 * idx + 0] = (T[0][0] * T[0][0] * dL_da + T[0][0] * T[1][0] * dL_db + T[1][0] * T[1][0] * dL_dc); dL_dcov[6 * idx + 3] = (T[0][1] * T[0][1] * dL_da + T[0][1] * T[1][1] * dL_db + T[1][1] * T[1][1] * dL_dc); dL_dcov[6 * idx + 5] = (T[0][2] * T[0][2] * dL_da + T[0][2] * T[1][2] * dL_db + T[1][2] * T[1][2] * dL_dc); dL_dcov[6 * idx + 1] = 2 * T[0][0] * T[0][1] * dL_da + (T[0][0] * T[1][1] + T[0][1] * T[1][0]) * dL_db + 2 * T[1][0] * T[1][1] * dL_dc; dL_dcov[6 * idx + 2] = 2 * T[0][0] * T[0][2] * dL_da + (T[0][0] * T[1][2] + T[0][2] * T[1][0]) * dL_db + 2 * T[1][0] * T[1][2] * dL_dc; dL_dcov[6 * idx + 4] = 2 * T[0][2] * T[0][1] * dL_da + (T[0][1] * T[1][2] + T[0][2] * T[1][1]) * dL_db + 2 * T[1][1] * T[1][2] * dL_dc; dŒ£\u0026rsquo;/dT (2024-02-13)\nTo solve the derivative of Loss w.r.t. $ùê≠$ (in camera space), the chain is:\n$$\\frac{‚àÇL}{‚àÇt} = \\frac{‚àÇL}{\\bm Œ£\u0026rsquo;_{ray}} \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇT} \\frac{‚àÇT}{‚àÇJ} \\frac{‚àÇJ}{‚àÇt}$$\nThe derivative of Loss w.r.t. 2D $\\bm Œ£\u0026rsquo;_{ray}$ has been obtained earlier. The next step is to calculate $\\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇT}$, where all variables except for T are treated as constants.\nBased on the relationship:\n$$ \\bm Œ£\u0026rsquo;_{ray} = ùêì‚ãÖ\\bm Œ£_{world}‚ãÖùêì·µÄ \\\\ = \\begin{bmatrix} T_{00} \u0026amp; T_{01} \u0026amp; T_{02} \\\\ T_{10} \u0026amp; T_{11} \u0026amp; T_{12} \\\\ T_{20} \u0026amp; T_{21} \u0026amp; T_{22} \\end{bmatrix} \\begin{bmatrix} v_{00} \u0026amp; v_{01} \u0026amp; v_{02} \\\\ v_{10} \u0026amp; v_{11} \u0026amp; v_{12} \\\\ v_{20} \u0026amp; v_{21} \u0026amp; v_{22} \\end{bmatrix} \\begin{bmatrix} T_{00} \u0026amp; T_{10} \u0026amp; T_{20} \\\\ T_{01} \u0026amp; T_{11} \u0026amp; T_{21} \\\\ T_{02} \u0026amp; T_{12} \u0026amp; T_{22} \\end{bmatrix} $$\nRepresent the matrix $\\bm Œ£\u0026rsquo;_{ray}$ as a single generic term with varying indices: (inspired by David Levin surfaced by perplexity)\nan element in the result of $\\bm Œ£_{world}‚ãÖùêì·µÄ$ is $q$; an element in the $\\bm Œ£\u0026rsquo;_{ray}$ is $p$ $$ \\begin{aligned} q_{ij} \u0026amp;= ‚àë‚Çñ‚Çå‚ÇÄ¬≤ v_{ik} T_{jk} \\\\ p_{mn} \u0026amp;= ‚àë‚Çõ‚Çå‚ÇÄ¬≤ T_{ms} q_{sn} = ‚àë‚Çõ‚Çå‚ÇÄ¬≤ T_{ms} ‚àë‚Çñ‚Çå‚ÇÄ¬≤ v_{sk} T_{nk}\\\\ \\bm Œ£\u0026rsquo;_{ray} \u0026amp;= \\begin{bmatrix} p_{00} \u0026amp; p_{01} \u0026amp; p_{02} \\\\ p_{10} \u0026amp; p_{11} \u0026amp; p_{12} \\\\ p_{20} \u0026amp; p_{21} \u0026amp; p_{22} \\end{bmatrix} \\end{aligned} $$\nExpand each $p_{mn}$:\n$$ \\begin{array}{ccc} \\begin{aligned} p_{00} =\u0026amp; T_{00}(v_{00}T_{00} + v_{01}T_{01} + v_{02}T_{02}) \\\\ +\u0026amp; T_{01}(v_{10}T_{00} + v_{11}T_{01} + v_{12}T_{02}) \\\\ +\u0026amp; T_{02}(v_{20}T_{00} + v_{21}T_{01} + v_{22}T_{02}) \\\\ \\end{aligned} \u0026amp; \\begin{aligned} p_{01} =\u0026amp; T_{00}(v_{00}T_{10} + v_{01}T_{11} + v_{02}T_{12}) \\\\ +\u0026amp; T_{01}(v_{10}T_{10} + v_{11}T_{11} + v_{12}T_{12}) \\\\ +\u0026amp; T_{02}(v_{20}T_{10} + v_{21}T_{11} + v_{22}T_{12}) \\\\ \\end{aligned} \u0026amp; \\begin{aligned} p_{02} =\u0026amp; T_{00}(v_{00}T_{20} + v_{01}T_{21} + v_{02}T_{22}) \\\\ +\u0026amp; T_{01}(v_{10}T_{20} + v_{11}T_{21} + v_{12}T_{22}) \\\\ +\u0026amp; T_{02}(v_{20}T_{20} + v_{21}T_{21} + v_{22}T_{22}) \\\\ \\end{aligned} \\\\ \\begin{aligned} p_{10} =\u0026amp; T_{10}(v_{00}T_{00} + v_{01}T_{01} + v_{02}T_{02}) \\\\ +\u0026amp; T_{11}(v_{10}T_{00} + v_{11}T_{01} + v_{12}T_{02}) \\\\ +\u0026amp; T_{12}(v_{20}T_{00} + v_{21}T_{01} + v_{22}T_{02}) \\\\ \\end{aligned} \u0026amp; p_{11} \u0026amp; p_{12} \\\\ \\begin{aligned} p_{20} =\u0026amp; T_{20}(v_{00}T_{00} + v_{01}T_{01} + v_{02}T_{02}) \\\\ +\u0026amp; T_{21}(v_{10}T_{00} + v_{11}T_{01} + v_{12}T_{02}) \\\\ +\u0026amp; T_{22}(v_{20}T_{00} + v_{21}T_{01} + v_{22}T_{02}) \\\\ \\end{aligned} \u0026amp; p_{21} \u0026amp; p_{22} \\end{array} $$\nThe 3-rd row and column are omitted to become the 2x2 $\\bm Œ£\u0026rsquo;_{ray}$. So, only 4 elements affect the derivative of the $\\bm Œ£\u0026rsquo;_{ray}$. For example, the derivative of $\\bm Œ£\u0026rsquo;_{ray}$ w.r.t. $T‚ÇÄ‚ÇÄ$ only related to components containing $T_{00}$.\nCalculate the derivative of each element $p_{mn}$ in the $\\bm Œ£\u0026rsquo;_{ray}$ w.r.t. $T_{00}$. A matrix is assembled:\n$$ \\begin{bmatrix} \\frac{‚àÇp_{00}}{‚àÇT_{00}} \u0026amp; \\frac{‚àÇp_{01}}{‚àÇT_{00}} \\\\ \\frac{‚àÇp_{10}}{‚àÇT_{00}} \u0026amp; \\frac{‚àÇp_{11}}{‚àÇT_{00}} \\end{bmatrix} = \\begin{bmatrix} \\substack{2T_{00}v_{00} + v_{01}T_{01} + v_{02}T_{02} \\\\ + T_{01}v_{10} + T_{02}v_{20} } \u0026amp; v_{00}T_{10} + v_{01}T_{11} + v_{02}T_{12} \\\\ T_{10}v_{00} + T_{11}v_{10} + T_{12}v_{20} \u0026amp; 0 \\end{bmatrix} $$\nTherefore, the derivative of the scalar Loss w.r.t. T‚ÇÄ‚ÇÄ, i.e., $\\frac{‚àÇL}{‚àÇT‚ÇÄ‚ÇÄ}$, is also a matrix:\n$$ \\frac{‚àÇL}{‚àÇT_{00}} = \\frac{‚àÇL}{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}‚äô \\frac{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}{‚àÇT_{00}} = \\begin{bmatrix} \\frac{‚àÇL}{‚àÇa} (2(T_{00}v_{00}+T_{01}v_{01}+T_{02}v_{02})) \u0026amp; \\frac{‚àÇL}{‚àÇb} (T_{10}v_{00}+T_{11}v_{01}+T_{12}v_{02} ) \\\\ \\frac{‚àÇL}{‚àÇb} (T_{10}v_{00}+T_{11}v_{10}+T_{12}v_{20} ) \u0026amp; 0 \\end{bmatrix} $$\nAs the loss $L$ is a scalar, the derivative of $L$ w.r.t. a certain variable (e.g., T‚ÇÄ‚ÇÄ) is the total quantity of changes in all elements of a matrix caused by that variable\u0026rsquo;s unit perturbation. Thus, $\\frac{‚àÇL}{‚àÇT‚ÇÄ‚ÇÄ}$ is a sum of all elements in the final \u0026ldquo;derivative matrix\u0026rdquo; of Loss w.r.t. T‚ÇÄ‚ÇÄ:\n$$ \\frac{‚àÇL}{‚àÇT_{00}} = \\frac{‚àÇL}{‚àÇa}(2(T_{00}v_{00}+T_{01}v_{01}+T_{02}v_{02}) ) + \\frac{‚àÇL}{‚àÇb}(2(T_{10}v_{00}+T_{11}v_{01}+T_{12}v_{02} ) ) $$\nTricks for identifying necessary terms Only focus on the terms that includes $T_{00}$:\nFor the first matmul between $\\bm Œ£_{world}$ and ùêì·µÄ, the full 1st row will be kept as it will times a $T_{00}$ during the second matmul, and additional 2 terms containing $T_{00}$ are left:\n$$\\begin{matrix} v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02} \u0026amp; v‚ÇÄ‚ÇÄT_{10} + v_{01}T_{11} + v_{02}T_{12} \u0026amp; v‚ÇÄ‚ÇÄT_{20} + v_{01}T_{21} + v_{02}T_{22} \\\\ v_{10} T_{00} \\\\ v_{20} T_{00} \\end{matrix}$$\nI disregard unrelated terms, to which the derivatives with respect are 0. Then, applying the left ùêì\n$$ \\begin{aligned} \\bm Œ£\u0026rsquo;_{ray}\u0026amp;= \\begin{bmatrix} T_{00} \u0026amp; T_{01} \u0026amp; T_{02} \\\\ T_{10} \u0026amp; T_{11} \u0026amp; T_{12} \\\\ T_{20} \u0026amp; T_{21} \u0026amp; T_{22} \\end{bmatrix} \\begin{bmatrix} v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02} \u0026amp; v‚ÇÄ‚ÇÄT_{10} + v_{01}T_{11} + v_{02}T_{12} \u0026amp; v‚ÇÄ‚ÇÄT_{20} + v_{01}T_{21} + v_{02}T_{22} \\\\ v_{10} T_{00} \\\\ v_{20} T_{00} \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} T_{00} (v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{01} v_{10} T_{00} + T_{02} v_{20} T_{00} \u0026amp; T_{00} (v‚ÇÄ‚ÇÄT_{10} + v_{01}T_{11} + v_{02}T_{12}) \u0026amp; T_{00} (v‚ÇÄ‚ÇÄT_{20} + v_{01}T_{21} + v_{02}T_{22}) \\\\ T_{10} (v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{11} v_{10} T_{00} + T_{12} v_{20} T_{00} \\\\ T_{20} (v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{21} v_{10} T_{00} + T_{22} v_{20} T_{00} \\end{bmatrix} \\end{aligned} $$\nAs the 3rd row and column don\u0026rsquo;t appear in the 2D plane cov matrix $\\bm Œ£\u0026rsquo;_{ray}$, only 4 terms contribute to the derivative of L w.r.t. $T‚ÇÄ‚ÇÄ$:\n$$\\begin{bmatrix} T_{00} (v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{01} v_{10} T_{00} + T_{02} v_{20} T_{00} \u0026amp; T_{00} (v‚ÇÄ‚ÇÄT_{10} + v_{01}T_{11} + v_{02}T_{12}) \\\\ T_{10} (v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{11} v_{10} T_{00} + T_{12} v_{20} T_{00} \\end{bmatrix}$$\nCompute derivative of each element w.r.t. $T_{00}$:\n$$\\begin{bmatrix} 2(v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02}) \u0026amp; v‚ÇÄ‚ÇÄT_{10} + v_{01}T_{11} + v_{02}T_{12} \\\\ T_{10}v‚ÇÄ‚ÇÄ + T_{11} v_{10} + T_{12} v_{20} \\end{bmatrix}$$\nCombine with $\\frac{‚àÇL}{‚àÇ\\bm Œ£\u0026rsquo;_{ray}}$ by element-wise product to get the ultimate \u0026ldquo;derivative matrix\u0026rdquo; of Loss w.r.t T‚ÇÄ‚ÇÄ.\n$$\\frac{‚àÇL}{‚àÇT‚ÇÄ‚ÇÄ} = \\begin{bmatrix} \\frac{‚àÇL}{‚àÇa} ‚ãÖ 2(v‚ÇÄ‚ÇÄT_{00} + v_{01}T_{01} + v_{02}T_{02}) \u0026amp; \\frac{‚àÇL}{‚àÇb} (v‚ÇÄ‚ÇÄT_{10} + v_{01}T_{11} + v_{02}T_{12}) \\\\ \\frac{‚àÇL}{‚àÇb} (T_{10}v‚ÇÄ‚ÇÄ + T_{11} v_{10} + T_{12} v_{20}) \\end{bmatrix}$$\nFinally, as L is a scalar, the derivative of L w.r.t. T‚ÇÄ‚ÇÄ is the sum of all derivatives of each element in the \u0026ldquo;derivative matrix\u0026rdquo;.\nThe dL_db in 3DGS Code had doubled:\n1 2 float dL_dT00 = 2 * (T[0][0] * Vrk[0][0] + T[0][1] * Vrk[0][1] + T[0][2] * Vrk[0][2]) * dL_da + (T[1][0] * Vrk[0][0] + T[1][1] * Vrk[0][1] + T[1][2] * Vrk[0][2]) * dL_db; Similarly, the derivatives of Loss w.r.t. T‚ÇÄ‚ÇÅ, T‚ÇÄ‚ÇÇ, T‚ÇÅ‚ÇÄ, T‚ÇÅ‚ÇÅ, T‚ÇÅ‚ÇÇ can be obtained. As T‚ÇÇ‚ÇÄ, T‚ÇÇ‚ÇÅ, T‚ÇÇ‚ÇÇ are not involved in the 2D $\\bm Œ£\u0026rsquo;_{ray}$, they don\u0026rsquo;t contribute the derivative of $\\bm Œ£\u0026rsquo;_{ray}$.\n$$ \\frac{‚àÇL}{‚àÇùêì} = \\begin{bmatrix} \\frac{‚àÇL}{‚àÇT_{00}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{01}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{02}} \\\\ \\frac{‚àÇL}{‚àÇT_{10}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{11}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{12}} \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\ndT/dJ (2024-02-14)\nBased on the relationship:\n$$ ùêì = ùêâùêñ = \\begin{bmatrix} J‚ÇÄ‚ÇÄ \u0026amp; J‚ÇÄ‚ÇÅ \u0026amp; J‚ÇÄ‚ÇÇ \\\\ J‚ÇÅ‚ÇÄ \u0026amp; J‚ÇÅ‚ÇÅ \u0026amp; J‚ÇÅ‚ÇÇ \\\\ J‚ÇÇ‚ÇÄ \u0026amp; J‚ÇÇ‚ÇÅ \u0026amp; J‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} W‚ÇÄ‚ÇÄ \u0026amp; W‚ÇÄ‚ÇÅ \u0026amp; W‚ÇÄ‚ÇÇ \\\\ W‚ÇÅ‚ÇÄ \u0026amp; W‚ÇÅ‚ÇÅ \u0026amp; W‚ÇÅ‚ÇÇ \\\\ W‚ÇÇ‚ÇÄ \u0026amp; W‚ÇÇ‚ÇÅ \u0026amp; W‚ÇÇ‚ÇÇ \\end{bmatrix} $$\nThe derivative of ùêì w.r.t. $J‚ÇÄ‚ÇÄ$\n$$ \\frac{‚àÇT}{‚àÇJ_{00}} = \\begin{bmatrix} W‚ÇÄ‚ÇÄ \u0026amp; W‚ÇÄ‚ÇÅ \u0026amp; W‚ÇÄ‚ÇÇ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe \u0026ldquo;derivative matrix\u0026rdquo; of Loss w.r.t. $J‚ÇÄ‚ÇÄ$\n$$ \\frac{‚àÇL}{‚àÇT} ‚äô \\frac{‚àÇT}{‚àÇJ_{00}} = \\begin{bmatrix} \\frac{‚àÇL}{‚àÇT_{00}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{01}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{02}} \\\\ \\frac{‚àÇL}{‚àÇT_{10}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{11}} \u0026amp; \\frac{‚àÇL}{‚àÇT_{12}} \\end{bmatrix} ‚äô \\begin{bmatrix} W‚ÇÄ‚ÇÄ \u0026amp; W‚ÇÄ‚ÇÅ \u0026amp; W‚ÇÄ‚ÇÇ \\\\ 0 \u0026amp; 0\u0026amp;0 \\end{bmatrix} \\\\ = \\begin{bmatrix} \\frac{‚àÇL}{‚àÇT_{00}} W‚ÇÄ‚ÇÄ \u0026amp; \\frac{‚àÇL}{‚àÇT_{01}} W‚ÇÄ‚ÇÅ \u0026amp; \\frac{‚àÇL}{‚àÇT_{02}} W‚ÇÄ‚ÇÇ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe derivative of Loss w.r.t. J‚ÇÄ‚ÇÄ is the sum of all elements in the \u0026ldquo;derivative matrix\u0026rdquo;:\n$$ \\frac{‚àÇL}{‚àÇJ‚ÇÄ‚ÇÄ} = \\frac{‚àÇL}{‚àÇT_{00}} W‚ÇÄ‚ÇÄ + \\frac{‚àÇL}{‚àÇT_{01}} W‚ÇÄ‚ÇÅ + \\frac{‚àÇL}{‚àÇT_{02}} W‚ÇÄ‚ÇÇ $$\nIn 3DGS code, the 3-rd row of the Jacobian matrix is set to 0 (as opacity isn\u0026rsquo;t obtained by integration along z-axis of ray space, but learned), thus no contribution to the derivative of Loss. Only compute the derivative of Loss w.r.t. J‚ÇÄ‚ÇÄ, J‚ÇÄ‚ÇÇ, J‚ÇÅ‚ÇÅ, J‚ÇÅ‚ÇÇ\n$$ \\frac{‚àÇL}{‚àÇùêâ} = \\begin{bmatrix} \\frac{‚àÇL}{‚àÇJ‚ÇÄ‚ÇÄ} \u0026amp; 0 \u0026amp; \\frac{‚àÇL}{‚àÇJ‚ÇÄ‚ÇÇ} \\\\ 0 \u0026amp; \\frac{‚àÇL}{J‚ÇÅ‚ÇÅ} \u0026amp; \\frac{‚àÇL}{‚àÇ‚ÇÅ‚ÇÇ} \\end{bmatrix} $$\n1 float dL_dJ00 = W[0][0] * dL_dT00 + W[0][1] * dL_dT01 + W[0][2] * dL_dT02; dJ/dt The 3D mean ùê≠ in ùêâ is the coordinates of a Gaussian center in camera space, Not the clip space, whereas the 2D mean ùõç indeed originates from the clip coordinates. Clip space is the scaled camera space for frustum clipping, and it\u0026rsquo;ll lead to 2D-plane projection coordinates (x,y) ranging in [-1,1]. However, the ùê≠ in ùêâ must be camera-space coordinates, instead of the clip coordinates, because the perspective division approximated by the Jacobian ùêâ uses coordinates of the 3D mean in camera space.\nThe ùêâ is the projective transform that transforms a point to 3D ray space (screen is the ray space with z-axis omitted). ùêâ is an approximation of the non-linear perspective division with the 1st-order Taylor expansion evaluated at the 3D Gaussian center ùê≠ in camera space.\nIn 3DGS, the 3rd row of ùêâ is set to 0, as the opacity of each Gaussian is learned through gradient descent, instead of an integral over the viewing ray in the ray space. Thus, the z-axis of the ray space is useless in 3DGS.\nBased on the relationship:\n$$ ùêâ = \\begin{bmatrix} f_x/t_z \u0026amp; 0 \u0026amp; -f_x t_x / t_z¬≤ \\\\ 0 \u0026amp; f_y/t_z \u0026amp; -f_y t_y/t_z¬≤ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe derivatives of ùêâ w.r.t. $t_x,\\ t_y,\\ t_z$:\n$$ \\begin{aligned} \\frac{‚àÇùêâ}{‚àÇt_x} \u0026amp;= \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; -f_x / t_z¬≤ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{‚àÇùêâ}{‚àÇt_y} \u0026amp;= \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -f_y / t_z¬≤\\end{bmatrix} \\\\ \\frac{‚àÇùêâ}{‚àÇt_z} \u0026amp;= \\begin{bmatrix} -f_x‚ãÖt_z^{-1} \u0026amp; 0 \u0026amp; 2f_x t_x ‚ãÖt_z^{-3} \\\\ 0 \u0026amp; -f_y‚ãÖt_z^{-1} \u0026amp; 2f_y t_y ‚ãÖt_z^{-3} \\\\ \\end{bmatrix} \\end{aligned} $$\nCombine upstream coefficients:\n$$ \\begin{aligned} \\frac{‚àÇL}{‚àÇt_x} \u0026amp;= \\frac{‚àÇL}{‚àÇùêâ}‚äô \\frac{‚àÇùêâ}{‚àÇt_x} = \\frac{‚àÇL}{‚àÇJ_{02}} ‚ãÖ -f_x/t_z^2 \\\\ \\frac{‚àÇL}{‚àÇt_y} \u0026amp;= \\frac{‚àÇL}{‚àÇùêâ}‚äô \\frac{‚àÇùêâ}{‚àÇt_y} = \\frac{‚àÇL}{‚àÇJ_{12}} ‚ãÖ -f_y/t_z^2 \\\\ \\frac{‚àÇL}{‚àÇt_z} \u0026amp;= \\frac{‚àÇL}{‚àÇùêâ}‚äô \\frac{‚àÇùêâ}{‚àÇt_z} = \\begin{bmatrix} \\frac{‚àÇL}{‚àÇJ‚ÇÄ‚ÇÄ} \u0026amp; 0 \u0026amp; \\frac{‚àÇL}{‚àÇJ‚ÇÄ‚ÇÇ} \\\\ 0 \u0026amp; \\frac{‚àÇL}{J‚ÇÅ‚ÇÅ} \u0026amp; \\frac{‚àÇL}{‚àÇ‚ÇÅ‚ÇÇ} \\end{bmatrix} ‚äô \\begin{bmatrix} -f_x‚ãÖt_z^{-2} \u0026amp; 0 \u0026amp; 2f_x t_x ‚ãÖt_z^{-3} \\\\ 0 \u0026amp; -f_y‚ãÖt_z^{-2} \u0026amp; 2f_y t_y ‚ãÖt_z^{-3} \\\\ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{‚àÇL}{‚àÇJ‚ÇÄ‚ÇÄ} ‚ãÖ-f_x‚ãÖt_z^{-2} \u0026amp; 0 \u0026amp; \\frac{‚àÇL}{‚àÇJ‚ÇÄ‚ÇÇ}‚ãÖ 2f_x t_x ‚ãÖt_z^{-3} \\\\ 0 \u0026amp; \\frac{‚àÇL}{J‚ÇÅ‚ÇÅ} ‚ãÖ-f_y‚ãÖt_z^{-2} \u0026amp; \\frac{‚àÇL}{‚àÇ‚ÇÅ‚ÇÇ} ‚ãÖ2f_y t_y ‚ãÖt_z^{-3} \\end{bmatrix} \\end{aligned} $$\nCode\n1 2 3 4 // h_x and h_y are f_x and f_y float dL_dtx = x_grad_mul * -h_x * tz2 * dL_dJ02; float dL_dty = y_grad_mul * -h_y * tz2 * dL_dJ12; float dL_dtz = -h_x * tz2 * dL_dJ00 - h_y * tz2 * dL_dJ11 + (2 * h_x * t.x) * tz3 * dL_dJ02 + (2 * h_y * t.y) * tz3 * dL_dJ12; dt/dx After the projective transform, the derivative of $ùê≠$ (Gaussian center coordinates) in camera space w.r.t. $ùê±$ in world space follows.\nBased on the relationship of w2c viewmatrix:\n$$ \\begin{aligned} ùê≠_{cam} \u0026amp;= ùêë_{w2c}‚ãÖùê±_{world} + ùêì_{w2c} \\\\ \u0026amp;=\\begin{bmatrix} R‚ÇÄ‚ÇÄ \u0026amp; R‚ÇÄ‚ÇÅ \u0026amp; R‚ÇÄ‚ÇÇ \\\\ R‚ÇÅ‚ÇÄ \u0026amp; R‚ÇÅ‚ÇÅ \u0026amp; R‚ÇÅ‚ÇÇ \\\\ R‚ÇÇ‚ÇÄ \u0026amp; R‚ÇÇ‚ÇÅ \u0026amp; R‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} + \\begin{bmatrix} T_0 \\\\ T_1 \\\\ T_2 \\end{bmatrix} \\\\ \u0026amp;=\\begin{bmatrix} R‚ÇÄ‚ÇÄx + R‚ÇÄ‚ÇÅy + R‚ÇÄ‚ÇÇz + T_0 \\\\ R‚ÇÅ‚ÇÄx + R‚ÇÅ‚ÇÅy + R‚ÇÅ‚ÇÇz + T_1 \\\\ R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T_2 \\end{bmatrix} \\end{aligned} $$\nThe derivative of $ùê≠_{cam}$ w.r.t. $x, y, z$ of $ùê±_{world}$ are:\n$$ \\frac{‚àÇùê≠}{‚àÇx} = \\begin{bmatrix} R‚ÇÄ‚ÇÄ\\\\ R‚ÇÅ‚ÇÄ\\\\ R‚ÇÇ‚ÇÄ \\end{bmatrix}, \\frac{‚àÇùê≠}{‚àÇy} = \\begin{bmatrix} R‚ÇÄ‚ÇÅ\\\\ R‚ÇÅ‚ÇÅ\\\\ R‚ÇÇ‚ÇÅ \\end{bmatrix}, \\frac{‚àÇùê≠}{‚àÇz} = \\begin{bmatrix} R‚ÇÄ‚ÇÇ\\\\ R‚ÇÅ‚ÇÇ\\\\ R‚ÇÇ‚ÇÇ \\end{bmatrix} $$\nThe derivative of $L$ w.r.t. $x, y, z$ of $ùê±_{world}$ are:\n$$ \\frac{‚àÇL}{‚àÇùê≠} \\frac{‚àÇùê≠}{‚àÇx} = \\frac{‚àÇL}{‚àÇt_x}\\begin{bmatrix} R‚ÇÄ‚ÇÄ\\\\ R‚ÇÅ‚ÇÄ\\\\ R‚ÇÇ‚ÇÄ \\end{bmatrix}, \\frac{‚àÇL}{‚àÇùê≠} \\frac{‚àÇùê≠}{‚àÇy} = \\frac{‚àÇL}{‚àÇt_y}\\begin{bmatrix} R‚ÇÄ‚ÇÅ\\\\ R‚ÇÅ‚ÇÅ\\\\ R‚ÇÇ‚ÇÅ \\end{bmatrix}, \\frac{‚àÇL}{‚àÇùê≠} \\frac{‚àÇùê≠}{‚àÇz} = \\frac{‚àÇL}{‚àÇt_z}\\begin{bmatrix} R‚ÇÄ‚ÇÇ\\\\ R‚ÇÅ‚ÇÇ\\\\ R‚ÇÇ‚ÇÇ \\end{bmatrix} $$\nAs the x, y, z are individual variables to be optimized, the derivatives of L w.r.t. each of them shouldn\u0026rsquo;t add up.\n1 2 3 4 5 6 7 8 9 __forceinline__ __device__ float3 transformPoint4x3(const float3\u0026amp; p, const float* matrix) { float3 transformed = { matrix[0] * p.x + matrix[4] * p.y + matrix[8] * p.z + matrix[12], matrix[1] * p.x + matrix[5] * p.y + matrix[9] * p.z + matrix[13], matrix[2] * p.x + matrix[6] * p.y + matrix[10] * p.z + matrix[14], }; return transformed; } bw preprocess (2024-02-15)\nThe preprocess performed conversions from SH to RGB, from quaternion to a 3x3 rotation matrix, and from a stretching vector to the 3x3 covariance matrix in the world space.\nTherefore, the inputs to the function BACKWARD::preprocessCUDA are the derivatives of Loss w.r.t. upstream variables: $\\frac{‚àÇL}{‚àÇ(RGB)},\\ \\frac{‚àÇL}{‚àÇ\\bm Œ£_{world}},\\ \\frac{‚àÇL}{‚àÇùê±_{world}},\\ \\frac{‚àÇL}{‚àÇ\\bm Œº_{2D}}$. to solve the derivatives of Loss w.r.t. stretching vector, quaternion, and SH coeffs.\nd_pix/d_world (2024-02-16)\nGaussian center transformation from world space to 2D screen:\n( Œº s ‚Çì c ùõç , r Œº e _ e y n ) D i v c ùõç l ·∂ú i p P r o j M a t ( x w , o ùê± y r , l z d ) The derivative of Loss w.r.t. the 3D Gaussian mean ùê± in world space comprises of 2 streams:\n$$\\frac{‚àÇL}{‚àÇùê±} = \\frac{‚àÇL}{‚àÇùêâ} \\frac{‚àÇùêâ}{‚àÇùê±} + \\frac{‚àÇL}{‚àÇ\\bm Œº} \\frac{‚àÇ\\bm Œº}{‚àÇùê±} $$\nThe first term resulted from ùêâ has been obtained above, and the relationship between the 2D mean ùõç on screen and 3D ùê± is the full_proj_transform , i.e., projection matrix @ w2c\n$$ \\begin{aligned} \\bm Œº·∂ú \u0026amp;= ùêè [ùêë|ùêì] ùê± \\\\ \\begin{bmatrix} Œº·∂ú_x \\\\ Œº·∂ú_y \\\\ Œº·∂ú_z \\\\ Œº·∂ú_w \\end{bmatrix} \u0026amp;= \\begin{bmatrix} \\frac{2n}{r-l} \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f}{f-n} \u0026amp; \\frac{-f n}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} R‚ÇÄ‚ÇÄ \u0026amp; R‚ÇÄ‚ÇÅ \u0026amp; R‚ÇÄ‚ÇÇ \u0026amp; T‚ÇÄ \\\\ R‚ÇÅ‚ÇÄ \u0026amp; R‚ÇÅ‚ÇÅ \u0026amp; R‚ÇÅ‚ÇÇ \u0026amp; T‚ÇÅ \\\\ R‚ÇÇ‚ÇÄ \u0026amp; R‚ÇÇ‚ÇÅ \u0026amp; R‚ÇÇ‚ÇÇ \u0026amp; T‚ÇÇ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{2n}{r-l} R‚ÇÄ‚ÇÄ + \\frac{r+l}{r-l} R‚ÇÇ‚ÇÄ \u0026amp; \\frac{2n}{r-l} R‚ÇÄ‚ÇÅ + \\frac{r+l}{r-l} R‚ÇÇ‚ÇÅ \u0026amp; \\frac{2n}{r-l} R‚ÇÄ‚ÇÇ + \\frac{r+l}{r-l} R‚ÇÇ‚ÇÇ \u0026amp; \\frac{2n}{r-l} T‚ÇÄ + \\frac{r+l}{r-l} T‚ÇÇ \u0026amp; \\\\ \\frac{2n}{t-b} R‚ÇÅ‚ÇÄ + \\frac{t+b}{t-b} R‚ÇÇ‚ÇÄ \u0026amp; \\frac{2n}{t-b} R‚ÇÄ‚ÇÅ + \\frac{t+b}{t-b} R‚ÇÇ‚ÇÅ \u0026amp; \\frac{2n}{t-b} R‚ÇÄ‚ÇÇ + \\frac{t+b}{t-b} R‚ÇÇ‚ÇÇ \u0026amp; \\frac{2n}{t-b} T‚ÇÄ + \\frac{t+b}{t-b} T‚ÇÇ \u0026amp; \\\\ \\frac{f}{f-n} R‚ÇÇ‚ÇÄ \u0026amp; \\frac{f}{f-n} R‚ÇÇ‚ÇÅ \u0026amp; \\frac{f}{f-n} R‚ÇÇ‚ÇÇ \u0026amp; \\frac{f}{f-n} T‚ÇÇ + \\frac{-f n}{f-n} \\\\ R‚ÇÇ‚ÇÄ \u0026amp; R‚ÇÇ‚ÇÅ \u0026amp; R‚ÇÇ‚ÇÇ \u0026amp; T‚ÇÇ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} (\\frac{2n}{r-l} R‚ÇÄ‚ÇÄ + \\frac{r+l}{r-l} R‚ÇÇ‚ÇÄ)x + (\\frac{2n}{r-l} R‚ÇÄ‚ÇÅ + \\frac{r+l}{r-l} R‚ÇÇ‚ÇÅ)y + (\\frac{2n}{r-l} R‚ÇÄ‚ÇÇ + \\frac{r+l}{r-l} R‚ÇÇ‚ÇÇ)z + (\\frac{2n}{r-l} T‚ÇÄ + \\frac{r+l}{r-l} T‚ÇÇ ) \\\\ (\\frac{2n}{t-b} R‚ÇÅ‚ÇÄ + \\frac{t+b}{t-b} R‚ÇÇ‚ÇÄ)x + (\\frac{2n}{t-b} R‚ÇÄ‚ÇÅ + \\frac{t+b}{t-b} R‚ÇÇ‚ÇÅ)y + (\\frac{2n}{t-b} R‚ÇÄ‚ÇÇ + \\frac{t+b}{t-b} R‚ÇÇ‚ÇÇ)z + (\\frac{2n}{t-b} T‚ÇÄ + \\frac{t+b}{t-b} T‚ÇÇ ) \\\\ (\\frac{f}{f-n} R‚ÇÇ‚ÇÄ)x + (\\frac{f}{f-n} R‚ÇÇ‚ÇÅ)y + (\\frac{f}{f-n} R‚ÇÇ‚ÇÇ)z + (\\frac{f}{f-n} T‚ÇÇ + \\frac{-f n}{f-n}) \\\\ R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ \\end{bmatrix} \\end{aligned} $$\nPerform perspective division to get the 2D-plane pixel coordinates:\n$$ \\begin{aligned} \\bm Œº \u0026amp;= \\frac{1}{Œº·∂ú_w} \\bm Œº·∂ú \\\\ \\begin{bmatrix} Œº_x \\\\ Œº_y \\\\ Œº_z \\\\ 1 \\end{bmatrix} \u0026amp;= \\begin{bmatrix} Œº·∂ú_x/Œº·∂ú_w \\\\ Œº·∂ú_y/Œº·∂ú_w \\\\ Œº·∂ú_z/Œº·∂ú_w \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} Œº·∂ú_x/(R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ) \\\\ Œº·∂ú_y/(R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ) \\\\ Œº·∂ú_z/(R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ) \\\\ 1 \\end{bmatrix} \\end{aligned} $$\nwhere $(Œº_x,\\ Œº_y)$ are the pixel coordinates, and $Œº_z$ is similar to the z in ND space, but ranges in [0,1]. As $Œº_z$ doesn\u0026rsquo;t involve into the image formation in 3DGS, it has no contribution to the Loss, and won\u0026rsquo;t participate the derivative of Loss w.r.t the world coordinates. The derivative of $\\bm Œº$ w.r.t. $ùê±$:\n$$ \\begin{aligned} \\frac{‚àÇŒº_x}{‚àÇx} \u0026amp;= \\frac{(\\frac{2n}{r-l} R‚ÇÄ‚ÇÄ + \\frac{r+l}{r-l} R‚ÇÇ‚ÇÄ)(R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ) - Œº·∂ú_x R‚ÇÇ‚ÇÄ }{(R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ)^2} \\\\ \u0026amp;+ \\frac{ (\\frac{2n}{t-b} R‚ÇÅ‚ÇÄ + \\frac{t+b}{t-b} R‚ÇÇ‚ÇÄ) (R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ) - Œº·∂ú_y R‚ÇÇ‚ÇÄ }{(R‚ÇÇ‚ÇÄx + R‚ÇÇ‚ÇÅy + R‚ÇÇ‚ÇÇz + T‚ÇÇ)^2} \\end{aligned} $$\nThe element indices in the float array proj is column-major: $\\begin{bmatrix} 0 \u0026amp; 4 \u0026amp; 8 \u0026amp; 12 \\\\ 1 \u0026amp; 5 \u0026amp; 9 \u0026amp; 13 \\\\ 2 \u0026amp; 6 \u0026amp; 10 \u0026amp; 14 \\\\ 3 \u0026amp; 7 \u0026amp; 11 \u0026amp; 15 \\end{bmatrix}$\n1 2 3 4 5 6 7 8 float m_w = 1.0f / (m_hom.w + 0.0000001f); glm::vec3 dL_dmean; float mul1 = (proj[0] * m.x + proj[4] * m.y + proj[8] * m.z + proj[12]) * m_w * m_w; float mul2 = (proj[1] * m.x + proj[5] * m.y + proj[9] * m.z + proj[13]) * m_w * m_w; dL_dmean.x = (proj[0] * m_w - proj[3] * mul1) * dL_dmean2D[idx].x + (proj[1] * m_w - proj[3] * mul2) * dL_dmean2D[idx].y; dL_dmean.y = (proj[4] * m_w - proj[7] * mul1) * dL_dmean2D[idx].x + (proj[5] * m_w - proj[7] * mul2) * dL_dmean2D[idx].y; dL_dmean.z = (proj[8] * m_w - proj[11] * mul1) * dL_dmean2D[idx].x + (proj[9] * m_w - proj[11] * mul2) * dL_dmean2D[idx].y; d_RGB/d_SH d_cov3D/d_M (2024-02-17)\nGoal: Compute the derivative w.r.t. the entire matrix ùêå (not per element).\nGiven a quaternion: $q = [r,x,y,z]$, the rotation matrix is:\n$$ ùêë = \\begin{bmatrix} 1 - 2(y¬≤ + z¬≤) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 2(xy + rz) \u0026amp; 1 - 2(x¬≤ + z¬≤) \u0026amp; 2(yz - rx) \\\\ 2(xz - ry) \u0026amp; 2(yz + rx) \u0026amp; 1 - 2(x¬≤+y¬≤) \\end{bmatrix} $$\nGiven a scaling vector $s=[s_x, s_y, s_z]$, the stretching matrix is:\n$$ ùêí = \\begin{bmatrix} s_x \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_z \\end{bmatrix} $$\nThe covariance matrix can be decomposed by SVD as:\n$$ \\bm Œ£ = ùêëùêíùêí·µÄùêë·µÄ $$\nUse ùêå to represent ùêëùêí:\n$$ ùêå = ùêëùêí \\\\ = \\begin{bmatrix} s_x(1 - 2(y¬≤ + z¬≤)) \u0026amp; s_y(2(xy - rz)) \u0026amp; s_z(2(xz + ry)) \\\\ s_x(2(xy + rz)) \u0026amp; s_y(1 - 2(x¬≤ + z¬≤)) \u0026amp; s_z(2(yz - rx)) \\\\ s_x(2(xz - ry)) \u0026amp; s_y(2(yz + rx)) \u0026amp; s_z(1 - 2(x¬≤+y¬≤)) s_x\\end{bmatrix} $$\nTherefore, $\\bm Œ£ = ùêå ùêå·µÄ$\n$$ \\bm Œ£ = \\begin{bmatrix} m‚ÇÄ‚ÇÄ \u0026amp; m‚ÇÄ‚ÇÅ \u0026amp; m‚ÇÄ‚ÇÇ \\\\ m‚ÇÅ‚ÇÄ \u0026amp; m‚ÇÅ‚ÇÅ \u0026amp; m‚ÇÅ‚ÇÇ \\\\ m‚ÇÇ‚ÇÄ \u0026amp; m‚ÇÇ‚ÇÅ \u0026amp; m‚ÇÇ‚ÇÇ \\\\ \\end{bmatrix} \\begin{bmatrix} m‚ÇÄ‚ÇÄ \u0026amp; m‚ÇÅ‚ÇÄ \u0026amp; m‚ÇÇ‚ÇÄ \\\\ m‚ÇÄ‚ÇÅ \u0026amp; m‚ÇÅ‚ÇÅ \u0026amp; m‚ÇÇ‚ÇÅ \\\\ m‚ÇÄ‚ÇÇ \u0026amp; m‚ÇÅ‚ÇÇ \u0026amp; m‚ÇÇ‚ÇÇ \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} m‚ÇÄ‚ÇÄ¬≤+m‚ÇÄ‚ÇÅ¬≤+m‚ÇÄ‚ÇÇ¬≤ \u0026amp; m‚ÇÄ‚ÇÄm‚ÇÅ‚ÇÄ+m‚ÇÄ‚ÇÅm‚ÇÅ‚ÇÅ+m‚ÇÄ‚ÇÇm‚ÇÅ‚ÇÇ \u0026amp; m‚ÇÄ‚ÇÄm‚ÇÇ‚ÇÄ+m‚ÇÄ‚ÇÅm‚ÇÇ‚ÇÅ+m‚ÇÄ‚ÇÇm‚ÇÇ‚ÇÇ \\\\ m‚ÇÅ‚ÇÄm‚ÇÄ‚ÇÄ+m‚ÇÅ‚ÇÅm‚ÇÄ‚ÇÅ+m‚ÇÅ‚ÇÇm‚ÇÄ‚ÇÇ \u0026amp; m‚ÇÅ‚ÇÄ¬≤+m‚ÇÅ‚ÇÅ¬≤+m‚ÇÅ‚ÇÇ¬≤ \u0026amp; m‚ÇÅ‚ÇÄm‚ÇÇ‚ÇÄ+m‚ÇÅ‚ÇÅm‚ÇÇ‚ÇÅ+m‚ÇÅ‚ÇÇm‚ÇÇ‚ÇÇ \\\\ m‚ÇÇ‚ÇÄm‚ÇÄ‚ÇÄ+m‚ÇÇ‚ÇÅm‚ÇÄ‚ÇÅ+m‚ÇÇ‚ÇÇm‚ÇÄ‚ÇÇ \u0026amp; m‚ÇÇ‚ÇÄm‚ÇÅ‚ÇÄ+m‚ÇÇ‚ÇÅm‚ÇÅ‚ÇÅ+m‚ÇÇ‚ÇÇm‚ÇÅ‚ÇÇ \u0026amp; m‚ÇÇ‚ÇÄ¬≤+m‚ÇÇ‚ÇÅ¬≤+m‚ÇÇ‚ÇÇ¬≤ \\\\ \\end{bmatrix} $$\nThe partial derivative of covariance matrix w.r.t. the element m‚ÇÄ‚ÇÄ:\n$$ \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÄ‚ÇÄ} = \\begin{bmatrix} 2m‚ÇÄ‚ÇÄ \u0026amp; m‚ÇÅ‚ÇÄ \u0026amp; m‚ÇÇ‚ÇÄ \\\\ m‚ÇÅ‚ÇÄ \u0026amp; 0 \u0026amp; 0 \\\\ m‚ÇÇ‚ÇÄ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe contribution of m‚ÇÄ‚ÇÄ to the ùö∫ is the summation of all elements. Similarly, other elements\u0026rsquo; contiribution to dùö∫ are as follows:\n$$ \\begin{aligned} \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÄ‚ÇÄ} = 2(m‚ÇÄ‚ÇÄ+m‚ÇÅ‚ÇÄ+m‚ÇÇ‚ÇÄ)\\ \u0026amp; \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÄ‚ÇÅ} = 2(m‚ÇÄ‚ÇÅ+m‚ÇÅ‚ÇÅ+m‚ÇÇ‚ÇÅ) \u0026amp; \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÄ‚ÇÇ} = 2(m‚ÇÄ‚ÇÇ+m‚ÇÅ‚ÇÇ+m‚ÇÇ‚ÇÇ) \\\\ \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÅ‚ÇÄ} = 2(m‚ÇÄ‚ÇÄ+m‚ÇÅ‚ÇÄ+m‚ÇÇ‚ÇÄ)\\ \u0026amp; \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÅ‚ÇÅ} = 2(m‚ÇÄ‚ÇÅ+m‚ÇÅ‚ÇÅ+m‚ÇÇ‚ÇÅ) \u0026amp; \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÅ‚ÇÇ} = 2(m‚ÇÄ‚ÇÇ+m‚ÇÅ‚ÇÇ+m‚ÇÇ‚ÇÇ) \\\\ \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÇ‚ÇÄ} = 2(m‚ÇÄ‚ÇÄ+m‚ÇÅ‚ÇÄ+m‚ÇÇ‚ÇÄ)\\ \u0026amp; \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÇ‚ÇÅ} = 2(m‚ÇÄ‚ÇÅ+m‚ÇÅ‚ÇÅ+m‚ÇÇ‚ÇÅ) \u0026amp; \\frac{‚àÇ\\bm Œ£}{‚àÇm‚ÇÇ‚ÇÇ} = 2(m‚ÇÄ‚ÇÇ+m‚ÇÅ‚ÇÇ+m‚ÇÇ‚ÇÇ) \\end{aligned} $$\n$$ \\frac{‚àÇ\\bm Œ£}{‚àÇùêå } = 2 \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} m‚ÇÄ‚ÇÄ \u0026amp; m‚ÇÄ‚ÇÅ \u0026amp; m‚ÇÄ‚ÇÇ \\\\ m‚ÇÅ‚ÇÄ \u0026amp; m‚ÇÅ‚ÇÅ \u0026amp; m‚ÇÅ‚ÇÇ \\\\ m‚ÇÇ‚ÇÄ \u0026amp; m‚ÇÇ‚ÇÅ \u0026amp; m‚ÇÇ‚ÇÇ \\\\ \\end{bmatrix} $$\nConcat with the upstream coefficient to get the derivative of Loss w.r.t. the matrix ùêå: $\\frac{‚àÇL}{‚àÇùêå } = \\frac{‚àÇL}{‚àÇ\\bm Œ£} \\frac{‚àÇ\\bm Œ£}{‚àÇùêå }$.\nIn the code, however, $\\frac{‚àÇ\\bm Œ£}{‚àÇùêå }$ is on the left: $\\frac{‚àÇL}{‚àÇùêå } = \\frac{‚àÇ\\bm Œ£}{‚àÇùêå } \\frac{‚àÇL}{‚àÇ\\bm Œ£}$\nBecause the $\\frac{‚àÇL}{‚àÇùêå }$ to be solved is a derivative w.r.t. the entire matrix ùêå, not for each element, the 2 \u0026ldquo;derivative matrices\u0026rdquo; should perform matrix multiplication to get all terms mixtured completely, rather than an elementise product:\n$$ \\frac{‚àÇL}{‚àÇùêå } = \\frac{‚àÇ\\bm Œ£}{‚àÇùêå } \\frac{‚àÇL}{‚àÇ\\bm Œ£} \\\\ =\\begin{bmatrix} 2(m‚ÇÄ‚ÇÄ+m‚ÇÅ‚ÇÄ+m‚ÇÇ‚ÇÄ) \u0026amp; ‚ãØ \u0026amp; ‚ãØ\\\\ 2(m‚ÇÄ‚ÇÄ+m‚ÇÅ‚ÇÄ+m‚ÇÇ‚ÇÄ) \u0026amp; ‚ãØ \u0026amp; ‚ãØ\\\\ ‚ãØ \u0026amp; ‚ãØ \u0026amp; ‚ãØ \\end{bmatrix} \\begin{bmatrix} \\frac{‚àÇL}{‚àÇv_0} \u0026amp; \\frac{‚àÇL}{‚àÇv_1} \u0026amp; \\frac{‚àÇL}{‚àÇv_2} \\\\ \\frac{‚àÇL}{‚àÇv_1} \u0026amp; \\frac{‚àÇL}{‚àÇv_3} \u0026amp; \\frac{‚àÇL}{‚àÇv_4} \\\\ \\frac{‚àÇL}{‚àÇv_2} \u0026amp; \\frac{‚àÇL}{‚àÇv_4} \u0026amp; \\frac{‚àÇL}{‚àÇv_5} \\end{bmatrix} $$\n1 glm::mat3 dL_dM = 2.0f * M * dL_dSigma; glm performs matrix multiplication according to the same rules as in the math textbook, and the only difference is that the row and column indices for an elements are reversed: mat[col][row] for an element; mat[col] for a column vector. Matrices and Quaternions FAQ - opengl-tutorial Tutorial 3 : Matrices - opengl-tutorial dM/ds Goal: Compute derivative of loss w.r.t. each element in the scaling vector.\nIn the code, the ùêå =ùêí ùêë, not ùêëùêí,\n$$ ùêå =\\begin{bmatrix} s_x \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_z \\end{bmatrix} \\begin{bmatrix} 1 - 2(y¬≤ + z¬≤) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 2(xy + rz) \u0026amp; 1 - 2(x¬≤ + z¬≤) \u0026amp; 2(yz - rx) \\\\ 2(xz - ry) \u0026amp; 2(yz + rx) \u0026amp; 1 - 2(x¬≤+y¬≤) \\end{bmatrix} \\\\ =\\begin{bmatrix} s_x(1 - 2(y¬≤ + z¬≤)) \u0026amp; s_x(2(xy - rz)) \u0026amp; s_x(2(xz + ry)) \\\\ s_y(2(xy + rz)) \u0026amp; s_y(1 - 2(x¬≤ + z¬≤)) \u0026amp; s_y(2(yz - rx)) \\\\ s_z(2(xz - ry)) \u0026amp; s_z(2(yz + rx)) \u0026amp; s_z(1 - 2(x¬≤+y¬≤)) \\end{bmatrix} $$\nCalculate the partial derivative of ùêå w.r.t. the scaling vector ùêí by element:\n$$ \\frac{‚àÇùêå }{‚àÇs_x} = \\begin{bmatrix} 1-2(y¬≤ + z¬≤) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp;0 \\end{bmatrix} $$\nAs here it\u0026rsquo;s computing the derivative of the loss w.r.t. each element, the multiplication with the upstream derivative part is an element-wise product:\n$$ \\frac{‚àÇL}{‚àÇs_x} = \\frac{‚àÇL}{‚àÇùêå }\\frac{‚àÇùêå }{‚àÇs_x} = \\\\ \\begin{bmatrix} s_x(1 - 2(y¬≤ + z¬≤)) \u0026amp; s_x(2(xy - rz)) \u0026amp; s_x(2(xz + ry)) \\\\ ‚ãØ \u0026amp; ‚ãØ \u0026amp; ‚ãØ \\\\ ‚ãØ \u0026amp; ‚ãØ \u0026amp; ‚ãØ \\\\ \\end{bmatrix} ‚äô \\begin{bmatrix} 1-2(y¬≤ + z¬≤) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp;0 \\end{bmatrix} $$\nBecause glm indexes elements by columns, to easily access a row in the above 2 matrices, their transposed matrices are used:\n1 2 3 4 5 6 glm::mat3 Rt = glm::transpose(R); glm::mat3 dL_dMt = glm::transpose(dL_dM); // Gradients of loss w.r.t. scale glm::vec3* dL_dscale = dL_dscales + idx; dL_dscale-\u0026gt;x = glm::dot(Rt[0], dL_dMt[0]);\t// scalar mat[0] takes out a column, as the indices in glm is column-major. 9. Matrix columns dM/dq Goal: Compute derivative of loss w.r.t. each element in the quaternion vector (r,x,y,z).\nBased on the realtionship:\n$$ ùêå =\\begin{bmatrix} s_x(1 - 2(y¬≤ + z¬≤)) \u0026amp; s_x(2(xy - rz)) \u0026amp; s_x(2(xz + ry)) \\\\ s_y(2(xy + rz)) \u0026amp; s_y(1 - 2(x¬≤ + z¬≤)) \u0026amp; s_y(2(yz - rx)) \\\\ s_z(2(xz - ry)) \u0026amp; s_z(2(yz + rx)) \u0026amp; s_z(1 - 2(x¬≤+y¬≤)) \\end{bmatrix} $$\n$$ \\frac{‚àÇùêå }{‚àÇr} = \\begin{bmatrix} 0 \u0026amp; -2s_x z \u0026amp; 2s_x y \\\\ 2s_y z \u0026amp; 0 \u0026amp; -2s_y x \\\\ -2s_z y \u0026amp; 2s_z x \u0026amp; 0 \\end{bmatrix} $$\nConcat with the obtained upstream derivative of Loss w.r.t. ùêå through element-wise (Hadamard) product:\n$$ \\frac{‚àÇL}{‚àÇr} = \\frac{‚àÇL}{‚àÇùêå } \\frac{‚àÇùêå }{‚àÇr} = \\\\ \\begin{bmatrix} \\frac{‚àÇL}{‚àÇm_{00}} \u0026amp; \\frac{‚àÇL}{‚àÇm_{01}} \u0026amp; \\frac{‚àÇL}{‚àÇm_{02}} \\\\ \\frac{‚àÇL}{‚àÇm_{10}} \u0026amp; \\frac{‚àÇL}{‚àÇm_{11}} \u0026amp; \\frac{‚àÇL}{‚àÇm_{12}} \\\\ \\frac{‚àÇL}{‚àÇm_{20}} \u0026amp; \\frac{‚àÇL}{‚àÇm_{21}} \u0026amp; \\frac{‚àÇL}{‚àÇm_{22}} \\\\ \\end{bmatrix} ‚äô \\begin{bmatrix} 0 \u0026amp; -2s_x z \u0026amp; 2s_x y \\\\ 2s_y z \u0026amp; 0 \u0026amp; -2s_y x \\\\ -2s_z y \u0026amp; 2s_z x \u0026amp; 0 \\end{bmatrix} \\\\ = \\begin{bmatrix} 0 \u0026amp; \\frac{‚àÇL}{‚àÇm_{01}} (-2s_x z) \u0026amp; \\frac{‚àÇL}{‚àÇm_{02}} 2s_x y \\\\ \\frac{‚àÇL}{‚àÇm_{10}} 2s_y z \u0026amp; 0 \u0026amp; \\frac{‚àÇL}{‚àÇm_{12}} (-2s_y x) \\\\ \\frac{‚àÇL}{‚àÇm_{20}} (-2s_z y) \u0026amp; \\frac{‚àÇL}{‚àÇm_{21}} 2s_z x \u0026amp; 0 \\\\ \\end{bmatrix} $$\n$$ $$\nSum all element in the result matrix $\\frac{‚àÇL}{‚àÇr}$:\n1 2 3 4 5 6 7 8 9 dL_dMt[0] *= s.x; dL_dMt[1] *= s.y; dL_dMt[2] *= s.z; // Gradients of loss w.r.t. normalized quaternion glm::vec4 dL_dq; dL_dq.x = 2 * z * (dL_dMt[0][1] - dL_dMt[1][0]) + 2 * y * (dL_dMt[2][0] - dL_dMt[0][2]) + 2 * x * (dL_dMt[1][2] - dL_dMt[2][1]); Finally, calculate the derivative of loss for the normalization process of the quaternion.\nBased on the normalization operation:\n$$ ùê™_{unit} = \\frac{1}{‚Äñùê™‚Äñ} ‚ãÖ ùê™ = \\frac{1}{\\sqrt{r¬≤+x¬≤+y¬≤+z¬≤}} \\begin{bmatrix} r \\\\ x \\\\ y \\\\ z \\end{bmatrix} \\\\ \\frac{‚àÇùê™}{‚àÇr} = \\frac{1}{r¬≤+x¬≤+y¬≤+z¬≤} - \\frac{r¬≤}{(r¬≤+x¬≤+y¬≤+z¬≤)^{3/2}} $$\nIt seems that the derivatives of the normalization step contributions are not considered in the code:\n1 2 3 4 // Gradients of loss w.r.t. unnormalized quaternion float4* dL_drot = (float4*)(dL_drots + idx); *dL_drot = float4{ dL_dq.x, dL_dq.y, dL_dq.z, dL_dq.w }; //dnormvdv(float4{ rot.x, rot.y, rot.z, rot.w }, float4{ dL_dq.x, dL_dq.y, dL_dq.z, dL_dq.w }); ","date":"2024-01-22T00:00:00Z","image":"https://user-images.githubusercontent.com/52232153/283030293-f9f3589c-fa59-431f-8bd0-478337426933.png","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-code/","title":"read: 3DGS | Code Understanding"},{"content":"CUDA Tutorials I Profiling and Debugging Applications - NVIDIA Developer\n(2024-01-20)\nSource video: GPU L16: Support: cuda-gdb - YouTube - HPC Education (Rupesh Nasre 2021)\nIt\u0026rsquo;s a gdb extension for real hardware (not a simulator). Comparing with Nsight having GUI, CUDA-GDB is CLI. Regretfully, cuda-gdb doesn\u0026rsquo;t have TUI. Capture Last Error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Filename: test_cuda-gdb.cu #include \u0026lt;cuda_runtime.h\u0026gt; // to synchronize #include \u0026lt;cstdio\u0026gt; __global__ void kernel(int* x) { *x = 0; printf(\u0026#34;%d\\n\u0026#34;, *x); } int main() { int* x; kernel\u0026lt;\u0026lt;\u0026lt;2, 10\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); // Capture error cudaError_t err = cudaGetLastError(); printf(\u0026#34;err=%d, %s, %s\\n\u0026#34;, err, cudaGetErrorName(err), cudaGetErrorString(err) ); return 0; } Build: nvcc test_cuda-gdb.cu. Execution: ./a.out\nNothing is printed out, although 0 is supposed to show.\nAnd no error is reported, because the CPU sometimes isn\u0026rsquo;t aware of the error (e.g., SegFault) that happens on the GPU.\nTo identify whether the error occurred on the GPU, cudaGetLastError()\n1 2 yi@yi-Alien:~/Downloads/CUDA_Study/Debug_CUDA$ ./a.out err=700, cudaErrorIllegalAddress, an illegal memory access was encountered x requires GPU memory allocated:\n1 2 3 4 5 6 7 8 9 10 11 int main() { int* x; cudaMalloc( (void**)\u0026amp;x, 1*sizeof(int) ); kernel\u0026lt;\u0026lt;\u0026lt;2,2\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); cudaFree(x); cudaError_t err = cudaGetLastError(); printf(\u0026#34;err=%d, %s, %s\\n\u0026#34;, err, cudaGetErrorName(err), cudaGetErrorString(err) ); return 0; } Output 1 2 3 4 5 6 yi@yi-Alien:~/Downloads/CUDA_Study/Debug_CUDA$ ./a.out 0 0 0 0 err=0, cudaSuccess, no error cudaError Homework: Write programs to invoke these errors.\nRef:\nProper CUDA Error Checking - Lei Mao\u0026rsquo;s Log Book\nCUDA DEBUGGING - Bob Crovella, 9/14/2021\nNVIDIA CUDA Library: cudaError\nCUDA-GDB CLI Set flags to include the symbol information (variable name, function name) into the binary file:\nNames of variables and functions are used only for programming, as execution is instructed by memory addresses. So, symbols will be discarded for efficiency after compilation by default. 1 nvcc -g -G main.cu -g is for __host__ functions, compiled by gcc.\n-G is for __device__ functions, compiled by nvcc.\nDisable optimizations (preventing remove unused code) for debugging line-by-line.\nDebugging with cuda-gdb:\n1 cuda-gdb a.out Given the erroneous code:\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;cuda.h\u0026gt; __global__ void kernel(int* x) { *x = 0; printf(\u0026#34;%d\\n\u0026#34;, *x); } int main() { int* x; kernel\u0026lt;\u0026lt;\u0026lt;2, 2\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); return 0; } Build: nvcc test_cuda-gdb.cu. Debug: cuda-gdb a.out.\nrun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (cuda-gdb) run Starting program: /home/yi/Downloads/CUDA_Study/Debug_CUDA/a.out [Thread debugging using libthread_db enabled] Using host libthread_db library \u0026#34;/lib/x86_64-linux-gnu/libthread_db.so.1\u0026#34;. [New Thread 0x7ffff5d9b000 (LWP 2434197)] [New Thread 0x7ffff4ab1000 (LWP 2434198)] [Detaching after fork from child process 2434199] [New Thread 0x7fffeef3d000 (LWP 2434215)] [New Thread 0x7fffed533000 (LWP 2434216)] CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x100002ede48 Thread 1 \u0026#34;a.out\u0026#34; received signal CUDA_EXCEPTION_14, Warp Illegal Address. [Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] 0x00000100002ede78 in kernel(int*)\u0026lt;\u0026lt;\u0026lt;(2,1,1),(2,1,1)\u0026gt;\u0026gt;\u0026gt; () LWP: Light weight process Switching focus to a specific thread info cuda kernels Intro to GPU: 06 Debugging on GPU - YouTube - NERSC\n","date":"2024-01-20T17:20:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/debug_gdb/","title":"memo: CUDA | Debug with CUDA-GDB"},{"content":" Source video: „ÄêÂπ∂Ë°åËÆ°ÁÆó„ÄëCUDAÂú®Áé∞‰ª£C++‰∏≠Â¶Ç‰ΩïËøêÁî®ÔºüÁúãËøô‰∏Ä‰∏™Â∞±Â§ü‰∫ÜÔºÅ- ÂèåÁ¨ôÂ≠ê‰ΩØË∞¨ - bilibili parallel101/course - Github Testing repo Textbook; pdf Enable CUDA in CMake 1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.10) set(CMAKE_CXX_STANDARD 17) set(CMAKE_BUILD_TYPE Release) # add CUDA project(hellocuda LANGUAGES CXX CUDA) add_executable(main main.cu) CUDA syntax is compatible with C++, so nvcc can compile a C++ project by chaning all .cpp files renamed to .cu. The nvcc can compile CPU and GPU code jointly. CPU-GPU Asyncronous For the sake of efficiency, after CPU tells GPU to run the kernel function (decorated by __global__), CPU proceeds to the next line of code without waiting for the GPU to finish the computation.\nTherefore, in the following code, the printf won\u0026rsquo;t be executed because programs returns directly after CPU pushes the task to GPU execution queue. However, the GPU didn\u0026rsquo;t have time to execute and return results.\n1 2 3 4 5 6 7 8 9 10 11 // Filename: test_async.cu #include \u0026lt;cstdio\u0026gt; __global__ void kernel() { printf(\u0026#34;Hello World!\\n\u0026#34;); } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(); return 0; } Compile: nvcc test_async.cu. Execute application: ./a.out. Set the program to wait for GPU completing all the tasks in its queue:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;iostream\u0026gt; __global__ void kernel() { printf(\u0026#34;Hello World!\\n\u0026#34;); // std::cout \u0026lt;\u0026lt; \u0026#34;out\u0026#34; \u0026lt;\u0026lt; std::endl; } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); return 0; } std::cout and std::endl are \u0026ldquo;host (CPU) functions\u0026rdquo;, which can\u0026rsquo;t be executed on GPU.\n1 test_async.cu(7): error: calling a __host__ function (\u0026#34;std::basic_ostream\u0026lt;char, st...\u0026#34;) from a __global__ function(\u0026#34;kernel\u0026#34;) is not allowed __host__ functions are compiled to callable only for other host functions. NV Forums\nFunction types Docs - Sec 7.1 Function Execution Space Specifiers\n__global__ function: called from the host or other devices, and executed on the device.\n__device__ function: called from other __device__ (or __global__) functions and executed on device.\n__host__ function: called from __host__ functions and executed on CPU.\nA function without decorated by any execution space specifier is compiled as a __host__ function.\nCalling a __device__ function (from other devices) doesn\u0026rsquo;t need \u0026lt;\u0026lt;\u0026lt; \u0026gt;\u0026gt;\u0026gt;, as it\u0026rsquo;s called on the GPU interally:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;cstdio\u0026gt; __device__ void say_hello() { printf(\u0026#34;hello\\n\u0026#34;); } __global__ void kernel() { say_hello(); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); return 0; } Since __gloabl__ functions are asyncronous and won\u0026rsquo;t return immediately, their return type must be void.\nHowever, the __device__ can have return value, like a normal function.\nA function can be called from either GPU or CPU with using both specifier: __host__ __device__\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; __host__ __device__ void say_hello() { printf(\u0026#34;hello~\\n\u0026#34;); } __global__ void kernel() { say_hello(); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); // gpu version cudaDeviceSynchronize(); say_hello(); // cpu version } Wil the computation in say_hello executed both by CPU and GPU?\nThe constexpr keyword can be replaced with __host__ __device__ by nvcc compiler to enable a constexpr function (e.g., math function) can be called from either a host or a device.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; constexpr const char* cuthead(const char* p) { return p + 1; } __global__ void kernel() { printf(cuthead(\u0026#34;Hello World!\\n\u0026#34;)); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); print(cuthead(\u0026#34;ABC\\n\u0026#34;)); return 0; } By decorating with __host__ __device__, the constexpr function will be inlined automatically.\nEnable the nvcc flag --expt-relaxed-constexpr with a \u0026ldquo;CMakeÁöÑÁîüÊàêÂô®Ë°®ËææÂºèÊù•ÂÆûÁé∞Âè™ÂØπ .cu Êñá‰ª∂ÊúâÊïàÔºåËÄå‰∏ç‰ºöÂú® gcc ÁºñËØë .c Êñá‰ª∂Êó∂ÁîüÊïàÔºå‰∏çÁÑ∂ÁªôÂà∞ gcc Â∞±Âá∫Èîô‰∫Ü\u0026rdquo; (?):\n1 2 add_executable(main main.cu foo.cpp) target_compile_options(main PUBLIC $\u0026lt;$\u0026lt;COMPILE_LANGUAGE:CUDA\u0026gt;:--expt-relaxed-constexpr\u0026gt;) However, on the contrary, __host__ __device__ can\u0026rsquo;t be replaced with constexpr, because constexpr function cannot call printf and GPU-specific functions, like _syncthreads. inline device function Docs - Sec 7.1.5\nIf appropriate, the compiler will inline __device__ functions automatically.\nWhen the function body is too big, the compiler may won\u0026rsquo;t insert code. __noinline__ declares a function that won\u0026rsquo;t be inserted into the place where it\u0026rsquo;s called.\nAnd __forceinline__ is the opposite.\n","date":"2024-01-19T14:58:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/tut_%E5%BD%AD%E4%BA%8E%E6%96%8C/","title":"watch: Parallel101 - ÂΩ≠‰∫éÊñå | CUDA Programming"},{"content":"(2024-02-14)\nDerivative is the amount of change in a target object caused by a variable\u0026rsquo;s change.\nA row a matrix consists of the coefficient of each term in a linear equation. And based on the \u0026ldquo;sum rule\u0026rdquo; of derivative ($(f+g)\u0026rsquo;=f\u0026rsquo;+g\u0026rsquo;$), the derivative of the linear equation w.r.t. a variable is the summation of the derivative of each element in the row w.r.t. the variable.\nd Ax (2024-01-13)\nSource video: Derivative of a Matrix : Data Science Basics - ritvikmath\nMatrix ùêÄ() stands for a linear transformation (function). And only the derivative of a function (ùêÄùê±) makes sense.\nMatrix is a representation of linear systems. $$ \\begin{aligned} f(x) \u0026amp;= ùêÄùê± \\\\ \u0026amp;= \\begin{bmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\end{bmatrix} \\begin{bmatrix} x‚ÇÅ \\\\ x_2 \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} x‚ÇÅ + 2 x‚ÇÇ \\\\ 3x‚ÇÅ + 4x‚ÇÇ \\end{bmatrix} ‚áí \\begin{bmatrix} f‚ÇÅ(x‚ÇÅ,x‚ÇÇ) \\\\ f‚ÇÇ(x‚ÇÅ,x‚ÇÇ) \\end{bmatrix} \\end{aligned} $$\n$$ \\frac{dùêÄùê±}{dùê±} = \\begin{bmatrix} ‚àÇf‚ÇÅ/‚àÇx‚ÇÅ \u0026amp; ‚àÇf‚ÇÅ/‚àÇx‚ÇÇ \\\\ ‚àÇf‚ÇÇ/‚àÇx‚ÇÅ \u0026amp; ‚àÇf‚ÇÇ/‚àÇx‚ÇÇ \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\end{bmatrix} $$\nThe derivative of the linear transformation ùêÄùê± w.r.t. x is A. It analog to single-variable function.\nA matrix $A$ is a \u0026ldquo;scalar\u0026rdquo;. More concretely, it\u0026rsquo;s a collection of scalars in a box.\nTherefore, the derivative of A means the derivative of a constant, which would be 0. So, it doesn\u0026rsquo;t make any sense.\nThereby, we are not calculating the derivative of a matrix, but the derivative of a linear transformation ùêÄùê± w.r.t. ùê±.\nd x·µÄAx $$ \\begin{aligned} ùê±·µÄùêÄùê± \u0026amp;= \\begin{bmatrix} x‚ÇÅ \u0026amp; x‚ÇÇ \\end{bmatrix} \\begin{bmatrix} a‚ÇÅ‚ÇÅ \u0026amp; a‚ÇÅ‚ÇÇ \\\\ a‚ÇÇ‚ÇÅ \u0026amp; a‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} x‚ÇÅ \\\\ x‚ÇÇ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} x‚ÇÅ \u0026amp; x‚ÇÇ \\end{bmatrix} \\begin{bmatrix} a‚ÇÅ‚ÇÅx‚ÇÅ+ a‚ÇÅ‚ÇÇx‚ÇÇ \\\\ a‚ÇÇ‚ÇÅx‚ÇÅ + a‚ÇÇ‚ÇÇx‚ÇÇ \\end{bmatrix} \\\\ \u0026amp;= a‚ÇÅ‚ÇÅx‚ÇÅ¬≤+ a‚ÇÅ‚ÇÇx‚ÇÅx‚ÇÇ + a‚ÇÇ‚ÇÅx‚ÇÅx‚ÇÇ + a‚ÇÇ‚ÇÇx‚ÇÇ¬≤ ‚áí f(x‚ÇÅ,x‚ÇÇ) \\end{aligned} $$\nConsider ùêÄ is a symmetric matrix, so a‚ÇÇ = a‚ÇÉ. Then, $ùê±·µÄùêÄùê± = a‚ÇÅ‚ÇÅx‚ÇÅ¬≤+ a‚ÇÅ‚ÇÇx‚ÇÅx‚ÇÇ + a‚ÇÇ‚ÇÅx‚ÇÅx‚ÇÇ + a‚ÇÇ‚ÇÇx‚ÇÇ¬≤ = f(x‚ÇÅ,x‚ÇÇ)$\nThe derivative of the linear transformation ùê±·µÄùêÄùê±:\n$$ \\begin{aligned} \\frac{dùê±·µÄùêÄùê±}{dùê±} \u0026amp;= \\begin{bmatrix} ‚àÇf/‚àÇx‚ÇÅ \\\\ ‚àÇf/‚àÇx‚ÇÇ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} 2a‚ÇÅ‚ÇÅx‚ÇÅ+2a‚ÇÅ‚ÇÇx‚ÇÇ \\\\ 2a‚ÇÅ‚ÇÇx‚ÇÇ + 2a‚ÇÇ‚ÇÇx‚ÇÇ \\end{bmatrix} \\\\ \u0026amp;= 2 \\begin{bmatrix} a‚ÇÅ‚ÇÅ \u0026amp; a‚ÇÅ‚ÇÇ \\\\ a‚ÇÅ‚ÇÇ \u0026amp; a‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} x‚ÇÅ \\\\ x‚ÇÇ \\end{bmatrix} \\\\ \u0026amp;= 2ùêÄùê± \\end{aligned} $$\nIt\u0026rsquo;s an analog to quadratic of matrix operations.\n3 cases Source article: The derivative matrix - Math Insight\nA matrix ùêÄ contains elements that are functions of a scalar x.\nThe $\\frac{dùêÄ}{dx}$ is a matrix of the same size as ùêÄ.\nRefer to Definition 5 in Matrix Differentiation - Department of Atmospheric Sciences\nThe derivative of a multi-variable scalar-valued function $f$ is a matrix of partial derivatives of each function with respect to each variable.\nDerivative of ùêü w.r.t. each coordinate axis. $\\frac{df}{dùê±} = [ \\frac{‚àÇf}{‚àÇx‚ÇÅ}\\ \\frac{‚àÇf}{‚àÇx‚ÇÇ}\\ ‚ãØ \\ \\frac{‚àÇf}{‚àÇx‚Çô} ]$ A matrix ùêÄ contains elements that are functions of a vector ùê±.\n$ùêÄ(ùê±) = ùêü(ùê±) = (f_1(ùê±),\\ f_2(ùê±),\\ \u0026hellip;, f_m(ùê±)) = \\begin{bmatrix} f_1(ùê±) \\\\ f_2(ùê±) \\\\ ‚ãÆ \\\\ f_m(ùê±) \\end{bmatrix}$\nThe $\\frac{dùêÄ}{dùê±}$ is a matrix with the size of mxn:\n$$ \\frac{dùêÄ}{dùê±} = \\begin{bmatrix} \\frac{f_1}{x_1} \u0026amp; \\frac{f_1}{x_2} \u0026amp; ‚ãØ \u0026amp; \\frac{f_1}{x‚Çô} \\\\ \\frac{f_2}{x_1} \u0026amp; \\frac{f_2}{x_2} \u0026amp; ‚ãØ \u0026amp; \\frac{f_2}{x‚Çô} \\\\ ‚ãÆ \u0026amp; ‚ãÆ \u0026amp; ‚ãÆ \u0026amp; ‚ãÆ \\\\ \\frac{f_m}{x_1} \u0026amp; \\frac{f_m}{x_2} \u0026amp; ‚ãØ \u0026amp; \\frac{f_m}{x‚Çô} \\\\ \\end{bmatrix} $$\nMatrix derivative (2023-02-12)\nMatrix derivatie is in terms of the whole matrix, instead of each element. Whereas partial derivatives of a matrix\nGiven a matrix $[^{a\\ b}_{d\\ c}]$,the derivative of its inverse matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$ w.r.t. the original matrix is the \u0026ldquo;coefficient\u0026rdquo; in their relation:\n$$ \\underbrace{ \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} }_{\\text{Coefficient}} \\begin{bmatrix} a \u0026amp; b \\\\ d \u0026amp; c \\end{bmatrix} = \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} $$\nThis transformation can be understood as that the original matrix first times its inverse $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$ to become the identity matrix $[^{1\\ 0}_{0\\ 1}]$, which gets multiplied by $[^{\\ c\\ -b}_{-d\\ a}]$ to yield the inverse matrix.\nTherefore, the coefficient is:\n$$ \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} = \\frac{1}{ac-bd} \\begin{bmatrix} c¬≤ + bd \u0026amp; -bc-ab \\\\ -cd-ad \u0026amp; bd+a¬≤\\end{bmatrix} $$\nIn this case, is the optimizing objective the whole matrix $[^{a\\ b}_{d\\ c}]$, with its coefficient serving as the gradient?\nperplexity\nOn the other hand, the partial derivatives of the inverse matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$ with respect to each element a, b, c, d can be conceptualized as:\nhow does changes in the 4 \u0026ldquo;variables\u0026rdquo; $a,\\ b,\\ c,\\ d$ affect the matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$\n$$ \\begin{aligned} \\frac{ ‚àÇ\\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix}}{‚àÇa} \u0026amp;= \\begin{bmatrix} \\frac{‚àÇ}{‚àÇa} (\\frac{c}{ac-bd} ) \u0026amp; \\frac{‚àÇ}{‚àÇa} (\\frac{-b}{ac-bd}) \\\\ \\frac{‚àÇ}{‚àÇa} (\\frac{-d}{ac-bd}) \u0026amp; \\frac{‚àÇ}{‚àÇa} (\\frac{a}{ac-bd} ) \\\\ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{-c¬≤}{(ac-bd)¬≤} \u0026amp; \\frac{bc}{(ac-bd)¬≤} \\\\ \\frac{dc}{(ac-bd)¬≤} \u0026amp; \\frac{-bd}{(ac-bd)¬≤} \\\\ \\end{bmatrix} \\end{aligned} $$\nThe total change of the matrix magnitude caused by moving $a$ by one unit would be:\n$$\\frac{‚àÇ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{‚àÇa} = \\frac{-c¬≤ + bc + dc - bd}{(ac-bd)¬≤} $$\nParticularly, with this derivative, $a$ can be optimized via gradient descent. Similarly, the partial derivatives of the matrix w.r.t. $b,\\ c,\\ d$ are:\n$$ \\begin{aligned} \\frac{‚àÇ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{‚àÇb} \u0026amp;= \\frac{cd-ac-d¬≤+ad}{(ac-bd)¬≤} \\\\ \\frac{‚àÇ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{‚àÇc} \u0026amp;= \\frac{-bd+ba+da-a¬≤}{(ac-bd)¬≤} \\\\ \\frac{‚àÇ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{‚àÇd} \u0026amp;= \\frac{cb-b¬≤-ac+ab}{(ac-bd)¬≤} \\\\ \\end{aligned} $$\n(2024-02-13)\nMatrix Derivatives: What\u0026rsquo;s up with all those transposes ? - David Levin\nGradient: Matrix form -\u0026gt; indices form -\u0026gt; matrix form\nMatrix Calculus - Online\nX·µÄwX (2024-04-06)\nÊãÜÂàÜÊàêÔºöÂêëÈáèÂáΩÊï∞ + Â§öÂÖÉÂáΩÊï∞\nÁ©∫Èó¥ÁöÑÂü∫ÂèØ‰ª•ÊòØÂ§öÈ°πÂºèÂáΩÊï∞, ÂπÇÂáΩÊï∞, ÊâÄ‰ª•Á∫øÊÄßÊñπÁ®ãÂèØ‰ª•Ë°®Á§∫ÈùûÁ∫øÊÄßÂáΩÊï∞\n„ÄêÂæÆÁßØÂàÜÂíåÁ∫øÊÄß‰ª£Êï∞Á¢∞ÊíûÁöÑÊï∞Â≠¶ÁõõÂÆ¥ÔºöÊúÄÂ∞è‰∫å‰πòÊ≥ïÂÖ¨ÂºèÊé®ÂØºÔºÅ„Äë-Êôì‰πãËΩ¶È´òÂ±±ËÄÅÂ∏à - bilibili\n(2024-05-15)\n„ÄêÈ´òÁ≠âÊï∞Â≠¶Á¨îËÆ∞„ÄëÂ§öÂÖÉÂêëÈáèÂÄºÂáΩÊï∞ÁöÑÂØºÊï∞‰∏éÂæÆÂàÜ_- CSDN - seh_sjlj ","date":"2024-01-13T09:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/matrix_derivative/","title":"memo: Calc | Derivative of \"Matrix\""},{"content":"Code | arXiv | Vickie Ye\nSurfaced by NeRF\u0026amp;Beyond 12.5Êó•Êä•(CustomNeRFÔºåVideoRFÔºåÁΩëÊ†ºÂºïÂØºÁºñËæëÔºåSANeRF-HQÔºåGPS-GaussianÔºå‰∏§‰∏™GaussianAvatarÔºåSpalTAMÔºågsplatÔºâ - JasonÈô™‰Ω†ÁªÉÁªùÊäÄÁöÑÊñáÁ´† - Áü•‰πé Feature image from: Understanding the Covariance Matrix - Janakiev Projection Old Notes on 2023-12-05 NDC is the de-homogeneous clip coordinates and ranges [-1,1].\nDe-homogeneous means the z value has been divided.\nMapping the coordinates of point (x,y,z) in camera space to clip coordinates, i.e., a cube of [-1,1] can be decomposed to two operations: perspective projection and scaling ranges, and then compound them.\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nThe individual perspective projection:\n$$ \\begin{bmatrix} f‚Çì \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} $$\nAfter that, the plane coordinates are (u, v), wher $u = \\frac{f‚Çìx}{z}, v=\\frac{f_yy}{z}$.\nThen scaling the ranges:\nScale the range of u from [-w/2,w/2] to [-1,1] through a linear mapping: Œ± u + Œ≤\nŒ± and Œ≤ be solved based on two points.\n$$ \\begin{array}{cc} \\begin{cases} Œ± (-w/2) + Œ≤ = -1 \\\\ Œ± w/2 + Œ≤ = 1 \\end{cases} ‚áí \\begin{cases} Œ± = 2/w \\\\ Œ≤ = 0 \\end{cases} \\end{array} $$\nSimilarly, scale the range of v from [-h/2,h/2] to [-1,1] through a linear mapping: Œ± v + Œ≤\nThus, $Œ±= 2/h, Œ≤=0$\nSo far, the first 2 rows are determined:\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2f‚Çì/w \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2f_y/h \u0026amp; 0 \u0026amp; 0 \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nWhen scaling z, it has nothing to do with x and y. Thus, the 3rd row is 0 0 ‚ñ° ‚ñ°.\nBeacuse NDC is the de-homogeneous clip coordinates, which requires divide by z to become NDC. Therefore, the 4-th row is 0 0 1 0.\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2f‚Çì/w \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2f_y/h \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; A \u0026amp; B \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nWith denoting the two unknowns as A and B, the NDC of z dimension is: $\\frac{A z + B}{z}$.\nAccording to the range constraint [-1,1], the A and B can be solved from:\n$$ \\begin{array}{cc} \\begin{cases} \\frac{A n + B }{n} = -1 \\\\ \\frac{A f + B }{f} = 1 \\end{cases} ‚áí \\begin{cases} A = (f+n)/(f-n) \\\\ B = -2fn/(f-n) \\end{cases} \\end{array} $$\nFinally, the mapping from the camera coordinates of a point to corresponding clip coordinates is:\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2f‚Çì/w \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2f_y/h \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nNDC Mapping (2024-01-01)\nThe perspective division (a 3D pixel coordinates are divided by the 3rd dimension) should be performed as the final step in the transformation pipeline, as it\u0026rsquo;s a non-linear operation.\nGiven a 3D point (x,y,z)·µÄ located in the camera space, the perspective projection and scaling are carried out in sequence to obtain its clip coordinates (not NDC yet).\n$$ \\begin{array}{c} \\text{[Scaling Matrix] [Perspective Projection] [Camera space] = [Clip space]} \\\\ \\\\ \\begin{bmatrix} ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\end{bmatrix} \\begin{bmatrix} f‚Çì \u0026amp; 0 \u0026amp; c‚Çì \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ w_c \\end{bmatrix} \\end{array} $$\nThe scaling matrix is built with the goal of mapping the view frustum to a [-1,1] NDC-space cube encompassing only valid points, whose clip coordinates satisfy: $-w_c \u0026lt; x_c,y_c,z_c \u0026lt; w_c$. Specifically, the projected coordinates are scaled and then perform perspective division to become the NDC.\nPerspective projection:\n$$ \\begin{bmatrix} f‚Çì \u0026amp; 0 \u0026amp; c‚Çì \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f‚Çìx + c‚Çì z \\\\ f_y y + c_y z \\\\ z \\\\ 1 \\end{bmatrix} $$\nScaling projected coordinates u and v to [-1, 1]\nTo scale u, the 1st row of the scaling matrix is A 0 B 0.\nPerform scaling first, followed by perspective division, to obtain the x in NDC space: $\\frac{A (f‚Çì x + c‚Çì z) + Bz}{z} = A (\\frac{f‚Çì x}{z} + c‚Çì) + B ‚àà [-1,1]$\nSince $(\\frac{f‚Çì x}{z} + c‚Çì) = u ‚àà [0,W]$\n$$ \\begin{cases} A0 + B = -1 \\\\ AW + B = 1 \\end{cases} ‚áí \\begin{cases} A = \\frac{2}{W} \\\\ B = -1 \\end{cases} $$\nTherefore, the first 2 rows are:\n$$ \\begin{bmatrix} 2/W \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 2/H \u0026amp; -1 \u0026amp; 0 \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\\\ ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \u0026amp; ‚ñ° \\end{bmatrix} \\begin{bmatrix} f‚Çìx + c‚Çì z \\\\ f_y y + c_y z \\\\ z \\\\ 1 \\end{bmatrix} $$\nScaling frustum $z ‚àà [n, f]$ to [-1,1]\nz is independent to x and y, so the 3rd row only has 2 unknows: 0 0 A B.\nScaling first, then perspective division, thereby the z in NDC space is: $\\frac{A z + B}{z} ‚àà [-1, 1]$\nSubstituting z = n and f:\n$$ \\begin{cases} \\frac{A n + B}{n} = -1 \\\\ \\frac{A f + B}{f} = 1 \\end{cases} ‚áí \\begin{cases} A = \\frac{f+n}{f-n} \\\\ B = \\frac{-2fn}{f-n} \\end{cases} $$\nFinally, since the denominators are z (i.e., the w of a point\u0026rsquo;s clip coordinates is z), the 4th row is 0 0 1 0:\n$$ \\begin{bmatrix} 2/W \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 2/H \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} f‚Çìx + c‚Çì z \\\\ f_y y + c_y z \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2 (f‚Çìx + c‚Çì z)}{W} - z \\\\ \\frac{2 (f_y y + c_y z) }{H} - z \\\\ \\frac{f+n}{f-n} z - \\frac{2fn}{f-n} \\\\ z \\end{bmatrix} $$\nThe result coordinates are in the clip space, which will become ND coordinates after perspective division, and only points within the cube of [-1,1] in NDC space will be rendered.\nIn summary, the Projection Matrix (GL_PROJECTION) transforming camera-space (x,y,z)·µÄ to clip coordinates is: $$ P = \\begin{bmatrix} \\frac{2}{W} \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2}{H} \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} f‚Çì \u0026amp; 0 \u0026amp; c‚Çì \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2f‚Çì}{W} \u0026amp; 0 \u0026amp; (\\frac{2c‚Çì}{W}) -1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2f_y}{H} \u0026amp; (\\frac{2c_y}{H}) -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $$\nWhen cx and cy are W/2 and H/2, it\u0026rsquo;s in the form of Ye\u0026rsquo;s article.\nWhen cx and cy are r and t, it\u0026rsquo;s in the form of songho.\n(2024-01-02)\nProject mean vector ùõç A mean vector ùõç in world space is changed to pixel space as follows:\nW C o o ùõç r o l r d d V t ùêì i r e a w n s f C C a o m o ùê≠ e r r d a P + e r S ùêè s c p a l p i r n o g j C C l o i o ùê≠ p r ' d C P ‚Üë l e d i œï r i p ‚Çñ s v ( p i ùê≠ e s ' c i ‚Üí ) t o i n v e N C ( D o C C o u r b d e S ) c a l e P C i o ùõç x o ' e r l d ùê≠ refers to coordinates of ùõç in the camera space as: $ùê≠ = \\begin{bmatrix} ùêë_{w2c} \u0026amp; ùê≠_{w2c} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} \\bm Œº \\\\ 1 \\end{bmatrix}$\nWhile the translation vector is denoted as $ùê≠_{w2c}$.\nAnd the extrinsics is represented as $ùêì_{w2c}$.\nThe clip coordinates of ùõç is $ùê≠\u0026rsquo; = ùêèùê≠$\nThe nonlinear perspective division is approximated by the projective transformation $œï‚Çñ(ùê≠\u0026rsquo;)$.\nPoints\u0026rsquo; coordinates conversion from world space to clip space:\n$$ \\begin{aligned} ùê≠\u0026rsquo; \u0026amp;= ùêè‚ãÖ ùêì_{w2c}‚ãÖ [^{\\bm \\mu}_1] \\\\ \u0026amp;=\\begin{bmatrix} \\frac{2f‚Çì}{W} \u0026amp; 0 \u0026amp; (2c‚Çì/W) -1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2f_y}{H} \u0026amp; (2c_y/H) -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} R_{w2c} \u0026amp; t_{w2c} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} Œº‚Çì \\\\ Œº_y \\\\ Œº_z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} t‚Çì\u0026rsquo; \\\\ t_y\u0026rsquo; \\\\ t_z\u0026rsquo; \\\\ t_w\u0026rsquo; \\end{bmatrix} \\end{aligned} $$\n$t_w\u0026rsquo;$ is the point\u0026rsquo;s camera-space depth $t_z$, which is \u0026gt; 0. Frustum clipping (clip-space culling) filters points that won\u0026rsquo;t appear in the frustum based on clip coordinates before perspective division:\nc C - n c e a 1 e a n m ‚ãÖ m t - a e s r p a r c f e a r i f c n r o u o s r 1 d s ‚ûî N C C D l a N i m D c p e o r c o c a o r o o d o c r : D r P o d i d r o i ( v : o r n x j d a ‚Çô b ( : t , y x + e _ ( o s y w c s x z u ‚Çô _ , c , ‚Çô t , c a y \u0026gt; y l , ‚ùå 1 o z _ e z f ‚Çô c , , , 1 f ) r 1 z u ) _ ‚àà s c t ‚àà , [ u 0 m [ w , 0 _ , c ‚àû ) ] ‚àû ] ‚àà ‚Üí [ [ 0 - ‚Üë ‚Üë , 1 , D C ‚àû 1 i l ] ] v i p w ‚ùó View Frustum Clipping - UofTexas - Lec9\nView frustum clipping aims to reduce computation. Essentially, it filters points by comparing the clip coordinate $w_c$ with $x_c, y_c, z_c$ for each point.\nIn the previous derivation, clip coordinates are the scaled projection coordinates: (A*ProjCoord+B). NDC space is defined by $w$, as the NDC cube is constrained by bounds where the clip coordinates divided by ùë§ equals 1 (ùë§ is the benchmark), such as $\\frac{A (f_x x+c_x z) + Bz}{w} = 1$\nIn other words, the NDC planes wrap around points whose clip coordinates $x_c,y_c,z_c$ less than or equal to $w_c$. In addition, $w_c$ must be the camera-space depth z for the final perspective division. Thus, if the clip coordinate of a point is bigger than w or less than $-w$, the \u0026ldquo;quotient\u0026rdquo; will be outside of [-1,1], i.e., the point is not located in the camera-space view frustum or the NDC-space cube.\nFrustum clipping retains points that satisfy: $-w_c \\leq x_c, y_c, z_c \\leq w_c$. Conversely, those points whose w (equals camera-space depth) is smaller than x,y,z will be filtered out.\nAlthough ND Coordinates are also able to identify points for clipping, to reduce the number of perspective divisions (executed at final), clipping is performed in the clip space for efficiency.\nOn the other hand, since the ND Coordinates of the Left, Right, Bottom, Top, Near, and Far frustum planes are -1 and 1, the clip coordinates of points located within the Frustum satisfy the relation: $-1 \u0026lt; \\frac{x_c}{w_c}, \\frac{y_c}{w_c}, \\frac{z_c}{w_c} \u0026lt;1$\nConsequently, only points in the frustum (i.e., NDC-space cube: $-1 \u0026lt; x‚Çô, y‚Çô, z‚Çô \u0026lt; 1$) are survived.\n(2024-01-03) It is the view frustum clipping that makes ND coordinates can be regarded as a cube space, because out-of-the-cube points have been disregared.\nPerforming perspective division on the clip coordinates resulting in NDC, whose all components ranges in [-1,1]:\n$$ NDC = \\begin{bmatrix} t‚Çì\u0026rsquo;/t_w\u0026rsquo; \\\\ t_y\u0026rsquo;/t_w\u0026rsquo; \\\\ t_z\u0026rsquo;/t_w\u0026rsquo; \\\\ 1 \\end{bmatrix} ‚àà [-1,1] $$\n(2024-02-08) NDC is 3D, including z‚Çô coord besides the 2D pixel coords. Scaling NDC to obtain pixel coordinates ùõç\u0026rsquo; (viewport transformation): songho\n$$ [-1, 1] \\overset{√óW}{‚Üí} [-W, W] \\overset{+1}{‚Üí} [-W+1,W+1] \\overset{√∑2}{‚Üí} [\\frac{-W+1}{2}, \\frac{W+1}{2}] \\\\ \\overset{+c_x}{‚Üí} [0.5, W+0.5] (\\text{ if $c_x =\\frac{W}{2}$}) $$\nTherefore, the final pixel coordinate ùõç\u0026rsquo; of a world-space mean vector ùõç is: $$ \\bm \\mu\u0026rsquo; = \\begin{bmatrix} (W‚ãÖt‚Çì\u0026rsquo;/t_w\u0026rsquo; + 1) /2 + c_x \\\\ (H‚ãÖt_y\u0026rsquo;/t_w\u0026rsquo; + 1) / 2 + c_y \\end{bmatrix} $$\nProject Covariance (2024-01-03)\nBecause the perspective projection is not a linear operation due to division, the 2D projection of a 3D Gaussian is not a 2D Gaussian:\nsnowball splat diffuse Is it a 2D Gaussian? Conic sections EWA Splatting doesn\u0026rsquo;t scale the projected coordinates to the [-1,1] NDC-space cube. It directly transforms points from camera space onto screen (or viewing-ray space) by dividing z. The result coordinates range in [0,W] and [0,H].\nProjective transformation œï(ùê≠) in EWA Splatting: Converting an arbitrary point\u0026rsquo;s camera-space coordinates $ùê≠=(t‚Çì,t_y,t_z)·µÄ$ to the coordinates in 3D ray space (pixel coordinates x0,x1 + \u0026ldquo;new depth\u0026rdquo; x2) has 2 steps:\nPixel coords = perspective projection + perspective division. The new depth is set to the L2 norm of the point\u0026rsquo;s camera-space coordinates. $$ œï(ùê≠) = \\begin{bmatrix} \\frac{f‚Çì t‚Çì}{t_z} + c‚Çì \\\\ \\frac{f_y t_y}{t_z} + c_y \\\\ \\sqrt{t_x^2 + t_y^2 + t_z^2} \\end{bmatrix} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} $$\nBecause EWA Splatting doesn\u0026rsquo;t consider frustum clipping, the approximation is based on the camera coordinates ùê≠.\n(2024-02-16) The clip coordinates shouldn\u0026rsquo;t be used in EWA splatting because the Gaussian center is used in the Jacobian that approximates the perspective projection, where the camera-space coordinates are supposed to be used.\nWhereas, 3DGS (or gsplat) requires to determine whether the point (Gaussian center) is in the frustrum to be rendered, the clip coordinates are utilized for frustum clipping. Therefore, the world coordinates ùê± are involved into 2 procedures: the covertion for covariance matrix from world space to ray space (JWùö∫W·µÄJ·µÄ), and the projection for Gaussian center from world space onto the screen.\nConsequently, the derivative of Loss w.r.t. the world coordinates ($\\frac{‚àÇL}{‚àÇùê±}$) has 2 portions.\nHowever, gsplat uses the clip space to filter points outside the camera frustum. So, after perspective projection and scaling for NDC with matrix ùêè, points are transferred into clip space as ùê≠\u0026rsquo; for clipping.\nAfter clipping, the nonlinear perspective division and x‚ÇÇ reassignment in œï(ùê≠), are approximated with an affine transformation based on the clip coordinates ùê≠\u0026rsquo;. Therefore, the projective transformation $œï(ùê≠)$ that maps camera space to ray space becomes a mapping from clip space to the ray space $œï(ùê≠\u0026rsquo;)$.\n$$ \\begin{aligned} ùê≠ \u0026amp;‚Üí ùê≠\u0026rsquo;=ùêèùê≠ = \\begin{bmatrix} \\frac{2 (f‚Çì t_x + c‚Çì t_z)}{W} - t_z \\\\ \\frac{2 (f_y t_y + c_y t_z) }{H} - t_z \\\\ \\frac{f+n}{f-n} z - \\frac{2fn}{f-n} \\\\ t_z \\end{bmatrix} = \\begin{bmatrix}ùê≠_x\u0026rsquo; \\\\ t_y\u0026rsquo; \\\\ t_z\u0026rsquo; \\\\ t_z\\end{bmatrix} \\\\ \u0026amp;‚Üí œï(ùê≠\u0026rsquo;) = \\begin{bmatrix} t‚Çì\u0026rsquo;/t_z\u0026rsquo; \\\\ t_y\u0026rsquo;/t_z\u0026rsquo; \\\\ ‚Äñùê≠\u0026rsquo;‚Äñ‚ÇÇ \\end{bmatrix} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} \\end{aligned} $$\nThe affine approximation is the first 2 terms of $œï(ùê≠\u0026rsquo;)$\u0026rsquo;s Taylor expansion evaluated at a Gaussian\u0026rsquo;s mean vector $ùê≠‚Çñ\u0026rsquo; = (t_{k,x}\u0026rsquo;, t_{k,y}\u0026rsquo;, t_{k,z}\u0026rsquo;)·µÄ$ in the clip space:\n$$ \\begin{aligned} \u0026amp; œï(ùê≠\u0026rsquo;) ‚âà œï‚Çñ(ùê≠\u0026rsquo;) = œï(ùê≠‚Çñ\u0026rsquo;) + ùêâ_{ùê≠‚Çñ\u0026rsquo;} ‚ãÖ (ùê≠\u0026rsquo; - ùê≠‚Çñ\u0026rsquo;) \\\\ \u0026amp; = \\begin{bmatrix} t_{k,x}\u0026rsquo;/t_{k,z}\u0026rsquo; \\\\ t_{k,y}\u0026rsquo;/t_{k,z}\u0026rsquo; \\\\ ‚Äñùê≠‚Çñ\u0026rsquo;‚Äñ¬≤\\end{bmatrix} + \\begin{bmatrix} 1/t_{k,z}\u0026rsquo; \u0026amp; 0 \u0026amp; -t_{k,x}/{t_{k,z}\u0026rsquo;}^2 \\\\ 0 \u0026amp; 1/t_{k,z}\u0026rsquo; \u0026amp; -t_{k,y}/{t_{k,z}\u0026rsquo;}^2 \\\\ t_{k,x}\u0026rsquo;/‚Äñùê≠‚Çñ\u0026rsquo;‚Äñ‚ÇÇ \u0026amp; t_{k,y}\u0026rsquo;/‚Äñùê≠‚Çñ\u0026rsquo;‚Äñ‚ÇÇ \u0026amp; t_{k,z}\u0026rsquo;/‚Äñùê≠‚Çñ\u0026rsquo;‚Äñ‚ÇÇ \\end{bmatrix} (ùê≠\u0026rsquo; - ùê≠‚Çñ\u0026rsquo;) \\end{aligned} $$\nIf using the camera-space coordinates ùê≠ to express the projective transformation as œï(ùê≠), focal lengths will be exposed:\n$$ \\begin{aligned} œï(ùê≠) ‚âà œï‚Çñ(ùê≠) \u0026amp;= œï(ùê≠‚Çñ)+ ùêâ_{ùê≠‚Çñ} ‚ãÖ (ùê≠ - ùê≠‚Çñ) \\\\ \u0026amp;=\\begin{bmatrix} 2(f‚Çì t_{k,x}/t_{k,z} + c‚Çì)/W -1 \\\\ 2(f_y t_{k,y}/t_{k,z} + c_y)/H -1 \\\\ ‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \\end{bmatrix} + ùêâ_{ùê≠‚Çñ} ‚ãÖ (ùê≠ - ùê≠‚Çñ) \\end{aligned} $$\nIf $c_x=W/2,\\ c_y=H/2$, then $œï‚Çñ(ùê≠) = \\begin{bmatrix} \\frac{2f_x t_{k,x}}{W‚ãÖ t_{k,z}} \\\\ \\frac{2f_y t_{k,y}}{H‚ãÖ t_{k,z}} \\\\ ‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \\end{bmatrix}$\nand the Jacobian $ùêâ_{ùê≠‚Çñ}$ will be:\n$$ ùêâ_{ùê≠‚Çñ} = \\begin{bmatrix} (2/W)‚ãÖf‚Çì/t_{k,z} \u0026amp; 0 \u0026amp; (2/W)‚ãÖ-f‚Çì t_{k,x} / {t_{k,z}}^2 \\\\ 0 \u0026amp; (2/H)‚ãÖf_y/t_{k,z} \u0026amp; (2/H)‚ãÖ-f_y t_{k,y} / {t_{k,z}}^2 \\\\ t_{k,x}/‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \u0026amp; t_{k,y}/‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \u0026amp; t_{k,z}/‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \\end{bmatrix} $$\nIf the camera-film coords x‚àà[0,W] and y‚àà[0,H] are not scaled to [-1,1], the scaling factors 2/W and 2/H won\u0026rsquo;t exist.\nThus, the Jacobian evaluated at center ùê≠‚Çñ in the camera space is:\n$$ ùêâ_{ùê≠‚Çñ} = \\begin{bmatrix} f‚Çì/t_{k,z} \u0026amp; 0 \u0026amp; -f‚Çì t_{k,x} / {t_{k,z}}^2 \\\\ 0 \u0026amp; f_y/t_{k,z} \u0026amp; -f_y t_{k,y} / {t_{k,z}}^2 \\\\ t_{k,x}/‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \u0026amp; t_{k,y}/‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \u0026amp; t_{k,z}/‚Äñùê≠‚Çñ‚Äñ‚ÇÇ \\end{bmatrix} $$\nBy expressing the projective transformation œï‚Çñ() with the camera coordinates ùê≠, the relation between ray-space coordinates and camera-space coordinates is constructed. Thereby, the derivative of Gaussian center œï‚Çñ(ùê≠‚Çñ) in the ray space is derived from camera-space coordinates ùê≠‚Çñ, with the clip coordinates ùê≠‚Çñ\u0026rsquo; skipped.\nFor the case of 2D projection, only x and y dimensions of the covariance matrix need consideration, with the 3rd row and column are omitted. Because of the affine approximation, the projective transformation œï(ùê≠\u0026rsquo;) becomes a linear operation. Thereby, a 3D Gaussian after perspective division is a 2D Gaussian.\nFiguratively, points surrounding the 3D Gaussian center in clip space will fall into an ellipse on the 2D screen. In the 3DGS code, the 2D ellipse is further simplified as a circle to count the overlapped tiles. The covariance matrix ùö∫‚Çñ\u0026rsquo; of the 2D Gaussian in the pixel space corresponding to the 3D Gaussian (ùö∫‚Çñ) in the world space can be derived based on properties of Gaussians as:\n$$\\bm Œ£‚Çñ\u0026rsquo; = ùêâ‚Çñ ùêë_{w2c} \\bm Œ£‚Çñ ùêë_{w2c}·µÄ ùêâ‚Çñ·µÄ$$\nBecause covariance matrix ùö∫\u0026rsquo; is symmetric, it can be decomposed to a stretching vector (diagonal matrix) and a rotation matrix by SVD, analogous to describing the configurations of an ellipsoid ^3DGS. (Essentially, the covariance matrix depicts a data distribution.)\nThe rotation matrix is converted to a quaternion during optimization.\nIn summary: A 3D Gaussian (ùõç‚Çñ,ùö∫‚Çñ) in world (object) space is transformed into 3D ray space (or the screen with the 3rd dim omitted), resulting in:\nMean vector: $\\bm Œº‚Çñ\u0026rsquo; = œï(ùê≠‚Çñ\u0026rsquo;)$, where $ùê≠‚Çñ\u0026rsquo; = ùêè‚ãÖ [^{ùêë_{w2c} \\ ùê≠_{w2c}}_{0 \\quad\\ 1}] ‚ãÖ[^{\\bm Œº‚Çñ}_1]$ is clip coordinates.\nCovariance matrix: $\\bm Œ£‚Çñ\u0026rsquo; = ùêâ‚Çñ‚ãÖ ùêë_{w2c}‚ãÖ \\bm Œ£‚Çñ‚ãÖ ùêë_{w2c}·µÄ‚ãÖ ùêâ‚Çñ·µÄ$\nA point ùê≠\u0026rsquo; in clip space within the Gaussian distribution is projected to ray space: $œï‚Çñ(ùê≠\u0026rsquo;) = œï(ùê≠‚Çñ\u0026rsquo;) + ùêâ‚Çñ‚ãÖ(ùê≠\u0026rsquo; - ùê≠‚Çñ\u0026rsquo;)$. The discrepancy between the approximated and the real projected locations is $œï‚Çñ(ùê≠\u0026rsquo;)-œï(ùê≠\u0026rsquo;)$.\nS c r e ‚¨Ø ùõç e ùê± ' n , ùö∫ ' C l i ùõç p ùê≠ ·∂ú ' , s ùö∫ p ·∂ú a c e O s b p j a e c ‚¨Ø ùêó ùõç c e , t ùö∫ Rasterizing Sorting Kernels (2024-01-09)\nSort Gaussians within each 16x16 tile based on depth\nEvery pixel has a perpendicular dot line, and the discs intersected with the line are visible to the pixel.\nThe depths of those discs are $x‚ÇÇ = ‚Äñùê≠\u0026rsquo;‚Äñ‚ÇÇ = \\sqrt{ {t‚ÇÄ\u0026rsquo;}¬≤ + {t‚ÇÅ\u0026rsquo;}¬≤ + {t‚ÇÇ\u0026rsquo;}¬≤}$, L2 norm of the clip coordinates.\nThe disc closer to the screen is more prominent.\nDifferent images are obtained given different viewing rays, as the opacities of the discs change in tandem with viewing rays. (Specifically, the opacity of a disc is an integral for the Gaussian in the 3D ray space along a viewing ray.)\nIn contrast, volume rendering method changes point-wise colors on different viewing rays.\n(2024-01-22) In splatting, image formation still relies on alpha compositing. Distinct from NeRF where a pixel-ray originates from the camera optical center, in splatting, a pixel emits a perpendicular ray from the screen. And it is the incoming viewing rays determine varying alpha (opacity) of the filters on the pixel-ray path. Such that the screen displays diverse colors with various viewing rays.\n(2024-04-20)\nI think the \u0026ldquo;screen\u0026rdquo; is indeed the camera film. The \u0026ldquo;viewing ray\u0026rdquo; doesn\u0026rsquo;t hit the screen\u0026quot;. The viewing ray is just required to pass through 3D Gaussians to calculate each Gaussian\u0026rsquo;s opacity by integrating the 3D Gaussian over the intersecting segment.\nRendering a pixel in the splatting method also emitting a ray from a pixel and compositing discs on the ray. The difference is that the opacity has been precomputed by splatting procedure: integrating the viewing ray.\nThus, it is the screen, i.e., pixels that shoot lines.\nIn the implementation of 3DGS, the splatting process is omitted, since the opacity of each Gaussian is accquired by optimizing it iteratively.\nSplatting is just one of the ways to get the opacity. As long as the opacities are obtained, any rendering method can be applied to form an image, e.g., rasterization, ray marching/tracing.\nAlpha Blending (2024-01-05)\nEWA splatting equation for N kernels existing in the space:\n$$ \\underset{\\substack{‚Üë\\\\ \\text{Pixel}\\\\ \\text{color}}}{C} = ‚àë_{k‚ààN} \\underset{\\substack{‚Üë\\\\ \\text{Kernel}\\\\ \\text{weight}}}{w‚Çñ}‚ãÖ \\underset{\\substack{‚Üë\\\\ \\text{Kernel} \\\\ \\text{color}}}{c‚Çñ}‚ãÖ \\underset{\\substack{‚Üë\\\\ \\text{Accumulated} \\\\ \\text{transmittance}}}{o‚Çñ}‚ãÖ (\\underset{\\substack{‚Üë\\\\ \\text{Kernel}\\\\ \\text{opacity}}}{q‚Çñ}‚äó \\underset{\\substack{‚Üë\\\\ \\text{Loss-pass}\\\\ \\text{filter}}}{h}) (\\underset{\\substack{‚Üë\\\\ \\text{2D} \\\\ \\text{coords}}}{ùê±}) $$\nIn 3DGS, each 3D Gassian in the object space has 4 learnable parameters:\nColor (c‚Çñ): SH for \u0026ldquo;directional appearance component of the radiance field\u0026rdquo;\nOpacity ($q‚Çñ‚äó h$). It results in accu. transmittance o‚Çñ as $‚àè_{m‚â§k}(1-\\text{opacity}‚Çò)$.\nPosition ùõç: determines the kernel\u0026rsquo;s weight w‚Çñ, as w‚Çñ is an evaluation of the projected kernel (2D Gaussian distribution) at the pixel.\nCovariance matrix: the stretching matrix and rotaion matrix (quaternion) jointly determine w‚Çñ as well.\nThe splatting equation can be reformulated as alpha compositing used in 3DGS:\n$$C = ‚àë_{n‚â§N} T‚Çô‚ãÖŒ±‚Çô‚ãÖc‚Çô, \\text{ where } T‚Çô = ‚àè_{m‚â§n}(1-Œ±‚Çò)$$\nAlpha can be expressed with sigma, which is an exponent of e, akin to NeRF.\n$$Œ±‚Çô = o‚Çô ‚ãÖ exp(-œÉ‚Çô), \\text{where } œÉ‚Çô = ¬Ω\\bm Œî‚Çô·µÄ \\bm Œ£\u0026rsquo;‚Åª¬π \\bm Œî‚Çô$$\no‚Çô is a kernel\u0026rsquo;s opacity, i.e., the above q‚Çñ‚äó h. And the negative exponential term is a scalar scaling factor. œÉ‚Çô is Mahalanobis distance.\n(2024-02-16)\nGaussian\u0026rsquo;s opacity o‚Çô is fixed after splatting with a specific viewing ray, so during alpha compositing, the variation in alpha among different Gaussian results from the different positions of a target rendering pixel relative to various Gaussians.\nWhen performing alpha compositing, the alpha of a Gaussian is the Gaussian\u0026rsquo;s opacity scaled by the \u0026ldquo;probability\u0026rdquo; for the position of the rendering pixel in the Gaussian distribution.\nHowever, the alpha value in NeRF is $Œ±·µ¢ = 1- exp(-œÉ·µ¢Œ¥·µ¢)$ and serves as the opacity in $‚àë·µ¢‚Çå‚ÇÅ·¥∫ T·µ¢ Œ±·µ¢ c·µ¢$. Alpha is a converted point opacity ranging in [0,1].\nIn other words, alpha $Œ±‚Çô$ equals a kernel\u0026rsquo;s opacity o‚Çô scaled by a weight G‚Çô (the above w‚Çñ), i.e., the evaluation of 2D Gaussian G‚Çô at the viewing pixel:\n$$Œ±‚Çô = o‚Çô ‚ãÖ G‚Çô, \\text{where } G‚Çô = e^{-\\frac{\\bm Œî‚Çô·µÄ \\bm Œî‚Çô}{2\\bm Œ£\u0026rsquo;}}$$\no‚Çô is the opacity (the footprint q‚Çô) of the n-th 3D Gaussian. q‚Çô is an integral of the Gaussian in the 3D ray space along the viewing ray: $q‚Çô(ùê±) = ‚à´r‚Çô\u0026rsquo;(ùê±,x‚ÇÇ)dx‚ÇÇ$.\no‚Çô will get optimized directly via gradient descent during training.\nG‚Çô is a 2D Gaussian with the normalization factor omitted. Its mean and covariance matrix will get optimized.\nŒî‚Çô is the displacement of a pixel center from the 2D Gaussian\u0026rsquo;s mean.\nInitially, the opacity of an arbitrary 3D location in the object space is considered as an expectation of the contributions from all 3D Gaussians on that location.\nAfter ‚ù∂ substituting the perspectively projected (\u0026ldquo;squashed\u0026rdquo;) kernel within the 3D ray space into the rendering equation, ‚ù∑ switching the sequence of integral and expectation, ‚ù∏ and applying simplifying assumptions, the opacity becomes a 1-D (depth) integral along the viewing ray for each kernel in the 3D ray space.\nIn summary, the changes of opacity before and after perspective projection:\nAspect Original form Post-projection Venue Object space 3D ray space, or screen Intuition Opacity combination Discs stack Scope Location-wise Ellipsoid-wise Operation Expectation Integral Formula $f_c(ùêÆ) = ‚àë_{k‚â§N} w‚Çñ r‚Çñ(ùêÆ)$ Footprint $q‚Çñ(ùê±) = ‚à´_{x‚ÇÇ=0}^L r‚Çñ\u0026rsquo;(ùê±,x‚ÇÇ) dx‚ÇÇ$ Basis Gauss. mixture Scene locality Locality: 3D positions are grouped into different ellipsoids. (2024-01-06) A 3D Gaussian (datapoint) in the camera space (or clip space) is \u0026ldquo;perspectively\u0026rdquo; projected (thrown) onto the screen (or the ray space, as its x2 is independtly assigned beside the screen coordinates x0,x1), and results in a 2D Gaussian (with applying the Taylor expansion to approximate the nonlinear perspective effects).\nThe viewing ray in camera space will be projected into the 3D ray space remaining a straight line segment (due to the linear approximation), and then the 3D line is projected onto the screen orthogonally.\nOrthogonal projection is because the 3D Gaussians have already been projected onto the screen (the location has been determined as x,y divided by z and covariance matrix ùö∫\u0026rsquo;= ùêâùêñ ùö∫ùêñ·µÄ ùêâ·µÄ) yielding 2D Gaussians, so each pixel is only derived from those projected kernels (2D Gaussians) that overlaps with it (\u0026ldquo;Overlapping\u0026rdquo; refer to 3DGS.), like a stack of filters in alpha compositing. That implies the alpha compositing is performed in the screen space, or the ray sapce, as the ray space is equivalent with screen (EWA paper: \u0026ldquo;the transformation from volume-data to ray space is equivalent to perspective projection.\u0026rdquo;).\nA p i ‚ñ† x e ‚Üí l = = d k S i e u s ‚ñ† ‚Üë ‚¨≠ n m c l 1 1 o f ‚úö 2 d k D i e s ‚ñ† ‚Üë ‚¨Ø n G c l a 2 2 u s s ‚úö ‚ãØ i a n d k s i e s ‚ñ† ‚Üë ‚¨≠ n O c l v N N e r l a o p ( m ( p 2 i 3 i D t D n ‚Üë g S \" R c d a w r e y i e p t e t s h n h p ) \" a i c t x e ‚ÇÇ ) With the \u0026ldquo;orthogonal correspondence\u0026rdquo; between the ray space and the screen, the ray integral (footprint function, or kernel\u0026rsquo;s opacity) in the 3D ray space becomes (??Not sure) an integral on the 2D screen plane, i.e., an integral of a 2D Gaussian.\nAnd the ray in the 3D ray space corresponds to a line on the screen, as rays in the 3D ray space are parallel (i.e., orthogonal projection). Thus, the opacity is an\nAlpha of an arbitrary point in screen (or 3D ray space) is a 2D-Gaussian mixture over all kernels. (Not the screen, object space is opacities combination, whereas the screen space is filter stacking.)\nThe alpha of a 3D location is calculated in each 3D Gaussian based on the distance to the center. And the final alpha on the location is the expectation of all the evaluations. (No location-wise alpha was calculated.)\nThe color of a pixel on the screen is a 2D-Gaussian mixture:\nG a i 2 u a D s n s s S - c r e e n ‚à´ D V e i V p d e o t i w l h f i . \" u f n ~ m h e g e e r r e r d e n a a t y t i a s v 3 e D h ‚Äñ i ‚Ä¢ a ùê≠ w ( s ' i V ‚Äñ n i b ‚ÇÇ g e e w e f r i n o a n r y g ~ t s ) . \" h c . r a R o l C a w c o y n u v l s o a m p . n t a a \" ‚Ä¢ \" t i t c . o n r e g i t x h o e p i P p a s e r s c r o c i ùö∫ s j r t ' p e y ‚ÇÉ e e ‚Çì c n g ‚ÇÉ C S . i . a p ‚¨≠ v m a e e c n r e a ‚¨Ø The alpha compositing process for a pixel is illustrated below:\nOpacities (o‚ÇÅ,o‚ÇÇ,o‚ÇÉ) of different kernels are various-length integral along the viewing ray in the 3D ray space.\nNot sure whether the integral in 3D ray space equal the integral on screen. Weight (w‚ÇÅ,w‚ÇÇ,w‚ÇÉ) of a kernel\u0026rsquo;s opacity is its evaluation at the pixel.\nAlpha (Œ±‚ÇÅ,Œ±‚ÇÇ,Œ±‚ÇÉ) of a kernel equals its opacity multiplied with its weight.\nAccumulated transmittance (T‚ÇÅ,T‚ÇÇ,T‚ÇÉ) equals the product of previously passed kernels\u0026rsquo; transmittance.\nPixel color is the sum of each kernel\u0026rsquo;s color scaled by alpha.\nThere is no volume rendering as there is no sampling points on the viewing ray. Pixel is a summation of visible 2D discs (referring to 3DGS). Only alphas of the explicitly existent discs require to be computed, unlike volume rendering where every sampling location need to compute alpha.\nGradient wrt Composite (2023-01-07)\nS P k 3 p r e D l o r a j n r t e e a t ‚ãØ c l y i t s n ‚ãØ e s g ‚ãØ d i p : ‚ãØ ‚ãØ n a ‚ãØ c U ‚ãØ ‚ãØ e s c ‚ãØ ‚ãØ e o ‚ãØ m ‚ãØ ‚ãØ ‚ãØ V p ‚ãØ i u ‚ãØ ‚ãØ e t w e i n o g p a r c a i y t i t e o s F B d ùõç o a i ' c r c s ‚ñ† ‚ÇÅ ‚ÇÅ w k c , , a 1 ùö∫ o r ' ‚ÇÅ d ‚úö ‚ÇÅ : d r i ùõç e s ‚ñ† ' c n F c ‚ÇÇ ‚ÇÇ d r 2 , , e o ùö∫ o r n ' ‚ÇÇ t ‚úö ‚ãØ ‚ÇÇ p i ( x f d e i i ùõç l r s ‚ñ† ' c s c ‚Çô ‚Çô c t N , , o ùö∫ o l b = ' ‚Çô o e p ‚Çô r i ‚ñ† s x e e n ) The dot lines in the left figure represent orthogonal correspondence, not projection. A pixel can only see 2D Gaussians located on its perpendicular line. Those visible 2D Gaussians to a pixel are sorted based on depth, and then their colors are composited from near to far with multiplying with their opacities that computed as an integral along the viewing ray. A synthesized pixel is a weighted sum of the related (overlapping) kernels\u0026rsquo; color in the whole space:\n$$C_{pred} = ‚àë_{n‚â§N} T‚Çô‚ãÖŒ±‚Çô‚ãÖc‚Çô, \\quad \\text{where } Œ±‚Çô=o‚Çô‚ãÖe^{-\\frac{\\bm Œî‚Çô·µÄ \\bm Œî‚Çô}{2\\bm Œ£\u0026rsquo;}}$$\nLoss:\n$$L = ‚ÄñC_{targ} - C_{pred}‚Äñ‚ÇÇ$$\nThe paper used Frobenius product to analyze. A Frobenius inner product is like a linear layer:\n$$‚ü®ùêó,ùêò‚ü© = \\operatorname{vec}(ùêó)·µÄ \\operatorname{vec}(ùêò)$$\n1 2 3 fc = nn.Linear(3, 10) # fc.weight is (10,3) x = torch.range(2,3) fc(x) # X‚ãÖW·µÄ = (2,3)‚ãÖ(3,10) = (2,10) is ‚ü®X·µÄ,W·µÄ‚ü© Chain rule:\nx A ‚Çò ‚Çì ‚Çö ( x ) Y ‚Çö ‚Çì ‚Çô ( x ) X ‚Çò ‚Çì ‚Çô f $$\\begin{aligned} \\frac{‚àÇf}{‚àÇx} \u0026amp;= \\frac{‚àÇf}{‚àÇX}‚ãÖ\\frac{‚àÇX}{‚àÇAY}‚ãÖ\\frac{‚àÇAY}{‚àÇx} \\\\ \u0026amp;= \\frac{‚àÇf}{‚àÇX}‚ãÖ\\frac{‚àÇX}{‚àÇAY}‚ãÖ(\\frac{‚àÇA}{‚àÇx}Y + A\\frac{‚àÇY}{‚àÇx})\\\\ \\end{aligned}$$\nSince the passed kernels on the ray path (starting from a pixel) have influences on the next kernel\u0026rsquo;s contribution, which is scaled by the previous accumulated transmittance, the order of solving derivatives should start from the most rear kernel, and then sequentially calculate the derivatives of front kernels in the reverse order of the forward pass.\nS G p r s l F B a o a o a d l t r N c ‚¨Æ v t w k - f e i a - i d n r ‚¨Æ r g d - s : : - t ‚¨Æ V P - i i - e x ‚¨Æ w e - T i i l - h n n ‚¨Æ e g r F - t e r - t h r n 1 o ‚¨Æ ‚Üë o e a d n p y e t = p c r e o ‚Üò i ‚ñ† s l n t o h g r i c t ‚Üê o s s l t o a s r c c k r e e n B ‚¨Æ N D a a - e l c ‚úö - r l k ‚¨Æ i w - v d a ‚úö - a o r ‚¨Æ t w d - i n : ‚úö - n v - ‚¨Æ + e s s - 1 t u - c r m ‚¨Æ n o e - m m g - e r ‚¨Æ s c a o d = f l i r o e ‚ñ† o r n m s t s The viewing ray travels from back to front and hits the screen. But the color nearest to the screen is the first to be seen by (or shown on) the camera (or eye).\nThe toppest color is based on downstream colors, so, its a function of all the preceding colors.\nIn the color stack, the color above depends on color below. Thus, the derivatives at the bottom should be sovled first.\n(2024-01-16) The toppest color is the base of all downstream colors, so, its derivative is contributed by all the behind colors.\nColor, Opacity (2024-01-08)\nGiven $\\frac{‚àÇL}{‚àÇC·µ¢(k)}$, the partial derivatives of the predicting pixel color $C·µ¢$ w.r.t. each parameter of a Gaussian G‚Çô (in the ray space) that contributes to the pixel are:\nThe parital derivative of C·µ¢ w.r.t. the kernel G‚Çô\u0026rsquo;s color c‚Çô, based on the forward pass: $C·µ¢ = T‚ÇÅ‚ãÖŒ±‚ÇÅ‚ãÖc‚ÇÅ+ T‚ÇÇ‚ãÖŒ±‚ÇÇ‚ãÖc‚ÇÇ + ‚ãØ T‚Çô‚ãÖŒ±‚Çô‚ãÖc‚Çô+ T‚Çô‚Çä‚ÇÅ‚ãÖŒ±‚Çô‚Çä‚ÇÅ‚ãÖc‚Çô‚Çä‚ÇÅ +‚ãØ + T_N‚ãÖŒ±_N‚ãÖc_N$:\n$$\\frac{‚àÇC·µ¢(k)}{‚àÇc‚Çô(k)} = Œ±‚Çô‚ãÖT‚Çô$$\nk represents one channel of RGB.\nThe furthest $T_N$ from the screen is saved at the end of the forward pass. And then the $T_{N-1}$ in front of it is calculated as $T_{N-1} = \\frac{T_N}{1-Œ±_{N-1}}$. The points in front follow this relation.\n3DGS Code\n1 2 T = T / (1.f - alpha); const float dchannel_dcolor = alpha * T; Alpha Œ±‚Çô\nTo solve the partial derivative of pixel color $C_i$ w.r.t. the kernel G‚Çô\u0026rsquo;s Œ±‚Çô, only consider the kernels that follow G‚Çô, as the transmittances of all the subsequent kernels rely on the currenct kernel G‚Çô: $$T‚Çô‚Çä‚ÇÅ = (1-Œ±‚Çô)T‚Çô$$\nThereby, the behind kernels will provide derivatives to the current kernel G‚Çô\u0026rsquo;s alpha.\nFor example, the color of the next kernel, G‚Çô‚Çä‚ÇÅ, behind G‚Çô is:\n$$\\begin{aligned} C‚Çô‚Çä‚ÇÅ \u0026amp;= c‚Çô‚Çä‚ÇÅ‚ãÖŒ±‚Çô‚Çä‚ÇÅ‚ãÖT‚Çô‚Çä‚ÇÅ \\\\ \u0026amp;= c‚Çô‚Çä‚ÇÅ‚ãÖŒ±‚Çô‚Çä‚ÇÅ‚ãÖ (1-Œ±‚Çô)T‚Çô \\\\ \u0026amp;= c‚Çô‚Çä‚ÇÅ‚ãÖŒ±‚Çô‚Çä‚ÇÅ‚ãÖ(1-Œ±‚Çô)‚ãÖ \\frac{ T‚Çô‚Çä‚ÇÅ}{1-Œ±‚Çô} \\\\ \\end{aligned}$$\nThus, the color $C‚Çô‚Çä‚ÇÅ$ contributes to the total partial derivative $\\frac{‚àÇC·µ¢}{‚àÇŒ±‚Çô}$ with the amount: $-\\frac{c‚Çô‚Çä‚ÇÅ‚ãÖŒ±‚Çô‚Çä‚ÇÅ‚ãÖT‚Çô‚Çä‚ÇÅ}{1-Œ±‚Çô}$ .\nContinuously, the following color C‚Çô‚Çä‚ÇÇ can be represented with Œ±‚Çô:\n$$ C‚Çô‚Çä‚ÇÇ = c‚Çô‚Çä‚ÇÇ‚ãÖŒ±‚Çô‚Çä‚ÇÇ‚ãÖT‚Çô‚Çä‚ÇÇ \\\\ = c‚Çô‚Çä‚ÇÇ‚ãÖŒ±‚Çô‚Çä‚ÇÇ ‚ãÖ\\cancel{(1-Œ±‚Çô‚Çä‚ÇÅ)} (1-Œ±‚Çô) \\frac{T‚Çô‚Çä‚ÇÇ}{ \\cancel{(1-Œ±‚Çô‚Çä‚ÇÅ)} (1-Œ±‚Çô)}$$\nThus, $\\frac{‚àÇC‚Çô‚Çä‚ÇÇ}{‚àÇŒ±‚Çô} = -\\frac{c‚Çô‚Çä‚ÇÇ‚ãÖŒ±‚Çô‚Çä‚ÇÇ‚ãÖT‚Çô‚Çä‚ÇÇ}{1-Œ±‚Çô}$ Similarly, the subsequent kernel G‚Çò, with m\u0026gt;n, also contribute to the overall partial derivative $\\frac{‚àÇC·µ¢}{‚àÇŒ±‚Çô}$.\nThereby, the ultimate partial derivatives of the pixel color $C·µ¢$ w.r.t. the G‚Çô\u0026rsquo;s alpha Œ±‚Çô is:\n$$\\frac{‚àÇC·µ¢}{‚àÇŒ±‚Çô} = c‚Çô‚ãÖT‚Çô - \\frac{‚àë_{m\u0026gt;n}c‚Çò‚ãÖŒ±‚Çò‚ãÖ T‚Çò}{1-Œ±‚Çô}$$\nOpacity o‚Çô, mean ùõç\u0026rsquo;, and covariance ùö∫':\nAccording to $Œ±‚Çô = o‚Çô e^{-œÉ‚Çô}$, where $œÉ‚Çô = \\frac{\\bm Œî‚Çô·µÄ \\bm Œî‚Çô}{2\\bm Œ£‚Çô\u0026rsquo;}$ (a scalar), and ùö´‚Çô is the offset from the pixel center to the 2D Gaussian G‚Çô\u0026rsquo;s mean $\\bm Œº\u0026rsquo;$, such as $\\bm Œî‚Çô = \\bm Œº‚Çô\u0026rsquo; - ùê±·µ¢$.\nPartial derivative of Œ±‚Çô w.r.t. opacity o‚Çô:\n$$ \\frac{‚àÇŒ±‚Çô}{‚àÇo‚Çô} = e^{-\\frac{\\bm Œî‚Çô·µÄ \\bm Œî‚Çô}{2\\bm Œ£‚Çô\u0026rsquo;}} $$\nPartial derivative of Œ±‚Çô w.r.t. the \u0026ldquo;exponent\u0026rdquo; sigma $œÉ‚Çô$:\n$$ \\frac{‚àÇŒ±‚Çô}{‚àÇœÉ‚Çô} = -o‚Çô e^{-œÉ‚Çô}$$\nPartial derivative of sigma œÉ‚Çô w.r.t. 2D mean ùõç‚Çô':\nBecause $\\bm Œî‚Çô$ is a function of ùõç‚Çô\u0026rsquo;, computing derivaties w.r.t. ùõç‚Çô\u0026rsquo; is equivalent to $\\bm Œî‚Çô$.\nThe Jacobian of œÉ‚Çô is:\n$$\\frac{‚àÇœÉ‚Çô}{‚àÇ\\bm Œº‚Çô\u0026rsquo;} = \\frac{‚àÇœÉ‚Çô}{‚àÇ\\bm Œî‚Çô\u0026rsquo;} = \\frac{‚àÇ(¬Ω\\bm Œî‚Çô\u0026rsquo;·µÄ \\bm Œ£\u0026rsquo;‚Åª¬π \\bm Œî‚Çô\u0026rsquo;)}{‚àÇ\\bm Œî‚Çô\u0026rsquo;} = \\bm Œ£‚Çô\u0026rsquo;‚Åª¬π \\bm Œî‚Çô\u0026rsquo;$$\nPartial derivative of sigma œÉ‚Çô w.r.t. 2D covariance matrix ùö∫':\n$$\\frac{‚àÇœÉ‚Çô}{‚àÇ\\bm Œ£‚Çô\u0026rsquo;} =$$\nGradient wrt Projection (2024-01-10)\nS 2 c D r e ‚¨Ø e n ùê± ‚ãØ O ùõÅ | ‚ãØ m ùìõ i R ( ‚ãØ t b a œï ùõç a y ‚Çñ ‚Çñ ‚ãØ d c 3 ( ' k D s ùê≠ , ‚¨Æ p p ' ùö∫ r a ) ‚Çñ o c ' œï + ( p e ) ‚Çñ D a ( ùêâ i g ùê≠ ‚ãÖ v a ' ( t ) ùê≠ a e | = ' p œï p C ( r l ( ùõç ùõç o i ùõç ‚Çñ ‚Çñ x p ‚Çñ ·∂ú ·∂ú . ùê≠ ·∂ú ) ) ) S ' , ‚¨Æ p ùö∫ a ‚Çñ c ·∂ú e ) | P C a m e ( r ùê≠ a ‚Çñ ùê≠ , ‚¨Æ s ùö∫ p ‚Çñ a ) c ‚åà ‚åä e ùêë 0  ∑ | ¬≤ ·∂ú O ' b ùê≠ 1 j  ∑ e ¬≤ c ( ·∂ú t ùõç ‚åâ ‚åã ùêó ‚Çñ s , p ùö∫ ‚¨Æ a ‚Çñ c ) e | A 3D Gaussian distribution centered at ùõç‚Çñ with a covariance matrix ùö∫‚Çñ in the object space will be projected onto 2D screen through the splatting step, resulting in a 2D Gaussian distribution, with applying the affine transformation approximation for the nonlinear perspective division.\nThe 3D ray space (or the screen) is constructed based on the perspective division (x,y divided by z), which however is non-linear. Therefore, the projective transformation $œï(ùê≠\u0026rsquo;)$ (i.e., perspective division + new depth) converting the clip space to the 3D ray space is approximated by a linear mapping: $œï‚Çñ(ùê≠\u0026rsquo;) = œï(ùê≠‚Çñ\u0026rsquo;) + ùêâ‚Çñ‚ãÖ(ùê≠\u0026rsquo; - ùê≠‚Çñ\u0026rsquo;)$, where ùê≠‚Çñ\u0026rsquo; = ùõç‚Çñ·∂ú, the mean vector in clip space.\nThe effects of the approximated affine mapping œï‚Çñ(ùê≠\u0026rsquo;) are as follows:\nThe transformed 2D Gaussian\u0026rsquo;s center ùõç‚Çñ\u0026rsquo; is the exact projective transformation œï(ùõç‚Çñ·∂ú), i.e., œï‚Çñ(ùê≠‚Çñ\u0026rsquo;)=œï(ùê≠‚Çñ\u0026rsquo;), without any error, with the 3rd dimension omitted.\n$$ \\underset{(3D)}{\\bm Œº‚Çñ\u0026rsquo;} = œï(\\bm Œº‚Çñ·∂ú) = \\begin{bmatrix} f_x‚ãÖŒº_{k,x}·∂ú/Œº_{k,z}·∂ú + c_x \\\\ f_y‚ãÖŒº_{k,y}·∂ú/Œº_{k,z}·∂ú + c_y \\\\ \\sqrt{ {Œº_{k,x}·∂ú}^2 + {Œº_{k,y}·∂ú}^2 + {Œº_{k,z}·∂ú}^2} \\end{bmatrix} \\overset{\\text{Omit 3rd dim}}{\\longrightarrow} \\underset{(2D)}{\\bm Œº‚Çñ\u0026rsquo;}= \\begin{bmatrix} \\frac{f_x‚ãÖŒº_{k,x}·∂ú}{Œº_{k,z}·∂ú} + c_x \\\\ \\frac{f_y‚ãÖŒº_{k,y}·∂ú}{Œº_{k,z}·∂ú} + c_y \\end{bmatrix} $$\nHowever, the approximated transformation œï‚Çñ(ùê≠\u0026rsquo;) of an arbitrary point ùê≠\u0026rsquo; around the clip-space 3D Gaussian center ùê≠‚Çñ' will deviate from the precise perspective projections œï(ùê≠\u0026rsquo;) gradually, as the (ùê≠\u0026rsquo; - ùê≠‚Çñ\u0026rsquo;) increases in the approximated mapping:\n$$ \\begin{aligned} œï(ùê≠\u0026rsquo;) ‚âà œï‚Çñ(ùê≠\u0026rsquo;) \u0026amp;= œï(ùê≠‚Çñ\u0026rsquo;) + ùêâ‚Çñ‚ãÖ(ùê≠\u0026rsquo; - ùê≠‚Çñ\u0026rsquo;) \\\\ \u0026amp;= \\bm Œº‚Çñ\u0026rsquo; + ùêâ‚Çñ‚ãÖ(ùê≠\u0026rsquo; - ùê≠‚Çñ\u0026rsquo;) \\end{aligned} $$\nThe projected 2x2 covariance matrix on screen is the 3x3 matrix in the ray space: $\\bm Œ£‚Çñ\u0026rsquo; = ùêâ‚Çñ‚ãÖ ùêë_{w2c}‚ãÖ \\bm Œ£‚Çñ‚ãÖ ùêë_{w2c}·µÄ‚ãÖ ùêâ‚Çñ·µÄ$, with the 3rd row and column omitted.\n(2024-01-11)\n‚≠êNote: The following $ùê≠‚Çñ$ is the coordinates of a Gaussian center in the camera space: $$ùê≠‚Çñ = [^{ùêë_{w2c} \\ ùê≠_{w2c}}_{0 \\quad\\ 1}] ‚ãÖ[^{\\bm Œº‚Çñ}_1]$$\nwhere ùõç‚Çñ¬≥·ïΩ¬π is the coordinates of the mean vector in world space.\n$ùê≠\u0026rsquo;$ is the clip coordinates, which is camera coordinates times the projection matrix ùêè: $ùê≠\u0026rsquo; = ùêèùê≠$\nùêè maps the camera-space coordinates to camera film (homogeneous) and scales for letting the ND Coordinates of points located within the camera frustum range in [-1,1]. With using clip coordinates, points whose w (i.e., z) is smaller than x,y,z are deleted.\nThe approximated projective transformation $œï‚Çñ(ùê≠\u0026rsquo;)$ fulfills perspective division after frustum clipping. Therefore, the 2D screen coordinates $\\bm Œº‚Çñ\u0026rsquo;_{(2D)} = œï‚Çñ(ùê≠‚Çñ\u0026rsquo;)_{(2D)}$ are NDC ‚àà [-1,1].\nThen the Gaussian center\u0026rsquo;s ND coordinates are scaled back to the screen size, yielding pixel coordinates ùõç‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé represented with clip coordinates as:\n$$ \\bm Œº‚Çñ\u0026rsquo;_{(\\text{pix})} = \\begin{bmatrix} (W‚ãÖt_{k,x}\u0026rsquo;/t_{k,w}\u0026rsquo; + 1) /2 + c_x \\\\ (H‚ãÖt_{k,y}\u0026rsquo;/t_{k,w}\u0026rsquo; + 1) / 2 + c_y \\end{bmatrix} $$\nThis relationship enables propagating gradients from pixel coordinates $\\bm Œº‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé$ to the clip coordinates $ùê≠‚Çñ\u0026rsquo;$ directly, without the 2D screen coordinates $\\bm Œº‚Çñ\u0026rsquo;_{(2D)}$ involved.\nCenter (2024-01-12)\nBecause ùõç‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé and ùö∫‚Çñ\u0026rsquo; on the 2D screen both are functions of 3D Gaussian center ùê≠‚Çñ, the partial derivatives of the loss w.r.t. ùê≠‚Çñ¬≥·ïΩ¬π is a sum:\n$$ \\frac{‚àÇL}{‚àÇùê≠‚Çñ} = \\frac{‚àÇL}{‚àÇ \\bm Œº‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé} \\frac{‚àÇ\\bm Œº‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé}{‚àÇùê≠‚Çñ} + \\frac{‚àÇL}{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}} \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇùê≠‚Çñ} \\\\ $$\nThe partial derivative of 2D Gaussian\u0026rsquo;s mean $\\bm Œº‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé$ w.r.t. the camera coordinates of 3D Gaussian center ùê≠‚Çñ:\n$$ \\begin{aligned} \\frac{‚àÇ\\bm Œº‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé}{‚àÇùê≠‚Çñ} \u0026amp;= \\frac{‚àÇ\\bm Œº‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé}{‚àÇ\\bm Œº‚Çñ\u0026rsquo;_{(2D)}} ‚ãÖ \\frac{‚àÇœï‚Çñ(ùê≠‚Çñ\u0026rsquo;)_{(2D)}}{ùê≠‚Çñ\u0026rsquo;} ‚ãÖ \\frac{‚àÇùê≠‚Çñ\u0026rsquo;}{‚àÇùê≠‚Çñ} \\qquad \\text{(Full process)} \\\\ \u0026amp;= \\frac{‚àÇ\\bm Œº‚Çñ\u0026rsquo;‚Çç‚Çö·µ¢‚Çì‚Çé}{‚àÇùê≠‚Çñ\u0026rsquo;} ‚ãÖ \\frac{‚àÇùê≠‚Çñ\u0026rsquo;}{‚àÇùê≠‚Çñ} \\qquad \\text{(Clip‚ÜíPix, skip screen coords)} \\\\ \u0026amp;= \\frac{1}{2} \\begin{bmatrix} W/t_{k,w}\u0026rsquo; \u0026amp; 0 \u0026amp; 0 \u0026amp; -W‚ãÖt_{k,x}\u0026rsquo;/{t_{k,w}\u0026rsquo;}^2 \\\\ 0 \u0026amp; H/t_{k,w}\u0026rsquo; \u0026amp; 0 \u0026amp; -H‚ãÖt_{k,y}\u0026rsquo;/{t_{k,w}\u0026rsquo;}^2 \\end{bmatrix}‚ãÖ ùêè \\end{aligned} $$\nBased on the properties of the Frobenius inner product, eq. (23) is obtained.\nThe partial derivative of the 2D Gaussian\u0026rsquo;s covariance ùö∫‚Çñ\u0026rsquo; w.r.t. the camera coordinates of 3D Gaussian center ùê≠‚Çñ:\n$$ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;}{‚àÇùê≠‚Çñ} = \\frac{‚àÇ(ùêâ‚Çñ‚ãÖ ùêë_{w2c}‚ãÖ \\bm Œ£‚Çñ‚ãÖ ùêë_{w2c}·µÄ‚ãÖ ùêâ‚Çñ·µÄ)_{2D} }{‚àÇùê≠‚Çñ} $$\n(2024-01-13) Derivation refers to 3D Gaussian Splatting‰∏≠ÁöÑÊï∞Â≠¶Êé®ÂØº - ÂÖ´Ê∞®ÂêàÊ∞ØÂåñÈíôÁöÑÊñáÁ´† - Áü•‰πé\nLetting $ùêî = ùêâ‚Çñ‚ãÖ ùêë_{w2c}$ (3DGS code refers to it as T.), the Gaussian covariance ùö∫‚Çñ\u0026rsquo; in the 3D ray space derived from the projective transformation œï‚Çñ(ùê≠\u0026rsquo;) is:\n$$\\bm Œ£‚Çñ\u0026rsquo; = ùêî ‚ãÖ \\bm Œ£‚Çñ‚ãÖùêî·µÄ = \\\\ \\begin{bmatrix} U‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÅ‚ÇÉ \\\\ U‚ÇÇ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÉ \\\\ U‚ÇÉ‚ÇÅ \u0026amp; U‚ÇÉ‚ÇÇ \u0026amp; U‚ÇÉ‚ÇÉ \\end{bmatrix} \\begin{bmatrix} œÉ‚ÇÅ‚ÇÅ \u0026amp; œÉ‚ÇÅ‚ÇÇ \u0026amp; œÉ‚ÇÅ‚ÇÉ \\\\ œÉ‚ÇÇ‚ÇÅ \u0026amp; œÉ‚ÇÇ‚ÇÇ \u0026amp; œÉ‚ÇÇ‚ÇÉ \\\\ œÉ‚ÇÉ‚ÇÅ \u0026amp; œÉ‚ÇÉ‚ÇÇ \u0026amp; œÉ‚ÇÉ‚ÇÉ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÅ \u0026amp; U‚ÇÉ‚ÇÅ \\\\ U‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÇ \u0026amp; U‚ÇÉ‚ÇÇ \\\\ U‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÉ \u0026amp; U‚ÇÉ‚ÇÉ \\end{bmatrix} = \\\\ \\begin{bmatrix} U‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÅ‚ÇÉ \\\\ U‚ÇÇ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÉ \\\\ U‚ÇÉ‚ÇÅ \u0026amp; U‚ÇÉ‚ÇÇ \u0026amp; U‚ÇÉ‚ÇÉ \\end{bmatrix} \\begin{bmatrix} \\boxed{œÉ‚ÇÅ‚ÇÅ}U‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; \\boxed{œÉ‚ÇÅ‚ÇÅ}U‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; \\boxed{œÉ‚ÇÅ‚ÇÅ}U‚ÇÉ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÉ‚ÇÉ \\end{bmatrix} = \\\\ \\Big[ \\begin{array}{c|c|c} U‚ÇÅ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÅ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÅ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ) \u0026amp; U‚ÇÅ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÅ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÅ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ) \u0026amp; U‚ÇÅ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÉ‚ÇÉ) + U‚ÇÅ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÉ‚ÇÉ) + U‚ÇÅ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÉ‚ÇÉ) \\\\ U‚ÇÇ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÇ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÇ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ) \u0026amp; U‚ÇÇ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÇ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÇ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ) \u0026amp; U‚ÇÇ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÉ‚ÇÉ) + U‚ÇÇ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÉ‚ÇÉ) + U‚ÇÇ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÉ‚ÇÉ) \\\\ U‚ÇÉ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÉ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÉ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ) \u0026amp; U‚ÇÉ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÉ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÉ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ) \u0026amp; U‚ÇÉ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÉ‚ÇÉ) + U‚ÇÉ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÉ‚ÇÉ) + U‚ÇÉ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÉ‚ÇÉ) \\end{array} \\Big] $$\nThe 3rd row and column in ùö∫‚Çñ\u0026rsquo; are omitted due to the orthogonal correspondence between the 3D ray space and the 2D screen. Thus, $\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}$ is only the upper-left 2√ó2 elements of the 3D ùö∫‚Çñ\u0026rsquo;, contributing to the gradient of 2D loss $L(ùêú‚Çñ, o‚Çñ, \\bm Œº‚Çñ\u0026rsquo;_{(2D)}, \\bm Œ£‚Çñ\u0026rsquo;_{(2D)})$, while the remaining 5 elements of ùö∫‚Çñ\u0026rsquo;‚Çç‚ÇÉ‚Çì‚ÇÉ‚Çé make no contributions.\n$$ \\bm Œ£‚Çñ\u0026rsquo;_{(2D)} = \\\\ \\Big[ \\begin{array}{c|c} U‚ÇÅ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÅ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÅ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ) \u0026amp; U‚ÇÅ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÅ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÅ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ) \\\\ U‚ÇÇ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÇ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ) + U‚ÇÇ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ) \u0026amp; U‚ÇÇ‚ÇÅ(œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÇ‚ÇÇ(œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ) + U‚ÇÇ‚ÇÉ(œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ) \\end{array} \\Big] $$\nEach element of $\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}$ is a \u0026ldquo;sub-\u0026rdquo; function, which is taken derivative w.r.t. each variable: œÉ‚ÇÅ‚ÇÅ, œÉ‚ÇÅ‚ÇÇ, œÉ‚ÇÅ‚ÇÉ, œÉ‚ÇÇ‚ÇÇ, œÉ‚ÇÇ‚ÇÉ, œÉ‚ÇÉ‚ÇÉ, to backpropagate the gradient $\\frac{‚àÇL}{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}$ to ùö∫‚Çñ. (Only these 6 elements of ùö∫‚Çñ need computation as ùö∫‚Çñ‚Çç‚ÇÉ‚Çì‚ÇÉ‚Çé is symmetric.)\nIt\u0026rsquo;s not proper to think of the derivative of a \u0026ldquo;matrix\u0026rdquo; w.r.t. a matrix. Instead, it\u0026rsquo;s better to consider the derivative of a function w.r.t. variables, as essentially a matrix stands for a linear transformation.\nThe partial derivative of $\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}$ w.r.t. $\\bm Œ£‚Çñ$ (the 3D covariance matrix in world space):\n$$ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÅ‚ÇÅ} = \\begin{bmatrix} U‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ \\\\ U‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ \\end{bmatrix} = \\begin{bmatrix} U‚ÇÅ‚ÇÅ \\\\ U‚ÇÇ‚ÇÅ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÅ \\end{bmatrix} \\\\ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÅ‚ÇÇ} = \\begin{bmatrix} U‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÇ \\\\ U‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÇ \\end{bmatrix} = \\begin{bmatrix} U‚ÇÅ‚ÇÅ \\\\ U‚ÇÇ‚ÇÅ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÇ \\end{bmatrix} \\\\ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÅ‚ÇÉ} = \\begin{bmatrix} U‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÉ \\\\ U‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÉ \\end{bmatrix} = \\begin{bmatrix} U‚ÇÅ‚ÇÅ \\\\ U‚ÇÇ‚ÇÅ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÉ \\end{bmatrix} \\\\ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÇ‚ÇÇ} = \\begin{bmatrix} U‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ \\\\ U‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ \\end{bmatrix} = \\begin{bmatrix} U‚ÇÅ‚ÇÇ \\\\ U‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÇ \\end{bmatrix} \\\\ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÇ‚ÇÉ} = \\begin{bmatrix} U‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÉ \\\\ U‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÉ \\end{bmatrix} = \\begin{bmatrix} U‚ÇÅ‚ÇÇ \\\\ U‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÉ \\end{bmatrix} \\\\ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÉ‚ÇÉ} = \\begin{bmatrix} U‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ \\\\ U‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ \\end{bmatrix} = \\begin{bmatrix} U‚ÇÅ‚ÇÉ \\\\ U‚ÇÇ‚ÇÉ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÉ \\end{bmatrix} $$\nœÉ‚ÇÅ‚ÇÅ, œÉ‚ÇÇ‚ÇÇ, œÉ‚ÇÉ‚ÇÉ are on the diagonal, while œÉ‚ÇÅ‚ÇÇ, œÉ‚ÇÅ‚ÇÉ, œÉ‚ÇÇ‚ÇÉ are off-diagonal. The partial derivative of the loss L w.r.t. each element of ùö∫‚Çñ:\n$$ \\begin{aligned} \\frac{‚àÇL}{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}} \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÅ‚ÇÅ} \u0026amp;= ‚àë_{row}‚àë_{col}{ \\begin{bmatrix} \\frac{‚àÇL}{‚àÇa} \u0026amp; \\frac{‚àÇL}{‚àÇb} \\\\ \\frac{‚àÇL}{‚àÇb} \u0026amp; \\frac{‚àÇL}{‚àÇc} \\end{bmatrix} ‚äô \\begin{bmatrix} U‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ \\\\ U‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ \\end{bmatrix} } \\\\ \u0026amp;= \\frac{‚àÇL}{‚àÇa} U‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ + 2√ó \\frac{‚àÇL}{‚àÇb} U‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ + \\frac{‚àÇL}{‚àÇc} U‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ \\\\ \\frac{‚àÇL}{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}} \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÅ‚ÇÇ} \u0026amp;= ‚àë_{row}‚àë_{col}{ \\begin{bmatrix} \\frac{‚àÇL}{‚àÇa} \u0026amp; \\frac{‚àÇL}{‚àÇb} \\\\ \\frac{‚àÇL}{‚àÇb} \u0026amp; \\frac{‚àÇL}{‚àÇc} \\end{bmatrix} ‚äô \\begin{bmatrix} U‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÇ \\\\ U‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÇ \u0026amp; U‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÇ \\end{bmatrix} } \\\\ \u0026amp;= \\frac{‚àÇL}{‚àÇa} U‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÇ + \\frac{‚àÇL}{‚àÇb} U‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÇ +\\frac{‚àÇL}{‚àÇb}U‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÇ + \\frac{‚àÇL}{‚àÇc} U‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÇ \\end{aligned} $$\n‚äô is Hadamard product (element-wise product). $‚àë_{row}‚àë_{col}$ means summation of all elements in the matrix.\n(2024-02-17) In this step, the derivative w.r.t. a matrix is determined by calculating the derivative w.r.t. each element individually, rather than the entire matrix. Thus, the multiplication between two \u0026ldquo;derivative matrices\u0026rdquo; is hadamard product, as essentially it\u0026rsquo;s the derivative w.r.t. a single scalar (in contrast to vector or matrix). However, for example, if $\\frac{‚àÇ\\bm Œ£}{‚àÇùêå}$ is the derivative of ùö∫ w.r.t. the matrix ùêå, the multiplication with the incoming upstream \u0026ldquo;derivative matrix\u0026rdquo; should be a normal matmul.\nWithin a chain of differentiation, the 2 manners of solving derivative for a matrix by computing the derivative for the entire matrix or calculating the derivative for each element can coexist simultaneously.\nNote: The $\\frac{‚àÇL}{‚àÇb}$ in the 3DGS code has been doubled. And each off-diagonal element is multiplied by 2, as the symmetrical element has the same gradient contribution.\nIn 3DGS code, the derivative of loss w.r.t. each element of 3D covariance matrix ùö∫‚Çñ‚Çç‚ÇÉ‚Çì‚ÇÉ‚Çé in the world space is computed individually:\n1 2 3 4 5 6 7 8 9 10 11 dL_da = denom2inv * (...); dL_dc = denom2inv * (...); dL_db = denom2inv * 2 * (...); dL_dcov[0] = (T[0][0]*T[0][0]*dL_da + T[0][0]*T[1][0]*dL_db + T[1][0]*T[1][0]*dL_dc); dL_dcov[3] = (T[0][1]*T[0][1]*dL_da + T[0][1]*T[1][1]*dL_db + T[1][1]*T[1][1]*dL_dc); dL_dcov[5] = (T[0][2]*T[0][2]*dL_da + T[0][2]*T[1][2]*dL_db + T[1][2]*T[1][2]*dL_dc); dL_dcov[1] = 2*T[0][0]*T[0][1]*dL_da + (T[0][0]*T[1][1] + T[0][1]*T[1][0])*dL_db + 2*T[1][0]*T[1][1]*dL_dc; dL_dcov[2] = 2*T[0][0]*T[0][2]*dL_da + (T[0][0]*T[1][2] + T[0][2]*T[1][0])*dL_db + 2*T[1][0]*T[1][2]*dL_dc; dL_dcov[4] = 2*T[0][2]*T[0][1]*dL_da + (T[0][1]*T[1][2] + T[0][2]*T[1][1])*dL_db + 2*T[1][1]*T[1][2]*dL_dc; The 2D covariance matrix $\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}$ is Not equivalent to the calculation where the 3rd row and column of ùö∫‚Çñ are omitted from the beginning, because œÉ‚ÇÅ‚ÇÉ, œÉ‚ÇÇ‚ÇÉ, œÉ‚ÇÉ‚ÇÉ are also involved in the projected covariance ùö∫‚Çñ\u0026rsquo;. However, the derivatives w.r.t. them ($\\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÅ‚ÇÉ},\\ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÇ‚ÇÉ},\\ \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇœÉ‚ÇÉ‚ÇÉ}$) can\u0026rsquo;t be derived from the following expression:\n$$\\begin{aligned} \\bm Œ£‚Çñ\u0026rsquo;_{(2D)} \u0026amp;= \\begin{bmatrix} U‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÅ‚ÇÇ \\\\ U‚ÇÇ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} œÉ‚ÇÅ‚ÇÅ \u0026amp; œÉ‚ÇÅ‚ÇÇ \\\\ œÉ‚ÇÇ‚ÇÅ \u0026amp; œÉ‚ÇÇ‚ÇÇ \\end{bmatrix} \\begin{bmatrix} U‚ÇÅ‚ÇÅ \u0026amp; U‚ÇÇ‚ÇÅ \\\\ U‚ÇÅ‚ÇÇ\u0026amp; U‚ÇÇ‚ÇÇ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} (U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÅ + U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÅ)U‚ÇÅ‚ÇÅ + (U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÇ + U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÇ) U‚ÇÅ‚ÇÇ \u0026amp; (U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÅ + U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÅ)U‚ÇÇ‚ÇÅ + (U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÇ + U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÇ) U‚ÇÇ‚ÇÇ \\\\ (U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÅ + U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÅ)U‚ÇÅ‚ÇÅ + (U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÇ + U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÇ) U‚ÇÅ‚ÇÇ \u0026amp; (U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÅ + U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÅ)U‚ÇÇ‚ÇÅ + (U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÇ + U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÇ) U‚ÇÇ‚ÇÇ \\end{bmatrix} \\end{aligned} $$\nThe partial derivative of the 2D Gaussian covariance matrix $\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}$ w.r.t. the 3D Gaussian center ùê≠‚Çñ in the camera space:\n$$ \\begin{aligned} \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇùêî‚Çñ} \\frac{‚àÇùêî‚Çñ}{‚àÇùê≠‚Çñ} = \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇùêî‚Çñ} \\frac{‚àÇ(ùêâ‚Çñ‚ãÖ ùêë_{w2c})}{‚àÇùê≠‚Çñ} \\\\ \\end{aligned} $$\n$\\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇùêî‚Çñ}$ (corresponding to $\\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;}{‚àÇT}$ inside $\\frac{‚àÇL}{‚àÇT}$ of eq.(25) in the gsplat paper.)\n$$ \\begin{aligned} \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÅ‚ÇÅ} \u0026amp;= \\begin{bmatrix} 2œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+(œÉ‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÅ)U‚ÇÅ‚ÇÇ+(œÉ‚ÇÅ‚ÇÉ+œÉ‚ÇÉ‚ÇÅ)U‚ÇÅ‚ÇÉ \u0026amp; œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; œÉ‚ÇÅ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ + œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÇ + œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÉ \u0026amp; 0 \u0026amp; 0 \\\\ œÉ‚ÇÅ‚ÇÅU‚ÇÉ‚ÇÅ + œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÇ + œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÉ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÅ‚ÇÇ} \u0026amp;= \\begin{bmatrix} U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+2œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ +U‚ÇÅ‚ÇÉœÉ‚ÇÉ‚ÇÇ \u0026amp; œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÇ + U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÇ+U‚ÇÇ‚ÇÉœÉ‚ÇÉ‚ÇÇ \u0026amp; 0 \u0026amp; 0 \\\\ U‚ÇÉ‚ÇÅœÉ‚ÇÅ‚ÇÇ + U‚ÇÉ‚ÇÇœÉ‚ÇÇ‚ÇÇ+U‚ÇÉ‚ÇÉœÉ‚ÇÉ‚ÇÇ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÅ‚ÇÉ} \u0026amp;= \\begin{bmatrix} U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÉ+œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+2œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÉ+U‚ÇÇ‚ÇÉœÉ‚ÇÉ‚ÇÉ \u0026amp; 0 \u0026amp; 0 \\\\ U‚ÇÉ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÉ‚ÇÇœÉ‚ÇÇ‚ÇÉ+U‚ÇÉ‚ÇÉœÉ‚ÇÉ‚ÇÉ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÇ‚ÇÅ} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÅ+U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÅ+U‚ÇÅ‚ÇÉœÉ‚ÇÉ‚ÇÅ \u0026amp; 0 \\\\ œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; 2œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ+U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÅ+U‚ÇÇ‚ÇÉœÉ‚ÇÉ‚ÇÅ \u0026amp; œÉ‚ÇÅ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ 0 \u0026amp; U‚ÇÉ‚ÇÅœÉ‚ÇÅ‚ÇÅ+U‚ÇÉ‚ÇÇœÉ‚ÇÇ‚ÇÅ+U‚ÇÉ‚ÇÉœÉ‚ÇÉ‚ÇÅ \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÇ‚ÇÇ} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÇ+U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÇ+U‚ÇÅ‚ÇÉœÉ‚ÇÉ‚ÇÇ \u0026amp; 0 \\\\ œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+2œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ+U‚ÇÇ‚ÇÉœÉ‚ÇÉ‚ÇÇ \u0026amp; œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ 0 \u0026amp; U‚ÇÉ‚ÇÅœÉ‚ÇÅ‚ÇÇ+U‚ÇÉ‚ÇÇœÉ‚ÇÇ‚ÇÇ+U‚ÇÉ‚ÇÉœÉ‚ÇÉ‚ÇÇ \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÇ‚ÇÉ} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÉ+U‚ÇÅ‚ÇÉœÉ‚ÇÉ‚ÇÉ \u0026amp; 0 \\\\ œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÉ+œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+2œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ 0 \u0026amp; U‚ÇÉ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÉ‚ÇÇœÉ‚ÇÇ‚ÇÉ+U‚ÇÉ‚ÇÉœÉ‚ÇÉ‚ÇÉ \u0026amp; 0 \\end{bmatrix} \\\\ \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÉ‚ÇÅ} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÅ+U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÅ+U‚ÇÅ‚ÇÉœÉ‚ÇÉ‚ÇÅ \\\\ 0 \u0026amp; 0 \u0026amp; U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÅ+U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÅ+U‚ÇÇ‚ÇÉœÉ‚ÇÉ‚ÇÅ \\\\ œÉ‚ÇÅ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; œÉ‚ÇÅ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; 2œÉ‚ÇÅ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÅ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÅ‚ÇÉU‚ÇÉ‚ÇÉ+ U‚ÇÉ‚ÇÇœÉ‚ÇÇ‚ÇÅ + U‚ÇÉ‚ÇÉœÉ‚ÇÉ‚ÇÅ \\\\ \\end{bmatrix} \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÉ‚ÇÇ} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÇ+U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÇ+U‚ÇÅ‚ÇÉœÉ‚ÇÉ‚ÇÇ \\\\ 0 \u0026amp; 0 \u0026amp; U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÇ+U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÇ+U‚ÇÇ‚ÇÉœÉ‚ÇÉ‚ÇÇ \\\\ œÉ‚ÇÇ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; œÉ‚ÇÇ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÇ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; U‚ÇÉ‚ÇÅœÉ‚ÇÅ‚ÇÇ+ œÉ‚ÇÇ‚ÇÅU‚ÇÉ‚ÇÅ+2œÉ‚ÇÇ‚ÇÇU‚ÇÉ‚ÇÇ+œÉ‚ÇÇ‚ÇÉU‚ÇÉ‚ÇÉ+ U‚ÇÉ‚ÇÉœÉ‚ÇÉ‚ÇÇ \\\\ \\end{bmatrix} \\\\ \\frac{‚àÇùêî‚Çñ {\\bm Œ£‚Çñ}_{(3D)} ùêî‚Çñ·µÄ}{‚àÇU‚ÇÉ‚ÇÉ} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; U‚ÇÅ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÅ‚ÇÇœÉ‚ÇÇ‚ÇÉ+U‚ÇÅ‚ÇÉœÉ‚ÇÉ‚ÇÉ \\\\ 0 \u0026amp; 0 \u0026amp; U‚ÇÇ‚ÇÅœÉ‚ÇÅ‚ÇÉ+U‚ÇÇ‚ÇÇœÉ‚ÇÇ‚ÇÉ+U‚ÇÇ‚ÇÉœÉ‚ÇÉ‚ÇÉ \\\\ œÉ‚ÇÉ‚ÇÅU‚ÇÅ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÅ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÅ‚ÇÉ \u0026amp; œÉ‚ÇÉ‚ÇÅU‚ÇÇ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÇ‚ÇÇ+œÉ‚ÇÉ‚ÇÉU‚ÇÇ‚ÇÉ \u0026amp; U‚ÇÉ‚ÇÅœÉ‚ÇÅ‚ÇÉ + U‚ÇÉ‚ÇÇœÉ‚ÇÇ‚ÇÉ + œÉ‚ÇÉ‚ÇÅU‚ÇÉ‚ÇÅ+œÉ‚ÇÉ‚ÇÇU‚ÇÉ‚ÇÇ+2œÉ‚ÇÉ‚ÇÉU‚ÇÉ‚ÇÉ \\\\ \\end{bmatrix} \\end{aligned} $$\n$\\frac{‚àÇùêî‚Çñ}{‚àÇùê≠‚Çñ} = \\frac{‚àÇ(ùêâ‚Çñ‚ãÖ ùêë_{w2c})}{‚àÇùê≠‚Çñ} = \\frac{‚àÇùêâ‚Çñ}{‚àÇùê≠‚Çñ}‚ãÖ ùêë_{w2c} + \\cancel{ ùêâ‚Çñ‚ãÖ\\frac{‚àÇùêë_{w2c}}{‚àÇùê≠‚Çñ} }$,\n(2024-01-16)\nThe derivative of ùêâ‚Çñ w.r.t. the camera-space center ùê≠‚Çñ could be obtained from the representation of ùêâ‚Çñ in terms of ùê≠‚Çñ, which includes focals fx,fy more than the representation with clip coordinates ùê≠‚Çñ\u0026rsquo;. In this way, the projection matrix P isn\u0026rsquo;t involved as ùê≠‚Çñ\u0026rsquo;=ùêèùê≠‚Çñ.\n$$ ùêâ_{ùê≠‚Çñ} = \\begin{bmatrix} f‚Çì/t_{k,z} \u0026amp; 0 \u0026amp; -f‚Çì t_{k,x} / {t_{k,z}}^2 \\\\ 0 \u0026amp; f_y/t_{k,z} \u0026amp; -f_y t_{k,y} / {t_{k,z}}^2 \\\\ t_{k,x}/‚Äñùê≠‚Çñ‚Äñ \u0026amp; t_{k,y}/‚Äñùê≠‚Çñ‚Äñ \u0026amp; t_{k,z}/‚Äñùê≠‚Çñ‚Äñ \\end{bmatrix} $$\nThe derivative of $ùêâ_{ùê≠‚Çñ}$ w.r.t. each component of ùê≠‚Çñ:\n$$ \\begin{aligned} \\frac{‚àÇùêâ_{ùê≠‚Çñ}}{‚àÇt_{k,x}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; -f_x/t_{k,z}¬≤ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 1/‚Äñùê≠‚Çñ‚Äñ - t_{k,x}^2/‚Äñùê≠‚Çñ‚Äñ^3 \u0026amp; -t_{k,y}t_{k,x}/‚Äñùê≠‚Çñ‚Äñ^3 \u0026amp; -t_{k,z}t_{k,x}/‚Äñùê≠‚Çñ‚Äñ^3 \\end{bmatrix} \\\\ \\frac{‚àÇùêâ_{ùê≠‚Çñ}}{‚àÇt_{k,y}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -f_y/t_{k,z}^2 \\\\ -t_{k,x}t_{k,y}/‚Äñùê≠‚Çñ‚Äñ^3 \u0026amp; 1/‚Äñùê≠‚Çñ‚Äñ - t_{k,y}^2/‚Äñùê≠‚Çñ‚Äñ^3 \u0026amp; -t_{k,z}t_{k,y}/‚Äñùê≠‚Çñ‚Äñ^3 \\end{bmatrix} \\\\ \\frac{‚àÇùêâ_{ùê≠‚Çñ}}{‚àÇt_{k,z}} = \\begin{bmatrix} -f_x/t_{k,z}^2 \u0026amp; 0 \u0026amp; 2 f_x t_{k,x}/t_{k,z}^3 \\\\ 0 \u0026amp; -f_y/t_{k,z}^2 \u0026amp; 2 f_y t_{k,y}/t_{k,z}^3 \\\\ -t_{k,x}t_{k,z}/‚Äñùê≠‚Çñ‚Äñ^3 \u0026amp; -t_{k,y}t_{k,z}/‚Äñùê≠‚Çñ‚Äñ^3 \u0026amp; 1/‚Äñùê≠‚Çñ‚Äñ - t_{k,z}^2/‚Äñùê≠‚Çñ‚Äñ^3 \\end{bmatrix} \\end{aligned} $$\nThe derivative of ùö∫‚Çñ\u0026rsquo; w.r.t. ùê≠‚Çñ\n(2024-01-17)\n$$ \\begin{aligned} \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇùêî‚Çñ} \\frac{‚àÇùêî‚Çñ}{‚àÇt_{k,x}} \\end{aligned} $$\nBased on the projective projection $œï(ùê≠) ‚âà œï(ùê≠‚Çñ) + ùêâ‚Çñ‚ãÖ(ùê≠ - ùê≠‚Çñ)$,\nwhere\nThe extrinsics of camera: $ùêì_{w2c} = [^{ùêë_{w2c} \\ ùê≠_{w2c}}_{0 \\quad\\ 1}]$\nùê≠ is the mean vector represented in the camera space: $ùê≠ = ùêì_{w2c} ‚ãÖ[^{\\bm Œº‚Çñ}_1]$\nThe Jacobian of the projective transformation evaluated at ùõç‚Çñ:\n$$ùêâ‚Çñ = \\begin{bmatrix} f‚Çì/Œº_{k,z} \u0026amp; 0 \u0026amp; -f‚Çì Œº‚Çì / Œº_{k,z}^2 \\\\ 0 \u0026amp; f_y/Œº_{k,z} \u0026amp; -f_y Œº_y / Œº_{k,z}^2 \\\\ Œº‚Çñ‚Çì/‚Äñ\\bm Œº‚Çñ‚Äñ‚ÇÇ \u0026amp; Œº_{k,y}/‚Äñ\\bm Œº‚Çñ‚Äñ‚ÇÇ \u0026amp; Œº_{k,z}/‚Äñ\\bm Œº‚Çñ‚Äñ‚ÇÇ \\end{bmatrix} $$\nTherefore, the partial derivatives of the loss w.r.t.\nThe partial derivative of the loss ùìõ w.r.t. the 3D Gaussian center ùê≠ in the world space:\n$$ \\frac{‚àÇL}{‚àÇùê≠} = \\frac{‚àÇL}{‚àÇ\\bm Œº\u0026rsquo;} \\frac{‚àÇ\\bm Œº\u0026rsquo;}{‚àÇùê≠} + \\frac{‚àÇL}{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}} \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{2D}}{‚àÇùê≠} $$\nThe partial derivative of the loss ùìõ w.r.t. the 3D Gaussian covariance matrix ùö∫‚Çñ in the world space:\n$$ \\frac{‚àÇL}{‚àÇ\\bm Œ£‚Çñ} = \\frac{‚àÇL}{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}} \\frac{‚àÇ\\bm Œ£‚Çñ\u0026rsquo;_{(2D)}}{‚àÇ\\bm Œ£‚Çñ} $$\nCovariance Because covariance matrix is symmetric (ùö∫‚Çñ = ùö∫‚Çñ·µÄ), it\u0026rsquo;s a square matrix, so it\u0026rsquo;s diagonalizable\n\u0026ldquo;Diagonalizable matrix ùêÄ can be represented as: ùêÄ = ùêèùêÉùêè‚Åª¬π.\u0026rdquo;\n\u0026ldquo;A diagonalizable matrix ùêÄ may (?) be decomposed as ùêÄ=ùêêùö≤ùêê·µÄ\u0026rdquo;\n\u0026ldquo;Quadratic form can be regarded as a generalization of conic sections.\u0026rdquo; Symmetric matrix\nSince the covariance matrix ùö∫ is a symmetric matrix, its eigenvalues are all real. By arranging all its eigenvectors and eigenvalues into matrices, there is:\n$$\\bm Œ£ ùêï = ùêï ùêã$$\nwhere each column in ùêï is an eigenvector, which are orthogonal to each other.\nùêã is a diagonal matrix. For example:\n$$ \\bm Œ£ ùêï = ùêï ùêã = \\begin{bmatrix} a \u0026amp; d \u0026amp; g \\\\ b \u0026amp; e \u0026amp; h \\\\ c \u0026amp; f \u0026amp; j \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 3 \\end{bmatrix} = \\begin{bmatrix} a \u0026amp; 2d \u0026amp; 3g \\\\ b \u0026amp; 2e \u0026amp; 3h \\\\ c \u0026amp; 2f \u0026amp; 3j \\end{bmatrix} $$\nIf ùêï is invertible, ùö∫ can be represented as ùö∫=ùêïùêãùêï‚Åª¬π\nThe eigenvectors matrix ùêï and eigenvalues matrix ùêã corresponds to the rotation matrix ùêë and the stretching matrix ùêí squared, which are solved from SVD: ùö∫=ùêëùêíùêí·µÄùêë·µÄ.\nThe rotation matrix rotates the original space to a new basis, where each axis points in the direction of the highest variance, i.e., the eigenvectors. And the stretching matrix indicates the magnitude of variance along each axis, i.e., the square root of the eigenvalues $\\sqrt{ùêã}$. janakiev-Blog\nAfter obtaining the magnitude of the variance in each direction, the extent range on each axis can be calculated based on the standard deviation according to the 3-sigma rule.\nTo optimize the 3D Gaussians in the world space based on rendered image, the derivative chain is like:\nI l m o a s g s e s s c p r a ùö∫ e c ' e e n ( 2 D ) R s a p y a ùö∫ c ' e ( 3 D ) c s a p m a e c ùö∫ r e a w s o p ùö∫ r a l c d e ","date":"2024-01-01T12:45:00Z","image":"https://janakiev.com/assets/covariance-matrix_files/covariance_visualization.jpg","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-math/","title":"read: 3DGS | Math Derivation"},{"content":"Code | Arxiv | Oral | GScholar | Rui Chen, Hao Su\nNotes (2023-12-24)\nAbs \u0026amp; Intro Use point cloud to refine depth map rather than regularizing (\u0026ldquo;squashing\u0026rdquo;) cost volumes.\nThe points move along the ray corresponding to each pixel. The step size is determined by neighbors. Refine the coarse point cloud by iteratively estimating the residual from the current depth to the target depth.\nThe target depth is obtained from the ground-truth piont cloud. F V e o a l t u u m r e e P p e r r o s j p F p e o a i t n u t r e c - l a o u u g d m e n t e d N N D r e e p s t i h d u a l P d o i i s n p t l a c e m e n t Output: a dense point cloud, as opposed to sparse point cloud from SfM (COLMAP) used by 3DGS.\nDepth map yielded from point cloud representation doesn\u0026rsquo;t suffer from the problem of resolution limitation.\nOptimization objective: minimize the distance from point to the surface, with the supervision of depth map\nDepth estimation doesn\u0026rsquo;t involve opacity for rendering, so can the RGB appearance be used as loss function? PointFlow: Move point towards the surface. In contrast, 3DGS split or clone a Gaussian along the position\u0026rsquo;s gradient with some hyper-parameters setting.\nHow is the generalizability of this method?\nPipeline Point cloud initialization from a small coarse depth map for reference image.\nS m a i l n l r C e o f s t c a V m o e l r u a m e 3 c D l a U s N s e i t f y d C e o p a t r h s e m a U p n p r o j P w o o i r n l t d c s l p o a u c d e 48 homographies (depths) for warping a source feature map with the 1/8 size of the original image. Assign 2D and 3D context feature to each point\n2D feature: projecting each point onto 3-level feature maps of each view with camera focals and cx,cy scaled.\nW o r l d s U p P p a o d c i a e n t t i s n g F i x f e e d a t 3 p u - y r s r e c a a m m l i a e d p s The retrieved feature vectors of N views are merged into a variance for a certain level j of the feature maps:\n$$ùêÇ ≤ = \\frac{\\sum (ùêÖ ≤ - \\bar{ùêÖ ≤}) }{N}$$\nOnly the feature vectors at the projection locations are taken, rather than processing the entire feature map, thereby improving efficiency. 3D feature: normalized point coordinates $ùêó‚Çö$\nConcatenate features: $C‚Çö = ùêÇ¬π ‚äï ùêÇ¬≤ ‚äï ùêÇ¬≥ ‚äï ùêó‚Çö$\nDefine displacement steps with point hypotheses.\nThe unprojected 3D point cloud from 2D depth map is determined. The displacement direction has a lot of freedom. Thus, each point is confined to move along the ray emitted from the reference camera. Such that the per-pixel depth can be refined.\n‚Üí R - e ‚ñ° f ‚Üì ‚Üë ‚ñ° ‚ñ° c - a ‚Üê m In the above figure, ‚Ä¢ is a real point, and o is a point hypothesis, denoted as $\\tilde{ùê©‚Çñ}$, which is associated with a read point $ùê©‚Çñ$:\n$$\\tilde{ùê©‚Çñ} = ùê©‚Çñ + k s ùê≠, \\quad k = -m, ‚Ä¶, m$$\nwhere k is the number of steps, s is the step size along the ray direction ùê≠.\nm = 1, so\nAggregate features of the n nearest neighbors\nUse Dynamic Graph CNN to aggregate neighboring points\nUse an MLP to map the aggregated feature to probabilities of point hypotheses.\nThe PointFlow module requires iterations to approach the surface iteratively.\nThe predicted depth residual against the target depth is a probabilistic weighted sum (expectation) of all predefined hypothetical steps.\n$$Œîd‚Çö = ùêÑ(ks) = ‚àë_{k=-m}^m ks √ó Prob( \\tilde{ùê©‚Çñ} )$$\nUpsampling the refined depth map and shrink the interval between point hypotheses.\nLoss: Accumulated absolute error between refined depth map and the ground-truth depth map over all previous iterations.\n$$L = ‚àë_{i=0}^l \\Big( \\frac{1}{s^{(i)}} ‚àë_{p‚ààùêè_{valid}} \\| ùêÉ_{GT}(p) - ùêÉ^{(i)}(p) \\|‚ÇÅ \\Big)$$\nMethod Feature concatenation is similar to PixelNeRF: point ocoordinates + feature vectors. But here the points position isn\u0026rsquo;t consistent, resulting in that different feature vectors that are sampled on the feature pyrimid.\nBecause the 2D CNN also needs to be trained, the feature pyrimid is changing as well.\nPlay Environment (2023-12-26)\nInstallation on Lambda server (Ubuntu 18.04.6 LTS, nvcc -V return 10.2, Driver Version: 470.103.01):\n1 2 bash install_dependencies.sh bash compile.sh Error: No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\nTraceback 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 No CUDA runtime is found, using CUDA_HOME=\u0026#39;/usr/local/cuda\u0026#39; running build_ext Traceback (most recent call last): File \u0026#34;setup.py\u0026#34;, line 20, in \u0026lt;module\u0026gt; \u0026#39;build_ext\u0026#39;: BuildExtension File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/setuptools/__init__.py\u0026#34;, line 153, in setup return distutils.core.setup(**attrs) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/core.py\u0026#34;, line 148, in setup dist.run_commands() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/dist.py\u0026#34;, line 955, in run_commands self.run_command(cmd) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/dist.py\u0026#34;, line 974, in run_command cmd_obj.run() File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/setuptools/command/build_ext.py\u0026#34;, line 79, in run _build_ext.run(self) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/command/build_ext.py\u0026#34;, line 339, in run self.build_extensions() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 404, in build_extensions self._check_cuda_version() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 777, in _check_cuda_version torch_cuda_version = packaging.version.parse(torch.version.cuda) File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/pkg_resources/_vendor/packaging/version.py\u0026#34;, line 49, in parse return Version(version) File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/pkg_resources/_vendor/packaging/version.py\u0026#34;, line 264, in __init__ match = self._regex.search(version) TypeError: expected string or bytes-like object Search the error with DDG.\ntorch.cuda.is_available() returns False. NV forums\npytorch is of cpu version. issue\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; conda list cpuonly 2.0 0 pytorch pytorch 1.10.2 py3.6_cpu_0 pytorch pytorch-mutex 1.0 cpu pytorch torchvision 0.11.3 py36_cpu [cpuonly] pytorch Reinstall with newer packages:\n\u0026ldquo;install_dependencies.sh\u0026rdquo;:\n1 2 3 4 5 6 #!/usr/bin/env bash conda create -n PointMVSNet python=3.10 source activate PointMVSNet conda install pytorch==1.12.1 torchvision==0.13.1 cudatoolkit=10.2 -c pytorch conda install -c anaconda pillow pip install -r requirements.txt Error: identifier \u0026quot;AT_CHECK\u0026quot; is undefined.\nFull message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 building \u0026#39;dgcnn_ext\u0026#39; extension creating build creating build/temp.linux-x86_64-cpython-310 creating build/temp.linux-x86_64-cpython-310/csrc /usr/local/cuda/bin/nvcc -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/zichen/anaconda3/envs/PointMVSNet/include/python3.10 -c csrc/gather_knn_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/gather_knn_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1011\\\u0026#34; -DTORCH_EXTENSION_NAME=dgcnn_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++14 /home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign /home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/ATen/Context.h(25): warning: attribute \u0026#34;__visibility__\u0026#34; does not apply here csrc/gather_knn_kernel.cu(34): error: identifier \u0026#34;AT_CHECK\u0026#34; is undefined csrc/gather_knn_kernel.cu(106): error: identifier \u0026#34;AT_CHECK\u0026#34; is undefined csrc/gather_knn_kernel.cu(125): error: identifier \u0026#34;THArgCheck\u0026#34; is undefined csrc/gather_knn_kernel.cu(145): error: identifier \u0026#34;THCudaCheck\u0026#34; is undefined csrc/gather_knn_kernel.cu(84): error: identifier \u0026#34;TH_INDEX_BASE\u0026#34; is undefined 5 errors detected in the compilation of \u0026#34;/tmp/tmpxft_00002cb5_00000000-6_gather_knn_kernel.cpp1.ii\u0026#34;. error: command \u0026#39;/usr/local/cuda/bin/nvcc\u0026#39; failed with exit code 1 Perplexity (GPT4): Identifiers have been deprecated after PyTorch 1.0.\nIn PyTorch 1.5.0 and later, AT_CHECK has been replaced with TORCH_CHECK. Similarly, THArgCheck and THCudaCheck are no longer used in newer versions of PyTorch. The identifier TH_INDEX_BASE is also undefined because it\u0026rsquo;s no longer used in PyTorch 1.0 and later.\nDowngrade pytorch:\nAIËëµ used torch 1.4. Issue: \u0026ldquo;TH_INDEX_BASE\u0026rdquo; is undefined #1\n1 2 3 4 5 6 #!/usr/bin/env bash conda create -n PointMVSNet python=3.8 source activate PointMVSNet conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch conda install -c anaconda pillow pip install -r requirements.txt torchvision older than v0.6.1 doesn\u0026rsquo;t exist in conda:\nFull message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels: - torchvision==0.5.0 Current channels: - https://conda.anaconda.org/pytorch/linux-64 - https://conda.anaconda.org/pytorch/noarch - https://conda.anaconda.org/conda-forge/linux-64 - https://conda.anaconda.org/conda-forge/noarch - https://repo.anaconda.com/pkgs/main/linux-64 - https://repo.anaconda.com/pkgs/main/noarch - https://repo.anaconda.com/pkgs/r/linux-64 - https://repo.anaconda.com/pkgs/r/noarch To search for alternate channels that may provide the conda package you\u0026#39;re looking for, navigate to https://anaconda.org and use the search bar at the top of the page. Use pip to install torch 1.4. (perplexity)\n1 2 3 4 5 conda create -n PointMVSNet python=3.8 source activate PointMVSNet conda install -c anaconda pillow pip install torch==1.4.0 torchvision==0.5.0 pip install -r requirements.txt And modify TH_INDEX_BASE -\u0026gt; 0 in \u0026ldquo;gather_knn_kernel.cu\u0026rdquo;.\nIt can be compiled successfully.\nP.S.:\n3DGS also has knn code: simple-knn Open3D also can find the nearest neighbors. Train \u0026amp; Eval Place the dataset (DTU) in the specified directory:\n1 2 mkdir data ln -s /home/zichen/Downloads/mvs_training/dtu/ ./data Training for 16 epochs cost 3 days approximately:\n1 2 export CUDA_VISIBLE_DEVICES=4,5,6,7 python pointmvsnet/train.py --cfg configs/dtu_wde3.yaml Evaluate\nThe \u0026ldquo;Recified\u0026rdquo; dataset can\u0026rsquo;t be download from the official website by clicking the hyperlink. It may be due to broswer limitations as the zip file is 123 GB (analyzed by Perplexity).\nWget works:\n1 wget http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Download message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (base) zichen@lambda-server:/data/zichen$ wget http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip wget: /home/zichen/anaconda3/envs/GNT/lib/libuuid.so.1: no version information available (required by wget) --2023-12-30 19:30:31-- http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Resolving roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)... 130.225.69.128 Connecting to roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)|130.225.69.128|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip [following] --2023-12-30 19:30:32-- https://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Connecting to roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)|130.225.69.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 129593443783 (121G) [application/zip] Saving to: ‚ÄòRectified.zip‚Äô Rectified.zip 100%[===========================\u0026gt;] 120.69G 3.48MB/s in 14h 18m 2023-12-31 09:48:33 (2.40 MB/s) - ‚ÄòRectified.zip‚Äô saved [129593443783/129593443783] Unzip it\n","date":"2023-12-16T21:30:00Z","image":"http://hansf.me/projects/PMVSNet/images/network.png","permalink":"https://zichen34.github.io/writenotes/model/mvs/b-note-point-mvsnet/","title":"read: Depth - MVG | Point-MVSNet"},{"content":"Code | Arxiv | GScholar | mvsnet pytorch\nNotes Task The model predicts a single depth map at once instead of an entire scene:\nMVSNet can reconstruct a large scene by performing per-view depth map estimation repeatedly.\n\u0026ldquo;Per-view\u0026rdquo; also appears in Point-Based Neural Rendering with Per-View Optimization - Inria.\nThe estimated depth map of the reference view is a weighted sum of multiple predefined depth values, where the weights are regressed from the variance of multi-view cost volumes by a 3D UNet-like network.\nA cost volume is a structure-fixed container (file holder) in the reference camera space, with each compartment holding a source view\u0026rsquo;s feature map that requires warping to align with the camera pose and depth before being inserted into each slot.\nC a r c o r e a s e f m t e e i r r V n e a o n l t c s u h e p m e a e c S s e a m e d e p t h - s l i c i n g A s f o o b u r u r n c o d e t l h e f e e r o a f t V u o w r l a e u r m p m e e a s d p s (2023-12-13) In contrast, NeRF lacks a unified spatial structure to facilitate multi-view geometry consistency.\nPer-ray sampling \u0026 Scene-specific Each view samples depths and maintains a point cloud in its own frustum for rendering (not for reconstructing the global geometry), and the fine-stage sampling results in each ray having different sampled points.\n(2024-02-19) The sampled points don\u0026rsquo;t have standardized geometric structure, because NeRF is a \u0026ldquo;continuous\u0026rdquo; filed, whose continuity is achieved by an MLP. NeRF encompassed multi views into a Radiance filed, so the scene geometry is not directly modeled and constrained. Consequently, multi-view (geometry) inconsistency (floater) is incurred.\nv i 1 e w v i e w 2 v i 3 e w NeRF samples per ray rather than the whole space because it employs the volume rendering method to generate view-dependent images. The primary goal is achieving accurate pixels by optimizing the alignment of volume density œÉ and voxel color c on an arbitrary piont due to the volume rendering equaiton, while the overall geometry is not focused specifically.\nIn other words, since what NeRF learned is the match between density and voxel color, even if the densities (geometry) are wrong, with colors compensated, the composition could be plausible. Despite the multi-view consistency constraint during training, the geometry estimation (surface) is inherently biased. (NeuS)\nOn the other hand, because the correspondence between volume density and voxel color is different for various scenes, the MLP network of NeRF is scene-specific.\nSimilarly, PixelNeRF regress densities and colors from pixel-wise image features in the frustum of each viewpoint as well, instead of the world space. And multiple views\u0026rsquo; results get aggregated by taking average finally.\nPer-ray and fine-stage sampling decides NeRF is a scene-specific representation because each region is not reconstructed equally. Although some generalizable NeRFs predict density and color from image features, the precision won\u0026rsquo;t be high if only image features are input without providing explicit geometry structures.\n(2023-12-15) The whole space to be reconstructed is split (divide-and-conquer strategy) with some assumption instead of estimating from null. CVPR2023|Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†MVSÁöÑÊúÄÊñ∞ËøõÂ±ï GeoMVSNet - 3DËßÜËßâÂ∑•Âùä\nThe resolution of a cost volume is fixed (point-mvsnet). A cost volume is a voxel grid to some extent.\nThe pipeline of MVSNet:\nR i f e m e f g a S v f t S v f r i e r i e c e a c e a w t w t R 1 e 2 p l i c C a o s W s a t r p V e o ‚úö d l u v i f m a n e e r a : i r V v t o e o a s w u f l r a s - u i r c m a p d a e n e e m c d p e t s f h p + e s a s a c 3 U o t e D N f s e t t m a a t x R e V 1 f o 9 ' l 2 s u m d p e e r p o s b D E e m x p a p t p e h c r e f i n e Implicit Camera Pose \u0026ldquo;Arbitrary camera geometries\u0026rdquo; prevent the direct comparision among all multi-view feature maps (due to the enormous disparity in observations?).\nIn contrast, 2-view stereo is free from that problem because after pre-rectification, the matching cost of a pair of feature maps is only attributed to the disparity between the pair of images. (?)\n\u0026ldquo;Arbitrary camera geometries\u0026rdquo; means that the epipolar constraint served as the base of two-view stereo could fail, e.g. when no overlapping regions exist in the 2 views. In such a case, however, homography still holds. homography - Carleton University\nTherefore, MVSNet transforms all feature maps into the reference-view camera space.\nHomography warping injects camera geometries into the 2D CNN implicitly.\nBecause the convolution and homography (warping) both are linear operations, they can be switched. Thus, warping the feature map is equivalent to warp the source images followed by convolution. Consequently, the convolution layer (feature extractor) is trained on an image set that implies the camera geometries.\n(2023-12-19) Note: The source image (or feature map) is not mapped onto the depth plane. They\u0026rsquo;re mapped into the reference camera viewpoint to watch the scene.\nBy applying different homographies solved with the predefined depth planes, the source image is warped uniquely. Thus, the features at the same location vary.\nOverall, as depth increases, the source images are go backward.\n(2024-03-17) The above effect is like that a source image is seen from the reference view. It\u0026rsquo;s like watching TV from another angle. This is because when restoring depth values for all pixels, they are assigned with a common depth value to become 3D points (on a depth plane) Code. During training, there are 192 assumed depth values to be assigned on pixels, and then the network is required to predict which depth each pixel is, resulting in a correct pixel-wise depth.\nWith the correct depth map, applying homography will lead to overlapping for the common area between src and ref images.\nThis differentiates homography from epipolar geometry, where a point is projected onto different views to retrieve feature vectors. (2023-12-20) As ray marches, for epipolar geometry, the projected pixel is moving along the epipolar line. However, for homography, the source image keeps warping using the increasing depth, resulting in the same effect that the projected location moves along the epipolar line.\nAnd the directions of data flow are opposite. Epipolar is from 3D points at different depths to a source image, whereas homography is from different warped source images to a common 3D point.\nThe reason of performing homography warping on feature map rather than source images could be reducing indexing time with a small resolution?\n(2023-12-15) Maybe to avoid performing too many convolutions. A source image does convolution once and the feature map is warped 192 times.\n(2023-12-23) Maybe the author got inspired by the CNN-based 2-view stereo matching, where camera parameters are disregarded when applying CNN onto 2 pre-rectified images. In contrast, MVSNet incorporates camera poses implicitly after CNN. In other words, \u0026ldquo;rectification\u0026rdquo; is performed on feature maps instead of original images, so as to encode the camera poses into the 2D CNN.\nIf warping the source image first, many black empty areas will appear and disrupt the following convolution. Thus, perform CNN on source images first and then sample feature maps.\nBecause the cost volume is built in the reference camera frustum rather than the world space, the coordinates transformation applied is homography $ùêä‚ÇÅ[ùêë‚ÇÅ|ùê≠‚ÇÅ](ùêä·µ¢[ùêë·µ¢|ùê≠·µ¢])‚Åª¬π$ rather than only unprojection $(ùêä·µ¢[ùêë·µ¢|ùê≠·µ¢])‚Åª¬π$. The \u0026ldquo;implicit\u0026rdquo; camera pose embedding manner is proposed to generalize the 2-view stereo to multi-view stereo.\nOur 3D cost volume is built upon the camera frustum instead of the regular Euclidean space.\nConvolution fuses the neighboring pixels into one feature vector for dense matching. A feature vector is a compact representation of a patch of pixels.\nIn this way, image size reduced but without losing context information required by matching. Hence, the efficiency is higher than matching the original images directly. The high-dimensional descriptor can lead to correct matching if they\u0026rsquo;re well-optimized.\nEvery source view passes the identical 2D CNN, so the differences between cost volumes indicate the probabilities of each depth for each pixel.\nEach \u0026ldquo;depth slot\u0026rdquo; within a cost volume stores a 32-channel feature map, which will be regularized to a 1-channel probability map by 3D UNet and softmax.\nEstimate by Variance The matching cost is measured by element-wise (pixel-wise, depth-wise, and channel-wise) variance of cost volumes, such that the number of source views is not limited.\nA property can be inferred from the mean of multiple views, serving as a synthesized representation. For example, PixelNeRF used the average of projected features from all source views to regress rgbœÉ per point. However, MVSNet maps the explicit difference (measured by variance) among the same-position feature vectors to a property, i.e., probability of each hypothetical depth for a pixel.\nThe feature vectors at the same location on the warped feature maps from different source images don\u0026rsquo;t correspond to the same 3D point, because a 3D point projected onto different camera films gets different pixel coordinates.\nThus, the homologous feature vectors (\u0026ldquo;allele\u0026rdquo;) at the same relative position from the 3 input images are distinct evidently.\nCyan squares are all at the same location [100:150, 165:215].\nThe ground truth depth is around 776:\n(2023-12-15) The variance for each channel of each matched feature vector from 3 cost volumes, for 192 depths is calculated as:\nF A e c a R r t e o s f s s f o S 3 r r c C d 1 o e s p t t S h r V 1 c o 2 l u m F e e s a t R s e f f o S r r c d 1 e p t S h r 2 c 2 ‚ãØ ‚ãØ F e a t R s e f f o S r r c d 1 e p 1 S 9 r 2 c 2 The 192 32-channel variance vectors will be fused to 1-D scalars by a 3D CNN.\nConv along the depth dimension, kernel size=2 and stride=1: aggregate every two variance vector at adjacent depths.\nOutput channel is 1: combine the covered vectors by channel-wise weighted summation only once.\nIn a kernel, weighted sum the same channels, and then sum up all channels to produce one of the output channels.\no 1 ‚ñ° u V t c a ‚ñ° : h r v a ‚Üì a t ‚ãØ r ‚ãØ s 1 D u 1 m ‚ñ° V a r a s t t ‚ãØ r ‚ãØ D i 2 d e = 1 ‚ãØ o 1 V ‚ñ° u a t c r ‚ñ° : h a v t ‚Üì a ‚ãØ r D ‚ãØ s 1 1 u 9 9 m 2 1 ‚ñ° V a r a t D ‚ãØ 1 9 2 192 variance scalars will be normalized by softmax to 192 probabilities for each depth value.\nDepth map is a summation of 192 preset depths weighted by probabilities.\nThe variance of the \u0026ldquo;position-matched\u0026rdquo; feature vectors from a pair of source feature maps at each depth is interpreted as each depth\u0026rsquo;s probability for a pixel:\nA source image is warped using a homography, solved with a certain depth $d‚Çô$, to supplement the reference view with information from a lateral perspective, although the scale doesn\u0026rsquo;t match:\nIf the scale matches, the pixels on the ref and source views projected from a common 3D point will overlapped, and the depth of that 3D point is found. (Refer to the test result in another post.)\nThe feature map of the source image is sampled into a warped feature map.\nFeature vectors at the same location are matched? do not match a common 3D point.\nThe variance of the feature vectors pertaining to a depth is calculated\nHigh variance means the patches that a point projected onto each view are distinct, leading to different feature vectors.\n3D CNN aggregates adjacent variance vectors along 3 directions into a scalar variance.\n3D CNN is a smooth constraint to filter the noise in the cost volume caused by non-Lambertian surfaces and object occlusions.\nNormalize 192 variance by Softmax to obtain the probability distribution of depths.\nWhy does the correct depth has the highest variance?\nWhy use variance? high variance means high probability.\n‰∏∫‰ªÄ‰πàÈ´òÊñπÂ∑ÆÊÑèÂë≥ÁùÄÈ´òÊ¶ÇÁéáÂë¢ÔºüÔºüÔºü\nÊää‰∏ÄÁªÑÁÖßÁâáÁöÑ homography ÊïàÊûúÂÅöÂá∫Êù•ÁúãÁúã Read: Rethink depth esti and IS-MVSNet Google search: \u0026ldquo;why does MVSNet use the variance to regress the probability of depths?\u0026rdquo; Because the exact depth for a pixel results in other views\u0026rsquo; \u0026ldquo;deny\u0026rdquo; since the matching pixel on other source images doesn\u0026rsquo;t correspond to that depth due to the viewpoint shifts.\nchanging viewpoint will change distance from the observing point to the camera as well. a feature vector represents a patch of pixels, the area with high rgb-variance means geometry changes.\n(2023-12-16) Each variance vector is attributed to 3 feature vectors:\nepipolar homography The epipolar reminds me the view fusion in GNT, where multi-view features are fused by \u0026quot; subtraction attention\u0026quot;.\n(2023-12-20) However, MVSNet is not epipolar. The feature 3 vectors are at the same location with the underlying feature maps changing.\nBut essentially, homography is epipolar in terms of variance vectors ( not actual 3D points). The feature vectors on the epipolar line are shifted to the location of the reference feature.\nSuccinctly, in epipolar, the projection changes (moves), while in homography, the feature map changes (warps).\nI probably thought of (on 2023-12-16) that after warping, the feature vectors at the same location correspond to a same 3D point, like eipolar. But that\u0026rsquo;s not right even for the warping with the accurate depth value (776 mm) as shown in the above demo. The image (or feature map) moved indeed after warping, but not got the exact position corresponding to the common 3D point.\nR f s f ‚ñ° e e s f o e f a o e u a t u a r t 1 r t 2 c c e m e m a 3 a p 2 p d c 1 h n l s 3 ‚ñ° ‚ñ° 2 d 2 c ‚ñ° ‚ñ° h n l s ‚ñ° ‚ñ° d 3 1 2 9 2 c h n l s However, if the epipoles are at infinite (no overlapping observation), the variance matching is poor (under-determined system has inifinitely many solutions normally), and a reference pixel is falsely matched resulting in false probabilities distribution: Multiple depth values are plausible and gain similar probabilities. The correct warping correspond to the correct depth, and vice versa.\n(2023-12-20) 3D CNN is a classifier to find the most possible depth value among 192 hypotheses based on the input variance, which is produced from a 2D CNN.\nTherefore, the job of the 2D CNN is to make the features cooresponding to the correct depth having the biggest variance. Such that the aggregated variance will still be the most salient after softmax. And finally, it takes the highest weight.\n(2023-12-20) If the above analysis that the homography is equivalent to epipolar geometry for building variance vectors is true, then the reference pixel should find the matched pixel on the epipolar line projected from the correct depth.\nThe source feature map is warped differently according to various depth values. The feature vectors at a common location of all the input images differs. Their variance is the matching cost for a hypothetical depth. Since a feature vector indicates the context around a pixel, the high variance means the behind pixels are not similar. So they should not be matched. But they use softmax to identify the highest variance.\ndoubt: I still believe the variance should be minimized at the correct depth. ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠cost-volumeÁöÑÊ¶ÇÂøµÂÖ∑‰ΩìÊåá‰ªÄ‰πàÔºü - Áü•‰πé\n(2024-02-19) The PointMVSNet paper said MVSNet used \u0026ldquo;soft argmin\u0026rdquo; in sec3.1.\nCost Volume A cost volume ùêï·µ¢ corresponding to a source view\u0026rsquo;s feature map ùêÖ·µ¢ constitutes multiple warped feature maps ùêï·µ¢(d), which can be obtained by specifying a depth and a [ùêë|ùê≠] that transforms the reference camera to the source view camera.\nAs shown below, a cost volume for a source feature map contains 192 warped feature maps at corresponding depths, while the cost volume for the reference feature map is a replication of itself 192 times.\nF o M S s f z N R F ùêÖ e f a a o e o e e ‚ÇÅ a p m u a a r f a t r p p r t x m t r e e l c i a i e m f D d e e ùêï s l m m p a e d ·µ¢ g a l p v p g ( o v ' p i i t r d f e s c ùêÖ e ùêä h i ) c a ‚ÇÅ w ·µ¢ : d R t s [ e o ùêë 1 ‚ñ¶ f r ·µ¢ | c o ùê≠ a f ·µ¢ 2 ‚ñ¶ m ] e p ( r l ùêä a a ‚ÇÅ 3 ‚ñ¶ n [ e ùêë s ‚ÇÅ ‚ãØ | : t : ùê≠ 1 ( o ( ‚ÇÅ 9 C C w C C ] 2 o = a o = ) s 3 r s 3 ‚Åª t 2 d t 2 ¬π , s , V V o N c o N l _ a l _ u d m u d m e e m e e p r e p t a t o h o h f = f = 1 1 v 9 R 9 i 2 e 2 e , f , w H v H i , i , W e W ) w ) A feature map has 32 channels, so a cost volume is a 4-D tensor: (C=32, D=192, H=128, W=160).\n(2023-12-12) Homography isn\u0026rsquo;t used to map the pixel of reference-view feature map to source-view feature map, but instead warp the source feat to reference feat via sampling cooresponding matching points.\n(2024-04-28)\nWhen searching \u0026ldquo;Ê∑±Â∫¶\u0026rdquo; in the QQ group, I found the comment of AURORA on mvsnet on 23/10/08: ‚ÄúÔºàMVSNetÂè™Âú® DTU Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁöÑÔºå‰∏∫Âï•Âú®ÂÖ∂‰ªñÊï∞ÊçÆÈõÜ‰∏ä‰πüÊúâÊïàÊûúÔºâÂõ†‰∏∫ costvolume Áõ∏ÂΩì‰∫éËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™ feature extractor‚Äù\nI think the inputs are already feature maps. The network gotten trained is the 3D UNet, that maps variance volume to probability volume.\nEven if the 3D UNet is really a \u0026ldquo;feature extractor\u0026rdquo;, the feature is extracted from the variance volume.\nCode Understand A complete demo of homography warping: gist\nAccording to the principle of homography, pixels on the reference view\u0026rsquo;s feature map are mapped linearly onto each source view. Thus, a warped source feature map is sampled (F.grid_sample) from the original source feature map at the mapped pixels.\nConstruct coordinates of pixels:\n1 2 3 4 h, w = 128, 160 vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed uv1 = torch.cat([uv, torch.ones(len(uv), 1)], dim=-1) # (hw,3) Map coordinates on the reference image to a source image.\nThe mapping matrix is ùêä·µ¢[ùêë·µ¢|ùê≠·µ¢] (ùêä‚ÇÅ[ùêë‚ÇÅ|ùê≠‚ÇÅ])‚Åª¬π. Therefore, each view calculates its own proj ùêä·µ¢[ùêë·µ¢|ùê≠·µ¢] in advance.\nUse $[\\^ùêë|\\^ùê≠]$ to represent the mapping for a pixel from the reference image to a source image, so that the pixel-wise (u,v,1) and depth-wise (d) matmul is:\n$$ \\begin{bmatrix} u\u0026rsquo; d\u0026rsquo; \\\\ v\u0026rsquo; d\u0026rsquo; \\\\ d\u0026rsquo; \\\\ 1 \\end{bmatrix}= \\begin{bmatrix} \\^ùêë \u0026amp;| \\^ùê≠ \\\\ 0 \u0026amp;| 1 \\end{bmatrix} \\begin{bmatrix} ud \\\\ vd \\\\ d \\\\ 1 \\end{bmatrix} = \\^ùêë_{3√ó3} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}‚ãÖd + \\begin{bmatrix} t‚ÇÅ \\\\ t‚ÇÇ \\\\ t‚ÇÉ \\\\ 1 \\end{bmatrix} $$\nwhere [ud, vd, d,1]·µÄ is homogeneous coordinates for translation, and [u, v, 1]·µÄ is homogeneous coordinates for perspective division to present a 3D scene on a 2D image.\nThe translation $\\^ùê≠$ is not affected by d, so it\u0026rsquo;s separated.issue 10\n1 2 3 4 5 6 7 8 src_KRt = intrisics @ extrinsics[:3,:4] proj = src_KRt @ torch.inverse(ref_KRt) rot, trans = torch.split(proj, [3,1], dim=-1) # (4,3), (4,1) rot_uv1 = rot[:3] @ uv1.t() # (3, hw) # depth_values: d = 425. + 1.06*2.5* torch.arange(192).view(1,-1,1) # (1,192,1) rot_uvd = rot_uv1.unsqueeze(1).expand(3,192,-1) *d # (3,192,hw) pix_proj = rot_uvd + trans[:3].unsqueeze(1).expand(3,192,1) The above procedures of determining projection locations are not involving learnable feature vectors. Thus, with torch.no_grad(): runtime context isn\u0026rsquo;t actually effective. And the \u0026ldquo;differentiable\u0026rdquo; is achieved by F.grid_sample(). Sample the source feature map at the mapped coordinates\n1 2 3 4 u_src = 2*(pix_proj[0] / pix_proj[2]) / (w-1) - 1 # (192, hw) v_src = 2*(pix_proj[1] / pix_proj[2]) / (h-1) - 1 uv_src = torch.stack([u_src, v_src], dim=-1) # (192, hw, 2) warped_feat = F.grid_sample(src_feat, uv_src.view(bs, 192*h, w, 2)) Merge all feature maps\u0026rsquo; cost volume into their variance:\n$$ \\begin{aligned} \\frac{ ‚àë_{i=1}^N (ùêï·µ¢ - \\bar{ùêï})¬≤ }{N} \u0026amp;= \\frac{‚àë_{i=1}^N (ùêï·µ¢¬≤ - 2ùêï·µ¢ \\bar{ùêï} + \\bar{ùêï}¬≤ ) }{N} \\\\ \u0026amp;= \\frac{‚àë_{i=1}^N ùêï·µ¢¬≤}{N} - \\frac{‚àë_{i=1}^N 2ùêï·µ¢ \\bar{ùêï} }{N} + \\frac{‚àë_{i=1}^N \\bar{ùêï}¬≤ }{N} \\\\ \u0026amp;= \\frac{‚àë_{i=1}^N ùêï·µ¢¬≤}{N} - \\frac{ 2\\bar{ùêï} ‚àë_{i=1}^N ùêï·µ¢ }{N} + \\frac{N \\bar{ùêï}¬≤}{N} \\\\ \u0026amp;= \\frac{‚àë_{i=1}^N ùêï·µ¢¬≤}{N} - \\bar{ùêï}¬≤ \\end{aligned} $$\n1 c = volume_sq_sum / n_src - (volume_sum / n_src)**2 Compress the 3D 32-channel volume variance to 1-channel scalar for each depth through a 3D UNet:\n3 2 ‚Üí 8 ‚Üì 1 6 ‚Üí 1 ‚Üì 3 6 2 ‚Üí 3 ‚Üì 6 2 4 ‚úö ‚úö + ‚Üí 3 6 2 ‚Üë 4 = 3 1 ‚Üë 2 6 = 1 8 ‚Üë 6 = 8 ‚Üí 1 Note: The original UNet concats the feature maps on the same level, whereas here the feature vectors are added up, like skip connections in restnet. Then, the logits will be normalized by softmax to become a vector of probabilities.\nThe depth of one pixel is a weighted sum (expectation) of 192 depth values.\nA d d e e p p p i t t x h h e s l = : ' s p 4 1 2 5 ‚úö p 4 2 2 7 ‚úö . 6 p 3 4 3 ‚úö 0 ‚ãØ ‚úö p 9 1 3 9 1 2 . 1 Kind of like \u0026ldquo;alpha compositing\u0026rdquo;, where the opacity corresponds to the probability and the color of each filter is the depth value here.\nOr it can be interpreted as a linear interpolation for 192 depth values.\nEach pixel has a distinct depth-probability distribution derived from feature maps.\nPhotometric consistency\nprob_volume ‚Üí prob_volume_sum4: 4 * average of every 4 depths\u0026rsquo; prob ‚Üí retrieve the \u0026ldquo;4 times average prob\u0026rdquo; from prob_volume_sum4 according to the depth_index, which is a weighted sum (expectation) of 0 ~ 192 using the prob_volume.\n1 9 2 p r A T o ‚ãØ v i b g m e e s v e 4 r . * y [ 0 4 p s - ; r u 1 o m 9 b 4 2 ] D e p t h p R e e ‚ãØ p r t l r a p i n i e e x v e e i l n d e x P c h o o n t f o i m d e e t n r c i e c The quality of depth estimation is measured by the sum of 4 nearby probabilities for a predicted depth. If this probability sum is high, the estimation is reliable. Refine Network\nd ‚äï e p r ‚Üì 4 t g h b 3 2 3 2 3 2 1 ‚úö = 1 T\u0026amp;T dataset doesn\u0026rsquo;t provide normal information to generate mesh surface, and render images for each viewpoint, so the depth map cannot be finetuned by the reference image. Point Cloud Filter Depth Map based on photometric (probability map\u0026gt;0.8) and geometric (re-projection error less than 1 pixel) consistencies\nr ‚ñ° e f ‚ñ° s r c Fuse multi-view depth maps based on visibility, and then unproject the unified depth map.\nPlay Profiler The training is slow, almost as slow as NeRF. Why?\nThere is a profile() function provided.\n(2023-12-09)\nDTU The preprocessed DTU includes 79 training scans (scenes) and 22 testing scans. Each scan has the same 49 camera poses with 7 different light conditions.\nTherefore, each scan has 79√ó49√ó7 = 27097 pictures serving as reference images. Each reference image is assigned with 10 images taken with nearest poses under the same light condition.\nDuring training, 3 source images are used to estimate the depth map of the corresponding reference image, while 5 source images are used during testing.\nThe depth range is determined based on the depth_min (425.0) and depth_interval * interval_scale (2.5 √ó 1.06) between two adjacent fronto-parallel planes. For example, if there are 192 planes, the max depth is 425 + (2.5 √ó 1.06) √ó (192-1) = 931.15\n1 2 |------|------|------ ... ---| depth: 425.0 427.65 430.3 931.1488 Code warps the source feat map in a backward way by reversing the target coordinates back to the source feat map and then sampling, rather than computing the target coordinates directly from the homography represented with planes.\nThe pixel transferring from the (target) reference plane to a source plane is performed by 3 steps: rotate first, then assign depths, finally add translation.\n1 2 rot@ *d +t (u,v,1) -\u0026gt; (u\u0026#39;,v\u0026#39;,w\u0026#39;) -\u0026gt; (u\u0026#39;d, v\u0026#39;d, w\u0026#39;d) -\u0026gt; (u\u0026#39;d, v\u0026#39;d, w\u0026#39;d) + t $$ \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} ‚Üí \\begin{bmatrix} u\u0026rsquo; \\\\ v\u0026rsquo; \\\\ w\u0026rsquo; \\end{bmatrix} ‚Üí \\begin{bmatrix} u\u0026rsquo;d \\\\ v\u0026rsquo;d \\\\ w\u0026rsquo;d \\end{bmatrix} ‚Üí \\begin{bmatrix} u\u0026rsquo;d+t \\\\ v\u0026rsquo;d+t \\\\ w\u0026rsquo;d+t \\end{bmatrix} ‚Üí \\begin{bmatrix} \\frac{u\u0026rsquo;d+t}{w\u0026rsquo;d+t} \\\\ \\frac{v\u0026rsquo;d+t}{w\u0026rsquo;d+t} \\\\ 1 \\end{bmatrix} $$\nRefer to eq.(11) in Multi-View Stereo‰∏≠ÁöÑÂπ≥Èù¢Êâ´Êèè(plane sweep) - ewrfcasÁöÑÊñáÁ´† - Áü•‰πé\n$$ R $$\nRead GT Depth (2023-12-19)\nFind the ground-truth depth map for an input image (512, 640).\nThe preprocessced DTU (\u0026ldquo;dtu_training.rar\u0026rdquo;) only contains 1/4 depth maps with size 128x160, aligned with the size of feature maps. So, each file (\u0026ldquo;DTU_Depths_raw/scan1_train/depth_map_0000.pfm\u0026rdquo;) is 80K.\nWhile the folder \u0026ldquo;DTU_Depths_raw/\u0026rdquo; (\u0026ldquo;Depth_raw.zip\u0026rdquo; used in MVSNeRF) includes full-size pfm with the same resolution 1200x1600 as the DTU images. An example: \u0026ldquo;DTU_Depths_raw/scan1/depth_map_0000.pfm\u0026rdquo;: 7.32M\nRead depth map with function read_pfm():\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import imageio import matplotlib.pyplot as plt from pathlib import Path from datasets.data_io import read_pfm depth_path = Path(\u0026#34;../DTU_Depths_raw/scan4_train/depth_map_0008.pfm\u0026#34;) depth_pfm, scale = read_pfm(depth_path) # (128,160), 1.0 depth_png = imageio.imread(\u0026#34;../DTU_Depths_raw/scan4_train/depth_visual_0008.png\u0026#34;) orig_rgb = imageio.imread(\u0026#34;../mvs_training/dtu/Rectified/scan4_train/rect_009_3_r5000.png\u0026#34;) orig_rgb = orig_rgb[::4,::4,:] # reduce 4 times q_x, q_y = int(190/4), int(125/4) # query point plt.text(x=q_x, y=q_y, s=f\u0026#39;{depth_pfm[q_x, q_y]:.2f}\u0026#39;) plt.vlines(x=q_x, ymin=q_y-12.5, ymax=q_y+12.5) plt.hlines(y=q_y, xmin=q_x-12.5, xmax=q_x+12.5) plt.imshow(depth_pfm) # plt.imshow(depth_png, alpha=1) plt.imshow(orig_rgb, alpha=0.2) plt.title(str(depth_path).split(\u0026#39;/\u0026#39;)[2:]) ### LLFF\nAnalyses:\nWhat\u0026rsquo;s the key of its generalizability?\nEnvironment (2024-02-21)\nReferring to this issue#101 and issue#149\n1 2 conda create --name MVSNet python=3.7 cudatoolkit=9.0 cudnn=7.6.4 -c conda-forge pip install -r requirements.txt # tf==1.15.0 Download pretrained model: 1 gdown 1-1JyFT9ClqPO0kz0d_5I1_IHX05paS4h Validate:\n1 2 3 4 python mvsnet/validate.py --regularization \u0026#39;3DCNNs\u0026#39; --validate_set dtu --max_w 640 --max_h 512 --max_d 128 \\ --pretrained_model_ckpt_path \u0026#39;tf_model_dtu/3DCNNs/model.ckpt-150000.data-00000-of-00001\u0026#39; \\ --dtu_data_root \u0026#39;~/Downloads/mvs_training/dtu\u0026#39; \\ --validation_result_path \u0026#39;validation\u0026#39; Related Mutli-view stereoÂ§öË¶ñËßíÁ´ãÈ´îÈáçÂª∫ÊäÄË°ì‰ªãÁ¥π - AIËëµ\nMVSNetÁ≥ªÂàó - MEGVII ÈªÑÁôæÂ∑ù\n„Äê‰ª£Á†ÅÁ≤æËØª„ÄëÂºÄÂ±±‰πã‰ΩúMVSNet PyTorchÁâàÊú¨Ë∂ÖËØ¶ÁªÜÂàÜÊûê - doubleZÁöÑÊñáÁ´† - Áü•‰πé\n„ÄêËÆ∫ÊñáÁ≤æËØª5„ÄëMVSNetÁ≥ªÂàóËÆ∫ÊñáËØ¶Ëß£-Point-MVSNet - CSDNÂçöÂÆ¢\nÂ§öËßÜÂõæÂá†‰Ωï‰∏âÁª¥ÈáçÂª∫ÂÆûÊàòÁ≥ªÂàó- Cascade-MVSNet - ÂìîÂì©ÂìîÂì©\nFollow-up 2019-2020. Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏âÁª¥ÈáçÂª∫‚Äî‚ÄîMVSNetÁ≥ªÂàóËÆ∫ÊñáËß£ËØª - Êú∫Âô®‰∫∫3DÊÑüÁü•ÁöÑÊñáÁ´† - Áü•‰πé\n","date":"2023-12-03T00:49:00Z","image":"https://ar5iv.labs.arxiv.org/html/1804.02505/assets/x1.png","permalink":"https://zichen34.github.io/writenotes/model/mvs/b-note-mvsnet/","title":"read: Depth - MVG | MVSNet"},{"content":" Êç¢ÂÖÉÊ≥ï The area of a cube in orthogonal coordinate system and the spherical coordinate system.\nÁõ¥ËßíÂùêÊ†á‰∏éÊûÅÂùêÊ†áÁöÑ‰∫íÂåñ‰∏≠Ôºå‰∏∫‰ªÄ‰πà dxdy=rdrdŒ∏Ôºü - ‰∫à‰∏Ä‰∫∫ÁöÑÂõûÁ≠î - Áü•‰πé\nÁ´ã‰ΩìËßí\nÂ¶Ç‰ΩïÊé®ÂØºÊü±ÂùêÊ†áÔºåÁêÉÂùêÊ†áÁöÑ‰ΩìÁßØÂÖÉÂíåÊûÅÂùêÊ†áÁöÑÈù¢ÁßØÂÖÉÔºü - Âçä‰∏™ÂÜØÂçöÂ£´ÁöÑÂõûÁ≠î - Áü•‰πé\nChange of variables | MIT 18.02SC Multivariable Calculus, Fall 2010\nChange of variables (single integral and substitution) | Lecture 30 | Vector Calculus for Engineers\nChange of Variables and the Jacobian - Serpentine Integral\nAn infinitesimal area varies in different coordinate system because shapes of a unit area are distinct. In other words, different coordinate systems have varous scales.\nIn polar coordinate system:\n$$ x = r‚ãÖcosŒ∏ \\\\ y = r‚ãÖsinŒ∏ $$\nJacobian matrix is a function that depicts how much each dimension of the source space should be scaled to align with another coordinate system at an arbitrary position.\nThe scale factor is the ratio between a target dimension and a source dimension. Thus, by multiplying those factors, the source space will be scaled to the target space.\n$$ ùêâ = \\begin{bmatrix} \\frac{dx}{dr} \u0026amp; \\frac{dx}{dŒ∏} \\\\ \\\\ \\frac{dy}{dr} \u0026amp; \\frac{dy}{dŒ∏} \\end{bmatrix} $$\nBecause an infinitesimal area $dx‚ãÖdy$ only has magnitude (without direction), the area scaling factor should be just a positive real number.\nTo make the area in the source space to be the same in the target space, the scaling factor should be the ratio of two unit areas.\n$$ \\frac{d(x,y)}{d(r,Œ∏)} $$\nThat is $f(x,y) = g(r,Œ∏) dx dy$, so the integrated areas equals:\n$$ ‚à¨_D f(x,y) dx dy = ‚à¨_E g(r,Œ∏) dx dy = ‚à¨_E g(r,Œ∏) ‚àÇ(x,y)/‚àÇ(r,Œ∏) drdŒ∏ $$\nAnd the calculation of an area is just the cumulative product of every dimensions.\nTherefore, the abosolute value of the determinant of the Jacobian matrix is taken.\n$$ |det(ùêâ)| = |dx/dr * dy/dŒ∏ - dx/dŒ∏ * dy/dr | = |cosŒ∏rcosŒ∏ - rsinŒ∏(-sinŒ∏)| = |r| $$\n11.9: Change of Variables - Mathematics LibreTexts\n(2023-11-28)\nThe determinant of Jacobian matrix is responsible for the infinitesimal area, not the integrand.\nThe new integrand is just substituting the old variable with the tranformation with new variable.\nf(x,y) -\u0026gt; g(rcosŒ∏, rsinŒ∏)\ndxdy -\u0026gt; r drdŒ∏\nf(x,y) = g(rcosŒ∏, rsinŒ∏) * r\nThe change of variables theorem has two aspects:\nf ( x , y ) C M h u a l n t g i i p n l g y v | a d r e i t a ( b J l ) e | g ( r , Œ∏ ) ","date":"2023-11-27T19:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/change_of_variables/","title":"memo: Calc | Change of Variables Theorem"},{"content":"Source video: Continuous time convolution with impulses - ProfKathleen\nConvolution For example, there is a man keep eating all the time. $f(t)$ is the food eaten at time t, and $h(t)$ is the food mass changing function for a period of digestion time t. For example, after the duration $(t-œÑ)$ of digestion, the amount of food eaten at time $œÑ$ becomes $h(t-œÑ)$.\nTherefore, the total food reamined in stomach at time t is f convolving with h:\n$$ y(t) = f(t) * h(t) = ‚à´_{-‚àû}^{+‚àû}f(œÑ)‚ãÖh(t-œÑ) dœÑ $$\nIntuitively, the value $x(œÑ)$ is scaled by a factor from œÑ time ago. Thus, the function $h(t)$ looks like in reverse.\nGiven $f = x(t)$ and $h = Œ¥(t)$, the existing amount at time t is:\n$$ y(t) = f * h = ‚à´_{-‚àû}^{+‚àû} x(œÑ) h(t-œÑ) dœÑ $$\nDelta function $Œ¥(œÑ)$:\n0 1 œÑ $Œ¥(œÑ-3)$: Right shift $Œ¥(œÑ)$ by 3.\nWhere the 0 was becomes -3. Therefore, all the coordinates minus 3, i.e., œÑ becomes œÑ-3.\n0 3 1 œÑ $Œ¥(-œÑ-3)$: Rverse $Œ¥(œÑ-3)$:\n- 3 1 0 œÑ $Œ¥(t-œÑ-3)$: Shift $Œ¥(œÑ-3)$ to t, which is a constant.\nWhere the œÑ-3 = 0 becomes t, so all coordinate plus t:\nt - 3 1 t œÑ Convolve with Impulse Given $x(t)$\nt The existing amount of a system containing two functions x(t) and Œ¥(t-3) at time t is:\n$$ y(t) = x(t) * Œ¥(t-3) = ‚à´_{-‚àû}^{+‚àû} x(œÑ) Œ¥(t-œÑ-3) dœÑ $$\nMultiply x(œÑ) by the reversed impluse:\nt - 3 t œÑ Delta function is 0 except for œÑ = t-3, therefore, only x(t-3) will be computed:\n$$ y(t) = x(t) * Œ¥(t-3) = ‚à´_{-‚àû}^{+‚àû} x(t-3) Œ¥(t-œÑ-3) dœÑ $$\nAnd in that integral, x(t-3) has nothing to do with œÑ, so it can be pulled outside the integral:\n$$ y(t) = x(t) * Œ¥(t-3) = x(t-3) ‚à´_{-‚àû}^{+‚àû} Œ¥(t-œÑ-3) dœÑ \\\\ = x(t-3)*1 \\\\ = x(t-3) $$\nBy convolving with an impulse function, x(t) is shifted (based on the origin) to where the impulse is.\nThis conclusion can be generalized to any f.\nBox function 0 1 f ( t ) 3 t ‚ú∂ 0 1 Œ¥ ( t - 1 ) t = 0 1 y 2 ( t ) 4 t Two impulses Convolution is linear. Compute separately and sum together.\n0 1 f ( 1 t ) 3 t ‚ú∂ 0 1 1 Œ¥ ( t - 1 ) 1 4 / 2 t = 1 2 1 y ( 4 t ) 1 6 / 2 t Impulse train Convolving with a infinite sum of delta function: $Œ£_{n=-‚àû}^{+‚àû} Œ¥(t-n)$\nThe replicas of the signal overlaps:\n0 0 1 1 f y 2 ( 2 ( t t 3 ) 3 ) t t ‚ú∂ s u m - 1 3 2 0 0 Œ¥ 1 ( t 1 2 - 1 2 y 3 ) ( 3 t ) t = t ","date":"2023-11-20T11:40:00Z","permalink":"https://zichen34.github.io/writenotes/calc/convol_impulses/","title":"memo: Calc | Convolution with Impulses"},{"content":"(Feature image from 3D Gaussian Splatting‰∏≠ÁöÑÊï∞Â≠¶Êé®ÂØº - ÂÖ´Ê∞®ÂêàÊ∞ØÂåñÈíôÁöÑÊñáÁ´† - Áü•‰πé)\nEWA Splatting - UMD\n(2023-11-20)\nAbs \u0026amp; Intro (2023-11-29)\nThis work combines elliptical splat kernels with a low-pass filter to avoid aliasing without introducing much blurring.\nFootprint function is an integral across a 3D reconstruction kernel along the line of sight, so it computes the contribution of an individual 3D kernel to a screen point. Those contributions form a splat, which can be precomputed, thus improving efficiency of volume rendering.\nTwo types of volume rendering approaches can be distinguished based on the availability of volume data.\nR ( a b S p y a a o c m i C k p n a w l t p s a e s i t r B x i d a e n ) c l g k ( f a r ) B V d a o a c l t k u a m S ( e p f l o a r p t w r t a o i r j n d e g ) c t S ‚¨Ø ‚¨Ø c ‚¨Ø ‚¨Ø r e F e r n o n t In NeRF, each sampled point is queried out from a MLP (smpling an implicit field), and then a pixel is synthesized by alpha-compositing along the viewing ray. Thus, there isn\u0026rsquo;t physical volume data in NeRF.\nIn splatting, each volume data (as a continuous distribution) is projected onto the screen perspectively yielding various splats (Êª©Ê∂Ç) before rendering (i.e., A pixel emits a ray). Splats don\u0026rsquo;t result in pixels directly as they still need to do combination (to produce the final weight of the 3D point corresponding to the pixel).\nWhen rendering from an arbitrary viewpoint, each splat is integrated along the viewing ray, and then the expectation of all splats\u0026rsquo; contributions is calculated, forming a function that maps a position to an rendered attribute (color).\n(2024-01-21) Their comparison is somewhat like that of a CPU and GPU:\nTwo ways of rendering a point dataset:\nPoints ‚ûî triangle meshes ‚ûî \u0026ldquo;surface rendering\u0026rdquo; ‚ûî image Points ‚ûî \u0026ldquo;splatting\u0026rdquo; ‚ûî image P o i n t s T l r a i t a i n o g n u S p l a m t e t s i h n e g s R e d u c t i o n I m a g e Gaussian is a universal rendering primitive for volume splatting and surface splatting.\nPrevious Work Slice-based volume rendering\nProgressive\nfor an arbitrary position in 3D, the contributions from all kernels must sum up to one.\n(2023-11-22)\nSplatting A point cloud (data set) is a discrete sampling of a continuous scene function.\nIf a point is considered as a continuous function, then the point cloud corresponds to a continuous representation on the 2-D screen space corresponding to the 3D scene.\nThus, an image (pixel grid) reflecting the scene comes from sampling the intemediate reconstructed continuous screen, instead of directly sampling the scene, that is, \u0026ldquo;resampling\u0026rdquo; (sampling again).\n(2024-03-16) After reading the code, I figured out the splatting and alpha compositing (forward) are 2 separate procedure. The point cloud is a sample of the scene. By performing splatting, each point is converted into a disc in the (viewing-) ray space. Such than, the opacities of each Gaussian have been determined. When rendering images, given a camera, each pixel emits a ray, and then the sampling points on a ray query their opacities from the ray sapce, to perform alpha compositing. In this way, an image is formed. Therefore, an image is a sampling of the ray space. By considering each point in dataset as a reconstruction kernel, which is a continuous attribute-weight function (e.g., opacity: the weight of color) in the object space, the continuous scene function can be constructed by aggregating all kernels.\nFormally, the weight assigned to a 3D point ùêÆ in the object space for a certain attribute (e.g., color) $f_c(ùêÆ)$ is a mixture (weighted sum) of weights calculated from each reconstruction kernel:\n$$f_c(ùêÆ) = ‚àë_{k‚ààIN} w‚Çñ r‚Çñ(ùêÆ)$$\n$r‚Çñ$ is a reconstruction kernel centered at a data point $ùêÆ‚Çñ$. A kernel is a function mapping a position to a weight (opacity) for a property (color).\nIn this work, r‚Çñ is chosen to be a 3D Gaussian whose mean vector is the position of a data point ùêÆ‚Çñ, and\u0026hellip; what\u0026rsquo;s the variance?\n$r‚Çñ(ùêÆ)$ is the \u0026ldquo;weight of an attribute\u0026rdquo; (opacity of a color) of an arbitrary voxel ùêÆ inside the kernel $r‚Çñ$.\nA kernel center ùêÆ‚Çñ will perform transformations as follows:\n‚åà ‚åä P e c x x r c e ‚ÇÄ ‚ÇÅ o t n ‚åâ ‚åã S j i t c - o e ‚¨Ø r n r e e ‚òÖ n ` F p i ‚ãÖ o r n o i t x ` t n e ‚ÇÇ - t g ‚ãÖ r a ‚¨Ø l L f o i g w l - t ‚äó p e a r h s s ‚åà | ‚åä R s ( s x x x ùê± a p N c ‚ÇÄ ‚ÇÅ ‚ÇÇ ‚Çñ y a D a ‚åâ | ‚åã c C l e i w n ùê≠ œï t / g + r o ) ùêâ p a ‚ãÖ r n ‚àÜ o s ùê≠ j f ‚åà | ‚åä C s ( t t t ùê≠ a p P ‚ÇÄ ‚ÇÅ ‚ÇÇ ‚Çñ m a e ‚åâ | ‚åã e c r r e s a p ùêñ œÜ t ( e r E c ùêÆ v a y t + i n e i ùêù e s ) v w f e ) ‚åà | ‚åä u u u O s ‚ÇÄ ‚ÇÅ ‚ÇÇ ùêÆ b p ‚åâ | ‚åã ‚Çñ j a e c c e t Prime $\u0026rsquo;$ indicates the ray space.\nx‚ÇÇ is the Euclidean distance (L2 norm) from the 3D point ùê± to the projection center for integrating the total opacity given by a kernel.\nRay space is a \u0026ldquo;plane\u0026rdquo; because the x‚ÇÇ is not a practical depth anymore and only used for object culling. And the screen is just a footprint table:\nThus essentially, ray space is the projection plane, while screen is another space recording the contributions of a 3D kernel to each 2D point.\nRoughly, the main focus on the ray space is on the near plane. For example, the pixel covered by a kernel projection is determined on the near plane.\n(2023-11-24)\nResampling \u0026ldquo;Ideal sampling\u0026rdquo; means no aliasing and the original continuous signal can be reconstucted from the sampled signal exactly.\nAntialiasing Spatial signal (Screen function) ‚ûî Samples (Image) ‚ûî Frequency response (Coefficient of each basis) given by Fourier Transform.\n0 f ‚ÇÅ f ‚ÇÇ f ‚ÇÉ T f ‚Çô x 2 œÄ ‚ÅÑ T ‚ãÆ w Sampling in time (or spatial) domain: the continuous signal is multiplied by an impulse train with a period T:\n$$ y(t) = x(t) ‚ãÖ ‚àë_{k=-‚àû}^{‚àû} Œ¥(t-kT) = ‚àë_{k=-‚àû}^{‚àû} x(t) ‚ãÖ Œ¥(t-kT) $$\nMultiplication in time domain corresponds to convolution ‚äó in frequency domain.\n$$ \\begin{aligned} Y(œâ)\u0026amp;= \\frac{1}{2œÄ} \\left(X(œâ) ‚äó \\frac{2œÄ}{T} ‚àë_{k=-‚àû}^{‚àû} Œ¥(œâ-k \\frac{2œÄ}{T}) \\right)\\\\ \u0026amp;= \\frac{1}{T} ‚à´_{-‚àû}^{‚àû} X(Œª)‚ãÖ‚àë_{k=-‚àû}^{‚àû} Œ¥(œâ-k \\frac{2œÄ}{T}-Œª) dŒª \\\\ \u0026amp;= \\frac{1}{T} ‚àë_{k=-‚àû}^{‚àû} X(œâ-k\\frac{2œÄ}{T}) \\end{aligned} $$\nThe frequency spectrum Y(œâ) of the sampled signal is a sum of the original signal\u0026rsquo;s spectrum X(œâ) shifted according to the corresponding impulse train in the frequcy domain.\nRefer to: Fourrier transform of an impulsion train - Inria\nSampling with an impulse train: Lecture 12 - Stanford University\n$2œÄ/T$ is the frequency of the sampling impulse. So, the space between 2 spectrum replicas is $\\frac{2œÄ}{T}$.\nDenoting the highest frequency of the temporal signal as $w_m$, to fully separate 2 neighbor spectrums, the interval $2œÄ/T$, i.e., the sampling frequency $w_s$, must be larger than (no equal) $2w_m$. Otherwise, spectrum replicas will overlap and an individual intact spectrum can\u0026rsquo;t be selected out by a box function to reconstruct the temporal signal.\nConversely, given a temporal impulse train with a sampling frequency $w_s$, the highest frequency of the continuous signal to be sampled shouldn\u0026rsquo;t exceed $\\frac{w_s}{2}$, which is called the Nyquist frequency.\nTo match the Nyquist frequency of the discrete desired grid, the time-domin signal can pass a low-pass filter before sampling, in contrast to increasing sampling frequency causing more computation cost.\nMoreover, the width of the low-pass filter to be convolved is a tradeoff for computation efficiency. Thus, aliasing is inevitable.\nMip-Splatting (231127) adds a constrains to the size of kernels based on the $w_s$ induced by the input views, and changes the 2D Gaussian low-pass filter to a 2D Mip filter.\nThus, alliviate the dilation effect when zooming out and the high frquency artifacts when zooming in.\nCont. Screen The \u0026ldquo;weights-mixture\u0026rdquo; function $f_c(ùêÆ)$ determinating an attribute\u0026rsquo;s weight at an arbitrary position ùêÆ in source space are transformed as follows:\nI D m i a s g c e r e t A C M I T e l o u m r p m l p a h p t u i a o i l n s p s i l e t y 2 C e g D o _ n c S t ' c i ( r n ùê± e u ) e o I k n u n e s t r e n g e r l a s t e L f o i w l - t p e a r s s ‚¶ø ‚¶ø ‚¶ø ‚¶ø g 3 S ‚¶ø _ D p ‚¶ø c a ‚¶ø ( R c ‚¶ø ùê± a e ) y P P e r r o s j p e e c c t t i v e ‚¶ø ‚¶ø ‚¶ø 3 C ‚¶ø f D o ‚¶ø _ n ‚¶ø c s t ‚¶ø ( c . ‚¶ø ùêÆ e ) n e The coordinates ùê± in the 3D ray space contain the 2D perspectively-projected screen coordinates (t0/t2, t1/t2) and the 3rd dimension is set to the L2 norm of ùê≠, which is the coordinates in the camera space.\nùê± is also used to refer to the corresponding screen point.\nThree operations in splatting equation:\nFor splatting approach, a scene consists of reconstruction kernels spreading out the space. Thus, a volume at ùêÆ in the source space is a combination of contributions from all reconstruction kernels:\n$$ f_c(ùêÆ) = ‚àë_{k‚àà N} w‚Çñ r‚Çñ(ùêÆ) $$\nùêÆ is a position in the source (object) space; $r‚Çñ$ is a reconstruction kernel centered at ùêÆ‚Çñ. $r‚Çñ(ùêÆ)$ is the weight for an attribute stored in the location ùêÆ computed according to $r‚Çñ$; $w‚Çñ$ is the coefficient of each weight to produce a unified weight for an attribute on the location ùêÆ; An attribute-weight function $f_c$ in the source space will be finally projected onto the screen as a 2D continuous function that produces the weight for an attribute on any screen position ùê±:\n$$ g_c(ùê±) = \\{ P( f_c ) \\}(ùê±) = \\left\\{ P \\left( ‚àë_{k‚àà N} w‚Çñ r‚Çñ \\right) \\right\\}(ùê±) $$\nùê± is not a pixel, because the screen is still continuous. A discrete image is sampled from the screen via an impulse train: $g(ùê±) = g_c(ùê±) i(ùê±)$ I use curly braces to imply a function. Because a non-affine projection operation can be approximated as a linear transformation through the 1st-order Taylor expansion, the weighted sum and linear projecting operations can be flipped:\n$$ g_c(ùê±) = ‚àë_{k‚àà N} w‚Çñ‚ãÖ P( r‚Çñ) ) (ùê±) $$\nThereby, each kernel performs projection first, and then combined together. Commutative: switching operators means switching the sequence of operations. To avoid aliasing, the screen function before being sampled to an image needs to pass a low-pass filter $h$ to meet the Nyquist frequency:\n$$ \\begin{aligned} \\^g_c(ùê±) \u0026amp;= g_c(ùê±) ‚äó h(ùê±) \\\\ \u0026amp;= ‚à´_{-‚àû}^{‚àû} g_c(\\bm Œ∑)‚ãÖh(ùê±- \\bm Œ∑) d \\bm Œ∑ \\\\ \u0026amp;= ‚à´_{-‚àû}^{‚àû} ‚àë_{k‚àà N} w‚Çñ‚ãÖ P(r‚Çñ)(\\bm Œ∑)‚ãÖh(ùê±-\\bm Œ∑) d\\bm Œ∑ \\\\ \u0026amp;= ‚àë_{k‚àà N} w‚Çñ ‚à´_{-‚àû}^{‚àû} P(r‚Çñ) (\\bm Œ∑) ‚ãÖ h(ùê±-\\bm Œ∑) d\\bm Œ∑ \\\\ \u0026amp;= ‚àë_{k‚àà N} w‚Çñ ‚ãÖ P(r‚Çñ) (ùê±) ‚äó h(ùê±) \\end{aligned} $$\nEvery projected kernel (mapping a position to a weight) is filtered by h. The weighted sum is done after projection and convolution. An ideal resampling kernel: $$ œÅ‚Çñ(ùê±) = ( P(r‚Çñ) ‚äó h )(ùê±) $$ Therefore, any location in the 2D continuous screen space is a combination of the projected and filtered reconstruction kernels $œÅ‚Çñ$ evaluated at that location.\n(2023-11-25)\nRendering (In NeRF,) Using opacity (self-occlusion) $Œ±‚Çú$ to compute the color based on alpha compositing:\n$$\\rm c = ‚à´_{t=n}^f T‚Çú Œ±‚Çú c‚Çú dt, \\quad T‚Çú = ‚àè_{s=n}^{t-1} (1-Œ±‚Çõ) ds $$\nIf using volume density œÉ to compute, the pixel color is:\n$$ \\rm c = ‚à´_{t=n}^f T‚Çú (1 - e^{-œÉ‚Çú Œ¥}) c‚Çú dt, \\quad T‚Çú = e^{- ‚à´_{s=n}^{t-1} œÉ‚Çú Œ¥ ds} $$\nAlpha = $1 - e^{-œÉŒ¥}$, when the density œÉ is 0, the alpha (opacity) is 0: The filter itself doesn\u0026rsquo;t show its color. It\u0026rsquo;s as if there were no particles there, and the rays pass through without any changes.\nŒ¥ is a unit interval on the ray. (The width of a filter.)\nNeRF can use 1 pixel to observe all points in the space, while splatting use all points to composite 1 pixel.\nC a m e r N a e ‚ñ° R s F p a c e R a E y y S e ‚¶ø p s ‚¶ø - l p - a a ‚¶ø ‚¶ø t c - t e S - - - - i c ‚ùö ‚ùö ‚ùö ‚ùö ‚ñ° n r g e P e r n o j I e n c t t e i g o r n a t c e e n t e r This paper uses the low albedo approximation of the volume rendering, so that the intensity (a 1D attribute) of a screen point ùê± corresponding to a ray with length ùêø is computed as:\n$$ I(ùê±) = ‚à´_{Œæ=0}^L c(ùê±,Œæ) ‚ãÖf_c\u0026rsquo;(ùê±,Œæ) ‚ãÖ e^{-‚à´_{Œº=0}^Œæ f_c\u0026rsquo;(ùê±,Œº)dŒº}dŒæ $$\nL is the distance from eye to the projection center on screen.\n‚ñπ $c(ùê±,Œæ)$ is the intensity on a 3D point (ùê±,Œæ) in ray space. ùê± is (x0,x1), Œæ is x2.\n‚ñπ $f_c\u0026rsquo;(ùê±,Œæ)$ is the point\u0026rsquo;s opacity (weight for color) in the ray space. ‚ñπ The exponential term is the accumulated transmittance prior to the point at Œæ. Why is it in this form? Is a transmittance $e^{-f_c\u0026rsquo;(ùê±,Œæ)}$?\nK e r n ( I l s e x 3 n i i l ‚ÇÄ D t n g , e e h r x R g t ‚Çñ ‚ÇÅ a r o , ' , y a f ( Œæ t L ùêÆ ) ‚ÑÆ S e ) ·µÄ y p ‚ÑÆ a ‚¶ø c - e ‚¶ø - - - ‚¶ø - - - - - S | ‚Ä¢ ‚ùö ‚ùö ‚ùö | c ‚Üô r P ùê± e r : e o ( n j c S x e e c ‚ÇÄ c n V r , t t i e x i e e e ‚ÇÅ o r w n ) n i ·µÄ n c g o o r r a d y i n a t e s The 3D ray space is not perspective because depth has been divided and then reset to the distance from the projection center. Thus, kernels in ray space are projected onto the screen orthogonally by just omitting their depth.\nAnd the probing line L from eye to the projection center are projected orthogonally onto the screen as well.\nTherefore, integrating along L in 3D space corresponds to integrating the viewing ray between (x‚ÇÄ,x‚ÇÅ) and the projection center on 2D screen.\nRewrite the volume rendering equation using projected kernels in the ray space.\nSince the position ùê± in the ray space is the projection of position ùêÆ in the source space after a viewing transformation œÜ (w2c) and a projective transformation œï (Note: NDC= projection with depths kept + scaling. Here is NDC without scaling), there are:\n$$ \\begin{aligned} ùê± \u0026amp;= œï(œÜ(ùêÆ)) \u0026amp; \\text{Project a location to ray space} \\\\ ùêÆ \u0026amp;= œÜ‚Åª¬π(œï‚Åª¬π(ùê±)) \\\\ f_c(ùêÆ) \u0026amp;= ‚àë_{k‚ààIN} w‚Çñ r‚Çñ(ùêÆ) \u0026amp;\\text{Combination in object space} \\\\ f_c\u0026rsquo;(ùê±) \u0026amp;= ‚àë_{k‚ààIN} w‚Çñ r‚Çñ(œÜ‚Åª¬π(œï‚Åª¬π(ùê±))) \u0026amp;\\text{Change of variable} \\\\ r‚Çñ\u0026rsquo;(ùê±) \u0026amp;= r‚Çñ(œÜ‚Åª¬π(œï‚Åª¬π(ùê±))) \u0026amp; \\text{Kernel k in the ray space} \\\\ f_c\u0026rsquo;(ùê±) \u0026amp;= ‚àë_{k‚ààIN} w‚Çñ r‚Çñ\u0026rsquo;(ùê±) \u0026amp;\\text{Written in a consistent form} \\end{aligned} $$\nWeights-mixture function $f_c\u0026rsquo;(ùê±)$ is the corresponding representation of $f_c(ùêÆ)$ in the ray space, merely substituting variables without scaling the axes.\nThus, the weight for an attribute on ùê± is a summation of all projected kernels. For example, the assigned opacity at ùê± in the ray space is a weighted sum of all opacities evaluated from each kernel. (Gassian mixture model)\nSubstituting the opacity $f_c\u0026rsquo;(ùê±)$ into the rendering equation:\n$$ \\begin{aligned} I(ùê±) \u0026amp;= ‚à´_{Œæ=0}^L c(ùê±,Œæ)‚ãÖf_c\u0026rsquo;(ùê±,Œæ) ‚ãÖ e^{-‚à´_{Œº=0}^Œæ f_c\u0026rsquo;(ùê±,Œº)dŒº}dŒæ \\\\ \u0026amp;= ‚à´_{Œæ=0}^L c(ùê±,Œæ)‚ãÖ ‚àë_{k‚ààIN} w‚Çñ ‚ãÖr‚Çñ\u0026rsquo;(ùê±,Œæ) ‚ãÖ e^{-‚à´_{Œº=0}^Œæ ‚àë_{j‚ààIN} w‚±º r‚±º\u0026rsquo;(ùê±,Œº) dŒº} ‚ãÖdŒæ \\\\ \u0026amp;= ‚àë_{k‚ààIN} w‚Çñ \\left( ‚à´_{Œæ=0}^L c(ùê±,Œæ)‚ãÖ r‚Çñ\u0026rsquo;(ùê±,Œæ) ‚ãÖ e^{-‚àë_{j‚ààIN} w‚±º ‚ãÖ‚à´_{Œº=0}^Œæ r‚±º\u0026rsquo;(ùê±,Œº) dŒº} ‚ãÖdŒæ \\right) \\\\ \u0026amp;= ‚àë_{k‚ààIN} w‚Çñ \\left( ‚à´_{Œæ=0}^L c(ùê±,Œæ)‚ãÖ r‚Çñ\u0026rsquo;(ùê±,Œæ) ‚ãÖ ‚àè_{j‚ààIN} e^{- w‚±º‚ãÖ‚à´_{Œº=0}^Œæ r‚±º\u0026rsquo;(ùê±,Œº) dŒº} ‚ãÖdŒæ \\right) \\end{aligned} $$\nA point (ùê±,Œæ) in ray space indicates the 3rd dim (x‚ÇÇ) is Œæ and the first 2 dims (x‚ÇÄ,x‚ÇÅ) is ùê±.\nThe rendered color is a composition of each point\u0026rsquo;s color c(ùê±,Œæ) multiplied by a weight given by the weights-mixture function $f_c\u0026rsquo;$ at the point on the line of sight ùêø. And the weights-mixture function $f_c\u0026rsquo;$ is a combination of the evaluations of all kernels $r‚Çñ$ in the ray space.\nCommutative: Sum all kernels evaluated at a point followed by integrating all points on a ray = integrate all the points in each kernel followed by summation for all kernels.\nThe integral for the evaluations at all ray points, considering r‚Çñ as the kernel to be evaluated: $q‚Çñ(ùê±) = ‚à´‚ÇÄ·¥∏ r‚Çñ\u0026rsquo;(ùê±,Œæ)dŒæ$ is called the footprint function of r‚Çñ, which is used as the approximate opacity for all voxels in the support of a kernel.\nAnd the exponential product term can be approximated as $‚àè‚±º·µè‚Åª¬π(1-w‚±ºq‚±º(ùê±))$ by applying Taylor expansion (see below), such that it can be regarded as the accumulated transmittance very harmoniously.\n(2023-12-31) In this way, the rendering equation aligns with the alpha compositing:\nequation opacity transmittance volume rendering Œ± (1-Œ±) splatting q‚Çñ (1-w‚Çñq‚Çñ) A projected rendered attribute in the screen space, such as the above $I(ùê±)$, is collectively referred to as $g_c(ùê±)$.\nApproximations To simplify the computation of $g_c(ùê±)$, 4 approximations are applied.\nLocal support: the regime of each 3D kernel is range-limited and not overlapped with other kernels on the ray.\nlocal support\nL s 3 i i D N n g k o e h e R t r a O o , n y v f L e l s e l S o u r s E p c p l y a a p a e c l o p ‚¶ø e r t ‚¶ø S - c ‚Ä¢ ‚ùö ‚ùö ‚ùö ‚ùö ‚ùö ‚ùö r e P c E e r e x n o n t j t e e e n c r t t i o n The attribute (e.g., color) in the local support region of a 3D kernel along a ray is assumed to be constant. Specifically, the $c(ùê±,Œæ)$ over the line of sight (for all Œæ) is constant, while other rays (i.e., ùê±) passing through the kernel can have different colors. Hence, c(ùê±,Œæ) can be put outside of the ray integral.\n$$g_c(ùê±) = ‚àë_{k‚ààIN} w‚Çñ c‚Çñ(ùê±) \\left( ‚à´_{Œæ=0}^L r‚Çñ\u0026rsquo;(ùê±,Œæ) ‚ãÖ ‚àè_{j‚ààIN} e^{- w‚±º‚ãÖ‚à´_{Œº=0}^Œæ r‚±º\u0026rsquo;(ùê±,Œº) dŒº} ‚ãÖdŒæ \\right)$$\nAssume the attenuation factor of voxels in a kernel along a ray is constant, i.e., the transmittance isn\u0026rsquo;t dependent on previous voxels passed through by the ray, but equals the accumulation of all voxels. Thus, the upper bound of the integral becomes L:\n$$\\rm exp(-‚à´_{Œº=0}^{Œæ} f_c\u0026rsquo;(ùê±, Œº) dŒº) ‚Üí exp(-‚à´_{Œº=0}^{L} f_c\u0026rsquo;(ùê±, Œº) dŒº)$$\nIn this way, the transmittance isn\u0026rsquo;t restricted by Œæ of the outer rendering integral and can be pulled outside.\n$$g_c(ùê±) = ‚àë_{k‚ààIN} w‚Çñ c‚Çñ(ùê±) ( ‚à´_{Œæ=0}^L r‚Çñ\u0026rsquo;(ùê±,Œæ)dŒæ ) ‚ãÖ ‚àè_{j‚ààIN} e^{- w‚±º‚ãÖ‚à´_{Œº=0}^L r‚±º\u0026rsquo;(ùê±,Œº) dŒº} $$\nThe integral of opacities of all voxels r‚Çñ\u0026rsquo;(ùê±,Œæ) on the line of sight L belonging to a kernel is denoted as:\n$$ q‚Çñ(ùê±) = ‚à´_{x‚ÇÇ=0}^L r‚Çñ\u0026rsquo;(ùê±,x‚ÇÇ) dx‚ÇÇ $$\nq‚Çñ(ùê±) is the footprint function for a 3D kernel $r‚Çñ\u0026rsquo;(ùê±,x‚ÇÇ)$ in the ray space.\nq‚Çñ(ùê±) is a 2D function that specifies the contribution (total opacity) of a 3D kernel to a screen location.\nR a y s e I p y n a e t c e e ‚ãØ g r a 3 t D e s ‚ãØ r u ‚Çñ p ‚ãØ ( p ùê± o ‚¶ø ‚ãØ , r Œæ t ‚ãØ ) o ‚ãØ a f l o r n ‚Çñ - g ‚ãØ t h e s l c i r ‚åà ‚åä n e x x e e ‚ÇÄ ‚ÇÅ n ‚åâ ‚åã o f s i g h t The 3rd dimension (x‚ÇÇ) of a kernel has been integrated out. Thus, inputting a 2D coordinates, it returns an attribute\u0026rsquo;s weight resulting from the corresponding kernel.\n(2023-12-31) The objects to be blended differ between EWA and NeRF.\na r e p r o P j o e c i c a n t m t e e s d r a ( i 1 n v e r s e l y ) L p V i i r i n n o s e e j c w 3 e r i D s c e ‚à´ f n e t e l o g r g e n i o a m d n t r y e e p a n o r y s t n s i s p s t e n a o g t i c a n e r s i t e c s e r g e r e a n l s (2024-01-04) In NeRF, points on a ray are composited, whereas EWA blends \u0026ldquo;line segments\u0026rdquo;. (refer to ÂÖ´Ê∞®)\nIn EWA, scene voxels are grouped into different ellipsoids, and then rendering for ellipsoids, instead of volume-wise compositing. The scene element is a line integral of an ellipsoid, analogous to a particle in the volume rendering, thereby boosting efficiency.\nIn this ellipsoidal scenario, the alpha of each ellipsoid is the footprint function (opacity integral over the line of sight).\nAnd 3D kernels in the dataset can be preintegrated before rendering.\nWith the opacity precomputed, the alpha compositing is a 2D convolution over the line of sight L for splatting, whereas ray-casting needs 3D convolutions. Thus, splatting is more efficient.\nWith this approximation, volume rendering equation becomes:\n$$g_c(ùê±) = ‚àë_{k‚ààIN} w‚Çñ ‚ãÖc‚Çñ(ùê±)‚ãÖ q‚Çñ(ùê±) ‚ãÖ ‚àè_{j‚ààIN} e^{- w‚±º‚ãÖq‚±º(ùê±)} $$\nAssume all kernels are ordered back to front (statements inconsistent in the paper), so that when computing the transmittance, only the opacities of kernels prior to the current kernel need to be accumulated.\ne y e b a c r k ' ‚Çñ ‚¶ø ‚Çã ‚ÇÅ ( r ùê± ' ‚¶ø ‚ãÖ , ‚Çñ Œæ ) r ' ‚Çñ ‚¶ø ‚Çä f ‚ÇÅ r o n t s c r e e n $$g_c(ùê±) = ‚àë_{k‚ààIN} w‚Çñ ‚ãÖc‚Çñ(ùê±)‚ãÖ q‚Çñ(ùê±) ‚ãÖ ‚àè_{j=0}^{k-1} e^{- w‚±º‚ãÖq‚±º(ùê±)} $$\nThe exponential term is approximated by its 1st-order Taylor expansion based on $e^{-x} = 1-x$ evaluated at x=0:\n$$ e^{- w‚±º‚ãÖq‚±º(ùê±)} \\approx 1-w‚±º q‚±º(ùê±) \\\\ g_c(ùê±) = ‚àë_{k‚ààIN} w‚Çñ ‚ãÖc‚Çñ(ùê±)‚ãÖ q‚Çñ(ùê±) ‚ãÖ ‚àè_{j=0}^{k-1} (1-w‚±º q‚±º(ùê±)) $$\n$g_c(ùê±)$ is the splatting equation representing the continuous screen.\nConsequently, with the above 4 assumptions, the point-based splatting becomes the same form as the NeRF-style volumetric rendering, because they\u0026rsquo;re both based on alpha compositing (image formation model). (Refer to 3DGS)\nCombine Filter Screen is a continuous 2D representation of the scene. A discrete image grid can be obtained by sampling it with an impulse train. To avoid aliasing when sampling the screen function, each projected 2D splat needs to be filtered to the Nyquist frequency of the output image by passing a proper loss-pass filter $h(ùê±)$.\n$$ \\begin{aligned} \u0026amp;\\^g_c(ùê±) = g_c(ùê±) ‚äó h(ùê±) \\\\ \u0026amp;= ‚à´_{Œ∑=-‚àû}^{‚àû} ‚àë_{k‚ààIN} w‚Çñ ‚ãÖ c‚Çñ(\\bm Œ∑) ‚ãÖ q‚Çñ(\\bm Œ∑) ‚ãÖ ‚àè_{j=0}^{k-1} (1-w‚±º q‚±º(\\bm Œ∑)) ‚ãÖ h(ùê±-\\bm Œ∑) d\\bm Œ∑ \\\\ \u0026amp;= ‚àë_{k‚ààIN} w‚Çñ ‚ãÖ ‚à´_{Œ∑=-‚àû}^{‚àû} c‚Çñ(\\bm Œ∑) ‚ãÖ q‚Çñ(\\bm Œ∑) ‚ãÖ ‚àè_{j=0}^{k-1} (1-w‚±º q‚±º(\\bm Œ∑)) ‚ãÖ h(ùê±-\\bm Œ∑) d\\bm Œ∑ \\\\ \\end{aligned} $$\nc‚Çñ(ùõà) is the color (emission coefficient) of the ray point (ùõà,Œæ) calculated relative to the kernel r‚Çñ. It\u0026rsquo;s a function of rays ùõà.\nq‚Çñ(ùõà) is the contribution (total opacity) of the kernel r‚Çñ to the screen point ùõà.\nThe cumulative product term is the transmittance (attenuation) of each voxel (ùõà,Œæ) in the kerenel r‚Çñ.\nBecause the color c‚Çñ(ùõà) and the transmittance of a voxel (ùõà,Œæ) in a kernel have no explicit formula to be integrated, two approximations are introduced to reach an analytical expression to compute.\nColor of any voxel on any ray ùõà in the support region of a 3D kernel r‚Çñ is a constant c‚Çñ:\n$$c‚Çñ(\\bm Œ∑) = c‚Çñ$$\nTransmittance of each voxel in the kernel r‚Çñ is a constant.\n$$‚àè_{j=0}^{k-1} (1-w‚±º q‚±º(\\bm Œ∑)) \\approx o‚Çñ $$\nThe transmittance variation inside a 3D kernel is omitted, so the sole splatting equation can\u0026rsquo;t avoid edge aliasing, which needs to be solved by other techniques. Therefore after filtering, the footprint function becomes band-limited in frequency domain:\n$$‚à´_{Œ∑=-‚àû}^{‚àû} q‚Çñ(\\bm Œ∑) h(ùê±-\\bm Œ∑)d\\bm Œ∑$$\nAnd the antialiased splatting equation (screen function):\n$$ \\begin{aligned} \\^g_c(ùê±) \u0026amp;= g_c(ùê±) ‚äó h(ùê±) \\\\ \u0026amp;‚âà ‚àë_{k‚ààIN} w‚Çñ‚ãÖc‚Çñ‚ãÖo‚Çñ‚ãÖ‚à´_{Œ∑=-‚àû}^{‚àû} q‚Çñ(\\bm Œ∑) ‚ãÖ h(ùê±-\\bm Œ∑) d\\bm Œ∑ \\\\ \u0026amp;= ‚àë_{k‚ààIN} w‚Çñ‚ãÖc‚Çñ‚ãÖo‚Çñ‚ãÖ (q‚Çñ ‚äó h)(ùê±) \\end{aligned} $$\nThe formula can be interpreted as a weighted sum (combination, expectation) of footprint function in the 2D screen space. Thus, the primitives are the projected, prefiltered reconstruction kernels, so called ideal volume resampling filter:\n$$œÅ‚Çñ(ùê±) = c‚Çñ‚ãÖo‚Çñ‚ãÖ(q‚Çñ ‚äó h)(ùê±)$$\nInstead of band limiting the output function $g_c(ùê±)$ directly, we band limit each footprint function q‚Çñ separately.\n(2023-11-26)\nEWA Splats The ideal volume resampling filter (splat primitive) is obtained after 3 steps: projection (viewing + projective transformation), footprint function, and convolving with a Gaussian loss-pass filter.\nGaussian will yields a Gaussian after affine transformations, convolving with another Gaussian, and integrating along one of its dimensions.\nTherefore, 3D Gaussian is used as 3D reconstruction kernels (in object space) to produce an analytical expression of the 2D Gaussian resampling filter $œÅ‚Çñ(ùê±)$ in screen space. So that splatting equation is an elliptical weighted average (EWA).\nA reconstruction kernel centered at datapoint ùêÆ‚Çñ in the object space is defined as the 3D Gaussian distribution with a mean vector ùêÆ‚Çñ and a covariance matrix ùêï‚Çñ:\n$$r‚Çñ(ùêÆ) = \\frac{1}{(2œÄ)^{3/2} det(ùêï‚Çñ)^{1/2}} e^{-¬Ω(ùêÆ-ùêÆ‚Çñ)^T ùêï‚Çñ^{-1} (ùêÆ-ùêÆ‚Çñ)}$$\n(2023-11-28)\nProject Kernels Project a 3D Gaussian distribution from object space to ray space through a viewing transformation and a projective transformation.\nViewing transformation œÜ Transform a Gaussian distribution from source (object) space to camera space through an affine mapping: $ùê≠=ùêñ ùêÆ+ùêù$. If ùêñ is invertible, there is $ùêÆ = ùêñ‚Åª¬π(ùê≠-ùêù)$.\nGiven a probability density function $f_ùêÆ$ of the variable ùêÆ, and a linear mapping ùê≠=ùêñ ùêÆ+ùêù, by substituting the ùêÆ of $f_ùêÆ(ùêÆ)$ with ùêñ‚Åª¬π(ùê≠-ùêù), the result expression is a new distribution $f_ùê≠$ represented in ùê≠\u0026rsquo;s space:\n$$f_ùê≠(ùê≠) = \\frac{1}{(2œÄ)^{3/2} det(ùêï‚Çñ)^{1/2}} e^{-¬Ω(ùêñ‚Åª¬π(ùê≠-ùêù)-ùêÆ‚Çñ)^T ùêï‚Çñ^{-1} (ùêñ‚Åª¬π(ùê≠-ùêù)-ùêÆ‚Çñ)}$$\nClarify: Subscripts indicate which variable\u0026rsquo;s distribution the function is depicting. The argument in parentheses is the variable building the function (in the associate space). On the other hand, to represent the PDF $f_ùêÆ$ in another space with a new variable, e.g., ùê≠ associating with $f_ùêÆ(ùê≠)$, rather than the generic $f_ùêÆ(ùêÆ)$. After substituting variable, the result representation in ùê≠\u0026rsquo;s space is $f_ùê≠(ùê≠)$ as stated above.\nBut the scale is mismatched. According to the Change of Variables theorem, the \u0026ldquo;absolute value of the determinant of the Jacobian matrix\u0026rdquo; must be multiplied as a scaling factor to align different units between ùê≠ and ùêÆ.:\n$$f_ùêÆ(ùê≠) = f_ùê≠(ùê≠) ‚ãÖ|det(\\frac{‚àÇùêÆ}{‚àÇùê≠})| = f_ùê≠(ùê≠)‚ãÖ |det(ùêñ‚Åª¬π)|$$\nAnd the new variable ùê≠\u0026rsquo;s PDF can be written as: $$f_ùê≠(ùê≠) = \\frac{1}{|det(ùêñ‚Åª¬π)|} f_ùêÆ(ùê≠)$$\n, corresponding to the equation (21) in the paper: $G_ùêï^n (œï‚Åª¬π(ùêÆ) - ùê©) = \\frac{1}{|ùêå‚Åª¬π|} G_{ùêå ùêï ùêå·µÄ}^n(ùêÆ-œï(ùê©))$\n(2023-11-28) doubt: I\u0026rsquo;m confused in eq. (21), is the LHS the $f_ùê≠(ùê≠)$? Why do they care the unscaled distribution?\n(2023-12-02) If the |det(J)| is missing and just substituting variable, the new distribution $f_ùê≠(ùê≠)$ can\u0026rsquo;t be written as a Gaussian (?). So, the author used the transformed \u0026ldquo;non-\u0026ldquo;Gaussian divided by the factor, such that the new distribution is a Gaussian as well.\n(2023-12-31) I forgot the meaning of the above comment on 2023-12-02. The eq. (21) can be understood as:\nGiven an affine mapping: ùêÆ = ùêå ùê±+c = œï(ùê±), and define its inverse as: ùê± = ùêå‚Åª¬π(ùêÆ-c) = œï‚Åª¬π(ùêÆ)\nGiven the distribution of ùê± is $G_ùêï^n (ùê± - ùê©)$, after applying the affine mapping ùêå ùê±+c, the mean and variance will become: œï(ùê©) and ùêå ùêï ùêå·µÄ. So the new distribution is represented as $G_{ùêå ùêï ùêå·µÄ}^n (ùêÆ - œï(ùê©))$.\nAccording to Change of Variables, the relations are:\n$$ G‚Åø_ùêï(ùê±-ùê©) \\overset{substitute}{‚Üí} G‚Åø_ùêï(œï‚Åª¬π(ùêÆ) - ùê©) \\overset{times |det(J)|}{‚Üí} G_ùêï^n(œï‚Åª¬π(ùêÆ) - ùê©) |ùêå‚Åª¬π| = G_{ùêå ùêï ùêå·µÄ}^n (ùêÆ - œï(ùê©)) $$\nSo the eq.(21) is indeed the unscaled transformed Gaussian in another space.\ndoubt: The unscaled, merely variable-changed distribution $G‚Åø_ùêï(œï‚Åª¬π(ùêÆ) - ùê©)$ is also a Guassian (?), as the 3 axes are stretched by the affine mapping linearly and separately, so the overall shape of Gaussian will be kept. And the |det(J)| is just a scalar coefficient ensuring consistent area quantity.\nSimilarly, the case of projecting a kernel from source space into camera space ($ùê≠=ùêñ ùêÆ+ùêù$) is changing mean vector and covariance matrix:\n$r‚Çñ(ùê≠)$ is the transformed representation of the kernel $k$ in the camera space, matching the above $f_ùêÆ(ùê≠)$, so it equals the variable-changed $r‚Çñ(ùêÆ)$ multiplied with $|det(‚àÇùêÆ/‚àÇùê≠)|$:\n$$ \\begin{aligned} \u0026amp;r‚Çñ(ùê≠) = r‚Çñ(ùêñ‚Åª¬π(ùê≠-ùêù)) ‚ãÖ |det(ùêñ‚Åª¬π)| \\\\ \u0026amp;= \\frac{1}{(2œÄ)^{3/2} \\sqrt{det(ùêï‚Çñ)} |det(ùêñ)|} e^{-¬Ω(ùêñ‚Åª¬π(ùê≠-ùêù)-ùêÆ‚Çñ)·µÄùêï‚Çñ‚Åª¬π(ùêñ‚Åª¬π(ùê≠-ùêù)-ùêÆ‚Çñ )} \\\\ \u0026amp;= \\frac{1}{(2œÄ)^{3/2} \\sqrt{det(ùêï‚Çñ)} |det(ùêñ)|} e^{-¬Ω(ùêñ‚Åª¬π(ùê≠-ùêù-ùêñùêÆ‚Çñ))·µÄùêï‚Çñ‚Åª¬π(ùêñ‚Åª¬π(ùê≠-ùêù-ùêñùêÆ‚Çñ) )} \\\\ \u0026amp;= \\frac{1}{(2œÄ)^{3/2} \\sqrt{det(ùêï‚Çñ)} |det(ùêñ)|} e^{-¬Ω(ùêñ‚Åª¬π(ùê≠-(ùêñùêÆ‚Çñ+ùêù)))·µÄùêï‚Çñ‚Åª¬π(ùêñ‚Åª¬π(ùê≠-(ùêñùêÆ‚Çñ+ùêù)) )} \\\\ \u0026amp;= \\frac{1}{(2œÄ)^{3/2} ‚ãÖ \\sqrt{det(ùêï‚Çñ)} ‚ãÖ |det(ùêñ·µÄ)|^¬Ω ‚ãÖ |det(ùêñ)|^¬Ω} \\\\ \u0026amp;\\quad ‚ãÖ e^{-¬Ω(ùê≠-(ùêñùêÆ‚Çñ+ùêù))·µÄ(ùêñ‚Åª¬π)·µÄùêï‚Çñ‚Åª¬πùêñ‚Åª¬π(ùê≠-(ùêñùêÆ‚Çñ+ùêù)) } \\\\ \u0026amp;= \\frac{1}{(2œÄ)^{3/2} \\sqrt{|det(ùêñ ùêï‚Çñ ùêñ·µÄ)}|} e^{-¬Ω(ùê≠-(ùêñùêÆ‚Çñ+ùêù))·µÄ(ùêñ·µÄ)‚Åª¬πùêï‚Çñ‚Åª¬πùêñ‚Åª¬π(ùê≠-(ùêñùêÆ‚Çñ+ùêù)) } \\\\ \u0026amp;= N(ùêñùêÆ‚Çñ+ùêù, ùêñ ùêï‚Çñ ùêñ·µÄ) \\end{aligned} $$\nHence, after performing an affine transformation, the distribution in the new space has a new mean vector ùêñùêÆ‚Çñ+ùêù, i.e., the original mean ùêÆ‚Çñ is shifted by the affine mapping, and the variance matrix becomes ùêñ ùêï‚Çñ ùêñ·µÄ.\nDerivation refers to: Linear Transformation of Gaussian Random Variable (Found by DDG with keywords: \u0026ldquo;affine transform for Gaussian distribution\u0026rdquo;)\nFact: $(W·µÄ)‚Åª¬π = (W‚Åª¬π)·µÄ$\nAffine property for x~N(0,1): STAT 830 The Multivariate Normal Distribution - SFU\nThe Multivariate Gaussian Distribution - Stanford\nProjective transformation œï (2024-01-04) Summary: The non-linear projective transformation is approximated via Taylor expansion. In this paper, a 3D point (x,y,z) is projected onto a plane perspectively as follows: the x, y coordinates are divided by z, and the z value is then reset to the Euclidean norm $‚Äñùê≠‚Äñ$ for object culling.\nIn this way, points in the camera space are transformed into the 3D ray space.\n$$ \\begin{bmatrix}x‚ÇÄ \\\\ x‚ÇÅ \\\\ x‚ÇÇ \\end{bmatrix} = œï(\\begin{bmatrix}t‚ÇÄ \\\\ t‚ÇÅ \\\\ t‚ÇÇ \\end{bmatrix}) = \\begin{bmatrix}t‚ÇÄ/t‚ÇÇ \\\\ t‚ÇÅ/t‚ÇÇ \\\\ \\|(t‚ÇÄ,t‚ÇÅ,t‚ÇÇ)·µÄ\\| \\end{bmatrix} $$\nAfter perspective division with pixel coords obtained, x‚ÇÇ is supposed to be 1, but it\u0026rsquo;s assigned with ‚Äñùê≠‚Äñ‚ÇÇ.\nDenote the L2 norm of ùê≠ as l: $‚Äñ(t‚ÇÄ,t‚ÇÅ,t‚ÇÇ)·µÄ‚Äñ = l = \\sqrt{t‚ÇÄ¬≤+t‚ÇÅ¬≤+t‚ÇÇ¬≤}$, i.e., the magnitude of the line connecting a point ùê≠ to the projection center on the screen.\nThe dpeth is not confined to [-1,1] like NDC. The points on a single ray all have the same (t‚ÇÄ,t‚ÇÅ)·µÄ, thus, the depth $x‚ÇÇ=\\sqrt{t‚ÇÄ¬≤+t‚ÇÅ¬≤+t‚ÇÇ¬≤}$ only depends on t‚ÇÇ.\nAlthough $y=\\sqrt{1+x¬≤}$ is not linear, it\u0026rsquo;s approaching linear: $y=x$ as x grows.\n(plot by Perplexity-Wolfram)\nBy using the L2 norm of the camera-space coordinates ùê≠ as the depth, the evenly sampled points on the line of sight in the ray space will almost remain evenly spaced after this non-linear projection (due to perspective division) from ùê≠ to ùê±, close to directly using the t‚ÇÇ of camera space as the depth:\n‚åà ‚åä S x x c ‚ÇÄ ‚ÇÅ r ‚åâ ‚åã e e n M a ‚Üì i n ‚Üì t R a a ‚Üì i y n ‚ÑÆ s e p v a x e c ‚ÇÇ n e ‚¶ø k w t s ( r e a h p x ‚Çñ r t e a ‚ÇÄ ' n c c , e h r i x l i a n ‚ÇÅ n y g , i g x s s ‚ÇÇ p ) a ·µÄ c e (2024-01-01) Unlike directly using t‚ÇÇ as the depth in the ray space, with which the uniform intervals can be exactly kept (as perspective projection with homogeneous coords is linear), this approach considers both t‚ÇÄ and t‚ÇÅ. After viewing transformation, it is the kernels that are viewing the screen. Thus, the screen seen by a kernel is perspective (i.e., near large, far small, as shown below Fig.3 bottom) after projective transformation. Each ray emitted from the kernel is parallel to each other in the ray space.\nMap a kernel from camera space to ray space.\n(2024-01-06)\nThe transformation œï from camera space to ray space should be understood as the datapoints are projected into the ray space (or the screen by disregarding the x‚ÇÇ) perspectively, because we want to display 3D world in a 2D plane. (Note: x‚ÇÇ is independtly assigned beside the screen coordinates x0,x1, so ray space = screen + x‚ÇÇ.)\nShortly, the near-large-far-small effect is desired. But it\u0026rsquo;s a nonlinear function, so its linear approximation is applied to simulate those \u0026ldquo;curves\u0026rdquo;. As shown in Fig. 9, the correct perspective curving effects (bottom-right) aren\u0026rsquo;t accurately represented by an linear projection (bottom-left). Intuitively, one might think that all points are using a common depth value, but for the exact perspective projection, each point should use its individual depth.\nIn addition, the x‚ÇÇ (‚Äñùê≠‚Äñ‚ÇÇ, depth in the ray space) can be disregarded, because the datapoints have already been projected onto the screen, i.e., (x‚ÇÄ,x‚ÇÅ). The existence of ray space may just for introducing the footprint function (ray integral). The \u0026ldquo;projective transformation œï\u0026rdquo; is solely assigning x‚ÇÇ based on the perspective projection.\nThe statement that \u0026ldquo;rays are parallel\u0026rdquo; may be misleading, as the perspective projection (x,y divided by z) has been performed, and one can focus on the screen directly. \u0026ldquo;Parallel\u0026rdquo; doesn\u0026rsquo;t mean that the kernel is projected onto the screen orthogonally, because they already are on the screen. The ray space is set for defining the footprint function of each kernel as an integral along a certain viewing ray, which remains a straight line in the 3D ray space after perspective projection due to the linear approximation.\nAnd it is the viewing ray that will be orthogonally projected onto the screen. Thereby, the ray integral in the 3D ray space becomes a line integral on the 2D screen plane, i.e., an integral for a 2D Gaussian.\n(2024-01-07) The 3D Gaussian integral in the ray space equals the 2D Gaussian integral in the screen space (?), because I think that the \u0026ldquo;voxels\u0026rdquo; perpendicular to a pixel at different depths are replicas of that pixel, which is projected by a certain kernel. S p c i r x e e ‚ñ† ‚ñ° e l n ( v ‚ñ† 1 ( ‚ñ° R v ‚ñ† a 2 y ‚ñ° v ‚ñ† s 3 p ‚ñ° a ‚ãØ c e v ‚ñ† ‚ãØ N ) ‚ñ° ) K K e e r r n n e e l l 1 2 The depth dimension x‚ÇÇ is ‚Äñùê≠‚Äñ‚ÇÇ, which is independent to x0, x1. So ray space is \u0026ldquo;orthogonal\u0026rdquo; to the screen. In other words, a screen pixel is a summation of only kernels that overlaps with it, because object-space 3D Gaussian have already been projected perspectively onto the screen (corresponding to 2D center and covariance matrix), and the ray space is constructed after that projection as \u0026ldquo;rays space = screen + x‚ÇÇ\u0026rdquo;. In short, ray space is for viewing rays, not for datapoints.\n(2024-01-07)\nThe viewing ray is only used to compute each kernel\u0026rsquo;s opacity, which is the contribution coefficient to the target pixel\u0026rsquo;s color.\nKernels contributing to the target pixel are determined by whether a kernel overlaps with the pixel after being thrown onto the screen. In essence, the contributing kernel in the ray space is located on the perpendicular line to the target pixel, and its squashed flat disc covers the target pixel.\nThe role of viewing rays in splatting differs from volume rendering, where a viewing ray yields a pixel color, but in splatting, color is generated from kernels and a viewing ray determines the weights of kernels.\n(2024-01-22)\nThe 2D location (x‚ÇÄ,x‚ÇÅ) of a Gaussian center on the screen and the 2D covariance matrix are determined by perspective projection, while the opacity of a Gaussian is determined by computing the integral over x‚ÇÇ along the viewing ray in the 3D ray space. However, this perspective projection is not affine (linear) because the depth t‚ÇÇ is divided and x‚ÇÇ is reassigned. Therefore, the first-order Taylor expansion of this transformation matrix is used as its linear approximation.\n(2024-01-05) The reassignment of x‚ÇÇ is not the reason for the approximation. x‚ÇÇ is just used for integration along the viewing ray. And after approximation, the shape of the 2D Gaussian doesn\u0026rsquo;t depend on depth, which (the 3rd row, col) will be omitted to obtain the 2D covariance matrix of the projected 2D ellipse. The Jacobian matrix of œï(ùê≠) is:\n$$ (\\begin{bmatrix}t‚ÇÄ/t‚ÇÇ \\\\ t‚ÇÅ/t‚ÇÇ \\\\ ‚Äñ(t‚ÇÄ,t‚ÇÅ,t‚ÇÇ)·µÄ‚Äñ \\end{bmatrix})\u0026rsquo;= ùêâ = \\begin{pmatrix} 1/t‚ÇÇ \u0026amp; 0 \u0026amp; -t‚ÇÄ/t‚ÇÇ¬≤ \\\\ 0 \u0026amp; 1/t‚ÇÇ \u0026amp; -t‚ÇÅ/t‚ÇÇ¬≤ \\\\ t‚ÇÄ/l \u0026amp; t‚ÇÅ/l \u0026amp; t‚ÇÇ/l \\end{pmatrix} $$\nThe first-order Taylor expansion evaluated at the kernel center point ùê≠‚Çñ is called the local affine approximation:\n$$ \\begin{aligned} \u0026amp; œï‚Çñ(ùê≠) = œï(ùê≠‚Çñ) + ùêâ‚Çñ ‚ãÖ (ùê≠-ùê≠‚Çñ) \\\\ \\\\ \u0026amp; ùêâ‚Çñ = \\frac{‚àÇœï}{‚àÇùê≠}(ùê≠‚Çñ) = \\begin{pmatrix} 1/t‚Çñ,‚ÇÇ \u0026amp; 0 \u0026amp; -t‚Çñ,‚ÇÄ/t‚Çñ,‚ÇÇ¬≤ \\\\ 0 \u0026amp; 1/t‚Çñ,‚ÇÇ \u0026amp; -t‚Çñ,‚ÇÅ/t‚Çñ,‚ÇÇ¬≤ \\\\ t‚Çñ,‚ÇÄ/‚Äñùê≠‚Çñ‚Äñ \u0026amp; t‚Çñ,‚ÇÅ/‚Äñùê≠‚Çñ‚Äñ \u0026amp; t‚Çñ,‚ÇÇ/‚Äñùê≠‚Çñ‚Äñ \\end{pmatrix} \\end{aligned} $$\n(2024-01-10) If the ùê≠ is ùê≠‚Çñ, (ùê≠-ùê≠‚Çñ). So, there is no approximation, the projection of ùê≠‚Çñ is exact œï(ùê≠‚Çñ). This is what the following sentence in the paper means.\n\u0026ldquo;As illustrated in Fig. 9, the local affine mapping is exact only for the ray passing through tk or xk, respectively.\u0026rdquo;\nAnd for the point far away from the Guassian center, the approximation result has a noticeble deviation from the actual case, as (ùê≠-ùê≠‚Çñ) is large.\nBy concatenating the viewing transform œÜ(ùê±) and projective transform œï(ùê≠), the conversion for a point ùêÆ in source space to ùê± in ray space is an affine mapping ùê¶(ùêÆ):\n$$ùê≠ = œÜ(ùêÆ) = ùêñ ùêÆ+ùêù \\\\ ùê± = ùê¶‚Çñ(ùêÆ) = œï‚Çñ(ùê≠) = œï(ùê≠‚Çñ) + ùêâ‚Çñ ‚ãÖ (ùê≠-ùê≠‚Çñ) \\\\ = ùê±‚Çñ + ùêâ‚Çñ ‚ãÖ (ùêñ ùêÆ+ùêù -ùê≠‚Çñ) = ùêâ‚Çñ‚ãÖùêñ ùêÆ + ùê±‚Çñ + ùêâ‚Çñ‚ãÖ(ùêù-ùê≠‚Çñ)$$\nTherefore, for this compound affine mapping, the multiplier is $ùêâ‚Çñ‚ãÖùêñ $, and the bias is $ùê±‚Çñ + ùêâ‚Çñ‚ãÖ(ùêù-ùê≠‚Çñ)$.\nAccording to the aforementioned viewing transformation derivation, the kernel in the 3D ray sapce r‚Çñ\u0026rsquo;(ùê±) has a shifted mean ùêÆ‚Çñ and a scaled variance ùêï‚Çñ:\n$$ r‚Çñ\u0026rsquo;(ùê±) ‚àº N(ùê¶(ùêÆ‚Çñ), ùêâ‚Çñùêñ ùêï‚Çñ ùêñ ·µÄùêâ‚Çñ·µÄ) $$\n(2023-12-02) doubt: Does the eq. (30) mean that the unscaled representation in ray space is the desired projected kernel?\nThe normalization factor of the transformed Gaussian is canceled, so that its integral isn\u0026rsquo;t a unit. Is that they want?\nIntegrate Kernels The footprint function is an integral over the voxels in a 3D kernel along the ray.\nGiven a 3D kernel in the ray space r‚Çñ\u0026rsquo;(ùê±) as above, the footprint function is the integral over depth:\nConvolve with Filter The anti-aliased splatting equation is achieved by using the band-limited footprint function under 2 assumptions described above.\nConvolving the footprint function with a Gaussian low-pass filter:\n","date":"2023-11-18T21:52:00Z","image":"https://pic2.zhimg.com/80/v2-7cbe3b0c3b67ce80593fad0d73a814b5_720w.webp","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-ewa_splatting/","title":"read: Render - Points | EWA Splatting"},{"content":"Example Source video: „ÄêcmakeÊïôÁ®ã„Äë‰∏∫‰Ω†ÁöÑÈ°πÁõÆÂºïÂÖ•Â§ñÈÉ®Á¨¨‰∏âÊñπÂ∫ì(‰ª•ÁúüÂÆûÈ°πÁõÆpartio‰∏∫‰æã) - Âè™ÂñùÁôΩÂºÄÊ∞¥\nPrepare: Download the code of partio.\n1 git clone https://github.com/wdas/partio.git --depth=1 Directory structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (base) yi@yi:~/Downloads/partio$ tree -L 2 . ‚îú‚îÄ‚îÄ build_wheels.sh ‚îú‚îÄ‚îÄ CMakeLists.txt ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ LICENSE ‚îú‚îÄ‚îÄ Makefile ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ setup.cfg ‚îú‚îÄ‚îÄ setup.py ‚îî‚îÄ‚îÄ src ‚îú‚îÄ‚îÄ data ‚îú‚îÄ‚îÄ doc ‚îú‚îÄ‚îÄ lib ‚îú‚îÄ‚îÄ Makefile ‚îú‚îÄ‚îÄ py ‚îú‚îÄ‚îÄ tests ‚îî‚îÄ‚îÄ tools 7 directories, 9 files Four steps for testing the introduced library:\nCopy codes\nOnly the codes partio/src/lib/ are needed in this example., so copy it to your own project, and place it under the directory example_cmake/external/partio/ as shown below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (base) yi@yi:~/Downloads/example_cmake$ tree . ‚îú‚îÄ‚îÄ CMakeLists.txt ‚îú‚îÄ‚îÄ external ‚îÇ¬†‚îî‚îÄ‚îÄ partio ‚îÇ¬†‚îú‚îÄ‚îÄ CMakeLists.txt # partio/src/lib/CMakeLists.txt ‚îÇ¬†‚îú‚îÄ‚îÄ core ‚îÇ¬†‚îú‚îÄ‚îÄ io ‚îÇ¬†‚îú‚îÄ‚îÄ PartioAttribute.h ‚îÇ¬†‚îú‚îÄ‚îÄ Partio.h ‚îÇ¬†‚îú‚îÄ‚îÄ PartioIterator.h ‚îÇ¬†‚îú‚îÄ‚îÄ PartioVec3.h ‚îÇ¬†‚îî‚îÄ‚îÄ test ‚îÇ¬†‚îî‚îÄ‚îÄ test_helloWorld.cpp ‚îî‚îÄ‚îÄ src ‚îú‚îÄ‚îÄ CMakeLists.txt ‚îî‚îÄ‚îÄ main.cpp Inherit the original CMakeListst settings:\nCopy all contents of the top-level CMakeLists.txt of the original partio project to the front of the copied lib\u0026rsquo;s external/partio/CMakeLists.txt, i.e., the previous partio/src/lib/CMakeLists.txt, as it\u0026rsquo;s included first.\nSo as to retain some macro settings and variables for compiling. And then delete unnecessary settings.\nThe top-level CMakeLists.txt of your own project is:\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.20) project(learn-cmake LANGUAGES CXX) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) # each sub-directory has its own CMakeLists.txt: add_subdirectory(external/partio) add_subdirectory(src) The src/ folder stores your own code, which will be built to an executable program. So, the executable in the src/CMakeLists.txt is your own main.cpp:\n1 add_executable(main main.cpp) An example of src/main.cpp:\n1 2 3 int main(int argc, char const *argv[]){ return 0; } Write an test executable program (test_helloWorld.cpp) inside external/partio.\nIn this case, test_helloWorld.cpp calls functions in external/partio \u0026ldquo;natively\u0026rdquo;. However, this isn\u0026rsquo;t the case for calling its functions from another project.\n1 2 3 4 #include \u0026#34;Partio.h\u0026#34; int main(){ Partio::ParticlesDataMutable* data = Partio::read(\u0026#34;bgeoFile\u0026#34;); std::cout \u0026lt;\u0026lt; \u0026#34;Number of \u0026#34; \u0026lt;\u0026lt; std::endl;} Set the external/partio/CMakeLists.txt to compile it as an executable application:\n1 2 3 4 add_library(partio ${PARTIO_LIBRARY_TYPE} ${io_cpp} ${core_cpp}) # Setting target_include_directories .... add_executable(test_helloWorld ${CMAKE_CURRENT_LIST_DIR}/test/test_helloWorld.cpp) target_link_libraries(test_helloWorld PRIVATE partio) After compiling, a binary file test_helloWorld will be generated under \u0026ldquo;external/partio\u0026rdquo;.\nThis test code has nothing to do with the other subdirectories\u0026rsquo; \u0026ldquo;src/\u0026rdquo;, where the main application will be build based on its own CMakeLists.txt.\nThe CMakeLists.txt of different projects are independent to each other.\nThe most commonly used rule is \u0026ldquo;One CMakeLists.txt per target\u0026rdquo;. cmake: add_subdirectory() vs include() - SO\nTest DiffRast I want to debug the library diff-gaussian-rasterization.\nI can create a project, and make the diffRast as an external library to call its methods.\n1 2 3 4 mkdir debug_diff_rast \u0026amp;\u0026amp; cd debug_diff_rast git init mkdir external \u0026amp;\u0026amp; cd external git submodule add https://github.com/graphdeco-inria/diff-gaussian-rasterization Directory structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 (base) yi@yi:~/Downloads/debug_diff_rast$ tree . ‚îú‚îÄ‚îÄ CMakeLists.txt ‚îú‚îÄ‚îÄ main.cpp ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ external ‚îî‚îÄ‚îÄ diff-gaussian-rasterization ‚îú‚îÄ‚îÄ CMakeLists.txt ‚îú‚îÄ‚îÄ cuda_rasterizer ‚îÇ ‚îú‚îÄ‚îÄ auxiliary.h ‚îÇ ‚îú‚îÄ‚îÄ backward.cu ‚îÇ ‚îú‚îÄ‚îÄ backward.h ‚îÇ ‚îú‚îÄ‚îÄ config.h ‚îÇ ‚îú‚îÄ‚îÄ forward.cu ‚îÇ ‚îú‚îÄ‚îÄ forward.h ‚îÇ ‚îú‚îÄ‚îÄ rasterizer.h ‚îÇ ‚îú‚îÄ‚îÄ rasterizer_impl.cu ‚îÇ ‚îî‚îÄ‚îÄ rasterizer_impl.h ‚îú‚îÄ‚îÄ diff_gaussian_rasterization ‚îÇ ‚îî‚îÄ‚îÄ __init__.py ‚îú‚îÄ‚îÄ ext.cpp ‚îú‚îÄ‚îÄ LICENSE.md ‚îú‚îÄ‚îÄ rasterize_points.cu ‚îú‚îÄ‚îÄ rasterize_points.h ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ setup.py ‚îî‚îÄ‚îÄ third_party ‚îú‚îÄ‚îÄ glm ‚îî‚îÄ‚îÄ stbi_image_write.h The original diff-gaussian-rasterization/CMakeLists.txt would build the project to a library (CudaRasterizer), not an executable application.\nThe command add_subdirectory() is used to add an external project, which will automatically build according to its own CMakeLists.txt when the root CMakeLists.txt starts building.\nIf without add_subdirectory, an error would occur: CMake Error at CMakeLists.txt:17 (target_include_directories): Cannot specify include directories for target which is not built by this project. Hence, the library CudaRasterizer is linked as a static library through target_link_libraries() to an executable program MyApp based on the main.cpp, and debug it.\nIn summary, the top-level (root) CMakeLists.txt needs to link 2 libraries: libtorch and CudaRasterizer, as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cmake_minimum_required(VERSION 3.20 FATAL_ERROR) project(MyApp) # ${PROJECT_NAME} find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_subdirectory(external/diff-gaussian-rasterization) add_executable(MyApp main.cpp) set_property(TARGET MyApp PROPERTY CXX_STANDARD 17) target_link_libraries(MyApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) target_link_libraries(MyApp CudaRasterizer) # rasterization_points.cu and ~.h aren\u0026#39;t in their CMakeLists.txt, so need: target_sources(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization/rasterize_points.cu) target_include_directories(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization) In the diff-gaussian-rasterization/CMakeLists.txt, the header files inside the folder cuda_rasterizer/ are included and set to PUBLIC, such that when the library is linked to the target, those headers will be appended to the target\u0026rsquo;s INCLUDE_DIRECTORIES. So they can be included via their filenames, for example #include \u0026quot;forward.h\u0026quot;, without relative path. (Not sure. 2023-11-16)\nHowever, the rasterize_point.h isn\u0026rsquo;t included into the library (CudaRasterizer), so it has to be included using its relative path.\nSpecifically, in the main.cpp:\n1 2 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026#34;external/diff-gaussian-rasterization/rasterize_points.h\u0026#34; With adding that header file (declaration) to executable target, the compilation of main.cpp can pass. However, if the function body rasterize_points.cu (function implementation) isn\u0026rsquo;t added to the executable in CMakeLists.txt, there will be a linking error:\n1 2 /usr/bin/ld: CMakeFiles/MyApp.dir/main.cpp.o: in function `main\u0026#39;: main.cpp:(.text.startup+0x383): undefined reference to `RasterizeGaussiansCUDA(at::Tensor const\u0026amp;, ...)\u0026#39; Two modifications for source code of diff-gaussian-rasterization:\nrasterize_point.cu isn\u0026rsquo;t included into the CudaRasterizer library, because it\u0026rsquo;s not specified within the add_library() function in the diff-gaussian-rasterization/CMakeLists.txt.\nTherefore, when the project diff-gaussian-rasterization is built by executing cmake -B ./build, the .cu file rasterize_point.cu isn\u0026rsquo;t compiled.\nThus, the following error of missing Python.h won\u0026rsquo;t be tiggered.\nrasterize_point.cu includes header \u0026lt;torch/extension.h\u0026gt;, which is used for interacting with Python. Based on that, the library CudaRasterizer can be called in Python scripts as a PyTorch extension.\nTherefore, the Python development headers must be present to compile it. Otherwise, an error would occur:\n1 2 3 4 5 6 7 8 9 In file included from /usr/local/libtorch/include/torch/csrc/Device.h:3, from /usr/local/libtorch/include/torch/csrc/api/include/torch/python.h:8, from /usr/local/libtorch/include/torch/extension.h:6, from /home/yi/Downloads/diff-gaussian-rasterization/rasterize_points.h:13, from /home/yi/Downloads//diff-gaussian-rasterization/test_DiffRast.cpp:2: /usr/local/libtorch/include/torch/csrc/python_headers.h:10:10: fatal error: Python.h: No such file or directory 10 | #include \u0026lt;Python.h\u0026gt; | ^~-~~-~~-~~-~~ compilation terminated. Previously, my trivial experiments only used LibTorch tensors without interacting with python. Therefore, no error was reported when building without specifying the path of Python.\nBut the Python development headers is already installed:\n1 2 sudo apt-get install python3-dev apt list python3-dev A possible solution is to include python in CMakeLists.txt: Similar issure\nSince here I just want to debug it as a C++ project, instead of for python, and I have LibTorch installed.\nSo I changed #include \u0026lt;torch/extension.h\u0026gt; to #include \u0026lt;torch/torch.h\u0026gt; in the 2 files: \u0026ldquo;rasterize_points.cu\u0026rdquo; and \u0026ldquo;rasterize_points.h\u0026rdquo;.\nBuild the top-level project \u0026ldquo;debug_diff_rast\u0026rdquo;:\n1 2 3 4 5 6 cd ~/Downloads/debug_diff_rast # configure: cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja # build: cmake --build ./build ./build/MyApp Ref:\n„ÄêÂÖ¨ÂºÄËØæ„ÄëÁé∞‰ª£CMakeÈ´òÁ∫ßÊïôÁ®ãÔºàÊåÅÁª≠Êõ¥Êñ∞‰∏≠Ôºâ- ÂèåÁ¨ôÂ≠ê‰ΩØË∞¨ - bilibili\nInclude Python cmake (2024-01-29)\nThe direct solution for Python.h: No such file or directory could be include Python in the CMakeLists.txt,\nThis is inspired by Example debugging mixed Python C++ in VS Code - Nadiah Pardede Kristensen, where #include \u0026lt;Python.h\u0026gt; will be reported the error by Intellisense, if without specifying \u0026ldquo;includePath\u0026rdquo; for Python in the .vscode/c_cpp_properties.json.\nCMake built-in module: FindPython3 mentioned in How do I get cmake to find python3.9 instead of python3.10 on ubuntu22.04\ng++ CLI (2024-01-31)\nCheck if the Python Development Libraries has been installed: dpkg -l | grep python3-dev. devicetests Install it: sudo apt-get install python3-dev\nFind where it is: find / -type f -iname 'python.h' 2\u0026gt;/dev/null Debian / Ubuntu: Fatal error: Python.h: No such file or Directory\nThe directory of Python is also required to be included: g++ -I/usr/include/python3.8 main.cpp. GeeksforGeeks\nA complete example command is:\n1 2 3 4 5 6 7 8 9 10 11 12 (AIkui) yi@yi-Alien:~/Downloads/CppCudaExt_PT_Tut_AIkui$ g++ \\ -g -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ -I/home/yi/anaconda3/envs/AIkui/include/python3.10 \\ -I./include \\ # Custom functions declarations -L/usr/local/libtorch/lib \\ -Wl,-rpath,/usr/local/libtorch/lib \\ test_None_tensor.cpp \\ # main function definition interpolation.cpp \\ # Custom functions definitions -ltorch -ltorch_cpu -lc10 A demo Refer to Nathan Sebhastian\n1 2 3 4 5 6 7 8 #include\u0026lt;Python.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } Compile: g++ test_python.cpp, an error occurs:\n1 2 3 4 5 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/Cpp_Study$ g++ test_python.cpp test_python.cpp:1:9: fatal error: Python.h: No such file or directory 1 | #include\u0026lt;Python.h\u0026gt; | ^~~~~~~~~~ compilation terminated. Locate header file:\n1 2 3 sudo apt install mlocate sudo updatedb locate Python.h Include Python dir: gcc test_python.cpp -I/usr/include/python3.8\n","date":"2023-11-16T11:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cmake_3rdparty/","title":"memo: CMake | Include 3rd-Party Library"},{"content":"Build add_executalbe() is for building a executable program. While add_library() is for building a library.\nBoth the source files and header files need to be added to executable target.\nOld fasion: How can i include header files from a directory into cmake\nMordern style: parallel101/course - ÂΩ≠‰∫éÊñå\n1 2 3 add_executable(MyApp) file(GLOB sources *.cpp *.h) target_source(main PUBLIC ${sources}) (2023-11-16)\nCcache Speed up recompilation. Docs\nUsage refer to parallel101/course\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.15) project(hellocmake) find_program(CCACHE_PROGRAM ccache) if (CCACHE_PROGRAM) message(STATUS \u0026#34;Found CCache: ${CCACHE_PROGRAM}\u0026#34;) set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM}) set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK ${CCACHE_PROGRAM}) endif() ","date":"2023-11-15T22:05:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cmake_misc/","title":"memo: CMake | Misc"},{"content":"Debug Config Open \u0026ldquo;launch.json\u0026rdquo; ‚û° Clik blue button \u0026ldquo;Add Configuration\u0026hellip;\u0026rdquo; at right-bottom corner ‚û° Select \u0026ldquo;C/C++: (gdb) Launch\u0026rdquo;.\nThen, a snippest of configuration with name \u0026quot;(gdb) Launch\u0026quot; is inserted into the \u0026ldquo;configurations\u0026rdquo; field in the config file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(gdb) Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/test\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;stopAtEntry\u0026#34;: false, \u0026#34;cwd\u0026#34;: \u0026#34;${fileDirname}\u0026#34;, \u0026#34;environment\u0026#34;: [], \u0026#34;externalConsole\u0026#34;: false, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true }, { \u0026#34;description\u0026#34;: \u0026#34;Set Disassembly Flavor to Intel\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-gdb-set disassembly-flavor intel\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;g++ compile\u0026#34; }, ] } This newly created configuration can be selected in the drop-down box beside the gear.\nAdd a task in tasks.json.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;g++ compile\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;g++\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;${workspaceFolder}/test.cpp\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;test\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, ] } The code \u0026ldquo;test.cpp\u0026rdquo; to be debugged is as follows:\n1 2 3 4 5 6 7 8 9 10 11 int main(){ int x = 1; int exampleArray[5]; for (int i=0; i\u0026lt;5; i++) exampleArray[i] = i; int* exampleArray_heap = new int[5]; for (int i=0; i\u0026lt;5; i++) exampleArray_heap[i] = i; delete[] exampleArray_heap } View Memory After debugging started,\nFind the address of a variable x:\nAdd expression \u0026amp;x into \u0026ldquo;Watch\u0026rdquo; panel. SO\nUse VS Code\u0026rsquo;s generic memory view\nInstall extention Hex Editor; Clik the binary data icon $[^{01}_{10}]$ following the address (not variables). Another ways to view memory\nUsing extension Memmory View\nPress F1, MemoryView: Add new memory view (for debugger)\nType the address of x: 0x7fffffffdbf0. Then, \u0026ldquo;MEMORY\u0026rdquo; section appers in the bottom panel.\nUse GDB command -exec x/64xb 0x7fffffffdc00\n1 2 3 -exec x/64xb 0x555558854e20 0x555558854e20:\t0x03\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x555558854e28:\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 The byte at 0x555558854e20 is 3. Inspect array Array on stack is the pointer to the array. The pointer is stored in: \u0026amp;exampleArray= 0x7fffffffdc00\nAfter initializing, memory at exampleArray is 0,1,2,3,4:\n1 2 00007fffffffdc00 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 00007fffffffdc10 04 00 00 00 ff 7f 00 00 00 50 58 2b fa da fc 53 This array takes 16 bytes. Array on heap is the address of pointer, which is stored in: \u0026amp;exampleArray_heap= 0x7fffffffdbf8\n1 00007fffffffdbf0 01 00 00 00 04 00 00 00 c0 b2 56 55 55 55 00 00 That\u0026rsquo;s an address: 00 00 55 55 55 56 b2 c0 (0x000055555556b2c0), because the endianness of computer is in reverse order.\nAfter initializing by the for loop, heap memory of the array is:\n1 2 000055555556b2c0 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 000055555556b2d0 04 00 00 00 Read Address Pointer\u0026rsquo;s value is an address.\nUse for loop to read 8 bytes one-by-one:\n1 2 3 4 5 6 7 8 // Read the \u0026#34;address\u0026#34; (0x55555556b2c0) from memory (0x7fffffffdbf0) in bytes unsigned char* bytePointer = (unsigned char*)\u0026amp;exampleArray_heap; const int numBytesToRead = 8; unsigned char buffer[numBytesToRead]; for (int i=0; i\u0026lt;numBytesToRead; i++) buffer[i] = *(bytePointer + i); for (int i=0; i\u0026lt;numBytesToRead; i++) std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; static_cast\u0026lt;int\u0026gt;(buffer[i]) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; Output:\n1 c0 b2 56 55 55 55 0 0 Another way is converting the type of pointer to (long long*)\n1 std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; *(long long*)\u0026amp;exampleArray_heap \u0026lt;\u0026lt; std::endl; And it will automatically reverse the number:\n1 55555556b2c0 Python Calls Cpp (2024-01-29)\nAttach the GDB used for C++ application to the running Python debugger, following steps in Example debugging mixed Python C++ in VS Code - Nadiah Pardede Kristensen\nPreparation: Compile the C++ code to a python package:\nMake a folder for compiling the cpp package:\n1 2 3 4 5 6 7 8 9 10 11 (AIkui) yi@yi-Alien:~/Downloads/Cpp_Study$ tree . ‚îú‚îÄ‚îÄ cpp_ext_myadd | ‚îú‚îÄ‚îÄ debug_w_cpp.py ‚îÇ ‚îú‚îÄ‚îÄ myAdd.cpp ‚îÇ ‚îú‚îÄ‚îÄ setup.cfg ‚îÇ ‚îî‚îÄ‚îÄ setup.py ‚îî‚îÄ‚îÄ .vscode ‚îú‚îÄ‚îÄ c_cpp_properties.json ‚îú‚îÄ‚îÄ launch.json ‚îî‚îÄ‚îÄ settings.json where the myAdd.cpp file refers to code\nThe \"setup.py\" looks like: 1 2 3 4 5 6 7 8 from distutils.core import setup, Extension def main(): setup(name=\u0026#34;myadd\u0026#34;, ext_modules=[Extension(\u0026#34;myadd\u0026#34;, [\u0026#34;myAdd.cpp\u0026#34;])], ) if __name__ == \u0026#34;__main__\u0026#34;: main() Install package:\n1 (AIkui) yi@yi-Alie:~/Downloads/Cpp_Study$ pip install ./cpp_ext_myadd/ The customize package myadd will be installed in the current environment. So, VS Code requires \u0026ldquo;Python:Select Interpreter\u0026rdquo; by pressing F1 to match the envrionment.\nPyhon code: 1 2 3 4 import myadd print(\u0026#34;going to ADD SOME NUMBERS\u0026#34;) x = myadd.myadd(5,6) print(x) Add a breakpoint at line 2 before getting into package functions. And add a breakpoint (at z = x + y;) in the cpp file. Otherwise, the debugger won\u0026rsquo;t pause.\nThe method that needs to manually find the process ID:\nAdd 2 configurations for \u0026ldquo;Python\u0026rdquo; and \u0026ldquo;C/C++: gdb (Attach)\u0026rdquo; separately in the launch.json.\nDefault contents generated after clicking the blue button \"Add Configuration...\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(gdb) Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;/home/yi/anaconda3/envs/AIkui/bin/python3.10\u0026#34;, \u0026#34;processId\u0026#34;: \u0026#34;${command:pickProcess}\u0026#34;, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true }, { \u0026#34;description\u0026#34;: \u0026#34;Set Disassembly Flavor to Intel\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-gdb-set disassembly-flavor intel\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Python: Current File\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${file}\u0026#34;, \u0026#34;console\u0026#34;: \u0026#34;integratedTerminal\u0026#34;, \u0026#34;justMyCode\u0026#34;: true }, ] } With focusing on debug_w_cpp.py, select the dubugger \u0026ldquo;Python: Current File\u0026rdquo;, and then click the green start button to kick off the debugging.\nWhen the debugger hits the breakpoint in debug_w_cpp.py, find its process ID:\n1 2 3 4 5 6 7 8 9 10 (base) yi@yi-Alien:~$ ps aux | grep python root 905 0.0 0.0 42744 1680 ? Ss 2023 0:00 /usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggers root 1278 0.0 0.0 121200 1436 ? Ssl 2023 0:00 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal yi 1605315 0.0 0.1 38840 26336 pts/5 S+ 16:17 0:04 /usr/bin/python3.8 -O /usr/bin/ranger yi 1698762 2.9 0.9 1179385796 150624 ? Sl 18:42 0:04 /usr/share/code/code --ms-enable-electron-run-as-node /home/yi/.vscode/extensions/ms-python.vscode-pylance-2023.12.1/dist/server.bundle.js --cancellationReceive=file:bad7d71dedf58b5bb22a36398d8eb2bcf4447338c7 --node-ipc --clientProcessId=1698586 yi 1700442 0.4 0.1 840012 18472 ? Sl 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter yi 1700450 0.3 0.1 250260 18376 pts/6 Sl 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher 45763 -- /home/yi/Downloads/Cpp_Study/cross_lang_debug_host.py yi 1700456 3.1 0.1 414788 30468 pts/6 Sl+ 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy --connect 127.0.0.1:55977 --configure-qt none --adapter-access-token 2bb19c19a6a1592229fd27c7c8e3e0a37c002b320568e3b3c8fe450a3d343404 /home/yi/Downloads/Cpp_Study/cross_lang_debug_host.py yi 1700778 0.0 0.0 12116 2776 pts/7 S+ 18:45 0:00 grep --color=auto python The desired process is the one attached with a token: 1700456\nSwitch the debugger to \u0026ldquo;gdb (Attach)\u0026rdquo;, and then click the green start button again, it will ask the process ID just found.\nThen, a prompt pops:\n1 2 (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ /usr/bin/env /bin/sh /tmp/Microsoft-MIEngine-Cmd-00cvveuw.5cg Superuser access is required to attach to a process. Attaching as superuser can potentially harm your computer. Do you want to continue? [y/N] However, after I input \u0026ldquo;y\u0026rdquo;, the terminal said executing GDB requires elevated permission, but it didn\u0026rsquo;t prompt me to enter password.\n1 2 3 4 5 ==== AUTHENTICATING FOR org.freedesktop.policykit.exec === Authentication is needed to run `/usr/bin/gdb\u0026#39; as the super user Authenticating as: Yi Cao,,, (yi) Password: [1] + Stopped (tty output) /usr/bin/pkexec \u0026#34;/usr/bin/gdb\u0026#34; --interpreter=mi --tty=${DbgTerm} 0\u0026lt;\u0026#34;/tmp/Microsoft-MIEngine-In-tkwd0bog.wdj\u0026#34; 1\u0026gt;\u0026#34;/tmp/Microsoft-MIEngine-Out-n1p4gbjg.vvb\u0026#34; You have stopped jobs. Extension: Python C++ Debugger eliminates the need of manually entering process ID.\nIts default configuration in launch.json is:\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Python C++ Debug\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;pythoncpp\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;pythonConfig\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;cppConfig\u0026#34;: \u0026#34;default (gdb) Attach\u0026#34;, } ] } However, the authentication error persists.\nBypass the authentication:\n1 echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope Refer to: Debugging mixed Python C++ in VS Code. Can\u0026rsquo;t enter sudo password - SO\n\u0026ldquo;This solution doesn\u0026rsquo;t require reboot, but it\u0026rsquo;s not permanent.\u0026rdquo; , as explained in Attaching gdb (C++ debugger) to remote python process in VSCode - SO (OP is Mark Harris) (Found by Perplexity)\nOriginal answer 1 2 3 4 5 6 7 $ gdb --pid=30428 ... Attaching to process 30428 Could not attach to process. If your uid matches the uid of the target process, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try again as the root user. For more details, see /etc/sysctl.d/10-ptrace.conf ptrace: Operation not permitted. In this example, I found that the debugger cannot step downward after entering the cpp file, instead, it gets stuck at the line following the breakpoint.\nHowever, the method works in the project of AIkui\u0026rsquo;s CUDA extension tutorial, where the debugger moves line-by-line after jumping into \u0026ldquo;interpolation_kernel.cu\u0026rdquo; files (function: trilinear_fw_cu()), whereas it can\u0026rsquo;t step into kernel functions, which may require CUDA-GDB, rather than GDB.\nIt seems that after GDB concludes the execution of C++ programm, it will stay at the last line, and won\u0026rsquo;t automatically return to the Python debugger. So, I have to click the \u0026ldquo;disconnect\u0026rdquo; button on the control bar to free the GDB. Other problems:\nIf the file currently open in the editor is not the Python file \u0026ldquo;debug_w_cpp.py\u0026rdquo;, but rather \u0026ldquo;myAdd.cpp\u0026rdquo;, an error pops upon clicking the start button:\nThe following command for installing cpp packege, referring How to pass \u0026ndash;debug to build_ext when invoking setup.py install?, requires the path to source file relative path to workspaceFolder in the \u0026ldquo;setup.py\u0026rdquo;: ext_modules=[Extension(\u0026quot;myadd\u0026quot;, [\u0026quot;./cpp_ext_myadd/myAdd.cpp\u0026quot;])]\n1 (AIkui) yi@yi-Alienware:~/Downloads/Cpp_Study$ python3.10 ./cpp_ext_myadd/setup.py build_ext --debug install (2024-04-18)\nWill\u0026rsquo;s practice: ‰ΩøÁî®PythonCppDebuggerËÅîÂêàË∞ÉËØïPython‰∏éCppÔºå‰ª•ÂèäÂ∫îÁî®Âà∞3DGSÁöÑËã•Âπ≤Â∞ùËØï - willÁöÑÊñáÁ´† - Áü•‰πé\nÂà©Áî®vscodeÁöÑÊèí‰ª∂PythonCppDebuggerÔºågdbÈÄâÁî®cudaÊãìÂ±ïÁöÑcuda-gdbÔºõ Áõ∏ÊØî‰∫éÔºö‚ÄúÂà©Áî®vscodeÁöÑÊèí‰ª∂PythonCppDebuggerÔºågdbÈÄâÁî®cppÊãìÂ±ïÁöÑcppdbg‚ÄùÔºå Á®ãÂ∫è‰ºöÁ™ÅÁÑ∂Â¥©Ê∫ÉÔºå(chatGPT)ÊÄÄÁñëÊòØgsÊú¨Ë∫´ÊòæÂ≠òÁÆ°ÁêÜËøá‰∫éÂ§çÊùÇÔºåÈïøÊó∂Èó¥ÁöÑdebugÂÆπÊòìÂ¥©Ê∫ÉÊâÄËá¥„ÄÇ ÊúÄÁªàÁî±‰∫éÂÆûÂú®‰∏çÁ®≥ÂÆöÔºåÁ¨îËÄÖÊîæÂºÉ‰ΩøÁî®ËØ•ÊñπÊ≥ïÔºåÈÄâÁî®Á¨®ÂäûÊ≥ïÔºö ÊääpythonÁ´ØÊï∞ÊçÆÂ≠òÊàêtxtÔºåÁÑ∂ÂêéÂè¶Ëµ∑‰∏Ä‰∏™c++È°πÁõÆË∞ÉËØïcuda‰ª£Á†Å„ÄÇ Âè¶ÂèØËßÅÁ¨îËÄÖÁöÑissueÔºö Question aboud cuda-gdb extension in attach process ¬∑ Issue #32 ¬∑ benibenj/vscode-pythonCpp\n","date":"2023-11-13T11:49:00Z","permalink":"https://zichen34.github.io/writenotes/lang/c++_debug/","title":"memo: C++ | Debug with VS Code"},{"content":"C++ Extensions - Docs\nInstall LibTorch libtorch is the C++ API for PyTorch. Guide: INSTALLING C++ DISTRIBUTIONS OF PYTORCH\nDownload the binary files with modified URL for specified version:\n1 2 3 4 5 6 7 # Download here (cxx11 ABI): wget https://download.pytorch.org/libtorch/cu116/libtorch-cxx11-abi-shared-with-deps-1.12.1%2Bcu116.zip upzip libtorch-cxx11-abi-shared-with-deps-1.12.1+cu116.zip # Optional sudo mv libtorch/ /usr/local/ Create folder \u0026ldquo;example-app\u0026rdquo;, and write code into \u0026ldquo;example-app.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;iostream\u0026gt; int main(){ torch::Tensor tensor = torch::rand({2,3}); std::cout \u0026lt;\u0026lt; tensor \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size of Tensor type: \u0026#34;\u0026lt;\u0026lt; sizeof(torch::Tensor) \u0026lt;\u0026lt; std::endl; } Set dependence for LibTorch via CMakeLists.txt:\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.18 FATAL_ERROR) project(MyLibTorchApp) # name find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_executable(MyLibTorchApp main.cpp) target_link_libraries(MyLibTorchApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) set_property(TARGET MyLibTorchApp PROPERTY CXX_STANDARD 17) Build (externally) inside the dir \u0026ldquo;example-app/build\u0026rdquo;\n1 2 3 4 # mkdir build # cd build cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch .. cmake --build . --config Release (Optional) For C/C++ extension, set up includePath in \u0026ldquo;c_cpp_properties.json\u0026rdquo; for correct intellisense:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/libtorch\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], \u0026#34;defines\u0026#34;: [], \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/bin/g++\u0026#34;, \u0026#34;cStandard\u0026#34;: \u0026#34;c17\u0026#34;, \u0026#34;cppStandard\u0026#34;: \u0026#34;gnu++14\u0026#34;, \u0026#34;intelliSenseMode\u0026#34;: \u0026#34;linux-gcc-x64\u0026#34; } ], \u0026#34;version\u0026#34;: 4 } Debug LibTorch (2023-11-12)\nlaunch.json Modify launch.json in .vscode to debug C/C++ binary file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;LibTorch Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/build/MyLibTorchApp\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, } ] } program is the compiled binary file, not the main.cpp file. Debug C++ in Visual Studio Code - Docs General attributes: Options for Debugging task.json Set up \u0026ldquo;tasks.json\u0026rdquo; for building project with cmake:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;cmake-configure\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-DCMAKE_BUILD_TYPE=Debug\u0026#34;, \u0026#34;-DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch\u0026#34;, \u0026#34;..\u0026#34;, ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;cmake-build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--build\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;--config\u0026#34;, \u0026#34;Debug\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;dependsOn\u0026#34;: [\u0026#34;cmake-configure\u0026#34;, \u0026#34;cmake-build\u0026#34;] } ] } Multiple tasks can be combined to one, which then is able tobe passed to preLaunchTask in \u0026ldquo;launch.json\u0026rdquo;. (Can\u0026rsquo;t pass 2 tasks at once.)\nIntegrate with External Tools via Tasks - Docs\nThe above 2 tasks both are custom tasks executed in shell.\nCMake requires two steps, so 2 tasks are needed.\nThe tag -DCMAKE_BUILD_TYPE=Debug is needed for generating debug info. Otherwise, the breakpoints won\u0026rsquo;t take effect:\n1 Module containing this breakpoint has not yet loaded or the breakpoint address could not be obtained. However, sometimes, camke build failed:\nExpand the error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 * Executing task: cmake --build . --config Debug -- Caffe2: CUDA detected: 11.6 -- Caffe2: CUDA nvcc is: /usr/local/cuda-11.6/bin/nvcc -- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.6 CMake Error at /usr/local/libtorch/share/cmake/Caffe2/public/cuda.cmake:88 (message): Caffe2: Couldn\u0026#39;t determine version from header: Change Dir: \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Run Build Command(s): /usr/local/bin/cmake -E env VERBOSE=1 /usr/bin/make -f Makefile cmTC_a6e2f/fast make[1]: Entering directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; /usr/bin/make -f CMakeFiles/cmTC_a6e2f.dir/build.make CMakeFiles/cmTC_a6e2f.dir/build make[2]: Entering directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Building CXX object CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o /usr/bin/c++ -I/usr/local/cuda-11.6/include -o CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o -c /home/yi/Downloads/example-app/build/detect_cuda_version.cc Assembler messages: Fatal error: can\u0026#39;t create CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o: No such file or directory make[2]: *** [CMakeFiles/cmTC_a6e2f.dir/build.make:78: CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o] Error 1 make[2]: Leaving directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; make[1]: *** [Makefile:127: cmTC_a6e2f/fast] Error 2 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include) /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:4 (find_package) -- Configuring incomplete, errors occurred! make: *** [Makefile:179: cmake_check_build_system] Error 1 * The terminal process \u0026#34;/usr/bin/bash \u0026#39;-i\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cmake --build . --config Debug\u0026#39;\u0026#34; terminated with exit code: 2. I tried g++ command, but I had problem with linking torch libraries (-L) and including header files (-I).\n1 2 3 4 5 6 7 * Executing task: C/C++: g++ build active file Starting build... /usr/bin/g++ -fdiagnostics-color=always -g /home/yi/Downloads/example-app/main.cpp -o /home/yi/Downloads/example-app/main /home/yi/Downloads/example-app/main.cpp:1:10: fatal error: torch/torch.h: No such file or directory 1 | #include \u0026lt;torch/torch.h\u0026gt; | ^~~-~~-~~-~~-~~-~~ Ref:\nCMake+VSCodeÁºñËØëËøêË°åC++Á®ãÂ∫èÁÆÄÂçïÊïôÁ®ã - Ëø∑Ê•ºÁöÑÊñáÁ´† - Áü•‰πé\nLinuxÁéØÂ¢É‰∏ã‰ΩøÁî®VScodeË∞ÉËØïCMakeÂ∑•Á®ã - ColorfulÁöÑÊñáÁ´† - Áü•‰πé\nlibtorch Â∏∏Áî®apiÂáΩÊï∞Á§∫‰æãÔºàÂè≤‰∏äÊúÄÂÖ®„ÄÅÊúÄËØ¶ÁªÜÔºâ- Êó†Â∑¶Êó†Âè≥ - ÂçöÂÆ¢Âõ≠\nInspect Tensor (2023-11-19)\nSave tensors to a file.\n(LibTorch -\u0026gt; PyTorch tensors) How to save a C++ Libtorch Tensor and load it into a Python Pytorch project? In GDB: *(float_t*)x.data_ptr()\n1 2 int main(){ const torch::Tensor bkg = torch::full({3}, 0., torch::device(torch::kCUDA)); GDB: *(float_t*)bkg.data_ptr() return 0.\nIs it possible to view values of a at::Tensor in Visual codes debug variable view (linux)?\n","date":"2023-11-09T20:04:00Z","permalink":"https://zichen34.github.io/writenotes/lib/libtorch_debug/","title":"memo: LibTorch | Debug"},{"content":"Create Tensor shape (2024-01-24)\n.sizes() is an \u0026ldquo;vector-like\u0026rdquo; object of class: IntArrayRef. It can be created with curly braces, e.g., {5.2}, or an std::vector\u0026lt;int64_t\u0026gt;{1,2,4}.\n1 2 3 4 5 6 7 #include \u0026lt;cassert\u0026gt; std::vector\u0026lt;int64_t\u0026gt; myVec = {5,2}; assert(torch::ones({5,2}).sizes() == myVec); // pass std::cout \u0026lt;\u0026lt; \u0026#34;Equal\u0026#34; \u0026lt;\u0026lt; std::endl; c10::IntArrayRef myArrRef = {5,2}; assert(myVec == myArrRef); // pass \u0026ldquo;Create vector out of the IntArrayRef constructor, , otherwise the vector is destroyed immediately afterward.\u0026rdquo; How to compare a torch::tensor shape against some other shapes? - SO Use tensor.size(i) (better than tensor.sizes()[i]) to access one of dimensions. Docs\n1 2 3 std::cout \u0026lt;\u0026lt; torch::ones(5).sizes() \u0026lt;\u0026lt; std::endl; // [5] std::cout \u0026lt;\u0026lt; torch::ones({5,2}).sizes() \u0026lt;\u0026lt; std::endl; // [5, 2] std::cout \u0026lt;\u0026lt; myTensor.size(1) \u0026lt;\u0026lt; std::endl; // 2 Use a Lambda function to reshape a tensor and return the updated shape:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 std::function\u0026lt;c10::IntArrayRef(c10::IntArrayRef newSize)\u0026gt;getResizedShape(torch::Tensor\u0026amp; t) { auto lambda = [\u0026amp;t](c10::IntArrayRef newSize){ t.resize_(newSize); return t.sizes(); }; return lambda; } int main() { torch::Tensor myTensor = torch::rand({1,2,3}); std::cout \u0026lt;\u0026lt; myTensor \u0026lt;\u0026lt; std::endl; auto getNewSize = getResizedShape(myTensor); std::cout \u0026lt;\u0026lt; getNewSize({3,2}) \u0026lt;\u0026lt; std::endl; } Output: 1 2 3 4 5 (1,.,.) = 0.9838 0.7854 0.6991 0.8325 0.1196 0.3780 [ CPUFloatType{1,2,3} ] [3, 2] Create from factory func Tensor Creation API ‚Äî PyTorch main documentation Test repo General schema:\n1 torch::\u0026lt;factory-func-name\u0026gt; (\u0026lt;func-specific-args\u0026gt;, \u0026lt;sizes\u0026gt;, \u0026lt;tensor-opt\u0026gt;) \u0026lt;factory-func-name\u0026gt; e.g., arange, empty, \u0026hellip; Create a tensor from the factory function torch::rand()\n1 2 3 4 5 6 7 #include \u0026lt;torch/torch.h\u0026gt; // unzipped to /usr/local/libtorch int main(){ const torch::Tensor a = torch::randint(1, 9, {1,2,3}); std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt;\u0026#34;size:\u0026#34; \u0026lt;\u0026lt; a.sizes() \u0026lt;\u0026lt; std::endl; } CMakeLists.txt\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.18 FATAL_ERROR) project(MyLibTorchApp) # name find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_executable(${PROJECT_NAME} main.cpp) target_link_libraries(${PROJECT_NAME} \u0026#34;${TORCH_LIBRARIES}\u0026#34;) set_property(TARGET ${PROJECT_NAME} PROPERTY CXX_STANDARD 17) Build:\n1 2 3 4 mkdir -p build cd build cmake -DCMAKE_PREFIX_PATH=/usr/local/libtorch .. cmake --build . --config Release Or in a modern way (under the workspace; no need cd to ./build):\n1 2 cmake -B build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja cmake --build build # build in ./build Execute it: ./MyLibTorchApp\nOutput: 1 2 3 4 5 (1,.,.) = 8 7 6 2 7 2 [ CPUFloatType{1,2,3} ] size: [1, 2, 3] Create with 4 Properties Pass an instance TensorOptions to the factory function:\n1 2 3 4 5 6 7 8 torch::TensorOptions options = torch::TensorOptions().dtype(torch::kFloat32) .layout(torch::kStrided) .device(torch::kCUDA, 0) .requires_grad(true); torch::Tensor a = torch::full({3,4}, 123, options); std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; a.device() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; a.requires_grad() \u0026lt;\u0026lt; std::endl; Only float and complex can .requires_grad. full(...) is not implemented for sparse layout Output: 1 2 3 4 5 6 123 123 123 123 123 123 123 123 123 123 123 123 [ CUDAFloatType{3,4} ] cuda:0 1 Omitting torch::TensorOptions(), which will be pre-configured and returned if calling the 4 properties directly from torch:: namespace.\n1 torch::Tensor a = torch::arange(1,9, torch::dtype(torch::kInt32).device(torch::kCUDA, 0)); If only one property needs to be specified, its property name (torch::dtype()) can be omitted even further.\n1 torch::Tensor a = torch::arange(8, torch::kInt32); Convert tensor by .to Use TensorOptions and .to() to create a new tensor on new memory based on a source tensor.\nConvert dtype:\n1 2 3 4 5 6 7 8 torch::Tensor src_tensor = torch::randn({3,2}); torch::Tensor a = src_tensor.to(torch::kInt32); // combinational torch::Tensor a = src_tensor.to(torch::dtype(torch::kInt32).device(torch::kCUDA,0)); auto opts = a.options(); std::cout \u0026lt;\u0026lt; opts \u0026lt;\u0026lt; std::endl; What does \u0026ldquo;new\u0026rdquo; mean? Options Alteration 1 2 3 4 5 6 torch::Tensor a = torch::randn(3); // change the property of dtype in the TensorOptions object auto int_opts = a.options().dtype(torch::kInt32); auto float_opts = a.options().dtype(torch::kFloat32); size of a tensor (2024-01-24)\nLibTorch sizeof tensor - SO\n1 2 3 4 5 6 torch::Tensor myTensor = torch::rand({1,2,3}, torch::kFloat32); int sizeOfFloat = torch::elementSize(torch::typeMetaToScalarType(myTensor.dtype())); std::cout \u0026lt;\u0026lt; \u0026#34;size of the kFloat32 type: \u0026#34; \u0026lt;\u0026lt; sizeOfFloat \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Number of elements in the tensor: \u0026#34; \u0026lt;\u0026lt; myTensor.numel() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Bytes occupied by the tensor: \u0026#34; \u0026lt;\u0026lt; myTensor.numel() * sizeOfFloat \u0026lt;\u0026lt; std::endl; Output: 1 2 3 size of the kFloatt32 type: 4 Number of elements in the tensor: 6 Bytes occupied by the tensor: 24 Manipulate Tensor ATen means \u0026ldquo;A Tensor Library\u0026rdquo;. The Tensor class under its namespace at:: lays the base for all tensor operations. ezyang\u0026rsquo;s blog\nResize (2023-11-12)\nAPI: Class Tensor in Namespace ATen - Docs\nReshape a tensor in place:\n1 2 3 4 torch::Tensor t = torch::arange(6).resize_({1,2,3}); std::cout \u0026lt;\u0026lt; t \u0026lt;\u0026lt; std::endl; t.resize_({6}); std::cout \u0026lt;\u0026lt; t \u0026lt;\u0026lt; std::endl; Output: 1 2 3 4 5 6 7 8 9 10 11 (1,.,.) = 0 1 2 3 4 5 [ CPULongType{1,2,3} ] 0 1 2 3 4 5 [ CPULongType{6} ] It can be resized to more than its elements:\n1 2 3 4 5 6 7 torch::Tensor t = torch::arange(6).resize_({1,2,3}); t.resize_({10}); std::cout \u0026lt;\u0026lt; \u0026#34;Allocated bytes:\u0026#34; \u0026lt;\u0026lt; t.numel() * torch::elementSize(torch::typeMetaToScalarType(t.dtype())) \u0026lt;\u0026lt; std::endl; for (size_t i = 0; i \u0026lt; t.numel(); ++i) { std::cout \u0026lt;\u0026lt; t[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Output 1 2 3 4 5 6 7 8 9 10 11 Allocated bytes:80 0 [ CPULongType{} ] 1 [ CPULongType{} ] 2 [ CPULongType{} ] 3 [ CPULongType{} ] 4 [ CPULongType{} ] 5 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] Flatten Reshape a tensor to 1D and return the pointer to it. Code from 3DGS\nUse a lambda function to resize the tensor and return the data pointer.\n.data_ptr() points to data of the tensor x, while x doesn\u0026rsquo;t point to data directly.\nreinterpret_cast\u0026lt;char*\u0026gt; converts the tensor-type pointer .data_ptr() to a char-type pointer pResizedX, which will read memory byte by byte.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdio\u0026gt; std::function\u0026lt;char*(size_t N)\u0026gt; resizeFunctional(torch::Tensor\u0026amp; t){ std::cout \u0026lt;\u0026lt; \u0026#34;size of the reference of the input tensor: \u0026#34; \u0026lt;\u0026lt; sizeof(t) \u0026lt;\u0026lt; std::endl; auto lambda = [\u0026amp;t](size_t N){ // Number of elements t.resize_({ (long long) N}); // shape: {N} std::cout \u0026lt;\u0026lt; \u0026#34;N is: \u0026#34; \u0026lt;\u0026lt; N \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size of t: \u0026#34; \u0026lt;\u0026lt; sizeof(t) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;dtype of t: \u0026#34; \u0026lt;\u0026lt; t.dtype() \u0026lt;\u0026lt; std::endl; return reinterpret_cast\u0026lt;char*\u0026gt;(t.contiguous().data_ptr()); // read memory byte by byte }; return lambda; } int main(){ torch::Tensor a = torch::arange(33,40, torch::kByte).resize_({1,2,3}); std::cout \u0026lt;\u0026lt; \u0026#34;Test tensor: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Tensor is a ptr, so its size is: \u0026#34; \u0026lt;\u0026lt; sizeof(torch::Tensor) \u0026lt;\u0026lt; std::endl; auto resizer = resizeFunctional(a); // lambda expression char* pTensor = resizer(a.numel()); // pointer to tensor\u0026#39;s data // Memory address printf(\u0026#34;char*: %p \\n\u0026#34;, pTensor); std::cout \u0026lt;\u0026lt; \u0026#34;size of pointer of a char: \u0026#34; \u0026lt;\u0026lt; sizeof(pTensor) \u0026lt;\u0026lt; std::endl; // The return address is the data_ptr() printf(\u0026#34;data_ptr(): %p \\n\u0026#34;, a.data_ptr()); // Print out the data stored in the returned address // Since a data is only 1 byte, the 1st byte is the 1st data. char data = *pTensor; // the first byte. // Note: unicode of 0-31 are invisible, so I test char 33-40 printf(\u0026#34;The first byte: %c \\n\u0026#34;, data); std::cout \u0026lt;\u0026lt; data \u0026lt;\u0026lt; std::endl; // Convert value (char) to integer printf(\u0026#34;Decimal: %d \\n\u0026#34;, data); // 33 std::cout \u0026lt;\u0026lt; \u0026#34;Convert 1st byte to int: \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;int\u0026gt;(*pTensor) \u0026lt;\u0026lt; std::endl; // Indexing elements like an array: std::cout \u0026lt;\u0026lt; \u0026#34;Use [0]: \u0026#34; \u0026lt;\u0026lt; pTensor[0] \u0026lt;\u0026lt; std::endl; // ! for (size_t i = 0; i \u0026lt; 6; ++i) { std::cout \u0026lt;\u0026lt; static_cast\u0026lt;char\u0026gt;(pTensor[i]) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; } Output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 (base) yi@yi:~/Downloads/LibTorch_Study$ ./build/MyLibTorchApp Test tensor: (1,.,.) = 33 34 35 36 37 38 [ CPUByteType{1,2,3} ] Tensor is a ptr, so its size is: 8 size of the reference of the input tensor: 8 N is: 6 size of t: 8 dtype of t: unsigned char char*: 0x55f202301640 size of pointer of a char: 8 data_ptr(): 0x55f202301640 The first byte: ! ! Decimal: 33 Convert 1st byte to int: 33 Use [0]: ! ! \u0026#34; # $ % \u0026amp; N is the total number of elements in the tensor t.\nresize_ requires the shape argument size to be c10::IntArrayRef type, which is an array of int64_t, i.e., signed 8-byte integer.\nTherefore, from the unsigned long int size_t (N) to a signed int64_t is a narrowing conversion.\nlong is at least 32-bit. In my computer, long is 8-byte. And long long is at least 64-bit. Because the signedness modifier is omitted, both long and long long are signed. Thus, the type casting (long long) N is equivalent to (int64_t) N\nint64_t is exact 8 bytes for all compilers, unlike long somewhere is 4-bytes. Definition of int64_t - SO\n1 2 3 4 5 6 std::cout \u0026lt;\u0026lt; sizeof(size_t) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(signed long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(unsigned long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(long long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(int64_t) \u0026lt;\u0026lt; std::endl; // 8 Attributes of tensor x:\n--- title: tensor x --- classDiagram direction RL class T[\"at::TensorBase\"]{ + c10::intrusive_ptr impl_ } class P[\"c10::intrusive_ptr\"]{ + c10::TensorImpl* target_ } note for P \"0x555557729610\" P --\u003e T View the memory via GDB command -exec:\n1 2 3 -exec x/64xb 0x555557729610 0x555557729610:\t0x60\t0x35\t0xfb\t0xf7\t0xff\t0x7f\t0x00\t0x00 0x555557729618:\t0x01\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x7FFFF7FB3560 is not the address storing x\u0026rsquo;s data. Char pointer pResizedX = 0x555557729500 points to the memory storing the x\u0026rsquo;s data:\n1 2 3 4 5 6 7 8 9 -exec x/64b 0x555557729500 0x555557729500:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729508:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729510:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729518:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729520:\t0\t0\t0\t0\t0\t0\t0\t0 0x555557729528:\t81\t0\t0\t0\t0\t0\t0\t0 0x555557729530:\t0\t0\t0\t0\t0\t0\t0\t0 0x555557729538:\t16\t80\t87\t85\t85\t85\t0\t0 There are four 3. A tensor takes 8-byte integer? DEBUG CONSOLE panel:\n1 2 x.data_ptr {void *(const at::TensorBase * const)} 0x55555555b392 \u0026lt;at::TensorBase::data_ptr() const\u0026gt; Don\u0026rsquo;t know what that address is? Get value (2023-11-12)\nReturn the pointer to data: Tensor.data\u0026lt;T\u0026gt;(), which is deprecated and changed to Tensor.data_ptr\u0026lt;T\u0026gt;() internally. Source code\n1 2 3 4 5 int main(){ torch::Tensor x = torch::full({1,3}, 2, torch::dtype(torch::kFloat)); std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x.contiguous().data\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x.contiguous().data_ptr\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl;} Output: 1 2 3 4 5 ~/l/build$ ./MyLibTorchApp 2 2 2 [ CPUFloatType{1,3} ] 0x557d9beab500 0x557d9beab500 .item\u0026lt;dtype\u0026gt;() can get scalar data, not vector. Torch C++: Getting the value of a int tensor by using *.data() - SO\n1 2 3 4 5 6 int main(){ torch::Tensor x = torch::randn({1,3}); std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x[0][0].item\u0026lt;int\u0026gt;() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x[0][0].item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; } Output: 1 2 3 4 5 ~/l/build$ ./MyLibTorchApp -0.6926 -0.2304 1.2920 [ CPUFloatType{1,3} ] 0 -0.692582 Use a vector to hold result tensor after inference: Part-2 Garry\u0026rsquo;s Blog\n1 2 3 4 5 6 7 8 9 // Extract size of output (of the first and only batch) // and preallocate a vector with that size auto output_size = output.sizes()[1]; auto output_vector = std::vector\u0026lt;float\u0026gt;(output_size); // Fill result vector with tensor items using `Tensor::item` for (int i = 0; i \u0026lt; output_size; i++) { output_vector[i] = output[0][i].item\u0026lt;float\u0026gt;(); } Copy cv::Mat to a tensor: Part-3 Garry\u0026rsquo;s Blog\n1 2 3 4 torch::Tensor tensor = torch::empty({mat.row, mat.cols, mat.channels()}, torch::TensorOptions().dtype(torch::kByte).device(torch::kCPU)); std::memcpy(tensor.data_ptr(), reinterpret_cast\u0026lt;void*\u0026gt;(mat.data), tensor.numel() * sizeof(at::kByte)); A more detailed post: Data Transfer to and from PyTorch - SimonWenkel.com libtorch Â∏∏Áî®apiÂáΩÊï∞Á§∫‰æãÔºàÂè≤‰∏äÊúÄÂÖ®„ÄÅÊúÄËØ¶ÁªÜÔºâ - ÂçöÂÆ¢Âõ≠\nAllentDan/LibtorchTutorials\nFrom PyTorch to Libtorch: tips and tricks - Marc Lalonde - Medium\nAnnouncing a series of blogs on PyTorch C++ API - Kushashwa Ravi Shrimali\nempty tensor (2024-01-28)\nIn 3DGS, the project diff-gaussian-rasterization is built as an cpp extension according to setup.py, which is called in Python program. Whereas the CMakeList.txt serves for building the project as a static library (.so) to be inserted into the C++ executable application.\nOriginally, I want to debug the diff-gaussian-rasterization as a static library, so I need to construct input tensors that mimic those passed from Python, where some tensors are assigned as None, such as cov3D_precomp.\nHowever, I don\u0026rsquo;t know how to create a \u0026ldquo;None\u0026rdquo; tensor in the C++ program (Perplexity said: \u0026ldquo;You can\u0026rsquo;t directly set a tensor to NULL as you would do in Python by setting a variable to None.\u0026rdquo;).\nI have tried torch::empty({0}), but its data_ptr() is not the desired nullptr. Consequently, a if judge statement later won\u0026rsquo;t enter into the branch that would happened when the extension is called by Python.\n(2024-01-31) It turns out that I forgot the re-build and make the application again. So, CUDA-GDB still steps through the old application.\nThe .data_ptr() of torch::empty({0}) and torch::full({0},0) both are nullptr.\n(2024-01-30)\nJust found the None Python tensors in 3DGS are reassigned with torch.Tensor([]):\n1 2 if cov3D_precomp is None: cov3D_precomp = torch.Tensor([]) The torch.Tensor([]) will be passed into the C++ package function: _C.rasterize_gaussians() (i.e., the forward method RasterizeGaussiansCUDA)\nA demo where Python calls C++ package referring to AIkui\u0026rsquo;s CUDA extension tutorial:\nExpand codes The code can be evaluated by commands: chmod +x test.sh and ./test.sh P t y o t r h c o h n . T e n s o r ( [ ] ) p a s s t o r L c i h b : T : o e r m c p h t y ( { 0 } ) r e t u r n t o r P c y h t . h T o e n n s o r ( [ ] ) ","date":"2023-11-09T19:40:00Z","permalink":"https://zichen34.github.io/writenotes/lib/libtorch_tensor/","title":"memo: LibTorch | Tensor APIs and Examples"},{"content":"Pointer (2023-11-09) Review:\nA pointer variable and a regular variable are 2 ways of indexing memory.\nA pointer variable stores the address of the start byte of a variable. And its type indicates how many bytes the complete data takes.\nPointer variable is an address, a unsigned integer, whose size depends on system architecture (32-/64-bit addr). While a variable represents the whole data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // \u0026#34;pointer.c\u0026#34; #include \u0026lt;stdio.h\u0026gt; int main(){ int a = 1; int* p = \u0026amp;a; printf(\u0026#34;a = %d \\n\u0026#34;, a); printf(\u0026#34;p = %p \\n\u0026#34;, p); printf(\u0026#34;address of a = %p \\n\u0026#34;, \u0026amp;a); printf(\u0026#34;#bytes of a = %ld \\n\u0026#34;, sizeof(a)); printf(\u0026#34;#bytes of p = %ld \\n\u0026#34;, sizeof(p)); int b[2][2] = {1,2,3,4}, *ptr_b = \u0026amp;b[2][2]; printf(\u0026#34;#bytes of b = %ld \\n\u0026#34;, sizeof(b)); printf(\u0026#34;#bytes of ptr_b = %ld \\n\u0026#34;, sizeof(ptr_b)); } Build gcc pointer.c -o pointer and run ./pointer.\nOutput: 1 2 3 4 5 6 7 a = 1 p = 0x7ffe45d7c88c address of a = 0x7ffe45d7c88c #bytes of a = 4 #bytes of p = 8 #bytes of b = 16 #bytes of ptr_b = 8 The *p (dereferenced p) and i are equivalent at all time.\n1 2 3 int i = 1; int* p = \u0026amp;i; bool cf_res = (*p == i); // 1 (2023-11-11) char* indicates the direction: from pointer * to char. Similarly, char** means from a pointer * to another pointer *, then to char.\nPOINTERS in C++ - YouTube - The Cherno\nvoid* ptr = 0. Address 0 is NULL or nullptr. void means dismissing the type of the data it points to.\nReference Note: C doesn\u0026rsquo;t have reference. SO\nA reference variable is an alias. Compared to pointer, it\u0026rsquo;s an already dereferenced address.\n1 2 3 4 5 int i = 10 int* ptr = \u0026amp;i; // an address int\u0026amp; ref = i; // an alias Declaration and initialization must be performed at the same time.\n1 2 3 4 5 int\u0026amp; ref; // incorrect ref = i; int* ptr; ptr = \u0026amp;i; // ok Reference variable cannot be reassigned.\n1 2 3 4 5 6 int i =0, j =1; int\u0026amp; ref1 = i; int\u0026amp; ref1 = j; // error: redeclaration of ‚Äòint\u0026amp; ref1‚Äô int\u0026amp; ref2 = ref1; // ok std::cout \u0026lt;\u0026lt; ref2 \u0026lt;\u0026lt; std::endl; // 0 Reference shares the same memory as the variable, not occupying another memory.\nPass reference into function to modify the source data directly. Pointer needs deferencing to access the data.\nPython only has reference without pointer.\n1 2 3 4 5 6 7 8 9 10 11 #include\u0026lt;iostream\u0026gt; int main(){ int i = 0; std::cout \u0026lt;\u0026lt; \u0026amp;i \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e74 int* ptr = \u0026amp;i; std::cout \u0026lt;\u0026lt; \u0026amp;ptr \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e78 int\u0026amp; ref = i; std::cout \u0026lt;\u0026lt; \u0026amp;ref \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e74 } Use reference for function parameters and return types.\nUse references when you can, and pointers when you have to. C++ FAQ\nIteration:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; int main(){ vector\u0026lt;int\u0026gt; myVec; myVec = {1,2,3,4,5}; // for (int i:myVec) // will copy item to i for (int\u0026amp; i : myVec) // use reference to avoid copy data std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } Pass reference to function to avoid copying:\n1 2 3 4 5 6 7 8 9 void Function(const std::vector\u0026lt;int\u0026gt;\u0026amp; myVec){ for (int i : myVec) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt;std::endl; } int main(){ std::vector\u0026lt;int\u0026gt; myVec={1,2,3}; Function(myVec); } A pointer to a class/struct uses -\u0026gt; to access its members, whereas a reference uses . (Same as python.) What are the differences between a pointer variable and a reference variable? - SO\nRef:\nPointers vs References in C++ - GeeksforGeeks Raw Array (2023-11-11)\nThe variable name exampleArray is an address of the starting byte, so it\u0026rsquo;s a pointer.\n1 f = Array is a row of contiguous memory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #include \u0026lt;iostream\u0026gt; int main(){ // Create an array on stack, which // will be destoryed when getting out of the scope int exampleArray[4]; for (int i=0; i\u0026lt;5; i++) exampleArray[i] = i; cout \u0026lt;\u0026lt; exampleArray \u0026lt;\u0026lt; \u0026#34; is the address of the array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026amp;(exampleArray[0]) \u0026lt;\u0026lt; \u0026#34; is the address of the first element\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; *exampleArray \u0026lt;\u0026lt; \u0026#34; is the first element by dereferencing the pointer\u0026#34; \u0026lt;\u0026lt; endl; // Get the 3rd elements can do arithmatic on pointer, // where the size of the type will be multiplied automatically. cout \u0026lt;\u0026lt; *(exampleArray + 2) \u0026lt;\u0026lt; \u0026#34; is the 3rd element\u0026#34;\u0026lt;\u0026lt; endl; // If access the 2nd elements by calculating in bytes, // cast integer pointer to char pointer with is 1-byte type. // After locating the address, cast back to integer pointer for assigning a integer value. *(int*)((char*)exampleArray + 8) = 30; cout \u0026lt;\u0026lt; exampleArray[2] \u0026lt;\u0026lt; \u0026#34; is the modified 3rd element\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; sizeof(exampleArray) \u0026lt;\u0026lt; \u0026#34; bytes taken by the entire array\u0026#34;\u0026lt;\u0026lt; endl; // For array created on stack, the number of elements can be known because stack pointer contains offset: int num_elements = sizeof(exampleArray) / sizeof(int); cout \u0026lt;\u0026lt; num_elements \u0026lt;\u0026lt; \u0026#34; elements in the array on stack\u0026#34; \u0026lt;\u0026lt; endl; // Create an array on heap with the `new` keyword, // This array is the same as exampleArray, except for its lifetime that lasts until calling `delete[] arrayname`. // So if an array created inside function needs to be returned, it must be created on heap int* exampleArray_heap = new int[4]; // is a pointer (address 0x55555556b2c0) to heap, not the 1st element 0, for (int i=0; i\u0026lt;4; i++) exampleArray_heap[i] = i; // Read the \u0026#34;address\u0026#34; from memory in bytes std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; *(long long*)\u0026amp;exampleArray_heap \u0026lt;\u0026lt; std::endl; cout \u0026lt;\u0026lt; \u0026amp;exampleArray_heap \u0026lt;\u0026lt; \u0026#34; stores the 8-byte address (the pointer) to the array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; exampleArray_heap \u0026lt;\u0026lt; \u0026#34; is the address (pointer) to the array on heap\u0026#34; \u0026lt;\u0026lt; endl; // cast type to integer pointer cout \u0026lt;\u0026lt; *(exampleArray_heap+0) \u0026lt;\u0026lt; \u0026#34; is the first element in the array\u0026#34; \u0026lt;\u0026lt; endl; // one more jump will reduce performance // exampleArray_heap is a pointer of pointer (memory indirection). An 32-bit address taking 4 bytes cout \u0026lt;\u0026lt; sizeof(exampleArray_heap) \u0026lt;\u0026lt; \u0026#34; bytes for the pointer of the array on heap\u0026#34; \u0026lt;\u0026lt; endl; // delete[] exampleArray_heap; // If array is not created on stack, or only has the pointer to an array, // the number of elements can\u0026#39;t be computed as sizeof(array)/sizeof(int). So it must be maintained. static const int exampleSize = 5; // must be known at compile-time int myArray[exampleSize]; } 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 b f 5 6 Create an array on heap.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Entity{ public: int* exampleArray_obj = new int[4]; Entity(){ // construction function for (int i=0; i\u0026lt;4; i++) exampleArray_obj[i] = i; } }; int main(){ Entity e; cout \u0026lt;\u0026lt; e.exampleArray_obj \u0026lt;\u0026lt; \u0026#34; is the address (pointer) to array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; *(e.exampleArray_obj) \u0026lt;\u0026lt; \u0026#34; is the 1st element of array\u0026#34; \u0026lt;\u0026lt; endl; // 0 } std::array has .size().\n1 2 3 4 5 6 7 8 9 #include \u0026lt;iostream\u0026gt; #include \u0026lt;array\u0026gt; int main(){ std::array\u0026lt;int, 4\u0026gt; myArray; for(int i=0; i\u0026lt;myArray.size(); i++) myArray[i] = i; } Ref:\nArrays in C++ - The Cherno Pointer Alignment (2023-11-14)\n(Unsure about my naming for this.)\nBecause the algorithm specifies processing 128 data at a time, i.e., the factor alignment = 128. Thus, each time 128*sizeof(dtype) memory will be accessed.\nTherefore, the pointers to data should all be multiples of 128.\nThe way to round a number to a multiple of 128 is setting its last 8 bits to 1000_0000 (128).\nThe type of pointer affects its arithmetic:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(){ // int-type pointer: int* chunk = nullptr; // 8-byte address: 0 0 0 0 0 0 0 0 int* ptr = chunk + 127; // 0 + 127*sizeof(int) = 508 = 0x1fc std::cout \u0026lt;\u0026lt; ptr \u0026lt;\u0026lt; std::endl; // 0x1fc // char-type pointer: char* ptr2 = (char*) chunk + 127; // 0 + 127*sizeof(char) = 127 = 0x7f std::cout \u0026lt;\u0026lt; static_cast\u0026lt;void*\u0026gt;(ptr2) \u0026lt;\u0026lt; std::endl; // 0x7f } Convert a pointer to an integer: SO\n1 2 char* chunk = nullptr; uint var = reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(chunk); // ok Ensure offset a multiple of 128:\nIf the currect pointer chunk is not a multiple of 128, add another 127, and then truncate the last 8 bits to 1000_000 to make it a multiple of 128.\n1 2 3 4 5 6 int main(){ char* chunk = (char*)130; std::size_t alignment = 128; // uint // 130 + 127 \u0026amp; ~127 = 256 std::size_t offset = (reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(chunk) + alignment - 1) \u0026amp; ~(alignment - 1); // uint } Bitwise operation:\n1 2 3 4 5 chunk: 0000_0000_1000_0010 alignment-1: 0000_0000_0111_1111 ~(alignment-1): 1111_1111_1000_0000 chunk + (alignment-1): 0000_0001_0000_0001 (257) (257)\u0026amp;~(127) = 256: 0000 0001 0000 0000 Code from 3DGS\nPossible related articles:\nWhat exactly is an \u0026lsquo;aligned pointer\u0026rsquo;? size_t (2024-01-24)\nsize_t is the unsigned integer type. For example, if it\u0026rsquo;s 8 bytes (64-bit unsigned long), its value ranges in [0, 2‚Å∂‚Å¥-1]. In other words, if a number is larger than 2‚Å∂‚Å¥-1, it can\u0026rsquo;t be declared as the size_t type.\nUsage: Given a variable that stores the number of elements of an array, or number of bytes of an object, it can be set as the size_t type, as the name indicated that size_t is used for representing a \u0026ldquo;size\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;cstddef\u0026gt; // For size_t #include \u0026lt;typeinfo\u0026gt; int main() { size_t arraySize = 10; // unsigned int (16 bits at least) int myArray[arraySize]; // a 10-integer array size_t sizeOfArray = sizeof(myArray); // 40 bytes // The type of `arraySize` is unsigned integer const std::type_info\u0026amp; typeInfo = typeid(arraySize); std::cout \u0026lt;\u0026lt; typeInfo.name() \u0026lt;\u0026lt; std::endl; // return: m } m refers to unsigned long integer. The returned strings are defined differently across various compilers. So, the correct meaning should be found in the implementation. Strange output of std::typeid::name() - SO Do not use typeid The returned string is not consistent; Need to check the documents.\nRef: typeid(xxxx).name() \u0026ldquo;m\u0026rdquo; returned? - C++ Forum\ncppreference.com Returning the Name and Value of a C Type Using typeid() Reserve Memory (2024-01-26)\nPad the lastCount value to a multiple of the alignment 128, and then allocate the following count bytes, which is the number of elements of the target data pointed by dataPtr:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // \u0026#34;reserve_memory.cpp\u0026#34; #include \u0026lt;torch/torch.h\u0026gt; template \u0026lt;typename T\u0026gt; // T can be int, float..., deduced when compiling void accum(char*\u0026amp; lastCount, T*\u0026amp; dataPtr, size_t count, size_t alignment) { // Decimal arithmetic size_t offset = (reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(lastCount) + alignment-1) \u0026amp; ~(alignment-1); // Update the pointer to the data to be filled dataPtr = reinterpret_cast\u0026lt;T*\u0026gt;(offset); // Accumulate the number of bytes of the data lastCount = reinterpret_cast\u0026lt;char*\u0026gt;(dataPtr+count); } int main(){ char* size = nullptr; // Count from 0x0 float* depth; // sizeof(float) = 4 bytes float* color; accum(size, depth, 100, 128); // 0 + 100*4 = 400 bytes = 0x190 bytes printf(\u0026#34;%p \\n\u0026#34;, size); // 0x190 accum(size, color, 100*3, 128); // 512 + 300*4= 1712 bytes = 0x6b0 bytes printf(\u0026#34;%p \\n\u0026#34;, size); // 0x6b0 return 0; } char*\u0026amp; size = nullptr: the variable size is an alias of the nullptr, sharing the same memory.\nSuch that the pointer (lastCount and dataPtr) is changed directly.\nCompilation. Ref: Troubles while compiling C++ program with PyTorch, HElib and OpenCV - SO\n1 2 3 4 5 6 7 8 g++ -g -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ -L/usr/local/libtorch/lib \\ -Wl,-rpath,/usr/local/libtorch/lib \\ reserve_memory.cpp \\ -ltorch -ltorch_cpu -lc10 # Order matters: After .cpp file doubt: If I add -O2 option, the libraries -ltorch -ltorch_cpu -lc10 can be omitted. Don\u0026rsquo;t know why. With -O2 optimization, debugging will become difficult.\nThe original anwser: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 g++ -g -O2 -std=c++17 \\ -pthread \\ -march=native \\ -I/home/lulu/helib_install/helib_pack/include \\ -I/usr/include/opencv4 \\ -I/home/lulu/libtorch/include \\ -I/home/lulu/libtorch/include/torch/csrc/api/include \\ -I/home/lulu/libtorch/include/torch \\ -L/home/lulu/helib_install/helib_pack/lib \\ -L/usr/include/opencv4 \\ -L/home/lulu/libtorch/lib \\ -Wl,-rpath,/home/lulu/libtorch/lib \\ prova.cpp \\ -lopencv_core -lopencv_highgui -lopencv_imgcodecs \\ -lhelib -lntl -lgmp -lm \\ -ltorch -ltorch_cpu -lc10 \\ -o prova ","date":"2023-11-09T18:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/c++_pointer/","title":"memo: C++ | Pointer Usages"},{"content":"auto Compiler will evaluate the expression and use the result\u0026rsquo;s type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // \u0026#34;auto1.cpp\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;typeinfo\u0026gt; using namespace std; int main(){ int a = 1, b = 2; auto s = a + b; // compiler will compute a+b and assign the type of result to s. cout \u0026lt;\u0026lt; \u0026#34;Sum = \u0026#34; \u0026lt;\u0026lt; sum \u0026lt;\u0026lt;endl; cout \u0026lt;\u0026lt; typeid(s).name() \u0026lt;\u0026lt; endl; return 0; } Build: g++ auto1.cpp -o auto1\nMultiple variables followed by auto should be initialized with a common type.\n1 2 3 4 auto i = 0, *p = \u0026amp;i; // pointer variable p stores the address of i, // And the type of i is deduced from the value 0, auto* p = \u0026amp;i; // ok Reference:\nCodes from The auto Type Specifier in C++ - Neso Academy Placeholder type specifiers (since C++11) - cppreference Vector (2023-11-10)\n1 2 3 4 5 6 7 8 9 #include \u0026lt;vector\u0026gt; void main(){ std::vector\u0026lt;int\u0026gt; myVec; myVec = {1,2,3,4,5}; for (int i:myVec) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } std::vector is a class template (class maker), from which the specific class is derived with additional specifications.\nTemplate in C++ is like the parent class in Python to some extent: compile-time polymorphism v.s. runtime polymorphsim. But templates are not functions or classes.\nSpecifications follow the template name and are enclosed by angle brackets.\nA std::vector is a container for a sequence of objects.\nA std::vector\u0026rsquo;s definition doesn\u0026rsquo;t need number of elements, because it\u0026rsquo;s a dynamic array with adjustable size, achieved by allocating new memory and copying data as needed.\nPass a customized type to std::vector\u0026lt;custom_type\u0026gt;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; struct Vertex{ float x,y,z; }; // Overload the \u0026lt;\u0026lt; operator for Vertex struct std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; stream, const Vertex\u0026amp; v){ stream \u0026lt;\u0026lt; v.x \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; v.y \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; v.z; return stream; } int main(){ std::vector\u0026lt;Vertex\u0026gt; vertices; vertices.push_back({1,2,3}); vertices.push_back({4,5,6}); // Range-based for loop // for (Vertex v : vertices) // will copy each vertex for (const Vertex\u0026amp; v : vertices) // reference won\u0026#39;t copy std::cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; std::endl; // Delete the 2nd element vertices.erase(vertices.begin() + 1); for (int i=0; i \u0026lt; vertices.size(); i++) std::cout \u0026lt;\u0026lt; vertices[i] \u0026lt;\u0026lt; std::endl; // Clear the vector vertices.clear(); // size = 0 std::cout \u0026lt;\u0026lt; vertices.size() \u0026lt;\u0026lt; std::endl; } Reference:\nThe Vector Type in C++ - Youtube - Neso Academy Dynamic Arrays in C++ (std::vector) - Youtube - The Cherno vector insert() Function in C STL - GeeksforGeeks Iterator Ref:\nITERATORS in C++ - The Cherno Raw Function Pointer A representation for a function with an alias. For example, myFunc(int i) is represented as: void (*alias)(int)\nA similar thing in Python:\n1 2 3 4 5 6 norm = torch.nn.LayerNorm myModel = nn.Sequential( nn.Linear(3,5), norm(5) ) The class name is assigned to a variable, which is an alias (reference). And objects are instantiated later by specifying arguments.\nThe instantiation is deferred.\nFuntion pointer stores a function\u0026rsquo;s address in the compiled binary code.\nIn C++, to define a function pointer variable, its type can be deduced by auto:\n1 2 3 4 5 6 7 8 9 10 #include\u0026lt;iostream\u0026gt; void MyFunc(int a){ std::cout \u0026lt;\u0026lt; \u0026#34;Hello\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; } int main(){ auto pFunc = MyFunc; // No (), otherwise calling. pFunc(1); } The \u0026ldquo;Function Pointer\u0026rdquo; type is void(*)(args). And a function pointer variable is void(*var_name)(args).\n1 2 3 4 int main(){ void(*pFunc)(int) = MyFunc; pFunc(1); } And it can be rewritten as a type by typedef:\n1 2 3 4 5 int main(){ typedef void(*MyFunc_type)(int); MyFunc_type pFunc = MyFunc; pFunc(1); } When passing a function to another function, the function pointer parameter should be defined as void (*func_name)(args_list):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; void PrintInt(int i){ std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, void(*func)(int) ){ # Apply func to each element in the vector for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,2,3}; ForEach(myVec, PrintInt); } For compactness, the function PrintInt can be written as a one-line (anonymous) lambda function :\n1 2 3 4 5 6 7 8 9 void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, void(*func)(int) ){ for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,2,3}; ForEach(myVec, [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;} ); } Pass an outside variable a into the lambda function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;string\u0026gt; void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, const std::function\u0026lt;void(int)\u0026gt;\u0026amp; func){ for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {1,3,5,6,0}; std::string a = \u0026#34;Current element:\u0026#34;; ForEach(myVec, [\u0026amp;a](int i){ std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}); } [\u0026amp;a] captures the outside variable a by reference without copying data.\nOutput 1 2 3 4 5 6 7 yi@yi:~/Downloads/Cpp_Study$ g++ function_pointer.cpp yi@yi:~/Downloads/Cpp_Study$ ./a.out Current element:1 Current element:3 Current element:5 Current element:6 Current element:0 Ref:\nFunction Pointers in C++ - YouTube - The Cherno Lambda expression (2023-11-11)\n[](){}:\nSquare brackets [] is for variables outside the scope of {} lambda function\n[\u0026amp;] all variables are captures by references. [=] all variables are captured by values. Parentheses () enclose parameter to be used in {} lambda function\nCurly braces {} enclose operations.\nUsage:\nIf there is a function pointer, lambda function can be in place of it. Example using lambda expression as a bool:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,5,4,3}; auto it = std::find_if(myVec.begin(), myVec.end(), [\u0026amp;](int val){return val \u0026gt; 3;} ); std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; std::endl; // Output: 5 } std::find_if examins myVec to find the fisrt element that is larger than 3 and returns an iterator it.\nIterator object it is the address of the first element in the container. And it+1 points to the 2nd element.\nSo, it needs to be dereferenced to get the value.\nRef:\nLambdas in C++ - The Cherno Lambda expressions - cppreference std::find, std::find_if, std::find_if_not - cppreference.com std::function (2023-11-11)\nA representation for a kind of callable objects, such as normal function, functor, lambda expression, struct.\n\u0026ldquo;Same kind\u0026rdquo; refers to having the same return type and input arguments, as indicated by the function signature.\n1 2 3 4 5 6 7 8 9 // Normal function can be represented as a std::function: void MyFunc(int x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; }; std::function\u0026lt;void(int)\u0026gt; f = MyFunc; // Lambda expression is assigned to a variable: std::function\u0026lt;void(int)\u0026gt; fl = [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}; // Equivalent to a function pointer: auto pf = [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}; The std::function\u0026lt;void(int)\u0026gt; represents any callable object that has no return value and only one int input argument.\nTherefore, it can be used as the type for a formal parameter when defining a function that takes as input a specific kind of function.\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; void funcsPrintNum(int x, std::function\u0026lt;void(int)\u0026gt; func){ func(x); } int main(){ auto myLambda = [](int i){ std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; }; funcsPrintNum(1, myLambda); } A vector storing multiple callable objects that have an identical function signature.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;vector\u0026gt; void aNormalFunc(int i){ std::cout \u0026lt;\u0026lt; i + 5 \u0026lt;\u0026lt; std::endl;} int main(){ std::vector\u0026lt;std::function\u0026lt;void(int)\u0026gt;\u0026gt; vf; vf.push_back([m=5](int i){std::cout \u0026lt;\u0026lt; m \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}); vf.push_back(aNormalFunc); vf[0](1); // Call the 1st function with the argument 1 vf[1](4); // Call the 2nd function } Output 1 2 51 9 Calling a function from a std::function is slower than calling it natively. This is the cost of unifying representation for callable objects with the same input and output.\nRef:\nC++ std::function Next Level Function Polymorphism - The Builder Template (2023-11-14)\nThe template argument typename T (a type) will be deduced when compiling and the corresponding function will be created to link.\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; template \u0026lt;typename T\u0026gt; // before the return type of the func void Print(T value){ std::cout \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; } int main(){ Print(5); Print(5.5f); Print(\u0026#34;A string\u0026#34;); } When calling a template in program, the teamplate argument \u0026lt;T\u0026gt; can be omitted as it can be deduced by compiler.\nOutput 1 2 3 5 5.5 A string The following int N is required at compile time, because an array created on stack needs size known when compiling. And it can be deduced when evaluating the template:\n1 2 3 4 5 6 7 8 9 10 11 12 template \u0026lt;typename T, int N\u0026gt; class ArrayClass{ private: // visible inside the class T m_Array[N]; public: // visible outside the class int GetSize() const {return N;} }; int main(){ ArrayClass\u0026lt;int, 5\u0026gt; myArray; std::cout \u0026lt;\u0026lt; myArray.GetSize() \u0026lt;\u0026lt; std::endl; } Ref:\nTemplates in C++ - YouTube - The Cherno Class vs Struct \u0026ldquo;Struct is the same as class.\u0026rdquo; \u0026ndash; CLASSES vs STRUCTS in C++ - YouTube - The Cherno\nDefault members (without setting visibility) of a class are all private.\nIn contrast, members in a struct are all public by default.\nStruct also can set members (variables and methods) to private.\nStruct can do inheritance as well, but not common. Struct is often used to group variables.\ndoubt: Why does a struct can include itself? (2024-01-27) Not itself. fromChunk is a function and its return type is the stuct. The code is from 3DGS.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct GeometryState { size_t scan_size; float* depths; char* scanning_space; bool* clamped; int* internal_radii; float2* means2D; float* cov3D; float4* conic_opacity; float* rgb; uint32_t* point_offsets; uint32_t* tiles_touched; static GeometryState fromChunk(char*\u0026amp; chunk, size_t P); }; Bitwise Operator (2023-11-14)\n~\n1 2 3 4 5 6 7 8 9 #include \u0026lt;iostream\u0026gt; #include \u0026lt;bitset\u0026gt; int main(){ std::size_t alignment = 128; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;(alignment) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;(alignment-1) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;( ~ (alignment-1)) \u0026lt;\u0026lt; std::endl; } Output:\n1 2 3 00000000000000000000000010000000 00000000000000000000000001111111 11111111111111111111111110000000 Ref:\nhow to output an int in binary? - SO #define (2023-11-15)\nShort for a for loop\n1 2 3 4 5 #define fori(x) for(int i=0; i\u0026lt;x; i++) fori(20){ ... } Ref: Why use #define instead of a variable\n#define 100_000 won\u0026rsquo;t work. #define 100000 is ok.\nHeader files (2023-11-15)\nFunctions\u0026rsquo; signature must be written in the file to use them.\nThose declarations can be stored in header files.\nThe preprocess directive #include will copy and paste a header file to there.\nHeader files can form a chain, whereas a potential probelm is repeated difinations. (e.g. structs need unique names.)\nThe header guide #pragma once marks a header file won\u0026rsquo;t be included multiple times into a single translation unit (cpp file).\nOld fasion is wrapping the entire header file with #ifndef symbol and #endif\n(2024-06-03)\nInclude Guards\n1 2 3 4 5 6 7 8 //x.h #ifndef __X_H_INCLUDED__ // if x.h hasn\u0026#39;t been included yet... #define __X_H_INCLUDED__ // #define this so the compiler knows it has been included class X { }; #endif Suffix .h is used to differentiate \u0026ldquo;C standard library\u0026rdquo; (\u0026lt;stdio.h\u0026gt;) and \u0026ldquo;C++ standard libray\u0026rdquo; (\u0026lt;iostream\u0026gt;)\nAngular brackets \u0026lt;fname\u0026gt; will search the \u0026ldquo;standard include directories\u0026rdquo; (of compiler) for the file named fname.\nWhile double qutoes \u0026quot;../myHeader.h\u0026quot; normaly enclose a relative path, although it can enclose a file that resides in \u0026ldquo;standard include directories\u0026rdquo; as well (e.g., \u0026quot;iostream\u0026quot;).\nDo not use namespace in a header file. Why I don\u0026rsquo;t \u0026ldquo;using namespace std\u0026rdquo; - The Cherno Ref:\nC++ Header Files - The Cherno Source file inclusion - cppreference.com (2024-05-13)\nRef: C/C++Â§¥Êñá‰ª∂ÈáåÊúâ‰ªÄ‰πà, ‰ªÄ‰πàÊòØÊé•Âè£‰∏éÂÆûÁé∞ÂàÜÁ¶ª, ‰∏∫‰ªÄ‰πàËøô‰πàÂπ≤? ‰ª£Á†ÅÁü•ËØÜ - ‰∏çÂÅúÊÑüÂèπÁöÑËÄÅÊûó\nÂ§¥Êñá‰ª∂ÁöÑÊÑè‰πâÔºöÂ§öÂ§ÑÂ£∞ÊòéÔºå‰∏ÄÂ§ÑÂÆö‰πâ (2024-06-03)\nRef: Headers and Includes: Why and How - C++ Articles - Disch (Found by DDG)\nSince each source file is compiled individually to object files (which are binary too) before linking them together, the header files are needed to make the function interface recognizable for the compiler, and keep the implementation individual.\nKeeping each file self-contained facilitates making modifications.\nAs each source file is compiled separately, the implementation of a function is unknown in another source file. So the header file is a \u0026ldquo;messenger\u0026rdquo; between them by telling funcitons\u0026rsquo; interface to each other.\nThus, functions\u0026rsquo; interface and their implementation are separated.\nSpliting the program to multiple piece files instead of a single comprehensive file helps debugging and facilitate compilation (only compile the needed file).\nThe compiler compiles (translate) each source file into a binary (object file), and the linker finds function implementations, and connects the object files and precompiled libraries together. 0.5 ‚Äî Introduction to the compiler, linker, and libraries #include is copy \u0026amp; paste during preprocessing.\nHeader files are pasted (#included) and not compiled, whereas source files are compiled and not included. Do not #include source files.\nOnly #include the necessary things in header files.\nTry to avoid #include the full .h file:\nDo nothing ‚ùÆ Forward declare A ‚ùÆ #include \u0026ldquo;a.h\u0026rdquo;\nForward declared dependencies: class aClass;\nThe right way to include: \u0026ldquo;Forward declare when you can, don\u0026rsquo;t #include unless it\u0026rsquo;s necessary\u0026rdquo;\nUse a pointer or reference to an object aClass* a, instead of instantiating a full object: aClass a, and then use forward declaration: class aClass; to avoid the circular inclusion problem where using #include \u0026quot;a.h\u0026quot; is necessary.\ninline functions require their function body to exist in the every cpp file that calls them.\nTheir function bodies can be put in a header file.\nThe forward declaration for a template class requires typedef, which could cause inconvenience as all files that use the template class need to be modified manually when the template class changes.\nThe solution is putting the typedef of the template class in a header file and then #include it in another header file.\n(2024-06-04)\nRef: How a compiler knows from a header file, that a source file exists somewhere? - SO (Found in DDG)\n#include \u0026quot;myfile.h\u0026quot; has a broader searching scope than #include \u0026lt;stdio.h\u0026gt;\nThe \u0026quot;\u0026quot; will search in the current working folder besides the predefined standard well-known directories.\nIncluding a header #include math.h but without linking the library libm.a into the executable binary, by specifying it in the gcc build command: -lm, the functions in libm.a can\u0026rsquo;t be called\nGcc convention: For a precompiled library: lib\u0026lt;name\u0026gt;.a, the argument to link it: -l\u0026lt;name\u0026gt;. -I and -L specify the additional searching paths to headers and libraries.\nRef: C/C++ Headers and Source Files: How Do They Work? - codeproject (Not accurate enough)\nEverything (function, struct, variable) in cpp needs a declaration before using it, so header files are pasted in the source file with #include.\nHeader files are declarations informing the compiler. Semicolon after the function signature indicates to the compiler that this is a function prototype declaration, rather than the definition.\nCompilation only process source files, as header files already have been pasted into source files.\nCompiling \u0026amp; Linking Compiling:\nConvert cpp files (translation units) to binary object file.\nPreprocess statements (directives) will be done when compiling.\n#include just copy and paste to where it is. #define replace symbol #if 1 (or 0) and #endif to use a code snippest or not. Once all preprocess statements are finished, the preprocessed \u0026ldquo;full\u0026rdquo; code will be tranformed to an object file\nMultiple cpp files can be combined into a single cpp file by #include, and then only one translation unit will be generated.\nHowever, if every cpp file doesn\u0026rsquo;t include others, each cpp file will have an translation unit.\nEach translation unit yields a object file\nconstant folding: constant arithmetics will be solved at compile time, such that there is only 1 asm instruction: put a number into a register.\nRef: How the C++ Compiler Works\nLinking:\nFind the binary code of functions, symbols and entry point main, according to their \u0026ldquo;function signatures\u0026rdquo;, and then form a executable file.\nCompiling is to preprocess and generate object file. Linking is organizing binary code to a executable fiel. Build = Compiling + Linking Link error and compile error\nError code starts with \u0026ldquo;LNK\u0026rdquo; means it\u0026rsquo;s an linking error:\nFunction signature mismatch with the definition (name, return type, parameters), thus the binary code can\u0026rsquo;t be found: LNK2019 Unresolve externel symbol\nNo entry point definition (e.g., main()) for Application(.exe).\nDuplicate symbols (function signature, variables) are found in the project\u0026rsquo;s obj files. LNK1169 One or more funcname defined symbols found\nEven if a function isn\u0026rsquo;t called by main(), it could be called in other cpp files.\nTherefore, linker also needs to link those \u0026ldquo;uncalled\u0026rdquo; functions, unless it has static keysword representing it will only be called internally in the cpp file it exists.\nError code starts with \u0026ldquo;C\u0026rdquo; means it\u0026rsquo;s an compiling error:\nSyntax errors, e.g., missing ; (C2143) No declaration or definition for functions used in a cpp file. Multiple definitions in a single file. Multiple definition if a function is defined in a header file without static or inline limitation, because #include is just pasting header file.\n\u0026ldquo;Log.h\u0026rdquo;:\n1 2 3 4 5 #pragma once #include \u0026lt;iostream\u0026gt; void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } \u0026ldquo;Log.cpp\u0026rdquo;\n1 2 3 4 #include \u0026#34;Log.h\u0026#34; // Log definition will be here void InitLog(){ Log(\u0026#34;Initialization\u0026#34;); } \u0026ldquo;main.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 #include \u0026#34;Log.h\u0026#34; // Log definition will be here int Multiply(int\u0026amp; a, int\u0026amp; b){ Log(\u0026#34;Multiplication:\u0026#34;); return a*b; } int main(){ int c = Multiply(5, 2); } There won\u0026rsquo;t be compiling error, since each cpp file knows functions to be used. But this project cannot be linked since there two Log function with the same signature, and the linker don\u0026rsquo;t know which one should be used.\nThere are 3 soulutions:\nstatic will limit the function used only in the file it resides in and won\u0026rsquo;t be visible to any other obj files.\n1 2 3 static void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } That means even though \u0026ldquo;Log.cpp\u0026rdquo; and \u0026ldquo;main.cpp\u0026rdquo; both have the definition of void Log(const char*), they use their own Log. The linker won\u0026rsquo;t be confused.\ninline will replace the calls of the function with its body:\n1 2 3 inline void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } Define the function in one of translation units. And header file only contain declarations.\nMove the definition into \u0026ldquo;Log.cpp\u0026rdquo;:\n1 2 3 void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } Ref: How the C++ Linker Works - The Cherno\n(2024-06-04)\nHow C++ Works: Understanding Compilation | Toptal (Found in DDG)\nPreprocess: Copy/paste header files to source files:\n\u0026ldquo;test-compile.cpp\u0026rdquo;\n1 2 3 #include \u0026lt;iostream\u0026gt; int main(int argc, char* argv[] ){ std::cout \u0026lt;\u0026lt; \u0026#34;Hello\u0026#34; \u0026lt;\u0026lt; std::endl;} Add -E option to stop the compiler after preprocessing: (Found by perplexity)\n1 2 gcc -E test-compile.cpp -o test-compile.ii wc test-compile.ii The preprocessing and compiling in c++ is similar to c lang. Compile each source file separately to generate a object file: (sec2)\nGiven a c file: \u0026ldquo;sum.c\u0026rdquo;:\n1 2 int sumI(int a, int b){ return a+b;} float sumF(float a, float b){return a+b;} -c stops the compiler after compiling before linking. manual (SO)\n1 yi@Alien:~/Downloads/Cpp_Study$ gcc -c test_compile.cpp -o sum.o nm lists symbols from object files.\n1 2 3 yi@Alien:~/Downloads/Cpp_Study$ nm sum.o 0000000000000018 T _Z4sumFff 0000000000000000 T _Z4sumIii No symbol is imported. Two symbols are exported as part of the .text segment T. (Don\u0026rsquo;t understand)\n_Z4sumFff and _Z4sumIii are the names used by other source files calling the 2 functions.\n#include header files is declaring the 2 functions\nMix calling C and C++ functions\nFunction symbols in cpp enable the feature of overload when several functions have the same name but different input arguments. Link all the object files to generte a binary file.\n(2024-06-06)\nCompile ÁöÑ‰∏≠ÊñáÁøªËØëÔºöÁºñËØë = ÁøªËØë + ÁºñÁ∫Ç„ÄÇÁºñÁ∫ÇÁöÑÊÑèÊÄùÊòØÊääÂ§ö‰∏™Êñá‰ª∂ÊîæÂà∞‰∏ÄËµ∑„ÄÇ\npile ÊòØ‚Äú‰∏ÄÊëû‚Äù, compile ÊòØÊää‰∏ÄÊëûÊñá‰ª∂ÊîæÂà∞‰∏ÄËµ∑„ÄÇ\nFind Next MSB (2023-11-17)\nFind the next-highest bit of the MSB (Most Significant Bit)\nRight shift:\n1 2 3 4 5 6 7 #include \u0026lt;iostream\u0026gt; #include \u0026lt;bitset\u0026gt; int main(){ uint16_t x = 32767; // 01111111_11111111 std::cout \u0026lt;\u0026lt; (x \u0026gt;\u0026gt; 16) \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; (x \u0026gt;\u0026gt; 12) \u0026lt;\u0026lt; std::endl; // 0000000_0000111 } Parse Args (2023-11-03)\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(int argc, char **argv) { std::cout \u0026lt;\u0026lt; \u0026#34;Number of input arguments: \u0026#34; \u0026lt;\u0026lt; argc \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt;= argc-1; i++) { std::cout \u0026lt;\u0026lt; argv[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } return 0; } Scope Resolution (2024-02-01)\nScope resolution operator: :: | Microsoft Learn :: : Scope resolution operator in C++ - GeeksforGeeks\nDifferentiate global and locatl variables with the same name; Identify classes with the same name in different namespace. Define a member function outside the class Access a class\u0026rsquo;s static member Distinguish members with the same name reside in multiple classes with inheritances. Refer to nesting class. . is used for member of object. Member access operators - cppreference\na-\u0026gt;b, where a is a pointer, equals to ((*a).b). In contrast, a.b where a is an object. member access operators - C++ Forum\nType Casting (2024-02-03)\n(uint32_t) is forceful casting in terms of the bits reading. whereas static_cast\u0026lt;uint32_t\u0026gt; cannot be applied on data with bits interpretion mismatched. Type conversions and type safety - Microsoft Learn\n1 2 3 4 5 6 7 #include \u0026lt;iostream\u0026gt; int main(){ float depth = 15.7; std::cout \u0026lt;\u0026lt; (uint32_t)depth; // 15 std::cout \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(depth); // 15 std::cout \u0026lt;\u0026lt; reinterpret_cast\u0026lt;uint32_t\u0026gt;(depth); //error } ","date":"2023-11-09T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/lang/c++_misc/","title":"memo: C++ | Misc"},{"content":" The 2 submodules are CUDA projects that should be debugged separately from the python project, because the python debugger can\u0026rsquo;t access the PyTorch CUDA extensions. Debug Settings (2023-11-08)\nIntellisense To enable intellisense, the configuration of \u0026ldquo;C/C++ extension\u0026rdquo; must be put in the \u0026ldquo;.vscode/\u0026rdquo; of (top-level) current working directory (.vscode/c_cpp_properties.json), not in submodule\u0026rsquo;s \u0026ldquo;.vscode/\u0026rdquo; (submodules/diff-gaussian-rasterization/.vscode/c_cpp_properties.json).\nOtherwise, the settings won\u0026rsquo;t be loaded.\nCUDA syntax (e.g.,__global__) and header (\u0026lt;cooperative_groups/reduce.h\u0026gt;, min) won\u0026rsquo;t be recognized, if includePath and compiler are incorrect,\nDoubtful Attempts Put the includes paths of torch before cuda\u0026rsquo;s include. Otherwise #include \u0026lt;cooperative_groups/reduce.h\u0026gt; in \u0026ldquo;forward.cu\u0026rdquo; can\u0026rsquo;t be found.\nSet the includePath in \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo; as:\n1 2 3 4 5 6 7 \u0026#34;includePath\u0026#34;: [ \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34;, \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/cuda-11.6/include\u0026#34; ], Error in \u0026ldquo;rasterizer_impl.cu\u0026rdquo;: cannot open source file glm/glm.hpp (dependency of cub/cub.cuh)\n1 sudo apt-get install libglm-dev Why can\u0026rsquo;t C++ find GLM headers? - SO\nError in \u0026ldquo;backward.cu\u0026rdquo;: namespace \u0026quot;cooperative_groups\u0026quot; has no member \u0026quot;this_grid\u0026quot;\nChange CMakelist: SO\nDidn\u0026rsquo;t fix\nError: identifier \u0026quot;min\u0026quot; is undefinedC/C++(20)\nDidn\u0026rsquo;t fix\nSolution: Use nvcc as compiler. Verified \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/include/python3.7m\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], \u0026#34;defines\u0026#34;: [], \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/local/cuda-11.6/bin/nvcc\u0026#34;, \u0026#34;cStandard\u0026#34;: \u0026#34;c17\u0026#34;, \u0026#34;cppStandard\u0026#34;: \u0026#34;gnu++14\u0026#34;, \u0026#34;intelliSenseMode\u0026#34;: \u0026#34;linux-gcc-x64\u0026#34; } ], \u0026#34;version\u0026#34;: 4 } Debug diffRast (2023-11-18)\nAction: Write a \u0026ldquo;main.cpp\u0026rdquo; that calls the methods of the compiled library \u0026ldquo;CudaRasterizer\u0026rdquo; for debugging. Repo for debugging DiffRast: debug_diff_rust\n\u0026ldquo;CMakeLists.txt\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cmake_minimum_required(VERSION 3.20 FATAL_ERROR) project(MyApp) # ${PROJECT_NAME} find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_subdirectory(external/diff-gaussian-rasterization-comments) add_executable(MyApp main.cpp) set_property(TARGET MyApp PROPERTY CXX_STANDARD 17) target_link_libraries(MyApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) target_link_libraries(MyApp CudaRasterizer) target_sources(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization-comments/rasterize_points.cu) target_include_directories(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization-comments) \u0026ldquo;launch.json\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DiffRast Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/build/MyApp\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, } ] } \u0026ldquo;tasks.json\u0026rdquo;\nopen 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;cmake-configure\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-DCMAKE_BUILD_TYPE=Debug\u0026#34;, \u0026#34;-DCMAKE_PREFIX_PATH=/usr/local/libtorch\u0026#34;, \u0026#34;..\u0026#34;, ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;cmake-build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--build\u0026#34;, \u0026#34;.\u0026#34;, // \u0026#34;--config\u0026#34;, // \u0026#34;Debug\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;dependsOn\u0026#34;: [\u0026#34;cmake-configure\u0026#34;, \u0026#34;cmake-build\u0026#34;] } ] } Step into CUDA Kernels Building project as above won\u0026rsquo;t allow to step into CUDA kernels, e.g., preprocessCUDA.\n(2024-01-20)\nEnable CUDA language in CMAKE and set for debugging:\n1 2 3 4 5 project(MyApp LANGUAGES CXX CUDA) set(CMAKE_BUILD_TYPE Debug) if(CMAKE_BUILD_TYPE STREQUAL \u0026#34;Debug\u0026#34;) set(CMAKE_CUDA_FLAGS \u0026#34;${CMAKE_CUDA_FLAGS} -g -G\u0026#34;) endif() Build to binary file:\n1 2 3 # cd ~/Downloads/debug_diff_rast cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja cmake --build ./build Launch cuda-gdb: cuda-gdb ./build/MyApp. It\u0026rsquo;s convenient to use cuda-gdb in the terminal of vscode, where I can jump to the code by clicking the path.\nAdd breakpoint inside preprocessCUDA: (cuda-gdb) b forward.cu:182 Not sure if it worked.\n(2024-01-26) Nsight (CUDA-GDB)\nGenerate Makefile with cmake:\n1 2 cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; cmake --build ./build To let cmake produce Makefile, do not use -GNinja. Why isn\u0026rsquo;t the command \u0026ldquo;cmake .\u0026rdquo; generating a makefile? - SO (2024-04-23)\nError: \u0026ldquo;Caffe2: Cannot find cuDNN library\u0026rdquo;\nSolved by installing cudnn-xxx.deb Error Message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 (gaussian_splatting) yi@yi-Alien:~/Downloads/debug_diff_rast$ cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; -- The CXX compiler identification is GNU 9.4.0 -- The CUDA compiler identification is NVIDIA 11.6.55 -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Detecting CUDA compiler ABI info -- Detecting CUDA compiler ABI info - done -- Check for working CUDA compiler: /usr/local/cuda-11.6/bin/nvcc - skipped -- Detecting CUDA compile features -- Detecting CUDA compile features - done -- Found CCache: /usr/local/bin/ccache -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found CUDA: /usr/local/cuda-11.6 (found version \u0026#34;11.6\u0026#34;) -- Caffe2: CUDA detected: 11.6 -- Caffe2: CUDA nvcc is: /usr/local/cuda-11.6/bin/nvcc -- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.6 -- Caffe2: Header version is: 11.6 -- Could NOT find CUDNN (missing: CUDNN_LIBRARY_PATH CUDNN_INCLUDE_PATH) CMake Warning at /usr/local/libtorch/share/cmake/Caffe2/public/cuda.cmake:120 (message): Caffe2: Cannot find cuDNN library. Turning the option off Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include) /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:19 (find_package) -- /usr/local/cuda-11.6/lib64/libnvrtc.so shorthash is 280a23f6 -- Autodetected CUDA architecture(s): 6.1 -- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61 CMake Error at /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:96 (message): Your installed Caffe2 version uses cuDNN but I cannot find the cuDNN libraries. Please set the proper cuDNN prefixes and / or install cuDNN. Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:19 (find_package) -- Configuring incomplete, errors occurred! I didn\u0026rsquo;t encounter this error before.\nThere is no libcudnn.so under directory: /usr/local/cuda-11.6/lib64/\nlibcudnn.so does\u0026rsquo;t apper in the output of python -m torch.utils.collect_env as shown in issue#30\nDownload cuDNN Library for CUDA11 Ubuntu 20.04. Referring to this answer: spconv - issues#277\n1 2 3 4 5 wget https://developer.download.nvidia.com/compute/cudnn/9.1.0/local_installers/cudnn-local-repo-ubuntu2004-9.1.0_1.0-1_amd64.deb sudo dpkg -i cudnn-local-repo-ubuntu2004-9.1.0_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu2004-9.1.0/cudnn-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cudnn-cuda-11 The libcudnn and cudnn are installed under /usr/lib/ and /usr/include:\n1 2 3 4 5 6 7 8 9 10 11 12 13 (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ whereis libcudnn libcudnn: /usr/lib/x86_64-linux-gnu/libcudnn.so (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ whereis cudnn cudnn: /usr/include/cudnn.h (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ dpkg -l | grep cudnn ii cudnn-local-repo-ubuntu2004-9.1.0 1.0-1 amd64 cudnn-local repository configuration files ii cudnn9-cuda-11 9.1.0.70-1 amd64 NVIDIA cuDNN for CUDA 11 ii cudnn9-cuda-11-8 9.1.0.70-1 amd64 NVIDIA cuDNN for CUDA 11.8 ii libcudnn9-cuda-11 9.1.0.70-1 amd64 cuDNN runtime libraries for CUDA 11.8 ii libcudnn9-dev-cuda-11 9.1.0.70-1 amd64 cuDNN development headers and symlinks for CUDA 11.8 ii libcudnn9-static-cuda-11 9.1.0.70-1 amd64 cuDNN static libraries for CUDA 11.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 (base) yi@yi-Alien:~$ apt-cache search cudnn cudnn9-cuda-11-8 - NVIDIA cuDNN for CUDA 11.8 cudnn9-cuda-11 - NVIDIA cuDNN for CUDA 11 cudnn9-cuda-12-4 - NVIDIA cuDNN for CUDA 12.4 cudnn9-cuda-12 - NVIDIA cuDNN for CUDA 12 cudnn9 - NVIDIA CUDA Deep Neural Network library (cuDNN) cudnn - NVIDIA CUDA Deep Neural Network library (cuDNN) libcudnn9-cuda-11 - cuDNN runtime libraries for CUDA 11.8 libcudnn9-cuda-12 - cuDNN runtime libraries for CUDA 12.4 libcudnn9-dev-cuda-11 - cuDNN development headers and symlinks for CUDA 11.8 libcudnn9-dev-cuda-12 - cuDNN development headers and symlinks for CUDA 12.4 libcudnn9-samples - cuDNN samples libcudnn9-static-cuda-11 - cuDNN static libraries for CUDA 11.8 libcudnn9-static-cuda-12 - cuDNN static libraries for CUDA 12.4 cudnn-local-repo-ubuntu2004-9.1.0 - cudnn-local repository configuration files After installation, the cmake configuration works fine.\nI didn\u0026rsquo;t copy it to /usr/local/cuda/lib64 or /usr/local/cuda/include, like: CuDNN not found while compiling PyTorch C++ extension - Forum\nI didn\u0026rsquo;t set environment variable neither.\n(2024-05-01)\nLambda server met the same problem. The current cudnn 9.1 doesn\u0026rsquo;t have deb option for Ubuntu 18.04. I downloaded the history version cudnn 8.9.7 (2023/12/05): Local Installer for Ubuntu18.04 x86_64 (Deb) (839MB)\nwget cannot download it. Have to use browser.\n1 wget https://developer.nvidia.com/downloads/compute/cudnn/secure/8.9.7/local_installers/11.x/cudnn-local-repo-ubuntu1804-8.9.7.29_1.0-1_amd64.deb/ Install the deb package with reference to cudnn 9.1:\n1 2 3 sudo dpkg -i cudnn-local-repo-ubuntu1804-8.9.7.29_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu1804-8.9.7.29/cudnn-local-AE31B5F1-keyring.gpg /usr/share/keyrings/ sudo apt-get update The name maybe mismatched:\n1 2 3 4 5 root@lambda-server:/data2/zi# sudo apt-get -y install cudnn-cuda-11 Reading package lists... Done Building dependency tree Reading state information... Done E: Unable to locate package cudnn-cuda-11 There are several cudnn package could be installed:\n1 2 3 4 5 6 7 root@lambda-server:/data2/zi# apt-cache search cudnn libcudnn8 - cuDNN runtime libraries libcudnn8-dev - cuDNN development libraries and headers libcudnn8-samples - cuDNN samples libcudnn7-dev - cuDNN development libraries and headers libcudnn7 - cuDNN runtime libraries cudnn-local-repo-ubuntu1804-8.9.7.29 - cudnn-local repository configuration files Search: How to install cudnn Local Installer for Ubuntu18.04 x86_64 (Deb) on DDG:\nFound: NVIDIA cuDNN - NVIDIA Documentation Hub\nReferring to the pdf docs, specify the versions:\n1 2 3 4 5 root@lambda-server:/data2/zi# sudo apt-get install libcudnn8-dev=8.9.7.29-1+cuda11.6 Reading package lists... Done Building dependency tree Reading state information... Done E: Version \u0026#39;8.9.7.29-1+cuda11.6\u0026#39; for \u0026#39;libcudnn8-dev\u0026#39; was not found The installation succeed without adding version:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 root@lambda-server:/data2/zichen# sudo apt-get install libcudnn8-dev The following additional packages will be installed: libcudnn8 The following NEW packages will be installed: libcudnn8 libcudnn8-dev 0 upgraded, 2 newly installed, 0 to remove and 337 not upgraded. Need to get 0 B/878 MB of archives. After this operation, 2,366 MB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 file:/var/cudnn-local-repo-ubuntu1804-8.9.7.29 libcudnn8 8.9.7.29-1+cuda11.8 [441 MB] Get:2 file:/var/cudnn-local-repo-ubuntu1804-8.9.7.29 libcudnn8-dev 8.9.7.29-1+cuda11.8 [437 MB] Selecting previously unselected package libcudnn8. (Reading database ... 231804 files and directories currently installed.) Preparing to unpack .../libcudnn8_8.9.7.29-1+cuda11.8_amd64.deb ... Unpacking libcudnn8 (8.9.7.29-1+cuda11.8) ... Selecting previously unselected package libcudnn8-dev. Preparing to unpack .../libcudnn8-dev_8.9.7.29-1+cuda11.8_amd64.deb ... Unpacking libcudnn8-dev (8.9.7.29-1+cuda11.8) ... Setting up libcudnn8 (8.9.7.29-1+cuda11.8) ... Setting up libcudnn8-dev (8.9.7.29-1+cuda11.8) ... update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v8.h to provide /usr/include/cudnn.h (libcudnn) in auto mode The debugger can step into kernel functions. However, the variables are not visible in the panel: Cannot instantiate printer for default visualizer\nCreate tasks.json\nFollowing this tutorial: Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;label\u0026#34;: \u0026#34;CUDA Make\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;make dbg=1\u0026#34;, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true }, \u0026#34;problemMatcher\u0026#34;: [ \u0026#34;$nvcc\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Makefile }, }, And then click the menu bar in vscode \u0026ldquo;Terminal\u0026rdquo; -\u0026gt; \u0026ldquo;Run Build Task \u0026hellip;\u0026rdquo; -\u0026gt; select \u0026ldquo;CUDA Make\u0026rdquo;.\nCreate launch.json:\n1 2 3 4 5 6 { \u0026#34;name\u0026#34;: \u0026#34;CUDA: Debug with CUDA-GDB\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/MyApp\u0026#34;, }, Select the configuration: \u0026ldquo;CUDA: Debug with CUDA-GDB\u0026rdquo; beside the \u0026ldquo;Run\u0026rdquo; button. Then, click \u0026ldquo;Run\u0026rdquo; to debug.\nThe program can hit the breakpoint in the kernel function preprocessCUDA, for example: float3 p_view; at \u0026ldquo;diff-gaussian-rasterization/cuda_rasterizer/forward.cu#192\u0026rdquo;\n(2024-04-24) On Ubuntu 20.04, CUDA-11.6, gcc 9.4.0\nRe-verified practice: 2 steps enable debugging with breakpoints set inside kernel functions.\nCompile with cmake:\n1 2 (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ cmake --build ./build without clicking: Terminal -\u0026gt; Run Build Task... -\u0026gt; CUDA Make\nLaunch debugger with the above launch.json: \u0026ldquo;CUDA: Debug with CUDA-GDB\u0026rdquo;\n","date":"2023-11-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-debug/","title":"read: 3DGS | Debug Code"},{"content":"Source video: 21. Eigenvalues and Eigenvectors - Gilbert Strang - MIT OpenCourseWare\n(2023-11-06) The 1st-time recitation note. Haven\u0026rsquo;t re-check.\nOut direction same as in Matrix multiply vector acting like MLP, where all current dimensions are combined in different weights to produce each dimension in another space.\n$$ ùêÄ ùêó \\\\ \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} = ùêÄ \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} $$\nMatrix ùêÄ is like a function that takes as input ùêó and output ùêÄùêó. Typically, ùêÄùêó points in different direction from the input ùêó. Eigenvectors are those ùêó make ùêÄùêó parallels to ùêó. Parallel means scaling: ùêÄùêó = Œªùêó, where the multiplier Œª is eigenvalue. Given a matrix ùêÄ, how to solve its eigenvalues Œª and eigenvectors ùêó?\nIf Œª = 0, ùêó solved from ùêÄùêó = 0 by elimination is ùüé, which is useless. Some hints can be found through the following examples\nProjection matrix Given a projection matrix ùêè, a 3D vector is projected on to a plane.\nz x p l a n e y Eigenvectors ùêó should parallel to the projected vectors ùêèùêó. So the vectors located in the plane all are eigenvectors, as $ùêèùêó=ùêó$, with Œª=1.\nVectors perpendicular to the plane statisfy: $ùêèùêó=0$, i.e., Œª=0.\nThose two sets of eigenvectors are perpendicular.\nPermutation matrix Given the 2D permutation matrix $ùêÄ=[^{0 \\ 1}_{1 \\ 0}]$, which vector can make ùêÄùêó=ùêó, i.e., identical after permutation.\nùêó = $[^1_1]$, with Œª=1.\nAs ùêÄ is a 2D matrix, there should be 2 eigenvalues. Which vector can statisfy Œª=-1 ?\nùêó = $[^{-1}_1]$\nThe 2 sets of eigenvectors ùêó are perpendicular as well.\nFact: The sum of eigenvalues equals to the sum of elements on diagonal of ùêÄ.\nHere, there is 1 + (-1) = 0 + 0.\nHow to solve Œª, ùêó Trick: Rearrange to ùêÄùêó - Œªùêó = 0.\nIf there is $(ùêÄ - Œªùêà)ùêó = 0$ and ùêó, a non-zero vector, becomes 0 after multiplied by (ùêÄ - Œªùêà), the (ùêÄ - Œªùêà) must be sigular: determinant is 0.\nThe formula $ùêÄ - Œªùêà = 0$ doesn\u0026rsquo;t include ùêó, so eigenvalues Œª can be solved first, as in the example below.\nPlus multiple identity Given a matrix ùêÄ = $[^{3 \\ 1}_{1 \\ 3}]$, to calculate Œª, solve:\n$$ \\begin{aligned} | ùêÄ - Œªùêà | = 0 \\\\ \\begin{bmatrix} 3-Œª \u0026amp; 1 \\\\ 1 \u0026amp; 3-Œª \\end{bmatrix} = 0 \\\\ (3-Œª)^2 -1 = 0 \\\\ Œª^2 - 6Œª + 8 = 0 \\\\ (Œª-4)(Œª-2) = 0 \\end{aligned} $$\nTwo roots: Œª‚ÇÅ=4, Œª‚ÇÇ=2. Then solve the 2 sets of eigenvectors.\nFor Œª‚ÇÅ=4,\n$$ \\begin{aligned} (ùêÄ - Œªùêà)ùêó = 0 \\\\ \\begin{bmatrix} -1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix} ùêó = 0 \\\\ \\end{aligned} $$\nBy letting the free variable to 1, ùêó can be solved as $[^1_1]$.\nHence, one of eigenvectors is $[^1_1]$\nFor Œª‚ÇÇ=2,\n$$ \\begin{aligned} (ùêÄ - Œªùêà)ùêó = 0 \\\\ \\begin{bmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \\end{bmatrix} ùêó = 0 \\\\ \\end{aligned} $$\nHence, one of eigenvectors is $[^{-1}_1]$\nThe sum of the two eigenvalues is the trace of ùêÄ, and their product is the determinant of ùêÄ.\nComparing the matrix $[^{3 \\ 1}_{1 \\ 3}]$ and the permutation matrix $[^{0 \\ 1}_{1 \\ 0}]$, there is:\nA $[^{3 \\ 1}_{1 \\ 3}]$ $[^{0 \\ 1}_{1 \\ 0}]$ Œª 4 and 2 1 and -1 ùêó $[^1_1]$ and $[^{-1}_1]$ $[^1_1]$ and $[^{-1}_1]$ If $ùêÄùêó =Œªùêó$, then $(ùêÄ+3ùêà)ùêó= ùêÄùêó+3ùêàùêó = (Œª+3ùêà)ùêó$\nThat means, if ùêÄ plus 3ùêà, then eigenvalues Œª will plus 3ùêà, while eigenvectors doesn\u0026rsquo;t change.\nCan\u0026rsquo;t generalize However, th above property (for multiple identity) can\u0026rsquo;t be generalized to plusing an arbitrary matrix ùêÅ.\nMatrix addition doesn\u0026rsquo;t imply eigenvalues addition, because the eigenvector of ùêÅ is typically not ùêó. Therefore, the following addition can\u0026rsquo;t be performed:\n$$ ùêÄùêó =Œªùêó \\\\ ùêÅùêó =Œ±ùêó \\\\ (ùêÄ+ùêÅ)ùêó = (Œª+Œ±)ùêó $$\nComplex eigenvalues Considering rotation matrix:\n$$ \\begin{aligned} \\begin{bmatrix} cosŒ∏ \\\\ sinŒ∏ \\end{bmatrix} = \\begin{bmatrix} cosŒ∏ \u0026amp; -sinŒ∏ \\\\ sinŒ∏ \u0026amp; cosŒ∏ \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{aligned} $$\nLet Œ∏ be 90¬∞, the rotation matrix is $[^{0 \\ -1}_{1 \\ 0}]$.\nHowever, according to the theory \u0026ldquo;eigenvector outcomes in the direction that it went in\u0026rdquo;, there seems to be no eigenvector intuitively, as any vector gets in will rotate 90 degree by this matrix.\nSolve its eigenvalues:\n$$ \\begin{aligned} | ùêÄ - Œªùêà | = 0 \\\\ |\\begin{bmatrix} 0 \u0026amp; -1 \\\\ 1 \u0026amp; 0 \\end{bmatrix} - \\begin{bmatrix} Œª \u0026amp; 0 \\\\ 0 \u0026amp; Œª \\end{bmatrix}| = \\begin{vmatrix} -Œª \u0026amp; -1 \\\\ 1 \u0026amp; -Œª \\end{vmatrix} = 0 \\\\ Œª^2 + 1 = 0 \\end{aligned} $$\nTwo roots: Œª‚ÇÅ=i, Œª‚ÇÇ=-i\nAnti-symmetric matrix has imaginary eigenvalues which are always in pairs, as they\u0026rsquo;re complex conjugate. While a symmetrix matrix\u0026rsquo;s eigenvalues are all real numbers.\nSolve eigenvectors ùêó:\nFor Œª‚ÇÅ=i,\n$$ \\begin{aligned} (ùêÄ - Œªùêà) ùêó = 0 \\\\ \\begin{bmatrix} 0-Œª‚ÇÅ \u0026amp; -1 \\\\ 1 \u0026amp; 0-Œª‚ÇÅ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\begin{bmatrix} -i \u0026amp; -1 \\\\ 1 \u0026amp; -i \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\end{aligned} $$\nLet the free variable b=1, then $[^a_b] = [^i_1]$\nFor Œª‚ÇÅ= -i,\n$$ \\begin{aligned} \\begin{bmatrix} i \u0026amp; -1 \\\\ 1 \u0026amp; i \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\end{aligned} $$\nLet the free variable b=1, then $[^a_b] = [^{-i}_1]$\nRepeated eigenvalues Eigenvalues of a triangular matrix is obvious: the diagonal, as the determinant is directly factorized.\nFor example, given matrix $ùêÄ = [^{3 \\ 1}_{0 \\ 3}]$:\nSolve its eigenvalues:\n$$ \\begin{aligned} | ùêÄ - Œªùêà | = 0 \\\\ \\begin{vmatrix} 3-Œª \u0026amp; 1 \\\\ 0 \u0026amp; 3-Œª \\end{vmatrix} = 0 \\\\ (3-Œª)(3-Œª) \\end{aligned} $$\nTwo roots: Œª‚ÇÅ=3, Œª‚ÇÇ= 3\nSolve eigenvectors:\nFor Œª‚ÇÅ=3,\n$$ \\begin{aligned} (ùêÄ - Œªùêà) ùêó = 0 \\\\ \\begin{bmatrix} 3-Œª‚ÇÅ \u0026amp; 1 \\\\ 0 \u0026amp; 3-Œª‚ÇÅ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\begin{bmatrix} 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\end{aligned} $$\nSo $[^a_b]$ can be $[^1_0]$\nFor Œª‚ÇÇ= 3, same formulas appear. There isn\u0026rsquo;t the 2nd (set of) independent eigenvectors.\nBut there are supposed to be two eigenvalues. It\u0026rsquo;s incomplete yet.\n","date":"2023-11-06T13:51:00Z","permalink":"https://zichen34.github.io/writenotes/calc/eigenvalues_vectors/","title":"watch: LA - G.S. 21 | Eigenvalues \u0026 Eigenvectors"},{"content":"vars() Create a class ParamGroup to store args into instance variables, then use vars(self) to read memebers from self.__dict__.\nAdd each argument into parser through parser.add_argument().\nSet shorthand to the initial character, e.g., --source_path use -s\nExample from gaussian-splatting:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class ParamGroup: def __init__(self, parser: ArgumentParser, name : str, fill_none = False): group = parser.add_argument_group(name) for key, value in vars(self).items(): shorthand = False if key.startswith(\u0026#34;_\u0026#34;): shorthand = True key = key[1:] t = type(value) value = value if not fill_none else None if shorthand: if t == bool: group.add_argument(\u0026#34;--\u0026#34; + key, (\u0026#34;-\u0026#34; + key[0:1]), default=value, action=\u0026#34;store_true\u0026#34;) else: group.add_argument(\u0026#34;--\u0026#34; + key, (\u0026#34;-\u0026#34; + key[0:1]), default=value, type=t) else: if t == bool: group.add_argument(\u0026#34;--\u0026#34; + key, default=value, action=\u0026#34;store_true\u0026#34;) else: group.add_argument(\u0026#34;--\u0026#34; + key, default=value, type=t) class ModelParams(ParamGroup): def __init__(self, parser, sentinel=False): self.sh_degree = 3 self._source_path = \u0026#34;\u0026#34; self._model_path = \u0026#34;\u0026#34; self._images = \u0026#34;images\u0026#34; self._resolution = -1 self._white_background = False self.data_device = \u0026#34;cuda\u0026#34; self.eval = False super().__init__(parser, \u0026#34;Loading Parameters\u0026#34;, sentinel) (2024-04-03)\nIn this way, there won\u0026rsquo;t be a long list of parser.add_argument() declaiming all arguments.\nInstead, related arguments are arranged into a group.\nCustomize Parsing (2023-09-27)\nAnalyse string manually\nRefer to Match-NeRF for an example.\n","date":"2023-11-04T13:40:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_args_parse/","title":"memo: Python | Parse Input Args"},{"content":"From C to CUDA Source article: CUDA 01 | Á¨¨‰∏Ä‰∏™Á®ãÂ∫è - Master KangKangÁöÑÊñáÁ´† - Áü•‰πé\nCUDA extends Cpp to GPU.\nC lang: \u0026ldquo;add.c\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 #include\u0026lt;stdio.h\u0026gt; int add(int a, int b){ int c = a + b; return c; } int main(){ int c = add(2, 3); printf(\u0026#34;c = %d\\n\u0026#34;, c); return 0; } CUDA: \u0026ldquo;add.cu\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include\u0026lt;stdio.h\u0026gt; __global__ void add(int a, int b, int *c){ *c = a + b; } int main(){ int c; // Allocate Unified Memory - accessible from CPU or GPU int* c_cuda; // a memory for a int data cudaMalloc((void**)\u0026amp;c_cuda, 1*sizeof(int)); //return pointer // Launch kernel on 1 block containing 1 thread add\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(2, 3, c_cuda); // Transfer data between GPU VRAM and CPU RAM cudaMemcpy(\u0026amp;c, c_cuda, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;c=%d\\n\u0026#34;, c); // Free the allocated unified memory cudaFree(c_cuda); return 0; } Adapt C to CUDA (kernel function):\nThe function add is declared as a __global__ function, and then it becomes a kernel, which is called from CPU and executed on GPU.\nKernels have no return value, as results are left in memory.\nThus, the GPU memory for output data must be allocated (by cudamalloc) and passed into the kernel. And free it (with cudaFree) at the end. A kernel needs execution configuration: \u0026lt;\u0026lt;\u0026lt;blocks, threads, shared_mem, stream\u0026gt;\u0026gt;\u0026gt; to sequentialize the host code before host compilation. Docs\nCompile\n1 2 3 4 5 6 7 8 # For C project gcc add.c -o add_c # For CUDA project nvcc add.cu -o add_cuda # Profiler nvprof ./add_cuda Blocks \u0026amp; Threads CUDA locates a thread via blocks.\nNumber of threads in a block is a multiple of 32, e.g., 256 threads. An Even Easier Introduction to CUDA - Nvidia Blog - Mark Harris, Jan25, 2017\nTotal threads can be reshaped to 2D or 3D, and accordingly the kernel needs to modify the threads indexing.\nNumber of blocks must be larger than the total elements.\nGiven N elements, there needs n = (N + blkDim.x - 1)/blkDim.x\nd a t a 0 b l 1 k 1 2 3 b l 4 k 2 5 ‚ãØ ‚ãØ ‚ãØ b l k n - 1 N - 1 b l ‚ãØ ‚ãØ k ‚ãØ ‚ãØ ‚ãØ ‚ãÆ ‚ãØ n ‚ãØ ‚ãØ - ‚ãØ ‚ãØ ‚ãÆ ' Index of a thread is blockIdx.x * blockDim.x + threadIdx.x.\nAnd a grid includes all the threads = gridDim.x * blockDim.x. (grdiDim in the following figure may be wrong.)\nRef\nTutorial 01: Say Hello to CUDA - CUDA Tutorial - Read the Docs\nTriple chevrons: How is the CUDA\u0026laquo;\u0026lt;\u0026hellip;\u0026raquo;\u0026gt;() kernel launch syntax implemented - SO\nExample Source article: An Easy Introduction to CUDA C and C++ - Nvidia Blog - Mark Harris, Oct31, 2012\nEach thread handles a single element in an array. Glossary SM: Streaming Multiprocessor Docs\ndim3: An integer vector based on uint3. Programming Guide\nTo do\nGetting Started With CUDA for Python Programmers\ncudaMemcpy (2024-02-02)\nDoc: NVIDIA CUDA Library | Example from 3DGS\nSpecify dest and src pointers, number of bytes to be transferred, and direction of copy.\n1 2 3 4 5 int num_rendered; // Declare and Allocate memory on the host cudaMemcpy(\u0026amp;num_rendered, // points to destination area geomState.point_offsets + P - 1, // points to source area sizeof(int), // copy an signed int (4 bytes) cudaMemcpyDeviceToHost) // direction Shared Memory Docs\n(2024-02-05)\nShared by threads within a thread block. Synchronization Docs-#7.6\n__syncthreads is a \u0026ldquo;checkpoint\u0026rdquo; to wait all thread arrive at this point.\nNVIDIA CUDA Tutorial 8: Intro to Shared Memory -Ytb - Creel\n","date":"2023-11-03T12:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/basic/","title":"memo: CUDA | Basics"},{"content":"Docs of Nsight: Getting Started with the CUDA Debugger\nDebug Demo (2023-11-03)\nEnvironment: Ubuntu 20.04, cuda-11.6 (in /usr/local/cuda-11.6), GPU 1050Ti.\nnvcc\n1 2 3 4 5 6 (base) yi@yi-Alienware:~/Downloads/CUDA_Study/Debug_CUDA$ nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_Mar__8_18:18:20_PST_2022 Cuda compilation tools, release 11.6, V11.6.124 Build cuda_11.6.r11.6/compiler.31057947_0 Prerequisite:\nInstall 2 extensions: Nsight and C/C++\nCreate 2 debugging configuration files: launch.json and tasks.json under \u0026ldquo;.vscode/\u0026rdquo;\nSelect debugger: CUDA C++ (CUDA-GDB) Example with nvcc Testing repo\n\u0026ldquo;test.cu\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(int argc, char **argv) { std::cout \u0026lt;\u0026lt; \u0026#34;Number of input arguments: \u0026#34; \u0026lt;\u0026lt; argc \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt;= argc-1; i++) { std::cout \u0026lt;\u0026lt; argv[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } return 0; } \u0026ldquo;launch.json\u0026rdquo; for debug configurations:\nSet program as the output binary program to be debugged:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;CUDA C++: Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/test.bin\u0026#34;, // binary file \u0026#34;preLaunchTask\u0026#34;: \u0026#34;mynvcc\u0026#34; }, // no need to change this: { \u0026#34;name\u0026#34;: \u0026#34;CUDA C++: Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34; } ] } \u0026ldquo;tasks.json\u0026rdquo; for building configurations:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;mynvcc\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;nvcc\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;${file}\u0026#34;, \u0026#34;-g\u0026#34;,\u0026#34;-G\u0026#34;, \u0026#34;-o\u0026#34;,\u0026#34;${fileDirname}/test.bin\u0026#34;, ] } ] } Error: /usr/bin/bash: nvcc: command not found\nAdd command on PATH before running startup scripts. tasks - VSCode - Docs\nTried add export PATH=\u0026quot;$PATH:/usr/local/cuda-12.3/bin\u0026quot; into \u0026ldquo;/etc/environment\u0026rdquo;, \u0026ldquo;/etc/profile\u0026rdquo;, \u0026ldquo;/etc/xprofile\u0026rdquo;, \u0026ldquo;/etc/bash.bashrc\u0026rdquo; all doesn\u0026rsquo;t work. How to permanently set $PATH on Linux/Unix -SO\nSolution: Set integrated terminal in user settings.json (My vscode version: 1.83.1, 2023-11-03)\n1 2 3 4 5 6 7 8 9 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { \u0026#34;bash\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;bash\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-i\u0026#34; ] } }, \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, Ref: VSCode tasks error: /bin/bash: npm: command not found (Found by DDG with searching \u0026ldquo;vscode debug tasks.json /usr/bin/bash: nvcc: command not found\u0026rdquo;)\nThen, with the \u0026ldquo;test.cu\u0026rdquo; file opening in the editor, click the start button to initiate debugging.\n(2024-01-26) I still don\u0026rsquo;t know how to include headers for libtorch in the CLI of nvcc. So, I didn\u0026rsquo;t manage to compile the 3DGS project with nvcc as above.\nPotentially useful:\nAn example: Include path problems for GPU library - SO Docs of nvcc: NVIDIA CUDA Compiler Driver NVCC (2024-01-27)\nBased on Troubles while compiling C++ program with PyTorch, HElib and OpenCV - SO, and reminded by perplexity, -Wl and -rpath are used in GCC for linking and specifying runtime library search path. In contrast, nvcc has -Xlinker for linking during compilation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Compile CUDA source files into object files with nvcc nvcc --compile -g -G -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ main_copy.cu \\ -o main.o # Link object files into an executable with g++ g++ main.o \\ -L/usr/local/libtorch/lib \\ -L/usr/local/cuda/lib64 \\ -Wl,-rpath,/usr/local/libtorch/lib \\ -ltorch -ltorch_cpu -lc10 -lcudart \\ -o my_executable Compiling is OK. But linking reports error:\n1 2 3 /usr/bin/ld: main.o: in function `main\u0026#39;: /home/yi/Downloads/debug_diff_rast/main_copy.cu:31: undefined reference to `RasterizeGaussiansCUDA(at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, float, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, float, float, int, int, at::Tensor const\u0026amp;, int, at::Tensor const\u0026amp;, bool, bool)\u0026#39; collect2: error: ld returned 1 exit status Not successful yet. Need to compile the library with nvcc.\nExample with Makefile Nvidia Tutorial Clip: Debugging CUDA kernels with VS Code\nMicrosoft VS CUDA Support in Visual Studio Code with Julia Reid\n(2024-01-26)\nUse cmake to produce Makefile, otherwise, error occurs: make *** no targets specified and no makefile found. stop\nAnd then edit the launch.json and tasks.json following this article: Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\nRefer to my repo for debugging 3DGS.\nExample with CMake Debugging CUDA kernels with VS Code\nRef\nSource articles: CUDA Áï™Â§ñÁØá | Visual Studio CodeÁöÑCUDAÁéØÂ¢É - Master KangKangÁöÑÊñáÁ´† - Áü•‰πé Adapted demo: vscodeËøúÁ®ãË∞ÉËØïLinux CUDAÁ®ãÂ∫è- oushaojun2 - CSDN Debug Cuda Samples (2023-11-02)\nDownload sample project: NVIDIA/cuda-samples for 12.3\n1 git clone https://github.com/NVIDIA/cuda-samples.git Make 12.3 failed with 11.6\n1 2 cd ./cuda-samples make dbg=1 Error: /usr/bin/ld: cannot find -lglut\nNeed: sudo apt-get install freeglut3 freeglut3-dev\nError:\n1 2 3 4 5 6 /usr/bin/ld: simpleCUFFT_callback.o: in function `main\u0026#39;: /home/yi/Downloads/cuda-samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu:103: undefined reference to `cudaGetDeviceProperties_v2\u0026#39; collect2: error: ld returned 1 exit status make[1]: *** [Makefile:373: simpleCUFFT_callback] Error 1 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback\u0026#39; make: *** [Makefile:45: Samples/4_CUDA_Libraries/simpleCUFFT_callback/Makefile.ph_build] Error 2 cudaGetDeviceProperties_v2 is not existed in cuda 11.x, but appear in cuda 12.2. SO\ncuda-sample-11.6 Download zip: Release pkg 11.6; Git tag-11.6\nmake\n1 2 3 4 5 6 7 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/cuda-samples-11.6$ make dbg=1 make[1]: Entering directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/3_CUDA_Features/ptxjit\u0026#39; /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -g -G --threads 0 --std=c++11 -gencode arch=compute_35,code=compute_35 -o ptxjit.o -c ptxjit.cpp nvcc fatal : Unsupported gpu architecture \u0026#39;compute_35\u0026#39; make[1]: *** [Makefile:396: ptxjit.o] Error 1 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/3_CUDA_Features/ptxjit\u0026#39; make: *** [Makefile:45: Samples/3_CUDA_Features/ptxjit/Makefile.ph_build] Error 2 Devices with compute capacity (cc) 3.x have been dropped by cuda 12.x. Solution is removing the requests of compute_35 in the make file. Forum Nv\nCuda Toolkit is compatible the devices with lower cc than it supports. CUDA 11.x supports a maximum cc of 8.x. CSDN\n(2023-11-02) Remove cc of 35 and 37 SO:\nReplace all the pattern SMS ?= 35 37 with SMS ?= through VSCode. Replace all the pattern compute_35 with compute_61 Make failed:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -g -G --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o reduction_kernel.o -c reduction_kernel.cu reduction_kernel.cu(558): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(558): error: expected an identifier __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(558): warning #1835-D: attribute \u0026#34;__shared__\u0026#34; does not apply here __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ Remark: The warnings can be suppressed with \u0026#34;-diag-suppress \u0026lt;warning-number\u0026gt;\u0026#34; reduction_kernel.cu(558): error: expected a \u0026#34;;\u0026#34; __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(561): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name auto cta = cg::experimental::this_thread_block(scratch); ^ reduction_kernel.cu(561): error: identifier \u0026#34;scratch\u0026#34; is undefined auto cta = cg::experimental::this_thread_block(scratch); ^ reduction_kernel.cu(563): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name auto multiWarpTile = cg::experimental::tiled_partition\u0026lt;MultiWarpGroupSize\u0026gt;(cta); ^ 6 errors detected in the compilation of \u0026#34;reduction_kernel.cu\u0026#34;. make[1]: *** [Makefile:358: reduction_kernel.o] Error 255 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/2_Concepts_and_Techniques/reduction\u0026#39; make: *** [Makefile:45: Samples/2_Concepts_and_Techniques/reduction/Makefile.ph_build] Error 2 VSCode didn\u0026rsquo;t find header with red underlines:\n1 2 3 #include errors detected. Please update your includePath. Squiggles are disabled for this translation unit (/home/yi/Downloads/cuda-samples-11.6/Samples/2_Concepts_and_Techniques/reduction/reduction_kernel.cu).C/C++(1696) cannot open source file \u0026#34;cooperative_groups/reduce.h\u0026#34;C/C++(1696) Edit \u0026ldquo;c_cpp_properties.json\u0026rdquo; as:\n1 2 3 4 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/cuda-11.6/include\u0026#34; ], Didn\u0026rsquo;t solve. And the header is there and can be found.\n(2023-11-03) Intellisense erros for CUDA syntax.\nEnsure selecting lang as \u0026ldquo;CUDA C++\u0026rdquo; rather than \u0026ldquo;C++\u0026rdquo;. VSCode Nsight Intellisense not detecting functions and datatypes for *cu; *cpp works - NV forum\nRed underlines disappeared after the C/C++ extension got disabled and only \u0026ldquo;Nsigh\u0026rdquo; extension left. But these 2 extensions both are installed in every tutorial.\n(2023-11-03)No error in a folder contains only .cu files. Thus, Python code can\u0026rsquo;t co-exist with CUDA code?\n(2023-11-08) ‚úÖ Refering the \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo; in the CUDA sample: \u0026ldquo;Samples/0_Introduction/matrixMul\u0026rdquo;, the compiler should be nvcc, not \u0026ldquo;/usr/bin/gcc\u0026rdquo;.\n1 \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/local/cuda/bin/nvcc\u0026#34;, Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\nSame error about reduction here issue#201. But he was 11.8.\nRe-install cuda toolkit 11.8 and test samples of 11.8.\nNote: the final line of the installing scripts provided on official site should be: sudo apt-get -y install cuda-11-8 instead of sudo apt-get -y install cuda\nError persists at reduction_kernel.cu.\nBuild with CMake (2023-11-17)\nThe library \u0026ldquo;diff-gaussian-rasterization-comments/cuda_rasterizer\u0026rdquo; is built according to CXX standard.\nHowever, I cannot step into the CUDA kernels when debugging.\nMaybe using nvcc to build can enable debugging.\nCreate a CMakeLists.txt using nvcc?\n","date":"2023-11-02T17:44:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/debug/","title":"memo: CUDA | Debugging"},{"content":"CosAnelWrmRst Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import torch from torch import nn from torch.utils.data import TensorDataset, DataLoader from matplotlib import pyplot as plt import numpy as np inps = torch.arange(50.).expand(10,-1).reshape(100, 5) tgts = torch.arange(50.).expand(10,-1).reshape(100, 5) dataset = TensorDataset(inps, tgts) loader = DataLoader(dataset, batch_size=1, pin_memory=True) model = nn.Sequential(nn.Linear(5,5)) criterion = torch.nn.MSELoss() optimizer = torch.optim.AdamW(model.parameters()) scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( optimizer, T_0=6, # First cycle is 6 epochs T_mult=2, # next cycle will be 2x epochs (int) eta_min=1e-5, # minimum lr last_epoch=-1, # set when resuming verbose=False) # print lr eps = np.arange(100) lrs = [] for idx, batch in enumerate(loader): x = batch[0] y = batch[1] y_pred = model(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step() lrs.append(scheduler.get_last_lr()) scheduler.step() plt.plot(eps, lrs) plt.title(\u0026#34;CosineAnnealingWarmRestarts\u0026#34;) scheduler.step(0) will set lr to the value at epoch 0. WarmUp + CosAnelWrmRst Ref: firstelfin/WarmUpLR\nThe original CosineAnnealingWarmRestarts doesn\u0026rsquo;t have warmup. WarmUpLR followed by CosineAnnealingWarmRestarts\n1 2 cosine = CosineAnnealingWarmRestarts(**param) warm_up_lr = WarmUpLR(cosine) The first 9 epochs use WarmUpLR, and the following use CosineAnnealingWarmRestarts.\nGallery A Visual Guide to Learning Rate Schedulers in PyTorch - Medium - Leonie Monigatti\n(2023-10-30)\nMax_lr Decay qu-gg/pytorch-cosine-annealing-with-decay-and-initial-warmup\nFound by github searching \u0026ldquo;CosineAnnealingWarmRestart\u0026rdquo;. Results\n","date":"2023-10-24T17:53:52Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_lr_sched/","title":"memo: PyTorch | LR \u0026 Scheduler"},{"content":"Code | Arxiv(2303) | ProjPage\nBriefs Author\u0026rsquo;s Talk Source Video: „ÄêTalk | ICLR'23 Oral ÁæéÂõΩ‰∏úÂåóÂ§ßÂ≠¶È©¨Êó≠ÔºöÂõæÂÉè‰∫¶ÊòØÁÇπÈõÜÔºàImage as Set of PointsÔºâ„Äë\nInsights:\nUse clustering to extract image features.\nFusing cluster members through Collapse followed by Reconstruction\nPaper Explain Source Video: „ÄêICLR 2023„ÄëImage as Set of Points.ËÆ°ÁÆóÊú∫ËßÜËßâÊñ∞ËåÉÂºèÔºåÂà©Áî®ËÅöÁ±ªÁöÑÊÄùÊÉ≥ÂÆûÁé∞ÂõæÂÉèÂª∫Ê®°„ÄÇÂú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏ä‰∏çËæìViTÂíåConvNets„Äë\nImage features are refined through multiple aggregation and dispatching using attention for pixels. Paper Notes Abstract Is the picture natively well-clustered?\nIf so, this method essentially is same as convolution, which extracts features by fusing pixels in the kernel.\nBut Cluster is discreate and sparse, can it capture high-level feature?\nClusters appeals to segmentation and interpretability.\nIntro Is this method designed for a single image?\nHowever, MVS can directly leverage epipolar lines on source views more concisely, without fusing neighbor pixels features..\nPixels with same color can imply fundamentally difference?\n\u0026ldquo;Similar pixels are grouped, but they are fundamentally different.\u0026rdquo; ?\nI think same color means rays hit same spatial geometry.\nMethod Pipeline:\nInitial feature of raw pixels are 5-D, including color (r,g,b) ‚àà [0,255] and normalized pixel center (x,y) ‚àà [0, 1]-0.5\nPixels are reduced to ùëü times at each stage\u0026rsquo;s start by fusing k (=4 or 9) neighbors around the ùëü evenly distributed anchors across the image, i.e., concatenating their channels and projecting to specified dimension by FC, unlike maxpooling.\nConv layer can perform points reduction as well if points\u0026rsquo; organization aligned with the raw image.\nIn their implementation, function PointReducer uses a conv layer instead of FC.\nFor model coc_base_dim64, before stage-0, image is reduced to 1/16 with kernels of size=4, i.e., 16 points are fused to 1. and 5-D features are mixed to 64-D.\n1 2 # Point Reducer is implemented by a layer of conv since it is mathmatically equal. nn.Conv2d(5, 64, kernel_size=4, stride=4, padding=0) Output points require reording for downstream pixel-wise tasks, like segmentation.\nCoC Block:\nEach stage repeats Context Cluster block several time. A blocks processes points set in 3 steps:\nPixels clustered $c$ groups in the feature space\nc centers are evenly distributed and form voronoi based on the cosine similarity of each point to each center feature.\nCenters are chose through nn.AdaptiveAvgPool2d((proposal_w, proposal_h)), which will set kernel and stride automatically for AvgPool2d.\nCenter feature is the average of k nearest neighbors, after placing the c centers.\nIntial features are 5-D including RGB and position (x,y).\nAggregation: Add features of all members to an aggregated feature g:\n$$g = \\frac{1}{C} (v_c + ‚àë_{i=1}^m sig(Œ± s_i + Œ≤) * v_i )$$\nThe aggregated feature g is computed by plusing center\u0026rsquo;s feature and the weighted sum of feature vectors of all m points v·µ¢ in a cluster, scaled by a tunable factor $sig(Œ± s·µ¢ + Œ≤)$ ‚àà (0,1), where s is similarity to the center feature and Œ±,Œ≤ are nn.parameters.\nThe denominator $C = 1+ ‚àë_{i=1}^m sig(Œ± s·µ¢+ Œ≤)$ aims to limit the magnitude.\nDispatching: Each member updates its feature from the aggregated feature, so as to fuse all other points and realize spatial interaction.\n$$p_i\u0026rsquo; = p_i + FC(sig (Œ± s·µ¢+ Œ≤) * g )$$\nThe amount of g assigned to a member is determined by the adaptive similarity again, inversing the summation.\nCommunication between pixel in a cluster is like server-client in a centralized network.\nCenters\u0026rsquo;s positions are fixed for efficiency, so it emphasizes locality.\ndoubt: Advanced postional embedding could be applied.\ndoubt: Will different selection strategies affect model performance? They mentioned Farthest Point Sampling (FPS) mehtod in appx.D\nArchitecture:\nContext Cluster is a hierarchical model composed 4 stages and points are reduced to 1/4 (ie, h/2, w/2) after each stage.\nPlay Model can be comprehended by debugging the file \u0026ldquo;context_cluster\u0026rdquo;, using environment of \u0026ldquo;AIM\u0026rdquo;.\n\\begin{algorithm} \\begin{algorithmic} \\STATE PointReducer: Conv2d(x), downsample 16 times, 256 dim \\STATE Partition feature maps: rearrange(x) \\STATE Centers feature from x: AdaptiveAvgPool2d((2,2))(x) \\STATE Simlarity matrix: vectors' inner product with multi-head \\STATE Clustering: .scatter\\_ \\STATE Aggregate feature $g$: sum members' feat based on similarity \\STATE Dispatch $g$ to members \\STATE Reverse partition \\STATE Project to out\\_dim: Conv2d \\STATE FFN: Mlp, out\\_dim ‚Üí hidden ‚Üí out\\_dim \\end{algorithmic} \\end{algorithm} Similarity and points\u0026rsquo; features are optimized separately:\nSimilarity: x ‚Üí center ‚Üí sim\nFeatures: x ‚Üí value ‚Üí val_center ‚Üí aggregated feature out\nsim and out are decoupled.\n","date":"2023-10-15T13:40:00Z","image":"https://pic2.zhimg.com/80/v2-2aac08c8a30f4726f3eb32b608a95589_720w.webp","permalink":"https://zichen34.github.io/writenotes/model/misc/b-note-image_as_points/","title":"read: Image as Set of Points"},{"content":" (Discussed in QQ group 706949479) Code | Arxiv | ProjPage\nAuthor\u0026rsquo;s blog: ICCV 2023 NeRFÊèêÁÇπÁöÑMagic Loss Âç≥ÊèíÂç≥Áî® ‚Äî‚Äî S3IMÈöèÊú∫ÁªìÊûÑÁõ∏‰ººÊÄß - Summer CloverÁöÑÊñáÁ´† - Áü•‰πé Notes Abs Previous NeRF didn\u0026rsquo;t utilize structural information on image level, but train and predict point-wise.\nMethod = 3 1 = 6 1 9 = 2 3 = = 2 5 = 3 2 S 5 - 4 = 3 - 1 3 ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚¨á I - = - M 8 - 0 = 9 - 4 ‚ÇÅ 7 - = 1 - = = 2 7 = 4 6 0 = 6 8 R e o r d e r = 9 7 = 6 2 1 = 8 0 = = 0 4 = 4 8 S - 6 = - 7 3 ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚¨á I - = - M 2 - 2 = 6 - ‚ÇÇ 1 - 5 = 2 - 3 = = 5 3 = 1 1 4 3 = 9 3 R e o r d e r Steps:\nApply SSIM on the randomly selected training pixel patch with a kernel size $K$ (=2) and stride size S (=K).\nRepeatedly reorder the predicted and target pixel patchs, and calculate S3IM multiple ($M$=10) times.\nThe final loss term is the average of them multiplied with a weight factor (hyperparameter) $Œª$.\n$$ \\rm L_{S3IM} = Œª ‚ãÖ (1 - \\frac{1}{M} \\sum_{m=1}^M SSIM(Patch_{rendered}, Patch_{target}) ) $$\nCompare with SSIM:\nS3IM applied on random pixel patches significantly outperforms SSIM applied on local continuous patches.\nThe authors explain this as the SSIM can only capture the local similarity, whereas S3IM can compare the nonlocal structural similarity over all training images.\nTraining NeRF with local continuous patches will hurt the performance (as stated at the end of section 3.1).\nPlay Code\n1 2 3 4 5 6 7 8 9 s3im_func = S3IM(kernel_size=args.s3im_kernel, stride=args.s3im_stride, repeat_time=args.s3im_repeat_time, patch_height=args.s3im_patch_height, patch_width=args.s3im_patch_width).cuda() if args.s3im_weight \u0026gt; 0: s3im_pp = args.s3im_weight * s3im_func(rgb_map, rgb_train) total_loss += s3im_pp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class S3IM(torch.nn.Module): r\u0026#34;\u0026#34;\u0026#34;Implements Stochastic Structural SIMilarity(S3IM) algorithm. Arguments: kernel_size (int): kernel size in ssim\u0026#39;s convolution(default: 4) stride (int): stride in ssim\u0026#39;s convolution(default: 4) repeat_time (int): repeat time in re-shuffle virtual patch(default: 10) patch_height (height): height of virtual patch(default: 64) patch_width (height): width of virtual patch(default: 64) \u0026#34;\u0026#34;\u0026#34; def __init__(self, kernel_size=4, stride=4, repeat_time=10, patch_height=64, patch_width=64): super(S3IM, self).__init__() self.kernel_size = kernel_size self.stride = stride self.repeat_time = repeat_time self.patch_height = patch_height self.patch_width = patch_width self.ssim_loss = SSIM(window_size=self.kernel_size, stride=self.stride) def forward(self, src_vec, tar_vec): r\u0026#34;\u0026#34;\u0026#34; src_vec: (ray_batch_size=4096=64*64, 3) \u0026#34;\u0026#34;\u0026#34; loss = 0.0 index_list = [] for i in range(self.repeat_time): if i == 0: tmp_index = torch.arange(len(tar_vec)) # (4096) index_list.append(tmp_index) else: ran_idx = torch.randperm(len(tar_vec)) index_list.append(ran_idx) res_index = torch.cat(index_list) # (M * ray_bs = 10*4096) tar_all = tar_vec[res_index] # (10*4096, 3) src_all = src_vec[res_index] tar_patch = tar_all.permute(1, 0).reshape(1, 3, self.patch_height, self.patch_width * self.repeat_time) src_patch = src_all.permute(1, 0).reshape(1, 3, self.patch_height, self.patch_width * self.repeat_time) loss = (1 - self.ssim_loss(src_patch, tar_patch)) return loss ","date":"2023-10-14T10:30:00Z","image":"https://miro.medium.com/v2/resize:fit:1400/1*SBVkh54RJZrMsozG-vQdCQ.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-s3im/","title":"read: Render - NVS | S3IM Loss for NeRF"},{"content":"(2023-10-15)\nTalk-221121 Source video: ‰∏ÅÈúÑÊ±âÔºöÁªìÊûÑÈáçÂèÇÊï∞ÂåñÊòØÊÄé‰πàÊù•ÁöÑ„ÄêÊ∑±Â∫¶Â≠¶‰π†„Äë„ÄêÁõ¥Êí≠ÂõûÊîæ„Äë-bilibili\nRepVGG: (2021)\nVGG has bad performance with fast inference because single stream can run in parallel efficientlly.\nAnd multiple branches means multiple sets of parameters, which help achieve better precision.\nIf a set of parameters can be transformed equivalently to another set of parameters, the corresponding structure would changed naturally.\nTherefore, the multi-branches architecture during training can be transformed to a single branch model in inference period.\nMethodology: Kernel size can vary while computation remains, e.g., a 1√ó1 kernel can be reshaped to 3√ó3\nThus, 3 branches with a 3√ó3 kernel, a 1√ó1 kernel, and a 3√ó3 identity kernel can be added to a single 3x3 kernel based on the property of linearity of convolution: $x * K_a + x * K_b = x * (K_a+K_b)$ Centripetal SGD: (2017)\nInception: Compressing (pretrained?) models by pruning redundent channels in feature maps. To create identical channels, let optimizer (SGD) guide some channels to become similar. Two same channels merged to a comprehensive channel, model gets concise while performance unchanged. Linear Redundancy Unit (Obsolete)\nMerge 2 feature maps: Training with two 3√ó3 kernels and merge the 2 kernels after training. This method brought marginal improvement though.\nThis indicates that two models with the same final structure, but experienced different training processes in different architectures, have different performances.\nAsymmetic Convolution Block (2019)\nBranches are different: 3√ó3 + 1√ó3 + 3√ó1 Research on Simple Models: (2020)\nUse identity branch to eliminate some shortcuts in ResNet.\nHow to make a ultimate simple yet powerful model without shortcut? ‚ñ∂ RepVGG Multiple branches like InceptionNet in just a single kernel.\nWhy can it work in an arbitrary model? ‚ñ∂ Diverse Branch Block (DBB) RepMLP: (2022)\nInject locality into MLP (CNN is a special MLP) by transforming an arbitrary conv kernel to a FC kerenel. RepLKNet: (2022)\nLarge kernel: 31x31 + 5x5 Misc:\nResRep for channel pruning.\nOutput channels can be controled through a 1x1 kernel after the original 3x3 kernel. Such that channel pruning can be performed on the 1x1 kernel. RepOptimizer: generalize to gradient reparameterization for fast training.\nIncorperating the prior knowledge (inductive bias) into optimizer instead of model structure. RepVGGplus: principles behind RepVGG Ideas moving forward:\nConnect Structural Rep with every element in a general vision model:\nTopology (RepVGG), Component (ACNet, DBB), Width (ResRep), Globality v.s. locality (RepMLP), Kernel size (RepLKNet), Optimizer (RepOptimizer)\nRethink classical problems.\nSimple model, like VGG, doesn\u0026rsquo;t work? (RepVGG) Can\u0026rsquo;t train a super deep model without shortcut? (RepVGGplus) Inception Net is too complex to be abandoned? (DBB) MLP can\u0026rsquo;t handle image tasks? (RepMLP) Large kernels are less effective? (RepLKNet) Related works:\nNon-deep Network; RepNAS, YOLO v6\u0026amp;v7, DyRep, Scaling up Kernels in 3D GNNs, RepUNet, RepSR (superres), De-IReps. Talk-220426 Source video: „ÄêËÆ∫ÊñáËøûËÆ≤ÔºöÁî®ÈáçÂèÇÊï∞ÂåñËµã‰∫àMLPÁΩëÁªúÂ±ÄÈÉ®ÊÄß„ÄÅË∂ÖÂ§ßÂç∑ÁßØÊ†∏Êû∂ÊûÑ„ÄêCVPR2022„Äë„ÄêÂü∫Á°ÄÊ®°Âûã„Äë„Äë- bilibili\n(2023-10-16)\nRepMLPNet \u0026ldquo;‰∏ÄÁßçÈááÁî®ÈáçÂèÇÊï∞ÂåñÊäÄÊúØÂºïÂÖ•Â±ÄÈÉ®ÊÄßÁöÑÂàÜÂ±Ç MLP ÁΩëÁªú\u0026rdquo;\nCode | Arxiv MLP has no locality, only global capacity, thus it\u0026rsquo;s not favorable to do linear projection on 2D images.\nLocality means the surrounding pixels of a input pixel should have larger contributions due to stronger correlation compared to distant pixels.\nHowever, MLP treats all pixels on the image equally without considering relative positions, resulting in that MLP is difficult to converge for images data due to high dimensionailty and individually training for each pixel.\nCNN perserves this inductive bias through kernels. But CNN doesn\u0026rsquo;t have long-range dependencies because different regions share the same parameters: the kernel. Thus, a CNN stacks multiple layers for a large receptive field. In contrast, MLP is a function of positions, sensitive to location.\nHence, one approach to inject locality is by creating parallel branches with various conv kernels (for different dimensions) alongside the fc layer.\nBy supplementing conv kernels, the model is competent both at long-range dependency and locality for 2D images.\nThe side effect is the mutiple disunified branches will hinder computation parallelism, and impair the inference efficiency finally.\nThe solution to maintain the inference efficiency and perserve conv branches is Structural Reparameterization:\nMerging multiple auxiliary branches to a single FC stream can be realized by transforming their parameters after training into one FC kernel, such that the inference speed and precision are unchanged.\nÈÄöËøáÂèÇÊï∞ÁöÑÁ≠â‰ª∑ËΩ¨Êç¢ÂÆûÁé∞ÁªìÊûÑÁöÑÁ≠â‰ª∑ËΩ¨Êç¢„ÄÇ\nGeneric CNNs with conv kernels include massive parameters. And multiple branch of conv kernels may be unfeasible if without reducing parameters.\nThere are three branches: FC3, 3x3 \u0026amp; 1x1 kernels, and \u0026ldquo;Identity\u0026rdquo; Identity branch performs FC1+FC2 after maxpooling shrinks (H,W) to only (1,1).\nThus, the FC layer only need 1 parameter. Plusing 4 parameters in BatchNorm (mean, std, scale factor, bias), this branch only has 5 parameters.\nThis branch functions like a SE block (Squeeze-Excitation) providing channel-wise \u0026ldquo;overall scaling\u0026rdquo;.\n3x3 and 1x1 conv layer perform \u0026ldquo;set-sharing\u0026rdquo; (depth-wise conv + group convolution), where total of C channels are split to S groups.\nC S c g h r n o l u s p s 0 1 2 3 4 5 6 7 8 9 1 0 1 1 Then the number of parameters in a conv layer reduced from (C√óH√óW)¬≤ to S√ó(H√óW)¬≤.\nThe main branch performs FC3 after depth-wise convolution for input feature maps.\nThe equivalent FC layer for a conv layer is required for adding conv layers to FC layer.\nFC kernel is the 2D weight matrix $W_{d‚Çí√ód·µ¢}$ in a linear layer.\nA 3D Conv kernel is a special FC kernel represented as a Toeplitz matrix, containing lots of shared parameters, so its associated FC kernel must exist.\nThen, 2 FC kernels can add up directly based on linearity.\nA FC layer processes a feature map through 4 steps: (n,c,h,w) ‚ûî (n, c√óh√ów) ‚ûî FC kernel ‚ûî (n, o√óh√ów) ‚ûî (n,o,h,w), denoted as: $\\rm MMUL(featmap, W_{d‚Çí√ód·µ¢})$, where d·µ¢ = c√óh√ów.\nA Conv layer with a 3D conv kernel $F$ and padding $p$ processes the feature map is denoted as $\\rm CONV(featmap, F,p)$\nThus, the problem is how to convert a 3D kernel to a 2D kernel.\nGiven the corresponding FC kernel of a conv kernel $W^{(F,p)}$, two operations are equivalent: $\\rm MMUL(featmap, W^{(F,p)}) = CONV(featmap,F,p)$\nConsidering a linear layer, it projects vectors: $\\rm V_{n√ód‚Çí} = V_{n√ód·µ¢} ‚ãÖW^{(F,q)\\ T}$\nInsert an identity matrix I:\n$$ V_{n√ód‚Çí} = V_{n√ód·µ¢} ‚ãÖI ‚ãÖ W^{(F,q)\\ T} = V_{n, d·µ¢} ‚ãÖ(I_{d·µ¢√ód·µ¢} ‚ãÖ W^{(F,q)\\ T}) $$\nThen, the term $(I‚ãÖW^{(F,q)\\ T})$ can be regarded as a convolution operation.\nA conv operation must be a Mat-Mul, but a Mat-Mul may not be a conv operation.\nWhat kind of Mat-Mul (FC layer) is a conv operation? It\u0026rsquo;s when the weight matrix is a Toeplitz matrix transformed from a conv kernel.\nBecause $W^{(F,p)}$ is transformed indeed from conv kernel, the Mat-Mul $\\rm I‚ãÖW^{(F,p)}$ is a convolution operation for sure.\n$$\\rm I_{d·µ¢√ód·µ¢}‚ãÖW^{(F,p)} ‚áî CONV(F,p,featmap)$$\nIn the convolution $I_{d·µ¢√ód·µ¢}‚ãÖW^{(F,p)}$, $I_{d·µ¢√ód·µ¢}$ is convoled. Thus, it\u0026rsquo;s supposed to be the featmap in CONV(). i.e., the $I_{d·µ¢√ód·µ¢}$ is reshaped from featmap $I_{(c√óh√ów, c, h, w)}$\nAdditional reshaping is needed to match the dimensionality:\n$$\\rm I_{d·µ¢√ód·µ¢}‚ãÖW^{(F,p)} = CONV(F,p,featmap).reshape(chw, c, h, w)$$\nFrom the above equation, the desired FC kernel $W^{(F,p)}$ is the result feature map of convolving the kernel F with a blank featmap:\n$$\\rm W^{(F,p)} = CONV(F,p,I_{(c√óh√ów, c, h, w)}).reshape(chw, c, h, w)$$\nFor example, if the conv kernel F is (c, o, (3,3)), then the corresponding FC kernel $W^{(F,p)}$ has shape: (o, h-3+2√óp+1, w-3+2√óp+1) = (c√óh√ów, o,h,w).\nThis \u0026ldquo;3D FC kernel\u0026rdquo; has finished the \u0026ldquo;sum\u0026rdquo; computation and gets waiting for Mat-Mul with the input feature maps.\nTo align with the squashed 2D input feature maps (n, c√óh√ów), it needs to be reshaped to 2D: (c√óh√ów, o√óh√ów).\nFinally, a 3D conv kernel becomes a 2D kernel.\nThe equivalent FC kernel of a conv kernel is the result of convolution on an identity matrix with proper reshaping.\nFuse the parameters (Œº,œÉ,Œ≥,Œ≤) of BatchNorm into convolution layer based on linearity.\n$$M\u0026rsquo; = Œ≥‚ãÖ[(MF -Œº)/œÉ] + Œ≤ = Œ≥‚ãÖ(MF)/œÉ + (Œ≤ - Œ≥‚ãÖŒº/œÉ)$$\nSo new kernel and bias: $F\u0026rsquo; = Œ≥‚ãÖF/œÉ, \\quad b\u0026rsquo; = (Œ≤ - Œ≥‚ãÖŒº/œÉ)$\nAfter that, bias-added conv kernels are converted to 2D kernels, which can be added up the main stream: FC3 kernel for inference with only MLP layers.\nResMLP-Net\nHierarchical design mimic popular vision models\nRepMLPBlock and FFN alternate.\nCan be used as the backbone for downstream tasks.\nAdjust the amount of parameters in each stage through \u0026ldquo;set-sharing\u0026rdquo;.\nNo need for large datasets (JFT300M) or many epochs (300~400) to train. (IN for 100 epochs).\nThroughput is higher than conventional CNN models. Speed has not much relation with the number of FLOPs.\nRepMLP is suitable for highly parallelized devices (GPU) rather than devices with lower computation capacity, like mobile.\n\u0026ldquo;Identity\u0026rdquo; branch is necessary for the performance with providing information in different scale and dimensions.\n\u0026ldquo;set-sharing\u0026rdquo; increase the number of groups will bring precision.\nLocality can be observed on the feature maps.\nRepMLPNet is robust for discontinuity between split patches from big images.\nThe resolution of Cityscapes dataset doesn\u0026rsquo;t match the pretrained model. They devided an entire image to small patches.\n1 2 3 4 5 class RepMLPNet: RepMLPNetUnit RepMLPBlock RepMLPBlock cannot resume training after model.locality_injection() because sub-modules have been deleted. Therefore, .locality_injection should be called with a new model before inference.\nBlog-210426 Source: ÁªìÊûÑÈáçÂèÇÊï∞ÂåñÔºöÂà©Áî®ÂèÇÊï∞ËΩ¨Êç¢Ëß£ËÄ¶ËÆ≠ÁªÉÂíåÊé®ÁêÜÁªìÊûÑ - ‰∏ÅÈúÑÊ±âÁöÑÊñáÁ´† - Áü•‰πé\nBlog-210517 Ëß£ËØªÊ®°ÂûãÂéãÁº©6ÔºöÁªìÊûÑÈáçÂèÇÊï∞ÂåñÊäÄÊúØÔºöËøõÂèØÊö¥ÂäõÊèêÊÄßËÉΩÔºåÈÄÄÂèØÊó†ÊçüÂÅöÂéãÁº© - ÁßëÊäÄÁåõÂÖΩÁöÑÊñáÁ´† - Áü•‰πé\nÁü©Èòµ‰πòÊ≥ïÂèØ‰ª•ÁúãÂÅöÂç∑ÁßØÔºö‰∏Ä‰∏™ 2D Êï∞ÊçÆÁü©Èòµ‰πò‰ª• $W^{(F,p)}$ÔºåÁõ∏ÂΩì‰∫éËøô‰∏™Êï∞ÊçÆÁü©ÈòµÂÖà reshape Êàê 4D ÁöÑ feature map ÂÅöÂç∑ÁßØÔºåÁªìÊûúÂÜç reshape Êàê 2D. Papers FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization\nCode\n","date":"2023-10-13T20:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/misc/c-symp-struct_reparam/","title":"sympo: Structural Reparameterization"},{"content":"watch: CppExt - AIËëµ 01 | Cpp Bridges PyTorch \u0026amp; CUDA\nSource video: Pytorch+cpp/cuda extension ÊïôÂ≠∏ tutorial 1 - English CC -\nCode Instructions The pure purpose of CUDA extensions is to make PyTorch programs faster.\nCUDA extensions are more efficient than PyTorch in two scenarios:\nProcedures can\u0026rsquo;t be executed in parallel, e.g., each ray has different numbers of points.\nMany sequential computations, like a nn.Sequential module including lots of conv layers. C++ can fuse multiple layers to a single function.\nRelations: PyTorch will call a C++ function, which will call the CUDA extension.\nP y T o r c h \" B C r p i p d g e \" C U D A Environment conda create -n cppcuda python=3.8\nLatest PyTorch: conda install pytorch==1.12.1 cudatoolkit=10.2 -c pytorch\nVersion of the (compiled) PyTorch needs to match the local CUDA version (checked by nvcc -V).\nUpgrade pip for building cpp programs: python -m pip install pip -U\nPybind11 The code: \u0026ldquo;interpolation.cpp\u0026rdquo; acts like the main function that calls the C++ function, and python will call the \u0026ldquo;main\u0026rdquo; function. The \u0026ldquo;main\u0026rdquo; function receives input tensors from PyTorch and return output tensors from CUDA code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Declare PyTorch #include \u0026lt;torch/extension.h\u0026gt; // Define starts with the type of return values torch::Tensor trilinear_interpolate( torch::Tensor features, // 8 corners torch::Tensor point // target point coord. No comma at the end ){ return features; } // API for Python PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ // Function name in python and the cpp function m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); } (2023-10-18) Didn\u0026rsquo;t update the includePath for PyTorch as follows because I didn\u0026rsquo;t find the entry \u0026ldquo;C/C++: Edit Configurations (JSON)\u0026rdquo; after pressing F1. It seems like VSCode finds PyTorch automatically.\n1 2 3 4 5 6 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], (2023-10-27) However, error intellisense occurs after I installed the \u0026lsquo;C/C++ Extension Pack\u0026rsquo; for VSCode. So setting includePath is necessary.\npybind11 connects Python and C++11 codes.\n1 2 pip install pybind11 pip install ninja Pip compile Build the cpp codes to a python package.\nCreate a \u0026ldquo;setup.py\u0026rdquo; for building settings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CppExtension setup( name=\u0026#34;my_cppcuda_pkg\u0026#34;, # python package name version=\u0026#34;0.1\u0026#34;, description=\u0026#34;cppcuda example\u0026#34;, long_description=\u0026#34;cpp-cuda extension\u0026#34;, author=\u0026#34;z\u0026#34;, author_email=\u0026#34;luckily1640@gmail.com\u0026#34;, ext_modules=[ CppExtension( name=\u0026#39;my_cppcuda_pkg\u0026#39;, sources=[\u0026#34;interpolation.cpp\u0026#34;,]) # code files ], cmdclass={ # commands to be executed \u0026#34;build_ext\u0026#34;:BuildExtension } ) Build and install the package:\n1 2 3 4 pip install . # setup.py path is cwd (since pip 21.3) # Or adding an arg to avoid the deprecation warning: pip install . --use-feature=in-tree-build (2024-03-06) Pybind11 module also can be compiled with cmake: Â¶Ç‰ΩïÂú®Python‰∏≠Ë∞ÉÁî®C++‰ª£Á†ÅÔºüpybind11ÊûÅÁÆÄÊïôÁ®ã - HexUp\nPyTorch Call \u0026ldquo;test.py\u0026rdquo; will call the cpp program.\nPackage torch must to be imported before the cuda extensions. 1 2 3 4 5 6 7 8 9 10 import torch import my_cppcuda_pkg features = torch.ones(8,1) point = torch.zeros(1,2) out = my_cppcuda_pkg.trilinear_interpolate(features, point) print(out) title: \u0026ldquo;watch: CppExt - AIËëµ 02 | Kernel Function\u0026rdquo; date: 2023-10-23T20:20:00\nSource video: Pytorch+cpp/cuda extension ÊïôÂ≠∏ tutorial 2 - English CC -\nDocs: CUDA C++ Programming Guide\nGPU Parallsiam Kernel ‚Üí Grid ‚Üí Block ‚Üí Thread\nC P K n U e e r l d a t a ‚ãÆ G P G U r i B d l T o c h k r 0 e a d B l T o c h k r 1 e a d ‚ãÖ ‚ãÖ ‚ãÖ A thread is the smallest computation unit that executes element arithmatic independently.\nThe number of threads in a block is limited up to 1024. To multiply the amount of threads, many Block are placed together in a Grid. Docs\nThe number of Blocks can be $(2^{31}-1) √ó 2^{16} √ó 2^{16}$\nIntroducetion to GPUs - NYU\nTrilinear Interpolate Each corner is sumed up with a weight which is the product of normalized distance from the point to the opposite side.\nAnalogy to Bilinear interpolation:\nf f ‚ÇÅ ‚ÇÉ = = = = = u = = 1 = - 1 v = v ‚Äñ 1 - u f 1 f ‚ÇÇ ‚ÇÑ $$\\rm f(u,v) = (1-u)(1-v)‚ãÖf‚ÇÅ + u(1-v)‚ãÖf‚ÇÇ + (1-u)v‚ãÖf‚ÇÉ +uv‚ãÖf‚ÇÑ$$\nFor Trilinear interpolation, each weight is the product of 3 normalized distances to the opposite plane.\n$$ \\begin{aligned} \\rm f(u,v,w) =\u0026amp; (1-u)(1-v)(1-w)f‚ÇÅ + u(1-v)(1-w)f‚ÇÇ + (1-u)v(1-w)f‚ÇÉ + uv(1-w)f‚ÇÑ \\\\ \u0026amp;+ (1-u)(1-v)w f‚ÇÖ + u(1-v)w f‚ÇÜ + (1-u)vw f‚Çá + uvwf‚Çà \\\\ \u0026amp; \\\\ =\u0026amp;\\rm (1-u) [ (1-v)(1-w)f‚ÇÅ + v(1-w)f‚ÇÉ +(1-v)wf‚ÇÖ +vw f‚Çá ] \\\\ \u0026amp;\\rm + u [ (1-v)(1-w)f‚ÇÇ + v(1-w)f‚ÇÑ + (1-v)w f‚ÇÜ + vwf‚Çà] \\end{aligned} $$\nf f ‚ÇÅ ‚ÇÉ f f ‚ÇÖ ‚Çá u w V f f ‚ÇÑ ‚ÇÇ f f ‚ÇÜ ‚Çà Input-Output Input: features (N, 8, F) and points coordinates in each cube (N, 3)\nOutput: features at points (N, F).\nOperations can be performed in parallel\nEach point can be computed individually; Each feature can be computed individually. Code Notes:\nIf input variables of CUDA kernel are torch.Tensor, they must be checked whether they\u0026rsquo;re on cuda and contiguous, because threads needs to read/write data without jumping.\nWhile if input variables are not tensor, the checking is not required.\ncpp Cpp: \u0026ldquo;trilinear_interpolate.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;torch/extension.h\u0026gt; #include \u0026#34;utils.h\u0026#34; torch::Tensor trilinear_interpolate( const torch::Tensor features, const torch::Tensor points ){ // Check input tensors for building successfully CHECK_INPUT(features); CHECK_INPUT(points); // Call the cuda kernel return trilinear_fw_cu(features, points); } PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); } header Header: \u0026ldquo;include/utils.h\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;torch/extension.h\u0026gt; // \u0026#34;one-line functions\u0026#34; // Any tensor must reside on cuda device. #define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \u0026#34; must be a CUDA tensor\u0026#34;) // Next element in x corresponds 1 step for R/W head, // thus, a multi-dim tensor is indexed like a flatten tensor. // Workers are contiguous, so tensor must be as well. #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \u0026#34; must be contiguous\u0026#34;) // Combine two conditions: #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x) // Declear the cuda kernel torch::Tensor trilinear_fw_cu( const torch::Tensor feats, const torch::Tensor points ); cu CUDA kernel: \u0026ldquo;interpolation_kernel.cu\u0026rdquo;\nSource video: part-3 Source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 #include \u0026lt;torch/extension.h\u0026gt; // kernel function template \u0026lt;typename scalar_t\u0026gt; // for type of scalar_t __global__ void trilinear_fw_kernel( // no return value // input variables are packed_accessor const torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; feats, const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; points, torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; feat_interp ){ // index thread along x for samples: const int n = blockIdx.x * blockDim.x + threadIdx.x; // index thread along y for features: const int f = blockIdx.y * blockDim.y + threadIdx.y; // Terminate exceeded threads without input data if (n \u0026gt;= feats.size(0) || f \u0026gt;= feats.size(2)) return; // Put results into output variable // normalized coordinates in each cell, [-1,1] -\u0026gt; [0,1] const scalar_t u = (points[n][0]+1)/2; const scalar_t v = (points[n][1]+1)/2; const scalar_t w = (points[n][2]+1)/2; // factors const scalar_t a = (1-v)*(1-w); const scalar_t b = v*(1-w); const scalar_t c = (1-v)*w; const scalar_t d = v*w; // Each thread will perform: feat_interp[n][f] = (1-u) * (a*feats[n][0][f] + b*feats[n][1][f] + c*feats[n][2][f] + d*feats[n][3][f]) + u * (a*feats[n][4][f] + b*feats[n][5][f] + c*feats[n][6][f] + d*feats[n][7][f]); } // foward pass torch::Tensor trilinear_fw_cu( torch::Tensor feats, // (N=20, 8, F=10) torch::Tensor points // (N=20, 3) ){ const int N = points.size(0); const int F = feats.size(2); // Initialize the output data residing on the same devices // as the input data torch::Tensor feat_interp=torch::empty({N,F}, feats.options()); // Allocate threads and blocks // #Threads per block: 256 (Rule of thumb). // Threads can be 3-D (cube) at most, where each dim can be set as proportional as the data\u0026#39;s shape. // Two dimensions will run in parallel: N (20) and F (10) const dim3 threads(16, 16, 1); // total 256. // #Blocks is determined by repeating `threads` to sufficiently cover the output data. const dim3 blocks( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ); // Launch threads to compute for each \u0026#34;voxel\u0026#34; in the \u0026#34;cube\u0026#34; of block AT_DISPATCH_FLOATING_TYPES(feats.type(), \u0026#34;trilinear_fw_cu\u0026#34;, ([\u0026amp;] { // call kernel function with passing input and output trilinear_fw_kernel\u0026lt;scalar_t\u0026gt;\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;( feats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;(), points.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), feat_interp.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;() ); } ) ); return feat_interp; } Since 2 dimensions of the output tensor both require parallism, 256 threads in a block are resized to a square of (16, 16).\nTo ensure each element of the output tensor assigned with a thread, 2 by 1 (2,1) blocks are required.\nSuch that each element will be computed by a thread (\u0026ldquo;box\u0026rdquo;) individually.\n2 - 0 ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚ãÖ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ' = = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = 1 = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = 0 = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = 1 = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = 6 = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° = = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = . ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚ãÖ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ' 1 6 I ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚ãÖ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ' s = o = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = l = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = a T = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = t h = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = e r = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = e = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = 1 U a = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = 6 n d = ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° = ‚ñ° ‚ñ° ‚ä† ‚ä† ‚ä† = u s ‚ä† ‚ä† ‚ä† = s = ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† = ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† = e = ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† = ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† = d = ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† = ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† ‚ä† = . ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚ãÖ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ' 1 6 Notes:\nthreads and blocks are not assigned with tuples:\n1 2 const dim3 threads = (16, 16, 1); // total 256. const dim3 blocks = ( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ) They\u0026rsquo;re object instantiated from classes:\n1 2 const dim3 threads(16, 16, 1); // total 256. const dim3 blocks( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ) If multiple tensors need return, the return type of the func should be std::vector\u0026lt;torch::Tensor\u0026gt;. And the end syntax: return {feat_interp, points};\npytorch.org/cppdocs/ Docs: CUSTOM C++ AND CUDA EXTENSIONS Kernel func Source video: P4\nAT_DISPATCH_FLOATING_TYPES got passed data type and a name for error prompt.\nscalar_t is used to allow various float types of input data to kernel function trilinear_fw_kernel, as AT_DISPATCH_FLOATING_TYPES can recieve float16, float32, float64.\nSpecify sepcific dtype rather than scalar_t and size_t:\n1 2 3 4 5 6 trilinear_fw_kernel\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;( feats.packed_accessor\u0026lt;float, 3, torch::RestrictPtrTraits\u0026gt;(), points.packed_accessor\u0026lt;float, 2, torch::RestrictPtrTraits\u0026gt;(), feat_interp.packed_accessor\u0026lt;float, 2, torch::RestrictPtrTraits\u0026gt;(), var_not_tensor // packed_accessor is only for tensor ) packed_accesor indicates how to index elements by stating \u0026ldquo;datatype\u0026rdquo; (scalar_t) and \u0026ldquo;number of dimensions\u0026rdquo; (3) for each input. And size_t means shape of an index aligned with scalar_t.\ntorch::RestrictPtrTraits: Memory is independent to any other variables.\nKernel trilinear_fw_kernel doesn\u0026rsquo;t return any value (void), with directly changing the memory of output data. Thus, output must be passed.\n__global__ means kernel function is called on cpu and excecuted on cuda devices.\n__host__ for functions called on cpu and run on cpu. __device for functions called and run both on cuda device. Indexing samples by n and indexing features by f.\nIf threads accessed empty area, program returns.\nsetup.py Building CudaExtension: \u0026ldquo;setup.py\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from setuptools import setup from torch.utils.cpp_extension import CUDAExtension, BuildExtension from pathlib import Path ROOT_DIR = Path.cwd() exts = [\u0026#34;.cpp\u0026#34;, \u0026#34;.cu\u0026#34;] sources = [str(p) for p in ROOT_DIR.rglob(\u0026#39;*\u0026#39;) if p.suffix in exts] include_dirs = [ROOT_DIR / \u0026#34;include\u0026#34;] setup( name=\u0026#34;my_cppcuda_pkg\u0026#34;, version=\u0026#34;0.1\u0026#34;, description=\u0026#34;cppcuda example\u0026#34;, long_description=\u0026#34;cpp-cuda extension\u0026#34;, author=\u0026#34;z\u0026#34;, author_email=\u0026#34;luckily1640@gmail.com\u0026#34;, ext_modules=[ CUDAExtension( name=\u0026#39;my_cppcuda_pkg\u0026#39;, sources=sources, # code files include_dirs=include_dirs, extra_compile_args={\u0026#39;cxx\u0026#39;: [\u0026#39;-O2\u0026#39;], \u0026#39;nvcc\u0026#39;: [\u0026#39;-O2\u0026#39;]} ) ], cmdclass={ # commands to be executed \u0026#34;build_ext\u0026#34;:BuildExtension } ) Build and install: pip install . Delete failed building history manually: \u0026ldquo;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/my_cppcuda_pkg-0.1.dist-info\u0026rdquo; test.py Python function: \u0026ldquo;test.py\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 import torch from my_cppcuda_pkg import trilinear_interpolate N = 65536; F = 256 feats = torch.rand(N,3, F, device=\u0026#39;cuda\u0026#39;) points = torch.rand(N,3, device=\u0026#39;cuda\u0026#39;)*2-1 # [0,1] -\u0026gt; [-1,1] out = trilinear_interpolate(feats, points) print(out.shape) title: \u0026ldquo;watch: CppExt - AIËëµ 05 | Validate\u0026rdquo; date: 2023-10-28T12:05:00\nSource video: Pytorch+cpp/cuda extension ÊïôÂ≠∏ tutorial 5 - English CC - Source code To validate if cuda kernel yields correct results, impelement a PyTorch version.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch import my_cppcuda_pkg import time def trilinear_interpolate_py(feats, points): r\u0026#34;\u0026#34;\u0026#34; feats: (N, 8, F), features on 8 vertices points: (N, 3) , coordinates [-1,1] \u0026#34;\u0026#34;\u0026#34; u,v,w = (points[:,0:1]+1)/2, (points[:,1:2]+1)/2, (points[:,2:3]+1)/2 a,b,c,d = (1-v)*(1-w), v*(1-w), (1-v)*w, v*w feat_interp = (1-u) * (a*feats[:,0] + b*feats[:,1] + c*feats[:,2] + d*feats[:,3]) \\ + u*(a*feats[:,4] + b*feats[:,5] + c*feats[:,6] + d*feats[:,7]) return feat_interp # (N,F) if __name__ == \u0026#34;__main__\u0026#34;: N=65536; F=256 feats = torch.rand(N,8,F, device=\u0026#34;cuda\u0026#34;).requires_grad_(True) points = torch.rand(N,F, device=\u0026#34;cuda\u0026#34;)*2-1 t = time.time() out_cuda = my_cppcuda_pkg.trilinear_interpolate(feats, points) torch.cuda.synchronize() print(f\u0026#39;CUDA time: {time.time()-t} s\u0026#39;) t = time.time() out_py = trilinear_interpolate_py(feats, points) torch.cuda.synchronize() print(f\u0026#39;PyTorch time: {time.time()-t} s\u0026#39;) print(f\u0026#34;fw all close? {torch.allclose(out_cuda, out_py)}\u0026#34;) print(f\u0026#34;Cuda has grad? {out_cuda.requires_grad}\u0026#34;) title: \u0026ldquo;watch: CppExt - AIËëµ 06 | Backward\u0026rdquo; date: 2023-10-28T16:40:00\nSource video: Pytorch+cpp/cuda extension ÊïôÂ≠∏ tutorial 6 ÂèçÂêëÂÇ≥Êí≠ - English CC -\nSource code Compute Partial Derivatives When loss L comes, the partial derivatives of L w.r.t. every trainable input variable of the function are required.\nTrilinear interpolation:\n$$ \\begin{aligned} f(u,v,w) = (1-u) * [ \u0026amp; (1-v)(1-w)f‚ÇÅ + v(1-w)f‚ÇÉ + (1-v)wf‚ÇÖ + vw f‚Çá ] \\\\ + u * [ \u0026amp; (1-v)(1-w)f‚ÇÇ + v(1-w)f‚ÇÑ + (1-v)w f‚ÇÜ + vwf‚Çà ] \\end{aligned} $$\nu,v,w are coordinates, which are constant (requires_grad is False). So only vertices features f‚ÇÅ, f‚ÇÉ, f‚ÇÖ, f‚Çá, f‚ÇÇ, f‚ÇÑ, f‚ÇÜ, f‚Çà need optimizing.\nGiven interpolated result f, their gradients for this operation are:\n$$ \\begin{aligned} \u0026amp;\\frac{‚àÇf}{‚àÇf‚ÇÅ} = (1-u)(1-v)(1-w); \u0026amp;\\frac{‚àÇf}{‚àÇf‚ÇÇ} \u0026amp;= u(1-v)(1-w); \\\\ \u0026amp;\\frac{‚àÇf}{‚àÇf‚ÇÉ} = (1-u)v(1-w); \u0026amp;\\frac{‚àÇf}{‚àÇf‚ÇÑ} \u0026amp;= uv(1-w); \\\\ \u0026amp;\\frac{‚àÇf}{‚àÇf‚ÇÖ} = (1-u)(1-v)w; \u0026amp;\\frac{‚àÇf}{‚àÇf‚ÇÜ} \u0026amp;= u(1-v)w; \\\\ \u0026amp;\\frac{‚àÇf}{‚àÇf‚Çá} = (1-u)vw \u0026amp;\\frac{‚àÇf}{‚àÇf‚Çà} \u0026amp;= uvw \\end{aligned} $$\nThe derivatives of L w.r.t. features f‚ÇÅ, f‚ÇÇ, f‚ÇÉ, f‚ÇÑ, f‚ÇÖ, f‚ÇÜ, f‚Çá, f‚Çà are:\n$$ \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚ÇÅ}; \\quad \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚ÇÇ}; \\quad \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚ÇÉ}; \\quad \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚ÇÑ}; \\quad \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚ÇÖ}; \\quad \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚ÇÜ}; \\quad \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚Çá}; \\quad \\frac{‚àÇL}{‚àÇf} \\frac{‚àÇf}{‚àÇf‚Çà} $$\nBw Kernel Write host function trilinear_bw_cu based on trilinear_fw_cu in \u0026ldquo;interpolation_kernel.cu\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 torch::Tensor trilinear_bw_cu( const torch::Tensor dL_dfeat_interp, // Inputs const torch::Tensor feats, const torch::Tensor points ){ const int N = points.size(0); const int F = feats.size(2); torch::Tensor dL_dfeats=torch::empty({N,8,F}, feats.options()); // output data const dim3 threads(16,16); const dim3 blocks((N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y); // Launch kernel function AT_DISPATCH_FLOATING_TYPES(feats.type(), \u0026#34;trilinear_bw_cu\u0026#34;, ([\u0026amp;] { trilinear_bw_kernel\u0026lt;scalar_t\u0026gt;\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;( dL_dfeat_interp.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), feats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;(), points.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), dL_dfeats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;() ); } ) ); return dL_dfeats; } Write kernel function trilinear_bw_kernel based on trilinear_fw_kernel in \u0026ldquo;interpolation_kernel.cu\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 template \u0026lt;typename scalar_t\u0026gt; __global__ void trilinear_bw_kernel( const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; dL_dfeat_interp, const torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; feats, const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; points, torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; dL_dfeats ){ const int n = blockIdx.x * blockDim.x + threadIdx.x; const int f = blockIdx.y * blockDim.y + threadIdx.y; if (n \u0026gt;= points.size(0) || f\u0026gt;= feats.size(2)) return; // Define helper variables const scalar_t u = (points[n][0]+1)/2; const scalar_t v = (points[n][1]+1)/2; const scalar_t w = (points[n][2]+1)/2; const scalar_t a = (1-v)*(1-w); const scalar_t b = v*(1-w); const scalar_t c = (1-v)*w; const scalar_t d = v*w; // Compute derivatives dL_dfeats[n][0][f] = dL_dfeat_interp[n][f]*(1-u)*a; dL_dfeats[n][1][f] = dL_dfeat_interp[n][f]*(1-u)*b; dL_dfeats[n][2][f] = dL_dfeat_interp[n][f]*(1-u)*c; dL_dfeats[n][3][f] = dL_dfeat_interp[n][f]*(1-u)*d; dL_dfeats[n][4][f] = dL_dfeat_interp[n][f]*u*a; dL_dfeats[n][5][f] = dL_dfeat_interp[n][f]*u*b; dL_dfeats[n][6][f] = dL_dfeat_interp[n][f]*u*c; dL_dfeats[n][7][f] = dL_dfeat_interp[n][f]*u*d; } Add the function signature into header file \u0026ldquo;include/utils.h\u0026rdquo;\n1 2 3 4 5 torch::Tensor trilinear_bw_cu( const torch::Tensor dL_dfeat_interp, const torch::Tensor feats, const torch::Tensor points ); Add a cpp function to call the backward method trilinear_bw_cu in \u0026ldquo;interpolation.cpp\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 torch::Tensor trilinear_interpolate_bw( const torch::Tensor dL_dfeat_interp, const torch::Tensor feats, const torch::Tensor points ){ CHECK_INPUT(dL_dfeat_interp); CHECK_INPUT(feats); CHECK_INPUT(points); return trilinear_bw_cu(dL_dfeat_interp, feats, points); } Give the function trilinear_interpolate_bw a name in PYBIND as a method of the package:\n1 2 3 4 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); m.def(\u0026#34;trilinear_interpolate_bw\u0026#34;, \u0026amp;trilinear_interpolate_bw); } Encapsulate Wrap forward and backward by a subclass inherited from torch.autograd.Function in \u0026ldquo;test.py\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class trilinear_interpolate_cuda(torch.autograd.Function): @staticmethod def forward(ctx, feats, points): feat_interp = my_cppcuda_pkg.trilinear_interpolate(feats, points) ctx.save_for_backward(feats, points) return feat_interp @staticmethod def backward(ctx, dL_dfeat_interp): # The number of input vars corresponds to return values of forward pass. # i.e., inputs are gradients of Loss w.r.t the forward\u0026#39;s outcomes. feats, points = ctx.saved_tensors dL_dfeats = my_cppcuda_pkg.trilinear_interpolate_bw( dL_dfeat_interp.contiguous(), feats, points) return dL_dfeats, None # return gradients of Loss w.r.t each input data forward Notes:\nThe nubmer of return values needs to match the input to forward pass. If some input doesn\u0026rsquo;t require grad, return a None.\nctx is mandatory for storing intermeidate data.\nVerify Graident Test the gradient of backward:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # test.py import torch import my_cppcuda_pkg import time def trilinear_interpolate_py(feats, points): r\u0026#34;\u0026#34;\u0026#34; feats: (N, 8, F), features on 8 vertices points: (N, 3) , coordinates [-1,1] \u0026#34;\u0026#34;\u0026#34; u,v,w = (points[:,0:1]+1)/2, (points[:,1:2]+1)/2, (points[:,2:3]+1)/2 a,b,c,d = (1-v)*(1-w), v*(1-w), (1-v)*w, v*w feat_interp = (1-u) * (a*feats[:,0] + b*feats[:,1] + c*feats[:,2] + d*feats[:,3]) \\ + u*(a*feats[:,4] + b*feats[:,5] + c*feats[:,6] + d*feats[:,7]) return feat_interp # (N,F) if __name__==\u0026#34;__main__\u0026#34;: N = 1024; F=256 feats = torch.rand(N,8,F, device=\u0026#34;cuda\u0026#34;) feats_py = feats.clone().requires_grad_() feats_cu = feats.clone().requires_grad_() points = torch.rand(N,3, device=\u0026#34;cuda\u0026#34;)*2-1 t = time.time() out_py = trilinear_interpolate_py(feats_py, points) torch.cuda.synchronize() print(f\u0026#34;py: {time.time() - t}\u0026#34;) t = time.time() out_cuda = trilinear_interpolate_cuda.apply(feats_cu, points) torch.cuda.synchronize() print(f\u0026#34;cu: {time.time() - t}\u0026#34;) loss_py = out_py.sum() loss_cuda = out_cuda.sum() loss_py.backward() loss_cuda.backward() print(f\u0026#34;Grad all close? {torch.allclose(feats_py.grad, feats_cu.grad)}\u0026#34;) ","date":"2023-10-11T16:23:00Z","image":"https://img.youtube.com/vi/l_Rpk6CRJYI/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/lang/cuda/tut_ai%E8%91%B5/","title":"watch: CppExt - AIËëµ | CUDA Extension for PyTorch"},{"content":"Yannic Source video: Retentive Network: A Successor to Transformer for Large Language Models (Paper Explained)\nRemove softmax outside the attention scores, then no all the results have to be hold and wait for softmax.\nT p r a a r n a i l n l L T f g i i r o s n a r m e n m a s e r - r T R r I L e a n o t n f w N s e e f r C t o e o r n s m c t e e r R N e e c t p u w S e r o t r r r r f e k o o n n r t g m a n c e RetNet is a kind of linear transforemr, like RWKV.\nRecurrent network each time train only 1 token because once the next work has been predicted, the backpropagation has to be done to optimize previous hidden states.\nw h s o i t r d a d d t s e e : n s b p a r c o k p Recurrent network cannot be trained parallelly because the non-linearity activation function\nG(c( G(b( G(ax+Œ≥)+Œ≥ )+Œ≥) +Œ≥) )\nHidden state is a shared buffer. The hidden state contains all the previous information, so the memory cost is consistent during training.\n$$ \\begin{aligned} ax+\\gamma = \\gamma \\\\ by + \\gamma = \\gamma \\\\ cz + \\gamma = \\gamma \\\\ \\end{aligned} $$\nTransformer can\u0026rsquo;t be recurrent because the existence of softmax, which requires all the attention scores (\u0026ldquo;hidden states\u0026rdquo;) not to be abandoned.\nRetNet achieved training parallism through matrix multiplication, like a Linear layer.\nTime-scaling mask replaces causal mask (blocking the subsequent words when doing attention in parallel)\nRetNet by chunks is a trade-off between recurrent and parallel.\nÁßãÂàÄÈ±º Source video: „ÄêËÆ∫ÊñáÈÄüËßà„Äë RetNet: A Successor to Transformer for Large Language Models2307.08621\nEquentions explaination and code walkthrough.\nA global state is maintained like recurrent network. With that, expand the equation of attention: Q K V Apply singular decomposition \u0026hellip;. ","date":"2023-10-10T23:41:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/d-vid-retnet/","title":"watch: RetNet"},{"content":" Source video: Á¨¨‰∏ÄÊ¨°Âú®ÁæéÂúãÁõ¥Êí≠ÔºàË¨õËß£Gaussian SplattingÁöÑcuda codeÔºâ- AIËëµ\nkwea123/gaussian_splatting_notes\nSelected comments:\n\u0026ldquo;3D Gaussian Mixture Model.\u0026rdquo; GMM?!\n\u0026ldquo;EWA Splatting paper (2001) contains all the necessary derivations and math.\u0026rdquo; - Matias Turkulainen (gsplat contributor).\nforward.cu Trade-off:\nThe 2D-projections of 3D ellipsoids are circles rather than ellipses to reduce shading tiles.\nDetermine visibility of tiles instead of pixels (shaded by circle shadows) for fast rasterization.\nMain steps:\nDetermine the radii of circles shadows (preprocessCUDA)\nProject a 3D ellipsoid will yield a 2x2 covariance matrix\nSolve the 2 Eigenvalues for the covariance matrix, and the bigger one is the length of the major axis.\nUse the major axis as the radius of the circle.\nDetermine pixels covered by the projected circles.\nIf the distance betwen a pixel to the circle center is smaller than the circle radius, the pixel is visible to the circle (a disc corresponding to a 3D Gaussian in the ray space).\nGetRect Sort 3D Gaussians (discs) by depths\nEach tile is shaded by multiple ellipsoids, i.e., visible to multiple Gaussians.\nPair each pixel with each contributing ellipsoid, and form a 64-bit identifier for each pair.\ni.e., stitching the index (32-bit) of a pixel and the depth (32-bit) of a ellipsoid.\nSort all the identifier, and obtain a sequence\nFor example:\n0 2 c a b 1 3 tile-0 has 2 pairs: 0-a, 0-c tile-1 has 3 pairs: 1-a, 1-b, 1-c Suppose the depths of 3 Gaussians are b \u0026gt; a \u0026gt; c, the sequence of pairs is as follows:\n1 2 3 4 5 0-c 0-a 1-c 1-a 1-b Alpha compositing for each pixel\u0026rsquo;s color (renderCUDA)\nImplementation tricks:\nA tile is a Block, in which each pixel is a worker. Every pixel in a tile uses the same Gaussian distributions, so those data are stored in __share__ memory. Docs Calculate alpha, which is proportional to the probability in a Gaussian distribution.\nThe probability is calculated according to the expression of 2D Gaussian.\nBlend alpha * color (SH) of each ellipsoid front-to-back.\nbackward.cu output input pixels\u0026rsquo; color (p,3) Gaussians\u0026rsquo; color (g,3) Gaussians\u0026rsquo; alpha Gaussians\u0026rsquo; position (g,3) Gaussians\u0026rsquo; rotation (g,4) Gaussians\u0026rsquo; length of axis (g,3) p is number of pixels; g is number of 3D Gaussians Loss = color + SSIM\nA full image is produced at once, so image metrics, like SSIM, can be added. Write parital derivative for each input tensor.\n","date":"2023-10-10T16:43:00Z","image":"https://img.youtube.com/vi/1buFrKUaqwM/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/model/splat/d-vid-3dgs-explain-%E8%91%B5/","title":"watch: 3DGS | AIËëµ Cuda Code Walkthrough"},{"content":"Metapost ChatGPT - code interpreter Data visualization\nProcesson Networks Architecture\n‰Ω†ÁöÑÁßëÁ†îËÉΩÂäõ‰ªé‰ªÄ‰πàÊó∂ÂÄôÂºÄÂßãÁ™ÅÈ£ûÁåõËøõÁöÑÔºü - Âπ≥Âá°ÁöÑÂõûÁ≠î - Áü•‰πé\n","date":"2023-10-07T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/latex_draw/","title":"memo: Plotting for Academics"},{"content":"Nvidia apex An example project using it is AIM.\ntorch amp An example: Automatic Mixed Precision recipe\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 use_amp = True net = make_model(in_size, out_size, num_layers) opt = torch.optim.SGD(net.parameters(), lr=0.001) scaler = torch.cuda.amp.GradScaler(enabled=use_amp) for epoch in range(epochs): for input, target in zip(data, targets): with torch.autocast(device_type=\u0026#39;cuda\u0026#39;, dtype=torch.float16, enabled=use_amp): output = net(input) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(opt) scaler.update() opt.zero_grad() # set_to_none=True here can modestly improve performance „Äêpytorch distributed„Äëamp ÂéüÁêÜÔºåautomatic mixed precision Ëá™Âä®Ê∑∑ÂêàÁ≤æÂ∫¶\n","date":"2023-09-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_amp/","title":"memo: PyTorch | Automatic Mixed Precision"},{"content":"Source video: CLIP ËÆ∫ÊñáÈÄêÊÆµÁ≤æËØª„ÄêËÆ∫ÊñáÁ≤æËØª„Äë- Ë∑üÊùéÊ≤êÂ≠¶AI ~ Bilibili 2022-02-10\nCLIP (Contrastive Language-Image Pre-Training) Code\nFeatures Large-scale dataset: 4e8 pairs of image and caption.\nSelf-supervised learning strategy (pretext task): Given an image, find the matched text vector from candidates\nContrastive learning needs positive and negative samples.\nThere is only one correct text vector for an image, while the remaining text vectors are served as negative samples.\nLoosen the target: pairing rather than predicting next word\nGood transferability: Able to generalize to unseen classes based on the text prompts.\nLeverage text to enhance image features with semantic understanding \\begin{algorithm} \\caption{CLIP} \\begin{algorithmic} \\STATE If = ImageEncoder(I) $\\quad$ \\COMMENT{(n,h,w,c)‚Üí(n, di)} \\STATE Tf = TextEncoder(T) $\\quad$ \\COMMENT{(n,l)‚Üí(n,dt)} \\STATE Ie = Linear projection (If) $\\quad$ \\COMMENT{(n,de)} \\STATE Te = Linear projection (Tf) $\\quad$ \\COMMENT{(n,de)} \\STATE logits = Inner Product (Ie, Te.T) \\STATE labels = np.arange(n) \\STATE loss·µ¢ = CrossEntropy(logits, labels, axis=0) \\STATE loss‚Çú = CrossEntropy(logits, labels, axis=1) \\STATE loss = (loss·µ¢ + loss‚Çú)/2 \\end{algorithmic} \\end{algorithm} Experiments Backbone model: The image encoder can be ResNet or ViT, text encoder is a transformer\nZero-shot transfer: No downstream task adaptation, apply the pre-trained model directly onto the unseen data.\nFew-shot transfer: Given a few images, fine-tune or linearly probe the pre-trained model. CLIP outperforms all the previous pre-trained models supervised by labels.\nFull-data transfer: Better than other zero-shot model.\nThe features extracted by previous pre-trained models only have the image modality, while the image features of CLIP are learned under the instructions of text description, so the image features have fused with text modality and guided to semantic understanding.\nMix precision training can save half of memory without losing performance.\nPrompt engineering: Fit the label into a sentence by putting it into prompt templates to close gap with the training set, i.e., image-caption pairs.\nThey made 80 templates for describing different situations in images, such that more specific context is confined to help find the solution in a small possible range.\nUnrealistic and abstract datasets, like MNIST, counting number of objects, are difficult for CLIP because they are hard to describe with language. Otherwise, as long as the describable object exists in the image, CLIP can recognize it.\nLimitations CLIP is not the SOTA on ImageNet, but only in the zero-shot task.\nCannot understanding abstract concepts: \u0026ldquo;abnormal\u0026rdquo;, \u0026ldquo;safe\u0026rdquo;\nOut-of-distribution when performing zero-shot inference will ruin the generaliability of CLIP: MNIST (different from natural images) isn\u0026rsquo;t included in the training set.\nZero-shot inference of CLIP requires the \u0026ldquo;new label\u0026rdquo; is provided in the candidates to do a multiple choice question.\nBy contrast, let model generate caption from image will make the data loop. But that is infesible because massive computation with low-efficient training techinics.\nData utilization is inefficient with too many training images. Dataloader spitting image one-by-one needs long time.\nDatasets bias: Hyperparameter tunning is based on ImageNet; The testing performance is based on chosen 27 datasets.\nTraining set is from internet without filtering, so the model may learned malicious information.\nPerformance of few-shot learning sometimes is inferior to zero-shot scenario weirdly.\nFooter:\nThe pre-trained method isn\u0026rsquo;t open-source. But the model is open source. Code Repo\nInstall CLIP:\n1 2 3 conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0 pip install ftfy regex tqdm pip install git+https://github.com/openai/CLIP.git Zero-shot classification:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch import clip from PIL import Image device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) image = preprocess(Image.open(\u0026#34;CLIP.png\u0026#34;)).unsqueeze(0).to(device) text = clip.tokenize([\u0026#34;a diagram\u0026#34;, \u0026#34;a dog\u0026#34;, \u0026#34;a cat\u0026#34;]).to(device) with torch.no_grad(): image_features = model.encode_image(image) text_features = model.encode_text(text) logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\u0026#34;Label probs:\u0026#34;, probs) # prints: [[0.9927937 0.00421068 0.00299572]] ","date":"2023-08-29T12:12:00Z","permalink":"https://zichen34.github.io/writenotes/model/transfer/d-vid-clip_paper/","title":"watch: CLIP Paper Walkthrough"},{"content":"Code | Arxiv | ProjPage | OpenReview | Ytb\nNote Abs Transfer between different modalities: image classification and video understanding\nAdd 3 new layers inside each transformer block, not at the very end of the model.\n3 new layers:\nSpatial adapter is after self-attention; Temporal adapter is after two self-attention, Joint adapter is a bypass branch of the MLP layers. Frozen pre-trained parameters and optimize only the new layers to transfer the pre-trained model onto another task.\nIntro Two directions: adding temporal module onto or inflating an image transformer model both have drawbacks: heavy-computation full fine-tunning is required. Related work Pre-trained image models have good transferability. Fully fine-tuning a transformer-based image model is uneconomical. Parameter-efficient finetuning was applied on LLM for downstream tasks. Method ViT consists of 12 encoder blocks (MSA and MLP).\nAn image is split into N pathes, which will be projected to D channels;\nThe input to MSA is each patch attached class token channel and added positional encoding.\nSpace-only model (baseline, no temporal modelong): Apply pre-trained frozen ViT onto video by processing each frame independently.\nEach frame will be represented by the final class token.\nThe token of each frames are averaged to form a vector for predicting\nSpatial adaptation is adding an adapter after the self-attention (pre-trained MSA) fuses N+1 patches.\nAn adapter is a bottleneck, i.e, Reduce-Act-Expand with skip connection.\nThis can achieve comparable performance compared with space-only baseline, because image model learns spatial feature well.\nTemporal modeling reused the self-attention parameters again, whereas the T frames got fused by reshapeing the tensor.\nAnother adapter is appended for adapting the generated temporal features.\nTemporal modeling is performed ahead of spatial modeling, so the adapter is removed skip connection and initialized as zero to avoid disrupting the perfomance of the original model.\nBy reusing the MSA, the number of parameters is maintained.\nJoint adapation jointly fits the temporal features and spatial features.\nThis adapter also doesn\u0026rsquo;t has skip connection.\nAverage the final class token of each frame and pass it to classification head.\nExperiments Task: classification video?\n8 frames Memory: AIM based on Swin-B pre-trained with IN-21K occupies 9GB. Underperform on temporal-heavy video because the temporal modeling is simply reusing the spatial modeling parameters. Discussion Deeper layer needs adaptation for task-specific features, while shallow layer may not. Conclusion Transfer models trained with other sequence data, like text and audio for video action recognition. flowchart TD input(\"Image (224,224,3)\") --\u003e cls(\"Class token (1,768)\") \u0026 pe(\"Position Embedding (197,768)\") input --\u003e feat(\"Conv2d (16x16,s16) (14,14)\") cls \u0026 feat --\u003e Cat pe \u0026 Cat --\u003e add1(\"Add\") add1 --\u003e msa1(\"MSA\") --\u003e Tadap --\u003e msa2(\"MSA\") --\u003e Sadap Sadap --\u003e ineck(\"Inverse bootleneck\") Sadap --\u003e Jadap add1 \u0026 ineck \u0026 Jadap --\u003e add2(\"Add\") --\u003e x Play Debug code with experiment settings in \u0026ldquo;run_exp.sh\u0026rdquo;\nEnvironment 1 2 3 4 5 6 7 8 9 10 11 12 13 conda env create -f ./environment.yml conda activate AIM # install CLIP pip install git+https://github.com/openai/CLIP.git # install mmaction2 python setup.py develop # install apex git clone https://github.com/NVIDIA/apex cd apex pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\u0026#34;--cpp_ext\u0026#34; --global-option=\u0026#34;--cuda_ext\u0026#34; ./ Dataset diving48 To prepare the dataset diving48 , I downloaded the repo MMAction2 Documentaions\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 conda create --name openmmlab python=3.8 -y conda activate openmmlab conda install pytorch torchvision -c pytorch # Step 1: pip install -U openmim mim install mmengine mim install mmcv mim install mmdet mim install mmpose # Step 2: git clone https://github.com/open-mmlab/mmaction2.git cd mmaction2 pip install -v -e . Following \u0026ldquo;Download from Official Source\u0026rdquo; section.\nDownload annotations using their shell script. bash download_annotations.sh Download videos \u0026ldquo;Diving48_rgb.tar.gz\u0026rdquo; (9.6G) Only extract the rgb frames: bash extract_rgb_frames_opencv.sh Generate file list using program: bash generate_videos_filelist.sh Make a symbolic link to \u0026ldquo;mmaction2/data\u0026rdquo; in \u0026ldquo;adapt-image-models\u0026rdquo;: ln -s /home/zichen/Downloads/mmaction2/data/ ./\nFormat\nannotation file \u0026ldquo;data/diving48/diving48_train_list_videos.txt\u0026rdquo; includes: filename and class label of each video Config for 1080Ti Train with 1 video cannot make the acc increase\nDefault configs (8 videos, 32 frames) will cause 1 1080Ti OOM. (\u0026ldquo;configs/recognition/vit/vitclip_large_diving48.py\u0026rdquo;)\nOverride the number of videos in config file with args:\n1 2 3 \u0026#34;args\u0026#34;:[ \u0026#34;--cfg-options\u0026#34;, \u0026#34;data.videos_per_gpu=1\u0026#34; ] But the top1_acc didn\u0026rsquo;t grow:\n1 2 3 4 5 2023-08-31 12:22:11,768 - mmaction - INFO - Epoch [1][4180/15027] lr: 6.003e-05, eta: 6 days, 8:30:03, time: 0.709, data_time: 0.001, memory: 5659, top1_acc: 0.0500, top5_acc: 0.3500, loss_cls: 3.4383, loss: 3.4383 \u0026quot;data.videos_per_gpu=2\u0026quot; will OOM.\nReduce num_frames\n.vscode/launch.json is made based on \u0026ldquo;run_exp.sh\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34;, \u0026#34;data.videos_per_gpu=8\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, // The follwings cannot change // \u0026#34;train_pipeline[1].clip_len=3\u0026#34;, // \u0026#34;val_pipeline[1].clip_len=3\u0026#34; \u0026#34;--train_clip_len\u0026#34;, \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; ] (2023-09-06) The cfg.data.train['pipeline']['clip_len'] didn\u0026rsquo;t changed, which still equals 32. Consequently, the images x passed to forward(self, x) of model ViT_CLIP has the shape (256, 197, 768)\nHowever, the instance variable self.num_frames of the backbone model ViT_CLIP was changed to 3.\nThen, the einops.rearrange cannot parse the dimensionality in: x = rearrange(x, '(b t) n d -\u0026gt; (b n) t d', t=self.num_frames)\n1 2 einops.EinopsError: Shape mismatch, can\u0026#39;t divide axis of length 256 in chunks of 3 Dataset is built based the key cfg.data.train, thus, its values are also required to update:\n1 2 3 cfg.merge_from_dict(dict(train_pipeline=args.train_clip_len, val_pipeline=args.train_clip_len)) update_option = {\u0026#39;data\u0026#39;: {\u0026#39;train\u0026#39;: {\u0026#39;pipeline\u0026#39;: args.train_clip_len}, \u0026#39;val\u0026#39;: {\u0026#39;pipeline\u0026#39;: args.train_clip_len}}} cfg.merge_from_dict(update_option) Start training:\n1 2 3 4 5 6 7 8 9 10 11 12 13 export CUDA_VISIBLE_DEVICES=4 python -m torch.distributed.launch \\ --nproc_per_node=1 --master_port=29500 \\ tools/train.py \\ \u0026#34;configs/recognition/vit/vitclip_base_diving48.py\u0026#34; \\ --launcher=\u0026#34;pytorch\u0026#34; \\ --test-last \\ --validate \\ --cfg-options model.backbone.pretrained=\u0026#34;openaiclip\u0026#34; \\ work_dir=\u0026#34;work_dirs_vit/diving48/debug\u0026#34; \\ data.videos_per_gpu=8 \\ model.backbone.num_frames=3 \\ --train_clip_len \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; Optimization Souce code\nAdamW: lr=3e-4, weight_decay=0.05, LR scheduler: CosineAnnealing Pseudocode With backbone: ViT_CLIP\n\\begin{algorithm} \\caption{main()} \\begin{algorithmic} \\PROCEDURE{Config}{cfg, args} \\STATE args = parse\\_args() \\PROCEDURE{Config.fromfile}{args.config} \\STATE model settings \\STATE dataset settings: ann\\_file, train\\_pipeline,... \\STATE optimizer settings \\STATE learning policy \\STATE runtime settings \\ENDPROCEDURE \\ENDPROCEDURE \\STATE $\\newline$ \\PROCEDURE{build-model}{cfg.model} \\COMMENT{Construct ViT with Adapters added} \\PROCEDURE{build-localizer}{cfg} \\PROCEDURE{LOCALIZERS.build}{cfg} \\PROCEDURE{BaseRecognizer}{} \\STATE $\\newline$ \\PROCEDURE {builder.build-backbone}{backbone} \\STATE BACKBONES.build(cfg) \\ENDPROCEDURE \\STATE $\\newline$ \\PROCEDURE {init-weights}{} \\STATE self.backbone.init\\_weights() \\COMMENT{Load pretrained state\\_dict} \\ENDPROCEDURE \\STATE $\\newline$ \\ENDPROCEDURE \\ENDPROCEDURE \\ENDPROCEDURE \\ENDPROCEDURE \\STATE $\\newline$ \\STATE datasets = [build\\_dataset(cfg.data.train)] \\STATE $\\qquad$ build\\_from\\_cfg(cfg, DATASETS) \\STATE $\\qquad$ 11 transforms operations \\STATE Freeze params.requires\\_grad=False \\STATE $\\newline$ \\PROCEDURE{train-model}{model,datasets,cfg,...} \\STATE dataloader\\_settings \\STATE data\\_loaders = build\\_dataloader(dataset, dataloader\\_setting) \\STATE optimizer = build\\_optimizer(model, cfg.optimizer) \\STATE amp settings \\STATE fp16 settings \\STATE register DistOptimizerHook \\STATE build validation dataset and dataloader \\STATE $\\newline$ \\PROCEDURE{runner.run}{data\\_loaders, cfg.workflow, cfg.total\\_epochs,**runner\\_kwargs} \\STATE DistOptimizerHook.before\\_run(self, runner): \\STATE $\\qquad$ runner.optimizer.zero\\_grad() \\STATE BaseRecognizer.train\\_step(self, data\\_batch,) \\STATE losses = self(imgs, label) \\PROCEDURE {Recognizer3D.forward-train}{img, label} \\STATE x = BaseRecognizer.extract\\_feat(imgs) \\STATE $\\qquad$ self.backbone(imgs) \\COMMENT{ViT\\_CLIP.forward()} \\ENDPROCEDURE \\STATE $\\qquad$ self.forward\\_test(img, label) \\ENDPROCEDURE \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} Debug VideoSwin The pretrained weights of ViT_CLIP are obtained from an initialized clip model:\n1 2 3 4 5 clip_model, preprocess = clip.load(\u0026#34;ViT-B/16\u0026#34;, device=\u0026#34;cpu\u0026#34;) pretrain_dict = clip_model.visual.state_dict() # param del clip_model del pretrain_dict[\u0026#39;proj\u0026#39;] msg = self.load_state_dict(pretrain_dict, strict=False) Source code\nHowever, the weights of Swin Transformer needs to be loaded from file. Source code\nReminded by this issue MMCV load pretrained swin transformer\nPretrained Swin Transformer (Swin-B 224x224, \u0026ldquo;swin-base_3rdparty_in21k.pth\u0026rdquo;) of open-mmlab (mmpretrain) doesn\u0026rsquo;t have the key: \u0026lsquo;model\u0026rsquo;, so it mismatches the code.\n1 2 3 def inflate_weights(self, logger): checkpoint = torch.load(self.pretrained, map_location=\u0026#39;cpu\u0026#39;) state_dict = checkpoint[\u0026#39;model\u0026#39;] While the pretrained swin from microsoft can be successfully loaded. The Swin Transformer has not been trained with CLIP, only on ImageNet21K.\nThe author adds adapters to \u0026ldquo;Swin-B_IN-21K\u0026rdquo; SwinTransformer2D (\u0026ldquo;swin2d.py\u0026rdquo;) in \u0026ldquo;mmaction/models/backbones/ swin2d_adapter.py\u0026rdquo; as clarified in issue18.\nThe \u0026ldquo;swin2d_adapter\u0026rdquo; is compared with SwinTransformer3D (VideoSwin, \u0026ldquo;swin_transformer.py\u0026rdquo;) in Table 6. And most of their experiments are based on ViT_CLIP and compared with TimeSformer.\nSwinTransformer2D is adapted by settings: \u0026ldquo;configs/recognition/swin/ swin2d_adapter_patch244_window7_kinetics400_1k.py\u0026rdquo;.\nWhereas, the config file: \u0026ldquo;configs/recognition/swin/ swin_base_patch244_window877_kinetics400_1k.py\u0026rdquo; is for the original VideoSwin SwinTransformer3D.\nArguments pretrained: str and pretrained2d: bool of class SwinTransformer3D originate in VideoSwin, which adapted pretrained 2D swin transfromer to 3D.\nAIM codes are based on VideoSwin.\nFollowing VideoSwin, pretrained is supposed to be a path to the pretrained model, which should be downloaded in advance. An example is KeyError: \u0026lsquo;patch_embed.proj.weight\u0026rsquo; #22\nBased on the above, the args in launch.json should be set as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Swin-B settings \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;1\u0026#34;, // GPUs \u0026#34;--master_port\u0026#34;, \u0026#34;29600\u0026#34;, \u0026#34;tools/train.py\u0026#34;, \u0026#34;configs/recognition/swin/swin2d_adapter_patch244_window7_kinetics400_1k.py\u0026#34;, \u0026#34;--launcher\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;--test-last\u0026#34;, \u0026#34;--validate\u0026#34;, \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=work_dirs_swin/swin_base_patch4_window7_224_22k.pth\u0026#34;, \u0026#34;work_dir=work_dirs_swin/K400/debug\u0026#34;, \u0026#34;data.videos_per_gpu=8\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, \u0026#34;--train_clip_len\u0026#34;, \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; ] Dataset SSv2 AIM-Swin only has configuration file for K400 and ssv2 datasets. K400 has 240K training videos, which are massive. So I choose the smaller one, SSv2, which has 169K training videos.\nRefer to the guide of SSv2 - mmaction2\nAnnotations: Once signed in your Qualcomm account, download \u0026ldquo;Labels\u0026rdquo; into \u0026ldquo;data/sthv2/annotations/\u0026rdquo; from homepage (Need to acknowledge the agreement before jumping to the download page)\n1 2 3 4 5 6 7 unzip 20bn-something-something-download-package-labels.zip # Rename to match the python code \u0026#34;parse_file_list.py\u0026#34; mv data/sthv2/annotations/labels/train.json data/sthv2/annotations/something-something-v2-train.json mv data/sthv2/annotations/labels/validation.json data/sthv2/annotations/something-something-v2-validation.json mv data/sthv2/annotations/labels/test.json data/sthv2/annotations/something-something-v2-test.json mv data/sthv2/annotations/labels/labels.json data/sthv2/annotations/something-something-v2-labels.json Videos: Download 20 files into \u0026ldquo;mmaction2/data/sthv2/\u0026rdquo;.\nBy executing the following 2 commands, 220847 webm videos (19G) are extracted into the folder: \u0026ldquo;sthv2/20bn-something-something-v2\u0026rdquo;\n1 2 3 4 5 unzip 20bn-something-something-v2-\\??.zip cat 20bn-something-something-v2-?? | tar zx # Rename to match the script below and configs in AIM mv 20bn-something-something-v2/ videos/ Split: Generate list\n1 2 cd mmaction2/tools/data/sthv2/ bash generate_videos_filelist.sh Two .txt files \u0026ldquo;sthv2_train_list_videos.txt\u0026rdquo; and \u0026ldquo;sthv2_val_list_videos.txt\u0026rdquo; are created under \u0026ldquo;data/sthv2/\u0026rdquo;.\nTo debug AIM-swin with SSv2, specify the config file as \u0026ldquo;configs/recognition/swin/swin2d_adapter_patch244_window7_sthv2_1k.py\u0026rdquo; in \u0026ldquo;launch.json\u0026rdquo;.\n2023-09-12 15:41:48,166 - mmaction - INFO - Epoch [1][28160/84457]\tlr: 6.601e-05, eta: 21 days, 10:31:39, time: 0.365, data_time: 0.001, memory: 1420, loss_cls: 4.3897, loss: 4.3897\nForward swin \\begin{algorithm} \\caption{SwinTransformer2d\\_Adapter} \\begin{algorithmic} \\PROCEDURE{forward}{x: (B,T,D,H,W)} \\STATE Conv3d extracts feat maps: (B, C, num\\_Ttokens, H', W') \\STATE $\\newline$ \\PROCEDURE{SwinTransformer2d-Adapter}{B*num\\_Ttokens, H*W, C} \\STATE 2 SwinTransformerBlock \\STATE $\\quad$ rearrange \\STATE $\\quad$ LN1 \\STATE $\\quad$ Temporal MSA mix \"num\\_Ttokens\" of feat maps \\COMMENT{even blks} \\STATE $\\quad$ Temporal Adapter \\STATE $\\quad$ rearrange back \\STATE $\\newline$ \\STATE $\\quad$ LN1 \\STATE $\\quad$ Shift window rows and cols \\STATE $\\quad$ window\\_partition \\COMMENT{reshape} \\STATE $\\quad$ WindowAttention mix \"pixels\" in each window \\STATE $\\quad$ Spatial Adapter \\STATE $\\quad$ window\\_reverse \\STATE $\\quad$ Shift window rows and cols \\STATE $\\newline$ \\STATE $\\quad$ Squash feat maps to 1D \\STATE $\\quad$ Skip connect with the features before S\\_adap \\STATE $\\quad$ LN2 \\STATE $\\quad$ MLP + Joint Adapter \\STATE PatchMerging: (B*num\\_Ttokens, H'/2*W'/2, 2*C) \\STATE $\\newline$ \\STATE 2 SwinTransformerBlock \\STATE PatchMerging: (B*num\\_Ttokens, H'/4*W'/4, 4*C) \\STATE $\\newline$ \\STATE 18 SwinTransformerBlock \\STATE PatchMerging: (B*num\\_Ttokens, H'/8*W'/8, 8*C) \\STATE $\\newline$ \\STATE 2 SwinTransformerBlock \\ENDPROCEDURE \\STATE $\\newline$ \\STATE LN \\STATE rearrange to (B,C,T,H,W) \\STATE cls\\_head, i.e. I3DHead (A linear layer) \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} The reason of setting window_size to 7 may be that the resolution of feature maps is (56,56), which can shrink gradually to (7,7).\nAdapter: Pass the attended features to a bottleenck (2-layer MLP) for adapting them.\nAdapted Swin Differences of the adapted Swin (\u0026ldquo;swin2d_adapter.py\u0026rdquo;) from the baseline model SwinTransformer2D (\u0026ldquo;swin_transformer.py\u0026rdquo;):\n1 2 diff mmaction/models/backbones/swin2d_adapter.py \\ mmaction/models/backbones/swin2d.py swin2d has a temporal adapter more than swin_transformer\nswin2d_adapter has\nNo joint adapter\n","date":"2023-08-23T00:00:00Z","image":"https://adapt-image-models.github.io/method.JPG","permalink":"https://zichen34.github.io/writenotes/model/transfer/b-note-aim-video/","title":"read: Transfer - Adapter | AIM for Video"},{"content":"Code-pytroch | Arxiv | OpenReview\nQ\u0026amp;A Condition image vs target image? Abstract A img2img diffusion model is conditioned with pose and a single source view to generate multiviews.\nStochastic conditioning: Randomly select a view from avaliable views as condition image at each denoising step during sampling?, rather than using only the given view.\nReconstruct a NeRF to measure 3D consistency of multi-views.\nNeRF is not their ultimate objective. Intro Regressive methods for NVS from sparse views based on NeRF are still not generalizable enough or able to produce high-quality completion for the occluded parts.\nRegularized NeRF (RegNeRF) suffer from artifacts when only few views are given because they and didn\u0026rsquo;t apply the features of commen prior of multiple scenes.\nRegressing a NeRF from image feataures (pixel-NeRF) tend to get blurred images.\nGeometry-free methods for NVS obtain colors that aren\u0026rsquo;t directly derived from volume rendering.\nLight field network Scene Representation Transformer EG3D combines StyleGAN and volume rendering 3D diffusion model is a generative and geometry-free method.\nUse pairs of images of the same scene to train a diffusion model. During training, one of them serves as the original, and the other is the condition image. The trained model can produce a multi-view set of a scene given one condition image. Model They consider multiple views from a scene are not independent, but follow the distribution of the training views, to enhance multi-view consistency.\nThe distributions of different views, given a scene with a total observation set ùêí, $p(ùê±|S)$ are conditionally independent (different).\nNeRF solves NVS under an even strict condition: each ray in the scen is conditionally independent.\nHowever, with this nature, the diffusion model cannot guarantee the samplings (generated images), conditioned with different source view, follow a common distribution, i.e., the diffusion model needs a unique distribution to learn.\nIdeally, the common distribution should be p(S), but it\u0026rsquo;s difficult to approximate the entire scene based on sparse views. (Not sure, my guess.)\nThat\u0026rsquo;s why they reused the generated views previously for later condition.\nPose-conditioned Given the data distribution p(ùê±‚ÇÅ, ùê±‚ÇÇ), diffusion model learns the distribution of one of the two images conditioned on the other image and both poses.\nNoise schedule involving signal-to-noise ratio Œª. Loss function of DDPM Stochastic condition Figure 3: Stochastic conditioning sampler\nMarkovian model didn\u0026rsquo;t perform well, where the next image is conditioned on (k) previously generated views. Thus, a scene can be represented as $p(ùêó) = ‚àè·µ¢p(ùê±·µ¢|ùê±_{\u0026lt;i})$.\nUsing all previous sampled views is imfesible due to the limited memory.\nThey found k=2 can achieve 3D consistency, but more previous states impair the sampling quality.\nAnd instead of conditioning on the last few samplings as in the Markovian model, 2 views are stochastically selected as condition images at each denoising step.\nGenerating a new view $ùê±‚Çñ‚Çä‚ÇÅ$ needs 256 denoising steps, where each time the condition image ùê±·µ¢ is randomly chosen from the current views set ùú≤ = {ùê±‚ÇÅ,\u0026hellip;,ùê±‚Çñ}.\nIn a denoising step, noise in the intermediate image $\\hat ùê±‚Çñ‚Çä‚ÇÅ$ will be subtracted from $ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª‚Çú)}$, which follows a forward noising distribution ùíí, given the noisy image $ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª‚Çú‚Çã‚ÇÅ)}$ of last step and the denoised image $\\hat ùê±‚Çñ‚Çä‚ÇÅ$.\n$$ \\hat ùê±‚Çñ‚Çä‚ÇÅ = \\frac{1}{\\sqrt{œÉ(Œª‚Çú)}} \\left( ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª‚Çú)} - \\sqrt{œÉ(-Œª‚Çú)}\\ Œµ_Œ∏(ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª‚Çú)}, ùê±·µ¢) \\right) \\\\ \\ \\\\ ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª‚Çú‚Çã‚ÇÅ)} \\sim q(ùê≥‚Çñ‚Çä‚ÇÅ^{Œª‚Çú‚Çã‚ÇÅ};\\ ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª‚Çú)}, \\hat ùê±‚Çñ‚Çä‚ÇÅ ) $$\n( I guess ùíí gets \u0026ldquo;reversed\u0026rdquo; after applying Bayes rule)\nThe first noisy image $ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª_T)}$ is Gaussian N(0,ùêà).\nAfter 256 steps finished, add the result image ùê±‚Çñ‚Çä‚ÇÅ to set ùú≤.\n256 can be larger to cover all the existing views.\nThis scheme approximate the true autoregressive sampling.\nAutoregressive model always use all previous states to predict the next state, unlike Markov chain only considering limited recent outputs,\nTherefore, to train an autoregressive model, a sequence, i.e. multi-view training data here, is needed.\n\u0026ldquo;True autoregressive sampling needs a score model of the form $log\\ q(ùê≥‚Çñ‚Çä‚ÇÅ^{(Œª)} | ùê±‚ÇÅ,\u0026hellip;,ùê±‚Çñ)$ and multi-view training data.\u0026rdquo;\nBut they\u0026rsquo;re not interesting in multiple source views here.\nX-UNet Figure 4: X-UNet Architecture\nUNet with only self-attention fails to generate images with multi-view consistency, given limited training images.\nDifferent frame has a different noise-level Positional encoding of pose is the same size of feature maps Use cross-attention to make two images attend to each other. Inputs:\nConcat two images? such that the weights of Conv2d and self-attention layers are shared for the noisy image and condition image\nExperiments Dataset: SRN ShapeNet (synthetic cars and chairs) github\nfile size cars_train.zip 3.26GB chairs_train.zip 60.3GB Use instant-NGP without view dependent modules\n","date":"2023-08-12T09:40:00Z","image":"https://ar5iv.labs.arxiv.org/html/2210.04628/assets/figures/training.png","permalink":"https://zichen34.github.io/writenotes/model/nvs/b-note-nvs-dm-posecond/","title":"read: NVS with Pose-conditioned Diffusion Models"},{"content":"Code | Arxiv (2307) | ProjPage\nAbs \u0026amp; Intro Fine-tune the pre-trained text-to-image diffusion model (SD) Insert cross-attention blocks between UNet blocks; Generate multiple views in parallel using a SD, and fuse multi views by attention; Freeze pre-trained weights while training the attention blocks Solving problems:\nGenerating panorama Extrapolate one perspective image to a full 360-degree view Preliminary MVDiffusion derives from LDM (Latent Diffusion Model¬π), which contains 3 modules:\nVAE for transfering the generation process to a latent space, denoising model (UNet) for sampling from the distribution of the inputs\u0026rsquo; latent codes, condition encoder for providing descriptors. Loss function is similar to original diffusion model: the MSE between original noise and predicted noise, which conditioned on noisy latents, timestep, and featues.\n$$L_{LDM} = ‚àë$$\nConvolution layers are insert in each UNet block:\nFeature maps at each level will be added into UNet blocks. Figure 2\nPipeline pixel-to-pixel correspondences enable the multi-view consistent generation of panorama and depth2img, because they have homography matrix and projection matrix to determine the matched pixel pairs in two images.\nPanorama Text-conditioned model: generate 8 target images from noise conditioned by per-view text prompts.\nThe final linear layer in CAA blocks are initialized to zero to avoid disrupt the SD\u0026rsquo;s original capacity.\nMultiple latents will be predict by UNet from noise images and then restored to images by the pre-trained VAE\u0026rsquo;s decoder.\nImage\u0026amp;Text-conditioned model: generate 7 target images based on 1 condition image and respective text prompts.\nBased on SD\u0026rsquo;s impainting modle as it takes 1 condition image.\nDon\u0026rsquo;t impaint the condition image by concatenating the noise input with a all-one mask (4 channels in total)\nCompeletly regenerate the input image by concatenating the noise input with a all-zero mask (4 channels in total)\nConcatenating all-one and all-zero channel during training make the model learn to apply different processes to condition image and target image.\ndepth2img S d \u0026amp; T e e e q p p x u t o t e h s n e p c m s r e a o p m o s p f t I A m n a T y g M S k e e i u e x t \u0026amp; d b y t m w i T m d s - o o m e o l e f c d a x d e t r o e c g t e a n l o e - l i o m d n s c m f e i s o a s t e n g i c d e o u i n t t e i i d v o e n e d k e i y m - a f g r e a s m e This two-stage design is because SD\u0026rsquo;s impainting modle doesn\u0026rsquo;t support depth map condition.\nSo the Text-conditioned model is reused to generate condition images. Then the Image\u0026amp;Text-conditioned model interpolate the two condition images. Correspondence-aware attention Aggregate the features of KxK neighbor pixels on every target feature maps to each pixel their own feature vector.\nThe source pixel $s$ perform positional encoding Œ≥(0)\nThe neighbor pixel $t_*^l$ around the corresponding pixel $t^l$ on the target image $l$ perform position encoding $Œ≥(s_*^l-s)$, which means the neighbor pixel $t_*^l$ need to be warpped back to source feature map to find the distance from $s_*^l$ to the source pixel $s$.\nFigure 3\nRef High-Resolution Image Synthesis with Latent Diffusion Models - Robin Rombach ","date":"2023-08-10T20:40:00Z","permalink":"https://zichen34.github.io/writenotes/model/nvs/b-note-mvdiffusion-scene/","title":"read: MVDiffusion generates multi-view images"},{"content":"Arxiv\nAbstract Embed diffusion model into stereo matching network Adopt multi-level network for high-resolution input Fuse generated depth map to reconstruct 3D human model. Introduction Sparse-view methods, which predict geometry based on appearance, cannot produce detailed human model because of lacking sufficient multiview stereo matching.\nContinuous models are basically obtained from traditional stereo methods based on a continuous varitional formulation, which can solved by diffusion model.\nPipeline:\nReconstruct coarse field first by using DoubleField; Render depth maps from multiple viewpoints Compute disparity flow masks Refine disparity flow with diffusion model Level 1: Use CNN to extract feature maps of disparity flow masks Level 2: Condition diffusion model with feature maps Fuse 3D points through interpolation. ","date":"2023-08-10T18:40:00Z","permalink":"https://zichen34.github.io/writenotes/model/nvs/b-note-diffustereo-human/","title":"read: DiffuStereo reconstruct 3D human"},{"content":" (2023-07-28)\ntaku Source video: ‰Ω†ÂøÖÈ°ªÊã•ÊúâRWKVÔºå‰ª•ÂèäÂÖ∂‰ªñÊ®°Âûã‰∏∫‰ΩïÊãâËÉØÔºåNLPÊùÇË∞à - takuÁöÑ‰∫§ÈîôÁîµÂè∞ - bilibili\nSequentially generating mimics the human speaking behavior naturally;\nNo need to perform Positional Encoding, RNN won\u0026rsquo;t mess up the order of tokens,\nbecause the next token always derives from the previous hidden state; RNN internally has time order. Vanilla transformer doesn\u0026rsquo;t include Positional Encoding. Cannot generate a sentence parallelly, but only word-by-word,\nbecause the \u0026ldquo;fusion matrix multiplication\u0026rdquo;, i.e., attn @ V has simplied to a RNN; Less multiplication: RNN module only consider previous two hidden states;\nAnd the matmul of Q*V and feedfoward module are kept. Locally optimizing can be done by giving a hidden state.\nTransformer has the ultimate precision because it attends all the token in the sequence, but it\u0026rsquo;s not necessary if the required performance can be met in some way.\nRWKV may forget former tokens along inputting. RWKV is good for inference and Transformer is good for training.\nRWKV incorporates RNN into transformer;\nSuch that RNN is combined with residual connection: hidden_state = hidden_state + attention(hidden_state)\nPrevious RNN is combine with attention but without residual connection;\nTransformer is attention + residual\nRWKV has a consistent memory cost at inference,\nbecause each generation only attends to the last two hidden states. That means it can accept infinite-long sequence when inference.?\nHowever, the memory cost grows up linearly when tranining, because the intermediate hidden states are required to store for calculating gradients.\nLarge model ranking\nSu, Jianlin Source article: GoogleÊñ∞‰ΩúËØïÂõæ‚ÄúÂ§çÊ¥ª‚ÄùRNNÔºöRNNËÉΩÂê¶ÂÜçÊ¨°ËæâÁÖåÔºü- ËãèÂâëÊûó\nOnly if the sequence length is significantly longer than hidden size, the standard attention will become slower quickly because its quadratic complexity. Otherwise, it\u0026rsquo;s almost linear complexity. So, it\u0026rsquo;s not necessary to make attention linear.\nOn LM (Language Model) tasks, RNN underperform attention may suffer from the hidden size.\nYannic Kilcher Source video: RWKV: Reinventing RNNs for the Transformer Era (Paper Explained) - Yannic Kilcher\n","date":"2023-07-28T17:59:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/c-sum-rwkv/","title":"sum: RWKV"},{"content":"ResNet Deep Residual Learning for Image Recognition ~ 2015 MSRA CVPR arxiv\nNetwork Architectures: Bottleneck Block: Figure 5: A deeper residual function ‚Ñ± for ImageNet. Left: a building block (on 56√ó56 feature maps) as in Fig. 3 for ResNet-34. Right: a ‚Äúbottleneck‚Äù building block for ResNet-50/101/152.\nCode: torchvision\nMobileNet MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications ~ 2017 Google arxiv\nFigure 3: Left: Standard convolutional layer with batchnorm and ReLU. Right: Depthwise Separable convolutions with Depthwise and Pointwise layers followed by batchnorm and ReLU.\nDepthwise + Pointwise convolution reduces FLOPs and parameters. Accuracy is slightly inferior to fully CNN MobileNet V3 Searching for MobileNetV3 ~ 2019 Google ICCV arxiv\nArchitecture of MobileNetV3-Large:\nTable 1 Specification For Mobilenetv3-Large. SE Denotes Whether there Is A Squeeze-and-Excite In That Block. NL Denotes the Type of Nonlinearity Used. Here, HS Denotes H-Swish and RE Denotes Relu. NBN Denotes No Batch Normalization. S Denotes Stride.\nTutorial-bili\nCode: torchvision\nEfficientNet EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ~ 2019 Google ICML arxiv\nFigure 2:Model Scaling. (a) is a baseline network example; (b)-(d) are conventional scaling that only increases one dimension of network width, depth, or resolution. (e) is our proposed compound scaling method that uniformly scales all three dimensions with a fixed ratio.\nSummary:¬≤\nScale proportionally the resolution and channels of feature maps, and number of blocks in a model. Use NAS (Neural Architecture Search) to search a structure for smaller models. Architecture:\nMBConv block is similar to MobileNetV3 InvertedResidualBlock. 8\nCode: torchvision\nVision Transformer An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ~ 2020 Google ICLR arxiv\nImage from 7\nConvert an image (224x224) to a vector using a 16x16 kernel with a stride of 16 and then flattening the result feature map (14x14) into a sequence (196).\nBy performing 728 times convolution for getting 728 vectors, an image is represented by a matrix (196, 728).\nTo encode this image\u0026rsquo;s information into a vector for classification, the class label are concatenated onto each vector and Positional encoding is added element-wise onto each vector.\nThen this matrix (197, 728) gets passed through Multi-head Self-Attention so that each token (728) will obtain a vector recording the similarity (mutual information?) between it and each other token.\nOnly taking out the vector belonging to the class label, it will be projected to the number of target categories for classifing.\nThe hybrid model didn\u0026rsquo;t downsample the image by a 16x16 kernel, but use ResNet50 to shrink the 224x224 image to 14x14.\nWhen training fewer epochs (7), hybrid model has higher accuracy than standard ViT. However, more epochs will make ViT better than hybrid model. ViT needs pre-traine on a large dataset (Google JFT) to perform better on ImageNet. However, if it\u0026rsquo;s trained on ImageNet-1K directly, the result won\u0026rsquo;t be good.\nSwin Transformer Swin Transformer: Hierarchical Vision Transformer using Shifted Windows ~ 2021 MSRA ICCV arxiv\nA unit component of swin transformer has two blocks: a Window MSA and a Shifted Window MSA.\nPatch merging\nImage from 5\nWindow MSA\nInstead of performing MSA for the all patches (sequence) of a feature map, a feature map is divided into finer grid, where severl patches is a group and a group of patches perform MSA.\nThis way reduces computation.\nDisadvantage is that the context between different group isn\u0026rsquo;t built.\nShifted Window MSA\nMove the grid (H/2, W/2) patches to the bottom right, then different groups can be fused through MSA.\nTo enhence parallelization, top row of patches are moved to the bottom and the left-most column of patches are moved the right-most.\nTo avoid fuse non-neighbor patches that are not adjacent in the original feature maps, masked MSA is used.\nThe masks are added onto the q-k weights corresponding to the non-neighbor patches for the current patch (q).\n(2023-09-28)\nSplit windows 1 image, 2 channels, H=4, W=6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 a = torch.arange(48).reshape(1,2,4,6) [[[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]] # a.is_contiguous() == True b = rearrange(a, \u0026#34;B C (nh H) (nw W) -\u0026gt; B C nh H nw W\u0026#34;, nh=2, nw=2) # b.is_contiguous() == True c = rearrange(b, \u0026#34;B C nh H nw W -\u0026gt; B nh nw C H W\u0026#34;) # c.is_contiguous() == False d = rearrange( c, \u0026#34;B nh nw C H W -\u0026gt; (B nh nw) C H W\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 b (1,2,2,2,2,3) c (1,2,2,2,2,3) d (4,2,2,3) [[[[[[ 0, 1, 2], [[[[[[ 0, 1, 2], [[[[ 0, 1, 2], [ 3, 4, 5]], [ 6, 7, 8]], [ 6, 7, 8]], [[ 6, 7, 8], [[24, 25, 26], [[24, 25, 26], [ 9, 10, 11]]], [30, 31, 32]]], [30, 31, 32]]], [[[12, 13, 14], [[[ 3, 4, 5], [[[ 3, 4, 5], [15, 16, 17]], [ 9, 10, 11]], [ 9, 10, 11]], [[18, 19, 20], [[27, 28, 29], [[27, 28, 29], [21, 22, 23]]]], [33, 34, 35]]]], [33, 34, 35]]], [[[[24, 25, 26], [[[[12, 13, 14], [[[12, 13, 14], [27, 28, 29]], [18, 19, 20]], [18, 19, 20]], [[30, 31, 32], [[36, 37, 38], [[36, 37, 38], [33, 34, 35]]], [42, 43, 44]]], [42, 43, 44]]], [[[36, 37, 38], [[[15, 16, 17], [[[15, 16, 17], [39, 40, 41]], [21, 22, 23]], [21, 22, 23]], [[42, 43, 44], [[39, 40, 41], [[39, 40, 41], [45, 46, 47]]]]]] [45, 46, 47]]]]]] [45, 46, 47]]]] The above 3 steps are equivalent to: e = rearrange(a, \u0026quot;B C (nh H) (nw W) -\u0026gt; (B nh nw) H W, nh=2, nw=2)\n= 1 1 0 6 0 6 = 2 8 = = 1 1 1 7 1 7 = 3 9 = = 1 2 2 8 2 - 8 = 4 - 0 ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ - = 1 - 2 2 3 3 - 9 = 5 - 1 4 0 = 1 = 1 2 2 3 4 0 = 6 2 5 1 = 1 = 1 2 2 3 5 1 = 7 3 6 3 3 9 4 1 2 3 = 3 4 0 4 0 = 6 2 = 5 1 2 3 = 3 4 1 5 1 = 7 3 = 2 3 2 3 = 3 4 7 3 6 - 2 = 8 - 4 ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ ‚Äñ 2 3 2 - 3 = 3 - 4 8 4 7 - 3 = 9 - 5 = 2 3 2 3 = 4 4 9 5 8 4 = 0 6 = 1 1 2 3 = 4 4 2 8 9 5 = 1 7 1 1 3 9 1 2 4 0 3 4 6 2 3 4 7 3 3 4 8 4 1 2 5 1 1 2 6 2 1 2 7 3 3 4 9 5 4 4 0 6 4 4 1 7 Restore feat maps .permute() changed .stride(), which can\u0026rsquo;t return to the structure that matches with the tensor\u0026rsquo;s original shape anymore.\nTherefore, .contiguous() is necessary before tensor .view() to the original size.\n1 2 e = d.view(1, 2, 2, 2, 2, 3) # (B, nh, nw, C, H, W) merge = e.permute(0, 3, 1, 4, 2, 5).contiguous().view(1, 2, 4, 6) # [B, C, H, W] Code from MatchNeRF.\nConvNeXt A ConvNet for the 2020s ~ 2022 FAIR CVPR arxiv\nModify ResNet50 according to Swin Transformer: 4\nStages: [3, 4, 6, 3] ‚û° [3, 3, 9, 3] (Tiny) ¬≥ Stem: First conv1 (kernel=7x7, stride=2, pad=3) and maxpool (stride=2) ‚û° Conv2dNormActivation (kernel=4x4, stride=4) Depthwise Conv: groups=1 ‚û° groups=input_channels Expand Input Chanls: Stage 0~4=(64,256,512,1024,2048) ‚û° Stage 0~4=(96,192,384,768) Expand Middle Chanls: Bottleneck (256‚ûû64‚ûû64‚ûû256) ‚û° Inverted Bottleneck (96‚ûû384‚ûû384‚ûû96) Conv First: fc + conv + fc ‚û° conv + fc + fc. Because in a transformer block, attention is ahead of fc. Large Kernel: 3 ‚û° 7 Activation: ReLU ‚û° GELU Fewer Activation: After each Conv2d ‚û° After 1st 1x1 conv Fewer Norms: After each Conv2d ‚û° After 1st 7x7 conv Norms: BatchNorm ‚û° LayerNorm Downsample Layer: Conv(stride=2) ‚û° LayerNorm + Conv2d(k=2,s=2) A ConvNext block mimics a transformer block: attention + feedforward (MLP), so Multi-Head Self-Attention corresponds to Depthwise Conv, and feedforward corresponds to 1x1 conv + activation. 6\nNetwork Architecture:\nCode flowchart from 4\nCodes: torchvision | csdn-AIÊµ© | official\nReference ConvNeXtÂÆûÊàòÔºö‰ΩøÁî®ConvNeXtÂÆûÁé∞Ê§çÁâ©ÂπºËãóÂàÜÁ±ªÔºàËá™ÂàõÔºåÈùûÂÆòÊñπÔºâ- AIÊµ© ÁªÜÂìÅEfficientNet - Ê≤àÊôØÂÖµÁöÑÊñáÁ´† - Áü•‰πé ConvNext | Less is More (Found this under the Images section of DDG with searching \u0026ldquo;convnext model\u0026rdquo;)\nConvNeXtÁΩëÁªúËØ¶Ëß£ - Â§™Èò≥Ëä±ÁöÑÂ∞èÁªøË±Ü - CSDN Swin-TransformerÁΩëÁªúÁªìÊûÑËØ¶Ëß£ - Â§™Èò≥Ëä±ÁöÑÂ∞èÁªøË±Ü - csdn A Basic Introduction to Separable Convolutions - Medium Vision TransformerËØ¶Ëß£ - Â§™Èò≥Ëä±ÁöÑÂ∞èÁªøË±Ü - csdn ","date":"2023-07-25T16:00:00Z","image":"https://pic2.zhimg.com/80/v2-35966819d100014901f3c819b7252c65_720w.webp","permalink":"https://zichen34.github.io/writenotes/model/misc/c-symp-vision_champions/","title":"sum: Champions on Vision"},{"content":"Source video: PyTorch Hooks Explained - In-depth Tutorial - Elliot Waite\nHooks for tensor 1 a.register_hook(hook_func) This will add a property _backward_hooks for the tensor a. And hooks for tensors only take effect when back-propagating (when the gradient is calculated).\nhook_func can be normal function or a lambda function, which takes as input the gradient grad for this tensor a coming from the last node, and pass the current gradient to the later backwards graph.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def hook_func(grad: Tensor): print(grad) return grad + 1 # the grad passed to the next operation increased by 1. a.register_hook(hook_func) # Second hook function a.register_hook(lambda grad: print(grad)) # Third hook function: changed grad a.register_hook(lambda grad: grad * 100) # Fourth hook function: save gradient for an intermediate node a.retain_grad() _backward_hooks is an OrderDict so it can contain multiple functions, and they\u0026rsquo;ll be executed according to their definition sequence.\nRegistering a hook for an intermediate node tensor will notify the corresponding tensor in the backwards graph.\nInside the associated node on the backwards graph, there will be an additional property: pre_hooks list, which will call the hook property a._backward_hooks of that tensor, before the grad getting into the method backward.\nSo the hook will be executed ahead of backward property during the back-propagating. backward will use the gradients returned from the hooks.\nHowever, when setting a hook for a leaf node, the hook function will only add the hook func into the _backward_hooks OrderDict of that leaf node.\nAnd the associated AccumulateGrad node of that leaf node will check if the leaf node has hook function needed to be executed before assigning grad from previous calculations.\nEach hook function has a handle index, which will be returned after executing the hook function, e.g., h = c.register_hook(hook_func) A hook can be removed via the handle index: h.remove()\nCaveat: Change grad in-place in the hook functions may affect other tensors\u0026rsquo; gradients, so later backward pass will be mess up.\nFor example, as for the grad_fn of operation e = c+d, the output gradients for tensor c and d are supposed to be the same. If the grad of d has changed, the grad of c will also changed.\ne.g. Gradient clipper ¬π Clamp the gradient of each tensor in a certain range by registering a hook for each parameter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torchvision import models def gradient_clipper(model: nn.Module, val: float) -\u0026gt; nn.Module: for parameter in model.parameters(): # in-place changing the gradient parameter.register_hook(lambda grad: grad.clamp_(-val, val)) return model resnet_clipped = gradient_clipper(models.resnet50(), val=0.01) dummy_input = torch.ones(1, 3, 224, 224) pred = resnet_clipped(dummy_input) loss = pred.log().mean() loss.backward() print(resnet_clipped.fc.bias.grad[:25]) Hooks for modules Hooks registered for modules can be automatically triggered before or after a nn.module.forward is called (even if a layer), so a hook can modify the input and output tensors of a nn.module\nHooks before forward register_forward_pre_hook(hook_func), where the hook_func can access the module and its positional input.\n1 2 3 4 5 def hook_func_pre_forward(module: nn.Module, inputs: Tensor): a, b = inputs return a+2, b myModel.register_forward_pre_hook(hook_func_pre_forward) Hooks after forward: register_forward_hook(hook_func), where the hook_func will recieve 3 arguments: the module, its input, and its output.\n1 2 3 4 def hook_func_forward(module: nn.Module, inputs: Tensor, output: Tensor): return output + 10 myModel.register_forward_hook(hook_func_forward) Hooks after backward: register_backward_hook() has been deprecated in favor of register_full_backward_hook()\ne.g. Inspect a model ¬π Printing the shape of output tensors after each layer by registering a hook for each layer in an external wrapper, rather than adding print inside the model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch from torch import nn, Tensor from torchvision import models class VerboseExecution(nn.Module): def __init__(self, model: nn.Module): super().__init__() self.model = model # Register a hook for each layer for name, module in self.model.named_children(): # conv1, bn1, relu, maxpool, layer1, ... module.__name__ = name module.register_forward_hook(self.print_shape()) def print_shape(self): def hook_func(module, inputs, output): print(f\u0026#34;{module.__name__}: {output.shape}\u0026#34;) return hook_func def forward(self, x: Tensor) -\u0026gt; Tensor: return self.model(x) # Print intermediate shape in ResNet50 resnet_verbose = VerboseExecution(models.resnet50()) dummy_input = torch.ones(1, 3, 224, 224) _ = resnet_verbose(dummy_input) e.g. Extract feature maps ¬π 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import torch from torch import nn, Tensor from torchvision import models from typing import Dict, Iterable, Callable class FeatureExtractor(nn.Module): def __init__(self, model: nn.Module, layer_ls: Iterable[str]): super().__init__() self.model = model self.layers_ex = layer_ls # Define a dict to store feature maps self._features = {layer: torch.empty(0) for layer in layer_ls} for name in layer_ls: # Pick out the selected layers by their names from a dictionary layer = dict([*self.model.named_modules()])[name] # Register a hook for each layer layer.register_forward_hook(self.save_outputs(name)) def save_outputs(self, layer_name: str) -\u0026gt; Callable: def hook_func(module, inputs, output): self._features[layer_name] = output return hook_func def forward(self, x: Tensor) -\u0026gt; Dict[str, Tensor]: _ = self.model(x) return self._features # Extract feature maps at each level before \u0026#34;avgpool\u0026#34; and \u0026#34;fc\u0026#34; resnet50 = models.resnet50() resnet_features = FeatureExtractor( resnet50, layer_ls = list(resnet50._modules)[:-2] ) dummy_input = torch.ones(1, 3, 224, 224) feature_maps = resnet_features(dummy_input) print({name: output.shape for name, output in feature_maps.items()}) Reference How to Use PyTorch Hooks ","date":"2023-07-22T15:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_hooks/","title":"memo: PyTorch | Hooks"},{"content":"How to extract features of an image from a trained model - PyTorch Forum\nHow can l load my best model as a feature extractor/evaluator?\nLoad and call torchvision.models | ResNet50 Docs\n1 2 3 4 5 6 7 8 9 10 11 import torch from torch import nn from torchvision import models, transforms import PIL resnet50 = models.resnet50(weights=\u0026#39;DEFAULT\u0026#39;) resnet50.eval() im_tensor = transforms.ToTensor()(PIL.Image.open(\u0026#39;data/nerf_llff_data/fern/images_4/image000.png\u0026#39;)) output = resnet50(im_tensor[None, ...]) Or using the weights object:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from torchvision import models from torchvision.io import read_image im = read_image(\u0026#34;data/nerf_llff_data/fern/images_4/image000.png\u0026#34;) # Step 1: Initialize model with the best available weights weights = models.ResNet50_Weights.DEFAULT model = models.resnet50(weights=weights) model.eval() # Step 2: Initialize the inference transforms preprocess = weights.transforms() # Step 3: Apply inference preprocessing transforms batch = preprocess(im).unsqueeze(0) # Step 4: Use the model and print the predicted category prediction = model(batch).squeeze(0).softmax(0) category_id = prediction.argmax().item() score = prediction[category_id].item() category_name = weights.meta[\u0026#34;categories\u0026#34;][category_id] print(f\u0026#34;{category_name}: {100*score:.1f}%\u0026#34;) triceratops: 11.5%\nBut the prediction seems not to be accurate.\nAdjust image input example\n1 2 3 4 5 6 # Initialize the Weight Transforms weights = ResNet50_Weights.DEFAULT preprocess = weights.transforms() # Apply it to the input image img_transformed = preprocess(img) (2023-07-23)\nInspect model\u0026rsquo;s modules Use hook to print layer name and the shape of their outputs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import torch from torch import nn, Tensor from torchvision import models class InspectModel(nn.Module): def __init__(self, model: nn.Module): super().__init__() self.model = model self.hook_handles = [] # print layers and their outputs\u0026#39; shape for name, module in self.model.named_children(): module.__name__ = name handle = module.register_forward_hook( lambda module, inputs, output: print(f\u0026#34;{module.__name__}: {output.shape}; Op: {module._get_name()}\u0026#34;)) self.hook_handles.append(handle) def forward(self, x: Tensor): self.model(x) for handle in self.hook_handles: handle.remove() Another ugly way is using a for loop:\n1 2 for name, module in self._modules.items(): print(name) Extract \u0026amp; Intrpl feature maps Retrieve specified layers\u0026rsquo;s feature maps and interpolate them to the same size.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import torch from torch import nn, Tensor from torchvision import models from typing import Callable, Iterable, Tuple import torch.nn.functional as F class ExtractIntrplFeatures(nn.Module): def __init__(self, model: nn.Module, layer_names: Iterable[str] = None, chnl_dim: int = 1) -\u0026gt; None: super().__init__() self.model = model self.layer_names = layer_names self._features = {} # Register a hook for each layer if layer_names is None: layer_names = list(model._modules) for layerName in layer_names: layer = dict([*self.model.named_children()])[layerName] layer.register_forward_hook(self.save_features( layerName, chnl_dim)) def save_features(self, layerName, chnl_dim) -\u0026gt; Callable: # print(layerName) def hook_func(module, inputs, output): if chnl_dim != 1: perm_order = list(range(len(output.shape))) perm_order.remove(chnl_dim) perm_order = [0, chnl_dim] + perm_order[1:] output = output.permute(*perm_order) self._features[layerName] = output return hook_func def forward(self, x: Tensor, size: Tuple[int] = None) -\u0026gt; Tensor: self.model(x) # Interpolate to the same size as the first conv feature for i in range(0, len(self._features)): layerName = list(self._features.keys())[i] if size is None: size = list(self._features.values())[0].shape[-2:] self._features[layerName] = F.interpolate( input=self._features[layerName], size=size, mode=\u0026#34;bilinear\u0026#34;, align_corners=True, ) return torch.cat(list(self._features.values()), dim=1) (2023-08-08)\nFeature extraction Get feature map at certain layers through create_feature_extractor() torchvision.models.feature_extraction ‚Äî Torchvision 0.11.0 documentation I guess it cannot realize fine-tuning the pre-trained model.\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from torchvision import models from torchvision.models.feature_extraction import get_graph_node_names from torchvision.models.feature_extraction import create_feature_extractor model = models.resnet50( weights=\u0026#34;DEFAULT\u0026#34;) train_nodes, eval_nodes = get_graph_node_names(model) print(train_nodes) featExtractor = create_feature_extractor(model, return_nodes={\u0026#39;layer4.2.relu_2\u0026#39;:\u0026#39;layer4_feat\u0026#39;}) inp = torch.ones(2, 3,224,224) with torch.no_grad(): out = featExtractor(inp) # dict (2023-08-08)\n.modules vs ._modules convnext.modules is a method. Its output content is in a sepcific format.\nconvnext.modules() is a generator. Docs - nn.Module(); Docs - Modules\nHowever, if I traverse it like for _ in convnext.modules(): print(_), it will repeatly print all the modules in the model.\nAnd chatGPT answer:\nmodules() function also iterates through sub-modules of each module, resulting in duplicate prints. To avoid this, you can use the children(). Using children() will only give you the immediate sub-modules of the features module\n1 2 3 4 5 import torchvision.models as models convnext = models.convnext_tiny(weights=\u0026#39;DEFAULT\u0026#39;) for module in convnext.features.children(): print(module) convnext._modules is an OrderedDict\nlist(convnext._modules) only has the keys (name of the modules).\nTruncate pre-trained model Question for chatGPT:\n\u0026ldquo;Given a pre-trained multiple-layer neural network in PyTorch, how to run a part of it and stop at certain layer?\u0026rdquo;\nJust extract features up to a certain layer, without performing classification or regresion steps.\n1 2 3 4 5 6 7 8 # 1. Load model import torch.nn as nn from torchvision import models pre_model = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) # 2. Create a new model sliced_model = nn.Sequential(*list(pre_model.features.children())[:5]) sliced_model.eval() The new model will inherit the pre-trained weights.\nAlexNet Source code\nAlexNet.features contains 13 modules\n1 2 3 4 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) alexnet_debug = InspectModel(alexnet.features) dummy_input = torch.ones(1,3,224, 224) alexnet_debug(dummy_input) Get pre-logits vector Retrieve the feature vector before it gets compressed to 1_000 categories, i.e., removing the last Linear layer of \u0026lsquo;classifier\u0026rsquo; module in alexnet and making a new model\n1 2 3 4 5 6 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) # drop the last \u0026#39;linear layer\u0026#39; in classifier module new_classifier = nn.Sequential(*list(alexnet.classifier.children())[:-1]) alexnet.classifier = new_classifier Get feature maps Make a new model that stops at a certain feature map\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class AlexNetConv4(nn.Module): def __init__(self, original_alexnet): super().__init__() # stop at conv4 layer_list = list(original_alexnet.features.children())[:-3] self.features = nn.Sequential(*layer_list) def forward(self, x): x = self.features(x) return x model = AlexNetConv4(models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;)) dummy_input = torch.ones(1, 3, 224, 224) features = model(dummy_input) # (1, 256, 13, 13) Retrieve the feature maps after ReLU each time:\n1 2 3 4 5 6 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) alexnet_features = ExtractIntrplFeatures( alexnet.features, layer_names=[\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;11\u0026#39;] ) dummy_input = torch.ones(1, 3, 224, 224) feat_alexnet = alexnet_features(dummy_input) # (1, 1152, 55, 55) ResNet Get feat map of resnet34 Pixel-NeRF obtains the feature map by copying the forward method until the layer4 and concat feature maps of each layer along the channels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class SpatialEncoder(nn.Module): def __init__(self, backbone): super().__init__() self.model = getattr(torchvision.model, backbone)(weights=\u0026#39;DEFAULT\u0026#39;) # Modifying model here doesn\u0026#39;t affect forward method. self.model.avgpool = nn.Sequential() self.model.fc = nn.Sequential() def forward(self, img): # DTU img: (1, 3, H=300, W=400) ... x = self.model.conv1(x) # (B, 64, H/2, W/2) x = self.model.bn1(x) x = self.model.relu(x) latents = [x] # store feature maps after different times of convlution if self.num_layers \u0026gt; 1: if self.use_first_pool: x = self.model.maxpool(x) # (B, 64, H/4, W/4) x = self.model.layer1(x) # (B, 64, H/4, W/4) latents.append(x) if self.num_layers \u0026gt; 2: x = self.model.layer2(x) # (B, 128, H/8, W/8) latents.append(x) if self.num_layers \u0026gt; 3: x = self.model.layer3(x) # (B, 256, H/16, W/16) latents.append(x) if self.num_layers \u0026gt; 4: x = self.model.layer4(x) # (B, 512, H/32, W/32) latents.append(x) self.latents = latents align_corners = None if self.index_interp == \u0026#34;nearest \u0026#34; else True latent_sz = latents[0].shape[-2:] # (H/2, W/2) # expand the feature maps to the original size for i in range(len(latents)): latents[i] = F.interpolate( input=latents[i], size=latent_sz, # (H/2, W/2) mode=self.upsample_interp, # bilinear align_corners=align_corners, ) self.latent = torch.cat(latents, dim=1) # (B, 64+64+128+256, H/2, W/2) self.latent_scaling[0] = self.latent.shape[-1] # W, 200 self.latent_scaling[1] = self.latent.shape[-2] # H, 150 self.latent_scaling = self.latent_scaling / (self.latent_scaling - 1) * 2.0 # tensor([200., 150.]) -\u0026gt; tensor([2.0101, 2.0134]) return self.latent # (B, 512, H/2, W/2) There is a flatten step in the forward method (Source code), so even the avgpool and fc are canceled as self.model.fc = nn.Sequential(), the output after calling the modified model will still become a vector, but not a feature map (planes).\nThe solution is to create a model whose forward method doesn\u0026rsquo;t contain the torch.flatten operation.\n1 2 3 4 5 resnet34 = models.resnet34(weights=\u0026#39;DEFAULT\u0026#39;) resnet_feat = nn.Sequential(*list(resnet34.children())[:-2]) im_tensor = transforms.ToTensor()(PIL.Image.open(\u0026#39;path/to/png\u0026#39;))[None,...] resnet_feat(im_tensor) However, if want to keep the feature map at every level, the forward method has to be rewritten. hooks for modules can realize this.\n(2023-07-23)\nHook feat maps 1 2 3 4 5 6 7 8 resnet34 = models.resnet34(weights=\u0026#39;DEFAULT\u0026#39;) # save feature maps after: relu, layer1, layer2, layer3. resnet_features = ExtractIntrplFeatures( resnet34, [\u0026#34;relu\u0026#34;, *list(resnet34._modules)[4:-3]] ) im_tensor = transforms.ToTensor()( PIL.Image.open(\u0026#39;data/nerf_llff_data/fern/images_4/image000.png\u0026#39;)) feat = resnet_features(im_tensor.unsqueeze(0)) The result is identical to pixel-NeRF\u0026rsquo;s SpatialEncoder, which I copied its definition to a ipynb and instantiate it:\n1 2 3 pixelNeRFEncoder = SpatialEncoder() feature_maps = pixelNeRFEncoder(im_tensor.unsqueeze(0)) torch.eq(feat, feature_maps).detach().numpy().all() (2023-07-21)\nMobileNet v3 MobileNetV3 contains 3 components: features, avgpool, and classifier.\nThe MobileNetV3.features starts with a Conv2dNormActivation layer followed by 15 InvertResidual blocks, and ends with a Conv2dNormActivation layer.\nAn InvertResidual module is a nn.Sequential model of 3 Conv2dNormActivation() layers, corresponding to \u0026ldquo;expand\u0026rdquo;, \u0026ldquo;depthwise\u0026rdquo;, and \u0026ldquo;project\u0026rdquo;. Source code\nInspect mobilenet 1 2 3 4 mobilenetv3 = models.mobilenet_v3_large(weights=\u0026#39;DEFAULT\u0026#39;) mobilenet_inspect = InspectModel(mobilenetv3.features) dummy_input = torch.ones(1, 3, 224, 224) mobilenet_inspect(dummy_input) Hook feature maps 1 2 3 4 5 6 mobilenetv3 = models.mobilenet_v3_large(weights=\u0026#39;DEFAULT\u0026#39;) mobilenet_feats = ExtractIntrplFeatures( mobilenetv3.features, layer_names=list(mobilenetv3.features._modules) ) feat_mbnet = mobilenet_feats(dummy_input) # (1,1224,112,112) faster-rcnn Obj Detect Everything you need to know about TorchVision‚Äôs MobileNetV3 implementation - PyTorch blog\nDocs | Faster RCNN Code\nMobileNet feature maps \u0026ndash;\u0026gt; Feature Pyramid Network \u0026ndash;\u0026gt; FasterRCNN detector for locating bounding box\n1 2 3 4 high_res = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) high_res.eval() out = high_res([im_tensor[0]])[0] out.keys() 1 2 3 4 5 6 7 low_res = models.detection.fasterrcnn_mobilenet_v3_large_320_fpn( weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT) low_res.eval() x = [im_tensor, torch.rand(3, 500, 400)] predictions = low_res(x) print(predictions[0].keys()) lraspp segment Outputs are masks for different classes. I guess these masks may be not suitable to serve as feature maps.\n1 2 3 4 5 6 lraspp = models.segmentation.lraspp_mobilenet_v3_large(pretrained=True) # input: (B, C, H, W); output: (B, C\u0026#39;, H, W) output = lraspp(im_tensor) out = output[\u0026#39;out\u0026#39;].detach().numpy() plt.imshow(out[0][3]) deeplabv3 segment 1 2 deeplabv3 = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True) deeplabv3(im_tensor) (2023-07-22)\nReplace forward of predefined model How can I replace the forward method of a predefined torchvision model with my customized forward function?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import torchvision.models as models import torch def new_forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) return x # define a resnet instance resent = models.resnet18() # add new_forward function to the resnet instance as a class method bound_method = new_forward.__get__(resent, resent.__class__) setattr(resent, \u0026#39;forward\u0026#39;, bound_method) (2023-07-22)\nExtract \u0026amp; interpolate feature maps Question: I want to extract and interpolate all the features maps after each stage of convolution to the same size form a pytorch pretrained model, for example, mobilenet v3. Please give the code.\nBing chat Here is an example code that extracts and interpolates all the feature maps after each stage of convolution to the same size from a PyTorch pre-trained model, MobileNet V3.\nClick to expand the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torch import torch.nn as nn import torch.nn.functional as F from torchvision.models.mobilenetv3 import mobilenet_v3_large class FeatureExtractor(nn.Module): def __init__(self): super(FeatureExtractor, self).__init__() self.features = nn.ModuleList([ nn.Sequential( m.conv, m.bn, nn.ReLU(inplace=True), m.hs, m.se, m.conv_pw, m.bn_pw, nn.ReLU(inplace=True), m.hs_pw, ) for m in mobilenet_v3_large().features[:-1] ]) def forward(self, x): out = [] for f in self.features: x = f(x) out.append(x) return out model = FeatureExtractor() x = torch.randn(1, 3, 224, 224) out = model(x) This code creates a FeatureExtractor class that extracts all the feature maps after each stage of convolution to the same size from a PyTorch pre-trained model, MobileNet V3. The forward method takes an input tensor x and returns a list of feature maps. The out variable contains all the feature maps after each stage of convolution to the same size.\nI hope this helps! Let me know if you have any other questions.\nRun it:\nAttributeError: \u0026lsquo;Conv2dNormActivation\u0026rsquo; object has no attribute \u0026lsquo;conv\u0026rsquo;\nChatGPT To extract and interpolate all the feature maps after each stage of convolution in a PyTorch pre-trained model like MobileNetV3, you can use the intermediate_layers functionality to access intermediate outputs and the nn.Upsample module for interpolation. Here\u0026rsquo;s a step-by-step guide on how to achieve this:\nLoad the pre-trained MobileNetV3 model. Modify the model to store intermediate feature maps during forward pass. Use the nn.Upsample module to interpolate the feature maps to a common size. First, make sure you have PyTorch and torchvision installed: pip install torch torchvision\nNow, let\u0026rsquo;s create the code:\nClick to expand the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import torch import torch.nn as nn import torchvision.models as models class MobileNetV3WithIntermediateOutputs(nn.Module): def __init__(self, model_name=\u0026#39;mobilenet_v3_large\u0026#39;, pretrained=True): super(MobileNetV3WithIntermediateOutputs, self).__init__() self.model = models.__dict__[model_name](pretrained=pretrained) # Register hooks to save intermediate outputs self.intermediate_outputs = [] self.hook_handles = [] for module in self.model.modules(): handle = module.register_forward_hook(self.hook_fn) self.hook_handles.append(handle) def hook_fn(self, module, input, output): self.intermediate_outputs.append(output) def forward(self, x): self.intermediate_outputs = [] # Clear previous intermediate outputs _ = self.model(x) return self.intermediate_outputs def interpolate_to_common_size(feature_maps, target_size): _, _, H, W = feature_maps[-1].size() if (H, W) == target_size: return feature_maps interpolated_feature_maps = [] for fmap in feature_maps: interpolated_feature_maps.append(nn.functional.interpolate(fmap, size=target_size, mode=\u0026#39;bilinear\u0026#39;, align_corners=False)) return interpolated_feature_maps # Example usage: model_name = \u0026#39;mobilenet_v3_large\u0026#39; input_size = 224 target_size = (56, 56) # Specify the target size you want to interpolate the feature maps to # Load the MobileNetV3 model with intermediate outputs model = MobileNetV3WithIntermediateOutputs(model_name=model_name, pretrained=True) # Set the model to evaluation mode model.eval() # Create a random input tensor (batch size 1, 3 channels, height=input_size, width=input_size) input_tensor = torch.rand(1, 3, input_size, input_size) # Get the intermediate feature maps intermediate_feature_maps = model(input_tensor) # Interpolate the feature maps to the common size interpolated_feature_maps = interpolate_to_common_size(intermediate_feature_maps, target_size) # The \u0026#39;interpolated_feature_maps\u0026#39; now contain all the intermediate feature maps, each resized to the \u0026#39;target_size\u0026#39;. In the code above, we create a MobileNetV3WithIntermediateOutputs class that inherits from nn.Module. This class will hold the pre-trained MobileNetV3 model and includes a hook function to store intermediate feature maps during forward pass.\nThe interpolate_to_common_size function takes a list of feature maps and resizes each one to the specified target_size using bilinear interpolation. If the size of the feature maps already matches the target size, they are left unchanged.\nFinally, we load the MobileNetV3 model with intermediate outputs, perform a forward pass with a random input tensor, and then interpolate all the intermediate feature maps to the common size specified by target_size.\nPlease note that the example assumes you want to interpolate the feature maps to a target size of (56, 56). You can modify the target_size to your desired dimensions. Additionally, you can change the model_name parameter to use other versions of MobileNetV3, such as \u0026lsquo;mobilenet_v3_small\u0026rsquo;, etc.\nRun it:\n1 2 3 4 5 6 7 /home/zichen/Downloads/GNT-comments/temptest.ipynb Cell 4 in interpolate_to_common_size(feature_maps, target_size) 25 def interpolate_to_common_size(feature_maps, target_size): ---\u0026gt; 26 _, _, H, W = feature_maps[-1].size() 27 if (H, W) == target_size: 28 return feature_maps ValueError: not enough values to unpack (expected 4, got 2) It used hook, but it registered a hook for each module. But it didn\u0026rsquo;t stop before torch.flatten when forwarding, so the last feature_maps is a vector. Maybe that\u0026rsquo;s my bad in prompting.\n(2023-07-24)\nConvNeXt-tiny Print the shape of output after each layer:\n1 2 3 4 convnext = models.convnext_tiny(weights=\u0026#39;DEFAULT\u0026#39;) convnext_inspect = InspectModel(convnext) dummy_input = torch.ones(1,3,754,1008) convnext_inspect(dummy_input) Extract specified feature maps and interpolate them to specified size:\n1 2 3 convnext_features = ExtractIntrplFeatures(convnext.features, [\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;]) H_maps, W_maps = dummy_input.size()[-2:] features = convnext_features(dummy_input, (H_maps, W_maps)) # (1,192,754,1008) ","date":"2023-07-18T19:25:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_pretrained_models/","title":"memo: PyTorch | Pre-trained models"},{"content":" Source Video: Diffusion Models | Paper Explanation | Math Explained - Outlier Code: dome272/Diffusion-Models-pytorch (2023-08-02)\nIdea \u0026amp; Theory Diffusion model is a generative model, so it learns the distribution of data ùêó. (Discrimitive model learns labels. And MLE is a strategy to determine the distribution through parameters ùöØ)\nThe essential idea is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. [1]\nForward diffusion process:\nSample noise from a normal distribution¬π and add it to an image iteratively, until the original distribution of the image has been completely destroyed, becoming the same as the noise distribution.\nThe noise level (mean, variance) of each timestep is scaled by a schedule to avoid the variance explosion along with adding more noise.\nThe image distribution should be destroyed slowly, and the noise redundency at the end stage should be reduced.\nOpenAI ¬≥ proposed Cosine schedule in favor of the Linear schedule in DDPM¬≤.\nReverse diffusion process:\nPredict the noise of each step.\nDo not predict the full image in one-shot because that\u0026rsquo;s intractable and results in worse results¬π.\nPredicting the mean of the noise distribution and predicting the noise in the image directly are equivalent, just being parameterized differently ¬≤.\nPredict noise directly, so it can be subtracted from image.\nThe variance œÉ¬≤ of the normal distribution followed by the noise can be fixed¬≤. But optimizing it together with the mean, the log-likehood will get improved ¬≥.\nArchitecture DDPM used UNet like model:\nMulti-level downsample + resnet block ‚ûî Low-res feature maps ‚ûî Upsample to origianl size Concatenate RHS feature maps with the LHS feature maps of the same resolution to supplement the location information for features at each pixel. Attend some of LHS and RHS feature maps by attention blocks to fuse features further. Time embedding is added to each level of feature maps for \u0026ldquo;identifying\u0026rdquo; the consistent amount of noise to be predicted during a forward pass at a timestep. OpenAI 2nd paper(4) made improvement by modifying the model in:\nIncrease levels, reduce width; More attention blocks, more heads; BigGAN residual block when downsampling and upsampling; Adaptive Group Normalization after each resnet block: GroupNorm + affine transform ( Time embedding * GN + Label embedding) Classifier guidance is a separate classifier that helps to generate images of a certain category. Math Derivation (2024-04-17)\nVAE and diffuseion model both follows MLE strategy to find the parameter corresponding to the desired data distribution. VAE solves the dataset distribution P(ùêó) by approximating ELBO; While diffusion model solves the dataset distribution P(ùêó) by minimizing the KL Divergence. (2023-08-04)\nVAE VAE also wants to get the distribution of dataset P(ùêó), and an ùê± is generated by a latent variable ùê≥.\nTherefore, based on Bayes theorem, p(ùê±) = p(ùê≥) p(ùê±|ùê≥) / p(ùê≥|ùê±), where p(ùê≥) is the prior.\nAnd p(ùê≥|ùê±) is intractable because in p(ùê≥|ùê±) = p(ùê≥)p(ùê±|ùê≥) / p(ùê±), the p(ùê±) can\u0026rsquo;t be computed through ‚à´f(ùê±,ùê≥)dùê≥ since ùê≥ is high dimensional continuous.\nBy introducing an approximated posterior q(ùê≥|ùê±), log p(ùê±) = ELBO + KL-divergence.\n$log p(ùê±) = E_{q(ùê≥|ùê±)} log \\frac{p(ùê±,ùê≥)}{q(ùê≥|ùê±)} + ‚à´ q(ùê≥|ùê±) log \\frac{q(ùê≥|ùê±)}{p(ùê≥)} dùê≥$\nThe KL-divergence can be integrated analytically.\nELBO is an expectation w.r.t q(ùê≥|ùê±), which can technically be estimated using Monte Carlo sampling directly.\nBut when sampled q(ùê≥‚Å±|ùê±) is around 0, the variance of $log\\ q(ùê≥|ùê±)$ would be high and make its gradint unstable, then cause the optimization difficult. And the function to be estimated $log(\\frac{p_Œ∏(ùê±,ùê≥)}{q_œÜ(ùê≥|ùê±)})$ involves two approximate models containing a lot error.\nThus, it\u0026rsquo;s not feasible to approximate ELBO directly.\nTo approximate ELBO, we analyse the generative model (Decoder) p(ùê±|ùê≥).\nBase on Bayes theorem, p(ùê±|ùê≥) = p(ùê±) p(ùê≥|ùê±)/p(ùê≥).\nBy introducing the posterior approximation q(ùê≥|ùê±), p(ùê±|ùê≥) can derive: E(log p(ùê±|ùê≥)) = ELBO + KL-divergence, i.e.,\n$E_{q(ùê≥|ùê±)}[ log p(ùê±|ùê≥)] = E_{q(ùê≥|ùê±)}[ log(\\frac{p(ùê≥|ùê±) p(ùê±)}{q(ùê≥|ùê±)})] + ‚à´ q(ùê≥|ùê±) log \\frac{q(ùê≥|ùê±)}{p(ùê≥)} dùê≥$\nGiven ùê≥, the likelihood p(ùê±|ùê≥) is supposed to be maximized. (The probiblity that the real ùê± is sampled should be maximum.)\nTherefore, the parameters Œ∏ of generative model p(ùê±|ùê≥) should be optimized via MLE (cross-entropy) loss.\nNow since ELBO = E(log p(ùê±|ùê≥)) - KL-divergence and KL-div is known, ELBO will be obtained by just computing E(log p(ùê±|ùê≥)).\nE(log p(ùê±|ùê≥)) can be estimated by MC: sample a ùê≥ then compute log p(ùê±|ùê≥), and repeat N times, take average.\nThe approximated E(log p(ùê±|ùê≥)) should be close to the original ùê±, so there is a MSE loss to optimize the parameters œï of the distribution of ùê≥.\n(2023-10-30) ùê≥\u0026rsquo;s distribution needs to be learned as well for sampling ùê±. But MC sampling is not differentiable, so œï cannot be optimized through gradient descent.\nTherefore, reparameterization considers that ùê≥ comes from a differentiable determinstic transform of Œµ, a random noise, i.e., ùê≥ = Œº + œÉŒµ.\nThen, parameters (Œº, œÉ¬≤) of ùê≥\u0026rsquo;s distribution (Encoder) will be optimized by MSE.\nForward process The forward diffusion process is like the \u0026ldquo;Encoder\u0026rdquo; p(ùê≥|ùê±) in VAE:\n$$q(ùê≥|ùê±) ‚áí q(ùê±‚Çú | ùê±‚Çú‚Çã‚ÇÅ)$$\nThe distribution of image ùê±‚Çú at timestep t is determined by the image ùê±‚Çú‚Çã‚ÇÅ at the previous timestep, where smaller t means less noise.\nSpecifically, ùê±‚Çú follows a normal distribution with a mean of $\\sqrt{1-Œ≤‚Çú}ùê±‚Çú‚Çã‚ÇÅ$ and a variance of $\\sqrt{Œ≤‚Çú}ùêà$:\n$$q(ùê±‚Çú | ùê±‚Çú‚Çã‚ÇÅ) = N(ùê±‚Çú; \\sqrt{1-Œ≤‚Çú} ùê±‚Çú‚Çã‚ÇÅ, \\sqrt{Œ≤‚Çú}ùêà)$$\nùê±‚Çú is similar to ùê±‚Çú‚Çã‚ÇÅ because its mean is around ùê±‚Çú‚Çã‚ÇÅ.\nAn image ùê± is a \u0026ldquo;vector\u0026rdquo;, and each element of it is a pixel.\nAs timestep t increase, Œ≤‚Çú increases and (1-Œ≤‚Çú) decreases, which indicates the variance gets larger and the mean value gets smaller.\nIntuitively, the value of the original pixel x‚Çú‚Çã‚ÇÅ is fading and more pixels become outliers resulting in a wider range of variation around the mean.\nBy introducing a notation $Œ± = 1-Œ≤‚Çú$, the t-step evolution from ùê±‚ÇÄ to ùê±‚Çú can be simplied to a single expression instead of sampling t times iteratively.\nReplace (1-Œ≤‚Çú) with Œ±, the distribution becomes:\n$q(ùê±‚Çú | ùê±‚Çú‚Çã‚ÇÅ) = N(ùê±‚Çú; \\sqrt{Œ±‚Çú} ùê±‚Çú‚Çã‚ÇÅ, (1-Œ±‚Çú)ùêà)$\nBased on the reparameterization trick, a sample from the distribution is:\n$ùê±‚Çú = \\sqrt{Œ±‚Çú} ùê±‚Çú‚Çã‚ÇÅ + \\sqrt{1-Œ±‚Çú} Œµ$\nSimilarly, $ùê±‚Çú‚Çã‚ÇÅ = \\sqrt{Œ±‚Çú‚Çã‚ÇÅ} ùê±‚Çú‚Çã‚ÇÇ + \\sqrt{1-Œ±‚Çú‚Çã‚ÇÅ} Œµ$, and plug it into ùê±‚Çú.\nThen $ùê±‚Çú = \\sqrt{Œ±‚Çú‚Çã‚ÇÅ} ( \\sqrt{Œ±‚Çú} ùê±‚Çú‚Çã‚ÇÇ + \\sqrt{1-Œ±‚Çú‚Çã‚ÇÅ} Œµ ) + \\sqrt{1-Œ±‚Çú} Œµ$. Now, the mean becomes $\\sqrt{Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ} ùê±‚Çú‚Çã‚ÇÇ$\nGiven variance = 1 - (mean/ùê±)¬≤ in the above normal distribution $N(ùê±‚Çú; \\sqrt{Œ±‚Çú} ùê±‚Çú‚Çã‚ÇÅ, (1-Œ±‚Çú)ùêà)$, and here mean = $\\sqrt{Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ} ùê±‚Çú‚Çã‚ÇÇ$,\nthe standard deviation should be $\\sqrt{1 - Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ}$, then ùê±‚Çú becomes:\n$ùê±‚Çú = \\sqrt{Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ} ùê±‚Çú‚Çã‚ÇÇ + \\sqrt{1 - Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ} Œµ$\nRepeatedly substituting intermediate states, the ùê±‚Çú can be represented with ùê±‚ÇÄ :\n$ùê±‚Çú = \\sqrt{Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ \u0026hellip; Œ±‚ÇÅ} ùê±‚ÇÄ + \\sqrt{1 - Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ \u0026hellip; Œ±‚ÇÅ} Œµ$\nDenote the cumulative product \u0026ldquo;Œ±‚ÇúŒ±‚Çú‚Çã‚ÇÅ \u0026hellip; Œ±‚ÇÅ\u0026rdquo; as $\\bar a‚Çú$, the ùê±‚Çú can be reached in one-shot.\n$ùê±‚Çú = \\sqrt{\\bar a‚Çú} ùê±‚ÇÄ + \\sqrt{1 - \\bar a‚Çú} Œµ$\nThe distribution of ùê±‚Çú given ùê±‚ÇÄ is:\n$q(ùê±‚Çú | ùê±‚ÇÄ) = N(ùê±‚Çú; \\sqrt{\\bar a‚Çú} ùê±‚ÇÄ, (1 - \\bar a‚Çú)ùêà)$\nWith this expression, the deterministic forward process is ready-to-use and only the reverse process needs to be learned by a network.\nThat\u0026rsquo;s why in the formula below, they \u0026ldquo;reverse\u0026rdquo; the forward q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ) to q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú) resulting in the equation only containing \u0026ldquo;reverse process\u0026rdquo;: ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, which then can be learned by narrowing the gap between q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú) and p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú). Reverse process The reverse diffusion process is like the Decoder in VAE.\n$$p(ùê±|ùê≥) ‚áí p(ùê±‚Çú‚Çã‚ÇÅùê±‚Çú‚Çã‚ÇÇ..ùê±‚ÇÄ | ùê±‚Çú)$$\nGiven a noise image ùê±‚Çú, the distribution of less-noise image ùê±‚Çú‚Çã‚ÇÅ is\n$p(ùê±‚Çú‚Çã‚ÇÅ | ùê±‚Çú) = N(ùê±‚Çú‚Çã‚ÇÅ; Œº_Œ∏(ùê±‚Çú, t), Œ£_Œ∏(ùê±‚Çú, t))$\nwhere the variance can be a fixed schedule as Œ≤‚Çú, so only the mean $Œº_Œ∏(ùê±‚Çú, t)$ needs to be learned with a network.\nVLB VLB is the loss to be minimized. VLB gets simplied by:\nApplying Bayes rule to \u0026ldquo;reverse\u0026rdquo; the direction of the forward process, which becomes \u0026ldquo;forward denoising\u0026rdquo; steps q(ùê±‚Çú‚Çã‚ÇÅ | ùê±‚Çú), because it\u0026rsquo;s from a noise image to a less-noise image;\nAdding extra conditioning on ùê±‚ÇÄ for each \u0026ldquo;forward denosing\u0026rdquo; step q(ùê±‚Çú‚Çã‚ÇÅ | ùê±‚Çú, ùê±‚ÇÄ).\nDerivation by step:\nDiffusion model wants a set of parameter ùõâ letting the likelihood of the original image ùê±‚ÇÄ maximum.\n$$\\rm Œ∏ = arg max_Œ∏\\ log\\ p_Œ∏(ùê±‚ÇÄ)$$\nWith adding a minus sign, the objective turns to find the minimum:\n-log p(ùê±‚ÇÄ) = -ELBO - KL-divergence\n$$ \\begin{aligned} \u0026amp; -log p(ùê±‚ÇÄ) \\left( = -log \\frac{p(ùê±‚ÇÄ, ùê≥)}{p(ùê≥|ùê±‚ÇÄ)} \\right) \\\\ \u0026amp;= -log \\frac{p(ùê±_{1:T}, ùê±‚ÇÄ)}{p(ùê±_{1:T} | ùê±‚ÇÄ)} \\\\ \u0026amp; \\text{(Introduce \u0026ldquo;approximate posterior\u0026rdquo; q :)} \\\\ \u0026amp;= -(log \\frac{ p(ùê±_{1:T}, ùê±‚ÇÄ) }{ q(ùê±_{1:T} | ùê±‚ÇÄ)} \\ + log (\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{1:T} | ùê±‚ÇÄ)}) ) \\\\ \\end{aligned} $$\nNote that $q(ùê±_{1:T} | ùê±‚ÇÄ)$ represents a joint distribution of N conditional distributions ùê±‚Çú and ùê±‚Çú‚Çã‚ÇÅ.\nIt is the step-by-step design that makes training a network to learn the data distribution possible. Meanwhile, the sampling process also has to be step-by-step.\nCompute expection w.r.t. $q(ùê±_{1:T} | ùê±‚ÇÄ)$ for both side.\n$$ E_{q(ùê±_{1:T} | ùê±‚ÇÄ)} [ -log p(ùê±‚ÇÄ) ] \\\\ \\ = E_{q(ùê±_{1:T} | ùê±‚ÇÄ)} \\left[-log \\frac{ p(ùê±_{0:T}) }{ q(ùê±_{1:T} | ùê±‚ÇÄ)}\\right] \\ + E_{q(ùê±_{1:T} | ùê±‚ÇÄ)} \\left[-log (\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{1:T} | ùê±‚ÇÄ)})\\right] $$\nExpectation is equivalent to integration.\n$$ \\begin{aligned} \u0026amp; \\text{LHS:} ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * (-log p(ùê±‚ÇÄ)) dùê±_{1:T} = -log p(ùê±‚ÇÄ) \\\\ \u0026amp; \\text{RHS:} \\ = E_{q(ùê±_{1:T} | ùê±‚ÇÄ)} \\left[-log \\frac{ p(ùê±_{0:T}) }{ q(ùê±_{1:T} | ùê±‚ÇÄ)}\\right] \\\\ \u0026amp; + ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * \\left(-log (\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{1:T} | ùê±‚ÇÄ)})\\right) dùê±_{1:T} \\end{aligned} $$\nSince KL-divergence is non-negative, there is:\n-log p(ùê±‚ÇÄ) ‚â§ -log p(ùê±‚ÇÄ) + KL-divergence =\n$$ \\begin{aligned} \u0026amp; -log p(ùê±‚ÇÄ) + D_{KL}( q(ùê±_{1:T} | ùê±‚ÇÄ) || p(ùê±_{1:T} | ùê±‚ÇÄ) ) \\\\ \u0026amp;= -log p(ùê±‚ÇÄ) \\ + ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * \\left(log (\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{1:T} | ùê±‚ÇÄ)})\\right) dùê±_{1:T} \\end{aligned} $$\nBreak apart the denominator $p(ùê±_{1:T} | ùê±‚ÇÄ)$ of the argument in the KL-divergence\u0026rsquo;s logarithm based on Bayes rule:\n$$p(ùê±_{1:T} | ùê±‚ÇÄ) = \\frac{p(ùê±_{1:T}, ùê±‚ÇÄ)}{p(ùê±‚ÇÄ)} = \\frac{p(ùê±_{0:T})}{p(ùê±‚ÇÄ)}$$\nPlug it back to KL-divergence:\n$$ \\begin{aligned} \u0026amp;‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * \\left( log(\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{1:T} | ùê±‚ÇÄ)})\\right) dùê±_{1:T} \\\\ \u0026amp;= ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * log (\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ) p(ùê±‚ÇÄ)}{p(ùê±_{0:T})}) dùê±_{1:T} \\\\ \u0026amp;= ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * [ log(p(ùê±‚ÇÄ) + log(\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{0:T})})] dùê±_{1:T}\\\\ \u0026amp;= ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * log(p(ùê±‚ÇÄ) dùê±_{1:T} \\\\ \u0026amp;\\quad + ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * log(\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{0:T})}) dùê±_{1:T} \\\\ \u0026amp;= log p(ùê±‚ÇÄ) + ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ) * log(\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{0:T})}) dùê±_{1:T} \\end{aligned} $$\nPlug this decomposed KL-divergence into the above inequality, and the incomputable log-likelihood (-log p(ùê±‚ÇÄ)) can be canceled, resulting in the Variational Lower Bound (VLB):\n$$-log p(ùê±‚ÇÄ) ‚â§ ‚à´_{ùê±_{1:T}} q(ùê±_{1:T} | ùê±‚ÇÄ)\\ log(\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_{0:T})}) dùê±_{1:T}$$\nThe argument of log is a ratio of the forward process and the reverse process.\nThe numerator is the distribution of $ùê±_{1:T}$ given the starting point ùê±‚ÇÄ. To make the numerator and denominator have symmetric steps, the starting point of the reverse process $p(ùê±_T)$ can be separated out.\nSeparate out $p(ùê±_T)$ from the denominator by rewriting the conditional probability as a cumulative product:\n$$ p(ùê±_{0:T}) = p(ùê±_T) Œ†_{t=1}^T p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú) $$\nPlug it back into the logarithm of the VLB, and break the numerator joint distribution as a product of N-1 steps as well:\n$$ log(\\frac{q(ùê±_{1:T} | ùê±‚ÇÄ)}{p(ùê±_T) Œ†_{t=1}^T p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)}) = log \\frac{ Œ†_{t=1}^T q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ)}{ p(ùê±_T) Œ†_{t=1}^T p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)} \\\\ = log \\frac{ Œ†_{t=1}^T q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ)}{ Œ†_{t=1}^T p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)} - log p(ùê±_T) \\\\ = ‚àë_{t=1}^T log (\\frac{q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ)}{p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)}) - log\\ p(ùê±_T) $$\nThis form includes every step rather than only focusing on the distribution of the all events $ùê±_{1:T}$.\n(2023-08-11) DM wants the data distribution, but it doesn\u0026rsquo;t rebuild the distribution transformation directly from Gaussian to data distribution, but approachs the corruption process step-by-step to reduce the difficulty (variance).\nSeparate the first item (first step, t=1) from the summation, so that the other terms can be conditioned on ùê±‚ÇÄ, thus reducing the variance:\n$$ log \\frac{q(ùê±‚ÇÅ|ùê±‚ÇÄ)}{p(ùê±‚ÇÄ|ùê±‚ÇÅ)} + ‚àë_{t=2}^T log (\\frac{q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ)}{p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)}) - log\\ p(ùê±_T) $$\nReformulate the numerator $q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ)$ based on Bayes rule:\n$$ q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ) = \\frac{q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)q(ùê±‚Çú)}{q(ùê±‚Çú‚Çã‚ÇÅ)} $$\nIn this form, forward adding noise $q$ and reverse denoising $p$ become the same process from ùê±‚Çú to ùê±‚Çú‚Çã‚ÇÅ. Such that, in one pass, the model can both perform forward process and reverse process once.\nMake each step conditioned on ùê±‚ÇÄ to reduce the variance (uncertainty).\n$$ q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ) = \\frac{q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ)q(ùê±‚Çú| ùê±‚ÇÄ)}{q(ùê±‚Çú‚Çã‚ÇÅ| ùê±‚ÇÄ)} $$\nAnd this distribution $q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ)$ has a closed-form solution.\nHere is why the first step is separated out: If t=1, the $q(ùê±‚ÇÅ|ùê±‚ÇÄ)$ conditioned on ùê±‚ÇÄ is:\n$$ q(ùê±‚ÇÅ|ùê±‚ÇÄ) = \\frac{q(ùê±‚ÇÄ|ùê±‚ÇÅ, ùê±‚ÇÄ)q(ùê±‚ÇÅ|ùê±‚ÇÄ)}{q(ùê±‚ÇÄ|ùê±‚ÇÄ)} $$\nThere is a loop of $q(ùê±‚ÇÅ|ùê±‚ÇÄ)$ if ùê±‚ÇÄ exists, and other terms $q(ùê±‚ÇÄ|ùê±‚ÇÅ, ùê±‚ÇÄ)$ and $q(ùê±‚ÇÄ|ùê±‚ÇÄ)$ don\u0026rsquo;t make sense.\nPlug the newly conditioned numerator back to the fraction, and break it apart based on log rule:\n$$ ‚àë_{t=2}^T log \\frac{q(ùê±‚Çú|ùê±‚Çú‚Çã‚ÇÅ)}{p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)} \\ = ‚àë_{t=2}^T log \\frac{q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ)q(ùê±‚Çú| ùê±‚ÇÄ)}{p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)q(ùê±‚Çú‚Çã‚ÇÅ| ùê±‚ÇÄ)} \\\\ \\ = ‚àë_{t=2}^T log \\frac{q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ)}{p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)} + ‚àë_{t=2}^T log \\frac{q(ùê±‚Çú| ùê±‚ÇÄ)}{q(ùê±‚Çú‚Çã‚ÇÅ| ùê±‚ÇÄ)} \\\\ $$\nThe second term will be simplied to $log \\frac{q(ùê±_T| ùê±‚ÇÄ)}{q(ùê±‚ÇÅ| ùê±‚ÇÄ)}$\nThen, the variational lower bound becomes:\n$$ D_{KL}(q(ùê±_{1:T}|ùê±‚ÇÄ) || p(ùê±_{0:T})) = \\\\ log \\frac{q(ùê±‚ÇÅ|ùê±‚ÇÄ)}{p(ùê±‚ÇÄ|ùê±‚ÇÅ)} \\ + ‚àë_{t=2}^T log \\frac{q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ)}{p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)} \\ + log \\frac{q(ùê±_T| ùê±‚ÇÄ)}{q(ùê±‚ÇÅ| ùê±‚ÇÄ)} \\ - log\\ p(ùê±_T) \\\\ \\ \\\\ \\ = ‚àë_{t=2}^Tlog \\frac{ q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ) }{ p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)} \\ + log \\frac{q(ùê±_T| ùê±‚ÇÄ)}{p(ùê±‚ÇÄ|ùê±‚ÇÅ)} \\ - log\\ p(ùê±_T) \\\\ \\ \\\\ \\ = ‚àë_{t=2}^T log \\frac{ q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ) }{ p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)} \\ + log \\frac{q(ùê±_T| ùê±‚ÇÄ)}{p(ùê±_T)} \\ - log p(ùê±‚ÇÄ|ùê±‚ÇÅ) $$\nWrite this formula as KL-divergence, so that a concrete expression can be determined later.\nHow are those two fractions written as KL-divergence? $$ \\begin{aligned} \u0026amp; ‚àë_{t=2}^T D_{KL} (q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ) || p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)) \\\\ \u0026amp; + D_{KL} (q(ùê±_T| ùê±‚ÇÄ) || p(ùê±_T)) \\\\ \u0026amp; - log\\ p(ùê±‚ÇÄ|ùê±‚ÇÅ) \\end{aligned} $$\nLoss function The VLB to be minimized is eventually derived as a MSE loss function between the actual noise and the predicted noise.\n$D_{KL} (q(ùê±_T| ùê±‚ÇÄ) || p(ùê±_T))$ can be ignored.\n$q(ùê±_T| ùê±‚ÇÄ)$ has no learnable parameters because it just adds noise following a schedule. And $p(ùê±_T)$ is the noise image sampled from normal distribution. Since $q(ùê±_T| ùê±‚ÇÄ)$ is the eventual image which is supposed to follow the normal distribution, this KL-divergence should be small. Then, the loss only contains the other two terms:\n$$L = ‚àë_{t=2}^T D_{KL} (q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ) || p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)) - log\\ p(ùê±‚ÇÄ|ùê±‚ÇÅ)$$\n$D_{KL} (q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ) || p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú))$ is the MSE between the actual noise and the predicted noise.\nFor the reverse pass, the distribution of the denoised image $p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú)$ has a parametric expression:\n$$p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú) = N(ùê±‚Çú‚Çã‚ÇÅ; Œº_Œ∏(ùê±‚Çú,t), Œ£_Œ∏(ùê±‚Çú,t)) \\\\ = N(ùê±‚Çú‚Çã‚ÇÅ; Œº_Œ∏(ùê±‚Çú,t), Œ≤ùêà)$$\nwhere Œ£ is fixed as Œ≤‚Çúùêà, and only the mean $Œº_Œ∏(ùê±‚Çú,t)$ will be learned and represented by a network (output) through the MSE loss of noise as below.\nFor the (\u0026ldquo;reversed\u0026rdquo;) forward pass, the distribution of noise-added image $q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ)$ has a closed-form solution, which can be written as a similar expression as p(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú): What\u0026rsquo;s the derivation?\n$$ q(ùê±‚Çú‚Çã‚ÇÅ|ùê±‚Çú, ùê±‚ÇÄ) = N(ùê±‚Çú‚Çã‚ÇÅ; \\tilde Œº‚Çú(ùê±‚Çú,ùê±‚ÇÄ), \\tilde Œ≤‚Çúùêà) \\\\ \\ \\\\ \\tilde Œ≤‚Çú = \\frac{1- \\bar Œ±‚Çú‚Çã‚ÇÅ}{1-\\bar Œ±‚Çú} ‚ãÖ Œ≤‚Çú \\\\ \\ \\\\ \\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ) = \\frac{\\sqrt{Œ±‚Çú} (1-\\bar Œ±‚Çú‚Çã‚ÇÅ) }{1-\\bar Œ±‚Çú} ùê±‚Çú \\ + \\frac{\\sqrt{\\bar Œ±‚Çú‚Çã‚ÇÅ} Œ≤‚Çú}{1-\\bar Œ±‚Çú} ùê±‚ÇÄ \\\\ \\ \\\\ \\rm Is there \\sqrt{Œ±‚Çú} or \\sqrt{\\bar Œ±‚Çú} ? $$\nwhere the $\\tilde Œ≤‚Çú$ is fixed, so only consider the $\\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ)$, which can be simplified by the one-step forward process expression: $ùê±‚Çú = \\sqrt{\\bar Œ±‚Çú} ùê±‚ÇÄ + \\sqrt{1 - \\bar Œ±‚Çú} Œµ$\n$$ ùê±‚ÇÄ = \\frac{ùê±‚Çú - \\sqrt{1 - \\bar Œ±‚Çú} Œµ}{\\sqrt{\\bar Œ±‚Çú}} $$\nPlug ùê±‚ÇÄ into $\\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ)$, then the mean of the noise-added image doesn\u0026rsquo;t depend on ùê±‚ÇÄ anymore:\n$$ \\begin{aligned} \\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ) \u0026amp; = \\frac{\\sqrt{Œ±‚Çú} (1-\\bar Œ±‚Çú‚Çã‚ÇÅ) }{1-\\bar Œ±‚Çú} ùê±‚Çú \\ + \\frac{\\sqrt{\\bar Œ±‚Çú‚Çã‚ÇÅ} Œ≤‚Çú}{1-\\bar Œ±‚Çú} \\ \\frac{ùê±‚Çú - \\sqrt{1 - \\bar Œ±‚Çú} Œµ}{\\sqrt{\\bar Œ±‚Çú}} \\\\ \\ \\\\ \u0026amp; = ???\\ How \\ to \\ do? \\ ??? \\\\ \u0026amp; = \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ) \\end{aligned} $$\nThe mean of the distribution from which the noise-added image (ùê±‚Çú,ùê±‚ÇÄ) at timestep t get sampled out is subtracting some random noise from image ùê±‚Çú.\nùê±‚Çú is known from the forward process schedule, and the $\\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ)$ is the target for the network to optimize weights to make the predicted mean $Œº_Œ∏(ùê±‚Çú,t)$ same as $\\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ)$.\nSince network only output Œº, the KL-divergence in the loss function can be simplified in favor of using MSE:\n$$ L‚Çú = \\frac{1}{2œÉ‚Çú¬≤} \\| \\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ) - Œº_Œ∏(ùê±‚Çú,t) \\|¬≤ $$\nThis MSE indicates that the noise-added image in the forward process and the noise-removed image in the reverse process should be as close as possible.\nSince the actual mean $\\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ) = \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ)$, where ùê±‚Çú is known, as it\u0026rsquo;s the input to the network. So the model is essentially estimating the actual $Œµ$ (random noise) every time.\nHence, the predicted mean $Œº_Œ∏(ùê±‚Çú,t)$ by the model can be written in the same form as $\\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ)$, where only the noise $Œµ_Œ∏$ has parameters:\n$$Œº_Œ∏(ùê±‚Çú,t) = \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ_Œ∏((ùê±‚Çú,t)))$$\nTherefore, the loss term becomes:\n$$ L‚Çú = \\frac{1}{2œÉ‚Çú¬≤} \\| \\tilde Œº(ùê±‚Çú,ùê±‚ÇÄ) - Œº_Œ∏(ùê±‚Çú,t) \\|¬≤ \\\\ \\ = \\frac{1}{2œÉ‚Çú¬≤} \\left\\| \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ) \\ - \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ_Œ∏(ùê±‚Çú,t)) \\right\\|¬≤ \\\\ \\ = \\frac{Œ≤‚Çú¬≤}{2œÉ‚Çú¬≤ Œ±‚Çú (1-\\bar Œ±‚Çú)} \\|Œµ - Œµ_Œ∏(ùê±‚Çú,t) \\|¬≤ $$\nDisregarding the scaling factor can bring better sampling quality and easier implementation, so the final loss for the KL-divergence is MSE between actual noise and predicted noise at time t:\n$$\\|Œµ - Œµ_Œ∏(ùê±‚Çú,t) \\|¬≤$$\nOnce the mean $Œº_Œ∏(ùê±‚Çú,t)$ has predicted out based on ùê±‚Çú and t, a \u0026ldquo;cleaner\u0026rdquo; image can be sampled from the distribution:\n$$ N(ùê±‚Çú‚Çã‚ÇÅ; Œº_Œ∏(ùê±‚Çú,t), \\sigma_Œ∏(ùê±‚Çú,t)) = N(ùê±‚Çú‚Çã‚ÇÅ; \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ_Œ∏(ùê±‚Çú,t), Œ≤‚Çúùêà) $$\nBy using reparameterization trick, this sampled image is: $$ ùê±‚Çú‚Çã‚ÇÅ = Œº_Œ∏(ùê±‚Çú,t) + œÉŒµ \\ = \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ_Œ∏(ùê±‚Çú,t) + \\sqrt{Œ≤‚Çú}Œµ $$\nThe last term $log p(ùê±‚ÇÄ|ùê±‚ÇÅ)$ in the VLB is the predicted distribution for the original image ùê±‚ÇÄ. Its goodness is measured by a probability that the original image $ùê±‚ÇÄ$ gets sampled from the estimated distribution $N(x; Œº_Œ∏‚Å±(ùê±‚ÇÅ,1), Œ≤‚ÇÅ)$.\nThe probability of an image should be a product of total D pixels. And the probability a pixel should be an integral over an interval [Œ¥‚Çã, Œ¥‚Çä] of the PDF curve:\n$$ p_Œ∏(ùê±‚ÇÄ|ùê±‚ÇÅ) = ‚àè_{i=1}^D ‚à´_{Œ¥‚Çã(x‚ÇÄ‚Å±)}^{Œ¥‚Çä(x‚ÇÄ‚Å±)} N(x; Œº_Œ∏‚Å±(ùê±‚ÇÅ,1), Œ≤‚ÇÅ) dx $$\nwhere $x‚ÇÄ$ is the pixel\u0026rsquo;s ground-truth. $N(x; Œº_Œ∏‚Å±(ùê±‚ÇÅ,1), Œ≤‚ÇÅ)$ is the distribution to be integrated. This interval is determined based on the actual pixel value as:\n$$ Œ¥‚Çä(x) = \\begin{cases} ‚àû \u0026amp; \\text{if x = 1} \\\\ x+\\frac{1}{255} \u0026amp; \\text{if x \u0026lt; 1} \\end{cases}, \\quad Œ¥‚Çã(x) = \\begin{cases} -‚àû \u0026amp; \\text{if x = -1} \\\\ x-\\frac{1}{255} \u0026amp; \\text{if x \u0026gt; -1} \\end{cases} $$\nThe original pixel range [0,255] has been normalized to [-1, 1] to align with the standard normal distribution $p(x_T) \\sim N(0,1)$\nIf the actual value is 1, the integral upper bound in the distribution is ‚àû, and the lower bound is 1-1/255 = 0.996, the width of the interval is from 0.996 to infinity.\nIf the actual value is 0.5, the upper bound is 0.5+1/255, and the lower bound is 0.5-1/255, the width of the interval is 2/255.\npic: area of the true pixel region in two predicted distributions.\nIf the area around the actual pixel value under the predicted distribution PDF curve is large, the predicted distribution is good. Howerver, if the area around real pixel value is small, the estimated mean is wrongly located.\nHence, this probability (log-likelihood) should be maximized, and by condering the minus sign in front of it, the corresponding loss term comes.\nHowever, the authors got rid of this loss term $-log p(ùê±‚ÇÄ|ùê±‚ÇÅ)$ when training the network. And the consequense is at inference time, the final step from ùê±‚ÇÅ to ùê±‚ÇÄ doesn\u0026rsquo;t add noise, because this step wasn\u0026rsquo;t get optimized. Therefore, The difference from other sampling steps is that the predicted ùê±‚ÇÄ doesn\u0026rsquo;t plus random noise.\n$$ \\begin{aligned} \\text{t\u0026gt;1:}\\quad ùê±_{t-1} \u0026amp;= \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ_Œ∏(ùê±‚Çú,t) + \\sqrt Œ≤‚Çú Œµ) \\\\ \\text{t=1:}\\quad ùê±_{t-1} \u0026amp;= \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{Œ≤‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ_Œ∏(ùê±‚Çú,t)) \\end{aligned} $$\nA simple reason is that we don\u0026rsquo;t want to add noise to the final denoised clear output image ùê±‚ÇÄ. Otherwise, the generated image is low-quality.\nThe complete loss function is MSE:\n$$ \\begin{aligned} \\rm L_{simple} \u0026amp;= E_{t,ùê±‚ÇÄ,Œµ} [ || Œµ - Œµ_Œ∏(ùê±‚Çú,t)||¬≤ ] \\\\ \u0026amp;= E_{t,ùê±‚ÇÄ,Œµ} [ || Œµ - Œµ_Œ∏( \\sqrt{\\bar a‚Çú} ùê±‚ÇÄ + \\sqrt{1 - \\bar a‚Çú} Œµ, t) ||¬≤ ] \\end{aligned} $$\nt is sampled from a uniform distribution between 1 and t; ùê±‚Çú is the one-step forward process. Algorithms DDPM paper\nTraining a model:\n\\begin{algorithm} \\caption{Training} \\begin{algorithmic} \\REPEAT \\STATE Sample a t from U(0,T) \\STATE Select an input image ùê±‚ÇÄ from dataset \\STATE Sample a noise from N(0,ùêà) \\STATE Perform gradient descent with loss: \\\\\\\\ $||Œµ - Œµ_Œ∏(\\sqrt{\\bar a‚Çú} ùê±‚ÇÄ + \\sqrt{1 - \\bar a‚Çú} Œµ, t)||¬≤$ \\UNTIL{converge} \\end{algorithmic} \\end{algorithm} Sampling from the learned data distribution by means of reparameterization trick:\n\\begin{algorithm} \\caption{Sampling} \\begin{algorithmic} \\STATE Sample a noise image $ùê±_T \\sim N(0,ùêà)$ \\FOR{t = T:1} \\COMMENT{Remove noise step-by-step} \\IF{t=1} \\STATE Œµ=0 \\ELSE \\STATE Œµ ~ N(0,ùêà) \\ENDIF \\STATE $ùê±‚Çú‚Çã‚ÇÅ = \\frac{1}{\\sqrt{Œ±‚Çú}} (ùê±‚Çú - \\frac{1-Œ±‚Çú}{\\sqrt{1 - \\bar Œ±‚Çú}} Œµ_Œ∏(ùê±‚Çú,t) + \\sqrt{œÉ‚Çú}Œµ$ \\COMMENT{Reparam trick} \\ENDFOR \\RETURN ùê±‚ÇÄ \\end{algorithmic} \\end{algorithm} In this reparametrization formula change Œ≤‚Çú and $\\sqrt{Œ≤‚Çú}$ to 1-Œ±‚Çú and œÉ‚Çú, which are different from the above equation.\nTraining and Sampling share the common pipeline:\nùê± t ‚Çú ‚Äñ U Œµ N ‚ãÆ - e Œµ t _ Œ∏ ‚Äñ ¬≤ Œµ _ Œ∏ ‚ãØ ‚ñ∂ Œº _ Œ∏ ( ùê± ‚Çú , t ) ‚ãØ ‚ñ∂ ùê± ‚Çú ‚Çã ‚ÇÅ Improvements Improvements from OpenAI\u0026rsquo;s 2021 papers.\nLearn a scale factor for interpolating the upper and lower bound to get a flexible variance:\n$$Œ£_Œ∏(x‚Çú,t) = exp(v\\ log Œ≤‚Çú +(1-v)\\ log(1- \\tilde{Œ≤‚Çú}))$$\nv is learned by adding an extra loss term $Œª L_{VLB}$, and Œª=0.001.\n$$L_{hybrid} = E_{t,ùê±‚ÇÄ,Œµ} [ || Œµ - Œµ_Œ∏(ùê±‚Çú,t)||¬≤ ] + Œª L_{VLB}$$\nUse cosine noise schedule $f(t)=cos(\\frac{t/T+s}{1+s}‚ãÖœÄ/2)¬≤$ in favor of linear schedule.\nReference Deep Unsupervised Learning using Nonequilibrium Thermodynamics, 2015 Denoising Diffusion Probabilistic Models, 2020 Improved Denoising Diffusion Probabilistic Models, 2021 Feb Diffusion Models Beat GANs on Image Synthesis, 2021 May ","date":"2023-07-14T21:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/d-vid-outlier/","title":"watch: Diffusion - Outlier | Explain 4 papers"},{"content":"Explicit Correspondence Matching for Generalizable Neural Radiance Fields\nCode | Arxiv\nNotes Idea Take the difference between reference image features of a 3D point as geometry prior.\n(2023-10-25) Method is like a simplified GPNR: feature differences between views measued by cosine similarity (dot product). However, GPNR is more ultimate, while MatchNeRF only computes differences for each two views and differences are taken averaged as the input feature.\n(2023-12-09) Image features alone cannot constrain geometry of the unseen scenes well. In contrast, MVSNet preset depths explicitly.\nAnd MVSNet used the variance of features to measure the depth misalignment among all feature maps, whereas MatchNeRF used differences of each 2 feature maps.\nPipeline CNN extract image feature (1/8 reso), which will be upsampled to (1/4 reso), for each reference view.\nSelect a pair of reference views and Mix their feature maps by cross-attention (GMFlow).\nProject a 3D point onto this pair of interacted feature maps.\nMeasuring the difference of feature vectors by cosine similarity (dot prodcut)\nThe feature difference indicates whether the 3D point is at surface, so that it provides a geometry prior. Dot product of two feature vectors is a scalar, which will lost much information. So they divided the total channel into many groups to do \u0026ldquo;group dot product\u0026rdquo;. And concatenate the dot product of each group as a vector ùê≥.\nAlso, for the 1/4 feature map, there is a \u0026ldquo;dot-products\u0026rdquo; vector $\\^ùê≥$ for a pair of reference views.\nGiven ùëÅ reference views, there are ùëÅ(ùëÅ-1)/2 pairs of reference views, corresponding to ùëÅ(ùëÅ-1)/2 \u0026ldquo;difference\u0026rdquo; vectors, which will merge together by taking their element-wise average as a single ùê≥\nThis \u0026ldquo;feature difference\u0026rdquo; vector ùê≥ (geometry prior) is fed along with the 3D point\u0026rsquo;s position and viewdir into decoder (MLP and ray-transformer), which regresses the color and volume density.\nExperiments Settings are following MVSNeRF.\nDatasets:\nStage Data Contents Resolution N_views Train DTU 88 scenes 512x640 49 Test DTU 16 scenes 3 Test NeRF real 8 scenes 640x960 4 Test Blender 8 scenes 800x800 4 Device: 16G-V100 Play (2023-08-24)\nCompare with GNT The architectures of Match-NeRF and GNT are similar.\n(2023-12-09) Overview: Souce images\u0026rsquo; features are extracted, mixed and regressed to rgbœÉ. Match-NeRF is trained only on DTU dataset, while GNT can be trained on multiple datasets (gnt_full).\nGNT merges multiple source views via subtract attention, while Match-NeRF fuses multi-view feature maps before getting into the model.\nMatch-NeRF mixes the entire feature maps for each two reference views, and then project 3D points onto the fused feature maps to index feature vectors.\nHowever, GNT directly mixes point\u0026rsquo;s feature vectors coming from each feature maps.\nDifferent training settings:\nHyper-params GNT MatchNeRF #rays for grad-descnt 2048 1024 #source views 8~10 3 1080Ti only supports --nerf.rand_rays_train=512 for MatchNeRF.\nThe opts.batch_size will be divided evenly to each gpu (self.opts.batch_size // len(self.opts.gpu_ids)), so bs (num of images)=1 cannot be split to multiple GPUs.\nAnd if setting bs=2, each card still have to process 1024 rays selected from an image.\nTesting with 1 1080Ti:python test.py --yaml=test --name=matchnerf_3v --nerf.rand_rays_test=10240\n(2023-09-27)\nCode Details GMFlow uses 6 transformer blocks consisting of self_attn and cross_attn for fusing windows, where the 1st and odd blocks perform window shift.\nMatchNeRF fully-finetuned the pre-trained GMFlow.\nDoes Inner product of a pair of features come from GMFlow?\n(2023-10-25) I guess it\u0026rsquo;s a simplified attention only for pairs, instead of among all views.\n(2023-12-09) Inferring geometry from the difference in high-dimensional features may have been present even earlier than MVSNet.\nSelf-attn and cross-attn for two samples data1 and data2 can be done in a single transformer block of GMFlow by concating 2 samples in the batch dimension twice in different order, i.e., source=[\u0026lsquo;data1\u0026rsquo;,\u0026lsquo;data2\u0026rsquo;] and target=[\u0026lsquo;data2\u0026rsquo;,\u0026lsquo;data1\u0026rsquo;].\nSuch that self-attn is performed on source and source. And cross-attn is source and target. If fused source returned after a block, the order requires reverse again to form the new target.\nviewdir didn\u0026rsquo;t perform positional embedding (same as PixelNeRF).\nRay transformer (MHA) in decoder mixes 16-dim feature vectors. (Unexpectedly tiny)\nTake the nearest views.\nCoordinates of 3D points are projected onto the image plane of the source view 0 to do positional embedding. Code\nThe encoder is supposed to provide the overall (geometry) prior, so they emphasized in the paper:\ndo not tune encoder for per-scene fine tuning\n","date":"2023-07-14T14:59:00Z","image":"https://i.imgur.com/rQN8Eg5.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-matchnerf/","title":"read: Render - NVS | Match-NeRF"},{"content":"Source video: Ëã±‰ºüËææÈ´ò‰øä: AIÈ´òË¥®Èáè‰∏âÁª¥ÂÜÖÂÆπÁîüÊàêÔºàÂÜÖÂÆπÁîüÊàêÁ≥ªÂàó„Äê‰∏Ä„ÄëÔºâ Âåó‰∫¨Êô∫Ê∫êÂ§ß‰ºö2023 ËßÜËßâ‰∏éÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã\nThe representation of 3D objects Implicit field is in favor of neural network, where it can be optimized by gradient. mesh can achieve real-time rendering and is handy for downstream creation, and good topology. Marching cube is not fully differentiable DMTet: A differentiable iso-surfacing is an implict field, and also a mesh.\nAn field where only the location at surface has value? a field only has one mesh? Diff-render 2D images supervise 3D generation 2D GAN advantages:\nvarious discriminator architecture powerful generator GAN3D\nThe latent codes of geometry and texture are sampled from 3D gaussian as prior 3D generator: Tri-plane consistute the implicit field. Get a mesh by DMTet from the generated geometry and texture, then render it to 2D image Use GAN to discriminate if the render is real and backward the gradient of loss Limitation: class label conditioned. One model can only can generate 1 category of objects. Text prompts generate 3D objects 2D diffusion used socre function to encourage high-fadality images score function needs a full image, but NeRF are trained batch-by-batch of rays, not a full image. Dream fusion can only render 64x64 images, so its geometry is low-quality. Coarse to fine: Use instant-ngp generate a rough geometry based on low-resolution diffusion modelÔºå then use DMTet convert the geometry to mesh; So that a highe-resolution image can be rendered, which can offer a strong gradient for fine geometry Future work a universal model can generate any category of objects. composite objects to form a scene dynamic objects ","date":"2023-07-13T18:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/%E9%AB%98%E8%B4%A8%E9%87%8F3d%E5%86%85%E5%AE%B9%E5%88%9B%E9%80%A0-%E9%AB%98%E4%BF%8A/","title":"watch: Jun Gao | ML for 3D content generation"},{"content":"Course page\nSampling Steps:\nSample a random noise image from normal distribution;\nUse the trained network to predict the noise as opposed to the meaningful object for one step;\nUse DDPM algorithm to compute noise-level scaling factors given a timestep: s1, s2, s3 = ddpm_scaling(t)\nSubtract the predicted noise from noise image and add extra noise, sample = s1 * (sample - s2 * predicted_noise) + s3 * extra_noise\nRepeat steps 2 to 4 to remove noise progressively.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 samples = torch.randn(N_imgs, 3, height, height) timesteps = 500 for i in range(timesteps, 0, -1): # reshape to match input image (N_imgs, 3, height, height) t = torch.tensor( [i/timesteps] )[:, None, None, None] # extra noise (except for the first step) z = torch.randn_like(samples) if i \u0026gt; 1 else 0 # predict noise eps = nn_model(samples, t) # remove noise and add extra noise samples = denoise_add_noise(samples, i, eps, z) Adding extra noise before removing noise in the next step avoids collapsing to the average thing of the training dataset.\nUNet UNet can output images of the same size as the input, and assign the image feature onto each pixel.\nCompress image for compact representation; Down sampling once, number of channel doubles Also UNet allows incorporating addtional information during the decoding period.\nEach time up-sampling, the sample is multiplied with context embeddings and plus time embeddings.\nTime embedding indicates timestep of the feature vector, so with that, the \u0026ldquo;time-dependent\u0026rdquo; noise level can be determined.\nContext embedding can be text description, so the UNet will be guided to generate specific output.\nf e ( h a 4 i t d u c d e h e r n n l m s a ) p s u a p m - p l e ( e e 2 C m m u o b T b p c n e i e 1 h t d m d n e d e d l x i i s n n ) g g ‚®Ç ‚®Å s u a p m - p l e ( 1 u p c 2 h n l ) 1 2 3 4 5 # embed context and timestep cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1) temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1) up2 = self.up(cemb1 * up1 + temb1, down2) (2023-07-10)\nTraining Train the UNet to identify the noise that was applied to the image.\nUNet can segment image (classify each pixel), so here is it used to identify whether every pixel is noise or not?\nNo, it\u0026rsquo;s used to make each pixel carried with extracted or introduced features. Training steps:\nSample a random timestep (noise-level) to make noise; Add the known noise onto a random training image; UNet takes as input the noise image and predicts the applied noise as output; Loss is the difference between the true noise and the predicted noise 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for ep in range(n_epoch): for x in dataloader: # perturb data t = torch.randint(1, timesteps + 1, (x.shape[0],) ).to(device) noise = torch.randn_like(x) x_pert = perturb_input(x, t, noise) # use network to recover noise pred_noise = nn_model(x_pert, t / timesteps) # loss is MSE loss = F.mse_loss(pred_noise, noise) optim.zero_grad() loss.backward() optim.step() (2023-07-11)\nControling Use embedding vector to control the predicted noise.\nEmbedding is a vector (a set of numbers) that is a representation for something in another space.\nEmbeddings can perform arithmetic operations.\nParis : - France : + England : = London : Noise is what should be removed from the image.\nOnce the noise is fully subtracted out, what left is the generated image. By injecting context embeddings into decoder, the output feature vector of the predicted noise becomes specific for that given context.\nFor example, the noise corresponding to \u0026ldquo;A ripe avocado\u0026rdquo; is pixels that are not \u0026ldquo;A ripe avocado\u0026rdquo;, and they\u0026rsquo;ll be removed eventually.\n' a A v o r c i n i a m o p d g i e o ' ‚®Å E C m o b n e U t d - e d N x i e n t g p n r o e i d l o s s And because the embedding vectors can be mixed, once the mixed noise is removed, what is left is the combination of two objects, i.e. the thing that the context embedding stands for.\nFor example, an embedding vector of \u0026ldquo;Avocado armchair\u0026rdquo; has the information of both \u0026ldquo;avocado\u0026rdquo; and \u0026ldquo;armchair\u0026rdquo;, so its context will lead the model to predict the noise that is neither \u0026ldquo;avocado\u0026rdquo; nor \u0026ldquo;armchair\u0026rdquo;.\n' a A r v m o c n c h o a a i d i o r ' C E U o m - n b N t e e e d t x d i n g s t n e o p i ‚äñ i m g Context can be one-hot encoded vector for indicating categories, which will result in a specific class of images.\nSpeeding Up DDIM skips some timesteps, so it breaks Markov chain process, where each timestep is probablisticly dependent on the previous one.\nThere is a hyper-parameter step_size to decide how many timesteps are skipped.\nDDIM performs better than DDPM under 500 timesteps. The quality of images from DDIM may differs as opposed to DDPM.\nDenoising Diffusion implict model predicts a \u0026ldquo;rough sketch\u0026rdquo; of the final output, and then it refines it with the denoising process.\n","date":"2023-07-09T17:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/d-vid-dlai/","title":"watch: DM - DLAI | How Diffusion Models Work"},{"content":"Code-matlab | Code-python | paper\nIntuitive from Shakil Goal Find the true matched keypoints and identify the outliers.\nAffinity matrix Measure the difference between any two edges in two graphs.\nG r a 1 2 p h - P G r a a b c p h - Q c is the outlier. Nodes can be SIFT feature points.\n$$W_{1a,2b} = 12 - ab$$\nThe index of each element is a notation representing if i was a and if j was b.\nW 1a 1b 1c 2a 2b 2c 1a 0 1b 0 1c 0 2a 0 2b 0 2c 0 Random walk Iterations will not walk out of the area, i.e., the middle area has higher probability.\na b c 1 1 0 0 2 0 1 0 probability node 0.75 a 0.1 b 0.05 c 0.25 a 0.5 b 0.25 c The initial probility for each point is the same: 1/6:\nprobability node 0.16 a 0.16 b 0.16 c 0.16 a 0.16 b 0.16 c Associate graph (2023-07-09)\nAbstract Association graph consistitue of nodes representing correspondence between two graphs. Random walk select nodes to enforce real correspondence on the association graph. Introduction Previous work didn\u0026rsquo;t make a objective function\nIQP is NP hard, so its solution needs approximation.\nRandom walk view: rank the nodes on the association graph.\nPrevious methods\nTensor eigen decomposition Problem Each graph have a set of nodes (vertices) ùêï, edges ùêÑ, and attributes ùêÄ.\nnode is local appearance of feature edge is geometrical relationship of two nodes. Matching two graphs is to find the correspondence of nodes in two graphs\nAffinity matrix ùêñ is recording the compatibility of pairs of edges\ndiagonal is unary affinity, e.g. $ùêñ_{ia;ia}$, between one correspondence and itself. non-diagonal $ùêñ_{ia;jb}$ is the affinity of a pair of correspondence: $(v·µ¢·¥æ,\\ v‚Çê^Q)$ and $(v‚±º·¥æ,\\ v_b^Q)$ W ia ib ic ja jb jc ia ib ic ja jb jc Each row is fixing one correspondence in one pair of correspondences, and changing the other correspondence.\nFor example, 1 corresponds to a is fixed, then the neighbor of 1 and a is changing:\nf i 1 1 x c h a 1 2 n g e f i a a a x c h a a b c n g e The correspondence assignment is stored in assignment matrix ùêó;\nwhere 1 means matched correspondence, while 0 means the two nodes are not matched. x a b c i j Column vector ùê± is the reshape of matrix ùêó with length of $n·¥æ√ón^Q$\nx_ia x_ib x_ic x_ja x_jb x_jc Indicator vector ùê±* is the target by maximizing the score of ùê±·µÄùêñùê±\nRandom Walks for Graph Matching Convert the affinity matrix to an association graph for random walk\nAssociation graph $G^{rw}$ is made up by nodes that represents a correspondence between $G·¥æ$ and $G^Q$.\nFor example, the correspondence $(v·µ¢·¥æ,\\ v‚Çê^Q)$ is node $v_{ia}$ on the association graph.\nso the edge attributes are the elements of affinity matrix ùêñ ;\nFor example, the edge $e_{ia; jb}$ on the association graph is the affinity $ùêñ_{ia;jb}$\nRanking the nodes of association graph by random walk process\nAffinity preserving random walk Random walk process:\nA walker starts off with an arbitrary node and select as the next step one of its out-going edges based on the Markov transition kernel.\nInternet democracy:\nTotal vote that every webpage has is 1. This is realized by dividing the weight of its every out-going edge by the total number of its out-going edges.\nRow stochastic:\nOn to the association graph, its edges set ùêñ is supposed to be normalized by ùêÉ. That is each row is divided by the sum of the affinity values in that row. Then the normalized affinity matrix is row stochastic matrix ùêè = ùêÉ‚Åª¬π, where ùêÉ is a diagonal matrix.\nHowever, because there are outliers on the association graph, which are suppose to have small weight, the Internet democracy doesn\u0026rsquo;t suit here.\nOutliers on the association graph are the mismatched correspondence.\nFor example, the actual corespondences are $(v_1^P, v_1^Q)$, and $(v_2^P, v_2^Q)$, which are nodes $v_{11}$ and $v_{22}$ on the association graph.\nTherefore, on the association graph, the outliers are nodes other than $v_{11}$ and $v_{22}$, i.e., $v_{12},\\ v_{13},\\ v_{23},\\ v_{21}$\nG ^ 1 2 P 1 G ^ Q 2 3 A s s o 1 c 3 i a t i o n g r a p h ","date":"2023-07-06T00:00:00Z","image":"https://cv.snu.ac.kr/research/~RRWM/paper_teaser.jpg","permalink":"https://zichen34.github.io/writenotes/model/misc/b-note-rrwm/","title":"read: RRWM"},{"content":"Source video: Lesson 3: Complex conjugates and dividing complex numbers - Khan Academy\nGiven a complex number\n$$ z = \\underset{‚Üë}{2} + \\underset{‚Üë}{\\underline{3}} i \\\\ \\quad \\quad Re(z) \\ Im(z) $$\nwhere $2$ is a real number, $3i$ is an imaginary number. The real part of z $Re(z)$ is 2, while the imaginary part of z $Im(z)$ is 3.\nComplex conjugate The complex conjugate of z is\n$$ \\bar z \\text{ or } z^* = 2 - 3i = \\overline{2+3i} $$\nwhere the real part Re(z*) remains the same, while the imaginary part Im(z*) has the opposite sign.\n- I b 0 b m a z z * R e This act like mirror reflecting over Real axis.\nThe sum of a complex number and its complex conjugate is two times of its real part, 2Re(z):\n$$ z + \\bar z = a + \\cancel bi + a - \\cancel bi = 2a = 2 Re(z) = 2 Re(\\bar z) $$\nGraphically, vector addition\n- I b 0 b m a z z * 2 ( a z + z * ) R e The product of a complex number and its complex conjugate is a real number, and it\u0026rsquo;s equal to the magnitude of the complex number squared:\n$$ z ‚ãÖ \\bar z = (a+bi) ‚ãÖ (a-bi) = a¬≤ - (bi)¬≤ = a¬≤ + b¬≤ = |z|¬≤ $$\nwhich is useful in the division of comple numbers: multiply the numerator and denominator by the conjugate of the denominator to convert the division to one complex number\nFor example:\n$$ \\begin{aligned} \u0026amp; \\frac{1+2i}{4-5i} \\\\ \u0026amp; = \\frac{1+2i}{4-5i} ‚ãÖ \\frac{4+5i}{4+5i} \\\\ \u0026amp; = \\frac{(1+2i)(4+5i)}{4¬≤-(5i)¬≤} \\\\ \u0026amp; = \\frac{4+5i + 8i-10}{16+25} \\\\ \u0026amp; = \\frac{-6}{41} + \\frac{13i}{41} \\end{aligned} $$\nFactoring sum of squares We can factor a difference of squares as:\n$$ x¬≤ - y¬≤ = (x-y)(x+y) $$\nBut the sum of squares x¬≤ + y¬≤ cannot be factorized if without considering imaginary unit i.\n$$ \\begin{aligned} x¬≤ + y¬≤ \u0026amp; = x¬≤ - (-y¬≤) \\\\ \u0026amp; = x¬≤ - (- 1 y¬≤) \\\\ \u0026amp; = x¬≤ - (i¬≤ y¬≤) \\\\ \u0026amp; = x¬≤ - (iy)¬≤ \\\\ \u0026amp; = (x-iy) (x+iy) \\end{aligned} $$\nModulus of complex value Source page Lesson 5\nThe absolute value (modulus) of a number is the distance away from zero.\nA complex number $3-4i$ plotted on the complex plane:\n4 I m 3 | 3 - 4 i | R e The absolute value of 3-4i is the hypotenuse of the right triangle.\nBased on the Pythagorean theorem, |3-4i|= ‚àö(3¬≤+4¬≤) = 5. (Because distance is positive, so only take the positive square root).\npolar \u0026amp; rectangular forms Source page: Lesson 5\nA complex number can be represented in two forms:\nRead number + imaginary number: a + bi Exponential form Both of them have the same diagram, and just are described in different coordinates:\nb I m r œÜ a z R e The two arguments of complex number z can be (r, œÜ) or (a,b)\nMagnitude: r = |z| = ‚àö(a¬≤+b¬≤)\nArgument (Polar angle): œÜ = arctan(b/a)\na = r‚ãÖcosœÜ\nb = r‚ãÖsinœÜ\nz = a+bi = r‚ãÖcosœÜ + r‚ãÖsinœÜi = r(cosœÜ + sinœÜ i) = $r e^{iœÜ}$\nwhere $cosœÜ + sinœÜ i = e^{iœÜ}$ can be derived by using Taylor series.\nMultiplying complex number Source page Lesson 7\nGiven z,\n3z has the same direction as z, but three times it\u0026rsquo;s magnitude;\n-3z is in the opposite direction, and has three times modulus of z\n-3iz = $1e^{iœÄ} ‚ãÖ 3e^{i‚ãÖœÄ/2} ‚ãÖ re^{iœÜ} = 3r e^{i(œÄ+œÄ/2+œÜ)}$ which turns into the opposite direction then rotates another 90 degrees in counter-clock wise.\nOr this can be derived from the loop of 1 -\u0026gt; i -\u0026gt; -1 -\u0026gt; -i with multiplying i each time.\nz‚ãÖ(-1-i) : the angle and modulus of $-1-i$ take effect on z separately.\nThe angle of -1-i is 225 degrees (observed from the complex plane), so it will rotate the z by 225 degrees.\nThe magnitude of -1-i is ‚àö2, so z will be scaled by ‚àö2.\n(I think it as a vector addition: z‚ãÖ(-1-i) = -z-zi,, but there\u0026rsquo;re too many steps).\nOperations in polar form Source page: Lesson 8\nMultiplication Given: $$ w‚ÇÅ = 3(cos(330¬∞) + i sin(330¬∞)) \\\\ w‚ÇÇ = 2(cos(120¬∞) + i sin(120¬∞)) $$\nWhat is w‚ÇÅ‚ãÖw‚ÇÇ ?\n$3 e^{i‚ãÖ330¬∞} ‚ãÖ 2 e^{i‚ãÖ120¬∞} = 6 e^{i(450¬∞)}$\nw‚ÇÅ‚ãÖw‚ÇÇ can be viewed as w‚ÇÅ transforming w‚ÇÇ, i.e., w‚ÇÇ is transformed by multiplying w‚ÇÅ\nThe modulus of w‚ÇÇ is scaled by the modulus of w‚ÇÅ (2), so |w‚ÇÅ‚ãÖw‚ÇÇ| = 3‚ãÖ2 = 6;\nThe argument (angle) of w‚ÇÅ is rotated by the argument of w‚ÇÅ (330¬∞), so arg(w‚ÇÅ‚ãÖw‚ÇÇ) = 120¬∞ + 330¬∞ = 450¬∞ = 90¬∞\nSo w‚ÇÅ‚ãÖw‚ÇÇ = 6 (cos(90¬∞) + i sin(90¬∞)) = 6i\nDivision $$ w‚ÇÅ = 8(cos( \\frac{4œÄ}{3} ) + i sin( \\frac{4œÄ}{3} )) \\\\ w‚ÇÇ = 2(cos( \\frac{7œÄ}{6} ) + i sin( \\frac{7œÄ}{6} )) \\\\ $$\nWhat is $\\frac{w‚ÇÅ}{w‚ÇÇ}$ ?\n$8e^{ i\\frac{4œÄ}{3} } / (2e^{ i\\frac{7œÄ}{6} }) = 4 e^{i(œÄ/6)}$\nAnother way to think about it is that w‚ÇÅ is transformed by w‚ÇÇ:\nthe modulus of w‚ÇÅ is divided by the modulus of w‚ÇÇ;\nthe argument of w‚ÇÅ is rotated clock-wise by the argument of w‚ÇÇ.\nmodulus argument |w‚ÇÅ|=8 arg(w‚ÇÅ)=4œÄ/3 |w‚ÇÇ|=2 arg(w‚ÇÇ)=7œÄ/6 |w‚ÇÅ/w‚ÇÇ|=4 arg(w‚ÇÅ/w‚ÇÇ) = 4œÄ/3 - 7œÄ/6 = œÄ/6 So $\\frac{w‚ÇÅ}{w‚ÇÇ} = 4(cos(œÄ/6) + i sin(œÄ/6)) = 4(‚àö3/2 + i/2)$\nPowers of complex number Consider the complex number $z = -1 + i \\sqrt 3$. Find $z^4$ in polar and rectangular form.\nModulus of z is 2, so its modulus is times itself four times: 2‚Å¥ = 16 Argument of z is œÜ = $\\rm arctan(\\sqrt 3)$=60¬∞=120¬∞, so its angle rotate by 4 times of its argument: œÜx4 = 480¬∞ = 120¬∞ The polar form is $z‚Å¥ = 16 (cos(120¬∞) + i sin(120¬∞))$, so the rectangular form is $z‚Å¥ = 16(1/2 + i ‚àö3/2) = 8 + i8‚àö3 $\nComplex number equations Given equation x¬≥=1, find all of the real and/or complex roots of this equation.\nFor a real number: $z= 1 = 1 + 0i = 1e^{i0¬∞}$, its argument arg(z) can be 0, 2œÄ, 4œÄ, \u0026hellip;, i.e., $1 = e^{i0} = e^{i2œÄ} = e^{i4œÄ} = e^{i6œÄ}$ \u0026hellip;\nPlug these exponential form into x¬≥=1:\nx¬≥=1 x¬≥=$e^{i2œÄ}$ x¬≥=$e^{i4œÄ}$ x¬≥=$e^{i6œÄ}$ cube root x=1 x=$e^{i2œÄ/3}$ x=$e^{i4œÄ/3}$ x=$e^{i6œÄ/3}$ modulus 1 1 1 1 angle 0 2œÄ/3 = 120¬∞ 4œÄ/3 = 240¬∞ 2œÄ root x‚ÇÅ x‚ÇÇ x‚ÇÉ redundant a+bi 1 -1/2 + i‚àö3/2 -1/2 - i‚àö3/2 1 x x ‚ÇÇ ‚ÇÉ I m x 1 ‚ÇÅ R e Fundamental theorem of Algebra Source page: Lesson 9\nThe Fundamental theorem of Algebra: a n-th degree polynomial has n roots.\n$P(x) = ax^n + bx^{n-1} + \u0026hellip; + K$\n","date":"2023-06-26T11:29:00Z","permalink":"https://zichen34.github.io/writenotes/calc/complex_conjugates/","title":"watch: Khan | Complex Number"},{"content":"Real Python Tutorial\nConstruct path Make the filename an object instead of strings.\n1 2 3 4 5 6 7 from pathlib import Path # Instantiate an object: datadir = Path(\u0026#34;./datadir\u0026#34;) # Joining Paths new_path = datadir / \u0026#34;sampled.npy\u0026#34; Iterate dir .iterdir() method iterates over all the files in the given directory\n1 2 3 4 5 from pathlib import Path from collections import Counter # count number of files of different types Counter(path.suffix for path in Path.cwd().iterdir()) Return: Counter({'.md': 2, '.txt': 4, '.pdf': 2, '.py': 1})\n.glob(\u0026quot;*.txt\u0026quot;) returns all the files with a .txt suffix in the current directory.\n1 2 \u0026gt;\u0026gt;\u0026gt; Counter(path.suffix for path in Path.cwd().glob(\u0026#34;*.p*\u0026#34;)) Counter({\u0026#39;.pdf\u0026#39;: 2, \u0026#39;.py\u0026#39;: 1}) .rglob() recursively find all the files in both the directory and its subdirectories.\n1 2 3 4 5 6 def tree(Path(directory)): print(f\u0026#34;+ {directory}\u0026#34;) for path in sorted(directory.rglob(\u0026#34;*\u0026#34;)): depth = len(path.relative_to(directory).parts) spacer = \u0026#34; \u0026#34; * depth print(f\u0026#34;{spacer}+ {path.name}\u0026#34;) glob two patterns python - How to glob two patterns with pathlib? - Stack Overflow\n1 2 3 4 5 6 7 8 9 10 11 12 13 from pathlib import Path exts = [\u0026#34;.jl\u0026#34;, \u0026#34;.jsonlines\u0026#34;] mainpath = \u0026#34;/path/to/dir\u0026#34; # Same directory files = [p for p in Path(mainpath).iterdir() if p.suffix in exts] # Recursive files = [p for p in Path(mainpath).rglob(\u0026#39;*\u0026#39;) if p.suffix in exts] # \u0026#39;files\u0026#39; will be a generator of Path objects, to unpack into strings: list(files) ","date":"2023-06-25T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_pathlib/","title":"memo: Python | pathlib"},{"content":"Overview Source video: Fourier Analysis: Overview\nMost major breakthrough started with coordinate transformation. (e.g., Theory of relativity)\nFourier transform is another coordinate transformation\nFourier derived Fourier series as a way of approximating the solutions of partial differential equations, e.g., Heat equations.\nA rectangular metal has a temperature distribution u(x,y,t) in 2 dimensions, which is governed by the heat equation:\n$$ \\frac{‚àÇu}{‚àÇt} = Œ± (\\frac{‚àÇ¬≤u}{‚àÇx¬≤} + \\frac{‚àÇ¬≤u}{‚àÇy¬≤}) = Œ± ‚àá¬≤u $$\nFourier Transform diagonalize the laplacian operator in the heat equation. That means the laplacian operator has eigen values and eigen functions, just like other linear operators do.\nThe eigen functions are sines and cosines with a particular frequency determined by the boundary conditions and geometry of this object. The eigen values are those spatial frequencies. An arbitrary function can be approximated by a sum of sines and cosines of increasing frequencies\nf s ( i x n ) ( w t ) ‚Äñ ‚úö ‚úö ‚ãÆ These sines and cosines form an orthogonal basis for the space of possible functions. (ËØª‰Ωú‚Äú‰∏Ä‰∏™Âü∫‚ÄùÔºåËÄå‰∏çÊòØ‚Äú‰∏ÄÁªÑÂü∫‚Äù) For example, in a vector space, the x-axis and y-axis form the basis for 2-D vector space.\nFast Fourier Transform (FFT) computes Fourier series efficiently to analyze and process data.\nFourier Series-1 Source video: Fourier Series: Part 1\n(2023-11-19) Series means a list of number, where the meaning of each term is implied. Thus in Fourier Series, each term stands for a basis, which is a trigonometric functions of a increasingly high frequence.\nWhile Fourier Transform is a function that gives an aribitrary number in the \u0026ldquo;series\u0026rdquo;.\nFourier series approximates arbitrary functions f(x) as a infinite sum of sines and cosines of increasingly high frequency.\n(2023-11-19) For 1D signal, sine (or cosine) is enough. For 2D signal, sine and cosine are both needed as they\u0026rsquo;re orthogonal. Considering f(x) is 2œÄ-periodic:\nf ( x ) = - œÄ œÄ The function f(x) can be represented as a sum from k=1 to infinity of 2œÄ-periodic sines and cosines of increasingly high frequency:\n$$ f(x) = A‚ÇÄ/2 + ‚àë‚Çñ‚Çå‚ÇÅ^‚àû (A‚Çñcos(kx) + B‚Çñsin(kx) ) $$\nwhere\nA‚Çñ, B‚Çñ are Fourier coefficients, which indicate how much of each sine and cosine is needed to add up to recover f(x);\nk stands for k-th frequency.\nWhen k=0, cos0=1, sin0=0, so there\u0026rsquo;s a constant A‚ÇÄ\nThese sine and cosine waves are 2œÄ periodic.\nCoefficients are inner products These Fourier coefficients can be computed as inner products (in Hilbert space) of the function f(x) with particular wave:\n$$ A‚Çñ = 1/œÄ ‚à´_{-œÄ}^œÄ f(x) cos(kx) dx\\\\ B‚Çñ = 1/œÄ ‚à´_{-œÄ}^œÄ f(x) sin(kx) dx $$\nMore explicitly, A‚Çñ, B‚Çñ are the inner products of f(x) with the k-th wave function normalized by the magnitude of (co)sine function\ndoubt: Isn\u0026rsquo;t the magnitude should be square-rooted? Why is it divided by the norm squared? (2023-07-11) ÂÇÖÈáåÂè∂Á≥ªÊï∞ÁöÑÂΩí‰∏ÄÂåñÈô§‰ª•‰∫Ü‰∏§‰∏™Âü∫ÂêëÈáèÁöÑÊ®°ÈïøÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Êù•Ëá™‚ÄúÊ±ÇÂÇÖÈáåÂè∂Á≥ªÊï∞‚ÄùÊó∂Ôºåf Ë¶Å‰∏é Âçï‰Ωç Âü∫ÂêëÈáèÂÅöÁÇπÁßØÔºå Á¨¨‰∫å‰∏™ÊòØÂú®‚ÄúÈáçÂª∫‚Äù‰ø°Âè∑Êó∂ÔºåÂÇÖÈáåÂè∂Á≥ªÊï∞Ë¶Å‰∏é Âçï‰Ωç Âü∫ÂêëÈáèÁõ∏‰πò‰ª•ÂæóÂà∞Êüê‰∏ÄÊñπÂêë‰∏äÁöÑÂàÜÈáè„ÄÇ $$ A‚Çñ = \\frac{1}{‚Äñcos(kx)‚Äñ¬≤} ‚ãÖ ‚ü® f(x), cos(kx) ‚ü© \\\\ \\ \\\\ B‚Çñ = \\frac{1}{‚Äñsin(kx)‚Äñ¬≤} ‚ãÖ ‚ü® f(x), sin(kx) ‚ü© $$\nWhen the function f(x) is projected into the direction cos(kx), the unit length of that direction is needed, so the norm of cos(kx) is divided.\n‚Äñcos(kx)‚Äñ¬≤ can be computed as the inner product of cos(kx) with itself, which is œÄ.\nProjection \u0026amp; reconstruction ùêÆ ` ` ` ` ` ùê≤ , ùêü , , , , ùêØ ùê± A test vector ùêü can be represented separately in two sets of orthogonal basis (2D vector space, 2D coordinate system: ùê±-ùê≤ and ùêÆ-ùêØ):\n$$ ùêü = ‚ü®ùêü, ùê±‚ü© ‚ãÖ \\frac{ùê±}{‚Äñùê±‚Äñ¬≤}\\ + ‚ü®ùêü, ùê±‚ü© ‚ãÖ \\frac{ùê≤}{‚Äñùê≤‚Äñ¬≤} \\\\ \\ \\\\ ùêü = ‚ü®ùêü, ùêÆ‚ü© ‚ãÖ \\frac{ùêÆ}{‚ÄñùêÆ‚Äñ¬≤}\\ + ‚ü®ùêü, ùêØ‚ü© ‚ãÖ \\frac{ùêØ}{‚ÄñùêØ‚Äñ¬≤} $$\nThe definition of Fourier Series is the same as projecting the vector ùêü on an orthogonal basis in a 2D vector space.\nThe coefficients are the projections of ùêü in each basis direction (i.e., inner products), and then they\u0026rsquo;re multiplied with the unit vectors, and add them up.\nThat means, in Fourier Series, the sines and cosines are the orthogonal basis functions. Coefficients indicate how much ùêü is in each direction.\nFourier Series-2 Source video: Fourier Series: Part 2\nArbitrary period L Generalize the period from (-œÄ, œÄ) to (0, L)\n$$ f(x) = \\frac{A‚ÇÄ}{2} \\ + ‚àë‚Çñ‚Çå‚ÇÅ^‚àû (A‚Çñcos( \\frac{2œÄkx}{L} ) + B‚Çñsin( \\frac{2œÄkx}{L} ) ) $$\nwhere:\ncos( 2œÄkx/L ) and sin( 2œÄkx/L ) are periodic between 0 and L.\nThat is when x=0, 2œÄkx/L=0, and when x=L, 2œÄkx/L=2œÄk, i.e., the wave returns to the start.\nThese cos( 2œÄkx/L ) and sin( 2œÄkx/L ) are orthogonal basis for Hilbert space of function f.\nThe Fourier coefficients change their bounds from (-œÄ, œÄ) to (0, L):\n$$ A‚Çñ = \\frac{2}{L} ‚à´_0^L f(x) cos(2œÄkx/L) dx\\\\ B‚Çñ = \\frac{2}{L} ‚à´_0^L f(x) sin(2œÄkx/L) dx $$\nApproximation is periodic The approximation $\\^f(x)$ is L-periodic as follows, because the f(x) is defined from 0 to L and all sines and cosines are L-periodic\n‚ãÖ ‚ãÖ ‚ãÖ - L 0 L 2 L ‚ãÖ ‚ãÖ ‚ãÖ The Fourier approximation $\\^f(x)$ is periodic just repeating forever the pattern of the target function $f(x)$, which is defined on an interval.\nInner Products in Hilbert Space Source video: Inner Products in Hilbert Space\nInner products of functions It\u0026rsquo;s consistent with the definition of inner products of vectors.\nGiven two functions f(x) and g(x),\nf g ‚ÇÅ : ‚ÇÅ : x a ‚ÇÅ f g ‚ÇÇ : ‚ÇÇ : x ‚ÇÇ f : g : : : x ‚ÇÉ ‚ÇÉ ‚ÇÉ f : g : : : : x ‚Çô ‚Çô ‚Çô b f g ( ( x x x ) ) The inner product between these two functions is defined as:\n$$ ‚ü®f(x), g(x)‚ü© = ‚à´‚Çê·µá f(x) g(x) dx $$\nIf they\u0026rsquo;re complex-valued functions (like $e^{iwx}$), then this inner product would use the complex conjugate (same real part, opposite imaginary part) of g(x): $‚ü®f(x), g(x)‚ü© = ‚à´‚Çê·µá f(x) \\bar g(x) dx$\nThis inner product indicates how similar these two functions are, just like the inner product of vectors.\nBy sampling these two functions at n evenly distributed x locations, a function is represented by a data vector.\nThe interval between 2 sampling x is Œîx = (b-a)/(n-1).\nAs n increases, more points are sampled and the interval is getting infinitely small, the continuous function can be recovered by a series of points.\n$$ \\underline ùêü = \\begin{bmatrix} f‚ÇÅ \\\\ f‚ÇÇ \\\\ ‚ãÆ \\\\ f‚Çô \\end{bmatrix} ; \\quad \\underline ùê† = \\begin{bmatrix} g‚ÇÅ \\\\ g‚ÇÇ \\\\ ‚ãÆ \\\\ g‚Çô \\end{bmatrix} $$\nCompute the inner product of these two vectors:\n$$ ‚ü®\\underline ùêü, \\underline ùê†‚ü© = \\underline ùê†·µÄ ‚ãÖ \\underline ùêü \\ = ‚àë‚Çñ‚Çå‚ÇÅ‚Åø f‚Çñ g‚Çñ $$\nwhere ùê†·µÄ is a row vector, ùêü is a column vector.\nIf data is complex-valued, ùê†·µÄ would be complex conjugate transpose ùê†* and g‚Çñ will be complex conjugate $\\bar g‚Çñ$:\n$‚ü®\\underline ùêü, \\underline ùê†‚ü© = \\underline ùê†^* ‚ãÖ \\underline ùêü = ‚àë‚Çñ‚Çå‚ÇÅ‚Åø f‚Çñ \\bar g‚Çñ$\nIf the number of samples (resolution) is doubled, this sum will get twice as large, which is not correct because the \u0026ldquo;similarity\u0026rdquo; of two functions doesn\u0026rsquo;t change.\nSo the inner product should be normalized by Œîx. (Expectation)\n$$ ‚ü®\\underline ùêü, \\underline ùê†‚ü© Œîx = ‚àë‚Çñ‚Çå‚ÇÅ‚Åø f(x‚Çñ) \\bar g(x‚Çñ) Œîx $$\nThe righ-hand term is the Riemann approximation of the continuous integral.\nIf take the limit as Œîx goes to 0, the resolution becomes infinitely fine corresponding to infinitely tall vectors, and the Riemann approximation becomes continuous integral:\n$$ ‚ü®f(x), g(x)‚ü© = ‚à´‚Çê·µá f(x) \\bar g(x) dx $$\nÂÜçÂæÄÂêéÊòØÂÖàÁúã‰∏ÄÈÅçÔºåÁÑ∂ÂêéÈªòÂÜô\n(2023-06-27)\nComplex Fourier Series Source video: Complex Fourier Series\nFourier series uses a infinite sum of sines and cosines functions of increasingly high frequencies to approximate an arbitrary periodic function.\nIf the function is complex-valued, the Fourier series should be reformulated to accomondate imaginary numbers.\nBy leveraging to Euler\u0026rsquo;s formula $e^{ikx} = cos(kx) + i sin(kx)$, the complex Fourier series can be written as:\n$$ f (x) = ‚àë_{-‚àû}^{+‚àû} C‚Çñ e‚Å±·µèÀ£ \\\\ \\ = ‚àë_{-‚àû}^{+‚àû} (Œ±+Œ≤i) (cos(kx) + i sin(kx)) $$\nwhere C‚Çñ is a complex coefficient.\nReal number is a subset of complex number, so this formula also adapt to read-valued functions, with subjecting to a constraint\nif f(x) is real-valued, $C‚Çñ = \\bar C‚Çã‚Çñ$.\nThis can be derived by expanding and kill all the imaginary terms:\n$$ f(x) = ‚àë_{-‚àû}^{+‚àû} (Œ±+Œ≤i) (cos(kx) + i sin(kx)) \\\\ \\ = ‚àë_{-‚àû}^{+‚àû} Œ±cos(kx) + \\cancel{ Œ±i sin(kx) } + \\cancel{ Œ≤icos(kx)} - Œ≤sin(kx) \\\\ \\ = ‚àë_{-‚àû}^{+‚àû} Œ±cos(kx) - Œ≤sin(kx) \\\\ \\ = ‚àë_{-‚àû}^{+‚àû} (Œ±-Œ≤i) (cos(-kx) + i sin(-kx) ) \\\\ $$\ne‚Å±·µèÀ£ are Orthogonal basis e‚Å±·µèÀ£ with taking different integer k stands for different basis function. And these basis functions are orthogonal, which are unique directions and form an infinite-dimensional function space.\nOrthogonal means the inner products against each other are 0, but the inner product with itself is its norm squared.\nLet e‚Å±·µèÀ£ be defined as Œ®‚Çñ (e‚Å±·µèÀ£ ‚âî Œ®‚Çñ), then the inner product in the Hilbert space of any two basis functions can be computed as an integral (the period is [-œÄ,œÄ]):\n$$ ‚ü®Œ®‚±º, Œ®‚Çñ‚ü© = ‚à´_{-œÄ}^œÄ Œ®‚±º ‚ãÖ \\bar Œ®‚Çñ dx \\\\ \\ = ‚à´_{-œÄ}^œÄ e‚Å± ≤À£ ‚ãÖ e‚Åª‚Å±·µèÀ£ dx \\\\ \\ = ‚à´_{-œÄ}^œÄ e‚Å±‚ÅΩ ≤‚Åª·µè‚ÅæÀ£ dx \\\\ \\ = \\frac{1}{i(j-k)} ‚ãÖ [e‚Å±‚ÅΩ ≤‚Åª·µè‚ÅæÀ£ ]_{-œÄ}^œÄ $$\nif (k-j) is non-zero, but an integar d, and because e‚Å±·µà$^œÄ$ = e‚Å±·µà$^{-œÄ}$, the inner product (definite integral) is 0.\nWhile if (k-j)=0, the inner product is 0/0:\n$$ \\frac{ [e‚Å±‚ÅΩ ≤‚Åª·µè‚ÅæÀ£]_{-œÄ}^œÄ }{ i(j-k) } = \\frac{0}{0} $$\nAccording to Loptial\u0026rsquo;s Rule, \u0026quot; the limit of the ratio of two functions is the same after we take the derivative of each function.\u0026quot;\nSo take the derivative of numerator and denominator w.r.t. (j-k):\n$$ \\begin{aligned} \u0026amp; lim_{(j-k)‚Üí0} \\frac{ [e‚Å±‚ÅΩ ≤‚Åª·µè‚ÅæÀ£ ]_{-œÄ}^œÄ }{ i(j-k) } \\\\ \u0026amp;= lim_{(j-k)‚Üí0} \\frac{e^{i(j-k)œÄ} - e^{i(j-k)(-œÄ)} }{i(j-k)} \\\\ \u0026amp;= lim_{(j-k)‚Üí0} \\frac{ iœÄ e^{i(j-k)œÄ} - (-œÄi) e^{i(j-k)(-œÄ)}}{i} \\\\ \u0026amp;= 2œÄ \\end{aligned} $$\nActually, it\u0026rsquo;s not necessary to take this limit.\nSince $‚ü®Œ®‚±º, Œ®‚Çñ‚ü© = ‚à´_{-œÄ}^œÄ e‚Å±‚ÅΩ ≤‚Åª·µè‚ÅæÀ£ dx$, this integral = 2œÄ when (j-k) = 0.\nTherefore, the inner product is\n$$ ‚ü®Œ®‚±º, Œ®‚Çñ‚ü© = \\frac{1}{i(j-k)} ‚ãÖ e‚Å±‚ÅΩ ≤‚Åª·µè‚ÅæÀ£ |_{-œÄ}^œÄ \\\\ \\ = \\begin{cases} 0 \u0026amp; \\text{if j $\\neq$ k} \\\\ 2œÄ \u0026amp; \\text{if j=k} \\end{cases} $$\nThe norm squared of a basis function Œ®‚±º (‚ÄñŒ®‚±º‚Äñ¬≤) is 2œÄ.\nAnalogy to regular 2D vector space above, a function can be written as a sum of projections that the function is projected onto each basis function:\n$$ ùêü(x) = ‚àë_{-‚àû}^{+‚àû} C‚Çñ e‚Å±·µèÀ£ = ‚àë_{k=-‚àû}^{+‚àû} \\frac{‚ü®ùêü(x), Œ®‚Çñ‚ü©}{2œÄ} ‚ãÖŒ®‚Çñ $$\ndoubt: Is Œ®‚Çñ unit length? Otherwise, Œ®‚Çñ needs normalization?, because the 1/2œÄ is for energy measurement instead of getting the unit length of Œ®‚Çñ?\nThe intuitive application of Fourier series is that: given a function, the approximated version of it can be obtained by truncating the summation of basis direction weighted by coefficients. And the coefficients is the projection (inner product) of the function onto each basis.\n(2023-06-28)\nFourier Transform Double integral for x then for w\nËøòÊòØÂÜÖÁßØÔºàÊäïÂΩ±ÔºâÔºåxÁöÑËåÉÂõ¥ÊòØË¥üÊó†Á©∑Âà∞Ê≠£Êó†Á©∑ÔºåÊäïÂΩ±Âà∞‰∏Ä‰∏™ÁâπÂÆöÈ¢ëÁéá w ÁöÑÂü∫ÂáΩÊï∞‰∏äÔºåÂæóÂà∞ Fourier coefficientÔºåËøôÂ∞±ÊòØ Fourier Transform\nFourier Series\u0026rsquo;s counterpart is inverse Fourier Transform\n","date":"2023-06-20T21:23:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ddse-steven/02_fourier/","title":"watch: Steven | Fourier Analysis"},{"content":"Source video: ÂÇÖÁ´ãÂè∂ÂèòÊç¢Â¶Ç‰ΩïÁêÜËß£ÔºüÁæéÈ¢úÂíåÂèòÂ£∞ÈÉΩÊòØ‰ªÄ‰πàÂéüÁêÜÔºüÊùéÊ∞∏‰πêËÄÅÂ∏àÂëäËØâ‰Ω†\nÂèòÊç¢ ÂêëÈáèÂèØ‰ª•‰ªéÂõæÂΩ¢‚ÄúÂèòÊç¢‚ÄùÊàêÊï∞Ôºå‰πüÂèØ‰ª•‰ªéÊï∞‚ÄúÈÄÜÂèòÊç¢‚ÄùÂõûÂõæÂΩ¢\ny B A ( ( 1 2 , , C - 4 2 ) ( ) 4 , 2 ) x Âèò Êç¢ = A B C ( ( 2 1 ( , , 3 , 4 - ) 2 2 ) ) ÂêëÈáèÁöÑÂõæÂÉè‰∏éÊï∞ÁªÑÊòØ‰∏Ä‰∏ÄÂØπÂ∫îÁöÑ„ÄÇÂêëÈáèÁõ∏Âä†ÂØπÂ∫îÊï∞ÁªÑÁõ∏Âä†„ÄÇ\nÂêëÈáè A Ë¢´Ë°®Á§∫‰∏∫ (2,4)ÔºåÂÖ∂Âê´‰πâ‰∏∫ A Âú® x ÊñπÂêë‰∏äÊúâ 2 ‰∏™Âçï‰ΩçÈïøÂ∫¶ exÔºåÂú® y ÊñπÂêë‰∏äÊúâ 4 ‰∏™Âçï‰ΩçÈïøÂ∫¶ ey„ÄÇ\nÂõ†‰∏∫Ôºö\nùíÜx Âíå ùíÜy ÁöÑÈïøÂ∫¶ÔºàËá™Â∑±‰∏éËá™Â∑±ÁöÑÂÜÖÁßØÔºâÈÉΩÁ≠â‰∫é 1 Ôºö ùíÜx‚ãÖex = ey‚ãÖey = 1\nex ‰∏é ey Ê≠£‰∫§ÔºàÂûÇÁõ¥ÔºâÔºå‰∏§ËÄÖÂÜÖÁßØÁ≠â‰∫é 0Ôºö ex ‚ãÖ ey = |ex|‚ãÖ|ey|‚ãÖcos90¬∞ = 0\nÊâÄ‰ª• ex Âíå ey ÊòØ‰∏ÄÁªÑÊ†áÂáÜÊ≠£‰∫§Âü∫„ÄÇ Á©∫Èó¥‰∏≠ÁöÑ‰ªª‰Ωï‰∏Ä‰∏™ÂêëÈáèÈÉΩÂèØ‰ª•ÂèòÊàêÊ†áÂáÜÊ≠£‰∫§Âü∫ÁöÑÁªÑÂêà„ÄÇ\nÂÇÖÈáåÂè∂Á∫ßÊï∞ ‰ªª‰Ωï‰∏Ä‰∏™Âë®ÊúüÊÄßÁöÑÂáΩÊï∞ f(t) ÈÉΩÂèØ‰ª•ÂèòÊç¢‰∏∫‰∏ÄÁ≥ªÂàóÊ≠£‰ΩôÂº¶ÂáΩÊï∞ÁöÑÂíå„ÄÇ Ê≠£Âº¶‰ΩôÂº¶ÂáΩÊï∞Â∞±ÊòØ‰∏ÄÁªÑÊ≠£‰∫§Âü∫„ÄÇ\nÂÆÉÁöÑÈÄÜÂèòÊç¢ÊòØÊòæËÄåÊòìËßÅÁöÑÔºåÂõ†‰∏∫ sin Âíå cos ÈÉΩÊòØÂë®ÊúüÊÄßÁöÑÔºåÂÆÉ‰ª¨ÁªÑÂêàÂá∫Êù•ÁöÑÂáΩÊï∞‰πü‰∏ÄÂÆöÊòØÂë®ÊúüÊÄßÁöÑ„ÄÇ\nÂØπ‰∫é‰∏Ä‰∏™Âë®ÊúüÊÄßÂáΩÊï∞ÔºåÊåâÁÖß‰∏çÂêåÈ¢ëÁéá w ÂàÜËß£Âá∫‰∏çÂêåÁöÑÊ≠£‰ΩôÂº¶ÂáΩÊï∞ÔºàÊ≥¢ÔºâÔºö\nÂ¶Ç‰∏äÂõæÔºåÂú®È¢ëÁéáÊñπÂêë‰∏äÔºå‰∏Ä‰∏™Âë®ÊúüÊÄßÊ≥¢ÂèØ‰ª•ÂàÜËß£Êàê‰∏Ä‰∏™È¢ëÁéá‰∏∫ w ÁöÑÊ≠£Âº¶ÂáΩÊï∞Ôºå‰∏é‰∏Ä‰∏™È¢ëÁéá‰∏∫ 2w ÁöÑÂàÜÈáèÔºàÊåØÂπÖÊØîËæÉÂ§ßÔºâÔºåÂÜç‰∏é‰∏Ä‰∏™È¢ëÁéá‰∏∫ 3w ÁöÑÂàÜÈáèÔºàÊåØÂπÖÊØîËæÉÂ∞èÔºâÁõ∏Âä†„ÄÇ\n‰ªéÂêéÊñπËßÇÂØüÔºå‰ª•Êó∂Èó¥‰∏∫ÂùêÊ†áÁ≥ªÔºåÂè™ËÉΩÁúãÂà∞‰ø°Âè∑ÈöèÊó∂Èó¥ÁöÑÂèòÂåñÔºõ ‰ªé‰æßÈù¢ËßÇÂØüÔºå‰ª•È¢ëÁéá‰∏∫ÂùêÊ†áÁ≥ªÔºå‰∏Ä‰∏™‰ø°Âè∑ÊòØÂ§ö‰∏™‰ø°Âè∑ÁöÑÁ¥ØÂä†„ÄÇ\nÂõ†‰∏∫ÂàÜÈáèÊòØÊúâÈôê‰∏™ÔºåÊâÄ‰ª•È¢ëË∞±Âõæ‰∏äÊòØ‰∏ÄÊù°Êù°Á´ñÁ∫øÔºå‰∏çÂêåÁöÑÈ´òÂ∫¶‰ª£Ë°®ÊåØÂπÖÁöÑÂ§ßÂ∞è„ÄÇ ‰∏çÂêåÈ¢ëÁéáÁöÑÂàÜÈáèÔºåÈô§‰∫ÜÊåØÂπÖ‰∏çÂêåÔºåËµ∑ÂßãÁÇπ‰πü‰∏çÂêåÔºåÊâÄ‰ª•ËøòÈúÄË¶Å‰∏Ä‰∏™ËΩ¥Ë°®Á§∫Áõ∏‰ΩçÔºö\nÊó∂ÂüüÈáåÁöÑ‰ø°Âè∑ f ÂèòÊç¢Âà∞È¢ëÂüüÈáåÁöÑ‰∏â‰∏™Áª¥Â∫¶ÔºöÈ¢ëÁéá wÔºåÊåØÂπÖ F(f)ÔºåÁõ∏‰Ωç œÜ„ÄÇ\nÂ¶ÇÊûúÂ∑≤Áü•È¢ëÂüü‰∏≠ÁöÑ w, F(f), œÜÔºåÂ∞±ÂèØ‰ª•ÊääÂÆÉ‰ª¨ÁªÑÂêàËµ∑Êù•ÔºåÈÄÜÂèòÊç¢Âà∞Êó∂Âüü‰∏≠ÁöÑ‰ø°Âè∑„ÄÇ\n$$ f(t) = \\frac{a_0}{2} + \\sum_i a_n sin(n w t + œÜ_n) \\\\ \\quad = \\frac{a_0}{2} + \\sum_i a_n sin(n w t) + \\sum_i b_n cos(n w t) $$\nn ÊòØÊï¥Êï∞Ôºånw ÊòØ‰∏çÂêåÁöÑÈ¢ëÁéáÔºåa‚Çô ÊòØÊåØÂπÖÔºåœÜ‚Çô ÊòØÁõ∏‰Ωç„ÄÇ ÊåâÁÖßÁ¨¨2Ë°åÁöÑÂÜôÊ≥ïÔºåÊ†áÂáÜÊ≠£‰∫§Âü∫Êúâ 3 ‰∏™Ôºö1Ôºå sin(nwt), cos(nwt)\n‰ªª‰Ωï‰∏Ä‰∏™Âë®ÊúüÊÄßÁöÑÂáΩÊï∞ÈÉΩÂèØ‰ª•ÂàÜËß£Êàê‰ª•‰∏ä 3 ‰∏™Ê†áÂáÜÊ≠£‰∫§Âü∫ÁöÑÁ∫øÊÄßÁªÑÂêà„ÄÇ Ôºàsin(nwt) ‰∏é cos(nwt) ÊòØ‰∏ÄÂØπÔºåÊãÜ‰∏çÂºÄÔºüÔºâ\nËøûÁª≠ÂÇÖÈáåÂè∂ÂèòÊç¢ ÈùûÂë®Êúü‰ø°Âè∑ÂèØ‰ª•Áúã‰ΩúÊòØÂë®ÊúüÊó†Á©∑Â§ßÁöÑÂë®ÊúüÊÄß‰ø°Âè∑„ÄÇ\nÊ¨ßÊãâÂÖ¨Âºè Áî®‰∏Ä‰∏™Âπ≥Èù¢ÂùêÊ†áÁ≥ªË°®Á§∫ËôöÊï∞ÔºåÂàôÊ®™ËΩ¥Ë°®Á§∫ÂÆûÈÉ®ÔºåÁ∫µËΩ¥Ë°®Á§∫ËôöÈÉ®„ÄÇ Ê®™ËΩ¥ÁöÑÂçï‰Ωç‰∏∫1ÔºåÁ∫µËΩ¥ÁöÑÂçï‰Ωç‰∏∫ËôöÊï∞Âçï‰Ωç iÔºåÂàôÂçï‰ΩçÂúÜ‰∏ä‰ªª‰Ωï‰∏ÄÁÇπ A ÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö cosŒ∏ + i‚ãÖsinŒ∏„ÄÇ\n‰ª§ Œ∏ = wtÔºåw Âèñ‰∏çÂêåÁöÑÈ¢ëÁéáÔºåÂØπÂ∫î A ÁÇπÂú®ÂúÜ‰∏äÈÄÜÊó∂ÈíàËΩ¨Âä®‰∏çÂêåÁöÑËßíÂ∫¶\nÊ†πÊçÆÊ¨ßÊãâÂÖ¨ÂºèÔºö\n$$ cosŒ∏ + i‚ãÖsinŒ∏ = e^{iŒ∏} \\\\ cos(wt) + i‚ãÖsin(wt) = e^{iwt} $$\nÊâÄ‰ª• A Âú®ÈÄÜÊó∂ÈíàÊóãËΩ¨Êó∂ÔºåÊØèÊó∂ÊØèÂàªÈÉΩ‰ª£Ë°®‰∫Ü‰∏§‰∏™Ê≠£‰∫§Âü∫ cosŒ∏, sinŒ∏„ÄÇ Â¶ÇÊûúÊòØ $e^{-iwt}$ Â∞±ÊòØÈ°∫Êó∂ÈíàÊóãËΩ¨„ÄÇ\nFT: Áî®Ê≠£‰∫§Âü∫\u0026quot;Êëò\u0026quot;ÂàÜÈáè ‰∏Ä‰∏™ÈùûÂë®ÊúüÊÄßÁöÑÊó∂Âüü‰ø°Âè∑ËÇØÂÆö‰πüÂåÖÂê´ÂêÑÁßçÈ¢ëÁéáÁöÑ‰ø°Âè∑ÂàÜÈáèÔºå ÂèàÂõ†‰∏∫Ê†áÂáÜÊ≠£‰∫§Âü∫‰∏éËá™Â∑±ÁöÑÂÜÖÁßØ‰∏∫ 1Ôºå‰∏éÂÖ∂‰ªñÊ≠£‰∫§Âü∫ÁöÑÂÜÖÁßØ‰∏∫ 0Ôºå ÊâÄ‰ª•ÂèØ‰ª•Áî®ÂêÑ‰∏™Ê≠£‰∫§Âü∫‰∏éËØ•‰ø°Âè∑ÂÜÖÁßØÔºåÂàôÁïô‰∏ãÁöÑÂ∞±Âè™ÊòØÂú®Ëøô‰∏™Âü∫ÂêëÈáè‰∏äÁöÑÂàÜÈáè„ÄÇ\nÈùûÂë®ÊúüÊÄßÊó∂Âüü‰ø°Âè∑ÁöÑÈ¢ëÁéáÂàÜÈáèÊúâÊó†Á©∑‰∏™ÔºåÊâÄ‰ª•ÂÇÖÈáåÂè∂ÂèòÊç¢ÊòØÁî®ÁßØÂàÜÔºö\n$$ \\^F_T(w) = ‚à´_{-‚àû}^{+‚àû} f(t) e^{-jwt} dt \\\\ \\ \\\\ f(t) e^{-jwt} \\begin{cases} =0 \u0026amp; \\text{f(t) ‰∏çÂê´ sin(wt),cos(wt)} \\\\ \\neq 0 \u0026amp; \\text{f(t) Âê´Êúâ sin(wt),cos(wt)} \\end{cases} $$\nÂÖ∂‰∏≠ $e^{-jwt}$ ÊòØ‰∏ÄÁªÑÊ≠£‰∫§Âü∫ sin(wt) Âíå cos(wt)„ÄÇ\n$\\^F_T(w)$ ÊòØ‰∏™Â§çÊï∞ÔºàÂ∏¶ iÔºâÔºåÂÆûÈÉ®Ë°®Á§∫ÊåØÂπÖÔºåËôöÈÉ®ÊòØÁõ∏‰ΩçÔºåËÄå‰∏îÊòØËøûÁª≠ÁöÑ ÔºåÊÑèÂë≥ÁùÄÊØè‰∏™È¢ëÁéáÁöÑÊ≠£Ôºà‰ΩôÔºâÂº¶ÂáΩÊï∞ÈÉΩÊúâÂàÜÈáèÔºå\nIFT: Ê≠£‰∫§Âü∫Âä†ÊùÉÂíå $F_T(w)$ ÊòØÂêÑÊ≠£‰∫§Âü∫ÔºàÊ≠£Âº¶‰ΩôÂº¶ÂáΩÊï∞ÔºâÁöÑ‚ÄùÁ≥ªÊï∞‚ÄúÔºåÂç≥Âú®ÂêÑÊ≠£‰∫§Âü∫‰∏äÁöÑ‚ÄùÈïøÂ∫¶‚Äú„ÄÇ\n‰ªéÈ¢ëÂüü‰ø°Âè∑ÈÄÜÂèòÊç¢ÂõûÊó∂Âüü‰ø°Âè∑ÔºåÂ∞±ÊääÁ≥ªÊï∞‰πòÂà∞Âü∫ÂêëÈáè‰∏äÔºåÂÜçÂä†Ëµ∑Êù•Ôºö\n$$ f(t) = ‚à´_{-‚àû}^{+‚àû} F_T(w) e^{-jwt} dw $$\nÂ∫îÁî® Â£∞Èü≥‰ø°Âè∑ÊòØÊó∂ÂüüÔºåÊ®™ÂùêÊ†áÊòØÊó∂Èó¥ÔºõÂõæÂÉè‰ø°Âè∑ÊòØÁ©∫Èó¥ÂüüÔºåÊ®™ÂùêÊ†áÊòØÁ©∫Èó¥‰ΩçÁΩÆ\n‰ΩéÈ¢ëÂàÜÈáèÊòØÂõæÂÉèÁöÑËΩÆÂªìÔºåÈ´òÈ¢ëÂàÜÈáèÊòØÁªÜËäÇ\n","date":"2023-06-20T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/d-vid-ft-%E6%9D%8E%E6%B0%B8%E4%B9%90/","title":"watch: FT - ÊùéÊ∞∏‰πê | Fourier Transform"},{"content":"NO code | arxiv\nFound by google scholar with searching: \u0026ldquo;fourier transform neural radiance field\u0026rdquo; 3. Generalized PlenOctree Fusion silhouettes Ââ™ÂΩ± Pipeline generalized NeRF Œ®: averaging the density œÉ and\nŒ® are queried at every position with several view directions to predicted density and color\nPlenOctree stores the averaged density and color\nFilter out leaves having low density ($œÉ \u0026lt; 1e-3$) by \u0026ldquo;averaging\u0026rdquo; the queried results of 100 rendered views at the points, where the (cumulated) transmittance $T \u0026gt; 1e-3$ (close to the camera?).\n\u0026ldquo;Coarse\u0026rdquo; means sparse adjacent views of a target view, while \u0026ldquo;fine\u0026rdquo; means dense adjacent views\n4. Fourier PlenOctree \u0026ldquo;adopt PlenOctree to dynamic scenes by compressing time-variant information in the frequency domain\u0026rdquo;\n4D Scene Representation: position and time (x,y,z,t)\nhigh dimensional frequency domain: Mapping the position to Fourier Transform coefficients of the density œÉ(t) and each SH coefficient ùê≥(t)\n$$Œ¶(x,y,z) = ùê§^œÉ, ùê§^ùê≥$$\nwhere $ùê§^œÉ ‚àà ‚Ñù^{n‚ÇÅ}$ (a sigma corresponds to n‚ÇÅ DFT coefficients),\nand $ùê§^ùê≥ ‚àà ‚Ñù^{n‚ÇÇ √ó (l_{max} + 1)^2 √ó 3 }$ (a point has $(l_{max} + 1)^2 √ó 3$ SH coefficients and each SH coefficient has n‚ÇÇ DFT coefficients.)\nReconstruct density œÉ at time t by summing n‚ÇÅ DFT coefficients of the Fourier PlenOctree with orthogonal basis:\n$$\\rm œÉ(t; ùê§^œÉ) = ‚àë·µ¢‚Çå‚ÇÄ^{n‚ÇÅ-1} ùê§^œÉ·µ¢ ‚ãÖ IDFT·µ¢(t)$$\nwhere IDFT·µ¢(t) = $\\{^{cos(\\frac{i\\pi}{T} t) \\quad \\text{if i is even}} _{sin(\\frac{(i+1)\\pi}{T} t) \\quad \\text{if i is odd}}$\nReconstruct each SH coefficient at time t by summing n‚ÇÇ DFT coefficients of the Fourier PlenOctree:\n$$\\rm z_{m, l}(t; ùê§^ùê≥) = ‚àë·µ¢‚Çå‚ÇÄ^{n‚ÇÇ-1} ùê§^ùê≥_{m,l,i} ‚ãÖ IDFT·µ¢(t)$$\nGeneralized PlenOctree Fusion Aggregating the PlenOctrees of different frames at T times.\nLeaves of different PlenOctrees at the same position are stacked, so the density œÉ and SH coefficients ùê≥(t) are stacked along the time axis.\nThe stacked vector performs DFT becoming Fourier coefficients $ùê§^œÉ, ùê§^ùê≥$, which are stored in Fourier PlenOctree.\nFourier PlenOctree Fine-tunning Based on PlenOctree fusion \u0026ldquo;training\u0026rdquo;, the Fourier PlenOctree can be continuous optimizing via gradient descent.\n","date":"2023-06-19T12:16:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-fourier_plenoctrees/","title":"read: Fourier PlenOctrees for Dynamic"},{"content":"(2023-11-22)\nDiscrete Convolve Source video: „ÄêÂÆòÊñπÂèåËØ≠„ÄëÈÇ£‰πà‚Ä¶‚Ä¶‰ªÄ‰πàÊòØÂç∑ÁßØÔºü- 3B1B\nTwo list of numbers are combined and form a new list: Make a table, calculate pairwise product for each cell and sum up all anti-diagonals.\n$(a * b)_n = ‚àë_{\\substack{i,j \\\\ i+j=n}} a_i * b_j$\nThe sum of indecies of the 2 number in each pair to be summed up is the same, just distinct combinations. Reverse the second list, slide it from left to righ, and sum the products of each pair aligned vertically.\nConvolution for image is for smoothing by averaging nearby pixels.\nThe multiplication of two polynomials is convolution, because multiply each two term and collect term with the same order.\n(2024-01-12) Áà∂‰∫≤ÂØøÂëΩ Âíå ÂÑøÂ≠êÁöÑÂπ¥ÈæÑÂÅöÂç∑ÁßØÔºåÁ≠â‰∫é‰∏ÄËµ∑ÁîüÊ¥ªÁöÑÂ≤ÅÊúà\n„ÄêÂÆòÊñπÂèåËØ≠„ÄëÂç∑ÁßØÁöÑ‰∏§ÁßçÂèØËßÜÂåñ|Ê¶ÇÁéáËÆ∫‰∏≠ÁöÑX+YÊó¢ÁæéÂ¶ôÂèàÂ§çÊùÇ - 3B1B\n„ÄêÂÆòÊñπÂèåËØ≠„Äë‰ΩÜÊòØ‰ªÄ‰πàÊòØ‰∏≠ÂøÉÊûÅÈôêÂÆöÁêÜÔºü- 3B1B\n„ÄêÂÆòÊñπÂèåËØ≠„Äë‰∏∫‰ªÄ‰πàÊ≠£ÊÄÅÂàÜÂ∏ÉÈáå‰ºöÊúâ‰∏Ä‰∏™œÄÔºüÔºà‰∏çÊ≠¢ÊòØÁßØÂàÜÊäÄÂ∑ßÔºâ- 3B1B\n(2023-11-07)\nCovariance Matrix is PSD Variance is the average of the squared distance between the mean and point for a single dimension.\nÊñπÂ∑ÆÊòØÂÅèÁ¶ªÊúüÊúõÁöÑÂπ≥ÊñπÁöÑÊúüÊúõ„ÄÇ\nVariance is a scalar: the averaged squared mangitude of the distance vector between a certain vector and the mean vector in a data space.\n$$Var(x) = \\frac{‚àë(x-Œº)¬≤}{n}$$\nCovariance is the product of 2 distances betwee mean and point for the 2 dimensions respectively.\n$$Covar(x,y) = \\frac{‚àë(x-Œº_x)(y-Œº_y)}{n}$$\nCovariance is a real number and indicates positive- or negtive correlation of 2 dimensions by its sign.\nCovariance matrix is a symmatric matrix recording covariance for each pair of dimensions.\n$$ \\begin{array}{c|ccc} ùê± \u0026amp; d‚ÇÅ \u0026amp; d‚ÇÇ \u0026amp; d‚ÇÉ \\\\ \\hline \\\\ d‚ÇÅ \u0026amp; \\frac{‚àë(x_{d‚ÇÅ}-Œº_{d‚ÇÅ})(x_{d‚ÇÅ}-Œº_{d‚ÇÅ})}{n} \u0026amp; \\frac{‚àë(x_{d‚ÇÅ}-Œº_{d‚ÇÅ})(x_{d‚ÇÇ}-Œº_{d‚ÇÇ})}{n} \u0026amp; \\frac{‚àë(x_{d‚ÇÅ}-Œº_{d‚ÇÅ})(x_{d‚ÇÉ}-Œº_{d‚ÇÉ})}{n} \\\\ \\\\ d‚ÇÇ \u0026amp; \\frac{‚àë(x_{d‚ÇÇ}-Œº_{d‚ÇÇ})(x_{d‚ÇÅ}-Œº_{d‚ÇÅ})}{n} \u0026amp; \\frac{‚àë(x_{d‚ÇÇ}-Œº_{d‚ÇÇ})(x_{d‚ÇÇ}-Œº_{d‚ÇÇ})}{n} \u0026amp; \\frac{‚àë(x_{d‚ÇÇ}-Œº_{d‚ÇÇ})(x_{d‚ÇÉ}-Œº_{d‚ÇÉ})}{n} \\\\ \\\\ d‚ÇÉ \u0026amp; \\frac{‚àë(x_{d‚ÇÉ}-Œº_{d‚ÇÉ})(x_{d‚ÇÅ}-Œº_{d‚ÇÅ})}{n} \u0026amp; \\frac{‚àë(x_{d‚ÇÉ}-Œº_{d‚ÇÉ})(x_{d‚ÇÇ}-Œº_{d‚ÇÇ})}{n} \u0026amp; \\frac{‚àë(x_{d‚ÇÉ}-Œº_{d‚ÇÉ})(x_{d‚ÇÉ}-Œº_{d‚ÇÉ})}{n} \\end{array} $$\nThus, element on the main diagonal is the variance of each variable.\nCovariance matrix is always positive semi-definite, symmetric, square.\nThe eigenvalues Œª and eigenvectors ùêØ of covariance matrix ùêÑ(ùê±ùê±·µÄ) can be solved from:\nùêÑ(ùê±ùê±·µÄ) ‚ãÖ ùêØ = ŒªùêØ\nSince ùêÑ(ùê±ùê±·µÄ) is a symmetric matrix, Œªs are all real numbers.\nŒª = ùêÑ(ùê±ùê±·µÄ) = $\\frac{‚àë(x·µ¢-Œº·µ¢)¬≤}{n}$ ‚â• 0,\nwhere n is the number of datapoints in the dataset. x·µ¢ is one of dimensions of ùê±. x·µ¢ is a column vector containing n points.\nCheck the definition of positive semi-definite:\nùêØ·µÄùêÑ ùêØ = ŒªùêØ·µÄùêØ = Œª[a b c] $[^a_{^b_c}]$ = Œª(a¬≤+b¬≤+c¬≤) ‚â• 0.\nThus, covariance matrix ùêÑ is positive semi-definite.\ntodo: This could be wrong, need to watch Dr. Strang\u0026rsquo;s lecture.\nThe following proof is from Is a sample covariance matrix always symmetric and positive definite? - SE\nCovariance matrix E(ùê±ùê±·µÄ) = (ùê±-ùõç)(ùê±-ùõç)·µÄ/n\nCheck the definition of positive semi-definite:\nùêØ·µÄ E(ùê±ùê±·µÄ) ùêØ = $\\frac{1}{n}$ ùêØ·µÄ (ùê±-ùõç)(ùê±-ùõç)·µÄ ùêØ = ((ùê±-ùõç)·µÄ ùêØ)¬≤/n ‚â• 0. A positive scalar.\nOther Proofs:\nProof: Positive semi-definiteness of the covariance matrix Since covariance matrix is positive semi-definite, there is global minima for all axis:\nHeatmap of a covariance matrix - Gowri Shankar\nCategorizing Quadratic Forms - Ximera, OSU - Categorizing Quadratic Forms\n(2023-11-08)\nRadius of 3D Gaussian If a dataset scattered as an ellipse following 2D Gaussian is circumscribed by a circle, to cover the most points, the radius of the circle could be $r = 3œÉ$, 3 times the standard deviation.\nThe standard deviation œÉ is the square root of the variance, which is the element on the main diagonal of the covariance matrix.\nAnd variances are the eigenvalues of the covariance matrix.\nGiven a covariance matrix $ùêÇ=[^{a \\ b}_{b \\ c}]$, its eigenvalue Œª and eigenvector ùêØ satisfy: ùêÇ ùêØ = ŒªùêØ.\nEigenvalues Œªs can be solved from: $\\rm det(ùêÇ - Œªùêà) = 0$:\n$$ \\begin{vmatrix} a-Œª \u0026amp; b \\\\ b \u0026amp; c-Œª\\end{vmatrix} = 0 \\\\ (a-Œª)(c-Œª) - b^2 = 0 \\\\ Œª^2 - (a+c)Œª + ac-b^2 =0 $$\nŒª‚ÇÅ = $\\frac{(a+c) + \\sqrt{(a+c)^2 - 4(ac-b^2)} }{2}$, Œª‚ÇÇ = $\\frac{(a+c) - \\sqrt{(a+c)^2 - 4(ac-b^2)} }{2}$\n","date":"2023-06-17T22:14:00Z","permalink":"https://zichen34.github.io/writenotes/calc/gaussian/","title":"memo: Calc | Gaussian"},{"content":"FNet: Mixing Tokens with Fourier Transforms - NAACL 2022\nCode: arxiv appendix | official-jax; | keras code\n(2023-07-05) Other implementations found by asking bing chat: \u0026ldquo;Could you give its pytorch code?\u0026rdquo;\nrishikksh20/FNet-pytorch erksch/fnet-pytorch1 jaketae/fnet vgundecha/fnet-google-pytorch (2023-06-16)\nVideo Intro Source video: FNet: Mixing Tokens with Fourier Transforms (Machine Learning Research Paper Explained) - Yannic Kilcher\n(2023-07-07)\nAbstract Use linear transformations replace self-attention sublayers resulting in speeding up;\nUse unparameterized Fourier Transform replace self-attention sublayers achieving over 90% accuracy of BERT counterparts.\nFNet has a light memory footprint (because it doesn\u0026rsquo;t have parameters?)\nIntroduction Attention connects each token by the relevance weights of every other token in the input.\nAnd more complex mixing help capture the relationship between tokens.\nCan attention, the relevance-based \u0026ldquo;token-mixer\u0026rdquo;, be replaced by simpler linear transformation (ùêóùêñ‚Åª¬π+ùêõ)?\nDecent results are gived by replacing attention with twice parametrized (optimizable) matrix multiplications, which are mixing the sequence dimension and then mixing hidden dimension.\nA sequence containing 5 tokens, which are 4-dimensional.\nS e q u e n c e d i m e n t i o n h d i i d m d e e n n s i o n Use the faster, structured linear transformation FFT without parameters, yielding similar performance of dense layer mixing and good scalability.\nContributions:\nattention may not be a necessary component. Hence, seeking new mixing mechanisms is valuable. FNet uses FFT to mix token speeding up the training while losing some accuracy. Attention do help increase accuracy to some extent. FNet scales well to long inputs. Code from: rishikksh20/FNet-pytorch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import torch from torch import nn class FeedForward(nn.Module): def __init__(self, dim, hidden_dim, dropout = 0.): super().__init__() self.net = nn.Sequential( nn.Linear(dim, hidden_dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout) ) def forward(self, x): return self.net(x) class PreNorm(nn.Module): def __init__(self, dim, fn): super().__init__() self.norm = nn.LayerNorm(dim) self.fn = fn def forward(self, x, **kwargs): return self.fn(self.norm(x), **kwargs) class FNetBlock(nn.Module): def __init__(self): super().__init__() def forward(self, x): # \u0026#34;2-D fft\u0026#34;? row-wise first, then column-wise. x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real return x class FNet(nn.Module): def __init__(self, dim, depth, mlp_dim, dropout = 0.): super().__init__() self.layers = nn.ModuleList([]) for _ in range(depth): self.layers.append(nn.ModuleList([ PreNorm(dim, FNetBlock()), PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)) ])) def forward(self, x): for attn, ff in self.layers: x = attn(x) + x x = ff(x) + x return x ","date":"2023-06-16T16:23:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/b-note-fnet/","title":"read: FNet"},{"content":"Arxiv | Code-keras | Code-cvae\nAnother paper: An Introduction to Variational Autoencoders Recognition model with parameters œÜ, $q_œÜ(ùê≥|ùê±)$, approximates the intractable posterior distribution;\nGenerative model with parameters Œ∏, $p_Œ∏(ùê≥)p_Œ∏(ùê±|ùê≥)$, maps a latent variable to a \u0026ldquo;sample\u0026rdquo;.\nAbstract directed probabilistic model\nThe distribution of an i.i.d. dataset with latent variables P(ùêó,ùêô)\ncontinuous latent variables with intractable posterior distributions\nThe latent variable ùê≥ per datapoint is continuous, so its posterior distribution $p_Œ∏(ùê≥|ùê±)$ cannot be computed explicitly.\nreparameterization of the variational lower bound yields a lower bound estimator\nLower bound estimator can be optimized using SGD,\nIntroduction Variational Bayesian approach involves the optimization of an approximation to the intractable posterior.\nUse q(ùê≥) to approximate $p_Œ∏(ùê≥|ùê±)$\nMean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior\nùîº_{q(ùê≥)} [ ]\nq(ùê≥) is also intractable\nMethod Problem scenario Each sample is generated by two steps:\nsample a ùê≥ from prior distribution $p_{Œ∏^*}(ùê≥)$ Sample an ùê± from the conditional distribution $p_{Œ∏^*}(ùê±|ùê≥)$ where Œ∏* is the true parameters.\nIntractability:\nThe integral of the marginal likelihood $p_Œ∏(ùê±) = ‚à´ p_Œ∏(ùê≥) p_Œ∏(ùê±|ùê≥) dùê≥ = ‚à´ p_Œ∏(ùê±,ùê≥) dùê≥$ cannot be computed, because ùê≥ is a high-dimensional continuous variable.\nSuch that the true posterior density $p_Œ∏(ùê≥|ùê±) = \\frac{ p_Œ∏(ùê±|ùê≥) p_Œ∏(ùê≥) }{ p_Œ∏(ùê±) }$ is also intractable, because the denominator marginal likelihood cannot be computed.\nThat means the EM algorithm cannot be used, because in each iteration the introduced prior distribution q(ùê≥) needs to be equal to $p_Œ∏(ùê≥|ùê±)$, which however is intractable.\nAnd any reasonable mean-field variational bayesian algorithms are intractable, where the $p_Œ∏(ùê≥|ùê±)$ is used as the objective of approximating by q(ùê≥), but it doesn\u0026rsquo;t work for $p_Œ∏(ùê≥|ùê±)$ that cannot be computed.\nA large dataset:\nSampling-based methods, e.g. Monte Carlo EM, would be too slow, because sampling is performed on every datapoint.\nThree meaningful problems:\nEstimating the parameter Œ∏ of the distribution via MLE or MAP can allow mimicking the data-generating process: Sample ùê≥ first then sample the x from the conditional distribution\nMLE try to find the Œ∏ that makes the probability of dataset X given Z maximum. MLE is to estimate parameter Œ∏, while VAE is to estimate the distribution of X.\nMAP believe that the Œ∏ having the maximum posterior probability given a dataset likelihood p(X|Œ∏) and prior probability p(Œ∏) according to, $p(Œ∏|X) = \\frac{p(X|Œ∏) p(Œ∏)}{p(X)}$, is the most possible.\nEM algorithm uses MLE to find the parameter Œ∏ of a probabilistic model involving latent variable, where assume the prior probability q(ùê≥) = posterior probability $p_Œ∏(ùê≥|ùê±)$ of the latent variable, then apply MLE find optimal Œ∏.\nApproximating the posterior probability $p_Œ∏(ùê≥|ùê±)$ given an observed value ùê± and a choice of parameters Œ∏ is useful for encoding data.\nThe latent variable ùê≥ can be regarded as a latent representation of a datapoint ùê±.\nApproximating the marginal likelihood $p_Œ∏(ùê±)$ enable to perform those inference tasks where the prior p(ùê±) is required, e.g. image denoising and inpainting.\nThe above three goals can be tackled by a recognition model $q_œÜ(ùê≥|ùê±)$, which is an approximation to the intractable posterior probability $p_Œ∏(ùê≥|ùê±)$\nThe recognition model $q_œÜ(ùê≥|ùê±)$ is a probabilistic encoder, which produces a distribution over all possible values of ùê≥ with given a datapoint ùê±.\n(Each datapoint ùê± corresponds to a distribution $q_œÜ(ùê≥|ùê±)$ with some parameters (e.g., Œº,œÉ).)\np ( ùê≥ D | i ( ùê± s b f ) t y r r o i g m b i u v E t e n i n c o o n a d e o ùê± r f ) ùê≥ ùê≥ p ( ùê± | D ùê≥ i ( ) s b f t y r r o i g m b i u v D t e e i n c o o n a d e o ùê≥ r f ) ùê± ùê± A ùê≥ produces a conditional distribution p(ùê±|ùê≥), from which a datapoint ùê± is generated. So the generative model $p_Œ∏(ùê±|ùê≥)$ is called probabilistic decoder.\n(2023-07-09)\nSo a training step is: sampling a ùê≥ from distribution p(ùê≥|ùê±), then with this ùê≥, find the parameter Œ∏ that makes the probability p(ùê±|ùê≥) largest based on MLE. That\u0026rsquo;s why the loss funciton has a cross-entropy term that measures the likelihood of output ùê±\u0026rsquo;, i.e., log p(ùê±|ùê≥).\nAccording to KL-divergence $-q(ùê≥) log \\frac{p(ùê≥|ùê±)}{q(ùê≥)}$, the approximated posterior q(ùê≥) is supposed to equal the real posterior p(ùê≥|ùê±), which, however, is intractable. So the authors use network and reparameterization to estimate the parameters (mean, variance) of the posterior.\n(I think) ùê≥ sampled from posterior can be one value or multiple times and doing average.\n2.2 The Variational bound The marginal likelihood of the dataset with N datapoints is a sum over the marginal likelihoods of individual datapoints:\n$log\\ p_Œ∏(ùê±‚ÅΩ¬π‚Åæ, \u0026hellip;, ùê±‚ÅΩ·¥∫‚Åæ ) = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ)$\nRewrite it by introducing posterior approximation $q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)$ to make up KL divergence:\n$$ \\begin{aligned} \u0026amp;log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ) \\\\ \u0026amp;= log\\ (\\frac{p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥) p_Œ∏(ùê≥)}{p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} ) \\\\ \u0026amp;= log\\ (p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥) p_Œ∏(ùê≥)) - log\\ p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) \\\\ \u0026amp;= log\\ (p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥) p_Œ∏(ùê≥)) - log\\ p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) \\\\ \u0026amp;\\quad + log\\ q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) - log\\ q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) \\\\ \u0026amp;= log\\ \\frac{p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥) p_Œ∏(ùê≥)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} - log\\ \\frac{p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)}\\\\ \\end{aligned} $$\nCompute expectations w.r.t. the approximate posterior $q_œÜ(ùê≥|ùê±)$ for both side:\n$$ ‚à´q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ) dùê≥ = \\\\ ‚à´q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) \\left[log\\ \\frac{p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥) p_Œ∏(ùê≥)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} - log\\ \\frac{p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} \\right] dùê≥ $$\nLeft-hand side remains marginal likelihood $log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ)$ (i.e. $ùîº_{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} [ log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ) ]$) because p(x) has nothing to do with z.\nWhile right-hand side is the lower bound plus KL divergence: $$ \\begin{aligned} \u0026amp; log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ) = \\\\ \u0026amp; ‚à´q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) log\\ \\frac{p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥) p_Œ∏(ùê≥)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} dùê≥ \\\\ \u0026amp;\\quad - ‚à´q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) log\\ \\frac{p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} dùê≥ \\\\ \u0026amp; = ‚Ñí(Œ∏,œÜ; ùê±‚ÅΩ‚Å±‚Åæ) + D_{KL} ( q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) || p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ ) \\\\ \u0026amp; \\\\ \u0026amp; = ùîº_{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} [log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ,ùê≥) - log\\ q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)] \\\\ \u0026amp;\\quad + D_{KL} ( q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) || p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ ) \\quad (2) \\end{aligned} $$\nIn another way, the lower bound can also be written as eq.(3): ùìõ$(Œ∏,œÜ; ùê±‚ÅΩ‚Å±‚Åæ) = -D_{KL} ( q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) || p_Œ∏(ùê≥) ) + ùîº_{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} [log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥)]$\nwhose derivation starts from the conditional probability $p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥)$:\nThe likelihood of the i-th datapoint: $$ \\begin{aligned} \u0026amp; log(p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥)) \\\\ \u0026amp;= log( \\frac{p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ))}{p_Œ∏(ùê≥)} )\\\\ \u0026amp;= log( \\frac{ p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ)* q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) ) }{ p_Œ∏(ùê≥)*q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) } ) \\\\ \u0026amp;= log( \\frac{ p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} ) - log( \\frac{p_Œ∏(ùê≥)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} ) \\end{aligned} $$\nCompute the expectation w.r.t. approximate posterior $q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)$:\n$$ \\begin{aligned} \u0026amp; ‚à´q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)\\ log(p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥)) dùê≥ = \\\\ \u0026amp; ‚à´q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) log( \\frac{ p_Œ∏(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)})dùê≥\\\\ \u0026amp; \\quad - ‚à´q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) log( \\frac{p_Œ∏(ùê≥)}{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} ) dùê≥ \\\\ \u0026amp; \\\\ \u0026amp; ùîº_{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} [log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥))] = \\\\ \u0026amp; \\quad ‚Ñí(Œ∏,œÜ; ùê±‚ÅΩ‚Å±‚Åæ) + D_{KL} ( q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ || p_Œ∏(ùê≥)) \\end{aligned} $$\nHowever, using Monte Carlo to estimate gradient of ùìõ (an expectation) will bring high variance.\n2.3 SGVB estimator and AEVB algo A practical estimator of the lower bound ùìõ of the likelihood and its derivatives w.r.t. the parameters.\n(With some mild conditions for a selected approximate posterior distribution $q_œÜ(ùê≥|ùê±)$,)\nConsider a variable $\\~ùê≥$ that comes from $\\~ùê≥ = g_œÜ$(ùõÜ,ùê±) with ùõÜ ~ p(ùõÜ), follows the posterior distribution $\\~ùê≥ \\sim q_œÜ(ùê≥|ùê±)$ (or not conditioned distribution $q_œÜ(ùê≥)$). And $g_œÜ$(ùõÜ,ùê±) is a deterministic differentiable transformation of an (auxiliary) noise variable ùõÜ.\nSince $\\~ùê≥$ is a transformation of ùõÜ, $\\~ùê≥$ follows the distribution p(ùõÜ) as well.\nMonte Carlo estimation\nTherefore, using Monte Carlo (i.e., averaging the L sampled values) to approximate an expectation of some function $f(ùê≥)$ w.r.t. the posterior approximation $q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)$ becomes:\n$$ ùîº_{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} [f(ùê≥)] = ùîº_{p(Œµ)} [f( g_œÜ(Œµ, ùê±‚ÅΩ‚Å±‚Åæ) )] \\\\ ‚âÉ \\frac{1}{L} ‚àë‚Çó‚Çå‚ÇÅ·¥∏ f( g_œÜ(Œµ‚ÅΩÀ°‚Åæ, ùê±‚ÅΩ‚Å±‚Åæ) ), $$ where ùõÜ‚ÅΩÀ°‚Åæ~ p(ùõÜ).\nApproximate lower bound with eq. (2)\nThe version A of SGVB estimator comes from eq. (2), the lower bound ùìõ should be finally equal to that ELBO expectation, ùìõ·¥¨ (ùõâ,ùõó,ùê±‚ÅΩ‚Å±‚Åæ) ‚âÉ ùìõ (ùõâ,ùõó,ùê±‚ÅΩ‚Å±‚Åæ) i.e., the KL divergence=0.\nAnd that expectation can be approximated via Monte Carlo (sampling), so the lower bound is approximated as:\n$$ \\~\\mathcal L^A (Œ∏,œÜ,ùê±‚ÅΩ‚Å±‚Åæ) = \\\\ \\frac{1}{L} ‚àë‚Çó‚Çå‚ÇÅ·¥∏ \\left[ log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ, ùê≥‚ÅΩ‚Å±\u0026rsquo;À°‚Åæ) - log\\ q_œÜ(ùê≥‚ÅΩ‚Å±\u0026rsquo;À°‚Åæ| ùê±‚ÅΩ‚Å±‚Åæ) \\right] \\\\ $$\nwhere $ùê≥‚ÅΩ‚Å±\u0026rsquo;À°‚Åæ= g_œÜ(Œµ‚ÅΩ‚Å±\u0026rsquo;À°‚Åæ, ùê±‚ÅΩ‚Å±‚Åæ))$ and ùõÜ‚ÅΩÀ°‚Åæ~ p(ùõÜ)\nApproximate lower bound with eq. (3)\nSince the KL-divergence $D_{KL}( q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) || p_Œ∏(ùê≥) )$ of eq. (3) can be integrated analytically, it can be substituted into eq. (3), then only that expectation (\u0026ldquo;expected reconstruction error\u0026rdquo; $ùîº_{q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ)} [log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ|ùê≥))]$) is approximated, so the second version of lower bound approximation: ùìõ·¥Æ (ùõâ,ùõó,ùê±‚ÅΩ‚Å±‚Åæ) ‚âÉ ùìõ (ùõâ,ùõó,ùê±‚ÅΩ‚Å±‚Åæ) is more relatively accurate than the version A.\n$$ \\tilde{\\mathcal L^B} = -D_{KL}( q_œÜ(ùê≥|ùê±‚ÅΩ‚Å±‚Åæ) || p_Œ∏(ùê≥) ) \\\\ \\qquad + \\frac{1}{L} ‚àë‚Çó‚Çå‚ÇÅ·¥∏ log\\ p_Œ∏(ùê±‚ÅΩ‚Å±‚Åæ | ùê≥‚ÅΩ‚Å±\u0026rsquo;À°‚Åæ) $$\nwhere $ùê≥‚ÅΩ‚Å±\u0026rsquo;À°‚Åæ= g_œÜ(Œµ‚ÅΩ‚Å±\u0026rsquo;À°‚Åæ, ùê±‚ÅΩ‚Å±‚Åæ))$ and ùõÜ‚ÅΩÀ°‚Åæ~ p(ùõÜ).\nThe KL-divergence there can be interpreted as regularizer, and the optimization objective is enlarging the \u0026ldquo;expected negative reconstruction error\u0026rdquo;, i.e., recover original ùê± from code ùê≥.\nTrain with minibatches\nGiven a dataset ùêó with N datapoints, each time M datapoints are drawn as a minibatch, then the marginal likelihood lower bound of the full dataset batch-by-batch is estimated as:\n$$ \\mathcal L(\\pmb{Œ∏,œÜ},ùê±) \\simeq \\mathcal L^M (\\pmb{Œ∏,œÜ},ùêó·¥π) = \\frac{N}{M} ‚àë·µ¢‚Çå‚ÇÅ·¥π \\tilde{\\mathcal L} (\\pmb{Œ∏,œÜ},ùê±‚ÅΩ‚Å±‚Åæ) $$\n2.4 Reparameterization trick Previously, ùê≥ is sampled directly, then input to model $p_Œ∏(ùê±|ùê≥)$, but the parameter œÜ of the distribution of ùê≥ is not able to be optimized via gradient descent, since Monte Carlo isn\u0026rsquo;t differentiable.\np ( z z ) p r i o r , q ·µ© ( z ) s a z M m . p z C l . i n g z - Œ∏ - x Instead of sampling the ùê≥ directly, but the ùê≥ is derived from the sampled ùõÜ, based on the deterministic differentiable transformation: ùê≥ = g·µ©(ùõÜ, ùê±).\np ( ùõÜ ) ùõÜ d i s t r i b u t i o s n a M m . p ùõÜ ùõÜ C l . i n g z = Œº + œÉ ùõÜ z - Œ∏ - x (The non-differentiable operation (M.C.) is put ahead of the leave nodes on the computational graph.)\nThen the parameters (e.g. Œº,œÉ¬≤) of ùê≥\u0026rsquo;s distribution can be learned by a network with parameters œÜ.\nx - œÜ l Œº o g œÉ ¬≤ z p ( = ùõÜ ) ùõÜ Œº d + i s œÉ s t m r ùõÜ a i p b l u i t n i g o n L z - ùõÜ Œ∏ - x R e c c l o t o n i s s o s t n r u - Such that the Monte Carlo estimate of the expectation is differentiable w.r.t. œÜ.\nThe reasoning is as follows:\nFor infinitesimals, there is $$q_œÜ(ùê≥|ùê±)‚àè·µ¢dz·µ¢ = p(\\pmb Œµ)‚àè·µ¢dŒµ·µ¢$$; where z·µ¢ is one of dimensions, dùê≥ = ‚àè·µ¢ dz·µ¢\nTherefore, $‚à´ q_œÜ(ùê≥|ùê±) f(ùê≥) dùê≥$ = ‚à´p(ùõÜ) f(ùê≥) dùõÜ = ‚à´ p(ùõÜ) $f(g_œÜ$(ùõÜ,ùê±)) dùõÜ\nUse Monte Carlo sampling approximation: $‚à´ q_œÜ(ùê≥|ùê±) f(ùê≥) dùê≥$ ‚âÉ 1/L ‚àë‚Çó‚Çå‚ÇÅ·¥∏ f( g·µ©(ùõÜ‚ÅΩÀ°‚Åæ, ùê±)), where ùõÜ‚ÅΩÀ°‚Åæ ~ p(ùõÜ)\nThe transformation (œÜ) from ùõÜ to ùê≥ can be learned by back-propagation and gradient descent from the reconstruction loss, since the transformation differentiable.\nSo the parameters (e.g. Œº,œÉ) of the approximate posterior distribution $q_œÜ(ùê≥|ùê±)$ of ùê≥ can be optimized through œÜ, and the parameter Œ∏ of the generative model is trained jointly.\nThis\u0026rsquo;s just a trick, the essence of the algorithm is still the coordinate ascent:\nSample a ùê≥ by sampling an ùõÜ:\n1 2 3 4 def reparameterize(mu, logvar): # logvar is logùõî¬≤ std = torch.exp(0.5 * logvar) # (e^{logùõî¬≤})^¬Ω eps = torch.randn_like(std) return mu + eps * std Use ùê≥ to generate ùê± with model $p_Œ∏(ùê±|ùê≥)$,\n1 2 3 def decode(z): result = nn.linear(latent_dim, hidden_dims)(z) return result then use ùê± to produce ùê≥ with model $q_œÜ(ùê≥|ùê±)$\n1 2 3 4 5 def encode(input): result = nn.linear(input_dim, hidden_dim)(input) mu = nn.Linear(hidden_dim, latent_dim)(result) logvar = nn.Linear(hidden_dim, latent_dim)(result) return [mu, logvar] With using reparameterization trick, both the two models can be optimized by gradient descent.\n(2024-04-16)\nReparameterization trick is a sampling method. It\u0026rsquo;s similar to inverse transform sampling: using a known distribution (uniform) to sample an unknown distribution.\nReparameterization trick makes the sampling from an unknown distribution differentiable, and enables the parameters of the distribution to be optimized.\nLoss func Loss function contains two parts: KL divergence and reconstruction error.\nKL divergence (i.e., cross entropy) can be computed with given the prior and assumed posterior of ùê≥.\nFor example, let the prior $\\rm p_Œ∏(ùê≥) = N(ùê≥,ùüé,ùêà)$ and assume the posterior q·µ©(ùê≥|ùê±) = N(ùê≥; ùõç, ùõî¬≤ùêà), then cross entropy can be derived by plugging them into Gaussian expression.\nThe reconstruction error is the log likelihood of the input datapoint log p(ùê±|ùê≥).\nFor a datapoint obeying multivariate Bernoulli (Yes or No), the log likelihood of ùê± is\n$$ log p(ùê±|ùê≥) = log ‚àè·µ¢‚Çå‚ÇÅ·¥∞ y·µ¢À£‚Å± (1-y·µ¢)¬π‚ÅªÀ£‚Å± \\\\ \\ = ‚àë·µ¢‚Çå‚ÇÅ·¥∞ [ x·µ¢log y·µ¢ + (1-x·µ¢)log (1-y·µ¢) ] $$\nwhere y·µ¢ should be like a probability (need to do sigmoid activation). So in this case, this loss term is a cross entropy.\nFor a datapoint following multivariate Gaussian distribution N(ùê±; ùõç, ùõî¬≤ùêà), refer to Su, Jianlin\n$$ \\begin{aligned} \u0026amp;p(ùê±|ùê≥) = \\frac{1}{‚àè·µ¢‚Çå‚ÇÅ·¥∞ \\sqrt{2œÄœÉ·µ¢¬≤(ùê≥)}} \\rm exp(-\\frac{1}{2} ‚Äñ\\frac{x-\\pmb Œº(ùê≥)}{\\pmb œÉ(ùê≥)}‚Äñ¬≤) \\\\ \\ \\\\ \u0026amp;log\\ p(ùê±|ùê≥) \\\\ \u0026amp;= log \\frac{1}{‚àè·µ¢‚Çå‚ÇÅ·¥∞ \\sqrt{2œÄœÉ·µ¢¬≤(ùê≥)}} \\ -\\frac{1}{2} ‚Äñ\\frac{x-\\pmb Œº(ùê≥)}{\\pmb œÉ(ùê≥)}‚Äñ¬≤ \\\\ \u0026amp;= -‚àë·µ¢‚Çå‚ÇÅ·¥∞ log \\sqrt{2œÄœÉ·µ¢¬≤(ùê≥)} -\\frac{1}{2} ‚Äñ\\frac{x-\\pmb Œº(ùê≥)}{\\pmb œÉ(ùê≥)}‚Äñ¬≤ \\\\ \u0026amp;= -‚àë·µ¢‚Çå‚ÇÅ·¥∞ [\\frac{1}{2} (log2œÄ + logœÉ·µ¢¬≤(ùê≥))] - \\frac{1}{2} ‚Äñ\\frac{x-\\pmb Œº(ùê≥)}{\\pmb œÉ(ùê≥)}‚Äñ¬≤ \\\\ \u0026amp;= -\\frac{D}{2} log2œÄ -‚àë·µ¢‚Çå‚ÇÅ·¥∞logœÉ·µ¢¬≤(ùê≥) -\\frac{1}{2} ‚Äñ\\frac{x-\\pmb Œº(ùê≥)}{\\pmb œÉ(ùê≥)}‚Äñ¬≤ \\\\ \\end{aligned} $$\nNormally, the variance ùõî¬≤ will be fixed, so this loss term is only related with mean ùõç(ùê≥):\n-log p(ùê±|ùê≥) ~ $\\frac{1}{2\\pmb œÉ¬≤}\\| ùê±-\\pmb Œº(ùê≥) \\|¬≤$\nTherefore, this loss term is MSE.\n","date":"2023-06-07T10:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/b-note-vae/","title":"read: Gen - ELBO | VAE"},{"content":"Code | Arxiv\nNotes Abstract (2023-05-24)\nModel the radiance field as a 4D tensor.\nOptimize a 4D tensor? Optimize a 4D matrix? MLP (network) is a 2D matrix? ùêó @ ùêñ·µÄ = ùêò; Nonlinear regession cannot be solved by linear algebra? Gauss-Newton algorithm (2023-05-29)\nTerminology:\n\u0026ldquo;4D scene tensor\u0026rdquo;: (x, y, z, feature). Each voxel is allocated with a sigma and appearance feature. (Alike a feature field?).\n(2023-10-19) Features on each point derived from the coefficients of each vector in each basis. \u0026ldquo;factorize\u0026rdquo;: Encode the data into \u0026ldquo;coordinates\u0026rdquo; (coefficients) in the principle components;\n\u0026ldquo;multiple compact\u0026rdquo;: Principle components are orthogonal between each other;\n\u0026ldquo;low-rank\u0026rdquo;: The number of principle components is small. Rank=1 is a vector, rank\u0026gt;1 will depend on the rank of the matrix;\n\u0026ldquo;component\u0026rdquo;: The most \u0026ldquo;iconic sample\u0026rdquo;, which can be reshaped (reconstructed) to a data that has the original dimensionality,\nIn other words, it\u0026rsquo;s a \u0026ldquo;space\u0026rdquo; constituted by several directions; e.g., an image space has X and Y 2 directions.\nIn TensoRF, a vec + a mat is a component.\nFor a scene placed in a cube, one of components is an equal-sized cube, and the 3 directions are the X,Y,Z axes.\n(2023-10-19) Because the scene is the reconstruction goal, it served as the template of each basis. If the scene isn\u0026rsquo;t be reconstructed directly, for example, reconstring each ray (ro,rd,l,s), what\u0026rsquo;s the basis then?\nA 1D signal\u0026rsquo;s basis is sin+cos.\n(2023-10-19) Voxels are discrete samples from a continuous scene function. Coefficients on vectors of a basis are inner product between the scene function and each basis. Given some sample points (voxels), their coefficients are the coordinates of their projection onto the basis.\n\u0026ldquo;mode\u0026rdquo;: A principle component, a coordinate system, a space, a pattern;\nIntroduction \u0026ldquo;CAN-DECOMP/PARA-FAC\u0026rdquo;: Every direction in a component is a 1-dimensional vector.\n\u0026ldquo;Vector-matrix decomposition\u0026rdquo;: two directions out of 3 are jointly represented by a \u0026ldquo;frame\u0026rdquo; (plane),\nSo the obtained factors for 1 component are 1 vector and 1 matrix.\nVM decomposition spends more computation for more expressive components.\nThe \u0026ldquo;similarity\u0026rdquo; is computed \u0026ldquo;frame-by-frame\u0026rdquo;, so it needs more calculation. And the original structure is more kept than 2 dimensions are analyzed individually, so the components could be more representitive and less components are needed.\nBetter space complexity: O(n) with CP or O(n¬≤) with VM,\nComparing with optimizing each voxel directly, which is O(n¬≥), optimizing factors takes less memory.\nGradient descent\nThey\u0026rsquo;re not encoding the radiance field into factors because the feature grid/tensor is unknown. They decompose the feature grid to simplify the optimization, which then turns to optimizing factors.\nImplementation A scene is a radiance field.\nRadiance field (4D tensor) = Volume density grid (3D tensor: X,Y,Z) + Appearance feature grid (4D tensor: X,Y,Z,feat)\nRadiance field is a tensor of (X,Y,Z,œÉ+feature), where volume density œÉ is 1D, appearance feature is 27D;\n1D Volume density (feature) is decomposed to 3 vectors (CP) or 3 vector-matrix combo (VM) for each component.\n27D Appearance feature is amortized into 3 vector-matrix combos for 16 components.\nThese components are coefficients to fuse the data \u0026ldquo;basis vectors\u0026rdquo; in different ways, which is acting like a network.\n1D density and 27D features are optimized jointly with coefficients.\n(2023-10-28) Features and coefficients are optimized simultaneously. R a d i a n c e f i e l d = V o l u m e d e n s i t y ‚úö S c S ( o ( m a f p p 2 e o p 7 a n D t e f n e t a ‚ú∂ t u r e 3 R s D G ‚ú∂ B c c o o = m m p p o o n n e e n n t t ) ‚ü© where S is two-layer MLP: 150 ‚Üí 128 ‚Üí 128 ‚Üí 3\n(2023-10-28) I guess the authors came up with tensor decomposition because they realized that Positional Encoding is Fourier decomposition, And NeRF then used MLP to learn the coefficients for the decomposed \u0026ldquo;Fourier basis\u0026rdquo;: sin and cos. Interpolate vector \u0026amp; matrix Instead of performing tri-linear interpolation at a point by computing 8 corners, a point at arbitrary position is computed by interpolating th vector and matrix.\nt r i - l i n e a r V . S . l i n e a r √ó b i - l i n e a r Code Notes Steps overview Dataset includes all_rgbs and corresponding ro, and normalized rd under world space all_rays, (N_imgs√óH√óW, 3)\nSplit the bounding box $[^{[-1.5, -1.5,\\ -1.5]}_{[1.5,\\quad 1.5,\\quad 1.5]}]$ into a voxel grid of the given resolution [128,128,128] determined by the number of voxels args.N_voxel_init\n1 reso_cur = N_to_reso(args.N_voxel_init, aabb) - 1 . 5 , - 1 . 5 , - 1 . 5 V ) o x ‚ï∞ e ‚ïå l ‚ïå ‚ïå s ‚ïå i ‚ïå z ‚ïå e ‚ïå ‚ïå 3 3 ‚ïå / z ‚ïå 1 ‚ïå 2 ‚ïå 8 ‚ïå , y ‚ïå ‚ïå s ‚ïØ t e p S i z ( x e 1 . i 5 s , a 1 . h 5 a , l f 1 . 5 ) Sampling number nSamples = voxel grid diagnoal ‚àõ(128¬≤+128¬≤+128¬≤) / stepSize\nLearnable Parameters init_svd_volume() creates basis vectors and matrices. A vector and a matrix are orthogonal because they\u0026rsquo;re a side and a plane of a cube.\nParameters to be optimized: 3 Vectors and 3 Matrices\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def init_one_svd(self, n_component, gridSize, scale, device): plane_coef, line_coef = [], [] # 3 kinds of vec-mat combo, each combo has 16 components; for i in range(len(self.vecMode)): # vecMode: [2, 1, 0] vec_id = self.vecMode[i] # matMode: [(0,1),(0,2),(1,2)] mat_id_0, mat_id_1 = self.matMode[i] # (1, 16, 128, 128) plane_coef.append(torch.nn.Parameter( scale * torch.randn((1, n_component[i], gridSize[mat_id_1], gridSize[mat_id_0])))) # (1, 16, 128, 1) line_coef.append(torch.nn.Parameter( scale * torch.randn((1, n_component[i], gridSize[vec_id], 1)))) return torch.nn.ParameterList(plane_coef).to(device), torch.nn.ParameterList(line_coef).to(device) Given a 3D tensor, it can be decomposed as Vector-Matrix combo in three directions:\n= ‚úö ‚úö Each direction has 16 components. In other words, an observation of the cube from a direction can be reconstructed by summing those components up.\ndoubt: These 3 directions are orthogonal because the cube is viewed from distinct directions, but how are those 16 channels guaranteed to be orthogonal?\n(2023-10-17) Based on the theory of PCA? (2023-10-28) Is it possible that 16 components are parallel instead of orthogonal? They\u0026rsquo;re summed directly, similar to an FC layer with 16 neurons representing 16 ways of combining features. doubt: Are those components parallel to each other? Do they have different importance or priority?\n(2023-06-22) I guess no. They\u0026rsquo;re just added together simply. A scene is decomposed to a set of components, then a scene can be reconstructed using a set of coefficients of those components.\nSepcificlly, each voxel is a summation of the products for corresponding projections on vector and matrix in 3 directions and 16 components.\nBased on those vector-matrix components, with the help of interpolation, the value at any location can be obtained.\nFiltering rays Filter the effective rays based on the ratio (deviation) betweem the direction of rays and the direction of bounding box corners.\nMask those rays inside bbox by compareing the ratio of the rd to the direction of the two bbox corners:\n1 2 3 4 5 6 7 8 9 10 11 12 13 if bbox_only: # Avoid denominator of 0, normalized rd, (chunk, 3) vec = torch.where(rays_d == 0, torch.full_like(rays_d, 1e-6), rays_d) # ratio of the direction of bbox corner xyz_min to testing ray rate_a = (self.aabb[1] - rays_o) / vec # (chunk, 3) # ratio of the direction of bbox corner xyz_max to testing rd rate_b = (self.aabb[0] - rays_o) / vec t_min = torch.minimum(rate_a, rate_b).amax(-1) # [chunk] t_max = torch.maximum(rate_a, rate_b).amin(-1) # [chunk] # rays located inside the bbox mask_inbbox = t_max \u0026gt; t_min An effective ray should end up inside the bounding box,\nr o v e c ` x y ` z _ r ` m d i ‚ú© n x y z _ m a x ‚ú© is an effective ray, while ‚óØ is a non-effective ray because it\u0026rsquo;s out of the bbox.\nReconstruct sigma The sigma value on each voxel is the summation of 16 components of all 3 directions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1D scalar: (args.batch_size*N_samples) sigma_feature = torch.zeros((xyz_sampled.shape[0],), device=xyz_sampled.device) # traverse 3 modes for idx_plane in range(len(self.density_plane)): # interpolate tensor (1,16,128,128) at coordinates (batch_size*N_samples, 1, 2) plane_coef_point = F.grid_sample(self.density_plane[idx_plane], coordinate_plane[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1]) # (16, batch_size*N_samples) # interpolate tensor (1,16,128,1) at coordinates:(bs*N_samples, 1, 2) line_coef_point = F.grid_sample(self.density_line[idx_plane], coordinate_line[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1]) # (16, batch_size*N_samples) # accumulate 16 components of 3 directions for each voxel, (batch_size*N_samples) sigma_feature = sigma_feature + torch.sum(plane_coef_point * line_coef_point, dim=0) The factors grid is composed of 3 planes self.density_plane and 3 vectors self.density_line\nThe factors corresponding to the sampled 3D points are obtained by F.grid_sample():\np l a n e \" 0 d - e 1 s \" n i t ‚¨£ 1 y 6 p c l ‚¨£ h a a n n e n c ‚¨£ e ( o l 1 o s 2 r 8 d ‚¨£ , i 1 n 2 a 8 t ) e _ 0 p l a n 2 e \" 1 1 - 2 \" c \" o 0 o \" r , d \" i 1 n \" a , t d e o e _ r n ( l s 1 i \" i 2 n 2 t 8 e \" y , 1 l ) i n e Retrieve the \u0026ldquo;factor in the vector direction\u0026rdquo; for each sample voxel is like the above figure right.\ns f i e g a m t a u r e = p v r ( e o 1 V c j 6 e t e c o c c r t o - i m 2 o Ôºã ‚ãÆ p x √ó n o n p e m r n M a o t a t j s t r e ) i c x t - i 0 o 1 n ‚úö p v r e o ( V c j 1 e t e 6 c o c r t c - i Ôºã o 1 o ‚ãÆ m x √ó n p o p n m r e M a o n a t j t t r e s i c ) x t - i 0 o 2 n ‚úö p v r e o ( V c j 1 e t e 6 c o c r t Ôºã c - i o 0 o ‚ãÆ m x √ó n p o p n m r e M a o n a t j t t r e s i c ) x t - i 1 o 2 n doubt: Were the coefficients not multiplied with basis vector, but simply summed up together as the reconstruction?\n(2023-06-29) TensoRF is not projecting voxel onto each basis vector (matrix), but retrieving the coefficients from the factor grid.\nWhat the TensoRF retrieved is the coefficient * vector because it samples the factor grid directly. The factor grid satifies orthogonality naturally, so a coefficient inside is equivalent to having already multiplied with basis vectors.\nReconstruct appear. feature For appearance feature of each voxel, the vector projection and matrix projection in 3 directions are concatenated together, then multiply together:\n1 2 3 4 5 6 7 8 9 10 11 12 plane_coef_point, line_coef_point = [],[] for idx_plane in range(len(self.app_plane)): plane_coef_point.append(F.grid_sample(self.app_plane[idx_plane], coordinate_plane[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1])) line_coef_point.append(F.grid_sample(self.app_line[idx_plane], coordinate_line[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1])) plane_coef_point, line_coef_point = torch.cat(plane_coef_point), torch.cat(line_coef_point) return self.basis_mat((plane_coef_point * line_coef_point).T) 1 1 l 4 p 4 i 4 l 4 f n a e 2 e c n c a a 7 _ h e h p t c a _ a p u d o n c n r i e n o n e m f e e e = = l = f l s s ( 4 ( ‚ù≤ p 8 p 4 v r m r 8 V e o c a o 1 e c j ‚ãÆ o t j ‚ãÆ c 4 c t e m r e o 4 o c p i c m √ó x r t o x t p d - i n - i o i M 2 o e 0 o n m a n n 1 n e t t n s t ‚ù≥ ) s ) √ó ‚äï ‚äï ( 1 4 ( 4 p 8 m p 4 4 v r a r 8 x e o c t o 2 c j ‚ãÆ o r j ‚ãÆ c 7 t e m i e o o c p x c m b r t o - t p a - i n 0 i o s 1 o e 2 o n i n n n e s t n s t m ) s a ) t ‚äï ‚äï ( 4 ( p 8 m p 4 v r a r 8 e o c t o c j ‚ãÆ o r j ‚ãÆ c t e m i e o o c p x c m r t o - t p - i n 1 i o 0 o e 2 o n n n n e t n s t ) s ) Then RGB is mapped from the appearance featrue:\nr g 3 b = M L P ‚ù≤ f e a a 2 p t 7 p u r e ‚äï v d i i 3 e r w ‚äï a p p e p f 1 e 0 ‚äï a 8 t v i p 1 e e 2 w ‚ù≥ Optimizing The learnable parameters includes:\n16 vectors and 16 matrices for density in 3 directions; 48 vec and 48 mat for app feature in 3 directions; linear layer transforming 48x3 dim appearance feat to 27D; linear layer mapping 27D feat+viewdir to 3-dim rgb. flowchart LR x(\"Sample a point\") y(\"Reconstruct its sigma and rgb by aggregating its components\") b(\"Use BP+GD (Adam) to optimize those parameters\") x --\u003e y --\u003e b Losses: L2 rendering loss + L1 norm loss + Total Variation loss.\nTV loss benefits real datasets with few input images, like LLFF. Q\u0026amp;A How does it ensure that the components are orthogonal during training? (2023-06-10) Reference {ËÆ∫Êñá‰ª£Á†ÅÈòÖËØª}TensoRF: Tensorial Radiance Fields - Áà±Áù°ËßâÁöÑ‰∫∫‰∫∫\n","date":"2023-05-29T18:18:00Z","image":"https://apchenstu.github.io/TensoRF/img/pipeline.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-tensorf/","title":"read: Render - NVS | TensoRF"},{"content":"Repo\nThis diagram is supported by Hugo.\nAscii-art Archive\nTrees 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 Overlaps Line Decorations Line Ends Dot Grids ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ Large Nodes A 1 2 3 4 B 5 C 6 8 7 D Small Grids a b a b a b a b Big Grids A B A B A B Complicated \u0026amp; A M S i o i q f b B x u j o e a ( - x d r a \u0026gt; e f R \u0026gt; c o C n u o b ( n r ) ) d n e e J d r o s i n N o R D t o i u a a n g d d l o i t n e D i a g o n a l s C V u e r r v t e i d c a l n o t A N C : o u l d r r i A a / I v n s i n e e h s t d - e - t r l B i h i i s i o n ' s r e q n . u * o o b t t o e l a s d ' * l i n e D o n S e e ? a r c 3 h Ascil art cube , 1 2 8 9 6 9 6 ` ` . . ` ` ` ` . . ` ` . . ` . ` ` . ` . Coordinates z x p l a n e y f g ‚ÇÅ : ‚ÇÅ : x a ‚ÇÅ f g ‚ÇÇ : ‚ÇÇ : x ‚ÇÇ f : g : : : x ‚ÇÉ ‚ÇÉ ‚ÇÉ f : g : : : : x ‚Çô ‚Çô ‚Çô b f g ( ( x x x ) ) f ‚ÇÅ x ‚ÇÅ f f ( ‚ÇÇ x x ‚ÇÇ ) f x ‚àë ‚ÇÉ ‚ÇÉ _ { k ‚àà ‚Ñ§ } Œ¥ ( f x t ‚Çô ‚Çô - k T ) x w w ( x ‚ÇÄ , x S ‚ÇÅ c ) r ·µÄ e e n I k n e x t r ‚ÇÇ e n g e ( P c ( r l O r e x a s r o n ‚ÇÄ l s t j t , œï R p h e e x ( a a o c r ‚ÇÅ œÜ y c g t , ( e o i r x ùêÆ n o ' ‚ÇÇ ) a n ( ) ) l ùê± ·µÄ œï t ) ) ùêñ r N a ùêÆ D n + C s ( b f P r œÜ C s o ( a p s ùêÆ m a p ) e c e r e c a t œÜ t i ùêå r v v a e ùêÆ i n ) + e s c w f K O e b r j n e e c l t P i s f r n p e a ‚äó f c i e h l t e r ùêÆ NeRF vs EWA:\nP r o j c e a c m t e r p a o i 1 n t s P r o j e c t R p r a a a l s y r y i c s a n r l s e e ‚à´ i a l p e L s r e a s n i e l c e n f e g e o i s o n s t e p g r i n t PixelNeRF:\nP r ‚ó∏ o j e c v t i e p w o i 1 v n i t e s w 2 Ellipses . \" ~ ‚Ä¢ - - - o ~ . \" \" - _ \" - - - ‚Ä¢ - - - ‚àò - . - _ \" - \" . \" ‚àò ‚Ä¢ \" . Splat\nG a i 2 u a D s n s s S - c r e e n . \" ‚Ä¢ \" . V D o e l p u t m h e h . \" d e ~ a r t e a i h s a s ‚Äñ ùê≠ b ‚Äñ ‚Ä¢ e ‚ÇÇ e . n C t o h v r o m ~ w a . \" n t r o i n x t o i s t h ùö∫ e ' . s c r e e n ","date":"2023-05-29T09:58:00Z","permalink":"https://zichen34.github.io/writenotes/lang/go_goat/","title":"memo: GoAT Ascii Diagram"},{"content":"Course page: AM301\nSingular Value Decomposition Source video: Lecture: The Singular Value Decomposition (SVD) - AMATH301\nlinalg: Matrix acts on vector All what matrix multiplication does is it rotates and stretch the vectors.\nPrincipal Component Analysis Source video: Lecture: Principal Componenet Analysis (PCA)\nPhysical example Considering a physical system as the following, a rod suspends a mass via a spring.\nr o d m s p k r i | n g f ( z ) Pull the mass down and let it oscillate up and down.\nThe displacement (position) of the mass oscillation can be measured by a function f(z). Then f(z) can be solved based on Newton\u0026rsquo;s second law:\n$$ \\begin{aligned} F \u0026amp;= ma = m \\frac{d¬≤f(z)}{dz¬≤} \\\\ \\ \\\\ -w¬≤f(z) \u0026amp;= m \\frac{d¬≤f(z)}{dz¬≤} f(z) = Acos(wz + w‚ÇÄ) \\end{aligned} $$\nData-driven approach can solve it too.\nSuppose the law $F=ma$ is unknown, infer the F=ma from the data alone. First of all, the complexity of this system should be figured out.\nMeasure this system using cameras\nc a m e r a 1 r o d m s p k r i | n g f ( z ) c a c m a e m r e a r a 3 2 Every camera records the mass coordinates on their projection plane:\nCamera 1: (ùê±$_a$, ùê≤$_a$); Camera 1: (ùê±$_b$, ùê≤$_b$); Camera 1: (ùê±$_c$, ùê≤$_c$).\nArrange them into a data matrix ùêó (6-rows):\n$$ ùêó = \\begin{bmatrix}ùê±_a \\\\ ùê≤_a \\\\ ùê±_b \\\\ ùê≤_b \\\\ ùê±_c \\\\ ùê≤_c \\end{bmatrix} $$\nTwo fundamental issuses associated with these data need to be addressed.\nNoise\nData with noise on top of it is not a good representation of the system.\nRedundancy\nMeasurements are not independent to each other, i.e., x and y are related. Different cameras take the similar infomation just from different angles.\nThe movement is only one degree of freedom, but the observed data has six sets.\nOne doesn\u0026rsquo;t know how to take the perfect observation ahead of time. PCA will reveal which camera at which angle is enough to describe the whole system.\nVariance and Covariance Assumption: big variance score means that vector is chaning a lot. It has a lot stuff happening.\nIf the diagonal terms that are big in the covariance matrix , those vectors are matter. Vectors having small variance don\u0026rsquo;t change much. MAKE DIAGNOAL (SVD) is remove all the redundancy\nPCA for Face Recognition Source video: Lecture: PCA for Face Recognition\nA 2D image are flattened into a vector. So each column in the U returned from SVD is an image. So a column essentially contains two directions: x and y. ","date":"2023-05-27T22:20:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/02_svd/","title":"watch: Nathan 02 | SVD"},{"content":"Jacobian matrix Khan Academy\nPrerequisite Matrix is a linear transformation of space by moving the basis vectors to new landing spot.\n$$ \\begin{bmatrix} 2 \u0026amp; -3 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\rightarrow \\begin{bmatrix} 2x + (-3)y \\\\ 1x+1y \\end{bmatrix} $$\n\u0026ldquo;linear\u0026rdquo;: grid lines remain parallel, evenly spaced and straight lines after transformation.\nTransform\nThe two basis vectors $[^1_0]$ and $[^0_1]$ are moved to $[^2_1]$ and $[^{-3}_{1}]$, which are the columns of the matrix.\n$$ \\begin{array}{ccc} \\begin{bmatrix} 2 \u0026amp; -3 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2+0 \\\\ 1+0 \\end{bmatrix} \\\\ \\\\ \\begin{bmatrix} 2 \u0026amp; -3 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0-3 \\\\ 0+1 \\end{bmatrix} \\end{array} $$\nReconstruct\nThe basis vectors $ùê¢ = [i‚ÇÅ,i‚ÇÇ], ùê£ = [j‚ÇÅ,j‚ÇÇ]$ of the new space are completely made up by original basis vectors $[^1_0]$ and $[^0_1]$:\n$$ \\begin{array}{ccc} ùê¢ = \\begin{bmatrix} i_1 \\\\ i_2 \\end{bmatrix} = \\begin{bmatrix} 2√ó1 + (-3)√ó0 \\\\ 1√ó1 + 1√ó0 \\end{bmatrix} \\begin{matrix} \\text{first component}\\\\ \\text{second component}\\end{matrix} \\\\ \\\\ ùê£ = \\begin{bmatrix} j_1 \\\\ j_2 \\end{bmatrix} = \\begin{bmatrix} 2√ó0 + (-3)√ó1 \\\\ 1√ó0 + 1√ó1 \\end{bmatrix} \\begin{matrix} \\text{first component}\\\\ \\text{second component}\\end{matrix} \\end{array} $$\nRecord\nMatrix records synthesising factor for transforming each original basis vector to the new basis vector:\nThe first row of the matrix is the combination factors for the original first basis vector to form the first component of the new first basis vector.\nThe second row corresponds to the second component of the new first basis vector, by re-combining the original first basis vector. And so forth.\nLiner Operator\nMatrix operator ùë≥ takes in a vector and spit out a vector.\nBased on the properties of linearity: scaling and adding, i.e.,\nL(aùêØ) = aL(ùêØ) L(ùêØ+ùê∞) = L(ùêØ) + L(ùê∞) Applying the operator on a vector $[^x_y]$ can be represented as:\n$$ \\begin{aligned} L(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) \u0026amp;= L(x \\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} + y \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}) \\\\ \u0026amp;= x L(\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}) + y L(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}) \\end{aligned} $$\nLet $[^x_y]$ equals to $[^2_1]$, the result vector should be 2 times of $L([^1_0])$ plus 3 times of $L([^0_1])$, i.e., $[^1_3]$\nThat means matrix transforms the space by modifying the original basis vectors. The coordinates of the given vector have no change either in the original space or the new space.\nSo the transformation can be found by determining the coefficient of the change of each component.\nThe first column in the transformation matrix is landing spot of the first basis vector (i.e., x axis)\nlocal linearity Nonlinear function $f([^x_y]) = \\rm [^{x+sin(y)}_{y+sin(x)}]$ has local linearity. Therefore, the local linear transformation can be represented by a 2x2 matrix\nJacobian matrix There are multiple functions and multiple variables. Jacobian records the effects of each variable on each function. Its organization comes down to local linearity, which allows the nonlinear transformation to be represented as a linear transformation.\nJacobian determinant Jacobian determinant is the factor of the neighbor area around a point scratched or squished by the local \u0026ldquo;linear transformation\u0026rdquo;.\ntodo: What is Jacobian? | The right way of thinking derivatives and integrals - Mathemaniac\nGauss-Newton algorithm ÊúÄ‰ºòÂåñÁÆóÊ≥ï‰πãÈ´òÊñØÁâõÈ°øÊ≥ï-path-int\nÁî®ÈõÖÂÖãÊØîÁü©ÈòµÁöÑ‰πòÁßØ J(ùê±) J(ùê±)·µÄ ‰ª£Êõø Hessian Áü©Èòµ\n-ÊãºÂáëÊ¢¶ÊÉ≥-\nÂàùÂÄº‰ºöÂΩ±ÂìçËÉΩÂê¶Ëµ∞Âà∞ÂÖ®Â±ÄÊúÄ‰ºò\nËßÜËßâslam ÂçÅÂõõËÆ≤Á¨¨6Á´†\nGauss-Newton algorithm for solving non linear least squares explained\nÔºàÂπ≤Ë¥ßÔºâ„ÄäÈõÖÂèØÊØîÁü©ÈòµÊòØ‰ªÄ‰πà‰∏úË•ø„Äã3Blue1BrownÔºåÊê¨Ëá™ÂèØÊ±óÂ≠¶Èô¢„ÄÇ „ÄêËá™Âà∂‰∏≠ÊñáÂ≠óÂπï„Äë\n(2023-12-24) \u0026ldquo;In [8], an LSTM [18] is used to model the Levenberg-Marquardt (LM) algorithm and predicts the update at each step directly.\u0026rdquo; mentioned in Fast-MVSNet.\nLevenberg Marquadt code example\npaper METHODS FOR NON-LINEAR LEAST SQUARES PROBLEMS\n","date":"2023-05-26T13:58:00Z","image":"https://img.youtube.com/vi/Vnga_psnCAo/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/calc/nonlinear_least_squares/","title":"memo: Calc | Nonlinear Least Squares"},{"content":"Book site; book\n‚Ö†. Curve Fitting Sec 4.1: Classic Curve Fitting and Least-Squares Regression\nRegression ‚âà curve fitting ‚âà ùêÄùê±=ùêõ (Least-square fitting), where ùêÄ is the data, ùê± is the parameters of the model, ùêõ is the target.\nRegression: Fitting a model to some data with some parameters\nOver- and Under-determined Models could be Over- and Under-determined.\nOver-determined system normally has No solution.\nThere\u0026rsquo;re massive constraints (equations, samples) given, but the complexity (#variables) of the system is not enough to describe the existing data.\nAn over-determined system can be \u0026ldquo;No solution\u0026rdquo; or \u0026ldquo;Infinitely many solution\u0026rdquo; Over~ wiki\nHomogeneous case: (no bias)\nThe coefficient matrix is a tall, skinny matrix, the \u0026ldquo;all-zero solution\u0026rdquo; always holds. If there\u0026rsquo;re enough equations are dependent, and after eliminating #non-zero row $\u0026lt;$ #cols in the coefficient matrix in row-echelon form, this over-determined homogeneous system has \u0026ldquo;infinitely many solutions\u0026rdquo; (including all-0 solution). Otherwise, \u0026ldquo;all-zero\u0026rdquo; is the only solution. Non-homogeneous case:\nIf the last non-zero row of the augmented matrix in row echelon form is only having the constant entry of the last column (giving an equaltion 0=c), this system has \u0026ldquo;No solution\u0026rdquo;.\nSince it\u0026rsquo;s a tall matrix, this case is likely to happen:\n$$ \\left[ \\begin{array}{cc|c} 2 \u0026amp; 1 \u0026amp; -1 \\\\ -3 \u0026amp; 1 \u0026amp; -2 \\\\ -1 \u0026amp; 1 \u0026amp; 1 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc|c} 2 \u0026amp; 1 \u0026amp; -1 \\\\ 0 \u0026amp; 5/2 \u0026amp; -7/2 \\\\ 0 \u0026amp; 3/2 \u0026amp; 1/2 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc|c} 2 \u0026amp; 1 \u0026amp; -1 \\\\ 0 \u0026amp; 5/2 \u0026amp; -7/2 \\\\ 0 \u0026amp; 0 \u0026amp; 13/5 \\\\ \\end{array} \\right] $$\nSince the equations are way more than unknowns, the coefficient matrix in row echelon form is most likely not having: #non-zero rows = #cols. So the case \u0026ldquo;single unique solution\u0026rdquo; is almost impossible, but there\u0026rsquo;ll be \u0026ldquo;Infinite many solutions\u0026rdquo;.\nUnless, enough rows can be eliminated (lines overlap), and the remaining coefficient matrix has the same rank as the augmented matrix, and also the #non-zero rows = #cols in the coefficient matrix, this system has \u0026ldquo;single unique solution\u0026rdquo;.\n(2024-05-31)\nÂ¶ÇÊûúÁ∫¶ÊùüÂ§™Â§öÔºåÂ∞±ÂèòÊàê‚ÄúË∂ÖÂÆöÁ≥ªÁªü‚Äù‰∫ÜÔºåÈÄöÂ∏∏Êó†Ëß£Ôºõ Ê≠§Êó∂Â∞±Ë¶ÅÂºïÂÖ•Êõ¥Â§öÁöÑ‚ÄúÊú™Áü•Èáè‚ÄùÔºåÂèòÊàê‰∏Ä‰∏™‚ÄúÊ¨†ÂÆöÁ≥ªÁªü‚ÄùÔºåÂ∞±ÊúâÊó†Á©∑Â§öËß£‰∫Ü\n‰πãÂâçÊàëËØ¥ËÄÅÂºüÈ°æÂøåÁöÑ‰∏úË•øÂ§™Â§öÔºåÊó†Ëß£ÔºõÊâÄ‰ª•Ë¶ÅÂºïÂÖ•‚ÄúÂèòÈáè‚Äù„ÄÇÂ§öÁúãÂ§öÂ∞ùËØï‰∏çÂêåÁöÑ‰∏úË•øÔºåÊúâ‰∫∫Â∞±ÊúâÂèòÊï∞ÔºåÂèòÂàôÈÄö„ÄÇ\nUnder-determined system normally has infinitely many solutions.\nBecause the #parameters is more than equations (samples), some variables are not restricted, which caused the infinitude of solutions. In other words, there\u0026rsquo;re not enough equations to uniquely determine each unknown. So the problems is how to determine which solution is the best.\nAn underdetermined linear system has either No solution or Infinitely many solutions.Under~ wiki\nHomogeneous case: The coefficient matrix is a short, fat matrix, #non-zero rows $\u0026lt;$ #cols. So the system always has \u0026ldquo;Infinitely many solutions\u0026rdquo; (including all-0 solution). Non-homogeneous case: Rank of augmented matrix \u0026gt; Rank of coefficient matrix: No solution Rank of augmented matrix = Rank of coefficient matrix: There must be some solutions. But since the #row is already less than #cols for the coefficient matrix, the single unique solution (except for all-0) is impossible. So this system indeed has infinitely many potential solutions. $$ \\left[ \\begin{array}{ccc|c} 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{ccc|c} 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \\end{array} \\right] $$ How to solve If there\u0026rsquo;re infinite optional solutions, then the best model should be selected according to other constraints, i.e., regularizers g(ùê±).\nFor instance, the penalty for L2 norm of the parameters vector ‚Äñùê±‚Äñ‚ÇÇ can lead to the model with smallest mse; And the penalty for L1 norm of the parameters vector ‚Äñùê±‚Äñ‚ÇÅ can lead to the model with sparse number of parameters.\nTherefore, the objective is:\n$argmin_ùê± g(ùê±)$ subject to ‚ÄñùêÄùê±-ùêõ‚Äñ‚ÇÇ=0.\nIf some error can be tolerated to just find the model that has the minimum L2-norm params, the constraint can be relaxed by a small error, like: ‚ÄñùêÄùê±-ùêõ‚Äñ‚ÇÇ‚â§Œµ\nIf there\u0026rsquo;s no solution, the expected model should have the minimum error.\nTherefore, the objective is: $argmin_ùê±$ (‚ÄñùêÄùê±-ùêõ‚Äñ‚ÇÇ).\nAdding regularizers can confine the optimizer to find the right model, so the objective can be\n$argmin_ùê±$ (‚ÄñùêÄùê±-ùêõ‚Äñ‚ÇÇ + Œªg(ùê±))\nOrdinary least squares gives the approximate solutions (when no exact solution exists) or the exact solution (when it exists) by minimizing the square error:\n$argmin_ùê±$ ‚ÄñùêÄùê±-ùêõ‚Äñ\nIts solution is ùê± = (ùêÄ·µÄùêÄ)‚Åª¬πùêÄ·µÄùêõ. And using QR factorization of A to solve the least squares can achieve good numerical accuracy.\nMore broadly, this generic architecture can be generalized to non-linear regressions by replacing ‚ÄñùêÄùê±-ùêõ‚Äñ‚ÇÇ to non-linear constraints f(ùêÄ,ùê±,ùêõ):\n$$ argmin_ùê± ( f(ùêÄ,ùê±,ùêõ) + Œªg(ùê±)) \\quad \\text{(Over-determined) or} \\\\ argmin_ùê± g(ùê±) \\ \\text{subject to } f(ùêÄ,ùê±,ùêõ) ‚â§ Œµ \\quad \\text{(Under-determined)} $$\nAn over-determined non-linear system can be solved by Gauss-Newton iteration\nOver- and Under-fitting Two canonical issues:\nOver-fitting: With enhencing the model complexity, the training error keeps dropping, but the evaluating error on withhold data goes up.\nUnder-fitting: No matter how you increase the model complexity (parameters), the training error doesn\u0026rsquo;t drop, either that\u0026rsquo;s a bad model or there\u0026rsquo;s not enough data.\nRegression framework Generic Regression: ùêò = ùëì(ùêó,ùõÉ), input ùêó into model ùëì, then target ùêò can be obtained.\nRegression needs 2 things: select a model ùëì and find its parameters beta that can map X to Y.\nDifferent norms are used to measure the error: L‚àû norm, L1 norm, L2 norm. And the selected norm (error metric) has big impact on the \u0026ldquo;Goodness of Fit\u0026rdquo;.\n‚Ö°. Nonlinear Regression Sec 4.2: Nonlinear Regression and Gradient Descent\n‚Ö¢. Regularization Sec 4.3: Over- and Under-determined Systems; Code demo\n‚Ö£. Optimization Sec 4.4: Optimization for regression\nOptimization is the cornerstone of regression.\nOptimization is to minimize some objective function. Regression is finding the parameters of the given model that maps the input to the output. Regularization is Critical\nRegularizers are what determine the type solutions obtained. Simple Example The data is generated from parabola + noise $f(x) = x^2 + ùêç(0,œÉ)$. However, the model in practice is unknown, and regularization can help find better models.\nGiven 100 Realizations of data. And the model is represented as $ùêò = ùëì(ùêó,Œ≤)$\nSmall noise results complex predicted models.\nDifferent regression 1 2 3 f = (x.^2 + 0.2 * rand(1,n)).\u0026#39;; lambda = 0.1; phi pinv(): Pseudo-inverse (Least square with the minimum L2 norm) \\ (bashslash): QR decomposition lasso() robustfit() () Least-square fit pinv get very different model for samll pertubation every time and all the coefficients are functioning. The uncertainty of this solver is high.\nThe high-dgree terms are highly volatile, which makes the model unstable.\nParsimony (Pareto front) Keep adding higher degree polynomials to the model, the error will no longer decrease at some point and instead even rise up.\nPareto front Sec 4.7: The Pareto front and Lex Parsimoniae\nSmall number of parameters can bring interperity\nConnect NNs Sec Neural Networks: 1-Layer Networks\n","date":"2023-05-23T17:24:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/10_regression_model_selection/","title":"watch: Nathan 10 | Regression and Model Selection"},{"content":"Course page: AM584\nDescovered by Ytb search: CANDECOMP/PARAFAC (CP) decomposition\nTensor Decomposition Source video Applied Linear Algebra: Tensor Decompositions\nTensor is the generalization of matrix. Tensor decomposition is doing multiple times SVD to extract the corelation in each dimension.\nOne piece of data could be a matrix, e.g., the temperture distribution in a region of latitude and longitude.\nBy stacking the data matrices of different time points, the obtained data forms a cube.\nX ‚±º ` ` . . ` ` X ` ` . . Vectorizing Vectorize a (n-dimensional) array is to reshape it into a vector.\nStacking the columns on top of each other.\nX ‚±º : X ‚±º O r ` ` . . ` ` ` ` . . : X ‚±º Vectorizing doesn\u0026rsquo;t bother the similarity against each smaples? because the total amount of difference is fixed? No matter using which kind of metric, the normalized distinctions between each other are fixed?\nSVD By flattening each sample, and arrange them into a matrix, then SVD can be applied.\nReconstruct the matrix A with low-rank structures?\nm n A = œÉ ‚ÇÅ u 1 v 1 + œÉ ‚ÇÇ u ‚ÇÇ v ‚ÇÇ + œÉ ‚ÇÉ u ‚ÇÉ v ‚ÇÉ + ‚ãØ ‚ãØ To avoid vectorization (flattening) messes up or mixes together the independent attributes (temperture, pressure, and humidity) and keep them in their own dimensions separately, SVD needs to be applied for every dimension.\nDecompose a data cube ùìú by performing SVD in 3 directions:\nc œÉ ‚±º ‚ÇÅ a ‚ÇÅ a ‚±º b ‚ÇÅ c ‚ÇÅ b ‚±º = + œÉ ‚ÇÇ a ‚ÇÇ b ‚ÇÇ c ‚ÇÇ + œÉ ‚ÇÉ a ‚ÇÉ b ‚ÇÉ c ‚ÇÉ + ‚ãØ ‚ãØ A dominant vector is responsible for its homologous vectors, having the same dimensionality.\nFor example, the above 5-dimensional vector is representing for the vectors parallel to it.\nAs the following hand illustration shows, Align the fingers with the vector and move towards the direction that the palm is facing. The vectors on the hand path will be represented.\n(image comes from here)\nReconstruct The r-rank approximation to the data cube M is the sum of the outer product of the dominant vectors in 3 directions.\nùìú = ‚àë‚±º‚Çå‚ÇÅ ≥ œÉ‚±º‚àòa‚±º‚àòb‚±º‚àòc‚±º\nAll individual dominant vectors a‚±º constitute ùìê·µ£. And ùìë·µ£ is the collection of all the b‚±º, and so do ùìí·µ£.\nùìú = ùìê·µ£ ‚àò ùìë·µ£ ‚àò ùìí·µ£\nInner and Outer product: Given two column vectors: $ùêÆ=[^{{u_1}}_{^{u_2}_{u_3}}]$ and $ùêØ=[^{v_1}_{^{v_2}_{v_3}}]$,\nInner product (equals dot product):\n$$ \\rm u·µÄv = (u‚ÇÅ,u‚ÇÇ,u‚ÇÉ) [^{v_1}_{^{v_2}_{v_3}}] = u‚ÇÅv‚ÇÅ +u‚ÇÇv‚ÇÇ + u‚ÇÉv‚ÇÉ \\text{= scalar} $$\nOuter prodcut: $$ \\rm ùêÆ‚äóùêØ = uv·µÄ = [^{{u_1}}_{^{u_2}_{u_3}}] (v‚ÇÅ,v‚ÇÇ,v‚ÇÉ) \\\\ = \\begin{bmatrix} \\rm u‚ÇÅv‚ÇÅ \u0026amp; \\rm u‚ÇÅv‚ÇÇ \u0026amp; \\rm u‚ÇÅv‚ÇÉ \\\\ \\rm u‚ÇÇv‚ÇÅ \u0026amp; \\rm u‚ÇÇv‚ÇÇ \u0026amp; \\rm u‚ÇÇv‚ÇÉ \\\\ \\rm u‚ÇÉv‚ÇÅ \u0026amp; \\rm u‚ÇÉv‚ÇÇ \u0026amp; \\rm u‚ÇÉv‚ÇÉ \\end{bmatrix} $$\nAny two vectors (with difference lengths) can perform outer product:\n$$ \\rm (^{v‚ÇÅ}_{v‚ÇÇ}) ‚äó (^{w‚ÇÅ}_{^{w‚ÇÇ}_{w‚ÇÉ}}) = \\begin{bmatrix} \\rm v‚ÇÅw‚ÇÅ \u0026amp; \\rm v‚ÇÅw‚ÇÇ \u0026amp; \\rm v‚ÇÅw‚ÇÉ \\\\ \\rm v‚ÇÇw‚ÇÅ \u0026amp; \\rm v‚ÇÇw‚ÇÇ \u0026amp; \\rm v‚ÇÇw‚ÇÉ \\end{bmatrix} $$\nEach entry in the matrix is a scalar and can be indexed by: (ùêØ ‚äó ùê∞)·µ¢‚±º= ùêØ·µ¢‚ãÖùê∞‚±º, the product of an element in the first vector with an element in the second vector.\nVector-matrix outer product\n1 2 3 4 5 6 7 8 9 \u0026gt;\u0026gt;\u0026gt; v = torch.tensor([0,1]) # [2] \u0026gt;\u0026gt;\u0026gt; m = torch.tensor([[2,0],[1,3]]) # [2, 2] \u0026gt;\u0026gt;\u0026gt; torch.einsum(\u0026#39;p,qr-\u0026gt; pqr\u0026#39;,v,m) tensor([[[0, 0], [0, 0]], [[2, 0], [1, 3]]]) Pytorch batch matrix vector outer product - SO\nThe SVD decomposition can be interpreted as the sum of outer products of each left (ùêÆ‚Çñ) and right (ùêØ‚Çñ) singular vectors, scaled by the corresponding nonzero signular value œÉ‚Çñ wiki\nùêÄ = ùêî ùö∫ ùêï·µÄ = $‚àë_{k=1}^{rank(A)}$ (ùêÆ‚Çñ ‚äó ùêØ‚Çñ)œÉ‚Çñ\nCode Demo Source video: Applied Linear Algebra: Implementing Tensor Decompositions\nDataset The data cube is generated by a spatial temporal function. It\u0026rsquo;s like a video having x and y directions and chaning in time.\ny x t = œÉ ‚ÇÅ m o t d ‚ÇÅ e 1 x ‚ÇÅ y ‚ÇÅ + œÉ ‚ÇÇ m o a d ‚ÇÇ e 2 x ‚ÇÇ y ‚ÇÇ The function mixes two fundamental features (modes) together. So the decomposition method should pull out the the two features back.\nFeature 1: $f‚ÇÅ(x,y,t) = e^{-(x¬≤+0.5*y¬≤)} cos(2t)$, the spatial structure is a elliptical shaped gaussian, and this bump oscillate up and down in time. Feature 2: $f‚ÇÇ(x,y,t) = sech(x)tanh(x) e^{-0.2y^2} sin(t)$, x direction is the shape -‚àø-, y direction is gaussian bell, and it oscillates with sine t Data matrix: F = f‚ÇÅ + f‚ÇÇ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 clear all; close all; clc % Basic domains (each frame is a rectangle) x = -5:0.1:5; y = -6:0.1:6; t = 0:0.1:10*pi; [X, Y, T] = meshgrid(x, y, t); % Data cube f1 = exp(-(X.^2 + 0.5*Y.^2)).*(cos(2*T)); % (Y,X,t): (121, 101, 315) f2 = (sech(X).*tanh(X).*exp(-0.2*Y.^2)).*sin(T) A = f1 + f2; % Tensor % Visualizing the shapes for j = 1:length(t): surfl(x, y, f1(:,:,j); %surfl(x, y, f2(:,:,j); %axis([-5,5], [-6,6], [-1,1]); colormap(hot) shading interp; drawnow end % Vectorizing (for performing standard SVD) nx = length(x); ny = length(y); for j = 1:length(t): % Every flattened column vector is appended to the matrix Af Af(:,j) = reshape(A(:,:,j), nx*ny, 1); % (nx*ny, t) end % Make matrix Af back to a tensor for j = 1:length(t) At(:,:,j) = reshape(Af(:,j), nx, ny) end % n-way array tensor decomposition (parallel factorization algorithm) % Given a hypercube of data, it will find the dominant factors in each direction. % Input tensor and rank (# of dominant components truncated) model = parafac(A,2); % Determine factors in each directions from each low-rank projections % If A is a 10-dim tensor, A1,A2,...,A10 directions are expected. % A1 has 2 factors, A2 has 2 factors, ... [A1, A2, A3] = fac2let(model); % Plot the 2 factors (vectors) in each component. subplot(3,1,1), plot(y, A1, \u0026#39;Linewidth\u0026#39;, [2]) % subplot(3,1,2), plot(x, A2, \u0026#39;Linewidth\u0026#39;, [2]) subplot(3,1,3), plot(t, A3, \u0026#39;Linewidth\u0026#39;, [2]) In y direction has two gaussian: $e^{-0.5y^2}$ and $e^(-0.2y^2)$\nIn x direction, there are a gaussian $e^(-x^2)$ and $sech(x)*tanh(x)$\nIn t direciton, cos(2t) and sin(t) are pulled out.\n","date":"2023-05-22T18:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/09_tensor_decompositions/","title":"watch: Nathan 09 | Tensor Decompositions"},{"content":"Code | Arxiv(2308) | ProjPage\nCode: wanmeihuali/taichi_3d_gaussian_splatting\nVideo Explain (2023-09-11)\nSource video: „ÄêËÆ∫ÊñáËÆ≤Ëß£„ÄëÁî®ÁÇπ‰∫ëÁªìÂêà3DÈ´òÊñØÊûÑÂª∫ËæêÂ∞ÑÂú∫ÔºåÊàê‰∏∫Âø´ÈÄüËÆ≠ÁªÉ„ÄÅÂÆûÊó∂Ê∏≤ÊüìÁöÑÊñ∞SOTAÔºÅ- ÊÑè„ÅÆËåó\nCG technique: Splatting\nEach point is an anisotropic 3D Gaussian distribution:\nmean is point location (xyz), covariance matrix determined the shape of the 3D Gaussian Optimize points\u0026rsquo; distribution:\nLocations are at mean, which are needless to learn; Co-variance matrix is converted to quaternion 3D Gaussian distribution can do clone and split to fit complex geometry.\nTile-based fast rendering rather volume rendering.\n3D Gaussian (2023-10-28)\n1D Gaussian distribution:\nGiven a scalar $x \\sim N(Œº,œÉ¬≤)$, its PDF:\n$$ p(x) = \\frac{1}{\\sqrt{2œÄœÉ¬≤}} e^{-\\frac{(x-Œº)¬≤}{2œÉ¬≤}} $$\n3D Gaussian composed of 3 independent 1D Gaussian in 3 directions can be represented as below.\nGiven a vector ùêØ: [a,b,c], its PDF: p(ùêØ) = p(a) p(b) p(c)\n$$ \\begin{aligned} p(ùêØ) \u0026amp;= \\frac{1}{(2œÄ)^{3/2}œÉ‚Çê œÉ_b œÉ_c} exp(-\\frac{(a-Œº‚Çê)¬≤}{2œÉ‚Çê¬≤} -\\frac{(b-Œº_b)¬≤}{2œÉ_b¬≤} -\\frac{(c-Œº_c)¬≤}{2œÉ_c¬≤}) \\\\ (\\text{vectorize}) \u0026amp;= \\frac{1}{(2œÄ)^{3/2} |Œ£|^¬Ω} exp(-¬Ω‚ãÖ(ùêØ-\\bm Œº)·µÄ(ùêØ-\\bm Œº) Œ£‚Åª¬π) \\end{aligned} $$\nWhere the $œÉ‚Çê œÉ_b œÉ_c$ is the square root of the determinant of covariance matrix Œ£:\n$$ |Œ£|^{¬Ω} = \\begin{vmatrix} œÉ‚Çê¬≤ \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; œÉ_b¬≤ \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; œÉ_c¬≤ \\end{vmatrix}^{1/2} = œÉ‚Çê œÉ_b œÉ_c $$\nThe exponent can be derived as:\n$$ \\begin{array}{ccc} \\begin{bmatrix} a-Œº_a \\\\ b-Œº_b \\\\ c-Œº_c \\end{bmatrix} \\begin{bmatrix} a-Œº_a \u0026amp; b-Œº_b \u0026amp; c-Œº_c \\end{bmatrix} \\begin{bmatrix} 1/œÉ‚Çê¬≤ \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1/œÉ_b¬≤ \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1/œÉ_c¬≤ \\end{bmatrix} \\\\ = \\frac{(a-Œº_a)¬≤}{œÉ‚Çê¬≤} + \\frac{(b-Œº_b)¬≤}{œÉ_b¬≤} + \\frac{(c-Œº_c)¬≤}{œÉ_c¬≤} \\end{array} $$\nIf those 3 1D Gaussian are all $N(Œº=0, œÉ¬≤=1)$, the 3D Gaussian becomes simpler:\n$$\\rm p(ùêØ) = \\frac{1}{(2œÄ)^{3/2}} exp(-\\frac{a¬≤+b¬≤+c¬≤}{2})$$\nDerivation for arbitrary 3D Gaussians:\n3D Gaussian is a product of three 1D Gaussian. If each 1D Gaussian is N(0,1), then 3D Gaussian is:\n$$ \\begin{aligned} p(ùê±) \u0026amp;= p(x)p(y)p(z) \\\\ \u0026amp;= \\frac{1}{(2œÄ)^{3/2}} exp(-\\frac{x¬≤+y¬≤+z¬≤}{2}) = \\frac{1}{(2œÄ)^{3/2}} exp(-\\frac{ùê±·µÄùê±}{2}) \\\\ \\end{aligned} $$\nFor an arbitrary vector ùê± = [x,y,z], where 3 Gaussians could be in various shapes. it will be moved to the origin of world space by subtracting the mean vecotr, and the 3 directions (covariance matrix) will be scaled to 1 (identity matrix) through a transformation matrix ùêÄ.\n$$\\rm ùê±\u0026rsquo; = ùêÄ(ùê±-\\bm Œº)$$\nBecause the mean vector is point\u0026rsquo;s position. This step means all points are moved to world\u0026rsquo;s origin and reshaped into a unit space for later optimization.\nPlug this \u0026ldquo;normalized\u0026rdquo; ùê±\u0026rsquo; into 3D Gaussian :\n$$p(ùê±\u0026rsquo;) = \\frac{1}{(2œÄ)^{3/2}} exp(-\\frac{(ùê±-\\bm Œº)·µÄùêÄ·µÄùêÄ(ùê±-\\bm Œº)}{2})$$\nTo reach a form about ùê±, integrate p(ùê±\u0026rsquo;):\n$$1 = ‚à≠_{-‚àû}^{+‚àû} \\frac{1}{(2œÄ)^{3/2}} exp(-\\frac{(ùê±-\\bm Œº)·µÄùêÄ·µÄùêÄ(ùê±-\\bm Œº)}{2}) dùê±\u0026rsquo;$$\nSubstitude $dùê±\u0026rsquo;$ with $dùêÄ(ùê±-\\bm Œº) = |ùêÄ|dùê±$\n$$1 = ‚à≠_{-‚àû}^{+‚àû} \\frac{|ùêÄ|}{(2œÄ)^{3/2}} exp(-\\frac{(ùê±-\\bm Œº)·µÄùêÄ·µÄùêÄ(ùê±-\\bm Œº)}{2}) dùê±$$\nThe function being integrated is p(ùê±). Since ùê± follows Gaussian, it can be rewritten with a mean vector ùõç and a covariance matrix ùö∫.\nThe covariance matrix ùö∫ is a symmetric matrix, which can be decomposed by SVD:\n$$ \\begin{aligned} Œ£ \u0026amp;= U Œõ U·µÄ \\\\ \u0026amp;= UŒõ^{¬Ω} Œõ^{¬ΩT} U·µÄ \\\\ \u0026amp;= UŒõ^{¬Ω} (UŒõ^{¬Ω})·µÄ \\end{aligned} $$\nOn a 2D plane, SVD is strecting and rotating, represented separately by a stretch matrix (diagnoal) $S = [^{s‚ÇÅ \\ 0}_{0\\ s‚ÇÇ}]$, and a rotate matrix (UU·µÄ=1) $R = [^{cosŒ∏ \\ -sinŒ∏}_{sinŒ∏ \\ cosŒ∏}]$.\nEach column of U is orthogonal to each other and of magnitude 1. For example, when U is R:\n‚åà ‚åä c s x o i s n Œ∏ Œ∏ - R s c y i o n s Œ∏ Œ∏ ‚åâ ‚åã ‚åà ‚åä p 1 0 P ‚ÇÅ ‚åâ ‚åã t s ‚åà ‚åä p 0 1 ‚ÇÇ ‚åâ ‚åã = ‚åà ‚åä c s R p o i o ‚ÇÅ s n t ' Œ∏ Œ∏ a ‚åâ ‚åã t e ‚åà ‚åä d - c p s o P ‚ÇÇ i s t ' n Œ∏ s Œ∏ ‚åâ ‚åã In 3D space, matrices are 3√ó3.\nùêî is a basis (a component, a coordinate system). By multiplying it with a diagnoal matrix, $UŒõ^¬Ω$ is a linear transformation, denoted as the transformation matrix ùêÄ. Thus, ùö∫ = ùêÄùêÄ·µÄ.\nIn other words, an identity matrix (basis) will be transformed to another basis $UŒõ^¬Ω$ by ùêÄ.\nTherefore, ùêÄ·µÄùêÄ = ùö∫‚Åª¬π, which will reverse an arbitrary covariance matrix ùö∫ to identity matrix ùêà, i.e., putting the ellipsoid into the \u0026ldquo;unit\u0026rdquo; space $[^{100}_{^{010}_{001}}]$, where the modulus of each axis is 1.\nSubstitute ùêÄ·µÄùêÄ with ùö∫‚Åª¬π, and $|ùêÄ| = |Œ£|^¬Ω$, the 3D Gaussian is the function being integrated:\n$$\\frac{1}{(2œÄ)^{3/2}|Œ£|^¬Ω} exp(-\\frac{(ùê±-\\bm Œº)·µÄŒ£‚Åª¬π(ùê±-\\bm Œº)}{2})$$\nThe 3D Gaussian used in this work is simplified as:\n$$ G(x) = exp(-\\frac{(ùê±)·µÄŒ£‚Åª¬π(ùê±)}{2}) \\tag{4} $$\nOmit the mean vector ùõç, becuase it\u0026rsquo;s 0. Every 3D Gaussian distribution\u0026rsquo;s center has been shifted to the origin. Once the optimization finished, ellipsoids will be reverted to the world space for rasterization.\nThe front fraction is omitted as the integral (\u0026ldquo;volume\u0026rdquo;) of 3D Gaussian isn\u0026rsquo;t limited to 1 to be a probability distribution, and considering a 3D Gaussian can be any size.\n(2024-05-15)\nseh_sjij Ëß£Èáä‰∏∫: Á≥ªÊï∞$\\frac{1}{(2œÄ)^{\\frac{3}{2}}|Œ£|^¬Ω}$Ë¢´ÂåÖÂê´Âà∞‰∫Ü opacity ‰∏≠ÔºåÁÑ∂Âêé opacity ‰ºöËá™Â∑±‰ºòÂåñ„ÄÇ CSDN ùêÄ is for rotating and stretching an ellipsoid. Thus, ùêÄ=ùêëùêí. And then ùö∫ = ùêëùêíùêí·µÄùêë·µÄ\nThe covariance matrix ùö∫ gets optimized during training, such that the shape and direction of ellipsoids get adjusted to fit the scene.\nOptimization 3D Gaussian representation based on point cloud from SfM.\nEach 3D Gaussian contains properties: 3D position (xyz), shape (transform matrix ùêÄ), SH coeffs (color), opacity (Œ±).\nOptimize rotate matrix ùêë using Quaternion instead of 3√ó3 matrix\nPoint cloud optimization\nRemove points whose opacity lower than threshold after a certain epochs;\nHigh positional gradients are inferred as a point is hard to reconstruct the geometry. So, small Gaussian do clone (for faster training), while large Gaussian do split (for recovering bkg).\nReset opacity to 0 periodically to remove floater around camera\nTile-based rasterization.\nAn image is split into 16√ó16 patches. Sorting 3D ellipses observed by a patch based on depth. Terminate alpha compositing on a pixel when opactiy reaches 1. Each tile has a CUDA block, and each pixel has a CUDA thread. Read Notes (2023-05-14)\nThree elements:\nBased on sparse points (colmap) and 3D Gaussians representation Point cloud optimization Fast rendering algorithm with GPU sorting (tile-based rasterization) How much meomery does the 1-5 million (1e6) Gaussians (for all scenes tested) cost?\nComparing with Mip-NeRF360 of 8.6MB, 3DGS has 523MB after 7K iterations on dataset \u0026ldquo;Mip-NeRF360\u0026rdquo;. Chinese translation: 3DGSÁ¨îËÆ∞ - bo233ÁöÑÊñáÁ´† - Áü•‰πé\nAbs (2023-10-29)\nNVS based on radiance field without neural network.\nSfM point cloud and 3D Gaussian representation. Point cloud adjustment: add and remove based on gradients. Tile-based rasterizer leveraging depth and cuda. Intro Points cloud format is chosen for rasterization. Then, 3D Gaussian is chosen to make point cloud a continuous field.\nEfficient method based on continuous representation (MLP) most performed interpolation.\nNeural nets representation is convinent to be optimized, but hinder fast rendering.\nRelated Works Splatting made point-based rendering more efficient by extending the rasterization beyond a single pixel to cover a spread-out area.\nSome methods used CNN to render.\nNeRF requires extensive sampling around the entire space.\nPulsar is \u0026ldquo;order-independent\u0026rdquo;, whereas alpha-blending for a pixel is performed based on visibility order.\ndiffuse - diffusion model\nOverview The key to the efficiency of our method is our tile-based rasterizer.\nExplicit scene representation appeals fast rendering without inferencing neural network.\nGradients are backpropagated to concrete 3D Gaussian.\nDifferentiable 3DGS \u0026ldquo;Unstructured\u0026rdquo; is opposite to \u0026ldquo;regular volume grid\u0026rdquo;. 3DGS is unstructured but able to do volume rendering. Point cloud data + 3DGS primitive -\u0026gt; scene representation\nComparing with small planar circle representation for each point, 3D Gaussian doesn\u0026rsquo;t need normal. And normals are intractable for a sparse (SfM) point cloud.\ncovariance matrices have physical meaning only when they are positive semi-definite\nThus, they use a decomposed, equivalent form: ùö∫ = ùêëùêíùêí·µÄùêë·µÄ , i.e., stretching and rotating an ellipsoid.\nScaling matrix ùêí is represented by a 3D vector; Rotation ùêë is represented by a quaternion. Reparameterization is changing spaces. For example, spherical coords (Œ∏,œÜ) of viewdir is recombinded to cartisian coords (x,y,z) in NeRF\u0026rsquo;s code. However, SVD doesn\u0026rsquo;t alter the number of dimensions, as a single space is split into 2 spaces. Optimization Create, delete or move points (3D Gaussian) to fit geometry.\nThe quality of covariance of 3D Gaussians is critical for the compactness.\nSGD performed by CUDA. Alpha comes from sigmoid. Covariance has done exponential activation.\nThe fast rasterization is critical in the efficiency of our optimization.\nLoss function: ùìõ = (1-Œª)ùìõ‚ÇÅ + Œªùìõ_dssim\nSince the output is a complete image, SSIM is measured holistically between the gt image and predicted image. In contrast, S3IM is for random-ray patches.\nUse absolute error of rgbs, instead of MSE.\nAdaptive Density Control Densify to better represent scene and remove transparent 3D Gaussians (points) every 100 iterations.\nTotal volume?\nReset alpha value close to zero every 3k iterations to reduce floaters near cameras.\nTile-based Gaussian Split image into 16x16 patches ‚ûî Cull 3D Gaussians intersected with frustum ‚ûî Project 3D Gaussians to 2D ‚ûî Assign depth observed by a patch to each 3D Gaussian ‚ûî Sorting Gaussians based on depth for each patch.\nA block is assigned to a patch and loads packets of Gaussians into shared memory.\nEach pixel has a thread to do alpha compositing.\nReuse the per-tile lists for each pixel in the tile.\nImplementation Central-object photos taken from the entire hemisphere without angular regions missing result in good SH coefficients.\nHowever, for incomplete observation, e.g., corners or \u0026ldquo;inside-out\u0026rdquo; photos, the 0th order SH prone to be corrupt during optimization.\nSo they first optimize only the 0-order coefficient for 1K iterations, then add 1 more coeff every 1K iters until all 4 orders are supplemented.\nResults As fast as instantNGP, and as good as MipNeRF360. Synthetic dataset can use 100K random intialized point cloud in the bbox. More compact than other point-based methods. Limitations Hight memory cost due to point cloud representation. Tradeoff of model size for speed.\nCompress point cloud. Defects occurs in the scene not well observed.\nPlay (2023-11-01)\nEnvironment Ubuntu 18.04 Lambda Server: nvcc 10.2, Ubuntu 18.04, driver 470.103.01 with support for CUDA 11.4 at highest.\nClone needs \u0026ndash;recursive: git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive\nOtherwise, there will be an error when running conda env create --file environment.yml:\n1 ERROR: Directory \u0026#39;submodules/diff-gaussian-rasterization\u0026#39; is not installable. Neither \u0026#39;setup.py\u0026#39; nor \u0026#39;pyproject.toml\u0026#39; found. Or fetch and install submodules individually:\n1 2 3 4 conda activate gaussian_splatting git submodule update --init --recursive pip install submodules/diff-gaussian-rasterization pip install submodules/simple-knn Submodule install error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Pip subprocess error: error: subprocess-exited-with-error √ó python setup.py egg_info did not run successfully. ‚îÇ exit code: 1 ‚ï∞‚îÄ\u0026gt; [12 lines of output] Traceback (most recent call last): File \u0026#34;\u0026lt;string\u0026gt;\u0026#34;, line 36, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;pip-setuptools-caller\u0026gt;\u0026#34;, line 34, in \u0026lt;module\u0026gt; File \u0026#34;/home/z/Downloads/gaussian-splatting/submodules/diff-gaussian-rasterization/setup.py\u0026#34;, line 13, in \u0026lt;module\u0026gt; from torch.utils.cpp_extension import CUDAExtension, BuildExtension File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/__init__.py\u0026#34;, line 201, in \u0026lt;module\u0026gt; _load_global_deps() File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/__init__.py\u0026#34;, line 154, in _load_global_deps ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL) File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/ctypes/__init__.py\u0026#34;, line 364, in __init__ self._handle = _dlopen(self._name, mode) OSError: /home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/lib/../../../../libcublas.so.11: symbol cublasLtGetStatusString version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference [end of output] Submodules (\u0026ldquo;diff-gaussian-rasterization\u0026rdquo;) needs compilation locally with CUDA Toolkit. issue#45\nAlternative steps refer to issue#406\n(2024-04-05)\nI previously thought that the driver 470 is insufficient to install cuda 11.6, but I found that driver 470 is able to install CUDA 11.x on Docs. However, the problem persists when using cuda 11.6.\nThe problem is that my PATH has been modified previously. I once added a line in my .bashrc: export LD_LIBRARY_PATH=/home/z/anaconda3/envs/GNT/lib\nThis caused the libcublas points to mismatched library. Refer to the answer of eval\nI remove that line, and having the nvidia-driver 470 + ctk 11.6 on ubuntu 18.04, the env installation succeeds with just one line: conda env create --file environment.yml\nHe said by setting env virable LD_LIBRARY_PATH=\u0026lt;anaconda dir\u0026gt;/python3.7/site-packages/torch/lib/nvidia/cublas/lib/:$LD_LIBRARY_PATH So that \u0026ldquo;the dlopen will firstly look for .so files in that directory.\u0026rdquo; (I didn\u0026rsquo;t try that at present.)\nSubmodules compile error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /usr/local/cuda/bin/nvcc -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/TH -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/z/anaconda3/envs/3dgs/include/python3.9 -c cuda_rasterizer/backward.cu -o build/temp.linux-x86_64-cpython-39/cuda_rasterizer/backward.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -I/home/z/Downloads/gaussian-splatting/submodules/diff-gaussian-rasterization/third_party/glm/ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1011\\\u0026#34; -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++14 cuda_rasterizer/backward.cu:15:10: fatal error: cooperative_groups/reduce.h: No such file or directory #include \u0026lt;cooperative_groups/reduce.h\u0026gt; ^~~------------------------- compilation terminated. ERROR: Could not build wheels for diff-gaussian-rasterization, which is required to install pyproject.toml-based projects And compiling simple_knn.h will encounter:\n1 2 simple_knn.cu:17:10: fatal error: cub/cub.cuh: No such file or directory #include \u0026lt;cub/cub.cuh\u0026gt; Possible method: Add cub to CmakeLists.txt. Nvidia-forum\nnvcc mismatch and maybe 10.2 doesn\u0026rsquo;t have that function yet.\nUbuntu 20.04 Install new cudatookit on Alien PC (Ubuntu 20.04) CUDA Toolkit 12.3 Downloads | NVIDIA Developer\n1 2 3 4 5 6 7 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2004-12-3-local_12.3.0-545.23.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-12-3-local_12.3.0-545.23.06-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2004-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda-toolkit-12-3 Install driver:\n1 2 sudo apt-get install -y cuda-drivers # reboot is required But nvcc -V still returns 10.2.\nI found there are multiple \u0026ldquo;cuda\u0026rdquo; on my Alien PC. Nv-Install Guide Linux\nUnder dir \u0026ldquo;/usr/local/\u0026rdquo;, there are \u0026ldquo;cuda/\u0026rdquo;, \u0026ldquo;cuda11/\u0026rdquo;, \u0026ldquo;cuda11.6/\u0026rdquo;, \u0026ldquo;cuda12/\u0026rdquo;, \u0026ldquo;cuda12.3/\u0026rdquo;.\nOnly one could be used. SO. And the default version may can be checked by: update-alternatives --display cuda Docs-Nv\nAdd 11.6 to PATH:\n1 2 3 export PATH=/usr/local/cuda-11.6/bin${PATH:+:${PATH}} export CUDADIR=/usr/local/cuda-11.6 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.6/lib64 Then nvcc -V change to 11.6.\nSuccess to create env on Alien PC with just the one line: conda env create --file environment.yml\nFor Lambda Server, although there is \u0026ldquo;cuda-11.2\u0026rdquo; folder in \u0026ldquo;/usr/local/\u0026rdquo;, but \u0026ldquo;bin/\u0026rdquo; doesn\u0026rsquo;t exist inside, so it can\u0026rsquo;t be added into PATH.\nTherefore, the full cudatoolkit 11.6 and compatible driver (as above) are required, instead of the subset cudatoolkit for runtime installed by conda.\nUbuntu 22.04 (2023-11-03)\nCUDA Toolkit 11.6 doesn\u0026rsquo;t have version for Ubuntu 22.04 CUDA Toolkit 11.8 corresponds PyTorch which is higher than 2.0.0 1 2 3 4 5 6 7 # nvcc -V # 11.8, Ubuntu 22.04, nvidia driver 545, 3090Ti conda create -n 3dgs python=3.9 conda activate 3dgs conda install pytorch==2.0.0 torchvision==0.15.0 pytorch-cuda=11.8 -c pytorch -c nvidia pip install plyfile tqdm ninja pip install submodules/diff-gaussian-rasterization/ pip install submodules/simple-knn/ SIBR viewer install fails (Ubuntu 22.04) #151\n(2024-05-17)\nThe glm module missed because I forgot add --recursive (when cloning submodule depth-diff-gaussian-rasterization solely: git submodule update --init --recursive). And the error about pyproject appears either:\n1 2 3 4 5 6 7 8 9 10 11 Compiling objects... - - - - - In file included from /home/zichen/Downloads/CasMVSNet_pl-comments/_3DGS/submodules/depth-diff-gaussian-rasterization/cuda_rasterizer/backward.cu:12: /home/zichen/Downloads/CasMVSNet_pl-comments/_3DGS/submodules/depth-diff-gaussian-rasterization/cuda_rasterizer/backward.h:19:10: fatal error: glm/glm.hpp: No such file or directory 19 | #include \u0026lt;glm/glm.hpp\u0026gt; | ^~-~-~-~-~-~-~-~-~-~-~-~ compilation terminated. - - - - - ERROR: Could not build wheels for diff_gaussian_rasterization, which is required to install pyproject.toml-based projects Train NeRF Synthetic\n1 python train.py -s ../nerf/data/nerf_synthetic/lego -r 8 DTU (2024-04-15)\nTrain with DTU dataset:\nInstall Colmap.\nTo get the CUDA support on Linux, a local compilation is required. Docs\nUse COLMAP to prepare training dataset:\n(2024-05-26)\n1 2 3 4 5 6 7 8 9 zichen@homepc:~/Downloads/gaussian-splatting$ tree . ‚îú‚îÄ‚îÄ convert.py ‚ãÆ ‚îî‚îÄ‚îÄ DTU_scan1 ‚îî‚îÄ‚îÄ‚îÄ input ‚îú‚îÄ‚îÄ 00000000.jpg ‚îú‚îÄ‚îÄ 00000002.jpg ‚îî‚îÄ‚îÄ 00000004.jpg Execute:\n1 python convert.py -s DTU_scan1 Train:\n1 (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ python train.py -s DTU_scan1 The result point cloud doesn\u0026rsquo;t perserve a good geometry:\ninput.ply iter30K Given 3 views, COLMAP produced very sparse points.\nThe gaussian clone and split maybe not accurate enough for recovering the geometry.\nT\u0026amp;T (2024-05-26)\nRef:\n3DGS guide Pixel-GS Steps:\nUse COLMAP to estimate camera poses from input images:\n1 2 3 4 5 6 7 8 9 zichen@homepc:~/Downloads/gaussian-splatting$ tree . ‚îú‚îÄ‚îÄ convert.py ‚ãÆ ‚îî‚îÄ‚îÄ TnT_Barn ‚îî‚îÄ‚îÄ‚îÄ input ‚îú‚îÄ‚îÄ 000010.jpg ‚îú‚îÄ‚îÄ 000179.jpg ‚îî‚îÄ‚îÄ 000184.jpg Execute:\n1 python convert.py -s TnT_Barn --resize I removed the magick_command variable when constructing commands: exit_code = os.system(\u0026quot; mogrify -resize 50% \u0026quot; + destination_file) as Ubuntu 22.04 doesn\u0026rsquo;t have magick. Train:\n1 (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ python train.py -s TnT_Barn Result model of Barn with 2 views (010.jpg and 184.jpg) input:\nAs Pixel-GS descriped (in abstract): there are lots of \u0026ldquo;needle-like artifacts\u0026rdquo;. SIBR viewer SIBR Core - Git Lab\nUbuntu 20.04 On Ubuntu 20.04, 1050Ti:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Get into submodule: cd SIBR_viewers # For Ubuntu 20.04, otherwise opencv version (4.2) mismatch 4.5: git checkout fossa_compatibility # Dependencies sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev # Project setup cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release # add -G Ninja to build faster # Don\u0026#39;t add \u0026#39;-j24\u0026#39;, otherwise crash? cmake --build build --target install Some details:\nInstall cmake by compiling source code:\nDownload tar.gz (cmake-.tar.gz), such as cmake-3.29.1.tar.gz, and extract: tar -zxvf cmake-{version number}.tar.gz\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Dependecy for OpenSSL sudo apt-get install libssl-dev cd cmake-{version number} # Check required dependencies: ./bootstrap # Build package: make # Install sudo make install cmake --version Ref How to Install CMake on Ubuntu 22.04 or 20.04 - LinuxCapable\nOpenCV issue on Ubuntu 20.04 resolved by switching branch to \u0026ldquo;fossa_compatibility\u0026rdquo;. Installing SIBR - cannot find correct version of OpenCV #10\nsm_30 error\nWith cudatoolkit 11.6 added to PATH, an error occurs:\n1 2 3 4 #$ ptxas -arch=sm_30 -m64 \u0026#34;tmp/CMakeCUDACompilerId.ptx\u0026#34; -o \u0026#34;tmp/CMakeCUDACompilerId.sm_30.cubin\u0026#34; ptxas fatal : Value \u0026#39;sm_30\u0026#39; is not defined for option \u0026#39;gpu-name\u0026#39; Check if old-version cuda exists: apt-cache policy nvidia-cuda-toolkit. „ÄêÂ∑≤Ëß£ÂÜ≥„Äë Compilation error ptxas fatal :\n1 2 3 4 5 6 7 nvidia-cuda-toolkit: Installed: 10.1.243-3 Candidate: 10.1.243-3 Version table: *** 10.1.243-3 500 500 http://ca.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages 100 /var/lib/dpkg/status Remove it: sudo apt remove nvidia-cuda-toolkit\nThen, re-build works.\nRun local viewer\n1 ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/c777580e-9 Error (Real-time viewer requires CC \u0026gt; 7.0):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 (gaussian_splatting) yi@yi-Alienware-Aurora-R8:~/Downloads/gaussian-splatting$ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/c777580e-9/ --no_interop [SIBR] -- INFOS --: Initialization of GLFW [SIBR] -- INFOS --: OpenGL Version: 4.6.0 NVIDIA 545.23.06[major: 4, minor: 6] Number of input Images to read: 300 Number of Cameras set up: 300 [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego. [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego.ply. [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego.obj. LOADSFM: Try to open /home/yi/Downloads/nerf/data/nerf_synthetic/legopoints3D.bin [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/legopoints3D.bin. [SIBR] !! WARNING !!: FILE /home/yi/Downloads/gaussian-splatting/SIBR_viewers/src/core/scene/ProxyMesh.cpp LINE 29, FUNC loadFromData proxy model not found at /home/yi/Downloads/nerf/data/nerf_synthetic/lego [SIBR] ## ERROR ##: FILE /home/yi/Downloads/gaussian-splatting/SIBR_viewers/src/projects/gaussianviewer/renderer/GaussianView.cpp LINE 339, FUNC GaussianView Sorry, need at least compute capability 7.0+!terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): See log for message errors Aborted (core dumped) Even the 1080 Ti\u0026rsquo;s compute capacity is only 6.1 ? Your GPU Compute Capability Ubuntu 22.04 (2023-11-04)\nInstall:\n1 2 3 4 5 6 # Dependencies sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev # Project setup cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release -G Ninja cmake --build build -j24 --target install Train: python train.py -s ../Dataset/nerf_synthetic/lego\nReal-time viewer: sudo ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/4ee28b3c-9\nRun Network viewer:\n1 2 3 4 5 6 7 8 (3dgs) z@homepc:~/Downloads/gaussian-splatting/SIBR_viewers$ ./install/bin/SIBR_remoteGaussian_app [SIBR] -- INFOS --: Initialization of GLFW [SIBR] ## ERROR ##: FILE /home/z/Downloads/gaussian-splatting/SIBR_viewers/src/core/graphics/Window.cpp LINE 30, FUNC glfwErrorCallback X11: The DISPLAY environment variable is missing terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): See log for message errors Aborted (core dumped) (2024-05-09)\nUse SIBR to view the trained model with point cloud initialization from CasMVSNet\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m ../CasMVSNet_pl-comments/output/no_densify_0509/ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app: error while loading shared libraries: libassimp.so.5: cannot open shared object file: No such file or directory (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev Êúâ‰∏Ä‰∫õËΩØ‰ª∂ÂåÖÊó†Ê≥ïË¢´ÂÆâË£Ö„ÄÇÂ¶ÇÊûúÊÇ®Áî®ÁöÑÊòØ unstable ÂèëË°åÁâàÔºåËøô‰πüËÆ∏ÊòØ Âõ†‰∏∫Á≥ªÁªüÊó†Ê≥ïËææÂà∞ÊÇ®Ë¶ÅÊ±ÇÁöÑÁä∂ÊÄÅÈÄ†ÊàêÁöÑ„ÄÇËØ•ÁâàÊú¨‰∏≠ÂèØËÉΩ‰ºöÊúâ‰∏Ä‰∫õÊÇ®ÈúÄË¶ÅÁöÑËΩØ‰ª∂ ÂåÖÂ∞öÊú™Ë¢´ÂàõÂª∫ÊàñÊòØÂÆÉ‰ª¨Â∑≤Ë¢´‰ªéÊñ∞Âà∞(Incoming)ÁõÆÂΩïÁßªÂá∫„ÄÇ ‰∏ãÂàó‰ø°ÊÅØÂèØËÉΩ‰ºöÂØπËß£ÂÜ≥ÈóÆÈ¢òÊúâÊâÄÂ∏ÆÂä©Ôºö ‰∏ãÂàóËΩØ‰ª∂ÂåÖÊúâÊú™Êª°Ë∂≥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºö libminizip-dev : ‰æùËµñ: libminizip1 (= 1.1-8build1) ‰ΩÜÊòØ 1.1.1-1+rebuild Ê≠£Ë¶ÅË¢´ÂÆâË£Ö E: Êó†Ê≥ï‰øÆÊ≠£ÈîôËØØÔºåÂõ†‰∏∫ÊÇ®Ë¶ÅÊ±ÇÊüê‰∫õËΩØ‰ª∂ÂåÖ‰øùÊåÅÁé∞Áä∂ÔºåÂ∞±ÊòØÂÆÉ‰ª¨Á†¥Âùè‰∫ÜËΩØ‰ª∂ÂåÖÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ sudo aptitude install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev (3dgs) zichen@homepc:~/Downloads/gaussian-splatting$ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m ../CasMVSNet_pl-comments/output/no_densify_0509/ Then, it works:\nUbuntu 18.04 (2024-04-05)\nThe current cmake on server is 3.10, which is too old.\nUsing the pre-built cmake cmake-3.29.1-linux-x86_64.tar.gz will lead to error.\n1 2 3 4 5 6 wget https://github.com/Kitware/CMake/releases/download/v3.29.1/cmake-3.29.1-linux-x86_64.tar.gz tar -zxvf cmake-3.29.1-linux-x86_64.tar.gz # Build SIBR cd ./Downloads/gaussian-splatting/SIBR_viewers /home/z/Downloads/cmake-3.29.1-linux-x86_64/bin/cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release Error 1 2 3 4 5 6 7 8 CMake Error at /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Call Stack (most recent call first): /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindGLEW.cmake:242 (find_package_handle_standard_args) cmake/linux/dependencies.cmake:69 (FIND_PACKAGE) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Using local cmake to compile remote folder, which is mounted via sshfs doesn\u0026rsquo;t work neither:\n1 2 3 4 (base) yi@yi:/mnt/Server/Downloads/gaussian-splatting/SIBR_viewers$ cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release CMake Error: The current CMakeCache.txt directory /mnt/Server/Downloads/gaussian-splatting/SIBR_viewers/CMakeCache.txt is different than the directory /home/z/Downloads/gaussian-splatting/SIBR_viewers where CMakeCache.txt was created. This may result in binaries being created in the wrong place. If you are not sure, reedit the CMakeCache.txt Clone the CMake repo and build it. git clone https://github.com/Kitware/CMake.git\nInstall lib for ssl: sudo apt-get install libssl-dev to avoid SSL error:\n1 2 3 4 -- Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the system variable OPENSSL_ROOT_DIR (missing: OPENSSL_CRYPTO_LIBRARY OPENSSL_INCLUDE_DIR) CMake Error at Utilities/cmcurl/CMakeLists.txt:644 (message): Could not find OpenSSL. Install an OpenSSL development package or configure CMake with -DCMAKE_USE_OPENSSL=OFF to build without OpenSSL. 1 2 3 cd CMake/ mkdir build \u0026amp;\u0026amp; cd build ../bootstrap \u0026amp;\u0026amp; make The using cmake to compile project:\n1 2 cd ~/Downloads/gaussian-splatting/SIBR_viewers ../../CMake/build/bin/cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release But this still encounters the same error as above.\n1 2 CMake Error at /home/z/Downloads/CMake/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Copy the compiled executable files onto system.\n1 2 cd sudo make install Cmake 3.29 gets installed (copied) into:\n1 2 3 4 5 6 7 8 9 10 11 /usr/local/doc/cmake-3.29/cmsys/Copyright.txt ... ... ... -- Installing: /usr/local/share/cmake-3.29/Modules/ ... -- Installing: /usr/local/share/cmake-3.29/Templates ... -- Installing: /usr/local/share/vim/vimfiles/indent ... -- Installing: /usr/local/share/emacs/site-lisp/cmake-mode.el -- Installing: /usr/local/share/aclocal/cmake.m4 -- Installing: /usr/local/share/bash-completion/completions/ Relaunch a terminal. Check the version:\n1 2 3 4 (base) z@lambda-server:~/Downloads/gaussian-splatting$ cmake --version cmake version 3.29.20240405-gc2949db CMake suite maintained and supported by Kitware (kitware.com/cmake). Build SIBR:\n1 2 3 4 5 git checkout fossa_compatibility git checkout master cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release The error persists!! as below: (Note: The following error corresponds to the master branch.)\n1 2 3 4 5 6 7 8 CMake Error at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Call Stack (most recent call first): /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) /usr/local/share/cmake-3.29/Modules/FindGLEW.cmake:245 (find_package_handle_standard_args) cmake/linux/dependencies.cmake:69 (FIND_PACKAGE) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Install dependencies:\n1 sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev # libembree-dev It prompts that E: Unable to locate package ibembree-dev.\nThen, the building will prompts error about libboost:\n1 2 3 4 5 6 7 8 9 10 CMake Error at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find Boost: Found unsuitable version \u0026#34;1.65.1\u0026#34;, but required is at least \u0026#34;1.71.0\u0026#34; (found /usr/include, found components: system chrono filesystem date_time) Call Stack (most recent call first): /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:598 (_FPHSA_FAILURE_MESSAGE) /usr/local/share/cmake-3.29/Modules/FindBoost.cmake:2394 (find_package_handle_standard_args) cmake/dependencies.cmake:173 (find_package) cmake/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) The header and libraries of libboost are in:\n1 2 3 -- Found Boost: /usr/include (found version \u0026#34;1.65.1\u0026#34;) -- Boost_INCLUDE_DIRS: /usr/include -- Boost_LIBRARY_DIRS: /usr/lib/x86_64-linux-gnu These information is return by adding command into CMakeLists.txt (guided by chatGPT):\n1 2 3 find_package(Boost REQUIRED) message(STATUS \u0026#34;Boost_INCLUDE_DIRS: ${Boost_INCLUDE_DIRS}\u0026#34;) message(STATUS \u0026#34;Boost_LIBRARY_DIRS: ${Boost_LIBRARY_DIRS}\u0026#34;) I don\u0026rsquo;t know how to update libboost.\nI found the CMakeLists.txt varies between different branch.\nThe branch of SIBR used by 3DGS is gaussian_code_release_union (4ae964a267):\n1 2 3 git checkout gaussian_code_release_union cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release Warnings about embree, and errors for OpenCV4.5:\nError 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Warnings CMake Warning at cmake/linux/dependencies.cmake:127 (find_package): By not providing \u0026#34;Findembree.cmake\u0026#34; in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \u0026#34;embree\u0026#34;, but CMake did not find one. Could not find a package configuration file provided by \u0026#34;embree\u0026#34; (requested version 3.0) with any of the following names: embreeConfig.cmake embree-config.cmake Add the installation prefix of \u0026#34;embree\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;embree_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;embree\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) # Errors: There is no provided OpenCV library for your compiler, relying on find_package to find it CMake Error at cmake/linux/dependencies.cmake:248 (find_package): Could not find a configuration file for package \u0026#34;OpenCV\u0026#34; that is compatible with requested version \u0026#34;4.5\u0026#34;. The following configuration files were considered but not accepted: /usr/share/OpenCV/OpenCVConfig.cmake, version: 3.2.0 Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Then I change to branch fossa_compatibility:\n1 2 3 git checkout fossa_compatibility cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release The error of OpenCV4 occurs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CMake Warning (dev) at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:438 (message): The package name passed to `find_package_handle_standard_args` (EMBREE) does not match the name of the calling package (embree). This can lead to problems in calling code that expects `find_package` result variables (e.g., `_FOUND`) to follow a certain pattern. Call Stack (most recent call first): cmake/linux/Modules/Findembree.cmake:87 (FIND_PACKAGE_HANDLE_STANDARD_ARGS) cmake/linux/dependencies.cmake:127 (find_package) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) This warning is for project developers. Use -Wno-dev to suppress it. -- EMBREE wasn\u0026#39;t found correctly. Set EMBREE_DIR to the root SDK installation directory. (missing: EMBREE_INCLUDE_DIR EMBREE_LIBRARIES) There is no provided OpenCV library for your compiler, relying on find_package to find it CMake Error at cmake/linux/dependencies.cmake:248 (find_package): Could not find a configuration file for package \u0026#34;OpenCV\u0026#34; that is compatible with requested version \u0026#34;4\u0026#34;. The following configuration files were considered but not accepted: /usr/share/OpenCV/OpenCVConfig.cmake, version: 3.2.0 Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Authors said the SIBR has no support for Ubuntu 18. Related issues with searching \u0026ldquo;18.04\u0026rdquo;\n","date":"2023-05-14T13:41:00Z","image":"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.bfDD9_L28UK7AsADDzB8vwHaEo%26pid%3DApi\u0026f=1\u0026ipt=3b34b7952e1e03305e5c085185ede1ad09e8edc71e9c1161a33c17b6b9f2b3ac\u0026ipo=images","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-read/","title":"read: Render - NVS | 3D Gaussian Splatting"},{"content":"DataLoader (2024-05-29)\nPyTorch DataLoaderÂ∑•‰ΩúÂéüÁêÜÂèØËßÜÂåñ collate_fn\ntorch.utils.data.IterableDataset pytorch forum 2020-02-26; Docs\nConcatDataset Docs\nExample 1: Pytorch DataLoader multiple data source - SO;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import os import torch.utils.data as data class SingeJsonDataset(data.Dataset): # implement a single json dataset here... ... list_of_datasets = [] for j in os.path.listdir(root_dir): if not j.endswith(\u0026#39;.json\u0026#39;): continue # skip non-json files list_of_datasets.append(SingeJsonDataset(json_file=j, root_dir=root_dir, transform=None)) # once all single json datasets are created you can concat them into a single one: multiple_json_dataset = data.ConcatDataset(list_of_datasets) Example 2: PyTorch forum - Praateek\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class LazyTextDataset(Dataset): def __init__(self, filename): self._filename = filename self._total_data = int(subprocess.check_output(\u0026#34;wc -l \u0026#34; + filename, shell=True).split()[0]) - 1 def __getitem__(self, idx): line = linecache.getline(self._filename, idx + 1) csv_line = csv.reader([line]) return next(csv_line) def __len__(self): return self._total_data path = /where_csv_files_are_dumped/ files = list(map(lambda x : path + x, (filter(lambda x : x.endswith(\u0026#34;csv\u0026#34;), os.listdir(path))))) datasets = list(map(lambda x : LazyTextDataset(x), files)) dataset = ConcatDataset(datasets) Comments of Thomans Ahle:\nThe problem with ConcatDataset is that it doesn\u0026rsquo;t work with multiprocessing. It calls len(ds) on each dataset in it\u0026rsquo;s initializer, so you end up loading every dataset in the main process.\nnp.load(path, mmap_mode=\u0026lsquo;r\u0026rsquo;) Load multiple .npy files (size \u0026gt; 10GB) in pytorch - SO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np import torch from bisect import bisect import os, psutil # used to monitor memory usage class BigDataset(torch.utils.data.Dataset): def __init__(self, data_paths, target_paths): self.data_memmaps = [np.load(path, mmap_mode=\u0026#39;r\u0026#39;) for path in data_paths] self.target_memmaps = [np.load(path, mmap_mode=\u0026#39;r\u0026#39;) for path in target_paths] self.start_indices = [0] * len(data_paths) self.data_count = 0 for index, memmap in enumerate(self.data_memmaps): self.start_indices[index] = self.data_count self.data_count += memmap.shape[0] def __len__(self): return self.data_count def __getitem__(self, index): memmap_index = bisect(self.start_indices, index) - 1 index_in_memmap = index - self.start_indices[memmap_index] data = self.data_memmaps[memmap_index][index_in_memmap] target = self.target_memmaps[memmap_index][index_in_memmap] return index, torch.from_numpy(data), torch.from_numpy(target) # Test Code if __name__ == \u0026#34;__main__\u0026#34;: data_paths = [f\u0026#39;data/d{index}.npy\u0026#39; for index in range(10)] target_paths = [f\u0026#39;data/s{index}.npy\u0026#39; for index in range(10)] process = psutil.Process(os.getpid()) memory_before = process.memory_info().rss dataset = BigDataset(data_paths, target_paths) used_memory = process.memory_info().rss - memory_before print(\u0026#34;Used memory:\u0026#34;, used_memory, \u0026#34;bytes\u0026#34;) dataset_size = len(dataset) print(\u0026#34;Dataset size:\u0026#34;, dataset_size) print(\u0026#34;Samples:\u0026#34;) for sample_index in [0, dataset_size//2, dataset_size-1]: print(dataset[sample_index]) ","date":"2023-04-21T09:57:00Z","permalink":"https://zichen34.github.io/writenotes/lib/load_multi_files/","title":"memo: Datasets load multiple data files"},{"content":"Callbacks I asked chat-GPT: \u0026ldquo;How to save the intermediate tensor in a tensorflow model?\u0026rdquo;.\ntf.keras.callbacks.LambdaCallback class will call a custom function after each epoch or each batch or the begining/ending of traininng. Docs.\nCallbacks are useful to get a view on internal states and statistics of the model during training. Docs: Writing your own callbacks\nExample of callbacks in model.fit() refer to: 7;\nHowever, the NeRF model constitutes low-level operations without implementing the method model.fit(). Then the callback functions need to be called manually at the appropriate points in the training loop:\nAsking Chat-GPT: \u0026ldquo;Please do not use model.fit(). Make the model and callback in low-level options. Please given an example of using callbacks and using TensorFlow 1.15.\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras.callbacks import LambdaCallback tf.enable_eager_execution() # Define the model architecture model = tf.keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation=\u0026#39;relu\u0026#39;), Dense(10) ]) # Define the loss function and optimizer loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = tf.keras.optimizers.Adam() # Define the learning rate decay schedule def lr_decay(epoch, lr): if epoch % 10 == 0 and epoch \u0026gt; 0: return lr * 0.1 else: return lr # Define the callback to update the learning rate lr_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: optimizer.lr.assign(lr_decay(epoch, optimizer.lr))) # Define a function to save the intermediate tensor def save_intermediate_tensor(batch, logs): intermediate_tensor = model.layers[2].output tf.summary.scalar(\u0026#39;intermediate_tensor\u0026#39;, intermediate_tensor, step=epoch) save_callback = LambdaCallback(on_batch_end=save_intermediate_tensor) # Load the MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # Normalize the pixel values x_train = x_train[:100] / 255.0 x_test = x_test[:100] / 255.0 y_test = y_test[:100] # Compile the model for model.evaluate() model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\u0026#39;accuracy\u0026#39;]) # Train the model for epoch in range(10): for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train, y_train)): x_batch_train = x_batch_train.reshape((-1, 28, 28)) y_batch_train = y_batch_train.reshape((-1,)) with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Call the save callback at the end of each batch save_callback.on_batch_end(batch,{}) # Call the learning rate callback at the end of each epoch lr_callback.on_epoch_end(epoch, {\u0026#39;lr\u0026#39;: optimizer.lr.numpy()}) # Evaluate the model on the test set test_loss, test_acc = model.evaluate(x_test, y_test) print(\u0026#39;Epoch {}, Test Loss: {:.4f}, Test Accuracy: {:.4f}\u0026#39;.format(epoch+1, test_loss, test_acc)) Error: couldn\u0026rsquo;t find summary writer\nNotFoundError: Resource localhost/logdir:./logdir/exp1/N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:FlushSummaryWriter]\nSolution: Restart the python kernel. tf issue#47100 tf.io.write_file() How to save the value of a tensor in Tensorflow found by 1\n1 2 one_string = tf.strings.as_string(tensor) tf.io.write_file(filename, one_string) Problem: the rank of tensor has to be 0? tf TFRecordWriter How do you save a Tensorflow dataset to a file? found by 1\nThis is for making (x,y) dataset. io_ops._save() Is there a way to save an intermediate output in Tensorflow to a file? found by 2\nMay have been deprecated. Create a new model Obtaining output of an Intermediate layer in TensorFlow/Keras found by 2\nGet values of KerasTensor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import tensorflow as tf def init_nerf_model(): ... print(output.numpy()) # AttributeError: \u0026#39;KerasTensor\u0026#39; object has no attribute \u0026#39;numpy\u0026#39; model = tf.keras.Model(inputs=inputs, outputs=outputs) return model model= init_nerf_model(input_ch=63, input_ch_views=27, use_viewdirs=True) # model.layers # Check memory address of each layer temp_mode = tf.keras.Model(model.input, model.layers[11].output) temp_mode.summary() # print description of each layer Oiginal NeRF model summary:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Model: \u0026#34;model\u0026#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 90)] 0 [] tf.split (TFOpLambda) [(None, 63), 0 [\u0026#39;input_1[0][0]\u0026#39;] (None, 27)] dense (Dense) (None, 256) 16384 [\u0026#39;tf.split[0][0]\u0026#39;] dense_1 (Dense) (None, 256) 65792 [\u0026#39;dense[0][0]\u0026#39;] dense_2 (Dense) (None, 256) 65792 [\u0026#39;dense_1[0][0]\u0026#39;] dense_3 (Dense) (None, 256) 65792 [\u0026#39;dense_2[0][0]\u0026#39;] dense_4 (Dense) (None, 256) 65792 [\u0026#39;dense_3[0][0]\u0026#39;] tf.concat (TFOpLambda) (None, 319) 0 [\u0026#39;tf.split[0][0]\u0026#39;, \u0026#39;dense_4[0][0]\u0026#39;] dense_5 (Dense) (None, 256) 81920 [\u0026#39;tf.concat[0][0]\u0026#39;] dense_6 (Dense) (None, 256) 65792 [\u0026#39;dense_5[0][0]\u0026#39;] dense_7 (Dense) (None, 256) 65792 [\u0026#39;dense_6[0][0]\u0026#39;] dense_9 (Dense) (None, 256) 65792 [\u0026#39;dense_7[0][0]\u0026#39;] tf.concat_1 (TFOpLambda) (None, 283) 0 [\u0026#39;dense_9[0][0]\u0026#39;, \u0026#39;tf.split[0][1]\u0026#39;] dense_10 (Dense) (None, 128) 36352 [\u0026#39;tf.concat_1[0][0]\u0026#39;] dense_11 (Dense) (None, 3) 387 [\u0026#39;dense_10[0][0]\u0026#39;] dense_8 (Dense) (None, 1) 257 [\u0026#39;dense_7[0][0]\u0026#39;] tf.concat_2 (TFOpLambda) (None, 4) 0 [\u0026#39;dense_11[0][0]\u0026#39;, \u0026#39;dense_8[0][0]\u0026#39;] ================================================================================================== Total params: 595,844 Trainable params: 595,844 Non-trainable params: 0 Only keep the MLP part:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Model: \u0026#34;model_1\u0026#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 90)] 0 [] tf.split (TFOpLambda) [(None, 63), 0 [\u0026#39;input_1[0][0]\u0026#39;] (None, 27)] dense (Dense) (None, 256) 16384 [\u0026#39;tf.split[0][0]\u0026#39;] dense_1 (Dense) (None, 256) 65792 [\u0026#39;dense[0][0]\u0026#39;] dense_2 (Dense) (None, 256) 65792 [\u0026#39;dense_1[0][0]\u0026#39;] dense_3 (Dense) (None, 256) 65792 [\u0026#39;dense_2[0][0]\u0026#39;] dense_4 (Dense) (None, 256) 65792 [\u0026#39;dense_3[0][0]\u0026#39;] tf.concat (TFOpLambda) (None, 319) 0 [\u0026#39;tf.split[0][0]\u0026#39;, \u0026#39;dense_4[0][0]\u0026#39;] dense_5 (Dense) (None, 256) 81920 [\u0026#39;tf.concat[0][0]\u0026#39;] dense_6 (Dense) (None, 256) 65792 [\u0026#39;dense_5[0][0]\u0026#39;] dense_7 (Dense) (None, 256) 65792 [\u0026#39;dense_6[0][0]\u0026#39;] dense_9 (Dense) (None, 256) 65792 [\u0026#39;dense_7[0][0]\u0026#39;] ================================================================================================== Total params: 558,848 Trainable params: 558,848 Non-trainable params: 0 Ref DDG search: save tensorflow tensor to file DDG search: save intermediate tensor from tensorflow model A Guide to TensorFlow Callbacks ","date":"2023-04-17T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf_save_tensors/","title":"memo: TF | Save Tensors"},{"content":"Via conda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Check the version of the CUDA: $ conda list cudatookit # Name Version Build Channel cudatoolkit 10.1.243 h8cb64d8_10 conda-forge $ conda list cudnn: # Name Version Build Channel cudnn 7.6.5.32 hc0a50b0_1 conda-forge # install/update CUDA and CUDNN through conda: $ conda install -c anaconda cudatoolkit $ conda install -c anaconda cudnn # show the highest supported CUDA version? $ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 | # $ nvcc --version Refer to 1; 3\nAnother way to check the version of cuDNN 4:\n1 cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2 Via PyTorch 1 2 3 import torch print(torch.version.cuda) \u0026gt;\u0026gt;\u0026gt; 10.2 Via TensorFlow 1 2 3 4 5 import tensorflow as tf sys_details = tf.sysconfig.get_build_info() cuda_version = sys_details[\u0026#34;cuda_version\u0026#34;] print(cuda_version) Not quiet right. Refer to How to check CUDA version in TensorFlow - gcptutorials\nCompatible combinations for TF Refer to Docs; 2;\nCompatible CUDA for cards Determine the driver version first and then determine the cuda version. Genearlly, cuda is backward compatible with the driver. Nvidia-page\nRef get the CUDA and CUDNN version on windows with Anaconda installed searched by DDG: \"What is the version of CUDA and cuDNN\" Which TensorFlow and CUDA version combinations are compatible? How to get the CUDA version? [NV] How to check CUDA and cuDNN version | by totokk | Medium ","date":"2023-04-09T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/check_cuda_version/","title":"memo: Check Versions of CUDA and cuDNN"},{"content":"tf 1.x enable_eager_execution() Invoke the function tf.enable_eager_execution() at program startup, and then you can use the .numpy() method. ¬π\n1 2 3 import tensorflow as tf tf.compat.v1.enable_eager_execution() pred_id = tf.multinomial(tf.exp(preds), num_samples=1)[0][0].numpy() (eager execution is enabled by default in TensorFlow 2.0.)\ntf.get_static_value(x) Answer from ¬π. Link to docs\n1 tf.get_static_value( tensor, partial=False ) Create sess Without enabling the eager execution, a tensor can be evaluated in a session. The session can be created as a runtime context by with tf.Session() as sess:, or using InteractiveSession() in place ‚Å¥.\nThe value of a Tensor can only be obtained after calling .eval(), because Tensor (in TF) is of lazy evaluation. ¬≤\n1 2 3 4 5 6 7 8 9 10 import tensorflow as tf a = tf.constant([1, 1.5, 2.5], dtype=tf.float32) b = tf.constant([1, -2, 3], dtype=tf.float32) c = a * b with tf.Session() as sess: result = c.eval() print(result) Use InteractiveSession() Initialize an InteractiveSession as the default session. The following snippet is from 2.\n1 2 3 4 5 6 7 8 9 import tensorflow as tf a = tf.constant([1, 1]) # SparseTensorÔºåÂè™Â≠òÂÇ®ÂÄºÂèäÂÖ∂ÂØπÂ∫îÁöÑindex b = tf.constant([2, 2]) c = tf.add(a, b) # TensorÔºåÂÆåÊï¥ÁöÑÁü©Èòµ sess = tf.InteractiveSession() # v1.15 print(\u0026#34;a[0]=%s, a[1]=%s\u0026#34; % (a[0].eval(), a[1].eval())) print(\u0026#34;c = %s\u0026#34; % c.eval()) sess.close() Session.run() ÈÄöËøá Session.run() Ëé∑ÂèñÂèòÈáèÁöÑÂÄº 6, 4\n1 2 3 4 5 6 x = tf.placeholder(tf.float32) y = tf.placeholder(tf.float32) bias = tf.Variable(1.0) y_pred = x**2 + bias loss = (y - y_pred)**2 print(\u0026#39;Loss(x,y) = %.3f\u0026#39; % session.run(loss, {x:3.0, y:9.0})) # Á®ãÂ∫èÂºÄÂ§¥Â∑≤ÂÆö‰πâsess Êä•ÈîôÔºö\n1 2 3 4 5 6 FailedPreconditionError: 2 root error(s) found. (0) Failed precondition: Attempting to use uninitialized value Variable_1 [[{{node Variable_1/read}}]] [[pow_3/_33]] (1) Failed precondition: Attempting to use uninitialized value Variable_1 [[{{node Variable_1/read}}]] ÂéüÂõ†Ôºö bias is a tf.Variable which needs initialization 5, like:\n1 2 3 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(session.run(loss, {x:3.0, y:9.0}) tf.keras.backend.get_value(x) Answer form DesiKeki‚Å¥. Link to docs\n1 2 3 4 print(product) \u0026gt;\u0026gt;tf.Tensor([[12.]], shape=(1, 1), dtype=float32) print(tf.keras.backend.get_value(product)) \u0026gt;\u0026gt;[[12.]] tf.print() Python print() cannot be run in a tf.Graph, but tf.print() can be. tf_guide\n1 2 3 4 5 @tf.function def double(a): print(\u0026#34;Tracing with\u0026#34;, a) # only be executed when (re)tracing tf.print(\u0026#34;Executing with\u0026#34;, a) # executed every time the graph is called return a + a doubt: tf.print Áî®Ê≥ïÔºü \u0026ldquo;ÂèØ‰ª•Âú®ËÆ°ÁÆó‰∏Ä‰∏™ÂèòÈáèÁöÑÂêåÊó∂ÔºåÊåáÂÆöÊâìÂç∞‰∏ÄÁªÑÂèòÈáè\u0026rdquo;(‰∏çÊáÇ) tensorflowÊùÇËÆ∞ 2016\nUse tf.print (instead of deprecated tf.Print). It returns None when executing eagerly. how to use tf.print (not tf.Print) in high-level Estimator api\nAttributeError: 'Tensor' object has no attribute '_datatype_enum' when executing: tf.print(inputs, output_stream=sys.stderr)\n(Deprecated) tf.Print() The pythonic print(feat_tensor) will only print the description of tensors once when the input function graph is constructed.\ntf.Print() is a graph node that splices (ÊâìÁªì) the \u0026ldquo;print call\u0026rdquo; into the graph by taking the tensor you wanna print as input, and output a \u0026ldquo;same\u0026rdquo; variable that will be passed to the later node in the graph.\n1 2 3 4 5 6 7 def input_fn(dataset): def _fn(): feat_tensor = tf.constant(dataset.data) feat_tesnor = tf.Print(feat_tensor, data=[feat_tensor, tensor2, tensor3], # feat_tensor will be printed \u0026amp; returned, tensor2,tensor3 are printed btw. message=\u0026#34;Inputs are: \u0026#34;) feat = {feat_name: feat_tensor} Using tf.Print() in TensorFlow -Yufeng Guo\n(2023-04-16)\nLambdaCallback doesn\u0026rsquo;t work sumNotes\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras.callbacks import LambdaCallback tf.enable_eager_execution() # Define the model architecture model = tf.keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation=\u0026#39;relu\u0026#39;), Dense(10) ]) # Define the loss function and optimizer loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = tf.keras.optimizers.Adam() # Define a function to save the intermediate tensor def save_intermediate_tensor(epoch,): intermediate_tensor = model.layers[0].output # Replace this with the layer whose output you want to save print(intermediate_tensor) # only show the description once # tf.print(intermediate_tensor) # AttributeError: \u0026#39;Tensor\u0026#39; object has no attribute \u0026#39;_datatype_enum\u0026#39; # np.savez(f\u0026#39;vec{epoch}\u0026#39;, intermediate_tensor.numpy()) # AttributeError: \u0026#39;Tensor\u0026#39; object has no attribute \u0026#39;numpy\u0026#39; (with egar execution enabled) # tf.io.write_file(f\u0026#39;vec{epoch}\u0026#39;, tf.strings.as_string(intermediate_tensor)) # rank has to be 0 save_callback = LambdaCallback(on_epoch_end=save_intermediate_tensor) # Load the MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # Normalize the pixel values x_train = x_train[:100] / 255.0 # Compile the model model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\u0026#39;accuracy\u0026#39;]) # Train the model for epoch in range(10): for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train, y_train)): x_batch_train = x_batch_train.reshape((-1, 28, 28)) y_batch_train = y_batch_train.reshape((-1,)) with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) save_callback.on_epoch_end(epoch) DDG search: \u0026lsquo;Tensor\u0026rsquo; object has no attribute \u0026rsquo;numpy\u0026rsquo;\nTF 2.0 \u0026lsquo;Tensor\u0026rsquo; object has no attribute \u0026rsquo;numpy\u0026rsquo; while using .numpy() although eager execution enabled by default#27519\nTF 2.x @tf.function Create a dataflow graph for a Python function. Then its outputs can be printed. Docs\n1 2 3 4 5 6 @tf.function def foo(): a = tf.random.uniform([1], maxval=5, seed=1) b = tf.random.uniform([1], maxval=5, seed=1) return a+b print(foo()) # 2*a Ref AttributeError: 'Tensor' object has no attribute 'numpy' - SO Tensorflow ÁöÑ Tensor Âíå OpKernel ÂàÜÊûê-È´òÈπè Âú®TensorFlow‰∏≠ÊÄé‰πàÊâìÂç∞TensorÂØπË±°ÁöÑÂÄº How to print the value of a Tensor object in TensorFlow? - SO FailedPreconditionError: Attempting to use uninitialized in Tensorflow - SO Tensorflow‰πãË∞ÉËØï(Debug)ÂèäÊâìÂç∞ÂèòÈáè - Shiyu_Huang -ÂçöÂÆ¢Âõ≠ ","date":"2023-04-08T10:48:00-05:00","permalink":"https://zichen34.github.io/writenotes/lib/tf_print_tensor/","title":"memo: TF | Print Tensors"},{"content":"Source video: „Äê‰øóËØ¥Áü©Èòµ„ÄëÈÄÜÁü©ÈòµÁöÑÂÜÖÊ∂µÂéüÊù•Ëøô‰πà‰∏∞ÂØåÔºÅÂø´Êù•ÁúãÁúãÂêßÔºÅ\nInverse of diagonal matrix A diagonal matrix with the given values: 2,3,4 as follows.\n$$ ùêÄ = \\text{diag\\{2, 3, 4\\}} = \\begin{bmatrix} 2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 4 \\end{bmatrix} $$\nThe augmented matrix is constructed and perform elementary row operations as:\n$$ ùêÄ|ùêà = \\begin{bmatrix} 2 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 4 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\overset{1/2r1,1/3r2, 1/4r3}{‚á¢} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 1/2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; | 0 \u0026amp; 1/3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1/4 \\end{bmatrix} $$\nFrom the results, the inverse matrix ùêÄ‚Åª¬π of a diagonal matrix is also a diagonal matrix. And its elements on the main diagonal are the reciprocal of elements on the original diagonal, where the element cannot be 0.\n$$ ùêÄ‚Åª¬π = \\begin{bmatrix} 1/2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1/3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1/4 \\end{bmatrix} = \\text{diag\\{1/2, 1/3, 1/4\\}} $$\nInverse of a scalar matrix If all elements on the diagonal are the same, the matrix is a schalar matrix. ùö≤ = diag{Œª, \u0026hellip;, Œª}, Œª‚â†0.\nIts inverse matrix is a scalar matrix where the elements on the diagonal are all 1/Œª, ùö≤‚Åª¬π = diag{Œª‚Åª¬π, \u0026hellip;, Œª‚Åª¬π}, Œª‚â†0.\nIf Œª = 1, the scalar matrix is identity matrix ùêà. So the inverse of a identity matrix is itself: ùêà‚Åª¬π = ùêà.\nInverse of the elementary matrix The elementary matrix is definitely invertible. So their inverse matrices must exist.\nInverse of a row-switching matrix For a row-switching matrix (permutation matrix) switching the first 2 rows of the identity matrix:\n$$ ùêÖ‚ÇÅ = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nWrite the augmented matrix: $$ ùêÖ‚ÇÅ|ùêà = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \u0026amp;| 1 \u0026amp; 0 \u0026amp;0\\\\ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp;| 0 \u0026amp; 1 \u0026amp;0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp;| 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} $$\nThe inverse matrix can be obtained by switching the first two rows and letting the left part become a identity matrix. Then the inverse matrix is the right part:\n$$ ùêÖ‚ÇÅ‚Åª¬π = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp;0\\\\ 1 \u0026amp; 0 \u0026amp;0\\\\ 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} $$\nIt indicates that the inverse of a row-switching matrix is itself. (Just like the identity matrix, its inverse is itself.)\nInverse of a row-multiplying matrix For a row-multiplying matrix scaling the second rows by 2 times:\n$$ ùêÖ‚ÇÇ = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nTherefore, its inverse is obtained by multiplying 1/2 onto the 2nd row of the identity matrix.\n$$ ùêÖ‚ÇÇ‚Åª¬π = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1/2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThat means the inverse of a row-multiplying matrix will still be a row-multiplying matrix. But the scale factor becomes the reciprocal.\nInverse of a row-addition matrix For a row-addition matrix: the second row is added by 2-times first row. $$ ùêÖ‚ÇÉ = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 2 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nIts inverse is obtained by lettingthe 2nd row in the identity matrix add the -2-times of the first row:\n$$ ùêÖ‚ÇÉ‚Åª¬π = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ -2 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThus, its inverse is just changing the scale factor to its opposite.\nIn summary, the inverse matrices of elementry matrices are the same type.\nThe inverse of a row-switching matrix is itself. The inverse of a row-multiplication matrix is changing the scale factor to its reciprocal. The inverse of a row-addition matrix is changing the scale factor to its opposite. Inverse of second-order matrix The number of rows and columns of a second-order matrix are both 2.\n","date":"2023-04-03T16:17:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/13_%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E6%89%A9%E5%B1%95%E5%BA%94%E7%94%A8/","title":"watch: LA - È´òÂ±± 13 | Application of Inverse Matrix"},{"content":"Source video: „Äê‰øóËØ¥Áü©Èòµ„ÄëÂàùÁ≠âÂèòÊç¢Ê±ÇÈÄÜÁü©ÈòµÂéüÊù•ÊòØËøô‰∏™ÂéüÁêÜÔºÅÊï∞Â≠¶ËÄÅÂ∏àÁúãÂêéÂ§ßËµûÔºÅ\nùêÄ can perform 9 elementary row operations to reach the identity matrix ùêà.\n$$ ùêÄ = [^{_{1\\ 3\\ 5}} _{^{2\\ 4\\ 6} _{1\\ 4\\ 9}}] ‚ûî [^{_{1\\ 3\\ 5}} _{^{1\\ 2\\ 3} _{1\\ 4\\ 9}}] ‚ûî [^{_{1\\ 2\\ 3}} _{^{1\\ 3\\ 5} _{1\\ 4\\ 9}}] ‚ûî [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{1\\ 4\\ 9}}] ‚ûî [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 2\\ 6}}] ‚ûî [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 0\\ 2}}] ‚ûî [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 0\\ 1}}] ‚ûî [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] ‚ûî [^{_{1\\ 2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] ‚ûî [^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] = ùêà $$\nùêÄ is multiplied with 9 elementary matrix on the left: ùêà = ùêÖ‚ÇâùêÖ‚Çà\u0026hellip;ùêÖ‚ÇÅùêÄ\nThe product of these 9 elementary matrix is called ùêÄ‚Åª¬π, the inverse of ùêÄ. So ùêÄ‚Åª¬πùêÄ = ùêà, and ùêÄ‚Åª¬π = ùêÖ‚ÇâùêÖ‚Çà\u0026hellip;ùêÖ‚ÇÅ.\nùêÄ‚Åª¬π will not change if it\u0026rsquo;s multiplied with ùêà: ùêÄ‚Åª¬π = ùêÖ‚ÇâùêÖ‚Çà\u0026hellip;ùêÖ‚ÇÅùêà\nThis equation indicates that an identity matrix ùêà ($[^{_{1\\ 0\\ 0}}_{^{0\\ 1\\ 0}_{0\\ 0\\ 1}}]$) can perform exactly the same 9 elementary row operations to reach the inverse matrix ùêÄ‚Åª¬π = $[^{_{-3\\ 7/4\\ 1/2}} _{^{‚ÄÇ3\\ -1\\ -1}_{-1\\ 1/4\\ 1/2}}]$.\n$$ \\begin{array}{c} ùêÄ = \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 2 \u0026amp; 4 \u0026amp; 6 \\\\ 1 \u0026amp; 4 \u0026amp; 9 \\end{bmatrix} \\rm \\overset{\\text{9 elementary row}}{operations‚Üí} ùêà = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\\\ ùêà = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\rm \\overset{\\text{9 elementary row}}{operations‚Üí} ùêÄ‚Åª¬π = \\begin{bmatrix} -3 \u0026amp; 7/4 \u0026amp; 1/2 \\\\ 3 \u0026amp; -1 \u0026amp; -1 \\\\ -1 \u0026amp; 1/4 \u0026amp; 1/2 \\end{bmatrix} \\end{array} $$\nIf an matrix ùêÄ can perform multiple elementary row operations to become an identity matrix ùêà, then ùêà can perform the same 9 elementary row operations simultaneously to reach the inverse matrix ùêÄ‚Åª¬π.\nIn another word, the transformation from ùêà to ùêÄ‚Åª¬π replicates the 9 elementary row opertations from ùêÄ to ùêà.\nAugmented matrix performs elementary row operations To keep the sequence of operations the same, the augmented matrix ùêÄ|ùêà is leveraged.\n$$ ùêÄ|ùêà = \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 2 \u0026amp; 4 \u0026amp; 6 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 4 \u0026amp; 9 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThis augmented matrix ùêÄ|ùêà can perform 9 elementary matrix to make the left part of the vertical line become a identity matrix. Such that the right part of the vertical line becomes the inverse matrix ùêÄ‚Åª¬π.\n$$ ùêÄ|ùêà = ùêà|ùêÄ‚Åª¬π $$\nThis method is applicable to the matrix that its elements are given.\nExample Given a matrix ùêÄ = $[^{_{1\\ 1\\ 2}} _{^{1\\ 2\\ 3} _{2\\ 4\\ 5}}]$, solve the inverse ùêÄ‚Åª¬π.\nConstruct the augmented matrix:\n$$ ùêÄ|ùêà = \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nPerform elementary row operations on this augmented matrix to make the left part become ùêà\n$$ \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} ‚Üí^{\\text{row echelon}}_{form}: \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; | -1 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} ‚Üí^{\\text{clear top-}}_{\\text{right part}}: \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 2 \u0026amp; -3 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; | -1 \u0026amp; -1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} $$\nRow echelon form: Clear the values on the lower left side of the main diagonal from top to bottom Clear the values on the top right side of the main diagonal from bottom to top The right-hand side of the vertical line individually is the inverse matrix ùêÄ‚Åª¬π.\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 2 \u0026amp; -3 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; | -1 \u0026amp; -1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} = ùêà|ùêÄ‚Åª¬π $$\nSo,\n$$ ùêÄ‚Åª¬π = \\begin{bmatrix} 2 \u0026amp; -3 \u0026amp; 1 \\\\ -1 \u0026amp; -1 \u0026amp; 1 \\\\ 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} $$\n2x2 (2024-02-12)\nThe inverse of $[^{a\\ b}_{d \\ c}]$ is:\n$$ \\underbrace{ \\begin{bmatrix} 1 \u0026amp; -\\frac{b}{a} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{d} \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{a}{ac-bd} \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \\\\ -1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} \\frac{d}{a} \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} }_{A‚Åª¬π} \\begin{bmatrix} a \u0026amp; b \\\\ d \u0026amp; c \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} $$\nThe combinatio nof 5 matrices is the inverse matrix:\n$$ A‚Åª¬π = \\begin{bmatrix} \\frac{c}{ac-bd} \u0026amp; -\\frac{b}{ac-bd} \\\\ -\\frac{d}{ac-bd} \u0026amp; \\frac{a}{ac-bd} \\end{bmatrix} = \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} $$\nHence, the 1/determinant is a part of the inverse matrix. Matlab A is invertible:\n1 2 3 4 5 6 7 8 A = [1, 1, 2; 1, 2, 3; 2, 4, 5] inv(A) ans = 2 -3 1 -1 -1 1 0 2 -1 B is not invertible:\n1 2 3 4 5 6 7 8 9 10 B = [1, 1, 1; 2, 2, 2; 3, 3, 3] inv(B) Warning: Matrix is singular to working precision. ans = Inf Inf Inf Inf Inf Inf Inf Inf Inf C is not a square matrix:\n1 2 3 4 5 C = [1, 2, 3; 4, 5, 6] inv(C) Error using inv Matrix must be square. ","date":"2023-04-03T12:31:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/12_%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E6%B3%95/","title":"watch: LA - È´òÂ±± 12 | How to find the inverse matrix?"},{"content":"Matrix multiplication doesn\u0026rsquo;t have commutative property (‰∫§Êç¢Âæã). But some square matrices satisfy ùêÄùêÅ=ùêÅùêÄ\nMatrix multiplication has Associative property (ÁªìÂêàÂæã)\n(ùêÄùêÅ)ùêÇ = ùêÄ(ùêÅùêÇ)\nCompund linear mapping\nTranspose and multiplication (ùêÄùêÅ)·µÄ ‚â† ùêÄ·µÄùêÅ·µÄ\n(ùêÄùêÅ)·µÄ = ùêÅ·µÄùêÄ·µÄ\nSquare matrix multiplication The number of rows = the number of columns\nùêÄ = $[^{1\\ 2} _{1\\ 4}]$ ; ùêÅ = $[^{4\\ -2} _{-1\\ 1}]$\nThere is ùêÄùêÅ = ùêÅùêÄ.\nSo these two matrices ùêÄ, ùêÅ are commutative 1. ùêÄ,ùêÅ must be the square matrices with the same size, and the result also has the identical-size square matrix.\nPower of square matrix ùêÄ·µè = ùêÄ‚ãÖùêÄ·µè‚Åª¬π = ùêÄ·µè‚Åª¬π‚ãÖùêÄ, where ùêÄ and ùêÄ·µè‚Åª¬π are commutative because of the commutative property of matrix multiplication.\nTherefore, the two positive integar powers of matrix ùêÄ are always commutative.\nPower of the matrix only applicable to square matrices. Otherwise, the mismatched dimensions prevent performing matrices multiplication.\nùêÄ¬π=ùêÄ ; ùêÄ‚Å∞ = ùêà\nThus, the range of power k is the set of natural numbers k‚àà ‚Ñï\nPower of diagonal matrix $$ ùêÄ = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 3 \\end{bmatrix} = diag\\{1, 2, 3\\} $$\nEach elements on the main diagonal is multiplied with itself many times.\n2\nScalar matrix $$ ùêÄ = \\begin{bmatrix} Œª \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; Œª \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; Œª \\end{bmatrix} = diag\\{Œª, Œª, Œª\\} $$\nIf Œª = 1, then ùêÄ = ùêà. So ùêà·µè=ùêà, and this is also applicable to zero matrix ùüé·µè= ùüé (k‚â†0)\nRef When are two matrices A and B: AB = BA? - StackExchange 11. Powers of matrices - Massachusetts Institute of Technology ","date":"2023-04-02T14:55:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/08_%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E7%9A%84%E6%80%A7%E8%B4%A8/","title":"watch: LA - È´òÂ±± 08 | Properties of matrix multiplication"},{"content":"Review Zero matrix ùêÄ = [^{4\\ 5\\ 6} _{5\\ 6\\ 7}]; ùüé = [^{_{0\\ 0}} _{^{0\\ 0} _{0\\ 0}}]‚ÇÉ‚Çì‚ÇÇ\nùêÄùüé = [^{0\\ 0} _{0\\ 0}] = ùüé‚ÇÇ‚Çì‚ÇÇ\nNon-homogeneous system has no zero solution ùêÄùêÅ ‚â† 0 means A, B both cannot be zero.\nDiagonal matrix and identical matrix ùêÅ is a diagonal matrix.\nùêÄùêÅ is scaling each column of ùêÄ by the value of element on the diagonal times.\nI = []\nVectors multiplication Rank relation ùêÄ = [^{1\\ 2} _{0\\ 3}]; ùêÅ = [^{2\\ 3} _{0\\ 4}]\nùêÄùêÅ = [^{2\\ 11} _{0\\ 12}]\nr(ùêÄ) = 2; r(ùêÅ) = 2; r(ùêÄùêÅ) = 2\nIf ùêÅ = [^{1\\ 0} _{0\\ 0}], r(ùêÅ) = 1, then ùêÄùêÅ = [^{1\\ 0}_{0\\ 0}]; r(ùêÄùêÅ) = 1.\nFurther letting ùêÄ = [^{1\\ 2} _{0\\ 0}], r(ùêÄ) = 1, then ùêÄùêÅ = [^{1\\ 0}_{0\\ 0}]; r(ùêÄùêÅ) = 1.\nIn addition, if ùêÄ = [^{1\\ 2} _{0\\ 0}], ùêÅ = [^{0\\ -2} _{0\\ 1}], then ùêÄùêÅ = [^{0\\ 0}_{0\\ 0}], r(ùêÄùêÅ) = 0\nConclusion: the rank of the product matrix is not greater than the rank of any multiplier.\nr(ùêÄùêÅ) ‚â§ min{ r(ùêÄ), r(ùêÅ)}\n","date":"2023-04-02T14:29:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/07_%E7%89%B9%E6%AE%8A%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E6%B3%95/","title":"watch: LA - È´òÂ±± 07 | Extension for matrix multiplication"},{"content":"Source video: „Äê‰øóËØ¥Áü©Èòµ„ÄëÂàùËØÜÁü©ÈòµÁöÑÁß©Ôºå‰ªéÊúÄÁÆÄÂçïÁöÑ‰æãÂ≠êËÆ≤Ëµ∑ÔºÅ\nFind the rank Rank of a matrix ùêÄ equals to the number of the non-zero rows of a matrix in row echelon form (after performing elementary row operations).\nDefinition of the rank determinant\nProperties The matrices appearing in the transforming process are able to reach the same row echelon form, so they have the same rank. In another word, the elementary row operations don\u0026rsquo;t change the rank of matrices. The rank of a matrix is no bigger than the number of rows or columns. Thus, the solution of a linear equation system can be represented as its rank.\nSolution judged by rank Homogeneous linear equation system:\nOnly zero solution: r(ùêÄ) = n, the rank of the coefficient matrix equals to the number of unknowns Exist non-zero solution: r(ùêÄ) \u0026lt; n, Non-homogeneous linear equation system:\nSingle unique solution: r(ùêÄ|ùêõ) = r(ùêÄ) = n, the rank of the augmented matrix = the rank of the coefficient matrix = the number of the unknowns; Inifinitely many solutions: r(ùêÄ|ùêõ) = r(ùêÄ) \u0026lt; n; No solution: r(ùêÄ|ùêõ) ‚â† r(ùêÄ) r(ùêÄ) = 0 ùêÄ = $[^{_{0\\ 0\\ 0}} _{^{0\\ 0\\ 0} _{0\\ 0\\ 0}}]$\nAll of its elements are 0. There is no non-zero rows. So r(ùêÄ) = 0.\nThis is the so-called zero matrix noted as ùêÄ = ùüé\nr(ùêÄ) = 0 and ùêÄ=ùüé are equivalent, because if any one of elements is not 0, then there is a non-zero row resulting the r(ùêÄ) ‚â† 0.\nr(ùêÄ) = 1 ùêÄ = $[^{_{1\\ 2\\ 3}} _{^{2\\ 4\\ 6}_{3\\ 6\\ 9}}]$ ‚ûî $[^{_{1\\ 2\\ 3}} _{^{0\\ 0\\ 0}_{0\\ 0\\ 0}}]$\nThe rows in the matrix of r(ùêÄ) = 1 are proportional to each other. In fact, the columns are also proportional. Their ratio factor can be 0, and also there must be at least 1 element which is not zero in the matrix. Otherwise, it\u0026rsquo;s the zero matrix. ùêö= [1‚ÄÇ2‚ÄÇ3] and ùêõ = $[^{_1} _{^2_3}]$ are 1x1 matrix with rank = 1.\nVectors are the matrix with only 1 row or 1 column.\nRow vector with at least 1 non-zero value is of rank=1.\nColumn vector has multiple rows with a single value. If it\u0026rsquo;s not a zero vector, the later rows can be reduced to 0 by adding the row1 multiplying with different factors. ùêõ = $[^{_1} _{^2_3}]$ ‚ûî $[^{_1} _{^1_1}]$\nFor all the non-zero vector, no matter row vector or column vector, their rank = 1.\nVector ùêö ‚â† 0 and r(ùêö) = 1 are equivalent.\n","date":"2023-04-02T12:13:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/04_%E5%88%9D%E8%AF%86%E7%9F%A9%E9%98%B5%E7%9A%84%E7%A7%A9/","title":"watch: LA - È´òÂ±± 04 | Rank of matrix"},{"content":"The 1-17 tips come from Faster Deep Learning Training with PyTorch ‚Äì a 2021 Guide - LORENZ KUHN\n1. Consider using another learning rate schedue torch.optim.lr_scheduler.CyclicLR torch.optim.lr_scheduler.OneCycleLR 2. Use multiple workers and pinned memory in DataLoader torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=4, pin_memory=pin_memory) DataLoader-Docs\nnum_workers Rule of thumb: set the num_workers to four times the number of available GPUs. Note that increasing num_workers will increase RAM consumption.\npin_memory When using a GPU it‚Äôs better to set pin_memory=True, this instructs DataLoader to use pinned (page-locked) memory and enables faster and asynchronous memory copy from the host to the GPU. Tutorial-Szymon Migacz\npin_memory avoid one implicit CPU-to-CPU (\u0026ldquo;Pageable Memory\u0026rdquo; to \u0026ldquo;Pinned Memory\u0026rdquo;) copy when perform a.cuda() operation. As the illustration shows in Nvidia Blog I fogot where I got this inspiration: \u0026ldquo;ÁÇπÂØπÁÇπÂ§çÂà∂\u0026rdquo;.\nWith pinned memory tensors, the copy process a.cuda(non_blocking=True) is asynchronous with respect to host (CPU) SO. If the code is structured as:\na.cuda(non_bloking=True) # copy from cpu to gpu Perform some CPU operations Perform GPU operations using a. The step 1 and step 2 can proceed parallelly. Hence, the maximum time can be saved is the duration of step 2.\n3. Max out the batch size Other hyperparameters, like learning rate, have to be adjusted. Rule of thumb: double the learning rate as double the batch size May cause worse generalization performance. 4. Use Automatic Mixed Precision (AMP) The optimizations of some operations use semi-precision (FP16) rather than single-precision (FP32) 5. Consider using another optimizer AdamW outperform Adam resulting from weight decay (rather than L2-regularization). 6. Turn on cudNN benchmarking Tutorial-Szymon Migacz\n7. Avoid unnecessary CPU-GPU synchronizations (Tutorial-Szymon Migacz):\ntensor.cpu() or tensor.cuda(), tensor.to() tensor.item() or tensor.numpy() print(cuda_tensor) cuda_tensor.nonzero() retrieves the indices of all non-zero elements; Avoid python control based on cuda tensors, e.g., if (cuda_tensor != 0).all() The good practice should let the CPU run ahead of the accelerator as much as possible to make sure that the accelerator work queue contains may operations.\nRef ","date":"2023-03-23T14:11:00-05:00","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_train_faster/","title":"memo: PyTorch | Tricks for Faster Training"},{"content":"Authors: Wandong Zhang, Jonathan Wu, Yimin Yang Neurocomputing (2020-07-18)\nAbstract Wide hierarchical subnetwork-based neural network (Wi-HSNN) Iterative training by adding subnetnetwork nodes into modle one-by-one batch-by-batch training instead of processing the entire dataset (one-batch) Place365 has 1.8 million samples 1 Introduction 3 Proposed Wi-HSNN Loss function of SLFN is MSE: min J= ¬Ω ‚Äñùêì - g(ùêó, ùê∞‚ÇÅ, ùêõ)‚ãÖùê∞‚ÇÇ‚Äñ¬≤\nThe output weights can be initialized from the Target data (letting ùêá denote g(ùêó, ùê∞‚ÇÅ, ùêõ)):\nùê∞‚ÇÇ = ( ùêá·µÄùêá + I/C )‚Åª¬π ùêá·µÄ ùêì\nIn this paper, the input data ùêó is first transformed to ùêÖ‚Çë‚Çô, which then passes a SNN (SLFN, subnetwork node) and becomes g(ùêÖ‚Çë‚Çô‚ãÖùêö‚Çë‚Çì + b‚Çë‚Çì). Next, the input ùêó is fed into multiple SNN sequentially. And their outputs are accumulated with the weight ùêö‚Çú.\nTherefore, if there are L SNNs, the loss function for this problem is:\nmin J = ¬Ω ‚Äñùêì - ‚àë·µ¢‚Çå‚ÇÅ·¥∏ g(ùêÖ‚Çë‚Çô‚Å±‚ãÖùêö‚Çë‚Çì‚Å± + b‚Çë‚Çì‚Å±) ‚ãÖ ùêö‚Çú·¥∏‚Äñ¬≤\n3.2 Training the Wi-HSNN Feedforward with randomly initialized (ùêö‚Çë‚Çô¬π, b‚Çë‚Çô¬π) and (ùêö‚Çë‚Çì¬π, b‚Çë‚Çì¬π) and calculate the optimal output weights based on pseudo-inverse:\nùêÖ‚Çë‚Çô¬π = g(ùêó‚ãÖùêö‚Çë‚Çô¬π + b‚Çë‚Çô¬π) ùêÖ‚Çú¬π = ùêÖ‚Çë‚Çì¬π = g(ùêÖ‚Çë‚Çô¬π ‚ãÖùêö‚Çë‚Çì¬π + b‚Çë‚Çì¬π) ùêö‚Çú¬π = (ùêÖ‚Çú·µÄùêÖ‚Çú + I/C)‚Åª¬π ùêÖ‚Çú·µÄ ùêì\nObtain the feedback error (\u0026ldquo;feature H\u0026rdquo;) matrix ùêè‚Çë‚Çì¬π and ùêè‚Çë‚Çô¬π for solving the weights ùêö‚Çë‚Çì (=ùê∞‚ÇÇ), ùêö‚Çë‚Çô (=ùê∞‚ÇÅ) of next iteration.\nùêè‚Çë‚Çì¬π = g‚Åª¬π (ùêû¬π ‚ãÖ (I/C + (ùêö‚Çú¬π)·µÄ‚ãÖùêö‚Çú¬π)‚Åª¬π ‚ãÖ (ùêö‚Çú¬π)·µÄ ) ùêè‚Çë‚Çô¬π = g‚Åª¬π (ùêè‚Çë‚Çì¬π ‚ãÖ (I/C + (ùêö‚Çë‚Çì¬π)·µÄ ‚ãÖ ùêö‚Çë‚Çì¬π )‚Åª¬π ‚ãÖ (ùêö‚Çë‚Çì¬π)·µÄ )\nCalculate the ùêö‚Çë‚Çì, ùêö‚Çë‚Çô for next SNN:\nùêö‚Çë‚Çô‚Å± = (ùêó·µÄùêó + I/C )‚Åª¬π ùêó·µÄ ùêè‚Çë‚Çì‚Å±‚Åª¬π (Entrance layer weights) ùêö‚Çë‚Çì‚Å± = ( (ùêÖ‚Çë‚Çô‚Å±)·µÄùêÖ‚Çë‚Çô‚Å± + I/C )‚Åª¬π (ùêÖ‚Çë‚Çô‚Å±)·µÄ ùêè‚Çë‚Çô‚Å±‚Åª¬π (Exit layer weights)\nSummarize outputs ùêÖ‚Çú‚Å± from all exisiting SNN, and update output weight ùêö‚Çú‚Å±.\nùêÖ‚Çú‚Å± = ‚àë‚Çñ‚Çå‚ÇÅ‚Å± ùêÖ‚Çë‚Çì·µè ùêö‚Çú‚Å± = ( (ùêÖ‚Çú‚Å±)·µÄùêÖ‚Çú‚Å± + I/C )‚Åª¬π (ùêÖ‚Çú‚Å±)·µÄ ùêì\nObtain the feedback error ùêè‚Çë‚Çì‚Å± and ùêè‚Çë‚Çô‚Å±:\nùêè‚Çë‚Çì‚Å± = g‚Åª¬π (ùêû‚Å± ‚ãÖ (I/C + (ùêö‚Çú‚Å±)·µÄ‚ãÖùêö‚Çú‚Å±)‚Åª¬π ‚ãÖ (ùêö‚Çú‚Å±)·µÄ ) ùêè‚Çë‚Çô‚Å± = g‚Åª¬π (ùêè‚Çë‚Çì‚Å± ‚ãÖ (I/C + (ùêö‚Çë‚Çì‚Å±)·µÄ ‚ãÖ ùêö‚Çë‚Çì‚Å± )‚Åª¬π ‚ãÖ (ùêö‚Çë‚Çì‚Å±)·µÄ )\nRepeat step 3 to step 5 L-2 times. ùêÖ‚Çú·¥∏ is the final encoding. And ùêò = ùêÖ‚Çú·¥∏ ùêö‚Çú·¥∏ is the final classification prediction.\n3.4 Batch-by-batch scheme with parallelism strategy The entire feature set ùêÖ·¥∫·ïΩ·µà (i.e., ùêá) and the target set ùêì are split into p subsets:\nùêá = $[^{ùêá(ùê±‚ÇÅ)}_{^{\u0026hellip;}_{ùêá(ùê±‚Çö)}}]$, ùêì = $[^{ùêì(ùê±‚ÇÅ)}_{^{\u0026hellip;}_{ùêì(ùê±‚Çö)}}]$\nThe desired weights matrix ùê∞‚ÇÇ (i.e., \u0026ldquo;ùõÉ\u0026rdquo;) represented weith pseudo-inverse matrix becomes\nùê∞‚ÇÇ = (ùêá·µÄùêá + I/C)‚Åª¬πùêÖ·µÄ‚ãÖùêì\nùê∞‚ÇÇ = ([ùêá‚ÇÅ·µÄ,\u0026hellip;, ùêá‚Çö·µÄ] $[^{_{ùêá(ùê±‚ÇÅ)}} _{^{\u0026hellip;}_{ùêá(ùê±‚Çö)}}] + ^I_{^-_C}$)‚Åª¬π [ùêá‚ÇÅ·µÄ,\u0026hellip;, ùêá‚Çö·µÄ] $[ ^{_{ùêì(ùê±‚ÇÅ)}}_{^{\u0026hellip;}_{ùêì(ùê±‚Çö)}} ]$\nùê∞‚ÇÇ = ([ùêá(ùê±‚ÇÅ)·µÄùêá(ùê±‚ÇÅ) + \u0026hellip; + ùêá(ùê±‚Çö)·µÄùêá(ùê±‚Çö)] + I/C)‚Åª¬π ‚ãÖ [ùêá(ùê±‚ÇÅ)·µÄùêì(ùê±‚ÇÅ) + \u0026hellip; + ùêá(ùê±‚Çö)·µÄùêì(ùê±‚Çö) ] ùê∞‚ÇÇ = (‚àë·µ¢‚Çå‚ÇÅ·µñ ùêá(ùê±·µ¢)·µÄùêá(ùê±·µ¢) + I/C)‚Åª¬π ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·µñ ùêá(ùê±·µ¢)·µÄùêì(ùê±·µ¢)\nFirst, calculate (‚àë·µ¢‚Çå‚ÇÅ·µñ ùêá(ùê±·µ¢)·µÄùêá(ùê±·µ¢) + I/C)‚Åª¬π.\nThe matrices are accumulated batch by batch in the code. After the 1st iteration, K=(ùêá‚ÇÅ·µÄùêá‚ÇÅ + I/C)‚Åª¬π has obtained and returned.\nWhen next batch ùêá‚ÇÇ is retrieved:\nK_new = (ùêá‚ÇÇ·µÄùêá‚ÇÇ + ùêá‚ÇÅ·µÄùêá‚ÇÅ + I/C)‚Åª¬π = (ùêá‚ÇÇ·µÄùêá‚ÇÇ + K‚Åª¬π)‚Åª¬π # analogy to (UBV + A)‚Åª¬π, where U=ùêá‚ÇÇ·µÄ, V=ùêá‚ÇÇ, B=I, A=K‚Åª¬π Woodbury: \u0026ldquo;A‚Åª¬π - A‚Åª¬π‚ãÖU‚ãÖ(I+BV‚ãÖA‚Åª¬π‚ãÖU)‚Åª¬π BV A‚Åª¬π \u0026quot; 1 = K - K‚ãÖùêá‚ÇÇ·µÄ‚ãÖ(I+ùêá‚ÇÇ‚ãÖK‚ãÖùêá‚ÇÇ·µÄ)‚Åª¬π ùêá‚ÇÇ K = (I - K‚ãÖùêá‚ÇÇ·µÄ‚ãÖ(I+ùêá‚ÇÇ‚ãÖK‚ãÖùêá‚ÇÇ·µÄ)‚Åª¬π ùêá‚ÇÇ ) ‚ãÖ K Let K‚Çö = (I - K‚ãÖùêá‚ÇÇ·µÄ‚ãÖ(I+ùêá‚ÇÇ‚ãÖK‚ãÖùêá‚ÇÇ·µÄ)‚Åª¬π ùêá‚ÇÇ ). So K_new = K‚Çö ‚ãÖ K\nThen, for the second item ‚àë·µ¢‚Çå‚ÇÅ·µñ ùêá(ùê±·µ¢)·µÄùêì(ùê±·µ¢),\nIn the first batch, ùõÉ=K‚ãÖùêá‚ÇÅ·µÄ‚ãÖùêì‚ÇÅ is obtained and returned.\nWhen the second batch coming, there should be (ùêá‚ÇÅ·µÄ‚ãÖùêì‚ÇÅ + ùêá‚ÇÇ·µÄ‚ãÖùêì‚ÇÇ)\nùõÉ_new = K_new ‚ãÖ (ùêá‚ÇÅ·µÄ‚ãÖùêì‚ÇÅ + ùêá‚ÇÇ·µÄ‚ãÖùêì‚ÇÇ) = K_new ‚ãÖ (ùêá‚ÇÅ·µÄ‚ãÖùêì‚ÇÅ + ùêá‚ÇÇ·µÄ‚ãÖùêì‚ÇÇ) = K‚Çö ‚ãÖ K ‚ãÖ ùêá‚ÇÅ·µÄ‚ãÖùêì‚ÇÅ + K_new ‚ãÖ ùêá‚ÇÇ·µÄ‚ãÖùêì‚ÇÇ = K‚Çö ‚ãÖ ùõÉ + K_new ‚ãÖ ùêá‚ÇÇ·µÄ‚ãÖùêì‚ÇÇ\nÁü©Èòµ‰πãÂíåÁöÑÈÄÜ (DDG search: \u0026ldquo;Áü©Èòµ‰πãÂíåÁöÑÈÄÜ\u0026rdquo;) ‰∏§‰∏™Áü©ÈòµÁõ∏Âä†ÂêéÊ±ÇÈÄÜ - ~Êµ∑Ê£†‰æùÊóß~ - CSDN\nÂÖ≥‰∫é‰∏§‰∏™Áü©Èòµ‰πãÂíåÈÄÜÈòµÁöÑËÆ®ËÆ∫ - docin\n(Google search: \u0026ldquo;‰∏§Áü©ÈòµÂíåÁöÑÈÄÜ\u0026rdquo;) ÂÖ©Áü©Èô£ÂíåÁöÑÈÄÜÁü©Èô£ - Á∑ö‰ª£ÂïüÁ§∫ÈåÑ - Âë®ÂøóÊàê(Èò≥Êòé‰∫§Â§ß)\nÁü©Èòµ‰πãÂíåÁöÑÈÄÜ ‰∏çÁ≠â‰∫é ÈÄÜÁü©ÈòµÁöÑÂíå (A+B)‚Åª¬π ‚â† A‚Åª¬π + B‚Åª¬π\n‚àµ (A+B)(A‚Åª¬π + B‚Åª¬π) = E + BA‚Åª¬π + AB‚Åª¬π + E ‚â† E\nÊúÄÁÆÄÂçïÁöÑ‰æãÂ≠êÔºöÂèñ A=B=EÔºå(A+B)(A‚Åª¬π + B‚Åª¬π) = E + BA‚Åª¬π + AB‚Åª¬π + E = 4E ‚â† E Áü©ÈòµÂíåÁöÑÈÄÜÁü©Èòµ ÈÄÜÁü©ÈòµÁöÑÂíå Áõ∏Á≠âÂêó -ÁôæÂ∫¶Áü•ÈÅì\nRef ÂÖ©Áü©Èô£ÂíåÁöÑÈÄÜÁü©Èô£ - Á∑ö‰ª£ÂïüÁ§∫ÈåÑ - Âë®ÂøóÊàê(Èò≥Êòé‰∫§Â§ß) ","date":"2023-03-15T20:13:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-batch-train/","title":"read: Wi-HSNN for dimension reduction"},{"content":"Inverse of Sigmoid 1 2 def inverse_sigmoid(x): return torch.log(x/(1-x)) Inverse of Softmax Determining Existence Only monotonic function has its inverse function. ¬π\nThe softmax function is not invertible? Any constant C will work. Or C=(1‚àílog(x‚ãÖy‚ãÖz))/3) Code Implement a layer to invert the softmax by tensorflow: ¬≤\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import tensorflow as tf def inv_softmax(x, C): return tf.math.log(x) + C import math input = tf.keras.layers.Input(shape=(1,10)) x = tf.keras.layers.Lambda(lambda x : inv_softmax(x, math.log(10.)),name=\u0026#39;inv_softmax\u0026#39;)(input) model = tf.keras.Model(inputs=input, outputs=x) a = tf.zeros([1, 1, 10]) a = tf.nn.softmax(a) a = model(a) print(a.numpy()) Ref ÂèçÂáΩÊï∞ÁöÑÂÆö‰πâÂèäÊ±ÇÊ≥ï - Âçä‰∏™ÂÜØÂçöÂ£´ÁöÑÊñáÁ´† - Áü•‰πé; (Searched by DDG: \"Ê±ÇÂíåÂáΩÊï∞ÁöÑÂèçÂáΩÊï∞\") How to create a layer to invert a softmax (TensforFlow,python)? - StackOverflow Can we have an explicit representation of inverse of a softmax function? -StackExchange (searched by DDG: \"softmax function inverse\") Invert the softmax function - StackExchange What is an intuitive interpretation for the softmax transformation?\n‰∏ÄÊñáËØ¶Ëß£SoftmaxÂáΩÊï∞ - Ëß¶Êë∏Â£πÁºïÈò≥ÂÖâÁöÑÊñáÁ´† - Áü•‰πé (searched by DDG: \"sotmax\") (2023-02-23)\nInverse of Sum Is the summation operation invertible?\n(DDG search: \u0026ldquo;inverse summation\u0026rdquo; or \u0026ldquo;invert a summation\u0026rdquo;)\n","date":"2023-02-19T11:00:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/inverse_softmax/","title":"memo: Calc | Inverse Function of Activations"},{"content":"Authors: Wandong Zhang et. al. Publish date: 2020-03-30 (Finished in 2019)\nIEEE Trans. Industrial Informatics | G.Drive | G.Scholar\nTry to summary (2023-02-26):\nDifferent features are concatenated and then fed into a \u0026ldquo;I-ELM with subnetwork nodes\u0026rdquo;. What is optimized it the combination weights, but the feature vectors themselves are not changed. It is the weights (IW, ùõÉ) are refined. Sepecifically, the new R-SNN node is improved by adding a part of unlearned wights accquired from the residual error of the last node. That is the weights are accumulated on the newest node. So the ultimate R-SNN node contains all the previous training outcomes. What we kept is only the last R-SNN node, i.e., a SLFN. Does that require the performance of the final R-SNN is the best among the former nodes. (In code) The update process of a SLFN is as follows:\nflowchart LR subgraph node1[SLFN1] h1((h1)) \u0026 h2((h2)) \u0026 he((he)) \u0026 hd((hd)) end In[\"X\\n data\\n matrix\"] --\u003e IW1 --\u003e h1 \u0026 h2 \u0026 he \u0026 hd node1 --- beta1(\"ùõÉ1\") --\u003e Yout1 --\u003e Error1 Target --\u003e|\"pinv\"| beta1 beta1 \u0026 Error1 --\u003e|\"inv: ùõÉ‚ãÖP=ùêû‚ÇÅ\"| P[\"P\\n ( The H\\n yielding\\n Error1) \"] P \u0026 In --\u003e IW'[\"residual\\n IW\"] IW' \u0026 IW1 --\u003esum((+)) --\u003e IW2 subgraph node2[SLFN2] h21((h1)) \u0026 h22((h2)) \u0026 h2e((he)) \u0026 h2d((hd)) end IW2 --\u003e h21 \u0026 h22 \u0026 h2e \u0026 h2d node2 --- beta2(\"ùõÉ2\") Target --\u003e |\"pinv\"| beta2 beta2 --\u003e Yout2 Abstract A supervised multi-layer subnetwork-based feature refinement and classification model for representation learning. Expand the width for a generalized hidden layer rather than stack more layers to go deeper One-shot solution for finding the meaningful latent space to recognize the objects rather than searching separate spaces to find a generalized feature space. Multimodal fusion fusing various feature sources into a superstate encoding instead of a unimodal feature coding in the traditional feature representation methods. ‚Ö†. Introduction (Task \u0026amp; Application \u0026amp; List of ralated research field \u0026amp; Problem \u0026amp; Existing solutions brif)\nTask: high-dimensional data processing and learning Problem definition: selecting the optimal feature descriptors 2 branch of solutions: hand-crafted descriptors and deep-learning-based features. (Criticize the former feature extraction solutions and introduce proposed method:)\nFeatures derived from approaches of those 2 categories are too inflexible to contribute a robust model. This method \u0026ldquo;encodes and refines these? raw features from multiple sources to improve the classification performance\u0026rdquo;. For example: 4 extracted features (from AlexNet, ResNet, HMP, and SPF) are concatenated into 1 vector taken as the input to a \u0026ldquo;3-layer\u0026rdquo; model, where only a single \u0026ldquo;genearlized\u0026rdquo; hidden layer (latent space) bridges the raw feature space (transformation ax+b) and the final target space (residual error). (Recap deep learning models and mention the theory base of this work)\nDeep networks often get \u0026ldquo;trapped in local minimum and are sensitive to the learning rate\u0026rdquo; because their training fundation is BP. Regression-based feature learning. Least-squares representation learning methods. (Problems to be solved) Drawbacks of regression-based approaches:\n\u0026ldquo;block\u0026rdquo; models? don\u0026rsquo;t perform one-shot training philosophy based on the relation between raw data and the target. A model trained by some \u0026ldquo;designed\u0026rdquo; process has a inferious generalizatio n capacity than the model derived from one-shot training strategy (least-squares). Drawbacks of multilayer neural networks \u0026amp; solution Deeper layer-stacked models suffer from overfitting with limited training samples. Network-in-network structure enhances the network\u0026rsquo;s generalization capacity for learning feature. ELM with subnetwork nodes. Contributions: Subnetwork neural nodes (SNN) realized multilayer representation learning. Unlike the ensembled network, the SNN is trained based on the error term. Feature space transformation and the classification are solved together by searching iteratively the optimal encoding space (hidden layer). Concatenation of multiple features result more discriminative representations for samples. ‚Ö°. Literature review A. Conventional Feature Coding \u0026quot; Supervised method of learning representaiton evaluates the importance of a specific feature through the correlation between features and categories.\u0026quot;\nConventional feature coding of images depends on prior knowledge of the problem. Thus, the features are not complete representations.\nThis paper enhances the feature by fusing (discriminative) hand-crafted features and (class-specific) CNN-based features.\nB. Least-Squares Encoding Methods The least-squares approximation methods, such as random forest and alternating minimization, have been exhaustively investigated in single-layer neural networks.\nRelated works: Moore-Penrose inverse; Universal approximation capacity of I-ELM, ELM autoe-ncoder14, Features combined with subnetwork nodes 18\nEach SNN is applied as a local feature descriptor. Hence, the subspace features can be extracted? from the original data independently, and the useful features are generated via the combination of these features.\n‚Ö¢. Proposed Method A. Algorithmic Summary Two steps:\nPreprocessing: concatenate various feature vectors into a single \u0026ldquo;supervector\u0026rdquo;. Train the width-growth model: Terminology: layer name marker params in out input Entrance (feature) layer ùëì ùêñ·µ¢·∂†, ùêõ·µ¢·∂† random vct linear combination ùêá hidden Refinement layer/subspace ùëü ùêñ·µ¢ ≥, ùêõ·µ¢ ≥ (ùêö,b) ùêá partial feature Œ® output Least square learning layr ùë£ ùêñ·µ¢·µõ (ùõÉ) Œ® sum up all partial features: ùö™ residual error ùêû (An entrance layer and a refinement layer both are \u0026ldquo;SNN\u0026rdquo;, and their combination is a \u0026ldquo;R-SNN\u0026rdquo;)\nInitialization: For the 1st R-SNN, ùêñ‚ÇÅ·∂†, ùêñ‚ÇÅ ≥ are random generating a false feature Œ®. Then the first least-square method (pseudoinverse) is performed to calculate ùêñ‚ÇÅ·µõ based on target ùêò and Œ®.\nIteratively add the R-SNN (2‚â§ i‚â§ L) (refinement subspace) into the hidden layer (optimal feature space)\nflowchart TB subgraph In[input feature] x1((1)) \u0026 x2((2)) \u0026 xe((\"‚ãÆ\")) \u0026 xn((n)) end EnW(\"Entrance layer\\n ùêñ·µ¢·∂†, ùêõ·µ¢·∂†\\n random\") subgraph H[\"entrance feature ùêá\"] h1((1)) \u0026 h2((2)) \u0026 he((\"‚ãÆ\")) \u0026 hD((D)) end RefineW(\"Refinement layer\\n ùêñ·µ¢ ≥, ùêõ·µ¢ ≥\") subgraph Psi[partial feature Œ®] Œ®1((1)) \u0026 Œ®2((2)) \u0026 Œ®e((\"‚ãÆ\")) \u0026 Œ®d((d)) end OW(\"Output layer\\n ùêñ·µ¢·µõ\") subgraph Out[\"Output vector\"] o1((1)) \u0026 o2((2)) \u0026 oe((\"‚ãÆ\")) \u0026 om((m)) end x1 \u0026 x2 \u0026 xe \u0026 xn --\u003e EnW --\u003e h1 \u0026 h2 \u0026 he \u0026 hD --\u003e RefineW --\u003e Œ®1 \u0026 Œ®2 \u0026 Œ®e \u0026 Œ®d --\u003e OW --\u003e o1 \u0026 o2 \u0026 oe \u0026 om Out --\u003e|\"- ùêû·µ¢‚Çã‚ÇÅ\"| erri[\"ùêû·µ¢\"] erri \u0026 OW -.-\u003e|pinv| newŒ®(\"ùêè \\n yielding\\n ùêû·µ¢\") subgraph H1[\"entrace feature ùêá·µ¢‚Çä‚ÇÅ\"] h11((1)) \u0026 h12((2)) \u0026 h1e((\"‚ãÆ\")) \u0026 h1D((D)) end In --\u003e EnW1(\"Entrance layer\\n ùêñ·µ¢‚Çä‚ÇÅ·∂†, ùêõ·µ¢‚Çä‚ÇÅ·∂†\\n random\") --\u003e h11 \u0026 h12 \u0026 h1e \u0026 h1D H1 --\u003e RefineW1(\"Refinement layer\\n ùêñ·µ¢‚Çä‚ÇÅ ≥, ùêõ·µ¢‚Çä‚ÇÅ ≥\") %%-.-|solved by P| newŒ® newŒ® -.-\u003e RefineW1 subgraph Psi1[partial feature Œ®] Œ®11((1)) \u0026 Œ®12((2)) \u0026 Œ®1e((\"‚ãÆ\")) \u0026 Œ®1d((d)) end RefineW1 --\u003e Œ®11 \u0026 Œ®12 \u0026 Œ®1e \u0026 Œ®1d --\u003e OW1(\"Output layer\\n ùêñ·µ¢‚Çä‚ÇÅ·µõ\") %%OW1 -.-|solved by| erri erri -.-\u003e OW1 subgraph Out1[\"Output vector\"] o11((1)) \u0026 o12((2)) \u0026 o1e((\"‚ãÆ\")) \u0026 o1m((m)) end OW1 --\u003e o11 \u0026 o12 \u0026 o1e \u0026 o1m Out1 --\u003e|\"- ùêû·µ¢\"| erri+1[\"ùêû·µ¢‚Çä‚ÇÅ\"] --\u003e newP B. Model Definition SLFN solves the regression problem can be expressed as:\nMLNN has nested transformation:\nProposed method is a generlized SLFN:\nminimize J = ¬Ω ‚Äñùêò-f(ùêá·µ¢·∂†, ùêñ·µ¢ ≥, ùêõ·µ¢ ≥)‚ãÖùêñ_L·µõ‚Äñ¬≤,\nf(ùêá·µ¢·∂†, ùêñ·µ¢ ≥, ùêõ·µ¢ ≥) = ‚àë·µ¢‚Çå‚ÇÅ·¥∏ g(ùêá·µ¢·∂† ‚ãÖ ùêñ·µ¢ ≥ + ùêõ·µ¢ ≥): sum all R-SNN ùêá·µ¢·∂† = g(ùêñ·µ¢·∂†, ùêõ·µ¢·∂†, ùêó) ùêò ‚àà ‚Ñù·¥∫·ïΩ·µê: expected output, target feature ùêó ‚àà ‚Ñù·¥∫·ïΩ‚Åø: input matrix L : number of R-SNN node g : activateion function 3 differences from other least-squares-based MLNNs\nSNN combines each dimension of the feature vector serving as local feature descriptor. While the R-SNN is the basic unit to refine feature vectors.\nOptimal feature is the aggregation of R-SNN added one by one. R-SNN is densly connected to input vector and output layer containing twice linear projection. Different R-SNNs are independent because they learn from different error.\nThe latent space is the aggregation of all R-SNN nodes subspace. So the parameters training has no block-wise communication between different spaces. That means the feature refinement and classification are doen together.\nC. Proposed Width-Growth Model Input weights and bias ùêñ·µ¢·∂†, ùêõ·µ¢·∂†: randomly initialized; Entrance feature: ùêá·µ¢·∂† = g(ùêóùêñ·µ¢·∂†+ ùêõ·µ¢·∂†); Refined partial feature: Œ®·µ¢=g(ùêá·µ¢·∂†ùêñ·µ¢ ≥+ ùêõ·µ¢ ≥), where ùêõ·µ¢ ≥ is random; Output weights: ùêñ·µ¢·µõ=(ùêà/C + ùö™·µÄùö™)‚Åª¬πùö™·µÄ‚ãÖùêò, where C is hyperparameter for regularization, and (ùêà/C + ùö™·µÄùö™)‚Åª¬πùö™·µÄ is the pseudoinverse of output vector ùö™ (label?) Error: ùêû·µ¢ = ùêò - ùêñ·µ¢·µõ\nùêè is the desired matrix generating ùêû·µ¢ by: ùêè·µ¢‚ãÖùêñ·µ¢·µõ=ùêû·µ¢, so ùêè·µ¢ = ùêû·µ¢‚ãÖ(I/C + (ùêñ·µ¢·µõ)·µÄùêñ·µ¢·µõ)‚Åª¬π(ùêñ·µ¢·µõ)·µÄ\nRefinement layer weights of next R-SNN: ùêñ·µ¢‚Çä‚ÇÅ ≥ = (I/C +ùêá·µ¢·µÄùêá·µ¢)‚Åª¬πùêá·µ¢·µÄ ‚ãÖ g‚Åª¬π(ùêè·µ¢), because g(ùêá·µ¢‚Çä‚ÇÅ‚ãÖùêñ·µ¢‚Çä‚ÇÅ ≥+ ùêõ·µ¢‚Çä‚ÇÅ ≥) = ùêè·µ¢. Next partial feature: Œ®·µ¢‚Çä‚ÇÅ = g(ùêá·µ¢‚Çä‚ÇÅ‚ãÖùêñ·µ¢‚Çä‚ÇÅ ≥+ ùêõ·µ¢‚Çä‚ÇÅ ≥)\nAccumulate the partial feature to the optimal feature: ùö™·µ¢‚Çä‚ÇÅ = ùö™·µ¢ + Œ®·µ¢‚Çä‚ÇÅ Update error: ùêû·µ¢‚Çä‚ÇÅ = ùêû·µ¢-ùêñ·µ¢·µõùö™·µ¢\nRepeat steps 4-6 L-2 times, and the final feature ùö™$_L$ is the generalized feature correponding to the best output parameter ùêñ $_L·µõ$ for classification.\nRef L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, ‚ÄúRepresentational learning with extreme learning machine for big data,‚Äù IEEE Intell. Syst.,Dec. 2013. Y. Yang and Q. J. Wu, ‚ÄúFeatures combined from hundreds of midlayers: Hierarchical networks with subnetwork nodes,‚Äù IEEE Trans. Neural Netw. Learn. Syst., Nov. 2019. J. Tang, C. Deng, and G.-B. Huang, ‚ÄúExtreme learning machine for multilayer perceptron,‚Äù IEEE Trans. Neural Netw. Learn. Syst., Apr. 2016. ","date":"2023-02-15T12:40:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-refine-weights/","title":"read: Width-Growth Model with Subnetwork Nodes"},{"content":"Authors: Yimin Yang; Yaonan Wang; Xiaofang Yuan\nCode | TNNLS (2012-06-20)\nTry to summary: (2023-02-15) B-ELM is a variant of I-ELM by dividing the hidden nodes into 2 types: odd and even, which differ in wehther the input parameters (ùêö,b) is randomly generated or calculated by twice inverse operations.\nSpecifically, the first node is solved from the target ùê≠. Then for subsequent nodes, the even node is solved based on the residual error of its previous odd node.\nThe output weights of the odd nodes are calculated as: ùêá‚ÇÇ‚Çô‚Çã‚ÇÅ ≥ + e‚ÇÇ‚Çô‚Çã‚ÇÇ (or ùê≠) ‚ûî ùõÉ‚ÇÇ‚Çô‚Çã‚ÇÅ (‚ûîe‚ÇÇ‚Çô‚Çã‚ÇÅ) The input weights ùêö and bias b of even nodes are solved based on the residual error: e‚ÇÇ‚Çô‚Çã‚ÇÅ ‚ûî ùêá‚ÇÇ‚Çô·µâ ‚ûî ùêö,b (‚ûî ^ùêá‚ÇÇ‚Çô·µâ ‚ûî ùõÉ‚ÇÇ‚Çô ‚ûî e‚ÇÇ‚Çô) Note: the superscript r and e stand for \u0026ldquo;random\u0026rdquo; and \u0026ldquo;error\u0026rdquo; marking the source of H. Abstract This algorithm tends to reduce network output error to 0\nby solving the input weights ùêö and bias b based on the network residual error. (In other words, the residual error is represented by a, b of subsequent nodes. Or the error is absorbed by others parameters besides ùõÉ.)\n‚Ö†. Introduction For ELM with a fixed structure, the best number of hidden nodes need to ffound by trial-and-error, because the residual error is not always decreasing when there are more hidden nodes in an ELM.\nFor incremental ELM, the hidden node is added one by one, so the residual error keeps decreasing. But the model training has to do multiple iterations, i.e., calculating inverse matrix is needed after adding each node.\nCompared with other incremental ELM, this method is\nFaster and with fewer hidden nodes showing the relationship between the network output residual error ùêû and output weights ùõÉ, which is named \u0026ldquo;error-output weights ellipse\u0026rdquo; The hidden layer (input weights) with determined parameters instead of random numbers would make the error reduce, or improve the accuracy. ‚Ö°. Preliminaries and Notation A. Notations and Definitions ‚ü®u,v‚ü© = ‚à´‚Çìu(ùê±)v‚Äæ(ùê±)dùê± is the Frobenius inner product of two matrices u,v, where the overline denotes the complex conjugate.\nB. I-ELM Lemma 1(proved by Huang‚Å∏): indicated that (For the incremental ELM,) the target function can be approximated with more nodes added into the network by reducing the residual error to 0, as the ùõÉ of each new node is calculated based on the error of the network last status e‚Çô‚Çã‚ÇÅ as:\nùõÉ = $\\frac{‚ü®e‚Çô‚Çã‚ÇÅ, ùêá‚Çô ≥‚ü©}{‚Äñùêá‚Çô ≥‚Äñ¬≤}$\nThe numerator is the inner product measuring the distance from the nth (random) hidden node ùêá‚Çô ≥ to be added and the error of the network with n-1 hidden nodes.\nSo the output of the newly added hidden nodes ùêá‚Çô ≥ are getting smaller and smaller, because they are learning something from the residual error.\n‚Ö¢. Proposed Bidirectional ELM Method A. Structure of the Proposed Bidirectional ELM Method Two types of node, the node with odd index {2n-1} has random ùêö,b, while the ùêö,b of the node with even index {2n} are calculated based on the residual error of the network with an odd number of nodes at the last moment.\nTheir output weights both are calculated based on Lemma 1. The ùêá of the even node is from residual error, not from the random a,b.\nThe odd node aims to approximate the target through ùêá‚ÇÇ‚Çô‚Çã‚ÇÅ ≥Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ, where ùêá‚ÇÇ‚Çô‚Çã‚ÇÅ is yield based on random a,b;\nThe odd node 2n-1 approximates the previous residual error with random generated a,b;\nBut the even node approximates the residual error ùêû‚ÇÇ‚Çô‚Çã‚ÇÅ through ùêá‚ÇÇ‚Çô·µâŒ≤‚ÇÇ‚Çô with calculated a,b, where ùêá‚ÇÇ‚Çô·µâ is yield with the weights a,b solved based on the residual error ùêû‚ÇÇ‚Çô‚Çã‚ÇÅ from the target (the job hasn\u0026rsquo;t done by all the previous nodes)\nBi-direction means the approximation is learned from both the target and the error, where the odd node (Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ) solved by the target, while the even node (Œ≤‚ÇÇ‚Çô) calculated by the error. So a pair of odd node and even node is a complete step toward to the target.\nBi-directional means H‚ÇÇ‚Çô·µâ is calculated first from eq.(6); Then it is calculated again using the ^a,^b, which are solved based on H‚ÇÇ‚Çô·µâ, to get the updated ^H‚ÇÇ‚Çô·µâ, which is used to calculate the output weight for the next random odd node based on the Lemma 1.\nflowchart LR subgraph in[inputs] x1 \u0026 xe[\"‚ãÆ\"] \u0026 xn end rand((\"random\\n ùêö,b\")) calculated((\"solved\\n ùêö,b\")) subgraph hid[hidden] H1 \u0026 he[\"‚ãÆ\"] \u0026 h2n-1 \u0026 h2n end x1 \u0026 xe \u0026 xn --\u003e rand --\u003e h2n-1[\"ùêá‚ÇÇ‚Çô‚Çã‚ÇÅ ≥\"] ---|Lemma 1| Œ≤2n-1((\"Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ\"))--\u003e e2n-1[\"e‚ÇÇ‚Çô‚Çã‚ÇÅ\"] x1 \u0026 xe \u0026 xn --\u003e calculated --\u003e h2n[\"ùêá‚ÇÇ‚Çô·µâ\"] ---|Lemma 1| Œ≤2n((\"Œ≤‚ÇÇ‚Çô\"))--\u003e e2n[\"e‚ÇÇ‚Çô\"] h2n -.- |\"‚¨Ö inv of\\n Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ\"| e2n-1 calculated -.-|\"‚¨Ö inv of\\n ùê±\"| h2n The dot lines represent the inverse calculation. Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ is derived from the network residual error of the last status.\nBlock diagram:\nflowchart TB init[Initialization: \\n Given training set,\\n expect accracy Œ∑ and \\n let #hidden nodes L=0] --\u003e incre[L = L+1] --\u003e OdEv{\"Is L \\n odd or even?\"} OdEv --\u003e|L=2n+1| I-ELM --\u003e calcE[Calculate \\n residual error E] OdEv --\u003e|L=2n| Theorem2 --\u003e update[\"Update ^H_L = ^ùêö_L ùê± + ^b)\\n and calculate the output weight \\n Œ≤_L based on eq.(7)\"] --\u003e calcE subgraph Theorem2 direction TB calcH[\"Calculate output matrix \\n H_L basd on eq.(6)\"] --\u003e calcab[\"Calculate hidden-node parameters \\n (^ùêö_L,^b_L) based on eq.(14)\"] end subgraph I-ELM direction TB rand[\"Randomly assign hidden-node\\n parameters (ùêö_L,b_L) \\n and obtain\\n output matrix H_L\"] --\u003e |Lemma 1| outW[\"Calculate the output weight \\n Œ≤L according to eq.(8)\"] end calcE --\u003e thres{\"‚ÄñE‚Äñ\u003cŒ∑\"} --\u003e |Yes| END %%thres ----\u003e|No| incre %% mess up the chart incre o---o|No| thres B. Bidirectional ELM method Theorem 1 states the target function ùëì is approximated by the existing network ùëì‚ÇÇ‚Çô‚Çã‚ÇÇ plus the last two nodes: ùêá‚ÇÇ‚Çô‚Çã‚ÇÅ ≥Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ and ùêá‚ÇÇ‚Çô·µâŒ≤‚ÇÇ‚Çô, when n‚Üí‚àû. On the other hand, the residual error the network is 0:\n$lim_{n‚Üí‚àû}‚Äñùëì-(ùëì‚ÇÇ‚Çô‚Çã‚ÇÇ + ùêá‚ÇÇ‚Çô‚Çã‚ÇÅ ≥Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ + ùêá‚ÇÇ‚Çô·µâŒ≤‚ÇÇ‚Çô)‚Äñ = 0$\nwhere the sequence of the ùêá‚ÇÇ‚Çô·µâ (the output of the even node calculated from the feedback error) is determined by the inverse of last output weight Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ:\n$ùêá‚ÇÇ‚Çô·µâ = e‚ÇÇ‚Çô‚Çã‚ÇÅ ‚ãÖ(Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ)‚Åª¬π$ (6)\nThat means $ùêá‚ÇÇ‚Çô·µâ ‚ãÖ Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ = e‚ÇÇ‚Çô‚Çã‚ÇÅ$, this even node is approaching the last residual error based on the known output weight Œ≤‚ÇÇ‚Çô‚Çã‚ÇÅ.\nThen its output weight can be calculated based on the Lemma 1 as:\n$Œ≤‚ÇÇ‚Çô = \\frac{‚ü®e‚ÇÇ‚Çô‚Çã‚ÇÅ, ùêá‚ÇÇ‚Çô·µâ‚ü©}{‚Äñùêá‚ÇÇ‚Çô·µâ‚Äñ¬≤}$ (7)\nOnce this even node is determined, the corresponding residual error e‚ÇÇ‚Çôcan be generated, and also the output weight of the next odd node (with ùêá‚ÇÇ‚Çô‚Çä‚ÇÅ ≥ from random weights) can be calculated based on Lemma 1:\n$Œ≤‚ÇÇ‚Çô‚Çä‚ÇÅ = \\frac{‚ü®e‚ÇÇ‚Çô, ùêá‚ÇÇ‚Çô‚Çä‚ÇÅ ≥‚ü©}{‚Äñùêá‚ÇÇ‚Çô‚Çä‚ÇÅ ≥‚Äñ¬≤}$ (8)\nTheorem 2 further states the updated ^ùêá‚ÇÇ‚Çô·µâ calculated with the optimal ^a, ^b solved based on ùêá‚ÇÇ‚Çô·µâ by the least-square (pseudo inverse) can also let the residual error converge to 0.\nRemark 1 clarifies the differences between this method and I-ELM, that is the input weights and bias of even nodes are calculated, not randomly generated. And the output weights are set similarly based on Lemma 1.\nBased on the eq. 6, the Œî = ‚Äñe‚ÇÇ‚Çô‚Çã‚ÇÅ‚Äñ¬≤ + ‚Äñe‚ÇÇ‚Çô‚Äñ¬≤ can be reformalized to an ellipse curve.\nCode Training needs to store parameters for each node. And testing needs to query each node sequentially.\nReference G. B. Huang, L. Chen, and C. K. Siew, ‚ÄúUniversal approximation using incremental constructive feedforward networks with random hidden nodes,‚Äù IEEE Trans. Neural Netw., Jul.2006. ","date":"2023-02-10T19:29:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-bidirec/","title":"read: Nets - SLFN | B-ELM"},{"content":"Source video: Ê∑±Â∫¶Â≠¶‰π†-ÂïÉËä±‰π¶0103‰º™ÈÄÜÁü©ÈòµÊúÄÂ∞è‰∫å‰πò (2021-07-22) - ÁßëÁ†îÂ∞èÁÅ∞ÁÅ∞\n(2023-02-04)\nSolve linear regression For a n-dimensional linear regression problem,\nThere are N input samples: x‚ÇÅ,x‚ÇÇ,\u0026hellip;,$x_N$, where each sample is a n-dimension vector x·µ¢‚àà ‚Ñù‚Åø Their corresponding target outputs are: y‚ÇÅ,y‚ÇÇ,\u0026hellip;,$y_N$, where each output is a scalar y·µ¢‚àà ‚Ñù Hence, these data can be represented as a linear system:\n$$ \\begin{cases} x‚ÇÅ‚ÇÅa‚ÇÅ + x‚ÇÅ‚ÇÇa‚ÇÇ + \u0026hellip; + x‚ÇÅ‚Çôa‚Çô = y‚ÇÅ \\\\ x‚ÇÇ‚ÇÅa‚ÇÅ + x‚ÇÇ‚ÇÇa‚ÇÇ + \u0026hellip; + x‚ÇÇ‚Çôa‚Çô = y‚ÇÅ \\\\ \\vdots \\\\ x_{N1}a‚ÇÅ + x_{N2}a‚ÇÇ + \u0026hellip; + x_{Nn}a‚Çô = y_N \\\\ \\end{cases} $$\nFurther, this system can be represented with matrix and vector:\n$$ \\begin{bmatrix} x‚ÇÅ‚ÇÅ \u0026amp; x‚ÇÅ‚ÇÇ \u0026amp; ‚Ä¶ \u0026amp; x‚ÇÅ‚Çô \\\\ x‚ÇÇ‚ÇÅ \u0026amp; x‚ÇÇ‚ÇÇ \u0026amp; ‚Ä¶ \u0026amp; x‚ÇÇ‚Çô \\\\ ‚ãÆ \u0026amp; ‚ãÆ \u0026amp; ‚ã± \u0026amp; ‚ãÆ \\\\ x_{N1} \u0026amp; x_{N2} \u0026amp; ‚Ä¶ \u0026amp; x_{Nn} \\end{bmatrix} \\begin{bmatrix} a‚ÇÅ \\\\ a‚ÇÇ \\\\ ‚ãÆ \\\\ a‚Çô \\end{bmatrix} = \\begin{bmatrix} y‚ÇÅ \\\\ y‚ÇÇ \\\\ ‚ãÆ \\\\ y_N \\end{bmatrix} $$\nThe objective of the linear regression is to solve the weights vector: $[ ^{^{a‚ÇÅ}_{a‚ÇÇ}} _{^{\\ ‚Åû}_{a‚Çô}} ]$ from the linear equation: $ùêó_{N√ón} ùêö_{n√ó1} = ùêò_{N√ó1}$.\nIf N=n (the coefficient matrix is a square matrix), and the data matrix $ùêó_{N√ón}$ is a invertible matrix, then there will be ùêö=ùêó‚Åª¬πùêò, such that the weights vector is determined directly.\nBut in general, the number of samples N is not equal to the number of features n (N ‚â† n), that is ùêó is not invertible and ùêö cannot be represented as ùêó‚Åª¬πùêò.\nShift from ùêó to ùêó·µÄùêó Therefore, when the ùêö cannot be reached directly, the solution should be as close to the optimal as possible.\nThat means the objective is to minimize the distance of two vectors:\nJ=‚Äñùêóùêö-ùêò‚Äñ¬≤ (without constraints)\nAnd the optimal solution is obtained when\n‚àÇJ/‚àÇùêö = ùêó·µÄ(ùêóùêö-ùêò) = 0 ‚áí ùêó·µÄùêóùêö = ùêó·µÄùêò.\nNow, the previous ùêó is shifted to here ùêó·µÄùêó ‚àà ‚Ñù‚Åø·ïÅ‚Åø, which is a square matrix. And if ùêó·µÄùêó is invertible, then the optimal ùêö can be calculated in one-shot.\nIs ùêó·µÄùêó invertible? An invertible matrix has to satisfy 2 conditions: it\u0026rsquo;s a square matrix and its rank equals to the number of variables n (#columns).\nAccording to this video, there are two cases:\nIf N \u0026gt; n, for example N=5, n=3, then (ùêó·µÄùêó)‚ÇÉ‚Çì‚ÇÉ is inverible generally. So\nùêö=(ùêó·µÄùêó)‚Åª¬πùêó·µÄùêò,\nwhere the coefficient in front of ùêò, (ùêó·µÄùêó)‚Åª¬πùêó·µÄ, is called the pseudo-inverse matrix. And ùêö = (ùêó·µÄùêó)‚Åª¬πùêó·µÄùêò is called the least-square solution. (ÊúÄÂ∞è‰∫å‰πòËß£)\n(Because ùêó has no inverse matrix, so we find its \u0026ldquo;pseudo\u0026rdquo; inverse matrix. Or if ùêó is invertible, ùêó‚Åª¬π=(ùêó·µÄùêó)‚Åª¬πùêó·µÄ, they\u0026rsquo;re equivalent, but the latter suits more general scenarios.). If $N \u0026lt; n$, for example N=3, n=5, then (ùêó·µÄùêó)‚ÇÖ‚Çì‚ÇÖ is not invertible, because:\nrank(ùêó·µÄùêó) ‚â§ rank(ùêó‚ÇÉ‚Çì‚ÇÖ) ‚â§ N=3 $\u0026lt;$ n=5.\nIn this case, ùêö cannot be calculated as (ùêó·µÄùêó)‚Åª¬πùêó·µÄùêò.\nThe problem can be understood that there are too many parameters (n is too high). When the parameters are much more than samples, there will be overfitting. And one of the solutions is regularization.\nEffect of the regularization term Since the reason why the optimal solution of the loss function J cannot be solved in one-shot is that there are too many parameters, a regularization is added to the loss funciton as follows:\n$$J = ‚Äñùêóùêö-ùêò‚Äñ¬≤ + Œª‚Äñùêö‚Äñ¬≤, Œª\u0026gt;0$$\nThen the derivative becomes:\n‚àÇJ/‚àÇùêö = ùêó·µÄùêóùêö - ùêó·µÄùêò + Œªùêö = 0.\nBy moving items, the equation becomes:\n(ùêó·µÄùêó + Œªùêà)ùêö = ùêó·µÄùêò,\nwhere the (ùêó·µÄùêó + Œªùêà) is invertible. The proof is as follows.\nProof Since ùêó·µÄùêó is a symmetric matrix, it can be diagonalized. Thus, ùêó·µÄùêó can be written as:\n$$ ùêó·µÄùêó = ùêè‚Åª¬π \\begin{bmatrix} Œª‚ÇÅ \u0026amp; \u0026amp; \\\\ \u0026amp; ‚ã± \u0026amp; \\\\ \u0026amp; \u0026amp; Œª‚Çô \\end{bmatrix} ùêè $$\nThe determinant of ùêó·µÄùêó is: |ùêó·µÄùêó| = |ùêè‚Åª¬π| ‚ãÖ |$^{^{Œª‚ÇÅ}_{\\quad ‚ã±}} _{\\qquad Œª‚Çô}$| ‚ãÖ |ùêè| = Œª‚ÇÅ ‚ãÖ Œª‚ÇÇ ‚Ä¶ ‚ãÖ Œª‚Çô\nAnd Œª‚ÇÅ, Œª‚ÇÇ ‚Ä¶, Œª‚Çô are the eigen values for the ùêó·µÄùêó\nThen the invertibility can be judged from this determinant:\nIf |ùêó·µÄùêó| = 0, then ùêó·µÄùêó is not invertible, because there are some zero lines in the matrix (after elementary row operations), that means rank(ùêó·µÄùêó) \u0026lt; n.\nBut if |ùêó·µÄùêó| \u0026gt; 0, the matrix ùêó·µÄùêó is invertible, because it has full rank, which is equal to the number of lines of rows. „Äê‰øóËØ¥Áü©Èòµ„ÄëË°åÂàóÂºèÁ≠â‰∫é0ÊÑèÂë≥ÁùÄ‰ªÄ‰πàÔºü‰Ω†‰∏ÄÂÆöË¶Å‰∫ÜËß£Âì¶~ - Êôì‰πãËΩ¶È´òÂ±±ËÄÅÂ∏à-bilibili\nAnalyze the two cases without and with adding the regularization term:\nOnly the ùêó·µÄùêó:\nLet this matrix be enclosed by a non-zero real row vector ùõÇ·µÄ and its column vector ùõÇ to constuct a quadratic form: ùõÇ·µÄ(ùêó·µÄùêó)ùõÇ, which is used to characterize the definiteness of ùêó·µÄùêó. Definite matrix -wiki\nBased on the combination law of the matrix multiplication, it can be written as: (ùõÇ·µÄùêó·µÄ)(ùêóùõÇ) = (ùêóùõÇ)·µÄ(ùêóùõÇ), which is the norm of the vector ùêóùõÇ. Because the norm is ‚â• 0 definitely, the ùõÇ·µÄ(ùêó·µÄùêó)ùõÇ ‚â• 0 (indicating ùêó·µÄùêó is positive semi-definite matrix).\nBased on the properties of quadratic form, eigen values Œª·µ¢ of ùêó·µÄùêó are all ‚â• 0\nThen the above determinant is |ùêó·µÄùêó|= Œª‚ÇÅ ‚ãÖ Œª‚ÇÇ ‚Ä¶ ‚ãÖ Œª‚Çô ‚â• 0, so when |ùêó·µÄùêó| = 0, the rank of the matrix ùêó·µÄùêó is not full (‚â† n), so the matrix ùêó·µÄùêó is not invertible.\nFor ùêó·µÄùêó+Œªùêà (where Œª is a hyper-parameter), it can be considered as a matrix:\nA quadratic form is constructed as: ùõÇ·µÄ(ùêó·µÄùêó+Œªùêà)ùõÇ = (ùõÇ·µÄùêó·µÄ)(ùêóùõÇ) + ŒªùõÇ·µÄùõÇ. The second item is always \u0026gt;0 because ùõÇ is not a zero vector, so its norm ùõÇ·µÄùõÇ \u0026gt; 0. Therefore, the matrix (ùêó·µÄùêó+Œªùêà) is a positive definite matrix. The eigen values Œª·µ¢ of (ùêó·µÄùêó+Œªùêà) are all \u0026gt; 0. The determinant of ùêó·µÄùêó+Œªùêà is the product of its eigen values, i.e., the determinant |ùêó·µÄùêó+Œªùêà|\u0026gt;0. So the matrix ùêó·µÄùêó+Œªùêà has full rank, and ùêó·µÄùêó+Œªùêà is invertible.\nTherefore, the optimal solution can be solved as ùêö = (ùêó·µÄùêó + Œªùêà)‚Åª¬π ùêó·µÄùêò. This is called the \u0026ldquo;least square - least norm solution\u0026rdquo;. (ÊúÄÂ∞è‰∫å‰πò-ÊúÄÂ∞èËåÉÊï∞Ëß£)\nL2 regularization is originally added to make the ùêó·µÄùêó invertible.\n(bilibili search: \u0026ldquo;‰º™ÈÄÜÁü©Èòµ\u0026rdquo;)\ntodo: 01 2„ÄÅÊúÄÂ∞è‰∫å‰πò‰∏épcaÔºàÊñ∞Ôºâ - Ê∑±Â∫¶‰πãÁúºÂÆòÊñπË¥¶Âè∑-bilibili\ntodo: „ÄêÁÜüËÇâ„ÄëÁ∫øÊÄß‰ª£Êï∞ÁöÑÊú¨Ë¥® - 06 - ÈÄÜÁü©Èòµ„ÄÅÂàóÁ©∫Èó¥‰∏éÈõ∂Á©∫Èó¥ - 3B1B -bili\ntodo: „Äê26 Ê∑±ÂÖ•ÁêÜËß£ÈÄÜÁü©Èòµ„Äë- cf98982002 -bili\nGPU Solve Inverse Use GPU to solve inverse faster?\n(DDG search: \u0026ldquo;Áü©ÈòµÊ±ÇÈÄÜ gpu\u0026rdquo;)\nThe acceleration ratio of GPU to CPU is more than 16 times. ‚Å¥. GPUÁü©ÈòµËÆ°ÁÆóÊòØÂê¶‰ºöÊõ¥Âø´ÔºüÔºàÂü∫‰∫éPytorchÔºâ - Âçä‰∏™ÂÜØÂçöÂ£´ÁöÑÊñáÁ´† - Áü•‰πé How to solve inverse matrix? Ê±ÇÈÄÜÁü©ÈòµÁöÑ4ÁßçÊñπÊ≥ï? - ÁñØÁãÇÁªÖÂ£´ÁöÑÂõûÁ≠î - Áü•‰πé Gaussian Eliminate LU decomposition, commenly used by computer because it can be performed parallelly. SVD decomposition QR decomposition Estimate Polynomial by LS (2024-06-10)\nLeast-square can be used to approximate the coefficients of a polynomial.\nThe optimal polynomial corresponds to the minimum error between the observed and predicted values.\nExample\nGiven a point set as below, use a linear polynomial to fit the data:\n1 2 x = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] y = [0, 4, 5, 14, 15, 14.5, 14, 12, 10, 5, 4] The general form of a linear polynomial is: ax + b*1 = y, a and b are the coefficients to be estimated:\n$$ \\begin{bmatrix} 0 \u0026amp; 1 \\\\ 0.1 \u0026amp; 1 \\\\ 0.2 \u0026amp; 1 \\\\ 0.3 \u0026amp; 1 \\\\ ‚ãØ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 4 \\\\ 5 \\\\ 14 \\\\ ‚ãØ \\end{bmatrix} $$\n$$XA = Y$$\nObviously, the 2 axes (column space) of X cannot be \u0026ldquo;recombined\u0026rdquo; to produce Y space. In other words, the $[^a_b]$ that makes the equation hold doesn\u0026rsquo;t exist.\nTherefore, the objective is shifted to minimize the error between Y_pred and target Y.\nProject the target Y to the column space of X resulting in Y\u0026rsquo; (that will be approximated by A\u0026rsquo;), and the error: Y\u0026rsquo;-Y is orthogonal to the column space of X, as the error can\u0026rsquo;t be explained by the X\u0026rsquo;s space (Don\u0026rsquo;t know a theoretical explanation yet).\nSince Y\u0026rsquo;-Y is orthogonal to X, there is:\n$$\\begin{aligned} X^T(Y\u0026rsquo;-Y)=0 \\\\ X^T(XA\u0026rsquo;-Y) = 0 \\end{aligned}$$\nThe best coefficients A\u0026rsquo; is:\n$$A\u0026rsquo;=(X^T X)^{-1} X^TY$$\nUse a quadratic polynomial ax¬≤+bx+c=y to fit the data:\n$$ \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0.01 \u0026amp; 0.1 \u0026amp; 1 \\\\ 0.04 \u0026amp; 0.2 \u0026amp; 1 \\\\ 0.09 \u0026amp; 0.3 \u0026amp; 1 \\\\ ‚ãØ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 4 \\\\ 5 \\\\ 14 \\\\ ‚ãØ \\end{bmatrix} $$\nx¬≤ (e.g., 0.01), x (e.g., 0.1), and 1 are the 3 basis functions that are combined with various ratios to form multiple basiss. The optimal coefficients of a polynomial can also be found by solving the equation that the partial derivative of the error w.r.t. each coefficient equals 0.\nDerivation referring to: An As-Short-As-Possible Introduction to the Least Squares, Weighted Least Squares and Moving Least Squares Methods for Scattered Data Approximation and Interpolation (may 2004) - Andy Nealen\nUse a quadratic bivariate polynomial to fit a point set (ùê±·µ¢,y·µ¢)\n$$ \\|Y\u0026rsquo; - Y\\|^2 $$\n$$ Œ± = \\big[ ‚àë_{i=1}^N \\big( b(x·µ¢)‚ãÖ b(x·µ¢)^T \\big) \\big]^{-1} ‚ãÖ ‚àë_{i=1}^N \\big( b(x·µ¢) ‚ãÖ y·µ¢ \\big) $$\nb means a basis (system), i.e., a series of basis functions. Weighted least squares (2024-06-11)\nThe value at each new point is predicted based on an approximated polynomial, which is found by minimizing the difference between the training point and predicted values, and the differences are weighted according to the distance from the new point to each training point. (2024-05-31)\nConsider some samples (errors) are more important, and the importance is determined by the sample\u0026rsquo;s neighbors:\n$$J_{LS} = \\sum_{i=1}^N w·µ¢ (\\^y·µ¢ - y·µ¢)^2$$\nAssume the target function has a \u0026ldquo;Compact Support\u0026rdquo;. Thus, each sample is only affected by its neighbors within the local support. ÁßªÂä®ÊúÄÂ∞è‰∫å‰πòÊ≥ïÂú®ÁÇπ‰∫ëÂπ≥ÊªëÂíåÈáçÈááÊ†∑‰∏≠ÁöÑÂ∫îÁî®-ÂçöÂÆ¢Âõ≠-Ê•∑Âì•\nBy only considering a narrow region, each sample has a variance $œÉ·µ¢¬≤$. And the weights can be the reciprocal of the variance $\\frac{1}{œÉ·µ¢¬≤}$:\nIn other words, WLS takes the correlation of adjacent errors into account. Wikipedia\nOther reasons analysis: Lecture 24-25: Weighted and Generalized Least Squares - CMU (Found in DDG)\nExamples of fitting circle, line, curves with IRLS: ÊúÄÂ∞è‰∫å‰πòÊ≥ïÔºåÂä†ÊùÉÊúÄÂ∞è‰∫å‰πòÊ≥ïÔºåËø≠‰ª£ÈáçÂä†ÊùÉÊúÄÂ∞è‰∫å‰πòÊ≥ï (Âê´‰ª£Á†Å)„ÄêÊúÄÂ∞è‰∫å‰πòÁ∫øÊÄßÊ±ÇËß£„Äë\nMLS for Resampling (2024-06-10)\nThe value of a new point is computed based on the input point set: Example\nGiven a point set: (x‚ÇÅ,y‚ÇÅ), (x‚ÇÇ,y‚ÇÇ), (x‚ÇÉ,y‚ÇÉ), \u0026hellip;($x_N,y_N$);\nPredict the value y\u0026rsquo; of a new point x\u0026rsquo; based on the approximated polynomial, which is found by minimizing the error between the y_pred (evaluated by the approximated polynomial) and the true y values for N points. This minimization process can be performed with a one-shot expression, i.e., pseudo inverse in least squares.\nThe error of each point is scaled by a weight:\n$$\\sum_{i=1}^N w_i (y_i - y\u0026rsquo;)^2$$\nwhere the weight is determined based on the distance from the new point to the training input point:\n$$ w = \\begin{cases} 0, \u0026amp; \\text{dist\u0026lt;0} \\\\ \\frac{2}{3} - 4 dist^2 + 4 dist^3, \u0026amp; \\text{0‚â§dist‚â§0.5} \\\\ \\frac{4}{3} - 4 dist + 4 dist^2 - \\frac{4}{3} dist^3, \u0026amp; \\text{0.5\u0026lt;dist‚â§1} \\\\ \\end{cases} $$\nThe optimal coefficients vector ùõÇ of a certain polynoimal is solved based on a \u0026ldquo;pseudo-inverse form\u0026rdquo; expression: (Derivation)\n$$ Œ± = \\big[ ‚àë_{i=1}^N Œ∏(s) \\big( b(x·µ¢)‚ãÖ b(x·µ¢)^T \\big) \\big]^{-1} ‚ãÖ ‚àë_{i=1}^N \\big( Œ∏(s) b(x·µ¢) ‚ãÖ y·µ¢ \\big) $$\ns is the distance from the new point to the point i. Code for resampling with MLS fitting the input point set with a linear polynomial:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import numpy as np import matplotlib.pyplot as plt def w(dis): dis = dis / 0.3 if dis \u0026lt; 0: return 0 elif dis \u0026lt;= 0.5: return 2/3 - 4 * dis**2 + 4 * dis**3 elif dis \u0026lt;= 1: return 4/3 - 4 * dis + 4 * dis**2 - 4/3 * dis**3 else: return 0 def mls(x_): sumxx = sumx = sumxf = sumf = sumw = 0 # Iterate every training point to calc coeffs for (a, b) in zip(x, y): weight = w(abs(x_ - a)) # dist2weight sumw += weight sumx += a * weight sumxx += a * a * weight sumf += b * weight sumxf += a * b * weight A = np.array([[sumw, sumx], [sumx, sumxx]]) B = np.array([sumf, sumxf]) ans = np.linalg.solve(A, B) # Calculate y-value at the new point return ans[0] + ans[1] * x_ # Input point set for training: x = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] y = [0, 4, 5, 14, 15, 14.5, 14, 12, 10, 5, 4] # Re-Sampling (for plotting the curve): xx = np.arange(0, 1, 0.01) yy = [mls(xi) for xi in xx] fig, ax = plt.subplots(figsize=(5,5),dpi=50,facecolor=\u0026#34;white\u0026#34;) ax.plot(xx, yy) ax.scatter(x, y, c=\u0026#39;r\u0026#39;) plt.show() MLS smoothing (Âπ≥Êªë) means \u0026ldquo;recomputing\u0026rdquo; the y-value at each input (training) point x using the found polynomial.\n1 2 y_smooth = [mls(xi) for xi in x] ax.scatter(x, y_smooth, c=\u0026#39;g\u0026#39;) ","date":"2023-02-04T12:23:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/pseudo-inverse/","title":"memo: Calc | Pseudo-Inverse"},{"content":"Source video: „Äê‰øóËØ¥Áü©Èòµ„ÄëÁü©Èòµ‰πòÊ≥ï‰∏çÁ•ûÁßòÔºåÊè≠ÂºÄÈù¢Á∫±ÂæàÂÆπÊòìÔºÅ\nGiven two linear systems:\n$$ \\begin{array}{cc} \\begin{cases} x‚ÇÅ + 2x‚ÇÇ = y‚ÇÅ \\\\ 3x‚ÇÅ + 4x‚ÇÇ = y‚ÇÇ \\end{cases} \u0026amp; \\begin{cases} 5y‚ÇÅ + 6y‚ÇÇ = 23 \\\\ 7y‚ÇÅ + 8y‚ÇÇ = 31 \\end{cases} \\end{array} $$\nUnlike the previous example, the constant terms of the left system is not fixed, but variables y‚ÇÅ, y‚ÇÇ. And the variables in the right system are y‚ÇÅ, y‚ÇÇ.\nNow, How to solve x‚ÇÅ, x‚ÇÇ based on these 2 systems (compound equation system)?\nThe first step is to eliminate the intermediate variables y‚ÇÅ, y‚ÇÇ. One of methods is substituting the y‚ÇÅ, y‚ÇÇ into the left system, so as to get a compound matrix:\n$$ \\begin{cases} 5(x‚ÇÅ + 2x‚ÇÇ) + 6(3x‚ÇÅ + 4x‚ÇÇ) = 23 \\\\ 7(x‚ÇÅ + 2x‚ÇÇ) + 8(3x‚ÇÅ + 4x‚ÇÇ) = 31 \\end{cases} ‚áí \\begin{cases} 23x‚ÇÅ + 34x‚ÇÇ = 23 \\\\ 31x‚ÇÅ + 46x‚ÇÇ = 31 \\end{cases} $$\nThe above three system of equations can be represented with matrices and vectors:\n$$ \\begin{array}{ccc} \\begin{array}{c} \\{^{x‚ÇÅ + 2x‚ÇÇ = y‚ÇÅ} _{3x‚ÇÅ + 4x‚ÇÇ = y‚ÇÇ} \\\\ ‚áì\\\\ [^{1\\ 2}_{3\\ 4}] [^{x‚ÇÅ}_{x‚ÇÇ}] = [^{y‚ÇÅ}_{y‚ÇÇ}] \\\\ ‚áì\\\\ f([^{x‚ÇÅ}_{x‚ÇÇ}]) = [^{y‚ÇÅ}_{y‚ÇÇ}] \\end{array} \u0026amp; \\begin{array}{c} \\{^{5y‚ÇÅ + 6y‚ÇÇ = 23} _{7y‚ÇÅ + 8y‚ÇÇ = 31} \\\\ ‚áì\\\\ [^{5\\ 6}_{7\\ 8}] [^{y‚ÇÅ}_{y‚ÇÇ}] = [^{23}_{31}] \\\\ ‚áì\\\\ g([^{y‚ÇÅ}_{y‚ÇÇ}]) = [^{23}_{31}] \\end{array} \u0026amp; \\begin{array}{c} \\{^{23x‚ÇÅ + 34x‚ÇÇ = 23} _{31x‚ÇÅ + 46x‚ÇÇ = 31}\\\\ ‚áì\\\\ [^{23\\ 34}_{31\\ 46}] [^{x‚ÇÅ}_{x‚ÇÇ}] = [^{23}_{31}] \\\\ ‚áì\\\\ g(f([^{x‚ÇÅ}_{x‚ÇÇ}])) = g‚àòf([^{x‚ÇÅ}_{x‚ÇÇ}]) = [^{23}_{31}] \\end{array} \\end{array} $$\nIn fact,the coefficient matrix maps the unknowns vector to the outcome vector. And the above 3rd system is a compound mapping from $[^{x‚ÇÅ}_{x‚ÇÇ}]$ to $[^{23}_{31}]$\nTherefore, each coefficient matrix is a mapping function: $[^{1\\ 2}_{3\\ 4}] ‚áî f$; $[^{5\\ 6}_{7\\ 8}]‚áî g$; and $[^{23\\ 34}_{31\\ 46}] ‚áî g‚àòf$\nCompound mapping matrix The relation between the compound mapping and its component mapping matrix.\nThe compound mapping matrix comes from \u0026ldquo;merging\u0026rdquo; two system through multiplication and add as follow:\n$$g‚àòf ‚áî [^{23\\ 34}_{31\\ 46}] = [^{5√ó1+6√ó3 \\quad 5√ó2+6√ó4} _{7√ó1+8√ó3 \\quad 7√ó2+8√ó4} ] = [^{5\\ 6}_{7\\ 8}]‚àò[^{1\\ 2}_{3\\ 4}] $$\nTherefore, this kind of compound operation rules between 2 matrices can be defined as matrix multiplication.\nFor the sake of convenience, the ring operator is omitted, then the matrix multiplication is written as: $[^{5\\ 6}_{7\\ 8}] [^{1\\ 2}_{3\\ 4}]$. Obviously, the result of 2-matrices multiplication is still a matrix.\nLaw of matrix multiplication $$ \\begin{array}{c} _{} \\\\ ^{r_{L1}} _{r_{L2}} [^{5\\ 6} _{7\\ 8} ] \\end{array} \\begin{array}{c} _{c_{L1}}\\ _{c_{L2}} \\\\ [^{1\\ 2} _{3\\ 4} ] \\end{array} \\begin{array}{c} _{}\\\\ = \\end{array} \\begin{array}{c} _{} \\\\ [^{5√ó1+6√ó3 \\quad 5√ó2+6√ó4} _{7√ó1+8√ó3 \\quad 7√ó2+8√ó4} ] \\end{array} $$\nr denotes row; c denotes column; L means left matrix; R means the right matrix. And use $[^{a‚ÇÅ‚ÇÅ\\ a‚ÇÅ‚ÇÇ} _{a‚ÇÇ‚ÇÅ\\ a‚ÇÇ‚ÇÇ} ]$ to represent the 4 elements in the outcome matrix.\na‚ÇÅ‚ÇÅ is the sum of the products of the corresponding elements in the first row in the left matrix and the first column in the right matrix, i.e., the inner product of two vectors: $a‚ÇÅ‚ÇÅ = r_{L1}‚ãÖc_{R1} = (5,6)‚ãÖ(1,3)$\nSimilarly,\n$a‚ÇÅ‚ÇÇ = r_{L1}‚ãÖc_{R2} = (5,6)‚ãÖ(2,4)$; $a‚ÇÇ‚ÇÅ = r_{L2}‚ãÖc_{R1} = (7,8)‚ãÖ(1,3)$; $a‚ÇÇ‚ÇÇ = r_{L2}‚ãÖc_{R2} = (7,8)‚ãÖ(2,4)$; Rule: Take a row vector from the left matrix, take a column vector from the right matrix, and calculate the inner product of them, then the element position is determined based on the index of row and column.\n04:18/08:58\n","date":"2023-02-03T11:00:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/06_%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/","title":"watch: LA - È´òÂ±± 06 | Matrix multiplication"},{"content":"Source video: „Äê‰øóËØ¥Áü©Èòµ„ÄëÁî®Áü©ÈòµÊ±ÇÈùûÈΩêÊ¨°Á∫øÊÄßÊñπÁ®ãÁªÑËΩªËΩªÊùæÊùæÔºåÂá†Ê≠•Â∞±ËÉΩÊêûÂÆöÔºÅ\n3 cases of solutions There are 3 cases of solutions for a non-homogeneous linear system:\nA single unique solution Infinitely many solutions No solution Augmented matrix When performing Gaussian elimination, the equations results will also change. Hence, the coefficient matrix and the \u0026ldquo;results\u0026rdquo; column(s) are composed together forming the augmented matrix. Such that the \u0026ldquo;two matrices\u0026rdquo; will perform the same elementary row operations.\n$$ \\begin{cases} x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ = 5 \\\\ x‚ÇÅ + 6x‚ÇÇ + 7x‚ÇÉ = 9 \\\\ x‚ÇÅ + 10x‚ÇÇ + 6x‚ÇÉ = 8 \\\\ \\end{cases} $$\n$$ \\begin{array}{cc} A = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 6 \u0026amp; 7 \\\\ 1 \u0026amp; 10 \u0026amp; 6 \\end{bmatrix} \u0026amp; A|b = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 1 \u0026amp; 6 \u0026amp; 7 \u0026amp; | 9 \\\\ 1 \u0026amp; 10 \u0026amp; 6 \u0026amp; | 8 \\end{bmatrix} \\end{array} $$\nThe augmented matrix can be leveraged to determine the case of the solution.\nSingle unique solution Performing elementary row operations on the augmented matrix:\nEliminate the unknown x‚ÇÅ in eq. 2 and eq. 3 by performing -r1 + r2 and -r1+r3: $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 0 \u0026amp; 4 \u0026amp; 4 \u0026amp; | 4 \\\\ 0 \u0026amp; 8 \u0026amp; 3 \u0026amp; | 3 \\end{bmatrix} $$\nCancel the common factor 4 for the 2nd line by multiplying the 2nd line with 1/4. $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; | 1 \\\\ 0 \u0026amp; 8 \u0026amp; 3 \u0026amp; | 3 \\end{bmatrix} $$\nEliminate the unknown x‚ÇÇ in eq. 3 by performing -8r2 + r3 : $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; | 1 \\\\ 0 \u0026amp; 0 \u0026amp; -5 \u0026amp; | -5 \\end{bmatrix} $$\nAfter that, the augmented matrix becomes row echelon form.\nNext, the unknowns in the system can be solved from bottom to top by substituting x‚ÇÉ into eq. 2, and then substituting x‚ÇÇ, x‚ÇÉ to eq. 1. $$ \\begin{cases} x‚ÇÅ+\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 5 \\\\ \u0026amp; x‚ÇÇ +\u0026amp; x‚ÇÉ = 1 \\\\ \u0026amp; \u0026amp; x‚ÇÉ = 1 \\\\ \\end{cases} $$\nTherefore, the unique solution for this non-homogeneous linear system is: ùê± = $[^{_{x‚ÇÅ}}_{^{x‚ÇÇ}_{x‚ÇÉ}} ] = [^{_{2}}_{^{0}_{1}} ]$\nAlso, by looking at the coefficient matrix and the augmented matrix after elementary row operations, the number of their non-zeros lines are both 3, which equals to the number of unknowns.\nThus, the conclusion is that if the augmented matrix has the same number of non-zeros lines of the coefficient matrix, where they both are in row echelon form, but also the number of non-zeros lines equals to the number of unknowns, then this non-homogeneous linear system has a single unique solution.\nInfinitely many solutions $$ \\begin{cases} x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ = 5 \\\\ x‚ÇÅ + 3x‚ÇÇ + 4x‚ÇÉ = 6 \\\\ x‚ÇÅ + 4x‚ÇÇ + 5x‚ÇÉ = 7 \\\\ \\end{cases} $$ Performing elementary row operations on the augmented matrix as follows:\nEliminate x‚ÇÅ of row2 and row3 by -r1+r2 and -r1+r3 : $[^{_{1\\ 2\\ 3\\ |5}} _{^{1\\ 3\\ 4\\ |6}_{1\\ 4\\ 5\\ |7}}]$ ‚ûî $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 2\\ 2\\ |2}}]$\nCancel the commen factor of row3 by multiplying with 1/2 : $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 2\\ 2\\ |2}}]$ ‚ûî $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 1\\ 1\\ |1}}]$\nEliminate the unknowns x‚ÇÇ, x‚ÇÉ in row3 by performing -r2+r3: $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 1\\ 1\\ |1}}]$ ‚ûî $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 0\\ 0\\ |0}}]$\nThen the equations become: $$ \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 5 \\\\ \u0026amp; x‚ÇÇ +\u0026amp; x‚ÇÉ = 1 \\\\ \u0026amp; \u0026amp; 0 = 0 \\\\ \\end{cases} $$\nBy representing x‚ÇÅ, x‚ÇÇ with x‚ÇÉ, the solution vector becomes: ùê± = $[^{_{3-x‚ÇÉ}}_{^{1-x‚ÇÉ}_{x‚ÇÉ}} ]$\nThat means x‚ÇÉ can be any value, and x‚ÇÅ and x‚ÇÇ can be any value too. Therefore, this non-homogeneous linear system has infinitely many solutions.\nHere, the number of non-zero lines in coefficient matrix and augmented matrix is less than the number of unknowns: r(ùêÄ|ùêõ) = r(ùêÄ) = 2 \u0026lt; 3\nSo the conclusion is if the augmented matrix and the coefficient matrix in row echelon form have the same number of non-zero rows, which is less than #unknowns, then the non-homogeneous linear system has infinitely many solutions.\nNo solutions $$ \\begin{cases} x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ = 5 \\\\ x‚ÇÅ + 3x‚ÇÇ + 4x‚ÇÉ = 6 \\\\ x‚ÇÅ + 4x‚ÇÇ + 5x‚ÇÉ = 9 \\\\ \\end{cases} $$\n$[^{_{1\\ 2\\ 3\\ |5}} _{^{1\\ 3\\ 4\\ |6}_{1\\ 4\\ 5\\ |9}}]$ $^{-r1+r2}_{-r1+r3}$ ‚ûî $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 2\\ 2\\ |4}}]$ ¬Ω√ór3 ‚ûî $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 1\\ 1\\ |2}}]$ -r2+r3 ‚ûî $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 0\\ 0\\ |1}}]$\nThe corresponding equations are: $$ \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 5 \\\\ \u0026amp; x‚ÇÇ +\u0026amp; 4x‚ÇÉ = 6 \\\\ \u0026amp; \u0026amp; 0 = 1 \\\\ \\end{cases} $$\nThe 3rd equation is a fault. That means no matther what values the unknowns are, this linear equation system cannot be satisfied. Hence, there is no solution.\nIn this situation, the number of non-zero rows in the coefficient matrix is not equal to the augmented matrix: r(ùêÄ|ùêõ) = 3 ‚â† r(ùêÄ) = 2.\nThe conclusion is if the number of non-zero lines in the augmented matrix mismatch with the coefficient matrix in row echelon form, then the non-homogeneous linear equation system has no solution.\nGeneral solution of infinitely many solution $$ \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 5 \\\\ \u0026amp; x‚ÇÇ +\u0026amp; x‚ÇÉ = 1 \\\\ \u0026amp; \u0026amp; 0 = 0 \\\\ \\end{cases} $$\nIts general solution being represented with x‚ÇÉ is ùê± = $[^{_{3-x‚ÇÉ}}_{^{1-x‚ÇÉ}_{x‚ÇÉ}} ]$\nTo structure the solution, the constant terms are separated out and x‚ÇÉ is replaced by k.\nùê± = $[^{_{3-x‚ÇÉ}}_{^{1-x‚ÇÉ}_{x‚ÇÉ}} ]$ = x‚ÇÉ$[^{_{-1}}_{^{-1}_{1}} ] + [^{_{3}}_{^{1}_{0}} ]$ = k$[^{_{-1}}_{^{-1}_{1}} ] + [^{_{3}}_{^{1}_{0}} ]$\nThis general solution consistitute two components: random number times a column vector k$[^{_{-1}}_{^{-1}_{1}} ]$ and a constant vector.\nIf the $k [^{_{-1}}_{^{-1}_{1}} ]$ is substituted into equation system individually, the results of all equations are 0s.\n$$ \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 0 \\\\ \u0026amp; x‚ÇÇ +\u0026amp; x‚ÇÉ = 0 \\\\ \u0026amp; \u0026amp; 0 = 0 \\\\ \\end{cases} $$\nSo this term is the general solution for the homogeneous linear equations system.\nAnd the constant vector can just make the system satisfied. Hence, it\u0026rsquo;s the specific solution of the non-homogeneous linear equation system.\nTherefore, the general solution for a non-homogeneous linear equation system is made up by two parts: the general solution of the corresponding homogeneous linear equation system and the specific solution of the non-homogeneous linear equation system.\nThe method to find the general solution of the corresponding homogeneous linear equation system has been introduced in the last video.\nFind the specific solution Based on the augmented matrix in row echelon form, the leading variables are x‚ÇÅ and x‚ÇÇ, while x3 is the free variable.\nTheoratically, any one of the solution satifying the non-homogeneous linear equation system is a specific solution. So, the free varaible can be assigned with any value. And in practice, free variables are all set to 0 for the purpose of solving convinently.\nIf Letting x3 = 0, then x2=1, x1 =3. Thus, the specific vector ùõà = $[^{_{3}} _{^{1}}_{0}]$\nSteps of solving Write the augmented matrix ùêÄ|ùêõ based on the equation system and determine the number of unknowns n.\n$$ \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ +\u0026amp; x‚ÇÑ \u0026amp;= 3 \\\\ x‚ÇÅ -\u0026amp; 4x‚ÇÇ -\u0026amp; x‚ÇÉ -\u0026amp; x‚ÇÑ \u0026amp;= 1 \\\\ 2x‚ÇÅ +\u0026amp; x‚ÇÇ +\u0026amp; 4x‚ÇÉ +\u0026amp; x‚ÇÑ \u0026amp;= 5 \\\\ x‚ÇÅ -\u0026amp; x‚ÇÇ +\u0026amp; x‚ÇÉ \u0026amp; \u0026amp;= 2 \\\\ \\end{cases} $$\nùêÄ|ùêõ = $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 1 \u0026amp; | 3 \\\\ 1 \u0026amp; -4 \u0026amp; -1 \u0026amp; -1 \u0026amp; | 1 \\\\ 2 \u0026amp; 1 \u0026amp; 4 \u0026amp; 1 \u0026amp; | 5 \\\\ 1 \u0026amp; -1 \u0026amp; 1 \u0026amp; 0 \u0026amp; | 2 \\end{bmatrix} $$ , n = 4\nTransform the augmented matrix to row echelon form by performing elementray row operations. $[^{^{1\\ 2\\ 3\\ 1\\ | 3}_{1\\ -4\\ -1\\ -1\\ |1}} _{^{2\\ 1\\ 4\\ 1\\ | 5}_{1\\ -1\\ 1\\ 0\\ | 2}}]$ $^{_{-(-r1+r2)}} _{^{-(-2r1+r3)} _{-(-r1+r4)}}$ ‚ûî $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 6\\ 4\\ 2\\ |2}} _{^{0\\ 3\\ 2\\ 1\\ | 1}_{0\\ 3\\ 2\\ 1\\ | 1}}]$ ¬Ωr2 ‚ûî $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 3\\ 2\\ 1\\ |1}} _{^{0\\ 3\\ 2\\ 1\\ | 1}_{0\\ 3\\ 2\\ 1\\ | 1}}]$ $^{-(-r2+r3)} _{-(-r2+r4)}$ ‚ûî $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 3\\ 2\\ 1\\ |1}} _{^{0\\ 0\\ 0\\ 0\\ | 0}_{0\\ 0\\ 0\\ 0\\ | 0}}]$\nCheck the number of non-zero rows r(ùêÄ|ùêõ) of the obtained simplified augmented matrix and r(ùêÄ) for the coefficient matrix. r(ùêÄ|ùêõ) = r(ùêÄ) = 2 \u0026lt; 3\nDetermine the case of the situations:\nr(ùêÄ|ùêõ) = r(ùêÄ) = n, single unique solution r(ùêÄ|ùêõ) = r(ùêÄ) \u0026lt; n, inifinitely many solutions r(ùêÄ|ùêõ) ‚â† r(ùêÄ), no solution Because r(ùêÄ|ùêõ) = r(ùêÄ) = 2 \u0026lt; 4, this non-homogeneous equation system has infinitely many solutions.\nFor the case of infinitely many solutions, determine the leading variables and free variables first, and solve the fundamental system of solution, then make up the general solution for the non-homogeneous equation system: General solution of the corresponding homogeneous equation system\nSpecific solution of the non-homogeneous equation system The leading variables are x1 and x2, while x3 and x4 are free variables.\nSolve the general solution of the homogeneous equation system by letting the result of all equations be 0. $$ \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ +\u0026amp; x‚ÇÑ \u0026amp;= 0 \\\\ \u0026amp; 3x‚ÇÇ +\u0026amp; 2x‚ÇÉ +\u0026amp; x‚ÇÑ \u0026amp;= 0 \\\\ 0 \u0026amp;= 0 \\\\ 0 \u0026amp;= 0 \\\\ \\end{cases} $$\nTransform the coefficient matrix to row echelon form $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 3\\ 2\\ 1\\ |1}} _{^{0\\ 0\\ 0\\ 0\\ | 0}_{0\\ 0\\ 0\\ 0\\ | 0}}]$ Determine the leading variables and free variables. By assigning values to free variables orthogonally, the general solution for homogeneous equation system can be constructed. ‚Ä¢ Take x3,x4 as 0,1, then x2=-1/3, x1=-1/3; ‚Ä¢ Take x3,x4 as 1,0, then x2=-2/3, x1=-5/3; ‚Ä¢ The fundamental system of solution constitute $[^{^1_1}_{^0_{-3}}]$ and $[^{^5_2}_{^{-3}_0}]$ ‚Ä¢ General solution for homogeneous equation system: ùê±\u0026rsquo; = k‚ÇÅŒæ‚ÇÅ +k‚ÇÇŒæ‚ÇÇ = k‚ÇÅ$[^{^1_1}_{^0_{-3}}]$ + k‚ÇÇ $[^{^5_2}_{^{-3}_0}]$ Find the specific solution of the non-homogeneous equation system by letting all free variables be 0 and solving leading variables from bottom to top: Take x3,x4 as 0, then x2=1/3, x1=7/3. So the specific solution is ùõà=$[^{^{7/3}_{1/3}}_{^0_0}]$\nAdd the above two parts together to get the final general solution of the non-homogeneous equation system. ùê± = ùê±\u0026rsquo; + ùõà = k‚ÇÅ$[^{^1_1}_{^0_{-3}}]$ + k‚ÇÇ $[^{^5_2}_{^{-3}_0}]$ + $[^{^{7/3}_{1/3}}_{^0_0}]$\n","date":"2023-02-03T10:39:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/03_%E8%A7%A3%E9%9D%9E%E9%BD%90%E6%AC%A1%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/","title":"watch: LA - È´òÂ±± 03 | Solve Non-homogeneous Linear System"},{"content":"Source videoÔºö„Äê‰øóËØ¥Áü©Èòµ„Äë Áî®Áü©ÈòµËß£ÈΩêÊ¨°Á∫øÊÄßÊñπÁ®ãÁªÑËøôË¶ÅËøô‰πàÂá†Ê≠•Â∞±ÊêûÂÆö‰∫ÜÔºÅÁúüÁöÑÂ•ΩÁÆÄÂçï - Êôì‰πãËΩ¶È´òÂ±±ËÄÅÂ∏à - bilibili\nThere are 2 cases of solutions for homogeneous linear system:\nHave and only have the zero solution Exist non-zero solution besides the zero solution How to judge the case of the solution? Given a homogeneous linear system in 3 variables:\n$$ \\begin{cases} x‚ÇÅ + \u0026amp; 2x‚ÇÇ + \u0026amp;3x‚ÇÉ = 0 \\\\ \u0026amp; 4x‚ÇÇ + \u0026amp; 5x‚ÇÉ = 0 \\\\ \u0026amp; \u0026amp; x‚ÇÉ = 0 \\end{cases} $$\nSubstitute the bottom variable to the topper equations to solve other variables. And the solved result is a zero vector: $ùê± = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$\nThe coefficient matrix is in row echelon form resulting from a Gaussian elimination.\n$$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3\\\\ 0 \u0026amp; 4 \u0026amp; 5\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nIn the above matrix, the first elements next to the right side of the steps line are all non-zero elements, while on the left-hand side of the steps, the elements are all 0.\nAll three rows are not all zeros. That means the number of rows that are not 0 is 3, which is the same as the number of the unknowns. Hence, only if the number of non-zero rows in the echelon-form coefficient matrix is equal to the number of unknowns, the homogeneous linear system has only the zero solution. case1: only zero solution Given the following homogeneous linear system: $$ \\begin{cases} x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ = 0 \\\\ x‚ÇÅ + 4x‚ÇÇ + 5x‚ÇÉ = 0 \\\\ x‚ÇÅ + 10x‚ÇÇ + x‚ÇÉ = 0 \\end{cases} $$\nIts coefficient matrix ($[^{_{1\\ 2\\ 3}}_{^{1\\ 6\\ 8}_{1\\ 10\\ 6}}]$) is not in row echelon form. But this linear system can be simplized through the (row) Gaussian elimination, i.e., the elementary row operations applied on its coefficients matrix.\nWith the row echelon form as the objective, the elementary row operations are operated on the coefficients matrix.\nUse row-1 to cancel the first element (1) in the row2 and row3 by multiplying the 1st row with -1 and adding it to the 2nd and 3rd rows. $[^{_{1\\ 2\\ 3}}_{^{1\\ 6\\ 8}_{1\\ 10\\ 6}}]$ ‚ûî $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 3}}]$\nUse row-2 to cancel the second element (8) in the 3rd row by multiplying the 2nd row with -2 and adding it to the 3rd rows. $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 3}}]$ ‚ûî $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ -7}}]$\nMultiply the 3rd row with -1/7. $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ -7}}] ‚ûî [^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ 1}}]$\nConclusion: Perform the elementary row operations on the coefficient matrix to transform it as in the row echelon form. Then if the number of non-zero rows is equal to the number of unknowns, the homogeneous linear system only has zero solution.\ncase2: infinitely many solution A homogeneous linear system is as follow: $$ \\begin{cases} x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ = 0 \\\\ x‚ÇÅ + 6x‚ÇÇ + 8x‚ÇÉ = 0 \\\\ x‚ÇÅ + 10x‚ÇÇ + 14x‚ÇÉ = 0 \\end{cases} $$\nEliminate the elements from bottom to top and from left to righ:\nUse row-1 to cancel the first element of the 2nd and 3rd rows by \u0026ldquo;-r1+r2\u0026rdquo; and \u0026ldquo;-r1+r3\u0026rdquo;. $[^{_{1\\ 2\\ 3}}_{^{1\\ 6\\ 8}_{1\\ 10\\ 13}}]$ ‚ûî $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 10}}]$\nUse row-2 to cancel the second element of the 3rd row by \u0026ldquo;-2r2+r3\u0026rdquo;. $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 10}}] ‚ûî [^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ 0}}]$\nThe bottom row of this echelon-form matrix is all 0. Hence, there are only 2 independent equations essiensially: $$ \\begin{cases} x‚ÇÅ + \u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 0 \\\\ \u0026amp; 6x‚ÇÇ +\u0026amp; 8x‚ÇÉ = 0 \\\\ \u0026amp; \u0026amp; 0 = 0 \\end{cases} $$\nIf the number of non-zero rows (2) is less than the number of unknowns (3), then there exists non-zero solutions besides the zero solution.\nAnd we can give a general solution for this system.\nWhat is general solution? Suppose the homogeneous linear system is: $$ \\begin{cases} x‚ÇÅ+x‚ÇÇ=0\\\\ 2x‚ÇÅ+2x‚ÇÇ=0 \\end{cases} $$\nAs long as the 2 variables have opposite signs: $\\rm \\{^{x‚ÇÅ=0}_{x‚ÇÇ=0}\\ or\\ \\{^{x‚ÇÅ=1}_{x‚ÇÇ=-1}\\ or\\ \\{^{x‚ÇÅ=2}_{x‚ÇÇ=-2}\\ or \u0026hellip; $,\nthe system is satisfied.\nSo the solutions has a general form: $\\{^{x‚ÇÅ=k}_{x‚ÇÇ=-k}$\nWrite the solution as a column vector: $ùê± = [^{x‚ÇÅ}_{x‚ÇÇ} ] = k [^{\\ 1}_{-1}]$\nHow to find the general solution? How to solve the general solution when there are infinitely many solutions?\n$$ \\begin{cases} x‚ÇÅ + \u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 0 \\\\ \u0026amp; 6x‚ÇÇ +\u0026amp; 8x‚ÇÉ = 0 \\\\ \u0026amp; \u0026amp; 0 = 0 \\end{cases} $$\nSimilarly, solve the variables from bottom to top. Fix the x‚ÇÉ, then x‚ÇÅ and x‚ÇÇ can be represented with x‚ÇÉ. Thus the solution is written as: $\\begin{bmatrix} -1/2‚ãÖx‚ÇÉ\\\\ -5/4‚ãÖx‚ÇÉ\\\\ x‚ÇÉ \\end{bmatrix} = x‚ÇÉ‚ãÖ\\begin{bmatrix}-1/2\\\\ -5/4\\\\ 1 \\end{bmatrix}$\nHere, any x‚ÇÉ can be the solution of this system. That purely numerical column vector constitutes the fundamental system of solutions(Âü∫Á°ÄËß£Á≥ª) for this homogeneous linear system.\nThe fundamental system can be scaled by any factor. For example, let x‚ÇÉ=-4k, it becomes a scaled fundamental system. Then the solution $k \\begin{bmatrix}2\\\\ 5\\\\ -4 \\end{bmatrix}$ containing only integars is the general solution. 2 free variables Given a homogeneous linear system: $$ \\begin{cases} x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ = 0 \\\\ 2x‚ÇÅ + 4x‚ÇÇ + 6x‚ÇÉ = 0 \\\\ 3x‚ÇÅ + 6x‚ÇÇ + 9x‚ÇÉ = 0 \\end{cases} $$\nPerform the elementary row operations for its coefficient matrix: $[^{_{1\\ 2\\ 3}} _{^{2\\ 4\\ 6}_{3\\ 6\\ 9}} ]$ by \u0026ldquo;-2r1+r2\u0026rdquo; and \u0026ldquo;-3r1+r3\u0026rdquo;. The echelon-form matrix is $[^{_{1\\ 2\\ 3}} _{^{0\\ 0\\ 0}_{0\\ 0\\ 0}} ]$, where the 2nd and 3rd rows are all 0. That means only the 1st equation is valid. $\\{^{x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ = 0}_{^{0=0}_{0=0}}$\nTherefore, only if the x‚ÇÇ and x‚ÇÉ both are set, then x‚ÇÅ can be determined.\nIn other words, x‚ÇÅ has to be represented by x‚ÇÇ and x3 collectively: $\\begin{bmatrix} -2x‚ÇÇ-3x‚ÇÉ\\\\ x‚ÇÇ\\\\ x‚ÇÉ\\end{bmatrix}$\nTo clarify the representation, it can be broken down to separate different variables:\n$$ [^{_{-2x‚ÇÇ-3x‚ÇÉ}} _{^{x‚ÇÇ}_{x‚ÇÉ}} ] = [^{_{-2x‚ÇÇ}} _{^{x‚ÇÇ}_{0}} ] + [^{_{-3x‚ÇÉ}} _{^{0}_{x‚ÇÉ}} ] = x‚ÇÇ[^{_{-2}} _{^{1}_{0}} ] + x‚ÇÉ[^{_{-3}} _{^{0}_{1}} ] $$\nThen the general solution is a linear combination of two numerical column vectors with x‚ÇÇ,x‚ÇÉ as their coefficients. And the x‚ÇÅ in these 2 vector (-2 and -3) are the solution when x‚ÇÇ,x‚ÇÉ are the corresponding values.\nThat means when x‚ÇÇ,x‚ÇÉ are assigned (orthogonally) repeatedly with (1,0) and (0,1), two x‚ÇÅ can be determined and then 2 numerical column vectors are generated, which constitute the fundamental system of solutions for this linear system.\nBy substituting x‚ÇÇ,x‚ÇÉ with constants k‚ÇÅ,k‚ÇÇ, the genearl solution is the linear combination of all the numerical column vector in the fundamental system: $$ k‚ÇÅ\\begin{bmatrix} -2\\\\ 1\\\\ 0\\end{bmatrix} + k‚ÇÇ\\begin{bmatrix} -3\\\\ 0\\\\ 0\\end{bmatrix} $$\nComparing the above two general solutions: $k [^2_{^5_{-4}} ]$ and $k‚ÇÅ[^{_{-2}} _{^{1}_{0}} ] + k‚ÇÇ[^{_{-3}} _{^{0}_{1}} ]$, there is only 1 vector in the fundamental system of solutions for the former system, and there are 2 vectors in the fundamental system of solutions for the latter system. Obviously, this is related to the number of non-zero rows in the matrix in the row-echelon form.\nLeading variables \u0026amp; free variables Commonly, for a matrix in the row-echelon form, the first non-zero element next to the steps line corresponds to a leading variable. And also in the simplified system of equations, the leading variables are embodied as the coefficient of the first variable is not zero. The variables exposed at the start of each line are leading variables; And the other variables are called free variables.\n$$ \\begin{array}{cc} \\begin{array}{c} \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 0\\\\ \u0026amp; 4x‚ÇÇ +\u0026amp; 5x‚ÇÉ=0\\\\ \u0026amp; \u0026amp; 0=0 \\end{cases} \\\\ ‚áì\\\\ [^{1\\ 2\\ 3} _{^{0\\ 4\\ 5}_{0\\ 0\\ 0}} ] \\end{array} \u0026amp; \\begin{array}{c} \\begin{cases} x‚ÇÅ +\u0026amp; 2x‚ÇÇ +\u0026amp; 3x‚ÇÉ = 0\\\\ \u0026amp; \u0026amp; 0=0\\\\ \u0026amp; \u0026amp; 0=0 \\end{cases} \\\\ ‚áì\\\\ [^{1\\ 2\\ 3} _{^{0\\ 0\\ 0}_{0\\ 0\\ 0}} ] \\end{array} \\end{array} $$\nIn the above left system, x‚ÇÅ and x‚ÇÇ are leading variables. While in the right system, only x‚ÇÅ is the leading variable.\nleading variables are variables whose column contains the leading entry of some row; And free variables are all the other variables. MST10030 notes wiki\nA few related conclusions:\nThe number of leading variables is equal to the number of non-zero rows in the matrix in the row echelon form, which is written as r(ùêÄ)\nIf there are total n unknowns, then the number of the free variables is t = n - r(ùêÄ).\nAll free variables need to be assigned orthogonally.\nIf there is only 1 free variable, then just assign it with 1 and calculate the other values (leading variables) in the numerical column vector constituting the fundamental system of solutions. If there are 2 free variables, they can be assigned with (1,0) and (0,1) twice separately\u0026hellip;. i.e., the number of free variables is the number of orthogonal assignmens required separately. The number of free variables is the number of vectors constituting the fundamental system of solutions.\nTo calculate the fundamental system of solutions, the free variables have to be assigned first, and then solve the other leading variables from bottom to top.\nComplete solution steps The steps of solving homogeneous linear system based on the coefficient matrix.\n$$ \\begin{cases} x‚ÇÅ + 2x‚ÇÇ + 3x‚ÇÉ + x‚ÇÑ = 0 \\\\ x‚ÇÅ - 4x‚ÇÇ - x‚ÇÉ - x‚ÇÑ = 0 \\\\ 2x‚ÇÅ + x‚ÇÇ + 4x‚ÇÉ + x‚ÇÑ = 0 \\\\ x‚ÇÅ - x‚ÇÇ + x‚ÇÉ = 0 \\end{cases} $$\nWrite down the coefficient matrix ùêÄ and the number of unknowns n=4 based on the given equations system. ùêÄ = $[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}_{1\\ -4\\ -1\\ -1}} _{^{2\\ \\ 1\\ \\ 4\\ 1}_{1\\ -1\\ 1\\ 0}} ]$\nPerform the elementary on the coefficient matrix ùêÄ = $[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}_{1\\ -4\\ -1\\ -1}} _{^{2\\ \\ 1\\ \\ 4\\ 1}_{1\\ -1\\ 1\\ 0}} ]$ to reach the matrix in the row-echelon form.\nUse row-1 to cancel the first element in 2nd, 3rd, and 4th rows by \u0026ldquo;-r1+r2\u0026rdquo;, \u0026ldquo;-2r1+r3\u0026rdquo;, \u0026ldquo;-r1+r4\u0026rdquo;:\nùêÄ = $[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}_{0\\ -6\\ -4\\ -2}} _{^{0\\ -3\\ -2\\ -1}_{0\\ -3\\ -2\\ -1}} ]$ Multiply 2nd, 3rd, and 4th rows with some numbers to make common factor (\u0026quot;-0.5r2\u0026quot;, \u0026ldquo;-r3\u0026rdquo;, \u0026ldquo;-r4\u0026rdquo;)\nùêÄ = $[^{^{1\\ 2\\ 3\\ 1}_{0\\ 3\\ 2\\ 1}} _{^{0\\ 3\\ 2\\ 1}_{0\\ 3\\ 2\\ 1}} ]$ Use row-2 to cancel the 3rd and 4th rows by \u0026ldquo;-r2+r3\u0026rdquo;, \u0026ldquo;-r2+r4\u0026rdquo;: ùêÄ = $[^{^{1\\ 2\\ 3\\ 1}_{0\\ 3\\ 2\\ 1}} _{^{0\\ 0\\ 0\\ 0}_{0\\ 0\\ 0\\ 0}} ]$ Check the number of non-zero rows in this echelon-form matrix, r(ùêÄ)=2\nIf r(ùêÄ) = n, then this system only has zero solution.\nOtherwise, if r(ùêÄ) \u0026lt; n, this system exists non-zero solution. Find out the leading varibles based on the position of the down edges of the steps line: x‚ÇÅ, x‚ÇÇ. So the free variables are x‚ÇÉ and x‚ÇÑ. Then x‚ÇÉ,x‚ÇÑ are assigned with (1,0) and (0,1) repeatedly to get the 2 (=n-r(ùêÄ)) numerical column vector constituting the fundamental system of solutions.\nWhen $\\{^{x‚ÇÉ=1}_{x‚ÇÑ=0}$, the leading variables are $\\{^{x‚ÇÅ=-5/3}_{x‚ÇÇ=-2/3}$. Thus, one of the vectors in the fundamental system of solutions is $Œæ‚ÇÅ = [^{^{5}_{2}} _{^{-3}_{0}} ]$ When $\\{^{x‚ÇÉ=0}_{x‚ÇÑ=1}$, the leading variables are $\\{^{x‚ÇÅ=-1/3}_{x‚ÇÇ=-1/3}$. Thus, one of the vectors in the fundamental system of solutions is $Œæ‚ÇÇ = [^{^{1}_{1}} _{^{0}_{-3}} ]$ Combine linearly those two numerical column vectors by constants to get the general solution:\nùê± = k‚ÇÅŒæ‚ÇÅ + k‚ÇÇŒæ‚ÇÇ = $k‚ÇÅ[^{^{5}_{2}} _{^{-3}_{0}} ] + k‚ÇÇ[^{^{1}_{1}} _{^{0}_{-3}}]$\n","date":"2023-02-01T20:21:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/02_%E8%A7%A3%E9%BD%90%E6%AC%A1%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/","title":"watch: LA - È´òÂ±± 02 | Solve Homogeneous Linear System"},{"content":"Source video: „Äê‰øóËØ¥Áü©Èòµ„ÄëÂàùÁ≠âÁü©ÈòµÂèØ‰∏ÄÁÇπ‰πü‰∏çÂàùÁ≠âÔºåÂÆÉÂ±ÖÁÑ∂ÊúâËøô‰πàÈáçË¶ÅÁöÑÊÑè‰πâÔºÅ\n3 elementary row operations Êç¢Ë°åÔºö‰∫§Êç¢‰∏§Ë°åÁöÑ‰ΩçÁΩÆ Êï∞‰πòÔºöÁªôÊüê‰∏ÄË°å‰πò‰ª•ÈùûÈõ∂Â∏∏Êï∞k ÂÄçÂä†ÔºöÂ∞ÜÊüê‰∏ÄË°åÁöÑ k ÂÄçÂä†Âà∞Âè¶‰∏ÄË°å‰∏ä By means of these 3 kind of elementary row operations, a matrix can be transformed to row echelon form, which is useful in analysing and solving the system of linear equations.\nIdentical matrix $$ I = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThe elements on the main diagonal are all 1 and the rest of elements in the martrix are all 0.\nIf multiplying a matrix with a identical matrix, the matrix doesn\u0026rsquo;t change. So the effect of I is similar to the 1 in numbers.\nA identical matrix can perform elementary row operations.\nElementary matrix ÂØπÂçï‰ΩçÁü©Èòµ I ÂÆûÊñΩ‰∏ÄÊ¨°ÂàùÁ≠âË°åÂèòÊç¢„ÄÇÂÆûÊñΩ2Ê¨°Â∞±‰∏çÊòØ‰∫Ü„ÄÇwikipedia\n$$ I = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} ‚ûî \\begin{cases} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \u0026amp; \\text{Row switch: r1 ‚ü∑ r2} \\\\ \\\\ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \u0026amp; \\text{Row multiplication: 3r1 ‚ûî r1} \\\\ \\\\ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 3 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \u0026amp; \\text{Row addition: 3r1+r2 ‚ûî r2} \\end{cases} $$\nÂàùÁ≠âÁü©ÈòµÂè™Êúâ3ÁßçÔºöÁΩÆÊç¢ÈòµÔºåÊï∞‰πòÈòµÔºåÂÄçÂä†Èòµ\nElementary matrices connect the matrix multiplication and elementary row operations.\nRow-switching transformations Row-multiplying transformations Row-addition transformations ÂØπÁü©ÈòµÂÆûÊñΩ‰∏ÄÊ¨°ÂàùÁ≠âË°åÂèòÊç¢ Â∑¶‰πò‰∏Ä‰∏™ÂàùÁ≠âÁü©ÈòµÔºåÂ∞±ÊòØÊääÂØπÂàùÁ≠âÁü©ÈòµÁöÑÂàùÁ≠âË°åÂèòÊç¢ÔºåÊñΩÂä†Âà∞Áü©Èòµ‰∏ä„ÄÇ\n","date":"2023-02-01T12:41:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/10_%E5%88%9D%E7%AD%89%E7%9F%A9%E9%98%B5/","title":"watch: LA - È´òÂ±± 10 | Elementary Matrix"},{"content":"Original VideoÔºö„Äê‰øóËØ¥Áü©Èòµ„ÄëÁü©ÈòµÂéüÊù•Ëøô‰πàÁÆÄÂçïÔºÅ‰ªé‰∫åÂÖÉ‰∏ÄÊ¨°ÊñπÁ®ãÁªÑÂºÄÂßãÊïô‰Ω†~ - Êôì‰πãËΩ¶È´òÂ±±ËÄÅÂ∏à - bilibili\nLinear equations system Linear equation in 2 variables (‰∫åÂÖÉ‰∏ÄÊ¨°ÊñπÁ®ã): An equation has 2 unknown variables with an exponent of 1.\nTwo linear equations in 2 variables can make up a system of linear equations (Á∫øÊÄßÊñπÁ®ãÁªÑ). $$ \\begin{cases} x + y = 3 \\\\ 2x + 3y = 8 \\end{cases} $$\nGaussian elimination Two kinds of Gaussian elimination to solve this linear system:\nReformalize the 1st equation and substitute it into the 2nd one, such that it comes down to a linear equation in 1 variable. (‰ª£ÂÖ•Ê∂àÂÖÉÊ≥ï) $\\begin{cases}y=3-x\\\\ 2x+3y=8 \\end{cases}$ ‚ûî 2x+3(3-x)=8 ‚ûî $\\{^{x=1}_{y=2}$\nMultiply the equation with a certain number and add the 2 equations together to cancel a variable, such that only 1 linear equation is left. (Âä†ÂáèÊ∂àÂÖÉÊ≥ï) $\\begin{cases}-2x-2y=-6\\\\ 2x+3y=8 \\end{cases}$ ‚ûî y=2 ‚ûî $\\{^{x=1}_{y=2}$\nGeometric interpretation A linear equation in 2 variables is a line on a plane. The solution of the linear system is the intersection of these lines.\nTwo kinds of linear system:\nHomogeneous system: all of the constant terms are zeroes. $\\begin{cases} x+y = 0\\\\ 2x+3y = 0 \\end{cases}$\nNonhomogeneous system: The constant term is not all zero. $\\begin{cases} x+y = 3\\\\ 2x+3y = 8 \\end{cases}$\nSolution of Homogeneous system has 2 cases The lines are all passing through the origin.\nIf the slopes are different ($\\{^{x+y=0}_{2x+3y=0}$ ‚ûî x=y=0), the only intersection of them is the origin. This means the homogenous system has only zero solution. If the slopes of them are identical ($\\{^{x+y=0}_{2x+2y=0}$ ‚ûî x=-y), the lines are overlapping. The solution set is a line: any point on the line is a solution of this system. That means there are infinitely many non-zero solutions besides the zero solution. There exists non-zero solution in this system. Solution of Non-homogeneous system has 3 cases Lines have distinct slopes ($\\{^{x+y=3}_{2x+3y=8}$), so there is only one intersection corresponding to the single unique solution.\nLines are overlapping ($\\{^{x+y=3}_{2x+2y=6}$). Any point on the line is a solution of this system. The system has infinitely many solutions.\nLines are parallel without any intersection ($\\{^{x+y=3}_{2x+2y=4}$), this system has no solution. (Its solution must not be zero.)\nTherefore, the assertion \u0026ldquo;The number of unknowns is the number of equations needed.\u0026rdquo; works only for the situation of \u0026ldquo;only zero solution\u0026rdquo; in a homogeneous system and \u0026ldquo;single unique solution\u0026rdquo; in a non-homogeneous system.\nIn the scenario with more variables, or the numbers of unknowns and equations are not equal, even if the gaussian elimination can be applied, the correct steps or the direction of elimination are hard to determine.\nHow to solve a linear system? A general homogeneous system is a combination of m linear equations in n unknowns.\n$$ \\begin{cases} a‚ÇÅ‚ÇÅx‚ÇÅ + a‚ÇÅ‚ÇÇx‚ÇÇ + \u0026hellip; + a‚ÇÅ‚Çôx‚Çô = 0 \\\\ a‚ÇÇ‚ÇÅx‚ÇÅ + a‚ÇÇ‚ÇÇx‚ÇÇ + \u0026hellip; + a‚ÇÇ‚Çôx‚Çô = 0 \\\\ \\dots \\\\ a‚Çò‚ÇÅx‚ÇÅ + a‚Çò‚ÇÇx‚ÇÇ + \u0026hellip; +a‚Çò‚Çôx‚Çô = 0 \\end{cases} $$\nwhere\nx‚ÇÅ, x‚ÇÇ, \u0026hellip;,x‚Çô are the n unkowns,\nThe coefficient a‚Çò‚Çô, the first subscript represents the ordinal number of the equation, and the second subscript stands for the unknown that this coefficient multiplies with.\nFor example a‚ÇÉ‚ÇÇ is the coefficient in the 3rd equation for the unknown x‚ÇÉ.\nCoefficients of each row cannot be all 0 and all of the parameters of an unknown cannot be all 0, so these m equations and n unknowns are effective.\nHomogeneous Linear system with 3 variables $$ \\begin{cases} x‚ÇÅ + 3x‚ÇÇ + 5x‚ÇÉ = 0 \\\\ 2x‚ÇÅ + 4x‚ÇÇ + 6x‚ÇÉ = 0 \\\\ 2x‚ÇÅ + 5x‚ÇÇ + 8x‚ÇÉ = 0 \\end{cases} $$\nEach row is a inner product of two 3-dimensional vectors:\nx‚ÇÅ + 3x‚ÇÇ + 5x‚ÇÉ = 0 ‚ûî (1 3 5)(x‚ÇÅ x‚ÇÇ x‚ÇÉ) = 0 2x‚ÇÅ + 4x‚ÇÇ + 6x‚ÇÉ = 0 ‚ûî (2 4 6)(x‚ÇÅ x‚ÇÇ x‚ÇÉ) = 0 2x‚ÇÅ + 5x‚ÇÇ + 8x‚ÇÉ = 0 ‚ûî (2 5 8)(x‚ÇÅ x‚ÇÇ x‚ÇÉ) = 0\nThere are 2 types of vector:\nRow vector (1 2 3) Column vector $[^1_{^2_3}]$ or (1 2 3)·µÄ Stack the above 3 expressions: $\\begin{bmatrix}1 \u0026amp; 3 \u0026amp; 5\\\\ 2 \u0026amp; 4 \u0026amp; 6\\\\ 2 \u0026amp; 5 \u0026amp; 8 \\end{bmatrix} ‚ãÖ (x‚ÇÅ\\ x‚ÇÇ\\ x‚ÇÉ) = \\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix}$\nThe left array is matrix. This linear system can be regarded as a function: $f( (x‚ÇÅ\\ x‚ÇÇ\\ x‚ÇÉ) ) = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$\nwhich maps 3 variables to 3 zeros, i.e., maps a unknown (row) vector to a zero (column) vector ùüé.\nTo normalize the notations, the unknown vector is written as a column vector ùíô, such that the linear system can be represented by matrix:\n$$ \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5\\\\ 2 \u0026amp; 4 \u0026amp; 6\\\\ 2 \u0026amp; 5 \u0026amp; 8 \\end{bmatrix} \\begin{bmatrix} x‚ÇÅ\\\\ x‚ÇÇ\\\\ x‚ÇÉ \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix} $$\nTherefore, the expression represents the mapping linearly from a column vector to another column vector.\n$f(\\begin{bmatrix} x‚ÇÅ\\\\ x‚ÇÇ\\\\ x‚ÇÉ \\end{bmatrix}) = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$\nHere, the matrix is arranging all the coefficients into an array based on their positions. Hence, it\u0026rsquo;s called coefficient matrix ùë®.\nThen this linear system can be expressed consiscely as: ùë®ùíô=ùüé.\nFrom the view of function, the function $f(\\begin{bmatrix} x‚ÇÅ \\\\ x‚ÇÇ \\\\ x‚ÇÉ \\end{bmatrix})=\\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$ can be written as f(ùíô)=ùüé.\nThe coefficient matrix ùë® plays the role of function f, that means the coefficient matrix maps the unknown vector to zero vector.\nNon-homogeneous linear system A non-homogeneous linear system contains m linear equations with n unknowns, where their constant termsare not all 0.\n$$ \\begin{cases} a‚ÇÅ‚ÇÅx‚ÇÅ + a‚ÇÅ‚ÇÇx‚ÇÇ + \u0026hellip; + a‚ÇÅ‚Çôx‚Çô = b‚ÇÅ \\\\ a‚ÇÇ‚ÇÅx‚ÇÅ + a‚ÇÇ‚ÇÇx‚ÇÇ + \u0026hellip; + a‚ÇÇ‚Çôx‚Çô = b‚ÇÇ \\\\ \\dots \\\\ a‚Çò‚ÇÅx‚ÇÅ + a‚Çò‚ÇÇx‚ÇÇ + \u0026hellip; + a‚Çò‚Çôx‚Çô = b‚Çô \\\\ \\end{cases} $$\nThe notation is the same as homogeneous system and b‚ÇÅ, b‚ÇÇ,\u0026hellip;, b‚Çô are not all 0.\nFor instance, the following non-homogeneous linear system can be represented as matrix:\n$$ \\begin{cases} x‚ÇÅ + 3x‚ÇÇ + 5x‚ÇÉ = 2 \\\\ 2x‚ÇÅ + 4x‚ÇÇ + 6x‚ÇÉ = 4 \\\\ 2x‚ÇÅ + 5x‚ÇÇ + 8x‚ÇÉ = 4 \\\\ \\end{cases} \\\\ ‚áì \\\\ \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5\\\\ 2 \u0026amp; 4 \u0026amp; 6\\\\ 2 \u0026amp; 5 \u0026amp; 8 \\end{bmatrix} \\begin{bmatrix} x‚ÇÅ\\\\ x‚ÇÇ\\\\ x‚ÇÉ \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 4\\\\ 4 \\end{bmatrix} \\\\ ‚áì \\\\ ùë®ùíô=ùêõ $$\nHere, the coefficient matrix ùë® maps the column vector to a known non-zero column vector: $$f(\\begin{bmatrix} x‚ÇÅ\\\\ x‚ÇÇ\\\\ x‚ÇÉ \\end{bmatrix})=\\begin{bmatrix} 2\\\\ 4\\\\ 4 \\end{bmatrix} ‚ûî f(ùíô)=ùêõ$$\nElementary row operations are equivalent to Gaussian elimination.\n","date":"2023-02-01T12:19:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/01_%E5%88%9D%E8%AF%86%E7%9F%A9%E9%98%B5/","title":"watch: LA - È´òÂ±± 01 | Walk into the Matrix"},{"content":"Source video: „Äê‰øóËØ¥Áü©Èòµ„ÄëÈÄÜÁü©ÈòµÂéüÊù•Ë¶ÅËøô‰πàÂ≠¶ÔºÅÊï∞Â≠¶ËÄÅÂ∏àÁõ¥ÂëºÂÜÖË°åÔºÅ\nRow echelon form ‚ûî Identity matrix ùêä‚ÇÜ includes 6 elementary row operations that transform the matrix ùêÄ to row echelon form.\n$$ ùêÄ‚ÇÜ = ùêä‚ÇÜùêÄ \\\\ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 0 \u0026amp; 1 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} 0 \u0026amp; 1/2 \u0026amp; 0 \\\\ 1 \u0026amp; -1/2 \u0026amp; 0 \\\\ -1 \u0026amp; 1/4 \u0026amp; 1/2 \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 2 \u0026amp; 4 \u0026amp; 6 \\\\ 1 \u0026amp; 4 \u0026amp; 9 \\end{bmatrix} $$\nùêÄ‚ÇÜ can further perform elementary row operations to make the elements at the top right of the main diagonal 0, i.e., reaching an identity matrix.\nùêÄ‚ÇÜ: $[^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 0\\ 1}}]$ $\\overset{-2√ór3+r2:\\ [^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ -2} _{0\\ 0\\ 1}}]}{{‚á¢}}$ ùêÄ‚Çá: $[^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]$ $\\overset{-3√ór3+r1:\\ [^{_{1\\ 0\\ -3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]}{{‚á¢}}$ ùêÄ‚Çà: $[^{_{1\\ 2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]$ $\\overset{-2√ór2+r1:\\ [^{_{1\\ -2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]}{{‚á¢}}$ ùêÄ‚Çâ: $[^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]$\nwhere,\n$$ [^{_{1\\ -2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] [^{_{1\\ 0\\ -3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] [^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ -2} _{0\\ 0\\ 1}}] ùêä‚ÇÜùêÄ = ùêä‚ÇâùêÄ = ùêà \\\\ ùêä‚Çâ = [^{_{-3\\ 7/4\\ 1‚ÅÑ2}} _{^{3\\ -1\\ -1}_{-1\\ ¬º\\ 1‚ÅÑ2}}] $$\nThe above example indicates that there exist a matrix that can modify the matrix ùêÄ to ùêà.\nLet ùêÄ‚Åª¬π denote the ùêä‚Çâ, and ùêÄ‚Åª¬π is called the inverse matrix of ùêÄ.\nSuch that there is ùêÄ‚Åª¬πùêÄ = ùêà. Since the matrix multiplication doesn\u0026rsquo;t has commutative property, what does ùêÄùêÄ‚Åª¬π equal?\nùêÄùêÄ‚Åª¬π = [^{_{1\\ 3\\ 5} _{^{2\\ 4\\ 6}_{1\\ 4\\ 9}}}] [^{_{-3\\ 7/4\\ 1‚ÅÑ2}} _{^{3\\ -1\\ -1}_{-1\\ ¬º\\ 1‚ÅÑ2}}] = ùêà\nIn fact, ùêÄ and ùêÄ‚Åª¬π are a pair of commutative matrices: ùêÄ‚Åª¬πùêÄ = ùêÄùêÄ‚Åª¬π = ùêà. Also, ùêÄ and ùêÄ‚Åª¬π are the inverse matrix of each other: (ùêÄ‚Åª¬π)‚Åª¬π = ùêÄ\nAnalogy to reciprocal Given two numbers a, b, if ab = ba = 1, then b = a‚Åª¬π, a‚â†0.\nAnd the identity matrix ùêà has the same effect as the number 1.\nTherefore, given two matrices ùêÄ, ùêÅ, if ùêÄùêÅ = ùêÅùêÄ = ùêà, then ùêÅ = ùêÄ‚Åª¬π\nNo all matrix has its inverse matrix, which is like the number 0 doesn\u0026rsquo;t has its reciprocal.\nSo what kind of matrix has an inverse? And is the inverse unique? How to solve the inverse matrix?\nInvertible matrix An invertible matrix can perform multiple elementary row operations to become an identity matrix.\nIdentity matrix is a square matrix (#row = #colmuns) and its rank = #rows (= #cols). And the elementary row operations do not change the size and rank note4.\nTherefore, an invertible matrix must be an square matrix and its rank = #rows.\nBased on the commutative property that ùêÄ‚Åª¬π and ùêÄ are the inverse matrices of each other, the ùêÄ‚Åª¬π must also be a square matrix with the same rank of ùêà.\nÂèØÈÄÜÁöÑ2‰∏™Êù°‰ª∂Ôºö1-ÊñπÈòµÔºå 2-Áß©=Âàó(Ë°å)Êï∞\nSince elementary row operation is that ùêÄ is multiplied by elementary matrix on the left. And the product of the series of elementary row operations that transformed ùêÄ to ùêà is represented as ùêÄ‚Åª¬π.\nTherefore, an invertible matrix and its inverse are both able to represented as the multiplication of multiple elementary matrix.\nSince the product of a series of elementary matrix must be invertible, a special case is that there is only a single elementary matrix. So, any of the elementary matrix is invertible.\nInverse is unique Let ùêÄùêÅ = ùêÅùêÄ = ùêà and ùêÄùêÇ = ùêÇùêÄ = ùêà, then ùêÅ = ùêÅùêà = ùêÅ(ùêÄùêÇ) = (ùêÅùêÄ)ùêÇ = ùêàùêÇ = ùêÇ\n","date":"2023-02-01T11:00:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/11_%E9%80%86%E7%9F%A9%E9%98%B5%E6%A6%82%E5%BF%B5/","title":"watch: LA - È´òÂ±± 11 | Concept of the Inverse Matrix"},{"content":" Wikipedia-ELM Controversy: RBF (1980s) raised the similar idea of ELM. Dispute about the originality of ELM: Origins of ELM Portal of ELM python toolbox: hpelm Facts ELM is ‚ÅΩ¬π‚Åæ\na type of single hidden layer feedforward neural network (SLFN).\nThe parameters (ùê∞,b) between input layer and hidden layer are set randomly. Thus, for N input n-dimensional samples and L hidden nodes, the output of the hidden layer is $ùêá = ùêó_{N√ón} ùêñ_{n√óL}+ùêõ_{n√óL}$\nOnly the number of hidden nodes needs to be predefined manually without other hyper-parameters.\nThe output weights are initialized randomly and solved based on the pseudo inverse matrix in one-shot.\nFor a n-dimensional sample ùê±‚±º and its target ùê≠‚±º=[t·µ¢‚ÇÅ, t·µ¢‚ÇÇ, \u0026hellip;, t·µ¢‚Çò]·µÄ‚àà ‚Ñù·µê, the output of ELM with L hidden nodes is ùê®‚±º = ‚àë·µ¢‚Çå‚ÇÅ·¥∏ ùõÉ·µ¢ g(ùê∞·µ¢·µÄ‚ãÖùê±‚±º + b·µ¢), where g(‚ãÖ) is activation function; ùõÉ·µ¢ is the weights of the ith ouput unit: ùõÉ·µ¢=[Œ≤·µ¢‚ÇÅ, Œ≤·µ¢‚ÇÇ, \u0026hellip;, Œ≤·µ¢‚Çô]·µÄ; ùê∞·µ¢ is input weight: ùê∞·µ¢=[w·µ¢‚ÇÅ, w·µ¢‚ÇÇ, \u0026hellip;, w·µ¢‚Çô]·µÄ; ùê±‚±º is a n-dimensional input: ùê±‚±º=[x·µ¢‚ÇÅ, x·µ¢‚ÇÇ, \u0026hellip;, x·µ¢‚Çô]·µÄ‚àà ‚Ñù‚Åø; b·µ¢ is the bias of the ith hidden unit; ùê®‚±º is a m-dimensional vector: ùê®‚±º=[o·µ¢‚ÇÅ, o·µ¢‚ÇÇ, \u0026hellip;, o·µ¢‚Çò]·µÄ‚àà ‚Ñù·µê; The ideal parameters (ùê∞,b,ùõÉ) should satisfy: ‚àë·µ¢‚Çå‚ÇÅ·¥∏ ùõÉ·µ¢ g(ùê∞·µ¢·µÄ‚ãÖùê±‚±º + b·µ¢) = ùê≠‚±º For total N samples, this mapping can be reforomalized with matrices: $ùêá_{N√óL} \\pmb\\beta_{L√óm} = ùêì_{N√óm}$, where\nùêá is the output of the hidden layer for N samples: $$ùêá(ùê∞‚ÇÅ,\u0026hellip;,ùê∞_L, b‚ÇÅ,\u0026hellip;,b_L, ùê±‚ÇÅ,\u0026hellip;ùê±_L) = \\\\ \\begin{bmatrix} g(ùê∞‚ÇÅ‚ãÖùê±‚ÇÅ+b‚ÇÅ) \u0026amp; \\dots \u0026amp; g(ùê∞_L‚ãÖùê±‚ÇÅ+b_L)\\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ g(ùê∞‚ÇÅ‚ãÖùê±_N+b‚ÇÅ) \u0026amp; \\dots \u0026amp; g(ùê∞_L‚ãÖùê±_N+b_L) \\end{bmatrix}_{N√óL}$$\nùõÉ is the output weights matrix: [ ùõÉ‚ÇÅ·µÄ ; \u0026hellip; ; ùõÉ$_L·µÄ ]_{L√óm}$\nTarget data: ùêì = $\\begin{bmatrix}ùêì‚ÇÅ·µÄ\\\\ \\vdots \\\\ùêì_N·µÄ\\end{bmatrix}_{N√óm}$\nGenerally, $ùêá_{N√óm}$ is not a square matrix (not invertible). Hence, ùõÉ=ùêá‚Åª¬πùêì cannot be applied. However, the optimal ùõÉ can be approached by minimizing the traning error iteratively: ‚àë‚±º‚Çå‚ÇÅ·¥∫‚Äñùê®‚±º-ùê≠‚±º‚Äñ.\nBest estimation: $\\^ùê∞·µ¢, \\^b·µ¢$, ^ùõÉ·µ¢ satisfy: ‚Äñùêá(^ùê∞·µ¢, ^b·µ¢)‚ãÖ^ùõÉ·µ¢- ùêì‚Äñ = min_{ùê∞·µ¢, b·µ¢, ùõÉ·µ¢} ‚Äñùêá(ùê∞·µ¢, b·µ¢)‚ãÖùõÉ·µ¢- ùêì‚Äñ, where i=1,\u0026hellip;,L\nLoss function: J = ‚àë‚±º‚Çå‚ÇÅ·¥∫ (‚àë·µ¢‚Çå‚ÇÅ·¥∏ ùõÉ·µ¢‚ãÖg(ùê∞·µ¢‚ãÖùê±‚±º + b·µ¢) - ùê≠‚±º)¬≤\nSolve ùõÉ based on the ‚àÇJ/‚àÇùõÉ=0, such that the optimal parameter is: ^ùõÉ = $ùêá^‚Ä† ùêì$ = (ùêá·µÄùêá)‚Åª¬πùêá·µÄ ùêì, where $ùêá^‚Ä†$ is the Moore-Penrose inverse (Pseudo-inverse) of ùêá. It can be proved that the norm of ^ùõÉ is the smallest and unique solution (for a set of random (ùê∞·µ¢, b·µ¢)).\nMoore-Penrose inverse Also called pseudoinverse or generalized inverse ‚ÅΩ¬≤‚Åæ.\n(bilibili search: \u0026ldquo;‰º™ÈÄÜÁü©Èòµ\u0026rdquo;) Ê∑±Â∫¶Â≠¶‰π†-ÂïÉËä±‰π¶0103‰º™ÈÄÜÁü©ÈòµÊúÄÂ∞è‰∫å‰πò\n(DDG search: \u0026ldquo;‰º™ÈÄÜÁü©Èòµ\u0026rdquo;)\n‰º™ÈÄÜÁü©ÈòµÁöÑÊÑè‰πâÂèäÊ±ÇÊ≥ïÔºü - Áü•‰πé\nnumpy.linalg.pinv() pinv(ùêó) = (ùêó·µÄ ùêó)‚Åª¬π ùêó·µÄ pinv(ùêó) ùêó = ùêà python‰πãnumpy‰πã‰º™ÈÄÜnumpy.linalg.pinv - ÂçÉË°åÁôæË°å - CSDN Example Code This matlab code ‚ÅΩ¬π‚Åæ trains and tests a ELM on the NIR spectra dataset (regression) and the Iris dataset (classification).\nNote that each column is a sample, and each row is an attribute/feature. Notations:\nQ: number of samples R: input features S: output features $P_{R√óQ}$: input pattern matrix $T_{S√óQ}$: target data matrix N: number of hidden nodes TF: transfer function $IW_{N√óR}$: input weights matrix $B_{N√óQ}$: bias matrix $LW_{N√óS}$: transposed output weights matrix Train (calculate the LW):\n$tempH_{N√óQ} = IW_{N√óR}‚ãÖP_{R√óQ} + B_{N√óQ}$ $H_{N√óQ} = TF(tempH)$ $LW_{S√óN} = T_{S√óQ}$‚ãÖ pinv(H), based on: ùõÉ$_{S√óN} ùêá_{N√óQ} = ùêì_{S√óQ}$ Test:\n$tempH_{N√óQ} = IW_{N√óR}‚ãÖP_{R√óQ} + B_{N√óQ}$ $H_{N√óQ} = TF(tempH)$ $Y_{S√óQ} = LW_{S√óN}‚ãÖH_{N√óQ}$ Example code (py) Build an Extreme Learning Machine in Python | by Glenn Paul Gara \u0026hellip; searched by DDG: \u0026ldquo;incremental elm python\u0026rdquo;\nI-ELM incremental just means adding neurons?\ngithub\nOS-ELM On-line elm\nDeep incremental RVFL Deep incremental random vector functional-link network: A non-iterative constructive sketch via greedy feature learning\nReference ÊûÅÈôêÂ≠¶‰π†Êú∫(Extreme Learning Machine, ELM)ÂéüÁêÜËØ¶Ëß£ÂíåMATLABÂÆûÁé∞ - Â•îË∑ëÁöÑYancy - CSDN Moore-Penorse - Wikipedia (Back to Top)\n","date":"2023-01-31T15:49:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/c-sum-elm/","title":"sum: ELM"},{"content":"‰∏ã‰∏Ä‰ª£ÂõΩÈôÖÂçé‰∫∫ÈùíÂπ¥Â≠¶Â≠êÈù¢ÂØπÈù¢ Á¨¨6Êúü 2022Âπ¥10Êúà20Êó• Âë®Âõõ\nOverview Research work Suggestions for graduate student (1st year) Research Contribution:\nNon-iterative learning strategy for training neural networks including single-layer networks, multi-layer networks, autoencoders, hierarchical networks, and deep networks. All the related publications in this category are my first-author papers\nThe proposed methods for pattern recognition related applications: Image Recognition, Video Recognition, Hybrid System Approximation, Robotics System Identification, EEG-brain Signal Processing. Most the related publications in this category are co-author papers with my HQPs.\nMy research works In the past 10 years, the works about Artificial neural networksÔºö\nTheoretical Contributions to ANN Machine Learning based Applications Single-layer network with non-iterative learning Data Analysis and Robotics System Identification (Ph.D.) Hierarchical NN with Subnetwork Neurons Image Recognitions (Post Doctoral Fellow) Deep Networks without iterative learning Pattern Recognition (since 2018) ‚Ö†. Single-layer network with non-iterative learning Starting with a small idea The labtorary mainly studies robots, control, mechanics. After 2008 Chinese Winter Storms, they got funding for creating Powerline De-icing robots.\nThe supervisor (Yaonan Wang): \u0026ldquo;Can you find a Neural Network for Identifying Robotics Dynamic Systems?\u0026rdquo; (in 2009 winter)\nLater, I found the following paper: \u0026ldquo;Universal approximation using incremental constructive feedforward networks with random hidden nodes\u0026rdquo;, By Huang, Guang-Bin et.al (Cannot be found on IEEE) version on elm portal\nWhat is the Neural Network? Single hidden layer feedforward NN:\nflowchart BT subgraph in[Input Layer] x1((1)) \u0026 xe((\"...\")) \u0026 xn((n)) end subgraph hid[Hidden Layer] h1((\"ùõÇ‚ÇÅ,‚ô≠‚ÇÅ,ùõÉ‚ÇÅ\")) \u0026 h2((\"ùõÇ‚ÇÇ,‚ô≠‚ÇÇ,ùõÉ‚ÇÇ\")) \u0026 he((\"......\")) \u0026 hL((\"ùõÇL,‚ô≠L,ùõÉL\")) end subgraph out[Output Layer] y1((1)) \u0026 ye((\"...\")) \u0026 ym((m)) end x1 \u0026 xn --\u003e h1 \u0026 h2 \u0026 hL --\u003e y1 \u0026 ym Output of additive hidden neurons: G(ùêö·µ¢, b·µ¢, ùê±) = g(ùêö·µ¢‚ãÖùê±+b·µ¢) Output of RBF hidden nodes: G(ùêö·µ¢, b·µ¢, ùê±) = g‚Äñùê±-ùêö·µ¢‚Äñ The output function of SLFNs is: f‚Çó(ùê±) = ‚àë‚Çó‚Çå‚ÇÅ·¥∏ ùõÉ·µ¢‚ãÖG(ùêö·µ¢, b·µ¢, ùê±) Network training Advantage: Approximate unknown system through learnable parameters.\nMathematical Model:\nApproximation capability: Any continuous target function f(x) can be approximated by Single-layer feedforward network with appropriate parameters (ùõÇ,‚ô≠,ùõÉ). In other words, given any small positive value e, for SLFN with enough number of hidden nodes, we have: ‚Äñf(ùê±)-f‚Çó(ùê±)‚Äñ \u0026lt; e\nIn real applications, target function f(ùê±) is usually unknown. One wishes that unknown f could be approximated by the trained network f‚Çó(ùê±).\nWhat is Extreme Learning Machine? Feed forward random network without using BP to train, such that it has a good real-time performance. And it fits the real-time robot task exactly.\nB-ELM (2011) \u0026ldquo;Bidirectional ELM for regression problem and its learning effectiveness\u0026rdquo;, IEEE Trans. NNLS., 2012 paper\n(23-02-10) This the inception of his subnetwork series work, and I was supposed to read this brief firt. paperNote\nMotivation\nOriginal ELM has 3 kinds of parameters: ùêö is called the \u0026ldquo;input weights\u0026rdquo;, b is the bias, ùõÉ is the \u0026ldquo;output weights\u0026rdquo;, which are consistent with earlier feedfoward network, though current single-layer feedfoward network has removed the ùõÉ.\nThe 1st layer in ELM is generated randomly and the 2nd layer is constructed based on Moore-Penrose inverse without iteration.\nflowchart LR in((X)) --\u003e lyr1[\"Layer1:\\n ùêö‚Åø, b\\n Random\\n Generated\"] --\u003e weightSum1((\"X‚ãÖùêö‚Åø + b\")) --\u003e act[Activation\\n Function\\n g] --\u003e z((\"g(X‚ãÖùêö‚Åø+b)\")) --\u003e lyr2[\"Layer2:\\n ùõÉ\"] --\u003e weightSum2((\"O =\\n ùõÉ‚ãÖg(X‚ãÖùêö‚Åø+b)\\n =T\")) -.-\u003e|\"ùõÉ =\\n g(X‚ãÖùêö‚Åø+b)‚Åª¬πT\"| lyr2 classDef lyrs fill:#ff9 class lyr1,act,lyr2 lyrs; Bidirectional ELM\nIn original ELM, the ùêö‚Åø,b are random numbers, but they can be yield if pulling the error back further, that is doing twice more inverse computation. Therefore, in order to calculate the ùêö‚Åø,b, there are 3 times inverse computation: for output weights ùõÉ, activation function g(‚ãÖ) and activation z (X‚ãÖùêö‚Åø+b) respectively.\n%%{ init: { 'flowchart': { 'curve': 'bump' } } }%% flowchart LR in((X)) --\u003e lyr1[\"Layer1:\\n ùêö‚Åø, b\\n Random\\n Generated\"] --\u003e weightSum1((\"X‚ãÖùêö‚Åø + b\")) --\u003e act[Activation\\n Function\\n g] --\u003e z((\"g(X‚ãÖùêö‚Åø+b)\")) --\u003e lyr2[\"Layer2:\\n ùõÉ\"] --\u003e out((\"O =\\n ùõÉ‚ãÖg(X‚ãÖùêö‚Åø+b)\\n =T\")) -.-\u003e|\"ùõÉ =\\n g(X‚ãÖùêö‚Åø+b)‚Åª¬πT\"| lyr2 classDef lyrs fill:#ff9 class lyr1,act,lyr2 lyrs; out -.-\u003e |\"X‚ãÖùêö‚Åø+b =\\n g‚Åª¬π T ùõÉ‚Åª¬π\"|weightSum1 out -.-\u003e |\"ùêö‚Åø =\\n X‚Åª¬π (g‚Åª¬π T ùõÉ‚Åª¬π -b ),\\n b = mse(O-T)\"|lyr1 ùõÉ = [g(X‚ãÖùêö‚Åø+b)]‚Åª¬πT X‚ãÖùêö‚Åø+b = g‚Åª¬π T ùõÉ‚Åª¬π ùêö‚Åø = X‚Åª¬π (g‚Åª¬π T ùõÉ‚Åª¬π -b ), and b = mse(O-T) The error is surprisingly small even with few hidden nodes. Compared with the original ELM, the required neurons in this method are reduced by 100-400 times, and the testing error reduced 1%-3%, and also the training time reduced 26-250 times over 10 datasets.\nMajor differences\nRandomized Networks Bidirectional ELM Classifier ELM; Echo State Netowrk;\nRandom Forest;\nVector Functional-link Network Only works for regression task Performance Similar performance;\nFaster speed;\nLess required neurons Learning strategy (Semi-)Randomized input weights;\nNon-iterative training;\nSingle-layer network Non-iterative training;\nSingle-layer network;\nCalculated weights in a network ‚Ö°. Hierarchical NN with Subnetwork Neurons Single-Layer Network with Subnetwork Neurons In 2014, deep learning is becoming popular. How to extend the B-ELM as a multi-layer network? \u0026ldquo;Extreme Learning Machine With Subnetwork Hidden Nodes for Regression and Classification\u0026rdquo;. paper; paperNote\nMotivation\nflowchart TB base[Bidirectional ELM] --\u003e theory[Theoretical Contributions on ANN] \u0026 app[Industrial Applications] theory --\u003e multilyr[1. Two-layer Neural Networks?\\n 2. Hierarchical Neural Networks?] app --\u003e tasks[1. Feature Extraction\\n 2. Dimension Reduction\\n 3. Image Recognition] Pull the residual error back to multiple B-ELMs sequentially:\nflowchart LR in((X)) --\u003e lyr11 \u0026 lyr21 \u0026 lyrN1 subgraph net1[B-ELM 1] lyr11[\"ùêö¬π,b¬π\"] ==\u003e|\"X‚ãÖùêö¬π+b¬π\"| act1[g] ==\u003e|\"g(X‚ãÖùêö¬π+b¬π)\"|lyr12[\"ùõÉ¬π\"] ==\u003e|\"ùõÉ¬π‚ãÖg(X‚ãÖùêö¬π+b¬π)\"| out((\"O ‚ûî T\\n E=T\")) -.-\u003e lyr12 -.-\u003e act1 -.-\u003e lyr11 lyr11 --\u003e act1 --\u003e lyr12 end lyr12 --\u003e out1((\"O¬π‚ûîT\\n E=T-O¬π\")) -.-\u003e lyr22 subgraph net2[B-ELM 2] lyr22[\"ùõÉ¬≤\"] -.-\u003e act2[g] -.-\u003e lyr21[\"ùêö¬≤,b¬≤\"] lyr21 --\u003e|\"X‚ãÖùêö¬≤ + b¬≤\"|act2 --\u003e|\"g(X‚ãÖùêö¬≤+b¬≤)\"|lyr22 end lyr22--\u003e out2((\"O¬≤+O¬π‚ûîT\\n E=T-(O¬≤+O¬π)\")) out2 -.-\u003e|\"Solve\\n multiple\\n B-ELMs\"| outn-1((\"E=T-‚àë·µ¢‚Çå‚ÇÅ·¥∫‚Åª¬π O‚Å±\"))-.-\u003elyrN2 %%out2 -.-\u003e lyrn2 %%subgraph nets[multiple B-ELMs] %%lyrn2[\"ùõÉ‚Åø\"] -.-\u003e actn[g] -.-\u003e lyrn1[\"ùêö‚Åø,b‚Åø\"] %%lyrn1 --\u003e|\"X‚ãÖùêö‚Åø + b‚Åø\"|actn --\u003e|\"g(X‚ãÖùêö‚Åø+b‚Åø)\"|lyrn2 %%end %%lyrn2 --\u003e outn-1((\"E=T-‚àë·µ¢‚Çå‚ÇÅ·¥∫‚Åª¬π O‚Å±\"))-.-\u003elyrN2 subgraph netN[\"B-ELM N\"] lyrN2[\"ùõÉ·¥∫\"] -.-\u003e actN[g] -.-\u003e lyrN1[\"ùêö·¥∫,b·¥∫\"] lyrN1 --\u003e|\"X‚ãÖùêö·¥∫ + b·¥∫\"|actN --\u003e|\"g(X‚ãÖùêö·¥∫+b·¥∫)\"|lyrN2 end lyrN2--\u003e outN((\"‚àë·µ¢‚Çå‚ÇÅ·¥∫ O‚Å±‚ûîT\")) classDef lyrs fill:#ff9 class lyr11,act1,lyr12,lyr21,act2,lyr22,lyrN1,actN,lyrN2 lyrs; linkStyle 9,10,11,15,16,17,22,23,24 stroke:#0af,stroke-width:3px %%classDef node font-size:20px; Dotted links are computation with inverse. Cyan links is the second feedforward using the updated parameters to give a trustworthy result O¬π. The objective is to approach the target T, so there is a residual error E=T-O¬π. Then another B-ELM (with same structure) is used to reduce the error continuously. And this time, the prediction is O¬≤+O¬π, which is the approximation of T. Here, the residual error is E=T-(O¬≤+O¬π)\nRepeatedly pulling the residual error to a new B-ELM N times is equivalent to N SLFNs. But B-ELM is fast without iteration and less computation with a few hidden nodes in each SLFN.\nBased on original SLFN structure, each node contains a SLFN.\nTwo-Layer Network with Subnetwork Neurons (2015) How to extend the Single-layer network with subnetwork nodes system to a two-layer network system?\nA general two-layer system was built in paper: \u0026ldquo;Multilayer Extreme Learning Machine with Subnetwork Hidden Neurons for Representation Learning\u0026rdquo; paper; paperNote\nThough it only contains two \u0026ldquo;general\u0026rdquo; layers, this system includes hundreds of networks, and it\u0026rsquo;s fast due to the modest quantity and no iteration.\nCompared with ELM and B-ELM, it got better performance over 35 datasets:\nClassification (vs ELM) Regression (vs B-ELM) Required Neurons Reduced 2-20 times Reduced 13-20% Metrics Accuracy increase 1-17% Testing error reduced 1-8% Training Speed faster 25-200times faster 5-20% Two-layer network system can perform image compression or reconstruciton, etc. This method is better than Deep Belief Network on small datasets. But it\u0026rsquo;s inferior than deep learning with transfer learning technics on huge datasets.\nHierarchical Network with Subnetwork Neurons \u0026ldquo;Features combined from hundreds of mid-layers: Hierarchical networks with subnetwork nodes\u0026rdquo; IEEE Trans. NNLS, 2019. paper\nFrom a single-layer network with subnetwork neurons to the multi-layer network, and then to a neural network system, these 3 papers cost 5 years or so.\nCompared with deep learning network, it\u0026rsquo;s extremely fast and performs well on small datasets, like Scene15, Caltech101, Caltech256. But for large datasets, deep learning is the winner.\n\u0026ldquo;Somewhat regretfully, I turned to deep learning a bit late. But been hesitant to do research along this approach.\u0026rdquo;\nMajor differences between ours and Deep Networks\nSGD based methods in DCNN Moore-Penrose inverse matrix based methods Hyper params lr; momentum; bs; L2 regulariation; epochs L2 regularization (non-sensitive) Performance higher performance in Computer Vision tasks (with huge datasets);\nGPU-based computation resource;\nMore parameters;\nMore required training time Faster learning speed/less tuning;\nPromising performance in Tabular Datasets;\nLess over-fitting problem. ‚Ö¢. Deep Networks without iterative learning Since 2018: How to combine the non-iterative method (M-P inverse matrix) with deep convolutional network to gain advantages? This took 2-3 years.\nThis is the age of Deep Learning.\nInteresting 20 years of cycles\nRosenblatt\u0026rsquo;s Perceptron proposed in mid of 1950s, sent to \u0026ldquo;winter\u0026rdquo; in 1970s. Back-Propagation and Hopfield Network Proposed in 1970s, reaching research peak in mid of 1990s. Support vector machines proposed in 1995, reaching research peak early this century. There are exceptional cases:\nMost basic deep learning algorithms proposed in 1960s-1980s, becoming popular only since 2012 (for example, LeNet proposed in 1989). ImageNet pushed deep learning, because only when the huge network structure of deep learning meets the matched huge dataset, it can achive good performance.\nThe success of deep learning enlist three factors: 1. NN structure and algorithm; 2. Big data; 3. GPU availability.\nHundreds of layers result in tedious training time. \u0026ldquo;The study intensity is infinitely small and the study duration is infinitely large.\u0026rdquo;\n\u0026ldquo;The improvement space of deep neural network is limited. So can we introduce non-iterative learning strategies for training deep networks\u0026rdquo;\nTraining speed is more important for scientific research than accuracy. And also it\u0026rsquo;s necessary to reduce the dependence on GPUs and the involvement of undeterministic hyper parameters (lr,bs,\u0026hellip;)\nRetraining DCNN with the non-iterative strategy (2019): \u0026ldquo;Recomputation of the dense layers for the performance improvement of DCNN.\u0026rdquo; IEEE TPAMI, 2020. link\nMotivation\ngraph LR base[\"1. Two-layer Neural Networks\\n 2. Hierarchical Neural Networks\"] --\u003e explore[\"1. Deep learning withe non-iterative method\"] In a DCNN, the first few layers are convolutional layers, maxpooling, then there\u0026rsquo;re 3 or 1 dense layer.\nIf I cannot train all of the hundreds layers in my non-iterative method, can I train only certain layers that are easy trained with my method, rather the SGD?\nOnly the fully-connected layers are trained by non-iterative method (inverse matrix), and the rest of layers are trained by gradient descent (SGD, SGDM, Adam).\nOn some medium-size datasets(CIFAR10, CIFAR100, SUN397), this approach brought a moderate improvement because there are only 3 dense layer out of a 50/100-layer network (most of layers are trained with SGD), but speeds up the training.\nOne layer can be trained within 1-2 seconds.\n","date":"2023-01-27T10:19:00Z","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/","title":"watch: SNN - Êù®ÊòìÊóª | WeChat Live"},{"content":"IEEE Cybernetics(2015-11-02) | Google Drive | G.Scholar\nThis is the first paper of ELM with subnetwork nodes by Yimin Yang. The second paper in the series is MltLyr ELM with subnetwork nodes The outline of Yang\u0026rsquo;s research works: Yang-WeChatLive-20221020; Abstract Learning effectiveness and speed of SLFN are bottleneck. ELM is fast. Grow subnetwork nodes by pulling back residual network error to the hidden layer. Better generalization performance with fewr hidden nodes. ‚Ö†. Introduction Bring out the subject: FNN (universial approximator) ‚ûî SLFN\nWhat is an SLFN? Input layer + hidden layer + output layer\nMath description: For N arbitary distinct samples {(ùê±·µ¢,ùê≠·µ¢)}·µ¢‚Çå‚ÇÅ·¥∫, where ùê±·µ¢‚àà ùêë‚Åø and ùê≠·µ¢‚àà ùêë·µê, the network output is:\n$ùêü_L(ùê±)$ = ‚àë·µ¢‚Çå‚ÇÅ·¥∏ ùõÉ·µ¢h(ùêö·µ¢‚ãÖùê±‚±º + b·µ¢) = ‚àë·µ¢‚Çå‚ÇÅ·¥∏ ùêá·µ¢‚ãÖùõÉ·µ¢, j=1,\u0026hellip;,N (1)\nSLFN output is the weighted sum of ùêø hidden nodes (perceptrons) with the factor ùõÉ. The ith perceptron receives the weighted sum of ùëÅ inputs through its parameters (ùêö·µ¢, b·µ¢), ùêö·µ¢‚àà ùêë‚Åø, b‚àà ùêë, and performs activation function h. Its contribution ratio to the all output nodes is ùõÉ·µ¢. ELM traits: NN (all params are adjustable) ‚ûî partial random networks ‚ûî ELM is a full-random learning method, where the input weights and bias (ùêö, b) are generated randomly and independent of training data. (Will the Glorot normalization has no effect?)\nELM advantages: An unification of FNNs and SVM/LS-SVM\nELM application: CV, da,\u0026hellip;, online learning\nProblems:\nThe choice of the regularization parameter C which affects the generalization performance of ELM mainly relies on trial-and-error method.\nHow many neurons should be used in ELM. Although Huang suggested to use more than 1000 hidden nodes, whether the number of hidden nodes can be further reduced without affecting learning effectiveness for large-size/high dimension data set. Several improved ELM methods, like B-ELM pulls the network residual error back to the hidden layer but it only works for regression task, and other methods bring a higher computation complexity when compared to standard ELM.\nSolution: Growing subnetwork hidden nodes to the exisiting network by pulling back the network residual error to hidden layers. A hidden node itself can be formed by several hidden nodes.\nContributions:\nFaster than BP, SVM and other ELMs and compatible to regreesion and classification problems. The regularized parameter C do not affect the generalization performance of this method. This method with m hidden nodes (the desired output dimensionality) can achieve better training accuracy than the original ELM with a large number of hidden nodes. ‚Ö°. Definitions and Basic-ELM A. Notations and Definitions ùêë : set of real numbers\n{(ùê±·µ¢,ùê≠·µ¢)}·µ¢‚Çå‚ÇÅ·¥∫ : N arbitrary distinct samples,\nùê±·µ¢ = [x·µ¢‚ÇÅ,x·µ¢‚ÇÇ,\u0026hellip;,x·µ¢‚Çô]·µÄ : n-dim input data, ùê±·µ¢‚àà ùêë‚Åø is a column vector;\nùê≠·µ¢ : m-dim desired output data, ùê≠·µ¢‚àà ùêë·µê\nùê± : input data matrix, ùê±=[ùê±‚ÇÅ,\u0026hellip;ùê±_N], ùê±·µ¢‚àà ùêë‚Åø·ïÅ·¥∫\nùê≠ : desired output data matrix, ùê≠=[ùê≠‚ÇÅ,\u0026hellip;ùê≠_N], ùê≠‚àà ùêë·µê·ïÅ·¥∫\n(^ùêö‚Çô,^b‚Çô) : parameters of the ùëõth subnetwork hidden node, ^ùêö‚Çô‚àà ùêë‚Åø·ïÅ·µê, ^b‚Çô‚àà ùêë (suppose the number of hidden nodes equals to the output dimension m, thus the mapping is from n to m.)\n^ùêö‚Çô = [ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò], n√óm weights matrix (for a n-dimension sample ùê±·µ¢), ùêö‚Çô‚Çò‚àà ùêë‚Åø\nùêû‚Çô : residual error of current network output ùëì‚Çô with ùëõ hidden nodes (for N samples), i.e., ùêû‚Çô=ùê≠-ùëì‚Çô, ùêû‚Çô‚àà ùêë·µê·ïÅ·¥∫.\nùêá : output matrix of the hidden layer (of SLFN) for tarining set {(ùê±·µ¢,ùê≠·µ¢)}·µ¢‚Çå‚ÇÅ·¥∫, ùêá = [h(ùê±‚ÇÅ),\u0026hellip;,h(ùê±_N)]·µÄ, ùêá‚àà ùêë·¥∫·ïÅ·µê, ùêá = g(ùê±·µÄùêö+ùêõ)???\nh(ùê±) : activation function. ELM feature mapping (or Huang\u0026rsquo;s transform)\nùêá·µ¢ : the ùëñth hidden node output w.r.t. inputs, i.e., the ùëñth column of ùêá\nùêà : unit matrix\nsum(ùêû) : the sum of all elements of the matrix ùêû\nB. Basic-ELM ELM is proposed for single-hidden-layer feedforward networks (SLFNs).\nThe output function of ELM with L hidden nodes for SLFNs is:\n$ùëì_L(ùê±)$ = ‚àë·µ¢‚Çå‚ÇÅ·¥∏ Œ≤·µ¢‚ãÖh(ùêö·µ¢‚ãÖùê±‚±º + b·µ¢) = ‚àë·µ¢‚Çå‚ÇÅ·¥∏ ùêá·µ¢‚ãÖùõÉ·µ¢, j=1,\u0026hellip;,N.\nwhere h(‚ãÖ) denotes an activation function, (ùêö·µ¢, b·µ¢), ùêö·µ¢‚àà ùêë‚Åø, b·µ¢‚àà ùêë, denotes the ith hidden node parameters, and ùõÉ·µ¢ is the ith output weight between the ith hidden node and the output nodes.\nBased on Bartlett\u0026rsquo;s theory, ELM theory aims to reach not only the smallest training error, but also the smallest norm of output weights (Least square-least norm solution, where the regularization makes an invertible matrix, such that a special solution can be determined.):\nMinimize: ‚ÄñùõÉ‚Äñ¬≤ + C‚ãÖ‚ÄñùêáùõÉ - ùê≠‚Äñ¬≤\n\u0026ldquo;then the generalization performance depends on the size of weights rather than the number of nodes.\u0026rdquo;\nLemma 1 (proved by Huang):\nGiven an SLFN with nonconstant piecewise continuous hidden nodes ùêá(ùê±, ùêö, b), then for any continuous target function ùëì and any function sequence ùêá‚Çô ≥(ùê±) = ùêá(ùê±, ùêö‚Çô, b‚Çô) randomly generated based on any continuous sampling distribution,\nlim$_{n‚ûù‚àû}$ ‚Äñùëì - (ùëì‚Çô‚Çã‚ÇÅ + ùêá‚Çô ≥‚ãÖùõÉ‚Çô)‚Äñ = 0\nholds with probabitliy 1 if: ùõÉ‚Çô = ‚ü®ùêû‚Çô‚Çã‚ÇÅ, ùêá‚Çô ≥‚ü© / ‚Äñùêá‚Çô ≥‚Äñ¬≤ (the weight of the ùëõth node)\nwhere\n\u0026ldquo;‚ü® , ‚ü©\u0026rdquo; stands for \u0026ldquo;dot product\u0026rdquo; (Frobenius inner product) of two matrices and is a scalar. n is the number of hidden nodes in the hidden layer. ùêû‚Çô‚Çã‚ÇÅ is the residual error of the last iteration, i.e., when there were n-1 hidden nodes. ùêá‚Çô ≥ is the output matrix of the current hidden layer (activated but havn\u0026rsquo;t scaled by ùõÉ). Intuitively, as the residual error reduces, the weight of the newer node gets smaller.\n‚Ö¢. Proposed ELM Method With Subnetwork Hidden Nodes A. Structure of the Proposed Method Motivations:\nSelecting an appropriate number of neurons can resort to optimization algorithms. The generalization performance depends on the size of the weights rather than the number of weights. Inspiration:\n\u0026ldquo;A hidden node itself can be a subnetwork formed by several nodes. And these subnetwork hidden nodes and output weights itself should be the smallest norm, and also aim to reach the smallest training error.\u0026rdquo;\nObjectives:\nGiven N training samples {(ùê±·µ¢,ùê≠·µ¢)}·µ¢‚Çå‚ÇÅ·¥∫, ùê±·µ¢‚àà ùêë‚Åø, ùê≠·µ¢‚àà ùêë·µê, generated from the same continuous system, if activation function h is invertible, the objectives are:\n‚Äñùêû‚Çô‚Çã‚ÇÅ‚Äñ ‚â• ‚Äñùêû‚Çô‚Çã‚ÇÅ - h(^ùêö‚Çô,ùê±)‚Äñ ‚â• ‚Äñùêû‚Çô‚Çã‚ÇÅ - ùêá‚Çô‚Äñ ‚â• ‚Äñùêû‚Çô‚Çã‚ÇÅ - ùêá‚Çô‚ãÖùõÉ‚Çô‚Äñ (residual error is decreasing.) ‚Äñh(^ùêö‚Çô,ùê±) - ùêû‚Çô‚Çã‚ÇÅ‚Äñ = min_{ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò} ‚Äñh(ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò) - ùêû‚Çô‚Çã‚ÇÅ‚Äñ (minimize weights inside nodes) ‚Äñùêá‚Çô‚ãÖ^ùõÉ‚Çô - ùêû‚Çô‚Çã‚ÇÅ‚Äñ = min_{ùõÉ} ‚Äñùêá‚Çô‚ãÖùõÉ - ùêû‚Çô‚Çã‚ÇÅ‚Äñ (minimize the weights outside nodes) where\n^ùêö‚Çô and ^ùõÉ‚Çô are the optimal (the ultimate status) parameters with the smallest norm among all the least squares solutions. ùêá‚Çô = h(^ùêö‚Çô, ^b‚Çô, ùê±) is the output of the nth hidden node with the optimal parameters. If activation function h is invertible, subnetwork hidden nodes in SLFN can be calculated by pulling back network residual error to hidden layers.\nFor example, with sine function as the activation function, training a subnetwork hidden node (^ùêö) is equivalent to finding a least-square solution ^ùêö‚Çô (letting the derivative of MSE=0) with the least norm for the linear system:\n[ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò]‚ãÖùê± = arcsin(ùêû‚Çô‚Çã‚ÇÅ), ùêû‚Çô‚Çã‚ÇÅ‚àà (0,1],\nsuch that the optimal ^ùêö‚Çô satifies:\n‚Äñsin(^ùêö‚Çô, ùê±) - ùêû‚Çô‚Çã‚ÇÅ‚Äñ = min_{ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò} ‚Äñsin(ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò, ùê±) - ùêû‚Çô‚Çã‚ÇÅ‚Äñ,\nThat means the output of the nth \u0026ldquo;subnetwork hidden node\u0026rdquo; ^ùêö‚Çô is approaching the residual error ùêû‚Çô‚Çã‚ÇÅ of the last status.\nThe input weights ^ùêö‚Çô of a node for this model is a matrix (instead of a vector), beacuse each \u0026ldquo;subnetwork (general) hidden node\u0026rdquo; contains a standard SLFN (several hidden nodes) internally.\nDifferences with standard ELM\nELM with subnetwork standard ELM hidden node m neurons: $ùêö_f‚àà ùêë‚Åø·ïÅ·µê, ùêõ_f‚àà ùêë·µê$ single neuron:\nùêö‚àà ùêë‚Åø, b‚àà ùêë construct calculated generated randomly # hidden nodes L x m (m ‚üÇ L, m = #output dim) L B. Proposed Method Lemma 2: Given a bounded nonconstant piecewise continuous activation function h, there is: lim$_{(ùêö,b)‚Üí(ùêö‚ÇÄ,b‚ÇÄ)}$ ‚Äñh(ùêö‚ãÖùê±+b) - h(ùêö‚ÇÄ‚ãÖùê±+b‚ÇÄ)‚Äñ = 0 (ËøûÁª≠ÊÄß)\nTheorem 1: Given N arbitrary distinct samples {(ùê±·µ¢,ùê≠·µ¢)}·µ¢‚Çå‚ÇÅ·¥∫, ùê±·µ¢‚àà ùêë‚Åø, ùê≠·µ¢‚àà ùêë·µê, a sigmoid or sine activation function h, and then for any continuous desired outputs ùê≠, the limit of error converges to 0:\nlim$_{n‚ûù‚àû}$ ‚Äñ ùê≠-{ u‚Åª¬π(h(^ùêö‚ãÖùê±+b))} ‚Äñ = 0\nProof:\nProve the sequence ‚Äñùêû‚Çô‚Äñ is decreasing with 0 as the lower bound and it converges.\nFor the ùëõth subnetwork hidden node containing m hidden nodes, the linear mapping is:\nùõå‚Çô = [ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò]‚ãÖùê±, ùõå‚Çô‚àà ùêë·µê\nThen ùõå‚Çô passes through the activation function. Because the target is error, which should become 0 at the end, the error at present is the output of activation function:\nùêû‚Çô‚Çã‚ÇÅ = h(ùõå‚Çô) ‚àà ùêë·µê\nThe inverse function of h is h‚Åª¬π, and its input value should range from (0,1]. Therefore, if trying to solve ùõå‚Çô from ùêû‚Çô‚Çã‚ÇÅ, every element in ùêû‚Çô‚Çã‚ÇÅ should be scaled to the range of (0,1] by the normalized function u(‚ãÖ). Then, ùõå‚Çô can be calculated through:\nùõå‚Çô = h‚Åª¬π(u(ùêû‚Çô‚Çã‚ÇÅ))\nFurther, the input weights of this subnetwork hidden node can be solved:\n$$\\^ùêö‚Çô = [ùêö‚Çô‚ÇÅ,\u0026hellip;,ùêö‚Çô‚Çò] = h‚Åª¬π(u(ùêû‚Çô‚Çã‚ÇÅ))‚ãÖùê±‚Åª¬π$$\nFor different activation functions, there will be:\n$$\\rm \\{^{\\^ùêö‚Çô = arcsin(u(ùêû‚Çô‚Çã‚ÇÅ))‚ãÖùê±‚Åª¬π,\\quad sine} _{\\^ùêö‚Çô = -log( (1/u(ùêû‚Çô‚Çã‚ÇÅ)) - 1)‚ãÖùê±‚Åª¬π,\\quad sigmoid}$$\n(This work is a continuation on ELM, that is once the ùõÉ calculated based on target ùê≠ and ùêá‚Åª¬π, the residual error is also fixed, so it serves as the target for ùêö,b. Still, applying least-square, the optimial a can be calculated based on \u0026ldquo;target\u0026rdquo; e and ùê±‚Åª¬π.)\n^b is the mean of hidden nodes.\nThe error can be reduced by adding the bias\nDo feedforward using the calculated ^ùêö‚Çô and ^b‚Çô, so this time the output of the hidden layer is:\n$$\\^ùêá‚Çô·µâ = u‚Åª¬π(h(\\^ùêö‚Çô‚ãÖùê±+\\^b‚Çô))$$\nBecause ùêû‚Çô‚Çã‚ÇÅ is the last output of the activation fuction, ùêû‚Çô‚Çã‚ÇÅ and $\\^ùêá‚Çô·µâ$ are the same things. So they can subtract from each other. Then the residual error for this time is:\nŒî = ‚Äñùêû‚Çô‚Çã‚ÇÅ‚Äñ¬≤ - ‚Äñùêû‚Çô‚Çã‚ÇÅ - ^ùêá‚Çô·µâ‚Äñ¬≤ = ‚Äñùêû‚Çô‚Çã‚ÇÅ‚Äñ¬≤ - (‚Äñùêû‚Çô‚Çã‚ÇÅ‚Äñ¬≤ - 2‚Äñùêû‚Çô‚Çã‚ÇÅ‚Äñ‚Äñ^ùêá‚Çô·µâ‚Äñ + ‚Äñ^ùêá‚Çô·µâ‚Äñ¬≤) = 2‚Äñùêû‚Çô‚Çã‚ÇÅ‚Äñ‚Äñ^ùêá‚Çô·µâ‚Äñ - ‚Äñ^ùêá‚Çô·µâ‚Äñ¬≤ = 2‚ü®ùêû‚Çô‚Çã‚ÇÅ, ^ùêá‚Çô·µâ‚ü© - ‚Äñ^ùêá‚Çô·µâ‚Äñ¬≤ = ‚Äñ^ùêá‚Çô·µâ‚Äñ¬≤ ( 2‚ü®ùêû‚Çô‚Çã‚ÇÅ, ^ùêá‚Çô·µâ‚ü©/‚Äñ^ùêá‚Çô·µâ‚Äñ¬≤ - 1 )\nŒî is ‚â• 0\nProve the limit converges to 0 when n tends to infinity.\nThe target value is approximated while the error is decreased.\nThe final estimation is the summation of the ouput of d subnetwork hidden nodes\nThe VC dimension is lower than standard ELM, i.e., the dimension of feature space m‚â™ L, so the generalization ability of this method is better.\n(Back to top)\n","date":"2023-01-20T11:46:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/","title":"read: ELM with Subnetwork Nodes"},{"content":"Authors: Yimin Yang, and Q. M. Jonathan Wu IEEE Cybernetics; Publish Date: 2015-10-09.\nThis is the 2nd paper in his series, and the first paper is this.\n(ÊÑüËßâIntroÂÜôÂæó‰∏çÈîôÔºåÈÄªËæëÊÄßÂº∫Ôºå‰ø°ÊÅØÈáèÂ§ßÔºõ‰ΩÜÂêéÈù¢methodÈÉ®ÂàÜÂ•ΩÂ§ötypo)\nAbstract Representation learning of multilayer ELM with subnetwork nodes outperform conventional feature learning methods.\nI. Introduction model performance ‚ûî data representaiton/features ‚ûî processing pipelines design and data transformations ‚ûî data representation ‚ûî effective learning\nFeature reduction and extraction techniques can be conducted in a supervised, unsupervised or semi-supervised manner.\nELMs learn representations of data to extract useful information when building classifiers or predictors.\nELMs provide a unified learning framework for \u0026ldquo;generalized\u0026rdquo; single-hidden layer feedforward NNs (SLFNs).\nIn ELM methods, the hidden layer parameters of NN need not be tuned during training, but generated randomly. ML-ELM is adding (multiple) general hidden nodes (subnetwork nodes) to existing single-hidden-layer ELM networks.\nA versatile platform with faster speed and better generalization performance on feature extraction. Its generalization performance is not sensitive to the parameters of the networks in the learning process. ML-ELM has universal approximation capability and representation learning. II. Preliminaries and Basic-ELM A. Notations ùêë : set of real numbers {(ùê±·µ¢,ùê≤·µ¢)·µ¢‚Çå‚ÇÅ·¥π} (ùê±·µ¢‚àà ùêë‚Åø,ùê≤·µ¢‚àà ùêë·µê) : M arbitrary distinct samples, ùê± : an input data matrix ùê±‚àà ùêë‚Åø·ïÅ·¥π ùê≤ : desired output data matrix ùê≤‚àà ùêë·µê·ïÅ·¥π ùõÇ·µ¢ : weight vector connecting the ùëñth hidden nodes and the input nodes ‚ô≠·µ¢ : bias of the ùëñth hidden nodes Œ≤·µ¢ : output weight between the ùëñth hidden node and the output node ùêû : residual error of current network output, i.e., ùêû=ùê≤-ùêü ùêà : unit matrix sum(ùêû) : the sum of all elements of the matrix ùêû g : sigmoid or sine activation function (TABLE 1)\n(ùõÇ,‚ô≠) : a hidden node (in basic ELM) (ùêö,ùëè) : a general hidden node (or subnetwork node) ^ùêö ≤_f : input weight of the jth general hidden node in feature mapping layer. ^ùêö ≤_f‚àà ùêë·µà·ïÅ‚Åø ^b ≤_f : bias of the ùëóth general hidden node in feature mapping layer ^b ≤_f‚àà ùêë (ùõÇ·µ¢ ≤_f,‚ô≠ ≤_f) : the ùëñth general hidden node in the ùëóth general hidden node. (^ùêö‚Çï,^ùëè‚Çï) : hidden nodes in ELM learning layer and ^ùêö‚Çï‚àà ùêë ·µê·ïÅ·µà u‚±º : normalized function in the ùëóth general node, u‚±º(‚ãÖ):ùêë ‚ûî (0,1], u‚±º‚Åª¬π represent its reverse function ùêá ≤_f : feature data generated by ùëógeneral nodes in a feature mapping layer, i.e., ùêá ≤_f = ‚àë·µ¢‚Çå‚ÇÅ ≤ u·µ¢‚Åª¬π ‚ãÖ g(ùê±, ^ùêö‚Å±_f, ^b‚Å±_f), ùêá ≤_f‚àà ùêë·µà·ïÅ·¥π ùêá ≤‚Å±_f : feature data generated by the ùëñth feature mapping layer M : number of training samples n : input data dimension m : output data dimension d : feature data dimension ùêû_L : the residual error of current two-layer network (L general nodes in the first layer and (ùêö‚Çï,ùëè‚Çï) in the second layer) ùêû ≤_L : the residual error of current two-layer network (L general nodes in the first layer and ùëógeneral nodes in the second layer) L : the numbers of general hidden nodes B. Basic-ELM The output function of ELM for SLFNs fed with input matrix ùê± is: f‚Çô(ùê±)=‚àë·µ¢‚Çå‚ÇÅ‚Åø Œ≤·µ¢ g(ùê±, ùõÇ·µ¢, ‚ô≠·µ¢).\n\u0026ldquo;ELM theory aims to reach the smallest training error but also the smallest norm of output weights\u0026rdquo; (regularization term?), so the objective is to minimize: ‚ÄñŒ≤·µ¢‚Äñ‚Çö·∂£¬π + C‚ãÖ‚Äñ‚àë·µ¢‚Çå‚ÇÅ‚Åø Œ≤·µ¢ g(ùê±, ùõÇ·µ¢, ‚ô≠·µ¢) - ùê≤‚Äñ·∂£¬≤_q, i=1,\u0026hellip;,n. (·∂£ signifies Œº)\nwhere Œº‚ÇÅ\u0026gt;0, Œº‚ÇÇ\u0026gt;0, p,q = 0, ¬Ω, 1, 2, \u0026hellip;, +‚àû, C is a positive value, g(ùê±, ùõÇ, ‚ô≠) is referred to as ELM feature mapping (linear projection+activation) or Huang\u0026rsquo;s transform.\n(Convergence proved by Huang et al.)\nLemma 1: Given M aribitrary distinct samples {(ùê±, ùê≤)}, ùê±‚àà ùêë‚Åø·ïÅ·¥π, ùê≤‚àà ùêë·µê·ïÅ·¥π sampled from a continuous system, an activation function g, then for any continous target function ùê≤ and any function sequence g(ùê±, ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥) randomly generated based on any continuous sampling distribution, lim_{n‚ûù‚àû} ‚Äñùê≤-Ôºàf‚Çô‚Çã‚ÇÅ + g(ùê±, ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥)Ôºâ‚Äñ=0 holds with probabiltiy one if Œ≤‚Çô = ‚ü®ùêû‚Çô‚Çã‚ÇÅ, g(ùê±, ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥)‚ü© / ‚Äñg(ùê±, ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥)‚Äñ¬≤,\nwhere (ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥) represesnts the ùëõth random hidden node, and ùêû‚Çô‚Çã‚ÇÅ = ùê≤-f‚Çô‚Çã‚ÇÅ\nIII. Proposed Method A. ELM With Subnetwork Nodes A hidden node can be a subnetwork formed by several hidden nodes. Hence, a single mapping layer can contain multiple networks.\nComparision of the feature mapping layer:\nflowchart LR subgraph A[basic ELM] direction BT x1[\"x‚ÇÅ\"]--\u003e h1((\"ùõÇ‚ÇÅ,‚ô≠‚ÇÅ, Œ≤‚ÇÅ\")) \u0026 he1((...)) \u0026 hL((\"ùõÇL,‚ô≠L, Œ≤L\")) xe[x...]--\u003e h1((\"ùõÇ‚ÇÅ,‚ô≠‚ÇÅ, Œ≤‚ÇÅ\")) \u0026 he1((...)) \u0026 hL((\"ùõÇL,‚ô≠L, Œ≤L\")) xn[\"x‚Çô\"]--\u003e h1((\"ùõÇ‚ÇÅ,‚ô≠‚ÇÅ, Œ≤‚ÇÅ\")) \u0026 he1((...)) \u0026 hL((\"ùõÇL,‚ô≠L, Œ≤L\")) h1 --\u003e y1[\"y‚ÇÅ\"] \u0026 ye[...] \u0026 ym[\"y‚Çò\"] he1--\u003e y1[\"y‚ÇÅ\"] \u0026 ye[...] \u0026 ym[\"y‚Çò\"] hL --\u003e y1[\"y‚ÇÅ\"] \u0026 ye[...] \u0026 ym[\"y‚Çò\"] subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 he1 \u0026 hL end end subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 he1 \u0026 hL end subgraph B[ELM with subnetwork nodes] direction BT x1_[\"x‚ÇÅ\"] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL xe_[x...] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL xn_[\"x‚Çô\"] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL ghn1--\u003e y1_[\"y‚ÇÅ\"] \u0026 ye_[...] \u0026 ym_[\"y‚Çò\"] ghne--\u003e y1_[\"y‚ÇÅ\"] \u0026 ye_[...] \u0026 ym_[\"y‚Çò\"] ghnL--\u003e y1_[\"y‚ÇÅ\"] \u0026 ye_[...] \u0026 ym_[\"y‚Çò\"] end subgraph ghn1[\"^ùõÇ¬π_f, ^‚ô≠¬π_f, with weight u‚ÇÅ‚Åª¬π\"] direction TB n11((\"ùõÇ¬π_f1,\\n ‚ô≠¬π_f1\")) \u0026 n1e((...)) \u0026 n1m((\"ùõÇ¬π_fm,\\n ‚ô≠¬π_fm\")) end subgraph ghne[\"general hidden nodes\"] direction TB ne1((1)) \u0026 nee((...)) \u0026 nem((m)) end subgraph ghnL[\"^ùõÇ·¥∏_f, ^‚ô≠_f, with weight u\\_L‚Åª¬π\"] direction TB nL1((\"ùõÇ·¥∏_f1,\\n ‚ô≠·¥∏_f1\")) \u0026 nLe((...)) \u0026 nLm((\"ùõÇ·¥∏_fm,\\n ‚ô≠·¥∏_fm\")) end Three differences between ELM feature mapping layer and this feature mapping layer.\nDifference Standard ELM ELM with subnetwork nodes hidden node single hidden node generated\none by one general hidden node having subnetwork # hidden node Independent to the output dim ùëö In a subnetwork, it equals to the output dim relation A special case of the subnetwork case B. Proposed Method for Representation Learning 1) Optimal Projecting Parameters and Optimal Feature Data flowchart LR x1[\"x‚ÇÅ\"]--\u003e h1((\"^ùõÇ¬π_f,^‚ô≠¬π_f, Œ≤¬π\")) \u0026 h2((\"^ùõÇ¬≤_f,^‚ô≠¬≤_f, Œ≤¬≤\")) \u0026 he1((\"‚ãÆ\")) \u0026 hL((\"^ùõÇ·¥∏_f,^‚ô≠·¥∏_f, ^Œ≤·¥∏\")) xe[x...]--\u003e h1((\"^ùõÇ¬π_f,^‚ô≠¬π_f, Œ≤¬π\")) \u0026 h2((\"^ùõÇ¬≤_f,^‚ô≠¬≤_f, Œ≤¬≤\")) \u0026 he1((\"‚ãÆ\")) \u0026 hL((\"^ùõÇ·¥∏_f,^‚ô≠·¥∏_f, ^Œ≤·¥∏\")) xn[\"x‚Çô\"]--\u003e h1((\"^ùõÇ¬π_f,^‚ô≠¬π_f, Œ≤¬π\")) \u0026 h2((\"^ùõÇ¬≤_f,^‚ô≠¬≤_f, Œ≤¬≤\")) \u0026 he1((\"‚ãÆ\")) \u0026 hL((\"^ùõÇ·¥∏_f,^‚ô≠·¥∏_f, ^Œ≤·¥∏\")) h1 \u0026 h2 \u0026 he1 \u0026 hL --\u003e feat[\"d-dimension\\n Feature data\"] feat --\u003e n1 \u0026 n2 \u0026 ne \u0026 nm --\u003e y1_[\"y‚ÇÅ\"] \u0026 ye_[\"‚ãÆ\"] \u0026 ym_[\"y‚Çò\"] subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 h2 \u0026 he1 \u0026 hL end subgraph elm[\"ELM-learning layer\"] n1((\"ùõÇ‚Çï‚ÇÅ,^‚ô≠‚Çï\")) \u0026 n2((\"ùõÇ‚Çï‚ÇÇ,^‚ô≠‚Çï\")) \u0026 ne((\"‚ãÆ\")) \u0026 nm((\"ùõÇ‚Çï‚Çò,^‚ô≠‚Çï\")) end Objective of representation learning: Represent the input features meaningfully in several different representations as follows.\nRepresen-\ntation feat dim (ùëë) vs\nin-dim (ùëõ) feature target output Dimension Reduction ùëë \u0026lt; ùëõ H_f ‚àà ùêë·µà·ïÅ·¥π ùê≤=label (Supervise)\nor ùê≤=ùê± (Unsp~) Expanded Dimension ùëë \u0026gt; ùëõ H_f ‚àà ùêë·µà·ïÅ·¥π ùê≤=label (Supervise)\nor ùê≤=ùê± (Unsp~) The feature data is ùêá_f(ùê±‚Çñ, ^ùêö_f, ^b_f), where the weights of feature mapping layer ^ùêö ≤_f, j=1,\u0026hellip;,L belongs to ùêë·µà·ïÅ‚Åø.\nDefinition 1: Given a nonlinear piecewise continous activation function g, we call {(^ùêö ≤_f, ^b ≤_f)‚±º‚Çå‚ÇÅ·¥∏} (^ùêö ≤_f ‚àà ùêë·µà·ïÅ‚Åø) the ùêø optimal general hidden nodes and ùêá‚É∞ ‚É∞_f= ‚àë·µ¢‚Çå‚ÇÅ·¥∏ g(ùê±, ^ùêö ≤_f, ^b ≤_f) the optimal feature data if it satisfies: ‚Äñùêû_L‚Äñ ‚â§ min_{ùêá‚É∞·¥∏_f‚àà ùêë·µà·ïÅ·¥π} ( min_{ùêö‚Çï‚àà ùêë ·µê·ïÅ·µà} ‚Äñùê≤-u‚Çï‚Åª¬π g(ùêá‚É∞·¥∏_f, ùêö‚Çï, b‚Çï)‚Äñ ) (4)\nwhere ùêû_L = ‚Äñùê≤-u‚Çï‚Åª¬π g(ùêá‚É∞·¥∏ ‚É∞_f, ^ùêö‚Çï, ^b‚Çï)‚Äñ and sequence ‚Äñùêû_L‚Äñ is decreasing and bounded below by zero.\nRemark 1: If the optimal projecting parameters are obtained in the feature mapping layer {(^ùêö ≤_f, ^b ≤_f)‚±º‚Çå‚ÇÅ·¥∏} (where ^ùêö_f ‚àà ùêë·µà·ïÅ‚Åø), the original n-dimension data points ùê± will be converted to d-dimension data points: ùêá‚É∞_f= ‚àë‚±º‚Çå‚ÇÅ·¥∏ g(ùê±‚Çñ, ^ùêö ≤_f, ^b ≤_f), which satisfy the inequality (4).\nThus the purpose is to find optimal projecting parameters that make the inequality (4) true for all data points.\n2) Learning Steps Based on the inverse of the activation function.\nGiven M arbitrary distinct training samples {(ùê±‚Çñ,ùê≤‚Çñ)‚Çñ‚Çå‚ÇÅ·¥π}, ùê±‚Çñ‚àà ùêë‚Åø, ùê≤‚Çñ‚àà ùêë·µê, which are sampled from a continuous system.\nSet j=1 to initialize a general node of the feature mapping layer randomly as: ùêá‚É∞ ≤_f = g(^ùêö ≤_f‚ãÖùê± + ^b ≤_f), (^ùêö ≤_f)·µÄ‚ãÖ^ùêö ≤_f=ùêà, (^b ≤_f)·µÄ‚ãÖ^b ≤_f=1,\nwhere ^ùêö ≤‚àà ùêë·µà·ïÅ‚Åø, ^b ≤_f‚àà ùêë is the orthogonal random weight and bias of feature mapping layer. ùêá‚É∞ ≤_f is current feature data.\nGiven a sigmoid or sine activation function g, for any continous desired outputs ùê≤, the parameters in the (general) ELM learning layer are obtained as:\n^ùêö‚Çï = g‚Åª¬π(u‚Çô(ùê≤)) ‚ãÖ (ùêá‚É∞ ≤_f)‚Åª¬π, ^ùêö ≤‚Çï‚àà ùêë·µà·ïÅ·µê, ^b‚Çï = ‚àömse(^ùêö‚Çï ≤ ‚ãÖ ùêá‚É∞ ≤_f - g‚Åª¬π(u‚Çô(ùê≤)) ), ^b ≤‚Çô‚àà ùêë, $g‚Åª¬π(‚ãÖ) = \\{^{arcsin(‚ãÖ) \\quad if\\ g(‚ãÖ)=sin(‚ãÖ)}_{-log(1/(‚ãÖ)-1) \\quad if\\ g(‚ãÖ) = 1/(1+e‚Åª‚ÅΩÀô‚Åæ)}$, _ where ùêá‚É∞‚Åª¬π = ùêá‚É∞·µÄ( (C/ùêà) + ùêá‚É∞ ùêá‚É∞·µÄ)‚Åª¬π; C is a positive value; u‚Çô is a normalized function u‚Çô(ùê≤): ùêë‚ûî(0,1]; g‚Åª¬π and u‚Çô‚Åª¬π represent their reverse function.\nUpdate the output error ùêû‚±º as ùêû‚±º = ùê≤ - u‚Çô‚Åª¬π g(ùêá‚É∞ ≤_f, ^ùêö‚Çï, ^b‚Çï) So the error feedback data is ùêè‚±º = g‚Åª¬π(u‚Çô(ùêû‚±º))‚ãÖ(^ùêö‚Çï)‚Åª¬π\nSet j=j+1, add a new general node (^ùêö ≤_f, ^b ≤_f) in the feature mapping layer by\n^ùêö ≤_f = g‚Åª¬π( u‚±º(ùêè‚±º‚Çã‚ÇÅ) ) ‚ãÖ ùê±‚Åª¬π, ^ùêö ≤_f‚àà ùêë‚Åø·ïÅ·µà ^b ≤_f = ‚àömse(^ùêö ≤_f ‚ãÖ ùê± - ùêè‚±º‚Çã‚ÇÅ), ^b ≤‚àà ùêë and update the feature data ùêá‚É∞ ≤_f = ‚àë·µ¢‚Çå‚ÇÅ ≤ u‚Çó‚Åª¬π g(ùê±, ^ùêöÀ°_f, ^bÀ°_f)\nRepeat step 2-4 ùêø-1 times. (Finally, ùêø nodes are added into feature mapping layer.) The set of parameters {^ùêö ≤_f,^b ≤_f}‚±º‚Çå‚ÇÅ·¥∏ are the optimal projecting parameters and the feature data ùêá‚É∞·¥∏_f = ‚àë‚±º‚Çå‚ÇÅ·¥∏ u‚±º‚Åª¬π g(ùê±, ^ùêö ≤_f, ^b ≤_f) = ùêá‚É∞ ‚É∞_f are the optimal feature data.\nC. Proof of the Proposed Method (Proof of Convergence)\nGiven M arbitrary distinct samples {(ùê±‚Çñ,ùê≤‚Çñ)}‚Çñ‚Çå‚ÇÅ·¥π (ùê±‚Çñ‚àà ùêë‚Åø, ùê≤‚Çñ‚àà ùêë·µê)\nLemma 2: Given a bounded nonconstant piecewise continuous activation function g, we have lim_{(ùõÇ,‚ô≠)‚Üí(^ùõÇ,^‚ô≠)} ‚Äñg(ùê±,ùõÇ,‚ô≠) - g(ùê±,^ùõÇ,^‚ô≠)‚Äñ = 0 where the (^ùõÇ,^‚ô≠) is one of the least-squares solutions of a general linear system ùõÇ‚ãÖùê±+‚ô≠.\nRemark 2:\nLemma 2 shows that SLFN training problem can be considered as finding optimal hidden parameters which satisfy: g(^ùõÇ‚ÇÅ,^‚ô≠‚ÇÅ) + \u0026hellip; + g(^ùõÇ_L,^‚ô≠_L) ‚Üí ùê≤. ùõÇ (alpha) stands for basic ELM hidden node.\nThus training an SLFN is equivalent to finding a least-square general input weight ^ùêö‚Çï of the (linear+activation) system g(^ùêö‚Çï‚ãÖùê±) = ùê≤.\nIf activation function g is invertible, the input weights matrix can be obtained by pulling back the residual error to the hidden layer.\nFor example, if g is a sine function,\nThe output of the hidden layer matrix is ùê≤=sin(ùêö‚Çï ‚ãÖ ùê±). Thus, ùêö‚Çï‚ãÖùê± = arcsin(ùê≤), ùê≤‚àà (0,1]. The smallest norm least-squares solution of the linear system sin(ùêö‚Çï‚ãÖùê±)=ùê≤ is: ^ùêö‚Çï = arcsin(ùê≤)‚ãÖùê±‚Åª¬π, where ùê±‚Åª¬π is the Moore-Penrose generalized inverse of matrix ùê±. ùê±‚Åª¬π = ùê±·µÄ( (C/ùêà) + ùê±ùê±·µÄ)‚Åª¬π Theorem 1: Given M arbitrary distinct samples {(ùê±·µ¢,ùê≤·µ¢)·µ¢‚Çå‚ÇÅ·¥π}, (ùê±·µ¢‚àà ùêë‚Åø, ùê≤·µ¢‚àà ùêë·µê) and a sigmoid or sine activation function g, for any continuous desired outputs ùê≤, we have:\nthe optimal weights ^ùêö‚Çï = argmin_{ùêö‚Çï‚àà ùêë·µê·ïÅ‚Åø} ‚Äñu‚Åª¬π(g(ùê±,ùêö‚Çï)) - ùê≤‚Äñ least square error ‚Äñg(ùê±,^ùêö‚Çï,^b‚Çï) - ùê≤‚Äñ ‚â§ min_{ùêö‚Çï‚àà ùêë·µê·ïÅ‚Åø} ‚Äñu‚Åª¬π(g(ùêö‚Çï‚ãÖùê±)) - ùê≤‚Äñ if the parameters are obtained by (similar to Algorithm step-2):\n^ùêö‚Çï = g‚Åª¬π( u(ùê≤))‚ãÖùê±‚Åª¬π, ^ùêö‚Çï ‚àà ùêë·µê·ïÅ‚Åø ^b‚Çï = ‚àömse(^ùêö‚Çï‚ãÖùê± - g‚Åª¬π(u(ùê≤))), ^b‚Çï‚àà ùêë $g‚Åª¬π(‚ãÖ) = \\{^{arcsin(‚ãÖ) \\quad if\\ g(‚ãÖ)=sin(‚ãÖ)}_{-log(1/(‚ãÖ)-1) \\quad if\\ g(‚ãÖ) = 1/(1+e‚Åª‚ÅΩÀô‚Åæ)}$, _ Proof:\nLet ùõå=ùêö‚Çï‚ãÖùê±, and ùõå satisfy g(ùõå) = ùê≤. Normalizing ùê≤ to (0,1] by u(ùê≤) to let ùõå‚àà ùêë. Thus, for a sine hidden node, ùõå = g‚Åª¬π(u(ùê≤)) = arcsin(u(ùê≤)). While for a sigmoid hidden node, ùõå = g‚Åª¬π(u(ùê≤)) = -log(1/u(ùê≤) - 1).\n^ùêö‚Çï is the solution for the linear system (g(ùêö‚Çï‚ãÖùê±)=ùê≤). For sine activation: ^ùêö‚Çï = g‚Åª¬π( u(ùê≤) )‚ãÖùê±‚Åª¬π = arcsin(u(ùê≤))‚ãÖùê±‚Åª¬π. For sigmoid activation: ^ùêö‚Çï = g‚Åª¬π( u(ùê≤) )‚ãÖùê±‚Åª¬π = -log(1/u(ùê≤) - 1)‚ãÖùê±‚Åª¬π\nOne of the least-squares solutions of a general linear system ùêö‚Çï‚ãÖùê±=ùõå is ^ùêö‚Çï = g‚Åª¬π( u(ùê≤) )‚ãÖùê±‚Åª¬π, which means the smallest error can be reached by this solution: ‚Äñ^ùêö‚Çï‚ãÖùê± -ùõå‚Çô‚Äñ = min_{ùêö‚Çï‚àà ùêë·µê·ïÅ‚Åø} ‚Äñùêö‚Çï‚ãÖùê± - g‚Åª¬π( u(ùê≤) )‚Äñ (18)\nThe special solution ^ùêö‚Çï = g‚Åª¬π( u(ùê≤) )‚ãÖùê±‚Åª¬π has the smallest norm among all the least-squares solutions of ùêö‚Çï‚ãÖùê± = ùõå. The error can be further reduced by adding bias b‚Çô: ^b‚Çï = ‚àömse(^ùêö‚Çï‚ãÖùê± - h‚Åª¬π( u(ùê≤) ))\nBased on eq. (18) and Lemma2, optimization by minimizing the L2-loss can be reformulated as: min_{ùêö‚Çï‚àà ùêë·µê·ïÅ‚Åø} ‚Äñu‚Åª¬π( g(ùêö‚Çï‚ãÖùê±) ) - u‚Åª¬π( g(ùõå))‚Äñ = ‚Äñu‚Åª¬π( g(^ùêö‚Çï‚ãÖùê±) ) - u‚Åª¬π( g(ùõå))‚Äñ ‚â• ‚Äñu‚Åª¬π( g(^ùêö‚Çï‚ãÖùê± + ^b‚Çï) ) - ùê≤‚Äñ (20)\nBased on eq. (18) and eq. (20), the optimal weights is proved as: ^a‚Çï = arg min_{ùêö‚Çï‚àà ùêë·µê·ïÅ‚Åø} ‚Äñg(ùê±,ùêö‚Çï) - ùê≤‚Äñ And it satisfy: ‚Äñg(ùê±,^ùêö‚Çï,^b‚Çï) - ùê≤‚Äñ ‚â§ min_{ùêö‚Çï‚àà ùêë·µê·ïÅ‚Åø} ‚Äñu‚Åª¬π( g(^ùêö‚Çï‚ãÖùê±) ) - ùê≤ ‚Äñ\nBased on Lemma 2 and Theorem 1, Theorem 2 is given:\nTheorem 2: Given M arbitrary distinct samples (ùê±, ùê≤), ùê±‚àà ùêë‚Åø·ïÅ·¥π, ùê≤‚àà ùêë·µê·ïÅ·¥π, a sigmoid or sine activation function g, and the initial orthogonal random weights ^ùêö¬π_f and bias ^b¬π_f. For any continuous desired output ùê≤, the optimal feature data is: ùêá‚É∞·¥∏‚É∞ _f(ùê±, (^ùêö¬π_f, \u0026hellip;, ^ùêö·¥∏_f), (^b¬π_f,\u0026hellip;,^b·¥∏_f)) = ‚àë‚±º‚Çå‚ÇÅ·¥∏ u‚±º‚Åª¬π g(^ùêö ≤_f ‚ãÖ ùê± + ^b ≤_f) which satisfy: ‚Äñùêû_L‚Äñ ‚â§ min_{^ùêö ≤_f‚àà ùêë‚Åø·ïÅ·µà} ( min_{ùêö‚Çï‚àà ùêë ·µê·ïÅ·µà} ‚Äñùê≤-u‚Çô‚Åª¬π g(ùêá‚É∞·¥∏_f, ùêö‚Çï, b‚Çï)‚Äñ) (21)\nand ‚Äñùêû_L‚Äñ is decreasing and bounded below by zero if these parameters are obtained by:\nùêá‚É∞ ≤_f = ‚àë·µ¢‚Çå‚ÇÅ ≤ u·µ¢‚Åª¬π g(ùê±, ^ùêö‚Å±_f, ^b‚Å±_f), ^ùêö‚Çï = g‚Åª¬π(u‚Çô(ùê≤)) ‚ãÖ (ùêá‚É∞ ≤_f)‚Åª¬π, ^ùêö‚Çï‚àà ùêë ·µê·ïÅ·µà, ^b‚Çï = ‚àömse(^ùêö‚Çï‚ãÖùêá‚É∞ ≤_f - g‚Åª¬π( u(ùê≤) )), ^b‚Çï‚àà ùêë g‚Åª¬π(‚ãÖ) = \\{^{arcsin(‚ãÖ) \\quad if\\ g(‚ãÖ)=sin(‚ãÖ)}_{-log(1/(‚ãÖ)-1) \\quad if\\ g(‚ãÖ) = 1/(1+e‚Åª‚ÅΩÀô‚Åæ)}$, _ ùêû‚±º = ùê≤ - u‚Çô‚Åª¬π( g(ùêá‚É∞ ≤_f, ^ùêö‚Çï, ^b‚Çï), ùêè‚±º = g‚Åª¬π(u‚Çô(ùêû‚±º))‚ãÖ(^ùêö‚Çï)‚Åª¬π ), ^ùêö ≤_f = g‚Åª¬π(u‚±º(ùêè‚±º‚Çã‚ÇÅ)) ‚ãÖ ùê±‚Åª¬π, ^ùêö ≤‚àà ùêë‚Åø·ïÅ·µà ^b ≤_f = ‚àömse(^ùêö ≤_f ‚ãÖ ùê± - ùêè‚±º‚Çã‚ÇÅ), ^b ≤_f‚àà ùêë Proof:\nBase on Theorem 1, the validity of (21) is obvious. So here, we just prove that the error ‚Äñùêû_L‚Äñ is decreasing and bounded below by zero.\nLet Œî = ‚Äñe‚±º‚Çã‚ÇÅ‚Äñ¬≤ - ‚Äñùê≤ - u‚Çô‚Åª¬πg(ùêá‚É∞ ≤_f, ^ùêö‚Çï, ^b‚Çï)‚Äñ¬≤ (last error-current output), and take the newest item apart: = ‚Äñe‚±º‚Çã‚ÇÅ‚Äñ¬≤ - ‚Äñùê≤ - u‚Çô‚Åª¬πg( (‚àë·µ¢‚Çå‚ÇÅ ≤‚Åª¬π u·µ¢‚Åª¬π g(ùê±, ^ùêö ≤_f, ^b ≤_f) + u·µ¢‚Åª¬πg(ùê±, ^ùêö ≤_f, ^b ≤_f) ), ^ùêö‚Çï, ^b‚Çï) ‚Äñ¬≤ (24)\nLet ^T ≤ = u‚Çô‚Åª¬πg(u‚±º‚Åª¬πg(ùê±, ^ùêö ≤_f, ^b ≤_f), ^ùêö‚Çï, ^b‚Çï). Because activation function is sigmoid or sine function, eq. (24) can be simplified as: Œî ‚â• ‚Äñùêû‚±º‚Çã‚ÇÅ‚Äñ¬≤ - ‚Äñùê≤ - u‚Çô‚Åª¬πg( (‚àë·µ¢‚Çå‚ÇÅ ≤‚Åª¬π u·µ¢‚Åª¬π g(ùê±, ^ùêö ≤_f, ^b ≤_f) ), ^ùêö‚Çï, ^b‚Çï) - ^T ≤‚Äñ¬≤ = ‚Äñùêû‚±º‚Çã‚ÇÅ‚Äñ¬≤ - ‚Äñùêû‚±º‚Çã‚ÇÅ - ^T ≤‚Äñ¬≤ (unfold) = ‚Äñùêû‚±º‚Çã‚ÇÅ‚Äñ¬≤ - (‚Äñùêû‚±º‚Çã‚ÇÅ‚Äñ¬≤ - 2\u0026lt;e‚±º‚Çã‚ÇÅ, ‚Äñ^T ≤‚Äñ\u0026gt; + ‚Äñ^T ≤‚Äñ¬≤) (\u0026quot;\u0026lt;\u0026gt;\u0026quot; is dot product of 2 matrices: Frobenius inner product)\n= 2\u0026lt;ùêû‚±º‚Çã‚ÇÅ, ‚Äñ^T ≤‚Äñ\u0026gt; - ‚Äñ^T ≤‚Äñ¬≤ = ‚Äñ^T ≤‚Äñ¬≤ ( 2\u0026lt;ùêû‚±º‚Çã‚ÇÅ, ‚Äñ^T ≤‚Äñ\u0026gt; / ‚Äñ^T ≤‚Äñ¬≤ - 1 ) (25)\nWe set ^T ≤ = u‚Çô‚Åª¬πg(u‚±º‚Åª¬πg( ùê±, ^ùêö ≤_f, ^b ≤_f) ), ^ùêö‚Çï, ^b‚Çï ) = ùêû‚±º‚Çã‚ÇÅ ¬± œÉ. (œÉ is variance, and ùêû‚±º‚Çã‚ÇÅ is the expectation) So ùêû‚±º‚Çã‚ÇÅ = ^T ≤ ¬± œÉ. Then \u0026lt;ùêû‚±º‚Çã‚ÇÅ, ‚Äñ^T ≤‚Äñ\u0026gt; = \u0026lt;^T ≤¬± œÉ, ‚Äñ^T ≤‚Äñ\u0026gt; = \u0026lt;‚Äñ^T ≤‚Äñ¬≤ ¬± \u0026lt;‚Äñ^T ≤‚Äñ,œÉ\u0026gt; \u0026gt;\nHence, eq. (25) can be reformulated: Œî ‚â• ‚Äñ^T ≤‚Äñ¬≤ ( 2\u0026lt; ‚Äñ^T ≤‚Äñ¬≤ ¬± \u0026lt;‚Äñ^T ≤‚Äñ,œÉ\u0026gt; \u0026gt; / ‚Äñ^T ≤‚Äñ¬≤ - 1 ) = ‚Äñ^T ≤‚Äñ¬≤ ( 1 ¬± 2‚ÄñœÉ‚ãÖ(^T ≤)·µÄ‚Äñ/‚Äñ^T ≤‚Äñ¬≤) (Wandong thinks there should be a 2.) ‚â•\nIn addition, based on Theorem 1 and eq. (7), there will be:\n‚Äñ^T ≤ - ùêû‚±º‚Çã‚ÇÅ‚Äñ ‚â§ min_{^ùêö ≤_f‚àà ùêë·µà·ïÅ‚Åø} ‚Äñu‚Çô‚Åª¬πg(u‚±º‚Åª¬πg( ùê±, ^ùêö ≤_f, ^b ≤_f), ^ùêö‚Çï, ^b‚Çï) -ùêû‚±º‚Çã‚ÇÅ‚Äñ ‚ÄñœÉ‚Äñ ‚â§ ‚Äñ^T ≤‚Äñ Thus Œî ‚â• 0 can be proved as: Œî ‚â• ‚Äñ^T ≤‚Äñ¬≤ (1 ¬± ‚ÄñœÉ‚Äñ / ‚Äñ^T ≤‚Äñ) ‚â• 0 (28)\nEq. (28) means ‚Äñùêû‚±º‚Çã‚ÇÅ‚Äñ ‚â• ‚Äñùêû‚±º‚Äñ and ‚Äñùêû‚Äñ is decreasing and bounded below by zero.\nBased on Theorem 2, Theorem 3 is given:\nTheorem 3: Given M arbitrary distinct samples (ùê±, ùê≤), ùê±‚àà ùêë‚Åø·ïÅ·¥π, ùê≤‚àà ùêë·µê·ïÅ·¥π, a sigmoid or sine activation function g, and optimal feature data ùêá‚É∞·¥∏_f obtained by Algorithm 1, then lim_{j‚ûù+‚àû} ‚Äñùê≤ - Œ≤‚ÇÅ‚ãÖu‚ÇÅ‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚ÇÅ, ùëè‚ÇÅ) - \u0026hellip; - Œ≤‚±º‚ãÖu‚±º‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚±º, ùëè‚±º)‚Äñ = 0 holds with probability one if :\nùêö‚±º = g‚Åª¬π( u(ùê≤) ) ‚ãÖ (ùêá‚É∞·¥∏_f)‚Åª¬π, ^ùêö‚±º‚àà ùêë·µê·ïÅ‚Åø b‚±º = ‚àömse(^ùêö‚±º‚ãÖ(ùêá‚É∞·¥∏_f) - g‚Åª¬π(u(ùê≤))), ^b‚±º‚àà ùêë Œ≤‚±º = ‚ü®ùêû‚±º‚Çã‚ÇÅ, g(ùêá‚É∞·¥∏_f, ùêö‚±º, b‚±º)‚ü© / ‚Äñg(ùêá‚É∞·¥∏_f, ùêö‚±º, b‚±º)‚Äñ¬≤, Œ≤‚±º‚àà ùêë Proof:\nFirst prove that the sequence ‚Äñùêû‚±º·¥∏‚Äñ is decreasing and bounded below by zero. Then prove that the lim_{j‚ûù+‚àû} ‚Äñùêû‚±º·¥∏‚Äñ = 0\nBased on Theorem 1 and Lemma 1, the network output error satisfies: ‚Äñùêû‚±º·¥∏‚Äñ = ‚Äñùê≤ - Œ≤‚ÇÅ‚ãÖu‚ÇÅ‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚ÇÅ, ùëè‚ÇÅ) - \u0026hellip; - Œ≤‚±º‚ãÖu‚±º‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚±º, ùëè‚±º)‚Äñ ‚â§ ‚Äñùê≤ -u‚ÇÅ‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚ÇÅ, ùëè‚ÇÅ)‚Äñ = ‚Äñùêû‚ÇÅ·¥∏‚Äñ\nBased on Theorem 2, there will be: ‚Äñùêû‚±º·¥∏‚Äñ ‚â§ ‚Äñùêû‚±º·¥∏‚Åª¬π‚Äñ ‚â§ \u0026hellip; ‚â§ ‚Äñùêû‚±º¬π‚Äñ\nThus, ‚Äñùêû‚±º·¥∏‚Äñ ‚â§ ‚Äñùêû‚±º·¥∏‚Åª¬π‚Äñ ‚â§ \u0026hellip; ‚â§ ‚Äñùêû‚±º¬π‚Äñ ‚â§ \u0026hellip; ‚â§ ‚Äñùêû‚ÇÅ¬π‚Äñ and ‚Äñùêû‚±º·¥∏‚Äñ is decreasing and bounded below by 0.\nBased on Lemma 1, when all hidden nodes randomly generated based on any continuous sampling distribution, lim_{n‚ûù‚àû} ‚Äñf - (f‚Çô‚Çã‚ÇÅ + Œ≤‚Çô‚ãÖg(ùê±, ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥) )‚Äñ = 0 holds with probability one if Œ≤‚Çô = ‚ü®ùêû‚Çô‚Çã‚ÇÅ, g(ùêá‚É∞·¥∏‚±º, ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥)‚ü© / ‚Äñg(ùêá‚É∞·¥∏‚±º, ùõÇ‚Çô ≥, ‚ô≠‚Çô ≥)‚Äñ¬≤.\nIn addition, ELM theories have shown that almost any nonlinear piecewise continuous random hidden node can be use in ELM, and the resultant networks have universal approximation capbilities. According to the definition of general hidden neurons, (a general hidden node contains m (basic) hidden node), a general hidden node (ùêö,b) = (ùõÇ ≥‚ÇÅ, \u0026hellip;, ùõÇ ≥‚Çò, b ≥‚ÇÅ, \u0026hellip;, b ≥‚Çò), . Thus its output is g(ùêá‚É∞·¥∏_f, ùêö‚±º ≥, ‚ô≠‚±º ≥) ‚â° ‚àë·µ¢‚Çå‚ÇÅ·µê g(ùêá‚É∞·¥∏_f, ùõÇ ≥·µ¢, b ≥·µ¢).\nTherefore, lim_{j‚ûù‚àû} ‚Äñ ùê≤ - Œ≤‚ÇÅ‚ãÖu‚ÇÅ‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚ÇÅ, ùëè‚ÇÅ) - \u0026hellip; - Œ≤‚±º‚ãÖu‚±º‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚±º, ùëè‚±º)‚Äñ\n= lim_{n‚ûù‚àû} ‚Äñf- (f‚Çô‚Çã‚ÇÅ + Œ≤‚±º‚ãÖu‚±º‚Åª¬πg(ùêá‚É∞·¥∏_f, ùêö‚±º ≥, ùëè‚±º ≥)) ‚Äñ = 0\nD. Proposed Method With Multinetwork Structures %%{ init: { 'flowchart': { 'curve': 'basis' } } }%% flowchart LR subgraph in[\"input feature\"] x1((x1)) \u0026 xe((\"‚ãÆ\")) \u0026 xn((xn)) end subgraph net1[\"Layer 1\"] l11((a,b)) \u0026 l1e((\"‚ãÆ\")) \u0026 l1L((a,b)) end subgraph net2[\"Layer 2\"] l21((a,b)) \u0026 l2e((\"‚ãÆ\")) \u0026 l2L((a,b)) end subgraph netC[\"Layer C\"] lC1((a,b)) \u0026 lCe((\"‚ãÆ\")) \u0026 lCL((a,b)) end x1 \u0026 xn --\u003e l11 \u0026 l1L l11 \u0026 l1L --\u003e l21 \u0026 l2L l21 \u0026 l2L -.-\u003e lC1 \u0026 lCL subgraph out[\"output y\"] direction LR y1((1)) \u0026 ye((\"‚ãÆ\")) \u0026 ym((m)) end subgraph belm1[\"Basic ELM 1\"] direction LR b11((\"a‚Çï,b‚Çï\")) \u0026 b1e((\"‚ãÆ\")) \u0026 b1m((a‚Çï,b‚Çï)) end subgraph belm2[\"Basic ELM 2\"] direction LR b21((a‚Çï,b‚Çï)) \u0026 b2e((\"‚ãÆ\")) \u0026 b2m((a‚Çï,b‚Çï)) end net1 --\u003e feat1[\"Feature\\n data\\n ùêá¬πf\"] --\u003e belm1 --\u003e out feat1 --\u003e net2 --\u003e feat2[\"Feature\\n data\\n ùêá¬≤f\"] --\u003e belm2 --\u003e out netC --\u003e featC[\"Feature\\n data\\n ùêá·∂úf\"] subgraph MultiLayer ELM in \u0026 net1 \u0026 net2 \u0026 netC end %% inverse for initialization weights linkStyle 12,13,14,16,17,18 stroke:#f0f Pink links will do inverse to calculate the weights for corresponding layers.\n","date":"2023-01-18T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/","title":"read: Multilayer Subnetwork Nodes"},{"content":"When generating random numbers, tensorflow calls:\nDDG search: \u0026ldquo;_pywrap_tensorflow.TFE_Py_FastPathExecute()\u0026rdquo;\nSimilar function: \u0026ldquo;Tile\u0026rdquo; Question about how TensorFlow API link with C++ code - reddit. The TF API name Tile is used to map it to C++ class or function name by a table.\nSimilar function \u0026ldquo;MatMul\u0026rdquo; Where can I find exactly how Tensorflow does matrix multiplication? - reddit:\n1 _result = _pywrap_tensorflow.TFE_Py_FastPathExecute( _ctx._context_handle, _ctx._eager_context.device_name, \u0026#34;MatMul\u0026#34;, name, _ctx._post_execution_callbacks, a, b, \u0026#34;transpose_a\u0026#34;, transpose_a, \u0026#34;transpose_b\u0026#34;, transpose_b) Run \u0026rsquo;nm \u0026ndash;demangle\u0026rsquo; on _pywrap_tensorflow_internal.so grep for MatMul, and get: tensorflow::SparseMatMulOp file: \u0026ldquo;tensorflow/tensorflow/core/kernels/sparse_matmul_op.cc\u0026rdquo; code \u0026ldquo;ÁúãpythonÂà∞C++Ë∞ÉÁî®ÂÖ≥Á≥ª\u0026rdquo; tensorflow‰∫åÊ¨°ÂºÄÂèë - Ê≤âÊÄùËØ≠ÂΩï20190227. Take matmul as an exampleÔºö\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 me@Server:~$ cd /mnt/Server/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core/python me@Server:~/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core/python$ grep -rni \u0026#34;tf_export.*matmul\u0026#34; # Ëøô‰∏™ÂáΩÊï∞ÈúÄË¶ÅÁî® tf_export ÂØºÂá∫ ops/math_ops.py:2565:@tf_export(\u0026#34;linalg.matmul\u0026#34;, \u0026#34;matmul\u0026#34;) ops/math_ops.py:2859:tf_export(v1=[\u0026#34;sparse_matmul\u0026#34;])(sparse_matmul) ops/gen_nn_ops.py:10155:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBias\u0026#34;)(QuantizedMatMulWithBias) ops/gen_nn_ops.py:10306:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBiasAndRelu\u0026#34;)(QuantizedMatMulWithBiasAndRelu) ops/gen_nn_ops.py:10471:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize\u0026#34;)(QuantizedMatMulWithBiasAndReluAndRequantize) ops/gen_sparse_ops.py:3078:tf_export(\u0026#34;raw_ops.SparseTensorDenseMatMul\u0026#34;)(SparseTensorDenseMatMul) ops/gen_linalg_ops.py:2531:tf_export(\u0026#34;raw_ops.TridiagonalMatMul\u0026#34;)(TridiagonalMatMul) ops/linalg/linalg_impl.py:552:@tf_export(\u0026#39;linalg.tridiagonal_matmul\u0026#39;) ops/sparse_ops.py:2188:@tf_export(\u0026#34;sparse.sparse_dense_matmul\u0026#34;, ops/gen_math_ops.py:1618:tf_export(\u0026#34;raw_ops.BatchMatMul\u0026#34;)(BatchMatMul) ops/gen_math_ops.py:1726:tf_export(\u0026#34;raw_ops.BatchMatMulV2\u0026#34;)(BatchMatMulV2) ops/gen_math_ops.py:6150:tf_export(\u0026#34;raw_ops.MatMul\u0026#34;)(MatMul) ops/gen_math_ops.py:7610:tf_export(\u0026#34;raw_ops.QuantizedMatMul\u0026#34;)(QuantizedMatMul) ops/gen_math_ops.py:10010:tf_export(\u0026#34;raw_ops.SparseMatMul\u0026#34;)(SparseMatMul) Read the usage description at math_ops.py:2565. It calls gen_math_ops.batch_mat_mul or gen_math_ops.mat_mul.\nGo to tensorflow.python.ops/gen_math_ops.py (This file maybe generated when compiling.)\nThe function batch_mat_mul calls:\n1 2 3 4 _result = _pywrap_tensorflow.TFE_Py_FastPathExecute( _ctx._context_handle, _ctx._thread_local_data.device_name, \u0026#34;BatchMatMul\u0026#34;, name, _ctx.post_execution_callbacks, x, y, \u0026#34;adj_x\u0026#34;, adj_x, \u0026#34;adj_y\u0026#34;, adj_y) So the Op function in C++ should be \u0026ldquo;BatchMatMul\u0026rdquo;.\nSeach all the place registering this Op by searching the definition of op in the source code/repo:\n1 2 3 4 5 6 7 8 # Cannot find anything in the python package installed by conda # yi@PC:/mnt/Server/anaconda3/pkgs/tensorflow-base-1.15.0-gpu_py37h9dcbed7_0$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; # yi@PC:/mnt/Server/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; yi@PC:~/Downloads/tensorflow_1.15$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; tensorflow/core/ops/math_ops.cc:946:REGISTER_OP(\u0026#34;MatMul\u0026#34;) tensorflow/compiler/mlir/tfr/resources/decomposition_lib.mlir:83:// REGISTER_OP(\u0026#34;MatMul\u0026#34;) tensorflow/c/experimental/ops/README.md:15:since `REGISTER_OP(\u0026#34;MatMul\u0026#34;)` appears in ***core/math_ops.cc***, the \u0026#34;MatMul\u0026#34; Search the kernel implementation of this Op:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 yi@PC:~/Downloads/tensorflow_1.15$ grep -rni \u0026#34;Name(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; tensorflow/core/transforms/remapper/tests/contraction.mlir:38: %MatMul, %ctl_1 = MatMul(%Placeholder, %Const) device(\u0026#34;/device:CPU:0\u0026#34;) name(\u0026#34;MatMul\u0026#34;) {T = f32, transpose_a = false, transpose_b = false} : (tensor\u0026lt;*xf32\u0026gt;, tensor\u0026lt;*xf32\u0026gt;) -\u0026gt; (tensor\u0026lt;*xf32\u0026gt;) tensorflow/core/transforms/remapper/tests/onednn_contraction.mlir:76: %MatMul, %ctl_1 = MatMul(%Placeholder, %Const) device(\u0026#34;/device:CPU:0\u0026#34;) name(\u0026#34;MatMul\u0026#34;) {T = f32, transpose_a = false, transpose_b = false} : (tensor\u0026lt;*xf32\u0026gt;, tensor\u0026lt;*xf32\u0026gt;) -\u0026gt; (tensor\u0026lt;*xf32\u0026gt;) tensorflow/core/grappler/utils/pattern_utils_test.cc:42: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), input, weight); tensorflow/core/grappler/optimizers/remapper_test.cc:1225: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/remapper_test.cc:1433: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/remapper_test.cc:1610: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/mkl_remapper_test.cc:466: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), input, filter); tensorflow/core/grappler/optimizers/mkl_remapper_test.cc:667: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/constant_folding_test.cc:2909: Output matmul = ops::MatMul(scope.WithOpName(\u0026#34;matmul\u0026#34;), a, b); tensorflow/core/grappler/optimizers/arithmetic_optimizer_test.cc:1155: auto matmul_op = s.WithOpName(\u0026#34;matmul\u0026#34;); tensorflow/core/grappler/optimizers/arithmetic_optimizer_test.cc:1227: Output matmul = ops::BatchMatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), trans_a, trans_b); tensorflow/core/grappler/costs/analytical_cost_estimator_test.cc:79: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), flat, w2); tensorflow/core/kernels/matmul_op_test.cc:107: root.WithOpName(\u0026#34;matmul\u0026#34;), tensorflow/core/kernels/matmul_op_test.cc:126: root.WithOpName(\u0026#34;matmul\u0026#34;), tensorflow/core/kernels/mkl/mkl_fused_ops_test.cc:931: Output next_op = ops::MatMul(root.WithOpName(\u0026#34;matmul\u0026#34;), input_op, tensorflow/core/kernels/matmul_op_impl.h:881: Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_CPU).TypeConstraint\u0026lt;TYPE\u0026gt;(\u0026#34;T\u0026#34;), \\ tensorflow/core/kernels/matmul_op_impl.h:892: Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_GPU).TypeConstraint\u0026lt;TYPE\u0026gt;(\u0026#34;T\u0026#34;), \\ tensorflow/core/framework/op_kernel_test.cc:1062:REGISTER_KERNEL_BUILDER(Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_CPU), DummyKernel); tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:427: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), feed, const_1); tensorflow/compiler/tf2xla/kernels/matmul_op.cc:102:REGISTER_XLA_OP(Name(\u0026#34;MatMul\u0026#34;).TypeConstraint(\u0026#34;T\u0026#34;, kMatmulTypes), MatMulOp); DDG search: \u0026ldquo;How to read tensorflow c++ source code\u0026rdquo;\n","date":"2023-01-16T18:19:49-05:00","permalink":"https://zichen34.github.io/writenotes/lib/tf_randomuniform/","title":"memo: TF1.15 RandomUniform in C++"},{"content":"Normalize Dataset (2023-11-23)\nNormalizing a dataset means making the variance to 1.\nÊñπÂ∑ÆÊòØ‰∏Ä‰∏™Ê†áÈáèÔºàÂÅèÁ¶ªÊúüÊúõÁöÑÂπ≥ÊñπÁöÑÊúüÊúõÔºâ: the average of squared difference between each value in a dataset from the mean value.\nSquare ignores the sign of deviations, and only focuses on magnitudes.\nGiven 2 datasets with different scales: 1,2,6 and 10,20,60.\n1 2 -*--*--‚ñ≤------* V.S. -*--*--‚ñ≤------* 1 2 3 6 10 20 30 60 Their variance are 4.66 and 46.6. After normalization, their variance will both become 1. (Plain number, without any practical meaning.)\nTransform the variance to 1: subtract mean and divide by std for each sample.\n$$ Var = \\frac{‚àë(\\frac{x-Œº}{œÉ} - 0)¬≤}{n} = \\frac{‚àë\\frac{(x-Œº)¬≤}{œÉ¬≤}}{n} = \\frac{1}{œÉ¬≤} œÉ¬≤ = 1 $$\nThe consequences of normalization have 2 aspects:\nBecause all the squared deviations (x-Œº)¬≤ are scaled by their average: œÉ¬≤, the measuring unit is eliminated (Ê∂àÈô§ÈáèÁ∫≤). Hence, different attributes (dimensions) can be compared equally.\nOn the other hand, the sum of the scaled squared deviation becomes $n$. Thus, the new variance is 1.\n$$ Var = \\frac{‚àë\\frac{(x-Œº)¬≤}{œÉ¬≤}}{n} = \\frac{‚àë\\frac{(x-Œº)¬≤}{ \\frac{‚àë(x-Œº)¬≤}{n} }}{n} = \\frac{n ‚àë\\frac{(x-Œº)¬≤}{‚àë(x-Œº)¬≤} }{n} = \\frac{n* 1}{n} =1 $$\nAnalogy: $\\frac{a}{a+b+c} + \\frac{b}{a+b+c} + \\frac{c}{a+b+c}=1$ Variance can be scaled to any value. Taking 1 is for interpretability and simplifying calculations (?) (2023-01-14)\nBatchNorm BatchNorm: ÂâçÂêëËøáÁ®ã‰∏≠ÊØèÂ±ÇÁöÑËæìÂÖ•ÁöÑÂàÜÂ∏É‰∏ÄÁõ¥Âú®ÂèòÂåñÔºå‰∏çÊª°Ë∂≥Áã¨Á´ãÂêåÂàÜÂ∏ÉÔºåÂØºËá¥ÂÜÖÈÉ®ÂçèÂèòÈáèÂÅèÁßªInternal Covariate ShiftÈóÆÈ¢òÔºå ÊâÄ‰ª•ÂØπÊØèÂ±ÇÁöÑÊøÄÊ¥ªÂÄºÂú®ËæìÂÖ•‰∏ã‰∏ÄÂ±Ç‰πãÂâçÔºåÊääËøô‰∏Ä‰∏™batchÁöÑÂàÜÂ∏ÉË∞ÉÊï¥‰∏∫0ÂùáÂÄºÔºåÊñπÂ∑Æ‰∏∫1ÁöÑÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏ÉÔºå Âç≥Âõ∫ÂÆöÊØè‰∏™ÈöêÂ±ÇËäÇÁÇπÁöÑÊøÄÊ¥ªËæìÂÖ•ÂàÜÂ∏ÉÔºåËÆ©ËæìÂÖ•ÁöÑÊøÄÊ¥ªÂÄºËêΩÂú®ÊøÄÊ¥ªÂáΩÊï∞Ê¢ØÂ∫¶ËæÉÂ§ßÁöÑÂå∫ÂüüÔºåÂä†Âø´Êî∂Êïõ‰∏éÈÅøÂÖçÊ¢ØÂ∫¶Ê∂àÂ§±„ÄÇ Batch NormalizationÂØºËØª-Âº†‰øäÊûó-Áü•‰πé\n(2024-02-21) ‰∏çÊòØ‚ÄúÊ≠£ÊÄÅÂàÜÂ∏É‚ÄùÔºåÊ†áÂáÜÂåñ‰∏ç‰ºöÊîπÂèòÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÁ±ªÂûãÔºåÊï∞ÊçÆÂéüÊù•ÊòØ‰ªÄ‰πàÂàÜÂ∏ÉÔºåÂÅöÂÆå normalization ËøòÊòØ‰ªÄ‰πàÂàÜÂ∏É„ÄÇ (mean=0, sigma=1) Âè™ÊòØ‰∏Ä‰∏™ Ê†áÂáÜÔºå‰πãÂêéÁªèËøá‰∏Ä‰∫õËøêÁÆóÔºåÂàÜÂ∏ÉÁöÑÂèÇÊï∞ÂèëÁîüÂèòÂåñ‰∫ÜÔºåËøòÂèØ‰ª•ÂõûÂà∞Ëøô‰∏™Ê†áÂáÜ„ÄÇ todo: L11.2 How BatchNorm Works\ntodo: L11.4 Why BatchNorm Works\nNormalization layer ÂØπÊï∞ÊçÆÂáèÂùáÂÄºÔºåÈô§‰ª•Ê†áÂáÜÂ∑Æ„ÄÇ‰ª•‰∏ãÁöÑ N ÊòØ‰∏Ä‰∏™ batch ‰∏≠ÁöÑÊ†∑Êú¨‰∏™Êï∞(batch size, B)Ôºå ÂõæÁâábatchÔºö(N,C,H,W)ÔºåÂ∫èÂàóbatch: (N, embed_dim, seq_len)„ÄÇ 1d,2d,3dÊñπÊ≥ïÁöÑÂå∫Âà´Âú®‰∫é input ÁöÑÁª¥Â∫¶„ÄÇ Learnable parameters ÊòØŒ≥ Âíå Œ≤ Áî®‰∫éÂØπ ŒºÂíåœÉ ÂÅö affine ÂèòÊç¢„ÄÇ running mean Âíå running var Âú®ËÆ≠ÁªÉÊó∂‰∏çÊñ≠‰ΩøÁî®Âú®ÂâçÂêëÊó∂ÂæóÂà∞ÁöÑ x ÁöÑmean Âíå var ÂÅöÂä†ÊùÉÊõ¥Êñ∞ÔºåÊùÉÈáç‰∏∫Âä®Èáè momentum\nBatchNorm ËÆ©1‰∏™batchÁöÑÔºåÊØè‰∏™ÈÄöÈÅìÁöÑÂùáÂÄº‰∏∫0ÔºåÊñπÂ∑Æ‰∏∫1ÔºàÊï¥‰∏™batchÁöÑŒº‰πü=0ÔºâÔºõ ÊåáÂÆöÈÄöÈÅìÊï∞Èáèinput.shape[1]: bn=nn.BatchNorm1d(num_features=C)\nLayerNorm ËÆ©ÊØè‰∏™Ê†∑Êú¨ÁöÑÂÖ®ÈÉ®ÈÄöÈÅìÔºàÊúÄÂêéÂá†Áª¥Ôºå‰∏Ä‰∏™ÂçïËØçÁöÑÁâπÂæÅÂêëÈáèÔºå‰∏ÄÂπÖÂõæÁâáÁöÑfeature mapÔºâÁöÑÂùáÂÄº‰∏∫0ÔºåÊñπÂ∑Æ‰∏∫1ÔºålayerÊåáÁöÑÊòØ fc net ÁöÑ‰∏ÄÂ±ÇÔºõ\nÊåáÂÆöÊúÄÂêéÂá†Áª¥: ln=nn.LayerNorm(normalized_shape=[C,H,W])Ôºâ\nInstanceNorm ËÆ©ÊØè‰∏™Ê†∑Êú¨ÁöÑÊØè‰∏™ÈÄöÈÅìÁöÑÂùáÂÄº‰∏∫0ÔºåÊñπÂ∑Æ‰∏∫1ÔºåÂêå‰∏ÄbatchÂÜÖÁöÑÊ†∑Êú¨Ê≤°ÊúâËÅîÁ≥ªÔºåÁî®‰∫éÈ£éÊ†ºËøÅÁßªÔºõ\nÊåáÂÆöÈÄöÈÅìÊï∞Èáè: in=nn.InstanceNorm1d(num_features=C)\nGroupNorm ‰ªã‰∫é LayerNorm Âíå Instance Norm‰πãÈó¥ÔºåÈÄöÈÅìÂàÜÁªÑ=1 Â∞±ÊòØLNÔºåÈÄöÈÅìÂàÜÁªÑ=C Â∞±ÊòØINÔºõ\nÊåáÂÆöÈÄöÈÅìÂàÜÁªÑ: gn=nn.GroupNorm(num_groups=2, num_channels=C) lecture 7b Á•ûÁªèÁΩëÁªúÁöÑËÆ≠ÁªÉ(ÂΩí‰∏ÄÂåñÔºåËøÅÁßªÂ≠¶‰π†Ôºâ-ranchlai-bili; github repo\nWu\u0026He ECCV2018 Weight Norm\n(2023-08-03)\nLayerNorm Customize LayerNorm, as referring to ConvNeXt-meta\u0026rsquo;s nn.LayerNorm, can only normalized the last few channels.\nF.layer_norm can be used to normlize one of the middle dimensions, as in ConvNeXt-torchvision\nLoop of ChatGPT\u0026rsquo;s Wrong Solutions:\nQ: In the following code, why is the grad of self.weight None after loss.backward()?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class LayerNorm(nn.Module): r\u0026#34;\u0026#34;\u0026#34; Input data: (batch_size, C, H, W) Use F.layer_norm to do normalization for the dim of channels. \u0026#34;\u0026#34;\u0026#34; def __init__(self, eps=1e-6): super().__init__() self.normalized_shape = (4, ) # C = 3, placeholder self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) self.eps = eps def forward(self, x): r\u0026#34;\u0026#34;\u0026#34; Since the `normalized_shape` (#channels) of x is unknown when initializing the model, implement normalization for the channels dimension here. \u0026#34;\u0026#34;\u0026#34; if self.normalized_shape != (x.shape[1],): self.weight = nn.Parameter(torch.ones(x.shape[1])) self.bias = nn.Parameter(torch.zeros(x.shape[1])) mean = torch.mean(x, dim=1, keepdim=True) var = torch.var(x, dim=1, correction=0, keepdim=True) # biased estimation x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x x = torch.ones(1,3,224,224) myLN = LayerNorm() myLN.weight.register_hook(lambda grad: print(grad.shape)) normalized_x = myLN(x) target_x = torch.rand(1,3,224,224) loss = nn.MSELoss()(normalized_x, target_x) loss.backward() Answer:\nThe redefined variables self.weight, self.bias inside the forward method are not part of the computation graph, so their gradients won\u0026rsquo;t be computed during the backward pass.\nAnd the new created parameter tensors have no connection to the original self.weight and self.bias (created during initialization), which are not used in the forward pass. so their grad attribute remain None.\nFinally, he suggested me don\u0026rsquo;t redefine in the forward.\nQ: But I don\u0026rsquo;t have information about the shape of self.weight to initialize it inside the init method.\nAnswer:\nInitialize them as None in the __init__ method and then initialize them with the correct shape during the first forward pass.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class LayerNorm(nn.Module): def __init__(self, eps=1e-6): super().__init__() self.eps = eps self.normalized_shape = None # Placeholder for the shape of self.weight self.weight = None # Placeholder for the weight parameter self.bias = None # Placeholder for the bias parameter def forward(self, x): if self.normalized_shape is None: # Initialize self.normalized_shape and create weight and bias with the correct shape self.normalized_shape = (x.shape[1],) self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) print(self.bias.shape) mean = torch.mean(x, dim=(2, 3), keepdim=True) var = torch.var(x, dim=(2, 3), correction=0, keepdim=True) x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x This code has an ValueError: optimizer got an empty parameter list\nAnwser:\nApologies for the oversight. The error is happening because the LayerNorm class does not define any learnable parameters (weights and biases) in its __init__ method. As a result, when you try to pass myLN.parameters() to the optimizer, it doesn\u0026rsquo;t find any parameters to optimize, leading to the \u0026ldquo;optimizer got an empty parameter list\u0026rdquo; error.\nIn order to use an optimizer like Adam, you need to define at least one learnable parameter within your model that will be optimized during training.\nSince you don\u0026rsquo;t have information about the shape of the input during initialization, you can create a placeholder for self.weight and self.bias in the __init__ method, and then initialize them with the correct shape during the first forward pass. To do this, we can use the register_buffer method, which allows us to create tensors that are not optimized during training but still part of the model.\nHere\u0026rsquo;s the updated code with the use of register_buffer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class LayerNorm(nn.Module): def __init__(self, eps=1e-6): super().__init__() self.eps = eps self.register_buffer(\u0026#39;normalized_shape\u0026#39;, torch.tensor([])) self.register_buffer(\u0026#39;weight\u0026#39;, torch.tensor([])) self.register_buffer(\u0026#39;bias\u0026#39;, torch.tensor([])) def forward(self, x): if self.normalized_shape.numel() == 0: # Initialize self.normalized_shape and create weight and bias with the correct shape self.normalized_shape = torch.tensor([x.shape[1]]) self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) print(self.bias.shape) mean = torch.mean(x, dim=(2, 3), keepdim=True) var = torch.var(x, dim=(2, 3), correction=0, keepdim=True) x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x Now the model will contain learnable parameters that can be optimized using an optimizer like Adam. The grad attribute of self.weight should be available and updated after loss.backward().\nAdditionally, you can also use torch.nn.init functions to initialize the weight and bias tensors in a more standard way. For example, you can use torch.nn.init.ones_ and torch.nn.init.zeros_ to initialize the self.weight and self.bias tensors, respectively.\nregister_buffer doesn\u0026rsquo;t add parameters to optimizer to optimize neither.\nIt may be necessary to know the number of channels.\nJust sum up the attribute out_chnnels of each layer, e.g., alexnet.features[0].out_channels. PyTorch Forum\nBut Swin transformer doesn\u0026rsquo;t have Conv2d.\nMaybe I can only hard code the numbers of channels.\nIf you add the norm layer following the convolution layer, the normalized_shape is the out_channels of the convolution layer.\nBut here, I want to normalize the stacked extracted feature maps. So I need to know the total channels of the incoming feature maps.\nIf the #channels is known, I can use F.layer_norm() directly.\n(2023-10-17)\nAbsorb BN Normalization is performed to keep each \u0026ldquo;stage\u0026rdquo; (layer) in a network maintain the same distribution, such that the training could be more efficient due to\nLeaving out modeling distributions shift of each layer\u0026rsquo;s input data.\nStabilizing optimization by suppressing \u0026ldquo;outliers\u0026rdquo; (high value) in the output feature map of each layer to avoid big variations in the final prediction.\nRefer to: BN„ÄÅBNÂêåÊ≠•„ÄÅÂê∏Êî∂BN - EveKÁöÑÊñáÁ´† - Áü•‰πé\nSpecifically, input data has (mean=0, std=1), but after a conv layer, the outcome feature maps may don\u0026rsquo;t persist (mean=0, std=1).\nIdeally, featue maps\u0026rsquo;s distribution should be transformed to (mean=0, std=1) to align with the input data. And that transformation requires its whitening matrix , which transforms co-variance matrix of the feature maps an identity matrix, meaning each dimension is unrelated. CSDN\nHowever, solving the whitening matrix for a high-dimension tensor is time consuming.\nTherefore, the \u0026ldquo;whitening\u0026rdquo; is performed only for each channel.\nOn the other hand, it\u0026rsquo;s difficult to get the exact distributions at once, because the data is trained batch-by-batch.\nTherefore, the distribution to be corrected pertains only to the small batch of data. (Or involving previous data by using moving average of history mean and std)\nAnd only 2 parameters (mean, std) of their distribution are considered to keep the distributions consistent.\nOnce a feature map $M_{(N, C, H, W)}$ spit out from a conv layer, BatchNorm normalizes the data at the same channel for all N samples in the batch:\n\\begin{algorithm} \\begin{algorithmic} \\FOR {i=0 \\TO M.size(1)} \\PROCEDURE{BN}{ M, mean ∞, std ∞, scale, bias} \\STATE mean = M[:, i, :, :].sum() / N \\STATE std = ( (M[:, i, :, :] - mean)¬≤ / N ).sqrt() \\STATE M‚Å∞¬π = (M[:, i, :, :] - mean) / std \\STATE M' = scale * M‚Å∞¬π + bias \\ENDPROCEDURE \\ENDFOR \\end{algorithmic} \\end{algorithm} mean ∞ and std ∞ are from history.\nThe scale factor Œ≥ and bias Œ≤ are for the situation where the variation is very small. Then the differences can be magnified through scaling to avoid representation capacity degradation.\nAnd Œ≥,Œ≤ are required to be learnable to automatically find the appropriate feature levels. Otherwise, bias will grow to infinity as explained in paper.\nCombine BN into wights and bias of the last layer\n$$ M\u0026rsquo; = Œ≥ \\frac{M - mean}{std} + Œ≤ = \\frac{Œ≥‚ãÖM}{std} \\left( Œ≤ - \\frac{Œ≥ ‚ãÖ mean}{std} \\right) $$\nIf M is calculated as $M = w‚ãÖx + b$, by substituting it, the equation becomes:\n$$ M\u0026rsquo;= \\frac{Œ≥‚ãÖw}{std} x + \\left( Œ≤ - \\frac{Œ≥‚ãÖmean}{std} + \\frac{Œ≥‚ãÖb}{std} \\right) $$\nThus, w becomes $\\frac{Œ≥‚ãÖw}{std}$ and b becomes $( Œ≤ - \\frac{Œ≥‚ãÖmean}{std} + \\frac{Œ≥‚ãÖb}{std} )$.\nGiven a conv layer with $C_i$ input channels and $C_o$ output channels, BN for this layer has $C_i √ó C_o$ parameters, where $C_i$ is for the history channels, $C_o$ is for target channels.\n","date":"2023-01-14T12:54:12-05:00","permalink":"https://zichen34.github.io/writenotes/calc/dl_normalization/","title":"memo: DL | Normalization"},{"content":"The maximum gradient of the sigmoid activation function is 0.25, which may cause partial derivative of loss with respect to the earlier weight w very small after passing throught multipler layers. And scaling the weights down can mitigate the gradient decrease.\nBased on the chain rule, the derivative of the weight in the fisrt layer (l=1) is ‚àÇloss/‚àÇw¬π = ‚àÇloss/‚àÇo ‚ãÖ ‚àÇo/‚àÇa¬≤ ‚ãÖ ‚àÇa¬≤/‚àÇa¬π ‚ãÖ ‚àÇa¬π/‚àÇw¬π, where a = g(z), g is an activation function.\nIf the weight w¬π is small, the activation z¬π is small (around zero), so ‚àÇa¬π/‚àÇz¬π corresponds to the highest derivative. And also if w¬≤ is small, ‚àÇa¬≤/‚àÇa¬π = ‚àÇa¬≤/‚àÇz¬≤‚ãÖ ‚àÇz¬≤/‚àÇa¬π, where ‚àÇa¬≤/‚àÇz¬≤ will be a big derivative. Hence, ‚àÇloss/‚àÇw¬π can maintain a high derivative.\nSo it\u0026rsquo;s importance to initialize the weights centered at zero with small variance for getting the maximum gradient. L11.5 Weight Initialization \u0026ndash; Why Do We Care?\nActivation z is a sum of w·µ¢x·µ¢, so it may be exploding or vanishing quickly if the W doesn\u0026rsquo;t have constriant. Weight Initialization in a Deep Network (C2W1L11) - Andrew Ng\nXavier (Glorot) initialization L11.6 Xavier Glorot and Kaiming He Initialization - Sebastian Raschka\nStep 1: Initialize weights from Gaussian or uniform distribution Step 2: Scale the weights proportional to the number of input features to the layer In particular, the weights of layer l is defined as: ùêñ ‚ÅΩÀ°‚Åæ ‚âî ùêñ ‚ÅΩÀ°‚Åæ‚ãÖ ‚àö(1/m‚ÅΩÀ°‚Åª¬π‚Åæ), where m is the number of input units of the previous layer (ùëô-1) to the next layer (ùëô).\nùêñ is initialized from Gaussion (or uniform) distribution: W·µ¢‚±º‚ÅΩÀ°‚Åæ~N(Œº=0, œÉ¬≤=0.01)\nRationale behind this scaling factor:\nHe (Kaiming) initialization Usage Three different commonly used initialization techniques. Here are what their variants need to be set to and which activation functions they work best with.\nInitialization Activation function Variance (œÉ¬≤) Mean Glorot Linear; Tanh; Sigmoid; Softmax œÉ¬≤ = 1/(¬Ω‚ãÖ(fan·µ¢‚Çô+fan‚Çí·µ§‚Çú)) 0 He ReLu; Variants of ReLU œÉ¬≤ = 2/fan·µ¢‚Çô 0 LeCun SELU œÉ¬≤ = 1/fan·µ¢‚Çô 0 Weight Initialization for Deep Feedforward Neural Networks\n","date":"2023-01-14T12:24:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/dl_weight_initialization/","title":"memo: DL | Weight initialization"},{"content":"ËßÜÈ¢ëÂ∞ÅÈù¢Ôºö‰∏Ä‰∏™Áªü‰∏ÄËßÜËßí‰∏ãÁöÑÊ¶ÇÁéáËÆ∫+ÁªüËÆ°Â≠¶+‰ø°ÊÅØËÆ∫\nÂéüËßÜÈ¢ëÔºö1. ‰ªéÂ§¥ÂºÄÂßãÔºåÊääÊ¶ÇÁéá„ÄÅÁªüËÆ°„ÄÅ‰ø°ÊÅØËÆ∫‰∏≠Èõ∂Êï£ÁöÑÁü•ËØÜÁªü‰∏ÄËµ∑Êù•-ÁéãÊú®Â§¥Â≠¶ÁßëÂ≠¶\nÊ¶ÇÁéáËÆ∫ÊúÄÂü∫Á°ÄÁöÑÈóÆÈ¢òÔºöÁî®Êï∞Â≠¶ÁöÑÊñπÂºèÊèèËø∞\u0026quot;‰∏çÁ°ÆÂÆöÊÄß\u0026quot;ÔºàÊàñ\u0026quot;ÂèØËÉΩÊÄß\u0026quot;Ôºâ\nÊääÊâÄÊúâ‰∫ã‰ª∂ÂèäÂÖ∂ÂèëÁîüÁöÑÂèØËÉΩÊÄßÂÜôÂà∞‰∏Ä‰∏™Ë°® f ÈáåÔºåÊØèÊ¨°ÈÄöËøáÊü•Ë°®Â∞±ËÉΩÁü•ÈÅìÂèØËÉΩÊÄßÊòØÂ§öÂ∞ë: f(S) = K„ÄÇ\nidx S K 1 ‰∫ã‰ª∂a Êï∞ÂÄº1 2 ‰∫ã‰ª∂b Êï∞ÂÄº2 3 ‰∫ã‰ª∂c Êï∞ÂÄº3 \u0026hellip; \u0026hellip;.. \u0026hellip;.. ËøôÈáåÁöÑ‚ÄúÂèØËÉΩÊÄß‚Äù Êª°Ë∂≥‰∏Ä‰∫õÈôêÂà∂Ôºö\nÊï∞ÂÄºË¶ÅÊª°Ë∂≥‰∫ã‰ª∂ÂèØËÉΩÊÄßÁöÑÁõ∏ÂØπÂÖ≥Á≥ª„ÄÇÊØîÂ¶ÇÔºåÂ¶ÇÊûú‰∫ã‰ª∂ a ÁöÑÂèØËÉΩÊÄß \u0026gt; ‰∫ã‰ª∂ b ÁöÑÂèØËÉΩÊÄßÔºåÂàô Êï∞ÂÄº1 \u0026gt; Êï∞ÂÄº2 Êï∞ÂÄºÈúÄË¶ÅÊª°Ë∂≥‰∫ã‰ª∂ÁöÑÂåÖÂê´ÂÖ≥Á≥ª„ÄÇÊØîÂ¶ÇÔºå‰∫ã‰ª∂ c = {‰∫ã‰ª∂aÔºå‰∫ã‰ª∂b}Ôºàa,b‰∏≠‰ªªÊÑè‰∏Ä‰∏™ÂèëÁîüÔºâÔºåÂàô Êï∞ÂÄº3 = Êï∞ÂÄº1 + Êï∞ÂÄº2 Ê≥®ÊÑèÔºåËøôÈáåÂπ∂Êú™Ë¶ÅÊ±ÇÊâÄÊúâÊï∞ÂÄºÂΩí‰∏ÄÂåñÔºåÊâÄ‰ª•Ëøò‰∏çÊòØÊ¶ÇÁéáÂÄº ËøôÁßçÊñπÂºèÂè™ÊòØÊää‚ÄúÂèØËÉΩÊÄß‚ÄùÂÅö‰∫ÜÊï∞Â≠¶Á¨¶Âè∑ÂåñÔºåÂπ∂‰∏çÊòØ‚ÄúÊï∞Â≠¶Âåñ‚Äù„ÄÇÈô§‰∫ÜË¶Å‰øùËØÅÂÆö‰πâÂá∫Êù•ÁöÑËøô‰∏™‰ΩìÁ≥ªËá™Ê¥Ω‰πãÂ§ñÔºåËøòË¶ÅÂ∞ΩÂèØËÉΩÁÆÄÁ∫¶„ÄÇ ÊØîÂ¶Ç‰∏äÈù¢ÁöÑ‰∫ã‰ª∂ c Âπ∂‰∏çÈúÄË¶ÅÂçïÁã¨ÂÆö‰πâÔºåÂÆÉÁöÑÊï∞ÂÄºÂèØ‰ª•‰ªé‰∫ã‰ª∂ a Âíå b Êé®ÂØºÂá∫Êù•„ÄÇ ÊâÄ‰ª•Ë°®Ê†º‰∏≠Âπ∂‰∏çÈúÄË¶ÅÂàó‰∏æÊâÄÊúâÁöÑ‰∫ã‰ª∂ÔºåÂè™ÈúÄË¶ÅÂåÖÂê´‰∏çÂèØÂÜçÂàÜÁöÑÂéüÂ≠ê‰∫ã‰ª∂ÂèäÂÖ∂ÂèØËÉΩÊÄß„ÄÇ\nV 2.0\nË°®1Ôºö\nidx S K 1 ÂéüÂ≠ê‰∫ã‰ª∂a Êï∞ÂÄº1 2 ÂéüÂ≠ê‰∫ã‰ª∂b Êï∞ÂÄº2 3 ÂéüÂ≠ê‰∫ã‰ª∂c Êï∞ÂÄº3 4 ÂéüÂ≠ê‰∫ã‰ª∂d Êï∞ÂÄº4 \u0026hellip; \u0026hellip;.. \u0026hellip;.. ÂÆÉÂèØ‰ª•ÂÆö‰πâÂá∫Ôºöf(S) = K„ÄÇÂè™Áïô‰∏ãÂéüÂ≠ê‰∫ã‰ª∂ÂêéÔºåÂ∞±ÂèØ‰ª•Á°ÆÂÆö‚ÄúÂèØËÉΩÊÄß‚ÄùÁöÑÊúÄÂ§ßÂÄº‰∏∫ÊâÄÊúâÂéüÂ≠ê‰∫ã‰ª∂‚ÄúÂêà‚ÄùÂú®‰∏ÄËµ∑ÁªÑÊàêÁöÑ‰∫ã‰ª∂ÂèëÁîüÁöÑÂèØËÉΩÊÄßÔºàÂè™Ë¶ÅÊúâ1‰∏™ÂéüÂ≠ê‰∫ã‰ª∂ÂèëÁîüÔºåËøô‰ª∂‰∫ãÂ∞±ÁÆóÂèëÁîüÔºâÔºö max(‚àëK) = ‚àë_{s‚ààall} f(s)\nÊúâ‰∫ÜÊúÄÂ§ßÂèØËÉΩÊÄß‰πãÂêéÔºåÂèØ‰ª•ÂÆö‰πâÂΩí‰∏ÄÂåñÁöÑÊï∞ÂÄº K = K/(‚àë_{s‚ààall} f(s)) ‚àà [0,1]\n‰ªé‰∏äÈù¢ÁöÑÂéüÂ≠ê‰∫ã‰ª∂ÂèØÊé®ÂØºÂá∫‰ª•‰∏ãË°®2:\nP(S) ‚àëK {1,2} Êï∞ÂÄº1+Êï∞ÂÄº2 {1,2,3} Êï∞ÂÄº1+Êï∞ÂÄº2+Êï∞ÂÄº3 {3,4} Êï∞ÂÄº3+Êï∞ÂÄº4 {2,4} Êï∞ÂÄº2+Êï∞ÂÄº4 \u0026hellip;\u0026hellip;. \u0026hellip;.. P(S) Ë°®Á§∫ÈõÜÂêà S ÁöÑÂπÇÈõÜ„ÄÇ\n‰ΩÜÊòØÂéüÂ≠ê‰∫ã‰ª∂ÂØπ‰∫é‰∏çÂêåÁöÑÈóÆÈ¢òÔºå‰∏çÂ•ΩÁ°ÆÂÆö„ÄÇÂØπ‰∫éÁ¶ªÊï£ÈóÆÈ¢òÔºàÊé∑È™∞Â≠êÔºâÔºåÂéüÂ≠ê‰∫ã‰ª∂Â∞±ÊòØÁÇπÊï∞„ÄÇ ‰ΩÜÂØπ‰∫éËøûÁª≠ÁöÑÂèòÈáèÔºàÊ∏©Â∫¶ÔºâÔºåÂéüÂ≠ê‰∫ã‰ª∂ÂèØ‰ª•Âèñ‰∏Ä‰∏™Â∞èÂå∫Èó¥„ÄÇÂõ†‰∏∫ÂèØ‰ª•Êó†ÈôêÁªÜÂàÜÔºåÂΩìÂå∫Èó¥Ë∂ãËøë‰∫éÊó†Á©∑Â∞èÊó∂ÔºåÂÆÉÂèëÁîüÁöÑÂèØËÉΩÊÄßÂ∞±Ë∂ã‰∫é0„ÄÇ Â¶ÇÊûúÊâÄÊúâÂéüÂ≠ê‰∫ã‰ª∂ÁöÑÂèØËÉΩÊÄßÈÉΩÊòØ0ÔºåÂ∞±Êó†Ê≥ï‰ªéË°® 1 Êé®ÂØºÂá∫Ë°® 2„ÄÇ\nÂèØ‰ª•‰ªéË°® 2 Âª∫Á´ãÊï∞Â≠¶‰ΩìÁ≥ª„ÄÇ\nV 3.0\nÂØπ‰∫éËøûÁª≠ÁöÑÊÉÖÂÜµÔºåÂéüÂ≠ê‰∫ã‰ª∂Â∞±ÊòØ‰∏Ä‰∏™‰∏™ÁÇπÔºåÂ¶ÇË°® 3ÔºåÂÆÉ‰ª¨ÂØπÂ∫îÁöÑÊï∞ÂÄºÂπ∂‰∏ç‰ª£Ë°®ÂèëÁîüÁöÑÂèØËÉΩÊÄßÔºåËÄåÊòØÊúâÂÖ∂‰ªñÊÑè‰πâÔºåÂõ†‰∏∫Êàë‰ª¨Ë¶ÅÁî®Ë°® 4 Âª∫Á´ãÂÆö‰πâÔºåÊâÄ‰ª•Ë°® 3 ÁöÑÊï∞ÂÄºÊòØÁî±Ë°® 4 Êé®ÂØºÂá∫Êù•ÁöÑ„ÄÇ\nË°® 3Ôºö\n‚Ñù S K 0.0 ÁÇπa Êï∞ÂÄº1 \u0026hellip; ÁÇπb Êï∞ÂÄº2 \u0026hellip; ÁÇπc Êï∞ÂÄº3 \u0026hellip; ÁÇπd Êï∞ÂÄº4 \u0026hellip; \u0026hellip;. \u0026hellip;.. Ë°® 4Ôºö\nP(‚Ñù) K 0.0 [1,2] f[Êï∞ÂÄº1,Êï∞ÂÄº2] \u0026hellip; [1,3] f[Êï∞ÂÄº1,Êï∞ÂÄº3] \u0026hellip; [3,4] f[Êï∞ÂÄº3,Êï∞ÂÄº4] \u0026hellip; [1,2]‚à™[3,4] f[Êï∞ÂÄº1,Êï∞ÂÄº2]+f[Êï∞ÂÄº3,Êï∞ÂÄº4] \u0026hellip; \u0026hellip;. \u0026hellip;.. ","date":"2023-01-08T16:09:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/22_%E6%A6%82%E7%8E%87%E8%AE%BA1-%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%81%94/","title":"watch: DL - ÁéãÊú®Â§¥ 22 | Overview of Probability theory, Statistics, Information theory"},{"content":"ÂèòÂàÜËá™ÁºñÁ†ÅÂô®Ôºà‰∏âÔºâÔºöËøôÊ†∑ÂÅö‰∏∫‰ªÄ‰πàËÉΩÊàêÔºü\nÈááÊ†∑‰∏ÄÊ¨°Â∞±Â§ü ÂÖàÊé®Êñ≠Â≠¶‰π† z ÁöÑÂêéÈ™åÂàÜÂ∏É p(z|x) ÁöÑÂèÇÊï∞ÔºåÂÜç‰ªéÂàÜÂ∏É‰∏≠ÈááÊ†∑‰∏Ä‰∏™ zÔºåÁî®ÂÆÉËÆ°ÁÆó x ÁöÑÂêéÈ™åÂàÜÂ∏É p(x|z) ÁöÑÂèÇÊï∞ÔºåÂÜçÁÆó‰ªéxÁöÑÂêéÈ™åÂàÜÂ∏É‰∏≠ÈááÊ†∑ÂæóÂà∞ x‚ÄòÁöÑÊ¶ÇÁéáÔºå\nÂú® VAE ÁöÑÊçüÂ§±ÂáΩÊï∞Ôºöùìõ = ùîº_p·ê¢(x) [ KL( p(z|x)||q(z) ) + ùîº_p(z|x) [ -ln q(x|z) ] ]‰∏≠Ôºå\nKLÊï£Â∫¶ÁöÑËÆ°ÁÆóÊòØ‰ΩøÁî®Á•ûÁªèÁΩëÁªúÊãüÂêàÂá∫ÁöÑzÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºàÊ≠£ÊÄÅÂàÜÂ∏ÉÔºâÁöÑÊúüÊúõÂíåÊñπÂ∑ÆÔºå ËÄåÁ¨¨ 2 È°πÂè™Èáá‰∫Ü‰∏Ä‰∏™Ê†∑Êú¨ x ÂÅöËøë‰ººÔºåÊâÄ‰ª•Ëøô‰∏ÄÈ°πÂèò‰∏∫Ôºö-ln q(x|z), z~p(z|x)\nÂõ†‰∏∫ KL Êï£Â∫¶‰πüÂèØÂÜôÊàêÊúüÊúõÔºöE_p(z|x) [ ln (p(z|x)/q(z)) ]ÔºåÊâÄ‰ª•ÂÆÉ‰πüÂèØ‰ª•Âè™ÈááÊ†∑‰∏Ä‰∏™ÁÇπÊù•Ëøë‰ºº„ÄÇÊâÄ‰ª•ÊçüÂ§±ÂáΩÊï∞Â∞±ÂèØÂÜô‰∏∫Ôºö\nùìõ = ùîº_p·ê¢(x) [ ln p(z|x) - ln q(z) - ln q(x|z) ], z~p(z|x) (5)\nËãèÁ•ûËØ¥Ôºå‰ª•‰∏äÁöÑÊçüÂ§±ÂáΩÊï∞‰πüËÉΩÊî∂ÊïõÂà∞Áõ∏‰ººÁöÑÁªìÊûú„ÄÇ\n‰ººÁÑ∂‰∏çËÉΩÂè™Áî®‰∏Ä‰∏™ÈááÊ†∑ÁÇπ‰º∞ËÆ° ÊûÅÂ§ß‰ººÁÑ∂ÁöÑÂÖ¨ÂºèÂèØ‰ª•ÂÜôÊàêÊúüÊúõÔºö q(x|z) = arg max_q(x|z) ‚à´ p·ê¢(x) ln (‚à´ q(x|z) q(z) dz) dx ÔºåÊï∞ÂÄºËÆ°ÁÆóÔºåË¶Å‰πòxÁöÑÊ¶ÇÁéá\n= arg max_q(x|z) ùîº_p·ê¢(x) [ ln ( ‚à´ q(x|z) q(z) dz) ] ÔºåÈááÊ†∑Ëøë‰ººÊ±ÇÁßØÂàÜ\n= arg max_q(x|z) ùîº_p·ê¢(x) [ ln (1/K ‚àë_‚Çñ‚Çå‚ÇÅ·¥∑ q(x|z‚Çñ) ], z1,z2,\u0026hellip;,2‚ÇñÔΩûq(z)\nÂØπËøô‰∏™ÊúüÊúõËøë‰ººÔºöÂÖà‰ªéÂÖàÈ™å q(z) ‰∏≠Èáá k ‰∏™ zÔºåÁÆóÁßØÂàÜÔºàÊ±ÇÂíåÔºâÔºåÂÜçÈáá 1 ‰∏™ xÔºåÊ±ÇÂÆÉÁöÑÊ¶ÇÁéáÁöÑÂØπÊï∞Â∞±Ë°å‰∫Ü„ÄÇ\nshuhuai ËØ¥ÊòØÂõ†‰∏∫ log ÁöÑÊñπÂ∑ÆÂ§ßÔºåÊâÄ‰ª•ÈááÊ†∑Â§™Â∞ë‰ºöÂ§±Êïà„ÄÇ ËãèÁ•ûËØ¥ÔºåÂõ†‰∏∫z Âíå x ÊòØ‰∏Ä‰∏ÄÂØπÂ∫îÁöÑÔºåÂ¶ÇÊûúÊ≤°ÊúâÈááÂà∞ z‚ÇñÔºåÈÇ£ÂÆÉÂØπÂ∫îÁöÑ x‚Çñ ‰πüÂ∞±Èáá‰∏çÂá∫Êù•ÔºåÊ¶ÇÁéáÂ∞±ÁÆó‰∏çÂá∫Êù•ÔºåÂõ†‰∏∫ÈááÊ†∑ÊòØÈöèÊú∫ÁöÑÔºå‰∏çËÉΩ‰øùËØÅÊØèÊ¨°ÈááÁöÑ k ‰∏™ zÔºåÂåÖÂê´‰∫ÜÊú¨ batch ‰∏≠ÊâÄÊúâ x ÂØπÂ∫îÁöÑ zÔºåÊâÄ‰ª•ÂÆπÊòìÂ§±Êïà„ÄÇ\nVAEÈáá‰∏Ä‰∏™ÁÇπÁ°ÆÂÆûÂ§ü‰∫Ü Ê†πÊçÆÂØπÊï∞ÊçÆÈõÜÁöÑ‰∫ÜËß£ÔºåÊï∞ÊçÆÈõÜXÊú¨Ë∫´Â∏¶ÊúâÂæàÂº∫ÁöÑÁ∫¶ÊùüÔºåÁúüÊ≠£Áã¨Á´ãÁöÑÁª¥Â∫¶ÂæàÂ∞ëÔºåÊâÄ‰ª•Êï∞ÊçÆÈõÜÂèØ‰ª•Ë¢´ÊäïÂΩ±Âà∞‰ΩéÁª¥Á©∫Èó¥ÁöÑ‰∏Ä‰∏™ÈöêÂèòÈáè‰∏ä„ÄÇ ËøôÂíåÊôÆÈÄöÁöÑËá™ÁºñÁ†ÅÂô®‰∏ÄÊ†∑Ôºå‰πüÂ∞±ÊòØ z ‰∏é x ‰∏Ä‰∏ÄÂØπÂ∫îÔºå‰πüÂ∞±ÊÑèÂë≥Ëøô p(z|x) Âíå q(x|z) ÁöÑÊñπÂ∑Æ‰∏∫0„ÄÇ Âú®ÂºïÂÖ•Ê†áÂáÜÊ≠£ÊÄÅÂΩ¢ÂºèÁöÑÂÖàÈ™åÂàÜÂ∏É q(z) ÂêéÔºåÁ≤óÁï•Âú∞ÁúãÔºåÂè™ÊòØÂØπÈöêÂèòÈáèÁ©∫Èó¥ÂÅö‰∫ÜÂπ≥ÁßªÂíåÁº©ÊîæÔºåÊâÄ‰ª•ÊñπÂ∑Æ‰πüÂèØ‰ª•‰∏çÂ§ß„ÄÇ\nÂõ†‰∏∫ x ÁöÑÂêéÈ™åÂàÜÂ∏ÉÁöÑÊñπÂ∑ÆÂæàÂ∞èÔºåÊØèÊ¨°ÈááÁöÑÁªìÊûúÈÉΩ‰∏ÄÊ†∑ÔºåÈÉΩÊòØÂùáÂÄº Œº(z)„ÄÇ Âõ†‰∏∫ z ‰∏é x ÊòØ‰∏Ä‰∏ÄÂØπÂ∫îÁöÑÔºåÊâÄ‰ª• z ÁöÑÂêéÈ™åÂàÜÂ∏ÉÁöÑÊñπÂ∑Æ‰πüÂæàÂ∞èÔºåÊâÄ‰ª•ÊØèÊ¨°‰ªé‰∏≠ÈááÁöÑ z ÈÉΩÁõ∏Âêå„ÄÇ ÊâÄ‰ª•ÈááÊ†∑‰∏ÄÊ¨°Ôºå‰∏éÈááÊ†∑Â§öÊ¨°Ê≤°‰ªÄ‰πàÂ∑ÆÂà´ÔºåÊúüÊúõÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ\nÂêéÈ™å‰πãÂ¶ô Áõ¥Êé•‰ªéÂÖàÈ™å q(z) ‰∏≠ÈááÊ†∑‰∏çÂèØË°åÔºå‰ΩÜÂú®ÂêéÈ™åÂàÜÂ∏É p(z|x) ‰∏≠ÈááÊ†∑‰∏Ä‰∏™ÁÇπÂ∞±Â§ü‰∫ÜÔºå Âõ†‰∏∫Ëá™ÁºñÁ†ÅÂô®ÈáåÁöÑÊñπÂ∑Æ‰∏∫0ÔºåÂºïÂÖ• z ÁöÑÂÖàÈ™åÔºàÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏ÉÔºâÔºåÊñπÂ∑Æ‰πü‰∏ç‰ºöÂ§™Â§ß„ÄÇ\nËÄøÁõ¥ÁöÑIWAE ÈáçË¶ÅÊÄßÂä†ÊùÉËá™ÁºñÁ†ÅÂô® Importance Weighted AutoEncoders arxiv\nÂØπ p(x) ÂÅö‰∫ÜÁ≠â‰ª∑ÂèòÊç¢Ôºö‰πò‰∏Ä‰∏™p(z|x),Èô§‰∏Ä‰∏™ p(z|x)\np(x) = ‚à´ q(x|z) q(z) dz = ‚à´ p(z|x) ‚ãÖ [q(x|z) ‚ãÖ q(z)] / p(z|x) dz = ùîº_p(z|x) [ q(x|z) ‚ãÖ q(z) / p(z|x) ]\nËøôÊ†∑Ôºå‰ªé q(z) ‰∏≠ÈááÊ†∑Â∞±ÂèòÊàê‰∫Ü‰ªé p(z|x) ‰∏≠ÈááÊ†∑ÔºåÊ≠§ÂâçÂ∑≤ËÆ∫Ëø∞‰∫ÜÂêéÈ™åÂàÜÂ∏É p(z|x) ÊñπÂ∑ÆËæÉÂ∞èÔºåÊâÄ‰ª•ÈááÊ†∑Âá†‰∏™ÁÇπÂ∞±Â§ü‰∫ÜÔºö\n‚à´ q(x|z) q(z) dz = 1/k ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ [ q(x|z‚Çñ)q(z‚Çñ)/p(z‚Çñ|x) ], z1,z2,..,z‚ÇñÔΩûp(z|x)\n‰ª£ÂÖ•‰ººÁÑ∂ÂáΩÊï∞Ôºö\nq(x|z) = arg max_q(x|z) ùîº_p·ê¢(x) [ ln ( ‚à´ q(x|z) q(z) dz) ] = arg max_q(x|z) ùîº_p·ê¢(x) [ ln ( 1/k ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ [ q(x|z‚Çñ)q(z‚Çñ)/p(z‚Çñ|x) ] ) ]\n= arg min_{q(x|z),p(z|x)} ùîº_p·ê¢(x) [ -ln ( 1/k ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ [ q(x|z‚Çñ)q(z‚Çñ)/p(z‚Çñ|x) ] ) ], z1,z2,..,z‚ÇñÔΩûp(z|x) ÔºåÂä†‰∏™Ë¥üÂè∑ÔºåÊ±ÇÊûÅÂ∞èÂÄº„ÄÇ\nÂΩì k=1 Êó∂Ôºå‰∏é (5) Âºè‰∏ÄÊ†∑Ôºå‰ªéËøô‰∏™ËßíÂ∫¶ÁúãÔºåIWAE ÊòØVAEÁöÑÂçáÁ∫ßÁâà„ÄÇ\nÂÖ∂ÂÆûÔºåÁ≠â‰ª∑ÂèòÊç¢ÂèØ‰ª•‰ΩøÁî® z ÁöÑ‰ªªÊÑèÂàÜÂ∏ÉÔºàÂè™Ë¶ÅËÉΩÈááÂá∫zÂ∞±Ë°åÔºâÔºå\n‚ÄúÈÄâÊã© p(z|x) Âè™ÊòØÂõ†‰∏∫ÂÆÉÊúâËÅöÁÑ¶ÊÄßÔºå‰æø‰∫éÈááÊ†∑„ÄÇËÄåÂΩì k Ë∂≥Â§üÂ§ßÊó∂Ôºåp(z|x) ÁöÑÂÖ∑‰ΩìÂΩ¢ÂºèÂ∞±‰∏çÈáçË¶Å‰∫Ü‚Äù\nIWAE ‰∏≠ÂâäÂº±‰∫ÜÊé®Êñ≠Ê®°Âûã p(z|x) ÁöÑ‰ΩúÁî®Ôºå‰∏çÂéªËøë‰ººÂêéÈ™åÂàÜÂ∏É\n","date":"2023-01-01T22:48:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/d-note-vae_3-su/","title":"read: Blog - ËãèÂâëÊûó | VAE-3"},{"content":"Use the built-in dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torchvision import datasets, transforms # Root directory for the dataset data_root = \u0026#39;data\u0026#39; # Spatial size of training images, images are resized to this size. image_size = 64 celeba_data = datasets.CelebA( data_root, download=True, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) ] ) ) This will download and extract the zip file.\nSOURCE CODE FOR TORCHVISION.DATASETS.CELEBA\nExtract the zip file 1 2 3 4 5 6 7 8 9 import zipfile data_root = \u0026#39;data/celeba\u0026#39; # Add shortcut of dataset to your google drive zip_path = \u0026#39;/content/drive/MyDrive/CelebA/Img/img_align_celeba.zip\u0026#39; with zipfile.ZipFile(zip_path, \u0026#39;r\u0026#39;) as ziphandler: ziphandler.extractall(\u0026#39;data\u0026#39;) How do I load the CelebA dataset on Google Colab, using torch vision, without running out of memory?\nZipFile - GfG\nDownload with gdown Install the package: pip install gdown. Copy the URL in the address bar. 1 2 3 4 import gdown url = \u0026#34;https://drive.google.com/u/0/uc?id=1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ\u0026amp;export=download\u0026#34; output = \u0026#34;celeba.zip\u0026#34; gdown.download(url, output) Then unzip it and its subfolder:\n1 2 3 unzip celeba.zip cd celeba unzip img_align_celeba.zip (Python) Use the gdown package to download files from Google Drive\n(2024-02-21)\ngdown 4.7.1 cannot download large dataset dtu.zip 554 MB with the following error reported:\n1 2 3 4 5 6 7 8 9 (base) zi@lambda-server:~/Downloads$ gdown 135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ Access denied with the following error: Cannot retrieve the public link of the file. You may need to change the permission to \u0026#39;Anyone with the link\u0026#39;, or have had many accesses. You may still be able to access the file from the browser: https://drive.google.com/uc?id=135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ Update gdown to 5.1.0 to avoid it: issue\n1 pip install --upgrade gdown Download GoogleDrive (2024-05-25)\nisl-org/TanksAndTemples has a function about downloading datasets from google drive.\n","date":"2022-12-31T23:36:00Z","permalink":"https://zichen34.github.io/writenotes/lib/load_celeba_colab/","title":"memo: load CelebA on Colab"},{"content":"Á¨îËÆ∞ for ÂèòÂàÜËá™ÁºñÁ†ÅÂô®Ôºà‰∫åÔºâÔºö‰ªéË¥ùÂè∂ÊñØËßÇÁÇπÂá∫Âèë\nÊï∞ÂÄºËÆ°ÁÆó vs ÈááÊ†∑ËÆ°ÁÆó Êï∞ÂÄºËÆ°ÁÆóÊòØÂÖàÁªô‰∏™Êï∞ÂàóÔºåÂØπÈáåÈù¢ÁöÑÊØè‰∏™Êï∞Ê±ÇÊ¶ÇÁéáp(x‚ÅΩ‚Å±‚Åæ)ÔºåÂÜçÂä†ÊùÉÊ±ÇÂíå ‚àë·µ¢‚Çå‚ÇÄ‚Åø x‚ÅΩ‚Å±‚Åæp(x‚ÅΩ‚Å±‚Åæ) (x‚ÅΩ‚Å±‚Åæ-x‚ÅΩ‚Å±‚Åª¬π‚Åæ)Ôºõ ÈááÊ†∑ËÆ°ÁÆóÊòØÂÖà‰ªéÂàÜÂ∏É‰∏≠ÈááÊ†∑ÔºåÊ±ÇÈááÊ†∑ÁÇπÊ¶ÇÁéáÁöÑÂπ≥ÂùáÔºåÊâÄ‰ª•‰∏çÈúÄË¶ÅÂÜç‰πòÊ†∑Êú¨ÁÇπÂá∫Áé∞ÁöÑÊ¶ÇÁéáÔºöE[x]‚âà1/n‚ãÖ‚àë·µ¢‚Çå‚ÇÄ‚Åø x‚ÅΩ‚Å±‚Åæ, x‚ÅΩ‚Å±‚Åæ‚àºp(x)„ÄÇ\nÊé®ÂØºVAE ÁöÑÊçüÂ§±ÂáΩÊï∞ ËãèÁ•û‰ªéÈÄºËøëËÅîÂêàÊ¶ÇÁéá p(x,z) Âá∫ÂèëÔºåËÄå‰∏çÊòØ‰ªéÈÄºËøëzÁöÑÂêéÈ™å p(z|x) Âá∫Âèë„ÄÇ ‰πüËÆ∏ÊòØÂõ†‰∏∫Ê≤øÁùÄ EM ÁöÑÊÄùË∑ØËµ∞ÔºåÂ∞±ÈúÄË¶ÅÂØπÂêéÈ™å p(z|x) Ê±ÇËøë‰ººÔºåÊâÄ‰ª•ÂæàÂ§ö‰∫∫ËÅöÁÑ¶‰∫éÊé®ÂØº p(z|x)„ÄÇ\nÂõ†‰∏∫ÊÉ≥Ê±ÇÊ†∑Êú¨ÈõÜÂêà xÁöÑÂàÜÂ∏ÉÔºå\n‰ΩÜÊòØÈöæ‰ª•Áõ¥Êé•ÊèèËø∞Â§çÊùÇÂàÜÂ∏ÉÔºåÊâÄ‰ª•ÈÄöËøáÂºïÂÖ•ÈöêÂèòÈáèÊääxÁöÑÂàÜÂ∏ÉÂèòÊàêÊù°‰ª∂ÂàÜÂ∏ÉÁöÑÂè†Âä†ÔºåËÄåÂêéÂèØ‰ª•ÂØπÈöêÂèòÈáèÁöÑÂàÜÂ∏ÉÂíåÊù°‰ª∂ÂàÜÂ∏ÉÂÅöÈÄÇÂΩìÁÆÄÂåñÔºàÊØîÂ¶ÇÈÉΩÂÅáËÆæ‰∏∫Ê≠£ÊÄÅÂàÜÂ∏ÉÔºâÔºåÂπ∂‰∏îÂèØ‰ª•Áî®Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãËøë‰ººÊ±ÇÔºàÈöêÂèòÈáèÔºâÊù°‰ª∂ÂàÜÂ∏ÉÁöÑÂèÇÊï∞ÔºåÂç≥‚ÄúÊ∑±Â∫¶Ê¶ÇÁéáÂõæÊ®°Âûã‚Äù„ÄÇ\nÂÅáËÆæ x ÊòØÁî± z ÁîüÊàêÁöÑÔºåÊâÄ‰ª•Ê±Ç p(x) ÂèØ‰ª•ÈÄöËøáÊääËÅîÂêàÂàÜÂ∏É p(x,z) ‰∏≠ÁöÑ p(z) ÁßØÊéâÊ±ÇÂæóÔºö p(x) = ‚à´p(x,z) dz = ‚à´ p(x|z)p(z) dz\nÁõÆÊ†áÊòØÁî®‰∏Ä‰∏™ q(x,z) ÈÄºËøë p(x,z)ÔºåÂèàÂõ†‰∏∫ p(z) ÊòØÂÖàÈ™åÔºàÂ∑≤Áü•ÔºâÔºåÊâÄ‰ª•ÂΩì q(x,z)‚âàp(x,z) Êó∂ÔºåÁîüÊàêÊ®°Âûã p(x|z) ‰πüÂ∞±Â≠¶Âà∞‰∫ÜÔºå‚Äú‰∏Ä‰∏æ‰∏§Âæó‚Äù„ÄÇ\nÈÄöËøáÊúÄÂ∞èÂåñ KL Êï£Â∫¶ÈÄºËøëÔºöKL( p(x,z) || q(x,z) ) = ‚à´‚à´ p(x,z) [ ln (p(x,z)/q(x,z)) ] dz dxÔºåÊòØ‰∏Ä‰∏™‰∫åÈáçÁßØÂàÜ\nÊää p(x,z) ÂÜôÊàê p·ê¢(x)‚ãÖp(z|x)Ôºå‰πüÂ∞±ÊòØÊé®Êñ≠ËøáÁ®ãÔºåÁî±xÁöÑÂÖàÈ™åÊé®Âá∫zÔºö\nKL( p(x,z) || q(x,z) ) = ‚à´‚à´ p·ê¢(x)‚ãÖp(z|x) [ ln ( p·ê¢(x)‚ãÖp(z|x) / q(x,z) ) ] dz dx = ‚à´ p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln ( p·ê¢(x)‚ãÖp(z|x) / q(x,z) ) dz ] dx = ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln ( p·ê¢(x)‚ãÖp(z|x) / q(x,z) ) dz]\nÂèØ‰ª•ËíôÁâπÂç°ÁΩóÈááÊ†∑Ëøë‰ººÊ±ÇËøô‰∏™ÊúüÊúõÔºå‰πüÂ∞±ÊòØÊääÊØè‰∏™Ê†∑Êú¨ x‚ÅΩ‚Å±‚Åæ ‰ª£ÂÖ•‰∏äÈù¢‰∏≠Êã¨Âè∑ÈáåÁöÑÂáΩÊï∞Ôºà‰ª£ÂÖ•Ê¶ÇÁéáÂØÜÂ∫¶ÂÖ¨ÂºèÂèØÁÆóÂá∫Ê¶ÇÁéáÂÄºÔºâÔºåÊääÂáΩÊï∞ÂÄºÊ±ÇÂùáÂÄº„ÄÇ\nËøô‰∏™ÊúüÊúõÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÁÆÄÂåñÔºåÊää ln ÊãÜÂºÄÔºö ln ( p·ê¢(x)‚ãÖp(z|x) / q(x,z) ) = ln p·ê¢(x) + ln (p(z|x)/q(x,z))\nùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln ( p·ê¢(x)‚ãÖp(z|x) / q(x,z) ) dz ] = ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln p·ê¢(x) dz] + ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln (p(z|x)/q(x,z)) dz]\n‰∏äÈù¢Á¨¨ 1 ‰∏™ÊúüÊúõÔºö ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln p·ê¢(x) dz] = ùîº_p·ê¢(x) [ ln p·ê¢(x)‚ãÖ‚à´ p(z|x) dz ] = ùîº_p·ê¢(x) [ ln p·ê¢(x) ]\nËøôÈáåÁöÑ p·ê¢(x) ÊòØÊ†πÊçÆÊ†∑Êú¨ x‚ÅΩ‚Å∞‚Åæ, x‚ÅΩ¬π‚Åæ,\u0026hellip;, x‚ÅΩ‚Åø‚Åæ Á°ÆÂÆöÁöÑÂÖ≥‰∫é x ÁöÑÂÖàÈ™åÂàÜÂ∏ÉÔºåÊòØÂ∑≤Áü•ÁöÑÁ°ÆÂÆöÁöÑÔºåÊâÄ‰ª•Ëøô‰∏ÄÈ°πÊòØ‰∏Ä‰∏™Â∏∏Êï∞„ÄÇ ÊâÄ‰ª• KL Êï£Â∫¶ = Â∏∏Êï∞ + ‰∏Ä‰∏™ÊúüÊúõÔºö\nKL( p(x,z) || q(x,z) ) = Â∏∏Êï∞ + ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln (p(z|x)/q(x,z)) dz]\nÊâÄ‰ª•ÊúÄÂ∞èÂåñKLÊï£Â∫¶ÔºåÂØπÂ∫îÁõÆÊ†áÂáΩÊï∞ ùìõ Â∞±ÊòØÁ¨¨2‰∏™ÊúüÊúõÔºö\nùìõ = KL( p(x,z) || q(x,z) ) - Â∏∏Êï∞ÔºåÂàôùìõ ÁöÑ‰∏ãÁïåÂ∞±ÊòØ\u0026quot;-Â∏∏Êï∞\u0026quot;: -ùîº_p·ê¢(x) [ ln p·ê¢(x) ]Ôºå ÂÖ∂‰∏≠ p·ê¢(x) ‰∏ç‰∏ÄÂÆöÊòØÊ¶ÇÁéáÔºåÂú®ËøûÁª≠ÊÉÖÂÜµÊó∂Ôºåp·ê¢(x) ÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÔºåÂÆÉÂèØ‰ª•Â§ß‰∫é1 ‰πüÂèØ‰ª•Â∞è‰∫é1ÔºåÊâÄ‰ª•‰∏ãÁïå‰∏ç‰∏ÄÂÆöÊòØÈùûË¥üÁöÑÔºåÂç≥ loss ÂèØËÉΩÊòØË¥üÊï∞„ÄÇ\nÂÜçÊää ùìõ ÈáåÁöÑ ln Âíå q(x,z) Â±ïÂºÄÔºö\nùìõ = ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln ( p(z|x) / q(x,z) ) dz] = ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln ( p(z|x) / (q(x|z)q(z)) dz ] = ùîº_p·ê¢(x) [ ‚à´ p(z|x) ‚ãÖ ln ( p(z|x)/q(z) ) dz - ‚à´ p(z|x) ‚ãÖ ln q(x|z) dz]\nÊääÈáåÈù¢ÁöÑÁßØÂàÜÂÜôÊàêÊúüÊúõÔºö\nùìõ = ùîº_p·ê¢(x) [ KL( p(z|x)||q(z) ) + ùîº_p(z|x) [ -ln q(x|z) ] ]\nÊã¨Âè∑ÈáåÁöÑÂ∞±ÊòØ VAE ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºöKLÊï£Â∫¶ÔºàÊ≠£ÂàôÂåñÈ°πÔºâ+ xÁöÑÂêéÈ™åÊåâÁÖß z ÁöÑÂêéÈ™åÊ±ÇÊúüÊúõ shuhuai008-30VAE\n‰∏çËÉΩÊääÊã¨Âè∑ÈáåÈù¢ÁöÑ‰∏§È°πÂàÜÂºÄÁúãÊàñÂàÜÂºÄÊúÄÂ∞èÂåñ„ÄÇ Â¶ÇÊûúÂè™‰ª§ KL( p(z|x)||q(z) )=0ÔºåÂç≥ÊØè‰∏™ÂêéÈ™åÈÉΩÊòØÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏ÉÔºå‰∏éxÊó†ÂÖ≥ÔºåÂØºËá¥ÁîüÊàêÁöÑ x ‰∏çÂáÜÔºåÊ¶ÇÁéá q(x|z) ‰ºöÂæàÂ∞èÔºå-ln q(x|z) Â∞±‰ºöÂæàÂ§ß„ÄÇ ËÄåÂ¶ÇÊûú -ln q(x|z) ÂæàÂ∞èÔºåÂç≥xÂêéÈ™åÊ¶ÇÁéá q(x|z) Â§ßÔºåÂêéÈ™åÂàÜÂ∏É p(z|x) ÁöÑÂ≥∞ËÇØÂÆöÈõÜ‰∏≠Âú® x ÈôÑËøëÔºåÂç≥ p(z|x) ÁöÑÊñπÂ∑ÆÂ∞èÔºå‰∏éÊ†áÂáÜÊ≠£ÊÄÅÁöÑÂ∑ÆË∑ùÂ§ßÔºåKL( p(z|x)||q(z) ) ‰∏ç‰ºöÂ∞è„ÄÇ ÊâÄ‰ª•Ëøô‰∏§ÈÉ®ÂàÜ loss ÊòØÁõ∏‰∫íÊãÆÊäóÁöÑÔºåùìõ Ë¶Å‰ª•Êï¥‰ΩìÊù•Áúã„ÄÇ ‰πüÂ∞±ÊòØË¶ÅÊé®Êñ≠ËøáÁ®ã‰∏éÁîüÊàêËøáÁ®ãÁõ∏‰∫íÂçöÂºà„ÄÇ\nÁÆóÊ≥ïËÆæËÆ° ÊçüÂ§±ÂáΩÊï∞‰∏≠Êú™Áü•ÁöÑÂàÜÂ∏ÉÂåÖÊã¨Ôºöz ÁöÑÂÖàÈ™å q(z)Ôºåz ÁöÑÂêéÈ™å p(z|x)Ôºåx ÁöÑÂêéÈ™å q(x|z) Ôºàx ÁöÑÂÖàÈ™åp·ê¢(x) ÊòØÂ∑≤Áü•ÁöÑÔºâ\n‰∏∫‰∫Ü‰æø‰∫éÈááÊ†∑ÔºåÂÅáËÆæ z ÁöÑÂÖàÈ™åÂàÜÂ∏É‰∏∫Ê†áÂáÜÂ§öÂÖÉÊ≠£ÊÄÅÂàÜÂ∏ÉÔºöq(z)=N(0,I)\nÁî®Á•ûÁªèÁΩëÁªúÊãüÂêà z ÁöÑÂêéÈ™å p(z|x) Âíå x ÁöÑÂêéÈ™å q(x|z)„ÄÇ\nËÆ°ÁÆó z ÁöÑÂêéÈ™åÊòØÊé®Êñ≠ËøáÁ®ãÔºåÂØπÂ∫î EM ÁöÑ EÊ≠•ÔºöËøë‰ººÊ±ÇÂæó p(z|x)ÔºõËÆ°ÁÆó x ÁöÑÂêéÈ™åÊòØÁîüÊàêËøáÁ®ãÔºåÂØπÂ∫î EM ÁöÑ MÊ≠•ÔºöÊääzÁöÑËøë‰ººÂêéÈ™å‰ª£ÂÖ•‰ººÁÑ∂ÂáΩÊï∞ÔºåÊ±ÇÊûÅÂ§ß‰ººÁÑ∂Êó∂ÔºåÂØπÂ∫îÁöÑÊ®°ÂûãÂèÇÊï∞„ÄÇ ÔºàEM‰∏≠Áî®‰∫éÈÄºËøë p(z|x) ÁöÑÁ•ûÁªèÁΩëÁªúÁöÑÂèÇÊï∞ÊòØ œïÔºõÁî®‰∫éÈÄºËøë q(x|z) ÁöÑÁ•ûÁªèÁΩëÁªúÔºà‰πüÂèØÁõ¥Êé•Ê±ÇÂØºÔºâÁöÑÂèÇÊï∞ÊòØ Œ∏Ôºâ\nÂêéÈ™åÂàÜÂ∏ÉËøë‰ºº ÂÅáËÆæ z ÁöÑÂêéÈ™åÊòØÔºàÂêÑÂàÜÈáèÁã¨Á´ãÁöÑÔºâ‰∏ÄËà¨Ê≠£ÊÄÅÂàÜÂ∏ÉÔºåÊâÄ‰ª•ÈúÄË¶ÅÁ•ûÁªèÁΩëÁªúÈÄºËøëÂÆÉÁöÑÊúüÊúõÂíåÊñπÂ∑Æ„ÄÇÊúüÊúõÂíåÊñπÂ∑ÆÈÉΩÁî± x ÂÜ≥ÂÆöÔºåÂç≥ÊòØ x ÁöÑÂáΩÊï∞ Œº_œï(x), Œ£_œï(x)\nÁÑ∂Âêé KL Êï£Â∫¶Â∞±ÂèØ‰ª•ÂÜôÂá∫Êù•‰∫Ü: 1/2(-logœÉ¬≤ + Œº¬≤ + œÉ¬≤ -1)ÔºåÂ∑≤Âú®VAEÁ¨¨‰∏ÄÁØáÊé®ÂØºËøá„ÄÇÂèòÂàÜËá™ÁºñÁ†ÅÂô®Ôºà‰∏ÄÔºâÔºöÂéüÊù•ÊòØËøô‰πà‰∏ÄÂõû‰∫ã\nÁîüÊàêÊ®°ÂûãËøë‰ºº ÂØπ‰∫éÁîüÊàêÊ®°ÂûãÈÉ®ÂàÜ q(x|z) ÁöÑÂÅáËÆæÔºåÂéü‰ΩúËÄÖÂú®ËÆ∫Êñá„ÄäAuto-Encoding Variational Bayes„Äã‰∏≠ÔºåÁªôÂá∫‰∫Ü‰∏§ÁßçÊñπÊ°àÔºö‰∫åÈ°πÂàÜÂ∏ÉÊàñÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ‚ÄúÊó¢Ë¶ÅÊª°Ë∂≥Ê¶ÇÁéáÁöÑÂÆö‰πâÔºàÂΩí‰∏ÄÂåñÔºâÔºåÂèàË¶ÅÂÆπÊòìÁÆóÔºåÊ≤°Â§öÂ∞ëÈÄâÊã©‚Äù\n‰∫åÈ°πÂàÜÂ∏ÉÂè™Êúâ‰∏Ä‰∏™ÂèÇÊï∞Ôºö\u0026ldquo;ÊäõÁ°¨Â∏ÅÂêë‰∏äÁöÑÊ¶ÇÁéá œÅ\u0026rdquo;„ÄÇÊâÄ‰ª•ÂØπ‰∫é‰∏Ä‰∏™ D Áª¥ÁöÑÊ†∑Êú¨ xÔºåx ÁöÑÊØè‰∏ÄÁª¥ÈÉΩÊòØ‰∏™‰∫åÂÄºÁöÑÔºåÊâÄ‰ª•‰∏Ä‰∏™ËæìÂÖ•Ê†∑Êú¨ x Âú® z ÊàêÁ´ãÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèëÁîüÁöÑÊ¶ÇÁéáÂ∞±ÊòØÔºö q(x|z) = ‚àè‚Çñ‚Çå‚ÇÅ·¥∞ (œÅ‚Çñ(z))À£·µè (1-œÅ‚Çñ(z))¬π‚ÅªÀ£·µè\nÊ≠§Êó∂ÁöÑ -ln q(x|z) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∞ [ -x‚Çñ ln œÅ‚Çñ(z) - (1-x‚Çñ) ln(1-œÅ‚Çñ(z)) ]\n‰πüÂ∞±ÊòØËØ¥Á•ûÁªèÁΩëÁªúÁöÑËæìÂá∫ œÅ(z) ÈúÄË¶ÅÊòØÂú® 0ÔΩû1 ‰πãÈó¥ÔºàÊØîÂ¶ÇÁî® sigmoid ÊøÄÊ¥ªÔºâÔºåÁÑ∂ÂêéÁî®‰∫§ÂèâÁÜµ‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞„ÄÇ\nÂ¶ÇÊûúÂÅáËÆæ q(x|z) ÊòØÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÁî®Á•ûÁªèÁΩëÁªú‰º∞ËÆ°ÂÆÉÁöÑÊúüÊúõ Œº_Œ∏(z) ÂíåÊñπÂ∑Æ Œ£_Œ∏(z)Ôºå‰∫éÊòØÔºö -ln q(x|z) = ¬Ω || (x-Œº_Œ∏(z)) / Œ£_Œ∏(z) ||¬≤ + D/2 ln 2œÄ + ¬Ω‚àë‚Çñ‚Çå‚ÇÅ·¥∞ lnŒ£_Œ∏(z)„ÄÇ\nÂæàÂ§öÊó∂ÂÄôÔºåËÆ≠ÁªÉÊó∂ÊñπÂ∑Æ‰ºöÂõ∫ÂÆö‰∏∫‰∏Ä‰∏™ËæÉÂ∞èÁöÑÂ∏∏Êï∞ÔºàÊØèÊ¨°ÈááÊ†∑ÈÉΩ‰ºöÈááÂà∞ŒºÔºâÔºåÊâÄ‰ª•Á•ûÁªèÁΩëÁªúÂè™ÈúÄ‰º∞ËÆ°ŒºÔºå‰πüÂ∞±ÊòØÊää Œº ÂΩì‰ΩúÁîüÊàêÁöÑ x\u0026rsquo;ÔºåÂàô‰∏äÂºèÈáçÊûÑËØØÂ∑ÆÂèØÁÆÄÂåñ‰∏∫Ôºö -ln q(x|z) = ¬Ω || (x-Œº_Œ∏(z)) / Œ£_Œ∏(z) ||¬≤\nx \u0026ndash;\u0026gt; z \u0026ndash;\u0026gt; x'\nÁªº‰∏äÔºåÂØπ‰∫é‰∫åÂÄºÊï∞ÊçÆÔºåÂÅáËÆæ q(x|z) ÊòØ‰ºØÂä™Âà©ÂàÜÂ∏ÉÔºà‰∫åÈ°πÂàÜÂ∏ÉÔºâÔºåÂèØ‰ª•ÂØπ decoder ÔºàÁ¨¨2‰∏™Á•ûÁªèÁΩëÁªúÔºâÁöÑËæìÂá∫Áî® sigmoid ÊøÄÊ¥ªÔºåÂπ∂Áî®‰∫§ÂèâÁÜµ‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞Ôºõ ËÄåÂØπ‰∫é‰∏ÄËà¨Êï∞ÊçÆÔºåÂÅáËÆæ q(x|z) ÊòØÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÂàô‰ΩøÁî® mse ‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞„ÄÇ\n‰ªéÂêéÈ™å‰∏≠ÈááÊ†∑ z ÁöÑÊäÄÂ∑ß ÊçüÂ§±ÂáΩÊï∞ÁöÑÁ¨¨ 2 È°πÊòØÔºöùîº_p(z|x) [ -ln q(x|z) ]ÔºåÊ†πÊçÆËíôÁâπÂç°ÁΩóÁöÑÊÄùÊÉ≥ÔºåËøô‰∏™ÊúüÊúõÁî®ÂùáÂÄºËøë‰ººÔºöÂÖàÈááÊ†∑ zÔºåÁî® z ËÆ°ÁÆó x ÁöÑÂêéÈ™åÂàÜÂ∏É q(x|z)ÔºåÂÜç‰ªé‰∏≠ÈááÊ†∑ x ËÆ°ÁÆóÂÆÉÂá∫Áé∞Ê¶ÇÁéáÁöÑÂØπÊï∞Ôºö-ln q(x|z)ÔºåÂÜçÊ±ÇÂùáÂÄºÔºö\nùîº_p(z|x) [ -ln q(x|z) ] = -1/n ‚àë·µ¢‚Çå‚ÇÅ·¥∫ ln q(x|z·µ¢), z·µ¢ÔΩûp(z|x)\nÂÅáËÆæ‰∫Ü p(z|x) ÊòØÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÂÆÉÁöÑÂèÇÊï∞ Œº_œï, Œ£_œï Â∑≤Áî±Á•ûÁªèÁΩëÁªúÁÆóÂá∫ÔºåÂÜç‰ΩøÁî®ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßÂ∞±ËÉΩÈááÊ†∑Âá∫ z „ÄÇ\n‰ΩÜÊòØË¶ÅÈááÊ†∑Â§öÂ∞ë‰∏™ÂêàÈÄÇÂë¢ÔºüÂõ†‰∏∫ÊØè‰∏™ z ÊòØ‰∏ìÂ±û‰∫é 1 ‰∏™ xÔºåÊâÄ‰ª•Âè™‰ªé p(z|x) ‰∏≠Èáá‰∏Ä‰∏™ z Êù•ËÆ°ÁÆó x ÁöÑÂàÜÂ∏É q(x|z)ÔºåÂÜçËÆ°ÁÆó -ln q(x|z)ÔºåÂ∞±ÊòØlossÂÄº„ÄÇ\nÊúÄÁªàÔºöùìõ = ùîº_p·ê¢(x) [ KL( p(z|x)||q(z) ) -ln q(x|z) ] , zÔΩûp(z|x)\nÂõ†‰∏∫ÊØèÊ¨° epoch ÁöÑÈöêÂèòÈáèÈÉΩÊòØÈöèÊú∫ÁîüÊàêÁöÑÔºåÂõ†Ê≠§ÂΩì epoch Êï∞Ë∂≥Â§üÂ§öÊó∂ÔºåÂèØ‰ª•‰øùËØÅÈááÊ†∑ÁöÑÂÖÖÂàÜË°å„ÄÇËãèÁ•ûËØïËøáÈááÊ†∑Â§ö‰∏™ÁöÑÊÉÖÂΩ¢ÔºåÊÑüËßâÁîüÊàêÁöÑÊ†∑Êú¨Ê≤°ÊúâÊòéÊòæÂèòÂåñ„ÄÇ\n","date":"2022-12-30T20:07:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/d-note-vae_2-su/","title":"read: Blog - ËãèÂâëÊûó | VAE-2"},{"content":"(2023-10-11)\n‰∏ÄÁª¥È´òÊñØ Êï∞ÊçÆ ÁöÑÁ©∫Èó¥ÂàÜÂ∏ÉÂÉèÊòØ‰∏Ä‰∏™‰∏≠Èó¥Èáç‰∏§Â§¥ËΩªÁöÑÈìÅÊ£í (Á∫ø)Ôºõ ‰∫åÁª¥È´òÊñØÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÂÉè‰∏Ä‰∏™ Ê§≠ÂúÜ (Èù¢)Ôºõ Êúç‰ªé‰∏âÁª¥È´òÊñØÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÁöÑÁ©∫Èó¥ÂàÜÂ∏ÉÂÉè‰∏Ä‰∏™ Ê§≠ÁêÉ (‰Ωì)„ÄÇ\nSource video: Êú∫Âô®Â≠¶‰π†-ÁôΩÊùøÊé®ÂØºÁ≥ªÂàó(‰∫å)-Êï∞Â≠¶Âü∫Á°Ä\n1. È´òÊñØÂàÜÂ∏É1-ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ° P1\nÈ´òÊñØÂàÜÂ∏ÉÂú®ÔºàÁªüËÆ°ÔºâÊú∫Âô®Â≠¶‰π†‰∏≠ÈùûÂ∏∏ÈáçË¶Å„ÄÇÊØîÂ¶ÇÁ∫øÊÄßÈ´òÊñØÊ®°ÂûãÊòØ‰∏ÄÊï¥Â•ó‰ΩìÁ≥ªÔºåÂç°Â∞îÊõºÊª§Ê≥¢Â∞±ÊòØ‰∏ÄÁßçÁ∫øÊÄßÈ´òÊñØÊ®°Âûã„ÄÇ ÈöêÂèòÈáè‰∏éÈöêÂèòÈáèÔºåÈöêÂèòÈáè‰∏éËßÇÊµãÂèòÈáè‰πãÈó¥Êúç‰ªéÈ´òÊñØÂàÜÂ∏É„ÄÇ\n1 2 3 z1 -\u0026gt; z2 -\u0026gt; ... -\u0026gt; zN ‚Üì ‚Üì ‚Üì ‚Üì x1 -\u0026gt; x2 -\u0026gt; ... -\u0026gt; xN ÊØîÂ¶Ç‰ªé z‚ÅΩ·µó‚Åæ ‚ûî z‚ÅΩ·µó‚Å∫¬π‚Åæ ÁöÑËΩ¨ÁßªÊúç‰ªé Á∫øÊÄßÈ´òÊñØÔºöz‚ÅΩ·µó‚Å∫¬π‚Åæ=Az‚ÅΩ·µó‚Åæ+B+ŒµÔºåÂÖàÂÅöÁ∫øÊÄßÂèòÊç¢ÂÜçÂä†‰∏ä Œµ È´òÊñØÂô™Â£∞„ÄÇ\nÂÜçÊØîÂ¶Ç P-PCAÔºàÊ¶ÇÁéáPCAÔºâ‰∏≠ÔºåÂéüÊï∞ÊçÆÊòØ p Áª¥ÁöÑ x‚àà‚Ñù·µñ Ë¶ÅÈôçÂà∞ q Áª¥Á©∫Èó¥ z‚àà‚Ñù^qÔºå ÂÅáËÆæ x ‰∏é z ‰πãÈó¥ÁöÑÂèòÊç¢‰∏∫Ôºöx=Wz+Œº+ŒµÔºåŒµÔΩûN(0,œÉ¬≤‚ãÖI)Ôºå0ÂùáÂÄºÁöÑÂêÑÂêëÂêåÊÄßÁöÑÔºàÂç≥ Œ£ ÊòØÂØπËßíÁü©ÈòµÔºåÂπ∂‰∏îÂØπËßíÁ∫ø‰∏äÁöÑÂÄºÈÉΩÊòØœÉ¬≤ÔºâÈ´òÊñØÂàÜÂ∏ÉÔºàÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏ÉÔºâ„ÄÇ Â¶ÇÊûúÂØπËßíÁ∫ø‰∏äÁöÑÂÄº‰∏çÁõ∏ÂêåÔºåÂ∞±ÂèòÊàê‰∫ÜÂõ†Â≠êÂàÜÊûê\nËÄå‰∏îÁ∫øÊÄßÈ´òÊñØÊ®°ÂûãÊúâÂæàÂ§öÁâπÊÄßÔºöÊØîÂ¶Ç‰∏Ä‰∏™È´òÁª¥ÁöÑÈöèÊú∫ÂèòÈáè x‚àà‚Ñù·µñ Êúç‰ªéÈ´òÊñØÂàÜÂ∏É xÔΩûN(Œº,Œ£)ÔºåÂ∞ÜÂÆÉÂàÜÊàê‰∏§‰∏™Â∞èÁªÑ x‚ÇÅ‚àà‚Ñù·µê, x‚ÇÇ‚àà‚Ñù‚Åø, m+n=p„ÄÇ Âàô x‚ÇÅ ‰πüÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºåÊù°‰ª∂Ê¶ÇÁéá x‚ÇÇ|x‚ÇÅ ‰πüÊúç‰ªéÈ´òÊñØÂàÜÂ∏É„ÄÇ\nÂèÇÊï∞‰º∞ËÆ° ÁªôÂÆöÊï∞ÊçÆ ùêóÔºöN‰∏™Ê†∑Êú¨ÔºåÊØè‰∏™Ê†∑Êú¨ x ÊòØ p Áª¥ÁöÑ x‚àà‚Ñù·µñÔºåùêó Â∞±ÊòØ‰∏Ä‰∏™ N x p ÁöÑÁü©ÈòµÔºö\nùêó = (x‚ÇÅ, x‚ÇÇ,\u0026hellip;, x‚Çô)·µÄ = $[^{^{ x‚ÇÅ·µÄ}_{x‚ÇÇ·µÄ}} _{^{‚ã±}_{ x‚Çô·µÄ}}]$‚Çô‚Çì‚Çö\n$$ \\pmb X = (x_1, x_2, \u0026hellip;,x_N)^T \\\\ = \\begin{pmatrix} x_{11} \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{1p} \\\\ x_{21} \u0026amp; x_{22} \u0026amp; \\cdots \u0026amp; x_{2p} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ x_{p1} \u0026amp; x_{p2} \u0026amp; \\cdots \u0026amp; x_{pp} \\end{pmatrix}^T \\\\ = \\begin{pmatrix} x_{11} \u0026amp; x_{21} \u0026amp; \\cdots \u0026amp; x_{p1} \\\\ x_{12} \u0026amp; x_{22} \u0026amp; \\cdots \u0026amp; x_{p2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ x_{1p} \u0026amp; x_{2p} \u0026amp; \\cdots \u0026amp; x_{pp} \\end{pmatrix} $$\n$x_i \\in \\R^p$ : pÁª¥ÂÆûÊï∞ÂêëÈáèÁ©∫Èó¥ÔºàpÁª¥Ê¨ßÊ∞èÁ©∫Èó¥ÔºâÔºåÂàóÂêëÈáè ÂÅáËÆæÊ†∑Êú¨ x·µ¢ ‰πãÈó¥ÊòØÁã¨Á´ãÂêåÂàÜÂ∏ÉÔºåÈÉΩÊòØ‰ªé‰∏Ä‰∏™È´òÊñØÂàÜÂ∏É‰∏≠ÊäΩÂá∫Êù•ÁöÑ: x·µ¢ÔΩûN(Œº,Œ£)„ÄÇ\n‰ª§ÂèÇÊï∞ Œ∏ = (Œº,Œ£) ÔºàŒ£ÊòØÂçèÊñπÂ∑ÆÁü©ÈòµÔºöÂØπËßíÁ∫ø‰∏äÊòØœÉ¬≤ÔºåŒº ÊòØ‰ΩçÁΩÆÂèÇÊï∞ÔºåœÉÊòØÂ∞∫Â∫¶ÂèÇÊï∞Ôºâ\nŒ∏ ÁöÑÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ° (MLE)ÔºöŒ∏‚Çò‚Çó‚Çë = arg max_Œ∏ P(X|Œ∏)\n‰∏∫ÁÆÄÂåñËÆ°ÁÆóÔºö‰ª§ p=1 (‰∏ÄÁª¥)ÔºåÂàô Œ∏ = (Œº,Œ£=œÉ¬≤)Ôºå‰∏ÄÁª¥È´òÊñØÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞: p(x) = 1/(‚àö(2œÄ)‚ãÖœÉ) ‚ãÖ exp(-(x-Œº)¬≤/2œÉ¬≤)\n$$ p(x) = \\frac{1}{\\sqrt{2œÄ}‚ãÖœÉ} exp(-\\frac{(x-Œº)¬≤}{2œÉ¬≤}) $$\nÈ´òÁª¥ÔºàpÁª¥ÔºâÁöÑÈ´òÊñØÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ (PDF)Ôºöp(x)=1/(2œÄ·µñ·êü¬≤‚ãÖ|Œ£|¬π·êü¬≤) ‚ãÖ exp(-¬Ω(x-Œº)·µÄ‚ãÖŒ£‚Åª¬π‚ãÖ(x-Œº))\n$$ p(ùê±) = \\frac{1}{(2œÄ)^{p/2} |Œ£|^¬Ω} exp(-\\frac{(ùê±-\\bm Œº)·µÄ(ùê±-\\bm Œº)}{2Œ£}) $$\nlog P(X|Œ∏) = log ‚àè·µ¢‚Çå‚ÇÅ·¥∫ p(x·µ¢|Œ∏) ÔºåÁã¨Á´ãÂêåÂàÜÂ∏ÉÔºåËÅîÂêàÊ¶ÇÁéáÂÜôÊàêËøû‰πò = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log p(x·µ¢|Œ∏) ÔºålogÊääËøû‰πòÂèòËøûÂä†\n= ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log 1/(‚àö(2œÄ)‚ãÖœÉ) ‚ãÖ exp(-(x·µ¢-Œº)¬≤/2œÉ¬≤)Ôºå‰ª£ÂÖ•È´òÊñØÂàÜÂ∏É\n= ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ log 1/‚àö(2œÄ) + log 1/œÉ -(x·µ¢-Œº)¬≤/2œÉ¬≤ ]\nÂÖàÊ±ÇÊúÄ‰Ω≥ÁöÑ Œº: ÂΩìÂØπÊï∞‰ººÁÑ∂ÊúÄÂ§ßÊó∂ÔºåŒº Á≠â‰∫éÂ§öÂ∞ëÔºü\nŒº‚Çò‚Çó‚Çë = arg max_Œº P(X|Œ∏) = arg max_Œº ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ log 1/‚àö(2œÄ) + log 1/œÉ -(x·µ¢-Œº)¬≤/2œÉ¬≤ ] = arg max_Œº ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ -(x·µ¢-Œº)¬≤/2œÉ¬≤ ] ÔºåÂè™‰øùÁïô‰∏éŒºÁõ∏ÂÖ≥ÁöÑÈ°π = arg min_Œº ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤ ÔºåœÉ¬≤ ‰∏ÄÂÆöÊòØÊ≠£ÁöÑ ÂØπ Œº Ê±ÇÂÅèÂØºÔºå‰ª§ÂÖ∂Á≠â‰∫é0Ôºö\n‚àÇ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤ / ‚àÇŒº = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ -2(x·µ¢-Œº) = 0 ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº) = 0 ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢ - ‚àë·µ¢‚Çå‚ÇÅ·¥∫ Œº = 0 ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢ = NŒº ÔºåŒº ‰∏é i Êó†ÂÖ≥ Œº‚Çò‚Çó‚Çë = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢ ÔºåÊúÄ‰ºòÁöÑ Œº Â∞±ÊòØÊ†∑Êú¨ÁöÑÂùáÂÄº„ÄÇ Œº‚Çò‚Çó‚Çë ÊòØÊó†ÂÅèÁöÑÔºåÂõ†‰∏∫ Œº‚Çò‚Çó‚Çë ÁöÑÊúüÊúõÔºö\nE[Œº‚Çò‚Çó‚Çë] = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [Œº‚Çò‚Çó‚Çë] = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢ ] = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [Œº] ÔºåÂõ†‰∏∫Ê†∑Êú¨iid,Êúç‰ªéÈ´òÊñØÂàÜÂ∏É = 1/N ‚ãÖ NŒº = Œº (ÁúüÂÆûÁöÑ Œº)\nŒº‚Çò‚Çó‚Çë ÁöÑÊúüÊúõÊòØÊó†ÂÅèÁöÑÔºå‰ΩÜÊØèÊ¨°ÁöÑ Œº‚Çò‚Çó‚Çë Âπ∂‰∏çÊòØÁúüÂÆûÁöÑ ŒºÔºåÊØèÊ¨°‰º∞Âá∫Êù•ÁöÑÂèØËÉΩÊØîŒºÂ§ßÊàñËÄÖÂ∞èÔºåËøô‰∏™ Œº Êó†Ê≥ïÈÄöËøá‰∏ÄÊ¨°‰º∞ËÆ°ÂæóÂà∞ÔºåÂè™ËÉΩÊòØÈöèÁùÄÊ†∑Êú¨ÈáèÁöÑÂ¢ûÂä†ÔºåÊääÊØèÊ¨°ËØïÈ™åÁöÑÁªìÊûúÊ±ÇÂπ≥ÂùáÊâç‰ºöÊó†ÈôêËøë‰ººÁúüÂÆûÁöÑ Œº„ÄÇ Ôºà‰ΩÜÊòØÊ†∑Êú¨‰∏çÂèØËÉΩÊúâÊó†Èôê‰∏™ÔºåÊâÄ‰ª•ÂÆûÈôÖ‰∏äÁúüÂÆûÂùáÂÄºÊòØÊó†Ê≥ïÂæóÂà∞ÁöÑÔºâ\nÊ±ÇÊúÄ‰ºòÁöÑ œÉ¬≤ Áî®Á±ª‰ººÁöÑËøáÁ®ã:\nœÉ¬≤‚Çò‚Çó‚Çë = arg max_œÉ¬≤ P(X|Œ∏) = arg max_œÉ¬≤ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ log 1/‚àö(2œÄ) + log 1/œÉ -(x·µ¢-Œº)¬≤/2œÉ¬≤ ] = arg max_œÉ¬≤ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ -log œÉ -(x·µ¢-Œº)¬≤/2œÉ¬≤ ] ÔºåÂè™‰øùÁïô‰∏éœÉ¬≤Áõ∏ÂÖ≥ÁöÑÈ°π ÁõÆÊ†áÂáΩÊï∞ÂØπ œÉ Ê±ÇÂÅèÂØºÔºö\n‚àÇ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ -log œÉ -(x·µ¢-Œº)¬≤/2œÉ¬≤ ] / ‚àÇœÉ = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [-1/œÉ - (x·µ¢-Œº)¬≤/2 ‚ãÖ -2œÉ‚Åª¬≥ = 0 Ôºå‰∏§ËæπÂêå‰πòœÉ¬≥ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [-œÉ¬≤ + (x·µ¢-Œº)¬≤ = 0 ÔºåÊää‚àë Â∏¶ËøõÂéª ‚àë·µ¢‚Çå‚ÇÅ·¥∫ œÉ¬≤ = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤ ÔºåœÉ¬≤ ‰∏é i Êó†ÂÖ≥ N œÉ¬≤ = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤ œÉ¬≤‚Çò‚Çó‚Çë = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº‚Çò‚Çó‚Çë)¬≤\nÊúÄ‰ºòÁöÑ œÉ¬≤ ‰πüÊòØÂØπÊ†∑Êú¨ÊñπÂ∑ÆÊ±ÇÊúüÊúõ„ÄÇ Ëøô‰∏™ œÉ¬≤‚Çò‚Çó‚Çë ÊòØÊúâÂÅè‰º∞ËÆ°ÔºåÂõ†‰∏∫ œÉ¬≤‚Çò‚Çó‚Çë ÁöÑÊúüÊúõ‰∏çÁ≠â‰∫é œÉ¬≤Ôºö\nÂÖàÂØπ œÉ¬≤‚Çò‚Çó‚Çë ÂÅöÁÆÄÂåñ:\nœÉ¬≤‚Çò‚Çó‚Çë = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢ - Œº‚Çò‚Çó‚Çë)¬≤ = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢¬≤ - 2x·µ¢Œº‚Çò‚Çó‚Çë + Œº‚Çò‚Çó‚Çë¬≤) Ôºå Â±ïÂºÄÊã¨Âè∑ÁöÑÂπ≥Êñπ\n= 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢¬≤ - 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ 2x·µ¢‚ãÖŒº‚Çò‚Çó‚Çë + 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ Œº‚Çò‚Çó‚Çë¬≤) Ôºå‚àëÂ∏¶ËøõÂéª = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢¬≤ - 2Œº‚Çò‚Çó‚Çë¬≤ + Œº‚Çò‚Çó‚Çë¬≤ ÔºåÁ¨¨2È°πÈáåÊúâ‰∏™Ê†∑Êú¨ÂùáÂÄºÔºåÁ¨¨3È°π‰∏éiÊó†ÂÖ≥ = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢¬≤ - Œº‚Çò‚Çó‚Çë¬≤\nÂØπ œÉ¬≤‚Çò‚Çó‚Çë Ê±ÇÊúüÊúõÔºö\nE[ œÉ¬≤‚Çò‚Çó‚Çë ] = E [ 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢ - Œº‚Çò‚Çó‚Çë)¬≤ ] = E [ 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢¬≤ - Œº‚Çò‚Çó‚Çë¬≤ ] = E [ 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢¬≤ -Œº¬≤ - (Œº‚Çò‚Çó‚Çë¬≤-Œº¬≤) ] ÔºåÊ∑ªÂä†Œº¬≤ Ê®™Á≠âÂèòÊç¢ = E [ 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢¬≤ -Œº¬≤] - E [Œº‚Çò‚Çó‚Çë¬≤-Œº¬≤] ÔºåÊãÜÊàê2‰∏™ÊúüÊúõ ÂÖàÁúãÁ¨¨ 1 È°πÔºö Êää Œº¬≤ ÂÜôÂà∞ ‚àë ÈáåÈù¢Ôºö E [ 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢¬≤ -Œº¬≤] = E [ 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢¬≤ -Œº¬≤)] ÔºåŒº¬≤‰∏éiÊó†ÂÖ≥ = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [E(x·µ¢¬≤ -Œº¬≤)] ÔºåÈáåÈù¢ÊòØ‰∏Ä‰∏™ÊúüÊúõÔºåÊääÂ§ñÈù¢ÁöÑÊúüÊúõÂ±ïÂºÄ = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ E(x·µ¢¬≤) - E(Œº¬≤) ] ÔºåŒº¬≤ÊòØÂ∏∏Êï∞ = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ E(x·µ¢¬≤) - Œº¬≤ ] ÔºåÂõ†‰∏∫ Œº ÊòØÈöèÊú∫ÂèòÈáè x·µ¢ ÁöÑÊúüÊúõÔºöE(x·µ¢)=Œº = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ E(x·µ¢¬≤) - E(x·µ¢)¬≤ ] ÔºåËøôÊòØx·µ¢ ÁöÑÊñπÂ∑ÆÔºàÂÆö‰πâÔºâ\n= 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ œÉ¬≤ = œÉ¬≤\n‰πüÂ∞±ÊòØËØ¥ÔºåÁ¨¨ 1 È°πÊòØ œÉ¬≤ÔºåÂØπ‰∫éÁ¨¨ 2 È°πÔºö E [Œº‚Çò‚Çó‚Çë¬≤-Œº¬≤] = E (Œº‚Çò‚Çó‚Çë¬≤) - E(Œº¬≤) = E (Œº‚Çò‚Çó‚Çë¬≤) - Œº¬≤ = E (Œº‚Çò‚Çó‚Çë¬≤) - E¬≤(Œº‚Çò‚Çó‚Çë) = Var(Œº‚Çò‚Çó‚Çë) ÔºåËøôÊòØ Œº‚Çò‚Çó‚Çë ÁöÑÊñπÂ∑Æ = Var( 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ x·µ¢ ) ÔºåÊää1/N ÊèêÂá∫Êù•Ôºå‰ºöÂèòÊàêÂπ≥Êñπ = 1/N¬≤ ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ Var(x·µ¢) = 1/N¬≤ ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ œÉ¬≤ = 1/N ‚ãÖ œÉ¬≤\nÊâÄ‰ª• œÉ¬≤‚Çò‚Çó‚Çë ÁöÑÊúüÊúõÁ≠â‰∫é‰∏äÈù¢‰∏§È°πÁõ∏ÂáèÔºö E[ œÉ¬≤‚Çò‚Çó‚Çë ] = œÉ¬≤ - 1/N ‚ãÖ œÉ¬≤ = (N-1)/N ‚ãÖ œÉ¬≤\nœÉ¬≤ ÁöÑÊó†ÂÅè‰º∞ËÆ°ÊòØ œÉ¬≤ = 1/(N-1) ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº‚Çò‚Çó‚Çë)¬≤\n2. È´òÊñØÂàÜÂ∏É2-ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºàÊó†ÂÅè‰º∞ËÆ°VSÊúâÂÅè‰º∞ËÆ°Ôºâ P2\n‰∏Ä‰∏™‰º∞ËÆ°Èáè T(Œ∏) ÁöÑÊúüÊúõ E(T(Œ∏) Á≠â‰∫éÂÆÉÁöÑÊú¨Ë∫´ÊúÄÂàùÁöÑÂÄºÔºåËøô‰∏™ÂèÇÊï∞ÁöÑ‰º∞ËÆ°ÊòØÊó†ÂÅèÁöÑÔºåÊØîÂ¶Ç E(Œº^) = Œº; E(œÉ^) = œÉ„ÄÇÂ¶ÇÊûú‰∏çÁõ∏Á≠âÂ∞±ÊòØÊúâÂÅèÁöÑ„ÄÇ\nÁî®MLE‰º∞ËÆ°Âá∫Êù•ÁöÑÊñπÂ∑Æ œÉ¬≤‚Çò‚Çó‚Çë ÁöÑÊúüÊúõÂ∞è‰∫éÊ®°ÂûãÁöÑÁúüÂÆûÊñπÂ∑Æ„ÄÇ Âõ†‰∏∫ËÆ°ÁÆóÊñπÂ∑ÆÊó∂ÔºåËÆ°ÁÆóÁöÑÊòØÊ†∑Êú¨Âà∞Ê†∑Êú¨ÂùáÂÄº ÁöÑË∑ùÁ¶ªÔºåËÄå‰∏çÊòØÂà∞ÁúüÂÆûÂùáÂÄºÁöÑË∑ùÁ¶ªÔºåÂõ†‰∏∫ÁúüÂÆûÂùáÂÄºË¶ÅÂÅöÊó†Êï∞Ê¨°ËØïÈ™åÊâçËÉΩÂæóÂà∞ÔºàÈô§ÈùûŒº=Ê†∑Êú¨ÂùáÂÄºÔºâ„ÄÇ Â¶ÇÊûúÁî®Ê†∑Êú¨ÂùáÂÄº‰ª£ÊõøÁúüÂÆûÂùáÂÄºŒº ÁöÑËØùÔºöVar(x)= E [(x-Œº)¬≤] ‚ûî Var(x)= E [( x-xÃÑ )]¬≤ÔºåÈô§Èùû Œº=Ê†∑Êú¨ÂùáÂÄº x‚ÅªÔºåVar(x) = E( x-xÃÑ )¬≤ \u0026lt; E(x-Œº)¬≤„ÄÇ ËØÅÊòéÂ¶Ç‰∏ãÊ†∑Êú¨ÊñπÂ∑Æ‰∏éÊÄª‰ΩìÊñπÂ∑Æ - Â∞èÊó∂ÂÄôÊå∫Ëèú -ÂçöÂÆ¢Âõ≠Ôºö\n(1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫(x·µ¢-x‚Åª)¬≤ = (1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [(x·µ¢-Œº)+ (Œº-x‚Åª)]¬≤ = (1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤ + (2/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫(x·µ¢-Œº)‚ãÖ(Œº-x‚Åª) + (1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫(Œº-x‚Åª)¬≤ = (1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤ + 2(x‚Åª-Œº)(Œº-x‚Åª) + (Œº-x‚Åª)¬≤ = (1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤ - (Œº-x‚Åª)¬≤\nÂ¶ÇÊûú Œº ‚â† x‚ÅªÔºåÂàô (1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫(x·µ¢-x‚Åª)¬≤ \u0026lt; (1/n)‚ãÖ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (x·µ¢-Œº)¬≤\nMLE ÊòØÁÇπ‰º∞ËÆ°Ôºå‰ºöÈÄ†ÊàêÂÅèÂ∑Æ\n3. È´òÊñØÂàÜÂ∏É3-‰ªéÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ËßíÂ∫¶ËßÇÂØü P3\nÂ§öÁª¥È´òÊñØÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞: ÂØπ‰∫é‰∏Ä‰∏™ p Áª¥ÁöÑÈöèÊú∫ÂêëÈáè ùê±‚àà‚Ñù·µñ Êúç‰ªéÂ§öÁª¥ÁöÑÈ´òÊñØÂàÜÂ∏ÉÔºåÂÖ∂Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏∫Ôºö\nùê±ÔΩûN(ùõç,Œ£)= 1/(2œÄ·µñ·êü¬≤‚ãÖ|Œ£|¬π·êü¬≤) ‚ãÖ exp(-¬Ω(ùê±-ùõç)·µÄ‚ãÖŒ£‚Åª¬π‚ãÖ(ùê±-ùõç))Ôºå ÂÖ∂‰∏≠Œº ÊòØÊúüÊúõ, Œ£ÊòØÊñπÂ∑ÆÁü©ÈòµÔºåexpÈáåÈù¢-1/2ÂêéÈù¢ÊòØÁ∫ø‰ª£‰∏≠ÁöÑ‰∫åÊ¨°Âûã\nÂØπ‰∫é‰∏Ä‰∏™Ê†∑Êú¨ÔºàÈöèÊú∫ÂêëÈáèÔºâp‰∏™Áª¥Â∫¶: ùê± = (x1,\\ x2,\\ \u0026hellip;,\\ xp)\nùõç ‰πüÊòØ p Áª¥ÁöÑÂêëÈáèÔºöùõç = (Œº1,\\ Œº,\\ \u0026hellip;,\\ Œºp)\nŒ£ Â∞±ÊòØ p√óp Áª¥ÁöÑÁü©ÈòµÔºö\nŒ£ =(œÉ11, œÉ12, \u0026hellip;, œÉ1p œÉ21, œÉ22, \u0026hellip;, œÉ2p ‚ãÆ ‚ãÆ ‚ãÆ ‚ãÆ œÉp1, œÉp2, \u0026hellip;, œÉpp)\nÈÄöÂ∏∏ÔºåËøô‰∏™Áü©ÈòµŒ£ÊòØÂçäÊ≠£ÂÆöÁöÑÔºåËÄå‰∏îÊòØÊ≤øÂØπËßíÁ∫øÂØπÁß∞ÁöÑÔºåÊØîÂ¶Ç œÉ12 = œÉ21„ÄÇ Âú®Êú¨ËäÇ‰∏≠ÂÅáËÆæ Œ£ ÊòØÊ≠£ÂÆöÁöÑÔºå‰ª•‰æøÂèôËø∞„ÄÇ\nÂú® PDF ‰∏≠Ôºåùê± ÊòØËá™ÂèòÈáèÔºåùõç,Œ£ ÊòØÂèÇÊï∞„ÄÇÂºè‰∏≠‰∏é ùê± Áõ∏ÂÖ≥ÁöÑÂè™Êúâ (ùê±-ùõç)·µÄÔºåÂÖ∂‰ªñÈÉ®ÂàÜËÆ§‰∏∫ÊòØÁ≥ªÊï∞ÔºåÊâÄ‰ª•ÈõÜ‰∏≠Áúã‰∏Ä‰∏ã exp ‰∏≠ÁöÑÈÉ®ÂàÜÔºö\n(ùê±-ùõç)·µÄ‚ãÖŒ£‚Åª¬π‚ãÖ(ùê±-ùõç) ÊòØ‰∏Ä‰∏™Ê†áÈáèÔºåËøô‰∏™ÂáΩÊï∞ÂèØ‰ª•Áúã‰Ωú È©¨Ê∞èË∑ùÁ¶ªÔºå‰∏§‰∏™ÂêëÈáèÔºöùê± Âíå ùõç ‰πãÈó¥ÁöÑË∑ùÁ¶ª\nÈ©¨Ê∞èË∑ùÁ¶ª Mahalanobis distance is used to measure multivariate distances between a point and a normal distribution with covariance ùö∫. janakiev-blog\nÂÅáËÆæÊúâ‰∏§‰∏™‰∫åÁª¥ÁöÑÂêëÈáèÔºöùê≥1=(z11,\\\\ z12)Ôºåùê≥2=(z21,\\\\ z22)\nÊ†πÊçÆÂÆö‰πâÊ±Ç‰∏§ÂêëÈáè‰πãÈó¥ÁöÑË∑ùÁ¶ªÔºö (ùê≥1-ùê≥2)·µÄ ‚ãÖ Œ£‚Åª¬π ‚ãÖ (ùê≥1-ùê≥2) = (z11-z21, z12-z22)·µÄ ‚ãÖ Œ£‚Åª¬π ‚ãÖ (z11-z21, \\\\ z12-z22)\nÂ¶ÇÊûúÊñπÂ∑ÆÁü©Èòµ Œ£ ÊòØÂçï‰ΩçÁü©ÈòµÔºöŒ£=IÔºåÈ©¨Ê∞èË∑ùÁ¶ªÂ∞±ÊòØÊ¨ßÊ∞èË∑ùÁ¶ª\n(z11-z21, z12-z22)·µÄ ‚ãÖ I ‚ãÖ (z11-z21, \\\\ z12-z22) = (z11-z21)¬≤ + (z12-z22)¬≤\nÊñπÂ∑ÆÁü©Èòµ Âõ†‰∏∫ÂÅáËÆæ‰∫Ü Œ£ ÊòØÊ≠£ÂÆöÁöÑÔºàÊØè‰∏™ÁâπÂæÅÂÄºŒª·µ¢ÈÉΩÊòØÂ§ß‰∫é0ÁöÑÔºå‰∏çËÉΩÁ≠â‰∫é0ÔºâÔºåËÄå‰∏îÊòØÂØπÁß∞ÁöÑÔºåÊâÄ‰ª•ÂØπ Œ£ ÂÅö‰∏Ä‰∏™ÁâπÂæÅÂÄºÂàÜËß£Ôºö\nŒ£ = UŒõU·µÄÔºåÂÖ∂‰∏≠ U ÊòØÊ≠£‰∫§Áü©ÈòµÔºöUU·µÄ=U·µÄU=IÔºåU=(ùêÆ‚ÇÅ,ùêÆ‚ÇÇ,\u0026hellip;, ùêÆ‚Çö)ÔºåÊØè‰∏™Â∞è ùêÆ ÊòØÂàóÂêëÈáèÔºåÊâÄ‰ª•UÊòØp√ópÁöÑÁü©ÈòµÔºõŒõ ÊòØÂØπËßíÁöÑÔºöŒõ=diag(Œª·µ¢), i=1,\u0026hellip;pÔºõ\nÊääÁü©ÈòµÂΩ¢ÂºèÂ±ïÂºÄÔºö\nŒ£ = UŒõU·µÄ = (ùêÆ‚ÇÅ,ùêÆ‚ÇÇ,\u0026hellip;, ùêÆ‚Çö) ‚ãÖ\n(Œª‚ÇÅ, 0, 0, \u0026hellip; 0 0, Œª‚ÇÇ, 0, \u0026hellip; 0 ‚ãÆ‚ãÆ‚ãÆ‚ãÆ 0, 0, 0, \u0026hellip; Œª‚Çö) ‚ãÖ (ùêÆ‚ÇÅ·µÄ,\\\\ ùêÆ‚ÇÇ·µÄ,\\\\ \u0026hellip;,\\\\ ùêÆ‚Çö·µÄ) = (ùêÆ‚ÇÅŒª‚ÇÅ, ùêÆ‚ÇÇŒª‚ÇÇ, \u0026hellip;, ùêÆ‚ÇöŒª‚Çö) ‚ãÖ (ùêÆ‚ÇÅ·µÄ,\\\\ ùêÆ‚ÇÇ·µÄ,\\\\ \u0026hellip;, ùêÆ‚Çö·µÄ) = ‚àë·µ¢‚Çå‚ÇÅ·µñ ùêÆ·µ¢Œª·µ¢ùêÆ·µ¢·µÄ\nŒ£‚Åª¬π = (UŒõU·µÄ)‚Åª¬π = (U·µÄ)‚Åª¬π Œõ‚Åª¬π U‚Åª¬π = U Œõ‚Åª¬π U‚Åª¬π ÔºåÔºàÊ≠£‰∫§Áü©ÈòµÁöÑËΩ¨ÁΩÆÁ≠â‰∫éÂÆÉÁöÑÈÄÜÔºâÔºåÂÖ∂‰∏≠ÁâπÂæÅÂÄºÁü©Èòµ Œõ‚Åª¬π = diag(1/ Œª·µ¢), ·µ¢=1,\u0026hellip;,p = ‚àë·µ¢‚Çå‚ÇÅ·µñ ùêÆ·µ¢ (1/Œª·µ¢) ùêÆ·µ¢·µÄ\nÊää Œ£‚Åª¬π ‰ª£ÂÖ•È©¨Ê∞èË∑ùÁ¶ªÔºö\n(ùê±-ùõç)·µÄ‚ãÖŒ£‚Åª¬π‚ãÖ(ùê±-ùõç) = (ùê±-ùõç)·µÄ ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·µñ [ ùêÆ·µ¢ (1/Œª·µ¢) ùêÆ·µ¢·µÄ ] ‚ãÖ (ùê±-ùõç) ÔºåÊää(ùê±-ùõç) ÊîæÂà∞ Œ£ ÈáåÈù¢ = ‚àë·µ¢‚Çå‚ÇÅ·µñ [ (ùê±-ùõç)·µÄ ‚ãÖ ùêÆ·µ¢ (1/Œª·µ¢) ùêÆ·µ¢·µÄ ‚ãÖ (ùê±-ùõç) ]\nÂÆö‰πâ‰∏Ä‰∏™ÂêëÈáè ùê≤ = (y‚ÇÅ, y‚ÇÇ, \u0026hellip;, y‚Çö)ÔºåÂÖ∂‰∏≠ÊØè‰∏ÄÁª¥ÊòØ‰∏Ä‰∏™Ê†áÈáèÔºöy·µ¢ = (ùê±-ùõç)·µÄ ‚ãÖ ùêÆ·µ¢ ÊâÄ‰ª•‰∏äÈù¢ÁöÑÈ©¨Ê∞èË∑ùÁ¶ªÂèØ‰ª•ÂÜôÊàêÔºö (ùê±-ùõç)·µÄ‚ãÖŒ£‚Åª¬π‚ãÖ(ùê±-ùõç) = ‚àë·µ¢‚Çå‚ÇÅ·µñ [ y·µ¢ (1/Œª·µ¢) y·µ¢·µÄ ] = ‚àë·µ¢‚Çå‚ÇÅ·µñ [ y·µ¢¬≤ (1/Œª·µ¢) ] Ôºå y·µ¢ ÊòØÊ†áÈáèÔºåÂ∞±ÊòØÂπ≥Êñπ\nÂÅáËÆæ p=2ÔºåÈ©¨Ê∞èË∑ùÁ¶ª= y‚ÇÅ¬≤/Œª·µ¢ + y‚ÇÇ¬≤/Œª‚ÇÇÔºåËÄå‰∏î‰ª§ È©¨Ê∞èË∑ùÁ¶ª= 1: y‚ÇÅ¬≤/Œª‚ÇÅ + y‚ÇÇ¬≤/Œª‚ÇÇ = 1ÔºåÂ∞±ÂæóÂà∞‰∏Ä‰∏™Ê§≠ÂúÜÊõ≤Á∫ø\nx1 Âíå x2 ÊòØÂéüÊù•ÁöÑÂùêÊ†áËΩ¥ÔºåùêÆ‚ÇÅ, ùêÆ‚ÇÇ ÊòØÊñ∞ÁöÑÂü∫ÂêëÈáè„ÄÇÊää x·µ¢ ÂèòÊç¢Êàê y·µ¢ Â∞±ÊòØÂêëÈáè ùê± ÂáèÂéªÂùáÂÄº‰πãÂêéÔºàÂéª‰∏≠ÂøÉÂåñÔºâÔºåÁÑ∂ÂêéÊäïÂΩ±Âà∞ÂêëÈáè ùêÆ·µ¢ ‰∏äÂæóÂà∞ÁöÑÂùêÊ†á„ÄÇ\nùê± Âíå ùõç ‰πãÈó¥ÁöÑÈ©¨Ê∞èË∑ùÁ¶ªÔºåÂú®ùêÆ‚ÇÅ, ùêÆ‚ÇÇ ÁöÑÂùêÊ†áÁ≥ª‰∏ãÊòØ‰∏Ä‰∏™Ê§≠ÂúÜÊõ≤Á∫øÔºå‰∏çÂêåÁöÑÊ†∑Êú¨xÂØπÂ∫îÊõ≤Á∫ø‰∏ä‰∏çÂêåÁöÑÁÇπ„ÄÇ Â¶ÇÊûú Œª‚ÇÅ \u0026gt; Œª‚ÇÇÔºåÂàôÂçäÈïøËΩ¥=‚àöŒª‚ÇÅÔºõÂçäÁü≠ËΩ¥=‚àöŒª‚ÇÇ\nùê± Âíå ùõç ‰πãÈó¥ÁöÑÈ©¨Ê∞èË∑ùÁ¶ªÂèØ‰ª•ÊòØ‰ªªÊÑèÂÄºÔºåÂç≥Ê§≠ÂúÜÊñπÁ®ã‰∏çÁ≠â‰∫é1ÔºåÂèñ‰∏çÂêåÁöÑÂÄº rÔºö\ny‚ÇÅ¬≤/Œª‚ÇÅ + y‚ÇÇ¬≤/Œª‚ÇÇ = r\nÂõ†‰∏∫È©¨Ê∞èË∑ùÁ¶ªÊòØÊ¶ÇÁéáÂØÜÂ∫¶‰∏≠ÁöÑ‰∏ÄÈ°πÔºåÈ©¨Ê∞èË∑ùÁ¶ª‰∏çÂêåÂØπÂ∫îÁöÑÊ¶ÇÁéáÂ∞±‰∏çÂêåÔºår-\u0026gt; p(r)\nÂΩì r Âèñ‰∏çÂêåÂÄºÁöÑÊó∂ÂÄôÔºåÂ∞±ÊòØ‰∏ÄÂúà‰∏ÄÂúàÁöÑÊ§≠ÂúÜ„ÄÇÂØπ‰∫é‰∏Ä‰∏™‰∏§Áª¥ÁöÑÈöèÊú∫ÂèòÈáè ùê±=(x1,x2)ÔºåÂÆÉÁöÑÊ¶ÇÁéáÂÄºyÊòØÁ¨¨3Áª¥ÔºåÊâÄ‰ª•Âú®Ëøô‰∏™‰∏âÁª¥Á©∫Èó¥‰∏≠ÔºåÂ¶ÇÊûúÂõ∫ÂÆö y ÂÄºÔºàÂç≥rÂÄºÔºâÔºåÂ∞±ÊòØÂØπÊõ≤Èù¢Ê∞¥Âπ≥Ê®™Âàá‰∫Ü‰∏ÄÂàÄÔºåÂæóÂà∞‰∏ÄÊù°Á≠âÈ´òÁ∫øÔºåÊòØÂú®ÁâπÂæÅÂêëÈáè ùêÆ ‰∏ãÁöÑÊ§≠ÂúÜ\nÂ¶ÇÊûú Œ£ ÂàÜËß£Âá∫Êù•ÁöÑÊâÄÊúâÁöÑÁâπÂæÅÂÄº Œª·µ¢ ÈÉΩÁõ∏ÂêåÔºåÊõ≤Á∫øÂ∞±ÂèòÊàêÂúÜ‰∫ÜÔºåÂú®ÂêÑ‰∏™ËΩ¥‰∏äÁöÑÊäïÂΩ±ÈÉΩÁõ∏Á≠â„ÄÇ\n4. È´òÊñØÂàÜÂ∏É4-Â±ÄÈôêÊÄß P4\nÂú®È´òÁª¥È´òÊñØÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏≠ÔºåÂè™Êúâ exp ÈáåÈù¢ÁöÑ‚Äú‰∫åÊ¨°Âûã‚Äù‰∏éÊ†∑Êú¨ x Áõ∏ÂÖ≥ÔºåËÄå‰∏îÊòØùê± Âíå ùõç ‰πãÈó¥ÁöÑÈ©¨Ê∞èË∑ùÁ¶ªÔºå ÂØπÊ≠£ÂÆöÁü©Èòµ Œ£ ÂÅöÁâπÂæÅÂÄºÂàÜËß£ÔºåÂèØ‰ª•ÂèëÁé∞‚Äú‰∫åÊ¨°Âûã‚ÄùÂØπÂ∫î‰∫é‰ª•ÁâπÂæÅÂêëÈáè ùêÆ ‰∏∫Âü∫Â∫ïÁöÑÂùêÊ†áÁ≥ª‰∏ãÁöÑÊ†áÂáÜÊ§≠ÂúÜÊõ≤Á∫øÔºå\nÁî® Œî Ë°®Á§∫‰∫åÊ¨°ÂûãÔºåÂàôÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Ôºöp(x)=1/(2œÄ·µñ·êü¬≤‚ãÖ|Œ£|¬π·êü¬≤) ‚ãÖ exp(-Œî/2)\nÂú®Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏≠ÔºåÂè™Êúâ x ÊòØËá™ÂèòÈáèÔºåùõç,Œ£ ÈÉΩÊòØÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇ ÁªôÂÆö‰∏Ä‰∏™Ê¶ÇÁéáÂÄº 0 \u0026lt;= p(x) \u0026lt;= 1ÔºåÊØîÂ¶Ç 0.5ÔºåÂÆÉ‰ºöÂØπÂ∫î‰∏Ä‰∏™ Œî= r1ÔºåÂØπÂ∫îÂà∞Ê§≠ÂúÜÊñπÁ®ãÁöÑÂè≥‰æßÔºåÂÜçÊää r1 ‰πòÂà∞Â∑¶ËæπÂ∞±ÂèòÊàê‰∫ÜÊ†áÂáÜÁöÑÊ§≠ÂúÜÊñπÁ®ã„ÄÇ ÂΩì p=0.5, Âàô Œî= r2Ôºå‰ºöÂæóÂà∞Âè¶‰∏Ä‰∏™Ê§≠ÂúÜÊõ≤Á∫ø„ÄÇ Â¶ÇÊûúÊääÊâÄÊúâÊ¶ÇÁéáÂèñ‰∏ÄÈÅçÔºåÂ∞±ÂØπÂ∫î‰∏Ä‰∏™Êõ≤Èù¢„ÄÇÊ§≠ÂúÜÂØπÂ∫î‰∏ÄÊù°Êù°Á≠âÈ´òÁ∫ø\n(1) ÊñπÂ∑ÆÁü©ÈòµÂèÇÊï∞Â§™Â§ö Œ£‚Çö‚Çì‚Çö Êúâ p¬≤ ‰∏™ÂèÇÊï∞Ôºå‰ΩÜÂèàÂõ†‰∏∫ÂÆÉÊòØÂØπÁß∞ÁöÑÔºåÂÆûÈôÖÁöÑÂèÇÊï∞‰∏™Êï∞= (p¬≤-p)/2 + p = (p¬≤+p)/2 = p(p+1)/2 = O(p¬≤)\nÊó∂Èó¥Â§çÊùÇÂ∫¶ÊòØÁª¥Êï∞ÁöÑÂπ≥ÊñπÔºåÂú®È´òÁª¥ÈóÆÈ¢ò‰∏≠Ôºåp ‰ºöÂæàÂ§ßÔºåÂèÇÊï∞Â§™Â§öÔºåËÆ°ÁÆóÂ§™Â§çÊùÇ„ÄÇ\nÂèØ‰ª•ÁÆÄÂåñÊñπÂ∑ÆÁü©ÈòµÔºöÂÅáËÆæ Œ£ ÊòØÂØπËßíÁü©ÈòµÔºåÂè™ÊúâÂØπËßíÁ∫ø‰∏äÊúâÂÄº (Œª‚ÇÅ 0 0 \u0026hellip; 0 \\ 0 Œª‚ÇÇ 0 \u0026hellip; 0 \\ \u0026hellip; \\ 0 0 0 \u0026hellip; Œª‚Çö)\nÂõ†‰∏∫ÂÆÉÊòØÂØπËßíÁü©ÈòµÔºàÂ∑≤ÁªèÁõ∏‰∫íÁã¨Á´ãÔºâÔºåÂ∞±‰∏çÈúÄË¶ÅÂØπÂÆÉÂÅöÁâπÂæÅÂÄºÂàÜËß£‰∫ÜÔºåŒ£Â∞±Áî®Ëá™Â∑±ÔºåÊ≤°ÊúâÂºïÂÖ•Êñ∞Âü∫ÂêëÈáè U=(ùêÆ‚ÇÅ,ùêÆ‚ÇÇ,\u0026hellip;, ùêÆ‚Çö)ÔºåUÂ∞±ÊòØÂçï‰ΩçÁü©ÈòµÔºåÊâÄ‰ª•Âü∫ÂêëÈáèËøòÊòØ (ùê±‚ÇÅ,ùê±‚ÇÇ,\u0026hellip;, ùê±‚Çö)Ôºå‰ªçÂêëùê±ÊäïÂΩ± journeyc ÁöÑËØÑËÆ∫\nŒî = (ùê±-ùõç)·µÄ‚ãÖŒ£‚Åª¬π‚ãÖ(ùê±-ùõç) = ‚àë·µ¢‚Çå‚ÇÅ·µñ (ùê±·µ¢-ùõç·µ¢)¬≤ (1/Œª·µ¢)\nÂú®ÂêÑ‰∏™ËΩ¥‰∏äÁöÑÊäïÂΩ±‰∏∫ y·µ¢ = (ùê±-ùõç)·µÄ ‚ãÖ ùê±·µ¢\nÊâÄ‰ª•ËøôÈáåÁöÑÊ§≠ÂúÜÂ∞±ÊòØÂú® (ùê±‚ÇÅ,ùê±‚ÇÇ,\u0026hellip;, ùê±‚Çö) ‰∏ãÁöÑÔºåÈïøÁü≠ËΩ¥‰∏éÂêÑxËΩ¥Âπ≥Ë°åÔºåËÄåÊ≤°ÊúâÊóãËΩ¨\nÂ¶ÇÊûú ‚àë ÊòØÂØπËßíÈòµÔºåÂπ∂‰∏î Œª‚ÇÅ = Œª‚ÇÇ = \u0026hellip; = Œª‚ÇöÔºåÊõ≤Á∫øÂ∞±ÂèòÊàê‚ÄúÊ≠£ÁöÑ‚ÄùÂúÜÂΩ¢ÔºåÁß∞‰∏∫ÂêÑÂêëÂêåÊÄßÁöÑÈ´òÊñØÂàÜÂ∏É\nÈÄöËøáÁÆÄÂåñÊñπÂ∑ÆÁü©ÈòµÔºåÂèòÊàêÂêÑÂêëÂêåÊÄßÁöÑÔºåËß£ÂÜ≥ÂèÇÊï∞ËøáÂ§öÁöÑÈóÆÈ¢ò„ÄÇ ÊØîÂ¶ÇÂú®Âõ†Â≠êÂàÜÊûê‰∏≠ÔºåÂÅáËÆæÈöêÂèòÈáè z ÊòØÂØπËßíÁü©Èòµ„ÄÇÊ¶ÇÁéáPCA ÔºàP-PCA) Â∞±ÊòØÂõ†Â≠êÂàÜÊûêÁöÑÁâπÊÆäÊÉÖÂÜµÔºåÂÅáËÆæ z ÊòØÂêÑÂêëÂêåÊÄßÁöÑÊ¶ÇÁéáÂàÜÂ∏É\n(2) ‰∏Ä‰∏™È´òÊñØË°®ËææÂäõÊúâÈôê ‰ªÖÁî®‰∏Ä‰∏™È´òÊñØÂàÜÂ∏ÉÔºåÂØπÊ®°ÂûãÁöÑÊèèËø∞ÂèØËÉΩ‰∏çÂáÜÁ°Æ„ÄÇGMM ÊòØÂ§ö‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÊ∑∑Âêà„ÄÇ\n5. È´òÊñØÂàÜÂ∏É5-Â∑≤Áü•ËÅîÂêàÊ¶ÇÁéáÊ±ÇËæπÁºòÊ¶ÇÁéáÂèäÊù°‰ª∂Ê¶ÇÁéá P5\nÂ∑≤Áü•‰∏Ä‰∏™Â§öÁª¥È´òÊñØÂàÜÂ∏ÉÔºåÊ±ÇÂÆÉÁöÑËæπÁºòÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÂíåÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏É\nÊää p Áª¥ÁöÑÈöèÊú∫ÂêëÈáè ùê± Áúã‰Ωú‰∏§ÁªÑÁöÑËÅîÂêàÔºöùê±=(ùê±‚Çê,\\ ùê±b)ÔºåÂÖ∂‰∏≠ x‚Çê‚àà‚Ñù·µê, xb‚àà‚Ñù‚Åø, m+n=p„ÄÇ Êää p Áª¥ÁöÑÊúüÊúõ ùõç ‰πüÂàÜÊàê‰∏§ÁªÑ: ùõç = (ùõç‚Çê,\\ ùõçb)Ôºõ ÊääÊñπÂ∑ÆÁü©Èòµ ‚àë ÊãÜÊàê 4 ÂùóÔºö‚àë = (‚àë‚Çê‚Çê , ‚àë‚Çêb \\ ‚àëb‚Çê , ‚àëbb)ÔºåËøôÊòØÂØπÁß∞Áü©ÈòµÔºåÊâÄ‰ª•‚àë‚Çêb = ‚àëb‚Çê\nÁÑ∂ÂêéÊääÈöèÊú∫ÂêëÈáè ùê± Áúã‰ΩúÊòØ (ùê±‚Çê,ùê±b) ÁöÑËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÊ±ÇËæπÁºòÊ¶ÇÁéáÂàÜÂ∏É P(ùê±‚Çê)Ôºå‰ª•ÂèäÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏É P(ùê±b|ùê±‚Çê)„ÄÇÊ†πÊçÆÂØπÁß∞ÊÄßÔºåP(ùê±b) Âíå P(ùê±‚Çê|ùê±b) ‰πüÂèØÊ±ÇÂæó\nÊ±ÇËß£ÊñπÊ≥ïÔºöÈÖçÊñπÊ≥ïÔºàPRML‰∏≠ÔºâÔºåËøôËäÇ‰ºöÈááÁî®‰∏ÄÁßçÊØîÈÖçÊñπÊ≥ïÁ®çÂæÆÁÆÄÂçï‰∏ÄÁÇπÁöÑÊñπÊ≥ï\nÈ¶ñÂÖàÂºïÂÖ•‰∏Ä‰∏™ÂÆöÁêÜÔºö Â∑≤Áü•‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáè x Êúç‰ªéÈ´òÊñØÂàÜÂ∏É xÔΩûN(Œº,Œ£)ÔºåÊúâ y=Ax+BÔºåÂàô y ‰πüÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºö yÔΩûN(AŒº+B, AŒ£A·µÄ)„ÄÇ Ôºàx ÊòØpÁª¥ÂêëÈáèÔºåy ÊòØ q Áª¥ÂêëÈáèÔºåÂàô A ÊòØqxp ÁöÑÁü©ÈòµÔºåŒ£ÊòØpxpÁöÑÔºåÂàô AŒ£A·µÄ ÊòØqxqÁöÑÔºâ\n‰∏ç‰∏•Ë∞®ÁöÑËß£ÈáäÔºö\nE[y] = E[Ax+B] = AE +B = AŒº+B\nVar[y] = Var [ Ax+B ] = Var [Ax] + Var [B] , BÊòØÂ∏∏Êï∞ÊñπÂ∑Æ=0 = A Var A·µÄ = AŒ£A·µÄ\nÊØîÂ¶ÇÂØπ‰∏ÄÁª¥ÈöèÊú∫ÂèòÈáè xÔΩûN(Œº, œÉ¬≤)Ôºåy=ax+bÔºåÂàô Var[y] = Var[ax+b] = a¬≤Var = a¬≤‚ãÖœÉ¬≤„ÄÇÁõ¥ËßÇ‰∏äÁúãÔºå‰∏ÄÁª¥ÊòØa¬≤ÔºåÂ§öÁª¥Â∫îËØ•ÊòØAA·µÄ\nÊ±ÇËæπÁºòÊ¶ÇÁéá P(ùê±‚Çê) ÁöÑÂàÜÂ∏É ÊûÑÈÄ†‰∏Ä‰∏™Áü©ÈòµÔºö ùê±‚Çê = (I‚Çò 0) (ùê±‚Çê \\ ùê±b)\nÂÖ∂‰∏≠ (I‚Çò 0) ÂØπÂ∫î AÔºå(ùê±‚Çê \\ ùê±b) Â∞±ÊòØ xÔºåB=0\nÊ†πÊçÆ‰∏äËø∞ÂÆöÁêÜ:\nE[ùê±‚Çê] = E[Aùê±+B] = Aùõç+B = (I‚Çò 0) (ùõç‚Çê,\\ ùõçb) = ùõç‚Çê\nVar[ùê±‚Çê] = AŒ£A·µÄ = (I‚Çò 0) (‚àë‚Çê‚Çê , ‚àë‚Çêb \\ ‚àëb‚Çê , ‚àëbb) (I‚Çò \\ 0) = (I‚Çò ‚àë‚Çê‚Çê, I‚Çò ‚àë‚Çêb) (I‚Çò \\ 0) = ‚àë‚Çê‚Çê\nÊâÄ‰ª• ùê±‚ÇêÔΩûN(ùõç‚Çê, ‚àë‚Çê‚Çê)\nÊ±ÇÊù°‰ª∂Ê¶ÇÁéá P(ùê±b|ùê±‚Çê) ÁöÑÂàÜÂ∏É ÔºàÂèØÁî®ÈÖçÊñπÊ≥ïÔºâ\nÊûÑÈÄ†3‰∏™ÂèòÈáèÔºö\nÂÖàÂÆö‰πâ‰∏Ä‰∏™ x_{b‚ãÖa} ÁöÑÂèòÈáèÔºöx_{b‚ãÖa} = x_b - ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π x‚Çê „ÄÇ Â¶ÇÊûú x_{b‚ãÖa} ÁöÑÂàÜÂ∏ÉÁü•ÈÅì‰∫Ü x_{b‚ãÖa}ÔΩûN(ùõç^, ‚àë^)ÔºåÈÇ£‰πà x_b = x_{b‚ãÖa} + ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π x‚ÇêÔºåx_b ‰∏é x‚Çê ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂ∞±ÊâæÂà∞‰∫Ü„ÄÇ rationalizable ÁöÑËØÑËÆ∫Ôºö‚ÄúÂ¶ÇÊûúËÉΩÊâæÂà∞‰∏Ä‰∏™Á∫øÊÄßÂèòÊç¢ Z = Xa+C‚ãÖXbÔºå‰ΩøÂæóZ‰∏éXb‰∏çÁõ∏ÂÖ≥ Cov(Z, Xb)=0ÔºåÈÇ£‰πà Var(Xa|Xb) = Var(Z|Xb) = Var(Z)ÔºåE(Xa|Xb)=E(Z)-C‚ãÖXbÔºåÂ∞±ÈÉΩÂèØ‰ª•ËÆ°ÁÆóÂá∫Êù•‰∫Ü„ÄÇ‚Äù\n‰∏éÊ≠§ÂØπÂ∫îÔºåÂÆö‰πâ ùõç_{b‚ãÖa} = ùõç_b - ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π ùõç‚Çê„ÄÇ\nÂõ†‰∏∫ ‚àë ÊòØÂàÜÂùóÁü©ÈòµÔºåÊää‚àë_{bb‚ãÖa} ÂÆö‰πâ‰∏∫‚àë_{aa} ÁöÑËàíÂ∞îË°•Schur complement: ‚àë_{bb‚ãÖa} =‚àë_{bb} - ‚àë_{ba} ‚àë_{aa}‚Åª¬π ‚àë_{ab}\n(1) ÂÖàÊ±Ç x_{b‚ãÖa} ÁöÑÂàÜÂ∏É: x_{b‚ãÖa} = (- ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π, I‚Çô) (x‚Çê,\\\\ x_b) = (- ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π, I‚Çô) ùê±\nÊâÄ‰ª• (- ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π, I‚Çô) Â∞±ÊòØ A\nE[x_{b‚ãÖa}] = Aùõç+B = (- ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π, I‚Çô)‚ãÖ(ùõç‚Çê,\\\\ ùõçb) = ùõçb - ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π‚ãÖùõç‚Çê = ùõç_{b‚ãÖa}\nVar[x_{b‚ãÖa}] = AŒ£A·µÄ = (- ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π, I‚Çô) ‚ãÖ (‚àë‚Çê‚Çê , ‚àë‚Çêb \\\\ ‚àëb‚Çê , ‚àëbb) ‚ãÖ ((-‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π)·µÄ,\\\\ I‚Çô) = (- ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π, I‚Çô) ‚ãÖ (‚àë‚Çê‚Çê , ‚àë‚Çêb \\\\ ‚àëb‚Çê , ‚àëbb) ‚ãÖ (-‚àë‚Çê‚Çê‚Åª¬π ‚àë_b‚Çê·µÄ, \\\\ I) ÔºåÂÖ∂‰∏≠ ‚àë‚Çê‚Çê‚Åª¬π ÊòØÂØπÁß∞ÁöÑÔºåËΩ¨ÁΩÆÊ≤°Âèò = (- ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π ‚ãÖ ‚àë‚Çê‚Çê + ‚àëb‚Çê, -‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π ‚ãÖ ‚àë‚Çêb + ‚àëbb) ‚ãÖ (-‚àë‚Çê‚Çê‚Åª¬π ‚àë_b‚Çê·µÄ, \\\\ I)\nÂú®Á¨¨ 1 È°π‰∏≠Ôºå‚àë‚Çê‚Çê ÊòØÂèØÈÄÜÔºå‚àë‚Çê‚Çê‚Åª¬π ‚ãÖ ‚àë‚Çê‚Çê = IÔºåÁÑ∂Âêé -‚àë_b‚Çê + ‚àëb‚Çê = 0Ôºö\n= (0, -‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π ‚ãÖ ‚àë‚Çêb + ‚àëbb) ‚ãÖ (-‚àë‚Çê‚Çê‚Åª¬π ‚àë_b‚Çê·µÄ, \\\\ I)\nÂõ†‰∏∫‰∏§‰∏™ÂêëÈáèÂèòÈáèÁöÑÂçèÊñπÂ∑Æ Cov(X,Y) ‰∏é Cov(Y,X) ‰∫í‰∏∫ËΩ¨ÁΩÆÁü©ÈòµÔºå‰ΩÜÂÆÉËá™Ë∫´Âπ∂‰∏çÊòØÂØπÁß∞Áü©ÈòµÔºåÂõ†Ê≠§ËΩ¨ÁΩÆÂπ∂‰∏çÊòØÂÆÉËá™Â∑±: ‚àë_b‚Çê·µÄ ‚â† ‚àë_b‚Çê\n= ‚àëbb - ‚àë_b‚Çê ‚àë‚Çê‚Çê‚Åª¬π ‚ãÖ ‚àë‚Çêb ÔºåÂ∞±ÊòØÂÆö‰πâÁöÑ ‚àë_{bb‚ãÖa}\nÔºàÂºπÂπïÔºöÂ∫îËØ•ÊòØÊ†πÊçÆËàíÂ∞îË°•ÂèçÂêëÊûÑÈÄ†ÁöÑ x_{b‚ãÖa} Âíå ùõç_{b‚ãÖa}„ÄÇ\u0026ldquo;ÊûÑÈÄ†ÂûãËØÅÊòé\u0026rdquo;?Ôºâ\nÊâÄ‰ª•ÂæóÂá∫ÁªìËÆ∫Ôºö x_{b‚ãÖa} ÔΩû N(ùõç_{b‚ãÖa}, ‚àë_{bb‚ãÖa})\n(2) ËØÅÊòéÔºöx_{b‚ãÖa} ‰∏é x‚Çê Áõ∏‰∫íÁã¨Á´ã ‰∏ìÊ†è\nËã• x ‰∏∫Êúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÁöÑÈöèÊú∫ÂèòÈáè xÔΩûN(Œº,Œ£)ÔºåÂàôÁõ∏‰∫íÁã¨Á´ãÁöÑ‰∏§ÂèòÈáèÂ∞±‰∏çÁõ∏ÂÖ≥Ôºö M‚ãÖx ‚üÇ N‚ãÖx ‚áî M‚àëN =0„ÄÇ ÂÖ∂‰∏≠ MÔºåN Âùá‰∏∫Áü©ÈòµÔºåM‚ãÖxÔºåN‚ãÖx ‰πüÊúç‰ªéÈ´òÊñØÂàÜÂ∏É„ÄÇ\nËØÅÔºöÂõ†‰∏∫ xÔΩûN(Œº,Œ£)ÔºåÊâÄ‰ª• M‚ãÖxÔΩûN(MŒº, MŒ£M·µÄ)ÔºåN‚ãÖxÔΩûN(NŒº, NŒ£N·µÄ)ÔºåËÆ°ÁÆó‰∫åËÄÖÁöÑÂçèÊñπÂ∑ÆÁü©ÈòµÔºö\nCov(M‚ãÖxÔºåN‚ãÖx) = E[(M‚ãÖx-M‚ãÖŒº) (N‚ãÖx-N‚ãÖŒº)·µÄ] = E[ M ‚ãÖ (x-Œº)‚ãÖ(x-Œº)·µÄ ‚ãÖ N] = M‚ãÖE[ (x-Œº) (x-Œº)·µÄ ]‚ãÖN = MŒ£N·µÄ\nÂõ†‰∏∫ M‚ãÖx ‚üÇ N‚ãÖx ‰∏îÂùá‰∏∫È´òÊñØÂàÜÂ∏ÉÔºåÊâÄ‰ª• Cov(M‚ãÖxÔºåN‚ãÖx) = MŒ£N·µÄ = 0\nÂú®‰πãÂâçÁöÑÊé®ÂØº‰∏≠ÔºåÂºïÂÖ•‰∫Ü x_{b‚ãÖa} = x_b - ‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π ‚ãÖ x‚Çê ÂèØ‰ª•ÊîπÂÜô‰∏∫Ôºö\nx_{b‚ãÖa} = (-‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π, I) (x‚Çê,\\\\ x_b)ÔºåËøôÈáå (-‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π, I) ÂØπÂ∫î MÔºå(x‚Çê,\\\\ x_b) ÊòØ x\nx‚Çê = (I, 0) (x‚Çê,\\\\ x_b)ÔºåËøôÈáå(I, 0)ÂØπÂ∫î NÔºå(x‚Çê,\\\\ x_b) ÊòØ x\nx_{b‚ãÖa} Â∞±ÊòØ M‚ãÖxÔºåx‚Çê Â∞±ÊòØN‚ãÖxÔºå ÊâÄ‰ª• MŒ£N·µÄ = (-‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π, I) (‚àë‚Çê‚Çê , ‚àë‚Çêb \\\\ ‚àëb‚Çê , ‚àëbb) (I,\\\\ 0)ÔºåÂÖ∂‰∏≠‚àëÊòØxÁöÑÊñπÂ∑ÆÁü©Èòµ = (0, ‚àëbb-‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π) (I,\\\\ 0) = 0\nÊâÄ‰ª• x_{b‚ãÖa} ‰∏é x‚Çê Áõ∏‰∫íÁã¨Á´ãÂèØÊé®Âá∫ x_{b‚ãÖa} ‰∏é x‚Çê ‰∏çÁõ∏ÂÖ≥Ôºö x_{b‚ãÖa} ‚üÇ x‚Çê ‚áí x_{b‚ãÖa} | x‚Çê = x_{b‚ãÖa}\nÊ≥®ÊÑèÔºö\n‰∏ÄËà¨ÊÉÖÂÜµ‰∏ã‰∏§‰∏™ÈöèÊú∫ÂèòÈáè‰πãÈó¥Áã¨Á´ã‰∏ÄÂÆö‰∏çÁõ∏ÂÖ≥Ôºå‰∏çÁõ∏ÂÖ≥‰∏ç‰∏ÄÂÆöÁã¨Á´ãÔºà‰πüÂ∞±ÊòØÁã¨Á´ãÁöÑÊ¶ÇÂøµÊõ¥‚ÄúËãõÂàª‚Äù‰∏ÄÁÇπÔºå‰∏çÁõ∏ÂÖ≥ÁöÑÊ¶ÇÁéáÁ®çÂæÆ‚ÄúÂº±‚Äù‰∏ÄÁÇπÔºâ Â¶ÇÊûú‰∏§‰∏™ÈöèÊú∫ÂèòÈáèÂùáÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºåÈÇ£‰πà‚Äú‰∏çÁõ∏ÂÖ≥‚ÄùÁ≠â‰ª∑‰∫é‚ÄúÁã¨Á´ã‚Äù ÈöèÊú∫ÂèòÈáèÁã¨Á´ãÊòØÁî±ÂàÜÂ∏ÉÂáΩÊï∞ÂÆö‰πâÁöÑÔºåËÄå‰∏çÁõ∏ÂÖ≥Âè™ÊòØÁî®‰∏ÄÈò∂Áü©ÔºàÂç≥Êï∞Â≠¶ÊúüÊúõÔºâÂÆö‰πâÁöÑ„ÄÇÂàÜÂ∏ÉÂáΩÊï∞ÊòØÊØîÁü©Êõ¥È´òÁöÑÊ¶ÇÂøµÔºåÂàÜÂ∏ÉÂáΩÊï∞ËÉΩÂÜ≥ÂÆöÁü©ÔºåËÄåÁü©Êú™ÂøÖËÉΩÂÜ≥ÂÆöÂàÜÂ∏ÉÂáΩÊï∞„ÄÇ Áã¨Á´ãÂíå‰∫íÊñ•ÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºüÁã¨Á´ãÂíå‰∏çÁõ∏ÂÖ≥ÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºü - Ê≠¶Ëæ∞ÁöÑÊñáÁ´† -Áü•‰πé\n(3) ÂÜçÊ±Ç xb|x‚Çê ÁöÑÂàÜÂ∏É Áü•ÈÅì‰∫Ü x_{b‚ãÖ‚Çê} ÁöÑÂàÜÂ∏ÉÂêéÔºåÂèØÁü• x_bÔºö\nx_b = x_{b‚ãÖa} + ‚àë_{ba} ‚àë‚Çê‚Çê‚Åª¬π x‚Çê\nÂõ†‰∏∫ x_b ‰∏é x‚Çê ÊòØÁõ∏‰∫íÁã¨Á´ãÁöÑÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÁöÑÈöèÊú∫ÂèòÈáèÔºåÊâÄ‰ª•Ôºö\nx_b|x‚Çê = x_{b‚ãÖa}|x‚Çê - ‚àë_{ba} ‚àë‚Çê‚Çê‚Åª¬π x‚Çê | x‚Çê = x_b = x_{b‚ãÖa} - ‚àë_{ba} ‚àë‚Çê‚Çê‚Åª¬π x‚Çê\nËøòÊòØÂêåÊ†∑Â•óÁî®Ax+BÔºåÊää x_{b‚ãÖa} Áúã‰Ωú x, A=I, B=‚àë_b‚Çê ‚ãÖ ‚àë‚Çê‚Çê‚Åª¬π ‚ãÖ x‚Çê\nE [x_b|x‚Çê] = ùõç_{b‚ãÖa} + ‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π‚ãÖx‚Çê\nVar [x_b|xa] = A‚ãÖVar(x_{b‚ãÖa})‚ãÖA·µÄ = ‚àë_{bb‚ãÖa}\nÊâÄ‰ª• xb|x‚Çê ÔΩûN ( ùõç_{b‚ãÖa} + ‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π‚ãÖx‚Çê, ‚àë_{bb‚ãÖa} )\nÁî®ÂêåÊ†∑ÁöÑÊñπÊ≥ïÔºåÂèØÂæó x_bÔΩûN(ùõç_b, ‚àë_{bb})„ÄÇ\nP(x‚Çê|x_b) Â∞±ÊòØÊää P(xb|x‚Çê) ‰∏≠ÁöÑ a,b ÂØπÊç¢„ÄÇ\n6. È´òÊñØÂàÜÂ∏É6-Â∑≤Áü•ËæπÁºòÂíåÊù°‰ª∂Ê¶ÇÁéáÊ±ÇËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É P6\nÂ∑≤Áü•ÔºàËæπÁºòÊ¶ÇÁéáÂàÜÂ∏ÉÔºâp(x) = N(x | Œº,Œõ‚Åª¬π) ÂíåÔºàÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÔºâp(y|x) = N(y | Ax+b, L‚Åª¬π) ÔºàÂÖ∂‰∏≠ Œõ ÊòØÁ≤æÂ∫¶Áü©Èòµ=ÂçèÊñπÂ∑ÆÁü©Èòµ ‚àë ÁöÑÈÄÜÔºåŒõ‚Åª¬π=‚àëÔºâÔºå Âπ∂‰∏îÂÅáËÆæ y ÁöÑÊúüÊúõ‰∏é x ‰πãÈó¥ÊúâÁ∫øÊÄßÂÖ≥Á≥ªÔºöŒº_y = Ax+bÔºå‰ΩÜ‰∫åËÄÖÁöÑÊñπÂ∑Æ‰πãÈó¥Ê≤°ÂÖ≥Á≥ª Ê±Ç p(y)Ôºåp(x|y)„ÄÇÔºàÁ±ª‰ºº‰∫éË¥ùÂè∂ÊñØÂÆöÁêÜÔºöp(x|y) = p(y|x)p(x) / p(y)Ôºâ\nËøô‰∏™ÈóÆÈ¢òÁªèÂ∏∏Âú®Á∫øÊÄßÈ´òÊñØÊ®°Âûã‰∏≠Âá∫Áé∞ÔºåÊØîÂ¶ÇÂú®Âç°Â∞îÊõºÊª§Ê≥¢‰∏≠ÔºåÈöêÁä∂ÊÄÅÔºàÈ´òÊñØÂàÜÂ∏ÉÔºâ‰∏éËßÇÊµãÂèòÈáè‰πãÈó¥ÊúâÁ∫øÊÄßÂÖ≥Á≥ªÔºö z‚ÅΩ·µó‚Å∫¬π‚Åæ = A z‚ÅΩ·µó‚Åæ + B + ŒµÔºåŒµÊòØÂô™Â£∞ÔºåŒµÔΩûN(0,Q)ÔºåŒµ‰∏éz Áõ∏‰∫íÁã¨Á´ãÔºå x‚ÅΩ·µó‚Åæ= Cz‚ÅΩ·µó‚Åæ+D+Œ¥ÔºåŒ¥‰πüÊòØÈ´òÊñØÂô™Â£∞ Œ¥ÔΩûN(0,R)\nËøòÊØîÂ¶ÇÂú®Ê¶ÇÁéápca‰∏≠ÔºåÊää p Áª¥ÁöÑ x ÈôçÂà∞ q Áª¥ÁöÑ z Á©∫Èó¥ÔºåÊª°Ë∂≥Á∫øÊÄßÂÖ≥Á≥ª x = Wz+b+ŒµÔºåŒµ ÊòØÊúç‰ªéÂêÑÂêëÂêåÊÄßÁöÑÈ´òÊñØÂàÜÂ∏É ŒµÔΩûN(0, œÉ¬≤I)Ôºåz ÁöÑÂÖàÈ™åÂèØ‰ª•ÊòØÊ†áÂáÜÈ´òÊñØÂàÜÂ∏É zÔΩûN(0,I)ÔºåŒµ ‰∏é z Áõ∏‰∫íÁã¨Á´ã„ÄÇ‰πüÊòØ‰∏ÄÁßçÁ∫øÊÄßÈ´òÊñØÊ®°Âûã„ÄÇ\nÔºàÂú®PRML ‰∏≠‰ªç‰ΩøÁî®ÈÖçÊñπÊ≥ïÊ±ÇËß£ÔºåËÄå‰∏îÊØî‰∏ä‰∏ÄËäÇÁöÑÊé®ÂØºÊõ¥Â§çÊùÇÔºâ\n(1) Ê±Ç p(y) ‰æùÊçÆ‰∏§‰∏™ÂâçÊèêÊù°‰ª∂ÔºåÂèØ‰ª•Êää y ÂÆö‰πâ‰∏∫Ôºö y = Ax + b + ŒµÔºå‰ª§ Œµ ÊòØ‰∏Ä‰∏™È´òÊñØÂô™Â£∞ ŒµÔΩûN(0, L‚Åª¬π)Ôºåx„ÄÅy„ÄÅŒµ ÈÉΩÊòØÈöèÊú∫ÂèòÈáèÔºåŒµÂíåxÁõ∏‰∫íÁã¨Á´ãÔºåAÂíåb ÈÉΩÊòØÁ≥ªÊï∞„ÄÇ Ôºày ÊòØÂú® x ÁªôÂÆöÁöÑÊÉÖÂÜµ‰∏ãÂèëÁîüÁöÑÔºåÂõ∫ÂÆö‰∫ÜxÂàôÂÆÉÁöÑÊñπÂ∑Æ‰∏∫0ÔºåÊâÄ‰ª•ÊñπÂ∑ÆL‚Åª¬π‰∏≠‰∏çÂê´ x ÁöÑÊñπÂ∑ÆÔºâ\nE[y] = E[Ax + b + Œµ] = E [ Ax+b ] + E [Œµ] ÔºåÊåâÁÖß‰∏ä‰∏ÄËäÇÂºïÂÖ•ÁöÑÂÆöÁêÜ\n= AŒº+b ÔºåŒµÁöÑÊúüÊúõÊòØ0\nVar[y] = Var[Ax + b + Œµ] = Var[ Ax+b ] + Var [E] = A‚ãÖŒõ‚Åª¬π‚ãÖA·µÄ + L‚Åª¬π\nÊâÄ‰ª•Â∞±ÂæóÂà∞‰∫Ü y ÁöÑÂàÜÂ∏ÉÔºö yÔΩûN(AŒº+b, A‚ãÖŒõ‚Åª¬π‚ãÖA·µÄ + L‚Åª¬π)\n(2) Ê±Ç p(z) ÊûÑÈÄ†ÂèòÈáè z ÊòØ x Âíå y ÁöÑËÅîÂêàÔºöz = (x,\\ y)„ÄÇ Ôºà‚Äú‰ªªÊÑè‰∏™ÊúâÈôêÁª¥ÁöÑÈ´òÊñØÂàÜÂ∏ÉÁöÑËÅîÂêàÂàÜÂ∏ÉÂùáÊòØÈ´òÊñØÂàÜÂ∏É‚ÄùÔºåÊòØËØ¥‰∏§‰∏™ÈöèÊú∫ÂèòÈáèÂêàËµ∑Êù•ÁöÑÂàÜÂ∏ÉËøòÊòØÈ´òÊñØÔºåÂπ∂‰∏çÊòØGMMÔºàÂØπ‰ººÁÑ∂Âä†ÊùÉÔºâÊ≠£ÊÄÅÂàÜÂ∏ÉÈöèÊú∫ÂèòÈáèÁöÑÂíåËøòÊòØÊ≠£ÊÄÅÂàÜÂ∏ÉÂêóÔºü - Ê±üÂüéÈõ®-Áü•‰πé„ÄÇÂπΩÂÜ•Ëã•ÁÇéÁöÑËØÑËÆ∫Ôºö‰∏çÈúÄË¶Å‰∏§‰∏™Â§öÁª¥ÈöèÊú∫ÂèòÈáèÁõ∏‰∫íÁã¨Á´ãÔºüx‰∏éyÁã¨Á´ãÔºåÂõ†‰∏∫x‰∏éŒµÁã¨Á´ãÔºâ\nÊ†πÊçÆ‰∏ä‰∏ÄËäÇÁöÑÁªìËÆ∫ÔºåËæπÁºòÊ¶ÇÁéáÂàÜÂ∏ÉÂ∞±Áõ∏ÂΩì‰∫é‰ªÖ‰ªÖËÄÉËôë‰∏ÄÈÉ®ÂàÜÁª¥Â∫¶ÔºåÊâÄ‰ª•zÊúç‰ªéÁöÑÂàÜÂ∏ÉÁöÑÊúüÊúõÂíåÊñπÂ∑ÆÂ∞±ÊòØ x ÁöÑŒº,‚àë ‰∏é y ÁöÑŒº,‚àë ÊãºËµ∑Êù•Ôºö\nE[z] = [Œº,\\\\ AŒº+b]\nVar[z] = [Œõ‚Åª¬π, unknown,\\\\ unknown, A‚ãÖŒõ‚Åª¬π‚ãÖA·µÄ + L‚Åª¬π]\n‰πüÂ∞±ÊòØÔºö z = (x,\\ y) ÔΩû N ( [Œº,\\\\ AŒº+b], [Œõ‚Åª¬π, unknown,\\\\ unknown, A‚ãÖŒõ‚Åª¬π‚ãÖA·µÄ + L‚Åª¬π] )\nÂõ†‰∏∫ÊñπÂ∑ÆÁü©ÈòµÊú¨Ë∫´Â∫îËØ•ÊòØÂØπÁß∞ÁöÑÔºåÊâÄ‰ª•‰∏§‰∏™ unknown ÊòØ‰∏ÄÊ†∑ÁöÑÔºåËÆ∞‰∏∫ ŒîÔºåÂÆÉÁöÑÊúÄÂêéÁªìÊûúÈáåÈù¢‰∏çÂ∫îËØ•Âá∫Áé∞x Âíå y„ÄÇ\nŒî = Cov(x,y) = E[ (x-E[x]) ‚ãÖ (y-E[y])·µÄ ] = E [ (x-Œº) ‚ãÖ (y-(AŒº+b)·µÄ) ÔºåÊää y ÁöÑË°®ËææÂºè‰ª£ÂÖ• = E [ (x-Œº) ‚ãÖ (Ax + b + Œµ -AŒº -b)·µÄ) = E [ (x-Œº) ‚ãÖ (Ax - AŒº + Œµ)·µÄ) Ôºå‰∏§È°πÈáåÊúâÂÖ±ÂêåÁöÑ(x-Œº) = E [ (x-Œº) ‚ãÖ (Ax - AŒº)·µÄ + (x-Œº) ‚ãÖ Œµ·µÄ) = E [ (x-Œº) ‚ãÖ (Ax - AŒº)·µÄ ] + E [ (x-Œº) ‚ãÖ Œµ·µÄ ]\nÂõ†‰∏∫ x ‰∏é Œµ Áã¨Á´ãÔºöx‚üÇŒµÔºåÊâÄ‰ª• x-Œº ‰∏é Œµ Áã¨Á´ãÔºåÊâÄ‰ª•Á¨¨2‰∏™ÊúüÊúõÂèØÊãÜÂºÄÔºö E [ (x-Œº) ‚ãÖ Œµ·µÄ ] = E [ (x-Œº) ] ‚ãÖ E[ Œµ·µÄ ]ÔºåÂèàÂõ†‰∏∫ Œµ ÁöÑÊúüÊúõÁ≠â‰∫é0ÔºåÊâÄ‰ª•Êï¥È°πÈÉΩ=0ÔºåÊâÄ‰ª•Â∞±Ââ©Á¨¨1È°π\nŒî = E [ (x-Œº) ‚ãÖ (Ax - AŒº)·µÄ ] = E [ (x-Œº) ‚ãÖ (x - Œº)·µÄ ‚ãÖ A·µÄ ] ÔºåA‰∏çÊòØÈöèÊú∫ÂèòÈáè = E [ (x-Œº) ‚ãÖ (x - Œº)·µÄ ] ‚ãÖ A·µÄÔºåÁ¨¨1È°πÊòØxÁöÑÊñπÂ∑Æ = Var[x] ‚ãÖ A·µÄ = Œõ‚Åª¬π ‚ãÖ A·µÄ\nÊâÄ‰ª• z ÁöÑÊñπÂ∑ÆÁü©ÈòµÔºö Var(z) = [Œõ‚Åª¬π, Œõ‚Åª¬π ‚ãÖ A·µÄ,\\\\ Œõ‚Åª¬π ‚ãÖ A·µÄ, A‚ãÖŒõ‚Åª¬π‚ãÖA·µÄ + L‚Åª¬π]\n(3) Ê±Ç p(x|y) Ê†πÊçÆ‰∏ä‰∏ÄËäÇÁöÑÁªìËÆ∫ÔºöÂ∑≤Áü• x=(x‚Çê,\\ x_b)ÔºåÂàôÊúâ P(x_b | x‚Çê) = N(ùõç_{b‚ãÖa} + ‚àë_b‚Çê‚ãÖ‚àë‚Çê‚Çê‚Åª¬π‚ãÖx‚Çê, ‚àë_{bb‚ãÖa})„ÄÇ\nz ÂØπÂ∫î x, x‚Çê,x_b ÂØπÂ∫î x,yÔºåz ÁöÑÊúüÊúõÂíåÊñπÂ∑ÆÂ∑≤Áü•Ôºå‰ª£ÂÖ•Âç≥ÂèØ„ÄÇ\n7. ‰∏çÁ≠âÂºè1-Jensen‰∏çÁ≠âÂºè P7\nÊØîÂ¶ÇÂú® EM ÁÆóÊ≥ïÊé®ÂØºÊó∂Ôºå‰ºöÁî®Âà∞Ê≠§‰∏çÁ≠âÂºè\nÂÅáËÆæ f(x) ÊòØÂá∏ convex function ÂàôËØ•ÂáΩÊï∞ÁöÑÊúüÊúõÂ§ß‰∫éÁ≠â‰∫éÊúüÊúõÁöÑÂáΩÊï∞Ôºö E(f(x)) ‚â• f(E(x))\n(1) ËØÅÊòé Ôºà‰ª•‰∏ãÊòØ‰∏Ä‰∏™ÊûÑÈÄ†ÊÄßËØÅÊòéÔºâ\nÊúâ‰∏Ä‰∏™Âá∏ÂáΩÊï∞ f(x)ÔºåÂú® x ËΩ¥‰∏äÂèñ x ÁöÑÊúüÊúõÂÄº E[x]ÔºåÂÆÉÂØπÂ∫îÁöÑÂáΩÊï∞ÂÄºÊòØ f(E[x])ÔºåËøáËøô‰∏™ÂáΩÊï∞ÂÄºÂÅöËøô‰∏™Âá∏ÂáΩÊï∞ÁöÑÂàáÁ∫øÔºåÂ∞ÜËøôÊù°ÂàáÁ∫øÂÆö‰πâ‰∏∫ l(x) = ax+b„ÄÇ\nf(E[x]) = l(E[x]) = aE(x)+b\nÂõ†‰∏∫ f(x) ÊòØÂá∏ÂáΩÊï∞ÔºåÊâÄ‰ª•ÂØπ‰∫é‰ªªÊÑèÁöÑ x ÈÉΩÊúâ f(x)‚â•l(x)ÔºåÂØπÊ≠§Âºè‰∏§ËæπÂêåÊó∂Ê±ÇÊúüÊúõÔºö E[ f(x) ] ‚â• E [ l(x) ] = E[ax+b] = E[ax] + E[b] = aE[x] +b = f(E[x])\n‰πüÂ∞±ÊòØ E[ f(x) ] ‚â• f(E[x])\nÊ≠ªÁ•û‰πãÂêç111 ÁöÑËØÑËÆ∫Ôºö ‚ÄúÁ¨¨‰∏ÉËäÇÔºöÂæà‰∏çÂπ∏ÔºåËøô‰∏™Êé®ÂØºÊòØÈîôËØØÁöÑÔºå‰Ω†ÂÖàËØ¥‰∫Üf=lÔºå‰Ω†ÂÖ∂ÂÆûÂè™ËØ¥Êòé‰∫ÜÁ∫øÊÄßÂáΩÊï∞ÁöÑjensen‰∏çÁ≠âÂºèÊàêÁ´ãÔºåÁúüÊ≠£‰∏•Ê†ºËØÅÊòé‰Ω†ÈúÄË¶ÅÂÅö‰∏§‰∏™ÁßØÂàÜÂ∑ÆÔºåÂÜçÂà©Áî®Âá∏ÊÄßÔºå‰∫åÈò∂ÂØºÔºû0ÔºåÁªºÂêà‰∏Ä‰∏ãÂ∞±ÊòØÂΩìÂâçÁªìÊûú„ÄÇ‚Äù\n(2) Áõ¥ËßÇÁêÜËß£ ÈÄöÂ∏∏ÁöÑË°®Ëø∞ÔºöÂú® x ËΩ¥‰∏äÂèñ‰∏§‰∏™ÁÇπ a Âíå bÔºåÁÑ∂ÂêéÂú®‰∏§ÁÇπ‰πãÈó¥‰ªªÊÑèÈÄâÂèñ‰∏Ä‰∏™ c ÁÇπÔºåÈÄâÂèñÊó∂ÈÄöÂ∏∏ÂÖàÂú® [0,1] ‰∏äÂéª‰∏Ä‰∏™ t ÂÄºÔºåÁÑ∂ÂêéÂÅö‚ÄúÁ∫øÊÄßÊèíÂÄº‚ÄùÔºöc=ta+(1-t)b„ÄÇ\nf(a) Âíå f(b) ËøûÁ∫ø‰∏∫ÂáΩÊï∞ gÔºåÂàô c ÁöÑÂáΩÊï∞ÂÄº‰∏∫ g(c)ÔºåÂèØ‰ª•ÁúãÂà∞ g(c) ‚â• f(c)\nËÆæÁ∫øÊÆµac ÁöÑÈïøÂ∫¶ÊòØ tÔºåÁ∫øÊÆµ cb ÁöÑÈïøÂ∫¶ÊòØ 1-tÔºàÂÆûÈôÖÂ∫îËØ•ÊòØta Âíå(1-t)bÔºâÔºåÊâÄ‰ª•‰∏§Ê¢ØÂΩ¢ÁöÑÊñúËæπ‰πãÊØî‰πüÊòØ t:1-tÔºå\nÁÑ∂ÂêéÂàÜÂà´Ëøá g(c) Âíå f(a) ÂÅöÊ∞¥Âπ≥Á∫øÔºåÂΩ¢ÊàêÁõ∏‰ºº‰∏âËßíÂΩ¢ÔºåÂàô f(b)-g(c) ‰∏é g(c)-f(a) ‰πãÊØîÊòØ 1-t : t„ÄÇ\nÊâÄ‰ª• g(c) = t ‚ãÖ f(a) + (1-t) ‚ãÖ f(b)\n‰ª£ÂÖ• g(c) ‚â• f(c) Â∞±ÊòØÔºö t ‚ãÖ f(a) + (1-t) ‚ãÖ f(b) ‚â• f(t‚ãÖa+(1-t)‚ãÖb)\n","date":"2022-12-28T00:31:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/02_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","title":"watch: ML - ÁôΩÊùø 02 | Mathematical Basis"},{"content":"Difference between plt and fig,ax object-oriented\nInverse the X-axis GeeksforGeeks\n1 2 3 import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.invert_xaxis() Add caption below the x-label SO\n1 2 3 fig, ax = plt.subplots() txt = \u0026#39;near and far are distance\u0026#39; fig.text(x=.5, y=.001, s=txt, ha=\u0026#39;center\u0026#39;) 1 2 3 fig, ax = plt.subplots() ax.set_xlabel(\u0026#39;\u0026#39;\u0026#39;z_cam near and far are distance\u0026#39;\u0026#39;\u0026#39;) ÂæÆÂàÜÊü±ÂΩ¢Âõæ SO\nSet xticks xticks is the (fixed) number on the x-axis, while xticklabels can be customized strings.\nFor plt (Matplotlib.pyplot.xticks()): Matplotlib Set_xticks - Detailed Tutorial - Python Guides\nMatplotlib.axes.Axes.set_xticklabels() Matplotlib Set_xticklabels -Python Guides\n1 2 3 fig, ax = plt.subplots() ax.set_xticks([0, np.pi, 2*np.pi, 3*np.pi]) ax.set_xticklabels(labels=[\u0026#39;0\u0026#39;, r\u0026#39;$\\pi$\u0026#39;, r\u0026#39;2$\\pi$\u0026#39;, r\u0026#39;3$\\pi$\u0026#39;], fontdict=None, fontsize=6, fontstyle=\u0026#39;italic\u0026#39;, color=\u0026#39;red\u0026#39;, verticalalignment=\u0026#39;top\u0026#39;, horizontalalignment=\u0026#39;left\u0026#39;, rotation=90, minor=True, Change the number and text of the showing Ticks GfG\nFor plt:\n1 2 plt.xticks(ticks=[1,2,3,4], labels=None, **kwargs) plt.yticks(ticks=[7,13,24,22], labels=None,) Use locator_param() to change the tightness and number of ticks.\nplt.locator_params(axis='both', nbins=4)\nUse xlim() to restrict the diplayed area, rather the whoel plot.\n1 2 plt.xlim(0,3) plt.locator_params(axis=\u0026#39;x\u0026#39;, nbins=3) Use Matplotlib.ticker.MaxNLocator class\nSet Number of Ticks in Matplotlib | Delft Stack\n1 2 3 4 5 6 7 8 from matplotlib.ticker import MaxNLocator fig, ax = plt.subplots(1,1) ax.yaxis.set_major_locator(MaxNLocator(5)) # hide specific ticks for i, tick in enumerate(ax.xaxis.get_ticklabels()): if i%2 !=0: tick.set_visible(False) Set y-axis view limits Matplotlib.axes.Axes.set_ylim() in Python - GeeksforGeeks\n1 2 3 4 5 6 7 fig, ax = plt.subplots(facecolor =\u0026#39;# A0F0CC\u0026#39;) x, y = 4*(np.random.rand(2, 100) - .5) ax.plot(x, y, \u0026#39;g\u0026#39;) ax.set_ylim(-3, 3) # Ê∏∏Ê†á cursor = Cursor(ax, useblit = True, color =\u0026#39;red\u0026#39;, linewidth = 2) Set the 1st x as 1 (not 0) Python MatplotLib plot x-axis with first x-axis value labeled as 1 (instead of 0)\nAdd extra ticks and labels Add extra ticks which must be number, not work for string SO\n1 2 3 fig, ax= plt.subplots() extraticks = [2.1, 3, 7.6] ax.set_xticks(list(ax.get_xticks()) + extraticks) Use ax.set_xticklabels() for string Create label list - SO\n1 2 3 4 5 near, far = 1, 15 ax.set_xticks(list(ax.get_xticks()) + [-near,-far]) # Their labels also have to be appended at the end. xticklabels= [\u0026#39;-16\u0026#39;, \u0026#39;-14\u0026#39;, \u0026#39;-12\u0026#39;, \u0026#39;-10\u0026#39;, \u0026#39;-8\u0026#39;, \u0026#39;-6\u0026#39;, \u0026#39;-4\u0026#39;, \u0026#39;-2\u0026#39;,\u0026#39;0\u0026#39;, \u0026#39;-near\u0026#39;, \u0026#39;-far\u0026#39;] ax.set_xticklabels(xticklabels, rotation=90) Example: Mark the yticks for the highest and lowest points:\n1 2 3 4 5 6 7 rmse = np.load(\u0026#34;./error_list.npz\u0026#34;)[\u0026#39;Train_RMSE\u0026#39;] idx = np.arange(len(rmse)) fig, ax = plt.subplots() ax.plot(idx, rmse) extraticks = [rmse.min(), rmse.max()] ax.set_yticks(list(ax.get_yticks()) + extraticks) Set xticks as integer (Not sure) ax.xaxis.set_major_locator(MaxNLocator(integer=True))\nPythonÔºö‰ΩøÁî®f-string‰øùÁïôÂ∞èÊï∞ÁÇπ‰ΩçÊï∞ csdn\nÁî®decimal Ê®°ÂùóÔºöw3cschool\n1 2 3 from decimal import Decimal a = 12.345 Decimal(a).quantize(Decimal(\u0026#34;0.00\u0026#34;)) # ‰ΩøÁî®ÈªòËÆ§ÁöÑËøõ‰ΩçÊñπÂºèÔºàÂêåroundÔºâ‚Äú0.00‚ÄùË°®Á§∫‰øùÁïôÂ∞èÊï∞ÁÇπÂêé‰∏§‰Ωç Margin to leave more space around the figure to show the ticks of the boundary Geeksforgeeks\nOr prevent the markers get clipped by the axes GfG plt xticks\n1 2 fig, ax = plt.subplots() ax.margins(0.5) Hide ticks GfG Legend gfg Docs\n1 2 ax.plot([1, 2, 3], label=\u0026#39;Inline label\u0026#39;) ax.legend(loc=\u0026#39;best\u0026#39;, bbox_to_anchor=(0.5, 0., 0.5, 0.5)) Put it below the subplots: ax.legend(loc=\u0026quot;upper center\u0026quot;, bbox_to_anchor=(0.5, -0.13),fancybox=False, shadow=False, ncol=5, fontsize=6)\nHow to add legend below subplots in matplotlib?\nDraw a horizontal line plt : gfg\n1 plt.axhline(y=0.5, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;) ax: SO\n1 ax.hlines(y=0.2, xmin=0.01, xmax=20, linewidth=2, color=\u0026#39;r\u0026#39;) Tight layout 1 plt.tight_layout() Or: plt.rcParams[\u0026quot;savefig.bbox\u0026quot;] = 'tight'\nSave as png gfg\n1 2 3 4 ax.set_facecolor(\u0026#39;pink\u0026#39;) # inner background plt.savefig(\u0026#39;depth_buffer.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, dpi =100, facecolor=\u0026#34;white\u0026#34;) # bkg color plt.savefig(\u0026#39;output.jpg\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, dpi =40) #is smaller Draw markers on axes plt.plot(x,y, zorder=10, clip_on=False)\nplotting markers on top of axes\nArrow of axis matplotlib axis arrow tip\nChange fonts Plot in Ubuntu has ugly fonts (which is like \u0026lsquo;sans-serif\u0026rsquo;).\nplt.rcParams[\u0026quot;font.family\u0026quot;] = \u0026quot;cursive\u0026quot;, This will change to your computer\u0026rsquo;s default monospace font. How to change fonts in matplotlib (python)?\nÊï£ÁÇπÂõæ plt.scatter(x,y, c='r', s=4), s can control marker size, docs\nmarker ÊòØÂèçÁùÄÁîªÁöÑ\nmatplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None, **kwargs) x,yÁªÑÊàê‰∫ÜÊï£ÁÇπÁöÑÂùêÊ†áÔºõs‰∏∫Êï£ÁÇπÁöÑÈù¢ÁßØÔºõc‰∏∫Êï£ÁÇπÁöÑÈ¢úËâ≤ÔºàÈªòËÆ§‰∏∫ËìùËâ≤\u0026rsquo;b\u0026rsquo;ÔºâÔºõmarker‰∏∫Êï£ÁÇπÁöÑÊ†áËÆ∞Ôºõalpha‰∏∫Êï£ÁÇπÁöÑÈÄèÊòéÂ∫¶Ôºà0‰∏é1‰πãÈó¥ÁöÑÊï∞Ôºå0‰∏∫ÂÆåÂÖ®ÈÄèÊòéÔºå1‰∏∫ÂÆåÂÖ®‰∏çÈÄèÊòéÔºâ;linewidths‰∏∫Êï£ÁÇπËæπÁºòÁöÑÁ∫øÂÆΩÔºõÂ¶ÇÊûúmarker‰∏∫NoneÔºåÂàô‰ΩøÁî®vertsÁöÑÂÄºÊûÑÂª∫Êï£ÁÇπÊ†áËÆ∞Ôºõedgecolors‰∏∫Êï£ÁÇπËæπÁºòÈ¢úËâ≤„ÄÇ csdn\nÊãâÈïøÂùêÊ†áÈó¥Ë∑ù PythonËÆæÁΩÆmatplotlib.plotÁöÑÂùêÊ†áËΩ¥ÂàªÂ∫¶Èó¥Èöî‰ª•ÂèäÂàªÂ∫¶ËåÉÂõ¥\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import matplotlib.pyplot as plt from matplotlib.pyplot import MultipleLocator # Áî®‰∫éËÆæÁΩÆÂàªÂ∫¶Èó¥Èöî x_vals = list(range(11)) y_vals = [x**2 for x in x_vals] plt.plot(x_vals, y_vals, c=\u0026#39;green\u0026#39;) plt.title(\u0026#39;Squares\u0026#39;, fontsize=24) plt.tick_params(axis=\u0026#39;both\u0026#39;,which=\u0026#39;major\u0026#39;,labelsize=14) plt.xlabel(\u0026#39;Numbers\u0026#39;,fontsize=14) plt.ylabel(\u0026#39;Squares\u0026#39;,fontsize=14) x_major_locator=MultipleLocator(1) #ÊääxËΩ¥ÁöÑÂàªÂ∫¶Èó¥ÈöîËÆæÁΩÆ‰∏∫1ÔºåÂπ∂Â≠òÂú®ÂèòÈáèÈáå y_major_locator=MultipleLocator(10) #ÊääyËΩ¥ÁöÑÂàªÂ∫¶Èó¥ÈöîËÆæÁΩÆ‰∏∫10ÔºåÂπ∂Â≠òÂú®ÂèòÈáèÈáå ax=plt.gca() #ax‰∏∫‰∏§Êù°ÂùêÊ†áËΩ¥ÁöÑÂÆû‰æã ax.xaxis.set_major_locator(x_major_locator) #ÊääxËΩ¥ÁöÑ‰∏ªÂàªÂ∫¶ËÆæÁΩÆ‰∏∫1ÁöÑÂÄçÊï∞ ax.yaxis.set_major_locator(y_major_locator) #ÊääyËΩ¥ÁöÑ‰∏ªÂàªÂ∫¶ËÆæÁΩÆ‰∏∫10ÁöÑÂÄçÊï∞ plt.xlim(-0.5,11) #ÊääxËΩ¥ÁöÑÂàªÂ∫¶ËåÉÂõ¥ËÆæÁΩÆ‰∏∫-0.5Âà∞11ÔºåÂõ†‰∏∫0.5‰∏çÊª°‰∏Ä‰∏™ÂàªÂ∫¶Èó¥ÈöîÔºåÊâÄ‰ª•Êï∞Â≠ó‰∏ç‰ºöÊòæÁ§∫Âá∫Êù•Ôºå‰ΩÜÊòØËÉΩÁúãÂà∞‰∏ÄÁÇπÁ©∫ÁôΩ plt.ylim(-5,110) #ÊääyËΩ¥ÁöÑÂàªÂ∫¶ËåÉÂõ¥ËÆæÁΩÆ‰∏∫-5Âà∞110ÔºåÂêåÁêÜÔºå-5‰∏ç‰ºöÊ†áÂá∫Êù•Ôºå‰ΩÜÊòØËÉΩÁúãÂà∞‰∏ÄÁÇπÁ©∫ÁôΩ plt.show() ËÆæÁΩÆÂõæÁâáÂ∞∫ÂØ∏Â§ßÂ∞è Â¶Ç‰ΩïÊåáÂÆömatplotlibËæìÂá∫ÂõæÁâáÁöÑÂ∞∫ÂØ∏Ôºü - pythonicÁîüÁâ©‰∫∫ÁöÑÂõûÁ≠î - Áü•‰πé https://www.zhihu.com/question/37221233/answer/2250419008\nplt.rcParams['figure.figsize'] = (12.0, 8.0)\nfig, ax = plt.subplots(figsize=(15,0.5),dpi=300) pool\nËÆæÁΩÆÂõæÁâáÂàÜËæ®Áéá plt.figure(figsize=(a, b), dpi=dpi)\nmatplotlibËÆæÁΩÆÂàÜËæ®Áéá\nÁΩëÊ†º plt.grid(visible=True,linestyle=\u0026quot;--\u0026quot;, color='gray', linewidth='1',)\nÁªòÂà∂CDF (221211) Repeat drawing How to change a matplotlib figure in a different cell in Jupyter Notebook?\n1 2 fig.gca().scatter(x3, y3, color=\u0026#39;g\u0026#39;, linewidth=5) fig (2022-12-13)\nGet the arguments name use lib inspect with two .f_back: callers_local_vars = inspect.currentframe().f_back.f_back.f_locals.items(). Getting the name of a variable as a string\nGet current color How to get color of most recent plotted line in Python\u0026rsquo;s plt plt.gca().lines[-1].get_color()\nuse ax Get default line colour cycle\n1 2 line = ax.plot(x,y) ax.plot(x, y+.3, color = line.get_color()) Combine two figures matplotlib: combine different figures and put them in a single subplot sharing a common legend Multiple subplots GfG\n1 2 3 4 5 6 7 fig, ax = plt.subplots(3, 3) # draw graph for i in ax: for j in i: j.plot(np.random.randint(0, 5, 5), np.random.randint(0, 5, 5)) plt.show() title of subplots: ax[0][0].set_title(\u0026quot;xxx\u0026quot;)\nhide x ticks for top subplots and y ticks for right plots: How to set a single, main title above all the subplots with Pyplot?\n1 2 plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False) plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False) Set xticks for subplots: How to set xticks in subplots\n1 2 3 4 5 6 7 8 9 10 11 12 fig, axes = plt.subplots(nrows=3, ncols=4) # Set the ticks and ticklabels for all axes plt.setp(axes, xticks=[0.1, 0.5, 0.9], xticklabels=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], yticks=[1, 2, 3]) # Use the pyplot interface to change just one subplot... plt.sca(axes[1, 1]) plt.xticks(range(3), [\u0026#39;A\u0026#39;, \u0026#39;Big\u0026#39;, \u0026#39;Cat\u0026#39;], color=\u0026#39;red\u0026#39;) fig.tight_layout() plt.show() Set the fontsize of numbers of xticks: plt.setp(ax.get_xticklabels(), fontsize=12, fontweight=\u0026quot;normal\u0026quot;, horizontalalignment=\u0026quot;left\u0026quot;, rotation=90) set_xticks() needs argument for \u0026lsquo;fontsize\u0026rsquo; #12318 set_xticklabels-Docs\nScale y-axis Docs\n1 2 3 4 x = np.arange(len(Train_RMSE)) fig, ax = plt.subplots() plt.yscale(\u0026#39;log\u0026#39;,) ax.plot(x, Train_RMSE) Composite 2 imgs (2023-12-20) Setting alpha:\n1 2 3 4 5 6 7 8 9 10 11 12 13 import cv2 import matplotlib.pyplot as plt img1 = cv2.imread(\u0026#39;rot1.png\u0026#39;) pts1 = [(24, 124), (49, 124), (98, 124), (104, 124), (120, 124), (18, 146), (37, 146), (65, 146), (102, 146), (133, 146)] for pt in pts1: cv2.circle(img1, pt, 0, (0, 0, 255)) plt.figure(figsize=(20,10)) plt.imshow(img1[:,:, ::-1]) img2 = cv2.imread(\u0026#39;rot2.png\u0026#39;) plt.imshow(img2[:,:,::-1], alpha=0.6) An example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 import matplotlib.pyplot as plt import numpy as np from matplotlib.ticker import MultipleLocator epoch = np.array( [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]) train_loss = np.array( [0.27195, 0.07782, 0.05188, 0.03321, 0.03054, 0.02322, 0.01962, 0.01537, 0.01347, 0.01266, 0.00895, 0.01107, 0.00755, 0.00633, 0.00915, 0.00872, 0.00431, 0.00733, 0.00584, 0.00477, 0.00502, 0.00384, 0.00282, 0.00506, 0.00307, 0.00325, 0.00588, 0.00277, 0.00445, 0.00318, 0.00181, 0.00532, 0.00218, 0.00379, 0.00190, 0.00201, 0.00242, 0.00164, 0.00215, 0.00176, 0.00374, 0.00149, 0.00145, 0.00170, 0.00481, 0.00136, 0.00184, 0.00165, 0.00162, 0.00265, 0.00285, 0.00172, 0.00241, 0.00139, 0.00181, 0.00161, 0.00135, 0.00264, 0.00165, 0.00158]) valid_loss = np.array( [0.04518, 0.02028, 0.01517, 0.01853, 0.00928, 0.01604, 0.01256, 0.01068, 0.01229, 0.01066, 0.01251, 0.00756, 0.00836, 0.00777, 0.00623, 0.00942, 0.00761, 0.00691, 0.00626, 0.00607, 0.00708, 0.00703, 0.01061, 0.00758, 0.00807, 0.00541, 0.00521, 0.00570, 0.00844, 0.00610, 0.00735, 0.00448, 0.00466, 0.00734, 0.00633, 0.00511, 0.00737, 0.00380, 0.00410, 0.00604, 0.00685, 0.00546, 0.00618, 0.00411, 0.00543, 0.00572, 0.00631, 0.00821, 0.00578, 0.00525, 0.00391, 0.00529, 0.00621, 0.00606, 0.00979, 0.00515, 0.00555, 0.00712, 0.00535, 0.00439]) # Ëß£ÂÜ≥Êõ≤Á∫øÂõæÈáåÈù¢‰∏≠ÊñáÊòæÁ§∫‰π±Á†ÅÈóÆÈ¢ò # plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;Hiragino Sans GB\u0026#39;] # Áî®Êù•Ê≠£Â∏∏ÊòæÁ§∫‰∏≠ÊñáÊ†áÁ≠æ # # plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False plt.figure(figsize=(8,6)) # Â¢ûÂä†Êõ≤Á∫øËØ¥Êòé A, = plt.plot(epoch, train_loss, lw=1, label=\u0026#39;train_loss\u0026#39;) B, = plt.plot(epoch, valid_loss, lw=1, label=\u0026#39;valid_loss\u0026#39;) font1 = {\u0026#39;family\u0026#39;: \u0026#39;Times New Roman\u0026#39;, \u0026#39;weight\u0026#39;: \u0026#39;normal\u0026#39;, \u0026#39;size\u0026#39;: 18, } # xËΩ¥label plt.xlabel(\u0026#34;epoch\u0026#34;, labelpad=2, fontsize=18,) # yËΩ¥label plt.ylabel(\u0026#34;loss\u0026#34;, labelpad=4, fontsize=18) # ËÆæÁΩÆÊ†áÈ¢ò # plt.title(\u0026#34;change in loss\u0026#34;, x=0.5, y=-0.2, pad=0.3, fontsize=18) plt.title(\u0026#34;change in loss\u0026#34;, fontsize=18) fig=plt.gcf() fig.set_facecolor(\u0026#39;white\u0026#39;) # ------ÈôêÂà∂ÊòæÁ§∫x,yËΩ¥ÊúÄÂ∞è-ÊúÄÂ§ßÂÄºËåÉÂõ¥ÔºàÂàªÂ∫¶‰∏ç‰∏ÄÂÆöÊòØÂ§öÂ∞ëÔºâ plt.xlim(0, 60) plt.ylim(0, 0.3) # ------ËÆæÁΩÆx,yËΩ¥ÂàªÂ∫¶ x_major_locator = MultipleLocator(10) y_major_locator = MultipleLocator(0.02) ax = plt.gca() # ax‰∏∫‰∏§Êù°ÂùêÊ†áËΩ¥ÁöÑÂÆû‰æã ax.xaxis.set_major_locator(x_major_locator) # ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) # ÊääxËΩ¥ÁöÑ‰∏ªÂàªÂ∫¶ËÆæÁΩÆ‰∏∫1ÁöÑÂÄçÊï∞ ax.yaxis.set_major_locator(y_major_locator) # ÊääyËΩ¥ÁöÑ‰∏ªÂàªÂ∫¶ËÆæÁΩÆ‰∏∫10ÁöÑÂÄçÊï∞ # ------ÊòæÁ§∫ÁΩëÊ†º # plt.grid() plt.grid(True, linestyle=\u0026#34;--\u0026#34;, color=\u0026#39;gray\u0026#39;, linewidth=\u0026#39;1\u0026#39;, axis=\u0026#39;both\u0026#39;) plt.legend(handles=[A, B], prop=font1) plt.tick_params(labelsize=18) labels = ax.get_xticklabels() + ax.get_yticklabels() [label.set_fontname(\u0026#39;Time New Roman\u0026#39;) for label in labels] font2 = {\u0026#39;family\u0026#39;: \u0026#39;Time New Roman\u0026#39;, \u0026#39;weight\u0026#39;: \u0026#39;normal\u0026#39;, \u0026#39;size\u0026#39;: 18} plt.show() plt.savefig(\u0026#39;saved_figure.jpg\u0026#39;) 3D interactive plot (2024-03-23)\nMake 3D interactive Matplotlib plot in Jupyter Notebook - GfG\n3D Scatter plot:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # for creating a responsive plot %matplotlib widget # importing required libraries from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt # creating random dataset xs = [14, 24, 43, 47, 54, 66, 74, 89, 12, 44, 1, 2, 3, 4, 5, 9, 8, 7, 6, 5] ys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 3, 5, 2, 4, 1, 8, 7, 0, 5] zs = [9, 6, 3, 5, 2, 4, 1, 8, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0] # creating figure fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # creating the plot plot_geeks = ax.scatter(xs, ys, zs, color=\u0026#39;green\u0026#39;) # setting title and labels ax.set_title(\u0026#34;3D plot\u0026#34;) ax.set_xlabel(\u0026#39;x-axis\u0026#39;) ax.set_ylabel(\u0026#39;y-axis\u0026#39;) ax.set_zlabel(\u0026#39;z-axis\u0026#39;) # displaying the plot plt.show() 3D Bar plot:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %matplotlib widget from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt import numpy as np # creating random dataset xs = [2, 3, 4, 5, 1, 6, 2, 1, 7, 2] ys = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] zs = np.zeros(10) dx = np.ones(10) dy = np.ones(10) dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # creating figure figg = plt.figure() ax = figg.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # creating the plot plot_geeks = ax.bar3d(xs, ys, zs, dx, dy, dz, color=\u0026#39;green\u0026#39;) # setting title and labels ax.set_title(\u0026#34;3D bar plot\u0026#34;) ax.set_xlabel(\u0026#39;x-axis\u0026#39;) ax.set_ylabel(\u0026#39;y-axis\u0026#39;) ax.set_zlabel(\u0026#39;z-axis\u0026#39;) plt.show() Camera Poses (2024-03-23)\nGenerate by ChatGPT with prompt: \u0026ldquo;How to plot multiple camera poses in a single figure?\u0026rdquo;\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 %matplotlib widget import numpy as np import matplotlib.pyplot as plt def plot_camera_poses(poses, axis_length=0.1): # Create a new figure fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # Plot each camera pose for pose in poses: # Extract camera position and orientation cam_position = pose[\u0026#39;position\u0026#39;] cam_orientation = pose[\u0026#39;rotation\u0026#39;] # Plot camera position ax.scatter(cam_position[0], cam_position[1], cam_position[2], c=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;) # Plot camera axes axes_endpoints = cam_position + axis_length * cam_orientation ax.plot3D([cam_position[0], axes_endpoints[0,0]], [cam_position[1], axes_endpoints[0,1]], [cam_position[2], axes_endpoints[0,2]], \u0026#39;r\u0026#39;) ax.plot3D([cam_position[0], axes_endpoints[1,0]], [cam_position[1], axes_endpoints[1,1]], [cam_position[2], axes_endpoints[1,2]], \u0026#39;g\u0026#39;) ax.plot3D([cam_position[0], axes_endpoints[2,0]], [cam_position[1], axes_endpoints[2,1]], [cam_position[2], axes_endpoints[2,2]], \u0026#39;b\u0026#39;) # Set plot limits and labels ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) # Show plot plt.show() # Example camera poses (positions and rotation matrices) poses = [ { \u0026#39;position\u0026#39;: np.array([0, 0, 0]), \u0026#39;rotation\u0026#39;: np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) }, { \u0026#39;position\u0026#39;: np.array([1, 1, 1]), \u0026#39;rotation\u0026#39;: np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]]) } ] # Plot camera poses plot_camera_poses(poses) 3D arrow (2024-03-23)\nPutting arrowheads on vectors in a 3d plot - SO\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyArrowPatch from mpl_toolkits.mplot3d import proj3d class Arrow3D(FancyArrowPatch): def __init__(self, xs, ys, zs, *args, **kwargs): super().__init__((0,0), (0,0), *args, **kwargs) self._verts3d = xs, ys, zs def do_3d_projection(self, renderer=None): xs3d, ys3d, zs3d = self._verts3d xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M) self.set_positions((xs[0],ys[0]),(xs[1],ys[1])) return np.min(zs) arrow_prop_dict = dict(mutation_scale=10, arrowstyle=\u0026#39;-|\u0026gt;\u0026#39;, color=\u0026#39;k\u0026#39;, shrinkA=0, shrinkB=0) fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) a = Arrow3D([0, 1], [0, 0], [0, 0], **arrow_prop_dict) ax.add_artist(a) a = Arrow3D([0, 0], [0, 1], [0, 0], **arrow_prop_dict) ax.add_artist(a) a = Arrow3D([0, 0], [0, 0], [0, 1], **arrow_prop_dict) ax.add_artist(a) Convex polygon on 2D (2024-03-26)\nHow to fill an area within a polygon in Python using matplotlib? - SO\nExample from Scipy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Convex hull of a random set of points: from scipy.spatial import ConvexHull, convex_hull_plot_2d import numpy as np rng = np.random.default_rng() points = rng.random((30, 2)) # 30 random points in 2-D hull = ConvexHull(points) # Plot it: import matplotlib.pyplot as plt plt.plot(points[:,0], points[:,1], \u0026#39;o\u0026#39;) for simplex in hull.simplices: plt.plot(points[simplex, 0], points[simplex, 1], \u0026#39;k-\u0026#39;) plt.plot(points[hull.vertices,0], points[hull.vertices,1], \u0026#39;r--\u0026#39;, lw=2) plt.plot(points[hull.vertices[0],0], points[hull.vertices[0],1], \u0026#39;ro\u0026#39;) plt.show() Fill 3D Polygon (2024-03-26)\nax.fill is used for 2D, and doesn\u0026rsquo;t work for 3D. ax.add_collection3d cannot follow the counter-clockwise order of convex hull to fill the polygon. For example, when drawing a rectangle, it will fill 2 triangles:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) verts = np.array([[ [0,0,0], [1,0,0], [0,1,0], [1,1,0] ]]) # (1,4,3) rec = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, edgecolors=\u0026#39;k\u0026#39;) ax.add_collection3d(rec) The Poly3DCollection code is generated by ChatGPT with prompt: \u0026ldquo;How to fill a 3D triangle with matlabplot\u0026rdquo; So, I draw 2 triangles to form a quadrilateral:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) verts = np.array([[ [0,0,0], [1,0,0], [0,1,0],]]) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) verts = np.array([[ [1,0,0], [0,1,0], [1,1,0] ]]) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) Plotting 3D Polygons - SO\nProject 3D point onto plane (2024-03-26)\nHow to project a point onto a plane in 3D? - SO\nThe distance from 3D point to the plane: dot product for 3D point and plane normal vector.\n3D point minus the distance, resulting in the projection.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection R = np.array([[ 0.97026268, 0.00747991, 0.24193878], # cam x-axis [-0.01474287, 0.99949291, 0.02822342], # cam y-axis [-0.24160499, -0.030951 , 0.96988095]]) #cam z-axis fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # fill rectangle verts = np.array([[ [0,0,0], R[0]*2, R[1]*2, ]]) # (1,4,3) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, ) ax.add_collection3d(tri) verts = np.array([[ R[0]*2, R[1]*2, (R[0]+R[1])*2 ]]) # (1,4,3) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, ) ax.add_collection3d(tri) p = np.array([1,1,1]) ax.scatter(*p, c=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;) plane_normal = R[2] len2plane = R[2]@ p.T projection = p-len2plane * plane_normal ax.scatter(*projection, marker=\u0026#39;*\u0026#39;) ax.plot3D([p[0], projection[0]], [p[1], projection[1]], [p[2], projection[2]], \u0026#39;gray\u0026#39;) ","date":"2022-12-26T17:05:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_draw/","title":"memo: Python | Plotting"},{"content":"args \u0026amp; kwargs (2022-08-05)\n*argsÔºöË°®Á§∫Êé•Âèó‰ªªÊÑè‰∏™Êï∞ÁöÑ Positional argumentsÔºåÁÑ∂ÂêéÂ≠òÊîæÂÖ•‰∏Ä‰∏™ÂÖÉÁªÑ‰∏≠Ôºõ‰º†ÂèÇÊó∂ ‰ΩçÁΩÆ Ë¶ÅÂØπÂ∫îÔºå‰æãÂ¶Ç\n1 2 3 4 5 def fun (*args) print(args) fun(\u0026#39;fruit\u0026#39;, \u0026#39;animal\u0026#39;, \u0026#39;human\u0026#39;) \u0026gt;\u0026gt;\u0026gt; \u0026#39;fruit\u0026#39;Ôºå\u0026#39;animal\u0026#39;Ôºå\u0026#39;human\u0026#39; **kwargsÔºöË°®Á§∫Êé•Âèó‰ªªÊÑèÈïøÁöÑ Keywords argumentsÔºåÁÑ∂ÂêéÂ≠òÊîæÂÖ•‰∏Ä‰∏™Â≠óÂÖ∏‰∏≠Ôºõ‰º†ÂèÇÊó∂Ë¶ÅÂØπÂ∫î keyÔºå‰æãÂ¶Ç\n1 2 3 4 5 def fun(**kwargs): for key, value in kwargs.items(): print(\u0026#34;%s:%s\u0026#34; % (key,value) fun(a=1,b=2,c=3)‰ºöËæìÂá∫ a=1 b=2 c=3 Positional args and keywords args\n1 2 3 4 5 6 def foo(a, b, **kwargs): var1 = a var2 = b args = kwargs var3 = args.[\u0026#39;key1\u0026#39;] Ê≥®Ëß£ annotation bilibili\nÁ±ªÂûã‰∏çÂåπÈÖçË≠¶Âëä\n1 2 3 4 from typing import * def func(a: int, b: List[int]): # Á¨¨‰∏Ä‰∏™ÂèÇÊï∞ÊòØintÔºåÁ¨¨‰∫å‰∏™ÂèÇÊï∞ÊòØlistÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂèÇÊï∞ÊòØint ... 1 2 3 4 5 6 def func(a: List[List[int]], # ‰∫åÁª¥intÊï∞ÁªÑ b: Dict, c: Set[int], d: Tuple[int] ): ... 1 2 3 4 5 6 7 8 9 class TreeNode: def __init__(self): self.val = None self.left = None self.right = None def dfs(node: TreeNode): # Á±ªÂûã‰∏∫Á±ªÂêçÔºåÊàñ\u0026#34;TreeNode\u0026#34; node.val = 1 ... 1 2 3 4 5 def func(x: float) -\u0026gt; str: # ËøîÂõûÂÄºÁ±ªÂûã return str(x) def func2(func1: Callable[[float], str]): ... 1 a: int = 15 # ÂèòÈáèÊ≥®Ëß£ 1 print(func.__annotations__) # Êü•ÁúãÂáΩÊï∞ÁöÑÊ≥®Ëß£Á±ªÂûã Class Variable \u0026amp; Instance Variable 1 2 3 4 5 class MyClass(): class_var = 1 # Class Variable def __init__(self): self.var = 123 # Instance Variable All instances of the class have access to class_var, as well as the class itself. If class_var is mutable type, like list, class_var will influence on all instances. Reference: Python Class Attributes: An Overly Thorough Guide\n@classmethod classmethod can only be called through the class name instead through an object instantiated from the class, because it gets passed the cls as the argument.\nSimilarly, self means a method belongs to an instance, and can only be called via instance. classmethod can access the class variables.\nclassmethod usually is used for instantiating an object with a set of specific arguments rather through __init__().\n1 2 3 4 5 6 7 8 9 10 11 12 13 class PositionalEncoding(torch.nn.Module): def __init__(self, ..): # instance method ... @classmethod def from_conf(cls, conf, d_in=3): # conf: \u0026#39;default.conf\u0026#39;.model.code{} # PyHocon construction return cls( conf.get_int(\u0026#34;num_freqs\u0026#34;, 6), d_in, conf.get_float(\u0026#34;freq_factor\u0026#34;, np.pi), conf.get_bool(\u0026#34;include_input\u0026#34;, True), ) Code snippest from pixel-nerf\nRef: An essential guide to Python class methods and when to use them\n@staticmethod @staticmethod in Python - AskPython\nNo need to pass an instance (self) or class (cls) to it as the first argument. So it can be called both by the class name and an instance. Can be overridden by children class. __name__ ÂΩì‰∏Ä‰∏™ py Êñá‰ª∂Ë¢´Áõ¥Êé•ÊâßË°åÊó∂Ôºå__name__ Ë¢´ËÆæÁΩÆ‰∏∫ '__main__'ÔºåÂ¶ÇÊûúÊòØË¢´ impotÔºåÂàôÂÆÉÁöÑ __name__ Ë¢´ËÆæÁΩÆ‰∏∫ÂÆÉÁöÑÊñá‰ª∂Âêç(‰∏çÂåÖÊã¨ÂêéÁºÄ)ÔºåÂπ∂‰∏îË¢´ÂØºÂÖ•Êó∂‰ºöÊâßË°åÈ°∂Â±ÇÁöÑ‰ª£Á†ÅÔºà‰∏çÂåÖÊã¨ function, classÔºâ\nfreeCodeCamp-Goran Aviani\n__file__ (2024-02-28) Ref: GfG\nThe file path of the function\u0026rsquo;s definition\n1 2 3 # Hello.py def HelloWorld(): print(\u0026#34;This is Hello.py\u0026#34;) Call that module in another file:\n1 2 3 4 5 6 7 8 9 10 11 # GFK.py import Hello import os Hello.HelloWorld() print(Hello.__file__) print(f\u0026#39;GFK.py\\\u0026#39;s __file__: {__file__}\u0026#39;) print(os.path.join(os.path.dirname(os.path.abspath(__file__)), \u0026#34;third_party/glm/\u0026#34;)) Output of python GFK.py\n1 2 3 4 This is Hello.py /home/yi/Downloads/ipynb_test/test_file_/Hello.py GFK.py\u0026#39;s __file__: GFK.py /home/yi/Downloads/ipynb_test/test_file_/third_party/glm/ Built-in functions Python Docs\ndir(object) returns a list of the names of attributes and methods of any kind of object, e.g., module, class, instance. No values are stored.\nvars(object) returns a dict: object.__dict__ containing changeable attributes.\nvars() acts like locals(): a dict of local variables for read. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class person: name = \u0026#34;jack\u0026#34; dir(person) # A list, len = 27 person.__dir__ # A method vars(person) # same as person.__dict__, len=5 # instance student = person() setattr(student, \u0026#39;age\u0026#39;, 14) vars(student) # out: {\u0026#39;age\u0026#39;: 14} dir(student) # a list of attr and methods of the object set(student.__dir__()) - set(dir(person)) # out: {\u0026#39;age\u0026#39;} set(student.__dir__()) == set(dir(student)) # True set(dir(student)) == set(person.__dir__(student)) # True set(dir(student)) - set(person.__dir__(person)) # {\u0026#39;__weakref__\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;name\u0026#39;} set(person.__dir__(person)) - set(dir(student)) # {\u0026#39;__abstractmethods__\u0026#39;, \u0026#39;__base__\u0026#39;, \u0026#39;__bases__\u0026#39;, ... [::-1] flip image 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import matplotlib.pyplot as plt from PIL import Image # pillow import cv2 Image.open(\u0026#39;1.jpg\u0026#39;) # RGB cv2.imread(\u0026#39;1.jpg\u0026#39;) # nd array, BGR plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)) plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[:,:, ::-1]) # ÊúÄÂêéÁöÑÈÄöÈÅìÂèñÂèç plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[::-1,:, ::-1]) # ‰∏ä‰∏ãÁøªËΩ¨ plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[::-1, ::-1, ::-1]) # Â∑¶Âè≥‰πüÁøªËΩ¨ Ë°®ËææÂºèÂÜÖËµãÂÄº PEP 572, Python 3.8+Ôºå Á±ª‰ºº n := xxx\nÊØîÂ¶ÇÔºåÊúâ‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤Êï∞ÁªÑÔºåÊÉ≥ÂéªÊéâÊØè‰∏™Â≠óÁ¨¶‰∏≤ÈáåÁöÑÁ©∫Ê†ºÂíåÁ©∫Â≠óÁ¨¶‰∏≤\n1 2 3 arr = [\u0026#39;a\u0026#39;, \u0026#39;bb \u0026#39;, \u0026#39; \u0026#39;] arr = [x.strip() for x in arr] # Âà†Èô§Â≠óÁ¨¶‰∏≤ÂâçÈù¢ÂíåÂêéÈù¢ÁöÑÁ©∫Ê†º arr = [x for x in arr if x] # Á©∫Â≠óÁ¨¶‰∏≤‰ºöÂΩì‰Ωúfalse ‰∏§Ë°åÂÜô‰∏ÄË°å: arr = [ y for x in arr if (y := x.strip)) ]\nÂõ¢ÈòüÂºÄÂèëÊúÄÂ•ΩËøòÊòØÁî®ÊòìÊáÇÁöÑÂÜôÊ≥ï„ÄÇ 1_00_00‰ªÄ‰πàÊÑèÊÄùÔºüx:=yÔºüPythonÈáåÈù¢ÁöÑÂá†‰∏™ÊúâË∂£ÁâπÊÄß\nAssignment expression - RealPython Index value from dict Get value by giving index from a dictionary\nAccessing dictionary value by index in python\nPython Dictionaries are Ordered from python 3.7 6, so the index can be used like:\nvalue_at_index = list(dic.values())[index]\nPython Dictionaries are Ordered now, but how?‚Ä¶and why? Make one-hot matrix Convert a vector to one-hot matrix\nConvert array of indices to one-hot encoded array in NumPy - StackOvf\n1 2 3 values = [1, 0, 3] n_values = np.max(values) + 1 np.eye(n_values)[values] if/else in a list comprehension SO\nFlatten a list of list Python - Flatten a list of lists to a single list (Dead link now)\nConcatenate list of tensor: torch.stack() first, then reshape Redirect print content (2023-08-08)\nRedirect the standard output (print stream) to a file using the sys module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sys # Save the current stdout stream to a variable original_stdout = sys.stdout # Write to file with open(\u0026#39;output.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: sys.stdout = f print(\u0026#34;The content given to sys.stdout \\ will be redirected to file, \\ instead of being printed out\u0026#34;) # Restore the original stdout stream sys.stdout = original_stdout argparse (2023-08-28)\naction=\u0026quot;store_true\u0026quot;, this option is True when this argument appears in the command\nnargs=+, this argument at least be assigned with 1 value. Similarly, nargs=3, the argument must be assigned with 3 values.\nArgparse: how to handle variable number of arguments (nargs=\u0026rsquo;*\u0026rsquo;) - SO\nExample:\n1 parser.add_argument(\u0026#39;--img_wh\u0026#39;, nargs=\u0026#34;+\u0026#34;, type=int, default=[1152, 864]) The command line: python eval.py --img_wh 640 512. The args in vscode \u0026ldquo;launch.json\u0026rdquo;: \u0026quot;args\u0026quot;: [\u0026quot;--img_wh\u0026quot;, \u0026quot;1152\u0026quot;, \u0026quot;864\u0026quot;,]\nUpdate Dict attributes (packge: mmcv)\nmmcv Docs\nmmsegmentÈÖçÁΩÆÂèÇÊï∞ËØ¥ÊòéÔºà‰∫îÔºâ- CSDN\n1 2 3 4 5 6 7 from mmcv import DictAction parser = argparse.ArgumentParser(description=\u0026#39;Train a recognizer\u0026#39;) parser.add_argument(\u0026#39;--cfg-options\u0026#39;, nargs=\u0026#39;+\u0026#39;, action=DictAction, default={}, help=\u0026#39;override some settings in the used config, the key-value pair \u0026#39; \u0026#39;in xxx=yyy format will be merged into config file. For example, \u0026#39; \u0026#34;\u0026#39;--cfg-options model.backbone.depth=18 model.backbone.with_cp=True\u0026#39;\u0026#34;) Example in launch.json of adapt-image-models:\n1 2 3 4 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34;, ] Set choices:\n1 2 3 4 5 parser.add_argument( \u0026#39;--launcher\u0026#39;, choices=[\u0026#39;none\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;slurm\u0026#39;, \u0026#39;mpi\u0026#39;], default=\u0026#39;none\u0026#39;, help=\u0026#39;job launcher\u0026#39;) Accept a dict (2023-08-31)\nAccepting a dictionary as an argument with argparse and python (duplicate) -SO\nargparse don\u0026rsquo;t have a type of dict, there\u0026rsquo;re 2 workarounds:\nSet type=str, then use json.loads() to parse:\nSet type=json.loads:\n1 2 3 4 5 import json parser.add_argument(\u0026#39;-d\u0026#39;, \u0026#39;--my-dict\u0026#39;, type=json.loads) args = parse.parse_args() mydict = args.my_dict # Will return a dictionary The DictAction of mmcv will only parse the top-level as a dict (key-value), but if the value passed is a dict too, it\u0026rsquo;ll be parsed as a string.\n1 2 3 4 5 6 7 8 9 10 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, // \u0026#34;train_pipeline[1].clip_len=3\u0026#34;, // Cannot identify [1] // \u0026#34;dict(train_pipeline={\u0026#39;1\u0026#39;: dict(clip_len=3)})\u0026#34;, // Error: has to 2 params, but got 1. // \u0026#34;train_pipeline= {\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: \\\u0026#34;3\\\u0026#34;}}\u0026#34; // The value will be parsed as a string, which will override the whole previous dict. ] Access a dict myDict.update({'aKey': \u0026quot;newVal\u0026quot;}) or myDict.update(aKey='newVal')\nmyDict.get('aKey', retVal)\nmyDict.setdefault('aKey', 'defVal'): if 'aKey' doesn\u0026rsquo;t exist, it will be inserted with the default value defVal.\nmyDict.copy() or newDict = dir(myDict) are shallow copy: when change the no-primitive data (such as list, nested data), the orignal dict will also change, because it only duplicate the top-level data, while the low-level data is referenced to the original data. Copy a Python Dictionary: A Complete Guide ‚Ä¢ datagy\nmyDict.pop('aKey') will return and remove an item.\nDelete items from dictionary while iterating Have to iterate the dict twice, the 1st round collects the keys to be deleted, and the 2nd round del those items.\nlogging Real Python\n1 2 3 4 5 6 7 import logging logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logger=logging.getLogger(__name__) logger.setLevel(logging.INFO) logger.info(\u0026#34;This is an info message\u0026#34;) Output: INFO:__main__:This is an info message\nlogging.info doesn\u0026rsquo;t show up on console tqdm (2023-10-20)\nDDG with searching \u0026ldquo;avoid writting tqdm bar into the log file\u0026rdquo;\nUse tqdm to iterate the outer loop and print in the inner loop. How can I make the tqdm progress bar be printed less frequently in the log file?\n1 2 3 for outer in tqdm(range(0, 1e5, 1e4)): for inner in range(1e4): print(outer+inner) Output tqdm to a null device, and print/log the status bar after desired steps. python - Making tqdm write to log files - Stack Overflow\n1 2 3 4 tqdm_bar = tqdm(range(20), file=open(os.devnull, \u0026#39;w\u0026#39;)) for i in tqdm_bar: if tqdm_bar.n % 5 ==0: print(str(tqdm_bar)) Redirecting console logging to tqdm.write(), leaving other logging handlers (e.g. log files) unaffected. tqdm.contrib.logging - Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 import logging from tqdm import trange from tqdm.contrib.logging import logging_redirect_tqdm LOG = logging.getLogger(__name__) if __name__ == \u0026#39;__main__\u0026#39;: logging.basicConfig(level=logging.INFO) with logging_redirect_tqdm(): for i in trange(9): if i == 4: LOG.info(\u0026#34;console logging redirected to `tqdm.write()`\u0026#34;) # logging restored Discussion: Pass progress bar to logger ¬∑ Issue #313 ¬∑ tqdm/tqdm tqdm output to stderr, which is not in sync with stdout. So only redict the stdout (print) to file: python test.py \u0026gt; output.log\nHow to \u0026ldquo;flush\u0026rdquo; tqdm progress bar explicitly?\nFound in Google\nMonitor log file in real-time: tail -f output.log. reddit\nDisable tqdm bar\nPresetting argument. Inspired by the comment of this answer of Why is tqdm printing to a newline instead of updating the same line?; Searched by DDG with searching \u0026ldquo;tqdm leave\u0026rdquo;\n1 2 3 4 5 from functools import partial from tqdm import tqdm tqdm = partial(tqdm, disable=True) for i in tqdm(iterable): Disable tqdm bar by environment variable after version 4.66.0:\n1 2 # pip install tqdm --upgrade eport TQDM_DISABLE=1 Silence tqdm\u0026rsquo;s output while running tests or running the code via cron; Searched by DDG with searching \u0026ldquo;tqdm disable\u0026rdquo;\nEnv variable overrides supported by tqdm==4.66.0\nleave=False means the bar will disappear instead of leaving it left after finishing. How to remove progressbar in tqdm once the iteration is complete\nUse Progress bar as status bar showing current entry, by removing all field but the description field and specifying position.\n1 2 3 4 5 6 7 8 outer = tqdm.tqdm(total=len(files), desc=\u0026#34;Files\u0026#34;, position=0) cur_file = tqdm.tqdm(total=0, position=1, bar_format=\u0026#34;{desc}\u0026#34;) for name in files: video = imageio.mimread(name, memtest=False) cur_file.set_description_str(f\u0026#34;Current file: {name}\u0026#34;) outer.update(1) Progress bar and status logging in python with tqdm\nRegistration mechanism A registry is a centralized \u0026ldquo;object\u0026rdquo; for convenience of interacting between differen components.\n1 2 3 4 5 from ..builder import BACKBONES # registry @BACKBONES.register_module() # add a component class ViT_CLIP(nn.Module): # backbone model def __init__(): isinstance() Check if an object is one of types\ndataset is expected to be a list or a tuple:\n1 dataset = dataset if isinstance(dataset, (list, tuple)) else [dataset] Convert str to classname Use eval\n1 eval(\u0026#34;cls_name\u0026#34;) Convert string to Python class object?\nUse getattr\n1 2 optim_type = \u0026#39;AdamW\u0026#39; self.optim = getattr(torch.optim, optim_type)(optim_params, **optim_kwargs) Code from MatchNeRF.\nEven and odd compatible 1 eval_batch_size = (self.eval_batch_size - 1) // sb + 1 Code from pixelNeRF\u0026rsquo;s \u0026ldquo;nerf.py\u0026rdquo;.\nSave argparse to file (2023-09-25)\nSaving python argparse file - SO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import json import time from pathlib import Path timestamp = time.strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;, time.localtime()) log_file = Path(cfg.work_dir) / f\u0026#39;{timestamp}.log\u0026#39; with open(f\u0026#34;log_file\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump({\u0026#34;CommandLine_Args\u0026#34;: args.__dict__, \u0026#34;ConfigFile_Settings\u0026#34;: conf }, f, indent=2 ) # --- Read --- parser = ArgumentParser() args = parser.parse_args() with open(\u0026#39;commandline_args.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: args.__dict__ = json.load(f) json has better readabiity, interoperability, security, but slower than pickle. Pickle or json?\njson.dumps convert any python object to JSON formatted string, while json.dump() write a serialized object to file. PYnative\nWrite JSON File (2024-03-21)\njson.dump() convert a python object to a json string. GfG\n1 2 3 4 json_data = {\u0026#34;camera_angle_x\u0026#34;: fovx, \u0026#34;frames\u0026#34;: frams_ls} with open(f\u0026#34;{dir_json}/transforms_train.json\u0026#34;, \u0026#39;w\u0026#39;) as file: file.write(json.dumps(json_data, indent=4)) Write a json string into a file using pathlib to avoid dangling with open(): SO\n1 2 3 json_data = {\u0026#34;camera_angle_x\u0026#34;: fovx, \u0026#34;frames\u0026#34;: frams_ls} pathlib.Path(f\u0026#34;{dir_json}/transforms_train.json\u0026#34;).write_text(json.dumps(json_data, indent=4)) But, in this way, I don\u0026rsquo;t know how to realize 'w' that always creats a new file. Working With JSON Data in Python - Real Python Move a dict to GPU (2023-09-27)\nGiven a dict containing list, tuple, and torch.Tensor,\nRecursive\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def move_to_device(X, device): if isinstance(X, dict): for k, v in X.items(): X[k] = move_to_device(v, device) elif isinstance(X, list): for i, e in enumerate(X): X[i] = move_to_device(e, device) elif isinstance(X, tuple) and hasattr(X, \u0026#34;_fields\u0026#34;): # collections.namedtuple dd = X._asdict() dd = move_to_device(dd, device) return type(X)(**dd) elif isinstance(X, torch.Tensor): return X.to(device=device) return X Code from MatchNeRF.\nMake pairs from 2 list Two for in one line:\n1 index_lists = [(a, b) for a in range(n_views - 1) for b in range(a + 1, n_views)] # [(0,1), (0,2), (1,2)] Code from MatchNeRF.\nSlice() for a dim (2023-09-29)\n1 2 3 a = torch.arange(5) x = slice(0, -1, 1) # specify start index, end index, step size a[x] # [0,1,2,3] Convert class to dict (2023-10-05)\nIn python, how do I cast a class object to a dict - SO\ncollections.namedtuple vs typing.NamedTuple- RealPython\ntyping.NamedTuple - Docs\nWhat are \u0026ldquo;named tuples\u0026rdquo; in Python? - SO\n1 2 3 4 5 6 7 8 9 10 11 from typing import NamedTuple class myNT(NamedTuple): r\u0026#34;\u0026#34;\u0026#34;a doc string\u0026#34;\u0026#34;\u0026#34; foo: int bar: str baz: list aTupleObj = myNT(1, \u0026#39;bar\u0026#39;, []) aTupleObj._asdict() NamedTuple (2024-04-09)\nNamedTuple is not mutable, so the following error will occur when trying to re-assign its atttibutes: AttributeError: can\u0026rsquo;t set attribute in python - SO\n1 2 3 4 5 6 7 from typing import NamedTuple class BasicPointCloud(NamedTuple): points : np.array colors : np.array normals : np.array pcd.points = new_points Format digits Fix length filling with zeros: How do I format a number with a variable number of digits in Python? - SO\n'123'.zfill(7): 0000123\nf-string round a float: Fixed digits after decimal with f-strings\n1 2 a = 10.1234 f\u0026#39;{a:.2f}\u0026#39; (2024-01-22)\nFiles Open multiple files: Source code\n1 2 with open(scene_info.ply_path, \u0026#39;rb\u0026#39;) as src_file, open(os.path.join(self.model_path, \u0026#34;input.ply\u0026#34;) , \u0026#39;wb\u0026#39;) as dest_file: dest_file.write(src_file.read()) Pass by Assignment Python is Pass by Reference - RealPython\n(2024-02-08)\nIn Python, passing arguments to a function is assigning the incoming objects to new variable name (identifier). This can be verified through \u0026ldquo;reference counter\u0026rdquo; and namespace, which is a dictionary, including key-value pairs representing a binding between target variable name and object.\n1 x = 2 # object 2\u0026#39;s refcount+1, namespace add: \u0026#39;x\u0026#39;:\u0026#39;2\u0026#39; When an object (value) is assigned to a variable name (identifier), the reference counter of the object is incremented. If the variable name is reassigned to another value, the pre-bounded object\u0026rsquo;s reference counter is decremented.\nEach function has its own namespace, which can be checked by print(locals())\nIf an attribute of the object supports modification, and the object is passed to a function argument, assigning values to the object\u0026rsquo;s attribute is in-place modification. Ref\nMisc:\nUse id() to get the address of a variable\nWhen Python modifies a variable through a function, the variable is not modified in place, but reassigned with the returned value.\nPassing by reference in Python can be replicated with: object\u0026rsquo;s attributes, dictionary (mapping type), and list-like data (subscriptable and mutable)\nExample refers to 3DGS:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class GaussianModel: def __init__(self,): self._xyz = torch.empty(0) class Scene: def __init__(self, gaussian: GaussianModel, exTen, color, ls): self.gaussian = gaussian # a binding created in locals() # The attribute _xyz supports assignment as tensor is mutable # object\u0026#39;s attribute is modified in place: self.gaussian._xyz = torch.ones(2,3) self.myTen = exTen # exTen\u0026#39;s refcount+1, self.myTen = torch.ones(1,4) # update to new binding self.color = color # a binding between self.color and color self.color.view(2,2) # new binding self.ls = ls self.ls[0] = 88 # affect the original list gaussian = GaussianModel() exTensor = torch.empty(0) color=torch.arange(4) myObj = Scene(gaussian, exTensor, color,ls:=[1,2]) print(gaussian._xyz) # changed to ones print(exTensor) # remain empty print(color) # unchanged print(ls) # changed to [88,2] Mutalbe - RealPython\nperplexity\nre Find numbers (2024-03-17)\nHow to extract numbers from a string in Python? - SO\n1 2 3 4 5 6 import re regx_pattern = r\u0026#39;\\d+\\.\\d+|\\d+\u0026#39; txt = \u0026#34;The price is $12.99 and the quantity is 5 21321 dasdsa 123213.\u0026#34; matches = re.findall(regx_pattern, txt) print(matches) Output: ['12.99', '5', '21321', '123213']\nimport (2024-04-09)\nImport file in 2 levels up.\nValueError: attempted relative import beyond top-level package - SO\n1 2 3 4 5 6 7 8 9 10 Project _3DGS scene __init__.py utils # for _3DGS # there is no __init__.py (Don\u0026#39;t know if it matters) utils # for current project __init__.py # empty is ok file2.py I want to use a method (write_j) implemented in file2.py inside scene/__init__.py. Although, the above SO post stated that all folder and subfolders should have an __init__.py, I tried the following syntax works:\n1 from utils.fuse_neighbors import fuse_neighbors Previously, the 3 dots: from ...utils.fuse_neighbors import fuse_neighbors, will lead to an error:\n1 ValueError: attempted relative import beyond top-level package Module (2024-05-16)\nA .py file is a module, not a function.\nAnd a module cannot be called:\n\u0026ldquo;combine_pcs.py\u0026rdquo;:\n1 2 3 def combine_plys(dataset): print(dataset) return eval.py:\n1 2 3 from utils import combine_pcs combine_pcs(dataset) error:\n1 2 3 File \u0026#34;/home/zi/Downloads/CasMVSNet_pl-comments/eval_refine.py\u0026#34;, line 418, in \u0026lt;module\u0026gt; combine_pcs(dataset) TypeError: \u0026#39;module\u0026#39; object is not callable Rectify: from module import the function:\n1 from utils.combine_plys import combine_plys ","date":"2022-12-25T23:37:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_misc/","title":"memo: Python | Misc"},{"content":"ÊÄªÁªì‰πãÂâçËÆ≤ËøáÁöÑÂêÑÁßçÊ®°ÂûãÔºåÂºïÂá∫‰πãÂêéË¶ÅËÆ≤ÁöÑÁîüÊàêÊ®°Âûã\n1. ÂÆö‰πâ P1 - „ÄêÊú∫Âô®Â≠¶‰π†„ÄëÁôΩÊùøÊé®ÂØºÁ≥ªÂàó(‰∏âÂçÅ) ÔΩû ÁîüÊàêÊ®°ÂûãÁªºËø∞(Generative Model Introduction)\nÁîüÊàêÊ®°ÂûãÈô§‰∫ÜÁîüÊàêÊï∞ÊçÆËøòËÉΩÂÅö‰ªÄ‰πà‰ªªÂä°Ôºü\nGAN Áî®Êù•ÁîüÊàêÊï∞ÊçÆ„ÄÇ GMM Áî®Êù•ÂÅöËÅöÁ±ª‰ªªÂä°ÁöÑ„ÄÇ Ëøô2‰∏™ÈÉΩÊòØÊó†ÁõëÁù£Â≠¶‰π†ÔºàÊó†Ê†áÁ≠æÔºâ„ÄÇ ÈÄªËæëÂõûÂΩíËôΩÁÑ∂‰∏éÊ¶ÇÁéáÁõ∏ÂÖ≥Ôºå‰ΩÜÂÆÉ‰∏çÊòØÁîüÊàêÊ®°Âûã„ÄÇP(Y=1 | X) = ?, P(Y=0 | X) = ? Âè™ÊòØÂØπÊù°‰ª∂Ê¶ÇÁéáÂª∫Ê®°ÔºåËÄå‰∏çÂÖ≥Ê≥®Ê†∑Êú¨Êï∞ÊçÆXÊú¨Ë∫´ÁöÑÂàÜÂ∏ÉÔºå\nÊâÄ‰ª•ÔºàÊ¶ÇÁéáÔºâÁîüÊàêÊ®°ÂûãÂÖ≥Ê≥®ÁöÑÊòØÊ†∑Êú¨ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÊú¨Ë∫´ÔºåËÄå‰∏éÂÆÉË¶ÅËß£ÂÜ≥ÁöÑ‰ªªÂä°Ê≤°ÊúâÂøÖÁÑ∂ÁöÑËÅîÁ≥ª„ÄÇ Êó¢ÂèØ‰ª•Ëß£ÂÜ≥ÁõëÁù£Â≠¶‰π†ÁöÑ‰ªªÂä°ÔºàÂØπ P(X,Y) Âª∫Ê®°ÔºâÔºå‰πüÂèØ‰ª•Ëß£ÂÜ≥Êó†ÁõëÁù£Â≠¶‰π†ÁöÑ‰ªªÂä°ÔºåÂØπ‰∫éÈöêÂèòÈáèÊ®°ÂûãÔºåÂèØ‰ª•ÊûÑÈÄ† P(X,Z) Âπ∂Âª∫Ê®°ÔºõÊàñËÄÖ‰∏çÁî®ÈöêÂèòÈáèÔºåÊØîÂ¶ÇËá™ÂõûÂΩíÊ®°ÂûãÔºåÁõ¥Êé•ÂØπ P(X) Âª∫Ê®°ÔºàÊääP(X) ÊãÜÊàêÂêÑ‰∏™Áª¥Â∫¶‰πãÁßØÔºâ„ÄÇ\nÊâÄ‰ª•ÂÖ≥Ê≥®Ê†∑Êú¨ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÁöÑÊ®°ÂûãÂ∞±ÊòØÁîüÊàêÊ®°Âûã\n2. Ê®°ÂûãÂàÜÁ±ª P2\nÊåâÁÖßËß£ÂÜ≥ÁöÑ‰ªªÂä°ÔºöÁõëÁù£ vs. ÈùûÁõëÁù£ ÔºàËá™ÁõëÁù£Êú™‰ªãÁªçÔºâ ÂØπÂêÑÁßçÊ®°ÂûãÂàÜÁ±ª\n‰ªªÂä°ÔºöÂàÜÁ±ªÔºåÂõûÂΩíÔºåÊ†áËÆ∞ÔºåÈôçÁª¥ÔºåËÅöÁ±ªÔºåÁâπÂæÅÂ≠¶‰π†ÔºåÂØÜÂ∫¶‰º∞ËÆ°ÔºåÁîüÊàêÊï∞ÊçÆ\nÊääÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÂàÜ‰∏∫‚ÄúÊ¶ÇÁéá‚Äù‰∏é‚ÄúÈùûÊ¶ÇÁéá‚Äù‰∏ÄËà¨Ê≤°Â§™Â§ßÂøÖË¶ÅÔºå‰ΩÜÊòØÁîüÊàêÊ®°Âûã‰∏ÄÂÆöÊòØ‰∏éÊ¶ÇÁéáÁõ∏ÂÖ≥ÁöÑÔºåÊâÄ‰ª•ËøôÈáå‰ª•Ê¶ÇÁéá‰∏∫ÂàÜÁ±ªÊ†áÂáÜ\nÁõëÁù£Â≠¶‰π†‰ªªÂä°\nÊ¶ÇÁéáÊ®°ÂûãÔºàÂª∫Ê®°‰∏éÊ¶ÇÁéáÁõ∏ÂÖ≥Ôºâ\nÂà§Âà´Ê®°Âûã\nÂØπ‚ÄùÊù°‰ª∂Ê¶ÇÁéá‚ÄúÁöÑÂàÜÂ∏ÉÂª∫Ê®°ÔºöÈÄªËæëÂõûÂΩí(LR)ÔºåÊúÄÂ§ßÁÜµ(MEMM)ÔºåÊù°‰ª∂ÈöèÊú∫Âú∫(CRF)ÔºåÔºàËæìÂá∫‰∏∫Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÂèÇÊï∞ÁöÑÔºâÁ•ûÁªèÁΩëÁªúNN\nÁîüÊàêÊ®°Âûã\nÁÆÄÂçïÁöÑÁ•ûÁªèÁΩëÁªúÂè™ËÉΩÊòØÂà§Âà´Ê®°ÂûãÔºå‰∏çÊòØÁîüÊàêÊ®°Âûã„ÄÇ‰ΩÜÊòØNNÈáåÂàÜÊ≠•ÂºèË°®Á§∫ÂèØ‰ª•ÂíåÊ¶ÇÁéáÂõæÊ®°ÂûãÁªìÂêàÔºåÂ∞±ÂèòÊàê‰∫ÜÔºàÊ∑±Â∫¶ÔºâÁîüÊàêÊ®°Âûã ÂÖ∑‰ΩìËßÅ‰∏ãÊñá ÈùûÊ¶ÇÁéáÊ®°ÂûãÔºàÂª∫Ê®°Êó∂Êú™ËÄÉËôëÊ¶ÇÁéáÂàÜÂ∏ÉÔºâÔºåËã•Ë¶ÅËß£ÂÜ≥ÂàÜÁ±ªÈóÆÈ¢òÔºåÂàôÂ§ßÊ¶ÇÁéáÊòØÂà§Âà´Ê®°Âûã\nÊÑüÁü•Êú∫PLAÔºåÔºàÁ°¨Èó¥ÈöîÔºâÊîØÊåÅÂêëÈáèÊú∫SVMÔºåKNNÔºåÔºàËæìÂá∫‰∏∫Á±ªÂà´ÁöÑÔºâÁ•ûÁªèÁΩëÁªúNNÔºåÊ†ëÊ®°Âûã ÈùûÁõëÁù£Â≠¶‰π†‰ªªÂä°\nÊ¶ÇÁéáÊ®°ÂûãÔºöÂøÖÁÑ∂ÊòØÁîüÊàêÊ®°ÂûãÔºåÂõ†‰∏∫ÈùûÁõëÁù£ÈáåÊ≤°ÊúâÊ†áÁ≠æYÊó†Ê≥ïÂà§Âà´ÔºåÂè™ËÉΩÊèèËø∞Ê†∑Êú¨XÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇÊ¶ÇÁéáÂõæÊ®°Âûã‰∏≠ÁöÑÂ§ßÈÉ®ÂàÜÊòØÁîüÊàêÊ®°Âûã\nÈùûÊ¶ÇÁéáÊ®°ÂûãÔºöPCAÈôçÁª¥ÔºàSVDÂàÜËß£ÔºâÔºåÊΩúËØ≠‰πâÂàÜÊûêLSA (pLSA, LDA)ÔºåK-meansÔºåÔºà‰∏çÂ∏¶Ê†áÁ≠æÁöÑNNÔºâËá™ÁºñÁ†ÅÂô®Auto-Encoder\nPCA ‰ªéÊ¶ÇÁéáËßíÂ∫¶ÁúãÔºåÂÆÉÂ∞±ÊòØP-PCAÁöÑ‰∏ÄÁßçÔºåÂ∞±ÊòØÂõ†Â≠êÂàÜÊûêFactor Analysis LSA ÁöÑÊ¶ÇÁéáÊ®°ÂºèÊòØ pLSA, ÂÜçÊîπÈÄ†Â∞±ÊòØ LDA K-means ‰ªéÊ¶ÇÁéáËßíÂ∫¶ÁúãÔºåÊòØÁâπÊÆäÁöÑGMM Auto-Encoder ÁöÑÊ¶ÇÁéáÊ®°ÂºèÂ∞±ÊòØ VAE ÂêÑÁßçÁîüÊàêÊ®°Âûã ÁîüÊàêÊ®°ÂûãÂàÜÊàêÁõëÁù£ÔºåÈùûÁõëÁù£‰∏çÈáçË¶Å\nNaive Bayes\nÊú¥Á¥†Ë¥ùÂè∂ÊñØÊòØÊúÄÁÆÄÂçïÁöÑÁîüÊàêÊ®°ÂûãÔºåÁõ¥Êé•ÊèèËø∞ xÔºåÂÅáËÆæÁÆÄÂçïÔºöÂõ†‰∏∫ÂÆÉÊòØÂà§Âà´Ê®°ÂûãÔºåÊâÄ‰ª•ÂÆÉÂÅáËÆæÊòØÂú®ÁªôÂÆö y ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ†∑Êú¨ x‚àà‚Ñù·µñ ÂèØ‰ª•Ë°®Á§∫ÊàêÂêÑÁª¥Â∫¶‰πãÁßØ\nP(x|y) = ‚àè·µ¢‚Çå‚ÇÅ·µñ p(x·µ¢|y)\nMixture model\nGMM ËÆ§‰∏∫ x Áî± z ÁîüÊàêÔºåÂú®ÁªôÂÆö z ÁöÑÊÉÖÂÜµ‰∏ãÔºåxÊúç‰ªéÈ´òÊñØÂàÜÂ∏É„ÄÇ\nTime-series model\n‰ªéÊó∂Èó¥Â∫èÂàóËßíÂ∫¶Ôºå‰ªéÊúâÈôêÂà∞Êó†ÈôêÔºåÊØîÂ¶Ç HMMÔºåÂç°Â∞îÊõºÊª§Ê≥¢ÔºåÁ≤íÂ≠êÊª§Ê≥¢\nNon-parametric\nÂú®ÂèÇÊï∞Á©∫Èó¥‰∏äÔºå‰ªéÊúâÈôêÂà∞Êó†ÈôêÔºöÈ´òÊñØËøáÁ®ã Gaussian process/ Dirichlet processÔºåÊòØÈùûÂèÇË¥ùÂè∂ÊñØÊ®°ÂûãÔºå ÂÆÉÁöÑÂèÇÊï∞‰∏çÂÜçÊòØÂõ∫ÂÆöÁöÑÔºåÊú™Áü•ÁöÑÂ∏∏Êï∞‰∫Ü„ÄÇÈ´òÊñØÂàÜÂ∏ÉÊòØÂèÇÊï∞Ê®°ÂûãÔºöÈÄöËøálearningÊääÔºàmu,sigmaÔºâÂ≠¶‰π†Âá∫Êù•Ôºå\nMixed Membership Model\n‰πüÊòØÊ∑∑ÂêàÊ®°ÂûãÔºåÊØîGMMÈÇ£‰∏≠Â§çÊùÇ‰∫õÔºåÂèòÈáèÂ§ö„ÄÇLDA ÈöêÂê´ÁãÑÂà©ÂÖãÈõ∑ÂàÜÂ∏ÉÔºåÁî®Êù•ÂÅöÊñáÊ°£ËÅöÁ±ª\nFactorial Modeel\nÂõ†Â≠êÊ®°ÂûãÔºöÂõ†Â≠êÂàÜÊûêFAÔºåÊ¶ÇÁéáPCA P-PCA, ICA, Á®ÄÁñèÁºñÁ†Åsparse coding\n‰ª•‰∏ä 6 ÁßçÈÉΩÊòØÁªìÊûÑÂåñÁöÑÊ¶ÇÁéáÂõæÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÊØè‰∏ÄÁ±ªÈÉΩÊúâÂõ∫ÂÆöÁöÑÂ•óË∑ØÔºåÊÄùÊÉ≥ÊØîËæÉÂ∫ïÂ±ÇÔºå‰∏ìÂÆ∂ËÆæËÆ°ÁöÑÂ§ÑÁêÜÁâπÂÆöÈóÆÈ¢òÁöÑÁÆóÊ≥ï„ÄÇ‰∏é‰∏ãÈù¢ÁöÑ‚ÄúÊ∑±Â∫¶‚ÄùÁîüÊàêÊ®°ÂûãÂØπÂ∫îÔºåÂÆÉ‰ª¨ÂàôÊòØ‚ÄúÊµÖÂ±Ç‚ÄùÁîüÊàêÊ®°Âûã„ÄÇ\n‰ª•‰∏ãÊòØ‰∏éÊ∑±Â∫¶Â≠¶‰π†Á•ûÁªèÁΩëÁªúÁõ∏ÁªìÂêàÁöÑÊ®°Âûã:\nEnergy-based Model\nBoltzmann MachineÁéªÂ∞îÂÖπÊõºÊú∫ÔºåÂåÖÊã¨ sigmoid networkÔºådeep belif networkÔºå ÈÉΩÊòØÊó†ÂêëÂõæÊ®°Âûã\nVAE\nËá™ÁºñÁ†ÅÂô®‰∏éÊ¶ÇÁéáÂõæÁªìÂêàÔºåÁî®ÂèòÂàÜÁöÑÊâãÊÆµÂ§ÑÁêÜ\nGAN\nAuto regressive model\nËá™ÂõûÂΩíÁΩëÁªú\nFlow-based model\n3. Ê®°ÂûãË°®Á§∫\u0026amp;Êé®Êñ≠\u0026amp;Â≠¶‰π† P3\n‰ªéÊ®°ÂûãË°®Á§∫ÔºåÊé®Êñ≠ÔºåÂíåÂ≠¶‰π†ÁöÑËßíÂ∫¶ÂéªËÆ§ËØÜ‰∏Ä‰∏™ÁîüÊàêÊ®°Âûã\nÊ®°ÂûãË°®Á§∫ ‚ÄúÂΩ¢Á•û‚ÄùÂÖºÂ§á\nÂΩ¢\nËäÇÁÇπÂèØ‰ª•ÊòØÁ¶ªÊï£ÁöÑÔºåÊúâÂèØ‰ª•ÊòØËøûÁª≠ÁöÑÔºõ ËæπÂèØ‰ª•ÊòØÊúâÂêëÁöÑÔºå‰πüÂèØ‰ª•ÊòØÊó†ÂêëÁöÑ„ÄÇÂ¶ÇÊûúÊâÄÊúâÁöÑËæπÊòØÊúâÂêëÁöÑÔºåÂàô‰∏∫ÊúâÂêëÂõæÊ®°Âûã ‰ª•‰∏äÂÜôÂá∫Êù•ÁöÑ11 ÁßçÈô§‰∫Ü ÁéªÂ∞îÂÖπÊõºÊú∫ÔºåÂÖ∂‰ªñÈÉΩÊòØÊúâÂêëÂõæÊ®°Âûã Âê´ÊúâÈöêÂèòÈáèËäÇÁÇπÂ∞±ÊòØÈöêÂèòÈáèÊ®°ÂûãÔºåËã•‰∏ç‰ΩøÁî®ÈöêÂèòÈáèÂ∞±ÊòØ‚Äúfully observed model‚Äù Ê¶ÇÁéáÂõæÁöÑÁªìÊûÑÔºö‰ªéÂ±ÇÊ¨°Êù•ÁúãÔºåshallow ÔºàÂâç6ÁßçÔºâ ÊàñËÄÖ deepÔºàÂêé5ÁßçÔºâÔºõ ÊàñËÄÖ‰ªéËøûÊé•Êï∞ÈáèÊù•ÁúãÔºåÂ∞±ÂØπÂ∫î sparse Âíå dense ‰∏§Á±ªÔºå ÊØîÂ¶ÇÁéªÂ∞îÂÖπÊõºÊú∫ÁöÑÂ±ÇÈó¥ÁöÑËøûÊé•Ê≤°ÊúâÁº∫Â§±,ÂÆÉÊòØÁ®†ÂØÜÁöÑ, ËÄåHMM‰∏Ä‰∏™ÈöêÂèòÈáèÂè™Êúâ2-3Êù°Ëæπ Á•ûÔºàÊ¶ÇÁéáÂàÜÂ∏ÉÊú¨Ë∫´Ôºâ\nÂØπ‰∫éÊ†∑Êú¨ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Êù•ËÆ≤ÔºåÂÆÉÊó¢ÂèØ‰ª•ÊòØÂèÇÊï∞ÂåñÊ®°ÂûãÔºåÂç≥ÂÆÉÁöÑÂèÇÊï∞ÊòØÂõ∫ÂÆöÁöÑÔºåÊú™Áü•ÁöÑÂ∏∏ÈáèÔºå‰πüÂèØ‰ª•ÊòØÈùûÂèÇÊï∞ÂåñÊ®°ÂûãÔºåÂç≥ÈùûÂèÇË¥ùÂè∂ÊñØ„ÄÇ\nÂèÇÊï∞ËßíÂ∫¶ parametric vs. Non-parametric models ÊòæÊÄß‰∏éÈöêÊÄßÂØÜÂ∫¶ÂáΩÊï∞ Implicit Density vs. Explicit Density„ÄÇÊòæÊÄßÊòØÁõ¥Êé•ÂØπP(X) Âª∫Ê®°ÔºåËã•ÊòØÈöêÂèòÈáèÊ®°ÂûãÔºåÂ∞±ÂØπP(X,Z)Âª∫Ê®°ÔºåËã•ÊòØfully observed modelÔºåÂ∞±Áõ¥Êé•ÂØπP(X) ÂàÜËß£„ÄÇ ÈöêÊÄß‰∏çÁõ¥Êé•ÂØπ P(X) Âª∫Ê®°ÔºåÂõ†‰∏∫ÂÆÉÁöÑ‰ªªÂä°‰∏çÊòØÂÖà‰º∞ËÆ°Âá∫Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÔºåÂÜç‰ªé‰∏≠ÁîüÊàêÊ†∑Êú¨„ÄÇÂè™ÈúÄË¶ÅÁ°Æ‰øùÊ†∑Êú¨ÊòØ‰ªé P(X) ‰∏≠ÁîüÊàêÁöÑÂç≥ÂèØ„ÄÇ‰∏äËø∞ÁöÑÂè™Êúâ GAN ÊòØImplicit ÁöÑÔºåÂÖ∂‰Ωô10ÁßçÈÉΩÊòØÊòæÂºèÁöÑ Êé®Êñ≠ Êé®Êñ≠ÊòØÂê¶ tractableÔºå (intractable)\nÂ≠¶‰π† Likelihood-based model ÔºàÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°) vs. Likelihood-free model (GAN ‰∏çÂÖ≥ÂøÉ P(X)Ôºå‰πüÂ∞±‰∏çÁÆ°Ê†∑Êú¨ÁöÑ‰ººÁÑ∂ÊòØÂ§öÂ∞ëÔºåÂÆÉÊúâËá™Â∑±ÁöÑÂà§Âà´Âô®ÂíåÁõÆÊ†áÂáΩÊï∞Ôºâ\n4. Ê®°ÂûãÂàÜÁ±ª P4\n‰∏ªË¶ÅÂÖ≥Ê≥®ÔºöÊó†ÁõëÁù£ÁöÑÔºåÊúâÂêëÂõæÁöÑÔºåÊ∑±Â±ÇÁªìÊûÑÁöÑÔºåÂèÇÊï∞ÂåñÁöÑÔºåÊ®°Âûã\nLikelihood-based model\nÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊòØÊòæÂºèÁöÑ Êé®Êñ≠ Tractable\nFully-observed model ÂÆÉÁöÑÊ¶ÇÁéá/‰ººÁÑ∂ÂèØÁõ¥Êé•Ê±ÇÂá∫ÔºåÊØîÂ¶ÇËá™ÂõûÂΩí Change of variable model ÂèòÈáèÊõøÊç¢ÔºåÊØîÂ¶Ç Flow-based Ê®°Âûã‰∏çÁõ¥Êé•Ê±ÇËß£Â§çÊùÇÁöÑP(X)Ôºå ËÄåÊääx‰∏é‰∏Ä‰∏™Êúç‰ªéÁÆÄÂçïÂàÜÂ∏ÉÁöÑÂèòÈáèzÔºåÁî®‰∏Ä‰∏™ËøûÁª≠ÂèØÈÄÜÁöÑÂ§çÊùÇÂáΩÊï∞ËÅîÁ≥ªËµ∑Êù•Ôºå‰ª•ÂºïÂÖ•ÈùûÁ∫øÊÄßËΩ¨Êç¢Ôºåx=g(z)ÔºåÁÑ∂ÂêéÂéªÂ≠¶‰π† g(z) Ôºå Âõ†‰∏∫ z=g‚Åª¬π(x)ÔºåÊâÄ‰ª• P‚Çì(X) = Pz(g‚Åª¬π(x) | ‚àÇg‚Åª¬π(x)/‚àÇx) Êé®Êñ≠ Intractable Â∞±Áî®Ëøë‰ººÊé®Êñ≠Approximate inference\nÂü∫‰∫éÂèòÂàÜÊé®Êñ≠ÔºåÊØîÂ¶ÇVAE Âü∫‰∫éÈ©¨Â∞îÁßëÂ§´ÈìæÔºåÊØîÂ¶Ç Energy-based model Likelihood-free model\nÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊòØÈöêÂºèÁöÑÔºå‰∏çÁõ¥Êé•ÂÖ≥ÂøÉP(X)Ôºå GAN Áõ¥Êé•Áî®generator ÁîüÊàêÊ†∑Êú¨ÔºåÁÑ∂ÂêéÁî® Âà§Âà´Âô® ÂéªËØÑ‰ª∑Ê†∑Êú¨Â•ΩÂùè„ÄÇÂÆÉÊòØÁõ¥Êé•ÁîüÊàêÊ†∑Êú¨ÔºåËÄå‰∏çÊòØÂÖà‰º∞ËÆ°PDFÔºåÂÜç‰ªéPDF‰∏≠ÈááÊ†∑ÁîüÊàêÊ†∑Êú¨„ÄÇ ‰∏çÁõ¥Êé•ÁîüÊàêÊ†∑Êú¨ÔºåÂèØ‰ª•Áî®MC ÈááÊ†∑ÔºåÊØîÂ¶Ç‚ÄúÁîüÊàêÈöèÊú∫ÁΩëÁªú‚Äù GSNÔºåÊúâÁÇπÁ±ª‰ºº‚ÄúÂéªÂô™Ëá™ÁºñÁ†ÅÂô®‚Äù 5. Ê¶ÇÁéáÂõæ vs. Á•ûÁªèÁΩëÁªú P5\n‰ªñ‰ø©‰∏çÊòØÈùûÊ≠§Âç≥ÂΩºÁöÑÂÖ≥Á≥ªÔºå‰∏çÊòØ‰∫íÊñ•ÁöÑÔºåÊòØÁã¨Á´ãÁöÑÔºå‰∏§ËÄÖÈÉΩÊúâÂèëÁîüÁöÑÂèØËÉΩÊÄß\nÁ•ûÁªèÁΩëÁªúÈáåÊúâ‚ÄúËÆ°ÁÆóÂõæ‚Äù\nÊ¶ÇÁéáÂõæÊòØÊ¶ÇÁéáÂàÜÂ∏ÉÁöÑË°®Á§∫ÔºåËÄå(ÂâçÈ¶à)Á•ûÁªèÁΩëÁªúÊòØÂáΩÊï∞ÈÄºËøëÂô®ÔºåÂè™‰∏çËøáÂáΩÊï∞ÂèØËÉΩÊØîËæÉÂ§çÊùÇÔºåÂ¶ÇÊûú‰∏çÁªôÁ•ûÁªèÁΩëÁªúÂä†‰øÆÊ≠£ÔºåÂÆÉ‰∏éÊ¶ÇÁéáÊ≤°ÂÖ≥Á≥ª„ÄÇ\nx \u0026ndash;\u0026gt; NN \u0026ndash;\u0026gt; yÔºåyÂèØ‰ª•ÊòØÁ¶ªÊï£ÁöÑÁ±ªÂà´Ôºå‰πüÂèØ‰ª•ÊòØËøûÁª≠ÁöÑÊï∞ÂÄº\nNN ÁöÑ‰ΩúÁî®Âè™Êúâ‰∏Ä‰∏™ÔºöÈÄºËøëÂáΩÊï∞„ÄÇËæìÂÖ•Ê†∑Êú¨ÔºåÂæóÂà∞ÁõÆÊ†áÂáΩÊï∞ÁöÑÂÄºÔºåÁî®Ê≠§ÂÄºÂØπ NN ‰∏≠ÁöÑÂèÇÊï∞ÔºàÊùÉÈáç„ÄÅÂÅèÁΩÆÔºâÊ±ÇÊ¢ØÂ∫¶ÔºåÁÑ∂ÂêéÂÅöÊ¢ØÂ∫¶‰∏ãÈôç\nÊ¶ÇÁéáÂõæÊ®°ÂûãÂèØÁ¨ºÁªüÁöÑÂàÜ‰∏∫ÔºöÊúâÂêëÂõæÊ®°ÂûãÔºàË¥ùÂè∂ÊñØÁΩëÁªúÔºâÔºåÊó†ÂêëÂõæÊ®°ÂûãÔºàÊØîÂ¶ÇBoltzmann machine)\nÁéªÂ∞îÂÖπÊõºÊú∫Êó¢Â±û‰∫éÊó†ÂêëÂõæÊ®°ÂûãÔºå‰πüÂ±û‰∫éÁ•ûÁªèÁΩëÁªúÔºå‰ª£Ë°®‚ÄúÂπø‰πâËøûÁªì‰∏ª‰πâ‚Äù„ÄÇ\nÁ•ûÁªèÁΩëÁªú‰πüÂèØÂàÜ‰∏∫ÔºöÁ°ÆÂÆöÊÄßÁ•ûÁªèÁΩëÁªúÔºàCNNÔºåRNNÔºâÔºåÈöèÊú∫ÊÄßÁ•ûÁªèÁΩëÁªúÔºàÊØîÂ¶Ç Boltzmann machineÔºåsigmoid belief network)\nÊâÄ‰ª•Âú®ËÆ®ËÆ∫Ê¶ÇÁéáÂõæ‰∏éÁ•ûÁªèÁΩëÁªúÁöÑÂå∫Âà´Êó∂Ôºå‰∏çËÄÉËôë Boltzmann machineÔºå Âè™ÊØîËæÉÊúâÂêëÂõæÊ®°ÂûãÔºàË¥ùÂè∂ÊñØÁΩëÁªúÔºâ‰∏éÁ°ÆÂÆöÊÄßÁ•ûÁªèÁΩëÁªú\n‰ªé‚ÄúË°®Á§∫‚ÄùÔºå‚ÄúÊé®Êñ≠‚ÄùÔºå‚ÄúÂ≠¶‰π†‚Äù 3‰∏™ËßíÂ∫¶ÂØπÊØî:\nÊ®°ÂûãË°®Á§∫\nË¥ùÂè∂ÊñØÁΩëÁªúÔºöÁªìÊûÑÂåñÁöÑÔºåÊµÖÂ±ÇÁöÑÔºåÁ®ÄÁñèÁöÑÔºàÈ´òÁª¥ÈóÆÈ¢òÊúâÂêÑÁßçÊù°‰ª∂Áã¨Á´ãÊÄßÂÅáËÆæÔºâÔºåËäÇÁÇπÊúâÊ¶ÇÁéáÊÑè‰πâÔºåÂÖ∑ÊúâÂèØËß£ÈáäÊÄßÔºåÊØè‰∏™ËäÇÁÇπÂú®Âª∫Ê®°Êó∂Ë¢´Ëµã‰∫àÊÑè‰πâÔºåÊØîÂ¶ÇLDAÂíåHMM\nÁ•ûÁªèÁΩëÁªúÔºöÊ∑±Â±ÇÁöÑÔºåÁ®†ÂØÜÁöÑÔºàÊó†Êù°‰ª∂Áã¨Á´ãÊÄßÂÅáËÆæÔºâÔºåËäÇÁÇπ‰ªÖÁî®‰∫éËÆ°ÁÆóœÉ(‚àëw·µ¢x·µ¢)ÔºåÊ≤°Êúâ‰ªª‰ΩïÊ¶ÇÁéáÊÑè‰πâ/Áâ©ÁêÜÊÑè‰πâ„ÄÇÂÆÉÁöÑËß£ÈáäÊÄßÊú™Áü•‰πü‰∏çÈáçË¶ÅÔºåÁ•ûÁªèÁΩëÁªúÊØèÂ±ÇÁöÑÊÑè‰πâÂú®Âª∫Ê®°Êó∂Âπ∂Êú™Ëµã‰∫à„ÄÇ\nÊé®Êñ≠\nË¥ùÂè∂ÊñØÁΩëÁªúÔºöÁ≤æÁ°ÆÊé®Êñ≠ÔºåËøë‰ººÊé®Êñ≠ÔºåMCÈááÊ†∑Êé®Êñ≠ÔºåÂèòÂàÜÊé®Êñ≠Ôºå‰º∞ËÆ°ÂêéÈ™åÂàÜÂ∏É Á•ûÁªèÁΩëÁªúÔºöÂÆÉÁöÑÊé®Êñ≠ÂæàÂÆπÊòìÔºàÂâçÂêëÔºâ‰ΩÜÊ≤°ÊúâÊÑè‰πâÔºåÂèÇÊï∞ÁöÑÂàÜÂ∏É‰∏çÈáçË¶ÅÔºåÂè™ÂÖ≥Ê≥®ËæìÂá∫ Â≠¶‰π†\nË¥ùÂè∂ÊñØÁΩëÁªúÔºöLikelihood-based: EM Á•ûÁªèÁΩëÁªúÔºöÊ¢ØÂ∫¶‰∏ãÈôçÔºàÂèçÂêë‰º†Êí≠Ôºö‰∏ÄÁßçÈ´òÊïàÁöÑÊ±ÇÂØºÊñπÊ≥ïÔºåÂ∞±ÊòØÈìæÂºèÊ±ÇÂØºÊ≥ïÂàô+Âä®ÊÄÅËßÑÂàíÔºàÈÄíÂΩí+CacheÔºâÔºâ Áî®Ê≥ï\nÊ¶ÇÁéáÂõæÊèèËø∞‰∫ÜÊ®°ÂûãÔºåÈÄÇÂêàÈ´òÁ∫ßÂà´ÁöÑÊé®ÁêÜ‰ªªÂä° high-level reasoning Á•ûÁªèÁΩëÁªúÔºàÁöÑËÆ°ÁÆóÂõæÔºâÂè™Áî®Êù•ËÆ°ÁÆóÔºåÈÄÇÂêà‰ΩéÁ∫ßÂà´ÁÆÄÂçïÁöÑÊé®ÁêÜ low-level reasoningÔºöÂº±Êé®ÁêÜÔºåÂè™ÊòØÂàÜÁ±ªÂõæÂÉèÔºåËÄåÊ≤°ÊúâÂÉè‰∫∫Á±ª‰∏ÄÊ†∑ÁêÜËß£ÔºõËøòÈÄÇÂêàË°®Á§∫Â≠¶‰π†ÔºöÂ£∞Èü≥„ÄÅÂõæÂÉèËØÜÂà´ÔºàÁé∞Âú®ÁöÑËØ≠Ë®ÄÊ®°ÂûãÊòØ‰∏§‰∏™ÁöÑÁªºÂêàÔºâ 6. ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßÔºàÈöèÊú∫ÂêéÂêë‰º†Êí≠Ôºâ P6\nÊúÄÂü∫Á°ÄÁöÑÁ•ûÁªèÁΩëÁªúÂ∞±ÊòØ‰∏Ä‰∏™ÂáΩÊï∞ÈÄºËøëÂô®„ÄÇÁî®Ê†∑Êú¨ (X,Y) ÂéªÈÄºËøëÂáΩÊï∞ y=f(x;Œ∏)„ÄÇÂü∫‰∫é y ÊûÑÂª∫ÁõÆÊ†áÂáΩÊï∞ÔºåÁî®BPÊ±ÇÁ•ûÁªèÁΩëÁªúÁöÑÊùÉÈáçÂíåÂÅèÁΩÆÁöÑÊ¢ØÂ∫¶ÔºåÈÄöËøáÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç‰øÆÊ≠£\nÁî®Á•ûÁªèÁΩëÁªúÈÄºËøëÊ¶ÇÁéáÂàÜÂ∏ÉÔºàÊ¶ÇÁéáÂõæÔºâÔºåÁªìÂêàÂà∞‰∏ÄËµ∑Â∞±Âè´ÈöèÊú∫ÂêéÂêë‰º†Êí≠ Stochastic Backpropagation ÊàñËÄÖÂè´ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ß Reparameterization Trick\nÂÅáËÆæ y ÊòØ‰∏Ä‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉÔºåÂÆÉÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊòØ P(y)„ÄÇ\nÂÅáÂÆö P(y) = N(Œº,œÉ¬≤)ÔºåÂΩìÂØπÂÆÉÈááÊ†∑Êó∂ÔºåÂÖàÂØπ‰∏≠Èó¥ÂèòÈáè z ÈááÊ†∑ÔºåÂÜçÁî± z ÂæóÂà∞ yÔºåÂÖ∂‰∏≠ z Êúç‰ªéÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏É zÔΩûN(0,1)ÔºåÈÇ£‰πà y ‰∏é z ÁöÑÂÖ≥Á≥ªÂ∞±ÊòØ y = Œº+œÉ‚ãÖz„ÄÇ\nÂõ†‰∏∫ÂØπÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏ÉÊòØÂæàÂÆπÊòìÈááÊ†∑ÁöÑÔºåÂÖàÈáá‰∏Ä‰∏™ z‚ÅΩ·∂¶‚ÅæÔΩûN(0,1)ÔºåÈÇ£‰πà y‚ÅΩ·∂¶‚Åæ=Œº+œÉ‚ãÖz‚ÅΩ·∂¶‚Åæ\nÁªôÂÆö zÔºåÂàô y ‰πüÂõ∫ÂÆöÔºåÊää Œº,œÉ Áúã‰ΩúÊòØÊú™Áü•‰ΩÜÁ°ÆÂÆöÁöÑÂèÇÊï∞ÔºåÊâÄ‰ª• y ‰∏é z ‰πãÈó¥Â∞±ÊòØ‰∏Ä‰∏™Á∫øÊÄßÂèòÊç¢„ÄÇ\nÂèØ‰ª•Â∞Ü y Áúã‰Ωú‰∏Ä‰∏™ÂáΩÊï∞ y = f(Œº,œÉ,z)ÔºåÂÖ∂‰∏≠ z ÊòØÈöèÊú∫ÂèòÈáèÔºåÈô§ÂÆÉ‰πãÂ§ñÈÉΩÊòØÁ°ÆÂÆöÊÄßÂèòÊç¢ÔºåÂèØ‰ª•Áî®Á•ûÁªèÁΩëÁªúÂéªÈÄºËøëËøô‰∏™Á∫øÊÄßÂáΩÊï∞\nz \u0026ndash;\u0026gt; NN \u0026ndash;\u0026gt; yÔºåNN ÈÄºËøë fÔºõÂõ†‰∏∫ z ÊòØ‰∏™ÈöèÊú∫ÂèòÈáèÔºåÊâÄ‰ª• y ‰πüÊòØ‰∏™ÈöèÊú∫ÂèòÈáè\n‰ª•‰∏äÂÅáËÆæ‰∫Ü P(y) ÊòØÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÊâÄ‰ª• y ‰∏é z ‰πãÈó¥ÊòØÁ∫øÊÄßÂÖ≥Á≥ªÔºåÊâÄ‰ª•Á•ûÁªèÁΩëÁªúÁöÑÂèÇÊï∞ÊòØ Œº, œÉÔºå‰ª§ Œ∏={Œº,œÉ¬≤}„ÄÇ\nÂèØ‰ª•ÊûÑÈÄ†ÂÖ≥‰∫é y ÁöÑÁõÆÊ†áÂáΩÊï∞: J(y) ÔºåÂõ†‰∏∫ y ÊòØÂÖ≥‰∫éŒº, œÉ ÁöÑÂáΩÊï∞ÔºåÊâÄ‰ª•Ê±ÇÊ¢ØÂ∫¶‚àá_Œ∏ J(y)Êó∂Ôºö\n‚àÇJ(y)/‚àÇŒ∏ = ‚àÇJ(y) / ‚àÇy ‚ãÖ ‚àÇy/‚àÇŒ∏\nÂ¶ÇÊûú ÁõÆÊ†áÂèòÈáè ÊòØ‰∏™Êù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏É: P(y|x) = N(x; Œº,œÉ¬≤)ÔºåÁÑ∂Âêé z ËøòÊòØÊúç‰ªé‰∏Ä‰∏™Ê†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏É zÔΩûN(0,1)ÔºåÈÇ£‰πà y ‰∏é z ÁöÑÂÖ≥Á≥ªÊòØÔºöy = Œº(x) + œÉ(x)‚ãÖz ÂÖ∂‰∏≠ x ÊòØËæìÂÖ•ÔºåÊâÄ‰ª• Œº,œÉ ÈÉΩÊòØ x ÁöÑÂáΩÊï∞„ÄÇ\n‰ªçÁÑ∂ÂèØÁî®Á•ûÁªèÁΩëÁªúÂéªÈÄºËøë y ‰∏é z ‰πãÈó¥ÁöÑÂáΩÊï∞ÔºåÁ•ûÁªèÁΩëÁªúÁöÑÂèÇÊï∞ÊòØŒ∏Ôºö\n1 2 3 x ‚Äî‚Äî\u0026gt; NN ‚Äî‚Äî\u0026gt; y ‚ñ≤ z ‚Äî‚Äî‚Äî‚Äî‚îò Êõ¥ÂèòÈáè‰πãÈó¥ÁöÑÂÖ≥Á≥ªÁîªÂæóÊõ¥ËØ¶ÁªÜÔºö Œº, œÉ ÈÉΩ‰ªéÁ•ûÁªèÁΩëÁªú‰∏≠Âá∫Êù•ÔºåÂÆÉ‰ª¨Â∞±ÊòØ Œ∏ ÁöÑÂáΩÊï∞ÔºåÂàô y = Œº_Œ∏(x) + œÉ_Œ∏(x)‚ãÖz\n1 2 3 4 x ‚îÄ\u0026gt; NN-Œ∏ ‚îÄ‚îÄ\u0026gt; Œº_Œ∏(x) ‚îÄ‚îÄ‚îÄ‚îÄ + ‚îÄ‚îÄ\u0026gt; y ‚îî‚îÄ‚îÄ‚îÄ\u0026gt; œÉ_Œ∏(x) ‚îÄ‚îê ‚ñ≤ ‚ñº ‚îÇ z ‚îÄ‚îÄ\u0026gt; √ó ‚îÄ‚îÄ‚îò ‰πüÂèØ‰ª•Áî®‰∏§‰∏™NN ÂàÜÂà´ÈÄºËøë Œº, œÉ : Œº(x) = f(x;Œ∏) œÉ(x) = g(x;Œ∏)\nÁÑ∂ÂêéÊûÑÈÄ†ÂÖ≥‰∫é y ÁöÑÁõÆÊ†áÂáΩÊï∞ J_Œ∏(y) = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ ||y-y‚Å±||¬≤ÔºåÁÑ∂ÂêéÂØπ Œ∏ Ê±ÇÊ¢ØÂ∫¶Ôºö\n‚àÇJ(y)/‚àÇŒ∏ = ‚àÇJ(y) / ‚àÇy ‚ãÖ ‚àÇy/‚àÇŒº ‚ãÖ ‚àÇŒº/‚àÇŒ∏ + ‚àÇJ(y) / ‚àÇy ‚ãÖ ‚àÇy/‚àÇœÉ ‚ãÖ ‚àÇœÉ/‚àÇŒ∏\nÊâÄ‰ª•Êó†ËÆ∫ÊÉ≥Ê±Ç‰∏Ä‰∏™ÊôÆÈÄöÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåËøòÊòØË¶ÅÊ±Ç‰∏Ä‰∏™Êù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÔºåÈÉΩÂèØ‰ª•Áî®Á•ûÁªèÁΩëÁªúÈÄºËøëÈÇ£‰∏™Ê¶ÇÁéáÂàÜÂ∏É\nÂú®Êú¨ËäÇÁöÑ‰æãÂ≠ê‰∏≠ÔºåÈÉΩÊòØÂÅáËÆæ P(y) ÊòØÈ´òÊñØÂàÜÂ∏ÉÔºåÂç≥ÂÆÉÊòØËøûÁª≠ÁöÑÔºåÂèØÂæÆÁöÑÔºåËÄå‰∏îË¶ÅÊ±Ç y Êú¨Ë∫´ÊòØ‰∏Ä‰∏™ËøûÁª≠ÁöÑÈöèÊú∫ÂèòÈáèÔºåÂõ†‰∏∫ÈúÄË¶Å y ÂØπ Œº,œÉ Ê±ÇÂÅèÂØº„ÄÇ Â¶ÇÊûú y ÊòØÁ¶ªÊï£ÈöèÊú∫ÂèòÈáèÔºåÂ∞±‰∏çËÉΩÁî®Ëøô‰∏™ÊñπÊ≥ï„ÄÇ\nÊúÄÂêéÔºåÂèØ‰ª•Êää‰∏äÈù¢‰∏§ÁßçÊÉÖÂÜµÔºåÂêàÂπ∂Ëµ∑Êù•ÊèèËø∞Ôºö P(y|w)ÔºåÂ¶ÇÊûúÂè™Ê±Ç P(y)ÔºåÂàôw Â∞±ÊòØÂèÇÊï∞ Œ∏ÔºõÂ¶ÇÊûúË¶ÅÊ±Ç P(y|x)ÔºåÈÇ£ w Â∞±‰ª£Ë°® x Âíå Œ∏Ôºåw={x;Œ∏}Ôºåx Êó†ÊâÄË∞ìÔºåÂÆÉÊòØÊù°‰ª∂Ê¶ÇÁéá‰∏≠ÁöÑÊù°‰ª∂ÔºåÊòØËæìÂÖ•„ÄÇ\nÁ•ûÁªèÁΩëÁªúÁöÑÂèÇÊï∞ wÔºåÁî®Á•ûÁªèÁΩëÁªúÂéªÈÄºËøëÊ¶ÇÁéáÂàÜÂ∏É P(y|w)\n","date":"2022-12-25T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/30-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/","title":"watch: ML - ÁôΩÊùø 30 | Review of Generative models"},{"content":"1 ËÉåÊôØ P1\nÈ¢ëÁéáËßíÂ∫¶Ôºö Ëß£‰∏Ä‰∏™‰ºòÂåñÈóÆÈ¢òÔºåÊØîÂ¶ÇÔºö\nÁ∫øÊÄßÂõûÂΩíÊ®°ÂûãÔºåÊï∞ÊçÆÊãüÂêàÔºåÁî®Êï∞ÊçÆ‰º∞ËÆ°Áõ¥Á∫øÁöÑWÔºöf(W) = W·µÄX: Á≠ñÁï•(loss func): ÊúÄÂ∞èÂåñÊâÄÊúâÊ†∑Êú¨ÁÇπÁöÑËØØÂ∑Æ‰πãÂíå(ÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°ÔºåÊó†Á∫¶ÊùüÁöÑ‰ºòÂåñÈóÆÈ¢ò): L(w) = Œ£·µ¢·¥∫(w·µÄx·µ¢-y·µ¢)¬≤ÔºåDataset: {(x·µ¢,y·µ¢)}·µ¢·¥∫Ôºåx·µ¢‚àà ‚Ñù·µñÔºåy·µ¢‚àà ‚Ñù„ÄÇ ÊúÄ‰ºò w^ = argmin L(w)„ÄÇ\nËß£Ê≥ïÔºö\nËß£ÊûêËß£ÔºöÊçüÂ§±ÂáΩÊï∞ÂØπ w Ê±ÇÂØº=0ÔºåÂàôW^ = (X·µÄX)‚Åª¬πX·µÄY, where X is train set Êï∞ÂÄºËß£ÔºöÔºàÈöèÊú∫ÔºâÊ¢ØÂ∫¶‰∏ãÈôçÔºå SVM Ê®°ÂûãÔºåÂàÜÁ±ªÈóÆÈ¢òÔºå‰º∞ËÆ°Á¨¶Âè∑ÂáΩÊï∞Ôºöf(w) = sign(w·µÄx+b)Ôºõ\nÁ≠ñÁï•(loss func): Á±ªÈó¥Â§ßÔºåÁ±ªÂÜÖÂ∞èÔºàÊúâÁ∫¶ÊùüÁöÑÂá∏‰ºòÂåñÈóÆÈ¢òÔºâÔºö L(w) = min 1/2 w·µÄw, s.t. y·µ¢(w·µÄx·µ¢+b)‚â•1, i=1,\u0026hellip;,N\nËß£Ê≥ïÔºö\nË∞ÉÁî® QP Â•ó‰ª∂ ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ EM Ê®°ÂûãÔºåËø≠‰ª£‰ºòÂåñÊ®°ÂûãÂèÇÊï∞Œ∏Ôºå‰Ωø‰ººÁÑ∂ÂÄºÂèñÂæóÊúÄÂ§ßÔºöŒ∏^ = argmax log P(X|Œ∏)\nÁ≠ñÁï•ÔºöËø≠‰ª£ÂÖ¨ÂºèÔºö $Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = argmax_Œ∏ ‚à´_Z log P(X,Z|Œ∏) P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) dZ$ ÔºàËæìÂÖ•Êï∞ÊçÆÂíåÈöêÂèòÈáèÁöÑÂÆåÂÖ®Êï∞ÊçÆÂàÜÂ∏ÉÊåâÁÖßZÁöÑÂêéÈ™åÂàÜÂ∏ÉÊ±ÇÊúüÊúõÔºàÂä†ÊùÉÂíåÔºåÊ±ÇÁßØÂàÜÔºâÔºâ Ë¥ùÂè∂ÊñØËßíÂ∫¶Ôºö Ëß£‰∏Ä‰∏™ÁßØÂàÜÈóÆÈ¢ò\nË¥ùÂè∂ÊñØÂÆöÁêÜÔºöP(Œ∏|X) = P(X|Œ∏) P(Œ∏) / P(X)ÔºåÂêéÈ™åÂàÜÂ∏É = ‰ººÁÑ∂ÂÄº‚ãÖÂÖàÈ™åÂàÜÂ∏É/ÂèÇÊï∞Á©∫Èó¥ÁöÑÁßØÂàÜ‚à´P(X|Œ∏)P(Œ∏)dŒ∏\nË¥ùÂè∂ÊñØÊé®Êñ≠InferenceÔºö‰æùÊçÆË¥ùÂè∂ÊñØÂÆöÁêÜÊ±ÇÂêéÈ™åÂàÜÂ∏ÉP(Œ∏|X)„ÄÇÔºàÁÑ∂ÂêéÂèØÊ±ÇÂàÜÂ∏ÉÁöÑÊúüÊúõÔºåÊñπÂ∑ÆÔºâ\nË¥ùÂè∂ÊñØÂÜ≥Á≠ñDecisionÔºöÂÄüÂä©ÂêéÈ™åÂàÜÂ∏ÉP(Œ∏|X)ÔºåÁî®Â∑≤ÊúâÊ†∑Êú¨ X È¢ÑÊµãÊñ∞Ê†∑Êú¨ x^ ÂèëÁîüÁöÑÊ¶ÇÁéáÔºö $P(\\^x|X) = ‚à´ P(\\^x,Œ∏|X) dŒ∏ = ‚à´_Œ∏ P(\\^x|Œ∏,X) P(Œ∏|X) dŒ∏ = E_{Œ∏|X} [P(\\^x|Œ∏)]$ÔºåÂç≥ÂØπ‰ººÁÑ∂P(x^|Œ∏)ÊåâÁÖßŒ∏ÁöÑÂêéÈ™åÂàÜÂ∏ÉÊ±ÇÊúüÊúõ\nÂ¶Ç‰ΩïÊ±ÇÂêéÈ™åÂàÜÂ∏ÉP(Œ∏|X)Ôºö\nÁ≤æÁ°ÆÊé®Êñ≠ÔºåÈóÆÈ¢òÁÆÄÂçïÔºàÂèÇÊï∞Á©∫Èó¥/ÈöêÂèòÈáèÁöÑÁª¥Â∫¶‰∏çÈ´òÔºâÂèØ‰ª•Áõ¥Êé•ËÆ°ÁÆóÂá∫ÂàÜÊØçÁöÑÁßØÂàÜ\nËøë‰ººÊé®Êñ≠ÔºåÂèÇÊï∞Á©∫Èó¥Áª¥Â∫¶È´òÔºåÊó†Ê≥ïÁõ¥Êé•Ê±ÇÂá∫ÂàÜÊØçÁßØÂàÜ\nÁ°ÆÂÆöÊÄßËøë‰ºº\nÂèòÂàÜÊé®Êñ≠ ÈöèÊú∫Ëøë‰ºº\nMCMC È©¨Â∞îÁßëÂ§´ÈìæËíôÁâπÂç°Ê¥õÊñπÊ≥ï MH, Metropolis-Hastings Gibbs, ÂêâÂ∏ÉÊñØÈááÊ†∑ 2 ÂÖ¨ÂºèÊé®ÂØº P2\nCreate on 2022-12-16\nÂèòÂàÜÊé®Êñ≠ÁöÑÁõÆÁöÑÔºöÊâæÂà∞‰∏Ä‰∏™ÂàÜÂ∏É q(Z) ÂéªÈÄºËøëÊó†Ê≥ïÂæóÂà∞Ëß£ÊûêËß£(intractable)ÁöÑÂêéÈ™åÂàÜÂ∏ÉP(Z|X,Œ∏)\nz: latent variable z + parameters Œ∏ÔºåÊääÂèÇÊï∞‰πüÁúã‰ΩúÈöèÊú∫ÂèòÈáè X: observed data Ê†∑Êú¨Êï∞ÊçÆ Â§ßZ: Ê†∑Êú¨ÁöÑÈöêÂèòÈáè (X,Z): Complete data P(X,Z) = P(Z|X)P(X)ÔºåËÅîÂêàÊ¶ÇÁéá P(X) = P(X,Z)/P(Z|X)Ôºå‚Äú‰ººÁÑ∂‚Äù ÁúÅÁï•‰∫ÜŒ∏ÔºåÈáçÁÇπ‰∏çÂú®Œ∏ ‰ººÁÑ∂ÂèñÂØπÊï∞Ôºå‰∏§Ê¶ÇÁéáÁõ∏Èô§ÂèòÊàêÁõ∏ÂáèÔºàÁ±ª‰ººÊé®ÂØº‰πüÂá∫Áé∞Âú®EMÔºå‰∏çËøá‰∏ãÈù¢ÁúÅÁï•‰∫ÜŒ∏ÔºåÂõ†‰∏∫ÂåÖÂê´Âú®‰∫ÜZ‰∏≠ÔºâÔºö\nlog P(X) = log P(X,Z) - log P(Z|X)\nÂºïÂÖ• Z ÁöÑÂàÜÂ∏É q(Z)Ôºö\nlog P(X) = log (P(X,Z)/q(Z)) - log (P(Z|X)/q(Z))\n‰∏§ËæπÂêåÊó∂ÊåâÁÖßÂ§ß Z ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ q(Z) Ê±Ç‰ººÁÑ∂ÁöÑÊúüÊúõ:\n$$ ‚à´_Z q(Z)‚ãÖlogP(X) dZ = \\\\ ‚à´_Z q(Z)‚ãÖlog (\\frac{P(X,Z)}{q(Z)}) dZ \\\\ - ‚à´_Z q(Z)‚ãÖlog (\\frac{P(Z|X)}{q(Z)}) dZ \\\\ log P(X) = ELBO + KL(q(Z)||P(Z|X)) $$\nÂõ†‰∏∫ logP(X) ‰∏é Z Êó†ÂÖ≥ÔºåÊâÄ‰ª•Á≠âÂè∑Â∑¶ËæπÊ≤°ÂèòÔºåËÄåÁ≠âÂè∑Âè≥ËæπÂèòÊàê‰∫Ü‰∏ãÁïå ELBO + KL(ZÁöÑÂàÜÂ∏É||ZÁöÑÂêéÈ™å)„ÄÇ ELBO ÊòØ q(Z) ÁöÑÂáΩÊï∞ÔºåËÆ∞‰∏∫ L(q(Z))ÔºåÊòØq(Z)ÁöÑ‰∏Ä‰∏™ÂèòÂàÜ„ÄÇ\nÂΩìXÂõ∫ÂÆöÔºålogP(X) ÊòØÂõ∫ÂÆöÁöÑÔºåÂèàÂõ†‰∏∫ KL Êï£Â∫¶‚â•0ÔºåÊâÄ‰ª• L(q(Z)) ÊúÄÂ§ß‰∏∫ logP(X)„ÄÇ Â∏åÊúõ KL Êï£Â∫¶ÊúÄÂ∞èÔºåËÆ© q(Z) ‰∏é P(Z|X) ÊúÄÊé•ËøëÔºå‰πüÂ∞±ÊòØËÆ© L(q(Z) ÊúÄÂ§ß\nÊï∞Â≠¶Ë°®ËææÔºö $\\rm \\^q(Z) = arg max_{q(Z)} L(q(Z)) = arg min KL(q(Z)||P(Z|X))$\nÊ±ÇËß£Ôºö\nz ÂåÖÂê´ÈöêÂèòÈáèÂíåÂèÇÊï∞ÔºåÊâÄ‰ª• q(z) ÊòØ‰∏Ä‰∏™ÂæàÂ§ßÁöÑËÅîÂêàÊ¶ÇÁéáÔºåÂÅáËÆæ q(z) ÂèØ‰ª•ÂàíÂàÜÊàê M ‰∏™Áõ∏‰∫íÁã¨Á´ãÁöÑÁªÑÔºàÁªüËÆ°Áâ©ÁêÜ‰∏≠ÁöÑÂπ≥ÂùáÂú∫ÁêÜËÆ∫ÔºâÔºö q(z) = ‚àè·µ¢‚Çå‚ÇÅ·¥π q·µ¢(z·µ¢)\nÊØèÊ¨°Âè™Ê±Ç 1 ÁªÑ q‚±ºÔºåÂêåÊó∂Âõ∫ÂÆöÂÖ∂‰ΩôÁöÑÁªÑ{1,2,\u0026hellip;,j-1,j+1,\u0026hellip;M}ÔºåÈÄê‰∏™Ê±ÇÂÆåÂêéÔºåÊääMÁªÑÁöÑ q Ëøû‰πòËµ∑Êù•Â∞±ÊòØÊï¥‰ΩìÁöÑ q(z)\nL(q(Z)) = ‚à´z q(Z)‚ãÖlog P(X,Z) dZ - ‚à´z q(Z)‚ãÖlog q(Z) dZ ÔºåÊää q(Z) ‰ª£ÂÖ• L = E1 + E2\nÂØπ‰∫éE1Ôºö ‚à´z q(Z)‚ãÖlog P(X,Z) dZ = ‚à´z log P(X,Z)‚ãÖ‚àè·µ¢‚Çå‚ÇÅ·¥π q·µ¢(Z·µ¢) dz‚ÇÅ,z‚ÇÇ,\u0026hellip;, z‚Çò\n. . . .\n3. ÂÜçÂõûÈ¶ñ P3\nÁ¨¶Âè∑‰øÆÊ≠£\n. . . .\n4. ÈöèÊú∫Ê¢ØÂ∫¶ÂèòÂàÜÊé®Êñ≠-SGVI-1 P4\nÂü∫‰∫éÂπ≥ÂùáÂú∫ÁêÜËÆ∫ÁöÑÂèòÂàÜÊé®Êñ≠Êó†Ê≥ïËß£ÂÜ≥Â§çÊùÇÈöêÂèòÈáè z ÁöÑÊÉÖÂÜµÔºåÊØîÂ¶Ç z ÊòØ‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÔºåÂπ≥ÂùáÂú∫Â∞±Â§±Êïà‰∫ÜÔºåz ÂèØ‰ª•ÊòØ‰ªªÊÑèÂ§çÊùÇÂ∫¶ÁöÑ„ÄÇ\nstateDiagram-v2 ÈöêÂèòÈáèz --\u003e ËßÇÊµãÂèòÈáèx: Generative model,\\n Êù°‰ª∂P(x|z),\\n Decoder ËßÇÊµãÂèòÈáèx --\u003e ÈöêÂèòÈáèz: Inference model,\\n ÂêéÈ™åP(z|x),\\n Encoder Âü∫‰∫éÂπ≥ÂùáÂú∫ÁêÜËÆ∫ÁöÑÂèòÂàÜÊé®Êñ≠Ôºàclassical VIÔºâ‰πüÊòØ‰∏ÄÁßçÂùêÊ†á‰∏äÂçáÊ≥ïCoordinate AscendÔºö‰∏ÄÊ¨°Ëø≠‰ª£ÈúÄË¶ÅÈÄê‰∏™Êõ¥Êñ∞ Z ÁöÑÊØè‰∏™Áª¥Â∫¶„ÄÇ ‰∏éÊ≠§Áõ∏ÂØπÁöÑÔºåÂèØ‰ª•Áî®ÔºàÈöèÊú∫ÔºâÊ¢ØÂ∫¶‰∏äÂçáÊ≥ïËß£ÂÜ≥ÊúÄÂ§ßÂåñÈóÆÈ¢òÔºåÂÅöÂèòÂàÜÊé®Êñ≠Â∞±Âè´ÂÅöSGVI / SGVB\nÂÖàÁúã‰∏ãÂÖ≥‰∫é q(z) ÁöÑÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶ËÉΩÂê¶Ê±ÇÂá∫Ôºü\nÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÂèÇÊï∞Êõ¥Êñ∞ÂÖ¨ÂºèÔºöŒ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = Œ∏‚ÅΩ·µó‚Åæ + Œª‚ÅΩ·µó‚Åæ‚ãÖ‚àáŒ∏‚ÅΩ·µó‚Åæ„ÄÇ ÂèòÂàÜÊé®Êñ≠‰ª•ÊúÄÂ∞èÂåñ‰∏§ÂàÜÂ∏ÉÔºàZ ÁöÑÂÅáËÆæÂàÜÂ∏É‰∏é Z ÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºâÁöÑKLÊï£Â∫¶ÔºåÊàñËÄÖÊúÄÂ§ßÂåñ‰∏ãÁïåELBOÔºà L(q(Z)) Ôºâ‰∏∫ÁõÆÊ†áÂáΩÊï∞Ôºö q^ = arg min_q KL( q(Z) || P(Z|X,Œ∏) ) = arg max_q L(q)Ôºå Ë¶ÅÊõ¥Êñ∞ qÔºåÂ∞±ÈúÄË¶ÅÊ±ÇÂá∫ L ÂØπ q ÁöÑÊ¢ØÂ∫¶Ôºö‚àÇL/‚àÇq„ÄÇ\nq(z) Êàñ q(z|x) ÊòØÈöêÂèòÈáè z ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåx ÊòØËßÇÊµãÂèòÈáèÔºåÂèØ‰ª•ÁÆÄÂåñÊéâ„ÄÇ ÂÅáËÆæ q(z) ÊòØÊåáÊï∞ÊóèÂàÜÂ∏ÉÔºåÂÆÉÂ∞±Êúâ‰∏Ä‰∏™ÂèÇÊï∞ÂΩ¢ÂºèÔºåÂõ†Ê≠§Ê±Ç q(z) Â∞±ÊòØÊ±ÇÂÆÉÁöÑÂèÇÊï∞„ÄÇ ÂÅáËÆæ z ÁöÑÊ¶ÇÁéáÂàÜÂ∏É q(z) ÊòØ‰ª• œï ‰∏∫ÂèÇÊï∞ÔºåÂ¶ÇÊûúËÉΩÊ±ÇÂá∫ÊúÄÂ•ΩÁöÑ œïÔºåÂ∞±Áõ∏ÂΩì‰∫éÂæóÂà∞‰∫Ü q(z)ÔºåÊâÄ‰ª•ÁõÆÊ†áÂèò‰∏∫ÂéªÊ±Ç œïÔºö\nL(œï) = ELBO = E_q·µ©(z) [ log (P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) / q·µ©(z)) ] = E_q·µ©(z) [ log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z) ] = ‚à´_z q·µ©(z) ‚ãÖ (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) dz - log q·µ©(z)) dz Ê†∑Êú¨‰ººÁÑ∂Ôºölog P(x‚ÅΩ‚Å±‚Åæ|Œ∏) = L(œï) + KL(q||P) ‚â• L(œï)\nÁõÆÊ†áÂáΩÊï∞Ôºöœï^ = arg max·µ© L(œï)\nÊääÊúüÊúõÂÜôÊàêÂØπÈöèÊú∫ÂèòÈáè z ÁöÑÁßØÂàÜ:\n‚àá·µ©L(œï) = ‚àá·µ©‚à´_z q·µ©(z) ‚ãÖ (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) dz - log q·µ©(z)) dz ÔºöÊ±ÇÂÅèÂØº ‚àá·µ© ‰∏éÁßØÂàÜ ‚à´_z ÂèØ‰∫§Êç¢ = ‚à´_z [‚àá·µ©q·µ©(z) ‚ãÖ (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z))] + [q·µ©(z) ‚ãÖ ‚àá·µ©(log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z))] dz ÔºöÂØπ‰∏§È°π‰πãÁßØÊ±ÇÂØº = ‚à´_z ‚ë† dz + ‚à´_z ‚ë° dz ÔºöÊãÜ‰∏∫‰∏§È°πÂàÜÊûê Âú®Á¨¨ ‚ë° È°π‰∏≠ÁöÑ log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) ‰∏é œï Êó†ÂÖ≥ÔºåÂØπÂÆÉÊ±ÇÂØº=0ÔºåÊâÄ‰ª• ‚ë° ÂØπÂ∫îÁöÑÁßØÂàÜ‰∏∫Ôºö\n‚à´z q·µ©(z) ‚ãÖ ‚àá·µ©(-log q·µ©(z)) dz = -‚à´z q·µ©(z) ‚ãÖ 1/q·µ©(z) ‚ãÖ ‚àá·µ©q·µ©(z) dz = -‚à´z ‚àá·µ©q·µ©(z) dz = -‚àá·µ©‚à´z q·µ©(z) dz = - ‚àá·µ©1 = 0 ÊâÄ‰ª•Á¨¨2È°πÂ∞±Á∫¶Âéª‰∫ÜÔºåLÂØπ œï Ê±ÇÊ¢ØÂ∫¶Â∞±Á≠â‰∫éÁ¨¨1È°πÔºö\n‚àá·µ©L(œï) = ‚à´z ‚àá·µ©q·µ©(z) ‚ãÖ (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z)) dz\nËøô‰∏™ÂºèÂ≠êÊ≤°ÂäûÊ≥ïÁõ¥Êé•ÂÜôÊàêÊúüÊúõÁöÑÂΩ¢ÂºèÔºåÂ¶ÇÊûúÂèØ‰ª•ÁöÑËØùÔºåÂ∞±ÂèØ‰ª•Âà©Áî®ËíôÁâπÂç°ÁΩóÈááÊ†∑ÔºåÊääÊú™Áü•ÂàÜÂ∏ÉÁöÑÊúüÊúõËøë‰ººÂá∫Êù•„ÄÇ\nÊäÄÂ∑ßÔºöÂà©Áî® ‚àá·µ©log q·µ©(z) = 1/q·µ©(z)‚ãÖ‚àá·µ©q·µ©(z) ÂÅöÁ≠â‰ª∑ÂèòÊç¢Ôºö‚àá·µ©q·µ©(z) = q·µ©(z) ‚ãÖ ‚àá·µ©log q·µ©(z)ÔºåÁÑ∂ÂêéÂ∞±ÂèØ‰ª•ÂÜôÊàê‰∏Ä‰∏™ÊúüÊúõÔºö\n‚àá·µ©L(œï) = ‚à´z q·µ©(z) ‚ãÖ ‚àá·µ©log q·µ©(z) ‚ãÖ (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z)) dz = E_q·µ©(z) [ ‚àá·µ©log q·µ©(z) ‚ãÖ (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z)) ]\nÊâÄ‰ª• L ÂØπ œï ÁöÑÊ¢ØÂ∫¶ ‚àá·µ©L(œï) Â∞±Á≠â‰∫éÈÇ£‰∏ÄÂù®ÂÖ≥‰∫éÈöèÊú∫ÂèòÈáè z ÁöÑÂáΩÊï∞ÊåâÁÖß q·µ©(z) Ê±ÇÊúüÊúõ„ÄÇÂèØ‰ª•Áî®ËíôÁâπÂç°ÁΩóÈááÊ†∑ÂØπÂÖ∂‰º∞ËÆ°Ôºö\nÂÅáÂÆöÁ¨¨ l ‰∏™Ê†∑Êú¨ z‚ÅΩÀ°‚Åæ~ q·µ©(z), l=1,2,..,LÔºå‰ªéÂàÜÂ∏É q·µ©(z) ‰∏≠ÈááÊ†∑ L ‰∏™Ê†∑Êú¨Ôºå Âàô L ‰∏™Ê†∑Êú¨ÁöÑÂπ≥ÂùáÂÄºÂ∞±ÊòØËøë‰ººÊúüÊúõÔºö ‚âà 1/L ‚ãÖ ‚àë‚Çó‚Çå‚ÇÅ·¥∏ [ ‚àá·µ©log q·µ©(z‚ÅΩÀ°‚Åæ) ‚ãÖ (log P(x‚ÅΩ‚Å±‚Åæ,z‚ÅΩÀ°‚Åæ|Œ∏) - log q·µ©(z‚ÅΩÀ°‚Åæ)) ]\nÊØè‰∏ÄÊ≠•ËÆ°ÁÆóËøô‰∏™ÊúüÊúõÂ∞±ÊòØÊ¢ØÂ∫¶ÔºåÁÑ∂ÂêéÂèØ‰ª•Áî®Ê¢ØÂ∫¶‰∏äÂçáÊ≥ï\n5. ÈöèÊú∫Ê¢ØÂ∫¶ÂèòÂàÜÊé®Êñ≠-SGVI-2 Source video: P5\n‰∏äÈù¢‰ΩøÁî® MC ÈááÊ†∑ÊòØÊúâÈóÆÈ¢òÁöÑÔºö\nÂÖàÂØπ z ÈááÊ†∑ÔºåÁÆóÂá∫ q·µ©(z)ÔºåÂΩì q·µ©(z) ÂæàÂ∞èÊó∂ÔºàÈù†Ëøë0ÔºâÔºåÂØπÂ∫îÁöÑ log ÂÄºÔºàlog q·µ©(z‚ÅΩÀ°‚Åæ)ÔºâÂèòÂåñÂâßÁÉàÔºåÊâÄ‰ª•Ê¢ØÂ∫¶ ‚àá·µ©log q·µ©(z‚ÅΩÀ°‚Åæ)Â∞±ÈùûÂ∏∏Â§ßÔºå ÈÄ†ÊàêÊï¥‰∏™ÁªüËÆ°ÈáèÔºàÊâÄÊ±ÇÊúüÊúõÁöÑÈáè:‚àá·µ©log q·µ©(Z) ‚ãÖ (log P(X‚ÅΩ‚Å±‚Åæ,Z|Œ∏) - log q·µ©(Z))ÔºâÁöÑÊñπÂ∑Æ‰ºöÈùûÂ∏∏Â§ßÔºå ÊÑèÂë≥ÁùÄÈúÄË¶ÅÊõ¥Â§öÁöÑÊ†∑Êú¨ÊâçËÉΩÂæóÂà∞ÊØîËæÉÂ•ΩÁöÑËøë‰ººÔºåÊàñËÄÖÂ¶ÇÊûúÊñπÂ∑ÆÈùûÂ∏∏Â§ßÔºåÂèØËÉΩÂ∞±Êó†Ê≥ïÈááÊ†∑ÔºåËøôÁßçÁõ¥Êé•Áî®MCÈááÊ†∑ÁöÑÊñπÊ≥ïÂ∞±Ë°å‰∏çÈÄö„ÄÇ\nËÄå‰∏îÂç≥‰æøÁî®ËíôÁâπÂç°ÁΩóÈááÊ†∑ÂæóÂà∞‰∫ÜËøë‰ººÁöÑÊúüÊúõÔºàÊñπÂ∑ÆËæÉÂ§ßÔºå‰∏çÂ§üÁ≤æÁ°ÆÔºâÔºåÂÆÉÁ≠â‰∫é‰∏ãÁïå L(œï) ÁöÑÊ¢ØÂ∫¶ÔºåÂÜçÁî®Ê¢ØÂ∫¶‰∏äÂçáÊ±ÇÂàÜÂ∏É q·µ©(Z) ÁöÑÂèÇÊï∞ œï^ÔºåËøôÊ†∑‰∏ÄÁéØÊâ£‰∏ÄÁéØÔºå‰∏çÁ≤æÁ°ÆÁöÑÊ¢ØÂ∫¶ÂÜçÂè†Âä†‰∏äÊ¢ØÂ∫¶‰∏äÂçáÊó∂ÂºïÂÖ•ÁöÑËØØÂ∑ÆÔºåËØØÂ∑ÆÔºàÊñπÂ∑ÆÔºâ‰ºöË∂äÊù•Ë∂äÂ§ßÔºåÊâÄ‰ª•Âú®ÂÆûÈôÖ‰∏≠‰∏çÂèØË°å„ÄÇ\nÂ¶Ç‰ΩïÈôç‰ΩéÁªüËÆ°ÈáèÁöÑÊñπÂ∑ÆÔºüVariance Reduction\nÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ß Reparameterization trick ÊúÄÂàùÔºå‰∏ãÁïå L(œï) ÁöÑÊ¢ØÂ∫¶ ‚àá·µ©L(œï) = ‚àá·µ©E_q·µ©(Z) [ log P(X‚ÅΩ‚Å±‚Åæ,Z|Œ∏) - log q·µ©(Z) ]Ôºå Âú®Ëøô‰∏™ÊúüÊúõ‰∏≠ÔºåË¢´ÁªüËÆ°ÈáèÔºà‰ººÁÑ∂-ZÂêéÈ™åÔºâÂíå‚ÄúÊùÉÈáç‚ÄùÔºàZÁöÑÂàÜÂ∏Éq·µ©(Z)ÔºâÈÉΩ‰∏é œï ÊúâÂÖ≥ÔºåÂè™ËÉΩÂÉè‰∏äÈù¢ÈÇ£Ê†∑ÂÖàÂ±ïÂºÄÔºåÊØîËæÉÂ§çÊùÇ„ÄÇ\nÂ¶ÇÊûúÂÅáÂÆö Z ÁöÑÂàÜÂ∏ÉÔºàÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Ôºâq·µ©(Z) ÊòØÂ∑≤ÁªèÁ°ÆÂÆöÁöÑÂàÜÂ∏É p(Œµ)Ôºå‰∏é œï Êó†ÂÖ≥ÔºåÁÑ∂ÂêéÊ¢ØÂ∫¶Âè∑Â∞±ÂèØ‰ª•Áõ¥Êé•ÂÜôÂà∞‰∏≠Êã¨Âè∑ÈáåÈù¢ÔºåÂÖàÂØπ‰ººÁÑ∂ÂÄºÊ±ÇÊ¢ØÂ∫¶ÔºåÂÜçÊåâËøô‰∏™Á°ÆÂÆöÂàÜÂ∏ÉÔºàÂ∏∏ÈáèÔºâp(Œµ)Ê±ÇÊúüÊúõÔºö E_p(Œµ) [ ‚àá·µ©(log P(X‚ÅΩ‚Å±‚Åæ,Z|Œ∏) - log q·µ©(Z)) ]\nÊç¢Âè•ËØùËØ¥Ôºåz ÊòØÂàÜÂ∏É q·µ©(Z|X) ‰∏≠ÁöÑÈááÊ†∑ zÔΩûq·µ©(Z|X)Ôºåz ÊòØ‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáèÔºåËÄÉËôëÊää z Âíå œï ‰πãÈó¥ÁöÑÂÖ≥Á≥ªËß£ËÄ¶Ôºå‰πüÂ∞±ÊòØÊää z ÁöÑÈöèÊú∫ÊàêÂàÜÂçïÊãéÂá∫Êù•„ÄÇ\nÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßÔºöÂÅáÂÆö z ‰∏é Œµ Âíå x ‰πãÈó¥ÊúâÂáΩÊï∞ÂÖ≥Á≥ªÔºöz = g·µ©(Œµ,x‚ÅΩ‚Å±‚Åæ)ÔºåËÄå‰∏î Œµ Êúç‰ªé‰∏Ä‰∏™ÁªôÂÆöÁöÑÔºàÁÆÄÂçïÁöÑÔºâÂàÜÂ∏É Œµ~P(Œµ)„ÄÇ ËøôÊ†∑ z ÊòØÂÖ≥‰∫éÈöèÊú∫ÂèòÈáè Œµ ÁöÑÂáΩÊï∞Ôºåz ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáèÔºå‰ΩÜÂÆÉÁöÑÈöèÊú∫ÊÄßËΩ¨ÁßªÂà∞‰∫Ü Œµ ‰∏äÔºàÂΩìŒµÂíåxÂÆö‰∫ÜÔºåzÂ∞±ÂÆö‰∫ÜÔºâ:\nZÔΩûq·µ©(Z|X) ‚Üì ‚ÄúÈöèÊú∫ÊÄßÈÄöËøáÂáΩÊï∞ÂÖ≥Á≥ª g ËΩ¨Áßª‚Äù ŒµÔΩûp(Œµ)\nÂõ†‰∏∫ ‚à´z q·µ©(z|x‚ÅΩ‚Å±‚Åæ)‚ãÖdz = 1Ôºå‚à´ p(Œµ)‚ãÖdŒµ =1ÔºåËÄå‰∏î z ÊòØ Œµ ÁöÑ‰∏Ä‰∏™ÔºàÁ∫øÊÄßÔºâÂèòÊç¢Ôºå Âõ†Ê≠§‚ÄúÂÆöÊÄßÂú∞‚ÄùËÆ§‰∏∫Ôºö|q·µ©(z|x‚ÅΩ‚Å±‚Åæ)‚ãÖdz| = |p(Œµ)‚ãÖdŒµ|\nÊää ‚àá·µ©L(œï) ‰∏≠ÁöÑ z ‰ª£Êç¢‰∏∫ÂèòÊç¢ g·µ©(Œµ,x‚ÅΩ‚Å±‚Åæ):\n‚àá·µ©L(œï) = ‚àá·µ©E_q·µ©(z) [ log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z) ] ÔºåÂÜôÊàêÂØπZÁßØÂàÜÁöÑÂΩ¢Âºè\n= ‚àá·µ© ‚à´z (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z)) ‚ãÖ q·µ©(z|x‚ÅΩ‚Å±‚Åæ)‚ãÖdz Ôºå‰ª£Êç¢ = ‚àá·µ© ‚à´z (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z)) ‚ãÖ p(Œµ)‚ãÖdŒµ ÔºåÂÜôÊàêÊúüÊúõ = ‚àá·µ© E_p(Œµ) [ log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z) ] Ôºåp(Œµ)‰∏éœïÊó†ÂÖ≥,‚àá·µ©ÂÜôÈáåÈù¢\n= E_p(Œµ) [‚àá·µ© (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z)) ] ÔºåÂÖàÂØπzÊ±Ç‚àáÔºåÂÜçÂØπœïÊ±Ç‚àá z ÊòØ œï ÁöÑÂáΩÊï∞: z=g·µ©(Œµ,x‚ÅΩ‚Å±‚Åæ) = E_p(Œµ) [‚àá_z (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z|x‚ÅΩ‚Å±‚Åæ)) ‚ãÖ‚àá·µ©z] ÔºåÈìæÂºèÊ≥ïÂàô = E_p(Œµ) [‚àá_z (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z|x‚ÅΩ‚Å±‚Åæ)) ‚ãÖ ‚àá·µ©g·µ©(Œµ,x‚ÅΩ‚Å±‚Åæ)]\nÂÖ∂‰∏≠ p(Œµ) ‰∏é œï Êó†ÂÖ≥Ôºå‰∏§‰∏™ log ÊòØ z ÁöÑÂáΩÊï∞ÔºàŒ∏ÊòØ‰∏ä‰∏ÄÊó∂ÂàªÁöÑÔºâÔºåzÊòØŒµÁöÑÂáΩÊï∞ÔºåÁÑ∂ÂêéÊòØ g ÂØπ Œµ ÁöÑÊ¢ØÂ∫¶„ÄÇ Ê≠§Êó∂ÂÜçÁî®ËíôÁâπÂç°ÁΩóÈááÊ†∑ÂØπ Œµ ÈááÊ†∑ÔºöŒµ‚ÅΩÀ°‚ÅæÔΩûp(Œµ), l=1,2,..,LÔºåÊää Œµ Â∏¶ÂÖ• ‰∏≠Êã¨Âè∑ÈáåÁöÑÈÇ£‰∏™ÂáΩÊï∞ÁÆóÂáΩÊï∞ÂÄºÔºåÂÜçÊ±ÇÂπ≥ÂùáÂÄºÂ∞±ÊòØËøë‰ººÁöÑÊúüÊúõÔºö\n‚àá·µ©L(œï) ‚âà 1/L ‚àë‚Çó‚Çå‚ÇÅ·¥∏ ‚àáz (log P(x‚ÅΩ‚Å±‚Åæ,z|Œ∏) - log q·µ©(z|x‚ÅΩ‚Å±‚Åæ)) ‚ãÖ ‚àá·µ©g·µ©(Œµ‚ÅΩÀ°‚Åæ,x‚ÅΩ‚Å±‚Åæ)]Ôºå ÂÖ∂‰∏≠ z=g·µ©(Œµ‚ÅΩÀ°‚Åæ,x‚ÅΩ‚Å±‚Åæ)\nÁÑ∂Â∞±ÂèØ‰ª•ÊääËøô‰∏™Ëøë‰ººÊ¢ØÂ∫¶Â∏¶ÂÖ•Ê¢ØÂ∫¶‰∏äÂçáÂÖ¨ÂºèÔºöœï‚ÅΩ·µó‚Å∫¬π‚Åæ = œï‚ÅΩ·µó‚Åæ + Œª‚ÅΩ·µó‚Åæ‚ãÖ‚àá·µ©L(œï)Ôºå ÊØè‰∏ÄÊ≠•Ê±Ç‰∏™Ëøë‰ººÊúüÊúõÔºåÂæóÂà∞Ê¢ØÂ∫¶ÔºåÂÜçÂÅöÊ¢ØÂ∫¶‰∏äÂçá\n","date":"2022-12-20T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/12-%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/","title":"watch: ML - ÁôΩÊùø 12 | Variational Inference"},{"content":"Gaussian Mixture Model È´òÊñØÊ∑∑ÂêàÊ®°Âûã\n1. Ê®°Âûã‰ªãÁªç Source video: P1\nÊ†πÊçÆ‰∏≠ÂøÉÊûÅÈôêÂÆöÁêÜÔºåÂÅáËÆæÊï∞ÊçÆÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÊòØÂêàÁêÜÁöÑ\n‰∏ÄÁª¥Êï∞ÊçÆ:\nÊ®™ËΩ¥ÊòØÊï∞ÊçÆÁÇπÔºåÊúç‰ªé‰∏§‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÂè†Âä†ÔºåÁ∫µËΩ¥ÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÂÄº(PDF)ÔºåÁÇπË∂äÂØÜ‰ª£Ë°®ËØ•Âå∫ÂüüÂá∫Áé∞Ê†∑Êú¨ÁöÑÊ¶ÇÁéáË∂äÂ§ß„ÄÇ\nÂèØ‰ª•ËÆ§‰∏∫Ëøô‰∫õÊ†∑Êú¨ÁÇπÊúç‰ªé‰∏Ä‰∏™È´òÊñØÂàÜÂ∏ÉÔºå‰ΩÜÂπ∂‰∏çÂêàÁêÜÔºåÁî®‰∏§‰∏™È´òÊñØÊØîËæÉÁ≤æÁ°ÆÔºåÁ∫¢Ëâ≤Êõ≤Á∫øÊòØ2‰∏™ÂàÜÂ∏ÉÁöÑÂè†Âä†ÔºàÊ∑∑ÂêàÔºâ„ÄÇ\n(1) ‰ªéÂá†‰ΩïËßíÂ∫¶Êù•Áúã GMM ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊòØÂ§ö‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÂä†ÊùÉÂπ≥ÂùáÔºåÂπ∂‰∏çÊòØÁõ¥Êé•Áõ∏Âä†ÔºåÂê¶ÂàôÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÁöÑÁßØÂàÜÂèØËÉΩÂ§ß‰∫é1„ÄÇÊùÉÈáçÊòØÂèñÂà∞ÂêÑÁßçÈ´òÊñØÂàÜÂ∏ÉÁöÑÊ¶ÇÁéá\nÊØè‰∏™È´òÊñØÊúâËá™Â∑±ÁöÑÂùáÂÄº Œº ÂíåÊñπÂ∑Æ Œ£ ÔºàÂ¶ÇÊûúÊòØÂ§öÁª¥ÁöÑÔºåÂ∞±ÊòØÂçèÊñπÂ∑ÆÁü©ÈòµÔºâÔºåÂÆÉ‰ª¨ÁöÑÂèÇÊï∞ÊòØË¶ÅÂ≠¶‰π†Âá∫Êù•ÁöÑ„ÄÇ\nGMM ‰∏≠‰∏Ä‰∏™Ê†∑Êú¨ x ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊòØK‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÂä†ÊùÉÂπ≥ÂùáÔºö p(x) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ Œ±‚Çñ N(x|Œº‚Çñ,Œ£‚Çñ), where ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ Œ±‚Çñ = 1ÔºåŒ± ÊòØÊùÉÈáç\n(2) ‰ªéÊ∑∑ÂêàÊ®°ÂûãÔºàÊàñÊ¶ÇÁéáÁîüÊàêÔºâËßíÂ∫¶Êù•Áúã GMM Áî®Á¶ªÊï£ÁöÑÈöèÊú∫ÂèòÈáè z ‰ª£Ë°®Ê†∑Êú¨ x ÂàÜÂà´Â±û‰∫é K ‰∏™È´òÊñØÂàÜÂ∏É Ê¶ÇÁéá„ÄÇN ‰∏™Ê†∑Êú¨ÊòØÈáçÂ§ç N Ê¨°ÁîüÊàêËøáÁ®ãÔºöÂÖà‰ªéK‰∏™È´òÊñØÂàÜÂ∏É‰∏≠ÈÄâ‰∏Ä‰∏™ÔºåÂÜçÂú®Ëøô‰∏™È´òÊñØÂàÜÂ∏É‰∏≠ÈááÊ†∑ÁîüÊàê‰∏Ä‰∏™Ê†∑Êú¨„ÄÇ\n‰∏§Áª¥Êï∞ÊçÆÁöÑ PDF ÊòØ3Áª¥ÁöÑÔºàÊõ≤Èù¢Ôºâ:\nÂ∞èx ÊòØËßÇÊµãÂèòÈáèobserved variableÔºå‰∏§Áª¥ÁöÑ: x=(x1,x2) ÂºïÂÖ•ÈöêÂèòÈáèlatent variable Â∞èzÔºå‰∏Ä‰∏™ z Ë°®Êòé‰∫Ü‰∏Ä‰∏™Ê†∑Êú¨ x ÂèØËÉΩÂ±û‰∫éÂì™‰∏Ä‰∏™È´òÊñØÂàÜÂ∏ÉÔºåÊâÄ‰ª• z ÊòØ‰∏Ä‰∏™Á¶ªÊï£ÂûãÁöÑÈöèÊú∫ÂèòÈáèÔºå ‰∏Ä‰∏™Ê†∑Êú¨ x Â±û‰∫éÂêÑ‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÂàÜÂà´‰∏∫Ôºö z C‚ÇÅ C‚ÇÇ \u0026hellip; C‚Çñ Ê¶ÇÁéáÂØÜÂ∫¶ p‚ÇÅ p‚ÇÇ \u0026hellip; p‚Çñ z ‰ª£Ë°®‰∏çÂêåÁöÑÁ±ªÂà´categoriesÔºà‰∏çÂêåÁöÑÈ´òÊñØÂàÜÂ∏ÉÔºâÔºåÂÖ∂‰∏≠Ê¶ÇÁéáÂØÜÂ∫¶Ê±ÇÂíåÁ≠â‰∫é1Ôºö‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ = 1\n‰∏Ä‰∏™Ê†∑Êú¨Êó¢ÂèØÂ±û‰∫éC1Ôºå‰πüÂèØÂ±û‰∫éC2\u0026hellip;, Âè™‰∏çËøáÊ¶ÇÁéá‰∏çÂêåÔºåÊâÄ‰ª•Áî®‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáèË°®Ëææ„ÄÇ Áî®ÈöêÂèòÈáè z ‰ª£Ë°®‰∏Ä‰∏™Ê†∑Êú¨ x ÊâÄÂ±ûÁöÑÈ´òÊñØÂàÜÂ∏ÉÊØîËæÉÊñπ‰æø: x ÔΩû zÔºåzÊòØ‰∏Ä‰∏™Á¶ªÊï£ÂàÜÂ∏ÉÔºåÂèØ‰ª•ÊòØ C‚ÇÅÔºå‰πüÂèØ‰ª•ÊòØ C‚ÇÇ,C‚ÇÉ,\u0026hellip;C‚Çñ,\nGMM ÊòØ‰∏Ä‰∏™ÁîüÊàêÊ®°ÂûãÔºàÊ∑∑ÂêàÊ®°Âûã‰∏ÄËà¨ÈÉΩÊòØÁîüÊàêÊ®°ÂûãÔºâÔºåÊØè‰∏™Ê†∑Êú¨ÈÉΩÊòØÊåâÁÖßÁîüÊàêËøáÁ®ãÈÄê‰∏ÄÁîüÊàêÁöÑÔºö ÂÅáËÆæÊúâ‰∏Ä‰∏™È™∞Â≠êÊúâ K ‰∏™Èù¢ÔºåËÄå‰∏îÈáçÈáèÂàÜÂ∏É‰∏çÂùáÂåÄÔºåÊâÄ‰ª•ÂêÑÈù¢ÁöÑÊ¶ÇÁéá‰∏çÂêåÔºàË∂äÈáçË∂äÂ§ßÔºâ\nÊé∑‰∏ÄÊ¨°ÂæóÂà∞‰∫ÜÁ¨¨ k Èù¢(Ê¶ÇÁéáÊòØp‚Çñ)ÔºåÂØπÂ∫î‰∫éÁ¨¨ k ‰∏™È´òÊñØÂàÜÂ∏É Âú®ËøôÁ¨¨ k ‰∏™È´òÊñØÂàÜÂ∏É‰∏≠ÂéªÈááÊ†∑ÔºåÂ∞±ÁîüÊàê‰∫Ü‰∏Ä‰∏™Ê†∑Êú¨ GMM ÊòØÊúÄÁÆÄÂçïÁöÑÁîüÊàêÊ®°ÂûãÔºåÂÆÉÁöÑÊ¶ÇÁéáÂõæÔºö\nN Ë°®Á§∫ÁîüÊàê N ‰∏™Ê†∑Êú¨ÔºåÂ∞è x ÊòØËßÇÊµãÂèòÈáèÔºåÁî®Èò¥ÂΩ±Êù•Ë°®Á§∫ÔºåÂ∞è z ÊòØÈöèÊú∫ÂèòÈáèÔºåÁî®Á©∫ÂøÉÁöÑÂúàË°®Á§∫ÔºåÂÆÉÊúç‰ªéÁöÑÂàÜÂ∏ÉÊòØ‰ª• p ‰∏∫ÂèÇÊï∞: p=(p1,p2,\u0026hellip;,p‚Çñ) ÊòØ‰∏Ä‰∏™ÂêëÈáèÔºåÊâÄ‰ª•Áî®ÂÆûÂøÉÁÇπË°®Á§∫ÂèÇÊï∞ pÔºõËÄå X Êúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºåÂÖ∂ÂèÇÊï∞‰∏∫ (Œº,Œ£)\n2. ÊûÅÂ§ß‰ººÁÑ∂ Source video: P2\n‰ªéÊ∑∑ÂêàÊ®°ÂûãËßíÂ∫¶ÔºåÂÜôÂá∫ GMM ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ p(x): Êó¢ÁÑ∂ÂºïÂÖ•‰∫ÜÈöêÂèòÈáè ùê≥Ôºå‰πüÊòØ‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáèÔºåÊääÂÆÉÁöÑÊ¶ÇÁéáÁßØÊéâÂ∞±Ë°å‰∫ÜÔºåÂèàÂõ†‰∏∫ ùê≥ ÊòØÁ¶ªÊï£ÂûãÁöÑÈöèÊú∫ÂèòÈáèÔºåÊâÄ‰ª•ÊòØÊääÂêÑ‰∏™ÂèØËÉΩÁöÑÂèñÂÄºÊ±ÇÂíå„ÄÇ\n‰∏Ä‰∏™Ê†∑Êú¨ x ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Ôºö\nP(x) = ‚àë_z P(x,z) = ‚àë_‚Çñ‚Çå‚ÇÅ·¥∑ P(x,z=C‚Çñ)ÔºåÂàÜÂà´‰ªé K ‰∏™È´òÊñØÂàÜÂ∏É‰∏≠ÈááÂá∫ x ÁöÑÊ¶ÇÁéá: P(x,z‚Çñ) Á¥ØÂä† = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ P(z=C‚Çñ) P(x|z=C‚Çñ)ÔºåËÅîÂêàÊ¶ÇÁéáÊãÜÊàê‰∏§È°πÁßØ = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ ‚ãÖ N(x | Œº‚Çñ,Œ£‚Çñ)ÔºåÁ¨¨k‰∏™È´òÊñØÂàÜÂ∏ÉÂá∫Áé∞ÁöÑÊ¶ÇÁéá ‰πò‰ª• Âú®Á¨¨k‰∏™È´òÊñØÂàÜÂ∏É‰∏≠ÈááÂá∫xÁöÑÊ¶ÇÁéá\nÊâÄ‰ª•Ê∑∑ÂêàÊ®°ÂûãÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏éÂä†ÊùÉÂπ≥ÂùáÊòØ‰∏ÄÊ†∑ÁöÑÔºåÂè™‰∏çËøá‚ÄúÊùÉÈáç‚Äù a‚Çñ ÂèòÊàê‰∫ÜËøôÈáåÁöÑÊ¶ÇÁéáÂÄº p‚Çñ\nÂÅö N Ê¨°ÈöèÊú∫ËØïÈ™åÔºåÊØèÊ¨°ÂÅöËØïÈ™åÊó∂(ÊâîÈ™∞Â≠ê)Ôºå(ÂÅáËÆæ)ÈöêÂèòÈáè ùê≥ ÊòØ‰∏çÂêåÁöÑÔºàÂç≥K‰∏™Ê≠£ÊÄÅÂàÜÂ∏ÉÂá∫Áé∞ÁöÑÊ¶ÇÁéá‰∏çÂêåÔºâÔºå ÊØèÊ¨°ÁîüÊàêÊ†∑Êú¨Êó∂Ôºå‰æùÊçÆ ùê≥ ‰ªé K ‰∏™È´òÊñØÂàÜÂ∏É‰∏≠ÈÄâ‰∏Ä‰∏™ÂàÜÂ∏ÉÔºåÂÜç‰ªéË¢´ÈÄâ‰∏≠ÁöÑÈ´òÊñØÂàÜÂ∏É‰∏≠Èáá‰∏Ä‰∏™ xÔºåÊâÄ‰ª•ÊØè‰∏™ x Âíå‰∏Ä‰∏™ ùê≥ ÂØπÂ∫î: (x·µ¢,ùê≥·µ¢)„ÄÇ Ë¶ÅÊ±Ç x·µ¢ ÁöÑ‰ººÁÑ∂ÔºåÂ∞±ÊääËøôÊ¨°ËØïÈ™å‰∏≠ÁöÑ ùê≥·µ¢ ‰ªéËÅîÂêàÊ¶ÇÁéáP(x·µ¢,ùê≥·µ¢)‰∏≠ÁßØÊéâ„ÄÇ\nN‰∏™Ê†∑Êú¨Â∞±ÊòØ N Ê¨°ËØïÈ™åÂêåÊó∂ÂèëÁîüÔºåÂèëÁîüÁöÑÊ¶ÇÁéáÔºà‰ººÁÑ∂ÔºâÂ∞±ÊòØÂ§öÊ¨°ËØïÈ™åÁöÑËøû‰πò P(X,Z)=p(x‚ÇÅ,z‚ÇÅ)p(x‚ÇÇ,z‚ÇÇ)\u0026hellip;p(xN,zN))ÔºåÊâÄ‰ª•Ê±Ç P(X)=‚à´z P(X,Z) dZ Â∞±ÊòØÂ§öÈáçÁßØÂàÜ: ÂÖàÂØπ z1 ÁßØÔºåÂÜçÂØπ z2 ÁßØÔºå\u0026hellip;.\nx observed variable, ËßÇÊµãÈöèÊú∫ÂèòÈáè z latent variable, ÈöêÂèòÈáèÔºåÊúç‰ªéÁ¶ªÊï£ÂàÜÂ∏É: K‰∏™Á±ªÂà´ÔºàÈ´òÊñØÂàÜÂ∏ÉÔºâC‚ÇÅ,C‚ÇÇ,\u0026hellip;,C‚Çñ ÂàÜÂà´ÂØπÂ∫îÊ¶ÇÁéá p‚ÇÅ,p‚ÇÇ,\u0026hellip;,pKÔºõ X observed data, N‰∏™ËßÇÊµãÊï∞ÊçÆ x1,x2,\u0026hellip;xN, Ê†∑Êú¨‰πãÈó¥Áõ∏‰∫íÁã¨Á´ã Z \u0026ldquo;latent data\u0026rdquo;, z1,z2,\u0026hellip;zN, N ‰∏™Ê†∑Êú¨ x ÂØπÂ∫îÁöÑÁîüÊàêÂÆÉÁöÑÈöêÂèòÈáè (X,Z): complete data, ‚ÄúÂÆåÊï¥Êï∞ÊçÆ‚Äù (x1,z1),(x2,z2),\u0026hellip;(xN,zN)ÔºåÊØè‰∏™Ê†∑Êú¨ x ÂØπÂ∫î‰∏Ä‰∏™ÈöêÂèòÈáè ‰∏∫‰∫ÜÂèôËø∞Êñπ‰æøÔºåÁî® Œ∏ ‰ª£Ë°®ÂèÇÊï∞ = {p‚ÇÅ,p‚ÇÇ,\u0026hellip;,pK, Œº‚ÇÅ,Œ£‚ÇÅ, Œº‚ÇÇ,Œ£‚ÇÇ,\u0026hellip;, ŒºK,Œ£K}ÔºåÈöêÂèòÈáè‰∏≠ K ‰∏™È´òÊñØÂàÜÂ∏ÉÂá∫Áé∞ÁöÑÊ¶ÇÁéá + K ‰∏™È´òÊñØÁöÑÂùáÂÄºÂíåÊñπÂ∑Æ Áî®ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÂèÇÊï∞ Œ∏: Œ∏^ = arg max_Œ∏ log P(X)\nGMM ÊòØÊ≤°ÊúâËß£ÊûêËß£ÁöÑÔºåÊâÄ‰ª•Áõ¥Êé•Áî® MLE ÊòØÂÅö‰∏çÂá∫Êù•ÁöÑÔºåÂè™ËÉΩÁî®Êï∞ÂÄºÊñπÊ≥ïÂæóÂà∞Ëøë‰ººËß£„ÄÇËÄå‰∏î GMM ÔºàÊ∑∑ÂêàÊ®°ÂûãÔºâ‰∏≠Âê´ÊúâÈöêÂèòÈáèÔºåÊâÄ‰ª•Áî® EM ÁÆóÊ≥ïÔºàËø≠‰ª£Ôºâ‰ºöÊõ¥ÊúâÊïàÁéá„ÄÇ\nŒ∏^ = arg max_Œ∏ log P(X) = arg max_Œ∏ log ‚àè·µ¢‚Çå‚ÇÅ·¥∫ P(x·µ¢) ÔºåN‰∏™Ê†∑Êú¨iid = arg max_Œ∏ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log P(x·µ¢) ÔºåËøû‰πòÂèòËøûÂä† = arg max_Œ∏ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log ( ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ ‚ãÖ N(x·µ¢ | Œº‚Çñ,Œ£‚Çñ) )ÔºåÂ∞ÜP(x·µ¢)‰ª£ÂÖ•\nË¶ÅÊ±ÇÊûÅÂ§ßÂÄºÂØπÂ∫îÁöÑÂèÇÊï∞ÔºåÂèØ‰ª•ÂØπ‰∏äÂºè‰∏≠ÁöÑÂèÇÊï∞Ôºàpk,Œº‚Çñ,Œ£‚ÇñÔºâÊ±ÇÂÅèÂØº=0Ôºå‰ΩÜÊòØÂõ†‰∏∫ log ÈáåÈù¢ÊòØÂ§öÈ°πËøûÂä†ÔºåËÄå‰∏îÈ´òÊñØÂàÜÂ∏ÉÂèØËÉΩÊòØÈ´òÁª¥ÁöÑÔºàË°®ËææÂºèÂæàÂ§çÊùÇÔºâÔºåÊó†Ê≥ïÊ±ÇÂá∫Ëß£ÊûêËß£\nÂØπ‰∫éÂçï‰∏™È´òÊñØÂàÜÂ∏ÉÔºåÂÆÉÁöÑŒº,œÉ¬≤ ÈÉΩÂèØ‰ª•Áõ¥Êé•Áî® MLE Ê±ÇÂá∫Êù•„ÄÇÂõ†‰∏∫Âçï‰∏Ä‰∏™Ê≠£ÊÄÅÂàÜÂ∏ÉÁöÑË°®ËææÂºèÁÆÄÂçïÔºålogÂèØ‰ª•Êää‰∏§È°π‰πãÁßØÊãÜÂºÄÔºå‰πüÂèØ‰ª•ÊääexpÂéªÊéâÔºåÁÆÄÂåñ‰πãÂêéÔºåÂØπ‰ººÁÑ∂Ê±ÇÂØºÔºåÂèØÂæóÂà∞Ëß£ÊûêËß£„ÄÇ\n3. EMÊ±ÇËß£-E-step Source video: P3\nÁî® EM ÁÆóÊ≥ïÈÄöËøá MLE Ê±ÇÂá∫ GMM ÁöÑÂèÇÊï∞ Œ∏„ÄÇ\nEM ÊòØË¶ÅÊúÄÂ§ßÂåñ‰ººÁÑ∂ÁöÑ‰∏ãÁïåÔºåÂç≥‰ººÁÑ∂ÁöÑÊúüÊúõÔºöŒ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = arg max_Œ∏ ‚à´_Z q(Z) ‚ãÖ log P(X,Z|Œ∏) dZÔºå Ëøô‰∏™ÁßØÂàÜÁöÑÊú¨ÊÑèÊòØÂØπÔºàÊØè‰∏™Ê†∑Êú¨ÁöÑÔºâ‰ººÁÑ∂ÂÄº log P(x,z|Œ∏) ÊåâÁÖßÈöêÂèòÈáè z ÁöÑÂàÜÂ∏É q(z) Ê±ÇÊúüÊúõ„ÄÇ Âõ†‰∏∫ z ÁöÑÂàÜÂ∏ÉÊòØÁ¶ªÊï£ÁöÑÔºåÂåÖÊã¨ K ‰∏™ÔºàÈ´òÊñØÂàÜÂ∏ÉÂá∫Áé∞ÁöÑÔºâÊ¶ÇÁéáÂÄºÔºåÊâÄ‰ª• logP(x,z|Œ∏) Âíå q(z) ‰πüÈÉΩÊòØÁ¶ªÊï£ÁöÑÔºå‰∫åËÄÖÁõ∏‰πòÁöÑÁßØÂàÜÂ∞±ÊòØÂä†ÊùÉÂíå ‚àë q(z)‚ãÖlog P(x,z|Œ∏)ÔºåÂ∞±ÊòØ‰∏Ä‰∏™Ê†∑Êú¨ÁöÑÊúüÊúõ„ÄÇ\nÂõ†‰∏∫Â§ß Z ÊòØÂ§ö‰∏™Â∞è z ÁöÑËÅîÂêàÔºåÊòØÂ§ö‰∏™‚Äù‰∫ã‰ª∂‚ÄúÂêåÊó∂ÂèëÁîüÁöÑËÅîÂêàÊ¶ÇÁéáÔºåÂèàÂõ†‰∏∫Ê†∑Êú¨‰πãÈó¥Áã¨Á´ãÂêåÂàÜÂ∏ÉÔºåÂ§ßZÁöÑÊ¶ÇÁéáÂ∞±ÊòØÂçï‰∏™ z ÁöÑÊ¶ÇÁéáËøû‰πòÔºöq(Z)=q(z‚ÇÅ)‚ãÖq(z‚ÇÇ)\u0026hellip;q(zN)Ôºå ÊâÄ‰ª•ÂØπÂ§ß Z ÁöÑÁßØÂàÜÂ∞±ÊòØÂ§öÈáçÁßØÂàÜÔºåÂç≥ÂØπÊØè‰∏™Â∞è z ÁßØÂàÜ: ‚à´z‚ÇÅ‚à´z‚ÇÇ\u0026hellip;‚à´zNÔºõ ËÄåÂè™ÂÅöÊúâÈôê N Ê¨°ËØïÈ™åÁöÑËØùÔºåÂ∞±ÊòØÊää N Ê¨°ÂÆûÈ™åÁöÑÂä†ÊùÉÂíåËøû‰πòËµ∑Êù•Ôºö\nE = ‚à´_Z q(Z) ‚ãÖ log P(X,Z|Œ∏) dZ ÔºåXÂíåZÈÉΩÊòØÂ§ßÂÜô, N‰∏™Ê†∑Êú¨ÈõÜÂêà. (|‰πüÂèØÂÜôÊàê;: Œ∏ÊòØÈöèÊú∫ÂèòÈáèvsÂõ∫ÂÆöÂÄº ¬π)\n= ‚à´_z‚ÇÅ ‚à´_z‚ÇÇ\u0026hellip;‚à´_zN q(Z) ‚ãÖ log P(X,Z|Œ∏) dz‚ÇÅ dz‚ÇÇ dzNÔºåÊääq(Z)ÂÜôÂºÄ\n= ‚àë_z‚ÇÅ ‚àë_z‚ÇÇ\u0026hellip;‚àë_z$_N$ ‚àè·µ¢‚Çå‚ÇÅ·¥∫ q(z·µ¢) ‚ãÖ log ‚àè·µ¢‚Çå‚ÇÅ·¥∫ P(x·µ¢,z·µ¢|Œ∏)Ôºåz·µ¢ÈÉΩÊòØÁ¶ªÊï£ÂàÜÂ∏É\n= ‚àë_z‚ÇÅ ‚àë_z‚ÇÇ\u0026hellip;‚àë_z$_N$ ‚àè·µ¢‚Çå‚ÇÅ·¥∫ P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ log ‚àè·µ¢‚Çå‚ÇÅ·¥∫ P(x·µ¢,z·µ¢|Œ∏) ÔºåÂΩì q(z·µ¢) Á≠â‰∫éZÁöÑÂêéÈ™åÂàÜÂ∏ÉÊó∂ÔºåKLÊï£Â∫¶=0Ôºå\u0026ldquo;ÁßØÂàÜ\u0026rdquo;Ôºà‰ººÁÑ∂ÁöÑÊúüÊúõÔºâÂèñÂà∞ÊúÄÂ§ß„ÄÇ = ‚àë_z‚ÇÅ ‚àë_z‚ÇÇ\u0026hellip;‚àë_z$_N$ [ ‚àè·µ¢‚Çå‚ÇÅ·¥∫ P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log P(x·µ¢,z·µ¢|Œ∏) ]ÔºålogËøû‰πòÂèòËøûÂä†\nÊääËøô‰∏™ÊúüÊúõ E ËÆ∞‰∏∫ Q(Œ∏,Œ∏‚ÅΩ·µó‚Åæ)ÔºåÊòØÂÖ≥‰∫é Œ∏ ÁöÑ‰∏Ä‰∏™ÂáΩÊï∞Ôºàlog ‰∏≠ÁöÑ Œ∏ ÊòØÂèòÈáèÔºåËÄåŒ∏‚ÅΩ·µó‚ÅæÊòØÂ∏∏Êï∞Ôºâ„ÄÇ\nÂÖàÊää log ÁöÑËøûÂä†Â±ïÂºÄÔºö\nQ(Œ∏,Œ∏‚ÅΩ·µó‚Åæ) = ‚àë_z‚ÇÅ ‚àë_z‚ÇÇ\u0026hellip;‚àë_z$_N$ ‚àè·µ¢‚Çå‚ÇÅ·¥∫P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ [ log P(x‚ÇÅ,z‚ÇÅ|Œ∏) + log P(x‚ÇÇ,z‚ÇÇ|Œ∏) + \u0026hellip; + log P($x_N,z_N$|Œ∏)]\nÂ§öÈáç\u0026quot;ÁßØÂàÜ\u0026quot;ÔºåÂÖàÂØπ z‚ÇÅ ÁßØÂàÜÔºöÂÖàÂè™ÂèñÂá∫Á¨¨ 1 È°π logP(x‚ÇÅ,z‚ÇÅ|Œ∏)ÔºåÂπ∂‰∏îÂè™Êää z‚ÇÅ ‰ªéËÅîÂêàÊ¶ÇÁéá‰∏≠ÂèñÂá∫Êù•Ôºö\n‚àë_z‚ÇÅ ‚àë_z‚ÇÇ\u0026hellip;‚àë_z$_N$ [ log P(x‚ÇÅ,z‚ÇÅ|Œ∏) ‚ãÖ ‚àè·µ¢‚Çå‚ÇÅ·¥∫ P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ]\n= ‚àë_z‚ÇÅ ‚àë_z‚ÇÇ\u0026hellip;‚àë_zN [ log P(x‚ÇÅ,z‚ÇÅ|Œ∏) ‚ãÖ P(z‚ÇÅ|x‚ÇÅ,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ ‚àè·µ¢‚Çå‚ÇÇ·¥∫P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ] ÔºåÊääÂè™‰∏é z‚ÇÅ Áõ∏ÂÖ≥ÁöÑÈ°πÂàÜÂá∫Êù• = ‚àë_z‚ÇÅ logP(x‚ÇÅ,z‚ÇÅ|Œ∏) ‚ãÖ P(z‚ÇÅ|x‚ÇÅ,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ ‚àë_z‚ÇÇ\u0026hellip;‚àë_zN [ ‚àè·µ¢‚Çå‚ÇÇ·¥∫P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ]\nÂØπ‰∫éÂêéÂçäÈÉ®ÂàÜÁöÑ‰ªé 2 Âà∞ N ÁöÑËÅîÂêàÊ¶ÇÁéáÔºö\n‚àë_z‚ÇÇ\u0026hellip;‚àë_zN [ ‚àè·µ¢‚Çå‚ÇÇ·¥∫ P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ] = ‚àë_z‚ÇÇ\u0026hellip;‚àë_zN [ P(z‚ÇÇ|x‚ÇÇ,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ P(z‚ÇÉ|x‚ÇÉ,Œ∏‚ÅΩ·µó‚Åæ) \u0026hellip; ‚ãÖ P(zN|xN,Œ∏‚ÅΩ·µó‚Åæ)] Âõ†‰∏∫ÊØè‰∏Ä‰∏™Á¥ØÂä†Âè∑ÈÉΩÂè™‰∏é‰∏Ä‰∏™ z Áõ∏ÂÖ≥Ôºå‰πüÂ∞±ÊòØÊääÊØè‰∏™ z ÂØπÂ∫îÁöÑÁ¶ªÊï£Ê¶ÇÁéáÂàÜÂ∏ÉÔºàp‚ÇÇ,\u0026hellip;,pKÔºâÊ±ÇÂíåÔºåÂπ∂‰∏îÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÁßØÂàÜÁ≠â‰∫é 1: = ‚àë_z‚ÇÇ P(z‚ÇÇ|x‚ÇÇ,Œ∏‚ÅΩ·µó‚Åæ) ‚àë_z‚ÇÉ P(z‚ÇÉ|x‚ÇÉ,Œ∏‚ÅΩ·µó‚Åæ) \u0026hellip; ‚àë_zN P(zN|xN,Œ∏‚ÅΩ·µó‚Åæ) = 1‚ãÖ1‚ãÖ\u0026hellip;‚ãÖ1 = 1\nÊâÄ‰ª•ÂØπ z‚ÇÅ ÁßØÂàÜÁöÑÁªìÊûúÔºåÈô§‰∫ÜÂè™‰∏é z1 Áõ∏ÂÖ≥ÁöÑÈ°π: ‚àë_z‚ÇÅ logP(x‚ÇÅ,z‚ÇÅ|Œ∏) ‚ãÖ P(z‚ÇÅ|x‚ÇÅ,Œ∏‚ÅΩ·µó‚Åæ)ÔºåÂÖ∂‰ΩôÈÉΩÊòØ1„ÄÇÂêåÁêÜ‰πãÂêéÂØπ z2, z3,\u0026hellip;zNÁßØÂàÜÊó∂Ôºå‰πüÂè™‰øùÁïô‰∏éËá™Â∑±Áõ∏ÂÖ≥ÁöÑÈ°π\nQ(Œ∏,Œ∏‚ÅΩ·µó‚Åæ) = ‚àë_z‚ÇÅ‚àë_z‚ÇÇ\u0026hellip;‚àë_zN ‚àè·µ¢‚Çå‚ÇÅ·¥∫ P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ [logP(x1,z1|Œ∏) + logP(x2,z2|Œ∏) +\u0026hellip; + logP(x_N,zN|Œ∏)]\n= ‚àë_z‚ÇÅ log P(x‚ÇÅ,z‚ÇÅ|Œ∏) ‚ãÖ P(z‚ÇÅ|x‚ÇÅ,Œ∏‚ÅΩ·µó‚Åæ) + ‚àë_z‚ÇÇ log P(x‚ÇÇ,z‚ÇÇ|Œ∏) ‚ãÖ P(z‚ÇÇ|x‚ÇÇ,Œ∏‚ÅΩ·µó‚Åæ) + \u0026hellip; + ‚àë_zN log P(xN,zN|Œ∏) ‚ãÖ P(zN|xN,Œ∏‚ÅΩ·µó‚Åæ) = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ ‚àë_z·µ¢ log P(x·µ¢,z·µ¢|Œ∏) ‚ãÖ P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ÔºåËøôÈáåx·µ¢,z·µ¢ÈÉΩÊòØÂ∞èÂÜôÔºåÂçï‰∏™Ê†∑Êú¨\nÁªº‰∏äÔºå‰ººÁÑ∂P(X)ÁöÑÊúüÊúõÁ≠â‰∫é N ‰∏™Ê†∑Êú¨ÁöÑ‰ººÁÑ∂ÊúüÊúõÁöÑÂíå„ÄÇ\nÂÖ∂‰∏≠ÔºåÂçï‰∏™Ê†∑Êú¨ÁöÑËÅîÂêàÊ¶ÇÁéáÔºàÊàñËÄÖËØ¥‰ººÁÑ∂Ôºâ: P(x,z|Œ∏) = P(z|Œ∏) P(x|z,Œ∏) = p_z ‚ãÖ N(x | Œº_z,Œ£_z)ÔºåËøôÈáå‰∏ãÊ†á z Ë°®Á§∫ËØ•Ê†∑Êú¨ÁöÑÈöêÂèòÈáè„ÄÇ Âõ†‰∏∫ P(x) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ ‚ãÖ N(x | Œº‚Çñ,Œ£‚Çñ)Ôºå ÊâÄ‰ª•Âçï‰∏™Ê†∑Êú¨ÁöÑÂêéÈ™åÊ¶ÇÁéáÔºöP(z|x,Œ∏) = P(x,z)/P(x) = p_z ‚ãÖ N(x | Œº_z,Œ£_z) / ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ ‚ãÖ N(x | Œº‚Çñ,Œ£‚Çñ)\nÊää‰ººÁÑ∂ÂíåÂêéÈ™å‰ª£ÂÖ• Q:\nQ(Œ∏,Œ∏‚ÅΩ·µó‚Åæ) = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ ‚àë_z·µ¢ log (p_z·µ¢ ‚ãÖ N(x·µ¢ | Œº_z·µ¢,Œ£_z·µ¢)) ‚ãÖ [ p_z·µ¢ ‚ãÖ N(x·µ¢ | Œº_z·µ¢,Œ£_z·µ¢) / ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ ‚ãÖ N(x·µ¢ | Œº‚Çñ,Œ£‚Çñ) ] Ôºåz·µ¢ÊòØ1‚ãØK‰∏≠ÁöÑ‰ªªÊÑè‰∏Ä‰∏™\n‰ª•‰∏äÂ∞±ÊòØ EM ‰∏≠ÁöÑ E-stepÔºöÊääÔºàN‰∏™Ê†∑Êú¨ÁöÑÔºâ‰ººÁÑ∂ÁöÑÊúüÊúõË°®Á§∫Âá∫Êù•„ÄÇM-step Ë¶ÅÂÖ≥‰∫éÂèÇÊï∞ Œ∏ Ê±Ç Q ÁöÑÊúÄÂ§ßÂÄº„ÄÇ\n4. EMÊ±ÇËß£-M-step Source video: P4\n‰∏äÈù¢ÂæóÂà∞‰∫Ü Q ÁöÑË°®ËææÂºèÔºåÂØπÊØè‰∏™Ê†∑Êú¨ÁöÑ‰ººÁÑ∂ÊåâÁÖßÂêéÈ™åÊ±ÇÊúüÊúõÔºåÁÑ∂ÂêéN‰∏™Ê†∑Êú¨Á¥ØÂä†„ÄÇ M-step Ë¶ÅËß£‰∏Ä‰∏™ÊúÄ‰ºòÂåñÈóÆÈ¢ò„ÄÇ\nÂêéÈ™åÂ±ïÂºÄÊòØÔºöp_z·µ¢ ‚ãÖ N(x·µ¢ | Œº_z·µ¢‚ÅΩ·µó‚Åæ,Œ£_z·µ¢‚ÅΩ·µó‚Åæ) / ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ‚ÅΩ·µó‚Åæ ‚ãÖ N(x·µ¢ | Œº‚Çñ‚ÅΩ·µó‚Åæ,Œ£‚Çñ‚ÅΩ·µó‚Åæ)\nÂú®ÂêéÁª≠Êé®ÂØº‰∏≠Ôºå‰ªçÁÑ∂Êää‰∏Ä‰∏™Ê†∑Êú¨z ÁöÑÂêéÈ™åÁÆÄËÆ∞‰∏∫Ôºö P(z·µ¢|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ)ÔºåÂõ†‰∏∫ Œ∏‚ÅΩ·µó‚ÅæÊòØ‰∏ä‰∏ÄÊó∂ÂàªÁöÑÂèÇÊï∞ÔºöŒ∏‚ÅΩ·µó‚Åæ={p‚ÇÅ‚ÅΩ·µó‚Åæ,p‚ÇÇ‚ÅΩ·µó‚Åæ,\u0026hellip;,pK‚ÅΩ·µó‚Åæ, Œº‚ÇÅ‚ÅΩ·µó‚Åæ,Œ£‚ÇÅ‚ÅΩ·µó‚Åæ, Œº‚ÇÇ‚ÅΩ·µó‚Åæ,Œ£‚ÇÇ‚ÅΩ·µó‚Åæ,\u0026hellip;, Œº_K‚ÅΩ·µó‚Åæ,Œ£_K‚ÅΩ·µó‚Åæ}ÔºåÊòØÂ∏∏Êï∞ÔºåÊâÄ‰ª•Â∞±ÁÆÄÂçïÂÜô„ÄÇ ËÄå‰ººÁÑ∂ logP(x·µ¢,z·µ¢|Œ∏) ‰∏≠ÁöÑŒ∏ ÊòØÂèòÈáèÔºåÊâÄ‰ª•‰ººÁÑ∂ÁöÑÊúüÊúõÁöÑÁ¥ØÂä†Â∞±Ë°®Á§∫‰∏∫Ôºö\nQ(Œ∏,Œ∏‚ÅΩ·µó‚Åæ) = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ ‚àë_z·µ¢ log (p_z·µ¢ ‚ãÖ N(x·µ¢ | Œº_z·µ¢, Œ£_z·µ¢)) ‚ãÖ P(z·µ¢|x·µ¢, Œ∏‚ÅΩ·µó‚Åæ)\n‰∫§Êç¢‰∏§‰∏™Á¥ØÂä†Á¨¶Âè∑È°∫Â∫èÔºö\nQ(Œ∏,Œ∏‚ÅΩ·µó‚Åæ) = ‚àë_z·µ¢ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log (p_z·µ¢ ‚ãÖ N(x·µ¢ | Œº_z·µ¢, Œ£_z·µ¢)) ‚ãÖ P(z·µ¢|x·µ¢, Œ∏‚ÅΩ·µó‚Åæ)\nz·µ¢ ÊòØ‰∏Ä‰∏™Ê†∑Êú¨‰∏äÔºåK‰∏™È´òÊñØÂàÜÂ∏ÉÂèØËÉΩÁöÑÊ¶ÇÁéáÔºåz Âú®ÊúÄÂ§ñÈù¢ÔºåÂèØ‰ª•Êää z·µ¢ ÊõøÊç¢ÊàêÂ∞è kÔºà‰ªé1Âà∞KÔºâÔºå‚àë_z·µ¢ ÊõøÊç¢‰∏∫ ‚àë‚Çñ‚Çå‚ÇÅ·¥∑Ôºö\nQ(Œ∏,Œ∏‚ÅΩ·µó‚Åæ) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log (p‚Çñ ‚ãÖ N(x·µ¢ | Œº‚Çñ, Œ£‚Çñ)) ‚ãÖ P(z·µ¢=C‚Çñ|x·µ¢, Œ∏‚ÅΩ·µó‚Åæ) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [ log p‚Çñ + log N(x·µ¢ | Œº‚Çñ, Œ£‚Çñ) ] ‚ãÖ P(z·µ¢=C‚Çñ|x·µ¢, Œ∏‚ÅΩ·µó‚Åæ)\nÁõÆÊ†áÂáΩÊï∞Â∞±‰∏∫Ôºö Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = arg max_Œ∏ Q(Œ∏,Œ∏‚ÅΩ·µó‚Åæ)\nŒ∏‚ÅΩ·µó‚Å∫¬π‚Åæ ÂåÖÊã¨ p‚Çñ‚ÅΩ·µó‚Å∫¬π‚Åæ={p‚ÇÅ‚ÅΩ·µó‚Å∫¬π‚Åæ,p‚ÇÇ‚ÅΩ·µó‚Å∫¬π‚Åæ,\u0026hellip;,p_K‚ÅΩ·µó‚Å∫¬π‚Åæ}, Œº‚Çñ‚ÅΩ·µó‚Å∫¬π‚Åæ={Œº‚ÇÅ‚ÅΩ·µó‚Å∫¬π‚Åæ,Œº‚ÇÇ‚ÅΩ·µó‚Å∫¬π‚Åæ,\u0026hellip;,Œº_K‚ÅΩ·µó‚Å∫¬π‚Åæ}, Œ£‚Çñ‚ÅΩ·µó‚Å∫¬π‚Åæ={Œ£‚ÇÅ‚ÅΩ·µó‚Å∫¬π‚Åæ,Œ£‚ÇÇ‚ÅΩ·µó‚Å∫¬π‚Åæ,Œ£_K‚ÅΩ·µó‚Å∫¬π‚Åæ}Ôºå\n‰∏ãÈù¢Âè™‰ªãÁªç p‚Çñ ÁöÑÊ±ÇÊ≥ïÔºö\nÂú® Q(Œ∏,Œ∏‚ÅΩ·µó‚Åæ) ‰∏≠ÔºåÂè™Êúâ log p‚Çñ ‰∏é p‚Çñ Áõ∏ÂÖ≥Ôºö\np‚ÅΩ·µó‚Å∫¬π‚Åæ = arg max_p‚Çñ ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log p‚Çñ ‚ãÖ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ), s.t. ‚àë‚Çñ‚Çå‚ÇÅ·¥∑p‚Çñ=1\nÊ±ÇËøô‰∏™Â∏¶Á∫¶ÊùüÁöÑÊúÄ‰ºòÂåñÈóÆÈ¢òÔºåÁî®ÊãâÊ†ºÊúóÊó•‰πòÂ≠êÊ≥ïÊ±ÇËß£:\nÂÜôÂá∫ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Ôºö\nL(p‚Çñ,Œª) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ log p‚Çñ ‚ãÖ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) + Œª(‚àë‚Çñ‚Çå‚ÇÅ·¥∑p‚Çñ - 1)\nÂØπ p‚Çñ Ê±ÇÂÅèÂØºÊó∂ÔºåÂêéÈù¢ÁöÑzÁöÑÂêéÈ™åÂàÜÂ∏ÉÊòØÂ∏∏ÈáèÔºåÊ±ÇÂØºÂêéÊòØÁ≥ªÊï∞„ÄÇp‚Çñ ÁöÑ k ÊòØ‰ªé1Âà∞KÁöÑ‰ªª‰∏ÄÊï∞ÔºåÂè™ÂØπ p‚Çñ Ê±ÇÂØºÔºåÊâÄ‰ª•Â∏¶Êúâ p‚ÇÅ,p‚ÇÇ,\u0026hellip;, p‚Çñ‚Çã‚ÇÅ ÁöÑÈ°πÈÉΩÊòØ0ÔºåÂç≥ÊúÄÂ§ñÂ±ÇÁöÑÁ¥ØÂä†Âè∑Ê≤°‰∫Ü„ÄÇ\n‚àÇL/‚àÇp‚Çñ = ‚àë·µ¢‚Çå‚ÇÅ·¥∫ 1/p‚Çñ ‚ãÖ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) + Œª ‚âú 0 Ôºå‰ª§ÂÖ∂Á≠â‰∫é0\n‰∏§ËæπÂêåÊó∂‰πò‰ª• p‚Çñ:\n‚àë·µ¢‚Çå‚ÇÅ·¥∫ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) + p‚Çñ ‚ãÖ Œª = 0\n‰∏∫‰∫ÜÂà©Áî®Á∫¶ÊùüÊù°‰ª∂ÔºåÊää k=1,2,\u0026hellip;,K ÁöÑÂºèÂ≠êÈÉΩÂä†Ëµ∑Êù•Ôºö\n‚àë·µ¢‚Çå‚ÇÅ·¥∫ ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) + ‚àë‚Çñ‚Çå‚ÇÅ·¥∑p‚Çñ ‚ãÖ Œª = 0\nÂÖ∂‰∏≠ ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) ÊòØÂØπÈöêÂèòÈáè z·µ¢ ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‚ÄúÁßØÂàÜ‚ÄùÔºå‰∏∫1ÔºåÊâÄ‰ª•\n‚àë·µ¢‚Çå‚ÇÅ·¥∫ 1 + Œª = 0 Œª = -N\nÊää Œª=-N ‰ª£ÂÖ•‰∏äÂºèÔºö\n‚àë·µ¢‚Çå‚ÇÅ·¥∫ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ) + p‚Çñ ‚ãÖ (-N) = 0 p‚Çñ‚ÅΩ·µó‚Å∫¬π‚Åæ = 1/N ‚ãÖ ‚àë·µ¢‚Çå‚ÇÅ·¥∫ P(z·µ¢=C‚Çñ|x·µ¢,Œ∏‚ÅΩ·µó‚Åæ)\n‰∏äÂºèÊòØÈÄöÈ°πÔºåÊää k=1,2,..KÔºå‰ª£ÂÖ•Â∞±ÂèØÂæóÂà∞ÊØè‰∏™ p„ÄÇ\nÊ±Ç Œº‚Çñ Âíå Œ£‚Çñ ÁöÑÊñπÊ≥ïÁ±ª‰ººÔºåÂè™ÂÖ≥ÂøÉ log N(x·µ¢ | Œº‚Çñ,Œ£‚Çñ)Ôºålog ÂèØ‰ª•ÊääÈ´òÊñØÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÁÆÄÂåñÔºà‰∏§È°πÁßØÊãÜÊàê‰∏§È°πÂíåÔºåexp‰πüÂèØÂéªÊéâÔºâÔºåËÄå‰∏îÊòØÊó†Á∫¶ÊùüÔºåÁõ¥Êé•ÔºàÂÖ≥‰∫éÁü©ÈòµÔºâÊ±ÇÂØºÔºå‰ª§ÂÖ∂=0Â∞±Ë°å\nRef Ê¶ÇÁéáËÆ∫Ôºöp(x|theta)Âíåp(x;theta)ÁöÑÂå∫Âà´ - -ÊüöÂ≠êÁöÆ- - CSDN ","date":"2022-12-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/11-gmm/","title":"watch: ML - ÁôΩÊùø 11 | GMM"},{"content":"Source videos: „ÄêÊú∫Âô®Â≠¶‰π†„ÄëÁôΩÊùøÊé®ÂØºÁ≥ªÂàó(‰∏âÂçÅ‰∫å) ÔΩû ÂèòÂàÜËá™ÁºñÁ†ÅÂô®(VAE)„Äë\n1. Ê®°ÂûãË°®Á§∫ P1\nVariation Êù•Ëá™Ê¶ÇÁéáÂõæÊ®°ÂûãÔºõ Auto Encoder Êù•Ëá™Á•ûÁªèÁΩëÁªú\nVAE ‰πüÊòØ‰∏ÄÁßç LVM (Latent Variable Model)ÔºåÊúÄÁÆÄÂçïÁöÑÈöêÂèòÈáèÊ®°ÂûãÊòØ GMMÔºåÂÆÉÁöÑÊ¶ÇÁéáÂõæÊ®°ÂûãË°®Á§∫‰∏∫Ôºö\nflowchart TB latent((z)) --\u003e observed((x)) Â∞è x ÊòØËßÇÊµãÂèòÈáèÔºåÂ∞è z ÊòØÂÅáÊÉ≥ÁöÑÊúç‰ªéÊüêÁßçÂàÜÂ∏ÉÁöÑÈöêÂèòÈáèÔºåÂÖàÂØπ z ÁöÑÂàÜÂ∏É P(z) ‰∏≠ÈááÊ†∑ÔºåÂÜçÂú® z Âõ∫ÂÆöÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªéÂàÜÂ∏É P(x|z) ‰∏≠ÈááÊ†∑ x„ÄÇ\nGMM K ‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÊ∑∑ÂêàÔºåÊ†∑Êú¨Êï∞ÊçÆ x ÂèØËÉΩÊù•Ëá™‰∫éËøô K ‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑ‰ªª‰∏Ä‰∏™ÔºåÂè™ÊòØÊ¶ÇÁéá‰∏çÂêå„ÄÇ ËÄå VAE ÊòØÊó†Èôê‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÊ∑∑Âêà„ÄÇ\nGMM ÂÅáËÆæÈöêÂèòÈáè z ÊòØÊúç‰ªé‰∏ÄÁª¥ÁöÑÁ¶ªÊï£ÂûãÊ¶ÇÁéáÂàÜÂ∏ÉÔºözÔΩûCategorical distÔºåÂàÜÂ∏ÉÂàóÔºö\nz 1 2 \u0026hellip; K p p‚ÇÅ p‚ÇÇ \u0026hellip; p‚Çñ z ‰ª£Ë°®‰∏çÂêåÁöÑÁ±ªÂà´categoriesÔºà‰∏çÂêåÁöÑÈ´òÊñØÂàÜÂ∏ÉÔºâÔºåÂÖ∂‰∏≠Ê¶ÇÁéáÂØÜÂ∫¶Ê±ÇÂíåÁ≠â‰∫é1Ôºö‚àë‚Çñ‚Çå‚ÇÅ·¥∑ p‚Çñ = 1„ÄÇx Âú® z ÈÄâÂÆöÁöÑÊÉÖÂÜµ‰∏ãÔºåÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºöx|z‚ÇñÔΩûN(x|Œº‚Çñ,Œ£‚Çñ)\nGMM È°∂Â§öÂèØ‰ª•Ë¢´Áî®Êù•ÂÅöËÅöÁ±ª‰ªªÂä°Ôºå‰ΩÜÊó†Ê≥ïËß£ÂÜ≥Â§çÊùÇÁöÑ‰ªªÂä°ÔºåÊØîÂ¶ÇÁõÆÊ†áÊ£ÄÊµãÔºåÂõ†‰∏∫ÂÆÉÁöÑ z Â§™ÁÆÄÂçï‰∫ÜÔºåÂè™Êúâ 1 Áª¥ËøòÊòØÁ¶ªÊï£ÂûãÁöÑÂèòÈáèÔºåÂè™ËÉΩÂØπ 1 ‰∏™Â±ûÊÄßÂàíÂàÜ K Á±ªÂà´Ôºå\nÊØîÂ¶ÇÊúâ‰∏ÄÁæ§‰∫∫‰Ωú‰∏∫Ê†∑Êú¨ÔºåÊÉ≥Â≠¶‰π†Âá∫‰∏Ä‰∏™z Áî®Êù•Ë°®Á§∫‰∏Ä‰∏™Ê†∑Êú¨„ÄÇ GMM ÂÅáËÆæ‰∫Ü z Âè™ÊòØ‰∏Ä‰∏™ 1 Áª¥KÁ±ªÁöÑÂèòÈáèÔºåÂ∞±Âè™ËÉΩÊääËøôÁæ§‰∫∫ÂàÜÊàê K Á±ª ÔºàÊØîÂ¶ÇÊåâ‚ÄúËÅå‰∏ö‚ÄùÂ±ûÊÄßÂàÜÊàêÔºöÂ∑•‰∫∫ÔºåÂÜúÊ∞ëÔºåÁü•ËØÜ‰ªΩÂ≠ê\u0026hellip;ÔºâÔºåÊâÄ‰ª•Âè™ËÉΩË°®ËææÂá∫‰∏Ä‰∏™‰∫∫ÊòØÊù•Ëá™‰∫éÂì™Á±ª„ÄÇ ‰πüÂ∞±ÊòØËØ¥ÔºåGMM ÂØπÊ†∑Êú¨ÁöÑË°®ËææÈùûÂ∏∏ËÇ§ÊµÖÔºåÂõ†‰∏∫‰∏Ä‰∏™‰∫∫ÊúâÂ§ö‰∏™Â±ûÊÄßÔºåÂøÖÈ°ªÁî®Áî®Â§ö‰∏™Áª¥Â∫¶Ë°®ËææÔºö ÊØîÂ¶Ç ‚ÄúÊÄßÂà´‚Äùz‚ÇÅ={Áî∑,Â•≥}Ôºå‚ÄúËÇ§Ëâ≤‚Äùz‚ÇÇ={ÈªÑ,ÁôΩ,Èªë}Ôºå‚ÄúÂπ¥ÈæÑ‚Äùz‚ÇÉ={‚Ñ§}Ôºå‚ÄúË∫´È´ò‚Äùz‚ÇÑËøûÁª≠ÁöÑÔºåËøô‰πàÂ§öÁª¥Â∫¶ GMM Êó†Ê≥ïË°®ËææÂá∫Êù•„ÄÇ\nÈöêÂèòÈáè z Â∫îËØ•ÊòØÈ´òÁª¥ÁöÑÔºåËøûÁª≠ÁöÑÈöèÊú∫ÂèòÈáèÔºåÂÅáËÆæ z Êúç‰ªéÈ´òÊñØÂàÜÂ∏É: zÔΩûN(Œº=0,Œ£=I)ÔºåÊª°Ë∂≥È´òÁª¥ËøûÁª≠„ÄÇP(z) ÊòØÂÖàÈ™åÔºåÂÆÉÂπ∂‰∏çÈáçË¶ÅÔºåÂè™ÊòØËæÖÂä©Âª∫Ê®°ÔºåÊàë‰ª¨ÊúÄÁªàÂÖ≥ÂøÉÁöÑÊòØ inference ËøáÁ®ãÔºöÁªô‰∏Ä‰∏™ x ËøîÂõûÂÆÉÁöÑ zÔºå‰πüÂ∞±ÊòØÂêéÈ™å P(z|x)\nÂ¶ÇÊûúÊ†∑Êú¨ x Êú¨Ë∫´ÊòØËøûÁª≠ÁöÑÔºåÂèØ‰ª•ÂÅáËÆæÂÆÉÁöÑÊù°‰ª∂Ê¶ÇÁéá‰πüÊúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºöx|zÔΩûN(Œº(z),Œ£(z))„ÄÇËã•ÊòØÁ¶ªÊï£ÁöÑÔºåÂàô‰ªçÁÑ∂Áî® categorical distribution„ÄÇ\nÊù°‰ª∂Ê¶ÇÁéá x|z ÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÈÉΩÊòØ z ÁöÑÂáΩÊï∞Ôºå‰πüÂ∞±ÊòØÂÖàÁªôÂÆö‰∫Ü zÔºåÁÑ∂ÂêéÊ±ÇÂá∫ Œº(z) Âíå Œ£(z)ÔºåÁõ∏ÂΩì‰∫éÂæóÂà∞‰∫Ü x„ÄÇÊâÄ‰ª•ÂÆûÈôÖ‰∏äË¶ÅÂ≠¶ Œº,Œ£ ‰∏é z ‰πãÈó¥ÂáΩÊï∞ÂÖ≥Á≥ª„ÄÇ ÂèØ‰ª•Áî®Á•ûÁªèÁΩëÁªúÈÄºËøëÂá∫Êù•Ëøô‰∏™ÂáΩÊï∞ÔºåÊâÄ‰ª• Œº,Œ£ ÊòØÁ•ûÁªèÁΩëÁªúÔºàÂèÇÊï∞Œ∏ÔºâÁöÑÂáΩÊï∞ÔºåÁî® Œº_Œ∏, Œ£_Œ∏ Ë°®Á§∫„ÄÇ\nËÄå‰∏çÁõ¥Êé•ÈÄöËøáÁÆó Likelihood Ê±Ç x|z ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÊòØÂõ†‰∏∫ z ÁöÑÁª¥Â∫¶Â§™È´òÔºå‰∏çÂ•ΩÊää z ÁßØÊéâÔºöP(x) = ‚à´_z P(x,z) dz =‚à´_z P(z) P(x|z) dz„ÄÇ Âõ†‰∏∫ÂÅáËÆæ‰∫Ü z ÁöÑÁª¥Â∫¶ÂæàÈ´òÔºåÈ´òÂà∞ÁßØÂàÜÁÆó‰∏çÂá∫Êù•ÔºåÂàô P(x) ÊòØ intractable ÁöÑÔºå ÂèàÂõ†‰∏∫ÂêéÈ™åÂàÜÂ∏É P(z|x) = P(x|z)P(z) / P(x)ÔºåÊâÄ‰ª•zÁöÑÂêéÈ™å‰πüÁÆó‰∏çÂá∫Êù•„ÄÇ‰πüÂ∞±Êó†Ê≥ï‰ªé x Âà∞ z ÂÅöinference„ÄÇ Âè™ËÉΩÁî®ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßÂíåÁ•ûÁªèÁΩëÁªúÈÄºËøëÂêéÈ™åÂàÜÂ∏É„ÄÇ\nÊé®Êñ≠\u0026amp;Â≠¶‰π† P2\nflowchart TB latent((z)) -- \"P·∂ø(x|z) \\n Decode \\n Generation\" --\u003e observed((x)) observed((x)) -. \"P·∂ø(z|x) or q·∂≤(z|x)\\n Encode \\n Inference\" .-\u003e latent((z)) z ÊòØÈöêÂèòÈáèÂÅáÂÆöÊúç‰ªéÈ´òÊñØÂàÜÂ∏É P(z)=N(0,I)Ôºåx ÊòØËßÇÊµãÂèòÈáèÂÅáÂÆöÊúç‰ªéÈ´òÊñØÂàÜÂ∏É P_Œ∏ (x|z)=N(Œº_Œ∏ (z), Œ£_Œ∏ (z))„ÄÇ ÂÅáÂ¶ÇÂèÇÊï∞ Œ∏ Â∑≤ÁªèËÆ≠ÁªÉÂ•Ω‰∫ÜÔºåÁîüÊàêÊ†∑Êú¨Êó∂ÔºåÂÖà‰ªé z ÁöÑÂàÜÂ∏É P(z) ‰∏≠ÈááÊ†∑‰∏Ä‰∏™ z‚ÅΩ‚Å±‚ÅæÔºåÁÑ∂ÂêéÂ∞±ËÉΩ‰ªé P_Œ∏( x|z‚ÅΩ‚Å±‚Åæ) ÈááÊ†∑Âá∫‰∏Ä‰∏™ x‚ÅΩ‚Å±‚Åæ„ÄÇ\nÂõ†‰∏∫ÂêéÈ™åÂàÜÂ∏É P(z|x) Êó†Ê≥ïÈÄöËøáË¥ùÂè∂ÊñØÂÖ¨ÂºèÁÆóÂá∫ÔºåÊâÄ‰ª•Áî® q_œï(z|x) ‰∏çÊñ≠ÈÄºËøëÂêéÈ™åÂàÜÂ∏ÉP_Œ∏(z|x)„ÄÇ\nÁî® EM Ê±ÇËß£ GMM Êó∂ÔºåÊòØÊúÄÂ§ßÂåñ‰ººÁÑ∂Ôºå‰ººÁÑ∂ÂèØ‰ª•ÂàÜÊàê‰∏ãÁïå ELBO Âíå KL( q_œï(z|x) || P_Œ∏(z|x))„ÄÇ\nÊúÄÂéüÂßãÁöÑ EM ÁöÑ E-step Ë¶ÅÂÜôÂá∫n‰∏™Ê†∑Êú¨‰ººÁÑ∂p(x)ÁöÑÔºàÊúÄÂ§ßÁöÑÔºâÊúüÊúõÔºöÂΩì q(z|x)=P(z|x) Êó∂, KL=0, ‰ººÁÑ∂ÁöÑÊúüÊúõÂ∞±ÊòØ ELBO„ÄÇ\nM-step Â∞±ÊòØËß£ÊúÄÂ§ßÂåñÈóÆÈ¢òÔºöŒ∏ = arg max_Œ∏ ELBO = arg max_Œ∏ E_P(z|x,Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ) [ log P(x,z|Œ∏‚ÅΩ·µó‚Åæ) ]. E-M Âç≥‚ÄúÊúÄÂ§ßÁöÑÊúüÊúõ ‰∏é ÊúüÊúõÁöÑÊúÄÂ§ß‚Äù VAE ‰∏çËÉΩÁî®Âü∫Á°ÄÁöÑ EM Ëß£ÂÜ≥ÔºåÂõ†‰∏∫ÈúÄË¶Å q(z|x) ËÉΩÂèñÂà∞ P(z|x)Ôºå‰ΩÜÊòØËøôÈáåÁöÑÂêéÈ™å P(z|x) ÊòØ intractable ÁöÑÔºåÊâÄ‰ª•Âè™ËÉΩËÆ© KL Êï£Â∫¶Ë∂≥Â§üÂ∞èÔºåÊâæÂá∫ÊúÄÂ•ΩÁöÑ qÔºå‰πüÂ∞±ÊòØÊâæÂá∫ÂÆÉÊúÄÂ•ΩÁöÑÂèÇÊï∞œïÔºåËÆ© q_œï(z|x) Ë∂≥Â§üÊé•Ëøë P_Œ∏(z|x)„ÄÇ\nÂú®EM‰∏≠ÔºåÈÄöËøáÂºïÂÖ•ÈöêÂèòÈáè z Âíå Z ÁöÑÂàÜÂ∏É q(Z)ÔºåÂØπÊï∞‰ººÁÑ∂ logP(X) Ë¢´ÊãÜÂàÜÊàê ELBO + KLÊï£Â∫¶(q(Z)||P(Z|X))„ÄÇ ÂØπÂ∫îÂà∞ËøôÈáåÁöÑÊÉÖÂÜµÔºåKL Êï£Â∫¶‰∏≠ÁöÑ q(Z) Â∫îËØ•ÊòØÂêéÈ™å q·µ©(z|x)Ôºå‰πüÂ∞±ÊòØÂØπ‰ººÁÑ∂ P(x|z) ÂºïÂÖ•ÁöÑÊòØ q·µ©(z|x)ÔºåÂêåÊ†∑ ELBO ‰∏≠‰πüÊòØÊåâ q(z|x) Ê±ÇÊúüÊúõ„ÄÇ ELBO ÊääÈáåÈù¢ÁöÑ log Â±ïÂºÄÔºåÂç≥ÂèØÂÜôÊàêËÅîÂêàÊ¶ÇÁéáP(z,x) + ÁÜµH[q(z|x)]ÔºõÂÜçÊääËÅîÂêàÊ¶ÇÁéáÊãÜÂºÄÔºåÂÖ∂‰∏≠ z ÁöÑÂÖàÈ™å P(z) ÂèØ‰∏éÂêéÈ™åÁöÑÁÜµ H[q(z|x)] ÁªìÂêàÊàê‰∏Ä‰∏™KLÊï£Â∫¶ÔºözÁöÑÂêéÈ™å‰∏ézÁöÑÂÖàÈ™åË¶ÅÈù†Ëøë„ÄÇ\n$\u0026lt;\\^Œ∏,\\^œï\u0026gt;$ = arg min KL( q_œï(z|x) || P_Œ∏(z|x) ) = arg max ELBO ÔºåÁ≠â‰ª∑‰∫éÊúÄÂ§ßÂåñ‰ººÁÑ∂P(x|z)ÁöÑ‰∏ãÁïå = arg max E_q·µ©(z|x) [ log (P_Œ∏ (z,x)/q·µ©(z|x)) ] Ôºå‰ª• z ÁöÑÂêéÈ™åÂä†ÊùÉ = arg max E_q·µ©(z|x) [ log P_Œ∏ (z,x) - log q·µ©(z|x)) ] = arg max E_q·µ©(z|x) [ log P_Œ∏(z,x) ] - E_q·µ©(z|x) [ log q(z|x) ] ÔºåÁ¨¨2È°πÊòØ q(z|x) ÁöÑÁÜµ = arg max E_q·µ©(z|x) [ log P_Œ∏(z,x) ] + H[q·µ©(z|x)] ÔºålogÈáåÈù¢ÁöÑËÅîÂêàÊ¶ÇÁéáÊãÜÂºÄ = arg max E_q·µ©(z|x) [ log P_Œ∏(x|z) ] + E_q·µ©(z|x) [ log P(z) ] + H[q·µ©] ÔºåP(z) ÊòØÂÖàÈ™åÂàÜÂ∏É‰∏çÂ∏¶ÂèÇÊï∞\n= arg max E_q·µ©(z|x) [ log P_Œ∏(x|z) ] + E_q·µ©(z|x) [ log P(z)-log q(z|x)] = arg max E_q·µ©(z|x) [ log (P_Œ∏(x|z) ] + E_q·µ©(z|x) [ log (P(z) / q(z|x))] = arg max E_q·µ©(z|x) [ log P_Œ∏(x|z) ] + ‚à´_z q·µ©(z|x)‚ãÖlog ( P(z) / q(z|x)) dz = arg max E_q·µ©(z|x) [ log P_Œ∏(x|z) ] - KL( q·µ©(z|x) || P(z) )\nÁõÆÊ†áÂáΩÊï∞ÊòØ‰∏Ä‰∏™ÊúüÊúõÂáè KL Êï£Â∫¶ÔºåÊúüÊúõË¶ÅÊúÄÂ§ßÔºåËÄåKLÊï£Â∫¶Ë¶ÅÊúÄÂ∞è„ÄÇ\nÂèòÂàÜÊé®Êñ≠Áî®Ê¢ØÂ∫¶‰∏äÂçáÊ±ÇÊúÄÂ§ßÂåñÈóÆÈ¢òÔºåÈ¶ñÂÖàÊ±ÇÁõÆÊ†áÂáΩÊï∞ÂØπ Œ∏,œï ÁöÑÊ¢ØÂ∫¶ÔºåÁÑ∂ÂêéÊõ¥Êñ∞\nÈááÁî® SGVI / SGVB / SVI / Armotized Inference ÔºàÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ß+Á•ûÁªèÁΩëÁªúÔºâÊ±ÇÂêéÈ™åÔºåËß£ÂÜ≥ Inference ÈóÆÈ¢ò„ÄÇ\nSGVI ÂÅáËÆæ z|x Êúç‰ªéÈ´òÊñØÂàÜÂ∏É z|xÔΩûN(Œº_œï(x),Œ£_œï(x))ÔºåÂπ∂‰∏î‰∏éÈöèÊú∫È´òÊñØÂô™Â£∞ ŒµÔΩûN(0,I) ‰πãÈó¥ÊúâÂáΩÊï∞ÂÖ≥Á≥ª z= Œº_œï(x) + Œ£_œï¬π·êü¬≤(x)‚ãÖŒµÔºåÂçèÊñπÂ∑ÆÁü©ÈòµÁöÑÊåáÊï∞ÊòØ¬ΩÔºåÂõ†‰∏∫ Œ£ ÈáåÈù¢ÊòØœÉ¬≤„ÄÇ\nflowchart LR x(\"Input x\") --\u003e net(\"NN-œï\") --\u003e a(\"Œº(x)\") --\u003e o((\"+\")) --\u003e z(\"latent z\") net --\u003e b(\"Œ£(x)\") --\u003e m((\"√ó\")) --\u003e o Œµ --\u003e m ÂØπ‰∫éÁõÆÊ†áÂáΩÊï∞‰∏≠ÁöÑ E_q·µ©(z|x) [ log P_Œ∏(x|z) ]Ôºå‰∏≠Êã¨Âè∑ÈáåÈù¢ÊòØÁªôÂÆö z ÁîüÊàê xÔºåËÄåÊúüÊúõÁöÑÊùÉÈáç q(z|x) ÊòØÁªôÂÆö x Êó∂Ôºåz ÁöÑÂêéÈ™åÊ¶ÇÁéá„ÄÇ Êï¥‰∏™ËÆ≠ÁªÉËøáÁ®ãÂ∞±ÊòØÔºåÂÖàÁªô‰∫ÜÊ†∑Êú¨ x ÂæóÂà∞‰∫ÜÂêéÈ™å q_œï(z|x)Ôºå‰ªé‰∏≠ÈááÊ†∑Âæó‰∏Ä‰∏™ z‚ÅΩ‚Å±‚ÅæÔºåÁÑ∂ÂêéÁî®ÂÆÉÁÆó‰ººÁÑ∂ logP(x|z‚ÅΩ‚Å±‚Åæ)ÔºåÊòØ‰∏Ä‰∏™ÁéØË∑Ø„ÄÇ ÊâÄ‰ª•Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºålog ÈáåÁöÑ z ‰∏çÊòØÂÖàÈ™å P(z)ÔºåËÄåÊòØÂêéÈ™å P(z|x)„ÄÇ Âú®ËÆ≠ÁªÉÂ•Ω‰πãÂêéÔºåÂæóÂà∞‰∫ÜŒ∏ÔºåÁîüÊàêÊ†∑Êú¨Êó∂ÔºåÂ∞±Áõ¥Êé•‰ªé P(z) ‰∏≠ÈááÊ†∑ÔºåÂÜç‰ª£ÂÖ• P_Œ∏(x|z) ÂæóÂà∞ x„ÄÇ\nflowchart LR x --\u003e Decoder --\u003e z --\u003e Encoder --\u003e x' ÁõÆÊ†áÂáΩÊï∞ÂêéÈù¢ÁöÑ KL Êï£Â∫¶Áõ∏ÂΩì‰∫é Ê≠£ÂàôÂåñÈ°π„ÄÇÂú®ËÆ≠ÁªÉÊó∂ÔºåË¶ÅËÆ© q_œï(z|x) Â∞ΩÈáè‰∏éÂÖàÈ™å P(z) Èù†ËøëÔºåÈÅøÂÖçÂùçÁº©Âà∞‰∏Ä‰∏™ÁÇπ‰∏äÔºåÂê¶ÂàôÁ¨¨ 1 È°π‰ººÁÑ∂ÁöÑÊúüÊúõÂæàÂèØËÉΩÂ∞±ËøáÊãüÂêà‰∫Ü„ÄÇ ÊàñËÄÖËØ¥ËÆ© q_œï ÁöÑÁÜµ H[q_œï] ÂÄæÂêë‰∫éÂ§ß„ÄÇ ÁÜµÊÑèÂë≥ÁùÄ‰ø°ÊÅØÈáèÔºå‰ø°ÊÅØÈáèÂ§ßÊÑèÂë≥ÁùÄÊúâÂπøÊ≥õÁöÑÂèØËÉΩÊÄßÔºåÂàÜÂ∏ÉÊõ¥Âπ≥ÂùáÔºåÈ´òÊñØÂàÜÂ∏ÉË∂äÊâÅÊñπÂ∑ÆË∂äÂ§ßÁÜµË∂äÂ§ßÔºåÈíüÂΩ¢Êõ≤Á∫øË∂äÁò¶È´òÔºåËØ¥ÊòéÂè™Âú®ÊúüÊúõÈÇ£‰∏Ä‰∏™ÁÇπ‰∏äÂèØËÉΩÊÄßÊúÄÂ§ßÔºåÂü∫Êú¨‰∏äÊòØÁ°ÆÂÆöÁöÑÔºåÁÜµÂ∞±ÂæàÂ∞èÔºå\nsumNote\n","date":"2022-12-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/32-vae/","title":"watch: ML - ÁôΩÊùø 32 | VAE"},{"content":"ÂÖàÈ™åÊ¶ÇÁéá Ê†áÁ≠æÁöÑÁõ¥ËßÇÂàÜÂ∏É\nÂêéÈ™åÊ¶ÇÁéáÔºö Âú®Êüê‰∏Ä‰∫ã‰ª∂ÂÖàÊàêÁ´ãÁöÑÊù°‰ª∂‰∏ãÔºåÊ†áÁ≠æÁöÑÂàÜÂ∏É„ÄÇ\nÂÅáËÆæÂ∑≤Áü•Áé©‰∏çÁé©Ëã±ÈõÑËÅîÁõüËøô‰ª∂‰∫ãÊÉÖ (ùêò) ‰∏äÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºàÂÖàÈ™åÔºâ‰∏∫Ôºö\nP(Y=Áé©)=0.6ÔºõP(Y=‰∏çÁé©)=0.4\nÂè¶Â§ñÔºåÂ∑≤Áü•ÊÄßÂà´ (ùêó) ÂàÜÂ∏ÉÔºà‰ººÁÑ∂/Á±ªÊù°‰ª∂Ê¶ÇÁéáÔºâÔºöÁé©LOL‰∫∫Áæ§‰∏≠:80%ÊòØÁî∑Áîü,20%Â•≥ÁîüÔºõ‰∏çÁé©LOLÁöÑ‰∫∫‰∏≠Êúâ:20%Áî∑Áîü,80%Â•≥ÁîüÔºå‰πüÂ∞±ÊòØÔºö\nP(X=Áî∑ÊÄß|Y=Áé©lol)=0.8ÔºåP(X=Â∞èÂßêÂßê|Y=Áé©lol)=0.2 P(X=Áî∑ÊÄß|Y=‰∏çÁé©lol)=0.2ÔºåP(X=Â∞èÂßêÂßê|Y=‰∏çÁé©lol)=0.8\nÊ±ÇÔºö‰∏Ä‰∏™Áî∑Áîü‰ªñÁé©LOLÁöÑÊ¶ÇÁéáÔºàÂêéÈ™åÔºåÂÆÉÊòØÂú®ÂÖàËßÇÂØüÂà∞ÊÄßÂà´X‰∫ã‰ª∂ÂèëÁîüÂêéÂæóÂà∞ÁöÑÔºâ\nÊ†πÊçÆË¥ùÂè∂ÊñØÂÆöÁêÜ P(Y|X)=(P(X|Y)‚ãÖP(Y))/P(X), P(Y=Áé© | X=Áî∑ÊÄß) = P(X=Áî∑ÊÄß|Y=Áé©)‚ãÖP(Y=Áé©) / (P(X=Áî∑ÊÄß|Y=Áé©)‚ãÖP(Y=Áé©) + P(X=Áî∑ÊÄß|Y=‰∏çÁé©)‚ãÖP(Y=‰∏çÁé©))\nÁü•‰πéÁî®Êà∑V6oo4r ÁöÑËØÑËÆ∫Ôºö ÂÖàÈ™åÊ¶ÇÁéáÊòØ‰ª•ÂÖ®‰∫ã‰ª∂‰∏∫ËÉåÊôØ‰∏ãÔºåA‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá: P(A|Œ©). ÂêéÈ™åÊ¶ÇÁéáÊòØ‰ª•Êñ∞‰∫ã‰ª∂B‰∏∫ËÉåÊôØ‰∏ãÔºåA‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá: P(A|B).\nÂÖ®‰∫ã‰ª∂‰∏ÄËà¨ÊòØÁªüËÆ°Ëé∑ÂæóÁöÑÔºåÊâÄ‰ª•Êàê‰∏∫ÂÖàÈ™åÊ¶ÇÁéáÔºåÊ≤°ÊúâÂÅöÂÆûÈ™åÂâçÁöÑÊ¶ÇÁéá„ÄÇ\nÊñ∞‰∫ã‰ª∂‰∏ÄËà¨ÊòØÂÆûÈ™åÔºåÂ¶ÇËØïÈ™åBÔºåÊ≠§Êó∂ÁöÑ‰∫ã‰ª∂ËÉåÊôØ‰ªéÂÖ®‰∫ã‰ª∂ÂèòÊàê‰∫ÜBÔºåËØ•‰∫ã‰ª∂BÂèØËÉΩÂØπAÁöÑÊ¶ÇÁéáÊúâÂΩ±ÂìçÔºåÈÇ£‰πàÈúÄË¶ÅÂØπAÁé∞Âú®ÁöÑÊ¶ÇÁéáËøõË°å‰∏Ä‰∏™‰øÆÊ≠£Ôºå‰ªéP(A|Œ©) ÂèòÊàê‰∫Ü P(A|B)ÔºåÊâÄ‰ª•Êàê P(A|B) ‰∏∫ÂêéÈ™åÊ¶ÇÁéáÔºå‰πüÂ∞±ÊòØËØïÈ™åÔºà‰∫ã‰ª∂BÂèëÁîüÔºâ‰πãÂêéÁöÑÊ¶ÇÁéá„ÄÇ P(A|B)= P(B|A)‚ãÖP(A|Œ©)/P(B|Œ©)\n‰æãÂ≠êÊù•Ê∫êÔºöÂ¶Ç‰ΩïÁêÜËß£ÂÖàÈ™åÊ¶ÇÁéá‰∏éÂêéÈ™åÊ¶ÇÁéá-ÊòåÁ°ï-Áü•‰πé‰∏ìÊ†è\nÊú∫Âô®Â≠¶‰π†-ÁôΩÊùøÊé®ÂØºÁ≥ªÂàó(‰∏Ä)-ÂºÄÁØá\nÈ¢ëÁéáÊ¥æ ÂèÇÊï∞Œ∏ÂèØËÉΩ‰∏ç‰ªÖÊòØ‰∏Ä‰∏™Ôºå‰πüÂèØËÉΩÊòØ‰∏ÄÁªÑÊï∞\nË¥ùÂè∂ÊñØÊ¥æ ÂèÇÊï∞ Œ∏ ÊòØÈöèÊú∫ÂèòÈáèÔºåÊúç‰ªé‰∏ÄÁßçÊ¶ÇÁéáÂàÜÂ∏ÉÔºàÂÖàÈ™åÂàÜÂ∏ÉÔºâ\nË¥ùÂè∂ÊñØÂÆöÁêÜÊääÂèÇÊï∞ÁöÑÂÖàÈ™åÂàÜÂ∏ÉÂíåÂêéÈ™åÂàÜÂ∏ÉÈÄöËøá‰ººÁÑ∂ÂÄºËÅîÁ≥ªËµ∑Êù•ÔºåÂèÇÊï∞ÁöÑÂêéÈ™åÂàÜÂ∏É= ‰ººÁÑ∂xÂÖàÈ™åÂàÜÂ∏É/Âú®ÂèÇÊï∞Á©∫Èó¥ÂØπÊ†∑Êú¨Êï∞ÊçÆÁßØÂàÜ„ÄÇ\nÂèÇÊï∞‰º∞ËÆ°ÁöÑÊñπÊ≥ïÔºöMAP ÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°\n","date":"2022-12-16T13:02:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/01_%E9%A2%91%E7%8E%87%E6%B4%BE-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE/","title":"watch: ML - ÁôΩÊùø 01 | Frequentist vs Bayesian"},{"content":"Ë¥ùÂè∂ÊñØËß£Èáä‚ÄúL1ÂíåL2Ê≠£ÂàôÂåñ‚ÄùÔºåÊú¨Ë¥®‰∏äÊòØÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°„ÄÇÂ¶Ç‰ΩïÊ∑±ÂÖ•ÁêÜËß£Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÔºü\n(Ë¥ùÂè∂ÊñØÂÖ¨Âºè‰∏≠ÁöÑÊ¶ÇÁéáÂèØ‰ª•ÊòØÈöèÊú∫‰∫ã‰ª∂ÁöÑÊ¶ÇÁéáÔºå‰πüÂèØ‰ª•ÊòØÊ¶ÇÁéáÂàÜÂ∏Éwiki)\n‰ººÁÑ∂ÂáΩÊï∞ L(Œ∏|X)ÔºöXÂèëÁîü‰πãÂêéÔºåÂÖ≥‰∫éŒ∏ÁöÑÂáΩÊï∞„ÄÇ ‰ººÁÑ∂ÂÄºÔºöÂèÇÊï∞ Œ∏ ÂèëÁîüÂêéÔºåX ÂèëÁîüÁöÑÊ¶ÇÁéáÔºåÂç≥ L(Œ∏|X) = P(X|Œ∏)„ÄÇ\nÈ¢ëÁéáÊ¥æËÆ§‰∏∫ËÆ≠ÁªÉÊï∞ÊçÆÊòØÊù•Ëá™ÁúüÂÆûÂàÜÂ∏ÉÁöÑÈááÊ†∑Ôºå‰ººÁÑ∂ÂÄºÔºàXÂèëÁîüÊ¶ÇÁéáÔºâÊúÄÂ§ßÁöÑ Œ∏ Â∞±ÊòØÊúÄÊé•ËøëÁúüÂÆûÁöÑÔºå‰ΩÜÂπ∂‰∏çËÉΩËØ¥ËøôÁªÑÂèÇÊï∞Â∞±ÊòØÊúÄÊúâÂèØËÉΩÁöÑÂàÜÂ∏É„ÄÇ Ë¥ùÂè∂ÊñØÊ¥æÊÉ≥Áõ¥Êé•Ê±ÇÂèÇÊï∞ Œ∏ ÁöÑÊ¶ÇÁéáÔºåËÆ§‰∏∫ÁúüÂÆûŒ∏ÂàÜÂ∏ÉÂèëÁîüÁöÑÊ¶ÇÁéáÂ∫îËØ•ÊúÄÂ§ß„ÄÇ\nMLE Âú®‰∏§‰∏™Âú∞ÊñπÂÅö‰∫ÜËøë‰ººÔºöÂØªÊâæ‰Ωø X ÂèëÁîüÊ¶ÇÁéáÊúÄÂ§ßÁöÑ Œ∏ Âπ∂‰∏çÁ≠â‰∫é Œ∏ ÂèëÁîüÁöÑÊ¶ÇÁéáÊúÄÂ§ßÔºõŒ∏ ÊòØ‰∏Ä‰∏™ÂàÜÂ∏ÉÔºåÊâæÂà∞ÁöÑÊòØÂÆÉÁöÑ‰ºóÊï∞„ÄÇ MAP Âè™Êúâ‰∏Ä‰∏™Ëøë‰ººÔºöÂØªÊâæÊ¶ÇÁéáÊúÄÂ§ßÁöÑ Œ∏ ÂàÜÂ∏ÉÊòØ‰∏•Ë∞®ÁöÑÔºåÂè™Ââ©\u0026quot;Œ∏ ÊòØ‰∏Ä‰∏™ÂàÜÂ∏ÉÔºåÊâæÂà∞ÁöÑÊòØÂÆÉÁöÑ‰ºóÊï∞\u0026quot;„ÄÇ\n(221216): Ê¶ÇÁéáÊ®°ÂûãÁöÑÂèÇÊï∞Œ∏Âá∫Áé∞ÁöÑÊ¶ÇÁéáÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Ôºå‰ΩÜÂú®Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏äÔºå‰ªª‰∏Ä‰∏™ÁÇπÔºà‰∏Ä‰∏™Œ∏ÔºâÁöÑÊ¶ÇÁéáÊòØ0ÔºåÂõ†‰∏∫ÂÆÉÂØπÂ∫îÁöÑÈù¢ÁßØ‰∏∫0ÔºåÊâÄ‰ª•Âè™ÊúâË∞àËÆ∫\u0026quot;‰∏ÄÊÆµÂå∫Èó¥\u0026quot;Âá∫Áé∞ÁöÑÊ¶ÇÁéáÊâçÊúâÊÑè‰πâÔºåÊØîÂ¶ÇŒ∏ËêΩÂú®0.2~0.3‰πãÈó¥ÁöÑÊ¶ÇÁéá‰∏∫60%„ÄÇ\nMAP Á≠â‰∫é MLE Âä†‰∏äÊ≠£ÂàôÂåñ Œ∏ Âú®ËÆ≠ÁªÉÈõÜ X ‰∏äÂèëÁîüÁöÑÊ¶ÇÁéá P(Œ∏|X) ÂèØÁî®Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÂ±ïÂºÄÔºö\nP(Œ∏|X) = (P(X|Œ∏)/P(X)) ‚ãÖ P(Œ∏)\nÊ®°ÂûãÂèÇÊï∞ Œ∏ ÁöÑÊ¶ÇÁéá = X Âú®ÂèÇÊï∞‰∏∫ Œ∏ Êó∂ÂèëÁîüÁöÑÊ¶ÇÁéá ‰∏é X Âú®ÊâÄÊúâÂèØËÉΩÁöÑ Œ∏ ‰∏ãÂèëÁîüÁöÑÊ¶ÇÁéá‰πãÂíå ÁöÑÊØîÂÄº √ó Œ∏ ÁöÑËµ∑ÂßãÊ¶ÇÁéá„ÄÇ ÂÖ∂‰∏≠ P(X) =‚à´ P(X|Œ∏)P(Œ∏) dŒ∏ „ÄÇ\nP(Œ∏|X) Âíå P(Œ∏) ÈÉΩÊòØ Œ∏ ÁöÑÊ¶ÇÁéáÔºå‰∏çËøáÂâçËÄÖÊòØÊúâÊù°‰ª∂ÁöÑÔºàÊòØXÂèëÁîü‰πãÂêéÁöÑÔºåÊòØÊõ¥Êñ∞ÂêéÁöÑÔºâÔºåÊâÄ‰ª•Âè´ÂÅö‚ÄúÂêéÈ™åÂàÜÂ∏É‚ÄùÔºàXÊòØËØïÈ™åÔºåÂç≥‚ÄúËØïÈ™å‰πãÂêé‚ÄùÔºâÔºõÂêéËÄÖÊòØÂú®XÂèëÁîü‰πãÂâçÔºåÔºà‰∏çÁúãXÔºâÂøÉ‰∏≠Â∑≤ÊúâÁöÑÁü•ËØÜÔºåÊòØÂÖàÈ™åÂàÜÂ∏É„ÄÇ\nP(X|Œ∏) Âíå P(X) ÈÉΩÊòØ X ÁöÑÊ¶ÇÁéáÔºå‰∏çËøáÂâçËÄÖÊòØÂú®ÂèÇÊï∞‰∏∫ Œ∏ Êó∂ÔºåX ÂèëÁîüÁöÑÊ¶ÇÁéáÔºåÂêéËÄÖÊòØÂêÑÁßçÂèÇÊï∞ÂæóÂà∞ X ÁöÑÊ¶ÇÁéá‰πãÂíåÔºåÂÆÉ‰ª¨ÁöÑÊØîÂÄºÊòØÂÖàÈ™åÂàÜÂ∏É P(Œ∏) ÁöÑÁΩÆ‰ø°Â∫¶„ÄÇ\n‰ªéÂÖàÈ™åÂàÜÂ∏ÉP(Œ∏)Âá∫ÂèëÔºåÁÆóÂá∫ÂÆûÈ™åÁªìÊûú X Âú®ÂèÇÊï∞‰∏∫ Œ∏ Êó∂ÂèëÁîüÁöÑÊ¶ÇÁéáÔºåÁÑ∂ÂêéÈô§‰ª• X Âú®ÂêÑÁßçŒ∏Êó∂ÂèëÁîüÁöÑÊ¶ÇÁéá‰πãÂíåÔºåÂÅö‰∏∫Á≥ªÊï∞ÔºåÂØπÂÖàÈ™åÂàÜÂ∏ÉÂÅö‰øÆÊ≠£„ÄÇ\nP(X) ÊòØÂØπÊâÄÊúâÂèØËÉΩÁöÑ Œ∏ ÂÅöÁßØÂàÜÔºà‰πüÁß∞ËæπÁºòÊ¶ÇÁéáÔºâÔºåÊØîÂ¶ÇÂØπ‰∫éÊäõÁ°¨Â∏ÅÂÆûÈ™åÔºåÂ∞±Ë¶ÅÂØπÊ≠£-ÂèçÊ¶ÇÁéáÔºö‰ªé (Ê≠£:0, Âèç:1) Âà∞ (Ê≠£:1, Âèç:0) ‰πãÈó¥ÁöÑÊâÄÊúâÊÉÖÂÜµÔºåÊ±ÇÂá∫XÂèëÁîüÁöÑÊ¶ÇÁéáÂÜçÁßØÂàÜÔºàÊòØ‰∏™Â∏∏Êï∞Ôºâ„ÄÇÊó†Ê≥ïÁõ¥Êé•Ê±ÇÔºåÂèØ‰ª•Áî®ËíôÁâπÂç°Ê¥õÊñπÊ≥ïËøë‰ºº„ÄÇ ‰∏çËøáÔºåÊÉ≥Ê±ÇÊ¶ÇÁéáÊúÄÂ§ßÊó∂ÁöÑ Œ∏ÔºåÂπ∂‰∏çÂøÖÊ±ÇÂá∫Ê¶ÇÁéáÊúÄÂ§ßÊòØÂ§öÂ∞ë„ÄÇÂõ†‰∏∫ P(X) ‰∏é Œ∏ Êó†ÂÖ≥ÔºåÊâÄ‰ª• P(Œ∏|X) ‚àù P(X|Œ∏) P(Œ∏)ÔºåÂè™ÈúÄÊ±Ç‰ººÁÑ∂Â∫¶‰πò‰ª•ÂÖàÈ™åÂàÜÂ∏É‰πòÁßØÊúÄÂ§ßÊó∂ÔºåÂØπÂ∫îÁöÑ Œ∏ Â∞±Ë°å„ÄÇ\nMLE ËÆ§ÂÆöÔºöŒ∏ = arg max_Œ∏ P(X|Œ∏)Ôºõ\nMAP ËÆ§ÂÆöÔºöŒ∏ = arg max_Œ∏ (P(X|Œ∏)‚ãÖP(Œ∏))\n(MLEÂíåMAPÁöÑÊ±ÇËß£Êé®ÂØº ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°/ÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°‚ÄîÈÄöËøáÊäõÁ°¨Â∏Å‰æãÂ≠êÁêÜËß£)\n‰ª£ÂÖ•Á•ûÁªèÁΩëÁªú‰∏≠ÁöÑÁ¨¶Âè∑Ôºö\nMLE: W = arg max_W P(X,Y|W) ‚àù arg max_W log P(X,Y|W)ÔºåËøôÂ∞±ÊòØÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÊçüÂ§±ÂáΩÊï∞Ôºà‚ÄúÊòØÁå´‚ÄùÁöÑÊ¶ÇÁéáÔºâ\nMAP: W = arg max_W P(W|X,Y) ‚àù arg max_W (log P(X,Y|W) + log P(W))ÔºåÊçüÂ§±ÂáΩÊï∞Áõ∏ÊØî MLE Â§ö‰∫ÜÂÖàÈ™åÂàÜÂ∏É P(W)\nÁêÜËÆ∫‰∏äÂÖàÈ™åÂàÜÂ∏ÉÂèØ‰ª•‰ªªÊÑèÈÄâÊã©ÔºåÂè™Ë¶Å‰ΩøÁî®Â§ßÈáèÊï∞ÊçÆËø≠‰ª£Êó†ÈôêÊ¨°ÂêéÔºåÂêéÈ™åÂàÜÂ∏ÉÁöÑÊúÄÂ§ßÊ¶ÇÁéáÂØπÂ∫îÁöÑÂ∞±ÊòØÁúüÂÆûŒ∏Ôºå‰ΩÜÂ¶ÇÊûúÊï∞ÊçÆÈáèÊúâÈôêÔºå‰∏çÂêåÁöÑÂÖàÈ™åÂàÜÂ∏É‰ºöÊî∂ÊïõÂà∞‰∏çÂêåÁöÑŒ∏„ÄÇ\nÂ¶ÇÊûúÈááÁî®Ê≠£ÊÄÅÂàÜÂ∏É w·µ¢~N(0,œÉ¬≤) ‰Ωú‰∏∫ÂÖàÈ™åÂàÜÂ∏É P(W)ÔºåÂàôÂÖ∂ÂØπÂ∫î L2 Ê≠£ÂàôÂåñÈ°πÔºöŒª||ùêñ||‚ÇÇ+CÔºö\n$$ log P(W) = log ‚àè_i \\frac{1}{œÉ\\sqrt{2œÄ}} e^{-\\frac{(w_i-0)¬≤}{2œÉ¬≤}} = -\\frac{1}{2œÉ¬≤}‚àë_i w_i¬≤ + C $$\nÂ¶ÇÊûúÈááÁî®ÊãâÊôÆÊãâÊñØÂàÜÂ∏É w·µ¢~Laplace(0,b) ‰Ωú‰∏∫ÂÖàÈ™åÂàÜÂ∏É P(W)ÔºåÂàôÂÖ∂ÂØπÂ∫î L1 Ê≠£ÂàôÂåñÈ°πÔºöŒª||ùêñ||‚ÇÅ+CÔºö\n$$ log P(W) = log ‚àè_i \\frac{1}{2b} e^{-\\frac{|w_i-0|}{b}} = -\\frac{1}{b}‚àë_i |w_i| + C $$\nÊ≠£ÊÄÅÂàÜÂ∏ÉÂíåÊãâÊôÆÊãâÊñØÂàÜÂ∏ÉÈÉΩÊòØÈôêÂÆö‰∫ÜÈ´òÁª¥Á©∫Èó¥‰∏≠ÁöÑÂêëÈáè ùêñ ÁöÑ‰ΩçÁΩÆÔºåL2Ê≠£ÂàôÂåñÁ∫¶Êùü‰∫ÜÂêëÈáèÊ®°ÈïøÔºàÂùêÊ†áÂπ≥ÊñπÂíåÂÜçÂºÄÊñπÔºâÊúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåL1Ê≠£ÂàôÂåñÁ∫¶Êùü‰∫Ü‰∏§ÂêëÈáèË∑ùÁ¶ªÔºàÂùêÊ†á‰πãÂ∑ÆÔºâÊúç‰ªéÊãâÊôÆÊãâÊñØÂàÜÂ∏É\nÂ¶ÇÊûúÂÖàÈ™åÂàÜÂ∏É P(W) ÊòØÂπ≥ÂùáÂàÜÂ∏ÉÔºåÊ¶ÇÁéáÊòØÂ∏∏Êï∞Ôºå‰∏éWÊó†ÂÖ≥ÔºåMAP Â∞±ÈÄÄÂåñÊàê‰∫Ü MLE\nMAP ÊØî MLE Â§ö‰∫Ü‰∏Ä‰∏™ÂÖàÈ™åÂàÜÂ∏ÉÔºåÂÖàÈ™åÂàÜÂ∏ÉÂ∞±ÊòØÊ≠£ÂàôÂåñÈ°π„ÄÇ\nË¥ùÂè∂ÊñØÂÖ¨ÂºèÊèèËø∞ÁöÑÊòØÔºöÁî®Êñ∞ÁöÑÂÆûÈ™åÁªìÊûúÂØπÂÖàÈ™åÂàÜÂ∏ÉÂÅö‰øÆÊ≠£ÔºåÂÖàÈ™åÂàÜÂ∏ÉÂ∞±ÊòØ‰ºòÂåñÁöÑËµ∑ÁÇπÔºå‰∏çÂêåÁöÑÂÖàÈ™åÂàÜÂ∏ÉÔºåÂØπÂ∫îÁöÑ‰ºòÂåñËµ∑ÁÇπ‰∏çÂêå.\nÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°ÂÉèÊòØ‰∏Ä‰∏™ÊçüÂ§±ÂáΩÊï∞ÁöÑÈõÜÂêàÔºåÈÄâÊã©‰∏çÂêåÁöÑÂÖàÈ™åÂàÜÂ∏ÉÔºåÁõ∏ÂΩì‰∫éÈÄâÊã©‰∫Ü‰∏çÂêåÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÂåÖÊã¨MLE, L1Ê≠£ÂàôÂåñÔºåL2Ê≠£ÂàôÂåñ„ÄÇ\nMAP ÊØî MLE Êõ¥Ê≠£Á°Æ Êúâ‰∏Ä‰∏™‰∫∫ÔºåÂ•≥ÊÄßÔºå27Â≤ÅÔºå985Á°ïÂ£´ÊØï‰∏öÔºåÂçïË∫´ÔºåÂú®‰∏äÊµ∑ÁîüÊ¥ªÔºåÂπ≥Êó∂ÂñúÊ¨¢Ë°®ËææÔºåÂπΩÈªòÔºåÊúâ‰∫õÁêÜÊÉ≥‰∏ª‰πâÔºåÂÖ≥ÂøÉÂ∞ëÊï∞‰∫∫Áæ§ÔºåÁªèÂ∏∏Âú®ÁΩë‰∏äÂèëË°®ÁäÄÂà©Ë®ÄËÆ∫ÔºåÈóÆËøô‰∏™‰∫∫Êõ¥ÊúâÂèØËÉΩÊòØ‰∏Ä‰∏™ËÑ±Âè£ÁßÄÊºîÂëòÂë¢Ôºü ËøòÊòØÊõ¥ÊúâÂèØËÉΩÊòØ‰∏Ä‰∏™Â•≥ÊÄß‰∏ª‰πâÁöÑËÑ±Âè£ÁßÄÊºîÂëòÔºü\nÁõ¥ËßâÈÄâÊã©ÂêéËÄÖÔºå‰ΩÜÂÖ∂ÂÆûÂâçËÄÖÊ¶ÇÁéáÊõ¥Â§ßÔºåÂõ†‰∏∫‚ÄúËÑ±Âè£ÁßÄÊºîÂëò‚ÄùÂåÖÂê´ÁöÑËåÉÂõ¥Êõ¥Â§ß„ÄÇ\n‰∫∫ÁöÑÁõ¥ËßâËøêÁî®‰∫ÜÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºåÈÄâÊã©‚ÄúÂ•≥ÊÄß‰∏ª‰πâÁöÑËÑ±Âè£ÁßÄÊºîÂëò‚Äù Êõ¥ÂèØËÉΩÂá∫Áé∞ÊèèËø∞ÁöÑÈÇ£‰∫õÁâπË¥®„ÄÇ Â¶ÇÊûúËøêÁî®ÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°ÔºåÊåâÁÖßÊ¶ÇÁéá‰πòÊ≥ïÂç≥ÂèØËÆ°ÁÆóÂá∫Âì™‰∏™ÊÉÖÂÜµÊ¶ÇÁéáËæÉÂ§ß„ÄÇ\nÁªôÂÆöÁâπÂæÅÈõÜÂêà D={Â•≥ÊÄßÔºå27Â≤ÅÔºå985Á°ïÂ£´ÊØï‰∏öÔºå\u0026hellip;, ÂèëË°®ÁäÄÂà©Ë®ÄËÆ∫}Ôºå ÁªìËÆ∫ÈõÜÂêà T = {t‚ÇÅ=p‚ÇÅ, t‚ÇÇ=p‚ÇÅ^p‚ÇÇ}ÔºåÂÖ∂‰∏≠ p‚ÇÅ ÊòØËÑ±Âè£ÁßÄÊºîÂëòÔºåp‚ÇÇ ÊòØÂ•≥ÊÄß‰∏ª‰πâËÑ±Âè£ÁßÄÊºîÂëò\nÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°Ôºö T = arg max_T P(D|T)\nÂΩì T = t‚ÇÇ Êó∂ÔºöP(D|T=t‚ÇÇ) = P(D|p‚ÇÅ^p‚ÇÇ) = P(p‚ÇÅ^p‚ÇÇ|D) P(D) / P(p‚ÇÅ^p‚ÇÇ) = P(p‚ÇÅ,p‚ÇÇ,D)/P(p‚ÇÅ,p‚ÇÇ) = P(p‚ÇÇ|p‚ÇÅ,D) P(D|p‚ÇÅ) P(p‚ÇÅ) / (P(p‚ÇÅ|p‚ÇÇ)P(p‚ÇÅ)) = P(p‚ÇÇ|p‚ÇÅ,D) P(D|p‚ÇÅ) / P(p‚ÇÅ|p‚ÇÇ) = (P(p‚ÇÇ|p‚ÇÅ,D)/P(p‚ÇÅ|p‚ÇÇ)) P(D|T=t‚ÇÅ)\nP(D|T=t‚ÇÇ) ‰∏é P(D|T=t‚ÇÅ) ‰πãÈó¥Áõ∏Â∑Æ‰∏Ä‰∏™Â§ß‰∫é1ÁöÑÁ≥ªÊï∞ÔºåÊâÄ‰ª•ÂΩì T=t‚ÇÇ Êó∂‰ººÁÑ∂ÂÄºÊõ¥Â§ß\nÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°Ôºö T = arg max_T P(T|D)\nÂΩì T = t‚ÇÇ Êó∂ÔºöP(T=t‚ÇÇ|D) = P(p‚ÇÅ^p‚ÇÇ|D) = P(D|p‚ÇÅ^p‚ÇÇ) P(p‚ÇÅ^p‚ÇÇ) / P(D) = P(p‚ÇÅ,p‚ÇÇ,D) / P(D) = P(p‚ÇÇ|p‚ÇÅ,D) P(p‚ÇÅ|D) P(D)/ P(D) = P(p‚ÇÇ|p‚ÇÅ,D) P(p‚ÇÅ|D) = P(p‚ÇÇ|p‚ÇÅ,D) P(T=t‚ÇÅ|D)\nP(T=t‚ÇÇ|D) ‰∏é P(T=t‚ÇÅ|D) ‰πãÈó¥Áõ∏Â∑Æ‰∏Ä‰∏™Â∞è‰∫é1ÁöÑÁ≥ªÊï∞ÔºåÊâÄ‰ª•ÂΩì T=t‚ÇÅ Êó∂ÂêéÈ™åÊ¶ÇÁéáÊõ¥Â§ß\nÊâÄ‰ª•Áî®Ë¥ùÂè∂ÊñØÊñπÂºèÊÄùËÄÉÊõ¥ÁêÜÊÄß\nÁî®Ë¥ùÂè∂ÊñØÁêÜËß£Ê¶ÇÁéá Ê¶ÇÁéáÁöÑ‰∏§ÁßçÁêÜËß£ÔºöÈ¢ëÁéáÊ¥æÂíåË¥ùÂè∂ÊñØÊ¥æ\nÈ¢ëÁéáÊ¥æËÆ§‰∏∫Ê¶ÇÁéáÊòØÊüê‰ª∂‰∫ãÂ§öÊ¨°ÂèëÁîüÁöÑÈ¢ëÁéáÔºåÊäõÁ°¨Â∏ÅÂèØ‰ª•Â§öÊ¨°ÈáçÂ§çÔºå‰ΩÜ‰∏çÈÄÇÂêàËß£ÈáäÁ•ûÁªèÁΩëÁªúÔºå(Â§öÂàÜÁ±ª)Á•ûÁªèÁΩëÁªúËï¥Âê´ÁöÑÊ®°Âûã‰∏é‰∫∫ËÑë‰∏≠ÁöÑÊ®°ÂûãÁöÑÂ∑ÆÂºÇÊòØÊçüÂ§±ÂáΩÊï∞ÔºåË¢´ÁúãÂÅö‰∏∫‰∏Ä‰∏™Ê¶ÇÁéáÔºà‚ÄúÊòØÁå´ÁöÑÊ¶ÇÁéá‚ÄùÔºâ„ÄÇËøô‰∏™Ê¶ÇÁéáÊòØÊòØsoftmax ÂØπÂêÑÁ±ªÂà´‰∏äÁöÑ activation z ÂÅöÂΩí‰∏ÄÂåñÂêéÂæóÂà∞ÁöÑÔºåÊòØÊ†∑Êú¨‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºå‰∏çÂ•ΩÁî®È¢ëÁéáËß£Èáä„ÄÇ\nË¥ùÂè∂ÊñØÂÖ¨Âºè‰∏≠Áî®ÁöÑÈÉΩÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÔºåÂÖàÈ™åÊ¶ÇÁéá P(Œ∏) ÁêÜËß£ÊàêÂú®Ê≤°ÊúâÊï∞ÊçÆÂèØ‰æõÂèÇËÄÉÊó∂ÔºåÂØπÁ•ûÁªèÁΩëÁªúÂèÇÊï∞ÁöÑÁõ∏‰ø°Á®ãÂ∫¶ÔºåÁõ∏‰ø°Á®ãÂ∫¶ÂèØ‰ª•ÊòØÊ≠£ÊÄÅÂàÜÂ∏ÉÔºõÂêéÈ™åÊ¶ÇÁéáÊòØÂ∑≤Áü•Êï∞ÊçÆX‰πãÂêéÔºåÁªèËøá‰øÆÊ≠£ÁöÑÂØπÁΩëÁªúÂèÇÊï∞ÁöÑÁõ∏‰ø°Á®ãÂ∫¶„ÄÇŒ∏ÂàÜÂ∏É‰øÆÊ≠£ÂêéÔºå‰ººÁÑ∂Â∫¶Â∞±ÂèëÁîüÂèòÂåñÔºåÂú®ÈÖçÂàÜÂáΩÊï∞‰∏≠ÁöÑÂç†ÊØîÂ∞±ÂèòÂåñÔºå‰ªéËÄåÁªßÁª≠‰øÆÊ≠£ÂàÜÂ∏ÉÔºåÂΩìÊüê‰∏™ÂàÜÂ∏ÉÁöÑ‰ººÁÑ∂Â∫¶Âú®ÈÖçÂàÜÂáΩÊï∞‰∏≠Âç†ÊØîÊúÄÂ§ßÊó∂ÔºåÂÆÉÂ∞±ÊòØÊúÄ‰ºòÁöÑ Œ∏ ÂàÜÂ∏ÉÔºåÂÖ∂ÂêéÈ™åÊ¶ÇÁéáÊúÄÂ§ßÔºåÂÆÉÂèëÁîüÁöÑÂèØËÉΩÊÄßÊúÄÂ§ßÔºåÂØπÂÆÉÁöÑÁõ∏‰ø°Á®ãÂ∫¶ÊúÄÈ´ò„ÄÇ\nÂàÜÊØç P(X) ÊòØËæπÁºòÊ¶ÇÁéáÔºå‰πüÂè´ÈÖçÂàÜÂáΩÊï∞ÔºåÂ∞ÜÂÖ∂ÊåâÊù°‰ª∂Ê¶ÇÁéáÂ±ïÂºÄÔºö\n$$ P(Œ∏_i | X) = \\frac{P(X|Œ∏_i) P(Œ∏_i)}{‚àë P(X|Œ∏) P(Œ∏)} = \\frac{P(X|Œ∏_i) P(Œ∏_i)}{\\int_Œ∏ P(X|Œ∏) P(Œ∏) dŒ∏}\n= \\frac{P(X|Œ∏_i) P(Œ∏_i)}{P(X|Œ∏_i) P(Œ∏_i) + ‚àë_{k‚â†i} P(X|Œ∏_k) P(Œ∏‚Çñ) dŒ∏} $$\nÂàÜÂ≠êÊòØÂÖ∑‰ΩìÁöÑ‰∏Ä‰∏™ Œ∏ÔºåÂàÜÊØçÊòØÊääÊâÄÊúâÂèØËÉΩÁöÑ Œ∏ ÂÖ®ÈÉ®Âèñ‰∏ÄÈÅçÂπ∂Áõ∏Âä†ÔºåÂ¶ÇÊûú Œ∏ ÊòØËøûÁª≠ÁöÑÔºåÂ∞±ÊòØÁßØÂàÜÁöÑËøáÁ®ã„ÄÇ\nÁΩÆ‰ø°Â∫¶ÊòØ [0,1] ‰πãÈó¥ÁöÑÊï∞Ôºå‰ΩÜ‰∏çÂ•ΩÊ±ÇÔºå‰ººÁÑ∂ÂÄºÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊ±ÇÔºåÈÖçÂàÜÂáΩÊï∞Áî®ËíôÁâπÂç°Ê¥õÊñπÊ≥ïÊ±Ç\nÁî®Ë¥ùÂè∂ÊñØÁêÜËß£Ê¢ØÂ∫¶‰∏ãÈôç ÊâæÊúÄ‰ºòÂèÇÊï∞ÁöÑËøáÁ®ãÊãÜËß£Êàê‰∏Ä‰∏™Â∫èÂàóÔºö P(Œ∏|X) = P(s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô|X)Ôºõ ÁªôÂÆöÊï∞ÊçÆÈõÜ XÔºå ÂÖàÂæóÂà∞ s‚ÇÅÔºå‰∏ãÈôç‰∏ÄÊ¨°ÂæóÂà∞ s‚ÇÇÔºåÂÜçÂæóÂà∞ s‚ÇÉÔºå‰∏ãÈôçnÊ¨°Âà∞Ëææ s‚Çô\nÁî®Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÂ±ïÂºÄÔºö\nP(Œ∏|X) = P(s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô|X) = P(X|s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô) P(s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô) / P(X) = P(s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô, X)/P(X) = P(s‚Çô | s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô‚Çã‚ÇÅ, X) P(s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô‚Çã‚ÇÅ, X)/P(X)\n= P(s‚Çô | s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô‚Çã‚ÇÅ, X) P(s‚Çô‚Çã‚ÇÅ| s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô‚Çã‚ÇÇ, X) \u0026hellip;. P(s‚ÇÉ|s‚ÇÅ,s‚ÇÇ,X) P(s‚ÇÇ|s‚ÇÅ,X) P(s‚ÇÅ|X) P(X)/P(X)\n= P(s‚Çô | s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô‚Çã‚ÇÅ, X) P(s‚Çô‚Çã‚ÇÅ| s‚ÇÅ s‚ÇÇ s‚ÇÉ \u0026hellip; s‚Çô‚Çã‚ÇÇ, X) \u0026hellip;. P(s‚ÇÉ|s‚ÇÅ,s‚ÇÇ,X) P(s‚ÇÇ|s‚ÇÅ,X) P(s‚ÇÅ|X)\n‰ªéÂè≥ÂêëÂ∑¶Ê±Ç„ÄÇ\nÂÅáËÆæ‰∏≠Èó¥ÁöÑÊØè‰∏ÄÊ≠•Âπ∂‰∏ç‰æùËµñÂâçÈù¢ÁöÑÊâÄÊúâÁªìÊûúÔºåÂè™‰æùËµñ‰∫éÂÆÉÂâçÈù¢‰∏ÄÊ≠•ÁöÑÁªìÊûú:\nP(Œ∏|X) = P(s‚Çô | s‚Çô‚Çã‚ÇÅ, X) P(s‚Çô‚Çã‚ÇÅ| s‚Çô‚Çã‚ÇÇ, X) \u0026hellip;. P(s‚ÇÉ|s‚ÇÇ,X) P(s‚ÇÇ|s‚ÇÅ,X) P(s‚ÇÅ|X)\nË¶ÅÊ±Ç P(Œ∏|X) ÁöÑÊúÄÂ§ßÂÄºÔºåËÆ°ÁÆóÈáèËøòÊòØÂ§™Â§ßÔºå‰∏çÊòØËÄÉËôëÊï¥‰Ωì‰πòÁßØËææÂà∞ÊúÄÂ§ßÔºåÂè™ËÄÉËôëÂ±ÄÈÉ®ÊØè‰∏ÄÂ∞èËäÇÔºå‰∏ã‰∏ÄÊ≠•Á≠â‰∫é‰∏ä‰∏ÄÊ≠•ÁöÑÊúÄÂ§ßÂÄº:\n$$s_{i+1} = arg max_{s_{i+1}} P(s_{i+1} | s_i, X)$$\n‰ªéÁ¨¨‰∏ÄÈ°πÂà∞ÊúÄÂêé‰∏ÄÈ°π‰æùÊ¨°ÈÉΩÂèñÊúÄÂ§ßÔºåÊúÄÂêéÁöÑÁªìÊûú‰∏ç‰∏ÄÂÆöÊòØÊï¥‰ΩìÊúÄÂ§ßÂÄºÔºå‰ΩÜ‰πüËÉΩÂæóÂà∞‰∏Ä‰∏™Ëøë‰ººÁöÑÁªìÊûú„ÄÇ‰∏äÂºèÂ∞±ÊòØÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°Ôºå ÊçüÂ§±ÂáΩÊï∞ÊòØÔºöP(s·µ¢‚Çä‚ÇÅ | s·µ¢, X)Ôºå ÂàôÊ¢ØÂ∫¶‰∏ãÈôç‰∏∫Ôºö s·µ¢‚Çä‚ÇÅ = s·µ¢ + Œ∑ ‚ãÖ ‚àá P(s·µ¢‚Çä‚ÇÅ | s·µ¢, X)\n","date":"2022-12-16T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/13_l1l2%E6%AD%A3%E5%88%99%E5%8C%963-%E8%B4%9D%E5%8F%B6%E6%96%AF/","title":"watch: DL - ÁéãÊú®Â§¥ 13 | L1L2 Reg (3), Bayesian Probability"},{"content":"1. ÁÆóÊ≥ïÊî∂ÊïõÊÄßËØÅÊòé Source video: P1\nEM \u0026ldquo;ÊúüÊúõÊúÄÂ§ß\u0026rdquo; Áî®‰∫éËß£ÂÜ≥ÂÖ∑ÊúâÈöêÂèòÈáèÁöÑÊ∑∑ÂêàÊ®°ÂûãÁöÑÂèÇÊï∞‰º∞ËÆ°ÔºåËß£ÂÜ≥ÊûÅÂ§ß‰ººÁÑ∂ÈóÆÈ¢ò„ÄÇ ÂØπ‰∫éÁÆÄÂçïÊ®°ÂûãÁöÑÂèÇÊï∞‰º∞ËÆ°ÈóÆÈ¢òÔºåËß£ÊûêËß£ÂèØ‰ª•Áõ¥Êé•Ê±ÇÂØºÂæóÂà∞ÔºåÂæó‰∏çÂà∞Ëß£ÊûêËß£ÁöÑÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊàñEMÊ±ÇÊï∞ÂÄºËß£„ÄÇ\nX: random variable, observed data X={x‚ÇÅ,x‚ÇÇ,..x‚Çô} ÂêÑÊ†∑Êú¨Áã¨Á´ãÂêåÂàÜÂ∏Éiid Œ∏: all parameters log P(X|Œ∏): log-likelihood, ÂØπÊï∞‰ººÁÑ∂ Áî®ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°Ôºö Œ∏_MLE = arg max_Œ∏ P(X|Œ∏) ‚ûî arg max log P(X|Œ∏)\nÂØπ‰∫éÂê´ÊúâÈöêÂèòÈáèÁöÑÊ∑∑ÂêàÊ®°ÂûãÔºåÁõ¥Êé•Ê±ÇËß£ÊûêËß£ÈùûÂ∏∏Âõ∞ÈöæÔºåÊØîÂ¶ÇÂú® GMM ÔºàÈ´òÊñØÊ∑∑ÂêàÊ®°ÂûãÔºâ‰∏≠ÔºåÂ∞±ÂæàÈöæÂÜô\nEMËø≠‰ª£ÂÖ¨Âºè Z: Latent variable ÈöêÂèòÈáè‰πüÊòØÈöèÊú∫ÂèòÈáèÔºå‰∏çÂêåÂÄºÊúâ‰∏çÂêåÁöÑÂá∫Áé∞Ê¶ÇÁéá (X,Z): Complete data ÂÆåÊï¥Êï∞ÊçÆ Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ: t+1 Êó∂ÂàªÁöÑÂèÇÊï∞ P(Z|X,Œ∏): ZÂá∫Áé∞ÁöÑÂêéÈ™åÊ¶ÇÁéá P(X,Z): X Âíå Z ÂêåÊó∂ÂèëÁîüÁöÑËÅîÂêàÊ¶ÇÁéá„ÄÇ log P(X,Z|Œ∏): ÂØπÊï∞ËÅîÂêàÊ¶ÇÁéáÔºåÂØπÊï∞ÂÆåÂÖ®Êï∞ÊçÆ Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = arg max_Œ∏ ‚à´_Z log P(X,Z | Œ∏) ‚ãÖ P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) dZÔºåÔºàÂæàÂÉèÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ° log P(X|Œ∏)ÔºåÂè™ÊòØËøôÈáåÁöÑÊï∞ÊçÆÈô§‰∫ÜXËøòÊúâÈöêÂèòÈáèZÔºå‰∏çÁî®Ê¢ØÂ∫¶‰∏ãÈôçËÄåÁî®Ëø≠‰ª£Êõ¥Êñ∞ÂèÇÊï∞Ôºâ\nÂèØ‰ª•Áúã‰ΩúÊòØÊåâÁÖß Z ÁöÑÂêéÈ™åÂàÜÂ∏É (Z|X,Œ∏‚ÅΩ·µó‚Åæ) Ê±ÇÂØπÊï∞ËÅîÂêàÊ¶ÇÁéáÁöÑÊúüÊúõ ÔºàÊúüÊúõÁöÑÂÆö‰πâ-wikiÔºâÔºö\nÁ¨¨‰∏ÄÊ≠• ExpectationÔºöÊ±ÇÂØπÊï∞ÂÆåÂÖ®Êï∞ÊçÆ log P(X,Z|Œ∏) ‰ª•ÂêéÈ™å P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) ‰∏∫Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÁöÑÊúüÊúõ:\n$E_{Z|X,Œ∏‚ÅΩ·µó‚Åæ}[log P(X,Z|Œ∏)]$\nÁ¨¨‰∫åÊ≠• MaximizationÔºöÊâæÂà∞ÊúüÊúõÊúÄÂ§ßÊó∂ÂØπÂ∫îÁöÑÂèÇÊï∞‰Ωú‰∏∫‰∏ã‰∏ÄÊó∂ÂàªÁöÑÂèÇÊï∞: $\\rm Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = arg\\ max_Œ∏ E_{P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)} [log P(X,Z|Œ∏)]$\nÊî∂ÊïõÊÄßËØÅÊòé ÔºàÈùû‰∏•Ê†ºÔºâÂè™ËØÅÊòé‰∏ÄÊ≠•Ôºå‰ªé Œ∏‚ÅΩ·µó‚Åæ ‚ûî Œ∏‚ÅΩ·µó‚Å∫¬π‚ÅæÔºå‰ººÁÑ∂‰ºöÂ¢ûÂ§ßÔºölog P(X|Œ∏‚ÅΩ·µó‚Åæ) ‚â§ log P(X|Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ)Ôºå‰∫ã‰ª∂ X Âú®Êñ∞‰∏ÄÊó∂ÂàªÁöÑÂèÇÊï∞ Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ ÊâÄ‰ª£Ë°®ÁöÑÊ¶ÇÁéáÊ®°Âûã‰∏ãÔºåÂèëÁîüÁöÑÂèØËÉΩÊÄßÂèòÂ§ß‰∫ÜÔºå‰∏çÊñ≠Ëø≠‰ª£ÊúÄÂêéÂèØÂèñÂæóÊúÄÂ§ß log-likelihood ÂØπÂ∫îÁöÑÂèÇÊï∞„ÄÇ\nÂÆåÂÖ®Êï∞ÊçÆÁöÑ‰ººÁÑ∂ÔºöP(X,Z|Œ∏) = P(Z|X,Œ∏) P(X|Œ∏)ÔºåŒ∏ÊòØËµ∑ÂßãÂÄºÂèØ‰ª•‰ªªÊÑèÂèñ\nÂèñÂØπÊï∞Ôºölog P(X|Œ∏) = log P(X,Z|Œ∏) - log P(Z|X,Œ∏)\n‰∏§ËæπÂÖ≥‰∫é Z ÁöÑÂêéÈ™åÂàÜÂ∏ÉÊ±ÇÁßØÂàÜÔºàÊ±ÇÂØπÊï∞‰ººÁÑ∂ÁöÑÊúüÊúõÔºâÔºö\nÂ∑¶ËæπÔºö\n‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog P(X|Œ∏) dZ = log P(X|Œ∏) ‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) dZ Ôºà‰ººÁÑ∂‰∏éZÊó†ÂÖ≥ÊèêÂà∞ÁßØÂàÜÂ§ñÈù¢Ôºâ\n= log P(X|Œ∏) ÔºàÂØπZÁöÑÊ¶ÇÁéáÁßØÂàÜ‰∏∫1Ôºâ\nÂè≥ËæπÔºö\n‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog P(X,Z|Œ∏) dz - ‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog P(Z|X,Œ∏) dz = Q(Œ∏, Œ∏‚ÅΩ·µó‚Åæ) - H(Œ∏,Œ∏‚ÅΩ·µó‚Åæ)ÔºåÂàÜÂà´ÂàÜÊûê‰∏§È°π\nQ Â≠òÂú®‰∫éEMËø≠‰ª£ÂÖ¨Âºè‰∏≠ÔºåÊ†πÊçÆEM ÁöÑÂÆö‰πâÔºöŒ∏‚ÅΩ·µó‚Å∫¬π‚Åæ ÂØπÂ∫îÊúÄÂ§ßÁöÑQÔºåÊâÄ‰ª•Ôºö Q(Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ, Œ∏‚ÅΩ·µó‚Åæ) ‚â• Q(Œ∏, Œ∏‚ÅΩ·µó‚Åæ) ÊòØÊàêÁ´ãÁöÑ„ÄÇ ‰∏äÂºèÁöÑŒ∏ÊòØÂàùÂßãÂÄºÔºåËã•‰ª§Œ∏ = Œ∏‚ÅΩ·µó‚ÅæÔºåÂàô Q(Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ, Œ∏‚ÅΩ·µó‚Åæ) ‚â• Q(Œ∏‚ÅΩ·µó‚Åæ, Œ∏‚ÅΩ·µó‚Åæ) ÂæóËØÅ\nÂõ†‰∏∫ H ÂâçÈù¢Êúâ‰∏™Ë¥üÂè∑ÔºåÊâÄ‰ª•Ë¶ÅËØÅ H(Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ, Œ∏‚ÅΩ·µó‚Åæ) ‚â§ H(Œ∏‚ÅΩ·µó‚Åæ, Œ∏‚ÅΩ·µó‚Åæ)\nÂêé - ÂâçÊó∂ÂàªÁöÑ HÔºö\nH(Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ, Œ∏‚ÅΩ·µó‚Åæ) - H(Œ∏‚ÅΩ·µó‚Åæ, Œ∏‚ÅΩ·µó‚Åæ) = ‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog P(Z|X,Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ) dz - ‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) dz = ‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog (P(Z|X,Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ / P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)) dz ÔºàÂêàÂπ∂Ôºâ = - KL( P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) || P(Z|X,Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ)) ÔºàÁõ∏ÂØπÁÜµ‚â•0Ôºâ ‚â§ 0\n(Âè¶‰∏ÄÁßçËØÅÊ≥ï) Áî®Jensen‰∏çÁ≠âÂºèÔºåËÄå‰∏ç‰ΩøÁî®Áõ∏ÂØπÁÜµ„ÄÇ\nÂõ†‰∏∫logÊòØ concave ÔºàÂáπÔºâÂáΩÊï∞ÔºöÂú®Êõ≤Á∫ø‰∏ä‰ªªÊÑèÂèñ‰∏§‰∏™ÁÇπÔºåËøûÊàêÁöÑÁõ¥Á∫øÂ∞è‰∫élogÂáΩÊï∞„ÄÇconcaveÂáΩÊï∞ÁöÑÊÄßË¥®Ôºöa,b‰∏§ÁÇπ‰πãÈó¥ÁöÑ‰∏ÄÁÇπcÔºà‰∏§Á´ØÁÇπÁöÑÁ∫øÊÄßÁªÑÂêàÔºâÂØπÂ∫îÂà∞Áõ¥Á∫øÂáΩÊï∞‰∏äÁöÑÂÄº‰∏ÄÂÆöÂ∞è‰∫éÂØπÂ∫îÂà∞logÂáΩÊï∞‰∏äÁöÑÂÄº\n‰∏äÈù¢ÁöÑÂêàÂπ∂‰∏∫ 1‰∏™log ‰πãÂêéÁöÑÂºèÂ≠êÔºåÂèØ‰ª•Áúã‰ΩúÊòØÂÖàÊ±ÇZÁöÑÂêéÈ™åÁöÑÂØπÊï∞ÔºåÂÜçÊ±Ç\u0026quot;ÂØπÊï∞ÂÄº\u0026quot;ÁöÑÊúüÊúõÔºå‰∏ÄÂÆöÂ∞è‰∫éÂÖàÊ±ÇÔºàËá™ÂèòÈáè\u0026quot;ZÁöÑÂêéÈ™å\u0026quot;ÁöÑÔºâÊúüÊúõÂÜçÊ±ÇÂØπÊï∞ÔºöÔºàE[log x] ‚â§ log E[x]Ôºâ\n‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog (P(Z|X,Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ / P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)) dz ‚â§ log ‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖ(P(Z|X,Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ / P(Z|X,Œ∏‚ÅΩ·µó‚Åæ))) = log ‚à´z (P(Z|X,Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ) dz = log 1 = 0\n2. Ëø≠‰ª£ÂÖ¨ÂºèÁöÑÂØºÂá∫ Source video: P2\nEM Ëø≠‰ª£ÂÖ¨ÂºèÊù•Ëá™‰∫éÂ∞ÜÂØπÊï∞‰ººÁÑ∂ logP(X|Œ∏) ÂàÜËß£‰∏∫ ELBO + KLÊï£Â∫¶„ÄÇ\nÂºïÂÖ•ÂèòÈáèZÔºåÂàô P(X) Â∞±ÂèòÊàê‰∫ÜËÅîÂêàÊ¶ÇÁéáÔºöP(X,Z|Œ∏) = P(Z|X,Œ∏)‚ãÖP(X|Œ∏)Ôºå ÊâÄ‰ª•ÂØπÊï∞‰ººÁÑ∂Â∞±Âèò‰∏∫Ôºö\nlogP(X|Œ∏) = log P(X,Z|Œ∏) - log P(Z|X,Œ∏)\nÂºïÂÖ• Z ÁöÑÊ¶ÇÁéáÂàÜÂ∏É q(Z): logP(X|Œ∏) = log P(X,Z|Œ∏) - log q(Z) - log P(Z|X,Œ∏) + log q(Z) = log (P(X,Z|Œ∏) / q(Z)) - log (P(Z|X,Œ∏) / q(Z))Ôºåq(Z)‚â†0\nÊäÄÂ∑ßÔºöÂØπÁ≠âÂºè‰∏§ËæπÔºåÊåâÁÖßÂàÜÂ∏É q(Z) Ê±Ç‰ººÁÑ∂ÁöÑÊúüÊúõÔºàÁßØÂàÜÔºâÔºö\nÂ∑¶ËæπÔºö\n‚à´z q(Z)‚ãÖlog P(X|Œ∏) dZ = log P(X|Œ∏)‚ãÖ‚à´z q(Z) dZ = log P(X|Œ∏)Ôºåq(Z)ÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÁßØÂàÜ‰∏∫1ÔºåÂõ†Ê≠§Â∑¶ËæπÊ±ÇÂÆåÊúüÊúõÊó†ÂèòÂåñÔºåÊâÄ‰ª•Ê±Ç‰ººÁÑ∂ logP(X|Œ∏) Â∞±ÂèòÊàê‰∫ÜÊ±Çq(Z)„ÄÇ\nÂè≥ËæπÔºö\n‚à´z q(Z)‚ãÖlog (P(X,Z|Œ∏)/q(Z)) dz - ‚à´z q(Z)‚ãÖlog (P(Z|X,Œ∏)/q(Z)) dz = ELBO + KL(q(Z) || P(Z|X,Œ∏)) ÂÖ∂‰∏≠Á¨¨1È°πÔºöEvidence Lower Bound (ELBO,‰∏ãÁïå)ÔºåÁ¨¨2È°πÊòØZÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞q(Z)‰∏éZÁöÑÂêéÈ™åÊ¶ÇÁéáP(Z|X,Œ∏)ÁöÑÁõ∏ÂØπÁÜµ„ÄÇ\nÂõ†‰∏∫ KLÊï£Â∫¶ÊÅí‚â•0ÔºåÊâÄ‰ª•Ê†∑Êú¨‰ººÁÑ∂ log P(X|Œ∏) ‚â• ELBO„ÄÇÂè™ÊúâÂΩì Z ÁöÑÊ¶ÇÁéáÂàÜÂ∏É q(Z) ‰∏é Z ÁöÑÂêéÈ™åÂàÜÂ∏É P(Z|X,Œ∏) Áõ∏Á≠âÊó∂ÔºåKLÊï£Â∫¶Á≠â‰∫é0Ôºå‰ººÁÑ∂=ELBO„ÄÇ\nELBO ÊòØ log (P(Z,X|Œ∏)/q(Z)) ÊåâÁÖß q(Z) Ê±ÇÊúüÊúõÔºàÂä†ÊùÉÂíå;Ê±ÇÁßØÂàÜÔºâ„ÄÇÂΩìÂØπÊï∞‰ººÁÑ∂=ELBOÔºåÂç≥ËææÂà∞ÊúÄÂ§ßÊó∂Ôºåq(Z)=P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)Ôºå‰πüÂ∞±ÊòØÂÖàÁî®‰∏ä‰∏ÄÊó∂ÂàªÁöÑŒ∏‚ÅΩ·µó‚Åæ Ê±ÇÂá∫q(Z)ÔºåÁÑ∂ÂêéELBOÈáåÂ∞±Âè™ÊúâlogÈáåÁöÑŒ∏ÊòØÂèòÈáèÔºåÊªëÂä®Ë∞ÉÊï¥Œ∏‰ΩøELBOÊúÄÂ§ßÔºåÂèñÂØπÂ∫îÁöÑŒ∏‰Ωú‰∏∫Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ„ÄÇ\nEM ÊÉ≥ËÆ© ELBO ËææÂà∞ÊúÄÂ§ßÔºåÂÖàÔºàÈÄöËøáKLÊï£Â∫¶=0ÔºâÊâæÂà∞Ê†∑Êú¨ÂØπÊï∞‰ººÁÑ∂ log P(X|Œ∏) ÁöÑÊúÄÂ§ßÂÄºÂØπÂ∫îÁöÑÂèÇÊï∞Œ∏Ôºå‰Ωú‰∏∫Êñ∞ÁöÑŒ∏ÔºåÁÑ∂ÂêéÂÜçÁÆóÂÆÉÂØπÂ∫îÁöÑÊúüÊúõÂπ∂ÂæóÂà∞ELBOÔºå ÂèñÂà∞ÊúÄÂ§ßÂÄºÊó∂ÁöÑÂΩ¢ÂºèÔºåÂç≥ ELBOÔºå ÁÑ∂ÂêéÂèñELBOÊúÄÂ§ßÊó∂ÂØπÂ∫îÁöÑÂèÇÊï∞Œ∏Ôºõ ‰∏çÊñ≠ÊèêÈ´ò‰∏ãÁïå‰ªéËÄåËÆ©ÂØπÊï∞‰ººÁÑ∂ log P(X|Œ∏) ‰πüÈÄêÊ∏êÂèòÂ§ßÔºåÊúÄÁªàELBOÁ≠â‰∫élogP(X|Œ∏)ÔºåKLÊï£Â∫¶=0ÔºåZÁöÑÊ¶ÇÁéáÂàÜÂ∏É‰∏éÂÆÉÁöÑÂêéÈ™åÂàÜÂ∏ÉÁõ∏Á≠âÔºå ÊúÄ‰ºòŒ∏^ ÊòØ ELBO ÂèñÊúÄÂ§ßÊó∂ÁöÑŒ∏\nŒ∏^ = arg max_Œ∏ ELBO = arg max_Œ∏ ‚à´z q(Z) ‚ãÖ log (P(X,Z|Œ∏) / q(Z)) dz Ôºå‰ª£Êç¢q(Z)\n= arg max_Œ∏ ‚à´_Z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ log (P(X,Z|Œ∏) / P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)) dZ\nËøô‰∏™ÂºèÂ≠ê‰∏é EM ÁöÑËø≠‰ª£ÂÖ¨ÂºèÁõ∏ÊØîÔºålog ÈáåÂ§ö‰∫Ü‰∏Ä‰∏™ÂàÜÊØçÔºöP(Z|X,Œ∏‚ÅΩ·µó‚Åæ)ÔºåÂ±ïÂºÄlogÔºö\nŒ∏^ = arg max_Œ∏ ‚à´_Z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ [log P(X,Z|Œ∏) - log P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)] dZ = arg max_Œ∏ ‚à´_Z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ log P(X,Z|Œ∏) dZ - P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ log P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)] dZ\nÂÖ∂‰∏≠Á¨¨2È°π‰∏éŒ∏Êó†ÂÖ≥ÔºåÂõ†‰∏∫ Œ∏‚ÅΩ·µó‚Åæ ÊòØ‰∏ä‰∏ÄÊó∂ÂàªÁöÑÂèÇÊï∞ÔºåÊòØ‰∏™Â∏∏Êï∞Ôºå‰∏çÊòØÂèòÈáèÔºåËÄå log ‰∏≠ÁöÑ Œ∏ ÊòØÂèòÈáèÔºå‰ºöÂèòÂà∞ELBO ÂèñÊúÄÂ§ßÊó∂ÂØπÂ∫îÁöÑÂèÇÊï∞„ÄÇÊâÄ‰ª•Â∞±ÂæóÂà∞‰∫ÜËø≠‰ª£ÂÖ¨ÂºèÔºö\nŒ∏^ = arg max_Œ∏ ‚à´_Z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ) ‚ãÖ log P(X,Z|Œ∏) dz\n3. ÂÖ¨ÂºèÂØºÂá∫‰πãELBO+Jensen\u0026rsquo;s Inequality Source video: P3\nEM Ëø≠‰ª£ÂÖ¨Âºè‰πüÂèØÊù•Ëá™‰∫éÂ∞ÜÂØπÊï∞‰ººÁÑ∂ logP(X|Œ∏)ÂàÜËß£‰∏∫ ELBO + Jensen\u0026rsquo;s Inequality\nJensen\u0026rsquo;s Inequality ÁªìËÆ∫ÔºöÂØπ‰∫é‰∏Ä‰∏™Âáπconcave ÂáΩÊï∞ f(x)ÔºåÂú®ÂÆö‰πâÂüüx‰∏äÂèñ‰∏§ÁÇπÔºöa,b ËøûÁ∫øÂ∞è‰∫éa,b‰πãÈó¥ÁöÑÂáΩÊï∞ÂÄº„ÄÇ ‰ªªÊÑèÂú® a,b ‰πãÈó¥Âèñ‰∏ÄÁÇπ c = t‚ãÖa+(1-t)b, where t‚àà[0,1]Ôºåf(c)=f(t‚ãÖa+(1-t)b) ‚â• t‚ãÖf(a) + (1-t)f(b)„ÄÇ\nÊØîÂ¶ÇÂΩì t=¬Ω Êó∂Ôºåf(a/2+b/2) ‚â• f(a)/2 + f(b)/2Ôºå‰∏§ËæπÈÉΩÊòØÊúüÊúõÔºàÂπ≥ÂùáÊï∞,Âä†ÊùÉÂíåÔºâÔºåÁÆÄËÆ∞‰∏∫:ÂÖàÊ±ÇÊúüÊúõÂÜçÊ±ÇÂáΩÊï∞ÂÄº Â§ß‰∫éÁ≠â‰∫é ÂÖàÊ±ÇÂáΩÊï∞ÂÄºÂÜçÊ±ÇÊúüÊúõÔºåf(E) ‚â• E[f]„ÄÇÂΩì f(x) ÊòØÂ∏∏ÂáΩÊï∞Êó∂ÔºåÁ≠âÂè∑ÊàêÁ´ã„ÄÇ\nlog P(X|Œ∏) = log ‚à´zP(X,Z|Œ∏) dzÔºåÂú®‰ººÁÑ∂‰∏≠ÂºïÂÖ•ÈöêÂèòÈáè ZÔºåÁÑ∂ÂêéÊ±ÇXÁöÑËæπÁºòÊ¶ÇÁéáÔºåÂç≥ÂØπZÊ±ÇÁßØÂàÜÔºà‚ÄúÊääÊúÄÁªàÁªìÊûú‰∏≠‰∏çÈúÄË¶ÅÁöÑ‰∫ã‰ª∂ÂêàÂπ∂ÊàêÂÖ∂‰∫ã‰ª∂ÁöÑÂÖ®Ê¶ÇÁéáËÄåÊ∂àÂ§±‚Äù marginal probability-wikiÔºâ\n= log ‚à´z ( P(X,Z|Œ∏) / q(Z)) * q(Z) dzÔºåÂºïÂÖ• Z ÁöÑÂàÜÂ∏É q(Z) = log E_q(z) [P(X,Z|Œ∏)/q(Z)]ÔºåÊääÁßØÂàÜÁúã‰ΩúÊ±ÇÊúüÊúõ ‚â• E_q(z) [ log(P(X,Z|Œ∏)/q(Z)) ]ÔºåJensen‰∏çÁ≠âÂºèÁ≠âÂè∑Âú® P(X,Z|Œ∏) / q(Z)=C ÊàêÁ´ã, log C ÊòØÂ∏∏ÂáΩÊï∞\nËøô‰∏™ÊúüÊúõ E_q(z) [ log(P(X,Z|Œ∏)/q(Z)) ] Â∞±ÊòØÂØπÊï∞‰ººÁÑ∂ÁöÑ‰∏ãÁïåÔºåÂ∞±ÊòØELBO„ÄÇ\nq(Z) = P(X,Z|Œ∏) / C ‚à´z q(Z) dZ = 1 = ‚à´z P(X,Z|Œ∏) / C dZ = 1/C ‚à´z P(X,Z|Œ∏) dZ = 1/C P(X|Œ∏) ÔºàÊ±ÇËæπÁºòÊ¶ÇÁéáÔºâ\nC = P(X|Œ∏) Êää C ‰ª£Êç¢Ôºöq(Z) = P(X,Z|Œ∏) / P(X|Œ∏) = P(Z|X,Œ∏)\nÊâÄ‰ª•ÂΩì Jensen ‰∏çÁ≠âÂºèÂèñÁ≠âÂè∑Êó∂ÔºåZÁöÑÂàÜÂ∏É q(Z) Â∞±ÊòØ Z ÁöÑÂêéÈ™åÂàÜÂ∏ÉP(Z|X,Œ∏)„ÄÇ\nÊâÄ‰ª• EM Á¨¨‰∏ÄÊ≠•ÂÖàÊåâÁÖß‰∏ä‰∏ÄÊó∂ÂàªÁöÑÂèÇÊï∞ Œ∏‚ÅΩ·µó‚Åæ ÂíåÊï∞ÊçÆ X Ê±ÇÂá∫ Z ÁöÑÂêéÈ™åÂàÜÂ∏É P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)Ôºå ÂÜçÂ∞ÜÂØπÊï∞‰ººÁÑ∂ log(P(X,Z|Œ∏)/P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)) ÊåâÁÖß Z ÁöÑÂêéÈ™åÂàÜÂ∏ÉÊ±ÇÊúüÊúõÔºåÂæóÂà∞ÂØπÊï∞‰ººÁÑ∂ÁöÑ‰∏ãÁïåELBOÔºå Ëøô‰∏™‰∏ãÁïåÊòØÂÖ≥‰∫é Œ∏ ÁöÑÂáΩÊï∞: ‚à´z P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)‚ãÖlog P(X,Z|Œ∏) dZÔºåÊâÄ‰ª•ÂèØÊâæÂà∞Ëøô‰∏™‰∏ãÁïåÊúÄÂ§ßÊó∂ÂØπÂ∫îÁöÑŒ∏Ôºå‰Ωú‰∏∫Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ„ÄÇ‰∏çÊñ≠Ëø≠‰ª£ÊèêÈ´ò‰∏ãÁïåÔºàÊúüÊúõÔºâÔºå‰ªéËÄåÊèêÈ´òÂØπÊï∞‰ººÁÑ∂\n4. ÂÜçÂõûÈ¶ñ Source video: P4\nEM ÊòØËß£ÂÜ≥‰ºòÂåñÈóÆÈ¢òÁöÑËø≠‰ª£ÁÆóÊ≥ïÔºàÂíåÊ¢ØÂ∫¶‰∏ãÈôçÊòØ‰∏Ä‰∏™levelÔºâÔºåËÄåHMMÔºåGMM ÊòØÊ®°Âûã\n‰ªé‰πãÂâçÁã≠‰πâÁöÑÔºàÁêÜÊÉ≥ÁöÑÔºâEM Êé®ÂπøÂà∞Âπø‰πâÁöÑÔºà‰∏ÄËà¨ÁöÑÔºâEM Áã≠‰πâÁöÑEM ÊòØÂπø‰πâEM ÁöÑ‰∏Ä‰∏™Áâπ‰æã EM ÁöÑÂèòÁßç EM ‰∏ªË¶ÅÁî®‰∫éÊ¶ÇÁéáÁîüÊàêÊ®°ÂûãÔºåÊï∞ÊçÆÔºàÈöèÊú∫ÂèòÈáèÔºâÂåÖÊã¨ËßÇÊµãÊï∞ÊçÆ X ÂèäÂÖ∂ÂØπÂ∫îÁöÑÈöêÂèòÈáè ZÔºåÊâÄ‰ª•(X,Z) Âè´ÂÅöÂÆåÂÖ®Êï∞ÊçÆcomplete dataÔºåŒ∏ ÊòØÊ¶ÇÁéáÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇZ ÊòØÂª∫Ê®°Êó∂ÂºïÂÖ•ÁöÑÔºåZÁîüÊàê‰∫ÜXÔºåÊàë‰ª¨Âè™ËÉΩËßÇÊµãÂà∞X„ÄÇ ÊØîÂ¶ÇGMM ‰∏≠ÔºåÂÅáËÆæ z ÊòØ‰∏Ä‰∏™Á¶ªÊï£ÁöÑÂàÜÂ∏ÉÔºåÊØîÂ¶Ç K ‰∏™Á±ªÂà´ z=1,2,\u0026hellip;,KÔºåÊØè‰∏™Á±ªÂà´ÈÉΩÊúâ‰∏ÄÂÆöÁöÑÊ¶ÇÁéáÔºö\nÈöêÂèòÈáè z 1 2 \u0026hellip; K Ê¶ÇÁéáÂØÜÂ∫¶ p‚ÇÅ p‚ÇÇ \u0026hellip; p‚Çñ ÁÑ∂ÂêéÂú® z ÁªôÂÆöÁöÑÊÉÖÂÜµ‰∏ãÔºåx Êª°Ë∂≥È´òÊñØÂàÜÂ∏ÉÔºöP(x|z) ~ GaussianÔºåÂõ†Ê≠§ÂØπÂÆåÊï¥Êï∞ÊçÆ P(X,Z) Âª∫Ê®°\nËøòÊúâ HMM ‰πüÂèØ‰ª•‰ªéÁîüÊàêÁöÑËßíÂ∫¶Êù•Ëß£Èáä„ÄÇÊØîÂ¶Ç N ‰∏™ÈöêÂèòÈáè: z‚ÇÅ, z‚ÇÇ, \u0026hellip;, z‚Çô ÊòØÈ©¨Â∞îÁßëÂ§´ÈìæÁöÑÁªìÊûÑ:\n$$ s1 ‚ûî s2 ‚ûî \u0026hellip; ‚ûî s‚Çô \\\\ ‚Üß \\quad\\ ‚Üß \\quad \u0026hellip; \\quad ‚Üß \\\\ x1 \\quad x2 \\quad \u0026hellip; \\quad x‚Çô $$\nÂèØ‰ª•Êää z‚ÇÅ, z‚ÇÇ, \u0026hellip;, z‚Çô ÁúãÊàêÊòØÁªü‰∏ÄÁöÑÂèòÈáè ZÔºåÊää x‚ÇÅ, x‚ÇÇ, \u0026hellip;, x‚Çô ÁúãÊàêÊòØ‰∏Ä‰∏™X\nËßÇÊµãÂà∞‰∫Ü XÔºåÂÅáËÆæÂÆÉÁöÑÊ¶ÇÁéáÊ®°ÂûãÁöÑÂèÇÊï∞ÊòØŒ∏ÔºåÂ∞±ÂèØ‰ª•Áî® EM Êù•‰º∞ËÆ°ÂèÇÊï∞ Œ∏„ÄÇ\n‰ΩøÁî®MLE Êù•‰º∞ËÆ°ÂèÇÊï∞ÔºöŒ∏^ = arg max P(X|Œ∏) = arg max ‚àè·µ¢‚Çå‚ÇÄ·∂∞P(x·µ¢|Œ∏) ‚ûî arg max log P(X|Œ∏)\n‰∏çËÉΩÁõ¥Êé•Ê±ÇËß£Ëøô‰∏™ÊúÄÂ§ßÂåñÈóÆÈ¢òÁöÑÂéüÂõ†ÊòØÔºå‰∏çÁü•ÈÅìP(X)ÔºåÂõ†‰∏∫Ê†∑Êú¨XÊòØÈùûÂ∏∏Â§çÊùÇÁöÑÔºåÊâÄ‰ª•‚Äú‰ºöÂºïÂÖ•Ëá™Â∑±ÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÔºåÂÅáÂÆöÂÆÉÊòØÊúç‰ªéÊüê‰∏™Ê®°ÂûãÁöÑ„ÄÇ‚Äù\nÁîüÊàêÊ®°ÂûãÂ∞±ÊòØÂÅáËÆæÊØè‰∏™Ê†∑Êú¨ x‚ÅΩ‚Å±‚Åæ ÈÉΩÊúâ‰∏Ä‰∏™ÈöêÂèòÈáè z‚ÅΩ‚Å±‚ÅæÔºåx‚ÅΩ‚Å±‚Åæ ÊòØÁî± z‚ÅΩ‚Å±‚Åæ ÁîüÊàêÁöÑÔºåÊâÄ‰ª•P(X) Â∞±ÂèòÊàê‰∫ÜËÅîÂêàÂàÜÂ∏É P(X,Z)ÔºàÂç≥ÊääXÂàÜËß£Â§ÑÁêÜÔºâÔºåÁÑ∂ÂêéÊää Z ÁßØÂàÜÊéâÂ∞±ÂèØ‰ª•‰∫Ü: P(X) = ‚à´z P(X,Z) dZ\nnote-ËãèÂâëÊûó-VAE\n5. Âπø‰πâEM Source video: P5\nEM Áî®‰∫éËß£ÂÜ≥ÂèÇÊï∞‰º∞ËÆ°ÈóÆÈ¢òÔºå‰ºòÂåñÂáΩÊï∞‰ΩøÁî® MLE ÔºåÊâæÂà∞ËÆ©ÂØπÊï∞‰ººÁÑ∂ log P(X|Œ∏) ËææÂà∞ÊúÄÂ§ßÁöÑÂèÇÊï∞ Œ∏„ÄÇ Ê†∑Êú¨X ÁöÑÂàÜÂ∏É P(X) Êú™Áü•ÔºåÊâÄ‰ª•ÂÅáËÆæÊØè‰∏™ x ÊòØÁî±ÈöêÂèòÈáè z ÁîüÊàêÁöÑ„ÄÇÊØîÂ¶Ç GMM ÂÅáËÆæ z Êúç‰ªéÁ¶ªÊï£ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåHMM ÂÅáËÆæ z Êª°Ë∂≥È©¨Â∞îÁßëÂ§´Èìæ„ÄÇÁÑ∂Âêé P(X) Â∞±Á≠â‰∫éËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏ÉP(X,Z) ÊØî‰∏äZÁöÑÂêéÈ™åÂàÜÂ∏ÉP(Z|X)Ôºå‰πüÂ∞±ÊòØÊú™Áü•XÂàÜÂ∏ÉÁªèËøáÁîüÊàêÊ®°ÂûãÁöÑÂÅáËÆæÔºåÊääÈóÆÈ¢òÂÖ∑‰ΩìÂåñ‰∫Ü„ÄÇ\nÂõ†‰∏∫ P(X|Œ∏) = P(Z,X|Œ∏) / P(Z|X,Œ∏)ÔºåÊâÄ‰ª•ÔºàÂ§çÊùÇÁöÑÊú™Áü•ÁöÑÔºâÂØπÊï∞‰ººÁÑ∂ÁõÆÊ†áÂáΩÊï∞ÂèØÂàÜËß£‰∏∫: ‰∏ãÁïåELBO+ZÁöÑÂàÜÂ∏Éq(Z)‰∏éZÁöÑÂêéÈ™åÂàÜÂ∏ÉP(Z|X,Œ∏)ÁöÑKLÊï£Â∫¶: log P(X|Œ∏) = ELBO + KL(q||P)\nELBO ÊòØ‰∏Ä‰∏™ÊúüÊúõÔºåÂÆÉÂíåq(Z) Âíå Œ∏ ÊúâÂÖ≥ÔºåÊâÄ‰ª•Â∞ÜÂÖ∂ËÆ∞‰∏∫ L(q,Œ∏); KL\u0026gt;=0, ÂΩìq=PÊó∂ÔºåKL=0ÔºåÊâÄ‰ª•Ôºölog P(X|Œ∏) ‚â• L(q,Œ∏)ÔºåËÄå‰∏îÊúÄ‰ºòÂèÇÊï∞ Œ∏^ ÊòØÂú® q^= P(Z|X,Œ∏) Êó∂ÂèñÂà∞„ÄÇ\n‰ΩÜÊòØ q^ Âπ∂‰∏ç‰∏ÄÂÆöËÉΩÂèñÂà∞ P(Z|X,Œ∏)ÔºåÂõ†‰∏∫Ëøô‰∏™ÂêéÈ™åÂèØËÉΩÊòØ intractableÔºåÊòØÁÆó‰∏çÂá∫Êù•ÁöÑ„ÄÇËøôÁî±ÁîüÊàêÊ®°ÂûãÁöÑÂ§çÊùÇÂ∫¶ÂÜ≥ÂÆöÔºå Â¶ÇÊûúÁîüÊàêÊ®°ÂûãÊØîËæÉÁÆÄÂçïÔºåÊØîÂ¶ÇGMMÁöÑ z Âíå HMM ÁöÑ zÔºå‰ªñ‰ª¨ÊòØÁ¶ªÊï£ÁöÑÔºåÁªìÊûÑÂåñÁöÑÔºåÊòØtractableÔºåÊòØÂèØ‰ª•ÔºàÁî®EMÔºâËÆ°ÁÆóÂá∫Êù•ÁöÑ\n‰ΩÜÊòØÂØπ‰∫éÁªùÂ§ßÂ§öÊï∞ÁöÑ z ÊòØÊó†Ê≥ïÊ±ÇÂá∫‰ªñÁöÑÂêéÈ™å P(z|x)„ÄÇÊØîÂ¶Ç VAE ÁöÑ z ÊòØÈ´òÁª¥ÁöÑÔºåÊó†Ê≥ïÊääÂÆÉ‰ªé P(x,z) ‰∏≠ÁßØÊéâÔºåÂ∞±Âæó‰∏çÂà∞ P(x)Ôºå‰πüÂ∞±Êó†Ê≥ïÔºàÁî®Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÔºâÂæóÂà∞ÂêéÈ™å P(z|x)ÔºåÊâÄ‰ª•ÊúÄ‰ºòÁöÑ q^(Z) Âèñ‰∏çÂà∞ÔºåÊâÄ‰ª•Â∞±ÈúÄË¶ÅÂèòÂàÜÔºàËøë‰ººÔºâÊé®Êñ≠ÔºöÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ß+Á•ûÁªèÁΩëÁªúÊ¢ØÂ∫¶‰∏ãÈôçÁî® q_œï(z|x) ÈÄºËøë P_Œ∏(z|x)„ÄÇ\nÊï∞Â≠¶Ë°®ËææÔºö ÂΩì Œ∏ Âõ∫ÂÆö ÁöÑÊó∂ÂÄôÔºåÂØπÊï∞‰ººÁÑ∂ log P(X|Œ∏) Â∞±ÊòØÂõ∫ÂÆöÁöÑÔºåÁÑ∂ÂêéÂΩì q(Z) Ë∂äÊé•Ëøë Z ÁöÑÂêéÈ™åÂàÜÂ∏É P(Z|X,Œ∏)ÔºåKLÊï£Â∫¶Â∞±Ë∂äÂ∞èÔºåÂêåÊó∂ ELBO Â∞±Ë∂äÂ§ß„ÄÇÊâÄ‰ª•Ê±ÇÊúÄ‰ºòÁöÑÂàÜÂ∏É q(Z) Â∞±ÂèòÊàê‰∏Ä‰∏™‰ºòÂåñÈóÆÈ¢òÔºö q^(Z) = arg min_q KL(q(Z) || P(Z|X,Œ∏)) = arg max_q L(q,Œ∏)\nÂΩì q^Âõ∫ÂÆö ÁöÑÊó∂ÂÄôÔºåÂÜçÂÅöÊûÅÂ§ß‰ººÁÑ∂Êâæ Œ∏Ôºå‰πüÂ∞±ÊòØÂÅö‚ÄúÁã≠‰πâ‚ÄùÁöÑEMÔºåÊúÄ‰ºòÁöÑ Œ∏^= arg max·∂± L(q^, Œ∏)\nÂπø‰πâEMÔºà‰∏§‰∏™ÊúÄÂ§ßÂåñÈóÆÈ¢òÔºâ:\nE-step: q‚ÅΩ·µó‚Å∫¬π‚Åæ = arg max_q L(q,Œ∏‚ÅΩ·µó‚Åæ)ÔºåÂõ∫ÂÆöŒ∏Ê±ÇÊúÄ‰ºòÁöÑq^ M-step: Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = arg max·∂± L(q‚ÅΩ·µó‚Å∫¬π‚Åæ, Œ∏)ÔºåÂõ∫ÂÆöq^Ê±ÇÊúÄ‰ºòÁöÑŒ∏ ÂØπ ELBO ÂÅöÂèòÂΩ¢ÔºàÂ±ïÂºÄlogÔºâÔºö\nL(q,Œ∏) = E_q(z) [log (P(Z,X|Œ∏)/q(Z))] = E_q(z) [ log P(Z,X|Œ∏) - log q(Z) ]\n= E_q(z) [log P(Z,X|Œ∏)] - E_q(z) [log q(Z)] = E_q(z) [log P(Z,X|Œ∏)] + H[q(Z)]\nÂÖ∂‰∏≠Á¨¨2È°π H[q(Z)] ÊòØÂàÜÂ∏É q(Z) ÁöÑÁÜµ: ‚à´ q(Z)‚ãÖlog (1/q(z)) dz Ôºå ÊâÄ‰ª• ELBO = ÂÆåÂÖ®Êï∞ÊçÆ‰ººÁÑ∂ÊåâÁÖß q(Z) Ê±ÇÊúüÊúõ+ q(Z) ÁöÑÁÜµ\n‰πãÂâçÁöÑEM ÊòØÂπø‰πâEM ÁöÑ‰∏Ä‰∏™ÁâπÊÆäÊÉÖÂÜµ„ÄÇÂØπ‰∫é E-step, Áã≠‰πâEM ÈªòËÆ§ q Áõ¥Êé•Â∞±ÂèñÂà∞‰∫ÜÂêéÈ™å P(Z|X,Œ∏‚ÅΩ·µó‚Åæ)ÔºåÂõ†‰∏∫ÂÅáÂÆö‰∫ÜÂêéÈ™åËÉΩÂ§üÊ±ÇÂá∫„ÄÇÂØπ‰∫é M-step, Áã≠‰πâEM ËÆ§‰∏∫ q^ Â∑≤ÁªèÊâæÂà∞‰∫ÜÔºåÈÇ£‰πàÂπø‰πâEM ÁöÑM-step ‰∏≠ÁöÑÁÜµÂ∞±ÊòØÁ°ÆÂÆöÂÄºÔºåË¶Å‰ºòÂåñÁöÑÂè™Êúâ‰ººÁÑ∂„ÄÇ\n6. EM ÁöÑÂèòÁßç Source video: P6\nÔºàÊ†áÂáÜÁöÑÔºå‰∏ÄËà¨ÊåáÁöÑÔºâÂπø‰πâÁöÑEMÔºöÂØπ‰ººÁÑ∂‰∏ãÁïå(ELBOËÅîÂêàÊ¶ÇÁéáÊåâÁÖßzÁöÑÂêéÈ™åÊ±ÇÊúüÊúõ) L(q,Œ∏) Ê±Ç‰∏§Ê¨°ÊúÄÂ§ßÂåñÔºåÂÖàÂõ∫ÂÆöŒ∏‚ÅΩ·µó‚ÅæÊ±Çq‚ÅΩ·µó‚Å∫¬π‚ÅæÔºåÁÑ∂ÂêéÂõ∫ÂÆöq‚ÅΩ·µó‚Å∫¬π‚Åæ Ê±Ç Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ\nÂõ†‰∏∫‰∏§Ê≠•ÈÉΩÊòØÊ±Ç MaximumÔºåÊâÄ‰ª• EM ‰πüÁß∞ MM (Maximation Maximation)\nÂØπ‰∫é‰∏§‰∏™ÂèÇÊï∞ÔºåÂÖàÂõ∫ÂÆö‰∏Ä‰∏™Ê±ÇÂè¶‰∏Ä‰∏™ÔºåÂÜçÂèçËøáÊù•ÔºåËøôÁßçÁÆóÊ≥ïÊòØÂùêÊ†á‰∏äÂçáÊ≥ïÔºåÊØîÂ¶ÇSMO„ÄÇÂ¶ÇÊûúÂèÇÊï∞ÊòØÂ§öÁª¥ÁöÑÔºåÂõ∫ÂÆöÂÖ∂‰∏≠Êüê‰∏Ä‰∏™/‰∏§‰∏™ÔºåÁÑ∂ÂêéÂéªÊ±ÇÂÖ∂‰ªñÁöÑ„ÄÇÊ±ÇÂèÇÊï∞ÁöÑÈ°∫Â∫èÊ≤°ÂÖ≥Á≥ª„ÄÇ\nÂùêÊ†á‰∏äÂçáÊ≥ï ‰∏é Ê¢ØÂ∫¶‰∏äÂçáÊ≥ïÂπ∂Âàó\nÊçüÂ§±ÂáΩÊï∞ÁöÑÁ≠âÈ´òÁ∫øÂ¶Ç‰∏ãÂõæÔºåÊ¢ØÂ∫¶‰∏äÂçáÊ≥ïÁöÑÂèÇÊï∞Ë∑ØÂæÑÊòØÊ≤øÁùÄÊ¢ØÂ∫¶ÁöÑÔºåËÄåÂùêÊ†á‰∏äÂçáÊ≥ïÁ±ª‰ººÊõºÂìàÈ°øË∑ùÁ¶ªÔºå\nÂ¶ÇÊûú E-step ‰∏≠ÊúÄ‰ºòÁöÑ q^(Z)Ôºå‰πüÂ∞±ÊòØZ ÁöÑÂêéÈ™åP(Z|X,Œ∏) Êó†Ê≥ïÊ±ÇÂæóÔºåÂ∞±ÂèØ‰ª•Áî®ÂèòÂàÜÊé®Êñ≠Ê±ÇËøë‰ººÊúÄ‰ºòÔºåÊØîÂ¶ÇÂü∫‰∫éÂπ≥ÂùáÂú∫ÁêÜËÆ∫ÁöÑÂèòÂàÜÊ≥ïËøë‰ººÂêéÈ™åÂàÜÂ∏ÉÔºåÂÜçÂÅö M-stepÔºåÁß∞Ëøô‰∏™ÁªÑÂêà‰∏∫VBEM ÂèòÂàÜË¥ùÂè∂ÊñØEM„ÄÇ Â¶ÇÊûúÁî®ËíôÁâπÂç°ÁΩóÈááÊ†∑ÂéªÊ±ÇËøë‰ººÂêéÈ™åÂàÜÂ∏ÉÔºåÂè´‰ΩúMCEMÔºåËíôÁâπÂç°ÁΩóEM„ÄÇ\nVIÔºàÂèòÂàÜÊé®Êñ≠Ôºâ Âíå VB ÔºàÂèòÂàÜË¥ùÂè∂ÊñØÔºâÊåáÁöÑÊòØÂêå‰∏Ä‰∏™‰∏úË•ø\n","date":"2022-12-16T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/10-em%E7%AE%97%E6%B3%95/","title":"watch: ML - ÁôΩÊùø 10 | EM Algorithm"},{"content":"2ËøûÂí∏È•º ‚ûî P(Ë¢ãÂ≠ê) ‰ªéÂêå‰∏ÄË¢ãÂ≠êÈáåËøûÁª≠ÂèñÂá∫‰∫Ü2‰∏™Âí∏È•ºÂπ≤ÔºåÈóÆÊúÄÊúâÂèØËÉΩÊòØ‰ª•‰∏ã5‰∏™Ë¢ãÂ≠ê‰∏≠ÁöÑÂì™‰∏™Ôºü\nÈ•ºÂπ≤ÊØî‰æã: 100%Áîú 75%Áîú25%Âí∏ 50%Áîú50%Âí∏ 25%Áîú75%Âí∏ 100%Âí∏ MLE 0 0.0625 0.25 0.5625 1 ËÆæ‰ªéÊØè‰∏™Ë¢ãÂ≠ê‰∏≠ÂèñÂá∫Âí∏È•ºÂπ≤ÁöÑÊ¶ÇÁéáÊòØpÔºåÂàôMLE Âè™ÈúÄÂØπÊØè‰∏™Ë¢ãÂ≠êÊ±Çp¬≤ÔºåÂàôÁ¨¨5‰∏™Ë¢ãÂ≠êÁöÑ‰ººÁÑ∂ÊúÄÂ§ß„ÄÇËÄåMAPËøòË¶ÅËÄÉËôëÂêÑË¢ãÂ≠êÂá∫Áé∞ÁöÑÊ¶ÇÁéágÔºåÊâÄ‰ª•MAPÂáΩÊï∞‰∏∫ p¬≤√ógÔºåÂèØÂæóÁ¨¨4‰∏™Ë¢ãÂ≠êÁöÑÂêéÈ™åÊúÄÂ§ß\nÂÖàÈ™åg (Ë¢ãÂ≠êÂá∫Áé∞ÁöÑÊ¶ÇÁéá) 0.1 0.2 0.4 0.2 0.1 Ë¢ã‰∏≠Âí∏È•ºÂπ≤ÊØî‰æã Œ∏ 0 0.25 0.5 0.75 1 ‰∫ã‰ª∂ X:‰∫åËøûÂí∏(MLE) 0 0.0625 0.25 0.5625 1 MAP 0 0.0125 0.1 0.1125 0.1 ÂêéÈ™å 0 3.85% 30.8% 34.6% 30.8% Âõ†‰∏∫ËøôÈáåÁöÑÊ†∑Êú¨Á©∫Èó¥Áî±‰∏§‰∏™ÈöèÊú∫ÂèòÈáèÁªÑÊàêÔºö‰∫ã‰ª∂XÂíåË¢ãÂ≠êÊ¶ÇÁéágÔºåÊâÄ‰ª•P(X)Â∞±ÊòØÂú®ÊâÄÊúâŒ∏ÂèØËÉΩÁöÑÊÉÖÂÜµ‰∏ãXÂèëÁîüÁöÑÊ¶ÇÁéáÂíå(0.325)„ÄÇÂõ†‰∏∫P(X)ÊòØ‰∏™Â∏∏Êï∞ÔºåÊâÄ‰ª•ÂÅö MAP Âè™ÁúãÂàÜÂ≠êÂ∞±Ë°å„ÄÇ\nÁî∑Áîü ‚ûî P(Êâìlol) ‰∏Ä‰∏™Áî∑ÁîüÔºåÊ±Ç‰ªñÊâìlolÁöÑÊ¶ÇÁéá\nÂÖàÈ™åg 60% 40% Áî∑Â•≥ÊØî‰æã Œ∏ 80%Áî∑20%Â•≥ 20%Áî∑80%Â•≥ ‰∫ã‰ª∂X:1Áî∑ (MLE) 0.8 0.2 ‰∫ã‰ª∂X:1Áî∑ (MAP) 0.48 0.08 ÂêéÈ™å 85.7% 14.3% 9Ê≠£1Âèç ‚ûî P(Á°¨Â∏Å) Êäõ10Ê¨°Á°¨Â∏Å9Ê≠£1ÂèçÔºåÊ±ÇËøôÊûöÁ°¨Â∏ÅÊúù‰∏äÁöÑÊ¶ÇÁéá? „ÄêÊú∫Âô®Â≠¶‰π†ÊàëÂà∞Â∫ïÂú®Â≠¶‰ªÄ‰πà„ÄëÂì≤Â≠¶ËßíÂ∫¶ËÅäËÅäË¥ùÂè∂ÊñØÊ¥æÂíåÈ¢ëÁéáÊ¥æÔºåÊï∞Â≠¶ËßíÂ∫¶ÁúãÁúãÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÂíåÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°\nÈ¢ëÁéáÊ¥æ‰ºöËØ¥ÊòØ0.9ÔºåÂõ†‰∏∫ËøôÊ†∑ÂØπÂ∫î\u0026quot;ÊäõÂá∫9Ê¨°Êúù‰∏ä\u0026quot;ÁöÑÊ¶ÇÁéáÊúÄÂ§ßÔºõ Ë¥ùÂè∂ÊñØÊ¥æ‰æùÊçÆË¥ùÂè∂ÊñØÂÖ¨ÂºèÔºåÂ∏¶ÂÖ•ÂÖàÈ™åÊ¶ÇÁéáÂíåÂÆûÈ™åÁªìÊûúÔºåËÆ§‰∏∫Âêë‰∏äÊ¶ÇÁéáËêΩÂú® 0.5~0.9 ‰πãÈó¥ÁöÑÊ¶ÇÁéáÊúÄÂ§ß: ÊØîÂ¶ÇÂèÇÊï∞Œ∏\u0026rsquo;=0.5 ÁöÑÂÖàÈ™åÊ¶ÇÁéáÊòØ P(Œ∏\u0026rsquo;)=0.8, ÊääŒ∏\u0026rsquo;=0.5 Â∏¶ÂÖ•‰∫åÈ°πÂàÜÂ∏ÉÂèØÁÆóÂá∫‰∫ã‰ª∂9Ê≠£1ÂèçÂèëÁîüÁöÑÊ¶ÇÁéáÔºöP(X|Œ∏\u0026rsquo;)=C‚Çâ¬π‚Å∞‚ãÖ(0.5)‚Åπ‚ãÖ(0.5)¬π=0.00976. ÊâÄ‰ª•Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÁöÑÂàÜÂ≠ê=0.8√ó0.00976=0.00781„ÄÇ ËÄåÂàÜÊØç P(X) Â∫î‰∏∫ Œ∏ ‰ªé0ÂèñÂà∞1Êó∂ÔºåÂêÑÁßç0ÂØπÂ∫îÁöÑ‰∫ã‰ª∂X:9Ê≠£1ÂèçÂèëÁîüÁöÑ‰ººÁÑ∂‰πãÂíåÔºå‰ΩÜ‰ªé0Âà∞1 ÊòØ‰∏çÂèØÊï∞ÁöÑÔºåÊó†Ê≥ïÂæóÂà∞Ëß£ÊûêËß£ÔºåÂèØ‰ª•ÂÅöÊï∞ÂÄºÊ®°Êãü„ÄÇ‰∏ãÈù¢ÁöÑ‰æãÂ≠êÂè™ÂÅáËÆæÊ†∑Êú¨Á©∫Èó¥Âè™Êúâ 3 ‰∏≠ÂèÇÊï∞Œ∏ ÁöÑÂèñÂÄºÔºö0.5, 0.9, 0.4„ÄÇ\nÂÖàÈ™å g 80% 1% 19% ÂèÇÊï∞ Œ∏ 50%‰∏ä50%‰∏ã 90%‰∏ä10%‰∏ã 40%‰∏ä60%‰∏ã ‰∫ã‰ª∂ X C‚Çâ¬π‚Å∞‚ãÖ(0.5)‚Åπ‚ãÖ(0.5)¬π C‚Çâ¬π‚Å∞(0.9)‚Åπ(0.1)¬π C‚Çâ¬π‚Å∞(0.4)‚Åπ(0.9)¬π MLE 0.00976 0.387 0.00236 MAP 0.00781 0.00387 0.000448 ÂêéÈ™å 64.4% 31.9% 3.7% ‰∏äË°®‰∏≠ MLE = P(X|Œ∏), MAP = P(X|Œ∏)P(Œ∏), ËÄå‚Äú‰∫ã‰ª∂XÂèëÁîüÔºåÂèÇÊï∞ÂèñÂà∞Œ∏ÁöÑÊ¶ÇÁéá‚Äù: P(Œ∏|X) = MAP/P(X)ÔºåÂÖ∂‰∏≠ P(X) ÊòØ‰∫ã‰ª∂XÂú®Ê†∑Êú¨Á©∫Èó¥‰∏≠ÂèëÁîüÁöÑÊ¶ÇÁéáÔºåÂ∫î‰∏∫ÂΩìÂèÇÊï∞ÂèñÂêÑÁßç Œ∏ Êó∂Ôºå‰∫ã‰ª∂X ÂèëÁîüÁöÑÊù°‰ª∂Ê¶ÇÁéá ‰πãÂíåÔºå‰πüÂ∞±ÊòØÂêÑÁßçŒ∏ ÂØπÂ∫îÁöÑ MAP Ê±ÇÂíå„ÄÇ\nÊÑüËßâ‰∏çÂ§™ÊÅ∞ÂΩì Áî≤Âú∞‰∏ãÈõ®‰∫ÜÔºåÊ±Ç‰πôÂú∞‰∏ãÈõ®Ê¶ÇÁéá Êù°‰ª∂Ê¶ÇÁéá ‰æãÈ¢ò\nÁâπÂæÅ ‚ûî P(Â´Å) Êï∞ÊçÆÊåñÊéòÂÆûÈ™å-Naive Bayes\n","date":"2022-12-15T23:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/map_examples/","title":"MAP examples"},{"content":"1 Peak signal-to-noise Ratio ‰∏§ÂπÖÂ§ßÂ∞è‰∏∫m√ónÁöÑÂçïËâ≤ÂõæÔºå‰∏ÄÂπÖÊòØÂéüÂõæIÔºåÂè¶‰∏ÄÂπÖÊòØËøë‰ººÂõæÂÉèK„ÄÇ ‰∏§ËÄÖ‰πãÈó¥ÁöÑÂπ≥ÂùáÂπ≥ÊñπËØØÂ∑ÆMean Squared ErrorÊòØÔºö\n$$ MSE = \\frac{1}{mn} \\sum_{i=1}^m \\sum_{j=1}^n\\ [I(i,j) - K(i,j)]^2 $$\nÂàôÂ≥∞ÂÄº‰ø°Âô™ÊØîÂÆö‰πâ‰∏∫Ôºö PSNR = 10 log‚ÇÅ‚ÇÄ (MAX·µ¢¬≤ / MSE)\nMAX·µ¢ ÊòØÂéüÂõæ‰∏≠ÂèØËÉΩÁöÑÊúÄÂ§ßÂÉèÁ¥†ÂÄºÔºåÂΩì‰∏Ä‰∏™ÂÉèÁ¥†Áî®8 bitsË°®Á§∫Êó∂ÔºåMAX·µ¢=255\n‰∏∫‰ªÄ‰πàÊòØËøôÊ†∑Â≠êÔºü\nÂ∏∏Áî®‰∫éË°°ÈáèÊúâÊçüÂéãÁº©ÁºñËß£Á†ÅÂô®ÁöÑÈáçÂª∫Ë¥®Èáè„ÄÇ wikipedia\nMSEË∂äÂ∞èÔºåPSNRË∂äÂ§ß„ÄÇÂΩì‰∏§ÂõæÊ≤°ÊúâËØØÂ∑ÆÊó∂ÔºåPSNRË∂ã‰∫éÊó†Á©∑\n2 Structural SIMilarity Index Áî®‰∫éÈ¢ÑÊµãÊï∞Â≠óÂõæÂÉèÁöÑÂèØÊÑüÁü•Ë¥®Èáè perceived quality SSIM ¬≤ is used to measure the distortion degree of an image, or measure the similarity between two images from 3 aspects: Luminance, Contrast, and Structure.\nRatio of luminance l(X,Y) = 2Œº‚ÇìŒº·µß+C‚ÇÅ/(Œº‚Çì¬≤+Œº·µß¬≤+C‚ÇÅ), where Œº‚Çì is the mean of the intensity of all pixels in image X: Œº‚Çì = 1/N ‚àë·µ¢‚Çå‚ÇÅ·¥∫ X·µ¢. And C‚ÇÅ prevents the divisor from being 0.\nBased on AM-GM inequality: (‚àöx - ‚àöy)¬≤‚â•0, if and only if Œº‚Çì=Œº·µß, l(X,Y)=1.\nRatio of contrast c(X,Y) = 2œÉ‚ÇìœÉ·µß+C‚ÇÇ/(œÉ‚Çì¬≤+œÉ·µß¬≤+C‚ÇÇ), where œÉ‚Çì is the (non-bias) standard deviation of the intensity of all pixels in the image X: œÉ‚Çì = (1/(N-1) ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (X·µ¢-Œº‚Çì)¬≤)¬π·êü¬≤. If and only if œÉ‚ÇÅ=œÉ‚ÇÇ, c(X,Y)=0.\nStructure is reflected by correlation coefficient: s(X,Y) = œÉ‚Çì·µß+C‚ÇÉ/(œÉ‚ÇìœÉ·µß+C‚ÇÉ), where œÉ‚Çì·µß is the covaraiance of the intensity of 2 images: œÉ‚Çì·µß= 1/(N-1) ‚àë·µ¢‚Çå‚ÇÅ·¥∫ (X·µ¢-Œº‚Çì)(Y·µ¢-Œº·µß).\nThe intent of the paper may be to measure the information aside from Luminance and Contrast ‚Åµ, so each pixel is reduced by the mean and devided by stddev: (X·µ¢-Œº‚Çì)/œÉ‚Çì and (Y·µ¢-Œº·µß)/œÉ·µß, i.e., normalization, and then compute the distance between corresponding pixels in 2 images by inner product.\nThus, S(X,Y) is 1/(N-1) ‚àë·µ¢‚Çå‚ÇÅ·¥∫ [(X·µ¢-Œº‚Çì)/œÉ‚Çì ‚ãÖ (Y·µ¢-Œº·µß)/œÉ·µß ] = œÉ‚Çì·µß/œÉ‚ÇìœÉ·µß\nThe final expression is the product of the above 3 features with specified power (weights) Œ± Œ≤ Œ≥:\nS(X,Y) = l(X,Y)·µÖ ‚ãÖ c(X,Y)·µù ‚ãÖ s(X,Y)·µû ‚àà [-1,1]\nwhere l(X,Y), c(X,Y), s(X,Y) ‚àà [-1,1], and since brightness ‚â•0, the actural range of l and c are (0,1]. If and only if the images X and Y are the same, the 3 items are equal to 1 at the same time.\nLet Œ±, Œ≤, Œ≥=1, the SSIM(X,Y) = $\\frac{ 2(Œº‚ÇìŒº·µß+C‚ÇÅ) (2œÉ‚Çì·µß + C‚ÇÇ) }{(Œº‚Çì¬≤+Œº·µß¬≤+C‚ÇÅ) (œÉ‚Çì¬≤+œÉ·µß¬≤+C‚ÇÇ)}$, where C‚ÇÅ = (K‚ÇÅL)¬≤, C‚ÇÇ = (K‚ÇÇL)¬≤, C‚ÇÉ=C‚ÇÇ/2, and L is the maximum for a pixel (L=2^b). Based on the rule of thumb, K‚ÇÅ = 0.01, K‚ÇÇ=0.03.\nIn practice, SSIM is not performed on the entire image, but calculating the mean and stddev in a local sliding window (kernel, filter of 11x11 with stddev=1.5, sum=1), which represents a circular-symmetric Gaussian Weighting Function.\nHence, the local mean and std-dev are confined within the kernel:\nŒº‚Çì = ‚àë·µ¢ w·µ¢ X·µ¢ œÉ‚Çì = ‚àë·µ¢ w·µ¢ (X·µ¢-Œº‚Çì)¬≤)¬π·êü¬≤ œÉ‚Çì·µß= ‚àë·µ¢ w·µ¢ (X·µ¢-Œº‚Çì)(Y·µ¢-Œº·µß) where w·µ¢ is the parameters of the Gaussian kernel smoothing the image, such that fine-details are smeared and compare mainly the general features.\nAt the end, the average of all local SSIM is M-SSIM(X,Y) = 1/M . ‚àë‚±º‚Çå‚ÇÅ·¥π SSIM(X·µ¢, Y‚±º)\nUsed as loss Since the SSIM measures the similarity between 2 images, it can be used in the supervised training. Thus, SSIM dissimilarity, SSIMD = (1-SSIM)/2 ‚àà (0,] is a kind of loss function.\nÊúâÊçüÂéãÁº©Lossy compression‰∏§ÁßçÂü∫Êú¨Êú∫Âà∂Ôºö wikipedia\nÊúâÊçüÂèòÊç¢ÁºñËß£Á†ÅÔºöÂØπÂõæÂÉè/Â£∞Èü≥ÈááÊ†∑Ôºå È¢ÑÊµãÁºñËß£Á†ÅÔºö 3 LPIPS The motivation of Learned Perceptual Image Patch Similarity (LPIPS) ¬≥ is that: the conclusion for the similarity between 2 images from deep neural networks are aligned with humans perception. But the structure-based metrics usually give opposite judgement on the too-smoothed images ‚Åµ. (2023-02-18)\nLPIPS compares the intermediate convolutional feature vectors at different levels.\nlpips.LPIPS(net=\u0026quot;alex\u0026quot;) Êää‰∏§ÂπÖÂõæËæìÂÖ•Á•ûÁªèÁΩëÁªú (VGG, Alexnet) ËøõË°åÂ§öÂ±ÇÁ∫ßÁâπÂæÅÈó¥ÁöÑÂØπÊØî„ÄÇ ÊØèÂ±ÇËæìÂá∫ÁöÑ feature map·¥¥À£·µÇÀ£·∂ú ÊøÄÊ¥ªÂêéÂΩí‰∏ÄÂåñÔºåÂÜçÁõ∏ÂáèÔºåÂ∞ÜÂêÑÂ±ÇÂ∑ÆÂºÇÊåâÂÉèÁ¥†Âä†ÊùÉÊ±ÇÂíåÔºåÂπ∂(Èô§‰ª•ÂÉèÁ¥†‰∏™Êï∞)ÂÅöspatial Âπ≥ÂùáÔºåÂÜçÊääÂêÑÂ±ÇÂ∑ÆÂºÇÂä†Ëµ∑Êù• ‚Å¥\nRef Image quality assessment: from error visibility to structural similarity The Unreasonable Effectiveness of Deep Features as a Perceptual Metric LPIPSÂõæÂÉèÁõ∏‰ººÊÄßÂ∫¶ÈáèÊ†áÂáÜ_Alocus_ÁöÑÂçöÂÆ¢-CSDNÂçöÂÆ¢_lpipsÂ∫¶Èáè NeRFÂ∏∏Áî®ËØÑ‰ª∑ÊåáÊ†áÈÉΩÊòØ‰ªÄ‰πàÊÑèÊÄùÔºüPSNR„ÄÅSSIM„ÄÅLPIPSËØ¶Ëß£ - ÊÑè„ÅÆËåó - bilibili ","date":"2022-12-08T12:59:00Z","permalink":"https://zichen34.github.io/writenotes/vis/metrics/","title":"memo: Vis | Visual Metrics"},{"content":"Code | Arxiv (2103) | ProjPage\n(2023-08-16) Re-read\nExperiments Train model only on (MVSNet) DTU dataset, where the objects are partitioned the same as PixelNeRF (imgsize: 300x400).\nDatasets (Download):\nStage Data Contents Resolution N_views Size Train DTU 88 scenes 512x640 19G Test DTU 16 scenes 3/20 Test LLFF Test Blender Feature map: 32 channels Cost volume: 128 planes MLP: 6 layers Coarse-to-fine: One field and fine-tune Ray pts: 128 Device: 2080Ti Batch size: 1024 rays Optimizer: Adam, lr=5e-4 Old Notes Old notes on 2022-12-06 Abstract\n3 nearby input views plane-swept cost volumes geometry-aware scene reasoning generalize across scenes 1 Introduction\nTopic: Novel view synthesis\nRecent progress: neural rendering\nFormer solutino and drawbacks:\nNeRF and its following works can produce photo-realistic novel view synthesis results. But they need a long-time per-scene optimization Own Solution:\nGoal: use \u0026hellip; to \u0026hellip; 1 sentences introduce the name and functionality, properties. Analysis: generalizability -\u0026gt; avoid tedious per-scene optimization and regress directly novel viewpoints quantitative outcomes Tech stack\nMVSNet -\u0026gt; generalizable net of 3D reconstruction Cost volume is built by warping 2D img features of src views onto sweeping planes Regress geometry and appearance from a cost volume (per-voxel neural features) 3D cnn aggregates the cost volumes to a neural scene encoding volume (2023-08-16)\nPlay Environment inplace-abn needs to be installed from source, as issue#36. But I cannot install it:\n1 2 3 RuntimeError: The detected CUDA version (10.2) mismatches the version that was used to compile PyTorch (11.3). Please make sure to use the same CUDA versions. My cuda version is 11.3 as shown by:\n1 2 import torch print(torch.version.cuda) Then I specified the version the same as the pl-tutorial of AIkui:\n1 2 3 4 5 6 - pip - pip: - torch==1.11.0 - torchvision==0.12.0 - pytorch_lightning==1.6.0 - inplace_abn With that, inplace_abn has installed.\nBut the API has change since v1.5 (as issue) and resulted in: Exception has occurred: TypeError __init__() got an unexpected keyword argument 'distributed_backend'\nTry this setting:\n1 2 3 4 - torch==1.10.1 - torchvision==0.11.2 - pytorch_lightning==1.3.5 - inplace_abn Cannot import inplace_abn:\n1 2 Exception has occurred: ImportError /home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/inplace_abn/_backend.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c1015SmallVectorBaseIjE8grow_podEPvmm I checked my cuda version, which is 10.2:\n1 2 3 4 5 (mvsnerf) zichen@lambda-server:~/Downloads/mvsnerf-comments$ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Wed_Oct_23_19:24:38_PDT_2019 Cuda compilation tools, release 10.2, V10.2.89 According to Previous PyTorch Versions | PyTorch, I install the versions compiled with cuda10.2. Then print(torch.version.cuda) returned 10.2 instead.\n1 2 3 4 5 6 7 8 - pip - pip: - torch==1.10.1+cu102 - torchvision==0.11.2+cu102 - torchaudio==0.10.1 - -f https://download.pytorch.org/whl/cu102/torch_stable.html - pytorch_lightning==1.3.5 - inplace_abn Still cannot import inplace_abn with the same ImportError as above:\nThen Install from source, and inplace_abn can be installed:\nAnother Error:\n1 2 3 4 Exception has occurred: ImportError cannot import name \u0026#39;get_num_classes\u0026#39; from \u0026#39;torchmetrics.utilities.data\u0026#39; (/home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/torchmetrics/utilities/data.py) File \u0026#34;/home/zichen/Downloads/mvsnerf-comments/train_mvs_nerf_pl.py\u0026#34;, line 17, in \u0026lt;module\u0026gt; from pytorch_lightning.callbacks import ModelCheckpoint Change to specific versions, as summary:\n1 2 pip install torchmetrics==0.5.0 pip install setuptools==59.5.0 Numpy version incompatible issue#77\n1 ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part. File \u0026ldquo;/home/zichen/Downloads/mvsnerf-comments/data/dtu.py\u0026rdquo;, line 98, in build_proj_mats self.proj_mats, self.intrinsics = np.stack(proj_mats), np.stack(intrinsics)\n","date":"2022-12-06T14:19:00Z","image":"https://ar5iv.labs.arxiv.org/html/2103.15595/assets/x2.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-mvsnerf/","title":"read: Render - NVS | MVSNeRF"},{"content":"(2024-05-16)\nHomography ÊòØÊää‰∏ÄÂº†ÁÖßÁâáÂèòÂà∞Âè¶‰∏Ä‰∏™ËßÜËßí‰∏ã\nex: Ê°åÂ≠ê‰∏äÊîæÁùÄ‰∏ÄÂº† ÁÖßÁâáÔºå‰ªé‰∏çÂêåËßÜËßíÂéªÁúãÔºåÈô§‰∫ÜÊ≠£ËßÜÔºå‰ªéÂÖ∂‰ªñËßÜËßíÁúãÔºåÁúãÂà∞ÁÖßÁâáÈáåÁöÑÂÜÖÂÆπÂ∞±ÂèòÂΩ¢‰∫Ü„ÄÇ\nÊ≥®ÊÑèÔºöÂøÖÈ°ªÊòØÁÖßÁâáÔºå‰∏çË¶ÅÂÖ≥Ê≥®ÁÖßÁâáÈáåÁöÑ 3D Áâ©‰Ωì„ÄÇ Homography Â∞±ÊòØÊèèËø∞‰ªé‰∏çÂêåËßíÂ∫¶Áúã ÁÖßÁâáÔºåÁúãÂà∞ÁöÑÁÖßÁâáÁöÑÊ†∑Â≠ê„ÄÇ ‰∏çËÆ∫‰Ω†‰ªéÂì™ÁúãÔºåÂÆÉÂ∞±Âè™ÊòØ‰∏Ä‰∏™Âπ≥Èù¢ÔºåÊâÄ‰ª•ÊñúÁùÄÁúãÁÖßÁâáÔºå‰∏äÈù¢ÁöÑÂÜÖÂÆπÂ∞±‰ºöÊãâ‰º∏Êî∂Áº©ÔºöÊØîÂ¶Ç‰∏äÈù¢ÁöÑÂ≠óÊîæÂ§ßÁº©Â∞èÔºå‰∏Ä‰∏™Ê≠£ÊñπÂΩ¢ÂèòÊàêÊ¢ØÂΩ¢„ÄÇ ÂèòÊç¢ËßÜËßíÊó∂Ôºå‰∏ç‰ºöÂá∫Áé∞Áâ©‰ΩìÈó¥ÁöÑÈÅÆÊå°ÔºåÂõ†‰∏∫Âè™ÊòØÂú®ÂàÜÊûê‰∏Ä‰∏™ Âπ≥Èù¢„ÄÇ Homography warps two images to a common plane, or maps a pixel from a camera plane to another camera plane.\nA pixel can be mapped to another camera because the 2 pixels are the projections of a common 3D point.\nHomography warps the images captured from different angles, observing a common plane at a certain depth, to a unified viewpoint for stitching or other applications.\nA homography corresponds to only one depth. If the image doesn\u0026rsquo;t only capture a plane at a certain depth, the part with a deviated depth will be distorted in the warped image. Thus, solving homographies requires specifying depth.\nHomography relation is built based on the pose transformation between two cameras, and a depth embodied by a co-observed point or a co-observed plane. Or it can be sovled from 4 pairs of matched pixels.\nSpecifically, two cameras are connected via the 2 camera-space coordinates conversion for a depth-known co-observed 3D point.\nThe conversion can be represented by two camera\u0026rsquo;s poses in the world space, or the direct transformation from one camera to the other.\nThe depth of the co-observed 3D point can be given by specifying either the z-coordinate directly or a plane (ùêß,d) indirectly.\n(2023-12-19) Two scenarios for homography: stitching images based on a co-observed plane, and transferring viewpoint for a plane surface. They both rely on the observed plane.\nTwo Cameras The homography matrix is solved based on matching points in a pair of images.\nùê≤ ‚ÇÅ c a W m o ùêè 1 r ‚ÇÅ l d s ùêá p a ùêê c e ùêè c ‚ÇÇ a m 2 ùê≤ ‚ÇÇ P ùê± 2 r o t p j o l e a c n t e s $$ ùê≤‚ÇÅ = ùêè‚ÇÅ ‚ãÖ ùêê \\\\ ùê≤‚ÇÇ = ùêè‚ÇÇ ‚ãÖ ùêê \\\\ ùê≤‚ÇÇ = ùêè‚ÇÇ ‚ãÖ ùêè‚ÇÅ‚Åª¬π * ùê≤‚ÇÅ $$\nùêè is the projecting operation that converts a world coordinates of a point ùê± to pixel coordinates ùê≤.\n$ùêè = ùêä[ùêë|ùê≠]$ and $ùêè‚Åª¬π = [ùêë|ùê≠]‚Åª¬πùêä‚Åª¬π$\nùêè‚ÇÇ ‚ãÖ ùêè‚ÇÅ‚Åª¬π is the homography matrix.\nBesides ùêä[ùêë|ùê≠], the z-coordinate of covisible point ùêê (or the \u0026ldquo;distance from camera center to plane\u0026rdquo; d) is necessary.\np l a n e ùêß C c a e m n d e t : : : r e a r ùêê ( x , y , z ) Since the inverse intrisics $ùêä‚Åª¬π ‚ãÖ(u,v,1)·µÄ$ solely can\u0026rsquo;t determine a unique 3D point as z has been canceled when $(u,v,1)·µÄ=\\frac{1}{z}‚ãÖùêä‚Åª¬π ‚ãÖ (x,y,z)·µÄ$, the z must be specified at first. 5\n$$ (x,y,z)·µÄ = ùêä‚Åª¬π ‚ãÖz ‚ãÖ(u,v,1)·µÄ $$\nIf assuming camera\u0026rsquo;s z-axis is perpendicular to the observed plane, then z is indeed $d$. Thus, the z-retained coordinates of the pixel ùê≤‚ÇÅ is (ud,vd,d), such that $ùêä‚Åª¬π ‚ãÖd‚ãÖ(u,v,1)·µÄ$ is a unique 3D point in the camera space.\nRecall the NeRF\u0026rsquo;s sampling points (get_rays_np): meshgrid as pixel coordinates ‚Üí divided by focal (with z=1) to be in camera space ‚Üí @c2w to be in world space ‚Üí z vals sampling\nThus $ùêä‚Åª¬π ‚ãÖ(u,v,1)·µÄ$ only determines the viewdir, not a 3D point.\nThen, roll back it to world space and project to the other camera to get the pixel ùê≤‚ÇÇ.\n$$ ùê≤‚ÇÇ = ùêä‚ÇÇ[^{ùêë‚ÇÇ \\quad ùê≠‚ÇÇ}_{0 \\quad 1}] [^{ùêë‚ÇÅ \\quad ùê≠‚ÇÅ}_{0 \\quad 1}]‚Åª¬πùêä‚ÇÅ‚Åª¬π ‚ãÖd‚ãÖùê≤‚ÇÅ $$\nOverview: Plane1 ‚Üí camera1 ‚Üí world space ‚Üí camera 2 ‚Üí plane 2\nHowever, if either ùêê\u0026rsquo;s $z$ (or the distance $d$), or poses of two cameras is unknown, the 3x3 homography matrix can be solved from 4 pairs of matching pixels.\n(2023-12-04)\nBasis Changes Refer to Planar homographies - Department of Computer Science, University of Toronto\nIn a \u0026ldquo;meta space\u0026rdquo; holding two cameras, for example the world space, a ray emitted from the world origin passed through 2 world points ùê¶, ùêß locating on 2 planes separately.\nc W o M a o r e m r i t e l g a r d i a n P c l a ùêö a m ‚ÇÅ n e ùêö e r ùê¶ ‚ÇÇ a o f 1 ùêö ‚ÇÄ ùêõ ùêß ‚ÇÅ P c l a a m n e ùêõ e r ‚ÇÄ a o ùêõ f 2 ‚ÇÇ R a y $\\vec{a‚ÇÄ}$ and $\\vec{b‚ÇÄ}$ are the normal vector of two planes.\n$\\vec{a‚ÇÅ}$, $\\vec{a‚ÇÇ}$ are orthogonal, so they form a basis of the plane, same as $\\vec{b‚ÇÅ}$, $\\vec{b‚ÇÇ}$.\nThus, the world point ùê¶ on the plane can be represented by its planar basis as ùê© = (p‚ÇÅ,p‚ÇÇ,1) :\n$$ ùê¶ = ùêö‚ÇÅ p‚ÇÅ + ùêö‚ÇÇ p‚ÇÇ + ùêö‚ÇÄ = \\begin{pmatrix}ùêö‚ÇÅ \u0026amp; ùêö‚ÇÇ \u0026amp; ùêö‚ÇÄ\\end{pmatrix} \\begin{pmatrix} p‚ÇÅ \\\\ p‚ÇÇ \\\\ 1 \\end{pmatrix} = ùêÄ \\begin{pmatrix} p‚ÇÅ \\\\ p‚ÇÇ \\\\ 1 \\end{pmatrix} $$\nSpecifically, ùêö‚ÇÄ, ùêö‚ÇÅ, ùêö‚ÇÇ make up the camera space.\nSince each column in A is the camera (source) space basis axis seen from the world (original) space, ùêÄ contains c2w, and $ùêÄ = [ùêë|ùê≠]‚Çò‚Åª¬πùêä‚Çò‚Åª¬π$\nSimilarly, the point ùêß represented by the basis of its plane is ùê™ = (q‚ÇÅ,q‚ÇÇ,1):\n$$ùêß = \\begin{pmatrix}ùêõ‚ÇÅ \u0026amp; ùêõ‚ÇÇ \u0026amp; ùêõ‚ÇÄ\\end{pmatrix} \\begin{pmatrix} q‚ÇÅ \\\\ q‚ÇÇ \\\\ 1 \\end{pmatrix} = ùêÅ \\begin{pmatrix} q‚ÇÅ \\\\ q‚ÇÇ \\\\ 1 \\end{pmatrix} $$\nwhere ùêÅ can be interpreted as $ùêÅ= [ùêë|ùê≠]‚Çô‚Åª¬πùêä‚Çô‚Åª¬π$ Because the two world points are on the common line, and they satisfy the perspective projection centered at the world origin, so the only difference between them is a scaling factor.\n$$ùê¶ = Œ± ùêß$$\nŒ± is a scalar that depends on ùêß. And eventually, on ùê™. So Œ± is a function of ùê™, i.e., Œ±(ùê™). Substituting their coordinates ùê© and ùê™:\n$$ \\begin{aligned} ùêÄùê© = Œ±(ùê™) ‚ãÖùêÅ‚ãÖùê™ \\\\ ùê© = Œ±(ùê™) ‚ãÖùêÄ‚Åª¬π‚ãÖùêÅ‚ãÖùê™ \\end{aligned} $$\nA is invertible because its 3 rows (base vectors) are independent and nonzero as the plane doesn\u0026rsquo;t cross zero. If ùê¶ and ùêß are overlapped, i.e., the same point, which is just observed from different angles, the scale factor Œ± = 1.\n(2023-12-05)\nTransfer Camera Source article: Pinhole Camera: Homography - Êãæ‰∫∫ÁâôÊÖß - ÁóûÂÆ¢ÈÇ¶\nHomography transfers a pixel on cam1 to another camera\u0026rsquo;s film.\nA homography can be solved if the rotation and translation from one camera to the other and the co-observed plane (ùêß,d) is defined under the original camera space.\nIn other words, the camera-1\u0026rsquo;s space is regraded as the world space. Thus, the extrinsics of cam1 is [ùêà|0], and cam2\u0026rsquo;s extrinsics is [ùêë|ùê≠].\nùêê c ùê© a m ùêë 1 , ùê≠ , ( ùêß c , a d ùê™ m ) 2 ùêê ' Note: In this setting, the co-observed 3D point ùêê\u0026rsquo;s coordinates aren\u0026rsquo;t given, so the plane {ùêß,d} is necessary to derive the crucial z-coordinate of ùêê.\nSupp: A plane in a 3D space can be defined by its normal vector ùêß (a,b,c) and a point ùêê (x,y,z) located on it as: $ùêßùêê = 0$ (12.5: Equations of Lines and Planes in Space).\n$$ ùêß‚ãÖùêê = (a,b,c) (x-x‚ÇÄ, y-y‚ÇÄ, z-z‚ÇÄ) \\\\ = ax-ax‚ÇÄ+by-by‚ÇÄ+cz-cz‚ÇÄ = 0 $$\nwhere (x‚ÇÄ,y‚ÇÄ,z‚ÇÄ) is the \u0026ldquo;origin\u0026rdquo;. If the plane is displaced from the origin along its normal vector by distance $d$, the plane is $ùêßùêê-d=0$. (Plane Equation - Song Ho)\n$$ ùêß‚ãÖùêê = (a,b,c) (x-da-x‚ÇÄ, y-db-y‚ÇÄ, z-dc-z‚ÇÄ) \\\\ = ax-ax‚ÇÄ+by-by‚ÇÄ+cz-cz‚ÇÄ -d = 0 $$\nThus, ùêê can be determined by ùêß and d.\nThe normal vector ùêß uses coordinates in the camera-1, as it\u0026rsquo;s used to locate the point ùêê observed by camera-1.\nùêÇ d ‚ÇÅ : : : : ùêë , ùê≠ ùê© ùêä ‚Åª ùêß ¬π ùêê ùêä ùêê C ' o p m l ùê™ m a o n ùêÇ n e ‚ÇÇ Within the camera-1 space, the distance from the camera center ùêÇ‚ÇÅ to the plane can be expressed as the inner product of $ùêê$ (or $ùêÇ‚ÇÅùêê$) and $ùêß$\n$$ d = ùêß·µÄ‚ãÖùêê = 1‚ãÖ|ùêê|‚ãÖcosŒ∏ $$\nIf ùêß points towards the side with the camera-1, then Œ∏ is larger than 90¬∞, so d is negative. Thus, a minus sign will be added.\nIn MVSNet, all preset depth planes are parallel to the reference camera, so ùêß represented in the reference camera space is $(0,0,-1)·µÄ$, and $ùêß ùêë_{ref}$ (i.e., fronto_direction) is the opposite z-axis of $ùêë_{ref}$ (extrinsics) in the world space.\nIf $d$ and $ùêß$ are known, the coordinate of ùêê under camera-1 space can be written as:\nIf $d$ and ùêß are known, although there is $ùêê = \\frac{d}{ùêß·µÄ}$, the ùêê=(Qx,Qy,Qz)·µÄ can\u0026rsquo;t be determined uniquely based on it alone. Because there are 3 unkonwns with only 1 constraint, there is infinitely many solutions.\nHowever, the following equation can be used:\n$$ 1 = - \\frac{ùêß·µÄ‚ãÖùêê}{d} $$\nAssume the conversion between two cameras is: $$ùêê\u0026rsquo; = ùêëùêê + ùê≠$$\nùêë and ùê≠ transform the coordinates in camera-1 to coordinates in camera-2, i.e., the camera-1 is transformed to camera-2.\nùêê\u0026rsquo; is the coordinates of the covisible point in the camera-2\u0026rsquo;s space. With it, the target pixel on camera-2 is: $ùêä\u0026rsquo;ùêê\u0026rsquo;$.\nBy substituting the above equation of 1, ùêê\u0026rsquo; can be expressed with plane\u0026rsquo;s parameters:\n$$ ùêê\u0026rsquo; = ùêëùêê + ùê≠‚ãÖ1 = (ùêë -ùê≠ \\frac{ùêß·µÄ}{d})ùêê $$\nEssentially, the z-coordinate of ùêê is determined by the plane uniquely.\nThus, ùêê can be given by specifying z directly or a plane (ùêß,d) equivalently.\nAnd ùêê\u0026rsquo; can be represented by two cameras\u0026rsquo; poses or the transformation from one camera to the other.\n(2024-05-30) The point ùêê is written as a form with a plane, because the object described by homography is a plane.\nSuppose two cameras have different intrinsics ùêä and ùêä\u0026rsquo;, so their projection pixels are ùê© and ùê™ respectively:\n$$ ùê© = ùêäùêê \\quad \\text{and} \\quad ùê™ = ùêä\u0026rsquo; ùêê\u0026rsquo; $$\nSubstituting ùêê\u0026rsquo;, the ùê™ will be derived from ùê©:\n$$ ùê™ = ùêä\u0026rsquo; (ùêë-ùê≠ \\frac{ùêß·µÄ}{d})ùêê = ùêä\u0026rsquo; (ùêë-ùê≠ \\frac{ùêß·µÄ}{d}) ùêä‚Åª¬πùê© $$\nHence, the homography from ùê© to ùê™ is: $$ ùêá = ùêä\u0026rsquo; (ùêë-ùê≠ \\frac{ùêß·µÄ}{d}) ùêä‚Åª¬π $$\n(2024-05-10)\nThe way of introducing plane parameters in this homography formula is representing the coordinates of ùêê in camera1 by plane normal and d.\nAlthough $\\frac{d}{ùêß·µÄ}$ is not enough to determine the ùêê, the transformation [ùêë|ùê≠] serves as another constraint to ensure that the relationship between ùêê\u0026rsquo; and ùêê holds.\n(2023-12-06)\n$ùêä‚Åª¬π ‚ãÖ(u,v,1)·µÄ$ can\u0026rsquo;t restore a 3D point uniquely because z has been absorbed into u,v. (i.e., z is unknown in $\\rm ùêê = ùêä‚Åª¬π‚ãÖ z‚ãÖ (u,v,1)·µÄ$) Instead, it represents a line of points with various z, corresponding to the epipolar line on the other camera film.5\nTherefore, the distance from a camera to the plane is used to specify the point on a certain plane, which will be transformed first to the other camera space and then projected perspectively onto camera film.\n$$ \\begin{aligned} ùêê \u0026amp;= -\\frac{d}{ùêß·µÄ} \u0026amp; \\text{unique point} \\\\ ùêê\u0026rsquo; \u0026amp;= ùêë ùêê + ùê≠ \u0026amp; \\text{Another camera space} \\\\ ùê™ \u0026amp;= ùêä ùêê\u0026rsquo; = ùêä(ùêë ùêê +ùê≠) \u0026amp; \\text{perspective project} \\\\ \u0026amp;= ùêä (ùêë -ùê≠ \\frac{ùêß·µÄ}{d}) ùêê \\\\ \u0026amp;= ùêä (ùêë -ùê≠ \\frac{ùêß·µÄ}{d}) ùêä^{-1} ùê© \\end{aligned} $$\nIn this way, the projection on a camera film is mapped to another camera\u0026rsquo;s film.\nNote: the pixel coordinates on the two films are not the same.\n(2023-12-07)\nGeneralize H Source article: 5. Multi-View Stereo‰∏≠ÁöÑÂπ≥Èù¢Êâ´Êèè(plane sweep) - ewrfcasÁöÑÊñáÁ´† - Áü•‰πé\nThe generalized homography describes the pixel mapping from the reference plane ùêÇ‚ÇÅ to the plane of an arbitrary camera ùêÇ·µ¢.\nW o o r ùêë ùê≠ r i ‚ÇÅ ‚ÇÅ l g ‚ãÖ d i n ùêá ·µ¢ ùêë ‚ÇÅ ·µ¢ R , e ùê≠ f ·µ¢ ùê© 1 S c o a ùêê u m ùê™ r e ùêê c r ' e a i To apply the above derived expression ùêá, the camera Ref-1 should always be the \u0026ldquo;world space\u0026rdquo;, and the target plane is various source camera-i.\nThus, the coordinates ùêê\u0026rsquo; in a source camera-i space is transformed from ùêê in the Ref-1 camera space as:\n$$ ùêê\u0026rsquo; =\\begin{bmatrix} ùêë·µ¢ \u0026amp; ùê≠·µ¢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} ùêë‚ÇÅ \u0026amp; ùê≠‚ÇÅ \\\\ 0 \u0026amp; 1 \\end{bmatrix}^{-1} ùêê $$\nThe matrices product can be simplified as:\n$$ \\begin{bmatrix} ùêë·µ¢ \u0026amp; ùê≠·µ¢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} ùêë‚ÇÅ \u0026amp; ùê≠‚ÇÅ \\\\ 0 \u0026amp; 1 \\end{bmatrix}^{-1} = \\begin{bmatrix} ùêë·µ¢ \u0026amp; ùê≠·µ¢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\frac{1}{ùêë‚ÇÅ}\\begin{bmatrix} 1 \u0026amp; -ùê≠‚ÇÅ \\\\ 0 \u0026amp; ùêë‚ÇÅ \\end{bmatrix} = \\\\ \\begin{bmatrix} ùêë·µ¢ \u0026amp; ùê≠·µ¢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix}ùêë‚ÇÅ‚Åª¬π \u0026amp; -ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ \\\\ 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} ùêë·µ¢ùêë‚ÇÅ‚Åª¬π \u0026amp; -ùêë·µ¢ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùê≠·µ¢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\\\ $$\nTherefore, the generalized relation between ùêê\u0026rsquo; and ùêê is:\n$$ ùêê\u0026rsquo; = \\tilde{ùêë}ùêê+ \\tilde{ùê≠} = ùêë·µ¢ùêë‚ÇÅ‚Åª¬π ùêê + (-ùêë·µ¢ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùê≠·µ¢)$$\nSubstituting $\\tilde{ùêë}$ and $\\tilde{ùê≠}$ into the planar homography:\n$$ ùê™ =ùêä·µ¢ (ùêë·µ¢ùêë‚ÇÅ‚Åª¬π - \\frac{(-ùêë·µ¢ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùê≠·µ¢) ùêß·µÄ}{d} ) ùêä‚ÇÅ‚Åª¬πùê© $$\nRearrange it to align with the MVSNet paper:\n$$ ùê™ =ùêä·µ¢ ùêë·µ¢(ùêë‚ÇÅ‚Åª¬π - \\frac{(-ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùêë·µ¢‚Åª¬πùê≠·µ¢)ùêß·µÄ}{d}) ùêä‚ÇÅ‚Åª¬πùê© \\\\ = ùêä·µ¢ ùêë·µ¢ ( ùêà - \\frac{(-ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùêë·µ¢‚Åª¬πùê≠·µ¢) ùêß·µÄ ùêë‚ÇÅ}{d})ùêë‚ÇÅ‚Åª¬π ùêä‚ÇÅ‚Åª¬πùê© $$\nThe eq. (1) in MVSNet was wrong: $ùêá_i(d) = ùêä·µ¢ ùêë·µ¢ ( ùêà - \\frac{(ùê≠‚ÇÅ-ùê≠·µ¢) ùêß·µÄ}{d})ùêë‚ÇÅ·µÄ ùêä‚ÇÅ·µÄ$, (issue #77)\n$ùêä‚ÇÅ‚Åª¬π \\neq ùêä‚ÇÅ·µÄ$ since ùêä may not be an orthogonal matrix. ùêë·µ¢‚Åª¬π and ùêë‚ÇÅ can\u0026rsquo;t be cancled. Corresponding to MVSNet-TF code, c_left is $-ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ$; c_right is $-ùêë·µ¢‚Åª¬πùê≠·µ¢$; c_relative is $-(-ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùêë·µ¢‚Åª¬πùê≠·µ¢)$ fronto_direction is $-ùêß·µÄ ùêë‚ÇÅ$, which is R_left[2,:]. Therefore, temp_vec = tf.matmul(c_relative, fronto_direction) is $(-ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùêë·µ¢‚Åª¬πùê≠·µ¢) ùêß·µÄ ùêë‚ÇÅ$.\nWarp An Image (2023-12-19) Homography warps a non-rectangle to a rectangle.\n(2023-02-08)\nHomography matrix warps an image to another plane. In the application of image stitching, multiple images are warped to a unified plane.\nThe images have to be observing the common plane.\nIt works for stitching planar panoramas because the scene is far from the camera.\n(2023-12-17) In other words, co-planar points remain co-planar after a homography transformation (matrix). For example, a point on a plane that in image-1 remains located on the plane viewed by image-2.\nTwo pairs of matched points will perform the same transformation: $$\\begin{cases}p_1 = H p_2 \\\\ q_1 = H q_2 \\end{cases}$$\nPut differently, \u0026ldquo;a plane remains a plane in another view\u0026rdquo;, while the portion that is not on the plane will be warped.\nR,T can be solved from H and K.\nDemo: Warp the image to simulate the scene seen from another displaced camera (Aerial View). Refer to: OTC4 HomographyË™™Êòé‰ª•ÂèäÂ∞èÂØ¶È©ó-AIËëµ\nSelect 10 points to form a plane, which remains a plane in another view after transformion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import cv2 import numpy as np import matplotlib.pyplot as plt %matplotlib inline img1 = cv2.imread(\u0026#34;rot1.png\u0026#34;) pts1 = [(24, 124), (49, 124), (98, 124), (104, 124), (120, 124), (18, 146), (37, 146), (65, 146), (102, 146), (133, 146)] for pt in pts1: cv2.circle(img1, pt, 0, (0, 0, 255)) plt.figure(figsize=(20,10)) plt.imshow(img1[:,:, ::-1]) img2 = cv2.imread(\u0026#34;rot2.png\u0026#34;) pts2 = [(45, 116), (64, 120), (110, 127), (118, 128), (135, 131), (31, 135), (46, 137), (69, 143), (105, 150), (140, 158)] src_points = np.array(pts1, dtype=np.float32).reshape(-1,1,2) dst_points = np.array(pts2, dtype=np.float32).reshape(-1,1,2) Homography, mask = cv2.findHomography(src_points, dst_points, 0, 0.0) # (3,3) # warp img1 to the view of img2 warped1 = cv2.warpPerspective(img1, Homography, (img1.shape[1], img1.shape[0])) plt.figure(figsize=(20,10)) plt.imshow(warped1[:,:,::-1]) # reverse plt.imshow(img2[:,:,::-1], alpha=0.6) # overlap Bird\u0026rsquo;s-Eye View:\n1 2 3 4 5 src_trapezoid = np.float32([[1, 128], [47, 91], [120, 91], [157, 128]]) dst_rectangle = np.float32([[64, 104], [64, 64], [104, 64], [104, 104]]) homoMatrix = cv2.getPerspectiveTransform(src_trapezoid, dst_rectangle) warped = cv2.warpPerspective(img, homoMatrix,(img.shape[1], img.shape[0])) The image got transformed by the homography to deform the trapezoidal road lane to a rectangle displaying on the image by changing the camera pose to looking down:\nBEV is a homography, but only make sense for the ground plane. ÂçïÂ∫îÊÄßHomograph‰º∞ËÆ°Ôºö‰ªé‰º†ÁªüÁÆóÊ≥ïÂà∞Ê∑±Â∫¶Â≠¶‰π† - ÁôΩË£≥ÁöÑÊñáÁ´† - Áü•‰πé Multi-Depth Planes (2023-12-17)\nGiven camera parameters of a reference view and a source view in the world space respectively, and multiple depth planes, solve the homographies that transforming the source image into the reference camera space:\nR e ùêä ùêë ùê≠ f ·µ£ ·µ£ ·µ£ v i ùêä ùêë ùê≠ e ‚Çõ ‚Çõ ‚Çõ w S i ùê≥ r m n ùêß c g o r m ùêá a l G i d v ‚ÇÅ e n d e p d t ‚ÇÇ h p l a n d e ‚ÇÉ s : d ‚ÇÑ The formula used in MVSNet-tf code is: $ùêä·µ¢ùêë·µ¢ ( ùêà - \\frac{-(-ùêë‚ÇÅ‚Åª¬πùê≠‚ÇÅ+ùêë·µ¢‚Åª¬πùê≠·µ¢) (-ùêß·µÄ ùêë‚ÇÅ)}{d})ùêë‚ÇÅ‚Åª¬πùêä‚ÇÅ‚Åª¬π$ because the $-ùêß·µÄ ùêë‚ÇÅ$ is the z-axis of ùêë‚ÇÅ in the world space.\nA source images is warped to the reference camera space and watching a common depth plane. Therefore, the points on the depth plane will have the same x coordinates as the points in the reference image.\nWarp by Sampling (2023-12-18)\nSample the source image to be warped at the locations that is determined by the mapping of homography inversely from the target view to the source view. Refer to MVSNet-pytorch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch from torchvision import transforms h, w = ref.shape[:2] vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed uv1 = torch.cat([uv, torch.ones(len(uv), 1)], dim=-1) # (hw,3) src_KRT = torch.eye(4) src_KRT[:3] = torch.from_numpy(K_s @ RT_s[:3, :4]) ref_KRT = torch.eye(4) ref_KRT[:3] = torch.from_numpy(K_r @ RT_r[:3, :4]) proj = src_KRT @ torch.inverse(ref_KRT) rot, trans = torch.split(proj, [3,1], dim=-1) rot_uv1 = rot[:3] @ uv1.t() # (3, hw) d = 425.0 + 1.06*2.5 * torch.arange(192).view(1,-1,1) #(1, 192, 1) rot_uvd = rot_uv1.unsqueeze(1).expand(-1, 192,-1) * d #(3, 192, hw) pix_proj = rot_uvd + trans[:3].unsqueeze(1).expand(-1, 192, -1) # (3, 192, hw) u_src = 2*(pix_proj[0] / pix_proj[2])/(w-1)-1 v_src = 2*(pix_proj[1] / pix_proj[2])/(h-1)-1 uv_src = torch.stack([u_src, v_src], dim=-1) # (192, hw, 2) uv_src_d1 = uv_src[-1] # plane at the furthest depth sampled = torch.nn.functional.grid_sample( transforms.ToTensor()(src).unsqueeze(0), # (1,3,512,640) uv_src_d1.view(1, h, w, 2), mode=\u0026#39;nearest\u0026#39;) sampled_scaled = torch.tensor(sampled[0] * 255, dtype=int) # (3, 512, 640) fig, ax = plt.subplots(1,3, figsize=(30,15)) ax[0].imshow(src[:,:,::-1]) # (512, 640, 3) ax[0].set_title(\u0026#34;Source view\u0026#34;) ax[1].imshow(sampled_scaled.permute(1,2,0).numpy()[:,:,::-1]) ax[1].set_title(\u0026#34;Warped source\u0026#34;) ax[2].imshow(ref[:,:,::-1]) ax[2].set_title(\u0026#34;Reference view\u0026#34;) The result is almost the same as the one yielded by opencv:\n4 Co-planar Points (2022-12-03)\nHomography matrix builds the relatioship between 2 images to make the pixels corresponding to the same 3D points overlap ¬≥ based on the projection from the common 3D point to different image planes.\nIn following example, two cameras look at a flat plane. And they both observe the points located on that flat plane.\nThen, the matching pixels on the two camera planes can be connected by the chain: img1 ‚ûî plane0 ‚ûî img2.\nWhen a 3D world point is projected to 2D camera plane, the coordinates transformation consists of two matrix: extrinsic matrix [R|T] transforming the coordinate system, and the intrinsic matrix [K] projecting the 3D point to a 2D pixel, that is:\n$$ \\begin{bmatrix} u\\\\ v\\\\ 1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f/d‚Çì \u0026amp; 0 \u0026amp; c‚Çì \u0026amp; 0 \\\\ 0 \u0026amp; f/d_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} \\begin{bmatrix} r‚ÇÅ‚ÇÅ \u0026amp; r‚ÇÅ‚ÇÇ \u0026amp; r‚ÇÅ‚ÇÉ \u0026amp; t‚ÇÅ \\\\ r‚ÇÇ‚ÇÅ \u0026amp; r‚ÇÇ‚ÇÇ \u0026amp; r‚ÇÇ‚ÇÉ \u0026amp; t‚ÇÇ \\\\ r‚ÇÉ‚ÇÅ \u0026amp; r‚ÇÉ‚ÇÇ \u0026amp; r‚ÇÉ‚ÇÉ \u0026amp; t‚ÇÉ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ W \\end{bmatrix} $$\nwhere the W is the homogeneous coordinate for accommodating the translation vector $[^{^{t‚ÇÅ}_{t‚ÇÇ}} _{^{t‚ÇÉ}_{1}}]$ in the extrinsic matrix.\nOmit z because all points on the plane have z=0.\nGiven $[^{_{x‚ÇÅ}}_{^{y‚ÇÅ}_{w‚ÇÅ}}]$ = H‚ÇÅ‚ÇÄ $[^{_{x‚ÇÄ}}_{^{y‚ÇÄ}_{w‚ÇÄ}}]$, then there is $$\\rm [^{_{x‚ÇÇ}}_{^{y‚ÇÇ}_{w‚ÇÇ}}] = H‚ÇÇ‚ÇÄ H‚ÇÅ‚ÇÄ‚Åª¬π[^{_{x‚ÇÅ}}_{^{y‚ÇÅ}_{w‚ÇÅ}}]$$, where H‚ÇÇ‚ÇÄ H‚ÇÅ‚ÇÄ‚Åª¬π is the homograhpy matrix H‚ÇÉ‚Çì‚ÇÉ. It represents the mapping between the pair of points: (x‚ÇÅ,y‚ÇÅ,w‚ÇÅ) and (x‚ÇÇ,y‚ÇÇ,w‚ÇÇ).\n$$ \\begin{bmatrix} x‚ÇÇ\\\\ y‚ÇÇ\\\\ w‚ÇÇ \\end{bmatrix} = \\begin{bmatrix} H‚ÇÅ‚ÇÅ \u0026amp; H‚ÇÅ‚ÇÇ \u0026amp; H‚ÇÅ‚ÇÉ \\\\ H‚ÇÇ‚ÇÅ \u0026amp; H‚ÇÇ‚ÇÇ \u0026amp; H‚ÇÇ‚ÇÉ \\\\ H‚ÇÉ‚ÇÅ \u0026amp; H‚ÇÉ‚ÇÇ \u0026amp; H‚ÇÉ‚ÇÉ \\\\ \\end{bmatrix} \\begin{bmatrix} x‚ÇÅ\\\\ y‚ÇÅ\\\\ w‚ÇÅ \\end{bmatrix} $$\nExpand the above equation, there are 2 equations (with considering x‚ÇÇ\u0026rsquo;=x‚ÇÇ/w‚ÇÇ, y‚ÇÇ\u0026rsquo;=y‚ÇÇ/w‚ÇÇ, such that w = 1):\n$$\\begin{cases} x‚ÇÇ\u0026rsquo; = \\frac{x‚ÇÇ}{w‚ÇÇ} = \\frac{H‚ÇÅ‚ÇÅx‚ÇÅ + H‚ÇÅ‚ÇÇy‚ÇÅ + H‚ÇÅ‚ÇÉw‚ÇÅ}{H‚ÇÉ‚ÇÅx‚ÇÅ + H‚ÇÉ‚ÇÇy‚ÇÅ + H‚ÇÉ‚ÇÉw‚ÇÅ} \\\\ y‚ÇÇ\u0026rsquo; = \\frac{y‚ÇÇ}{w‚ÇÇ} = \\frac{H‚ÇÇ‚ÇÅx‚ÇÅ + H‚ÇÇ‚ÇÇy‚ÇÅ + H‚ÇÇ‚ÇÉw‚ÇÅ}{H‚ÇÉ‚ÇÅx‚ÇÅ + H‚ÇÉ‚ÇÇy‚ÇÅ + H‚ÇÉ‚ÇÉw‚ÇÅ}\\end{cases} \\\\ ‚áì \\\\ \\rm\\{^{x‚ÇÇ\u0026rsquo;H‚ÇÉ‚ÇÅx‚ÇÅ + x‚ÇÇ\u0026rsquo;H‚ÇÉ‚ÇÇy‚ÇÅ + x‚ÇÇ\u0026rsquo;H‚ÇÉ‚ÇÉw‚ÇÅ - H‚ÇÅ‚ÇÅx‚ÇÅ - H‚ÇÅ‚ÇÇy‚ÇÅ - H‚ÇÅ‚ÇÉw‚ÇÅ = 0} _{y‚ÇÇ\u0026rsquo;H‚ÇÉ‚ÇÅx‚ÇÅ + y‚ÇÇ\u0026rsquo;H‚ÇÉ‚ÇÇy‚ÇÅ + y‚ÇÇ\u0026rsquo;H‚ÇÉ‚ÇÉw‚ÇÅ - H‚ÇÇ‚ÇÅx‚ÇÅ - H‚ÇÇ‚ÇÇy‚ÇÅ - H‚ÇÇ‚ÇÉw‚ÇÅ = 0} $$\nThe last element H‚ÇÉ‚ÇÉ is determined when other elements are set, so the degree of freedom for this matrix is 8.\nTherefore, the homography matrix can be solved from 4 pairs of matching points: (x‚ÇÅ,y‚ÇÅ)-(x‚ÇÇ,y‚ÇÇ); (x‚ÇÉ,y‚ÇÉ)-(x‚ÇÑ,y‚ÇÑ); (x‚ÇÖ,y‚ÇÖ)-(x‚ÇÜ,y‚ÇÜ); (x‚Çá,y‚Çá)-(x‚Çà,y‚Çà).\nAfter reformalizing the equation system to a matrix form, the sigular value decomposition can be applied.\n$$\\rm{ [^{^{^{-x‚ÇÅ\\; -y‚ÇÅ\\; -w‚ÇÅ\\; 0\\quad 0\\quad 0\\quad x‚ÇÇ\u0026rsquo;x‚ÇÅ\\quad x‚ÇÇ\u0026rsquo;y‚ÇÅ\\quad x‚ÇÇ\u0026rsquo;w‚ÇÅ} _{0\\quad 0\\quad 0\\quad -x‚ÇÅ\\; -y‚ÇÅ\\; -w‚ÇÅ\\; y‚ÇÇ\u0026rsquo;x‚ÇÅ\\quad y‚ÇÇ\u0026rsquo;y‚ÇÅ\\quad y‚ÇÇ\u0026rsquo;w‚ÇÅ}} _{^{-x‚ÇÉ\\; -y‚ÇÉ\\; -w‚ÇÉ\\; 0\\quad 0\\quad 0\\quad x‚ÇÑ\u0026rsquo;x‚ÇÉ\\quad x‚ÇÑ\u0026rsquo;y‚ÇÉ\\quad x‚ÇÑ\u0026rsquo;w‚ÇÉ} _{0\\quad 0\\quad 0\\quad -x‚ÇÉ\\; -y‚ÇÉ\\; -w‚ÇÉ\\; y‚ÇÑ\u0026rsquo;x‚ÇÉ\\quad y‚ÇÑ\u0026rsquo;y‚ÇÉ\\quad y‚ÇÑ\u0026rsquo;w‚ÇÉ}}} _{^{^{pair3-1}_{pair3-2}} _{^{pair4-1}_{pair4-2}}}] \\ [^{^{^{H‚ÇÅ‚ÇÅ}_{H‚ÇÅ‚ÇÇ}} _{^{H‚ÇÅ‚ÇÉ}_{H‚ÇÇ‚ÇÅ}}} _{^{^{H‚ÇÇ‚ÇÇ}_{H‚ÇÇ‚ÇÉ}} _{^{H‚ÇÉ‚ÇÅ}_{^{H‚ÇÉ‚ÇÇ}_{H‚ÇÉ‚ÇÉ}}}}] = 0 } $$\nSolution This is a homogeneous linear system ùêÄùê°=0, so its solution has 2 cases:\nIf the number of non-zero rows of the coefficients matrix ùêÄ after Gaussian elimination is less than the number of unknowns of ùê°, i.e., (rank(ùêÄ) \u0026lt; #h), there is only the zero solution;\nOr infinitely many non-zero solution, but only the solution satisfying constraints of magnitude is selected.\nIf ùêÄ is invertible (square matrix \u0026amp; rank(ùêÄ) = #col), then ùê°=ùêÄ‚Åª¬π‚ãÖ0 = 0 ???\nOtherwise, if ùêÄ is not invertible, there would be inifinetly many non-zero solutions. So the optimal solution should be approached by minimizing the error $$J = ¬Ω‚ÄñùêÄùê°-0‚Äñ¬≤$$ Therefore, the least-square solution is when the derivative\n$$ ‚àÇJ/‚àÇùê° = 0 \\\\ ‚áí ùêÄ·µÄ(ùêÄùê°-0) = 0\\\\ ‚áí ùêÄ·µÄùêÄùê°-ùêÄ·µÄ‚ãÖ0 = 0 $$\nIf ùêÄ·µÄùêÄ is invertible, the optimal ùê°=(ùêÄ·µÄùêÄ)‚Åª¬πùêÄ·µÄ‚ãÖ0 = 0 ???, where the (ùêÄ·µÄùêÄ)‚Åª¬πùêÄ·µÄ is the pseudo-inverse matrix.\nUse all the matching pints to compute a robust H, even though 4 pairs are enough to solve the 8 unkowns (An element is 1 after scaling the matrix by it). Constrained Least Squares. Solve for ùê°: Aùê°=0, such that ‚Äñùê°‚Äñ¬≤=1. Define least squares problem: min_ùê° ‚ÄñAùê°‚Äñ¬≤, such that ‚Äñùê°‚Äñ¬≤=1. We know that: ‚ÄñAùê°‚Äñ¬≤ = (Aùê°)·µÄ(Aùê°) = ùê°·µÄA·µÄAùê°, and ‚Äñùê°‚Äñ¬≤= ùê°·µÄùê° =1. So the problem is: min‚Çï(ùê°·µÄA·µÄAùê°), such that ùê°·µÄùê° =1.\nDefine Loss function L(ùê°,Œª): L(ùê°,Œª)= ùê°·µÄA·µÄAùê° - Œª(ùê°·µÄùê° -1). And take derivatives of L(ùê°,Œª) w.r.t. ùê°: 2A·µÄAùê° - 2Œªùê° =0. It\u0026rsquo;s the classical eigenvalue problem: A·µÄAùê° = Œªùê°.\nSVD. Find the eigenvalue and eigenvectors of the matrix A·µÄA. ùê° is he eighenvector with the smallest eigenvalue Œª.\nFirst Principles of CV-Dr.Shree;\nMisc (2023-12-07) Epipolar geometry requires translation between cameras, whereas homography doesn\u0026rsquo;t. homography - Carleton University Ref OpenCV4 - Docs - Basic concepts of the homography explained with code Homography in computer vision explained - Behnam Asadi 3DÊäïÂΩ±ÂèòÊç¢ÔºàÂê´ÈÄèËßÜÊäïÂΩ±Perspective ProjectionÔºâ-Â≠êÁáïËã•Ê∞¥ -CSDN ","date":"2022-12-03T17:30:00-05:00","permalink":"https://zichen34.github.io/writenotes/vis/planar_homography/","title":"memo: Vis | Planar Homography"},{"content":"ÂéüËßÜÈ¢ëÔºö‚ÄúL1ÂíåL2Ê≠£ÂàôÂåñ‚ÄùÁõ¥ËßÇÁêÜËß£(‰πã‰∏Ä)Ôºå‰ªéÊãâÊ†ºÊúóÊó•‰πòÊï∞Ê≥ïËßíÂ∫¶ËøõË°åÁêÜËß£ - ÁéãÊú®Â§¥Â≠¶ÁßëÂ≠¶ -bilibili\n(2023-01-30) Âä†‰∏äÊ≠£ÂàôÂåñÈ°πÔºåÂ∞±Âèò‰∏∫Ê±ÇÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄºÔºåÂç≥Ê±Ç\u0026quot;ÊçüÂ§±ÂáΩÊï∞+ÊùÉÈáçÊ®°Èïø\u0026quot;Êï¥‰ΩìÁöÑÊúÄÂ∞èÂÄº„ÄÇ Œª ÊòØ‰∏™ÊØîÂÄºÔºåŒª‰∏çÂêåÂØπÂ∫îÁöÑWÁöÑ‰ΩçÁΩÆ‰∏çÂêåÔºåÂç≥ŒªÁ°ÆÂÆö‰∫ÜWÁöÑ‰ΩçÁΩÆ„ÄÇÁÑ∂Âêé ùêñ Ëá™Â∑±‰∏çÊñ≠Ë∞ÉÊï¥Ôºå‰Ωø J(ùêñ) ‰∏é Œª‚Äñùêñ‚Äñ ‰∫åËÄÖÁöÑÊ¢ØÂ∫¶Á≠âÂ§ßÂèçÂêë(ÊäµÊ∂à)Ôºå‰ªéËÄå‰Ωø $‚àá_ùêñ L(ùêñ,Œª)=0$„ÄÇŒª‰∏çÂêåÔºåÂàôÊúÄ‰ºòWÂ∞±‰∏çÂêå„ÄÇ_\nÊú∫Âô®Â≠¶‰π†ÁöÑ‰∏§‰∏™Ê†∏ÂøÉËÆÆÈ¢òÔºö‰ºòÂåñ‰∏éÊ≠£ÂàôÂåñ„ÄÇ‰ºòÂåñÊâæÂà∞ÊúÄ‰ºòÂèÇÊï∞ÔºõÊ≠£ÂàôÂåñÂáèÂ∞ëÊ®°ÂûãÁöÑËøáÊãüÂêàÔºöÂØπÊ®°ÂûãÊùÉÈáçùêñÔºà‰∏çÂ§ÑÁêÜbÔºâËøõË°åL1ÂíåL2Ê≠£ÂàôÂåñ\n‰ªé3‰∏™ËßíÂ∫¶ÁêÜËß£ L1 Âíå L2 Ê≠£ÂàôÂåñÔºö\nÊãâÊ†ºÊúóÊó•‰πòÊï∞Ê≥ï ÊùÉÈáçË°∞Âáè Ë¥ùÂè∂ÊñØÊ¶ÇÁéá Ê≠£ÂàôÂåñ ‰∏é‚ÄúÊ≠£ÂàôË°®ËææÂºè‚ÄùÊ≤°ÊúâÂÖ≥Á≥ªÔºåÊ≠£ÂàôË°®ËææÂºèÊòØÁºñÁ®ã‰∏≠Áî®Êù•Â§ÑÁêÜÂ≠óÁ¨¶‰∏≤ÁöÑÊäÄÊúØ„ÄÇ Ê≠£ÂàôÂåñÂ∏∏Â∏∏ÊåáÊ®°ÂûãÊùÉÈáçÁöÑL1ÂíåL2ËåÉÊï∞„ÄÇDropout ‰πüÊòØ‰∏ÄÁßçÊ≠£ÂàôÂåñÔºåÂú®ËÆ≠ÁªÉÊó∂ÔºåÈöèÊú∫ÁöÑËÆ©‰∏Ä‰∫õÁ•ûÁªèÂÖÉÂ§±Êïà„ÄÇ\nËä±‰π¶ÔºöÂá°ÊòØÂèØ‰ª•ÂáèÂ∞ëÊ≥õÂåñËØØÂ∑ÆÔºàËøáÊãüÂêàÔºâÔºåËÄå‰∏çÊòØÂáèÂ∞ëËÆ≠ÁªÉËØØÂ∑ÆÁöÑÊñπÊ≥ïÔºåÈÉΩÂèØ‰ª•Áß∞‰ΩúÊ≠£ÂàôÂåñÊñπÊ≥ï„ÄÇ\nL1, L2 Ê≠£ÂàôÂåñÈ°πÊòØÂä†Âú®ÊçüÂ§±ÂáΩÊï∞‰∏äÔºåÁ∫¶ÊùüÊ®°ÂûãÂèÇÊï∞ùêñÁöÑ L1, L2 ËåÉÊï∞ÁöÑÈ°π„ÄÇL1ËåÉÊï∞‰ΩøWÂèòÂæóÁ®ÄÁñèÔºåL2ËåÉÊï∞‰ΩøWÂèòÂæóÂ∞è„ÄÇ\nÊãüÂêàÊó∂‰∏ÄËà¨Â∏åÊúõ‰øùÁïôÊõ¥Â§öÁâπÂæÅÔºåÈ´òÊ¨°ÁöÑÂáΩÊï∞Ë°®ËææËÉΩÂäõÊõ¥Âº∫ÔºåÊâÄ‰ª•Ê®°ÂûãÂÄæÂêë‰∫éÂ≠¶Âá∫È´òÊ¨°È°πÁöÑÁ≥ªÊï∞Ôºå‰ΩøËÆ≠ÁªÉËØØÂ∑ÆÊõ¥Â∞èÔºå‰ΩÜ‰∏çËÉΩÂæàÂ•ΩÂú∞È¢ÑÊµãÊñ∞Êï∞ÊçÆÔºåÊ≤°ÊúâÊâæÂà∞Âü∫Á°ÄËßÑÂæãÔºåÂá∫Áé∞ËøáÊãüÂêà 3„ÄÇ\nÂ¶ÇÊûúÁâπÂæÅÔºàÂêÑÂ±ûÊÄßÔºâÂ∑≤Áü•ÔºåÂèØ‰ª•Êää polynomials ÁöÑÈ´òÊ¨°È°πÁ≥ªÊï∞ÁΩÆ‰∏∫0ÔºõÂê¶ÂàôÔºåÈúÄÈááÁî®‰∏é\u0026quot;ÁâπÂæÅÁöÑÂÆö‰πâ\u0026quot;Êó†ÂÖ≥ÁöÑÁ∫¶ÊùüÔºåÂèØ‰ª•ÈÄâÊã©Ê®°ÈïøÊúÄÂ∞èÁöÑÔºåËØØÂ∑Æ(mse)‰πüÊúÄÂ∞èÁöÑÂèÇÊï∞„ÄÇ Problem is now well-posed for any degree„ÄÇEven very high polynomials, simple function tend to be learned with regularization2 ÊâÄ‰ª•ÂØπÁªÜËäÇÁâπÂæÅÁöÑÁ≥ªÊï∞ÔºàŒ∏‚ÇÉ¬≤+Œ∏‚ÇÑ¬≤ÔºâÂ¢ûÂ§ßÊÉ©ÁΩöÂäõÂ∫¶Ôºåbias them not to be large Ôºàmin ‚àë·µ¢·µê(y^(x·µ¢)-y·µ¢)¬≤ + 1000Œ∏‚ÇÉ¬≤+1000Œ∏‚ÇÑ¬≤Ôºâ1\nËåÉÊï∞ÔºöÂêëÈáèÈïøÂ∫¶„ÄÇÁΩëÁªúÊùÉÈáç ùêñ ÊòØÈ´òÁª¥Á©∫Èó¥‰∏≠ÁöÑ‰∏Ä‰∏™ÂêëÈáèÔºà‰πüÂèØËÆ§‰∏∫WÊòØÈ´òÁª¥Á©∫Èó¥‰∏Ä‰∏™ÁÇπÔºåËåÉÊï∞Âç≥‰∏∫Ëøô‰∏™ÁÇπÂà∞ÂéüÁÇπÁöÑË∑ùÁ¶ªÔºâ„ÄÇ Ëã•‰ΩøÁî®Ê¨ßÂá†ÈáåÂæóË∑ùÁ¶ªÔºàÂùêÊ†áÂ∑ÆÊ±ÇÂπ≥ÊñπÂíåÂÜçÂºÄÊ†πÂè∑ÔºâÂ∫¶ÈáèÂÆÉÁöÑÊ®°ÈïøÔºåÂ∞±ÊòØ L2 ËåÉÊï∞: ‚Äñùêñ‚Äñ‚ÇÇ=‚àö(|w‚ÇÅ|¬≤+|w‚ÇÇ|¬≤+\u0026hellip;+|w·µ¢|¬≤)Ôºõ Ëã•Áî®ÊõºÂìàÈ°øË∑ùÁ¶ªËÆ°ÁÆóÔºàÂùêÊ†áÂ∑ÆÁöÑÁªùÂØπÂÄºÊ±ÇÂíåÔºâÔºåÂ∞±ÊòØ L1 ËåÉÊï∞: ‚Äñùêñ‚Äñ‚ÇÅ=|w‚ÇÅ|+|w‚ÇÇ|+\u0026hellip;+|w·µ¢|„ÄÇ\nÂΩì Lp ËåÉÊï∞ÁöÑ p ÂèñÂÄºÂ§ß‰∫éÁ≠â‰∫é 1 Êó∂ÔºåÊúâÁõ∏ÂêåË∑ùÁ¶ªÁöÑÁÇπÊûÑÊàêÁöÑÈõÜÂêàÊòØ‰∏Ä‰∏™Âá∏ÈõÜÔºõ ÂΩì p Âèñ 0-1 ‰πãÈó¥ÔºåÊòØÈùûÂá∏ÈõÜ„ÄÇ ÂΩìpÂ∞è‰∫éÁ≠â‰∫é1Êó∂Ôºå‰ºöÂ∏¶Êù•Á®ÄÁñèÊÄßÔºåÊâÄ‰ª• the L1 norm is the only norm which both induces some sparsity in the solution and remains convex for easy optimization. Â¶ÇÊûúÈóÆÈ¢òÂØπÂ∫îÁöÑÂáΩÊï∞ÊòØ‰∏Ä‰∏™Âá∏ÂáΩÊï∞ÔºåÂÆÉÁöÑÂèñÂÄºËåÉÂõ¥ÔºàÂèØË°åÂüüÔºâÊòØ‰∏Ä‰∏™Âá∏ÈõÜÔºåËøôÂ∞±ÊòØ‰∏Ä‰∏™Âá∏‰ºòÂåñÈóÆÈ¢òÔºåÂÆπÊòìÊ±ÇËß£„ÄÇL1ÂíåL2Ê≠£ÂàôÂåñÂú®ÊüêÁßçÁ®ãÂ∫¶‰∏äÔºåÂ∞±ÊòØÂú®Âà©Áî® L1 Âíå L2 ËåÉÊï∞ÁöÑÂá∏ÈõÜÁâπÊÄß„ÄÇ\nÂá∫Áé∞ËøáÊãüÂêàÁöÑÂéüÂõ†‰πã‰∏ÄÔºöWÂ§™Â§ß‰ºöÊääÂô™Â£∞ÊîæÂ§ß Á•ûÁªèÁΩëÁªúÁöÑËæìÂá∫Â±ÇÂÅö‰∫Ü‰∏â‰ª∂‰∫ãÔºö\nÊé•Êî∂‰∏ä‰∏ÄÂ±ÇÁöÑÊøÄÊ¥ªÂÄº a‚ÅΩÀ°‚Åª¬π‚ÅæÔºå‰πò‰ª•ÊùÉÈáç WÀ°·µÄÔºåÂä†‰∏äÂÅèÁßªb·µÄÔºåÂæóÂà∞Á∫øÊÄßÂè†Âä†ÂêéÁöÑ zÀ°Ôºö zÀ°= WÀ°·µÄ‚ãÖa‚ÅΩÀ°‚Åª¬π‚Åæ+b·µÄ„ÄÇ\nÂÜçÂÅöÔºàÊøÄÊ¥ªÂºïÂÖ•ÈùûÁ∫øÊÄß/ÈôêÂà∂ÂèñÂÄº0-1ÔºåÁÑ∂ÂêéÔºâsoftmax ÊääÁ∫øÊÄßÁªìÊûú zÀ° ÂèòÊàêÊ¶ÇÁéáÂàÜÂ∏É: aÀ° = softmax (zÀ°)„ÄÇ\nËæìÂá∫Â±ÇÁöÑÊçüÂ§±ÂáΩÊï∞ÔºöJ(aÀ°) = MLE (aÀ°)„ÄÇ\nÊûÅÂ§ß‰ººÁÑ∂ MLE ‰∏é CrossEntropy Á≠â‰ª∑ÔºåÂâçËÄÖÊúÄÂ§ßÂåñÈáçÂ§çÈááÊ†∑ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊ¶ÇÁéáÔºåÂêéËÄÖÊúÄÂ∞èÂåñÁΩëÁªúÁªôÂá∫ÁöÑÁ±ªÂà´Ê¶ÇÁéá‰πò‰ª•ÁúüÂÆûÊ¶ÇÁéáÂØπÂ∫îÁöÑ‰ø°ÊÅØÈáè„ÄÇ Ôºàsoftmax Êê≠ÈÖçMLEÔºåÁ≠â‰ª∑‰∫éÊúÄÂ§ßÁÜµ„ÄÇÔºâ\nË∞ÉÊï¥ ùêñ,ùêõ ‰ΩøÊçüÂ§±ÂáΩÊï∞J(ùêñ,ùêõ)ÊúÄÂ∞èÔºå‰ΩÜÂΩìÊçüÂ§±ÂÄºÊúÄÂ∞èÊó∂ÔºåÂØπÂ∫îÁöÑ ùêñ,ùêõ ‰∏çÊòØÂîØ‰∏ÄÁöÑ„ÄÇÊØîÂ¶Ç:\nÊääÂâçÈù¢ÊâÄÊúâÈöêËóèÂ±ÇÁöÑÊùÉÈáçÂíåÂÅèÁΩÆÈÉΩÊîæÂ§ß2ÂÄçÔºàÁî®ReLuÊøÄÊ¥ªÔºâÔºå‰ΩÜÂêåÊó∂ËæìÂá∫Â±ÇÁöÑÊùÉÈáçÂíåÂÅèÁΩÆÁº©Â∞è 1/2‚ÅΩÀ°‚Åª¬π‚ÅæÂÄçÔºåÂæóÂà∞ÁöÑÁ∫øÊÄßÁªìÊûú‰∏çÂèòÔºåÂàôÊçüÂ§±ÂÄº‰∏çÂèòÔºåÊâÄ‰ª•ÊúÄÁªà‰ºòÂåñÂæóÂà∞ÁöÑÂèÇÊï∞‰∏éÂàùÂßãÂÄºÁõ∏ÂÖ≥„ÄÇ\nÂ¶ÇÊûúËæìÂÖ•ÊîæÂ§ß 2 ÂÄçÁöÑÂêåÊó∂ËæìÂá∫Â±ÇÊùÉÈáçÁº©Â∞è 2 ÂÄçÔºåÁ∫øÊÄßÁªìÊûú‰∏çÂèòÔºåÊçüÂ§±ÂÄº‰πü‰∏çÂèò„ÄÇÊâÄ‰ª•ÊúÄÁªà‰ºòÂåñÂæóÂà∞ÁöÑ ùêñ,ùêõ ‰∏éËæìÂÖ•Êï∞ÊçÆÁõ∏ÂÖ≥„ÄÇ\nËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÊó∂ÁöÑÁõÆÁöÑÊòØÊâæÂà∞ÊçüÂ§±ÂáΩÊï∞ J(W,b) ÁöÑÊúÄÂ∞èÂÄºÔºå‰ΩÜÁõ∏ÂêåÁöÑÊçüÂ§±ÂÄºÂèØËÉΩÂØπÂ∫îÂ§öÁªÑÁªùÂØπÂÄºÔºàÊ®°ÈïøÔºâ‰∏çÂêåÁöÑ(W,b)„ÄÇ\nÁªùÂØπÂÄº‰∏çÂêåÁöÑ ùêñ,ùêõ ÂèØËÉΩÂú®ËÆ≠ÁªÉÈõÜ‰∏äÁöÑÊçüÂ§±ÂÄºÁõ∏ÂêåÔºå‰ΩÜÂú®ÊµãËØïÈõÜ„ÄÅunseenËæìÂÖ•„ÄÅ\u0026ldquo;Â∏¶Âô™Â£∞ÁöÑËæìÂÖ•\u0026quot;‰∏äÁöÑË°®Áé∞ÊúâÂ∑ÆÂà´Ôºå‰∏çÂêåÂèÇÊï∞ÁöÑÊ≥õÂåñËÉΩÂäõ‰∏çÂêå„ÄÇ ÁªùÂØπÂÄºÂ§ßÁöÑÂèÇÊï∞‰ºöÊääÂô™Â£∞ÊîæÂ§ßÔºå‰ªéËÄåÂΩ±ÂìçÈ¢ÑÊµãÁªìÊûú„ÄÇ\nÊ≠£ÂàôÂåñÂ∞±ÊòØ‰∫∫‰∏∫ÁöÑËÆæÂÆöÂèÇÊï∞ùêñ ÁöÑÂèñÂÄºËåÉÂõ¥ÔºàÂèØË°åÂüüÔºâÔºåËÆ©W‰∏çËÉΩË∂ÖÂá∫ËØ•ËåÉÂõ¥Ôºå‰ªéËÄåÂú®ÂèØË°åÂüüËåÉÂõ¥ÂÜÖÔºåÊ±ÇÊçüÂ§±ÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄº„ÄÇÂ∏¶Êù°‰ª∂ÁöÑ‰ºòÂåñÈóÆÈ¢òÂèØ‰ª•Áî®ÊãâÊ†ºÊúóÊó•‰πòÊï∞Ê≥ïÊ±ÇËß£„ÄÇ Êàë‰ª¨Âè™ÈúÄË¶ÅËßÑÂÆö ùêñ ÁöÑËåÉÂõ¥ÔºàÂ§öÈ°πÂºèÂêÑÊú™Áü•Êï∞ÁöÑÁ≥ªÊï∞ÔºâÔºåÂõ†‰∏∫ W Áõ¥Êé•ÂÜ≥ÂÆö‰∫ÜÊ®°ÂûãÊõ≤Á∫øÊòØ‰ªÄ‰πàÊ†∑Â≠êÔºåËÄå b ‰∏éÊúÄÁªàÊãüÂêàÁöÑÊõ≤Á∫øÁöÑÂΩ¢Áä∂ÔºàËøáÊãüÂêà‰∏éÂê¶ÔºâÊó†ÂÖ≥ÔºåbÂè™ÂΩ±ÂìçÂπ≥ÁßªÔºåÂõ†Ê≠§Âè™Ë¶Åùêñ Ë¢´Á∫¶ÊùüÂ•Ω‰∫ÜÔºåbÂú®ËÆ≠ÁªÉÊó∂‰ºöËá™Âä®Ë∞ÉÊï¥Â•ΩÔºåÊâÄ‰ª•‰∏çÈúÄÈ¢ùÂ§ñÂÅöÁ∫¶Êùü„ÄÇ\nw ÁöÑÂèØË°åÂüüËåÉÂõ¥ Âú®Ê±ÇÊçüÂ§±ÂáΩÊï∞ÊúÄÂ∞èÂÄºÊó∂ÔºåÊùÉÈáç ùêñ ÁöÑÊ®°Èïø‰∏çË¶ÅË∂ÖËøáCÔºö\nËã•ÈááÁî®ÊõºÂìàÈ°øË∑ùÁ¶ªÂ∫¶ÈáèÊ®°ÈïøÔºåÂç≥ËÆ°ÁÆóÂêëÈáèÁöÑL1ËåÉÊï∞ÔºåÂàô‰ºòÂåñÈóÆÈ¢ò‰∏∫Ôºö\nmin J(ùêñ , ùêõ, ùêó), s.t. ‚Äñùêñ‚Äñ‚ÇÅ-C‚â§0\nËã•ÈááÁî®Ê¨ßÂá†ÈáåÂæóË∑ùÁ¶ªËÆ°ÁÆóÊ®°ÈïøÔºàÂêëÈáèÁöÑL2ËåÉÊï∞ÔºâÔºåÂàô‰ºòÂåñÈóÆÈ¢ò‰∏∫Ôºö\nmin J(ùêñ , ùêõ, ùêó), s.t. ‚Äñùêñ‚Äñ‚ÇÇ-C‚â§0\nÊää‰ºòÂåñÈóÆÈ¢òÂÜôÊàêÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ = ÁõÆÊ†áÂáΩÊï∞ + Á∫¶ÊùüÊù°‰ª∂‰πò‰ª•ÊãâÊ†ºÊúóÊó•‰πòÂ≠ê ŒªÔºö L(ùêñ, Œª) = J(ùêñ) + Œª(‚Äñùêñ‚Äñ‚ÇÅ - C), ÁÑ∂ÂêéÊ±ÇËß£Ôºö$\\rm min_ùêñ \\ max_Œª L(ùêñ,Œª), s.t. Œª‚â•0$ Ôºà‰∏çËÄÉËôëb,XÔºâ\n‰∏äÂõæÂùêÊ†áËΩ¥ÊòØ ùêñ ÁöÑÂêÑ‰∏™Áª¥Â∫¶ÔºåÊ§≠ÂúÜÊòØÊçüÂ§±ÂáΩÊï∞J(ùêñ)ÁöÑÁ≠âÈ´òÁ∫øÔºå‰∏≠ÂøÉÊòØÊçüÂ§±ÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄºÂØπÂ∫îÁöÑ ùêñÔºåÁªøËâ≤Ê°Ü‰ª£Ë°®ÂèØË°åÂüüËåÉÂõ¥Ôºàùêñ ÁöÑÊ®°ÈïøÈôêÂà∂ÔºâÔºåËìùËâ≤ÁÇπÊòØÂú®Êª°Ë∂≥Á∫¶ÊùüÊù°‰ª∂Êó∂ÔºåÊçüÂ§±ÂáΩÊï∞ËÉΩÂèñÂà∞ÁöÑÊúÄÂ∞èÂÄºÔºåÂØπÂ∫îÁöÑùêñ ÔºõÊàñËÄÖËØ¥Âú®ÂèØË°åÂüüËåÉÂõ¥ÂÜÖÊâæÊçüÂ§±ÂáΩÊï∞Ê¢ØÂ∫¶+ùêñÂèØË°åÂüüÊ¢ØÂ∫¶=0ÁöÑÁÇπ„ÄÇ\n‚ÄúÂΩìÁõÆÊ†áÂáΩÊï∞ÊòØ‰∏Ä‰∏™Âá∏ÂáΩÊï∞ÊàñÊòØ‰∏Ä‰∏™ÂáπÂáΩÊï∞ÔºåÂπ∂‰∏îÂØπÂ∫îÁöÑÁ∫¶ÊùüÊù°‰ª∂ÊòØ‰∏Ä‰∏™Âá∏ÈõÜÔºåÈÇ£‰πàÊï¥‰∏™ÈóÆÈ¢òÔºàÁõÆÊ†áÂáΩÊï∞+Œª‚ãÖÁ∫¶ÊùüÊù°‰ª∂ÔºâÂ∞±ÊòØ‰∏Ä‰∏™Âá∏‰ºòÂåñÈóÆÈ¢ò‚Äù„ÄÇ Á∫¶ÊùüÊù°‰ª∂ÈááÁî®L1ÊàñL2ËåÉÊï∞Êó∂ÔºåÂèØË°åÂüüÊòØ‰∏Ä‰∏™Âá∏ÈõÜ„ÄÇÂØπÂ∫î‰∫éÂá∏ÈõÜÁöÑÁ∫¶ÊùüÊù°‰ª∂Âπ∂‰∏ç‰ºöÊîπÂèòÂéüÊù•ÈóÆÈ¢òÁöÑÊÄßË¥®Ôºå Âç≥Â¶ÇÊûúÂéüÊù•ÁöÑÈóÆÈ¢òÊú¨Ë∫´ÊòØ‰∏Ä‰∏™Âá∏ÈóÆÈ¢òÔºåÂä†‰∏äËøô‰∏™Á∫¶ÊùüÊù°‰ª∂ÂêéÔºå‰ªçÁÑ∂ÊòØÂá∏ÈóÆÈ¢òÔºåÂ¶ÇÊûúÂéüÊù•ÁöÑÈóÆÈ¢òÊòØÈùûÂá∏ÈóÆÈ¢òÔºåÂä†‰∏äËøô‰∏™Á∫¶ÊùüÊù°‰ª∂‰πü‰∏ç‰ºöËÆ©Ëøô‰∏™ÈóÆÈ¢òÂèòÂæóÊõ¥Á≥üÁ≥ï„ÄÇ\n(2022-11-07) ÊçüÂ§±ÂáΩÊï∞‰∏éÊ≠£ÂàôÂåñÈ°πÈÉΩÁîªÂú®‰∫åÁª¥Âπ≥Èù¢‰∏äÔºåÊòØÂõ†‰∏∫Á†îÁ©∂ÁöÑÂè™ÊòØ‚Äú‰∏§‰∏™ÂØπË±°‚ÄùÔºöËæìÂÖ•XÂíåËæìÂá∫Y‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºü\n‰∏çÔºåÂõ†‰∏∫ÊòØË¶ÅÈôêÂà∂wÁöÑÊ®°ÈïøÔºå‰∫åÁª¥Âπ≥Èù¢Âè™Ë°®Á§∫‰∫Ü‰∫åÁª¥w„ÄÇÂä†‰∫ÜÊ≠£ÂàôÂåñÈ°πÔºåÁõ∏ÂΩì‰∫é‰øÆÊîπ‰∫ÜÂàùÂßãÊó∂ÁöÑwÔºåÁº©Áü≠ÂÆÉÂà∞ÈÇ£Êù°ËôöÁ∫øÁöÑË∑ùÁ¶ªÔºå‰ΩÜ‰∏çËÄÉËôëÂÆÉÂØπÂ∫îÁöÑÊçüÂ§±ÂÄºÂèòÂ§ßËøòÊòØÂèòÂ∞è‰∫ÜÔºåËôöÁ∫øÊÄªÊòØÊ≤øÁùÄÊçüÂ§±ÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶ÊñπÂêëÔºåËÄå‰∏îËøûÊé•ÁùÄÊçüÂ§±ÂÄºÊúÄÂ∞èÁöÑÈÇ£‰∏™wÔºå‰∏çËøáÈÇ£Êù°ËôöÁ∫ø‰∏äÁöÑwÊòØÁ≠âÊØîÁöÑÔºåÁ≠âÊØî‰æãÁº©ÊîæÁöÑ w ËÉΩÊî∂ÊïõÂà∞ÁöÑÊúÄÂÄºÊòØÁõ∏ÂêåÁöÑ\n(2023-01-30) Ê≠£ÂàôÂåñÈ°π Œª‚Äñùêñ‚Äñ ÁöÑ‰ΩúÁî®ÊòØÊåÅ‰πÖÁöÑ„ÄÇÂú® Œª‚Äñùêñ‚Äñ ÁöÑÂü∫Á°Ä‰∏äÔºåùêñ ÂÅöË∞ÉÊï¥‰ª•‰ΩøÊçüÂ§±ÂÄºÊúÄÂ∞èÔºå‰πüÂ∞±ÊòØ‰Ωø J(ùêñ) ‰∏é Œª(‚Äñùêñ‚Äñ‚ÇÅ - C) ‰∏§È°πÁöÑÊ¢ØÂ∫¶ÊäµÊ∂àÔºàÁ≠âÂ§ßÂèçÂêëÔºâ„ÄÇ\u0026ldquo;Âú®ÂéüÂßãÊúÄÂ∞è‰∫å‰πòÁöÑÁªìÊûú‰∏äÂÅö‰∫ÜÁº©Êîæ\u0026rdquo;4„ÄÇ\nL1, L2 Ê≠£ÂàôÂåñÈ°π ÊääÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Â±ïÂºÄÔºö\nL(ùêñ,Œª) = J(ùêñ) + Œª(‚Äñùêñ‚Äñ‚ÇÇ-C) = J(ùêñ) + Œª‚Äñùêñ‚Äñ‚ÇÇ - ŒªC\n‰ΩÜÊòØÂ∏∏ËßÅÁöÑ ‚ÄúÊçüÂ§±ÂáΩÊï∞+ L2Ê≠£ÂàôÂåñÈ°π‚Äù ÁöÑË°®ËææÂºèÊòØ ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞+ŒªCÔºö\nL\u0026rsquo;(ùêñ,Œª) = L(ùêñ,Œª) + ŒªC = J(ùêñ) + Œª‚Äñùêñ‚Äñ‚ÇÇ\nÂú®Ê±ÇL\u0026rsquo; Âíå L Ëøô‰∏§‰∏™ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÊúÄÂ∞èÂÄºÊó∂ÔºåËôΩÁÑ∂ÊúÄÂ∞èÂÄºÂèØËÉΩ‰∏ç‰∏ÄÊ†∑Ôºå‰ΩÜÂØπÂ∫îÁöÑ ùêñ ÊòØ‰∏ÄÊ†∑ÁöÑ ÔºàÂõ†‰∏∫Âè™Ë¶ÅÊ±ÇËß£ $‚àá_ùêñ L\u0026rsquo;(ùêñ,Œª) =‚àá_ùêñ L(ùêñ,Œª)=0$ÔºâÔºö\n$arg_ùêñ (min_ùêñ\\ max_Œª L\u0026rsquo;(ùêñ,Œª), s.t.\\ Œª‚â•0) = arg_ùêñ (min_ùêñ\\ max_Œª L(ùêñ,Œª), s.t.\\ Œª‚â•0)$\nÂ¶Ç‰ΩïÁõ¥ËßÇÁêÜËß£ L\u0026rsquo; Âë¢Ôºü ÊàñËÄÖËØ¥Ôºö‰∏∫‰ªÄ‰πà\u0026quot;Â∏∏Áî®ÁöÑL2Ê≠£ÂàôÂåñË°®ËææÂºè\u0026quot;ÊØî\u0026quot;ÊçüÂ§±ÂáΩÊï∞Âä†‰∏ä ùêñ Á∫¶ÊùüÊù°‰ª∂ÂÜôÊàêÁöÑÊãâÊ†ºÊúóÊó•ÂáΩÊï∞\u0026rdquo;ÔºåÂ∞ë‰∏Ä‰∏™ ŒªC Âë¢Ôºü\nC ÂÜ≥ÂÆö‰∫Ü ùêñ ÁöÑÊ®°ÈïøÔºå‰∏çÊåáÂÆö C ÊòØÂõ†‰∏∫ Œª ÂèØ‰ª•ÊéßÂà∂Ê®°ÈïøÁöÑËåÉÂõ¥„ÄÇÊãâÊ†ºÊúóÊó•‰πòÂ≠êŒªÁöÑ‰ΩúÁî®ÊòØË∞ÉËäÇ\u0026quot;Á∫¶ÊùüÊù°‰ª∂Œª‚Äñùêñ‚ÄñÁöÑÊ¢ØÂ∫¶\u0026quot;ÁöÑÂ§ßÂ∞èÔºå‰Ωø‰πã‰∏é\u0026quot;ÁõÆÊ†áÂáΩÊï∞J(ùêñ)ÁöÑÊ¢ØÂ∫¶\u0026quot;Á≠âÂ§ßÂèçÂêëÊäµÊ∂àÔºàÊ±ÇÂØº=0Â∞±ÊòØÊ±ÇÊ¢ØÂ∫¶=0ÁöÑÁÇπÔºâ„ÄÇ ÂΩìŒª=0Êó∂ÔºåÁõ∏ÂΩì‰∫éÊ≤°ÊúâÁ∫¶ÊùüÔºõËÄåÂΩìŒª=infÊó∂ÔºåW‰ºöË∂ã‰∫é0ÔºåÂ§±ÂéªÊãüÂêàËÉΩÂäõ‰∫Ü„ÄÇŒª ÂèØ‰ª•Áî®cross validation Êù•ÈÄâÊã©\nÁ∫¢Ëâ≤ÊòØÊçüÂ§±ÂáΩÊï∞ÁöÑÁ≠âÈ´òÁ∫øÔºåÁªøËâ≤ÊòØÁ∫¶ÊùüÊù°‰ª∂ÁöÑÁ≠âÈ´òÁ∫øÔºåÁÆ≠Â§¥Ë°®Á§∫Ê¢ØÂ∫¶\nŒª ÁöÑÁªùÂØπÂÄºÁ≠â‰∫é\u0026quot;ÊçüÂ§±ÂáΩÊï∞J(ùêñ)ÁöÑÊ¢ØÂ∫¶Â§ßÂ∞è\u0026quot;Èô§‰ª•\u0026quot;Á∫¶ÊùüÊù°‰ª∂ÂØπÂ∫îÂáΩÊï∞Œª‚Äñùêñ‚ÄñÁöÑÊ¢ØÂ∫¶Â§ßÂ∞è\u0026quot;ÔºåÊâÄ‰ª•Á©∫Èó¥‰∏≠ÂêÑÁÇπÂ§ÑÁöÑŒªÂ∞±ËÉΩÁÆóÂá∫Êù•ÔºåÂèçËøáÊù•‰∏çÂêåÁöÑ Œª ÂØπÂ∫îÁöÑÁÇπÁöÑ‰ΩçÁΩÆ‰∏çÂêå„ÄÇÂè™ÊúâÂú®ÊÅ∞ÂΩìÁöÑ‰ΩçÁΩÆJ(W)‰∏éŒª‚Äñùêñ‚ÄñÁöÑÊ¢ØÂ∫¶‰πãÂíåÊâçÁ≠â‰∫é0„ÄÇ ~~‰∏çÂêåÁöÑ Œª ÂØπÂ∫îÁöÑÊ¢ØÂ∫¶=0ÁöÑÁÇπÁöÑ‰ΩçÁΩÆ‰∏çÂêåÔºåÊçüÂ§±ÂáΩÊï∞J(ùêñ)ÊúÄÂ∞èÂÄº‰∏çÂêåÔºå‰ΩÜÂØπÂ∫îÁöÑÔºàÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÁöÑÊúÄ‰ºòÔºâWÊòØÁõ∏ÂêåÁöÑ??? ~~\nŒªÂíåC‰πã‰∏≠Âè™Êúâ‰∏Ä‰∏™Ë∂ÖÂèÇÊï∞ÔºöÂú®ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞‰∏≠ÔºåCÊòØË∂ÖÂèÇÊï∞Ôºå‰∫∫‰∏∫ÊåáÂÆöÊ®°ÈïøËåÉÂõ¥CÂêéÔºåŒª ‰πüÂ∞±Ë∑üÁùÄÁ°ÆÂÆö‰∫ÜÔºõËÄåÂú® L\u0026rsquo; ‰∏≠ÔºåŒª ÊòØË∂ÖÂèÇÊï∞Ôºå‰∫∫‰∏∫ÊåáÂÆöÂêéÔºåÂ∞±ËÉΩÂîØ‰∏ÄÁ°ÆÂÆöÊ¢ØÂ∫¶=0ÁöÑÁÇπÁöÑ‰ΩçÁΩÆ„ÄÇ\nL2Ê≠£ÂàôÂåñÔºàÁªôÂÆöŒªÂêéÔºâÁ°ÆÂÆöÁöÑÊúÄÂÄºÁÇπÔºàÊúÄ‰ºòùêñÔºâÂü∫Êú¨‰∏ç‰ºöËêΩÂú®ÂùêÊ†áËΩ¥‰∏äÔºàÂèØË°åÂüüÊòØÂúÜÂΩ¢ÔºâÔºå ËÄåL1Ê≠£ÂàôÂåñÊâæÂà∞ÁöÑÊª°Ë∂≥Á∫¶ÊùüÊù°‰ª∂‰∏éÁõÆÊ†áÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶‰πãÂíå=0ÁöÑÁÇπÔºàÂèØË°åÂüüËåÉÂõ¥‰∏éÊçüÂ§±ÂáΩÊï∞Á≠âÈ´òÁ∫øÁõ∏ÂàáÁöÑÁÇπÔºâÂÆπÊòìÂèñÂú®ÂùêÊ†áËΩ¥‰∏äÔºàÂèØË°åÂüüÊúâÂ∞ñËßí‰∏îËêΩÂú®ÂùêÊ†áËΩ¥‰∏äÔºåÂÆπÊòì‰∏éÊçüÂ§±ÂáΩÊï∞Áõ∏ÂàáÔºâÔºå ÂΩìÊúÄ‰ºò ùêñ ËêΩÂú®Êüê‰∏ÄÂùêÊ†áËΩ¥‰∏äÊó∂ÔºåÂè™ÊúâÈÇ£‰∏ÄÁª¥‰∏çÊòØ0ÔºåÂÖ∂‰ªñÁª¥Â∫¶ÁöÑÂùêÊ†áÈÉΩÊòØ0„ÄÇ\nÊØîÂ¶Ç‰æùÊçÆ‰∏§‰∏™ÁâπÂæÅÂÅöÂà§Êñ≠Êó∂ÔºåÊúÄ‰ºò ùêñ ‰∏çËêΩÂú®ÂùêÊ†áËΩ¥‰∏äÔºåÂàô‰∏§‰∏™ÁâπÂæÅÈÉΩÊúâ‰∏ÄÂÆöÁöÑÂÜ≥ÂÆö‰ΩúÁî®Ôºå ËÄåÈÄöËøáË∞ÉÊï¥ Œª ‰ΩøÊúÄ‰ºòWËêΩÂú®ÂùêÊ†áËΩ¥‰∏äÊó∂ÔºåÂàôÂè™‰ºöÂÖ≥Ê≥®ËØ•ËΩ¥ÁâπÂæÅÁöÑÊúâÊó†ÔºåÊâÄ‰ª•‰ΩøÁî® L1 ÂèØ‰ª•‰ΩøÂÜ≥Á≠ñÂèòÂæóÁÆÄÂçïÔºå‰Ωø W ÂêëÈáèÂèòÂæóÁ®ÄÁñèÔºàÂè™ÊúâÊüê‰∏™‚ÄúÈáçË¶Å‚ÄùÁª¥Â∫¶(ÂØπÁªìÊûúË¥°ÁåÆÂ§ß)ÊúâÂÄºÔºåÂÖ∂‰ªñÁª¥ÈÉΩÊòØ0ÔºâÔºåÊääÁâπÂæÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂéªËÄ¶Âêà‰∫ÜÔºåÊääÊ®°ÂûãÂ§çÊùÇÂ∫¶Èôç‰Ωé‰∫ÜÔºå‰ªéËÄåÂáèÂ∞ëËøáÊãüÂêà„ÄÇ ‰ΩÜÊòØL1 ÁöÑËß£‰∏çÂ§™Á®≥ÂÆöÔºåËÆ≠ÁªÉÊó∂ÂêÑÊâπÊ¨°Êï∞ÊçÆÁöÑÊçüÂ§±ÂáΩÊï∞‰∏çÂêåÔºåÊ§≠ÂúÜ‰ºöÂèòÂåñÔºåÂàáÁÇπÂèØËÉΩ‰ªé‰∏Ä‰∏™ËΩ¥Êç¢Âà∞Âè¶‰∏Ä‰∏™ËΩ¥‰∏äÔºåÂØπÂ∫îÁöÑWÂèòÂåñÂ§ß„ÄÇ3\nL1 Âíå L2 ÈÉΩÊòØÂØπ W ËøõË°åÁ∫¶ÊùüÔºå‰ΩÜÊïàÊûú‰∏çÂêåÔºåÂèØ‰ª•Êää‰∫åËÄÖÁªìÂêàËµ∑Êù•‰∏ÄËµ∑Áî®„ÄÇL2Ê≠£ÂàôÂåñÂè™ÊòØÈôêÂà∂Ê®°ÈïøÔºåL1 Ê≠£ÂàôÂåñËøòÂ∏¶Êù•‰∫ÜÁ®ÄÁñèÊÄß\nÊ≠£ÂàôÂåñÂ∏¶Êù•ÁöÑÊçüÂ§±ÂÄºËØØÂ∑ÆÔºü‰∏çÈáçË¶ÅÂïä ËØÑËÆ∫Ôºö‰∏äÂõæÁªòÂà∂ÁöÑÊ§≠ÂúÜÂØπÂ∫îÁöÑÊçüÂ§±ÂáΩÊï∞J(ùêñ)ÁöÑÊé•Êî∂ÁöÑËæìÂÖ•ÊòØÊï¥‰∏™ÂÆûÊï∞ÂüüÔºåËÄåÂú®Á•ûÁªèÁΩëÁªú‰∏≠ÔºåÊçüÂ§±ÂáΩÊï∞Êé•Êî∂ÁöÑÊòØÁΩëÁªúÁöÑËæìÂá∫ÂÄºÔºåÁΩëÁªú‰∏çÂêåÁöÑÊùÉÈáçÂèØËÉΩÂØπÂ∫îÁõ∏ÂêåÁöÑËæìÂá∫ÂÄºÔºåÊçüÂ§±ÂÄº‰πüÂ∞±Áõ∏Âêå\nÂõ†‰∏∫‰∏çÂêåÁöÑÂàùÂßãÂèÇÊï∞ ùêñ ÊúÄÁªàÊî∂ÊïõÂà∞ÁöÑÊçüÂ§±ÂáΩÊï∞ÊúÄÂ∞èÂÄºÂèØËÉΩÊòØÁõ∏Á≠âÁöÑÔºåÊØîÂ¶ÇÁ≠âÊØîÁº©ÊîæÂèÇÊï∞Êó∂Ôºåmin J(ùêñ ,ùêõ) = min J(aùêñ ,aùêõ)Ôºå (ùêñ ,ùêõ) ‰∏é (aùêñ ,aùêõ) ÂÖ±Á∫øÔºàËôöÁ∫øÔºâÔºå\nÂä†Á∫¶ÊùüÊù°‰ª∂Œª‚Äñùêñ‚Äñ‰∏é‰∏çÂä†Á∫¶ÊùüÊù°‰ª∂(Œª=0)ÔºåÈÄöËøáÊúÄÂ∞èÂåñÊãâÊ†ºÊúóÊó•ÂáΩÊï∞L(ùêñ,Œª)Êî∂ÊïõÂà∞ÁöÑ W ‰∏çÂêåÔºàWÁî±ŒªÂÜ≥ÂÆöÔºâÔºå‰ΩÜÊòØÂ¶ÇÊûú‰∏§‰∏™ÂêëÈáèWÊòØÂÖ±Á∫øÁöÑÔºåÂÆÉ‰ª¨ÂØπÂ∫îÁöÑ L(W,Œª) ËÉΩÂ§üÊî∂ÊïõÂà∞ÁöÑÊúÄÂ∞èÂÄºÔºàwhere L(W,Œª)ÁöÑÊ¢ØÂ∫¶=0ÔºâÊòØÁõ∏ÂêåÁöÑÔºåÂè™‰∏çËøáŒªÂèñÁöÑ‰∏çÂ•ΩÔºåËøòÊ≤°Ëµ∞Âà∞ÊúÄÂ∞èÂÄºÔºåËøòÊ≤°Âπ≥Ë°°Â•Ω J(W) Âíå ||W||‰πãÈó¥ÁöÑÊùÉÈáçÊØî‰æã„ÄÇ\nÂêå‰∏ÄËôöÁ∫ø‰∏äÁöÑ W ÊúÄÁªàËÉΩÂ§üÊî∂ÊïõÂà∞ÁöÑÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÂÄºÊòØÁõ∏ÂêåÁöÑ„ÄÇËôöÁ∫ø‰∏äÁöÑÁÇπÊòØÊú¨ÂèØËÉΩÊî∂ÊïõÂà∞ÁöÑÊúÄÂÄº\nÂ¶ÇÊûú‰∏çÂä†Á∫¶ÊùüÊù°‰ª∂ÔºåÂàùÂßãÁöÑËæÉÂ§ßÁöÑ W Êú¨ÂèØ‰ª•Êî∂ÊïõÂà∞J(W)Ê§≠ÂúÜ‰∏≠ÂøÉÔºàÊçüÂ§±ÊúÄÂ∞èÂÄºÔºâÔºå‰ΩÜÊòØÂõ†‰∏∫ÊåáÂÆö‰∫ÜÁ∫¶ÊùüÊù°‰ª∂ŒªÔºåÊ®°ÈïøË¢´ÈôêÂà∂‰∫ÜÔºåË¢´ÊãâÂêëÂéüÁÇπ‰∫ÜÔºåÊâæÂà∞ÁöÑÊúÄ‰ºòW‰∏éÊ§≠ÂúÜ‰∏≠ÂøÉÁ¶ªÂæóÂæàËøúÔºåÊçüÂ§±ÂÄºJ(W)ÂèòÂ§ß‰∫ÜÔºåÁúãËµ∑Êù•Â∞±Â∏¶Êù•‰∫ÜËØØÂ∑Æ„ÄÇ ‰ΩÜÊòØÔºàÊúÄÂ∞èÂåñÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ËøáÁ®ã‰∏≠ÁöÑÔºâËØØÂ∑ÆÂπ∂‰∏çÊòØÂà∞Ê§≠ÂúÜ‰∏≠ÂøÉÁöÑË∑ùÁ¶ªÔºåËÄåÊòØÂà∞‚ÄúËôöÁ∫ø‚ÄùÁöÑË∑ùÁ¶ªÔºåËôöÁ∫ø‰∏äÁöÑ W Â∑≤ÁªèË∞ÉÊï¥Âà∞ËÉΩ‰Ωø J(W) ‰∏é ||W||ÁöÑÊ¢ØÂ∫¶ÂÖ±Á∫øÔºåÂõ†‰∏∫ Œª ÊòØ‰∫∫‰∏∫ËÆæÁΩÆÁöÑÔºåÊâÄ‰ª•WËøòË¶ÅÁªßÁª≠Ë∞ÉÊï¥ÔºåÊúÄÁªàËôöÁ∫ø‰∏äÁöÑW‰Ωø L(W,Œª)ÁöÑÊ¢ØÂ∫¶=0„ÄÇ\nÂú®ËôöÁ∫ø‰∏äÁöÑ‰∏çÂêå W ÂØπÂ∫îÁöÑÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÂÄºL(W,Œª)ÁöÑÊ¢ØÂ∫¶ÊòØÁõ∏ÂêåÁöÑÔºàÂú®ËôöÁ∫ø‰∏äÁöÑÁÇπÔºåÊñπÂêëÂÖ±Á∫øÂ∑≤Êª°Ë∂≥ÔºåÂè™ÊòØŒª‰∏çÂêåÔºâÔºå ËôΩÁÑ∂ÂèØË°åÂüüË∂äÂ∞èÔºåÂØπÊçüÂ§±ÂáΩÊï∞J(W)ÁöÑÊúÄÂ∞èÂÄºÂÅèÁ¶ªÁöÑËØØÂ∑ÆÂ∞±Ë∂äÂ§ßÔºåÂõ†‰∏∫WÂàùÂßãÂÄºÂÜ≥ÂÆö‰∫ÜÊúÄÁªàËÉΩÊî∂ÊïõÂà∞Âì™‰∏™J(W)Ôºå‰ΩÜÂÖ≥Ê≥®ÁöÑ‰∏çÊòØÂÆÉJ(W)„ÄÇ\nWË∞ÉÊï¥Âà∞ÊúÄÂêéÔºöJ(W)ÁöÑÊ¢ØÂ∫¶ÊñπÂêë‰∏é||W||Ê¢ØÂ∫¶ÊñπÂêëÂÖ±Á∫øÔºåÂπ∂‰∏îÂØπ‰∫éË∂ÖÂèÇÊï∞Áº©ÊîæÂõ†Â≠êŒªÔºåÊúâL(W,Œª)ÁöÑÊ¢ØÂ∫¶=0„ÄÇ\nÊâÄ‰ª•ÁúüÊ≠£ÂÖ≥Ê≥®ÁöÑÂà∞ÊúÄ‰ºòËß£ÁöÑÂÅèÂ∑ÆÔºåÊòØÂà∞ËôöÁ∫øÁöÑË∑ùÁ¶ªÔºåË∂ÖÂèÇÊï∞ŒªÈÄâÁöÑÂ•Ω‰∏éÂùèÔºåÂ∏¶Êù•ÁöÑÂÅèÂ∑Æ‰∏çÂ§ßÔºåÊâÄ‰ª•ËÉΩÁî®Ê≠£ÂàôÂåñÂ∞±Áî®„ÄÇ\n- ‰æãÂ≠êÔºöÂè™Êúâ‰∏§‰∏™ÂàÜÈáèÁöÑW ‰∏éÂØπÂ∫îÁöÑÊçüÂ§±ÂÄº ÁªòÂà∂ÁöÑÂõæÂÉè‰∏∫Ôºö\nÂêåÂøÉÊ§≠ÂúÜÊòØÊçüÂ§±ÂáΩÊï∞ J ÁöÑÁ≠âÈ´òÁ∫øÔºåÂç≥ÊØè‰∏ÄÂúàÊ§≠ÂúÜ‰∏äÔºåÊçüÂ§±ÂÄºÊòØÁõ∏ÂêåÁöÑÔºå‰∏≠Èó¥ÁöÑÁÅ∞Ëâ≤ÂõæÂΩ¢ÁöÑÊúÄÂ§ñÊ≤øÁöÑ‰∏ÄÂúà‰ª£Ë°® W ÁöÑËåÉÊï∞„ÄÇÊ≠£ÊñπÂΩ¢‰ª£Ë°® L1 ËåÉÊï∞ÁöÑÂõæÂÉèÔºåÂúÜÂΩ¢‰ª£Ë°® L2 ËåÉÊï∞ÁöÑÂõæÂÉè\nÁúã‰∏ãËøôÁØáÔºöÊú∫Âô®Â≠¶‰π†‰πãÊ≠£ÂàôÂåñÔºàRegularizationÔºâ- Acjx -ÂçöÂÆ¢Âõ≠\nDDGÊêúÁ¥¢Ôºöentropy regularization ÁÜµÊ≠£ÂàôÂåñ\n(2023-01-30) DDG search: \u0026ldquo;Ê≠£ÂàôÂåñÈ°π\u0026rdquo;\nÊ≠£ÂàôÂåñÁöÑ‰ΩúÁî® ÂéüÊñáÈìæÊé•ÔºöÊ≠£ÂàôÂåñÂèäÊ≠£ÂàôÂåñÈ°πÁöÑÁêÜËß£ - guuuuu - CSDN\nÈò≤Ê≠¢ËøáÊãüÂêàÔºöùêñ Ë∂äÂ∞èÔºåÊãüÂêàÁöÑÂáΩÊï∞Êõ≤Á∫øË∂äÁÆÄÂçïÂÖâÊªëÔºåË∂ä‰∏çÂÆπÊòìËøáÊãüÂêàÔºõ\nÊ≠£ÂàôÂåñÈ°π‰ª£Ë°®ÂÖàÈ™å‰ø°ÊÅØÔºöËØïÈ™å‰πãÂâçÁöÑÂØπ‚Äñùêñ‚ÄñÁöÑËÆ§Áü•ÔºåŒª ÊòØÂØπÂÖàÈ™å‰ø°ÊÅØÁöÑÁõ∏‰ø°Á®ãÂ∫¶„ÄÇ\nÈ¢ëÁéáÊ¥æÁõ¥Êé•ÂØπÂèÇÊï∞ùêñËøõË°åÂàÜÊûêÔºàËÄåË¥ùÂè∂ÊñØÊ¥æÊòØÂØπÂèÇÊï∞Âá∫Áé∞ÁöÑÊ¶ÇÁéáP(ùêñ)ËøõË°åÂàÜÊûêÔºâÔºõ ÂèÇÊï∞ ùêñ Áõ¥Êé•Âá∫Áé∞Âú®ÊçüÂ§±ÂáΩÊï∞‰∏≠ÔºåÊâÄ‰ª•È¢ëÁéáÊ¥æÂØπÊçüÂ§±ÂáΩÊï∞ÂÅö‰øÆÊ≠£ÔºöÂä†‰∏ä‰∫ÜÂÖàÈ™åÈÉ®ÂàÜÁü•ËØÜÔºåÂç≥Ê≠£ÂàôÂåñÈ°π„ÄÇ\nÂØπ‰∫éÊ®°ÂûãÔºöy = Œ∏‚ÇÄ + Œ∏‚ÇÅx‚ÇÅ + \u0026hellip; + Œ∏‚±ºx‚±º + \u0026hellip; + Œ∏‚Çôx‚ÇôÔºåËß£ÊúÄ‰ºòÂåñÈóÆÈ¢òÔºö\n$arg\\ min_{Œ∏‚ÇÄ,Œ∏‚ÇÅ,\u0026hellip;,Œ∏‚Çô}\\ J(Œ∏) = 1/2m ‚ãÖ(‚àë·µ¢‚Çå‚ÇÅ·µê(h_Œ∏(x‚Å±)-y‚Å±)¬≤ + Œª‚àë‚±º‚Çå‚ÇÅ‚Åø(Œ∏‚±º-\\^Œ∏‚±º)¬≤)$Ôºå ÂÖ∂‰∏≠ ^Œ∏‚±º ‰∏∫ÂÖàÈ™åËß£„ÄÇ Œª ‰∏çÂêåÂ§ßÂ∞èÁöÑÈÄâÊã©Ôºå‰ΩìÁé∞‰∫ÜËøô‰∏™ÂÖàÈ™åËß£ ^Œ∏‚±º ÁöÑÂèØ‰ø°Á®ãÂ∫¶„ÄÇÂ¶ÇÊûú Œª ÊòØ‰∏Ä‰∏™ÂæàÂ∞èÁöÑÊï¥Êï∞ÔºåÈÇ£Ê≠£ÂàôÂåñÈ°πÂ∞Ü‰∏çËµ∑‰ªÄ‰πà‰ΩúÁî®ÔºåËØ¥ÊòéÁªôÁöÑÂÖàÈ™åËß£ÊúâÂæàÂ§ßÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÊòØ‰∏çÂèØ‰ø°ÁöÑÔºõ\nÂ¶ÇÊûú Œª ÂæàÂ§ßÔºåÂàôÊ≠£ÂàôÂåñÈ°πÂç†ÊîØÈÖçÂú∞‰ΩçÔºåÊúÄÂêéÁöÑËß£Â∞ÜÈù†Ëøë‰∫é ^Œ∏‚±º„ÄÇ\nÈ¢ëÁéáÊ¥æ Ë¥ùÂè∂ÊñØÊ¥æ ÊâæÊúÄ‰ºòùêñ Áõ¥Êé•ÂØπÂèÇÊï∞ ùêñ Êú¨Ë∫´‰∏ãÊâã ÂØπÂèÇÊï∞Âá∫Áé∞ÁöÑÊ¶ÇÁéá P(ùêñ) ‰∏ãÊâã ÊÄùË∑Ø ÊúÄ‰ºò ùêñ ‰ΩøËÆ≠ÁªÉËØØÂ∑ÆÊúÄÂ∞è ÊúÄ‰ºò ùêñ Âá∫Áé∞ÁöÑÂêéÈ™åÊ¶ÇÁéá P(ùêñ ‰ºòÂåñÂÖ¨Âºè ÊçüÂ§±ÂáΩÊï∞ Ë¥ùÂè∂ÊñØÂÖ¨Âºè ÂÖàÈ™å Ê≠£ÂàôÂåñÈ°πÔºàÂØπùêñ ÁöÑÂÖàÈ™åËÆ§Áü•Ôºâ Ëá™Â∏¶ÂÖàÈ™åÊ¶ÇÁéáÔºàP(ùêñ)Ôºâ ‰øÆÊ≠£ Âä†‰∏ä‰∏ÄÈÉ®ÂàÜÂÖàÈ™å‰ø°ÊÅØ ÂØπ‰ººÁÑ∂ÂÄºÂÅö \u0026ldquo;P(ùêñ)/ÈÖçÂàÜÂáΩÊï∞\u0026rdquo; ÁöÑÁº©Êîæ Ë∂ÖÂèÇÊï∞ Œª; L1 or L2ËåÉÊï∞ ÂÖàÈ™åÊ¶ÇÁéáÁöÑÂàÜÂ∏É: Laplace, Gaussian ÊúâÂä©‰∫éÂ§ÑÁêÜÊù°‰ª∂Êï∞Ôºàcondition numberÔºâ‰∏çÂ•ΩÁöÑÊÉÖÂÜµ‰∏ãÔºåÁü©ÈòµÊ±ÇÈÄÜÂõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇ\nÊ¶ÇÂøµÔºöÂ¶ÇÊûúÊñπÈòµ A ÊòØÈùûÂ•áÂºÇÁöÑÔºàAÁöÑË°åÂàóÂºè‰∏çÁ≠â‰∫é0ÔºåÊ≠£ÂÆöÁü©Èòµ‰∏ÄÂÆöÊòØÈùûÂ•áÂºÇÁöÑÔºâÔºåÈÇ£‰πà A ÁöÑ condition number ÂÆö‰πâ‰∏∫ÔºöùúÖ(A) = ‚ÄñA‚Äñ ‚ÄñA‚Åª¬π‚Äñ ÂèØ‰ª•ÁúãÂá∫ÔºöÂ¶ÇÊûú A ÊòØÂ•áÂºÇÁöÑÔºåÈÇ£‰πà A ÁöÑÊù°‰ª∂Êï∞‰∏∫Êó†Á©∑Â§ß„ÄÇÊù°‰ª∂Êï∞Ë∂äÂ∞èÔºåÊâÄËé∑ÂæóÁöÑËß£Ë∂äÂèØÈù†ÔºåÊ®°ÂûãÈ≤ÅÊ£íÊÄßË∂äÂ•ΩÔºåÊäóÂπ≤Êâ∞ËÉΩÂäõË∂äÂº∫„ÄÇ ‰æãÂ¶ÇÂØπ‰∫éÊ®°Âûã AX=bÔºåA ÁöÑÊù°‰ª∂Êï∞Ë∂äÂ∞èÔºàAÁöÑË°åÂàóÂºèËøú‰∏çÊé•Ëøë‰∫é0ÔºâÔºåÈÇ£‰πà AÔºåb ÁöÑÁ®çÂæÆÁöÑÂèòÂåñÂØπËß£ X ÁöÑÂΩ±ÂìçË∂äÂ∞èÔºåÂØπ X ÁöÑÊ±ÇËß£ÂØπÊ†∑Êú¨ÈõÜÔºàAÔºåbÔºâ‰∏≠ÂºïÂÖ•ÁöÑÂπ≤Êâ∞ÁöÑÊäµÊäóËÉΩÂäõË∂äÂº∫ÔºåÂç≥ÊâÄÊ±ÇËß£ X Ë∂äÂèØÈù†„ÄÇ\nDDG search: \u0026ldquo;Ê≠£ÂàôÂåñÈ°π pytorch\u0026rdquo;\nPyTorch 12.Ê≠£ÂàôÂåñ-ÁßëÊäÄÁåõÂÖΩ-Áü•‰πé 1 2 3 # optimizer w and w/o regularization optim_normal = torch.optim.SGD(net_normal.parameters(), lr=lr_init, momentum=0.9) optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-2) DDG search: \u0026ldquo;pytorch optimizer weight decay\u0026rdquo; SGD-pytorch docs\n(2023-02-26) DDG serach: \u0026ldquo;Ê≠£ÂàôÂåñÈ°π ÂèØ‰ª•ÂÆåÂÖ®ÈÅøÂÖçËøáÊãüÂêàÂêó\u0026rdquo;\nÂú®Êú∫Âô®Â≠¶‰π†‰∏≠ÔºåL2Ê≠£ÂàôÂåñ‰∏∫‰ªÄ‰πàËÉΩÂ§üÁºìËøáÊãüÂêàÔºü - Áü•‰πé\nRef [Áü•ËØÜÊ¢≥ÁêÜ-03] Regularization Ê≠£ÂàôÂåñ-Âá©Â≠êÁôΩ-bilibili Linear regression (6): Regularization (UCI cs273a) - Alexander Ihler ‰ªÄ‰πàÊòØ L1 L2 Ê≠£ËßÑÂåñ Ê≠£ÂàôÂåñ Regularization (Ê∑±Â∫¶Â≠¶‰π† deep learning) ","date":"2022-11-07T11:52:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/11_l1l2%E6%AD%A3%E5%88%99%E5%8C%961-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/","title":"watch: DL - ÁéãÊú®Â§¥ 11 | L1L2 Reg (1), Larange Multiplier"},{"content":"(2022-5-25) ÊçüÂ§±ÂáΩÊï∞ÊòØËæìÂÖ•Ê†∑Êú¨batchÁöÑÂáΩÊï∞Ôºå‰∏çÂêåbatchÁöÑËØØÂ∑ÆÂáΩÊï∞‰∏çÂêåÔºåÂ¶ÇÊûúÂú®‰∏Ä‰∏™batch‰∏äÊüê w ÁöÑÂØºÊï∞‰∏∫Èõ∂ÔºåÂú®‰∏ã‰∏Ä‰∏™batch‰∏äËØ• w ÁöÑÂØºÊï∞‰∏ç‰∏∫Èõ∂ÔºåÂ∞±ÂèØ‰ª•ÁªßÁª≠‰øÆÊ≠£ÔºåËÄå‰∏ç‰ºöÂÅúÊªûÂú®ÈûçÁÇπ„ÄÇËØØÂ∑ÆÂáΩÊï∞Ê®™ËΩ¥ÊòØ wÔºåÁ∫µËΩ¥ÊòØ error„ÄÇ0\n3 loss functions ÊçüÂ§±ÂáΩÊï∞ÊòØ‰∏∫‰∫Ü: Ë°°Èáè‰∏§‰∏™Ê¶ÇÁéáÊ®°ÂûãÈó¥ÁöÑÂ∑ÆÂà´Ôºå ‰∏âÁßçÊÄùË∑ØÔºöÊúÄÂ∞è‰∫å‰πòÊ≥ï(MSE)ÔºåÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°(MLE)Ôºå‰∫§ÂèâÁÜµ(CE) ¬π\nMSE ÂØπ‰∫éÂçïÂàÜÁ±ªÈóÆÈ¢òÔºàÊòØor‰∏çÊòØÔºâÔºå‰πüÂ∞±ÊòØÊäõÁ°¨Â∏ÅÔºàÂèçÈù¢ÁöÑÊ¶ÇÁéáÂ∑≤ÁªèËï¥Âê´Âú®Ê≠£Èù¢ÁöÑÊ¶ÇÁéá‰πã‰∏≠‰∫ÜÔºâÔºåÈÇ£‰πàÔºö\nÊúÄÂ∞è‰∫å‰πòÂ∞±ÊòØÔºöÊ¶ÇÁéá(sigmoidËæìÂá∫0~1)ÂáèÊ†áÁ≠æ„ÄÇ‰∏∫‰∫Ü‰æùÊçÆËØØÂ∑Æ‰øÆÊ≠£wÔºåËØØÂ∑ÆÂèñÂπ≥Êñπ‰ΩøÂÖ∂ÂèØÂØº„ÄÇÂ¶ÇÊûúÊçüÂ§±ÂáΩÊï∞‰∏≠‰ª£ÂÖ•ÁöÑÊòØÈ¢ÑÊµãÁöÑÊ≠£(Âèç)Ê¶ÇÁéáÔºåÈÇ£ÊçüÂ§±ÂáΩÊï∞ÊòØ‰∏™‰∫åÊ¨°Êõ≤Á∫ø L = (prob - label)¬≤ÔºåÊ®™ÂùêÊ†áÊòØÊ¶ÇÁéáÔºåÁ∫µÂùêÊ†áÊòØlossÔºåÂΩìprob=target, È¢ÑÊµãÂá∫Êù•ÁöÑÂÄº‰∏éÁ¶ªÊï£ÁöÑËßÇÂØüÂÄºÊúÄÊé•Ëøë„ÄÇ\nÂØπ‰∫éËæìÂá∫ÊòØÂ§öÁª¥ÁöÑÔºåÂú®ÂêÑ‰∏™Áª¥Â∫¶‰∏äÈÉΩÊòØ‰∫åÊ¨°Êõ≤Á∫øÔºåÂ§öÂÖÉÊúÄÂ∞è‰∫å‰πòÔºàÂ§öÂÖÉÁ∫øÊÄßÂõûÂΩíÔºâ: J(w) = (Xw-Y)·µÄ(Xw-Y)„ÄÇËß£‚àáJ(w) = 0ÔºåÂ∞±ÊòØÊ±Ç pseudoinverse matrix ‚Å∏„ÄÇ\nÂ¶ÇÊûúËøôÊ†∑ËÉΩ‰∏ÄÊ¨°Ê±ÇÂá∫ wÔºå‰∏∫‰ªÄ‰πàËøòË¶ÅÊ¢ØÂ∫¶‰∏ãÈôçÂë¢Ôºü Âõ†‰∏∫ÊúâÊó∂ X ‰∏çÂèØÈÄÜÔºàÂèØ‰ª•Âä†Ê≠£ÂàôÂåñÈ°πËß£ÂÜ≥ noteÔºâÔºå ËÄå‰∏îÊúâÁöÑÊøÄÊ¥ªÂáΩÊï∞ g ‰πü‰∏çÂèØÈÄÜÔºàÈÄöËøáÊâãÂä®ÂÅöÂΩí‰∏ÄÂåñËß£ÂÜ≥Ôºå‰æãÂ¶Ç B-ELM ‰∏≠‰ΩøÁî®‰∫Ü minmax ÂáΩÊï∞Ôºâ„ÄÇ\nÂΩìMSE Áî®‰∫éÂõûÂΩíÈóÆÈ¢òÔºåloss=‚àë·µ¢(y·µ¢-wx·µ¢-b)¬≤ÊòØÂá∏ÂáΩÊï∞ÔºåÁõ¥Êé•Ê±ÇÂØºÁ≠â‰∫éÈõ∂ÔºåÂç≥ÂèØÊ±ÇÂá∫Ëß£ÊûêËß£Ôºõ‰ΩÜÊòØÁî®‰∫éÂàÜÁ±ªÈóÆÈ¢òÔºåËæìÂá∫ÈúÄË¶ÅÁªèËøásigmoid/softmaxÂèòÊàêÊ¶ÇÁéáÔºåloss=‚àë·µ¢(y·µ¢-1/(1+e‚Åª ∑À£‚Å±))¬≤ÊòØÈùûÂá∏ÁöÑÔºå‰∏çËÉΩÁõ¥Êé•Ê±ÇËß£ÊûêËß£ÔºåËÄå‰∏î‰∏çÂÆú‰ºòÂåñ ¬≥„ÄÇ\nMLE ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºöÂêÑ‰∏™ÂèØËÉΩÁöÑÂÅáËÆæÊ®°Âûã‰∫ßÁîüËÆ≠ÁªÉÊ†∑Êú¨Ê†áÁ≠æÁöÑÂàÜÂ∏ÉÁöÑÊ¶ÇÁéáÊòØÂ§öÂ∞ëÔºåÁõÆÊ†áÂ∞±ÊòØÊâæÂà∞Ê¶ÇÁéáÊúÄÂ§ßÊó∂ÂØπÂ∫îÁöÑÊ®°ÂûãÔºàÂä†‰∏™Ë¥üÂè∑ÂèñÊúÄÂ∞èÔºâÔºõ‚àè·µ¢ p·µ¢À£ (1-p·µ¢)¬π‚ÅªÀ£\nCE ‰∫§ÂèâÁÜµÔºöÁΩëÁªúÊ®°ÂûãË¶Å‰∏é‰∫∫ËÑë‰∏≠ÁöÑÊ®°ÂûãË∂≥Â§üÊé•ËøëÔºåÊüê‰∏Ä‰∫ã‰ª∂Âú®ÁΩëÁªúÊ®°Âûã‰∏≠ÂèëÁîüÂØπÂ∫îÁöÑ‰ø°ÊÅØÈáèË¶ÅÊé•ËøëÂú®‰∫∫ËÑë‰∏≠ÂèëÁîüÂØπÂ∫îÁöÑ‰ø°ÊÅØÈáèÔºåÂ§ö‰∏™‰∫ã‰ª∂Ë¶Å‰ª•‰ªñ‰ª¨Âú®‰∫∫ËÑë‰∏≠ÂèëÁîüÁöÑÊ¶ÇÁéáÂä†ÊùÉ„ÄÇ‚àë·µ¢ human·µ¢(-log‚ÇÇ net·µ¢)\nÊúÄÂ∞è‰∫å‰πòÂèØ‰ª•Áî®‰∫éÂõûÂΩíÔºåÂç≥ÁΩëÁªúËæìÂá∫ÂèØ‰ª•ÊòØ‰ªªÊÑèÁöÑÊï∞ÂÄºÔºõËÄåÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°Âíå‰∫§ÂèâÁÜµÈÉΩÊòØÂü∫‰∫éÊ¶ÇÁéáÁöÑÔºåÁΩëÁªúÁöÑËæìÂá∫ÊòØÊ¶ÇÁéáÔºå‰Ωç‰∫é0-1‰πãÈó¥ÔºåÊâÄ‰ª•ÈááÁî®MLEÊàñCEÊçüÂ§±ÂáΩÊï∞Êó∂ÔºåËæìÂá∫Â±ÇÁ•ûÁªèÂÖÉÁöÑÊøÄÊ¥ªÂáΩÊï∞ÈúÄË¶ÅÁî®sigmoidÔºåÊääËæìÂá∫ÂéãÁº©Âà∞0-1‰πãÈó¥; ËÄåÈöêËóèÂ±ÇÈÉΩÂèØ‰ª•Áî®ReLu„ÄÇ Â§öÁ±ªÂà´ÈóÆÈ¢òËæìÂá∫Áî® softmax ÊøÄÊ¥ªÔºåÂæóÂà∞ÂêÑÁ±ªÂà´ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ\n‰∫§ÂèâÁÜµËÆ§‰∏∫ÂêÑÁ±ªÂà´Áõ∏‰∫íÁã¨Á´ãÔºåÊØè‰∏ÄÁª¥ÊòØ‰∏Ä‰∏™‰∫åÂàÜÁ±ªÂô®ÔºåÂçï‰∏™Ê†∑Êú¨ÁöÑÊ¶ÇÁéáÔºà‰ººÁÑ∂ÔºâÊòØÔºöP‚ÇÅ ∏¬π ‚ãÖ P‚ÇÇ ∏¬≤ ‚ãÖ \u0026hellip; ‚ãÖ P‚Çñ ∏·µèÔºå ÊâÄ‰ª•ÈúÄË¶ÅÁî® softmax ÂÅö‰∏Ä‰∏ãÂΩí‰∏ÄÂåñ\nMSE ‰∏é CE Âå∫Âà´ (Google search: \u0026ldquo;‰∏∫‰ªÄ‰πà‰∏çÁî®mseÂÅöÊçüÂ§±ÂáΩÊï∞\u0026rdquo;)\nMSE ‰∏çÈÄÇÂêàÂàÜÁ±ªÈóÆÈ¢ò¬≤ Â∑•Á®ãËßíÂ∫¶ÔºöÂ¶ÇÊûúÁî®MSEÂÅöÂàÜÁ±ªÔºåÂØπ softmax ÁöÑËæìÂá∫‰ΩøÁî® MSEÔºåÂç≥Ê≠£Á°ÆÁ±ªÁöÑÊ¶ÇÁéáË∂äÊé•Ëøë 1 Ë∂äÂ•ΩÔºåÂÖ∂‰ªñÁ±ªÁöÑÊ¶ÇÁéáË∂äÂ∞èË∂äÂ•Ω: minimize Loss = (prob_true-1)¬≤ + ‚àë(prob_other)¬≤„ÄÇ ‰ΩÜÊòØÂú® Loss ÁöÑÊ¢ØÂ∫¶Ë°®ËææÂºè‰∏≠Â≠òÂú® prob_true Ëøô‰∏™Âõ†Â≠êÔºåÂèØËÉΩÂú®ËÆ≠ÁªÉÂàùÊúü prob_true ÂæàÂ∞èÔºåÊ¢ØÂ∫¶Ë∂ã‰∫é0ÔºåÊó†Ê≥ïÊõ¥Êñ∞„ÄÇ ËÄåÂú®Áî® CE ÂÅöLossÊó∂ÔºåÂÆÉÁöÑÊ¢ØÂ∫¶‰∏≠‰∏çÂê´ÊúâÂçïÁã¨ÁöÑ prob_true Ëøô‰∏ÄÈ°πÔºàË¢´Ê∂àÊéâ‰∫ÜÔºâÔºåÂ∞±‰∏çÊòìÂèëÁîüÊ¢ØÂ∫¶Ê∂àÂ§±¬≤ „ÄÇ\n(2022-11-06) ÂàÜÁ±ªÈóÆÈ¢òÂ∏∏‰ΩøÁî® softmaxÔºåÊâÄ‰ª•ÈÄÇÂêà‰ΩøÁî®CEÔºõËÄåÂõûÂΩíÈóÆÈ¢ò‰∏çÂ∏∏‰ΩøÁî®softmaxÔºåÊâÄ‰ª•ÈÄÇÂêà‰ΩøÁî® MSE„ÄÇ\nÁêÜËÆ∫ËßíÂ∫¶Ôºö‰∫åËÄÖÂÅáËÆæ‰∏çÂêåÔºåMSEÂÅáËÆæËßÇÂØüÂà∞ÁöÑ y\u0026rsquo;=ÁúüÂÆûy+È´òÊñØÂô™Â£∞ÔºåÊâÄ‰ª•ÈÄöËøáÊûÅÂ§ß‰ººÁÑ∂Ê≥ïÊ±ÇËß£‰∏ÄÁªÑÂèÇÊï∞‰ΩøÂæóÂØπÂ∫îÁöÑÈ´òÊñØÂô™Â£∞ÊúÄÂ∞èÁöÑÊÉÖÂÜµ„ÄÇÊâÄ‰ª•MSEÊ±ÇËß£Âá∫Êù•ÁöÑÂÄº‰ºöÊõ¥ÂÅèÂêë‰∫éÂêÑ‰∏™Á¶ªÊï£ÁöÑËßÇÂØüÂÄº„ÄÇËÄåCEÁöÑÂÅáËÆæÂ∫îËØ•ÊòØÂ§öÂàÜÁ±ªÊÉÖÂÜµ‰∏ãÔºåÊãüÂêà‰∏çÂêåÁ±ªÂà´ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ\u0026ldquo;Â§öÂàÜÁ±ªÈóÆÈ¢òÁöÑÂàÜÂ∏ÉÁ¨¶ÂêàÂ§öÈ°πÂºèÂàÜÂ∏ÉÔºåCEÊòØÂ§öÈ°πÂºèÂàÜÂ∏ÉÁöÑÊúÄÂ§ß‰ººÁÑ∂‚Åµ\u0026rdquo;\n‰∫§ÂèâÁÜµ‰∏çÈÄÇÁî®ÂõûÂΩíÈóÆÈ¢ò‚Å¥ Âú®(Â§ö)ÂàÜÁ±ªÈóÆÈ¢ò‰∏≠Ôºå‰∫§ÂèâÁÜµÁöÑÊçüÂ§±ÂáΩÊï∞Âè™ÂíåÂàÜÁ±ªÊ≠£Á°ÆÁöÑÈ¢ÑÊµãÁªìÊûúÊúâÂÖ≥Á≥ªÔºåËÄåMSEÁöÑÊçüÂ§±ÂáΩÊï∞ËøòÂíåÈîôËØØÁöÑÂàÜÁ±ªÊúâÂÖ≥Á≥ªÔºåÂõ†Ê≠§ËØ•\u0026quot;ÂàÜÁ±ª\u0026quot;ÂáΩÊï∞Èô§‰∫ÜËÆ©Ê≠£Á°ÆÁöÑÂàÜÁ±ªÂ∞ΩÈáèÂèòÂ§ßÔºåËøò‰ºöËÆ©ÈîôËØØÁöÑÂàÜÁ±ªÂèòÂæóÂπ≥ÂùáÔºå‰ΩÜÂÆûÈôÖÂú®ÂàÜÁ±ªÈóÆÈ¢ò‰∏≠ÔºåMSE ÁöÑËøô‰∏™Ë∞ÉÊï¥ÊòØÊ≤°ÊúâÂøÖË¶ÅÁöÑ ‚Å∂„ÄÇ\nÊää Â§öÂàÜÁ±ªÈóÆÈ¢ò ‰∏≠ÁöÑ \u0026ldquo;Á±ªÂà´\u0026rdquo; ÂØπÂ∫îÂà∞ Â§öÂÖÉÂõûÂΩíÈóÆÈ¢ò ‰∏≠ÁöÑ \u0026ldquo;ÁâπÂæÅ\u0026rdquo;„ÄÇÂØπ‰∫é‰∏Ä‰∏™ËøûÁª≠ÁöÑËæìÂá∫ÈáèÔºåÂ∫îÊòØÁî±ÂêÑ‰∏™ÁâπÂæÅÂÖ±Âêå‰ΩúÁî®ÁöÑÔºåÂàÜÂà´Êúâ‰∏çÂêåÁöÑË¥°ÁåÆÔºåËÄå‰∏çËÉΩÂè™ÁúãÈáçÊüê‰∏Ä‰∏™ÁâπÂæÅÔºåÊâÄ‰ª•CE‰∏çÈÄÇÂêàÂõûÂΩíÈóÆÈ¢ò„ÄÇ‰ΩÜ‰πüÂèØ‰ª•Áî® ‚Å∑„ÄÇ\nÊçüÂ§±ÂáΩÊï∞ÁöÑÊÄßË¥® (2023-02-17)\nÂèØÂæÆÂàÜÊÄß ÂèØÂØºÊÄß ÂçïË∞ÉÊÄß Âá∏ÊÄß ÂèØÂàÜÁ¶ªÊÄß ÂèØË°®Á§∫ÊÄß ÂÄüÂä© pytorch ÂèØËßÜÂåñÊçüÂ§±ÂáΩÊï∞ÁöÑÂØºÊï∞9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch import matplotlib.pyplot as plt def abs_func(x): return x.abs() x = torch.linspace(-2,2,100) x.requires_grad_(True) y = abs_func(x) plt.plot(x.detach().numpy(), y.detach().numpy()) plt.show() y_prime = torch.autograd.grad(y.sum(), x, create_graph=True)[0] plt.plot(x.detach().numpy(), y_prime.detach().numpy()) plt.show() Ref „ÄêÊ©üÂô®Â≠∏Áøí2021„ÄëÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØË®ìÁ∑¥‰∏çËµ∑‰æÜÊÄéÈ∫ºËæ¶ (‰∫å)Ôºö ÊâπÊ¨° (batch) ËàáÂãïÈáè (momentum)-ÊùéÂÆèÊØÖ ‚ÄúÊçüÂ§±ÂáΩÊï∞‚ÄùÊòØÂ¶Ç‰ΩïËÆæËÆ°Âá∫Êù•ÁöÑÔºüÁõ¥ËßÇÁêÜËß£‚ÄúÊúÄÂ∞è‰∫å‰πòÊ≥ï‚ÄùÂíå‚ÄúÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°Ê≥ï‚Äù Êú∫Âô®Â≠¶‰π†Èù¢ËØï‰πãMSE‰∏éCEÁöÑÂå∫Âà´Ôºü- ÁÆÄ‰π¶ ‰∫§ÂèâÁÜµÊçüÂ§±(Cross-entropy)ÂíåÂπ≥ÊñπÊçüÂ§±(MSE)Á©∂Á´üÊúâ‰ΩïÂå∫Âà´Ôºü ‰∏∫‰ªÄ‰πàÂùáÊñπÂ∑ÆÔºàMSEÔºâ‰∏çÈÄÇÂêàÂàÜÁ±ªÈóÆÈ¢òÔºü‰∫§ÂèâÁÜµÔºàcross-entropyÔºâ‰∏çÈÄÇÂêàÂõûÂΩíÈóÆÈ¢òÔºü ÁÆÄÂçïÁöÑ‰∫§ÂèâÁÜµÔºå‰Ω†ÁúüÁöÑÊáÇ‰∫ÜÂêóÔºü - Ëî°Êù∞ÁöÑÊñáÁ´† - Áü•‰πé ÂàÜÁ±ªÊ®°Âûã‰∏≠‰∫§ÂèâÁÜµÊØîMSEÊõ¥ÂêàÈÄÇ - ÈÉùÊõåÈ™è- github ÂàÜÁ±ªÂøÖÁÑ∂‰∫§ÂèâÁÜµÔºåÂõûÂΩíÊó†ËÑëMSEÔºüÊú™ÂøÖ - ÂÜ∑ÊØîÁâπerÁöÑÊñáÁ´† - Áü•‰πé Êú∫Âô®Â≠¶‰π†Âü∫Á°ÄÂ≠¶‰π†-Â§öÂÖÉÁ∫øÊÄßÂõûÂΩíÈóÆÈ¢òÔºàÊï∞Â≠¶Ëß£ÂÆûÁé∞Ôºâ- csdn 6.2 ÊçüÂ§±ÂáΩÊï∞ÊÄßË¥® - Ê¢óÁõ¥Âì•‰∏∂(Ë¥®Èáè‰∏çÈ´òÔºåÂè™Áúã‰ª£Á†Å) ","date":"2022-11-06T22:23:00Z","permalink":"https://zichen34.github.io/writenotes/calc/dl_loss_functions/","title":"memo: DL | Loss Functions"},{"content":" iterable ÊòØÂ≠òÂÇ®Â§ö‰∏™ÂÄºÁöÑÂØπË±°ÔºåÂπ∂ÂèØ‰ª•ÈÄê‰∏™ËøîÂõûÂÆÉÁöÑÂÄº generator defined by yield itertools.cycle(iterable) vs while True: ÂâçËÄÖÁîüÊàêÁöÑ iterator ÂèØ‰ª•Êó†ÈôêÊ¨°ÈáçÂ§çÈÅçÂéÜ‰∏Ä‰∏™ÊúâÈôêÁöÑ iterableÔºåËÄåwhile True Âè™‰ºöÈÅçÂéÜ‰∏ÄÈÅç iterable„ÄÇ so\n1 2 3 4 5 6 7 8 9 10 def myGenerator():\t# define a iterable yield 1 yield 2 def loop_iterator(iterable=[]): for val in itertools.cycle(iterable): # turn to iterator yield val\t# this func is a generator gen = loop_iterator(iterable=myGenerator())\t# create a generator next(gen) Contrast:\n1 2 3 4 5 6 def loop_while(iterable): while True: for val in iterable: yield val\t# define a generator gen = loop_while(myGenerator()) next(gen) yield ÂÖ≥ÈîÆÂ≠óÁî®‰∫é generator function ‰∏≠ÔºåËøîÂõû‰∏Ä‰∏™ÂÄºÔºå‰ΩÜ‰∏çÁªàÊ≠¢ÂáΩÊï∞ÊâßË°åÔºåÁ≠âÂæÖ‰∏ã‰∏ÄÊ¨°Ë∞ÉÁî®ÂÜçÁªßÁª≠ GfG-yield„ÄÇ Generator function ÂàõÂª∫‰∏Ä‰∏™ generator objectÔºå‰πüÂ∞±ÊòØiterableÔºåÂÆÉÂèØ‰ª•Áî® __next__() ÊñπÊ≥ïÊàñ for Âæ™ÁéØÈÅçÂéÜ„ÄÇ ÂÆö‰πâ generator function Âè™ÈúÄË¶Å yield ÂÖ≥ÈîÆÂ≠óÔºåÊØîËæÉÊñπ‰æøÔºåËÄåÂÆö‰πâ iterator ÈúÄË¶ÅÂÆö‰πâ __next__() Âíå __iter__() ÊñπÊ≥ï GfG-gener\niterator has next iterator ÊòØÂÆûÁé∞‰∫Ü__iter__() Âíå __next__() ÊñπÊ≥ïÁöÑÂØπË±°ÔºåËÄå list, tuple, dict, set ÈÉΩÊòØ iterable ÂØπË±°ÔºåÂÆÉ‰ª¨ÈÉΩÂèØ‰ª•Ë∞ÉÁî®Ëá™Â∑±ÁöÑ iter() ÊñπÊ≥ïÂèòÊàê‰∏Ä‰∏™ iterator„ÄÇ for Âæ™ÁéØÈÅçÂéÜ iterable Êó∂ÂÖ∂ÂÆûÂàõÂª∫‰∫Ü‰∏Ä‰∏™iteratorÔºåÊØèÊ¨°Âæ™ÁéØÊâßË°å__next__()ÊñπÊ≥ï w3school\n1 2 3 myList = [1,2,3] myIterator = iter(myList) # Êàñ ls.__iter__() print(next(myIterator)) # traverse the next value ","date":"2022-10-24T10:57:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_iter/","title":"memo: Python | iter"},{"content":"2022-8-21: Experiments were conducted on the code of PixeNerf. Trying to get the identical loss curves every time.\nÂú® train.py ‰∏≠ËÆæÁΩÆnp.random.seed(0)Âíåtorch.manual_seed(0)Ôºå‰ΩøÊØèÊ¨°ËÆ≠ÁªÉÊó∂ÁöÑÂõæÁâáÂíåÂÉèÁ¥†Ôºå‰ª•ÂèäÈ™åËØÅÊó∂ÁöÑobjectÂíåËßÜÂõæÊòØ‰∏ÄÊ†∑ÁöÑÔºõ\nÂú® trainer.py ‰∏≠ËÆæÁΩÆ worker_fn\nÂú® nerf.py ‰∏≠ËÆæÁΩÆtorch.manual_seed(2201)ÔºåÊØèÊ¨°Âèñ‰∏ÄÊ†∑ÁöÑÈöèÊú∫Êï∞ÔºålossÊõ≤Á∫øÊúâÁöÑÂú∞ÊñπËøòÊòØÊúâ0.1ÁöÑÂ∑ÆÂºÇ„ÄÇ\ntrain_set ÈÄöËøáColorJitterDataset ÂÅö‰∫ÜÈ¢úËâ≤Â¢ûÂº∫Ôºådata.util.py‰∏≠Âä†np.random.seed(0)ÔºåÁÑ∂ÂêéE 0 ÁöÑÁ¨¨1‰∏™batch ÁöÑpsnrÂ∞±Áõ∏Âêå‰∫Ü(10.55053)„ÄÇ\nËÆæÁΩÆ models.py ‰∏≠ÁöÑËøêÁÆó‰∏∫ deterministicÔºåpytorch=1.6.0 ÔºàDocs‰∏≠ÔºâÂè™Êúâ‰ª•‰∏ã‰∏§‰∏™ËÆæÁΩÆÔºå‰ΩÜÂ•ΩÂÉèÊ≤°ÊïàÊûú„ÄÇ\n1 2 torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False pytorch=1.8 ÊâçËÉΩ‰ΩøÁî® torch.use_deterministic_algorithms()Ôºå‰∫éÊòØ:\nÂç∏ËΩΩconda uninstall pytorchÔºåÈáçË£Ö‰∫Ü1.12.1ÔºåÊä•Èîô:\npyparsing.exceptions.ParseException: Expected '}', found '='„ÄÇ\nÂèàÈáçË£Ö‰∫Ü 1.10.2 ‰πãÂêé: Âú®from torch.utils.tensorboard import SummaryWriterÂ§ÑÊä•Èîô: AttributeError: module 'distutils' has no attribute 'version'\nÈôçÁ∫ß:\n1 2 pip uninstall setuptools pip install setuptools==59.5.0 ÁÑ∂ÂêéÈÇ£‰∏™ÂèÇÊï∞Ëß£ÊûêpyparsingËøòÊòØÊä•ÈîôÔºå‰∏çÁü•ÈÅìÊÄé‰πàËß£ÂÜ≥„ÄÇ pytorch forum\nÊääÁéØÂ¢ÉÂà†‰∫ÜÔºå‰øÆÊîπ environment.yml ‰∏≠ÁöÑÁâàÊú¨Ôºöpytorch==1.11.0, torchvision==0.12.0 (ÁâàÊú¨Âè∑Áõ∏Â∑Æ1)ÔºåÈáçÊñ∞ÂàõÂª∫ÁéØÂ¢ÉÔºöconda env create -f envxx.yml„ÄÇÁÑ∂ÂêéÂú® torch.matmul() Â§ÑÊä•Èîô:\n1 2 3 4 5 6 7 RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA \u0026gt;= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility Âú®Á®ãÂ∫èÂºÄÂ§¥ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºöexport CUBLAS_WORKSPACE_CONFIG=:4096:8„ÄÇ\nÂèàÂú® loss.backward() ‰∏≠ autograd Êó∂Êä•ÈîôÔºö\n1 2 3 4 5 6 RuntimeError: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set \u0026#39;torch.use_deterministic_algorithms(True)\u0026#39;. You can turn off determinism just for this operation, or you can use the \u0026#39;warn_only=True\u0026#39; option, if that\u0026#39;s acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.` Âä†‰∏äÂèÇÊï∞Ôºötorch.use_deterministic_algorithms(True, warn_only=False) ÂèØ‰ª•ËøêË°åÔºõÂÜçÂä†‰∏ätorch.backends.cudnn.benchmark = FalseÔºåÈô§batch 1Â§ñÔºå‰∏§Ê¨°ÂÆûÈ™åÁªìÊûú‰ªç‰∏çÂÆåÂÖ®‰∏ÄËá¥ÔºåËÄå‰∏îÊÄßËÉΩ‰∏ãÈôçÂæàÂ§öÔºåÂ•ΩÂÉèÊòØpytorchÁâàÊú¨ÂØºËá¥ÁöÑÔºåÊîæÂºÉ„ÄÇÂèàÈáçË£ÖÂõûÂéüÁéØÂ¢É„ÄÇ torch-reproducibility-doc; It\u0026rsquo;s introduced in 1.8\npython train/train.py --name dtu_origin --conf conf/exp/dtu.conf --datadir data/DTU_Dataset/rs_dtu_4 --nviews 3 --gpu_id='0 2' --epochs 400_000\n","date":"2022-10-24T10:57:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_reproducibility/","title":"memo: PyTorch | Reproducibility"},{"content":"Pytorch-Transformer Model Parallelism using Transformers and PyTorch\nLoading the data\nInstantiate a model\nCreate torch Dataset and DataLoader\n1 class myDataset(torch.utils.data.Dataset): Split the data into train and val sets:\n1 2 from sklearn.model_selection import train_test_split df_train, df_val = train_test_split(imdb_df, test_size=0.3, random_state=2021 create DataLoader for train set and val set:\nMake a wrapper for the model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MultiGPUClassifier(torch.nn.Module): def __init__(self, roberta_model): super(MultiGPUClassifier, self).__init__() # Embedding layer --\u0026gt; cuda:0 self.embedding = roberta_model.roberta.embeddings.to(\u0026#39;cuda:0\u0026#39;) # Encoder Layer --\u0026gt; cuda:1 self.encoder = roberta_model.roberta.encoder.to(\u0026#39;cuda:1\u0026#39;) # Classifier --\u0026gt; cuda:1 self.classifier = roberta_model.classifier.to(\u0026#39;cuda:1\u0026#39;) def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None): # Pass the input_ids to cuda:0 since embedding layer in cuda:0 emb_out = self.embedding(input_ids.to(\u0026#39;cuda:0\u0026#39;)) # Move the outputs of embedding layer to cuda:1 as input to encoder layer enc_out = self.encoder(emb_out.to(\u0026#39;cuda:1\u0026#39;)) classifier_out = self.classifier(enc_out[0]) return classifier_out # Initialize the model multi_gpu_roberta = MultiGPUClassifier(roberta_model) Upon constructing the model, the memory usage can be seen using nvidia-smi.\nCreate optimizer and loss function for the model:\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import get_linear_schedule_with_warmup, AdamW EPOCHS = 2 LR = 1e-5 optimizer = AdamW(multi_gpu_roberta.parameters(), lr=LR) total_steps = len(train_data_loader)*EPOCHS scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) loss_fn = torch.nn.CrossEntropyLoss().to(\u0026#39;cuda:1\u0026#39;) # match with the roberta.classifier layer Create a helper function for training the model and returning accuracy and losses:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def train_model (model, data_loader, loss_fn, optimizer, scheduler, n_examples): model = model.train() # losses = [] correct_predictions = 0 for d in data_loader: # take a batch input_ids = d[\u0026#39;input_ids\u0026#39;] attention_mask = d[\u0026#39;attention_mask\u0026#39;] # Reshaping attention mask for adapting the forward pass reshaped_attention_mask = attention_mask.reshape(d[\u0026#39;attention_mask\u0026#39;].shape[0],1,1,d[\u0026#39;attention_mask\u0026#39;].shape[1]) targets = d[\u0026#39;labels\u0026#39;] outputs = model(input_ids = input_ids, attention_mask = reshaped_attention_mask) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets.to(\u0026#39;cuda:1\u0026#39;)) # move targets to cuda:1 to calculate loss correct_prediction += torch.sum(preds == targets.to(\u0026#39;cuda:1\u0026#39;)) losses.append(loss.item()) loss.backward() # Clip the gradients of the model to prevent exploding gradients using clip_grad_norm torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() # gradient descent scheduler.step() # lr decay optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) Create a helper function for evaluating the model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def eval_model(model, data_loader, loss_fn, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for d in data_loader: input_ids = d[\u0026#39;input_ids\u0026#39;] attention_mask = d[\u0026#39;attention_mask\u0026#39;] reshaped_attention_mask = attention_mask.reshaped(d[\u0026#39;attention_mask\u0026#39;].shape[0],1,1,d[\u0026#39;attention_mask\u0026#39;].shape[1]) targets = d[\u0026#39;labels\u0026#39;] outputs = model(input_ids = input_ids, attention_mask=reshaped_attention_mask) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets.to(\u0026#39;cuda:1\u0026#39;)) correct_predictions += torch.sum(preds == targets.to(\u0026#39;cuda:1\u0026#39;)) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) Create the training loop and only store the best one:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from collections import defaultdict history = defaultdict(list) best_accuracy = 0 %%time for epoch in range(EPOCHS): print(f\u0026#39;Epoch {epoch+1}/{EPOCHS}) print(\u0026#39;-\u0026#39; * 10) train_acc, train_loss = train_model(multi_gpu_roberta, train_data_loader, loss_fn, optimizer, scheduler, len(df_train)) print(f\u0026#39;Train Loss:{train_loss}; Train Accuracy: {train_acc}\u0026#39;) val_acc, val_loss = eval_model(multi_gpu_roberta, val_data_loader, loss_fn, len(df_val)) print(f\u0026#39;Val Loss: {val_loss}; Val Accuracy: {val_acc}\u0026#39;) history[\u0026#39;train_acc\u0026#39;].append(train_acc) history[\u0026#39;train_loss\u0026#39;].append(train_loss) history[\u0026#39;val_acc\u0026#39;].append(val_acc) history[\u0026#39;val_loss\u0026#39;].append(val_loss) if val_acc \u0026gt; best_accuracy: torch.save(multi_gpu_roberta.state_dict(), \u0026#39;multi_gpu_roberta_best_model_state.bin\u0026#39;) best_accuracy = val_acc Visualizing model performance\nCombining DDP with Model Parallelism DDP tutoria\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class ToyMpModel(nn.Module): # model parallel, multi-gpu model def __init__(self, dev0, dev1): # use 2 gpu super(ToyMpModel, self).__init__() self.dev0 = dev0 self.dev1 = dev1 self.net1 = torch.nn.Linear(10,10).to(dev0) # move 0th layer to dev0 self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10,5).to(dev1) # move 1st layer to dev1 def forward(self, x): x = x.to(self.dev0) # move input to the dev0 same as 0th layer x = self.relu(self.net1(x)) x = x.to(self.dev1) # move output of 0th layer to dev1 return self.net2(x) DDP wraps a multi-GPU model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def demo_model_parallel(rank, world_size): # rank indicates the index of this process, world_size is the total numer of gpu will be used. print(f\u0026#34;Running DDP with model parallel example on rank {rank}.\u0026#34;) setup(rank, world_size) # set env vars, and initialize process group # set up multi-GPU model and devices for this process dev0 = (rank * 2) % world_size dev1 = (rank * 2 + 1) % world_size mp_model = ToyMpModel(dev0, dev1) ddp_mp_model = DDP(mp_model) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001) optimizer.zero_grad() # outputs will be on dev1 outputs = ddp_mp_model(torch.randn(20,10)) labels = torch.randn(20, 5).to(dev1) loss_fn(outputs, labels).backward() optimizer.step() cleanup() if __name__ == \u0026#34;__main__\u0026#34;: n_gpus = torch.cuda.device_count() assert n_gpus \u0026gt;=2, f\u0026#34;Requires at least 2 GPUs to run, but got {n_gpus}\u0026#34; world_size = n_gpus run_demo(demo_model_parallel, world_size) Apply Model Parallel to Existing Modules SINGLE-MACHINE MODEL PARALLEL BEST PRACTICES ResNet50\nnn.Sequential\nGNT model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class GNTModel(object): ‚îú __init__(self,args) ‚îÇ ‚îú‚îÄ self.net_coarse = GNT().to(device) ‚îÇ ‚îÇ ‚îî‚îÄ __init__() ‚îÇ ‚îÇ ‚îú‚îÄ self.rgbfeat_fc = nn.Sequential(Liner, ReLU, Linear) ‚îÇ ‚îÇ ‚îú‚îÄ for i in range(args.trans_depth): ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.view_trans.append( Transformer2D() ) # ModuleList() ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.fc1 = nn.Linear(dim, ff_hid_dim) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.fc2 = nn.Linear(ff_hid_dim, dim) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.dp = nn.Dropout(ff_dp_rate) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ self.activ = nn.ReLU() ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ self.attn = Attention2D(dim,attn_dp_rate) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.q_fc = nn.Linear(dim, dim, bias=False) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.k_fc = nn.Linear(dim, dim, bias=False) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.v_fc = nn.Linear(dim, dim, bias=False) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.pos_fc = nn.Sequential(Linear, ReLU, Linear) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.attn_fc = nn.Sequential(Linear, ReLU, Linear) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.out_fc = nn.Linear(dim, dim) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ self.dp = nn.Dropout(dp_rate) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.ray_trans.append( Transformer() ) # nn.ModuleList() ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ self.attn = Attention(dim, n_heads, attn_dp_rate, attn_mode, pos_dim) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ if attn_mode in [\u0026#34;qk\u0026#34;,\u0026#34;gate\u0026#34;]: ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ if attn_mode in [\u0026#34;pos\u0026#34;, \u0026#34;gate\u0026#34;]: ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ if attn_mode == \u0026#34;gate\u0026#34;: ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.v_fc = nn.Linear(dim, dim, bias=False) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.out_fc = nn.Linear(dim, dim) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.dp = nn.Dropout(dp_rate) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.n_heads = n_heads ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ self.attn_mode = attn_mode ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ if i % 2 == 0: self.q_fc.append( nn.Sequential()) ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ else: self.q_fc.append(nn.Identity()) # nn.ModuleList() ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.pos_enc = Embedder() # 21x3=63 funcs ‚îÇ ‚îÇ ‚îî‚îÄ self.view_enc = Embedder() # 21x3=63 funcs ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.net_fine = GNT().to(device) ‚îÇ ‚îú‚îÄ self.feature_net = ResUNet(coarse_out_ch, fine_out_ch, signle_net).to(device) ‚îÇ ‚îÇ ‚îú‚îÄ self.conv1 = nn.Conv2d() ‚îÇ ‚îÇ ‚îú‚îÄ self.bn1 ‚îÇ ‚îÇ ‚îú‚îÄ self.relu ‚îÇ ‚îÇ ‚îú‚îÄ self.layer1 = self._make_layer(block,planes,blocks,stride,dilate) ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ norm_layer ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ previous_dilation ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ if dilate: self.dilation *= stride ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ if stride !=1 or inplanes differs from outchanl * expansion: ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ downsample = nn.Sequential(conv1x1(), norm_layer()) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ layers.append(BasicBlock(self.inplanes, planes, stride,downsample,)) # list ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ norm_layer ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.conv1 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.bn1 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.relu ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.conv2 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.bn2 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.downsample ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ self.stride ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.inplanes = planes * block.expansion ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ for _ in range(1,blocks): ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ layers.append(BasicBlock(self.inplanes,planes,)) ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ norm_layer ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.conv1 ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.bn1 ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.relu ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.conv2 ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.bn2 ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.downsample ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ self.stride ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ self.layer2 ‚îÇ ‚îÇ ‚îú‚îÄ self.layer3 ‚îÇ ‚îÇ ‚îú‚îÄ self.upconv3 ‚îÇ ‚îÇ ‚îú‚îÄ self.iconv3 ‚îÇ ‚îÇ ‚îú‚îÄ self.upconv2 ‚îÇ ‚îÇ ‚îú‚îÄ self.iconv2 ‚îÇ ‚îÇ ‚îî‚îÄ self.out_conv ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ learnable_params ‚îÇ ‚îú‚îÄ self.optimizer = torch.optim.Adam() ‚îÇ ‚îú‚îÄ self.scheduler ‚îÇ ‚îú‚îÄ self.start_step ‚îÇ ‚îî‚îÄ if args.distributed: ‚îÇ ‚îú‚îÄ self.net_coarse = torch.nn.parallel.DDP ‚îÇ ‚îú‚îÄ self.feature_net = torch.nn.parallel.DDP ‚îÇ ‚îî‚îÄ self.net_fine = torch.nn.parallel.DDP ‚îÇ ‚îú switch_to_eval(self): ‚îÇ ‚îî‚îÄ render_kwargs_train{network_query_fn, model, ...} ‚îú switch_to_train(self): ‚îú save_model(self, filename): ‚îú load_model(self, filename): ‚îî load_from_ckpt(self, out_folder): ‚îÇ () ‚îú‚îÄ h In a new file: model_parallel.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class GNTModel(object): ‚îú __init__(self, args, load_opt=True, load_schedular=True): ‚îÇ ‚îú‚îÄ device = [\u0026#39;cuda:0\u0026#39;,\u0026#39;cuda:1\u0026#39;,\u0026#39;cuda:2\u0026#39;,\u0026#39;cuda:3\u0026#39;] ‚îÇ ‚îú‚îÄ self.net_coarse = GNT(..., device) # transformer_network_parallel.py ‚îÇ ‚îú‚îÄ __init__(self, args, in_feat_ch=...) ‚îÇ ‚îú‚îÄ super(GNT, self).__init__() ‚îÇ ‚îú‚îÄ self.rgbfeat_fc = nn.Sequential().to(device[0]) ‚îÇ ‚îú‚îÄ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ In train.py:\n1 from gnt.model_parallel import GNTModel Pytorch-DDP-RPC Invited Talk: PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li ytb\n(DDG search: tensorflow model split distributed parallel)\n","date":"2022-10-21T20:35:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_model_parallel/","title":"memo: PyTorch | Model Parallel"},{"content":"Elliot Waite Source video: PyTorch Autograd Explained - In-depth Tutorial-Elliot Waite\nTwo tensors are created with the values of 2 and 3 and assigned to variables a and b. Then the product of a and b is assigned to variable c.\nIn the following diagram, each struct is a tensor containing several attributes.\ndata holds the data of the tensor grad holds the calculated gradient value grad_fn, gradient function points to a node in the backwards graph is_leaf marks is this tensor a leaf of a graph requires_grad is False by default for all tensors that are being input (like a,b) into or output (like c) from an operation. Such that no backwards graph will be created. However, if the attribute requires_grad of a is set to True and a is passed into any operation (Mul), the output tensor (c) will also have requires_grad=True and be apart of the backwards graph, because c is no longer a leaf. And c\u0026rsquo;s grad_fn points to MulBackward, which calculates the gardient of its operation w.r.t. input tensor a: ‚àÇc/‚àÇa = ‚àÇ(a‚àób)/‚àÇa = b\nThe blue contents are the backwards graph behind the tensors and their operations.\nWhen the Mul function is called, the ctx context variable will save the values to be used in the backwards pass: MulBackward operation.\nSpecifically, the input tensor a is stored by the method ctx.save_for_backward(...) and referenced by the property ctx.saved_tensors in the backward pass.\nThe MulBackward has another attribute next_functions is a list of tuples, each one associated with an input tensor that were passed to the Mul operation.\n(AccumulatedGrad, 0) corresponds to the input tensor a meaning the gradient of a will be calculated continuously by AccumulatedGrad operation. (None, 0) is associated with input tensor b, no further calculation is needed for its gradient. The AccumulateGrad operation is used to sum the gradients (from multiple operations) for the input tensor a.\nWhen executing c.backward(), the backward pass of gradients starts. The initial gradient is 1.0 and then passed into MulBackward, where it times b getting 3.0. Then by looking at the next_functions, it needs to get into AccumulatedGrad to obtain the gradient w.r.t. tensor a. Finally, the attribute grad of a comes from AccumulateGrad.\nSimple example Two input tensors are both requires_grad=True. They\u0026rsquo;re multiplied together to get c. If here executing c.backward(), the initial gradient 1.0 will start the backward pass from its grad_fn. Then tensor d is created to multiply with c to get e. If executing e.backward() here, to calculate the gradient of e w.r.t. the leaft nodes on the graph, the backward pass is as follows: The initial gradient 1.0 is first passed into e.grad_fn, i.e., MulBackward, where it multiplies with gradient of the operation w.r.t. the leaf nodes: ‚àÇ(c‚àód)/‚àÇd (=c=6.0). (Since c is not a leaf, ‚àÇe/‚àÇc doesn\u0026rsquo;t need to compute.)\nThen by looking at property next_functions, the gradients go continuously into MulBackward and AccumulateGrad separately.\nIn MulBackward, to get ‚àÇe/‚àÇa, the incoming gradient ‚àÇe/‚àÇc multiplies with gradient of the Mul operation w.r.t. a: ‚àÇ(a‚àób)/‚àÇa, so the gradient of leaf a is ‚àÇe/‚àÇa = ‚àÇe/‚àÇc √ó ‚àÇ(a‚àób)/‚àÇa = 4√ó3=12. Also to get ‚àÇe/‚àÇb, the incoming gradient ‚àÇe/‚àÇc multiplies with ‚àÇ(a‚àób)/‚àÇb, ‚àÇe/‚àÇb = ‚àÇe/‚àÇc √ó ‚àÇ(a‚àób)/‚àÇb = 4√ó2 = 8 While ‚àÇe/‚àÇc gets into AccumulateGrad, no other operations needed to calculate the gradients w.r.t. d, the grad of leaf node d is 6.0. Avoid In-place operation When the MulBackward operation retrieve input tensors through ctx.saved_tensors, it is necessary to ensure that the correct values are referenced. Therefore, each tensor must maintains its attribute _version, which will be increamented (+1) when performing in-place operation (e.g., c+=1) each time.\nThus, if calling e.backward() after c+=1, the ctx.saved_tensors will get an error, beacuse the attribute _version of the input tensor c is not matched with the previously saved one. In this way, the input tensors to be used are ensured haven\u0026rsquo;t changed in the time since the operation was performed in the forward pass.\nHowever, the Add function doesn\u0026rsquo;t affect the graidents, so it doesn\u0026rsquo;t need to save any its input tensors for the backward pass. Hence, the ctx will not store its input tensors. And the initial gradient will directly looking at the next_functions after getting into AddBackward node. In this case, doing c+=1 before e.backward(), no errors will occur because the input tensor c is not retrieved by ctx.saved_tensors. Unbind operation A tensor is created with 1-D list holding 3 values and assigned to variable a. Then by executing b,c,d = a.unbind(), tensor a is split along the 1st dimension and tensors b, c, d are created. This operation will generate the graph as follows: All of the grad_fn of b, c, d point to the same UnbindBackward function.\nIf b, c, d are multiplied together to get e, there will be two Mul operation in the forward pass and two MulBackward is the backward pass. The property next_functions of those 2 MulBackward functions both have tuple (UnbindBackward, i ), but their indecies are different, where i is 0, 1, 2 corresponding to the 3 outputs from the Unbind function.\n(UnbindBackward, 0) means the current gradient is associated with the first input tensor b of the (1st) Mul operation, which is also the first output from the Unbind function. (UnbindBackward, 1) is saying this is the gradient for the second output fromt he Unbind function. (UnbindBackward, 2) indicates this gradient is for the third output of the Unbind function. The index value is used to inform the UnbindBackward which output tensor the gradient is calculated for. Such that the UnbindBackward can output a list of gradients.\nThe gradient of the leat node can be calculated by calling e.backward(). The backward pass is started off with the initial gradient of 1. Complicated example The following scalar tensor can be replaced with any vector or matrix or any n-dimension array.\nThe tensors are created with the same value of 2 and they both don\u0026rsquo;t require grad. a and b are multiplied together to get c, which doesn\u0026rsquo;t require grad too. Then by executing c.requires_grad = True, the tensor c will be a port of the backwards graph. So that any future operations done using c as an input will start to build the backwards graph.\nThe full forward graph is build as:\nAnother tensor d is created with requires_grad=False. Multiply d with c to get e. Another leaf f is constructed with requires_grad=False (not on the graph). Multiply f with e to get g Another tensor h is created with requires_grad=True (a leaf on the graph). Devide g by h to get i; Add i and h together to get j. (h is leaf and fed into 2 operations, so its AccumulteGrad has two inputs from DivBackward and AddBackward. Multiply j and i together to get k. (i is passed into both Add and Mul, so its (grad_fn) DivBackward has 2 streams from AddBackward and MulBackward. Unlike h has its 2 backward streams converge at AccumulateGrad, the 2 backward streams of i instead converge at the DivBackward that corresponds to the operation Div generating the i. (Yellow means the leaf that doesn\u0026rsquo;t on the graph. Green means the leaf on the graph. Brown means the non-leaf)\nAt the end, by executing k.backward(), the backward pass will start with a gradient of 1.0, which will times the gradient of each operation w.r.t. local input tensors sequentially from bottom to top. If an operation\u0026rsquo;s input is a leaf, the next_functions will point to the AccumulateGrad for this leaf node. Once all gradients streams are accumulated, the sum is put into the attribute grad of the left node. Finally, ‚àÇk/‚àÇleaf will obtained. retain_grad() By default, the gradients will only calculated for the leaf nodes, and the grad of the intermediate nodes are kept None. But an intermediate tensor can retain their gradient by calling its retain_grad() method. For example, i.retain_grad(), which will set up a hook \u0026ldquo;that gets called in the backward pass that will basically tell the DivBackward function that any gradients passed into it should be saved on the grad attribute of tensor i. detach() m = k.detach() will create a new tensor sharing the same underlying data as k, but m will no longer require gradients. m will be a leaft node but not on the graph because its grad_fn is None without pointing any node on the graph.\nUsually, the backwards graph is expected to get garbage collected after finishing the training loop. Once the k.backward() is executed, some values have no references (get freed) in the graph. Specifically, the references to the saved_tensors. But the actual graph still exists in memory. If the output tensor k is needed to kept for longer than the training loop, k can be detached from the graph.\nSimilar functions:\nk.numpy() : ndarray k.item() : python int or python float k.tolist() : original tensor that holds multiple values will be converted to python list (old summary on 2022-10-19)\nThose tensors whose attribute requires_grad=True will be nodes on a backwards graph. Any output tensor yielded from any operation is not leaf node.\ngrad_fn \u0026amp; grad The grad_fn of the \u0026rsquo;non-leaf\u0026rsquo; (intermediate) node points to a node (like MulBackward) which will multiply the incoming gradient by the gradient of this operation w.r.t. its inputs (leaf nodes with requires_grad=True), or pass the grad of the \u0026rsquo;non-leaf\u0026rsquo; node up to next gradient-computing node for other leaf nodes before. This\u0026rsquo;s like the divergence of ‚àÇL/‚àÇw·∂¶ and ‚àÇL/‚àÇw·∂¶‚Åª¬π.\nIf an \u0026ldquo;non-leaf\u0026rdquo; node is passed into multiple operations, its gradient equals the sum of the gradients coming from each operation. Its gradient will be fed into the \u0026ldquo;gradient-computing\u0026rdquo; node as the incoming gradient of the operation that generates it.\nWhile if a leaf node is used by multiple functions, its gradient equals to the sum of the gradients comeing from the \u0026ldquo;gradient-computeing\u0026rdquo; nodes of every operation. Hence, the grad of the leaf node is accumulated by AccumulateGrad function.\nctx ctx (context) stores the operators doing the operation for computing the gradient in backward graph. The gradients at different time are different, so the version of variables needs to be recorded. The in-place operation will increment the version of the variable, and then calling the backward method will cause an error due the mismatch of the _version value. However, if an operation doesn\u0026rsquo;t bring gradients (like Add), its operators are not necessary to store. The incoming gradient will not change when passing through its corresponding backwards function (AddBackward). In this case, modifying its operators before calling .backward() is okay, becaue they are not involved with gradient. Gradient for a one-dim tensor is a list of partial derivative. The second value in the tuple is the index among the mutliple outputs of the operation, indicating to who the gradient belongs\noutput don\u0026rsquo;t have grad by default The \u0026ldquo;non-leaf\u0026rdquo; node will not store grad by default, unless set its .retain_grad(), from which a hook can be set up to make its grad_fn to save any gradient passed to it into grad of this node.\ndetach() m=k.detach() will create a leaf node m sharing the same data as k, but will no longer require gradient. Its grad_fn=None meaning it doesn\u0026rsquo;t have a reference to the backwards graph. This operation is used to store a output value separately, but free the backwards graph after a training loop (forward+backward). Alternatives for detaching from the graph:\nk.numpy(), k.item() convert one-element tensor to python scalar. k.tolist(). (2022-11-01)\ngradient is the upstream gradients until this calling tensor (from the intermediate node to the network ouput). Since the explicit expression of output is unknown, the derivative $\\rm\\frac{ d(output) }{ d(input) }$ cannot be calculated directly. But the derivative $\\rm\\frac{ d(output) }{ d(intermediate) }$ and $\\rm\\frac{ d(intermediate) }{ d(input) }$ are all known. Therefore, by passing the d(output)/d(intermediate), say 0.05, to gradient, the d(output)/d(input) is the returned value of intermeidate.backward(gradient=0.05,). The gradient argument in PyTorch backward - devpranjal Blog (2023-08-02)\nFreeze params module.requires_grad_(False) will change all its parameters. PyTorch Forum\nIf I explicitly add the learnable params one-by-one, like this tutorial (if p.requires_grad), only the ordinal of the layers in the state dict ckpt['optimizer]['state'] changed, while the total number of states remains the number of layers to be performing gradient descent.\n1 2 3 4 5 6 7 # Test in GNT with multiple encoders: self.optimizer.add_param_group({\u0026#34;params\u0026#34;: self.multiEncoders.parameters()}) # Saved `ckpt[\u0026#39;optimizer\u0026#39;][\u0026#39;state\u0026#39;]` has a max index: 1390, but total 262 layers. params = [p for p in self.multiEncoders.parameters() if p.requires_grad] self.optimizer.add_param_group({\u0026#34;params\u0026#34;: params}) #`ckpt[\u0026#39;optimizer\u0026#39;][\u0026#39;state\u0026#39;]` has a max index: 281, but total 262 layers too. with torch.no_grad(): will stop backward-propagation for a block of operations wrapped in its region (in a context). This is equivalent to module.requires_grad_(False). Demo-SO\nInference mode is similar with no_grad, but even faster. module.eval() = module.train(False) affects the settings of nn.Dropout and nn.BatchNorm2d. And it has nothing to do with grad. Docs - Autograd\nIn pixelNeRF, self.net.eval() and self.net.train() will jump to SwinTransformer2D_Adapter.train(mode=True) requires_grad_ vs requires_grad requires_grad_ can do for non-leaf node, while requires_grad will have error. PyTorch Forum\nTwice nn.Parameter (2024-04-11)\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from torch import nn from torchviz import make_dot x = torch.rand(2,3) w = torch.rand(3,2).requires_grad_(True) for i in range(2): f = nn.Parameter(w @ x) # shape (3,3) loss = f.sum() loss.backward() # dot = make_dot(loss, params={\u0026#39;w\u0026#39;: w, \u0026#39;x\u0026#39;: x}) # dot.render(\u0026#39;computational_graph\u0026#39;, format=\u0026#39;png\u0026#39;) As shown above, once \u0026ldquo;re-convert\u0026rdquo; the w@x as the nn.Parameter, the forward graph doesn\u0026rsquo;t include w and the operation Matrixmultiplication. The graph is starting from f instead.\nTherefore, even though w is a leaf node and require_grad is True, it won\u0026rsquo;t obtain .grad: w.grad is None.\nIn contrast, do not re-set the nn.Parameter won\u0026rsquo;t change the leaf tensors.\n1 2 3 4 5 6 7 import torch x = torch.rand(2,3) w = torch.rand(3,2).requires_grad_(True) for i in range(2): f = w @ x loss = f.sum() loss.backward() In this way, the leaf tensors are prepared outside the for loop (specifally forward()). And the leaf tensors (i.e., w) will be added onto the graph correctly during each iteration rebuilding.\nGraph in For Loop (2024-04-11)\nThe computational graph (of PyTorch 1.x) is build in each iteration, as the previous graph has been destroyed after executing .backward()\nTherefore, the tensors to be optimized, i.e., the leaf node of the graph, have to be present within the for loop as the starting point of the graph.\nIn the following code, w needs to be optimized.\n1 2 3 4 5 6 7 8 9 import torch x = torch.rand(2,3) w = torch.rand(2,3).requires_grad_(True) optimizer = torch.optim.Adam([w], lr=0.0001) for i in range(2): f = w * x loss = f.sum() loss.backward() optimizer.step() The forward graph can be plotted as:\nThe endpoint (green box) is the tensor loss The w has performed Multiplication and Sum to get loss The graph will be rebuilt in each iteration, with the w and x as the starting points. And the graph will be freed after loss.backward() optimizer.step() updates leaf tensors that require grad. Docs Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch from torchviz import make_dot x = torch.rand(2,3) w = torch.rand(2,3).requires_grad_(True) optimizer = torch.optim.Adam([w], lr=0.0001) for i in range(2): f = w * x loss = f.sum() loss.backward() optimizer.step() if i == 0: dot = make_dot(loss, params={\u0026#39;w\u0026#39;: w, \u0026#39;x\u0026#39;: x}) dot.render(\u0026#39;computational_graph\u0026#39;, format=\u0026#39;png\u0026#39;) However, if moving the operation f=w*x outside the for loop, the graph only contain an operation: f.sum()\n1 2 3 4 5 6 7 import torch x = torch.rand(2) # leaf w = torch.rand(2).requires_grad_(True) # leaf f = w * x # non-leaf tensor won\u0026#39;t have .grad for i in range(5): loss = f.sum() loss.backward() From the second run, the rebuilt graph only includes \u0026ldquo;Sum\u0026rdquo;, and no longer includes w (out of this graph). In other words, the w doesn\u0026rsquo;t belong to this newly rebuilt graph.\nTherefore, the .backward() method cannot be completed for w.\nw ‚àó x = f r f e o b w u a i r f l d o d ( r ) t l h o e o p g r l a o p s h s e a c h i t e r In the second iteration, the graph doesn\u0026rsquo;t include w. So, when the gradient backpropagates, the gradient cannot reach w.\nOn the other hand, the w still points to the previous graph, without updating.\nError:\n1 2 3 4 5 6 7 File \u0026#34;/home/yi/Downloads/CppCudaExt_PT_Tut_AIkui/test_debug.py\u0026#34;, line 8, in \u0026lt;module\u0026gt; loss.backward() RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. ","date":"2022-10-19T21:17:00-05:00","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_autograd/","title":"memo: PyTorch | Autograd"},{"content":"ÊâãÂÜôÊï∞Â≠óÊï∞ÊçÆÈõÜÊúâ10‰∏™Á±ªÂà´Ôºå\nÂ¶ÇÊûúÂØπÊØè‰∏ÄÁ±ªÂà´ÊåâÁÖß‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºàÊòØ/‰∏çÊòØÔºâËÆ°ÁÆóÊ¶ÇÁéáÁöÑËØùÔºåÊØèÁßçÁ±ªÂà´ÁöÑÊ¶ÇÁéá‰∫íÁõ∏Áã¨Á´ãÔºå‰∏éÁúüÂÆûÊÉÖÂÜµ‰∏≠ÔºåÂêÑÁªìÊûú‰πãÈó¥‰∫íÁõ∏ÊäëÂà∂ÁöÑ‰∫ãÂÆûÁüõÁõæ„ÄÇ\nÊ†∑Êú¨ÁöÑÂàÜÁ±ªÁªìÊûúÊª°Ë∂≥‰∏Ä‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉÔºåËøôÂ∞±Ë¶ÅÊ±ÇÂ±û‰∫éÂêÑÁ±ªÁöÑÊ¶ÇÁéáÈÉΩË¶ÅÂ§ß‰∫é0ÔºåËÄå‰∏îÊ¶ÇÁéá‰πãÂíå‰∏∫1„ÄÇ\n‰∫åÂàÜÁ±ªÈóÆÈ¢òÂè™ÈúÄË¶ÅËÆ°ÁÆó‰∏Ä‰∏™Ê¶ÇÁéáÔºàÂè¶‰∏Ä‰∏™ÊòØ‰∫íË°•ÔºâÔºåÊâÄ‰ª•ÂçÅÂàÜÁ±ªÈóÆÈ¢òÂè™ÈúÄË¶ÅËÆ°ÁÆó9‰∏™Ê¶ÇÁéáÔºå‰ΩÜÊòØÁ¨¨10‰∏™ÂàÜÁ±ªÁöÑËÆ°ÁÆóÊñπÂºè‰∏éÂâç9‰∏™‰∏çÁªü‰∏ÄÔºåÂ∞±ÂØºËá¥ÈúÄË¶ÅÊûÑÈÄ†‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑËÆ°ÁÆóÂõæÂ§ÑÁêÜÁâπÊÆäÊÉÖÂÜµÔºåÊó†Ê≥ïÊúÄÂ§ßÂåñÂú∞ÂÆûÁé∞Âπ∂Ë°åËÆ°ÁÆóÔºåÊâÄ‰ª•Â∏åÊúõÊâÄÊúâÁ±ªÂà´ÁöÑÊ¶ÇÁéáËøêÁÆóÂ§ÑÁêÜÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ\nÊâÄ‰ª•Èô§‰∫ÜÊúÄÂêé‰∏ÄÂ±ÇÔºåÂâçÈù¢ÁöÑÂ±ÇËøòÊòØÁî®sigmoidÔºåÊúÄÂêé‰∏ÄÂ±ÇÁî®softmaxÊøÄÊ¥ªÂáΩÊï∞ÔºåÊª°Ë∂≥Ôºö\n$$ \\begin{cases} P(y=i) ‚â• 0 \\ \\sum·µ¢‚Çå‚ÇÄ^9 P(y=i) =1 \\end{cases} $$\nÂÅáËÆæ $Z^l \\in \\mathbb R^K$ ÊòØÊúÄÂêé‰∏Ä‰∏™Á∫øÊÄßÂ±Ç $l$ ÁöÑËæìÂá∫ÔºåÂÖ±ÊúâK‰∏™Á±ªÂà´ÔºåÂàôÁªèËøá SoftmaxÂáΩÊï∞ÔºåÁ∫øÊÄßÂ±ÇÁöÑËæìÂá∫ÂèòÊàêÊ¶ÇÁéáÂàÜÂ∏ÉÔºö\n$$ P(y=i) = \\frac{e^{Z_i}}{\\sum_{j=0}^{K-1} e^{Z_j}},\\ i\\in{0,\\cdots, K-1 } $$\nÂàÜÂ≠ê‰ΩøÁî®ÊåáÊï∞ËøêÁÆó‰ªéËÄåÊÅíÂ§ß‰∫éÈõ∂ÔºåÂàÜÊØçÊòØÂêÑËæìÂá∫‰πãÂíåÔºåÂÆûÁé∞ÂΩí‰∏ÄÂåñ„ÄÇ\nÂØπ‰∫é‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºàÊ†∑Êú¨Ê†áÁ≠æY=1,0Ôºâ‰∫§ÂèâÁÜµÔºö$-(1\\cdot log \\hat{Y} + 0\\cdot log(1-\\hat{Y}))$„ÄÇ\nÂØπ‰∫é‰∏âÂàÜÁ±ªÈóÆÈ¢òÔºàÊ†∑Êú¨Ê†áÁ≠æY=1,0,0ÔºâÔºå‰∫§ÂèâÁÜµÔºö$-(1 \\cdot log \\hat{Y}‚ÇÅ+ 0 + 0)$\n‰∏çÁÆ°ÊúâÂ§öÂ∞ëÁ±ªÔºåÂè™Êúâ1È°πÊòØÈùûÈõ∂ÁöÑ„ÄÇÈõ∂È°πÂØπËÆ≠ÁªÉÊ≤°ÊúâÊÑè‰πâÔºåÊâÄ‰ª•ÊçüÂ§±ÂáΩÊï∞Áõ¥Êé•ÂÜô‰∏∫Ôºö$Loss(\\hat{Y}, Y) = -Y log \\hat{Y}$\n‰æãÂ¶ÇÊúÄÂêé‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÁöÑËæìÂá∫‰∏∫Ôºö\n$$ [^{_{0.2}} _{^{0.1} _{-0.1}}] \\overset{\\rm Exponent}{\\longrightarrow} [^{_{1.22}} _{^{1.11} _{0.90}}] \\overset{\\rm Divide\\ sum}{\\longrightarrow} [^{_{0.38}} _{^{0.34} _{0.28}}] \\overset{-Y log \\hat{Y}}{\\longrightarrow} Loss $$\nÂØπÈ¢ÑÊµãÂÄºÂÖàÊ±ÇÂØπÊï∞ÔºåÂÜçÊï∞‰πò‰ª•Ê†∑Êú¨ label (-Y)ÔºåË¢´Áß∞‰∏∫Negative Log Likelihood Loss (NLLLoss)ÔºåÁî®numpyÂÆûÁé∞Ê≠§ËÆ°ÁÆóËøáÁ®ãÔºö\n1 2 3 4 5 6 import numpy as np y = np.array([1,0,0]) #Ê†∑Êú¨Ê†áÁ≠æ z = np.array([0.2, 0.1, -0.1]) #Á∫øÊÄßÂ±ÇÁöÑËæìÂá∫ y_pred = np.exp(z) / np.exp(z).sum() #È¢ÑÊµãÂÄºÂΩí‰∏ÄÂåñ loss = (- y* np.log(y_pred)).sum() #ÂèñÂØπÊï∞‰πò‰ª•-YÔºåÂ∞±ÊòØNLLLoss print(loss) Â¶ÇÊûúÊääsoftmaxÂáΩÊï∞‰πüÁÆóÂà∞ÊçüÂ§±ÂáΩÊï∞‰∏≠ÔºåÂú®pytorch‰∏≠Âè´ÂÅö‰∫§ÂèâÁÜµÊçüÂ§±ÔºöTorch.nn.CrossEntropyLoss()„ÄÇËøôÊ†∑ÁöÑËØùÔºåÁ•ûÁªèÁΩëÁªúÁöÑÊúÄÂêé‰∏Ä‰∏™Á∫øÊÄßÂ±Ç‰∏çË¶ÅÂÅöÊøÄÊ¥ªÔºåÁõ¥Êé•‰º†Áªô‰∫§ÂèâÁÜµÊçüÂ§±Ôºö\n1 2 3 4 5 6 import torch y = torch.LongTensor([0]) #ÈïøÊï¥Âûã (Á¨¨0‰∏™Á±ªÂà´) z = torch.Tensor([[0.2, 0.1, -0.1]]) #Á∫øÊÄßÂ±ÇËæìÂá∫ criterion = torch.nn.CrossEntropyLoss() #ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞ loss = criterion(z,y) #ËÆ°ÁÆóÊçüÂ§± print(loss) Mini-Batch: batch_size=3„ÄÇ\n1 2 3 4 5 6 7 8 9 10 11 12 import torch criterion = torch.nn.CrossEntropyLoss() Y = torch.LongTensor([2,0,1]) #‰∏âÊù°Ê†∑Êú¨ÔºåÂàÜÂà´Â±û‰∫éÁ¨¨2Á±ªÔºåÁ¨¨0Á±ªÔºåÁ¨¨1Á±ª,Áî®‰∫éÁ¥¢ÂºïÁúüÂÆûÁ±ªÂà´ÂØπÂ∫îÁöÑÈ¢ÑÊµãÂÄº Y_pred1 = torch.Tensor( [0.1, 0.2, 0.9], #(2)classified [1.1, 0.1, 0.2], #(0)classified [0.2, 2.1, 0.1]) #(1)classified Y_pred2 = torch.Tensor( [0.8, 0.2, 0.3], #(0)misclassified [0.2, 0.3, 0.5], #(2)misclassified [0.2, 0.2, 0.5]) #(2)misclassified loss1 = criterion(Y_pred1, Y) #ÊçüÂ§±ËæÉÂ∞è 0.4966 loss2 = criterion(Y_pred1, Y) # 1.2389 print(\u0026#34;Batch Loss1=\u0026#34;, loss1.data,\u0026#34;\\nBatch Loss2=\u0026#34;,loss2.data) MNIST Dataset ÂõæÂÉèÊòØ28√ó28ÁöÑÁü©Èòµ„ÄÇ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## ÂºïÂÖ•ÂåÖ import torch from torchvision import transforms #Â§ÑÁêÜÂõæÂÉè from torchvision import datasets from torch.utils.data import DataLoader import torch.nn.functional as F #ÊøÄÊ¥ª import torch.optim as optim ## ÂáÜÂ§áÊï∞ÊçÆ batch_size = 64 transform = transforms.Compose([ #Êää‰∏ÄÁ≥ªÂàóÂØπË±°ÁªÑÊàê‰∏Ä‰∏™pipeline transforms.ToTensor(), #ÊääÊï¥Êï∞ÂÉèÁ¥†ÂÄº0-255ËΩ¨Âèò‰∏∫ÂõæÂÉèÂº†ÈáèÔºöÂÄº0-1ÔºåÁª¥Â∫¶ÔºöCxWxH (1x28x28)ÔºåÊñπ‰æøÂç∑ÁßØ transforms.Normalize((0.1307,), (0.3081,)) ]) #ÂΩí‰∏ÄÂåñÔºåÂáèÂéªÂùáÂÄº,Èô§‰ª•Ê†áÂáÜÂ∑Æ, ‰ΩøÊâÄÊúâÁöÑÂÉèÁ¥†ÂÄºÊª°Ë∂≥0-1ÂàÜÂ∏É train_dataset = datasets.MNIST(root=\u0026#39;../dataset/mnist/\u0026#39;, train=True, download=True, transform=transform) #ËØªÂèñÊï∞ÊçÆÊó∂Â∞±ÂÅöËΩ¨Âèò train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size) test_dataset = dataset.MNIST(root=\u0026#39;../dataset/mnist/\u0026#39;, train=False, download=True, transform=transform) test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) #‰∏çÊâì‰π±ÔºåÊØèÊ¨°ÊµãËØïÈ°∫Â∫è‰∏ÄÊ†∑ÔºåÊñπ‰æøÂØπÊØîÁªìÊûú ## ËÆæËÆ°Ê®°Âûã class Net(torch.nn.Module): def __init__(self): self.l1 = torch.nn.Linear(784, 512) #Á∫øÊÄßÂ±ÇÊää784Áª¥ÂèòÊàê512Áª¥ self.l2 = torch.nn.Linear(512, 256) #Â∞ÜÂà∞256 self.l3 = torch.nn.Linear(256, 128) #Â∞ÜÂà∞128 self.l4 = torch.nn.Linear(128, 64) #Â∞ÜÂà∞64 self.l5 = torch.nn.Linear(64, 10) #Â∞ÜÂà∞10ÔºåËæìÂá∫(N,10)ÁöÑÁü©Èòµ def forward(self, x): #ÂêëÂâçËÆ°ÁÆóËæìÂá∫ x = x.view(-1, 784) #ÊîπÂèòÂº†ÈáèÁöÑÂΩ¢Áä∂ÔºåÊää‰∏ÄÂº†ÂõæÂÉèÂèòÊàê‰∏Ä‰∏™‰∫åÈò∂ÁöÑÂº†ÈáèÔºàÁü©ÈòµÔºâ784ÂàóÔºå-1Ë°®Á§∫Ëá™Âä®ËÆ°ÁÆóË°åÊï∞N x = F.relu(self.l1(x)) #ËæìÂÖ•l1ÔºåÂØπËæìÂá∫ÂÅöÊøÄÊ¥ª x = F.relu(self.l2(x)) x = F.relu(self.l3(x)) x = F.relu(self.l4(x)) return self.l5(x) #ÊúÄÂêé‰∏Ä‰∏™Á∫øÊÄßÂ±Ç‰∏çÊøÄÊ¥ª model = Net() ## ÊûÑÈÄ†ÊçüÂ§±Âíå‰ºòÂåñÂô® criterion = torch.nn.CrossEntropyLoss() #ÁªèËøásoftmaxÔºåÊ±ÇÂØπÊï∞Ôºå‰πò‰ª•-Y optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) #Ê®°ÂûãËæÉÂ§ßÔºåÁî®ÂÜ≤Èáè ## ËÆ≠ÁªÉÂíåÊµãËØï def train(epoch): #‰∏ÄËΩÆËÆ≠ÁªÉÁöÑËøêÁÆó running_loss = 0.0 for batch_idx, data in enumerate(train_loader, 0): #ÂèñÂá∫ËÆ≠ÁªÉÊ†∑Êú¨ inputs, target = data #ÂèñÂá∫Ê†∑Êú¨ÂíåÊ†áÁ≠æ optimizer.zero_grad() #Ê¢ØÂ∫¶Ê∏ÖÈõ∂ outputs = model(inputs) loss = criterion(outputs, target) #ÂâçÈ¶à:ËÆ°ÁÆóËæìÂá∫ÂíåÊçüÂ§± loss.backward() #ÂèçÈ¶à optimizer.step() #Êõ¥Êñ∞‰∏ÄÊ≠•ÊùÉÈáç running_loss += loss.item() #Á¥ØËÆ°ÊçüÂ§± if batch_idx %300 == 299 #ÊØè300ÊâπÔºàÂõ†‰∏∫‰ªé0ÂºÄÂßãÊï∞ÔºâËæìÂá∫‰∏ÄÊ¨°loss print(\u0026#39;[%d, %5d] loss: %.3f\u0026#39; % (epoch+1, batch_idx + 1, running_loss/300)) running_loss = 0.0 def test(): correct = 0 total = 0 with torch.no_grad(): #‰∏çÈúÄË¶ÅÂèçÂêë‰º†Êí≠ÔºåÂ∞±‰∏çÈúÄË¶ÅËÆ°ÁÆóÊ¢ØÂ∫¶ for data in test_loader: images, labels = data #ÂèñÂá∫ÊµãËØïÊ†∑Êú¨ÂèäÂÖ∂Ê†áÁ≠æ outputs = model(image) #ËÆ°ÁÆóÈ¢ÑÊµãÂÄº Nx10 ÁöÑÁü©Èòµ _, predicted = torch.max(outputs.data, dim=1) #ÊâæÂá∫ÊØè‰∏ÄË°å‰∏≠ÊúÄÂ§ßÂÄºÁöÑ‰∏ãÊ†á, Âç≥ÊâÄÂ±ûÁ±ªÂà´ÔºåÂíåÂÆÉÁöÑÂÄº„ÄÇdim=1Ë°®Á§∫Ê≤øÁùÄË°åÊñπÂêëÂØªÊâæÔºà0ÊòØÂàóÊñπÂêëÔºâ total += labels.size(0) #ÊµãËØïÊ†∑Êú¨ÊÄªÊï∞N correct += (predicted == labels).sum().item() #Ê≠£Á°ÆÂàÜÁ±ªÁöÑ‰∏™Êï∞ print(\u0026#34;Accuracy on test set: %d %%\u0026#34; % (100*correct /total)) if __name__ == \u0026#39;__main__\u0026#39;: for epoch in range(10): #ËÆ≠ÁªÉ10ËΩÆ train(epoch) if epoch %10 ==9: test() ","date":"2022-10-18T22:11:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/9_%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/","title":"watch: PyTorch - Âàò‰∫å 09 | Multi-classification Task"},{"content":"Source video: ‚ÄúÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç„ÄÅÁâõÈ°øÊ≥ï„ÄÅÂä®ÈáèÊ≥ï„ÄÅNesterov„ÄÅAdaGrad„ÄÅRMSprop„ÄÅAdam‚ÄùÔºåÊâìÂåÖÁêÜËß£ÂØπÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÁöÑ‰ºòÂåñ\nÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑ‰ºòÂåñÔºö\nÂáèÂ∞èÊØèÊ¨°Ê¢ØÂ∫¶‰∏ãÈôçÁöÑËÆ°ÁÆóÈáèÔºöÈöèÊú∫ÔºàÂàÜÊâπÊ¨°ÔºâÊ¢ØÂ∫¶‰∏ãÈôç ÂáèÂ∞ëËø≠‰ª£Ê¨°Êï∞ÔºåÂç≥‰ºòÂåñ‰∏ãÈôçË∑ØÂæÑ ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç ÂØπ‰∫é‰∏Ä‰∏™Âá∏ÈóÆÈ¢òÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏éËØØÂ∑ÆÈáèÁ∫ßÁöÑÂÖ≥Á≥ªÔºö\nÂØπ‰∫é‰∏Ä‰∏™Âº∫Âá∏ÈóÆÈ¢òÔºåËÉΩÊî∂ÊïõÂæóÊõ¥Âø´\nÊ≠£Â∏∏ÊÉÖÂÜµ‰∏ãÔºåÊ†áÂáÜÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºà1‰∏™batchÔºâÂ∫îËØ•ÊØîÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂø´Ôºå‰ΩÜÂèØ‰ª•ËØÅÊòéÔºå‰∏ç‰ºöÂø´Ëøá 1/k\n‰∏Ä‰∏™ batch ‰∏äÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÂèØËÉΩ‰∏éÊï¥‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÊçüÂ§±ÂáΩÊï∞‰∏çÂêåÔºåÂêÑÂ§ÑÂØπÂ∫îÁöÑÊ¢ØÂ∫¶‰πü‰∏çÂêåÔºå ÊâÄ‰ª•ÊØèÊ¨°Ëø≠‰ª£Êó∂ÁöÑÊ¢ØÂ∫¶ÊñπÂêë‰∏ç‰∏ÄÂÆöÊòØ‚ÄúÂÖ®‰ΩìÊï∞ÊçÆÁöÑËØØÂ∑ÆÂáΩÊï∞‚Äù‰∏äÁöÑÊúÄ‰ºòÔºåÊØè‰∏ÄÊ≠•ÁöÑË°åËøõÂèØËÉΩ‰ºöÂÅèÁ¶ª‰∏ãÈôçÊúÄÂø´ÁöÑÊúÄ‰ºòË∑ØÂæÑÔºå‰ªéËÄåÂØºËá¥ÈúÄË¶ÅÊõ¥Â§öÊ¨°ÁöÑËø≠‰ª£ÔºåÊâçËÉΩÂà∞ËææÊûÅÂÄºÁÇπ„ÄÇ\nÂ¶Ç‰∏ãÂõæÔºå‰ªé A ÁÇπ Âà∞ A\u0026rsquo; ÁÇπÁöÑÊúÄ‰ºòË∑ØÂæÑÊòØÊ©ôËâ≤Á∫øÔºåÂ¶ÇÊûúÂàÜ‰∏§Ê≠•ÔºåÂÖàËµ∞Âà∞ B ÔºåB ÂÜçÊ≤øÁùÄÂÆÉÁöÑÊ¢ØÂ∫¶ÊñπÂêëËµ∞ÔºåÂ∞±Ëµ∞ÂÅè‰∫Ü„ÄÇ\nÂáèÂ∞èÊ≠•ÈïøÔºåÂèØ‰ª•ËÆ©‰∏ãÈôçË∑ØÂæÑÊõ¥Ë¥¥ËøëÊúÄ‰ºò‰∏ãÈôçË∑ØÂæÑÔºå‰ΩÜÊòØËÆ°ÁÆóÈáèÂ§™Â§ß„ÄÇ\nÁâõÈ°øÊ≥ï ÁâõÈ°øÊ≥ïÊòØÁî®Êù•ÊãüÂêàÊõ≤Á∫øÁöÑÔºåÂú®Ê¢ØÂ∫¶‰∏ãÈôç‰∏≠ÔºåÂ∞±ÊòØÊãüÂêàÊçüÂ§±ÂáΩÊï∞Ë°®Èù¢‰∏äÁöÑÊúÄ‰ºò‰∏ãÈôçË∑ØÂæÑÂØπÂ∫îÁöÑÊõ≤Á∫ø„ÄÇ\nÂØπ‰∫é‰∏Ä‰∏™Âè™Êúâ‰∏ÄÁª¥ÂèòÈáèÁöÑÈóÆÈ¢òÔºåÁ∫µËΩ¥ÊòØÂêÑÂèòÈáèÂèñÂÄºÂØπÂ∫îÁöÑËØØÂ∑ÆÔºåËìùËâ≤Êõ≤Á∫øÂç≥ÊòØÊçüÂ§±ÂáΩÊï∞„ÄÇ Ë¶ÅÂà∞ËææÊçüÂ§±ÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄºÂ§ÑÔºåÊ†πÊçÆÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºåÂÖàÊ±ÇÂá∫ÊçüÂ§±ÂáΩÊï∞Âú®ÂΩìÂâçÁÇπÁöÑÊ¢ØÂ∫¶ÔºàÂêÑ‰∏™ÊñπÂêëÂàÜÈáèÔºåÊåâÂêëÈáèÂä†Ê≥ïÁõ∏Âä†ÔºâÔºåËøôÈáåÂè™Êúâ‰∏Ä‰∏™ÂèòÈáèÔºà‰∏Ä‰∏™ÊñπÂêëÔºâÔºåÂ∞±ÊòØÊ±ÇÊçüÂ§±ÂáΩÊï∞ÁöÑÂàáÁ∫ø„ÄÇ ÁÑ∂ÂêéÂèòÈáèÊ≤øÁùÄÊ¢ØÂ∫¶ÔºàÂàáÁ∫øÔºâÊñπÂêëÁßªÂä®‰∏ÄÁÇπÔºåÁúãÁúãÊ≠§Êó∂ÁöÑËØØÂ∑ÆÂÄº„ÄÇ\nÊäõÁâ©Á∫øÊØîÁõ¥Á∫øÊõ¥Ë¥¥ËøëÊçüÂ§±ÂáΩÊï∞Ôºå‰ªéËÄå‰Ωø‰∏ãÈôçË∑ØÂæÑ‰∏éÊçüÂ§±ÂáΩÊï∞Êõ¥Ë¥¥ÂêàÔºåËÄå‰∏çÊòØÊäòÁ∫ø„ÄÇ Âõ†‰∏∫Êï¥‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÊçüÂ§±ÂáΩÊï∞Êú™Áü•ÔºåÊØè‰∏ãÈôç‰∏ÄÊ≠•ÔºåÂ∞±Âú®ÂΩìÂâçÁÇπÁöÑÈÇªÂüüËåÉÂõ¥ÂÜÖÂÅöÊ≥∞ÂãíÂ±ïÂºÄÔºåÁî®‰∏ÄÊÆµÈ´òÊ¨°ÂáΩÊï∞ÂØπÊçüÂ§±ÂáΩÊï∞ÂÅöËøë‰ºº‰ª£ÊõøÔºõ ÂèàÂõ†‰∏∫ÊòØÊâæ‰∏ãÈôçÁöÑÊñπÂêëÔºåÊâÄ‰ª•Ë¶Å‰øùÁïôÂà∞‰∫åÊ¨°È°πÔºåËøôÊ†∑Â∞±ËÉΩ ÊãüÂêàÂá∫Âú®ÊçüÂ§±ÂáΩÊï∞ÔºàË°®Èù¢Ôºâ‰∏äÁöÑ‰∏ãÈôçË∑ØÂæÑ„ÄÇ ÁâõÈ°øÊ≥ïÁöÑÊØè‰∏ÄÊ≠•ÊòØÁ°ÆÂÆöÁöÑÔºöÊäõÁâ©Á∫øÁöÑÈ°∂ÁÇπÂØπÂ∫îÁöÑÊ®™ÂùêÊ†áÂ∞±ÊòØËøô‰∏ÄÊ≠•Ë¶ÅËµ∞Âà∞ÁöÑ‰ΩçÁΩÆÔºåÊâÄ‰ª•ÁâõÈ°øÊ≥ïÈáåÊ≤°ÊúâÊ≠•Èïø„ÄÇ ‰∏ãÈôçÁöÑÊñπÂêëÂ∞±‰∏çÊòØÊ¢ØÂ∫¶ÁöÑÊñπÂêë‰∫Ü?\nÂ¶Ç‰∏äÂõæÔºåÁÅ∞Ëâ≤ÁöÑÁõ¥Á∫øÊòØÂà∞ÊûÅÂÄºÁÇπÁöÑÊúÄ‰ºòË∑ØÂæÑÔºå‰ΩÜÊòØÊú™Áü•„ÄÇ ÁâõÈ°øÊ≥ïÂ∏åÊúõÊØè‰∏ÄÊ≠•ÈÉΩËµ∞Âú®ÊçüÂ§±ÂáΩÊï∞‰∏äÔºåÂç≥ÊãüÂêàÂá∫ÊçüÂ§±ÂáΩÊï∞ÔºàË°®Èù¢Ôºâ‰∏äÁöÑÊúÄ‰ºò‰∏ãÈôçË∑ØÂæÑ„ÄÇ\nÊ≥∞ÂãíÂ±ïÂºÄ‰øùÁïô‰∫åÈò∂ÂØºÔºåÁî®‰∫åÊ¨°ÂáΩÊï∞Ëøë‰ººË°®ËææÊçüÂ§±ÂáΩÊï∞‰∏äÁöÑÊØè‰∏ÄÁÇπÔºö\nf(x) = J(a‚ÇÄ) + J\u0026rsquo;(a‚ÇÄ)(x-a‚ÇÄ) + 1/2 J\u0026quot;(a‚ÇÄ)(x-a‚ÇÄ)¬≤\nÊ±Ç f(x) ÁöÑÊûÅÂÄºÔºåÂ∞±ÊòØÊ±ÇÈ°∂ÁÇπÊâÄÂú®‰ΩçÁΩÆÔºå‰ª§ f\u0026rsquo;(x) = 0:\n$$ f\u0026rsquo;(x) = 0 + J\u0026rsquo;(a‚ÇÄ) + J\u0026quot;(a‚ÇÄ)(x-a‚ÇÄ) = 0 \\\\ x = a‚ÇÄ - J\u0026rsquo;(a‚ÇÄ) / J\u0026quot;(a‚ÇÄ) $$\nÁÑ∂ÂêéËÆ©ÂèòÈáèËµ∞Âà∞È°∂ÁÇπÁöÑ‰ΩçÁΩÆÔºåÂØπÂ∫î‰∫éÊùÉÈáçÊõ¥Êñ∞ÔºöW = W - J\u0026rsquo;(a‚ÇÄ) / J\u0026quot;(a‚ÇÄ)ÔºåÊ≤°ÊúâÂ≠¶‰π†ÁéáŒ∑„ÄÇ Áî®‰∏ÄÈò∂ÂØºÊï∞‰∏é‰∫åÈò∂ÂØºÊï∞ÁöÑÊØîÂÄºÂØπÔºà‰∏ÄÂÖÉÔºâÂèòÈáèWÂÅöÊõ¥Êñ∞„ÄÇpath-int\nÁõ¥ËßÇÁêÜËß£ÔºöÊåâÂõ∫ÂÆöÊ≠•ÈïøÂÅöÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊòØÊ©ôËâ≤Á∫øÔºåÊ≠•ÈïøÊó†Á©∑Â∞èÊó∂Ôºå‰∏ãÈôçË∑ØÂæÑÊòØÁÅ∞Ëâ≤Á∫øÔºåÁâõÈ°øÊ≥ïÁöÑ‰∏ãÈôçË∑ØÂæÑÊòØÁªøËâ≤Á∫øÔºåÊØîÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊõ¥Ë¥¥ËøëÊúÄ‰ºòË∑ØÂæÑÁöÑÁÅ∞Ëâ≤Á∫ø„ÄÇ ÊâÄ‰ª•ÁâõÈ°øÊ≥ïÊòØÂú®ÊãüÂêàÊúÄ‰ºòË∑ØÂæÑÂØπÂ∫îÁöÑÊõ≤Á∫ø„ÄÇ\nÂØπÈ´òÁª¥ÂáΩÊï∞Ê±Ç‰∫åÈò∂ÂÅèÂØºÔºåÈúÄË¶ÅÁÆó Hessian Áü©Èòµ„ÄÇÂØπ‰∫éÈ´òÁª¥ÁöÑÊçüÂ§±ÂáΩÊï∞ J(W)ÔºåÂèÇÊï∞Êõ¥Êñ∞ÂÖ¨Âºè‰∏∫Ôºö ùêñ = ùêñ - ùõÅùêâ¬≤(ùêñ)‚Åª¬π ‚ãÖ ùõÅùêâ(ùêñ)ÔºåÂÖ∂‰∏≠ ùõÅùêâ¬≤(ùêñ) Â∞±ÊòØ Hessian ÊñπÈòµ ùêá(ùêñ)\n$$ ùêá(ùêñ) = \\begin{bmatrix} \\frac{‚àÇ}{‚àÇW‚ÇÅ}\\frac{‚àÇf}{‚àÇW‚ÇÅ} \u0026amp; \\frac{‚àÇ}{‚àÇW‚ÇÇ}\\frac{‚àÇf}{‚àÇW‚ÇÅ} \u0026amp; \u0026hellip; \u0026amp; \\frac{‚àÇ}{‚àÇW‚Çô}\\frac{‚àÇf}{‚àÇW‚ÇÅ}\\\\ \\frac{‚àÇ}{‚àÇW‚ÇÅ}\\frac{‚àÇf}{‚àÇW‚ÇÇ} \u0026amp; \\frac{‚àÇ}{‚àÇW‚ÇÇ}\\frac{‚àÇf}{‚àÇW‚ÇÇ} \u0026amp; \u0026hellip; \u0026amp; \\frac{‚àÇ}{‚àÇW‚Çô}\\frac{‚àÇf}{‚àÇW‚ÇÇ}\\\\ \u0026hellip; \\\\ \\frac{‚àÇ}{‚àÇW‚ÇÅ}\\frac{‚àÇf}{‚àÇW‚Çô} \u0026amp; \\frac{‚àÇ}{‚àÇW‚ÇÇ}\\frac{‚àÇf}{‚àÇW‚Çô} \u0026amp; \u0026hellip; \u0026amp; \\frac{‚àÇ}{‚àÇW‚Çô}\\frac{‚àÇf}{‚àÇW‚Çô}\\\\ \\end{bmatrix} $$\nÂàóÂêëÈáèÊõ¥Êñ∞ÂÖ¨ÂºèÔºöùêñ‚Çô‚Çì‚ÇÅ = ùêñ‚Çô‚Çì‚ÇÅ - ùêá(ùêñ)‚Åª¬π‚Çô‚Çì‚Çô ‚ãÖ ùõÅùêâ(ùêñ)‚Çô‚Çì‚ÇÅ\nËôΩÁÑ∂Ëø≠‰ª£Ê¨°Êï∞ÊØîÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂ∞ëÔºå‰ΩÜÂπ∂‰∏çÂÆûÁî®ÔºåÁº∫ÁÇπ:path-int\nËÆ°ÁÆóÈáèÂ§™Â§ßÔºåÊØè‰∏ÄÊ≠•ÈÉΩË¶ÅËÆ°ÁÆónÁª¥ÂêëÈáèÁöÑ‰∏ÄÈò∂Â∑ÆÂàÜ O(n)ÔºåHessianÁü©Èòµ O(n¬≤)ÔºåÂèäÂÖ∂ÈÄÜ ‰∏çËÉΩ‰øùËØÅÁõÆÊ†áÂáΩÊï∞ÂÄºÂú®Ëø≠‰ª£ËøáÁ®ã‰∏≠‰∏ÄÁõ¥‰∏ãÈôçÔºåÂèØËÉΩ‰ºöÂÖàÂçáÈ´òÔºåÂÜç‰∏ãÈôçÔºõ ‰∏çËÉΩ‰øùËØÅÊî∂ÊïõÔºöÂõ†‰∏∫ÁâõÈ°øÊ≥ï‰ΩøÁî®‰∫åÈò∂Ê≥∞ÂãíÂ±ïÂºÄËøë‰ººÔºåÈúÄË¶ÅÂàùÂßãÁÇπÂú®ÊûÅÂ∞èÁÇπÈôÑËøëÔºåÊØè‰∏ÄÊ≠•ÈÉΩÊª°Ë∂≥Ëøë‰ººÊù°‰ª∂ÔºàHessianÁü©ÈòµÊòØÊ≠£ÂÆöÁöÑÔºâÔºåÊïàÊûúÊâç‰ºöÊØîËæÉÂ•Ω„ÄÇ Â¶ÇÊûúÁ¶ªÂæóÂæàËøúÔºåËé∑ÂæóÁöÑÁªìÊûúÂèØËÉΩÈùûÂ∏∏Â•áÊÄ™„ÄÇ‰∏ÄËà¨Â∫îÁî®ÂÖ∂‰ªñÊñπÊ≥ïÂÖàÊêúÁ¥¢Âà∞ÊûÅÂ∞èÁÇπÈôÑËøëÔºåÂÜçÁî®ÁâõÈ°øÊ≥ïÔºàÊàñÊãüÁâõÈ°øÊ≥ïÔºâÊù•ÁªßÁª≠Êõ¥È´òÁ≤æÂ∫¶ÁöÑÊêúÁ¥¢„ÄÇ Â¶ÇÊûúÁõÆÊ†áÂáΩÊï∞ f(x) Âè™ÊòØ‰∏ÄÈò∂ÂèØÂæÆÔºå‰∫åÈò∂‰∏çÂèØÂæÆÔºàHessianÁü©Èòµ‰∏çÂ≠òÂú®ÔºâÔºåÁâõÈ°øÊ≥ïÂ∞±‰∏çÈÄÇÁî®‰∫Ü„ÄÇ Â¶ÇÊûú‰∫åÈò∂ÂèØÂæÆÔºåÁêÜËÆ∫‰∏äÁâõÈ°øÊ≥ïÊî∂ÊïõÈÄüÂ∫¶ÊØîÊ¢ØÂ∫¶Ê≥ïË¶ÅÂø´„ÄÇÁâõÈ°øÊ≥ïÊî∂ÊïõÈò∂Êï∞Ëá≥Â∞ëÊòØ2ÔºåÊ¢ØÂ∫¶Ê≥ïÊî∂ÊïõÈò∂Êï∞ÊúÄÂ∑ÆÊÉÖÂÜµ‰∏ãÊòØ1„ÄÇ Â¶ÇÊûúÊòØ‰∏Ä‰∏™Â§öÂÖÉÂá∏ÂáΩÊï∞Ôºå‰ΩÜÊòØ‰∏çÊòØÂ§ÑÂ§ÑÂèØÂØºÔºåTaylorËøë‰ººÂ±ïÂºÄ‰∏çËÉΩÈÄÇÂ∫îÔºåÁâõÈ°øÊ≥ï‰∏çÂèØÂ∫îÁî®„ÄÇ Ëã• H ‰∏çÂèØÈÄÜÔºåÈúÄË¶ÅÁî® Levenberg-Marquadt ‰øÆÊ≠£ÔºöÂä†Â∏∏ÈáèÈòµ ŒªùêàÔºåÂç≥ÁªôÂØπËßíÁ∫øÂä†‰∏äË∂≥Â§üÂ§ßÁöÑÂÄºÔºå‰ΩøÊâÄÊúâÁöÑ eigenvalue ÈÉΩÂ§ß‰∫é0ÔºåÊÑèÂë≥ÁùÄÂú®‰ªª‰ΩïÊñπÂêë‰∏äÁöÑ‰∏ÄÈò∂ÂØºÊï∞ÈÉΩÂ§ß‰∫é0. path-int Âä®ÈáèÊ≥ï ÁâõÈ°øÊ≥ïÂêåÊó∂ËÄÉËôëÊçüÂ§±ÂáΩÊï∞ÁöÑÊâÄÊúâÁª¥Â∫¶ÔºåÊâæÂá∫ÊúÄ‰ºò‰∏ãÈôçË∑ØÂæÑ„ÄÇ\nÊ¢ØÂ∫¶ÊòØ‰∏Ä‰∏™Â§öÁª¥ÁöÑÂêëÈáèÔºåÂèØ‰ª•ÊääÂêÑ‰∏™Áª¥Â∫¶ÊãÜÂºÄÔºàÂêëÈáèÁöÑÂàÜËß£ÔºâÔºåÂçïÁã¨ÂàÜÊûêÊØè‰∏™ÊñπÂêë‰∏äÁöÑÂèòÂåñ\nÂä†ÊùÉÊòØË¶ÅÊääÁª¥Â∫¶ÂàÜÂºÄËÄÉËôë\nËÄÉËôë‰∏ãÈôç‰∏§Ê≠•ÔºåÊäµÊ∂àÁõ∏ÂèçÊñπÂêëÁöÑÊ¢ØÂ∫¶ ÊÉØÊÄß mvÔºåÂäõ‰ΩúÁî®ÁöÑÊïàÊûúÔºüÊääÈÄüÂ∫¶ÊäµÊ∂àÔºåÂáèÂ∞ëÈúáËç°\nÂâçÂá†Ê≠•Ê¢ØÂ∫¶Â§ßÔºåÁõ¥Êé•Âä†ÁöÑËØùÔºåÂç†‰∏ªÂØºÔºåËÄå‰∏î‰∏çÂáÜÔºåÁî®‰∏Ä‰∏™Á≥ªÊï∞ÊéßÂà∂ÂâçÂêé‰∏§‰∏™ÈÉ®ÂàÜÁöÑÊùÉÈáçÂ∞±Áî® beta Âíå 1-beta Ôºàexponentially weighted moving averageÔºâÔºåË∂äÊòØÂÖàÂâçÂèëÁîüÁöÑÁä∂ÊÄÅÔºå‰πò‰ª•ÁöÑÔºà1-betaÔºâÊ¨°Êï∞Ë∂äÂ§öÔºåÂç†ÊØîË∂äÊù•Ë∂äÂ∞èÔºåÂØπÂΩìÂâçÁöÑÂÄºÂΩ±ÂìçË∂äÂ∞è\nNesterov Á≠âÈ´òÁ∫øÂùêÊ†áÁ≥ª‰∏ãÁöÑÁÇπÊòØÊùÉÈáçWÔºåNesterov Êää‰∏ä‰∏ÄÊó∂ÂàªÁöÑÊùÉÈáçÊåâÁÖß‰∏ä‰∏ÄÊó∂ÂàªÁöÑÊ¢ØÂ∫¶ÊñπÂêëËµ∞‰∫Ü‰∏ÄÊ≠•ÔºåÊ≤°Êúâ‰∏éÂΩìÂâçÊó∂ÂàªÁöÑÊ¢ØÂ∫¶Âä†ÊùÉÔºåÊääÊùÉÈáçÁÇπÁßªÂä®Âà∞ÁöÑÊñ∞‰ΩçÁΩÆÔºåËøô‰∏™‰ΩçÁΩÆÊòØÊØîÂ¶ÇÊûúÈááÁî®Âä†ÊùÉÊó∂ÁöÑÊùÉÈáçË∂ÖË∞É‰∏Ä‰∫õÁöÑÔºåÊääÊñ∞‰ΩçÁΩÆÁöÑÊ¢ØÂ∫¶Â∏¶ÂõûÂéªÔºå‰πüÂ∞±ÊòØËÄÉËôë‰∫Ü‰∏ã‰∏ÄÊ≠•‰ΩçÁΩÆÁöÑÊÉÖÂÜµ\nÂΩìÂâçÊ¢ØÂ∫¶ÊñπÂêëd1ÔºåÂéÜÂè≤Ê¢ØÂ∫¶ÊñπÂêëd2ÔºåÊú™Êù•Ê¢ØÂ∫¶ÊñπÂêëd3ÔºåÂÖ±ÂêåÂÜ≥ÂÆöÂêëÂì™‰∏™ÊñπÂêëËµ∞„ÄÇÊú™Êù•Â∞±ÊòØÂÖàÊåâd2Ëµ∞‰∏ÄÊ≠•Ê¢ØÂ∫¶‰∏ãÈôçÔºåËµ∞Âà∞‰∏Ä‰∏™ÁÇπÔºåËÆ°ÁÆóÈÇ£‰∏™ÁÇπÁöÑÊ¢ØÂ∫¶„ÄÇ‰∏çÊòØÊåâd1Âíåd2ÁöÑÂä†ÊùÉÊñπÂêëËµ∞ÔºåÈÇ£Ê†∑Â∞±ÊòØÊèêÂâçÂÅö‰∫ÜÂÜ≥Á≠ñÔºåËøòÊääÂÜ≥Á≠ñÁöÑÁªìÊûúÊãøËøáÊù•Áî®‰∫ÜÔºåÂ∫îËØ•Áî®ÂéÜÂè≤Êï∞ÊçÆÊù•ÂÜ≥Á≠ñ\nAdaGrad ÂΩìÂâçÊ¢ØÂ∫¶Èô§‰ª•Á¥ØËÆ°ÂéÜÂè≤Ê¢ØÂ∫¶ÂÜÖÁßØÂÜçÂºÄÊ†πÂè∑ÔºåÊ¢ØÂ∫¶ÁöÑÂêÑÊñπÂêëÂàÜÈáèÊï∞ÈáèÁ∫ß‰∏ç‰∏ÄËá¥ÔºåÊâÄ‰ª•Èô§‰ª•Á¥ØËÆ°ÂéÜÂè≤Ê¢ØÂ∫¶Ê®°ÈïøÔºåÊ¢ØÂ∫¶ÂÄºÂ§ßÁöÑÈÇ£‰∏™ÊñπÂêëÔºåÈô§‰ª•‰∏Ä‰∏™Êï∞ÔºàÂèØËÉΩ‰∏éËØ•ÊñπÂêëÊï∞ÂÄºÂú®Âêå‰∏ÄÊï∞ÈáèÁ∫ßÔºâÂ∞±Ê≤°ÈÇ£‰πàÂ§ß‰∫ÜÔºå‰∏çËá≥‰∫éÂú®ÈÇ£‰∏™ÊñπÂêë‰∏äËµ∞Â§™Â§öÔºåËÄå‰∏îÂàÜÊØçË∂äÂä†Ë∂äÂ§ßÔºåÊ¢ØÂ∫¶Ë∂äÂæÄÂêéË∂äÂ∞è„ÄÇ Á®ÄÁñèÊï∞ÊçÆÈõÜÔºöÂè™Ë¶ÅÂÖ≥ÂøÉÁª¥Â∫¶/ÁâπÂæÅÁöÑÊúâÊó†Â∞±ËÉΩÊääÁ±ªÂà´ÂàÜÂºÄÔºåËÄå‰∏çÈúÄÂÖ≥ÂøÉÁâπÂæÅÊòØÂê¶ÊòéÊòæÔºàÁå¥Â≠êÂíå‰∫∫ÔºâÔºõÈùûÁ®ÄÁñèÊï∞ÊçÆÔºöÈúÄË¶ÅÂÖ≥Ê≥®Âêå‰∏ÄÁâπÂæÅ‰∏äÁöÑÂ∑ÆÂà´ÔºåÂú®Âêå‰∏ÄÁâπÂæÅ‰∏äÁöÑÊï∞ÂÄºÔºåÊâçËÉΩÂàÜÁ±ªÔºàÈïøÂ∞æÁå´ÂíåÁü≠Â∞æÁå´Ôºâ Á®ÄÁñèÊï∞ÊçÆ‰∏çÂêåÊ†∑Êú¨Ê≤°ÊúâÂÖ±ÂêåÁöÑÁâπÂæÅÔºå‰∏çÂêåÁâπÂæÅÁöÑÁªùÂØπÂÄºÂ∑ÆÂºÇÂ§ßÔºåÊï∞ÂÄºÁõ∏ÂáèÂêéÂ∑ÆÂÄºÂ§ßÔºåÊ¢ØÂ∫¶Â§ß\nÁª¥Â∫¶Ë∂äÂ§öÔºåË∂ä‰∏çÈúÄË¶ÅÂå∫ÂàÜÂú®Âêå‰∏ÄÁâπÂæÅ‰∏äÁöÑÊï∞ÂÄºÂ∑ÆÂºÇÔºåÂè™ÈúÄÂÖ≥ÂøÉÁâπÂæÅÁöÑÊúâÊó†ÔºåÂçï‰ΩçÁêÉÂÜÖÁöÑÁÇπÂàÜÊï£Âà∞ÂêÑ‰∏™Áª¥Â∫¶‰∏ä‰∫ÜÔºå‰∏çÂÜçÊå§Âú®Âêå‰∏ÄÁª¥Â∫¶‰∏ä‰∫ÜÔºåÂêÑ‰∏™ËΩ¥‰∏äÁöÑÈïøÂ∫¶Â∞±‰∏çÁî®ÈÇ£‰πàÈïø‰∫ÜÔºåÊâÄ‰ª•‰ΩìÁßØÂ∞±ÂèòÂ∞è‰∫Ü\nÁéãÊú®Â§¥Ëß£ÈáäÊàêÂ≠¶‰π†ÁéáÁöÑË°∞Âáè‰∏çÂ§™ÂêàÈÄÇ\nÊ¢ØÂ∫¶ÊåâÊ≥∞ÂãíÂÖ¨ÂºèÂ±ïÂºÄÔºåÁâõÈ°øÊ≥ïÁî®‰∫Ü‰∫åÊ¨°È°πÔºåÂä®ÈáèÊ≥ï‰øÆÊ≠£0Ê¨°È°πÂü∫Á°ÄÂÄºÔºåAdaGrad‰øÆÊ≠£1Ê¨°È°πÔºàÊ¢ØÂ∫¶ÂèòÂåñÈáèÔºâ\n","date":"2022-10-02T12:07:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/08_%E6%94%B9%E8%BF%9B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/","title":"watch: DL - ÁéãÊú®Â§¥ 08 | Advanced Gradient Descent"},{"content":"10‚ÄúÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ÈóÆÈ¢ò‚ÄùÂ¶Ç‰ΩïÁõ¥ËßÇÁêÜËß£Ôºü‚ÄúKKTÊù°‰ª∂‚Äù ‚ÄúSlaterÊù°‰ª∂‚Äù ‚ÄúÂá∏‰ºòÂåñ‚ÄùÊâìÂåÖÁêÜËß£\n10.1 ÊãâÊ†ºÊúóÊó•‰πòÊï∞Ê≥ï ‰∏ÄÁßçÂØªÊâæÂ§öÂÖÉÂáΩÊï∞Âú®ÂÖ∂ÂèòÈáèÂèóÂà∞‰∏Ä‰∏™ÊàñÂ§ö‰∏™Á∫¶ÊùüÊù°‰ª∂Êó∂ÁöÑÊûÅÂÄºÁöÑÊñπÊ≥ïÔºàËá™ÂèòÈáèÁöÑÂèñÂÄºËåÉÂõ¥ÊúâÈôêÂà∂Ôºâ\nÂ∞Ü‰∏Ä‰∏™Êúân‰∏™ÂèòÈáè‰∏ék‰∏™Á∫¶ÊùüÊù°‰ª∂ÁöÑÊúÄ‰ºòÂåñÈóÆÈ¢òËΩ¨Êç¢‰∏∫‰∏Ä‰∏™Ëß£Êúâ n+k ‰∏™ÂèòÈáèÁöÑÊñπÁ®ãÁªÑÁöÑËß£ÁöÑÈóÆÈ¢ò„ÄÇ\nÂØπÊØè‰∏™Á∫¶ÊùüÊù°‰ª∂Áî®ÊãâÊ†ºÊúóÊó•‰πòÂ≠ê(ÂæÖÂÆöÁ≥ªÊï∞) Œª·µ¢ Âä†ÊùÉÔºåÂä†Âà∞ÁõÆÊ†áÂáΩÊï∞ÂêéÈù¢ÔºåÂ∞±ÊòØÊãâÊ†ºÊúóÊó•ÂáΩÊï∞„ÄÇ ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞‰∏éÁõÆÊ†áÂáΩÊï∞ÁöÑÊúÄÂÄºÊòØ‰∏ÄÊ†∑ÁöÑÔºåÊâÄ‰ª•Ê±ÇÁõÆÊ†áÂáΩÊï∞ÁöÑÊúÄÂÄºÂ∞±ËΩ¨Âåñ‰∏∫Ê±ÇÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÁöÑÊúÄÂÄº„ÄÇ\n$$ \\begin{aligned} ÁõÆÊ†áÂáΩÊï∞Ê±ÇÊúÄÂ∞èÂÄºÔºö\u0026amp; min\\ f_0(ùê±),\\quad ùê± ‚àà ‚Ñù‚Åø \\\\ m‰∏™Á∫¶ÊùüÊù°‰ª∂Ôºö\u0026amp; s.t. \\quad f_i(ùê±) ‚â§ 0,\\ ÂÖ∂‰∏≠ i=1,2,3\u0026hellip;m \\\\ ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Ôºö\u0026amp; L(ùê±,\\pmb{Œª}) = f_0(ùê±) + \\sum Œª_i f_i(ùê±) \\end{aligned} $$\nÁõÆÊ†áÂáΩÊï∞Ë¢´Âä†‰∏ä‰∫ÜÁ∫¶ÊùüÊù°‰ª∂ÔºåÂèòÈáèÂè™ËÉΩÂú®ËßÑÂÆöÁöÑËåÉÂõ¥ÂÜÖÂèñÂÄºÔºåÊ±ÇÂØºÂæóÂà∞ÁöÑÊûÅÂÄºÁÇπÂèØËÉΩ‰∏çÂú®ËßÑÂÆöËåÉÂõ¥ÂÜÖÔºåËÄåÂú®ËßÑÂÆöËåÉÂõ¥ÂÜÖÂèØËÉΩ‰πüÊ≤°ÊúâÊûÅÂÄºÁÇπ„ÄÇÊãâÊ†ºÊúóÊó•‰πòÊï∞Ê≥ïÊääÂ∏¶Á∫¶ÊùüÈóÆÈ¢òËΩ¨Âåñ‰∏∫Êó†Á∫¶ÊùüÈóÆÈ¢ò„ÄÇ\nÊØè‰∏™Á∫¶ÊùüÊù°‰ª∂Áî®ÊãâÊ†ºÊúóÊó•‰πòÂ≠êÂä†ÊùÉ\n10.2 Áî®Ê¢ØÂ∫¶ÁêÜËß£Lagrange Multiplier ‰∏Ä‰∏™Á∫¶ÊùüÊù°‰ª∂ÁöÑÊÉÖÂÜµÔºö\nÊ±Ç $f(x,y)$ ÁöÑÊúÄÂ∞èÂÄºÔºåÂπ∂‰∏îÊúâ‰∏Ä‰∏™Á∫¶ÊùüÊù°‰ª∂ $y=g(x)$„ÄÇ\nËØ•ÈóÆÈ¢òÁöÑÊãâÊ†ºÊúóÊó•ÂáΩÊï∞‰∏∫Ôºö$L(x,y) = f(x,y) + Œª(y-g(x))$Ôºõ[Á∫¶ÊùüÊù°‰ª∂=0 Ë°®Á§∫‰∏ÄÊù°Á∫ø]\nÂØπÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Ê±ÇÊ¢ØÂ∫¶ÔºåÊ¢ØÂ∫¶Á≠â‰∫é0ÁöÑÁÇπÂ∞±ÊòØÊûÅÂÄºÂØπÂ∫îÁöÑÁÇπÔºõ\nÊää $‚àá L(x,y) = 0$ÔºåÊ≤øx,y‰∏§‰∏™ÊñπÂêëÂ±ïÂºÄÔºåË∞ÉÊï¥Œª‰Ωø‰∏§‰∏™ÊñπÂêë‰∏äÁöÑÂÅèÂØºÈÉΩ‰∏∫0Ôºå‰πüÂ∞±ÊòØÊ¢ØÂ∫¶Âú®‰∏§‰∏™ÊñπÂêë‰∏äÁöÑÂàÜÈáèÈÉΩ‰∏∫0Ôºö\n$$ \\begin{cases} \\frac{‚àÇf(x,y)}{‚àÇx} + Œª\\frac{‚àÇ(y-g(x))}{‚àÇx} = 0 \\\\ \\frac{‚àÇf(x,y)}{‚àÇy} + Œª\\frac{‚àÇ(y-g(x))}{‚àÇy} = 0 \\end{cases} $$\n1 2 Âú®Âõæ1‰∏≠ÔºåÂùêÊ†áÁ≥ªÊòØx-yÔºåÂêåÂøÉÂúÜÊòØÁõÆÊ†áÂáΩÊï∞ $f(x,y)$ ÁöÑÁ≠âÈ´òÁ∫øÔºåÂúÜÂøÉÁÇπÂØπÂ∫îÁöÑÂáΩÊï∞ÂÄºÊúÄÂ∞èÔºåË∂äÂæÄÂ§ñÂÄºË∂äÂ§ß„ÄÇÁ∫¢Á∫øÊòØ(x,y)Á∫¶ÊùüÊù°‰ª∂„ÄÇ\nÂõæ2ÊòæÁ§∫‰∫Ü‰∏§‰∏™ÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶ÔºåÂè™ÊúâÂú®Áõ∏ÂàáÁöÑ‰ΩçÁΩÆÔºåÁõÆÊ†áÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶ÊñπÂêë‰∏éÁ∫¶ÊùüÊù°‰ª∂ÁöÑÊ¢ØÂ∫¶ÊñπÂêëÊâçÊòØÂÖ±Á∫øÁöÑÔºåÂÜçÈÄöËøáÊãâÊ†ºÊúóÊó•‰πòÂ≠ê Œª Ë∞ÉÊï¥ÂêëÈáèÈïøÁü≠Ôºå‰Ωø‰∏§‰∏™Ê¢ØÂ∫¶Áõ∏Âä†ÊâçÂèØËÉΩ‰∏∫Èõ∂„ÄÇÈô§‰∫ÜÁõ∏ÂàáÁÇπ‰ΩçÁΩÆÔºåÈÉΩÊó†Ê≥ïÂÆûÁé∞‰∏§Ê¢ØÂ∫¶‰πãÂíå‰∏∫Èõ∂„ÄÇ\nÂ§ö‰∏™Á∫¶ÊùüÊù°‰ª∂Ôºö\n$$ \\begin{aligned} ÁõÆÊ†áÂáΩÊï∞Ôºö\u0026amp; min \\ f(ùê±), \\quad ùê±‚àà ‚Ñù‚Åø \u0026amp; \\text{(ùê± ÊòØ‰∏™nÁª¥ÂêëÈáè)}\\\\ m‰∏™Á∫¶ÊùüÊù°‰ª∂Ôºö\u0026amp; s.t. \\quad g_i(ùê±) = \\pmb{a_i^T} ‚ãÖ ùê± + b_i ‚â§ 0, \u0026amp;\\text{(m‰∏™Ë∂ÖÂπ≥Èù¢Âõ¥ÊàêÁöÑÂå∫Âüü)}\\\\ \u0026amp; ÂÖ∂‰∏≠i=1,2,3\u0026hellip;m, \\pmb{a_i} ‚àà ‚Ñù‚Åø, \\ b_i ‚àà ‚Ñù‚Åø \\\\ ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Ôºö\u0026amp; L(ùê±, \\pmb Œª) = f(ùê±) + \\sum Œª_i g_i(ùê±) \\end{aligned} $$\nÂÅáËÆæÊúâ5‰∏™‰∏ÄÁª¥ÁöÑÁ∫¶ÊùüÊù°‰ª∂ÔºåÂ∞±ÊòØ5Êù°Áõ¥Á∫øÔºåÂπ∂‰∏îÂÅáËÆæÂÆÉ‰ª¨Âõ¥Êàê‰∫Ü‰∏Ä‰∏™‰∫îËæπÂΩ¢ÔºåËøòË¶ÅÂÅáËÆæ‰∫îËæπÂΩ¢ÁöÑÂÜÖÈÉ®ÊòØ5‰∏™Á∫¶ÊùüÊù°‰ª∂ÂêåÊó∂Êª°Ë∂≥ÁöÑÂå∫ÂüüÔºåÂ¶Ç‰∏ãÂõæ1„ÄÇ\n(ÂØπ‰∫é‰∫åÁª¥Âπ≥Èù¢‰∏äÁöÑ‰∏ÄÊù°Áõ¥Á∫ø:$y=ax+b$ÔºåÊÉ≥Áü•ÈÅì $y‚â§0$ Ë°®Á§∫ÁöÑÊòØÂì™ÂùóÂå∫ÂüüÔºåÂèØ‰ª•Ê†πÊçÆ(x,0)Ëøô‰∏™ÁÇπÂà§Êñ≠„ÄÇ $y=ax+b ‚â§0 \\Rightarrow x‚â§\\frac{-b}{a}$Ôºå ÊâÄ‰ª•Âú®$\\frac{-b}{a}$Â∑¶‰æßÂ∞±ÊòØËßÑÂÆöÁöÑÂå∫Âüü„ÄÇ Â¶ÇÊûúaÊòØË¥üÊï∞ÔºåÁ¨¶Âè∑Â∞±‰ºöÊîπÂèòÔºå‰ªéËÄåÂ§öÊù°Áõ¥Á∫øËÉΩÂõ¥Âá∫‰∏Ä‰∏™Â∞ÅÈó≠ÁöÑÈôêÂà∂Âå∫Âüü„ÄÇ )\n1 2 ÂØπÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Ê±ÇÊ¢ØÂ∫¶Ôºå‰ª§ÂÖ∂Á≠â‰∫éÈõ∂Ôºö\n$$ \\begin{aligned} \\pmb ‚àá L(ùê±,\\pmb Œª) = 0 \\Downarrow -\\pmb ‚àá f(\\pmb Œª) = ‚àë Œª_i ‚ãÖ \\pmb‚àá g_i(ùê±) \\end{aligned} $$\nÁõÆÊ†áÂáΩÊï∞Ê¢ØÂ∫¶ÁöÑÂèçÊñπÂêëÂ∫îÁ≠â‰∫éÊâÄÊúâÁ∫¶ÊùüÊù°‰ª∂ÁöÑÊ¢ØÂ∫¶Âä†ÊùÉÂíå„ÄÇ\nÁî±Âõæ2ÂèØÁü•ÔºåÁúüÊ≠£Ëµ∑Ë¥°ÁåÆÁöÑÂè™Êúâ‰∏§‰∏™Á∫¶ÊùüÊù°‰ª∂Âú®ÔºåÂè™ÈúÄÊ±Ç‰∏§‰∏™Á∫¶ÊùüÊù°‰ª∂ÁöÑÊ¢ØÂ∫¶ÂíåÔºå‰∫îËæπÂΩ¢ÂÜÖÈÉ®ÊòØ ‚â§ 0ÔºåÂàôÂ§ñÈÉ®ÊòØ\u0026gt;0ÔºåÊâÄ‰ª•Ê¢ØÂ∫¶ÊñπÂêëÊåáÂêëÂ§ñÈù¢Ôºå‰∏§‰∏™Ê¢ØÂ∫¶ÈÄöËøá Œª Ë∞ÉËäÇÊàê‰∏éÁõÆÊ†áÂáΩÊï∞Ê¢ØÂ∫¶Á≠âÂ§ßÂèçÂêë.\n‰∏§Á∫¶ÊùüÊù°‰ª∂ÁöÑ‰∫§ÁÇπ x\u0026rsquo; Êó¢Êª°Ë∂≥Ëøô‰∏§‰∏™Á∫¶ÊùüÊù°‰ª∂ÔºåÂú®ËøôÁÇπÁöÑÊ¢ØÂ∫¶=0Ôºö\n$$ \\begin{aligned} \u0026amp; g_Œ±(x\u0026rsquo;) = g_Œ≤(x\u0026rsquo;) = 0 \\ \u0026amp; Œª_Œ± ‚àá g_Œ±(x\u0026rsquo;) + Œª_Œ≤ ‚àá g_Œ≤(x\u0026rsquo;) = -‚àá f(x) ‚áí Œª_Œ±, Œª_Œ≤‚â†0 \\end{aligned} $$\nËÄåËØ•ÁÇπ x\u0026rsquo; ‰πüÊª°Ë∂≥ÂÖ∂‰ªñÁ∫¶ÊùüÊù°‰ª∂ g·µ¢(x\u0026rsquo;)\u0026lt;0Ôºå‰ΩÜÂÆÉ‰ª¨ÁöÑ Œª·µ¢ ÈÉΩÁ≠â‰∫é0Ôºå‰∏çËµ∑‰ΩúÁî®: Œª·µ¢‚àá g·µ¢(x)=0 ‚áí Œª·µ¢=0 (i‚â†Œ±,Œ≤) Œª ‰∏çËÉΩÂ∞è‰∫é 0ÔºåÂê¶Âàô‰ºöÊääÊ¢ØÂ∫¶ÊñπÂêëÂèñÂèçÔºå‰ºöÂèÇ‰∏éÊ¢ØÂ∫¶Âè†Âä†„ÄÇ\nÊâÄÊúâÁöÑ Œª·µ¢ ÈÉΩ‚â•0ÔºåÂ¶ÇÊûúŒª·µ¢=0, ÂÆÉÂØπÂ∫îÁöÑÁ∫¶ÊùüÊù°‰ª∂ g·µ¢(x) ÊòØÊùæÂºõÁöÑÔºõÂ¶ÇÊûúŒª·µ¢\u0026gt;0, ÂÆÉÂØπÂ∫îÁöÑÁ∫¶ÊùüÊù°‰ª∂ g·µ¢(x) ÊòØÁ¥ßËá¥ÁöÑ„ÄÇÁ∫¶ÊùüÊù°‰ª∂ÂÉèÁöÆÁ≠ãÔºåÊääÊúÄ‰ºòËß£‰ªéÊúÄÂÄºÁÇπÊãΩÂà∞‰∫ÜÊûÅÂÄºÁÇπ„ÄÇ ÂΩìÁõÆÊ†áÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄºËêΩÂú®ÂèØË°åÂüüËåÉÂõ¥ÂÜÖÔºåÂàôÊâÄÊúâÁöÑÁ∫¶ÊùüÊù°‰ª∂ÈÉΩÊòØÊùæÂºõÁöÑ„ÄÇ\nÊãâÊ†ºÊúóÊó•‰πòÊï∞Ê≥ïÊú¨Ë¥®ËøòÊòØÊ±ÇÂØº=0ÔºåÊ¢ØÂ∫¶Á≠â‰∫é0ÁöÑÁÇπÊòØÊûÅÂÄºÁÇπÔºå‰ΩÜ‰∏ç‰∏ÄÂÆöÊòØÊúÄÂÄºÁÇπ\nKKT Êù°‰ª∂‰∏≠ÁöÑ‰∫íË°•ÊùæÂºõ\n","date":"2022-10-01T14:14:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/10_%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/","title":"watch: DL - ÁéãÊú®Â§¥ 10 | Method of Lagrange Multipliers"},{"content":"Source video: 5-‚Äú‰∫§ÂèâÁÜµ‚ÄùÂ¶Ç‰ΩïÂÅöÊçüÂ§±ÂáΩÊï∞ÔºüÊâìÂåÖÁêÜËß£‚Äú‰ø°ÊÅØÈáè‚Äù„ÄÅ‚ÄúÊØîÁâπ‚Äù„ÄÅ‚ÄúÁÜµ‚Äù„ÄÅ‚ÄúKLÊï£Â∫¶‚Äù„ÄÅ‚Äú‰∫§ÂèâÁÜµ‚Äù\n‰ø°ÊÅØÈáè ‰∫ã‰ª∂ÂèëÁîü Ê¶ÇÁéá ÁöÑË¥üÂØπÊï∞ -log‚ÇÇ p\nËã•$f(p)$ Ë¢´ÂÆö‰πâ‰∏∫‰ø°ÊÅØÈáèÔºåË¶ÅËÆ©‰ΩìÁ≥ªËá™Ê¥ΩÂàôÈúÄÊª°Ë∂≥Ôºö\n$$ \\begin{aligned} f(p) \u0026amp; \\coloneqq ‰ø°ÊÅØÈáè \\\\ f(p_1 ‚ãÖ p_2) \u0026amp; = f(p_1) + f(p_2) \\end{aligned} $$\nÂØπ‰∫éÈòøÊ†πÂª∑‰ªéÂÖ´Âº∫ÊâìÂà∞ÂÜ†ÂÜõËøô‰ª∂‰∫ãÔºåÂèØ‰ª•ÊãÜÊàê‰∏§‰ª∂‰∫ãÔºöÈòøÊ†πÂª∑ËøõÂÖ•ÂÜ≥Ëµõ+ÈòøÊ†πÂª∑Ëµ¢‰∫ÜÂÜ≥Ëµõ„ÄÇ\nËøô‰∏§ÁßçÊèèËø∞ÁöÑ‰ø°ÊÅØÈáèÊòØ‰∏ÄÊ†∑ÁöÑÔºö$f$(ÈòøÊ†πÂª∑Â§∫ÂÜ†) = $f$(ÈòøÊ†πÂª∑ËøõÂÖ•ÂÜ≥Ëµõ) + $f$(ÈòøÊ†πÂª∑Ëµ¢‰∫ÜÂÜ≥Ëµõ) $\\Rightarrow f(\\frac{1}{8}) = f(\\frac{1}{4}) + f(\\frac{1}{2})$\nÂè¶Â§ñËøòË¶ÅÊª°Ë∂≥‰∫ã‰ª∂Èó¥ÁöÑÊ¶ÇÁéáÂÖ≥Á≥ªÔºöP(ÈòøÊ†πÂª∑Â§∫ÂÜ†) = P(ÈòøÊ†πÂª∑ËøõÂÖ•ÂÜ≥Ëµõ) ‚ãÖ P(ÈòøÊ†πÂª∑Ëµ¢‰∫ÜÂÜ≥Ëµõ)\nÊâÄ‰ª•ÈÄâÊã© log ÂáΩÊï∞ÔºåÂèØ‰ª•Êª°Ë∂≥Ëá™Ê¥ΩÔºõ‰ªéÁõ¥ËßÇÊù•ÁúãÔºåÂèëÁîüÊ¶ÇÁéáË∂äÂ∞èÔºåÊâÄÂê´‰ø°ÊÅØÈáèË∂äÂ§ßÔºå ËÄå log ÂáΩÊï∞ÊòØÈÄíÂ¢ûÁöÑÔºåÊâÄ‰ª•Á≥ªÊï∞Âèñ -1ÔºõËÄåÂ∫ïÊï∞ÂèØ‰ª•ÈÄâ eÔºå‰πüÂèØÈÄâ 2„ÄÇ\n$$ f(x) \\coloneqq -log_2 x $$\nÂ∫ïÊï∞ÈÄâ2ÔºåÁõ∏ÂΩì‰∫éÁî®ÊäõÁ°¨Â∏Å‰∫ã‰ª∂Êù•Ë°°Èáè‰ø°ÊÅØÈáè„ÄÇÊüê‰∫ã‰ª∂ÁöÑÂèëÁîüÊ¶ÇÁéáÊòØ 1/8ÔºåÁõ∏ÂΩì‰∫éÊäõ3‰∏™Á°¨Â∏ÅÂÖ®ÈÉ®Êúù‰∏äÁöÑÊ¶ÇÁéá„ÄÇÂπ∂‰∏î‰ª• 2 ‰∏∫Â∫ïËÆ°ÁÆóÂá∫ÁöÑ‰ø°ÊÅØÈáèÁöÑÂçï‰ΩçÊòØÊØîÁâπ„ÄÇÁ±ª‰ººÂú∞ÔºåËæìÂÖ• 16 ÊØîÁâπÁöÑÊï∞ÊçÆÂ∞±ÊòØÊää 16 ‰∏™ 0/1 Á°ÆÂÆö‰∏ãÊù•\nÁÜµ Á≥ªÁªü‰∏≠ÂêÑ‰∫ã‰ª∂‰ø°ÊÅØÈáèÁöÑ ÊúüÊúõ\n$$ \\begin{aligned} \u0026amp;H(P) \\coloneqq E(P_f) \\\\ \u0026amp;= ‚àë_{i=1}^m p_i ‚ãÖ f(p_i) = ‚àë_{i=1}^m p_i(-log_2p_i) \\\\ \u0026amp;= -‚àë_{i=1}^m p_i ‚ãÖ log_2 p_i \\end{aligned} $$\nË°°ÈáèÊï¥‰∏™Á≥ªÁªü‰∏≠ÁöÑÊâÄÊúâ‰∫ã‰ª∂ÁöÑ‰∏çÁ°ÆÂÆöÊÄß\nKLÊï£Â∫¶ (Áõ∏ÂØπÁÜµ) Á≥ªÁªüQÁõ∏ÂØπ‰∫éÁ≥ªÁªüPÂ∑ÆÂ§öÂ∞ëÔºö‰∫ã‰ª∂ i Âú®‰∏§Á≥ªÁªü‰∏≠ÁöÑ ‰ø°ÊÅØÈáè‰πãÂ∑ÆÔºåÊåâÁÖß i Âú®Á≥ªÁªü P ‰∏≠ÁöÑÊ¶ÇÁéáÂä†ÊùÉÊ±ÇÂíå„ÄÇ\n$$ D_{KL}(P\\|Q) = ‚àë_{i=1}^m p_i ‚ãÖ (-log_2 q_i) - ‚àë_{i=1}^m p_i ‚ãÖ (-log_2 p_i) $$\nÊó†Ê≥ïÁõ¥Êé•ÂØπÊØî‰∏§‰∏™‰∏çÂêåÁßçÁ±ªÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºàÊó†Ê≥ïÂÖ¨Â∫¶ÔºâÔºåËÄå‰∏î‰∫∫ËÑë‰∏≠ÁöÑÊ¶ÇÁéáÊ®°Âûã‰∏çÊ∏ÖÊ•öÔºåÊó†Ê≥ïÊ±ÇÁÜµÔºåÈúÄË¶Å Áõ∏ÂØπÁÜµ\nQÁ≥ªÁªüPÁ≥ªÁªüÁöÑÊ¶ÇÁéáÂàÜÂ∏É\nÂØπ‰∫éÊüê‰∫ã‰ª∂ $i$ Âú®Á≥ªÁªü Q ‰∏≠ÁöÑ‰ø°ÊÅØÈáè $f_Q(q_i)$ ÂáèÂéªÂÆÉÂØπÂ∫îÂà∞Âú®Á≥ªÁªü P ‰∏≠ÁöÑ‰ø°ÊÅØÈáè $f_P(p_i)$ÔºåÂÜçÊåâÁÖßÂú®Á≥ªÁªü P ‰∏≠ÁöÑÊ¶ÇÁéá $p$ Ê±ÇÊúüÊúõÔºö\n$$ \\begin{aligned} D_{KL} (P \\| Q) \u0026amp; \\coloneqq ‚àë_{i=1}^m p_i ‚ãÖ \\left(f_Q(q_i) - f_P(p_i)\\right) \\\\ \u0026amp;= ‚àë_{i=1}^m p_i ‚ãÖ \\left((-log_2 q_i) - (-log_2 p_i)\\right) \\\\ \u0026amp;= \\underbrace{‚àë_{i=1}^m p_i ‚ãÖ (-log_2 q_i)}_{‰∫§ÂèâÁÜµH(P,Q)}- \\underbrace{‚àë·µ¢‚Çå‚ÇÅ·µê p·µ¢‚ãÖ (-log‚ÇÇp·µ¢)}_{PÁöÑÁÜµ} \\end{aligned} $$\nÂ¶ÇÊûú‰∫ã‰ª∂ i Âú®‰∏§Á≥ªÁªü‰∏≠ÁöÑ‰ø°ÊÅØÈáèÁõ∏Á≠âÔºåÂ∑ÆÂÄº‰∏∫0ÔºåËØ¥Êòé‰∏§‰∏™Á≥ªÁªüÂÆåÂÖ®Áõ∏Á≠â„ÄÇ\nÊ†πÊçÆÂêâ‰∏çÊñØ‰∏çÁ≠âÂºèÔºö\nÂ¶ÇÊûúÊúâ‰∏§‰∏™Ê¶ÇÁéáÁ≥ªÁªüÔºå$‚àë_{i=1}^n p_i = ‚àë_{i=1}^n q_i = 1$Ôºå‰∏î $p_i, q_i \\in (0,1]$ÔºåÂàôÊúâÔºö\n$$ - ‚àë_{i=1}^n p_i log_{p_i} ‚â§ -‚àë_{i=1}^n p_i log_{q_i} $$\nÂΩì‰∏î‰ªÖÂΩì $p_i = q_i\\ ‚àÄ i$ Êó∂ÔºåÁ≠âÂè∑ÊàêÁ´ã„ÄÇ\n‰πüÂ∞±ÊòØËØ¥ KL Êï£Â∫¶ÊÅíÂ§ß‰∫éÁ≠â‰∫é0ÔºàË∑ùÁ¶ªÔºâ„ÄÇ\n‰∫§ÂèâÁÜµ $$ H(P,Q) = ‚àë_{i=1}^m p_i ‚ãÖ (- \\operatorname{log}_2 q_i) $$\n‰∫ã‰ª∂ i Âú®Ê¶ÇÁéáÁ≥ªÁªü Q ‰∏≠ÁöÑ‰ø°ÊÅØÈáèÔºåÊåâÁÖß i Âú®Á≥ªÁªü P ‰∏≠ÁöÑÊ¶ÇÁéáÂä†ÊùÉÊ±ÇÂíå\n‰∏∫‰∫Ü‰Ωø KL Êï£Â∫¶ÊúÄÂ∞èÔºåÂç≥Ê¶ÇÁéáÁ≥ªÁªü Q ÊúÄÊé•ËøëÁ≥ªÁªüPÔºåÂ∞±ÈúÄË¶Å‰∫§ÂèâÁÜµÊúÄÂ∞èÔºàÊúÄÊé•ËøëÁ≥ªÁªüPÁöÑÁÜµÔºâ\nÂØπ‰∫é‰∏ÄÂº†ÂõæÁâáÔºå‰∫∫ËÑëÁ≥ªÁªüÔºàÁõÆÊ†áÁ≥ªÁªü PÔºâÂè™Êúâ 2 ‰∏™‰∫ã‰ª∂ÔºöÊòØÁå´Âíå‰∏çÊòØÁå´ (\u0026ldquo;ÊòØÁå´\u0026quot;‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá‰∏∫ xÔºåÂàô\u0026quot;ÈùûÁå´\u0026quot;‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá‰∏∫ 1-x)Ôºå Á•ûÁªèÁΩëÁªúÔºàÁ≥ªÁªüQÔºâÁöÑÁªìÊûúÔºàyÔºâÊòØÂÉèÁå´ÁöÑÊ¶ÇÁéáÔºåÊâÄ‰ª•‰∫§ÂèâÁÜµ‰∏∫Ôºö\n$$ \\begin{aligned} H(P,Q) \u0026amp;= ‚àë_{i=1}^2 p_i ‚ãÖ (-log_2 q_i) \\\\ \u0026amp;= x ‚ãÖ log(y) + (1-x) ‚ãÖ log(1-y) \\\\ \u0026amp;= 1 ‚ãÖ log(y) + 0 ‚ãÖ log(1-y) \\end{aligned} $$\n","date":"2022-09-30T13:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/05_%E4%BA%A4%E5%8F%89%E7%86%B5/","title":"watch: DL - ÁéãÊú®Â§¥ 05 | Info Quantity \u0026 Cross Entropy"},{"content":"4-‚ÄúÊçüÂ§±ÂáΩÊï∞‚ÄùÊòØÂ¶Ç‰ΩïËÆæËÆ°Âá∫Êù•ÁöÑÔºüÁõ¥ËßÇÁêÜËß£‚ÄúÊúÄÂ∞è‰∫å‰πòÊ≥ï‚ÄùÂíå‚ÄúÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°Ê≥ï‚Äù\nÊçüÂ§±ÂáΩÊï∞ ÂÆöÈáèË°°Èáè‰∏§‰∏™Ê¶ÇÁéáÊ®°ÂûãÁöÑÂ∑ÆÂºÇ ‰∏âÁßçÊñπÊ≥ïÔºö ÊúÄÂ∞è‰∫å‰πòÊ≥ï ÊûÅÂ§ß‰ººÁÑ∂ ‰∫§ÂèâÁÜµ ÊúÄÂ∞è‰∫å‰πòÊ≥ï Áõ¥Êé•ÊØîËæÉÂà§Êñ≠ÁªìÊûú„ÄÇ\nmin $\\sum_{i=1}^n (x_i - y_i)^2$\n‰∫∫ËÑë‰∏≠ÁöÑÊ¶ÇÁéáÊ®°ÂûãÊó†Ê≥ïÂáÜÁ°ÆËØ¥Âá∫ÔºåËÄåÁ•ûÁªèÁΩëÁªúÁöÑÊ¶ÇÁéáÊ®°ÂûãËï¥ËóèÂú®ÂèÇÊï∞ÈáåÈù¢ÔºåÊ≤°ÊúâÁªü‰∏ÄÁöÑË°®ËææÔºüÂè™ËÉΩ‰ªéÁªìÊûúÂÖ•Êâã„ÄÇ\nÂ∞ÜÊØèÊ¨°‰∫∫ÁöÑÂà§Êñ≠ÁªìÊûú $x_i$ (1/0) ‰∏é Á•ûÁªèÁΩëÁªúÁöÑÂà§Êñ≠ÁªìÊûú $y_i$ (45%:1, 55%:0) ÁöÑËØØÂ∑Æ $|x_i -y_i|$ Ê±ÇÂíåÂèñÊúÄÂ∞èÔºå‰ªéËÄå‰øùËØÅÂú®ÁªìÊûú‰∏äÁúãÊòØÊúÄÊé•ËøëÁöÑ„ÄÇ\n$$ min \\sum_{i=1}^n |x_i - y_i| $$\nÂõ†ÁªùÂØπÂÄºÂú®ÂÆö‰πâÂüü‰∏ä‰∏çÊòØÂÖ®Á®ãÂèØÂØºÁöÑÔºåÊ±ÇÂπ≥Êñπ‰∏çÂΩ±Âìç x Âíå y ÁöÑÂ§ßÂ∞èÂÖ≥Á≥ªÔºåËÄå‰∏îÂÖ®Á®ãÂèØÂØºÔºåÂä†1/2ÊòØ‰∏∫‰∫ÜÊ±ÇÂØºÊñπ‰æø„ÄÇ\nÁî®ÊúÄÂ∞è‰∫å‰πòÊ≥ï‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂæàÈ∫ªÁÉ¶\u0026hellip;\nÊûÅÂ§ß‰ººÁÑ∂ ÂØπ‰∫éÂ∑≤ÂèëÁîüÁöÑÁé∞ÂÆû‰∫ã‰ª∂ÔºåÊúâÂæàÂ§öÊ¶ÇÁéáÊ®°ÂûãÈÉΩËÉΩÂØºËá¥Ëøô‰∏™ÊÉÖÂÜµÂèëÁîüÔºåÂèñ‰ººÁÑ∂ÂÄºÊúÄÂ§ßÁöÑÈÇ£‰∏™Ê¶ÇÁéáÊ®°Âûã‰Ωú‰∏∫ÊúÄ‚ÄúÁúüÂÆûÁöÑ‚ÄùÊ¶ÇÁéáÊ®°Âûã„ÄÇ\nÁî±‰∫éÂô™Â£∞ÁöÑÂ≠òÂú®ÔºåÁé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÊ¶ÇÁéáÊ®°ÂûãÂÅèÁßª‰∫ÜÁêÜÊÉ≥‰∏ñÁïå‰∏≠ÁöÑÊ®°ÂûãÔºåÁî±‰∫éÁêÜÊÉ≥‰∏ñÁïå‰∏éÁé∞ÂÆû‰∏ñÁïå‰πãÈó¥ÊúâÊ¨°ÂÖÉÂ£ÅÔºåÊó†Ê≥ïÁõ¥Êé•Áü•ÈÅìÁêÜÊÉ≥‰∏ñÁïå‰∏≠ÁúüÂÆûÁöÑÊ¶ÇÁéáÊ®°ÂûãÔºåÊâÄ‰ª•Âè™ËÉΩ‰ªéÁé∞ÂÆû‰∏ñÁïåÂèçÊé®Ôºå‰º∞ËÆ°Âá∫‰∏Ä‰∏™Ê¶ÇÁéáÊ®°ÂûãÔºåÂÆÉ‰ΩøËØ•Áé∞ÂÆû‰∫ã‰ª∂ÂèëÁîüÁöÑÂèØËÉΩÊÄßÊúÄÂ§ß„ÄÇ\nÊØîÂ¶ÇÊé∑10Ê¨°Á°¨Â∏ÅÁöÑÁªìÊûúÊòØ7Ê¨°Ê≠£Èù¢Ôºå3Ê¨°ÂèçÈù¢„ÄÇÊúâ3ÁßçÊ¶ÇÁéáÊ®°ÂûãÔºö\nÊ¶ÇÁéáÁªüËÆ°Ê®°Âûã Œ∏ Ê≠£Èù¢ ÂèçÈù¢ 1 0.1¬†0.9¬†2 0.7 0.3 3 0.8¬†0.2¬†Ëøô‰∏âÁßçÊ¶ÇÁéáÊ®°ÂûãÈÉΩÂèØ‰ª•Êé∑Âá∫7Ê¨°Ê≠£Èù¢Ôºå3Ê¨°ÂèçÈù¢„ÄÇ‰∏çËøáÁ¨¨2ÁßçÊ¶ÇÁéáÊ®°ÂûãÊé∑Âá∫7Ê≠£3ÂèçÁöÑÊ¶ÇÁéáÔºà‰ººÁÑ∂ÂÄºÔºâÊúÄÂ§ßÔºåÊâÄ‰ª•ËÆ§‰∏∫Á¨¨2ÁßçÊòØÊúÄÊé•Ëøë‚ÄúÁúüÂÆûÁöÑ‚ÄùÊ¶ÇÁéáÊ®°Âûã„ÄÇ\n$$ P(C‚ÇÅ, C‚ÇÇ, C‚ÇÉ, \u0026hellip;, C_10 | \\theta) = \\prod_{i=1}^{10} P(C_i | \\theta) $$\n‰ººÁÑ∂ÂÄºÔºöÁî®ÂèØËÉΩÂØºËá¥Áé∞ÂÆû‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéáÊ®°ÂûãÔºåËÆ°ÁÆóÂá∫Êù•ÁöÑËøôÁßçÊÉÖÂÜµÂèëÁîüÁöÑÊ¶ÇÁéáÂÄº.\nÁî®Â∑≤ÁªèÊ†áÊ≥®Â•ΩÁöÑ n Âº†ÂõæÁâáÔºàÁ°¨Â∏ÅÊ≠£ÂèçÔºâÂéªËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÔºåÁ•ûÁªèÁΩëÁªúÁöÑÊ¶ÇÁéáÊ®°Âûã (ùêñ, ùêõ) ‰∫ßÁîüÂá∫ËøônÂº†ÂõæÁâáÁöÑÊ¶ÇÁéáÂ∞±ÊòØÊ®°ÂûãÁöÑ‰ººÁÑ∂ÂÄºÔºö\n$$ \\begin{aligned} \u0026amp;P(x_1, x_2, x_3, \u0026hellip;, x_n | \\mathbf {W,b}) \\\\ \u0026amp;= \\prod_{i=1}^n P(x_i | \\mathbf{W,b}) \u0026amp; \\text{$x_i \\in {0,1}$,Ë°®Á§∫\u0026quot;ÊòØÁå´\u0026quot;,\u0026ldquo;‰∏çÊòØÁå´\u0026rdquo;}\\\\ \u0026amp;= \\prod_{i=1}^n P(x_i | y_i) \u0026amp; \\text{ùêñ,ùêõÂÜ≥ÂÆö‰∫ÜÊ®°ÂûãÔºàÁªôÂá∫ÁöÑ\u0026quot;ÊòØÁå´ÁöÑÊ¶ÇÁéá\u0026quot;$y_i$}Ôºâ \\\\ \u0026amp;= \\prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-{x_i}} \u0026amp;\\text{ÈááÊ†∑Êúç‰ªé0-1ÂàÜÂ∏É: $P=\\begin{cases} y_i, \u0026amp; x=1ÊòØÁå´ \\\\ 1-y_i, \u0026amp; x=0‰∏çÊòØÁå´\\end{cases}$} \\end{aligned} $$\nÊØîÂ¶ÇÊúâ7Âº†ÊòØÁå´Ôºå3Âº†‰∏çÊòØÁå´ÔºåÂπ∂ÂÅáËÆæÁ•ûÁªèÁΩëÁªúÂú® ùêñ,ùêõ ÁöÑÂèÇÊï∞‰∏ãÔºåÂà§Êñ≠ÊòØÁå´ÁöÑÊ¶ÇÁéáÊòØ 45%Ôºå‰∏çÊòØÁå´ÁöÑÊ¶ÇÁéáÊòØ 55%„ÄÇÈÇ£‰πàÊ≠§Ê®°ÂûãÁöÑ‰ººÁÑ∂ÂÄº=$(0.45)^7 (0.55)^3$\nËøû‰πòÂèòËøûÂä†Ôºö\n$$ \\begin{aligned} \u0026amp;log \\left( \\prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-x_i} \\right) \\\\ \u0026amp;= \\sum_{i=1}^n log(y_i^{x_i} (1-y_i)^{1-x_i}) \\\\ \u0026amp;= \\sum_{i=1}^n (x_i ‚ãÖ log y_i + (1-x_i) ‚ãÖ log(1-y_i)) \\end{aligned} $$\nÂΩì‰ººÁÑ∂ÂÄºËææÂà∞ÊúÄÂ§ßÁöÑÊó∂ÂÄôÔºåÂ∞±ÊòØÊúÄÊé•Ëøë‰∫∫ËÑëÁöÑÊ®°Âûã„ÄÇ\nÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°Ôºö$max \\sum_{i=1}^n (x_i ‚ãÖ log y_i + (1-x_i) ‚ãÖ log(1-y_i))$\n‰π†ÊÉØÊ±ÇÊûÅÂ∞èÔºö$min - \\sum_{i=1}^n (x_i ‚ãÖ log y_i + (1-x_i) ‚ãÖ log(1-y_i))$\nx ÊòØËÆ≠ÁªÉÊï∞ÊçÆÔºà‰ªéÁúüÂÆûÂàÜÂ∏É‰∏≠ÈááÊ†∑ÂæóÂà∞ÁöÑÔºâÔºåÊúâËá™Â∑±ÁöÑÂàÜÂ∏ÉÔºàÁõÆÊ†áÂàÜÂ∏ÉÔºâÔºåÂ∏åÊúõÊ®°ÂûãËæìÂá∫ÁöÑÊ†áÁ≠æ‰∏éËæìÂÖ•ÁöÑÊ†áÁ≠æÂàÜÂ∏É‰∏ÄËá¥ÔºåÊâÄ‰ª• \u0026ldquo;ËÆ©Â≠¶‰π†Âà∞ÁöÑÂàÜÂ∏É‰∫ßÁîüËæìÂÖ•Êï∞ÊçÆÁöÑÊ¶ÇÁéáÊúÄÂ§ß ‚àëlog P(ËæìÂÖ•Êï∞ÊçÆ | learnedÂàÜÂ∏É)\u0026rdquo; ‰∏é \u0026ldquo;ÊúÄÂ∞èÂåñ‰∏§ÂàÜÂ∏ÉÂ∑ÆÂºÇ ‚àë P(ÁõÆÊ†áÂàÜÂ∏É) log(P(learnedÂàÜÂ∏É))‚Äù Á≠â‰ª∑Ôºå‰πüÂ∞±ÊòØÊûÅÂ§ß‰ººÁÑ∂‰∏é‰∫§ÂèâÁÜµÁ≠â‰ª∑„ÄÇ\n","date":"2022-09-30T13:26:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/04_%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","title":"watch: DL - ÁéãÊú®Â§¥ 04 | Loss Functions"},{"content":"Á±ª‰ººÂàÜÂ∏ÉÂºèÁ≥ªÁªü‰∏≠ÁöÑÊ¶ÇÂøµ 1\ngroup: ÂΩìÂâçprocess group (world)Ôºå‰∏Ä‰∏™jobÊòØ‰∏Ä‰∏™ÁªÑ„ÄÇ‰∏Ä‰∏™ÁªÑÈáåÈù¢ÊúâÂ§ö‰∏™‰∏ªÊú∫ÔºànodeÔºâ world_size: ÂèÇ‰∏éjobÔºàÊï¥‰∏™ÁΩëÁªú‰∏≠ÔºâÁöÑËøõÁ®ã‰∏™Êï∞ (or gpu‰∏™Êï∞ or Êï∞ÊçÆÈõÜË¢´ÂàáÊàêworld_size‰ªΩÔºâ rank: ÂΩìÂâç‰∏ªÊú∫ÁöÑÁºñÂè∑ÔºågroupÂÜÖÂêÑËøõÁ®ãÁöÑÊ†áÂøóÁ¨¶ÊòØ‰ªé 0 Âà∞ world_size ÁöÑËøûÁª≠Êï¥Êï∞ local_rank: ‰∏∫ÂΩìÂâç‰∏ªÊú∫ÂÜÖÁöÑ‰∏Ä‰∏™ËøõÁ®ãÂàÜÈÖçGPUÔºàÊØèÂè∞‰∏ªÊú∫ÂèØ‰ª•ÂºÄÂêØÂ§ö‰∏™ËøõÁ®ãÔºàÊâßË°åÂêå‰∏Ä‰ªΩ‰ª£Á†ÅÔºâÔºâ DP vs DDP Comparison between Dataparallel and DistributedDataParallel 8\nDataParallel is single-process, multi-thread, and only works on single machine (with multiple card). While DistributedDataParallel is multi-process and works for both single- and multi- machine training. DDP works with model parallel torch.distributed ‰ΩøÁî®ÊµÅÁ®ã Refer to 2\nÂàõÂª∫ËøõÁ®ãÁªÑ:\n1 torch.distributed.init_process_group(backend=\u0026#39;nccl\u0026#39;, init_method=\u0026#39;env://\u0026#39;) Â¶ÇÊûúÈúÄË¶Å group ÂÜÖÈõÜ‰ΩìÈÄö‰ø°ÔºåÁî®new_group ÂàõÂª∫Â≠êÂàÜÁªÑ\nÂàõÂª∫DDPÂØπË±°Ôºö\n1 2 ddp_model = torch.nn.parallel.DistributedDataParallel( net, device_ids=[args.local_rank], output_device=args.local_rank) net‰Ωç‰∫élocal_rankÊåáÂÆöÁöÑgpu‰∏ä\n‰∏∫Êï∞ÊçÆÈõÜÂàõÂª∫sampler:\n1 train_sampler = DistributedSampler(train_set, num_replicas=world_size, rank=rank) Á°Æ‰øùÊØè‰∏™ËøõÁ®ãÁöÑ dataloader Âè™‰ºö load Âà∞Êï¥‰∏™Êï∞ÊçÆÈõÜÁöÑ‰∏Ä‰∏™ÁâπÂÆöÂ≠êÈõÜÔºåËÄå‰∏çÈáçÂ§ç 3\nÂú®ÊØè‰∏™‰∏ªÊú∫‰∏äÁî®ÂëΩ‰ª§ torch.distributed.launch ÂêØÂä®ËøõÁ®ãÔºàÂ¶ÇÊûúÂ∑≤ÂºÄÂêØÁöÑËøõÁ®ãÊú™ËææÂà∞world_sizeÔºåÂàôÊâÄÊúâËøõÁ®ã‰ºö‰∏ÄÁõ¥Á≠âÂæÖÔºâÔºåÂºÄÂßãËÆ≠ÁªÉ\nÊÅ¢Â§çËÆ≠ÁªÉÊó∂ÔºåÁªôÁ¨¨‰∏Ä‰∏ªÊú∫ÁöÑÂëΩ‰ª§Âä†‰∏ä--resume\nÈîÄÊØÅËøõÁ®ãÁªÑÔºödestory_process_group()\ninit_method ÊåáÂÆö‰∫ÜÂêÑËøõÁ®ã(‰∏ªÊú∫)Âêë rank=0 ÁöÑËøõÁ®ãÂèëÈÄÅ‰ø°ÊÅØÁöÑ(url)Âú∞ÂùÄ„ÄÇ\nTCP ÊñπÂºèÈúÄË¶ÅÊåáÂÆö rank 0 ËøõÁ®ãÁöÑipÂú∞ÂùÄ (init_method='tcp://10.1.1.20:23456')ÔºåÂπ∂‰∏îÈúÄË¶ÅÊâãÂä®ÊåáÂÆöÂêÑËøõÁ®ãÁöÑrank„ÄÇ\nÁ¨¨2ÁßçÊòØ‰ΩøÁî®‰∏Ä‰∏™Âú®ÂêåÁªÑÂÜÖÂêÑËøõÁ®ãÂÖ±‰∫´ÁöÑÊñá‰ª∂‰∫§Êç¢‰ø°ÊÅØ, urlÂ∫î‰ª•file://ÂºÄÂ§¥+Êñá‰ª∂Âú∞ÂùÄÔºå‰æãÂ¶Ç init_method=file:///mnt/nfs/sharedfileÔºå‰∏ç‰ºöËá™Âä®Âà†Èô§„ÄÇ\nÁ¨¨3ÁßçÔºàÈªòËÆ§Ôºâ‰ªéÁéØÂ¢ÉÂèòÈáè‰∏≠ËØªÂèñÈÖçÁΩÆÔºöMASTER_PORT, MASTER_ADDR, WORLD_SIZE, RANK„ÄÇ init_method=env:// 7\n‰ΩøÁî®tcpÂàùÂßãÂåñÔºå‰ΩøÁî®3Âè∞‰∏ªÊú∫ÔºåÊâßË°å3Ê¨°ÂëΩ‰ª§:\nMulti-node multi-gpu All you need to do is 4\nCreate a process group by RANK and WORLD_SIZE (auto set from the command arguments nproc_per_node,nnodes,and node_rank of torchrun) Wrap the model by torch.nn.parallel.DistributedDataParallel() move the model to gpu through LOCAL_RANK Wrap the dataset by DistributedSampler() resnet_ddp.py 5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def train(): # read hyper params from command ... parser.add_argument(\u0026#34;--local_rank\u0026#34;, type=int, help=\u0026#34;Local rank. Necessary for using the torch.distributed.launch utility.\u0026#34;) # specify gpu index # initialize process group torch.distributed.init_process_group(backend=\u0026#34;nccl\u0026#34;) # construct model model = torchvision.models.resnet18(pretrained=False) # Wrap the model on the GPU assigned to the current process device = torch.device(f\u0026#34;cuda:{local_rank}\u0026#34;) model = model.to(device) ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) # only save (and restore) the model on the gpu whose local_rank=0 if resume == True: map_location = {\u0026#34;cuda:0\u0026#34;: \u0026#34;cuda:{}\u0026#34;.format(local_rank)} ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location)) # prepare dataset train_set = torchvision.datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True,transform=transform) # a process will only use its own subset train_sampler = DistributedSampler(dataset=train_set) tarin_loader = DataLoader(dataset=train_set, batch_size=128, sampler=train_sampler, num_workers=8) # loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5) # training cycle for epoch in range(1000): # check accuracy of the model on local_rank=0 # train mode ddp_model.train() # iter all train_set for data in train_loader: inputs, labels = data[0].to(device), data[1].to(device) outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.zero_grad() optimizer.step() if __name__ == \u0026#34;__main__\u0026#34;: train() The training script resnet_ddp.py will run on two nodes, and each of nodes has 8 gpus and each gpu would launch one process.\nIn the terminal of the first node, excute the following command.\n1 2 3 4 python -m torch.distributed.launch \\ --nproc_per_node=8 --nnodes=2 --node_rank=0 \\ --master_addr=\u0026#34;192.168.0.1\u0026#34; \\ --master_port=1234 resnet_ddp.py Excute the same command but with different node_rank on the second node:\n1 2 3 4 python -m torch.distributed.launch \\ --nproc_per_node=8 --nnodes=2 --node_rank=1 \\ --master_addr=\u0026#34;192.168.0.1\u0026#34; \\ --master_port=1234 resnet_ddp.py Single-node multi-worker Refer to 6\n1 2 3 4 torchrun --standalone --nnodes=1 \\ --nproc_per_node=$NUM_TRAINERS \\ YOUR_TRAINING_SCRIPT.py \\ (--arg1 ... train script args ...) (2023-08-27)\nExample of launch.json For running AIM\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Python: Current File\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;torch.distributed.launch\u0026#34;, \u0026#34;console\u0026#34;: \u0026#34;internalConsole\u0026#34;, \u0026#34;justMyCode\u0026#34;: true, \u0026#34;env\u0026#34;: {\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;: \u0026#34;4\u0026#34;}, \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;1\u0026#34;, // GPUs \u0026#34;--master_port\u0026#34;, \u0026#34;29500\u0026#34;, \u0026#34;tools/train.py\u0026#34;, \u0026#34;configs/recognition/vit/vitclip_base_diving48.py\u0026#34;, \u0026#34;--launcher\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;--test-last\u0026#34;, \u0026#34;--validate\u0026#34;, \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34; ] } ] } Pass env to args Settings below in \u0026ldquo;launch.json\u0026rdquo; don\u0026rsquo;t work:\n1 2 3 4 5 6 7 8 \u0026#34;env\u0026#34;: { \u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;GPUS\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;PORT\u0026#34;: \u0026#34;29500\u0026#34; }, \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;${env:GPUS}\u0026#34;, \u0026#34;--master_port\u0026#34;, \u0026#34;${env:PORT}\u0026#34;, May refer to\nLaunch.json: how to reference an environment variable Resolve Environment Variables in Args defined in launch.json for Debugging #91053 Ref PyTorch Â§öËøõÁ®ãÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÂÆûÊàò-murphypei-githubio Pytorch ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ - ‰ºöÈ£ûÁöÑÈó≤È±ºÁöÑÊñáÁ´† - Áü•‰πé PytorchÂ§öÊú∫Â§öÂç°ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ - Ë∞ú‰∏ÄÊ†∑ÁöÑÁî∑Â≠êÁöÑÊñáÁ´† - Áü•‰πé Multi node PyTorch Distributed Training Guide For People In A Hurry-Lambda PyTorch Distributed Training - Lei Mao TORCHRUN (ELASTIC LAUNCH) DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED GETTING STARTED WITH DISTRIBUTED DATA PARALLEL „Äêpytorch distributed„Äënccl ÈõÜÂêàÈÄö‰ø°Ôºàcollective communicationÔºâ\n(2024-06-02)\n„Äêpytorch distributed„Äëaccelerate Âü∫Êú¨Áî®Ê≥ïÔºàconfigÔºålaunchÔºâÊï∞ÊçÆÂπ∂Ë°å - ‰∫îÈÅìÂè£Á∫≥‰ªÄ\n","date":"2022-09-22T19:49:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_distributed/","title":"memo: PyTorch | Data Parallel"},{"content":"Basics Èïø‰º™‰ª£Á†ÅË∑®Ê†è Placing a single algorithm in two columns\n1 2 \\usepackage[linesnumbered,ruled,vlined]{algorithm2e} \\usepackage{multicol} ÊääÁÆóÊ≥ïÂÜÖÂÆπ Áî® \\begin{multicols}[2] ... \\end{multicols} ÂåÖËµ∑Êù•ÔºåËÄå‰∏çÊòØÂú® \\begin{algorithm} ... \\end{algorithm}Â§ñÈù¢„ÄÇ\nÊñáÊ°£È°µÈù¢ÊóãËΩ¨90Â∫¶ Rotate a page 180 degrees in LaTeX\nFor the whole document:\n1 \\documentclass[landscape]{article} TableÊØèË°åËá™Âä®Ê†áÂè∑ Reference table row in LaTeX\n‰ΩøÁî® counter\n1 2 3 4 5 6 7 8 9 10 11 12 13 \\usepackage{array} \\newcounter{rowcntr}[table] \\renewcommand{\\therowcntr}{\\arabic{rowcntr}} % A new columntype to apply automatic stepping \\newcolumntype{N}{\u0026gt;{\\refstepcounter{rowcntr}\\therowcntr}w{c}{0.1em}} % Reset the rowcntr counter at each new tabular % \\AtBeginEnvironment{tabular}{\\setcounter{rowcntr}{0}} ....... \\begin{tabular}{|N|c|c|c|} \\end{tabular} How does the \\newcolumntype command work?\nw{c}{0.1em} ÊòØÊéßÂà∂ÂØπÈΩêÂíåÂàóÂÆΩ\nÈïøË°®Ê†ºËá™Âä®Êç¢È°µ latex long table automatic move to next page\n\\usepackage{longtable}\nMulti-page tables using \\longtable\nÈáåÈù¢‰∏çËÉΩÁî® tabular ÁéØÂ¢ÉÔºå‰∏çËøáÊçÆËØ¥ÂÆÉÊîØÊåÅÂ§ßÈÉ®ÂàÜtabular ÁâπÊÄß\n1 2 3 4 \\begin{longtable}{|c|c|c|c|} Title \u0026amp; Nerf‚Äôs Problems \u0026amp; Solutions \u0026amp; Flaw \\endhead % ËÆ©Ë°®Â§¥Âá∫Áé∞Âú®ÊØèÈ°µ‰∏ä \\end{longtable} Ë°®Ê†ºÂÜÖÊç¢Ë°å cell ÂÜÖÈÉ®ÂÜçÁî®‰∏™ tabular ÁéØÂ¢É:\n\\begin{tabular}[t]{@{}p{\\linewidth}@{}}PlenOctrees for Real-time Rendering of Neural Radiance Fields \\\\ ICCV 2021 Oral \\end{tabular}\n\\newline ICCV 2021\n‰∫§ÂèâÂºïÁî®ÊõøÊç¢‰∏∫ÊñáÂ≠ó LaTeX/Hyperlinks\nÁî®Ê≥ïÔºö\n1 2 3 \\usepackage{hyperref} ... \\hyperref[label_name]{\u0026#39;\u0026#39;link text\u0026#39;\u0026#39;} \\ref{label_name} Âè™ÊòØÊï∞Â≠ó\nAdvanced LaTeX Cross-references post\n\\pageref{key} command, which prints the number of the page where the \\label{key} was inserted.\nÁîªÁ®ãÂ∫èÊµÅÁ®ãÂõæ Code using algorithm2e package\nÊèíÂÖ•‰ª£Á†ÅÂùó matlab Highlighting MATLAB Code in LaTeX with mcode\nÁîªÂùêÊ†áÁ≥ª Coordinate system in LaTeX with TikZ\nPgfplots package overleaf\nOverfull \\hbox with pgfplots graph\nÁîªÂ§πËßí How to draw a simple angle, two intersecting lines Tikz\nÁîªÂúÜÊü±‰Ωì generate simple cylindrical shape with text in latex (tikz)\nLearn How to Draw a Cylinder Shape in TikZ\nÂêåÊ≠•Áº©Êîæ TikZ ‰∏éÂÖ∂‰∏≠ÁöÑ node Âú® LaTeX ‰∏≠ÂêåÊ≠•Áº©Êîæ TikZ ‰∏éÂÖ∂‰∏≠ÁöÑ node\nÁº©ÊîæË°®Ê†º Is there a way to slightly shrink a table, including font size, to fit within the column boundaries?\nÂ§öÂº†ÂõæÁâáÂπ∂Êéí subfigure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \\usepackage{subfigure}%Âπ∂ÊéíÂ≠êÂõæ ÂÖ±‰∫´Ê†áÈ¢ò ÊúâÂ≠êÊ†áÈ¢ò \\begin{document} \\begin{figure}[H] \\centering \\subfigure[‰∏ÄÊ¨°ÂáΩÊï∞]{ \\label{fig:subfig:onefunction} \\includegraphics[scale=0.3]{figureDemo2}} \\hspace{0.5in} % ‰∏§ÂõæÁâá‰πãÈó¥ÁöÑË∑ùÁ¶ª \\subfigure[‰∫åÊ¨°ÂáΩÊï∞]{ \\label{fig:subfig:twofunction} \\includegraphics[scale=0.3]{figureDemo3}} \\caption{2‰∏™ÂõæÁâáÂπ∂ÊéíÊºîÁ§∫} \\label{fig:twopicture} \\end{figure} \\end{document} minipage\n1 2 3 4 \\begin{figure}[htbp] \\centering \\end{figure} Áü©ÈòµÊØèË°åÂä† label 1 \\usepackage{blkarray} overleaf DIP assign 3\nSuitable for R markdown Label rows and columns of a matrix in R Markdown with Latex and HTML rendering\nBasic usage (2021-01-15)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 \\documentclass{article}\t%ÊñáÊ°£Á±ªÂûãÔºöÊñáÁ´† \\documentclass{book}\t%‰π¶ \\documentclass{beamer}\t%ÂπªÁÅØÁâáÊ†ºÂºè \\documentclass[UTF8]{ctexart}\t%ctexartÊîØÊåÅ‰∏≠Ëã±ÊñáÊ∑∑ÊãçÔºåÊåáÂÆöÊñáÊ°£ÁºñÁ†ÅÁ±ªÂûã \\documentclas[UTF8]{ctexbook}\t%ÂØπ‰π¶Á±çÊéíÁâà \\title{ÊñáÁ´†Ê†áÈ¢ò} \\author{ÊñáÁ´†‰ΩúËÄÖ} \\date{ÊñáÊ°£ÁöÑ‰øÆÊîπÊó•Êúü} \\date{\\today}\t%Ëá™Âä®ÁîüÊàêÂΩìÂ§©Êó•Êúü \\begin{document}\t%‰∏ãÈù¢ÊòØÊ≠£ÊñáÔºå‰∏äÈù¢ÊòØÂâçË®ÄÔºå\\beginÂíå\\end‰πãÈó¥ÊòØÁéØÂ¢É/‰ΩúÁî®ÂüüÔºå‰Ωç‰∫éÂêå‰∏Ä‰∏™ÁéØÂ¢É‰∏≠ÁöÑÂÜÖÂÆπÂ∞Ü‰ºöÂÖ±‰∫´Áõ∏ÂêåÁöÑÊñáÂ≠óÊ†ºÂºè \\maketitle\t%Âú®ÂΩìÂâç‰ΩçÁΩÆÁîüÊàêÊñáÊ°£ÁöÑÊ†áÈ¢òÔºàÂâçË®ÄÂå∫ËÆæÁΩÆÁöÑ‰ø°ÊÅØÔºâ \\textbf{Âä†Á≤óÊñáÂ≠ó}\t%bold font \\textit{Êñú‰Ωì}\t%italic \\underline{Âä†‰∏ãÂàíÁ∫ø} %‰∏§‰∏™ÂõûËΩ¶ÊòØÊç¢ÊÆµÔºå‰∏Ä‰∏™ÂõûËΩ¶ÊòØ‰∏Ä‰∏™Á©∫Ê†º \\part{‰π¶Á±çÁöÑÁ¨¨‰∏ÄÈÉ®ÂàÜ} \\chapter{‰π¶Á±çÁöÑÁ¨¨‰∏ÄÁ´†} \\section{ËøôÊòØÂú∞‰∏Ä‰∏™Á´†ËäÇ} ‰Ω†Â•ΩÔºÅ \\subsection{Â≠êÁ´†ËäÇ} ‰∫åÁ∫ßÁ´†ËäÇ‰∏ãÁöÑÂÜÖÂÆπ \\subsubsection{ËøôÊòØ‰∏Ä‰∏™‰∏âÁ∫ßÁ´†ËäÇ} ‰∏âÁ∫ßÁ´†ËäÇ‰∏ãÁöÑÂÜÖÂÆπ \\section{ËøôÊòØÁ¨¨‰∫å‰∏™Á´†ËäÇ} Á¨¨‰∫åÁ´†ËäÇ‰∏ãÁöÑÂÜÖÂÆπ %ÊèíÂÖ•ÂõæÁâá \\usepackage{graphicx}\t%ÂºïÁî®ÂåÖÔºåÂåÖÂê´‰∫ÜËã•Âπ≤ÁªòÂà∂ÂõæÁâáÁöÑÊåá‰ª§ \\begin{figure}\t%ÊääÂõæÁâáÂµåÂ•óÂà∞figureÁéØÂ¢É‰∏≠ÔºåÂèØ‰ª•ÊåáÂÆöÊ†áÈ¢ò \\centering\t%Â∞ÜÂõæÁâáÂ±Ö‰∏≠ÊòæÁ§∫ \\includegraphics[width=0.5\\textwidth]{ÂõæÁâáÂêçÂ≠óÂèØÁúÅÁï•ÂêéÁºÄ}\t%Âú®ÂΩìÂâç‰ΩçÁΩÆÊ∑ªÂä†‰∏ÄÂº†ÂõæÁâá,ÂõæÁâáÂÆΩÂ∫¶Á≠â‰∫é0.5ÂÄçÁöÑÂΩìÂâçÊñáÊú¨Âå∫ÂüüÁöÑÂÆΩÂ∫¶ \\caption{ËøôÊòØÂõæÁâáÁöÑÊ†áÈ¢ò} \\end{figure} %ÂàóË°® \\begin{itemize}\t%Êó†Â∫èÂàóË°®ÁöÑÂàõÂª∫ÔºöÂàóË°®‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ÈÉΩÈúÄË¶Å‰ª•\\itemÂºÄÂ§¥ \\item ÂàóË°®È°π1 \\item ÂàóË°®È°π2 \\item ÂàóË°®È°π3 \\end{itemize} \\begin{enumrate}\t%ÊúâÂ∫èÂàóË°®:ÂâçÈù¢Â∏¶ÁºñÂè∑ \\item ÂàóË°®È°π1 \\item ÂàóË°®È°π2 \\item ÂàóË°®È°π3 \\end{enumerate} %Êï∞Â≠¶ÂÖ¨Âºè Ë¥®ËÉΩÊñπÁ®ãÔºö$E=mc^2$ \\begin{equation} E=mc^2 \\end{equation} \\[ E=mc^2 \\] \\over ÊòØÂá†ÂàÜ‰πãÂá† \\[ d={k \\varphi(n)+1} \\over e \\] codecogs ÂèØ‰ª•ÊµãËØïÂÖ¨Âºè %Ë°®Ê†º \\begin{table}\t%tableÁéØÂ¢ÉËÆæÁΩÆÊ†áÈ¢ò \\caption{Ë°®Ê†ºÁöÑÊ†áÈ¢ò} \\center\t%Ë°®Ê†ºÂ±Ö‰∏≠ÊòæÁ§∫ \\begin{tabular}{|c| c| c|}\t%Êúâ‰∏âÂàóÔºåÊØèÂàóÈÉΩÂ±Ö‰∏≠ÔºàcenteringÔºâÔºå| ‰ª£Ë°®Á´ñÁõ¥ËæπÊ°Ü„ÄÇ % {l c c} ÂàôË°®Á§∫Á¨¨‰∏ÄÂàóÂ∑¶ÂØπÈΩê(left) % {p{2cm} c c}\tËÆæÁΩÆÂàóÂÆΩ2cm \\hline\t#Ê∞¥Âπ≥ËæπÊ°Ü ÂçïÂÖÉÊ†º1 \u0026amp; ÂçïÂÖÉÊ†º2 \u0026amp; ÂçïÂÖÉÊ†º3 \\\\ \\hline\\hline\t%ÂèåÊ®™Á∫ø ÂçïÂÖÉÊ†º4 \u0026amp; ÂçïÂÖÉÊ†º5 \u0026amp; ÂçïÂÖÉÊ†º6 \\\\ \\hline ÂçïÂÖÉÊ†º7 \u0026amp; ÂçïÂÖÉÊ†º8 \u0026amp; ÂçïÂÖÉÊ†º9 \\\\ \\hline \\end{tabular} \\end{table} Paper tips (2024-03-21)\nguanyingc/latex_paper_writing_tips\nPlots Tikz Examples (2024-04-11)\nExample of Petar Veliƒçkoviƒá X\nInkscape (2024-04-23)\nHow I draw figures for my mathematical lecture notes using Inkscape - Gilles Castel Take Lecture Notes (2024-04-23)\nHow I\u0026rsquo;m able to take notes in mathematics lectures using LaTeX and Vim (Found Gilles Castel as he was followed by cxzhou35 on Github.)\nHow I manage my LaTeX lecture notes\nPseudoCode (2024-06-01)\nCurious how to add comments like this:\n","date":"2022-09-12T17:40:00Z","permalink":"https://zichen34.github.io/writenotes/lang/latex_misc/","title":"memo: Latex"},{"content":"Arxiv\nNotes UNet Â∏∏Áî®‰∫éÂõæÂÉè Segmentation„ÄÇ\nCNNÁöÑÁõÆÊ†áÊòØÊèêÂèñÁâπÂæÅÔºàÂáèÂ∞ëÂÜó‰Ωô‰ø°ÊÅØÔºâÔºåÁªèËøáÂ§ö‰∏™poolingÂ±ÇÔºà‰ª•Âèästrides\u0026gt;1ÔºâÔºåÊúÄÂêéÁöÑ feature map ÁöÑÂ∞∫ÂØ∏ÔºàÂàÜËæ®ÁéáÔºâÊòØÊúÄÂ∞èÁöÑÔºå ‰ΩÜÂõæÂÉèÂàÜÂâ≤‰ªªÂä°ÈúÄË¶Å‰∏∫ÂõæÁâáÁöÑÊØè‰∏™ÂÉèÁ¥†Âà§Êñ≠ labelÔºåÊâÄ‰ª•ÈúÄË¶ÅÊää CNN ÊúÄÂêéËæìÂá∫ÁöÑ feature map ÊÅ¢Â§çËá≥ÂéüÊù•ÁöÑÂ∞∫ÂØ∏Ôºå U-Net ÈÄöËøáÈÄêÁ∫ß upsampling (ÊèíÂÄº) ÂæóÂà∞‰∫Ü‰∏écnnÂØπÁß∞ÁöÑfeature maps„ÄÇ ÁÑ∂ÂêéÂ∞±ÂèØ‰ª•Êää CNN ‰∏≠Èó¥ËøáÁ®ã‰∫ßÁîüÁöÑÂàÜËæ®ÁéáËæÉÈ´òÁöÑ feature maps ‰∏éÂØπÂ∫îÁöÑ upsampled feature maps ÁªìÂêàËµ∑Êù•Ôºå‰ªéËÄåËæìÂá∫Êõ¥Á≤æÁ°ÆÁöÑ segmentation map„ÄÇ U-Net architecture Ronneberger 2015 Â∑¶‰æßÊòØ CNN \u0026ldquo;Êî∂Áº©\u0026quot;Ë∑ØÂæÑÔºàencoderÔºâÔºåÂè≥‰æßÊòØ\u0026quot;Êâ©Â±ï\u0026quot;Ë∑ØÂæÑÔºàdecoderÔºâ„ÄÇ\nÂ∑¶‰æß CNN ÁöÑÊØè‰∏ÄÁ∫ßÂÅö‰∏§Ê¨° conv3x3 (unpadded)ÔºåÁÑ∂ÂêéReLUÊøÄÊ¥ªÂπ∂ÈÄöËøá 2x2 max-pooling ÂÅö‰∏ãÈááÊ†∑Ôºå ÊØèÊ¨°‰∏ãÈááÊ†∑‰ºöÊää ÈÄöÈÅìÊï∞ÈáèÂä†ÂÄç„ÄÇ\nÂè≥‰æßÊØèÊ¨°ÂÖàÂØπ feature map ÂÅö 2 ÂÄç‰∏äÈááÊ†∑Âíå‰∏§Ê¨° con3x3 ÊääÈÄöÈÅìÊï∞Âáè‰∏ÄÂçä„ÄÇ\nÁÅ∞Ëâ≤ÁÆ≠Â§¥ÊòØ concatenationÔºàResnet ÁöÑ skipconnect ÊòØÁõ¥Êé•Áõ∏Âä†Ôºå‰∏çÊòØÊãºÊé•ÔºâÔºåÊääÊù•Ëá™cnnÁöÑfeature map ÁöÑËæπÁºòË£ÅÂâ™‰∏Ä‰∏ãÔºåÊãºÊé•Âà∞Âè≥‰æßÁöÑfeature map‰∏ä„ÄÇÊúÄÂêéÁöÑfeature map ÂÅöconv1x1 Êää64ÈÄöÈÅìÂèòÊç¢Âà∞ÊâÄÈúÄÁöÑÁ±ªÂà´‰∏™Êï∞\n‰∏äÈááÊ†∑‰∏ç‰ºöÂ¢ûÂä†(ÊÅ¢Â§ç)Á©∫Èó¥‰ø°ÊÅØ\nskip connectionÁöÑÂéüÁêÜÊòØ‰ªÄ‰πàÔºü‰∏∫‰ªÄ‰πàU-net‰∏≠Ë¶ÅÁî®Âà∞skip connection?-akkaze-ÈÉëÂÆâÂù§ÁöÑÂõûÁ≠î\n(2023-07-10)\nExample Tensorflow: CV2020 - 16 - Object Segmentation.ipynb\nPyTorch: U-Net: Training Image Segmentation Models in PyTorch\nSegmentation needs to give a label for each pixel, so the output should have the same size as the input image.\nThe hidden feature vector has lost spatial information along contracting. And up-sampling (interpolation) doesn\u0026rsquo;t restore the location of features, but just kind of \u0026ldquo;copy\u0026rdquo; the features to around pixel.\nThe feature on each pixel is concatenated with the feature before contracting which still locates in its original position. Then the convolution later on will \u0026ldquo;fuse\u0026rdquo; one pixel\u0026rsquo;s location \u0026ldquo;characteristic\u0026rdquo; into its feature vector.\nThe output feature map is an expansion of the compact hidden featuer map, but conditioned with spatial location.\nThis way the trained model can classify a pixel based on spatial position and surrouding RGB feature.\nU-Net Explained: Understanding its Image Segmentation Architecture - medium\n","date":"2022-09-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/misc/b-note-unet/","title":"read: UNet"},{"content":"Note GL_PROJECTION matrix: camera space point(xe,ye,-ze) -\u0026gt; perspective projection to near plane -\u0026gt; homo:(xp,yp,1) -\u0026gt; scale -\u0026gt; homo:(xn,yn,zn,1) -\u0026gt; (x_clip,y_clip,z_clip, w_clip) , where \u0026lsquo;xn,yn,zn\u0026rsquo; are variables raning from -1 to 1.\nRefer to OpenGL Projection Matrix - songho\nNormalized Device Coordinates (NDC) is used to determine whether a 3D point can appear on the computer monitor, which is a cube with length 1. The transformation from eye coordinates to NDC is mapping the truncated pyramid frustum to a cube.\nPerspective Projection - songho\nAccording to the perspective projection, the projection point of a world point (x‚Çë,y‚Çë,z‚Çë) on the near plane is\n$$ \\begin{cases} x_p = \\frac{n}{-z_e} x_e \\\\ y_p = \\frac{n}{-z_e} y_e \\end{cases} $$\n(Camera coordinates is right-hand system looking in the -z direction, while NDC is looking in the z direction under left-hand coordinates)\nIn order to use a matrix to represent NDC transformation, the homogeneous coordinates are used to enable division, so the transformation can be represented as:\n$$ \\begin{pmatrix} x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{pmatrix} = M_{projection} \\cdot \\begin{pmatrix} x_e \\\\ y_e \\\\ z_e \\\\ w_e \\end{pmatrix} $$\n(2024-02-15) w‚Çë is the homogeneous coordinate for storing the original depth value of the camera point after the multiplication with the projection matrix, where the 4-th row is [0 0 -1 0], so w‚Çë = 1. And the depth will be divided at the very end step: the perspective division, so as to make the intermediate processes linear operations.\nComparing merely projecting a 3D point onto plane with a w2c, the projection matrix specifies specific behavior for the z-axis of the ND space (not the source camera space any more).\nTherefore, the NDC is:\n$$ \\begin{pmatrix} x_{ndc} \\\\ y_{ndc} \\\\ z_{ndc} \\end{pmatrix} = \\begin{pmatrix} x_{clip} / w_{clip} \\\\ y_{clip}/w_{clip} \\\\ z_{clip}/w_{clip} \\end{pmatrix} $$\nBecause $w_{clip}$ is the denominator, it should equal to -z‚Çë; Hence, the forth row of matrix should be $[0\\ 0\\ -1\\ 0]$\nMapping [l, r] and [b, t] to [-1, 1] with linear realtionship: Two points (l,-1),(r,1) can be used to determine the line:\n$$ x_{NDC} = \\frac{1-(-1)}{r-l} \\cdot x_p + Œ≤ $$\nand then substitute (r,1) for $(x_p,x_{NDC})$ to solve Œ≤ = -(r+l)/(r-l).\nTherefore, $x_{NDC} = \\frac{2}{r-l}x_p - \\frac{r+l}{r-l}$.\nSimilarly, $y_{NDC} = \\frac{2}{t-b} y_p- \\frac{t+b}{t-b}$\nSubstitute xp, yp with the form of x‚Çë, y‚Çë:\n$$ x_{NDC} = (\\frac{2n}{r-l} \\cdot x_e + \\frac{r+l}{r-l} \\cdot z_e) / -z_e \\\\ y_{NDC} = (\\frac{2n}{t-b} \\cdot y_e + \\frac{t+b}{t-b} \\cdot z_e) / -z_e $$\nTherefore, the first two row elements of the matrix can be determined.\nSuppose the third row is $[0\\ 0\\ A\\ B]$ (z value is independent to x and y), so:\n$$ z_{NDC} = \\frac{A z_e + B w_e}{-z_e} $$\nSubstitute the correspondence between (-n, -f) and (-1, 1) into the above equation:\n$$ \\begin{cases} \\frac{-A n + B}{n} = -1 \\newline \\frac{-A f + B}{f} = 1 \\end{cases} $$\nTherefore, A = -(f+n)/(f-n), and B = -2fn / (f-n)\nFinally, the matrix $M_{projection}$ is\n$$ \\begin{pmatrix} \\frac{2n}{r-l} \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{-(f+n)}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} $$\nCode (2023-10-02)\n3D world coords multiplied with Inverse intrinsics matrix, Scale the [near, far] to [0,1] Code credits MatchNeRF\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_coord_ref_ndc(extr_ref, intr_ref, pts_3D, inv_scale, near_far=None, lindisp=False): \u0026#39;\u0026#39;\u0026#39; Warp the provided position to the reference coordinate, and normalize to NDC coordinate. pts_3D [batch, N_rays N_sample 3] \u0026#39;\u0026#39;\u0026#39; bs, N_rays, N_samples, N_dim = pts_3D.shape # (bs=1, n_rays=1024, n_pts=128, n_dim=3) pts_3D = pts_3D.reshape(bs, -1, N_dim) # (bs, n_rays*n_pts, n_dim) near, far = torch.split(near_far, [1, 1], dim=-1) # (1,2) -\u0026gt; both are (1,1) # wrap to ref view if extr_ref is not None: # 3D pts in world space -\u0026gt; camera space of a src view pts_ref_world = world2cam(pts_3D, extr_ref) if intr_ref is not None: # using projection # pts in camera space -\u0026gt; image plane coords with z point_samples_pixel = pts_ref_world @ intr_ref.transpose(-1, -2) # normalize to 0~1 point_samples_pixel[..., :2] = (point_samples_pixel[..., :2] / point_samples_pixel[..., -1:] + 0.0) / inv_scale.reshape(bs, 1, 2) if not lindisp: point_samples_pixel[..., 2] = (point_samples_pixel[..., 2] - near) / (far - near) # normalize to 0~1 else: point_samples_pixel[..., 2] = (1.0/point_samples_pixel[..., 2]-1.0/near)/(1.0/far - 1.0/near) else: # using bounding box near, far = near.view(bs, 1, 3), far.view(bs, 1, 3) point_samples_pixel = (pts_ref_world - near) / (far - near) # normalize to 0~1 point_samples_pixel = point_samples_pixel.view(bs, N_rays, N_samples, 3) # (bs, n_rays*n_pts, 3) -\u0026gt; (bs, n_rays, n_pts, 3) return point_samples_pixel near-far = [0,1] (2024-01-16)\nProjection Matrix ËØ¶Ëß£ - Ë¥∞ËäçÁöÑÊñáÁ´† - Áü•‰πé\n","date":"2022-09-02T11:39:00Z","permalink":"https://zichen34.github.io/writenotes/vis/ndc/","title":"memo: Vis | NDC"},{"content":"Stereographic Projection Source video: Visualizing quaternions (4d numbers) with stereographic projection\nLeveraging projection to reduce the dimensionality by 1. Such that quaternion in 4D space can be learned through the projection counterparts in 3D space.\nÊää (i-ÂÆûËΩ¥) Âçï‰ΩçÂúÜ ÂÅöÁêÉÊûÅÊäïÂΩ± stereographic projection Âà∞ y ËΩ¥‰∏ä:\n‰ªé x ËΩ¥ÁöÑ -1 ÁÇπÂá∫ÂèëËøáÂúÜ‰∏äÊØè‰∏™ÁÇπÂÅöÂ∞ÑÁ∫øÔºå‰∏é y ËΩ¥ÁöÑ‰∫§ÁÇπÂç≥ÊòØÂúÜ‰∏äÁÇπÁöÑÊäïÂΩ±Ôºå\nÂú®ËøôÊù°ÊäïÂΩ±Á∫ø‰∏äÔºåÈù†Ëøë 0 ÁöÑÁÇπÂØÜÈõÜÔºåË∂äÂæÄÂ§ñË∂äÁ®ÄÁñèÔºå-i Âíå i ÊòØÁúüÂÆûÁöÑÂúÜ‰∏äÁöÑÁÇπÔºà‰ª£Ë°®ÁùÄ x ‰∏∫ 0 ÁöÑÂúÜ‰∏äÁÇπÔºâÔºå(-i,i) ‰πãÈó¥ÊòØÂè≥ÂçäÂúÜÁöÑÊäïÂΩ±Ôºõ\nÂ∑¶‰πò i Â∞±ÊòØÂúÜÈÄÜÊó∂ÈíàÊóãËΩ¨ 90 Â∫¶ÔºåÂ∞±ÊòØ 1 ‚ûî i ‚ûî -1 ‚ûî -i ÂÅö‰∏ÄÊ¨°Âæ™ÁéØÁßª‰Ωç„ÄÇ\nÊää‰∏âÁª¥ (i-j-ÂÆûËΩ¥) Âçï‰ΩçÁêÉÊäïÂΩ±Âà∞Ê∞¥Âπ≥Èù¢‰∏ä:\n‰ªéÂÆûËΩ¥ÁöÑ -1 ÁÇπÂá∫ÂèëËøûÊé•ÁêÉÈù¢‰∏äÂêÑÁÇπÔºå‰∏é i-j Âπ≥Èù¢‰∫§ÁÇπÂç≥‰∏∫ÊäïÂΩ±ÁÇπÔºå\nËµ§ÈÅìÊòØÁúüÂÆûÁöÑÁêÉÈù¢‰∏äÁöÑÁÇπÔºà‰ª£Ë°®ÁùÄÂÆûÈÉ®‰∏∫ 0 ÁöÑ‰∏âÁª¥Â§çÊï∞ÔºâÔºåËµ§ÈÅì‰πãÂÜÖÊòØÂåóÂçäÁêÉÁöÑÊäïÂΩ±Ôºå‰πãÂ§ñÊòØÂçóÂçäÁêÉÁöÑÊäïÂΩ±Ôºå Ëµ§ÈÅì‰∏§‰æßÊòØÂØπÁß∞ÁöÑÔºåÊûÅÁÇπÈôÑËøëÁöÑÊäïÂΩ±ÁÇπÂØÜÈõÜÔºõ\nÂ∑¶‰πò i Â∞±ÊòØÁêÉÂú® i ËΩ¥ÊñπÂêëÈ°∫Êó∂ÈíàÊóãËΩ¨ 90¬∞ÔºåËµ§ÈÅìÂíåÈõ∂Â∫¶ÁªèÁ∫øÂÖ±ÂêåÂú® 1-i Âπ≥Èù¢ÂÜÖÂÆåÊàê‰∫Ü‰∏ÄÊ¨° 1 ‚ûî i ‚ûî -1 ‚ûî -i Âæ™ÁéØÁßª‰ΩçÔºå ‰πò‰ª• j Â∞±ÊòØÁêÉÂú® j ËΩ¥ÊñπÂêë‰∏äÈ°∫Êó∂ÈíàÔºåÂú® 1-j Âπ≥Èù¢ÂÜÖÂÆåÊàê‰∏ÄÊ¨° 1‚ûî j ‚ûî -1 ‚ûî -j Âæ™ÁéØÁßª‰Ωç\nÊääÂõõÁª¥ (i-j-k-ÂÆûËΩ¥) Âçï‰ΩçË∂ÖÁêÉÊäïÂΩ±Ôºå\n‰ªéÂÆûËΩ¥ÁöÑ -1 ÁÇπÂá∫ÂèëËøûÊé•Ë∂ÖÁêÉÈù¢‰∏äÂêÑÁÇπÔºå‰∏éÊüê‰∏™Âπ≥Èù¢ÁöÑ‰∫§ÁÇπÂΩ¢Êàê‰∏Ä‰∏™‰∏™ÁêÉÈù¢ÔºåÊØè‰∏™ÁêÉÈù¢Â∞±ÊòØÊØè‰∏™ÂõõÂÖÉÊï∞ÁöÑÊäïÂΩ±Ôºå\nÂú® i-j-k Á©∫Èó¥ÁöÑ‰∏âÁª¥Âçï‰ΩçÁêÉÊòØÁúüÂÆûÁöÑË∂ÖÁêÉÈù¢‰∏äÁöÑÁÇπÔºà‰ª£Ë°®ÁùÄÊâÄÊúâÂÆûÈÉ®‰∏∫ 0 ÁöÑÂõõÂÖÉÊï∞ÔºâÔºå Âçï‰ΩçÁêÉÂÜÖÊòØÂÆûÈÉ®‰∏∫ 0-1 ‰πãÈó¥ÁöÑÂõõÂÖÉÊï∞ÁöÑÊäïÂΩ±ÔºåË¥üÊï∞ÂÆûÈÉ®ÁöÑÂõõÂÖÉÊï∞Ë¢´ÊäïÂΩ±Âà∞‰∫ÜÂçï‰ΩçÁêÉÂ§ñÔºåÂÆûÈÉ®=-1 ÁöÑÂõõÂÖÉÊï∞Âú®Êó†Á©∑ËøúÂ§Ñ(Êàë‰ª¨Áúã‰∏çÂà∞)Ôºå\nÊàë‰ª¨ËÉΩÁúãÂà∞ÁöÑÊäïÂΩ±ÁÇπÈÉΩÊúâÁõ∏ÂêåÁöÑÊ®°ÈïøÔºàÔºü‰∏çÊáÇÔºâÔºåÂõõÂÖÉÊï∞ÊóãËΩ¨Êó∂ÔºåÂ∞±ÊòØ‰∏â‰∏™Áª¥Â∫¶ÂÖ±ÂêåÁßª‰ΩçÔºå\n(2023-11-07) Â¶ÇÊûúÁªòÂà∂Âá∫Êù•ÁöÑ‰∏âÁª¥ÁêÉ‰ª£Ë°® i,j,k ‰∏â‰∏™ËôöËΩ¥ÔºåÈÇ£ÂÆûËΩ¥Â∞±Áîª‰∏çÂá∫Êù•‰∫ÜÔºåÂè™ËÉΩÂÖàÂõ∫ÂÆöÂÆÉÂÜçÂàÜÊûê„ÄÇ ÔºàÊää‰ª•‰∏ä‰∏§ÁßçÊÉÖÂÜµÂêàËµ∑Êù•:Ôºâi ÊòØÂúÜÁöÑÁêÉÊûÅÊäïÂΩ±Ôºå\u0026lsquo;j+k\u0026rsquo; ÊòØÁêÉÁöÑÁêÉÊûÅÊäïÂΩ±Ôºå i ÁöÑÁßª‰ΩçÈ°∫Â∫èÊòØ 1,i,-1,-iÔºå\u0026lsquo;jk\u0026rsquo; ÁöÑÁßª‰ΩçÈ°∫Â∫èÊòØ j,k,-j,-k„ÄÇ i ‰∏é jk Êª°Ë∂≥Âè≥ÊâãÂÆöÂàôÔºöÊãáÊåáÊåáÂêë iÔºåÂàôÂõõÊåáÂç∑Êõ≤ÊñπÂêëÂ∞±ÊòØ jk ÊóãËΩ¨ÊñπÂêëÔºàÊãáÊåáÊåáÂêë jÔºåk ‚ûî i ‚ûî -k ‚ûî -i Âæ™ÁéØÁßª‰ΩçÔºâ\nÂõõÂÖÉÊï∞ÁöÑÂèØËßÜÂåñ-3B1B + KrasjetËØÑËÆ∫\n(2023-11-07)\n3D Rotation Because projection has been performed, the real axis is collapsed.\nSo when \u0026ldquo;rotating\u0026rdquo;Ôºåthe real axis is fixed and only imaginary axes rotate.\nTherefore, a 4-D quaternion can be used to describe the rotation in 3D space through its mutable 3 imaginary axes.\nCompared with Euler angles that define the hierarchical order of three angles, quaternion depicts the evolution of a single 4D vector. Convert a quaternion to a 3x3 rotation matrix\n","date":"2022-08-31T22:00:00Z","image":"https://img.youtube.com/vi/d4EgbgTm0Bg/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/calc/quaternion/","title":"memo: Calc | Quaternion"},{"content":"Define multi layers by for loop 1 2 3 4 5 6 for i in range (n_layers): setattr(self, f\u0026#34;layer{i}\u0026#34;, nn.Linear(2, n_hidden_units), nn.ReLU(True)) # ÂèñÂá∫Â§öÂ±ÇÔºö for i in range(n_layers): layer = getattr(self, f\u0026#34;layer{i}\u0026#34;) Access all weights of a model 1 for name, param in model.named_parameters: Refer:\npytorchÊïôÁ®ã‰πãnn.ModuleÁ±ªËØ¶Ëß£‚Äî‚Äî‰ΩøÁî®ModuleÁ±ªÊù•Ëá™ÂÆö‰πâÊ®°Âûã- CSDN Initialize weights of nn.Linear model.apply(fn) will apply function fn to every children submodule. Therefore, let fn be init_weights()\n1 2 3 4 5 6 7 8 9 10 @torch.no_grad() # this func won\u0026#39;t create graph def init_weights(m): print(m) if type(m) == nn.Linear: torch.nn.init.ones_(m.weight) m.bias.data.fill_(0.01) print(m.weight) model = Net() model.apply(init_weights) Refer:\nHow to initialize weights in PyTorch? - StackOverflow init_weights Example code ModuleList usage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026#39;\u0026#39;\u0026#39; ModuleList is like a iterator \u0026#39;\u0026#39;\u0026#39; class LinearNet(nn.Module): def __init__(self, in_features, out_features, num_layers, layer_size): super(LinearNet, self).__init__() self.linears = nn.ModuleList([nn.Linear(in_features, layer_size)]) # 1st layer self.linears.extend([nn.Linear(layer_size, layer_size) for i in range(1, num_layers-1)]) self.linears.append(nn.Linear(layer_size, out_features)) def forward(self, x): for idx, layer in enumerate(self.linears): x = layer(x) # x = self.linears[idx // 2](x) + layer(x) # pass x through each layer return x Refer:\nWhen should I use nn.ModuleList and when should I use nn.Sequential? torch.Tensor ÂåÖÂê´Âçï‰∏ÄÊï∞ÊçÆÁ±ªÂûãÂÖÉÁ¥†ÁöÑÂ§öÁª¥Áü©Èòµ Êúâ10ÁßçÂº†ÈáèÁ±ªÂûãÔºåtorch.TensorÊòØÈªòËÆ§Âº†ÈáèÁ±ªÂûãtorch.FloatTensorÁöÑÂà´Âêç Note Âº†ÈáèÂèòÂºÇÊñπÊ≥ïÈÉΩÂ∏¶Êúâ‰∏ãÂàíÁ∫øÂêéÁºÄ,ÂÆÉ‰ª¨Áõ¥Êé•ÂéüÂú∞‰øÆÊîπÂéüÂº†ÈáèÁöÑÂ±ûÊÄßÔºåËÄå‰∏çÊ¥æÁîüÊñ∞Âº†Èáè„ÄÇ‰æãÂ¶Çtorch.FloatTensor.abs_()Áõ¥Êé•ËÆ°ÁÆóÂπ∂‰øÆÊîπÂéüÂº†ÈáèÔºåËÄåtorch.FloatTensor.abs()Âú®Êñ∞Âº†Èáè‰∏≠ËÆ°ÁÆóÁªìÊûú„ÄÇ\nGet value of a tensor a = torch.tensor([3])\na.data ‚Æï tensor([3]) a.item() ‚Æï 3 (\u0026ldquo;Convert one-element tensor to python scalar.\u0026rdquo;) a.numpy() ‚Æï array([3]) b = (a/2).requires_grad_(True)\nb.detach().cpu().numpy() ‚Æï array([1.5], dtype=float32) (2023-07-27)\nMemory allocated for a tensor 1 2 a = torch.arange(4) # torch.int64 Num_bytes = a.element_size() * a.nelement() # 8 * 4 How to know the memory allocated for a tensor on gpu?\nMove specified axis Move the specified axis to the second axis\nQuestion for ChatGPT:\nIf I was given a variable channel_dim, and it can be 3, that means the tensor is (B, H, W, channel_dim), so how can I transform the tensor to make the channel_dim the 2nd axis?\nI mean the channel_dim is a variable, it can be any axis.\nAnswer:\nApologies for the confusion. If the channel_dim is a variable and can be any axis, you can use the following code snippet to move the specified axis to the second axis of the tensor:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch # Assuming you have a tensor with size (B, H, W, C) and channel_dim can be any axis tensor = torch.randn(B, H, W, C) channel_dim = 2 # Replace this with the desired axis you want to move to the second position # Permute the tensor to move the specified axis (channel_dim) to the second axis # Generate the permutation order: (0, channel_dim, 1, 2, ..., channel_dim-1, channel_dim+1, ..., C-1) perm_order = list(range(len(tensor.shape))) perm_order.remove(channel_dim) perm_order = [0, channel_dim] + perm_order[1:] tensor_permuted = tensor.permute(*perm_order) # The resulting tensor will have size (B, C, H, W) if channel_dim was initially the third axis (2) # The resulting tensor will have size (B, W, C, H) if channel_dim was initially the second axis (1), and so on. print(tensor_permuted.size()) (2023-08-01)\nDynamically change Conv2d layer Question of chatGPT:\nI\u0026rsquo;m writing pytorch. I want to use nn.Conv2d, but the in_channels is the result generated in the forward method, how could I make the parameter of Conv2d optimized during training?\nMake the in_channels a instance variable, like self.in_chnls = in_channels.\nThen in the __init__(self, in_channels) method, a Conv2d layer can be construct: self.conv = nn.Conv2d(self.in_chnls, out_chnls, ...)\nThen update it in the forward before calling self.conv layer: self.in_chnls = x.shape[1]\n(DDG search: \u0026ldquo;use nn.Conv2d with dynamically determined in_channels during the forward pass\u0026rdquo;)\nSimilar question: Dynamically set Conv2d based on input channels - PyTorch Forum\nHow to create a custom layer that accepts the input during forward pass - PyTorch forum\nMake other class (nn.Module), in which the prefix model is called. But the in_channels is required when initializing Conv2d(), so the parameters of it can be defined. \u0026ldquo;Can we not define the filter size at runtime?\u0026rdquo; Why does nn.Conv2d require in_channels? - PyTorch Forum\nnn.LazyConv2d(out_channels, ...) doesn\u0026rsquo;t need in_channels. Docs torch.nn.modules.lazy.LazyModuleMixin Get device of a module 1 dev = next(model.parameters()).device How to get the device type of a pytorch module conveniently?\nModule.apply(fn) Docs Example - AIM Apply the function fn to each submodule and the Module itself. This can be used to initialize weights, like the code in AIM:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class ViT_CLIP(nn.Module): def __init__(self,): self.__init__() def init_weights(self,): # A class method def _init_weights(m): # Pass a submodule if isinstance(m, nn.Linear): trunc_normal_(m.weight, std=.02) if isinstance(m, nn.Linear) and m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.LayerNorm): nn.init.constant_(m.bias, 0) nn.init.constant_(m.weight, 1.0) # Call _init_weights self.apply(_init_weights) Customize Autograd Op (2024-01-23)\nThe .apply() method of a customized operation that subclasses autograd.Function requires forward and backward to be static methods. Docs\nExample - 3DGS\ntorch.roll Circular shift (\u0026ldquo;Âæ™ÁéØÁßª‰Ωç\u0026rdquo;) along some dimensions. Docs\nExample - AIM - swin2d Move 1 step along 1 dimension:\n1 2 x = torch.tensor([[1,2,3],[4,5,6]]) torch.roll(x, shifts=1, dims=0) It will shift 1 element in the dimension 0, i.e., [4,5,6] to the next position, so x will become: [[4,5,6],[1,2,3]]\nMove steps (2, 1) along 2 dimensions separately:\n1 torch.roll(x, shifts=(2,1), dims=(0,1)) The dimension 0 will shift twice, and the dimension 1 will shift once. So x becomes: [[3,1,2],[6,4,5]]\ntorch.max torch.max(x, dim) compares each atom element on the equal position according to the dimension dim\n1 2 3 4 5 6 7 8 a = torch.randint(8, (2,3,3) tensor([[[0, 3, 2], [6, 1, 5], [2, 7, 0]], [[4, 7, 4], [6, 1, 0], [0, 1, 6]]]) torch.max(a, 0) will compare: 0-4, 3-7,, 2-4; 6-6, 1-1, 5-0; 2-0, 7-1, 0-6, so the result is [[4,7,4] [6, 1, 5] [2, 7, 6].\ntorch.max(a,1) will compare: 0-6-2, 2-1-7, 2-5-0; 4-6-0, 7-1-1, 4-0-6; then the result is [[6,7,5], [6,7,6]]\ntorch.diff The back one minus the front one.\n1 2 3 4 5 6 a = torch.arange(12, (2,2,3)) tensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) torch.diff(a): the last dim subtract [ [ [1-0, 2-1], [4-3, 5-4]]; [7-6, 8-7], [10-9, 11-10] ] = [ [ [1,1], [1,1] ]; [1,1], [1,1] ] ]\ntorch.diff(a, n=2): do again for 1st-time result: [ [ [0], [0]; [[0],[0]]]\ntorch.diff(a, append=a), the append tensor needs to have the same dimensions of input.\nCount #params From Match-NeRF:\n1 2 3 4 5 for name, param in self.model.named_parameters(): log.info(f\u0026#39;{name}: {param.requires_grad}\u0026#39;) num_param = sum(p.numel() for p in self.model.parameters() if p.requires_grad) num_total_param = sum(p.numel() for p in self.model.parameters()) log.info(\u0026#39;Number of total parameters: {}, tunable parameters: {}\u0026#39;.format(num_total_param, num_param)) (2023-12-21) From MVSNet:\n1 sum([p.data.nelement() for p in model.parameters()]) param_group (2024-04-11)\nPer-parameter options: an iterable of parameter groups. Docs\nHave practiced in MatchNeRF exp1 before: different modules are trained with separate LRs. Example in 3DGS\ntorch.unbind Docs\n(2024-05-17)\nBreak one dimension apart.\n1 2 3 4 a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.unbind(a) # out a tuple: ([1,2,3], [4,5,6], [7,8,9]) torch.chunk can get the same effect, but it requires specifying the number of chunks:\n1 2 \u0026gt;\u0026gt;\u0026gt; torch.chunk(a, 3, dim=0) (tensor([[1, 2, 3]]), tensor([[4, 5, 6]]), tensor([[7, 8, 9]])) In contrast, torch.split requires specifying the number of entries contained in a chunk:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;\u0026gt;\u0026gt; torch.split(a, 3, dim=0) # return a tuple (tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),) \u0026gt;\u0026gt;\u0026gt; torch.split(a, 2, dim=0) (tensor([[1, 2, 3], [4, 5, 6]]), tensor([[7, 8, 9]])) \u0026gt;\u0026gt;\u0026gt; torch.split(a, 1, dim=0) (tensor([[1, 2, 3]]), tensor([[4, 5, 6]]), tensor([[7, 8, 9]])) ","date":"2022-08-26T20:25:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_misc/","title":"memo: PyTorch | Misc"},{"content":"(2022-12-28)\nÁôΩÊùøÊé®ÂØº: VAE Source videoÔºö„ÄêÊú∫Âô®Â≠¶‰π†„ÄëÁôΩÊùøÊé®ÂØºÁ≥ªÂàó(‰∏âÂçÅ‰∫å) ÔΩû ÂèòÂàÜËá™ÁºñÁ†ÅÂô®(VAE)„Äë\nVAE Âíå GMM ‰∏ÄÊ†∑‰πüÊòØÁîüÊàêÊ®°ÂûãÔºöÈöêÂèòÈáè z ÁîüÊàêËßÇÊµãÂèòÈáè xÔºå‰ªéËÄåÂ≠¶‰π†Ê†∑Êú¨Êï∞ÊçÆ x Êú¨Ë∫´ÁöÑÂàÜÂ∏É„ÄÇ‰∏çËøá VAE ÁöÑ z ‰∏çÊòØ1Áª¥ÁöÑÔºåËÄåÊòØÂ§öÁª¥ÁöÑ„ÄÇ\nÁî® EM Ëß£ GMM ÁöÑÊúÄ‰ºòÂèÇÊï∞ Œ∏ Êó∂ÔºåÂÅáËÆæ z ÁöÑÂêéÈ™åÂàÜÂ∏É P(z|x;Œ∏‚ÅΩ·µó‚Åæ) ËÉΩÂ§üÂèñÂà∞ÔºåËÄå‰∏îÂõ†‰∏∫ z ÊòØÁ¶ªÊï£ÁöÑÔºåP(x) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑P(x,z=C‚Çñ;Œ∏) Â∞±ËÉΩÁÆóÂá∫Êù•Ôºå ÊâÄ‰ª•ÂêéÈ™å P(z|x)=P(x|z)P(z)/P(x) ‰πüËÉΩÁÆóÂá∫Êù•ÔºåÂÖ∂‰∏≠ÂàÜÂ≠ê‰∏§È°πÊòØÂÅáËÆæÁöÑÂàÜÂ∏É„ÄÇEÊ≠•ÊääÁõÆÊ†áÂáΩÊï∞: \u0026ldquo;ÊúÄÂ§ßÁöÑÊúüÊúõ\u0026rdquo; $E_{P(z|x)} [ log P(x,z|Œ∏) ]$ ÂÜôÂá∫Êù•ÔºåMÊ≠•ÂØπÂÖ∂Ê±ÇÂØºÊâæÂá∫ÔºàÊúüÊúõÊúÄÂ§ßÊó∂ÁöÑÔºâÊúÄ‰Ω≥Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ„ÄÇ\nËÄå VAE ÁöÑ z ÊòØÈ´òÁª¥ËøûÁª≠ÁöÑÔºåP(x) = $‚à´_z P(x,z) dz$ ÁßØ‰∏çÂá∫Êù•ÔºåÂêéÈ™åP(z|x) Â∞±Ê≤°Ê≥ïÁî®Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÂØºÂá∫Ôºå‰ΩÜÂèØ‰ª•Áî®ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÂèòÂàÜÊé®Êñ≠(SGVI)Ëøë‰ººÂêéÈ™å„ÄÇ\n‚ÄúÊé®Êñ≠‚ÄùÊòØ‰ªé x Âà∞ z ÁöÑËøáÁ®ãÔºöÁî®Ê†∑Êú¨ x ‰øÆÊ≠£ z ÁöÑÂÖàÈ™å P(z) ÂæóÂà∞ z ÁöÑÂêéÈ™å P(z|x)ÔºõËÄå‚ÄúÁîüÊàê‚ÄùÊòØ‰ªé z ÁöÑÂêéÈ™åÂàÜÂ∏É P(z|x) ‰∏≠ÈááÊ†∑Âá∫ zÔºåÂÜçÂèòÊç¢ÊàêÊ†∑Êú¨ÁöÑÂàÜÂ∏É P(x)„ÄÇ\nflowchart LR; observed((x)) --\u003e|Inference| latent((z)); latent --\u003e|Generation| observed Áî® q(z|x;œï) ÈÄºËøëÂêéÈ™å P(z|x;Œ∏) Êó∂ÔºåË¶ÅÊúÄÂ∞èÂåñ $KL(q_{œï(z|x)} || P_{Œ∏(z|x)})$ÔºåÂú® Œ∏ Âõ∫ÂÆöÔºåÂàô‰ººÁÑ∂ P(X) ‰πüÂõ∫ÂÆöÁöÑÊÉÖÂÜµ‰∏ãÔºåÁ≠â‰ª∑‰∫éÊúÄÂ§ßÂåñ ELBOÔºö arg max $E_{q·µ©(z|x)} [ log (P(z,x;Œ∏)/q·µ©(z|x)) ]) ]$„ÄÇÁ±ª‰ººÂπø‰πâ EM ÁöÑ EÊ≠•ÔºåÂú® Œ∏ Âõ∫ÂÆöÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ±ÇÂêéÈ™åÁöÑËøë‰ºº„ÄÇ\nÈÄöËøáÊääËÅîÂêàÊ¶ÇÁéáÊãÜÂºÄÔºåËøô‰∏™ ELBO ‰πüÂèØÂÜôÊàê: $E_{q·µ©(z|x)} [ log P(x|z;Œ∏) ] - KL(q(z|x;œï) ||P(z))$„ÄÇ ÂΩì z ÁöÑÂÖàÈ™åÂàÜÂ∏É P(z) ÊòØÊ†áÂáÜÊ≠£ÊÄÅÊó∂ÔºåÈÇ£‰πàKLÊï£Â∫¶Â∞±ÊòØÂ∏åÊúõ q·µ©(z|x) ÁöÑÊñπÂ∑Æ‰øùÊåÅ‰∏∫IÔºå‰∏çË¶ÅÂ§™Â∞èÔºåÂàÜÂ∏ÉÂùçÁº©Âà∞‰∏ÄÁÇπÂ∞±ÊòØËøáÊãüÂêàÔºöx‰∏éz‰∏Ä‰∏ÄÂØπÂ∫îÔºåÂ∞±ÂèòÊàê AE‰∫ÜÔºåÂè™ËÉΩÂØπËÆ≠ÁªÉÊ†∑Êú¨ x Êé®Âá∫Ê≠£Á°ÆÁöÑ z„ÄÇ\nÊ±ÇÁîüÊàêÊ®°ÂûãÁöÑÊúÄ‰ºòÂèÇÊï∞ Œ∏ ÂêåÊ†∑ÊòØË¶Å‰Ωø‰ººÁÑ∂ÊúüÊúõÊúÄÂ§ßÔºöarg max ELBOÔºàxÂêéÈ™å‰∏éKLÊï£Â∫¶ÂêåÊó∂‰ºòÂåñÔºâÔºåÔºàÊ≤°Ê≥ïÁõ¥Êé•Ê±ÇÂØºÔºâÂèØÈááÁî®Ê¢ØÂ∫¶‰∏äÂçáÊ≥ïÔºåÊâÄ‰ª• VAE ÊòØÊää EM ÁöÑ‰∏§Ê≠•ÂêàËµ∑Êù•‰∫ÜÔºåÊó¢ÈÄºËøëÂêéÈ™å p(z|x) ÁöÑÂèÇÊï∞ œïÔºåÂèàÈÄºËøëÁîüÊàêÊ®°Âûã p(x|z) ÁöÑÂèÇÊï∞Œ∏„ÄÇ\nÂú®ËÆ°ÁÆó ELBO ÂØπ œï ÁöÑÊ¢ØÂ∫¶Êó∂Ôºå‚àá·µ©L(œï) ÂèØ‰ª•ÂÜôÊàê‰∏Ä‰∏™ÊúüÊúõÔºåÁõ¥Êé•ÂØπ z ÈááÊ†∑Ê±ÇÂùáÂÄºÂèØËÉΩÁî±‰∫éÊñπÂ∑ÆÂ§™Â§ßËÄåÂ§±ÊïàÔºàËÄå‰∏îÈááÊ†∑Êìç‰Ωú‰∏çÂèØÂØºÔºåÂ∞±Êó†Ê≥ïÂØπzÊ±ÇÊ¢ØÂ∫¶ÔºâÔºå ÊâÄ‰ª•ÂÖàÂØπ‰∏Ä‰∏™È´òÊñØÂô™Â£∞ Œµ ÈááÊ†∑ÔºåÂÜçÊ†πÊçÆÂèòÊç¢Ôºöz=Œº_œï(x) + Œ£_œï¬π·êü¬≤(x)‚ãÖŒµ ÂæóÂà∞ z (ÈáçÂèÇÊï∞Âåñ)„ÄÇÁÑ∂ÂêéÊ±ÇÂùáÂÄº(=Ê¢ØÂ∫¶)ÔºåÂ∏¶ÂÖ•Ê¢ØÂ∫¶‰∏äÂçáÂÖ¨ÂºèÔºåÊõ¥Êñ∞œï„ÄÇ\nËÆ≠ÁªÉNNÊó∂ÔºåËæìÂÖ•‰∏Ä‰∏™ xÔºåÁ•ûÁªèÁΩëÁªúËæìÂá∫ÂêéÈ™åÂàÜÂ∏ÉÔºàÁºñÁ†ÅÔºâp(z|x) ‰ª•ÂèäÈááÊ†∑Âá∫‰∏Ä‰∏™z„ÄÇÁî®Ëøô‰∏™z ÈÄöËøáÂè¶‰∏Ä‰∏™ÁΩëÁªúÈÄºËøë x ÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºàÁîüÊàêÊ®°ÂûãÔºâp(x|z)Ôºå ‰πüÂ∞±ÊòØÂú®Â≠¶Âà∞ÁöÑ z ÊàêÁ´ãÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªé p(x|z) ‰∏≠ÈááÂà∞ x ÁöÑÊ¶ÇÁéáÔºåÁõÆÊ†áÂáΩÊï∞Â∞±ÊòØÂ∏åÊúõËøô‰∏™Ê¶ÇÁéáË∂äÂ§ßË∂äÂ•ΩÔºåÂèØ‰ª•ÂÅáËÆæ xÊúç‰ªé‰∫åÈ°πÊàñÊ≠£ÊÄÅÔºåÊääÂèÇÊï∞‰ª£ÂÖ•ÂÖ¨ÂºèÂç≥ÂæóÊ¶ÇÁéá ÊàñËÄÖËØ¥ÔºåÂàÜÂ∏É p(x|z) ÁöÑ\u0026quot;‰ºóÊï∞\u0026quot; x\u0026rsquo;Ë¶Å‰∏éËæìÂÖ• x ÁöÑË∑ùÁ¶ªË∂äÂ∞èË∂äÂ•ΩÔºåÂΩìÊñπÂ∑ÆÂæàÂ∞èÊó∂Ôºå‰ºóÊï∞Â∞±ÊòØÊúüÊúõÔºåÂç≥ÊØèÊ¨°ÈááÊ†∑ÈÉΩ‰ºöÈááÂà∞ÊúüÊúõ Œº(z)ÔºåÊâÄ‰ª•‰ª• Œº(z) ‰∏é x ÁöÑË∑ùÁ¶ª‰Ωú‰∏∫ÁõÆÊ†áÂáΩÊï∞„ÄÇ ‰∏Ä‰∏™xÊòØÂ§öÁª¥ÁöÑÔºåÂÆÉÊúâËá™Â∑±ÔºàÁª¥Â∫¶‰πãÈó¥ÔºâÁöÑÂàÜÂ∏É„ÄÇ\nflowchart LR subgraph Encoder x(\"Input x\") --\u003e net(\"NN-œï\") --\u003e a(\"Œº(x)\") \u0026 b(\"Œ£(x)\") end subgraph Decoder a --\u003e o((\"+\")) --\u003e z(\"sampled z \\n from N(Œº(x),Œ£(x))\") --\u003e net2(\"NN-Œ∏\\n(Œº(z))\") --\u003er(\"reconstructed\\n x'\") b --\u003e m((\"√ó\")) --\u003e o Œµ(\"ŒµÔΩûN(0,ùêà)\") --\u003e m end ÂÆåÊï¥Á¨îËÆ∞Ôºöshuhuai008-32Ôºõ ÂèÇËÄÉÔºöÈöêÂèòÈáèÊ®°ÂûãÂà∞EMÂà∞ÂèòÂàÜËá™ÁºñÁ†ÅÂô® - ÊàëË¶ÅÁªô‰∏ªÊí≠ÁîüÁå¥Â≠ê -Áü•‰πé\n(2022-12-29)\nËãèÂâëÊûó: VAEÔºà‰∏ÄÔºâ Source articleÔºöÂèòÂàÜËá™ÁºñÁ†ÅÂô®Ôºà‰∏ÄÔºâÔºöÂéüÊù•ÊòØËøô‰πà‰∏ÄÂõû‰∫ã-ËãèÂâëÊûó\nÈÄöËøáÈöêÂèòÈáè z Ê±ÇÊ†∑Êú¨ x ÁöÑÂàÜÂ∏ÉÔºöp(x) = ‚à´_z p(x|z) p(z) dz„ÄÇ ÂÖàÂ≠¶‰π† z ÁöÑÂêéÈ™åÂàÜÂ∏É p(z|x)Ôºà ÁºñÁ†Å ÔºâÔºåÂÜçÁî±ÂÆÉ ÁîüÊàê $\\^x$Ôºå$\\^x$Â∫î‰∏éx‰∏ÄÊ†∑„ÄÇ\nflowchart LR sample(x) --\u003e|\"N(Œº·∂ø,Œ£·∂ø)\"| latent(\"p(z|x)\") --\u003e|\"sampling z \\n P(x)=Œ£p(x|z)‚ãÖp(z)Ôºü\"| recon(x^) sample -.-|=| recon Ê≥®ÊÑèÂå∫ÂàÜÂÖàÈ™å p(z) ‰∏éÂêéÈ™å p(z|x)„ÄÇÂõ†‰∏∫ÂÅáËÆæ‰∫ÜÂêéÈ™åÊòØÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÂΩ¢ÂºèÔºåÊâÄ‰ª•ÊòØÂØπ Œº·∂ø,Œ£·∂ø ÂÅöÈáçÂèÇÊï∞Âåñ„ÄÇËÄåÂÖàÈ™åÂè™Âá∫Áé∞Âú®Ê≠£ÂàôÂåñÈ°π‰∏≠Ôºå‰∏çÂèÇ‰∏éÂêéÈ™åÁöÑËÆ≠ÁªÉ„ÄÇ\n‰∏Ä‰∏™Ê†∑Êú¨ÁÇπx‚ÅΩ‚Å±‚ÅæÂØπÂ∫î‰∏Ä‰∏™ÔºàÁã¨Á´ãÁöÑ„ÄÅÂ§öÂÖÉÁöÑÔºâÂêéÈ™åÂàÜÂ∏É p(z|x‚ÅΩ‚Å±‚Åæ)ÔºåËøôÊ†∑‰ªé‰∏≠ÈááÂá∫ÁöÑ z Â∞±‰∏ÄÂÆöÂØπÂ∫îËøô‰∏™Ê†∑Êú¨ÁÇπ„ÄÇÊâÄ‰ª•ÊØè‰∏™Ê†∑Êú¨ÊúâËá™Â∑±ÁöÑÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ\nz ÊòØÂêéÈ™åÂàÜÂ∏ÉÁöÑ‰∏Ä‰∏™ÈááÊ†∑ÔºåÈááÊ†∑Â∞±‰ºöÊúâÂÅèÂ∑ÆÔºàÊñπÂ∑ÆÔºâÔºåÂØºËá¥ÈáçÊûÑËØØÂ∑Æ‰∏ç‰∏∫0„ÄÇÂ¶ÇÊûú‰∏çÂä†Ê≠£ÂàôÂåñÔºå‰∏∫‰∫ÜÂáèÂ∞èËØØÂ∑ÆÔºåŒ£‰ºö‰∏çÊñ≠ÂáèÂ∞èÂà∞0ÔºåÈÄÄÂåñÊàêAE„ÄÇ ‰ªéËøô‰∏™ËßíÂ∫¶Áúã vae ÊòØ AE Âä†‰∏äÂô™Â£∞ÔºåÂπ∂Á∫¶ÊùüÂô™Â£∞ÁöÑÂº∫Â∫¶ÔºàÊñπÂ∑ÆÔºâÂ∞ΩÈáè‰∏∫1.\nvae Â∏åÊúõÊâÄÊúâÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºà‚Äù‰∏ÄËà¨Ê≠£ÊÄÅ‚ÄúÔºâÈÉΩ‰∏éÊ†áÂáÜÊ≠£ÊÄÅÁõ∏‰ººÔºöŒº=0ÔºåŒ£=IÔºåÈááÊ†∑zÊó∂Â∞±‰øùËØÅ‰∫ÜÁîüÊàêËÉΩÂäõ„ÄÇÂõ†‰∏∫ÂêÑÂàÜÈáèÁã¨Á´ãÔºåÊâÄ‰ª•ÊòØdÁª¥‰∏ÄÂÖÉNÁöÑÂä†ÂíåÔºö KL( N(Œº,Œ£)|| N(0,I) ) = ¬Ω Œ£ [(-logœÉ¬≤ + Œº¬≤ + œÉ¬≤ -1) ]\nÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßÊääÊúç‰ªéN(Œº,Œ£)ÁöÑÈöèÊú∫ÂèòÈáè z ÁöÑÊ¶ÇÁéáÊãÜÊàê‰∏Ä‰∏™Êúç‰ªéÊ†áÂáÜÊ≠£ÊÄÅÁöÑÂèòÈáèŒµÂíå‰∏Ä‰∏™ÂèÇÊï∞ÂèòÊç¢Œº+Œµ√óœÉÔºå‰ªéËÄåÂÆûÁé∞ËôΩÁÑ∂ÈááÊ†∑Êìç‰Ωú‰∏çÂèØÂØºÔºå‰ΩÜÂÆÉ‰∏çÂèÇ‰∏éÊ¢ØÂ∫¶ÁöÑÂèçÂêë‰º†Êí≠„ÄÇ\nÊù°‰ª∂VAEÔºöÊ†∑Êú¨Â±û‰∫é‰∏çÂêåÁöÑÁ±ªÂà´-ÊúüÊúõ‰∏çÂêå cvae‰ª£Á†ÅÔºöÁî®2‰∏™Á∫øÊÄßÂ±ÇÂàÜÂà´ÊãüÂêàŒºÂíåŒ£ÔºåÁî®ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßÈááÊ†∑zÔºåx‰∏éx\u0026rsquo;‰πãÈó¥ÁöÑÈáçÊûÑÊçüÂ§±Áî®‰∫Ü‰∫§ÂèâÁÜµ modelÔºö\nflowchart LR x(x\\n original_dim) --\u003e|\"Dense\\n(intermediate_dim)\"| h --\u003e|\"Dense\\n(latent_dim)\"| Œº(\"z_mean\") \u0026 Œ£(\"z_log_var\"); y(y\\n num_classes) --\u003e|\"Dense\\n(latent_dim)\"| yh Œº \u0026 Œ£ --\u003e rp{sampling} --\u003e|reparame\\n terization| z --\u003e|\"Dense\\n (intermediate_dim)\"| h_decoded --\u003e|\"Dense\\n(original_dim)\"| x_decoded_mean (2022-12-30)\nËãèÂâëÊûó: VAEÔºà‰∫åÔºâ ÂéüÊñáÔºöÂèòÂàÜËá™ÁºñÁ†ÅÂô®Ôºà‰∫åÔºâÔºö‰ªéË¥ùÂè∂ÊñØËßÇÁÇπÂá∫Âèë\nÊúüÊúõÁöÑÊï∞ÂÄºËÆ°ÁÆó‰∏éÈááÊ†∑ËÆ°ÁÆó‰∏çÂêåÔºöÊï∞ÂÄºËÆ°ÁÆóÊòØÂÖàÁªô‰∏Ä‰∏™Êï∞Âàó xÔºàÂÖ∂‰∏≠$x‚Å∞ \u0026lt; x¬π \u0026lt; x¬≤\u0026lt;\u0026hellip;x‚Åø$ÔºâÔºåÁÑ∂ÂêéÂØπÈáåÈù¢ÁöÑÊØè‰∏™Êï∞ x‚ÅΩ‚Å±‚Åæ ÊåâÂÆÉÁöÑÊ¶ÇÁéáÂä†ÊùÉÊ±ÇÂíåÔºöE[x]=‚à´ xp(x) dx„ÄÇ ‰ΩÜÂ¶ÇÊûú x‚ÅΩ‚Å±‚Åæ ÊòØÂàÜÂ∏É p(x) ‰∏≠ÁöÑÈááÊ†∑ÔºåÊ¶ÇÁéáÂ§ßÁöÑ‰ºöË¢´Â§öÈááÂá†Ê¨°ÔºåÊ†∑Êú¨ÈõÜÂêà x ‰∏≠Â∞±ÂåÖÂê´‰∫ÜÊ¶ÇÁéá‰ø°ÊÅØÔºå‰∏çÁî®ÂÜç‰πò p(x‚ÅΩ‚Å±‚Åæ)‰∫ÜÔºöE[x]‚âà1/n‚ãÖ‚àë·µ¢‚Çå‚ÇÄ‚Åø x‚ÅΩ‚Å±‚Åæ, x‚ÅΩ‚Å±‚Åæ‚àºp(x)\nÊé®ÂØºÁõÆÊ†áÂáΩÊï∞Êó∂ÔºåÂÖàÊûÑÈÄ†‰∫Ü p(x,z)=p(z|x)‚ãÖp^(x)ÔºåÂÜçÊûÑÈÄ† q(x,z)=q(x|z)q(z)ÔºåËøô‰∏§‰∏™ÊûÑÈÄ†ÊØ´Êó†ÂÖ≥Á≥ªÔºåÂ∏åÊúõÂÆÉ‰ø©‰∫íÁõ∏Èù†ËøëÔºåËÄå‰∏çÊòØ‰∏∫‰∫ÜÈÄºËøëzÁöÑÂêéÈ™åp(z|x)„ÄÇnotes; (vae‰∏â-josh00ÁöÑËØÑËÆ∫)\nÂú®ÁîüÊàêÈò∂ÊÆµÔºåËã•ÂÅáËÆæ p(x|z) Êúç‰ªé‰∫åÈ°πÂàÜÂ∏ÉÔºåÂàôÈáçÊûÑËØØÂ∑ÆÂ∞±ÊòØ‰∫§ÂèâÁÜµÔºõËã•ÂÅáËÆæ p(x|z) Êúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÂàôÈáçÊûÑËØØÂ∑ÆÂ∞±ÊòØMSE\nËÆ≠ÁªÉÊó∂ÔºåÁîüÊàêÈò∂ÊÆµÂè™‰ªé z ÁöÑÂêéÈ™åÂàÜÂ∏É‰∏≠ÈááÊ†∑‰∏Ä‰∏™ÔºåÂõ†‰∏∫ z ÊòØ‰∏ìÂ±û‰∫é‰∏Ä‰∏™x„ÄÇP(x)‚ûî Œº_œï(x), Œ£_œï¬π·êü¬≤(x) ‚ûî z ‚ûî Œº_Œ∏(z) ‚ûî x'\n(2022-12-31)\nËãèÂâëÊûó: VAEÔºà‰∏âÔºâ ÂéüÊñáÔºöÂèòÂàÜËá™ÁºñÁ†ÅÂô®Ôºà‰∏âÔºâÔºöËøôÊ†∑ÂÅö‰∏∫‰ªÄ‰πàËÉΩÊàêÔºü\nvaeÁîüÊàêÊó∂ÔºåÂè™Èáá‰∏Ä‰∏™zÔºöÂõ†‰∏∫x‰∏éz‰∏Ä‰∏ÄÂØπÂ∫îÔºàËá™ÁºñÁ†ÅÂô®ÔºâÊñπÂ∑Æ‰∏∫0ÔºåvaeÂºïÂÖ•‰∫ÜÂÖàÈ™åq(z)=N(0,I)ÔºåÊñπÂ∑Æ‰πü‰∏ç‰ºöÂ§™Â§ßÔºå‰πüÂ∞±ÊòØÊØèÊ¨°ÈááÊ†∑ÔºåÁªìÊûúÈÉΩ‰∏ÄÊ†∑„ÄÇÂ¶ÇÊûúÁõ¥Êé•ÂÅöÊúÄÂ§ß‰ººÁÑ∂p(x|z)ÔºåÂ∞±ÈúÄË¶Å‰ªézÁöÑÂÖàÈ™åp(z)‰∏≠ÈááÂ§ö‰∏™Ê†∑Êú¨ÂÖà‰º∞ËÆ°Âá∫ÊØè‰∏™xÁöÑ‰ººÁÑ∂ÔºåÂÜçÊ±Ç‰ººÁÑ∂ÁöÑÊúüÊúõÊúÄÂ§ßÂåñ„ÄÇ‰ΩÜÂ¶ÇËøáÊ≤°ÈááÂà∞ z‚ÇñÔºåÂÆÉÂØπÂ∫îÁöÑ x‚ÇñÁöÑ‰ººÁÑ∂Â∞±ÊòØ0Ôºåln0ÊòØ-‚àûÔºåÂØºËá¥ËÆ≠ÁªÉÂ§±Ë¥•„ÄÇ\nVAE ÁöÑÈáçÂª∫ÁîüÊàêÁõ∏ÂΩì‰∫éÂú®AE‰∏äÂä†‰∫ÜÂô™Â£∞ÔºàÊñπÂ∑ÆÔºâÔºåÊâÄ‰ª•ÂèØ‰ª•ÁîüÊàê‰∏éÂéüÂßãÊ†∑Êú¨‰∏çÂêåÁöÑÊï∞ÊçÆ„ÄÇ\n%%{ init: { 'flowchart': { 'curve': 'linear' } } }%% flowchart TB x[\"x\\n(Ê†∑Êú¨)\"] --\u003e nn[\"ÈöêÂèòÈáèÁöÑÂàÜÂ∏É Œº(x), Œ£(x)\"] --\u003e z --\u003e nn2[\"Êï∞ÊçÆÁöÑÂàÜÂ∏É p(x|z)\\n ÊñπÂ∑ÆÂæàÂ∞è\"] --\u003e recon[x'\\nÈáçÂª∫Ê†∑Êú¨] nn --\u003e |\"ÈÄöËøáÈááÊ†∑Ôºå‰ªé ‚ÄúÂ§ö‰∏™‚Äù Âà∞ ‚Äú‰∏Ä‰∏™‚ÄùÔºå\n‰ªé ‚ÄúÊó†Èôê‚Äù Âà∞ ‚ÄúÂîØ‰∏Ä‚Äù\"| recon IWAE ÂØπp(x,z)=‚à´p(x|z)p(z)dz ÂÅöÁ≠â‰ª∑ÂèòÊç¢Ôºå‰ªéËÄåÂèØ‰ªéÂêéÈ™åp(z|x)‰∏≠ÈááÊ†∑z„ÄÇ\n(2023-06-04)\nPCA ‰∏é VAE They\u0026rsquo;re both learning the distribution of data.\nDDG search: \u0026ldquo;PCA ‰∏é VAE ÂØπÊØî\u0026rdquo;\nUnderstanding Variational Autoencoders (VAEs) - Joseph Rocca\nVariational Autoencoders explained ‚Äî with PyTorch Implementation - Sanna Persson\n1 import torch Instead of mapping the input into a fixed vector, we want to map it into a distribution. From Autoencoder to Beta-VAE - Lil\u0026rsquo;Log\n","date":"2022-08-26T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/c-sum-vae/","title":"sum: VAE"},{"content":"Code | Arxiv\n‰∏ÄÂè•‰∏≠ÊñáÔºöÊûÑÂª∫ point-alinged feature field, ÁÑ∂ÂêéÁî® attention ÊääÁâπÂæÅËûçÂêàÊàêÂõæÂÉèÔºåÁâπÂæÅÁöÑattn score ÂèçÊò†‰∫ÜÊ∑±Â∫¶\nInsights:\nNeRF is in a backward optimization fashion. The color is mapped to points along with the optimization. So the radience field is recovered backward. While Generalizable NeRFs assign feature onto points in the feed-forward process.\nProjecting the points onto feature maps exerts the inductive bias of epipolar constraints for injecting geometry prior.\nIt\u0026rsquo;s inferior to NLF\nSamples on a single ray cannot recover refraction and scattering, in which the ray will bend. So GNT managed this by its view transformer?\nOcclusion-aware is realized by giveing the most visible reference view the most weight. Depth-aware is endowed by the importance of each point to the pixel color. \u0026ldquo;importance=density\u0026rdquo;\nGNT doesn\u0026rsquo;t care the quality of 3D geometry reconstruction.\nOnly net_coarse is used and trained with 4096 rays, 192 N_samples in one iteration. They didn\u0026rsquo;t split the N_rand into chunks in the train.py, but they did when rendering a full imagein evaluation period. So I need to distribute the 4 blocks onto 4 crads for accomodating the 4096 rays in one batch, if I want to reproduce their expriment. In my previous training, the number of points fed into MLP is N_rand x N_samples = 2048x64 = 131072.\nAbstract Generalizable NeRF Transformer (GNT) two transformer-based stages view transformer: multi-view geometry, coordinate-aligned features, epipolar lines ray transformer: ray marching rendering, decode point features reconstruct single scene without rendering formula attention map 1 Introduction Topoic: novel view synthesis by NeRF (coordinate-based model + differentiable volumetric rendering)\nProblems: one network overfits one scene with long optimization.\nFormer solutions \u0026amp; drawbacks: Ibrnet, pixelNerf, NLF proved the coordinates are not necessary, but the novel view can be interpolated from other views\u0026rsquo; image features.\nTask:\nContributions \u0026amp; Reason\ncooridnate network and volumetric renderer are composed into a transformer architecture. use multi-view image features to infer coordinate-aligned features. Later these features are decoded to novel view directly without volume rendering. Results statement\nÊúâÊ≥õÂåñËÉΩÂäõÊÑèÊÄùÊòØÔºåËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂèØ‰ª•Áõ¥Êé•Áî®ËæìÂÖ•ÂõæÂÉèÈáçÂª∫ unseen Âú∫ÊôØÁöÑÊñ∞ËßÜÂõæÔºü\n2 Related Work Advances in Transformer\nNeural Radiance Fiels: NeRF, Mip-NeRF; surface representation, dynamic scenes, reflection modeling; Generalization nerf: PixelNeRF, IBRNet, MVSNeRF, NeuRay; accelerate nerf.\nTransformer Meets Radiance Fields: IBRNet, NerFormer, Generalizable neural radiance field, NLF, Vision transformer for nerf-based view synthesis; SRT\n3 Preliminaries 4 Method: Make Attention All NeRF needs 4.1 Epipolar Geometry Constrained Scene Representation Multi-View Feature Aggregation.\nVanilla NeRF use MLP to parameterize scene in a backward optimization fashion? (ÂÖàÊ∏≤ÊüìÂá∫ÂõæÁâáÔºå‰æùÊçÆÂõæÁâáËØØÂ∑ÆÂÜçËøîÂõûÂéªÊõ¥Êñ∞ËæêÂ∞ÑÂú∫Ôºâ\nIn contrast, generalizable NeRFs construct the radiance field in a feed-forward scheme\nÁî®ÂõæÂÉèÁâπÂæÅ‰ºòÂåñËæêÂ∞ÑÂú∫ÔºåËÆ≠ÁªÉÂ•ΩÂêéÔºåÂ∞±ÂèØ‰ª•ÂØπËæêÂ∞ÑÂú∫Â∫îÁî®volume renderingÊù•Ê∏≤ÊüìÊñ∞ËßÜÂõæÔºõËÄåÊú¨ÊñáÊòØ‰ªéËæêÂ∞ÑÂú∫Êò†Â∞ÑÂõûÂõæÂÉèÁâπÂæÅÔºå‰ªéËÄåÁîüÊàêÊñ∞ËßÜÂõæ\n(2023-12-16) NeRF ÊòØ‰ªéÂæÖÊ∏≤ÊüìÁöÑÂÉèÁ¥†Âá∫ÂèëÔºåÂêëÁ©∫Èó¥ÂèëÂá∫Â∞ÑÁ∫øÔºåÂéªÊü•ËØ¢È¢úËâ≤„ÄÇËÄå Forward ÊòØÊääÁ©∫Èó¥ÁÇπÊäïÂΩ±Âà∞ÂÉèÁ¥†‰∏ä„ÄÇ\nRepresent the scene as a feature field, where each point in the space has a part of image feature.\nUse attention to fuse all pixel on ResUNet feature maps of source views is memory prohibitive\nOnly fuse the pixels locating in the paring epipolar lines of \u0026ldquo;neighboring views\u0026rdquo; ÔºàÂíå PixelNeRF ‰∏ÄÊ†∑Ôºå‰∏çËøá‰∫∫ÂÆ∂Ê≤°ÊèêÂØπÊûÅÂá†‰ΩïËøô‰∏™ËØçÔºåÂ∞±ÊòØÊääÂÖâÁ∫ø‰∏äÁöÑÁÇπÊäïÂΩ±Âà∞‰∏çÂêåËßÜÂõæ‰∏äÔºâ\nMemory-Efficient Cross-View Attention\nOnly use one read-out token in the query sequence to iteratively fuse features from neighbor views\nThe similarity is not computed by dot multiplication, but subtraction, so the attention score is calculated for every channel of the features.\nThe attention scores matrix and \u0026lsquo;V\u0026rsquo; are added by the relative directions between source views and the target view.\nDiscussion on Occlusion Awareness.\n4.2 Attention Driven Volumetric Rendering Different illumination effects and material scenarios need to apply specific handcrafted rendering formula. Data-driven renderer decode the image features into images realizing various phenomena in one way.\nRay Feature Aggregation:\nAnalogy the token features as the color in the volum rendering fomula. Therefore, do attention for coordinate-aligned features to aggregate the final color (rgb) for the novel pixel/ray. mean pooling to compress the feature patch into a pixel use dot-product attention to fully mix features of other points getting comprehensive contextual information. ÂÖ≥‰∫é auto-regressive rendering ÁöÑÂª∂‰º∏ËÆ®ËÆ∫Ôºü Discussion on Depth Cuing\ndepth is the average of marching distance with attention score NLF ÊúâÁõ∏ÂêåÁöÑÊû∂ÊûÑÔºåÂå∫Âà´Âú®Âì™Ôºü 5 Experiments single scene; generalization to unseen scenes\n5.1 Implementation Details Source and Target view sampling Âú® Blender Êï∞ÊçÆÈõÜ‰∏äÁöÑ PSNR Êó†ÊòéÊòæÊèêÂçá\n(2023-08-16)\nCode train.py\n\\begin{algorithm} \\begin{algorithmic} \\PROCEDURE{train}{args} \\STATE data: [ rgb$_{(1,H,W,3)}$, $\\newline\\qquad$ camera$_{(1,34)}$, $\\newline\\qquad$ rgb\\_path$_{(str)}$,$\\newline\\qquad$ src\\_rgbs$_{(8,H,W,3)}$, $\\newline\\qquad$ src\\_camera$_{(8,34)}$, $\\newline\\qquad$ depth\\_range$_{(1,2)}$ ] \\STATE $\\newline$ \\STATE ray\\_sampler = RaySamplerSingleImage(data) \\STATE ray\\_batch = ray\\_sampler.random\\_sample(N\\_rand) \\STATE featmaps = model.feature\\_net(ray\\_batch[\"src\\_rgbs\"]) \\STATE ret = render\\_rays(ray\\_batch, model, projector, featmaps) \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} llff_test.py __init__() of a training dataset:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def LLFFDataset(Dataset): def __init__(self, args, mode, scenes=(), random_crop=True): scenes = os.listdir(folder_path) # Stuff all scenes into lists for i, sceneName in enumerate(scenes): rgb, poses, bds, render_poses, i_test, imgnames = load_llff_data(sceneName) near, far = np.min(bds), np.max(bds) intrinsics, c2w = batch_parse_llff_poses(poses) self.train_intrinsics.append(intrinsics[i_train]) self.train_poses.append(c2w[i_train]) self.train_rgb_files.append(imgnames[i_train]) self.render_rgb_files.extend(imgnames[i_train]) self.render_intrinsics.extend(intrinsics[i_train]) self.render_poses.extend(c2w[i_train]) self.render_depth_range.extend([[near,far]]*len(i_train)) self.render_train_set_ids.extend([i]*len(i_train)) ","date":"2022-08-23T00:00:00Z","image":"https://vita-group.github.io/GNT/assets/overview.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-gnt/","title":"read: Render - NVS | GNT"},{"content":" torch.view() Âè™ËÉΩÂ§ÑÁêÜ contiguous tensorÔºå‰ΩÜ torch.reshape() ‰∏çÈôê„ÄÇ\nÊàëÂú®ÊîπÂèòÂΩ¢Áä∂Êó∂Ôºölatent = latent.view(SB, NS, self.latent_size, B).permute(0,3,1,2).view(-1, NS, self.latent_size)Ôºå\nÊä•ÈîôÔºö RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\ncontiguous means \u0026ldquo;Two adjacent cells in a row of tensor are neighbors on the memory as well\u0026rdquo;. What is the difference between contiguous and non-contiguous arrays?\n(2023-12-23) \u0026ldquo;Êï∞ÁªÑÂ≠òÂÇ®È°∫Â∫è‰∏éÊåâË°åÂ±ïÂºÄÁöÑÈ°∫Â∫è‰∏ÄËá¥\u0026rdquo;„ÄÇ „Äê‰ª£Á†ÅÁ≤æËØª„ÄëÂºÄÂ±±‰πã‰ΩúMVSNet PyTorchÁâàÊú¨Ë∂ÖËØ¶ÁªÜÂàÜÊûê - doubleZÁöÑÊñáÁ´† - Áü•‰πé\nview vs reshape: What\u0026rsquo;s the difference between reshape and view in pytorch?\nFind whether a tensor is copied or not after reshape: Check if the reshaped tensor will be affected by the original tensor.\nStride (2023-09-28) contiguous means the relative sequence (underlying 1D memory arrangement when they were first created) of elements in a tensor hasn\u0026rsquo;t been changed, although the shape can be mutated. (The data on memory blocks never reshape.)\nE.g., the layout of a = torch.arange(4) means 0,1,2,3 are next to each other in this order. a.stride() is (1,).\nb = a.reshape(2,2) $[^{[0,1]}_{[2,3]}]$ is still contiguous. Because only the data\u0026rsquo;s dimensional interpretation changed, and the read order of 0,1,2,3 didn\u0026rsquo;t change. b.stride() is (2,1).\ntorch.flip(b, [0]) $[^{[2,3]}_{[0,1]}]$ is contiguous, because they\u0026rsquo;re next to each other in terms of the tensor\u0026rsquo;s .stride() which remains (2,1).\nThe stride is responsible to the initial underlying layout, if the stride gets changed, the tensor won\u0026rsquo;t be contiguous anymore.\nThe operation transpose and permute will change stride, which isn\u0026rsquo;t row-contiguous any longer.\nso b.transpose(0,1) or b.permute(1,0): $[^{[0,2]}_{[1,3]}]$ is not contiguous any more.\n.contiguous() will change the stride to match the current shape. What does .contiguous() do in PyTorch?\n(2023-12-22) .contiguous() will copy the data to a new memory strip, which can be checked via .storage(). The new and the original tensors won\u0026rsquo;t affect each other. Pytorch - Contiguous vs Non-Contiguous Tensor / View ‚Äî Understanding view(), reshape(), transpose() - Kathryn\n.permute() made .stride() decoupled with the tensor\u0026rsquo;s varying shapes mutating from the original shape.\nThat means when the tensor reverts to the original size, the .stride() won\u0026rsquo;t return to the structure matched with the tensor\u0026rsquo;s original shape.\n1 2 3 4 5 6 7 8 9 a = torch.arange(8).reshape(2,4) # contiguous b = a.transpose(0,1) # (4,2) not contiguous b.view(2,4) # Can\u0026#39;t work # because the stride don\u0026#39;t match the target shape b.contiguous().view(2,4) # Or use b.reshape(2,4) # will copy data if .view() don\u0026#39;t work. Therefore, .contiguous() is necessary before view() after transpose() or permute().\nRead-Write Head (2023-10-13) stride ÊòØ‰∏∫‰∫Ü‰ªé‰∏ÄÊù°ÂÜÖÂ≠ò‰∏äÁ¥¢ÂºïÔºà\u0026ldquo;ÊãºÂáë\u0026rdquo;Ôºâ ÂèñÂá∫ Âº†ÈáèÁöÑÊüê‰∏Ä‰∏™Áª¥Â∫¶ÔºåËØªÂÜôÂ§¥ÁöÑÊ≠•ÂπÖ„ÄÇ How does NumPy\u0026rsquo;s transpose() method permute the axes of an array?\nËÆ°ÁÆó‰∏Ä‰∏™ tensor ÁöÑ strideÔºöÁªôÂÆö‰∏Ä‰∏™ contiguous ÁöÑtensorÔºö x.shape = (2,2,4), Âàô x.stride() = (8,4,1), ÊâÄ‰ª• ‰∏Ä‰∏™Áª¥Â∫¶‰∏äÁöÑ stride Á≠â‰∫éÂÆÉÂêéÈù¢ÁöÑÁª¥Êï∞Á¥Ø‰πòÔºö8=2√ó4, 4=4√ó1, 1=1.\n„Äêpytorch Tensor shape ÂèòÂåñ view ‰∏é reshapeÔºàcontiguous ÁöÑÁêÜËß£Ôºâ„Äë\ntranspose ËΩ¨ÁΩÆÊüê2‰∏™Áª¥Â∫¶ÔºåÂç≥‰∫§Êç¢ÈÇ£‰∏§‰∏™Áª¥Â∫¶‰∏äÁöÑ strideÔºå‰∏çÂêåÁöÑÁª¥Â∫¶Âú®ÂÜÖÂ≠ò‰∏äËµ∞‰∏çÂêåÁöÑË∑ùÁ¶ª„ÄÇ\n.T ‰ºöÂÄíÂ∫èÂÖ®ÈÉ®Áª¥Â∫¶‰∏äÁöÑ stride, e.g., (2,2,4) -\u0026gt; (4,2,2)\n(2023-10-23) Examples:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026gt;\u0026gt;\u0026gt; a = torch.arange(6).reshape(2,3,1) # stride: (3,1,1) tensor([[[0], [1], [2]], [[3], [4], [5]]]) \u0026gt;\u0026gt;\u0026gt; b = a.transpose(0,1) # stride: (1,3,1), not contiguous tensor([[[0], [3]], [[1], [4]], [[2], [5]]]) \u0026gt;\u0026gt;\u0026gt; c = a.transpose(1,2) # stride: (3,1,1), contiguous tensor([[[0, 1, 2]], [[3, 4, 5]]]) \u0026gt;\u0026gt;\u0026gt; d = a.transpose(0,2) # stride: (1,1,3), not contiguous tensor([[[0, 3], [1, 4], [2, 5]]]) \u0026gt;\u0026gt; e = a.T # stride: (1,1,3) .transpose swaps strides, .T reverses strides, and permute reorders strides, while .view changes stride to match the shape while keeping elements \u0026ldquo;‰∫íÁõ∏Êé•Â£§ÁöÑ\u0026rdquo;„ÄÇ\ncontiguous \u0026ldquo;‰∫íÁõ∏Êé•Â£§ÁöÑ\u0026rdquo;Ôºö Áü©ÈòµÁöÑ‰∏ÄË°å‰∏≠Áõ∏ÈÇªÁöÑ 2 ‰∏™ÂÖÉÁ¥†ÔºåÂú®ÂÜÖÂ≠ò‰∏ä‰πüÁõ∏ÈÇª„ÄÇ ÂÖ∂‰∏≠ÔºåÁü©Èòµ‰∏ÄË°åÁöÑÊú´Â∞æ‰∏é‰∏ã‰∏ÄË°åÁöÑÂºÄÂ§¥Áõ∏ÈÇª„ÄÇ\n(2023-10-24) contiguous means that no matter how the shape changes, the movement of the read/write head acts like indexing a flattened tensor.\nRectangle\u0026rsquo;s Shape (2023-12-10) Changing the shape of a tensor is like stretching an area-fixed rectangle, although h and w are changed, the relative order of internal elements is not changed.\nImagine dragging the bottom right corner to change the height and width of the rectangle. Alternatively, one can imagine the data as sand grains being enclosed by a size-changble frame. And once the shape changed, the rows are filled up first:\nm e m o r y : R ‚ãÖ ‚ãÖ ‚ãÖ W 3 ‚ãÖ ‚ãÖ ‚ãÖ 1 ‚Üë h x e ‚ãÖ ‚ãÖ ‚ãÖ 2 a 6 d ‚ãÖ ‚ãÖ ‚ãÖ 3 ‚ãÖ ‚ãÖ ‚ãÖ 4 ‚ãÖ ‚ãÖ ‚ãÖ ‚ãØ D f r i H c a l , h g l a W n h i g e n e r t e o ‚ãÖ ‚ãÖ ‚ãÖ ‚ãÖ 2 ‚ãÖ ‚ãÖ x ‚ãÖ ‚ãÖ 9 ‚ãÖ ‚ãÖ ‚ãÖ ‚ãÖ ‚ãÖ ‚ãÖ ‚ãÖ ‚ãÖ 1 8 ‚ãÖ ‚ãÖ The reading order is unchanged, keeping the sequence from 1 to 18:\n1 7 1 3 2 8 1 4 3 9 1 5 4 1 1 0 6 5 1 1 1 7 6 1 1 2 8 1 1 0 2 1 1 3 1 2 4 1 3 5 1 4 6 1 5 7 1 6 8 1 7 9 1 8 (ÊàëÂøò‰∫ÜÊòØ‰∏çÊòØÊúâËøô‰πà‰∏ÄÁßçÁé©ÂÖ∑ÔºöÊúâ‰∏Ä‰∏™ÈïøÊñπÂΩ¢ÔºåÊ°Ü‰Ωè‰∫Ü‰∏Ä‰∫õ‚ÄúÊ£ãÂ≠ê‚Äù„ÄÇ ÂΩì‰Ω†ÊãñÂä®ÈïøÊñπÂΩ¢ÁöÑ‰∏Ä‰∏™ËßíÁöÑÊó∂ÂÄôÔºåÂõ†‰∏∫ÂÆÉÈù¢ÁßØÊòØÂõ∫ÂÆöÁöÑÔºåÊâÄ‰ª•ÈáåÈù¢ÁöÑÊ£ãÂ≠ê‰ºöÈáçÊñ∞ÊéíÂàó„ÄÇÂØπÂ∫îÂà∞ .view() Â∞±ÊòØÊØèË°åÂÖàÂ°´Êª°„ÄÇ)\nSince the view doesn\u0026rsquo;t changes the stride of the read/write header, the target shape requires to match the current stride. torch view - Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 xy= torch.cartesian_prod(torch.arange(3), torch.arange(2)) # tensor([0, 0], # [0, 1], # [1, 0], # [1, 1], # [2, 0], # [2, 1]]) # memory: 0 0 0 1 1 0 1 1 2 0 2 1 # shape: (2,6), stride: (2,1) print(xy.view(2,6)) # contiguous, stride: (4,1) # tensor([[0, 0, 0, 1, 1, 0], # [1, 1, 2, 0, 2, 1]]) # ‰∏ÄÊù°Èæô‰∏≤‰∏ãÊù• xy.t().is_contiguous() # False, stride: (1,2) # tensor([[0, 0, 1, 1, 2, 2], # [0, 1, 0, 1, 0, 1]]) view is filling an empty box from the innermost dimension to outermost by consuming the 1D memory data.\n(2023-12-22) The tensors created by torch.meshgrid() are not contiguous.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 y, x = torch.meshgrid(torch.arange(3), torch.arange(2)) # y: vertical coordinate # tensor([[0, 0], # [1, 1], # [2, 2]]) # Memory is: 0 1 2 # Not contiguous. stride on each dim: (1,0) y.storage() # 0 # 1 # 2 # [torch.LongStorage of size 3] # x: horizontal coordinate # tensor([[0, 1], # [0, 1], # [0, 1]]) # Memory is 0 1 # Not contiguous. stride on each dim: (0,1) x.storage() # 0 # 1 # [torch.LongStorage of size 2] y.contiguous().stride() # stride: (2,1) When reading y and x, the read-write head has to go back or repeat some bytes, instead iterates the 1D memory sequence once, so they\u0026rsquo;re not contiguous.\nrepeat will copy the data, while expand won\u0026rsquo;t, with a stride of 0 on the singleton dimension. (singleton means the size of that dimension is 1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 y.unsqueeze(2).repeat(1,1,2) # shape: (3,2,2), contiguous # tensor([[[0, 0], # [0, 0]], # # [[1, 1], # [1, 1]], # # [[2, 2], # [2, 2]]]) # stride: (4,2,1) y.unsqueeze(2).expand(-1,-1,2) # shape: (3,2,2), Not contiguous # tensor([[[0, 0], # [0, 0]], # # [[1, 1], # [1, 1]], # # [[2, 2], # [2, 2]]]) # stride: (1,0,0) ","date":"2022-08-21T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_contiguity/","title":"memo: PyTorch | Contiguous \u0026 Stride"},{"content":"Background ÂÖ®ËøûÊé•ÔºöÁ∫øÊÄßÂ±Ç‰∏≤Ë°å\nËæìÂÖ•ÂõæÂÉè in pytorchÔºö(C, H, W) ToTensor doc\nCNN = Feature Extraction + Classification\nCCD ÂÖâÊïèÁîµÈòª + ÈÄèÈïúÁ≥ªÁªüÔºå ‰∏Ä‰∏™ÁîµÈòªÂè™Ê£ÄÊµã‰∏Ä‰∏™ÂÖâÈî•Âå∫ÂüüÁöÑÂÖâÁ∫øÔºåÊ†πÊçÆÁîµÈòª‰∏éÂÖâÂº∫ÁöÑÂáΩÊï∞ÂÖ≥Á≥ªÔºåÂæóÂà∞ÁÅ∞Â∫¶Âõæ\ninput channels: rgb\nConv Layer ÊØèÊ¨°Âèñ‰∏Ä‰∏™ÂùóÔºö(C * kernel_h * kernel_w) ÈÄöËøá Conv layer ÂæóÂà∞ (C\u0026rsquo; * kernel_h\u0026rsquo; * kernel_w\u0026rsquo;)„ÄÇ\nÂÅö Conv layer Êó∂Ôºåinput ÁöÑÊâÄÊúâ channel ÈÉΩÂÅöÂç∑ÁßØÊìç‰ΩúÔºàÂêÑÈÄöÈÅì‰ΩøÁî®ÁöÑÂç∑ÁßØÊ†∏‰∏çÂêåÔºâÔºåÁÑ∂Âêé(Êåâ‰∏çÂêåÊùÉÈáç)Áõ∏Âä†„ÄÇ Ëøô‰∏§Ê≠•ÂÅö n Ê¨°ÔºåÈÇ£‰πà outputÁöÑ channels Â∞±ÊòØ n„ÄÇÊâÄ‰ª•ÂêéÁª≠ÁöÑÊØè‰∏ÄÂ±Ç channel ÈÉΩÂåÖÂê´‰∫Üinput ÁöÑÂÖ®ÈÉ®ÈÄöÈÅìÁöÑ‰ø°ÊÅØ„ÄÇ CNN ÁöÑÊùÉÈáçÂ≠òÂú®‰∫éÂç∑ÁßØÊ†∏‰∏≠„ÄÇ\n(2023-10-22) ÂêÑËæìÂÖ•ÈÄöÈÅì ‰πò‰ª•ÁöÑ kernel ‰∏çÂêåÔºåÁÑ∂ÂêéÂêÑÈÄöÈÅìÁõ¥Êé• Áõ∏Âä†:\n1 2 3 4 5 6 7 8 9 10 a = torch.arange(18.).reshape(1, 2,3,3) # (bs, C, h,w) n = 1 # convolution repeat times conv_lyr = torch.nn.Conv2d(a.size(1), n, kernel_size=3, bias=False) print(conv_lyr.weight) # shape (1,2,3,3) conv_lyr.weight = torch.nn.Parameter( torch.stack([torch.ones(3,3), torch.zeros(3,3)]).unsqueeze(0)) print(conv_lyr.weight) out_a = conv_lyr(a) # (1,1,1,1), tensor([[[[36.]]]]) # i.e. 0+1+2+3+4+...+8 + 0+0+..0 = 36. (2023-10-22) Conv2d ‰∏é FC ÂÅöÁöÑÊï∞Â≠¶ËøêÁÆó Áõ∏ÂêåÔºö\np p p p i i i i F x x x x C e e e e l l l l L 1 2 3 4 a y c e h r n R G B R G B R G B R G B ' l - - - - - - - - - - - - s s d w i w ‚ÇÇ w w m ‚ÇÅ ‚ÇÉ ‚ÇÑ 1 d d d d i i i i m m m m 1 1 1 1 o o o o f f f f o o o o u u u u t t t t 1 2 3 4 1 s t A S O u c p u m h o t n r o l t = f i C o o a o f n l n l v 4 o f ‚Å∫ w l p e a i o i y x x u g e e t h r l p t ' s u ‚Å∫ e s t d w w ' c ‚ÇÅ ‚ÇÉ s i h n n 1 p l w w s u ‚ÇÇ ‚ÇÑ t t 1 c c h h n n l l s Â¶ÇÊûúÊòØ 1x1 ÁöÑÂç∑ÁßØ (stride=1)ÔºåÂç≥Ê≤°ÊúâÈÇªÂ±ÖÂÉèÁ¥†Áõ∏Âä†ÔºåConv Â∞±‰ºöÂíå FC Á≠â‰ª∑: ÊääÊØè‰∏™ÂÉèÁ¥†ÊäïÂΩ±Âà∞Âè¶‰∏Ä‰∏™Áª¥Â∫¶ÁöÑÁ©∫Èó¥„ÄÇ\nchnl1 √ó w\u0026rsquo; + chnl2 √ó w\u0026rsquo;\u0026rsquo; + chnl3 √ó w\u0026rsquo;\u0026rsquo;\u0026rsquo; = out-chnl1\ndim1 √ó w\u0026rsquo; + dim2 √ó w\u0026rsquo;\u0026rsquo; + dim3 √ó w\u0026rsquo;\u0026rsquo;\u0026rsquo; = out-dim1\n(2023-12-01) I already forgot the original meaning of the left picture because I didn\u0026rsquo;t write descriptions.\nFor a FC layer given by 4 samples of 3 dimensions, each dimension multiplied by a factor w‚Çì becomes a portion of the out dimension. This corresponds to the operations in Conv2d layer: every channel is multiplied by a kernel and then sum up all weighted channels to form one of out channels.\nThe difference is that in FC the 4 samples aren\u0026rsquo;t merged, but Conv2d merged 4 pixels to 1 pixel. Thus, the number of pixels in FC is consistent, whereas conv layer reduces pixels.\nEach channel in feature maps always shares a common 2-D kernel. A Conv2d layer stands for a 4-D kernel as each channel uses different kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch in_channels, out_channels = 5, 10 width, height = 100, 100 kernel_size = 3 batch_size = 1 input = torch.randn(batch_size, in_channels, width, height) # (1,5,100,100) conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size) output = conv_layer(input) # (1,10,98,98) print(conv_layer.weight.shape) # (10,5,3,3) Padding ‰øùÊåÅËæìÂá∫ÂõæÂÉèÂ∞∫ÂØ∏‰∏çÂèò„ÄÇ3x3Ê†∏pad 1ÂúàÔºå5x5 pad 2Âúà\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) conv_layer = torch.nn.Conv2d(1,1, kernel_size=3, padding=1, bias=False) kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3) # (out_chnls, in_chnls, h, w) conv_layer.weight.data = kernel.data output = conv_layer(input) print(output) stride Âç∑ÁßØÊ†∏ÁßªÂä®Ê≠•ÈïøÔºåÔºàÂáèÂ∞ëÊ≠•Êï∞ÔºâÁî®‰∫éÁº©Â∞èfeature map ÁöÑÂ§ßÂ∞è\nMax Pooling ‰∏ãÈááÊ†∑, ÊØè h x w Âå∫ÂùóÈáåÈù¢ÂèñÊúÄÂ§ßÂÄºÔºåkerenl_size=2 Â∞∫ÂØ∏Áº©Â∞è‰∏∫ÂéüÊù•ÁöÑ‰∏ÄÂçä„ÄÇËØ•Êìç‰ΩúÊó†ÂèÇÊï∞\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2) output = maxpooling_layer(input) print(output) \u0026gt;\u0026gt;\u0026gt; tensor([[[[4., 8.], \u0026gt;\u0026gt;\u0026gt; [9., 8.]]]]) ÂÆö‰πâÁΩëÁªúÔºö\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5) self.pooling = torch.nn.MaxPool2d(kernel_size=2) self.fc = torch.nn.Linear(320, 10) def forward(self, x): batch_size = x.size(0) x = F.relu(self.pooling(self.conv1(x))) x = F.relu(self.pooling(self.conv2(x))) x = x.review(batch_size, -1) # or flatten x = self.fc(x) return x # ‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÊâÄ‰ª•‰∏çÂÅöÊøÄÊ¥ª model = Net() device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_avaliable() else \u0026#34;cpu\u0026#34;) model.to(device) ","date":"2022-08-14T16:52:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/10_cnn%E5%9F%BA%E7%A1%80/","title":"watch: PyTorch - Âàò‰∫å 10 | CNN Basics"},{"content":"„ÄêÂåó‰∫¨Â§ßÂ≠¶„ÄëTensorflow2.0 - bilibili\nÂàõÂª∫ÁéØÂ¢É 1 2 3 4 conda create -n tf2.1 python=3.7 conda install -c conda-forge cudatoolkit=10.1 cudnn=7.6 pip install --upgrade pip pip install tensorflow==2.1 ÂØºÂÖ•Â§±Ë¥•ÔºåÊä•ÈîôÔºö\n1 2 3 4 5 TypeError: Descriptors cannot not be created directly. If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc \u0026gt;= 3.19.0. If you cannot immediately regenerate your protos, some other possible workarounds are: 1. Downgrade the protobuf package to 3.20.x or lower. 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower). Êü•Áúãprotobuf ÁâàÊú¨Ôºöpip show protobuf\n1 2 pip uninstall protobuf pip install protobuf==3.19.0 StackOverflow ans\n","date":"2022-08-14T15:15:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/0/","title":"watch: TF2 - PKU 00 | Environment Setup"},{"content":"ÁºñÁ†ÅÂô®‰∏≠ÊòØ self-attentionÔºåÊòØËá™ÁºñÁ†ÅÔºåq„ÄÅk„ÄÅvÂêåÊ∫êÔºåËÆ°ÁÆóÂá∫ÂêÑÂçïËØç(query)Âú®Êï¥‰∏™Âè•Â≠ê(values„ÄÅkeys) ‰∏≠ÁöÑ‰ªΩÈáèÔºõ Ëß£Á†ÅÂô®‰∏≠ÊòØ encoder-decoder attentionÔºåq„ÄÅk(v)‰∏çÂêåÊ∫êÔºåqueryÊòØËß£Á†ÅÂô®Â∑≤ÁîüÊàêÂÜÖÂÆπÁöÑËØçÂêëÈáèÔºåvalues„ÄÅkeys Êù•Ëá™ÁºñÁ†ÅÂô®Ôºõ decoder ËæìÂá∫‰∏Ä‰∏™ËØçÂêëÈáèÔºåËøòË¶ÅÁªèËøá linear Âíå softmax ÊâçËÉΩÂèòÊàêÂçïËØç„ÄÇ\n17 Transformer ÁöÑËß£Á†ÅÂô®ÔºàDecodersÔºâ‚Äî‚ÄîÊàëË¶ÅÁîüÊàê‰∏Ä‰∏™Âèà‰∏Ä‰∏™ÂçïËØç- Áà±ÈíìÈ±ºÁöÑÁ®ãÂ∫èÁåø\n","date":"2022-08-06T17:20:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/17/","title":"watch: Transf - Nick 17 | Decoder"},{"content":"Reference:\nPL Viedo tutorial of AIËëµ youtube\nkwea123/pytorch-lightning-tutorial\nPyTorch Lightning official Tutorial\nPL example mnist source code\npytorch-lighting ‰ºòÁÇπÔºö\nÊñπ‰æøÂú®ÂêÑÁßçËÆæÂ§á(cpu/gpu/tpu/Â§öËäÇÁÇπ)‰∏äËøêË°åÔºåÂè™ÈúÄÂÖ≥ÂøÉÁÆóÊ≥ïÂÆûÁé∞ ÊéßÂà∂ÈöèÊú∫Êï∞ÔºåÈáçÁé∞ÂÆûÈ™åÁªìÊûúÔºåÂõ∫ÂÆöÂàíÂàÜÂêÑbatch Â∑•Á®ãÊñá‰ª∂Â§πÁõÆÂΩïÔºö\nrequirements.txt : ÊâÄÈúÄ‰æùËµñÂåÖ train.py : main opt.py : hyperparameter models Êñá‰ª∂Â§πÔºö ÂåÖÂê´‰∏çÂêåÁöÑÊ®°Âûã networks1.py datasets Êñá‰ª∂Â§πÔºö ÂåÖÂê´‰∏çÂêåÊï∞ÊçÆÈõÜÁöÑ dataloader losses.py: ÂêÑÁßçloss func .gitignore: ckpts/, logs/, MNIST/ requirements.txt\n1 2 3 torch==1.11.0 torchvision==0.12.0 pytorch_lightning==1.6.0 Versioning Policy opt.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import argparse def get_opts(): parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--root_dir\u0026#39;, type=str, default=\u0026#39;./data/\u0026#39;, help=\u0026#39;root directory of dataset\u0026#39;) parser.add_argument(\u0026#39;--hidden_dim\u0026#39;, type=int, default=128, help=\u0026#39;number of hidden dimensions\u0026#39;) parser.add_argument(\u0026#39;--val_len\u0026#39;, type=int, default=5000, help=\u0026#39;number of validation samples split from train set\u0026#39;) parser.add_argument(\u0026#39;--batch_size\u0026#39;, type=int, default=128, help=\u0026#39;number of training samples in one batch\u0026#39;) parser.add_argument(\u0026#39;--lr\u0026#39;, type=float, default=1e-4, help=\u0026#39;learning rate\u0026#39;) parser.add_argument(\u0026#39;--num_epochs\u0026#39;, type=int, default=1, help=\u0026#39;number of epochs\u0026#39;) parser.add_argument(\u0026#39;--num_gpus\u0026#39;, type=int, default=1, help=\u0026#39;number of gpus to be used\u0026#39;) parser.add_argument(\u0026#39;--expname\u0026#39;, type=str, default=\u0026#39;test\u0026#39;, help=\u0026#39;experiment name\u0026#39;) # return parser.parse_args() args, unknown = parser.parse_known_args() return args networks1.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torch import nn class myLinearModel(nn.Module): def __init__(self, hidden_dim): super().__init__() self.net = nn.Sequential( nn.Linear(28*28, hidden_dim), nn.ReLU(True), nn.Linear(hidden_dim, 10) ) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; x: (B, 1, 28, 28) channel=1 \u0026#39;\u0026#39;\u0026#39; x = x.flatten(start_dim=1) # (B, 28*28) return self.net(x) train.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 import torch from torch import nn from torch.nn import functional as F from torchvision import datasets, transforms from torch.utils.data import DataLoader, random_split from torch.optim.lr_scheduler import CosineAnnealingLR # from opt import get_opts # from models.networks1 import myLinearModel from pytorch_lightning import LightningModule, Trainer, seed_everything from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar from pytorch_lightning.loggers import TensorBoardLogger seed_everything(1234, workers=True) def get_learning_rate(optimizer): # for recording logs for param_group in optimizer.param_groups: return param_group[\u0026#39;lr\u0026#39;] class MNISTSystem(LightningModule): # LightningModule puts all parts together # Design model def __init__(self, hparams): \u0026#39;\u0026#39;\u0026#39; hparams: all hyper parameters \u0026#39;\u0026#39;\u0026#39; super().__init__() # self.hparams = hparams self.save_hyperparameters(hparams) # store exp conditions for reproduction and loss visualization # network components self.net = myLinearModel(self.hparams.hidden_dim) def forward(self, x): return self.net(x) # Prepare data def prepare_data(self): \u0026#39;\u0026#39;\u0026#39; Download train set and test set (execute only once) \u0026#39;\u0026#39;\u0026#39; datasets.MNIST(self.hparams.root_dir, train=True, download=True) datasets.MNIST(self.hparams.root_dir, train=False, download=True) def setup(self, stage=None): \u0026#39;\u0026#39;\u0026#39; Preprocessing Split the train and validation set (called by every device) \u0026#39;\u0026#39;\u0026#39; dataset = datasets.MNIST(self.hparams.root_dir, train=True, download=False, transform=transforms.ToTensor()) # load data (B, channel, H,W) train_length = len(dataset) self.train_set, self.val_set = random_split(dataset, \\ [train_length - self.hparams.val_len, self.hparams.val_len]) def train_dataloader(self): \u0026#39;\u0026#39;\u0026#39; Split the train set into multiple batches for one epoch \u0026#39;\u0026#39;\u0026#39; return DataLoader(self.train_set, shuffle=True, # every epoch is different num_workers=4, # cpu threads batch_size=self.hparams.batch_size, pin_memory=True) def val_dataloader(self): return DataLoader(self.val_set, shuffle=False, # comparing acc between epochs num_workers=4, batch_size=self.hparams.batch_size, pin_memory=True) # Construct optimizer and loss func def configure_optimizers(self): self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.hparams.lr) schedular = CosineAnnealingLR(self.optimizer, T_max=self.hparams.num_epochs, \\ eta_min=self.hparams.lr/1e2) return [self.optimizer], [schedular] # different models use different optimizers,schedulars # Training cycle def training_step(self, batch, batch_idx): \u0026#39;\u0026#39;\u0026#39; batch: come from iterable train_dataloader batch_idx: index of the current batch \u0026#39;\u0026#39;\u0026#39; imgs, labels = batch # images\u0026#39; pixels, labels logits = self(imgs) # call forward loss = F.cross_entropy(logits, labels) # including softmax # tensorboard self.log(\u0026#39;train/loss\u0026#39;, loss) self.log(\u0026#39;lr\u0026#39;,get_learning_rate(self.optimizer)) return loss def validation_step(self, batch, batch_idx): \u0026#39;\u0026#39;\u0026#39; Compute acc for every batch \u0026#39;\u0026#39;\u0026#39; imgs, labels = batch logits = self(imgs) loss = F.cross_entropy(logits, labels) acc = torch.sum(torch.eq(torch.argmax(logits, -1),labels).to(torch.float32))/len(labels) log = {\u0026#39;val_loss\u0026#39;:loss, \u0026#39;acc\u0026#39;: acc} return log def validation_epoch_end(self, batch_outputs) -\u0026gt; None: \u0026#39;\u0026#39;\u0026#39; Compute average loss/acc among all batches for one epoch \u0026#39;\u0026#39;\u0026#39; mean_loss = torch.stack([x[\u0026#39;val_loss\u0026#39;] for x in batch_outputs]).mean() mean_acc = torch.stack([x[\u0026#39;acc\u0026#39;] for x in batch_outputs]).mean() self.log(\u0026#39;val/loss\u0026#39;, mean_loss, prog_bar=True) # record loss of every step self.log(\u0026#39;val/acc\u0026#39;, mean_acc, prog_bar=True) # show on the progress bar if __name__ == \u0026#39;__main__\u0026#39;: hparams = get_opts() mnistsystem = MNISTSystem(hparams) # construct training system # save weights to files ckpt_cb = ModelCheckpoint(dirpath=f\u0026#39;ckpts/{hparams.expname}\u0026#39;, filename=\u0026#39;{epoch:d}\u0026#39;, # epoch=0,... monitor=\u0026#39;val/acc\u0026#39;, mode=\u0026#39;max\u0026#39;, save_top_k=5) # only store 5 max acc models\u0026#39; weights (-1 all) # progress bar pbar = TQDMProgressBar(refresh_rate=1) callbacks = [ckpt_cb, pbar] # tensorboard events logger = TensorBoardLogger(save_dir=\u0026#39;logs\u0026#39;, name=hparams.expname, default_hp_metric=False) # trainer = Trainer(max_epochs=hparams.num_epochs, callbacks=callbacks, logger = logger, enable_model_summary = True, # print model structure accelerator=\u0026#39;auto\u0026#39;, # devices type devices = hparams.num_gpus, num_sanity_val_steps = 1, # run once val before training to verfiy if it\u0026#39;s normal benchmark=True, # cudnn accelerate need each batch has same size profiler=\u0026#34;simple\u0026#34; if hparams.num_gpus==1 else None, # count time for every operation ) trainer.fit(mnistsystem) ","date":"2022-08-06T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pl_template-aikui/","title":"memo: PL | Template - AIkui"},{"content":"Â∑•ÂÖ∑Á±ªÔºö\nDataset Áî®‰∫éÊûÑÈÄ†Êï∞ÊçÆÈõÜÔºåÂèØ‰ª•Áî®Á¥¢ÂºïÂèñÂá∫Êï∞ÊçÆ\nDataLoader: ÂèñÂá∫‰∏Ä‰∏™ mini-batchÔºåÂÅöËÆ≠ÁªÉ\n\u0026ldquo;Batch\u0026rdquo; ÊòØÊääÂÖ®ÈÉ®Êï∞ÊçÆËæìÂÖ•Á•ûÁªèÁΩëÁªúÔºåËÆ°ÁÆóÈ¢ÑÊµãÂÄºÔºåËøôÊ†∑ÂèØ‰ª•ÂÖÖÂàÜÂèëÊå•ÂêëÈáèËÆ°ÁÆóÁöÑÂπ∂Ë°åÊÄßÔºåÈÄüÂ∫¶ÂæàÂø´Ôºå‰ΩÜÊòØÂèØËÉΩÂà∞ËææÈûçÁÇπÔºåÊó†Ê≥ïÁªßÁª≠ËÆ≠ÁªÉ„ÄÇÂèØ‰ª•Âà©Áî®Êï∞ÊçÆÁöÑÂô™Â£∞ÈÅøÂÖçÂÅúÁïôÂú®ÈûçÁÇπ‰∏äÔºå‰ªéËÄåËææÂà∞ÂÖ®Â±ÄÊúÄ‰ºòÔºåÊâÄ‰ª•ÈúÄË¶Å1‰∏™Ê†∑Êú¨Ôºå1‰∏™Ê†∑Êú¨Âú∞ËÆ°ÁÆóÔºåÂè´‰ΩúÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçSGDÔºåÊÄßËÉΩËæÉÂ•Ω‰ΩÜÊòØËÆ≠ÁªÉÊó∂Èó¥Â§™Èïø„ÄÇÊâÄ‰ª•ÈááÁî® \u0026ldquo;mini-batch\u0026rdquo; Êù•Âπ≥Ë°°ÈÄüÂ∫¶ÂíåÊÄßËÉΩ„ÄÇÔºàÈÄöÂ∏∏Êäämini-batchÂè´ÂÅöbatchÔºâ\nEpoch: ËÆ≠ÁªÉËΩÆÊï∞ÔºåÊâÄÊúâÁöÑÊ†∑Êú¨ÈÉΩÂÅöËøá‰∏ÄÊ¨°ÂâçÈ¶àÂíåÂèçÈ¶à\nBatch-size: ÊØèÊ¨°ËÆ≠ÁªÉ‰ΩøÁî®ÁöÑÊ†∑Êú¨ÈáèÔºåÁªèËøá‰∏ÄÊ¨°ÂâçÈ¶à„ÄÅÂèçÈ¶àÂíåÊõ¥Êñ∞\nIterationÔºöËø≠‰ª£Ê¨°Êï∞ÔºåÂàÜ‰∫ÜÂá†‰∏™batch\nDataLoader: ÂØπÊîØÊåÅÁ¥¢ÂºïÔºåÊîØÊåÅËé∑ÂèñÂÖ∂ÈïøÂ∫¶ÁöÑÊï∞ÊçÆÈõÜËøõË°åÂä†ËΩΩÔºåÁîüÊàê‰∏Ä‰∏™ iterable loader, ÂØπÊï∞ÊçÆÂàÜÁªÑÔºåÊØèÊ¨°Ëø≠‰ª£ÁªôÂá∫‰∏Ä‰∏™batchÁöÑXÂíåyÔºåÂπ∂Ëá™Âä®ËΩ¨Êç¢ÊàêTensor„ÄÇ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torch from torch.utils.data import Dataset #ÊäΩË±°Á±ª,‰∏çËÉΩÂÆû‰æãÂåñÔºåÂè™ËÉΩË¢´ÂÆÉÁöÑÂ≠êÁ±ªÁªßÊâø from torch.utils.data import DataLoader #Âä†ËΩΩ, ÂàíÂàÜÊï∞ÊçÆ class DiabetesDataset(Dataset): #Ëá™ÂÆö‰πâÁöÑÁ±ªÁªßÊâøËá™Dataset def __init__(self): pass # def __getitem__(self, index): #‰ΩøÂØπË±°ÊîØÊåÅ‰∏ãÊ†áÊìç‰ΩúÔºåÊ†πÊçÆÁ¥¢ÂºïËøîÂõûÊï∞ÊçÆ pass def __len__(self): #ËøîÂõûÊï∞ÊçÆÈõÜÁöÑÊù°Êï∞ pass dataset = DiabetesDataset() #ÂÆû‰æãÂåñÊï∞ÊçÆÈõÜÔºåÊîØÊåÅÁ¥¢ÂºïÊï∞ÊçÆÂíåËé∑ÂèñÈïøÂ∫¶ train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) #ÂÆû‰æãÂåñÂä†ËΩΩÂô®, Ê¥óÁâå‰ΩøÊØèÊ¨°epochÁöÑÊï∞ÊçÆÈÉΩ‰∏ç‰∏ÄÊ†∑Ôºå‰ΩøÁî®2‰∏™Âπ∂Ë°åËøõÁ®ãËØªÂèñÊï∞ÊçÆ Âä†ËΩΩÊï∞ÊçÆÊúâ‰∏§ÁßçÊñπÂºèÔºåÂ¶ÇÊûúÊï∞ÊçÆÈáèÂ∞èÔºåÁõ¥Êé•ÊääÊâÄÊúâÁöÑÊï∞ÊçÆÂä†ËΩΩËøõÂÜÖÂ≠òÔºå‰πãÂêé‰ΩøÁî®getitemËØªÂèñÔºõÂΩìÊï∞ÊçÆÈõÜÂæàÂ§ßÊó∂ÔºåÈúÄË¶ÅÂ∞ÜÂÖ∂ÊãÜÂàÜÊàêÂ∞èÊñá‰ª∂ÔºåÂú®__init__ÊñπÊ≥ï‰∏≠ÂàùÂßãÂåñÂàóË°®ÔºåÊääÂ∞èÊñá‰ª∂ÂêçÊîæÂÖ•ÂàóË°®‰∏≠ÔºõÊúâÊó∂Ê†áÁ≠æ‰πüÂæàÂ§ßÔºåÊØîÂ¶ÇÂØπÂõæÁâáÁöÑÊØè‰∏™ÂÉèÁ¥†È¢ÑÊµãËØ≠‰πâ‰ø°ÊÅØÔºåyÁöÑÁª¥Â∫¶‰∏éxÁõ∏ÂêåÔºåÊâÄ‰ª•‰πüÈúÄË¶ÅÊãÜÂàÜÔºåÊñá‰ª∂ÂêçÊîæÂÖ•ÂàóË°®„ÄÇÂú®ÈúÄË¶Å‰ΩøÁî®ÊüêÊï∞ÊçÆÁöÑÊó∂ÂÄôÔºåÁî®__getitem__ÊñπÊ≥ïËØªÂèñ„ÄÇ\npytorch 0.4‰∏≠Âá∫Áé∞‰∏Ä‰∏™ÈóÆÈ¢òÔºöÁî±‰∫éwindows Âíå linuxÁöÑÂ§öËøõÁ®ãÂ∫ì‰∏çÂêåÔºåspawn‰∏éfork‰∏çÂêå„ÄÇWindows‰∏ãÈúÄË¶ÅÊääËø≠‰ª£ÁöÑloaderÂ∞ÅË£ÖËµ∑Êù•Ôºå‰∏çËÉΩÁõ¥Êé•‚ÄúÈ°∂Ê†º‚ÄùÂÜô\n1 2 3 4 5 6 train_loader = DataLoder(...) if __name__ == \u0026#39;__main__\u0026#39;: for epoch in range(100): for i, data in enumerate (train_loader, 0): ... Âä†ËΩΩÁ≥ñÂ∞øÁóÖÊï∞ÊçÆÈõÜ:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import torch from torch.utils.data import Dataset, DataLoader class DiabetesDataset(Dataset): def __init__(self, filepath): #ÈúÄË¶ÅÊñá‰ª∂Âêç xy = np.loadtxt(filepath, delimiter=\u0026#39;,\u0026#39;, dtype=np.float32) self.len = xy.shape[0] #Ê†∑Êú¨Êù°Êï∞ÔºöÁü©ÈòµÁöÑÁ¨¨‰∏Ä‰∏™Áª¥Â∫¶ self.x_data = torch.from_numpy(xy[:, :-1]) #Ê†∑Êú¨, Áõ¥Êé•ÊîæÂú®ÂÜÖÂ≠ò‰∏≠ self.y_data = torch.from_numpy(xy[:, [-1]]) #Ê†áÁ≠æÂèñÊúÄÂêé‰∏ÄÂàó def __getitem__(self, index): return self.x_data[index], self.y_data[index] #ËøîÂõû‰∏Ä‰∏™ÂÖÉÁªÑ def __len__(): return self.len data = DiabetesDataset(\u0026#39;diabetes.csv.gz\u0026#39;) train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) for epoch in range(100): for i, data in enumerate(train_loader, 0): #Ëø≠‰ª£Âä†ËΩΩÂô®ÔºåenumerateËé∑ÂæóÁ¨¨Âá†Ê¨°Ëø≠‰ª£ÔºåxÂíåyÁöÑÂÖÉÁªÑÊîæÂÖ•data inputs, labels = data #ÂèñÂá∫Ê†∑Êú¨ÂíåÊ†áÁ≠æ y_pred = model(inputs) #ËÆ°ÁÆóÈ¢ÑÊµãÂÄº loss = criterion(y_pred, labels) #ËÆ°ÁÆóÊçüÂ§± optimizer.zero_grad() #Ê¢ØÂ∫¶Ê∏ÖÈõ∂ loss.backward() #ÂèçÂêë‰º†Êí≠ optimizer.step() #Êõ¥Êñ∞Ê¢ØÂ∫¶ ","date":"2022-08-05T19:25:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/8_%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/","title":"watch: PyTorch - Âàò‰∫å 08 | Load Datasets"},{"content":"Ê≥®ÊÑèÂäõÊ†∏ÂøÉ‰ΩúÁî®ÔºöÁâπÂæÅËûçÂêà„ÄÇÂú®ËæìÂÖ•ÁöÑÂ∫èÂàóÂÜÖÂÅöÁâπÂæÅËûçÂêà„ÄÇ\nbilibili\ncnblog\n‰ªéÊµ∑ÈáèÊï∞ÊçÆ‰∏≠ÊâæÂá∫ÈáçË¶ÅÁöÑÈÉ®ÂàÜÔºåËÆæËÆ°‰∏ÄÁßçËøêÁÆóËÆ©Êú∫Âô®Ëá™Âä®ÁîÑÂà´Êï∞ÊçÆÔºåÂØπ‰∫éÊ∑±Â∫¶Â≠¶‰π†È¢ÜÂüüÔºå‰∏∫Ê®°ÂûãÊ∑ªÂä†‰∏Ä‰∏™Ê®°ÂùóÔºåÂè™ÂÖ≥ÂøÉËæìÂÖ•ËæìÂá∫Âç≥ÂèØ\nÂ¶Ç‰ΩïÂú®Ê∑±Â∫¶Â≠¶‰π†Ê®°Âûã‰∏äÂÅöÊ≥®ÊÑèÂäõ?\n‰∏ÄÂº†ÂõæÁâáÁöÑ‰ø°ÊÅØÂàÜÂ∏ÉÊòØ‰∏çÂùáÂåÄÁöÑÔºå‰∫∫Á±ª‰ºöÁªôËï¥Âê´Êõ¥Â§ö„ÄÅÊõ¥ÈáçË¶Å‰ø°ÊÅØÁöÑÂå∫ÂüüÊõ¥Â§öÁöÑÊ≥®ÊÑèÂäõ„ÄÇ\nÂèÇ‰∏é‚ÄúÊ≥®ÊÑèÂäõ‚ÄùÁöÑ‰∏§‰∏™‰∏ª‰ΩìÔºö‰∫∫ËÑëÔºàqueryÔºâÔºåÂõæÁâáÔºàvalue=keyÔºâÔºå‰∫∫ËÑëÁü•ÈÅìÂì™‰∫õÂú∞ÊñπÈáçË¶ÅÔºåÂÆÉ‰ºöÁùÄÈáçÁúãÂõæÁâá‰∏≠ÁöÑÈÇ£‰∫õÂú∞ÊñπÔºåÊâÄ‰ª•Â∫îËØ•ÁªôÈÇ£‰∫õÂå∫ÂüüÊõ¥È´òÁöÑÊùÉÈáçÔºå‰πüÂ∞±ÊòØËÆ°ÁÆó‰∫∫ËÑë‰∏≠ÁöÑ‚ÄúÈáçË¶ÅÂ∫¶ÂàÜÂ∏É‚ÄùÊ®°Áâà‰∏éÂõæÁâáÁöÑÁõ∏‰ººÂ∫¶„ÄÇ\nquery ‰∏é key (=value) ÂèØ‰ª•ÂÅöÁÇπ‰πòÔºåËÆ°ÁÆó‰ΩôÂº¶Áõ∏‰ººÂ∫¶„ÄÇ ‰∏Ä‰∏™ query ‰∏é keys ÁöÑÂêÑ‰∏™ÈÉ®ÂàÜÂÅöÂÜÖÁßØÂæóÂà∞Â§ö‰∏™Ê†áÈáèÔºåÂ∞±ÊòØÂêÑ‰∏™ÈÉ®ÂàÜ‰∏é query ÁöÑÁõ∏‰ººÂ∫¶ÔºåÂç≥Ê≥®ÊÑèÂäõÂàÜÊï∞\nÂØπËøô‰∫õÂàÜÊï∞ÂÅö‰∏ÄÊ¨° softmax()ÔºåÂæóÂà∞ÂêÑ‰∏™key ‰∏é queryÁöÑÁõ∏‰ººÂ∫¶ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÂ∞±ÊòØÂêÑ‰∏™ÈÉ®ÂàÜÁöÑÊùÉÈáçÔºåÊúâ‰∫õÈÉ®ÂàÜÊùÉÈáçË∂ã‰∫é0ÔºåÂàôÂÆÉ‰ª¨Ë¢´ËàçÂºÉ‰∫ÜÔºåÂè™ÊääÈáçÁÇπÊèêÂèñÂá∫Êù•ÔºåÊääÂêÑ‰∏™ÈÉ®ÂàÜÂä†ÊùÉÊ±ÇÂíåÔºåÂ∞±ÂæóÂà∞\u0026quot;Ë¢´Ê≥®ÊÑèËøáÂêé\u0026quot;ÁöÑÂõæÁâá‰∫ÜÔºåÊØîÂéüÂßãÂõæÁâáÂ§ö‰∫ÜÊØèÈÉ®ÂàÜÁöÑÈáçË¶ÅÂ∫¶„ÄÇÂØπËøô‰∏™Êñ∞ÂõæÁâáÂÜçÂÅöÂç∑ÁßØÔºåÈáçË¶ÅÁöÑÈÉ®ÂàÜ‰ºöË¢´‰øùÁïô‰∏ãÊù•„ÄÇ\n(Â¶ÇÊûú‰∏ÄÂº†ÂõæÁâá‰ª•ÂÉèÁ¥†Á∫ßÂà´ÂÅöÊ≥®ÊÑèÂäõÔºåËÆ°ÁÆóÈáèÂ§™Â§ßÔºåÊâÄ‰ª•ÂèØ‰ª•ÂÖàÁî®Âç∑ÁßØÊèêÂèñ‰∏Ä‰∏ãÁâπÂæÅÔºåÊääÂ∞∫ÂØ∏Èôç‰∏ãÊù•ÔºåÂ¶ÇÊûúÂç∑ÁßØÂ§™Â§öÊ¨°‰∫ÜÔºå‰∏éÂéüÂßãÂõæÁâáÂ∑ÆÂºÇÂ§™Â§ß‰∫ÜÔºåÂèØ‰ª•Áî®Ë∑≥ËøûÊé•Ë°•ÂÖÖÂéüÂßã‰ø°ÊÅØ)\n‰∏ÄËà¨ keys = valuesÔºå‰πüÂèØ‰ª•‰∏çÁõ∏Á≠âÔºå‰ΩÜ‰∫åËÄÖ‰πãÈó¥ÂøÖÊúâÊüêÁßçËÅîÁ≥ªÔºåËøôÊ†∑ÊâçËÉΩ‰æùÊçÆ query ‰∏é keys ÁöÑÁõ∏‰ººÂ∫¶ÊâæÂá∫ values ‰∏≠ÈáçË¶ÅÁöÑÂíå‰∏çÈáçË¶ÅÁöÑÈÉ®ÂàÜ„ÄÇ\nÊØè‰∏Ä‰∏™ query ÈÉΩ‰∏é keys ÁÇπÁßØÔºåÂàô‰ºö‰∫ßÁîü num_query x num_key ÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞Áü©Èòµ\nÂÜÖÁßØÂæóÂà∞ÂêÑ‰∏™ key ÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞‰πãÂêéÔºåÈô§‰ª•Ê†πÂè∑‰∏ãÊñπÂ∑Ædk (Ê†áÂáÜÂ∑Æ)ÔºåÂÅö‰∏Ä‰∏™Áº©ÊîæÊòØ‰∏∫‰∫ÜÂπ≥ÊªëÂêÑÂàÜÊï∞Èó¥ÁöÑÂ∑ÆË∑ùÔºåÂ¶ÇÊûúÂ∑ÆÈ¢ùÂ§™Â§ßÔºåÂÅö‰∫ÜeÊåáÊï∞ÔºåÈ´òÂàÜÊï∞Ê¶ÇÁéá‰ºöÂæàÂ§ßÔºåËÄå‰ΩéÂàÜÊï∞ÁöÑÊ¶ÇÁéá‰ºöÂæà‰ΩéÔºåÁÑ∂Âêé‰πò‰ª• value Â∞±ÊòØÂèòÊàêÂæàÂ∞èÁöÑÂÄºÔºåÂØºËá¥Âú®ÂèçÂêë‰º†Êí≠Êó∂ÔºåËøô‰∫õÂ∞èÊï∞ÂÄºÊ≤°ÊúâË∂≥Â§üÁöÑÊ¢ØÂ∫¶ÔºàÊ¢ØÂ∫¶Ê∂àÂ§±ÔºâÔºåÊõ¥Êñ∞‰∏ç‰∫ÜÂÆÉÂØπÂ∫îÁöÑÊùÉÈáç„ÄÇÊàñËÄÖËØ¥Â¶ÇÊûú‰∏çÈô§‰ª•Ê†áÂáÜÂ∑ÆÔºåÂêÑkeyÊ≥®ÊÑèÂäõÂàÜÊï∞ÁöÑÂàÜÂ∏ÉÁöÑÊñπÂ∑ÆÂèØËÉΩÂæàÂ§ßÔºåÂ§ßÁöÑÂæàÂ§ßÔºåÂ∞èÁöÑÂæàÂ∞èÔºåÂÅö‰∫ÜsoftmaxÂêéÔºåÂ∞èÊï∞Êõ¥Â∞è„ÄÇÊâÄ‰ª•Èô§‰ª•Ê†áÂáÜÂ∑ÆÔºåËÆ©ÊñπÂ∑Æ‰∏∫1ÔºåÂâäÂº±Â∑ÆË∑ù„ÄÇ(‰∏∫‰ªÄ‰πà‰∏çËÆ©ÊñπÂ∑ÆÂÜçÂ∞èÂë¢Ôºü)\n","date":"2022-08-05T17:37:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/9_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","title":"watch: Transf - Nick 09 | Attention Mechanism"},{"content":"Code | Arxiv | ProjPage\n‰∏ÄÂè•ËØùÔºöÁî®ÂõæÂÉèÁâπÂæÅËÆ≠ÁªÉNeRFÔºåÔºàÊâÄ‰ª•ÊúâÊ≥õÂåñËÉΩÂäõ,‰∏çÂ±ÄÈôê‰∫éÂçïÂú∫ÊôØÔºå‰∏çÂÉènerfÂè™ÂÅö‰ºòÂåñÔºâ\nÂ¶ÇÂõæ1ÔºåÂÖâÁ∫øÊòØÈöèÊú∫‰ªéÂÉèÁ¥†Á©∫Èó¥ÈÄâÊã©ÁöÑÔºà‰∏∫‰∫ÜÈöèÊú∫ÈÄâ‰∏Ä‰∫õ(N‰∏™)Á©∫Èó¥ÁÇπÔºâÔºåÁ©∫Èó¥ÁÇπÊäïÂΩ±Âà∞‰∏çÂêåËßÜÂõæÁöÑ feature maps (512 chnls) ‰∏äÔºåÂæóÂà∞ (N‚àóNviews‰∏™) feature volumeÔºåÈÄÅÂÖ• mlp ÂèòÊç¢Êàê r,g,b,densityÔºåÂÜçÂÅö volume rendering\nAbstract NeRF conditioned by few images costly independent optimizing use convolution to learn scene prior no explicit 3D supervision single image novel view synthesis task 1 Introduction Topic: Synthesis novel views for a scene from sparse views\nProblem\nFormer solution \u0026amp; drawbacks:\nDifferentiable neural rendering: represent the sence as a neural network which generates rendered images NeRF: encode the 3D position and view dirs to volume density and color. It needs too many images. Task: predicting NeRFs from one or several images.\nContributions: Utilize pixel-aligned spatial image features to learn scene priors (specifically) Input image ‚Üí convolutional feature grid ‚Üí ùê±,ùêù,feature(residual) ‚Üí NeRF (pool-based multi-view features fusion) ‚Üí œÉ,rgb\nResults statement\nno 3D supervision: 3D shape or object masks not trained in a canonical coordinate system, but in each camera coordinate system \u0026ldquo;convolution preserves the spatial alignment between the image and the output 3D representation\u0026rdquo; flexibility on number of input views in test period (Experiments) ShapeNet; DTU\n2 Related Work Novel View Synthesis Learning-based 3D reconstruction Viewer-centric 3D reconstruction 3 Background NeRF (job) (input-output) x,d -\u0026gt; density, color (training manner) volume rendering (loss func) Limitation: only use geometric consistency, resulting in individual optimization for every view 4 Image-conditioned NeRF (improvement) input spatial image features into network. (Specifically) Two components: convolutional encoder + nerf mlp. Spatial query position is drawn from camera space.\n(paragraph order) From simple case to general case\n4.1 Single-Image pixelNeRF (main idea) The inputs are all from view space. (steps) Input image ‚Üí feature volume W=E(I) ‚Üí assign image feature to sample points by projecting the 3D position to 2D location ‚Üí NeRF network (pipeline) (the role of query view direction) 4.2 Incorporating Multiple Views (main idea) extract information from multi views to resolve geometric ambiguities and able to accept multiple images in test time. No relation with world space\nWorld space can base on any view. (notion explain)\nProject the world query point into each input view space. The fisrt layer processes each view independently and the final layer aggregates all views.\n(model pipeline)\nEncode each input img into feature volume and retrieve the corresponding feature at the sample point\u0026rsquo;s projecting location. The features companied with (xyz,viewdirs) are input to the first layer and all the intermediate vectors are fused by average pooling before entering the final layers, which compress the vector into density and color. 5 Experiments Datasets:\nShapeNet (category-specific and category-agnostic view synthesis); ShapNet scenes with unseen categories and multiple objects; DTU rescaled to 1/4: 400x300 Baselines: SRN, DVR\nMetrics: PSNR, SSIM, LPIPS\nImplementation Details:\nImage encoder: ResNet34; Features: non-pooling, upsampled using bilinear interpolation, concatenated feature maps of every image. Combine the points\u0026rsquo; position and view direction in a ResNet manner (residual) 5.1 ShapeNet Benchmarks ShapeNet for category-specific and category-agnostic\nCategory-specific View Synthesis Benchmark\none-shot and two-shot view synthesis\nA model is trained for reconstructing one category and the used images contains 50 random views per object instance. Only 2 are encoded and fed into network.\nAblations. The benefit of local features\nCategory-agnostic Object Prior\nTrain a single model to the 13 largest categories of ShapeNet pick one view randomly from 24 fixed elevation view of each object instance. 5.2 Pushing the Boundaries of ShapeNet (tasks) less controlled capture scenarios in ShapeNet:\nunseen object categories, multiple-object scene, simulation-to-real transfer on car images. Generalization to novel categories: apply the model on unseen categories\nMultiple-object scenes: The geometric features need can be applied in any view direction (360¬∞)\n5.3 Scene Prior on Real Images Code (2023-07-20)\ncalc_losses():\n\\begin{algorithm} \\caption{Main steps of clac$\\_$losses()} \\begin{algorithmic} \\PROCEDURE{clac-losses}{} \\STATE Sample 4 objects from 88 training scanned objects (folder) as a super batch \\STATE Each object samples randomly 1 or 3 from its total of 49 NV as target images for training \\STATE Sample a batch of rays (N$\\_$rand) \\STATE Extract latent feature maps `self.latent` \\STATE Sample z$\\_$coarse points on the rays \\STATE Transform points onto NS camera space \\STATE Perspective project xyz to 2D location uv on feature maps \\STATE Indexing latent vectors from feature maps by `F.grid$\\_$sample()` \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} (2024-06-06)\nNeuRay: PixelNeRF is an analogy to stereo matching. Intro\nMVSNet projects multiple hypotheses depth planes onto source images (feature maps) and then matches multi-view features with a UNet. Similarly, PixelNeRF projects multiple points on each ray at sampled depth onto the source images.\nMVSNet don\u0026rsquo;t have the problem of occulsion because all it deals with is a photo (plane) without 3D structure from begining to end.\nWhereas NeRF handles the entire 3D space, the occulusion could happen when viewpoint changes.\nNeuRay use image features to determine if a sample point on a ray is visble or not.\nDTU dataset (2023-08-17)\nget_split_dataset() (in \u0026ldquo;data/init.py\u0026rdquo;) will construct different Dataset objects for different datasets.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 flags[\u0026#34;list_prefix\u0026#34;] = \u0026#34;new_\u0026#34; if training: flags[\u0026#34;max_imgs\u0026#34;] = 49 flags[\u0026#34;sub_format\u0026#34;] = \u0026#34;dtu\u0026#34; flags[\u0026#34;scale_focal\u0026#34;] = False flags[\u0026#34;z_near\u0026#34;] = 0.1 flags[\u0026#34;z_far\u0026#34;] = 5.0 # Apply color jitter during train train_aug = ColorJitterDataset train_aug_flags = {\u0026#34;extra_inherit_attrs\u0026#34;: [\u0026#34;sub_format\u0026#34;]} train_set = DVRDataset(path=datadir, stage=\u0026#34;train\u0026#34;, **flags, **kwargs) if train_aug is not None: train_set = train_aug(train_set, **train_aug_flags) ","date":"2022-08-04T00:00:00Z","image":"https://ar5iv.labs.arxiv.org/html/2012.02190/assets/x2.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-pixelnerf/","title":"read: Render - NVS | PixelNeRF"},{"content":"Self-Attention(Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂) - bilibili\nvalue ÊòØ‰∏Ä‰∏™ d Áª¥ÁöÑÂêëÈáèÔºõ Áé∞ÂàÜÊûê n ‰∏™ value:\nself-attention ÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫Ü‰ΩøÊØè‰∏™ value Â∏¶‰∏äÊâÄÊúâ value ÁöÑ‰ø°ÊÅØÔºå‰πüÂ∞±ÊòØÊâÄÊúâ value Âä†ÊùÉÁõ∏Âä†ÔºåÁõ∏‰ººÈÉ®ÂàÜÁöÑÊùÉÈáçÂ§ß‰∫õÔºåÊó†ÂÖ≥ÈÉ®ÂàÜÁöÑÊùÉÈáçÂ∞è‰∫õ„ÄÇ\nÊùÉÈáçÂç≥ÊòØÂêÑ value ‰∏é value ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÔºåÂè™ÈúÄÊØè‰∏™ value ÈÉΩ‰∏éÊâÄÊúâ value ÂÅöÂÜÖÁßØÂ∞±ÂèØÂæóÂà∞ÔºåÂÜÖÁßØÂ§ßËØ¥Êòé‰∏§‰∏™valueÂ§πËßíÂ∞èÔºåÂê´‰πâÁõ∏‰ººÔºõÂ¶ÇÊûúÂêÑvalue‰πãÈó¥Ê≠£‰∫§ÔºåÊØîÂ¶Çone-hotÁºñÁ†ÅÔºåÈÇ£‰πàÂè™ÊúâËá™Â∑±ÂíåËá™Â∑±‰πòÁªìÊûúÊòØ1ÔºåÂÖ∂‰ΩôÈÉΩÊòØ0„ÄÇ\nÁî®Áü©ÈòµËøêÁÆóË°®Á§∫Â∞±ÊòØ‚ÄúËá™Â∑±‰πò‰ª•Ëá™Â∑±ÁöÑËΩ¨ÁΩÆ‚Äù (n,d)‚ãÖ(n,d)·µÄ‚Üí(n,n)ÔºåÂæóÂà∞ nxn ÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞ÊñπÈòµ\nÊääËøôÂ•óÊú∫Âà∂ÊîæÂà∞AttentionËøêÁÆó‰∏≠Ôºåqueries Âíå keys ÈÉΩÁ≠â‰∫é values„ÄÇ\n‰ΩÜÊòØËøô‰∫õÊùÉÈáçÊòØ‰∏çÂèòÁöÑÔºåÂõ†‰∏∫ÊØèÊ¨° querys ÈÉΩÁ≠â‰∫é keysÔºåÂØπÂçèÊñπÂ∑ÆÁü©ÈòµÂÅöÂÆåsoftmax ÂæóÂà∞ÁöÑÂàÜÂ∏ÉÊÄªÊòØ‰∏ÄÊ†∑ÁöÑÔºü doubt: ÂÆûÈ™åÔºö‰∏§‰∏™Áü©ÈòµÂàÜÂà´Ëá™Â∑±ÂíåËá™Â∑±ÂÅöÁü©Èòµ‰πòÊ≥ïÔºåÁÑ∂ÂêéÂÅösoftmaxÔºåËßÇÂØü‰∏§‰∏™ÁªìÊûú\nÂ¶ÇÊûú‰ªÖÂá≠ values Ëá™Â∑±Ë∞ÉÊï¥ÔºåË¶ÅËææÂà∞‰ªªÂä°ÊâÄÈúÄÁöÑÂêëÈáèË°®Á§∫ÔºåÂèØËÉΩÈÄüÂ∫¶Â§™ÊÖ¢ÔºåÊâÄ‰ª•ÂàÜÂà´Áªô q,k,v Âä†‰∫Ü‰∏ÄÂ±ÇÁ∫øÊÄßÂèòÊç¢ÔºåÂèçÂêë‰º†Êí≠Êó∂‰πüË∞ÉÊï¥Ëøô‰∏™Á∫øÊÄßÂ±ÇÁöÑÊùÉÈáçÔºåËÆ© value ÁöÑÊñ∞ËØçÂêëÈáèÊõ¥Âø´ÁöÑÁßªÂä®Âà∞ÂáÜÁ°ÆÁöÑ‰ΩçÁΩÆ„ÄÇ\u0026ldquo;‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊãüÂêàËÉΩÂäõ1\u0026quot;„ÄÇ\nSelf-attention ‰∏≠ÊØè‰∏Ä‰∏™ input ÈÉΩ‰∏éÊâÄÊúâ input ÂÅöÂÜÖÁßØÔºåÊ≤°ÊúâËÄÉËôëÂà∞ input ÁöÑÈ°∫Â∫èÔºåÊâÄ‰ª•ÂéüÂßãÊñáÊú¨ÁöÑÈ°∫Â∫è‰ø°ÊÅØ‰∏¢Â§±‰∫ÜÔºåÊâÄ‰ª•ÈúÄË¶Å‰ΩçÁΩÆÁºñÁ†Å 1\n‰æãÂ≠êÔºö\n‰∏§‰∏™ÂçïËØç thinking ‰∏é machinesÔºåÂàÜÂà´‰πò‰∏ä Wq, Wk, Wv ÂæóÂà∞Á∫øÊÄßÂèòÊç¢ÂêéÁöÑ queries (q1,q2), keys (k1,k2), values (v1,v2)\ns11 = q1 √ó k1, s12 = q1 √ó k2; ÁÑ∂Âêé s11, s12 ÂÅöscaleÔºåÂÜçÂÅösoftmaxÂæóÂà∞‰∏§‰∏™ÊùÉÂÄº s11\u0026rsquo; (0.88), s12\u0026rsquo; (0.12), Âàô z1 = s11\u0026rsquo; √ó v1 + s12\u0026rsquo; √ó v2 Â∞±ÊòØ thinking ÁöÑÊñ∞ÁöÑÂêëÈáèË°®Á§∫„ÄÇ\nÂØπ‰∫é thinkingÔºåÂàùÂßãÁöÑËØçÂêëÈáèÔºàone-hot, ElmoÔºâ‰∏∫ x1ÔºåËøô‰∏™x1 ‰∏éÂÖ∂‰ªñËØçÁöÑÂêëÈáèÊ≠£‰∫§ÔºåÊó†ÂÖ≥Ôºå‰∏çÂåÖÂê´ÂÖ∂‰ªñÂçïËØçÁöÑ‰ªª‰Ωï‰ø°ÊÅØÔºåÂØπ thinking machines Ëøô‰∏§‰∏™ËØçÂÅöÂÆå self-attention ‰πãÂêéÔºåthinking Êñ∞ÁöÑËØçÂêëÈáèÂ∏¶‰∏ä‰∫Ü machines ÁöÑ‰ø°ÊÅØÔºåÂ∏¶‰∏ä‰∫ÜÂ§öÂ∞ëÂë¢ÔºüÂ∏¶‰∏ä‰∫Ü machines ‰∏é thinking Áõ∏‰ººÁöÑÈÉ®ÂàÜ„ÄÇÊàñËÄÖËØ¥ÔºöËøô‰∏™Êñ∞ËØçÂêëÈáèËï¥Âê´‰∫Ü thinking machines ËøôÂè•ËØùÂØπ‰∫é thinking ËÄåË®ÄÂì™‰∏™ËØçÊõ¥ÈáçË¶ÅÁöÑ‰ø°ÊÅØ„ÄÇ\nÊñ∞ÁöÑËØçÂêëÈáèÂåÖÂê´‰∫ÜÊï¥‰∏™Âè•Â≠êÊâÄÊúâÂçïËØçÁöÑ‰ø°ÊÅØÔºåÈáçË¶ÅÁöÑÂçïËØçÂç†ÊØîÂ§ö‰∏ÄÁÇπ„ÄÇ\nAttention ‰∏é Self-attention Âå∫Âà´Ôºö QKVÁõ∏‰πòÂ∞±ÊòØÊ≥®ÊÑèÂäõÔºåÂè™ÊòØ‰∏ÄÁßçËøêÁÆóÔºå‰ΩÜÂπ∂Ê≤°ÊúâËßÑÂÆö QKV ÊòØÊÄé‰πàÊù•ÁöÑ„ÄÇÈÄöËøá‰∏Ä‰∏™Êü•ËØ¢ÂèòÈáè Q ÂéªÊâæÂà∞ V ÈáåÈù¢ÊØîËæÉÈáçË¶ÅÁöÑ‰∏úË•øÔºöQKÁõ∏‰πòÊ±ÇÁõ∏‰ººÂ∫¶ SÔºåÁÑ∂Âêé SV Áõ∏‰πòÂæóÂà∞ V ÁöÑÊñ∞ÁöÑÂêëÈáèË°®Á§∫ÔºàÂØπ‰∫éËØçÂêëÈáèÔºåÂàôÂÆÉÂåÖÂê´‰∫ÜÂè•Ê≥ï/ËØ≠‰πâÁâπÂæÅÔºâ„ÄÇQ ÂèØ‰ª•ÊòØ‰ªª‰Ωï‰∏Ä‰∏™‰∏úË•øÔºåV‰πüÊòØ‰ªª‰Ωï‰∏Ä‰∏™‰∏úË•øÔºåK ÂæÄÂæÄÊòØÂíåV ÂêåÊ∫êÁöÑ\nËá™Ê≥®ÊÑèÂäõË¶ÅÊ±Ç QKV ÂêåÊ∫êÔºåQKV ÊòØÂØπÂêå‰∏Ä‰∏™ X ‰πò‰∏ä‰∫Ü‰∏çÂêåÁöÑÊùÉÈáçÁü©ÈòµÔºåÂÅö‰∫Ü‰∏çÂêåÁöÑÁ∫øÊÄßÂèòÊç¢Ôºå‰ΩøÂÆÉ‰ª¨Âú®Á©∫Èó¥‰∏≠Â≤îÂºÄ‰∫ÜÔºåX Â∞±ÊòØ‰∏Ä‰∏™ËØçÂêëÈáèÔºå‰ΩÜÂÆÉË°®ËææÁöÑ‰ø°ÊÅØÂèØËÉΩÊ≤°ÈÇ£‰πàÂáÜÁ°ÆÔºåÈÄöËøáÂèçÂêë‰º†Êí≠Ë∞ÉÊï¥ÂÆÉ‰ª¨ÁöÑÊùÉÈáçÁü©ÈòµÔºåÊääÂÆÉ‰ª¨ÁßªÂä®‰∫ÜÂêàÈÄÇÁöÑ‰ΩçÁΩÆ‰∏äÔºåËøô‰∏™‰ΩçÁΩÆÁöÑËØçÂêëÈáèÂèØ‰ª•‰∏∫ÊàëÁöÑ‰ªªÂä°ÂáÜÁ°ÆÂú∞ÂàÜÈÖçÂêÑÈÉ®ÂàÜÁöÑÈáçË¶ÅÊÄß„ÄÇ\n‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºöQ Âíå V ‰∏çÂêåÊ∫êÔºå‰ΩÜ K Âíå V ÂêåÊ∫ê\nSelf-Attention ‰ª£Á†ÅÂÆûÁé∞pytorch 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from math import sqrt import torch import torch.nn class SelfAttentionLayer(nn.Module): def __init__(self, input_dim, dim_k, dim_v): \u0026#39;\u0026#39;\u0026#39; Inputs: input_dim: dim of input feature vector dim_k: dim of key is same as query, because they do dot product dim_v: dim of value, suitable for task \u0026#39;\u0026#39;\u0026#39; super(SelfAttentionLayer, self).__init__() self.q = nn.Linear(input_dim, dim_k) self.k = nn.Linear(input_dim, dim_k) self.v = nn.Linear(input_dim, dim_v) self._norm_fact = 1/sqrt(dim_k) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; Input: x: (batch_size, seq_len, input_dim) \u0026#39;\u0026#39;\u0026#39; Q = self.q(x) # (batch_size, seq_len, dim_k) K = self.k(x) # (batch_size, seq_len, dim_k) V = self.v(x) # (batch_size, seq_len, dim_v) atten = nn.Softmax(dim=-1)(torch.bmm(Q,K.permute(0,2,1))) * self._norm_fact # (bs,seq_len,seq_len) output = torch.bmm(atten, V) # Q * K.T() * V, (bs, seq_len, dim_v) return output if __name__ == \u0026#39;__main__\u0026#39;: X = torch.randn(4,3,2) self_atten = SelfAttentionLayer(input_dim=2, dim_k=4, dim_v=5) res = self_atten(X) print(res.size()) Multi-head self-attention:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SelfAttentionMultiHead(nn.Module): def __init__(self, input_dim, dim_k, dim_v, num_heads): super(SelfAttentionMultiHead, self).__init__() assert dim_k % num_heads == 0 # dim_k is divided by num_heads assert dim_v % num_heads == 0 self.q = nn.Linear(input_dim, dim_k) self.k = nn.Linear(input_dim, dim_k) self.v = nn.Linear(input_dim, dim_v) self.num_heads = num_heads self.dim_k = dim_k self.dim_v = dim_v self._norm_fact = 1/sqrt(dim_k) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; Input: x: (batch_size, seq_len, input_dim) \u0026#39;\u0026#39;\u0026#39; # ËøáÁ∫øÊÄßÂ±Ç‰πãÂêéÔºåÊãÜÂàÜÊàêÔºànum_heads, bs, seq_len, dim_k√∑num_headsÔºâ Q = self.q(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.num_heads) K = self.k(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.num_heads) V = self.v(x).reshape(-1, x.shape[0], x.shape[1], self.dim_v // self.num_heads) print(x.shape) print(Q.size()) # Q * K.T() atten = nn.Softmax(dim=-1)(torch.matmul(Q, K.permute(0,1,3,2))) # (bs, seq_len, seq_len) # Q * k.T() * V output = torch.matmul(atten, V).reshape(x.shape[0], x.shape[1], -1) # (bs, seq_len, dim_v) return output Self-Attention - TF 2 1 2 3 4 5 6 7 8 9 10 11 12 def single_head_attention(z): # z: [None, n, dm] Q = tf.keras.layers.Dense(units = dk)(z) # Q: [None, n, dk] K = tf.keras.layers.Dense(units = dk)(z) V = tf.keras.layers.Dense(units = dv)(z) score = tf.matmul(Q, K, transpose_b = True)/ tf.sqrt(dk * 1.0) # score : [None, n, n] W = tf.nn.softmax(score, axis = -1) # W: [None, n, n] H = tf.matmul(W, V) # H: [None, n, dv] return H Multi-head self-attention 3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, dk, dv, num_heads): \u0026#39;\u0026#39;\u0026#39; Inputs: dk: dim of keys after linear layer num_heads: the dk is divided into num_heads parts \u0026#39;\u0026#39;\u0026#39; super(MultiHeadAttention, self).__init__() assert dk % num_heads == 0 assert dv % num_heads == 0 self.dk = dk self.dv = dv self.num_heads = num_heads self.wq = tf.keras.layers.Dense(dk) self.wk = tf.keras.layers.Dense(dk) self.wv = tf.keras.layers.Dense(dv) def call(self, x): \u0026#39;\u0026#39;\u0026#39; x: a batch of sequences that need to do self-attention (batch_size, seq_len, d_input) \u0026#39;\u0026#39;\u0026#39; seq_len = tf.shape(x)[1] q = self.wq(x) # (batch_size, seq_len, dk) k = self.wk(x) # (batch_size, seq_len, dk) v = self.wv(x) # (batch_size, seq_len, dv) # Split the last dimension and transpose to (bs, num_heads, seq_len_q, dk) Q = tf.transpose(tf.reshape(q, (-1, seq_len, self.num_heads, self.dk//self.num_heads)), perm=[0,2,1,3]) K = tf.transpose(tf.reshape(k, (-1, seq_len, self.num_heads, self.dk//self.num_heads)), perm=[0,2,1,3]) V = tf.transpose(tf.reshape(v, (-1, seq_len, self.num_heads, self.dv//self.num_heads)), perm=[0,2,1,3]) # Attention score = tf.matmul(Q, K, transpose_b=True)/ tf.sqrt(self.dk // self.num_heads * 1.0) # scale by the \u0026#34;dk\u0026#34; of one head attention_weights = tf.keras.activations.softmax(score) scaled_attention = tf.matmul(attention_weights, V) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, d_subspace) concat_attention = tf.reshape(scaled_attention, (-1, seq_len, self.dv)) # (batch_size, seq_len_q, dv) return concat_attention, attention_weights Refer:\nË∂ÖËØ¶ÁªÜÂõæËß£Self-Attention - ‰ºüÂ§ßÊòØÁÜ¨Âá∫Êù•ÁöÑÁöÑÊñáÁ´† - Áü•‰πé Transformer I - zaidalyafeai/AttentioNN - github TF tutorial transformer ","date":"2022-08-02T23:37:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/10_%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","title":"watch: Transf - Nick 10 | Self-attention Mechanism"},{"content":"bilibili\nÊØè‰∏™ÂçïËØçÈÉΩË¶Å‰∏éÂâçÂêéÊâÄÊúâÂçïËØçÂä†ÊùÉÁõ∏Âä†ÔºåÊØè‰∏™ÂçïËØçÂåÖÂê´‰∫ÜÂè•Â≠êÁöÑÂÖ®Âëò‰ø°ÊÅØÔºåÂè™ËÄÉËôë‰∫ÜÁõ∏‰ººÊÄßÔºåËÄå‰∏¢Â§±‰∫Ü Input sequence ‰ø°ÊÅØ Ôºà‰πüÂ∞±ÊòØÊîπÂèòÂçïËØçÈ°∫Â∫èÔºå‰∏ç‰ºöÊîπÂèòÂêÑÂçïËØçÁöÑËØçÂêëÈáèÔºâ„ÄÇÊâÄ‰ª•ÈúÄË¶ÅÂÖàÂä†‰∏ä‰ΩçÁΩÆ‰ø°ÊÅØ\n‰∏∫ÊØè‰∏™ input ÂÅö‰∏ÄÊ¨°‰ΩçÁΩÆÁºñÁ†ÅÔºàÂêÑ input ÁöÑÂÖ®Â±ÄÈ°∫Â∫èÔºâ\n\u0026hellip;.\n","date":"2022-07-29T17:28:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/14_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/","title":"watch: Transf - Nick 14 | Positional Encoding"},{"content":"bilibili\nÂ§öÂ§¥ÊòØ‰∏∫‰∫ÜÁî®‰∏çÂêåÁöÑÊùÉÈáçÂàùÂßãÂåñÁ∫øÊÄßÂèòÊç¢\nSelf-attention ËæìÂá∫ÁöÑÊñ∞ËØçÂêëÈáèÊØîËæìÂÖ•ÁöÑËØçÂêëÈáèÊúâÊõ¥Â§öÁöÑÂè•Ê≥ïÁâπÂæÅÂíåËØ≠‰πâÁâπÂæÅ\nMulti-Head self-attention ËæìÂá∫ÁöÑÊñ∞ËØçÂêëÈáè ÊØî self-attention ÂæóÂà∞ÁöÑËØçÂêëÈáèÂåÖÂê´Êõ¥Â§öÁöÑ\nÂ§¥ÁöÑ‰∏™Êï∞Áî® h Ë°®Á§∫Ôºå‰∏ÄËà¨ h=8 „ÄÇÂπ∂‰∏çÊòØÁõ¥Êé•Áî®ËæìÂÖ• X ÂÅöAttention ËÆ°ÁÆóÔºåËÄåÊòØÂÖàÊää X ÂàÜÊàê 8 ÂùóÔºàÂàÜÂà´‰Ωç‰∫é8‰∏™Â≠êÁ©∫Èó¥ÔºâÔºåÂàÜÂà´ËÆ°ÁÆóÂæóÂà∞ 8 ‰∏™Êñ∞ËØçÂêëÈáèÔºåÁÑ∂ÂêéÊääÂÆÉ‰ª¨ÊãºËµ∑Êù•ÔºåÂÜçÂÅö‰∏ÄÊ¨°Á∫øÊÄßÂèòÊç¢ÔºàÊîπÂèòÁª¥Â∫¶‰∏é X ÂåπÈÖçÔºâÂæóÂà∞ÊúÄÁªàËØçÂêëÈáè\nÊú∫Âô®Â≠¶‰π†ÁöÑÊú¨Ë¥®ÔºöÈùûÁ∫øÊÄßÂèòÊç¢ y = activation(wx+b)ÔºåÊää‰∏Ä‰∏™ÂêëÈáè‰ªé‰∏Ä‰∏™‰ΩçÁΩÆÂèòÊç¢Âà∞Âè¶‰∏Ä‰∏™‰ΩçÁΩÆ‰∏ä\nÂ§öÂ§¥ÁöÑ‰ΩúÁî®ÔºöÊää‰∏Ä‰∏™ÂêëÈáèÊãÜÂàÜÊàê8‰∏™Â≠êÂêëÈáèÔºå8‰∏™ÂêëÈáèÂêåÊó∂‰ºòÂåñÔºåÂèØ‰ª•Êõ¥Âø´ÁöÑÊî∂ÊïõÂà∞ÂêàÈÄÇÁöÑ‰ΩçÁΩÆ„ÄÇÂàÜÂ§™Â§ö‰πü‰∏çÂ•Ω„ÄÇ\nËØçÂêëÈáè‰∏ÄËà¨ 512 Áª¥ÔºåÂàÜ 8 ÂùóÔºõËã•ÊòØËßÜÈ¢ëÂêëÈáè 5120ÔºåÂèØ‰ª•ÂàÜ 80 Âùó\nÂèÇÊï∞ÈáèÊõ¥Â§ßÔºå8Â•óÂèÇÊï∞(Wq,Wk,Wv)\n","date":"2022-07-29T17:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/13_%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B/","title":"watch: Transf - Nick 13 | Multi-head Self-Attention Mechanism"},{"content":"import torch Â∞±ÊòØÂØºÂÖ•pytorch„ÄÇPython‰∏≠ÁöÑtorchÂ∞±ÊòØpytorchÔºåÊâÄ‰ª•ËøôÈáå‰∏çÊòØimport pytorchÔºåËÄåÊòØimport torch ‰πüÂêàÁêÜ„ÄÇ\ntorch.Tensor ÂåÖÂê´Âçï‰∏ÄÊï∞ÊçÆÁ±ªÂûãÂÖÉÁ¥†ÁöÑÂ§öÁª¥Áü©Èòµ Êúâ10ÁßçÂº†ÈáèÁ±ªÂûãÔºåtorch.TensorÊòØÈªòËÆ§Âº†ÈáèÁ±ªÂûãtorch.FloatTensorÁöÑÂà´Âêç Note Âº†ÈáèÂèòÂºÇÊñπÊ≥ïÈÉΩÂ∏¶Êúâ‰∏ãÂàíÁ∫øÂêéÁºÄ,ÂÆÉ‰ª¨Áõ¥Êé•ÂéüÂú∞‰øÆÊîπÂéüÂº†ÈáèÁöÑÂ±ûÊÄßÔºåËÄå‰∏çÊ¥æÁîüÊñ∞Âº†Èáè„ÄÇ‰æãÂ¶Çtorch.FloatTensor.abs_()Áõ¥Êé•ËÆ°ÁÆóÂπ∂‰øÆÊîπÂéüÂº†ÈáèÔºåËÄåtorch.FloatTensor.abs()Âú®Êñ∞Âº†Èáè‰∏≠ËÆ°ÁÆóÁªìÊûú„ÄÇ\nTensors Operation on Tensors Âº†ÈáèÊúâ100Â§öÁßçËøêÁÆóÔºåËøô‰∫õËøêÁÆóÂú®GPU‰∏äËøêË°åÊØîÂú®CPU‰∏äÂø´„ÄÇÂèØ‰ΩøÁî®Âº†ÈáèÁöÑ.toÊñπÊ≥ïËΩ¨ÁßªÂà∞GPU‰∏äÔºåÂØπÂ§ßÂº†ÈáèÁöÑÁßªÂä®ÈúÄË¶ÅËä±Ë¥πÂæàÂ§öÊó∂Èó¥ÂíåÂÜÖÂ≠ò„ÄÇ\nÂº†ÈáèÁöÑÁ¥¢ÂºïÂíåÂàáÁâá‰∏éNumpyÂæàÂÉè„ÄÇ\ntensor[...,-1] Âíå tensor[:,-1] ÈÉΩË°®Á§∫ÂèñÂº†ÈáèÁöÑÊúÄÂêé‰∏ÄÂàó\nCreation Ops torch.tensor(data) ÊòØ‰∏Ä‰∏™ÊûÑÈÄ†Âô®construtor Â§çÂà∂data, ÊûÑÈÄ†‰∏Ä‰∏™Âº†Èáè ÂΩìdataÊòØ‰∏Ä‰∏™Âº†ÈáèxÊó∂ÔºåËøôÁßçÊñπÊ≥ïÁ≠âÊïà‰∫éx.clone().detach()ÔºöÂàõÂª∫Êñ∞ÁöÑleaf Âº†ÈáèÔºåÂπ∂‰∏çÂú®ÂΩìÂâçËÆ°ÁÆóÂõæ‰∏≠ Â¶ÇÊûú‰ªÖÂ∏åÊúõÊîπÂèòrequires_gradÊ†áÂøóÔºå‰ΩøÁî®requires_grad_()Âíådetach()ÊñπÊ≥ïÊù•ÈÅøÂÖçÂ§çÂà∂Êï∞ÊçÆ„ÄÇÂ¶ÇÊûúdataÊòØndarryÔºå‰ΩøÁî®torch.as_tensor()ÂàõÂª∫Âº†ÈáèÔºå‰∏çÂ§çÂà∂Êï∞ÊçÆ„ÄÇ torch.tensor.requires_grad_() ÊääËØ•Âº†ÈáèÁöÑÂ±ûÊÄßrequires_grad ÁΩÆ‰∏∫True ÊúâÁöÑÂº†ÈáèÊòØ‰ªéDataLoader‰∏≠Êù•ÁöÑÔºåÈúÄË¶ÅÂÅö‰∏Ä‰∫õÈ¢ÑÂ§ÑÁêÜÔºåÂÜçÂºÄÂßãËÆ©autogradÂºÄÂßãËÆ∞ÂΩïËøô‰∏™Âº†Èáè‰∏äÁöÑÊìç‰Ωú Âú®ÂéüÂú∞‰øÆÊîπÔºå‰∏çÈúÄÂàõÂª∫Êñ∞ÂèòÈáèÔºåÊ≤°ÊúâÂ§çÂà∂ torch.tensor.detach() ÂàõÂª∫‰∏Ä‰∏™Êñ∞Âº†ÈáèÔºå‰∏éÂéüÂº†ÈáèÊåáÂêëÂêå‰∏ÄÂùóÂÜÖÂ≠òÔºå‰ΩÜ‰∏çÂÖÅËÆ∏‰øÆÊîπ‰∫åËÄÖÁöÑsize/stride/storageÔºåÂê¶ÂàôÊä•Èîô Êñ∞Âº†Èáè‰ªéÂΩìÂâçËÆ°ÁÆóÂõæ‰∏≠ÂàÜÁ¶ªÔºå‰∏çÈúÄËÆ°ÁÆóÊ¢ØÂ∫¶ Ê≤°ÊúâÂ§çÂà∂ torch.as_tensor() ÊäädataËΩ¨Êç¢‰∏∫tensor ‰∏é torch.tensor ‰∏çÂêåÔºåËøôÁßçÂàõÂª∫ÊñπÂºèÂ∞ΩÈáèÈÅøÂÖçÂ§çÂà∂Êï∞ÊçÆ(ÊåáÂêëÂêå‰∏ÄÂùóÂÜÖÂ≠ò) Â¶ÇÊûúdataÊòØndarryÔºàÊàñtensorÔºâÔºåÂπ∂‰∏îÂÆÉÁöÑdtypeÂíådeviceÈÉΩ‰∏éÁõÆÊ†áËæìÂá∫ÂØπÂ∫î‰∏ÄËá¥ÔºåÈÇ£‰πàÂ∞±‰∏ç‰ºöÂ§çÂà∂Êï∞ÊçÆÔºåËÄåÊòØÊñ∞Âº†ÈáèÂíådataÂÖ±ÂêåÊåáÂêëÈÇ£ÂùóÂÜÖÂ≠òÔºåÊîπÂèòÂº†ÈáèÔºåÂéüdata‰πü‰ºöÊîπÂèò„ÄÇ ÂΩìdataÊòØlist, tuple, scalar ÊàñÂÖ∂‰ªñarray_likeÁöÑÊï∞ÊçÆÔºåÊàñËÄÖdtype‰∏ç‰∏ÄËá¥ÔºåÊàñËÄÖdevice‰∏ç‰∏ÄÊ†∑ÔºåÈÉΩ‰ºöÂ§çÂà∂Êï∞ÊçÆÂàõÂª∫Êñ∞tensor„ÄÇ torch.sparse COO tensors ‰∏ÄÁßçÂ≠òÂÇ®ÂΩ¢Âºè: tensor is stored as 2 tensor: indices and values indices are coordinate in tensor Reduce memory consumption Strided tensor stores each elements, while COO tensor only record non-zero numbers. torch.as_strided() ÂàõÂª∫‰∏Ä‰∏™Á™óÂè£viewÔºåÂØπÂ∫ïÂ±ÇËøûÁª≠ÁöÑ(‰∏ÄÁª¥)Êï∞ÊçÆÈáçÊñ∞ÊéíÂàó(\u0026ldquo;Ë£ÅÂâ™\u0026rdquo;) ËæìÂÖ•‰∏Ä‰∏™Âº†ÈáèÔºåÊåáÂÆöËæìÂá∫Âº†ÈáèÁöÑsizeÂíåÊØè‰∏™Áª¥Â∫¶Ë∑≥Ë∑ÉÁöÑÊ≠•Èïøstride„ÄÇ ÂØπ‰∫é 1 2 x = torch.randn(3,5,5) #‰∏âÁª¥Âº†ÈáèÔºå‰∏ÄÂÖ±125‰∏™Êï∞ÊçÆ t = torch.as_strided(x, (3,3,3), (5,3,1)) #ËæìÂá∫‰∏Ä‰∏™(3,3,3)ÁöÑÂº†ÈáèÔºåÊúÄ‰ΩéÁª¥Â∫¶ÁöÑËµ∑ÁÇπ‰ªéÁ¨¨‰∏Ä‰∏™Êï∞ÂºÄÂßãÔºåË∑≥Ë∑ÉÊ≠•Èïø‰∏∫1ÔºåÂÄíÊï∞Á¨¨2Áª¥Â∫¶ÁöÑÊØè‰∏™Ëµ∑ÁÇπ,Ë∑≥Ë∑ÉÊ≠•Èïø‰∏∫3ÔºåÊúÄÈ´òÁª¥Â∫¶ÁöÑÊØè‰∏™Ëµ∑ÁÇπÈó¥ÈöîÊ≠•Èïø‰∏∫5„ÄÇ Ê†πÊçÆsizeÂíåstrideÂèØ‰ª•Âà§Êñ≠ÂÜÖÂ≠ò‰∏äÊòØÂê¶ËøûÁª≠„ÄÇÂØπpytorch‰∏≠TensorÁöÑÂâñÊûê torch.from_numpy(ndarray) Êñ∞Âº†Èáè‰∏éndarryÂÖ±‰∫´Âêå‰∏ÄÂùóÂÜÖÂ≠òÔºå‰ΩÜÊ≠§Âº†Èáè‰∏çËÉΩ‰øÆÊîπsize Reverse a dim, e.g. from bottom to top, or from right to left.\n1 xy = torch.flip(xy, [1]) # to match the code ","date":"2022-07-28T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_torch/","title":"memo: PyTorch | Tensor Ops"},{"content":"Âç∑ÁßØÂ∞±ÊòØÁâπÂæÅÊèêÂèñÂô®ÔºåÂ∞±ÊòØCBAPD: convolution, batchnorm, activateion, pooling, dropout\ntf‰∏≠ÂÆö‰πâÂç∑ÁßØÂ±ÇÔºåÊåáÂÆöÂç∑ÁßØÊ†∏ÁöÑ‰∏™Êï∞ÔºåkernelÊ†∏Èïø\n1x1Âç∑ÁßØÊ†∏ÊòØ‰∏∫‰∫ÜÂáèÂ∞ëËÆ°ÁÆóÈáèÔºåÂáèÂ∞ëÁâπÂæÅÂõæÔºàÈÄöÈÅìÔºâÁöÑ‰∏™Êï∞\n","date":"2022-07-28T11:30:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/5_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"watch: TF2 - PKU 05 | CNN"},{"content":"ÁªüËÆ°ËØ≠Ë®ÄÊ®°ÂûãÔºöÁî®Ê¶ÇÁéá‰º∞ËÆ°‰∏ã‰∏Ä‰∏™ÂçïËØçÊòØÂì™‰∏™\nn ÂÖÉËØ≠Ë®ÄÊ®°ÂûãÔºöÂè™ÂàÜÊûê n ‰∏™ÂçïËØç\nÁ•ûÁªèÁΩëÁªúËØ≠Ë®ÄÊ®°ÂûãÔºöÁî®ËØçÂêëÈáèË°®Á§∫ÊØè‰∏™ÂçïËØçÔºåËØçÂêëÈáèË∂äÁ¥ßÂáëË∂äÁ≤æÁÇºË∂äÂ•ΩÔºàÁõ∏ÂØπ‰∫éone-hotÁºñÁ†ÅÔºâ\n","date":"2022-07-26T22:41:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/4_%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","title":"watch: Transf - Nick 04 | ÁªüËÆ°ËØ≠Ë®ÄÊ®°Âûã"},{"content":"4 ÁΩëÁªúÂÖ´ËÇ°Êâ©Â±ï 4.1 Ëá™Âà∂Êï∞ÊçÆÈõÜ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 import tensorflow as tf from PIL import Image import numpy as np import os train_dir = \u0026#39;./mnist_image_label/mnist_train_jpg_60000/\u0026#39; train_txt = \u0026#39;./mnist_image_label/mnist_train_jpg_60000.txt\u0026#39; x_train_savepath = \u0026#39;./mnist_image_label/mnist_x_train.npy\u0026#39; y_train_savepath = \u0026#39;./mnist_image_label/mnist_y_train.npy\u0026#39; test_dir = \u0026#39;./mnist_image_label/mnist_test_jpg_10000/\u0026#39; test_txt = \u0026#39;./mnist_image_label/mnist_test_jpg.txt\u0026#39; x_test_savepath = \u0026#39;./mnist_image_label/mnist_x_test.npy\u0026#39; y_test_savepath = \u0026#39;./mnist_image_label/mnist_y_test.npy\u0026#39; def generateDS(dir, labels_path): f = open(labels_path, \u0026#39;r\u0026#39;)\t# Â≠òÊúâ Êñá‰ª∂Âêç ÂèäÂÖ∂ Ê†áÁ≠æ contents = f.readlines()\t# ËØªÂèñÊâÄÊúâË°å f.close()\t# ÂÖ≥Èó≠ x, y_ = [], []\t# ÊØèÂº†ÂõæÂØπÂ∫îÁöÑÁÅ∞Â∫¶ÂÄºÊï∞ÊçÆÂíåÊ†áÁ≠æ for content in contents:\t# ÈÄêË°åËØªÂá∫ value = content.split()\t# ‰ª•Á©∫Ê†ºÂàÜÂºÄ img_path = dir + value[0]\t# ÂõæÁâáÂêç img = Image.open(img_path) img = np.array(img.convert(\u0026#39;L\u0026#39;)) # 8‰ΩçÁÅ∞Â∫¶ÂõæÂÉè img = img/255. x.append(img)\t# ÁÅ∞Â∫¶ÂÄºnp.array ÊîæÂÖ•ÂàóË°® y_.append(value[1])\t# Ê†áÁ≠æ ÊîæÂÖ•ÂàóË°® print(\u0026#39;loading: \u0026#39;+content) x = np.array(x)\t# ÂàóË°®Âèò np.array y_ = np.array(y_) y_ = y_.astype(np.int64)\t# Ê†áÁ≠æÊòØ64‰ΩçÊï¥Âûã return x, y_ if os.path.exists(x_train_savepath) and os.path.exists(y_train_savepath) and os.path.exists(x_test_savepath) and os.path.exists(y_test_savepath): print(\u0026#39;---Loading dataset---\u0026#39;) x_train_save = np.load(x_train_savepath)\t# ËØª y_train = np.load(y_train_savepath) x_test_save = np.load(x_test_savepath) y_test = np.load(y_test_savepath) x_train = np.reshape(x_train_save, (len(x_train_save),28,28))\t# ÂèòÂΩ¢ x_test = np.reshape(x_test_save, (len(x_test_save),28,28)) else:\t#‰∏çÂ≠òÂú®ÔºåË¶ÅÂà∂‰ΩúÊï∞ÊçÆÈõÜ print(\u0026#39;---Generating dataset---\u0026#39;) x_train, y_train = generate(train_dir, train_txt) x_test, y_test = generate(test_dir, test_txt) print(\u0026#39;---Saving dataset\u0026#39;)\t# ‰øùÂ≠ò‰∏∫‰ª•Âêé‰ΩøÁî® x_train_save = np.reshape(x_train, (len(x_train), -1)) x_test_save = np.reshape(x_test, (len(x_test), -1)) np.save(x_train_savepath, x_train_save) np.save(y_train_savepath, y_train) np.save(x_test_savepath, x_test_save) np.save(y_test_savepath, y_test) 4.2 Êï∞ÊçÆÂ¢ûÂº∫ Áî®‰∫éÊâ©Â±ïÊï∞ÊçÆÈõÜÔºåÂØπÂõæÂÉèÁöÑÂ¢ûÂº∫Â∞±ÊòØÂØπÂõæÂÉèÁöÑÁÆÄÂçïÂΩ¢ÂèòÔºåÁî®Êù•Â∫îÂØπÂõ†ÊãçÁÖßËßíÂ∫¶‰∏çÂêåÂºïËµ∑ÁöÑÂõæÁâáÂèòÂΩ¢\n1 2 3 4 5 6 7 8 9 image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator( rescale = ÊâÄÊúâÊï∞ÊçÆÂ∞Ü‰πò‰ª•ËØ•Êï∞ÂÄºÔºå rotation_range = ÈöèÊú∫ÊóãËΩ¨ËßíÂ∫¶Êï∞ËåÉÂõ¥Ôºå width_shift_range = ÈöèÊú∫ÂÆΩÂ∫¶ÂÅèÁßªÈáèÔºå height_shift_range = ÈöèÊú∫È´òÂ∫¶ÂÅèÁßªÈáèÔºå horizontal_flip = ÊòØÂê¶ÈöèÊú∫Ê∞¥Âπ≥ÁøªËΩ¨Ôºå zoom_range = ÈöèÊú∫Áº©ÊîæÁöÑËåÉÂõ¥[1-n, 1+n]) image_gen_train.fit(x_train) ÂÖ∂‰∏≠ x_train ÈúÄË¶ÅÊòØÂõõÁª¥ÔºåÈúÄË¶ÅÂèòÂΩ¢Ôºöx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)ÔºåÊää (60000, 28, 28) -\u0026gt; (60000, 28, 28, 1)\n‰æãÂ¶ÇÔºö\n1 2 3 4 5 6 7 8 9 image_gen_train = ImageDataGenerator( rescale = 1. / 1., # Ëã•‰∏∫ÂõæÂÉèÔºåÂàÜÊØç‰∏∫255Êó∂ÔºåÂèØÂΩíËá≥0~1 rotation_range = 45, # ÈöèÊú∫45Â∫¶ÊóãËΩ¨ width_shift_range=.15, # ÂÆΩÂ∫¶ÂÅèÁßª height_shift_range=.15, # È´òÂ∫¶ÂÅèÁßª horizontal_flip=False, # Ê∞¥Âπ≥ÁøªËΩ¨ zoom_range = 0.5 # Â∞ÜÂõæÂÉèÈöèÊú∫Áº©ÊîæÈòàÈáè50% ) image_gen_train.fit(x_train) Ê®°ÂûãËÆ≠ÁªÉ‰πüË¶ÅÊîπÔºåÊää model.fit(x_train, y_train, batch_size=32, ...) Êîπ‰∏∫ model.fit(image_gen_train.flow(x_train, y_train, batch_size=32), ...)\n4.3 Êñ≠ÁÇπÁª≠ËÆ≠, Â≠òÂèñÊ®°Âûã ËØªÂèñÊ®°ÂûãÔºöload_weights(Êñá‰ª∂Ë∑ØÂæÑ)\n1 2 3 4 checkpoint_save_path = \u0026#39;./checkpoint/mnist.ckpt\u0026#39; # ÂÆö‰πâÊñá‰ª∂Ë∑ØÂæÑ if os.path.exists(checkpoint_save_path + \u0026#39;.index\u0026#39;): # Â≠òÂú®Á¥¢ÂºïË°®ËØ¥ÊòéÂ∑≤Áªè‰øùÂ≠òËøáÂèÇÊï∞‰∫Ü print(\u0026#39;---- Loading the model ----\u0026#39;) model.load_weights(checkpoint_save_path) ‰øùÂ≠òÊ®°ÂûãÔºö\n1 2 3 4 5 6 7 8 9 cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_save_path, # ‰øùÂ≠òË∑ØÂæÑ save_weights_only = True, # ÊòØÂê¶Âè™‰øùÁïô weights save_best_only = True, # ÊòØÂê¶Âè™‰øùÁïôÊúÄ‰ºòÁªìÊûú # Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰øùÂ≠òÔºåËÆ∞ÂΩïÂà∞history‰∏≠ history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) 4.4 ÊèêÂèñÂèØËÆ≠ÁªÉÂèÇÊï∞ ËøîÂõûÊ®°Âûã‰∏≠ÂèØËÆ≠ÁªÉÁöÑÂèÇÊï∞ model.trainable_variables\nËÆæÁΩÆprintËæìÂá∫Ê†ºÂºèÔºönp.set_printoptions(threshold=Ë∂ÖËøáÂ§öÂ∞ëÁúÅÁï•ÊòæÁ§∫) np.inf Ë°®Á§∫Êó†ÈôêÂ§ß\n1 2 3 4 5 6 7 8 9 10 print(model.trainable_variables) # ÊääÂèÇÊï∞Áõ¥Êé•ÊâìÂç∞Âá∫Êù• file = open(\u0026#39;./weights.txt\u0026#39;, \u0026#39;w\u0026#39;) # ÊääÂèÇÊï∞Â≠òÂÖ•Êñá‰ª∂ for v in model.trainable_variables: file.write(str(v.name) + \u0026#39;\\n\u0026#39;) file.write(str(v.shape) + \u0026#39;\\n\u0026#39;) file.write(str(v.numpy()) + \u0026#39;\\n\u0026#39;) file.close() 4.5 acc/locc ÂèØËßÜÂåñ Áî®‰∫éÊü•ÁúãËÆ≠ÁªÉÊïàÊûú„ÄÇhistory‰∏≠ËÆ∞ÂΩï‰∫ÜËÆ≠ÁªÉÈõÜÁöÑlossÂíåsparse_categorical_accuracyÔºõÊµãËØïÈõÜ‰∏äÁöÑval_loss Âíå val_sparse_categorical_accuracy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 acc = history.history[\u0026#39;sparse_categorical_accuracy\u0026#39;] val_acc = history.history[\u0026#39;val_sparse_categorical_accuracy\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] plt.subplot(1,2,1) plt.plot(acc, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(val_acc, label=\u0026#39;Validation Accuracy\u0026#39;) plt.title(\u0026#39;Training and Validation Accuracy\u0026#39;) plt.legend() plt.subplot(1,2,2) plt.plot(loss, label=\u0026#39;Training Loss\u0026#39;) plt.plot(val_loss, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training and Validation Loss\u0026#39;) plt.legend() plt.show() 4.6 Ë∞ÉÁî®Ê®°Âûã ËøîÂõûÂâçÂêë‰º†Êí≠ËÆ°ÁÆóÁªìÊûúÔºöpredict(ËæìÂÖ•ÁâπÂæÅÔºåbatch_size=Êï¥Êï∞)\n‰∏âÊ≠•ÔºöÂ§çÁé∞Ê®°ÂûãÔºåÂä†ËΩΩÂèÇÊï∞ÔºåÈ¢ÑÊµã\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from PIL import Image import numpy as np import tensorflow as tf model_save_path = \u0026#39;./checkpoint/mnist.ckpt\u0026#39; model = tf.keras.models.Sequential([ # Â§çÁé∞ÁΩëÁªú tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)]) model.load_weights(model_save_path) # Âä†ËΩΩÁΩëÁªú preNum = int(input(\u0026#34;Input the number of test pictures: \u0026#34;)) # Êé•Êî∂Áî®Êà∑ËæìÂÖ• for i in range(preNum): image_path = input(\u0026#34;the path of test picture: \u0026#34;) # ËæìÂÖ• 1.png img = Image.open(image_path) # ËØªÂèñÂõæÁâá # È¢ÑÂ§ÑÁêÜËæìÂÖ•ÂõæÁâáÁöÑÊ†ºÂºè ‰∏é ËÆ≠ÁªÉÊï∞ÊçÆ‰∏ÄËá¥ img = Img.resize((28, 28), Image.ANTIALIAS) # Âèò‰∏∫ 28x28 ‰∏éËÆ≠ÁªÉÂõæÁâáÂ∞∫ÂØ∏Áõ∏Âêå img_arr = np.array(img.convert(\u0026#39;L\u0026#39;)) # Âèò‰∏∫ÁÅ∞Â∫¶Âõæ img_arr = 255 - img_arr # ÈªëÁôΩÂèçËΩ¨ # È¢ÑÂ§ÑÁêÜÊàñËÄÖÂèò‰∏∫Âè™ÊúâÈªëÁôΩÂÉèÁ¥†ÁöÑÈ´òÂØπÊØîÂ∫¶ÂõæÁâá, 2ÁßçÊñπÊ≥ï‰∫åÈÄâ‰∏Ä for i in range(28): for j in range(28): if img_arr[i][j] \u0026lt; 200: # Â∞è‰∫é200 ÂÖ®ÂèòÁôΩ. ÂèØÊª§ÂéªËÉåÊôØÂô™Â£∞ÔºåÂΩìÈòàÂÄºÈÄâÊã©ÂêàÈÄÇËØÜÂà´ÊïàÊûúÊõ¥Â•Ω img_arr[i][j] = 255 else: img_arr[i][j] = 0 img_arr = img_arr / 255.0 # ÂΩí‰∏ÄÂåñ x_predict = img_arr[tf.newaxis, ...] # (28, 28) -\u0026gt; (1, 28, 28) result = model.predict(x_predict) # ËæìÂÖ•ÁΩëÁªú pred = tf.argmax(result, axis=1) # ËøîÂõûÊúÄÂ§ßÊ¶ÇÁéáÂÄºÁöÑÁ¥¢Âºï print(\u0026#39;\\n\u0026#39;) tf.print(pred) ","date":"2022-07-24T16:41:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/4_%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1%E6%89%A9%E5%B1%95/","title":"watch: TF2 - PKU 04 | NN Framework Extension"},{"content":"3 kerasÊê≠Âª∫Á•ûÁªèÁΩëÁªú 3.1 Á•ûÁªèÁΩëÁªúÊê≠Âª∫ÂÖ´ËÇ° import train data, test data model = tf.keras.models.Sequential model.compile ËÆæÁΩÆ‰ºòÂåñÂô®ÔºåÊçüÂ§±ÂáΩÊï∞ÔºåËØÑ‰ª∑ÊåáÊ†á model.fit ËÆæÁΩÆËÆ≠ÁªÉËøáÁ®ãÔºåËæìÂÖ•ËæìÂá∫ epochÔºåbatch model.summary ÊâìÂç∞ÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ÁªüËÆ° model = tf.keras.models.Sequential([ÁΩëÁªúÁªìÊûÑ]) # ÊèèËø∞ÂêÑÂ±ÇÁΩëÁªú\nÁΩëÁªúÁªìÊûÑ‰∏æ‰æãÔºö\nÊãâÁõ¥Â±ÇÔºötf.keras.layers.Flatten() ÊääËæìÂÖ•ÁâπÂæÅÂèòÊàê‰∏ÄÁª¥Êï∞ÁªÑ\nÂÖ®ËøûÊé•Â±ÇÔºötf.keras.layers.Dense(Á•ûÁªèÂÖÉ‰∏™Êï∞Ôºåactivation=\u0026quot;ÊøÄÊ¥ªÂáΩÊï∞\u0026quot;Ôºåkernel_regularizer=Âì™ÁßçÊ≠£ÂàôÂåñ) „ÄÇÊøÄÊ¥ªÂáΩÊï∞ÂèØÈÄâÔºö\u0026lsquo;relu\u0026rsquo;, \u0026lsquo;softmax\u0026rsquo;, \u0026lsquo;sigmoid\u0026rsquo;, \u0026rsquo;tanh\u0026rsquo;Ôºõkernel_regularizerÂèØÈÄâÔºötf.keras.regularizers.l1()„ÄÅtf.keras.regularizers.l2()\nÂç∑ÁßØÂ±ÇÔºötf.keras.layers.Conv2D(filters=Âç∑ÁßØÊ†∏‰∏™Êï∞Ôºåkernel_size=Âç∑ÁßØÊ†∏Â∞∫ÂØ∏Ôºåstrides=Âç∑ÁßØÊ≠•ÈïøÔºåpadding=\u0026quot;valid\u0026quot; or \u0026quot;same\u0026quot;)\nLSTMÂ±ÇÔºötf.keras.layers.LSTM()\nmodel.compile(optimizer=‰ºòÂåñÂô®Ôºåloss=ÊçüÂ§±ÂáΩÊï∞Ôºåmetrics=[\u0026quot;ÂáÜÁ°ÆÁéá\u0026quot;])\noptimizerÂèØÈÄâÔºö\n\u0026lsquo;sgd\u0026rsquo; Êàñ tf.keras.optimizers.SGD(lr=Â≠¶‰π†ÁéáÔºåmomentum=Âä®ÈáèÂèÇÊï∞) \u0026lsquo;adagrad\u0026rsquo; Êàñ tf.keras.optimizers.Adagrad(lr=Â≠¶‰π†Áéá) \u0026lsquo;adadelta\u0026rsquo; Êàñ tf.keras.optimizers.Adadelta(lr=Â≠¶‰π†Áéá) \u0026lsquo;adam\u0026rsquo; Êàñ tf.keras.optimizers.Adam(lr=Â≠¶‰π†ÁéáÔºåbeta_1=0.9, beta_2=0.999) loss ÂèØÈÄâÔºö\n\u0026lsquo;mse\u0026rsquo; or tf.keras.losses.MeanSquaredError() \u0026lsquo;sparse_categorical_crossentropy\u0026rsquo; or tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) , from_logits ‰∏∫TrueË°®Á§∫Ê≤°ÁªèËøásoftmaxÔºåÊòØÁΩëÁªúÁöÑÁõ¥Êé•ÁªìÊûú Metrics ÂèØÈÄâÔºö\n\u0026lsquo;accuracy\u0026rsquo;Ôºöy_ Âíå y ÈÉΩÊòØÊï∞ÂÄºÔºåÁ±ªÂà´ÊòØÊï∞Â≠óÔºåÂ¶Ç y_=[1], y=[1] \u0026lsquo;categorical_accuracy\u0026rsquo;ÔºåtargetÊòØÁã¨ÁÉ≠Á†ÅÔºåpredÊòØÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÂ¶Ç y_=[0,1,0], y=[0.256, 0.695, 0.048] \u0026lsquo;sparse_categorical_accuracy\u0026rsquo;: targetÊòØÊï∞ÂÄºÔºåpredÊòØÁã¨ÁÉ≠Á†Å (Ê¶ÇÁéáÂàÜÂ∏É)ÔºåÂ¶Ç y_ = [1], y=[0.256, 0.695, 0.048] model.fit(ËÆ≠ÁªÉÈõÜÁöÑËæìÂÖ•ÁâπÂæÅÔºåËÆ≠ÁªÉÈõÜÁöÑÊ†áÁ≠æÔºåbatch_size= , epoches= , validation_data=(ÊµãËØïÈõÜÁöÑËæìÂÖ•ÁâπÂæÅÔºåÊµãËØïÈõÜÁöÑÊ†áÁ≠æ)„ÄêÊàñÁî® validation_split=‰ªéËÆ≠ÁªÉÈõÜÂàíÂàÜÂ§öÂ∞ëÊØî‰æãÁªôÊµãËØïÈõÜ„ÄëÔºåvalidation_freq=Â§öÂ∞ëÊ¨°epochÊµãËØï‰∏ÄÊ¨°)\n3.2 Iris ‰ª£Á†ÅÁî®kerasÈáçÂÜô 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf from sklearn import datasets import numpy as np x_train = datasets.load_iris().data y_train = datasets.load_iris().target np.random.seed(116) np.random.shuffle(x_train) np.random.seed(116) np.random.shuffle(y_train) tf.random.set_seed(116) # ËÆæËÆ°Ê®°Âûã model = tf.keras.models.Sequential([ tf.keras.layers.Dense(3, activation=\u0026#39;softmax\u0026#39;, kernel_regularize=tf.keras.regularizers.l2()) ]) # ÈÖçÁΩÆËÆ≠ÁªÉÊñπÊ≥ï model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.1), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) # ËÆ≠ÁªÉËøáÁ®ã model.fit(x_train, y_train, batch_size=32, epoches=500, validataion_split=0.2, validation_freq=20) # ËÆ≠ÁªÉÈõÜÁöÑ20%ÂÅöÊµãËØïÈõÜ model.summary() Sequential Êê≠Âª∫‰∏äÂ±ÇËæìÂá∫Â∞±ÊòØ‰∏ãÂ±ÇËæìÂÖ•ÁöÑÈ°∫Â∫èÁΩëÁªúÁªìÊûÑÔºå‰ΩÜÊó†Ê≥ïÂÜôÂá∫Â∏¶ÊúâË∑≥ËøûÊé•ÁöÑÈùûÈ°∫Â∫èÁΩëÁªúÁªìÊûÑÔºå\nÁªßÊâøModelÁ±ª Ëá™ÂÆö‰πâÔºö\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from tensorflow.keras.layers import Dense from tensorflow.keras import Model class MyModel(Model): def __init__(self): super(MyModel, self).__init__() #ÂÆö‰πâÁΩëÁªúÁªìÊûÑÂùó self.d1 = Dense(3, activation=\u0026#39;sigmoid\u0026#39;) def call(self, x): #Ë∞ÉÁî®ÁΩëÁªúÁªìÊûÑÂùóÔºåÂÆûÁé∞ÂâçÂêë‰º†Êí≠ y = self.d1(x) return y model = MyModel() MNIST Êï∞ÊçÆÈõÜ ÊÄªÂÖ±7‰∏áÂº†ÂõæÁâáÔºå6‰∏áÁöÑËÆ≠ÁªÉÈõÜÔºå1‰∏á‰Ωú‰∏∫ÊµãËØïÈõÜ\n‰ΩøÁî®Sequential\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import tensorflow as tf from matplotlib import pyplot as plt # ËØªÂèñMNISTÊï∞ÊçÆÈõÜ mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train/255.0, x_test/255.0\t# ËæìÂÖ•Êï∞ÂÄºÂèòÂ∞èÊõ¥ÈÄÇÂêàÁ•ûÁªèÁΩëÁªú model = tf.keras.models.Sequential([ # Â∞ÜËæìÂÖ•Ê†∑Êú¨ÊãâÁõ¥‰∏∫‰∏ÄÁª¥ÂêëÈáèÔºå784‰∏™ÂÉèÁ¥†ÁÇπÁöÑÁÅ∞Â∫¶ÂÄº tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) model.fit(x_train, y_train, batch_size=32, epochs=5, validation=(x_test, y_test), validation_freq=1) model.summary() ‰ΩøÁî® ModelÁ±ª\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras import Model mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class MyModel(Model): def __init__(self): super(MyModel, self).__init__() self.flatten = Flatten() self.d1 = Dense(128, activation=\u0026#39;relu\u0026#39;) self.d2 = Dense(10, activation=\u0026#39;softmax\u0026#39;) def call(self, x): x = self.flatten(x) x = self.d1(x) y = self.d2(x) return y model = MyModel() model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) model.fit(x_train, y_train, batch_size=32, epochs=5, validation=(x_test, y_test), validation_freq=1) model.summary() FASHION Êï∞ÊçÆÈõÜ\n1 2 fashion = tf.keras.datasets.fashion_mnist (x_train, y_train), (x_test,y_test) = fashion.load_data() ","date":"2022-07-24T14:01:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/3_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%85%AB%E8%82%A1/","title":"watch: TF2 - PKU 03 | NN Building Steps"},{"content":"2 Á•ûÁªèÁΩëÁªú‰ºòÂåñ 2.1 Â∏∏Áî®ÂáΩÊï∞ Âú®ÊØè‰∏™ÂÖÉÁ¥†‰∏äÊâßË°åÊù°‰ª∂ËØ≠Âè•Ôºå‰∏∫ÁúüÂàôËøîÂõûAÔºå‰∏∫ÂÅáÂàôËøîÂõûBÔºötf.where(Êù°‰ª∂ËØ≠Âè•ÔºåÁúüËøîÂõûAÔºåÂÅáËøîÂõûB)\n1 2 3 4 a = tf.constant([1, 2, 3, 1, 1]) b = tf.constant([0, 1, 3, 4, 5]) c = tf.where(tf.greater(a,b), a, b)\t# Ëã•a\u0026gt;b, ËøîÂõûaÂØπÂ∫î‰ΩçÁΩÆÁöÑÂÖÉÁ¥†ÔºåÂê¶ÂàôËøîÂõûbÂØπÂ∫î‰ΩçÁΩÆÁöÑÂÖÉÁ¥† print(c)\t# tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32) ËøîÂõû‰∏Ä‰∏™ [0,1) ‰πãÈó¥ÁöÑÈöèÊú∫Êï∞Ôºönp.random.RandomState.rand(Áª¥Â∫¶) Áª¥Â∫¶‰∏∫Á©∫Êó∂ËøîÂõûÊ†áÈáè\n1 2 3 4 5 rdm = np.random.RandomState(seed=1)\t# ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠êÊØèÊ¨°ÁîüÊàêÁöÑÈöèÊú∫Êï∞Áõ∏Âêå a = rdm.rand() b = rdm.rand(2,3) print(a) # 0.4170220047 print(b) # [[7.20324493e-01 1.14374817e-04 3.02332573e-01] [1.46755891e-01 9.23385948e-02 1.86260211e-01]] Â∞Ü‰∏§‰∏™Êï∞ÁªÑÊåâÂûÇÁõ¥ÊñπÂêëÂè†Âä† np.vstack(Êï∞ÁªÑ1ÔºåÊï∞ÁªÑ2)\n1 2 3 4 a = np.array([1, 2, 3])\t# shape=(3,) b = np.array([4, 5, 6]) c = np.vstack((a,b)) print(c)\t# [[1 2 3] [4 5 6]] shape=(2,3) ÁîüÊàêÁΩëÊ†ºÂùêÊ†á\nnp.mgrid[Ëµ∑ÂßãÂÄºÔºöÁªìÊùüÂÄºÔºöÊ≠•ÈïøÔºåËµ∑ÂßãÂÄºÔºöÁªìÊùüÂÄºÔºöÊ≠•ÈïøÔºå...] ÁîüÊàêËã•Âπ≤Áª¥Â∫¶ÁöÑÁ≠âÂ∑ÆÊï∞ÁªÑÔºå‰∏çÂåÖÊã¨ÁªìÊùüÂÄº\nx.ravel() Â∞ÜÂ§öÁª¥Êï∞ÁªÑxÂèò‰∏∫‰∏ÄÁª¥Êï∞ÁªÑÔºå‚ÄúÊääÂèòÈáèÊãâÁõ¥‚Äù\nnp.c_[Êï∞ÁªÑ1ÔºåÊï∞ÁªÑ2Ôºå...] Êï∞ÁªÑÂØπÂ∫î‰ΩçÁΩÆÂÖÉÁ¥†ÈÖçÂØπ\n1 2 3 4 5 6 7 import numpy as np x, y = np.mgrid[1:3:1, 2:4:0.5] # xÂùêÊ†á+1ÈÄíÂ¢ûÔºåyÂùêÊ†á+0.5ÈÄíÂ¢û grid = np.c_[x.ravel(), y.ravel()] # ‰∏§‰∏™(8,) arrayÈÖçÂØπ print(x) # [[1. 1. 1. 1.] [2. 2. 2. 2.]] shape=(2,4) print(y) # [[2. 2.5 3. 3.5] [2. 2.5 3. 3.5]] print(grid) # [[1. 2.] [1. 2.5] [1. 3.] [1. 3.5] [2. 2.] [2. 2.5] [2. 3.] [2. 3.5]] shape=(8,2) Á•ûÁªèÁΩëÁªúÂ§çÊùÇÂ∫¶\nÂ§öÁî®ÁΩëÁªúÂ±ÇÊï∞ÂíåÁΩëÁªúÂèÇÊï∞ÁöÑ‰∏™Êï∞Ë°®Á§∫:\nÁ©∫Èó¥Â§çÊùÇÂ∫¶ÔºöÂ±ÇÊï∞=ÈöêËóèÂ±ÇÁöÑÂ±ÇÊï∞+1‰∏™ËæìÂá∫Â±ÇÔºõÊÄªÂèÇÊï∞=ÊÄªw+ÊÄªb\nÊó∂Èó¥Â§çÊùÇÂ∫¶Ôºö‰πòÂä†ËøêÁÆóÊ¨°Êï∞ÔºàÂ§öÂ∞ëÊ¨°wx+bÔºâ\n2.2 ÊåáÊï∞Ë°∞ÂáèÂ≠¶‰π†Áéá ÂÖàÁî®ËæÉÂ§ßÁöÑÂ≠¶‰π†ÁéáÔºåÂø´ÈÄüÂæóÂà∞ËæÉ‰ºòËß£ÔºåÁÑ∂ÂêéÈÄêÊ≠•ÂáèÂ∞èÂ≠¶‰π†ÁéáÔºå‰ΩøÊ®°ÂûãÂú®ËÆ≠ÁªÉÂêéÊúüÁ®≥ÂÆö\nÊåáÊï∞Ë°∞ÂáèÂ≠¶‰π†Áéá = ÂàùÂßãÂ≠¶‰π†Áéá x Â≠¶‰π†ÁéáË°∞ÂáèÁéá ^ (ÂΩìÂâçepoch(Êàñbatch) / Â§öÂ∞ëepoch(Êàñbatch)Ë°∞Âáè‰∏ÄÊ¨°)\n1 2 3 4 5 6 7 8 9 10 11 12 13 epoch = 40 LR_BASE = 0.2 LR_DECAY = 0.99 LR_STEP = 1 for epoch in range(epoch): lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP) with tf.GradientTape() as tape: loss = tf.square(w+1) grads = tape.gradient(loss, w) w.assign_sub(lr * grads) print(f\u0026#34;After {epoch} epoch, w is {w.numpy()}, loss is {loss}, lr is {lr}\u0026#34;) 2.3 ÊøÄÊ¥ªÂáΩÊï∞ sigmoid ÂáΩÊï∞ ‰ΩúÁî®ÔºöÊääÊó†ÈôêÂèòÊúâÈôêÔºåÊääÊó†Á©∑ÂΩí‰∏ÄÔºåÂºïÂÖ•ÈùûÁ∫øÊÄßÔºåÂÖ®ÂüüÂèØÂØºÔºàÂØºÊï∞Âú®0-0.25‰πãÈó¥ÔºâÔºõÁº∫ÁÇπÔºöÂ∫îÁî®ÈìæÂºèÊ≥ïÂàôÂÆπÊòìÈÄ†ÊàêÊ¢ØÂ∫¶Ê∂àÂ§±ÔºõËæìÂÖ•ÁâπÂæÅÊúÄÂ•Ω‰ΩøÂùáÂÄº‰∏∫0ÁöÑÂ∞èÊï∞Ôºå‰ΩÜÁªèËøásigmoidÂêéÂèò‰∏∫Ê≠£Êï∞ÔºåÂØºËá¥Êî∂ÊïõÊÖ¢ÔºõÂÖ∂‰∏≠ÊúâÂπÇËøêÁÆóÔºåËÆ°ÁÆóÊó∂Èó¥Èïø\ntf.math.tanh(x) ÊøÄÊ¥ªÂêéÁöÑËæìÂá∫ÁöÑÂùáÂÄºÊòØ0ÔºåÂØºÊï∞Âú®0-1‰πãÈó¥Ôºå‰ªçÊòìÈÄ†ÊàêÊ¢ØÂ∫¶Ê∂àÂ§±Ôºå‰∏§Ê¨°ÂπÇËøêÁÆóÔºåËÆ°ÁÆóÊó∂Èó¥ Êõ¥Èïø\ntf.nn.relu(x) f(x)=max(x,0) ‰ºòÁÇπÔºöÂØºÊï∞‰∏çÊòØ0Â∞±ÊòØ1ÔºåÂú®Ê≠£Âå∫Èó¥ÂÜÖËß£ÂÜ≥‰∫ÜÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÔºõÂè™ÈúÄÂà§Êñ≠ËæìÂÖ•ÊòØÂê¶Â§ß‰∫é0ÔºåËÆ°ÁÆóÈÄüÂ∫¶Âø´ÔºõÊî∂ÊïõÈÄüÂ∫¶ËøúÂø´‰∫ésigmoid Âíå tanh„ÄÇÁº∫ÁÇπÔºöËæìÂá∫‰∏çÊòØ‰ª•0‰∏∫ÂùáÂÄºÔºåÊî∂ÊïõÊÖ¢ÔºõDead ReluÈóÆÈ¢òÔºåÂΩìËæìÂÖ•ÁâπÂæÅÊòØË¥üÊï∞Êó∂ÔºåÊøÄÊ¥ªÂáΩÊï∞ËæìÂá∫‰∏∫0ÔºåÂèçÂêë‰º†Êí≠Êó∂ÔºåÊ¢ØÂ∫¶‰∏∫0ÔºåÂØºËá¥Áõ∏Â∫îÁöÑÂèÇÊï∞Êó†Ê≥ïÊõ¥Êñ∞„ÄÇÂèØ‰ª•ÊîπËøõÈöèÊú∫ÂàùÂßãÂåñÔºåÈÅøÂÖçËøáÂ§öÁöÑË¥üÊï∞ÁâπÂæÅËæìÂÖ•ReluÂáΩÊï∞ÔºõÂèØ‰ª•ÈÄöËøáÂáèÂ∞èÂ≠¶‰π†ÁéáÔºåÂáèÂ∞èÂèÇÊï∞ÂàÜÂ∏ÉÁöÑÂ∑®Â§ßÂèòÂåñÔºåÈÅøÂÖçËÆ≠ÁªÉ‰∏≠‰∫ßÁîüËøáÂ§öÁöÑË¥üÊï∞ÁâπÂæÅ\ntf.nn.leaky_relu(x) f(x) = max(ax, x) ÊòØ‰∏∫‰∫ÜËß£ÂÜ≥ReluÂú®Ë¥üÂå∫Èó¥ÁöÑÂØºÊï∞‰∏∫0 ÂºïËµ∑Á•ûÁªèÂÖÉÊ≠ª‰∫°ÈóÆÈ¢òËÄåËÆæËÆ°ÁöÑÔºåÂÆÉÂú®Ë¥üÂå∫Èó¥ÂºïÂÖ•‰∫ÜÂõ∫ÂÆöÁöÑÊñúÁéáa„ÄÇÁêÜËÆ∫‰∏äÊù•ËÆ≤ÔºåLeaky Relu Êúâ Relu ÁöÑÊâÄÊúâ‰ºòÁÇπÔºåÂ§ñÂä†‰∏ç‰ºöÊúâ Dead ReluÈóÆÈ¢òÔºå‰ΩÜÊòØÂú®ÂÆûÈôÖÊìç‰Ωú‰∏≠ÔºåÂπ∂Ê≤°ÊúâÂÆåÂÖ®ËØÅÊòé Leaky Relu ÊÄªÂ•Ω‰∫éRelu\nÂØπ‰∫éÂàùÂ≠¶ËÄÖÁöÑÂª∫ËÆÆÔºö\nÈ¶ñÈÄâreluÊøÄÊ¥ªÂáΩÊï∞ Â≠¶‰π†ÁéáËÆæÁΩÆËæÉÂ∞èÂÄº ËæìÂÖ•ÁâπÂæÅÊ†áÂáÜÂåñÔºåÂç≥ËÆ©ËæìÂÖ•ÁâπÂæÅÊª°Ë∂≥‰ª•0‰∏∫ÂùáÂÄºÔºå‰ª•1‰∏∫Ê†áÂáÜÂ∑ÆÁöÑÊ≠£ÊÄÅÂàÜÂ∏É ÂàùÂßãÂèÇÊï∞‰∏≠ÂøÉÂåñÔºåÂç≥ËÆ©ÈöèÊú∫ÁîüÊàêÁöÑÂèÇÊï∞Êª°Ë∂≥‰ª•0‰∏∫ÂùáÂÄºÔºåÊ†πÂè∑‰∏ãÂΩìÂâçÂ±ÇËæìÂÖ•ÁâπÂæÅ‰∏™Êï∞ÂàÜ‰πã2 ‰∏∫Ê†áÂáÜÂ∑ÆÁöÑÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ 2.4 ÊçüÂ§±ÂáΩÊï∞ È¢ÑÊµãÂÄºy ‰∏é Â∑≤Áü•Á≠îÊ°à y_ ÁöÑÂ∑ÆË∑ù\nNN‰ºòÂåñÁõÆÊ†áÔºölossÊúÄÂ∞èÔºå‰∏ªÊµÅ‰∏âÁßçËÆ°ÁÆóÊñπÊ≥ïÔºömseÔºå‰∫§ÂèâÁÜµÔºåËá™ÂÆö‰πâ\nloss_mse = tf.reduce_mean(tf.square(y_ - y))\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import tensorflow as tf import numpy as np SEED = 23455 rdm = np.random.RandomState(seed=SEED) x = rdm.rand(32, 2)\t# ÁúüÂÆûy = x1+x2 y_ = [[x1+x2 + (rdm.rand()/10.0 - 0.05)] for (x1, x2) in x] # Âä†‰∏äÂô™Â£∞[0,1)/10 = [0,0.1); [-0.05, 0.05) x = tf.cast(x, dtype=tf.float32) w1 = tf.Variable(tf.random.normal([2,1], stddev=1, seed=1)) # ÂàùÂßãÂåñÁΩëÁªúÂèÇÊï∞ 2x1 epoch = 15000 lr = 0.002 for epoch in range(epoch): with tf.GradientTape() as tape: y = tf.matmul(x, w1)\t# ÂâçÂêë loss_mse = tf.reduce_mean(tf.square(y_ - y)) grads = tape.gradient(loss_mse, w1)\t# ÊçüÂ§±ÂáΩÊï∞ÂØπÂèÇÊï∞Ê±ÇÂÅèÂØº w1.assign_sub(lr * grads)\t# Êõ¥Êñ∞ÂèÇÊï∞ if epoch % 500 == 0:\t# ÊØèËø≠‰ª£500ËΩÆÔºåÊâìÂç∞ÂèÇÊï∞ print(f\u0026#34;After {epoch} training setps, w1 is {w1.numpy()}\u0026#34;) print(f\u0026#34;final w1 is {w1.numpy()}\u0026#34;) # [[1.0009] [0.9977]] „ÄÇ„ÄÇ„ÄÇ„ÄÇ\n2.6 ‰ºòÂåñÂô® ÂèÇÊï∞Êõ¥Êñ∞Ôºö‰∏ã‰∏ÄÊó∂ÂàªÁöÑÂèÇÊï∞ Á≠â‰∫é ÂΩìÂâçÊó∂ÂàªÁöÑÂèÇÊï∞ ÂáèÂéª Œ∑ ÔºàÊ¢ØÂ∫¶‰∏ãÈôçÔºâ„ÄÇŒ∑Á≠â‰∫éÂ≠¶‰π†Áéá‰πò‰∏äÔºà‰∏ÄÈò∂Âä®ÈáèÈô§‰ª•Ê†πÂè∑‰∏ã‰∫åÈò∂Âä®ÈáèÔºâ„ÄÇ\nSGD‰∏≠Êó†Âä®ÈáèÔºå‰∏ÄÈò∂Âä®ÈáèÁ≠â‰∫élossÂØπÂèÇÊï∞ÁöÑÂÅèÂØºÊï∞Ôºå‰∫åÈò∂Âä®Èáè=1ÔºåÊâÄ‰ª• Œ∑ ÊòØÊ≤øÊ¢ØÂ∫¶ÊñπÂêëÊîπÂèòÁöÑÊ≠•Èïø„ÄÇ\nÂçïÂ±ÇÁΩëÁªú w*x+b Â∫îÁî® SGDÔºö\n1 2 w1.assign_sub(lr * grads[0]) b1.assign_sub(lr * grads[1]) SGDM Âú®SGDÂü∫Á°Ä‰∏äÂ¢ûÂä†‰∫Ü‰∏ÄÈò∂Âä®ÈáèÔºå‰∏ÄÈò∂Âä®ÈáèÁ≠â‰∫é‰∏ä‰∏ÄÊó∂ÂàªÁöÑ‰∏ÄÈò∂Âä®Èáè‰∏éÂΩìÂâçÊó∂ÂàªÁöÑÊ¢ØÂ∫¶ÂÖ±Âêå‰ΩúÁî®Ôºöm‚Çú = Œ≤‚ãÖm‚Çú‚Çã‚ÇÅ + (1-Œ≤)‚ãÖg‚Çú Ôºå Œ≤ÊòØÊé•Ëøë1ÁöÑÁ≥ªÊï∞ÔºàÁªèÈ™åÂÄº0.9ÔºâÔºåÊâÄ‰ª•‰∏ä‰∏ÄÊó∂ÂàªÁöÑÂä®ÈáèÂç†‰∏ªÂØºÔºõ‰∫åÈò∂Âä®Èáè=1„ÄÇ ‰∏ÄÈò∂Âä®ÈáèÊòØ‚ÄúÊåáÊï∞ÊªëÂä®Âπ≥ÂùáÂÄº‚ÄùÔºüËøáÂéª‰∏ÄÊÆµÊó∂Èó¥ÁöÑÂπ≥ÂùáÂÄºÔºü\n1 2 3 4 5 6 7 8 m_w, m_b = 0, 0 beta = 0.9 # sgd-momentum m_w = beta * m_w + (1-beta) * grads[0]\t# gradsÊòØlossÂØπÂêÑÂèÇÊï∞ÁöÑÂÅèÂØºÊï∞ m_b = beta * m_b + (1-beta) * grads[1] w1.assign_sub(lr * m_w) b1.assign_sub(lr * m_b) Adagrad Âú®SGD Âü∫Á°Ä‰∏äÂ¢ûÂä†‰∫Ü‰∫åÈò∂Âä®ÈáèÔºåÂèØ‰ª•ÂØπÊ®°Âûã‰∏≠ÁöÑÊØè‰∏™ÂèÇÊï∞ÂàÜÈÖçËá™ÈÄÇÂ∫îÂ≠¶‰π†Áéá‰∫Ü„ÄÇ‰∏ÄÈò∂Âä®Èáè‰∏éSGDËÆæÁΩÆÁõ∏ÂêåÔºåÁ≠â‰∫élossÁöÑÊ¢ØÂ∫¶Ôºå‰∫åÈò∂Âä®ÈáèÊòØ‰ªéÂºÄÂßãÊó∂ÂàªÂà∞Áé∞Âú®ÔºåÊ¢ØÂ∫¶Âπ≥ÊñπÁöÑÁ¥ØËÆ°Âíå $V_t = \\sum_{œÑ=1}^t g_œÑ^2$\n1 2 3 4 5 6 7 v_w, v_b = 0, 0\t# ‰∫åÈò∂Âä®ÈáèÈõ∂Êó∂ÂàªÂàùÂÄº‰∏∫0 # Âú® for Âæ™ÁéØÈáåÔºåÊØèbatchÊõ¥Êñ∞‰∏ÄÊ¨°Ôºö v_w += tf.square(grads[0])\t# Ê¢ØÂ∫¶Âπ≥ÊñπÁöÑÁ¥ØËÆ°Âíå v_b += tf.square(grads[1]) w1.assign_sub(lr * grads[0] / tf.sqrt(v_w)) b1.assign_sub(lr * grads[1] / tf.sqrt(v_b)) RMSProp Âú®SGDÂü∫Á°Ä‰∏äÂ¢ûÂä†‰∫Ü‰∫åÈò∂Âä®ÈáèÔºå‰∏ÄÈò∂Âä®Èáè‰ªçÁ≠â‰∫éÂΩìÂâçÊó∂ÂàªlossÁöÑÊ¢ØÂ∫¶Ôºå‰∫åÈò∂Âä®Èáè $V_t = Œ≤‚ãÖV_{t-1} + (1-Œ≤)‚ãÖg_t^2$\n1 2 3 4 5 6 7 8 v_w, v_b = 0, 0\t# ‰∫åÈò∂Âä®ÈáèÈõ∂Êó∂ÂàªÂàùÂÄº‰∏∫0 beta = 0.9 # Âú® for Âæ™ÁéØÈáåÔºåÊØèbatchÊõ¥Êñ∞‰∏ÄÊ¨°Ôºö v_w += beta*v_w + (1-beta) * tf.square(grads[0])\t# Ê¢ØÂ∫¶Âπ≥Êñπ ‰∏é ‰∏ä‰∏ÄÊó∂ÂàªÁöÑ‰∫åÈò∂Ê¢ØÂ∫¶ÁöÑÂä†ÊùÉÂíå v_b += beta*v_b + (1-beta) * tf.square(grads[1]) w1.assign_sub(lr * grads[0] / tf.sqrt(v_w)) b1.assign_sub(lr * grads[1] / tf.sqrt(v_b)) Adam ÂêåÊó∂ÁªìÂêà‰∫ÜSGDMÁöÑ‰∏ÄÈò∂Âä®ÈáèÂíåRMSPropÁöÑ‰∫åÈò∂Âä®ÈáèÔºåÂπ∂ÂàÜÂà´Â¢ûÂä†‰øÆÊ≠£Á≥ªÊï∞Ôºöm‚Çú = Œ≤‚ÇÅ‚ãÖm‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)‚ãÖg‚Çú Ôºå$\\hat{m_t} = \\frac{m_t}{1-Œ≤_1^t}$ ÔºåV‚Çú = Œ≤‚ÇÇ‚ãÖV‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÇ)‚ãÖg‚Çú¬≤Ôºå$\\hat{V_t}=\\frac{V_t}{1-Œ≤_2^t}$ , t ÊòØ‰ªéÂºÄÂßãÂà∞Áé∞Âú®ÁªèÂéÜÁöÑÊâÄÊúâbatchÊï∞\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 m_w, m_b = 0, 0 v_w, v_b = 0, 0 beta1, beta2 = 0.9, 0.999 delta_w, delta_b = 0, 0 global_step = 0 m_w = beta1 * m_w + (1 - beta1) * grads[0]\t# ‰∏ÄÈò∂Âä®Èáè=‰∏ä‰∏ÄÊó∂Âàª‰∏éÊ≠§ÂàªÊ¢ØÂ∫¶Âä†ÊùÉÂíå m_b = beta1 * m_b + (1 - beta1) * grads[1] v_w = beta2 * v_w + (1-beta2) * tf.square(grads[0])\t# ‰∫åÈò∂Âä®Èáè=‰∏ä‰∏ÄÊó∂Âàª‰∏éÊ≠§ÂàªÊ¢ØÂ∫¶Âπ≥ÊñπÁöÑÂä†ÊùÉÂíå v_b = beta2 * v_b + (1-beta2) * tf.square(grads[1]) # ‰øÆÊ≠£ m_w_correction = m_w / (1-tf.pow(beta1, int(global_step))) m_b_correction = m_b / (1-tf.pow(beta1, int(global_step))) v_w_correction = v_w / (1-tf.pow(beta2, int(global_step))) v_b_correction = v_b / (1-tf.pow(beta2, int(global_step))) w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction)) b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction)) ‰∏çÂêå‰ºòÂåñÂô®ËÆ≠ÁªÉÈÄüÂ∫¶‰∏çÂêå\n","date":"2022-07-24T13:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/2_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/","title":"watch: TF2 - PKU 02 | NN Optimization Methods"},{"content":"ÂàõÂª∫Âº†Èáè tf.constant(Âº†ÈáèÂÜÖÂÆπ, dtype=Êï∞ÊçÆÁ±ªÂûã(ÂèØÈÄâ))\n1 2 3 4 5 import tensorflow as tf a = tf.constant([1,5], dtype=tf.int64) # ÂàõÂª∫‰∏ÄÈò∂Âº†Èáè, 2‰∏™ÂÖÉÁ¥† print(a) # tf.Tensor([1,5], shape=(2,), dtype=int64). tf1.x‰∏ç‰ºöÊòæÁ§∫ÂÖÉÁ¥† print(a.dtype) # \u0026lt;dtype: \u0026#39;int64\u0026#39;\u0026gt; print(a.shape) # (2,) Â∞ÜnumpyÁöÑÊï∞ÊçÆÁ±ªÂûãËΩ¨Êç¢‰∏∫ Tensor Êï∞ÊçÆÁ±ªÂûã:\ntf.convert_to_tensor(Êï∞ÊçÆÂêçÔºådtype=Êï∞ÊçÆÁ±ªÂûãÔºàÂèØÈÄâÔºâ)\n1 2 3 4 5 6 import tensorflow as tf import numpy as np a = np.arange(0, 5) b = tf.convert_to_tensor(a, dtype=tf.int64) print(a) # [0 1 2 3 4] print(b) # tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64) ÁîüÊàêÂÖ®0ÔºåÂÖ®1ÔºåÂÖ®ÊåáÂÆöÂÄºÁöÑÂº†ÈáèÔºö\n1 2 3 4 5 tf.zeros([2,3]) #tf.Tensor([[0. 0. 0.] [0. 0. 0 .]],shape=(2,3), dtype=float32) tf.ones(4) #tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32) tf.fill([2,2], 9) # tf.Tensor([[9 9] [9 9]], shape=(2,2),dtype=int32) ÁîüÊàêÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÈöèÊú∫Êï∞ÔºàÈªòËÆ§ÂùáÂÄº‰∏∫0ÔºåÊ†áÂáÜÂ∑Æ‰∏∫1ÔºâÔºåÂ∏∏Áî®‰∫éÂàùÂßãÂåñÂèÇÊï∞:\ntf.random.normal(Áª¥Â∫¶Ôºåmeans=ÂùáÂÄºÔºåstddev=Ê†áÂáÜÂ∑Æ)\nÁîüÊàêÊà™Êñ≠ÂºèÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÈöèÊú∫Êï∞ÔºåÂàÜÂ∏ÉÊõ¥ÈõÜ‰∏≠Âú®ÂùáÂÄºÈôÑËøëÔºåÈöèÊú∫Êï∞ÂèñÂÄºÂú®Ê≠£Ë¥ü2‰∏™Ê†áÂáÜÂ∑Æ‰πãÂÜÖ (mu-2sigma, mu+2sigma)ÔºåÂ¶ÇÊûúËêΩÂú®Â§ñÈù¢ÂàôÈáçÊñ∞ÁîüÊàêÔºö\ntf.random.truncated_normal(Áª¥Â∫¶Ôºå mean=ÂùáÂÄºÔºåstddev=Ê†áÂáÜÂ∑Æ)\n1 2 d = tf.random.normal([2,2], mean=0.5, stddev=1) e = tf.random.truncated_normal([2,2], mean=0.5, stddev=1) ÁîüÊàêÂùáÂåÄÂàÜÂ∏ÉÈöèÊú∫Êï∞ [min, max) Â∑¶Èó≠Âè≥ÂºÄÔºö\ntf.random.uniform(Áª¥Â∫¶Ôºåminval=ÊúÄÂ∞èÂÄºÔºåmaxval=ÊúÄÂ§ßÂÄº)\nÂ∏∏Áî®ÂáΩÊï∞ Âº∫Âà∂ tensor ËΩ¨Êç¢‰∏∫ÊåáÂÆöÁ±ªÂûã tf.cast(Âº†ÈáèÂêçÔºådtype=Êï∞ÊçÆÁ±ªÂûã) ËÆ°ÁÆóÂº†ÈáèÁª¥Â∫¶‰∏äÂÖÉÁ¥†ÁöÑÊúÄÂ∞èÂÄº tf.reduce_min(Âº†ÈáèÂêç) ÊâæÂà∞Âº†Èáè‰∏≠ÁöÑÊúÄÂ§ßÂÖÉÁ¥†Ôºötf.reduce_max(Âº†ÈáèÂêç) 1 2 3 4 5 6 7 x1 = tf.constant([1., 2., 3.], dtype=tf.float64) print(x1) # tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64) x2 = tf.cast(x1, tf.int32) print(x2) # tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64) print(tf.reduce_min(x2), tf.reduce_max(x2)) # tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(1, shape=(), dtype=int32) axisÊåáÂÆöÊìç‰ΩúÊñπÂêëÔºåÂØπ‰∫é‰∫åÁª¥Âº†ÈáèÔºåaxis=0Ë°®Á§∫ÂØπÁ¨¨0Áª¥Êìç‰ΩúÔºõËã•‰∏çÊåáÂÆöaxisÔºåÂàôÊâÄÊúâÂÖÉÁ¥†ÂèÇ‰∏éËÆ°ÁÆó 1 2 3 4 5 x = tf.constant([[1,2,3], [2,2,3]]) print(tf.reduce_mean(x)) # ÊâÄÊúâÂÖÉÁ¥†ÁöÑÂπ≥ÂùáÂÄº tf.Tensor(2, shape=(), dtype=int32) print(tf.reduce_mean(x, axis=0)) # tf.Tensor([1 2 3], shape=(3,) dtype=int32) print(tf.reduce_sum(x, axis=1)) # ÂØπÁ¨¨1Áª¥Ê±ÇÂíå tf.Tensor([6 7], shape=(2,) dtype=int32) tf.Variable() Â∞ÜÂèòÈáèÊ†áËÆ∞‰∏∫‚ÄúÂèØËÆ≠ÁªÉ‚ÄùÔºåË¢´Ê†áËÆ∞ÁöÑÂèòÈáè‰ºöÂú®ÂèçÂêë‰º†Êí≠‰∏≠ËÆ∞ÂΩïÊ¢ØÂ∫¶‰ø°ÊÅØ„ÄÇÂú®Á•ûÁªèÁΩëÁªúËÆ≠ÁªÉ‰∏≠ÔºåÂ∏∏Áî®ËØ•ÂáΩÊï∞Ê†áËÆ∞ÂæÖËÆ≠ÁªÉÂèÇÊï∞„ÄÇ w = tf.Variable(tf.random.normal([2,2], mean=0, stddev=1)), ÊääÁîüÊàêÁöÑÈöèÊú∫Êï∞Ê†áËÆ∞‰∏∫ÂèØËÆ≠ÁªÉ\nÂ∏∏Áî®Êï∞Â≠¶ËøêÁÆóÔºötf.add, tf.subtract, tf.multiply, tf.divide, tf.square, tf.pow, tf.sqrt, tf.matmul„ÄÇ 1 2 3 4 tf.add(Âº†Èáè1ÔºåÂº†Èáè2) tf.subtract(Âº†Èáè1ÔºåÂº†Èáè2) tf.multiply(Âº†Èáè1ÔºåÂº†Èáè2) tf.divide(Âº†Èáè1ÔºåÂº†Èáè2) Âè™ÊúâÁª¥Â∫¶Áõ∏ÂêåÁöÑÂº†ÈáèÊâçÂèØ‰ª•ÂÅöÂõõÂàôËøêÁÆó\n1 2 3 4 5 6 a = tf.ones([1,3]) b = tf.fill([1,3], 3.] print(tf.add(a,b)) print(tf.subtract(a,b)) # tf.Tensor([[-2. -2. -2.]], shape=(1,3), dtype=float32) print(tf.multiply(a,b)) print(tf.divide(b,a)) # tf.Tensor([[3. 3. 3.]], shape=(1,3), dtype=float32) ‰∏§Áü©ÈòµÁõ∏‰πòÔºö\n1 2 3 a = tf.ones([3,2]) b = tf.fill([2,3], 3.) print(tf.matmul(a,b)) # tf.Tensor([[6. 6. 6.] [6. 6. 6.] [6. 6. 6.], shape=(3,3), dtype=float32) ÊääÁâπÂæÅÂíåÊ†áÁ≠æÈÖçÂØπ tf.data.Dataset.from_tensor_slices((ËæìÂÖ•ÁâπÂæÅÔºåÊ†áÁ≠æ))ÔºåNumpyÂíåTensorÊ†ºÂºèÈÉΩÈÄÇÁî® 1 2 3 4 5 6 features = tf.constant([12, 23, 10, 17]) # ‰∏Ä‰∏™Êï∞ÊòØ‰∏Ä‰∏™Ê†∑Êú¨ labels = tf.constant([0, 1, 1, 0]) dataset = tf.data.Dataset.from_tensor_slices((features, labels)) print(dataset) for element in dataset: print(element) ËøêË°åÁªìÊûúÔºö\n1 2 3 4 5 \u0026lt;TensorSliceDataset shapes: ((),()), types: (tf.int32, tf.int32)\u0026gt; ÔºàÁâπÂæÅÔºåÊ†áÁ≠æÔºâÂØπ (\u0026lt;tf.Tensor: id=9, shape=(), dtype=int32, numpy=12\u0026gt;, \u0026lt;tf.Tensor: id=10, shape=(), dtype=int32, numpy=0\u0026gt;) (\u0026lt;tf.Tensor: id=11, shape=(), dtype=int32, numpy=23\u0026gt;, \u0026lt;tf.Tensor: id=12, shape=(), dtype=int32, numpy=1\u0026gt;) (\u0026lt;tf.Tensor: id=13, shape=(), dtype=int32, numpy=10\u0026gt;, \u0026lt;tf.Tensor: id=14, shape=(), dtype=int32, numpy=1\u0026gt;) (\u0026lt;tf.Tensor: id=15, shape=(), dtype=int32, numpy=17\u0026gt;, \u0026lt;tf.Tensor: id=16, shape=(), dtype=int32, numpy=0\u0026gt;) ÂÆûÁé∞ÊüêÂáΩÊï∞ÂØπÊåáÂÆöÂèÇÊï∞ÁöÑÊ±ÇÂØºËøêÁÆóÔºåÁî® with ÁªìÊûÑËÆ∞ÂΩïËÆ°ÁÆóËøáÁ®ãÔºö\n1 2 3 with tf.GradientTape() as tape: Ëã•Âπ≤‰∏™ËÆ°ÁÆóËøáÁ®ã grad = tape.gradient(ÂáΩÊï∞ÔºåÂØπË∞ÅÊ±ÇÂØº) ‰æãÂ¶ÇÔºö\n1 2 3 4 5 with tf.GradientTape() as tape: w = tf.Variable(tf.constant(3.0))\t# ÂàùÂÄº‰∏∫3ÔºåÂèØ‰ª•Ê±ÇÂØº loss = tf.pow(w, 2) grad = tape.gradient(loss, w)\t# loss ÂØπ w Ê±ÇÂØºÔºå2w=6 print(grad)\t# tf.Tensor(6.0, shape=(), dtype=float32) Âú®ÈÅçÂéÜÊó∂ËøîÂõûÁ¥¢ÂºïÂè∑ enumerate(iterable)\n1 2 3 seq = [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;] for count, value in enumerate(seq): print(count, value) ÂàÜÁ±ªÈóÆÈ¢ò‰∏≠ÔºåÁî®Áã¨ÁÉ≠Á†ÅË°®Á§∫Ê†áÁ≠æÔºåtf.one_hot(ÂæÖËΩ¨Êç¢Êï∞ÊçÆ, depth=Âá†ÂàÜÁ±ª) Â∞ÜÊ†áÁ≠æÂàóË°®ËΩ¨Êç¢‰∏∫ one-hot ÂΩ¢ÂºèÁöÑÊï∞ÊçÆ\n1 2 3 4 classes = 3 labels = tf.constant([1,0,2])\t# ËæìÂÖ•ÁöÑÂÖÉÁ¥†ÂÄºÊúÄÂ∞è‰∏∫0ÔºåÊúÄÂ§ß‰∏∫2 output = tf.one_hot(labels, depth=classes) print(output)\t# [[0. 1. 0.] [1. 0. 0.] [0. 0. 1.]],shape=(3,3), dtype=float32) ÂÖàÂÅöÂ§ßÂ∞èÊéíÂ∫èÔºü ‰ΩøÁΩëÁªúËæìÂá∫Á¨¶ÂêàÊ¶ÇÁéáÂàÜÂ∏É tf.nn.softmax(x)\n1 2 3 y = tf.constant([1.01, 2.01, -0.66]) y_prob = tf.nn.softmax(y) print(y_prob)\t# tf.Tensor([0.25598174 0.69583046 0.0481878], shape=(3,), dtype=float32) ÂèÇÊï∞Ëá™Êõ¥Êñ∞ (Ëá™Âáè) assign_sub()ÔºåÂèÇÊï∞Ë¶ÅÁî®tf.VariableÂÆö‰πâ‰∏∫‚ÄùÂèØËÆ≠ÁªÉ‚Äú\n1 2 3 w = tf.Variable(4) w.assign_sub(1)\t# w -=1, Âç≥ w=w-1 print(w)\t# \u0026lt;tf.Variable \u0026#39;Variable:0\u0026#39; shape=() dtype=int32,numpy=3\u0026gt; ËøîÂõûÊåáÂÆöÁª¥Â∫¶ÁöÑÊúÄÂ§ßÂÄºÁöÑÁ¥¢Âºï tf.argmax(Âº†ÈáèÂêçÔºåaxis=Êìç‰ΩúËΩ¥)\n1 2 3 test = np.array([[1, 2, 3] [2,3,4] [5,4,3] [8,7,2]])\t# shape=(4,3) print(tf.argmax(test, axis=0))\t# tf.Tensor([3 3 1], shape=(3,), dtype=int64) print(tf.argmax(test, axis=1))\t# tf.Tensor([2 2 0 0 ],shape=(4,),dtype=int64) È∏¢Â∞æËä±ÂàÜÁ±ª 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # 1. Prepare data # read from sklearn.datasets import datasets x_data = datasets.load_iris().data\t# features y_data = datasets.load_iris().target\t# labels # mess up np.random.seed(116) np.random.shuffle(x_data) np.random.seed(116)\t# ‰ΩøÁî®Áõ∏ÂêåÁöÑseedÔºåÁâπÂæÅ‰∏éÊ†áÁ≠æ‰øùÊåÅÂØπÂ∫î np.random.shuffle(y_data) tf.random.set_seed(116) # separate Ê∞∏‰∏çÁõ∏ËßÅÁöÑËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ x_train = x_data[:-30]\t# Ââç120 y_train = y_data[:-30] x_test = x_data[-30:] y_test = y_data[-30:] # pairÔºå ÊØèÊ¨°ËæìÂÖ•‰∏Ä‰∏™batch train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) # ÂÆö‰πâÁΩëÁªú‰∏≠ÊâÄÊúâÂèØËÆ≠ÁªÉÂèÇÊï∞ w1 = tf.Variable(tf.random.truncated_normal([4,3], stddev=0.1, seed=1)) # ‰∏ÄÂ±ÇÁΩëÁªúÊåáÁöÑ‰ΩøÈÇ£‰∏™ÂØÜÂØÜÈ∫ªÈ∫ª‰∫§ÁªáÁöÑÁΩëÁªúÂïäÔºåÈáçÁÇπ‰∏çÂú®ËäÇÁÇπÔºåËÄåÂú®ÁΩëÔºÅ b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1)) lr=0.1 train_loss_results = []\t# Â≠òÂÇ®ÂêÑepochÁöÑloss test_acc = []\t# Â≠òÂÇ®ÂêÑepochÂêéÂú®ÊµãËØïÈõÜ‰∏äÁöÑÂáÜÁ°ÆÁéá epoch = 500 loss_all = 0\t# ÂêÑbatchÁöÑlossÊ±ÇÂíå # ÂµåÂ•óÂæ™ÁéØËø≠‰ª£ÔºåwithÁªìÊûÑÊõ¥Êñ∞ÂèÇÊï∞ÔºåÊòæÁ§∫ÂΩìÂâçloss for epoch in range(epoch):\t# Êï∞ÊçÆÈõÜÁ∫ßÂà´Ëø≠‰ª£ for step, (x_train, y_train) in enumerate(train_db): # batch Á∫ßÂà´Ëø≠‰ª£ with tf.GradientTape() as tape:\t# ËÆ∞ÂΩïÊ¢ØÂ∫¶‰ø°ÊÅØ y = tf.matmul(x_train, w1) + b1\t# Á∫øÊÄß y = tf.nn.softmax(y)\t# ÂèòÊàêÊ¶ÇÁéá y_ = tf.one_hot(y_train, depth=3)\t# Ê†áÁ≠æÂèò‰∏∫Áã¨ÁÉ≠Á†ÅÔºåÊñπ‰æøÂàÜÁ±ª loss = tf.reduce_mean(tf.square(y_ - y))\t# ÂùáÊñπËØØÂ∑ÆÊçüÂ§± loss_all += loss.numpy()\t# ÂêÑbatchÁöÑlossÁ¥ØÂä† grads = tape.gradients(loss, [w1, b1])\t# 1‰∏™batchÁöÑloss ÂØπ w1,b1 Ê±ÇÂÅèÂØº w1.assign_sub(lr*grads[0])\t# ÂèÇÊï∞Ëá™Êõ¥Êñ∞ b1.assign_sub(lr*grads[1]) print(\u0026#34;Epoch {}, loss: {}\u0026#34;.format(epoch, loss_all/4))\t# 4‰∏™batchÁöÑlossÂπ≥Âùá‰∏Ä‰∏ã train_loss_results.append(loss_all / 4) loss_all = 0 # ÊØè‰∏™epoch‰πãÂêéÔºåÂú®testÈõÜ‰∏äÁöÑË°®Áé∞ total_correct, total_number = 0,0 for x_test, y_test in test_db: y = tf.matmul(x_test,w1) + b1\t# y y = tf.nn.softmax(y) pred = tf.argmax(y, axis=1)\t# ÊèêÂèñÁ±ªÂà´ pred = tf.cast(pred, dtype=y_test.dtype)\t# ËΩ¨Êç¢Âà∞‰∏éy_testÁõ∏ÂêåÁöÑÊï∞ÊçÆÁ±ªÂûã correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32) # ÊääboolÁªìÊûúËΩ¨Êç¢‰∏∫int correct = tf.reduce_sum(correct)\t# ÂêÑbatchÁöÑÊ≠£Á°ÆÊï∞Âä†Ëµ∑Êù• total_correct += int(correct)\t# ÂΩìÂâçÊ≠£Á°ÆÁéá total_number += x_test.shape[0]\t# ÂΩìÂâçÂ∑≤ÊµãËØïËøáÁöÑÊ†∑Êú¨Êï∞ÁõÆ acc = total_correct / total_number\t# ‰∏Ä‰∏™epochÂêéÔºåÂú®ÊµãËØïÈõÜ‰∏äÁöÑÂáÜÁ°ÆÁéá test_acc.append(acc) print(\u0026#34;Test_acc:\u0026#34;, acc) print(\u0026#34;----------------\u0026#34;) # ÁªòÂà∂lossÊõ≤Á∫ø plt.title(\u0026#39;Loss Function Curve\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.plot(train_loss_results, label=\u0026#34;$Loss$\u0026#34;) plt.legend() plt.show() plt.title(\u0026#34;ACC Curve\u0026#34;) plt.xlabel(\u0026#34;Epoch\u0026#34;) plt.ylabel(\u0026#39;Acc\u0026#39;) plt.plot(test_acc, label=\u0026#34;$Accuracy$\u0026#34;) plt.legend() plt.show() ","date":"2022-07-24T12:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B/","title":"watch: TF2 - PKU 01 | NN Computation Process"},{"content":"F.grid_sample F.grid_sample(tensor,p,mode) Êää tensor ÊèíË°•Êàê‰∏éÁΩëÊ†º p ÊúâÁõ∏ÂêåÂ§ßÂ∞èÁöÑ tensor„ÄÇ ÁΩëÊ†ºpÊåáÂÆö‰∫ÜÂú® input ‰∏äÁöÑÈááÊ†∑ÁÇπÂùêÊ†áÔºåÂú®ÈááÊ†∑ÁÇπÈôÑËøëÊèíÂÄºÂΩ¢Êàê‰∏Ä‰∏™Êñ∞ÂÉèÁ¥†ÔºåÂêÑ channel ‰∏äÈááÊ†∑ÁÇπÁõ∏Âêå„ÄÇ grid ‰∏≠ÁöÑ (x,y) Â∑≤Áº©ÊîæÂà∞ [-1,1]ÔºåÂú®ÂáΩÊï∞ÂÜÖÂèòÊç¢Âà∞ [0,W],[0,H] Á¥¢ÂºïÂÉèÁ¥† PyTorch‰∏≠grid_sampleÁöÑ‰ΩøÁî®ÊñπÊ≥ï-csdn\nmode = \u0026lsquo;bilinear\u0026rsquo; ÊòØ‰∏âÁ∫øÊÄßÊèíÂÄº: ÂèåÁ∫øÊÄßÊèíÂÄºÔºàbilinearÔºâÁöÑ3DÂΩ¢Âºè(ÂçøÂçøÂ∞èÂæêÁöÑËØÑËÆ∫); c++Ê∫êÁ†Å\n4D input ÂØπÂ∫î‰∏Ä‰∏™ batch ÁöÑÂõæÁâá (B,C,H,W)„ÄÇ\nalign_corners = True ËÆ§‰∏∫input‰ª•ÂÉèÁ¥†‰∏∫Âçï‰ΩçÔºåÂêÑÂÉèÁ¥†Áî±ÂÖ∂‰∏≠ÂøÉÁÇπ‰ª£Ë°®Ôºå‰ªéËÄåÂÅöÂèåÁ∫øÊÄßÊèíÂÄºÁöÑÊó∂ÂÄôÔºåÂ∞±Áî®ÂÉèÁ¥†ÁöÑÂõõ‰∏™ËßíÁÇπÔºõ (2023-10-08: ËßíÁÇπÔºüÈÇªÁÇπÔºü)\nFalse ÊòØÊää input ÂΩì‰ΩúÂêÑÂÉèÁ¥†ËßíÁÇπÁöÑÈõÜÂêàÔºåÊ≠§Êó∂ input ÁöÑËæπÁïå (W-1,H-1) Â∞è‰∫éÂéüÊù•ÁöÑÂõæÁâáËæπÁïå (W,H)ÔºåÂàôgridÈááÊ†∑ÁÇπÁöÑÂùêÊ†áÂèØËÉΩË∂ÖÂá∫ inputËæπÁïåÔºàÊØîÂ¶ÇÈááÊ†∑ÁÇπËêΩÂú®ÂõæÁâáËæπÁºòÔºåËÄåÂèåÁ∫øÊÄßÊèíÂÄºÈúÄË¶ÅÁî®Âë®Âõ¥ 4 ‰∏™ÁÇπÔºâÔºåÊâÄ‰ª•ÈúÄË¶ÅÂú®inputÂ§ñÂõ¥paddingÔºåÂÜç‰∏éÂêÑneighborÁÇπÂÅöÊèíÂÄº„ÄÇ\ntorch.nn.functional.grid_sample() ÂáΩÊï∞ÂÆû‰æã-csdn\nAs shown below, black points are input datapoints. In the left figure, the datapoints fit the pixels corners of the input image to be scaled, whereas in the right figure, the datapoints forms an independent image to perform interpolation.\na = l i T g r n u _ e c o r n e r s a = l i F g a n l _ s c e o r n e r s A numerical example of difference: Docs - nn.Upsample\n(2024-03-09) In the left figure, the corners of the grid formed by pixels and sampling points are aligned. Whereas, the right figure isn\u0026rsquo;t.\nÂÉèÁ¥†‰∏≠ÂøÉÁÇπÁöÑ‰ΩçÁΩÆ‰∏é‰∏Ä‰∏™ÂÉèÁ¥†ÁöÑÂ§ßÂ∞èÊúâÂÖ≥ÔºåÊâÄ‰ª•ÂØπ‰∫éÁõ∏ÂêåÁª¥Â∫¶ÁöÑËæìÂÖ•ÔºåÈááÊ†∑ÁÇπÂùêÊ†áÂèØËÉΩ‰∏çÂêå„ÄÇ ÊâÄ‰ª•‰ΩøÁî®ÂÉèÁ¥†ËßíÁÇπÂÅö‰∏∫Âü∫ÂáÜ (align_corners = False)Ôºå‰∏éÂÉèÁ¥†Â∞∫ÂØ∏Êó†ÂÖ≥ÔºåÈááÊ†∑ÁÇπ‰ΩçÁΩÆÊòØÁõ∏ÂØπÁöÑ„ÄÇ Docs\nÁ∫øÊÄßÊèíÂÄºÊòØ‰∏ÄÁª¥ÈïøÂ∫¶ÁöÑÂä†ÊùÉÂπ≥ÂùáÔºåÂèåÁ∫øÊÄßÊèíÂÄºÊòØ‰∫åÁª¥Èù¢ÁßØÔºà‰∏§‰∏™ÊñπÂêëÔºâÁöÑÂä†ÊùÉÂπ≥ÂùáÔºöÊØè‰∏™È°∂ÁÇπÁöÑÊùÉÈáçÊòØÂÖ∂ÂØπËßí‰ΩçÁΩÆ‰∏äÁöÑÁü©ÈòµÈù¢ÁßØÂç†ÊØîÔºõ‰∏âÁ∫øÊÄßÊèíÂÄºÊòØ‰∏âÁª¥‰ΩìÁßØÁöÑÂä†ÊùÉÂíå„ÄÇ\n„Äê‰∏âÁª¥ÈáçÂª∫ÂíåNeRFÂ∏∏Áî®ÁöÑ‰∏âÁ∫øÊÄßÊèíÂÄºÔºåÂéüÁêÜËÆ≤Ëß£+‰ª£Á†ÅÂÆûÁé∞„Äë-ÊÑè„ÅÆËåó\nÂèåÁ∫øÊÄßÊèíÂÄºÔºöÂú®xÊñπÂêëÂíåyÊñπÂêë‰∏äÂÅöÁ∫øÊÄßÂõûÂΩíÂπ∂È¢ÑÊµãÂú®ÁõÆÊ†áÁÇπ‰∏äÁöÑÊï∞ÂÄº„ÄÇÊâÄÊèíÂÄºÊòØÂë®Âõ¥4‰∏™neighborÁöÑÂä†ÊùÉÂíåÔºåÊùÉÈáçÊòØ‰∏§ÊñπÂêë‰∏äneighborÂà∞ÁõÆÊ†áÁÇπË∑ùÁ¶ª‰∏é‰∏§ÁÇπÈó¥Ë∑ù‰πãÊØîÁöÑ‰πòÁßØ\ngrid_sample()ÂáΩÊï∞ÂèäÂèåÁ∫øÊÄßÈááÊ†∑ - 180Â§©ÂêéÂÜçÊîπÂêçÁöÑÊñáÁ´† - Áü•‰πé\n(2023-12-18) F.grid_sample() cannot sample images?\n1 2 3 import cv2 src = cv2.imread(\u0026#34;dtu/Rectified/scan11_train/rect_011_3_r5000.png\u0026#34;) # ndarray, (h,w,3) F.grid_sample(torch.from_numpy(src).unsqueeze(0), uv_src.view(1, 192*h, w, 2)) And it will lead to: RuntimeError: grid_sampler_2d_cpu not implemented for Byte\nTherefore, the image requires to be normalized to [0,1]:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import PIL import torch.nn.functional as F from torchvision import transforms from matplotlib import pyplot as plt h, w = 128, 160 y,x = torch.meshgrid(torch.arange(h), torch.arange(w)) normalized_y, normalized_x = y/h, x/w normalized_xy = torch.stack([normalized_x, normalized_y], dim=-1) src = PIL.Image.open(\u0026#34;dtu/Rectified/scan11_train/rect_009_6_r5000.png\u0026#34;) src = transforms.ToTensor()(src) samp= F.grid_sample(src.unsqueeze(0), normalized_xy.unsqueeze(0)) # revert to an image samp_scaled = torch.tensor(samp[0] * 255, dtype=int).permute(1,2,0) plt.imshow(samp_scaled) Example in GNT train_imgs is a 4D tesnor.\nfeatmaps is a 5D tensor (N, Chanl, D, H, W,), the first dimension also vary determined by the indexing tensor\n1 2 3 4 5 6 7 8 9 10 11 12 # compute the projection of the query points to each reference image pixel_locations, mask_in_front = self.compute_projections(xyz, train_cameras) # pixel coords: (n_views, n_rays, n_samples, 2), (n_views, n_rays, n_samples) normalized_pixel_locations = self.normalize( # pixel coords range: ([0,h],[0,w])-\u0026gt; [-1,1] for F.grid_sample pixel_locations, h, w ) # [n_views, n_rays, n_samples, 2] # rgb sampling rgbs_sampled = F.grid_sample(input=train_imgs, grid=normalized_pixel_locations, align_corners=True) # (n_views, 3, n_rays, n_samples) rgb_sampled = rgbs_sampled.permute(2, 3, 0, 1) # [n_rays, n_samples, n_views, 3] # deep feature sampling # sample n_view feature maps for each 3D point. All chanls on the location in a feature map will be taken. feat_sampled = F.grid_sample(featmaps, normalized_pixel_locations, align_corners=True) # (n_views, out_chnls, H, W)-\u0026gt;(n_views, out_chnls, n_rays,n_samples) F.interpolate F.interpolate(input, size, [scale_factor,] mode, ) Êää input Áº©ÊîæÂà∞ sizeÔºåÊàñËÄÖÂêÑÁª¥Â∫¶Áº©Êîæ factor ÂÄç„ÄÇ ÊèíÂÄºÁÆóÊ≥ï‰∏∫mode„ÄÇÂèØ‰ª•Â§ÑÁêÜ3D,4D,5D input„ÄÇ4D inputÁöÑÁª¥Â∫¶Ôºö(B, chnls, H, W) Docs\n1 2 3 4 5 6 7 8 import torch depth_ref = np.load(\u0026#34;mvsnet_depth.npy\u0026#34;).reshape(480,640) depth_data = torch.tensor(depth_ref) print(depth_data.shape) rsz = torch.nn.functional.interpolate(depth_data[None,None], size=(60, 80), mode=\u0026#34;bicubic\u0026#34;, align_corners=False,).squeeze() print(rsz.shape) # (480,640) plt.imshow(rsz) plt.savefig(\u0026#34;PT_F_intrpl_exmpl.jpg\u0026#34;,bbox_inches=\u0026#39;tight\u0026#39;, dpi =100) scipy zoom (2024-05-06)\n1 2 3 4 from scipy.ndimage import zoom scale_f_depth = 1/8 depth_rsz = zoom(depth_ref, (scale_f_depth, scale_f_depth), order=3) # (8,10) plt.imshow(depth_rsz) If mode=\u0026quot;constant\u0026quot;, the bottom row is 0 (cval).\n(2023-10-10)\nnn.Upsample This \u0026ldquo;layer\u0026rdquo; has no learnable parameters:\n1 2 3 4 upsampler = torch.nn.Upsample(size=(3,4), mode=\u0026#34;bilinear\u0026#34;) a = torch.arange(4).view(1,1, 2,2).float() dict(upsampler.named_parameters()).items() # out: dict_items([]) So it may be equal to torch.nn.functional.interpolate, which can be used as a layer in a model: Which function is better for upsampling: upsampling or interpolate?\ntorch.nn.functional layers require passing the learnable parameters from outside, since they don\u0026rsquo;t contain nn.Parameter like nn.Module does. Are torch.nn.Functional layers learnable?\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.weight = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) # Corresponding to a nn.module: self.linear = nn.Linear(in_features, out_features) def forward(self, input): return F.linear(input, weight, bias) # return self.linear(input) (2023-10-22)\nDownsample Conv layer can perform evenly downsampling, referring to Context Cluster Reducing points by fusing neighbors covered by kernels:\n1 2 proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=padding) Pooling can also be used for placing centers, referring to Context Cluster\n1 centers_proposal = nn.AdaptiveAvgPool2d((proposal_w, proposal_h)) F.interpolate\nTrilinear Interp (2024-02-28)\nPerform 2-point interpolation in 3 directions sequentially:\nEach point has 3 weights: u, v, w (or (1-u), (1-v), (1-w))\n","date":"2022-06-23T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_sample_interpolate/","title":"memo: PyTorch | Sample \u0026 Interpolation"},{"content":"ÊúÄÊµÖÊòæÊòìÊáÇÁöÑ KMP ÁÆóÊ≥ïËÆ≤Ëß£-Â•á‰πêÁºñÁ®ãÂ≠¶Èô¢-bilibili\nKMPÁöÑnextÊï∞ÁªÑ‰∏≠‰øùÂ≠ò‰∫Ü: ‰∏ª‰∏≤‰∏≠ÊúÄÈïøÁöÑÂ∑≤ÂåπÈÖçÁöÑÂ≠óÁ¨¶„ÄÇÁ∫øÊÄßÊó∂Èó¥Â§çÊùÇÂ∫¶Ôºö‰∏ª‰∏≤ÊåáÈíà‰∏çÂõûÈÄÄÔºåÂ≠ê‰∏≤ÊåáÈíàÂõûÈÄÄÂà∞Â∑≤ÁªèÂåπÈÖçÁöÑÈÉ®ÂàÜÁöÑÂêéÈù¢ÔºàË∑≥ËøáÂ∑≤ÂåπÈÖçÁöÑÈÉ®ÂàÜ)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def kmp_search(string, patt): # string‰∏ª‰∏≤ÔºåpattÂ≠ê‰∏≤ next = build_next(patt): i = 0 # ‰∏ª‰∏≤‰∏≠ÁöÑÊåáÈíà j = 0 # Â≠ê‰∏≤‰∏≠ÁöÑÊåáÈíà while i \u0026lt; len(string): if string[i] == patt[j]: # Â≠óÁ¨¶ÂåπÈÖçÔºåÊåáÈíàÂêéÁßª i += 1 j += 1 elif j \u0026gt; 0: # Â≠óÁ¨¶Â§±ÈÖçÔºåÊ†πÊçÆ next Êï∞ÁªÑË∑≥ËøáÂ≠ê‰∏≤ÂâçÈù¢ÁöÑ‰∏Ä‰∫õÂ≠óÁ¨¶ j = next[j - 1] else: # Â≠ê‰∏≤Á¨¨‰∏Ä‰∏™Â≠óÁ¨¶Â∞±Â§±ÈÖç i += 1 if j == len(patt): # ÂåπÈÖçÊàêÂäü return i - j next Êï∞ÁªÑÁªüËÆ°‰∫ÜÂêÑÂ≠óÁ¨¶ÔºàÂåÖÊã¨ÂÆÉÂú®ÂÜÖÔºâ‰πãÂâçÂ∑≤ÈáçÂ§çÈÉ®ÂàÜÁöÑÈïøÂ∫¶„ÄÇÊûÑÈÄ†Êó∂Êúâ‰∏§‰∏™ÊåáÈíàÔºåÂâçÊñπÊåáÈíàÊåáÂêëÂâçÁºÄÈïøÂ∫¶ÁöÑÂêéÈù¢‰∏Ä‰∏™Â≠óÔºàÂâçÁºÄÈïøÂ∫¶Â∞±ÊòØËØ•ÊåáÈíà‰πãÂâçÁöÑ(‰∏çÂåÖÊã¨ÊåáÂêëÁöÑ)Â≠óÁ¨¶‰∏™Êï∞ÔºâÔºõÂêéÊñπÁöÑÊåáÈíàÊåáÂêëÂΩìÂâçÂ≠óÁ¨¶ÔºàÂêéÁºÄÔºâ„ÄÇÂêéÊñπÊåáÈíà‰∏ÄÁõ¥ÂêéÁßªÔºåËÄåÂâçÊñπÊåáÈíàË¶ÅÂõûÈÄÄÊü•ÁúãÂêéÁºÄÊòØÂê¶Â≠òÂú®‰∏éÂâçÁºÄ‰∏≠„ÄÇ Ëã•‰∏§ÊåáÈíàÊåáÂêëÁöÑÂ≠óÁ¨¶Áõ∏ÂêåÔºåÂàôÂêéÊñπÊåáÈíàÂØπÂ∫îÁöÑnextÊï∞ÁªÑÂÖÉÁ¥†‰∏∫: ÂâçÁºÄÈïøÂ∫¶Êï∞ÂÄº+1ÔºåÁÑ∂Âêé‰∏§‰∏™ÊåáÈíàÈÉΩÂêéÁßª‰∏Ä‰ΩçÔºõ Â¶ÇÊûúÂÆÉ‰ª¨ÊåáÂêëÁöÑÂ≠óÁ¨¶‰∏çÁõ∏ÂêåÔºåÂ∞±ÁúãÁúãÂêéÁºÄÊòØÂê¶ÊòØÂâçÁºÄÁöÑ‰∏Ä‰∏™Â≠êÈõÜÔºàÂâçÁºÄ‰∏≠ÊòØÂê¶ÂåÖÂê´‰ª•ÂêéÁºÄ‰∏∫ÁªìÂ∞æÁöÑÁü≠‰∏≤ÔºåÊâÄ‰ª•Â∞±Ë¶ÅÂõûÂà∞ÂÆÉÂâçÈù¢ÁöÑÂè¶‰∏ÄÊ¨°ÈáçÂ§çÁúãÊòØÂê¶‰∏éÂêéÁºÄÁõ∏Êé•ÔºâÔºåÂàôÂêéÊñπÊåáÈíà‰∏çÂä®ÔºåÂâçÊñπÊåáÈíàÂõûÈÄÄÂà∞ÂÆÉÂ∑¶‰æßÂ≠óÁ¨¶Á¨¨‰∏ÄÊ¨°Âá∫Áé∞Êó∂ÁöÑ‰∏ã‰∏Ä‰∏™Â≠óÁ¨¶‰∏äÔºà‰πüÂ∞±ÊòØÂ∑¶‰æßÂ≠óÁ¨¶ÂØπÂ∫îÁöÑÂâçÁºÄÈïøÂ∫¶ÁöÑ‰∏ã‰∏Ä‰∏™Â≠óÁ¨¶‰∏äÔºå‰πüÂ∞±ÊòØË∑≥ËøáÊØîËæÉÂ∑¶‰æßÂ≠óÁ¨¶ÂØπÂ∫îÁöÑÂâçÁºÄÈïøÂ∫¶‰∏™Â≠óÁ¨¶ÔºâÔºåÂõ†‰∏∫ÂÜçÂæÄÂâçÂ∞±ÊòØ‰∏éÊ≠§Â∑¶‰æßÂ≠óÁ¨¶ÁöÑÂâçÁºÄÈáçÂ§çÁöÑÈÉ®ÂàÜÔºåÊØîËæÉËøô‰∏§‰∏™ÊåáÈíàÊåáÂêëÁöÑÂ≠óÁ¨¶ÔºåÁõ∏ÂêåÁöÑËØùÔºå‰πüÂ∞±ÊòØËÉΩËøû‰∏äËøô‰∏™ÂêéÁºÄÔºåÂàôÂêéÊñπÊåáÈíàÊåáÂêëÁöÑnextÊï∞ÁªÑÂÖÉÁ¥†Â∞±ÊòØÂâçÊñπÊåáÈíàÁöÑÂâçÁºÄÈïøÂ∫¶+1ÔºåÁÑ∂Âêé‰∏§ÊåáÈíàÂâçÁßªÔºõ‰∏çÂêåÁöÑËØùÔºåÂâçÊñπÊåáÈíàÂÜçÂõûÈÄÄÔºåÂÜç‰∏éÂêéÁºÄÊØîËæÉÔºåÁßªÂä®Âà∞Á¨¨‰∏Ä‰∏™Â≠óÁ¨¶(ÂâçÁºÄÈïøÂ∫¶=0)ËøòÊòØ‰∏éÂêéÁºÄ‰∏çÂêåÁöÑËØùÂ∞±ÁΩÆ0ÔºåÂêéÊñπÊåáÈíàÂâçÁßªÔºåÂâçÊñπÊåáÈíàÂÅúÂú®Á¨¨‰∏ÄÂ≠óÁ¨¶‰∏ä„ÄÇ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def build_next(patt): \u0026#34;\u0026#34;\u0026#34; ÊûÑÈÄ†nextÊï∞ÁªÑ \u0026#34;\u0026#34;\u0026#34; next = [0] # nextÊï∞ÁªÑ ÂàùÂÄºÂÖÉÁ¥†‰∏Ä‰∏™0 prefix_len = 0 # ÂΩìÂâçÂâçÁºÄÈïøÂ∫¶ i = 1 # ÂêéÊñπÊåáÈíà while i \u0026lt; len(patt): # ‰∏ÄÊ¨°ÈÅçÂéÜ if patt[prefix_len] == patt[i]: # ÂâçÊñπÊåáÈíàÂ≠óÁ¨¶ = ÂêéÊñπÊåáÈíàÊåáÂêëÂ≠óÁ¨¶ prefix_len += 1 # ÂâçÁºÄÈïøÂ∫¶+1 next.append(prefix_len) i += 1 # ÂêéÊñπÊåáÈíàÂêéÁßª else: if prefix_len == 0: # ÂâçÊñπÊåáÈíàÂõûÂà∞‰∫ÜÁ¨¨‰∏Ä‰∏™Â≠óÁ¨¶ next.append(0) # next[i]=0 i += 1 else: prefix_len = next[prefix_len - 1] # ÂâçÊñπÊåáÈíàÂõûÈÄÄÂà∞Â∑¶‰æßÂ≠óÁ¨¶ÁöÑÂâçÁºÄÈïøÂ∫¶ÁöÑ‰∏ã‰∏Ä‰∏™Â≠ó‰∏äÔºå‰πüÂ∞±ÊòØÂâçÁºÄÈïøÂ∫¶ÂèòÊàêÂ∑¶‰æßÂ≠óÁ¨¶ÁöÑÂâçÁºÄÈïøÂ∫¶ return next ","date":"2022-06-19T22:00:00Z","permalink":"https://zichen34.github.io/writenotes/algo/kmp/","title":"KMP algorithm"},{"content":"Áõ∏Êú∫ÊóãËΩ¨ (2022-05-22)\nÂùêÊ†áÁ≥ªÊóãËΩ¨ÊòØ point ÊóãËΩ¨ÁöÑÈÄÜÔºàÁÇπÈÄÜÊó∂ÈíàÊóãËΩ¨Œ∏ Á≠âÊïà‰∫éÂùêÊ†áÁ≥ªÈ°∫Êó∂ÈíàÊóãËΩ¨Œ∏Ôºâ\nÁÇπPÂú®ÂàùÂßãÂùêÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†á‰∏∫ P‚ÇíÔºåÂú®ÁõÆÊ†áÂùêÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†á‰∏∫ P‚ÇúÔºå ‰ªé P‚Çí Âà∞ P‚Çú ‰∏≠Èó¥ÊòØ target ÂùêÊ†áÁ≥ªÂú® origin ÂùêÊ†áÁ≥ª‰∏ãÁöÑË°®Á§∫ÔºàÊñπÂêëÂêëÈáèÔºâ[ùê´‚Çì ùê´·µß]·µÄ:\n$$ \\begin{bmatrix} x‚Çú \\\\ y‚Çú \\end{bmatrix} = \\begin{matrix} r‚Çì: \\\\ r_y: \\end{matrix} \\begin{bmatrix} a‚ÇÅ \u0026amp; b‚ÇÅ \\\\ a‚ÇÇ \u0026amp; b‚ÇÇ \\end{bmatrix} \\begin{bmatrix} x‚Çí \\\\ y‚Çí \\end{bmatrix} $$\nÊ®™ÁùÄÁúãÔºöË°åÂêëÈáè (a‚ÇÅ,b‚ÇÅ) ÊòØ target Á≥ªÁöÑ x ËΩ¥Âú® origin Á≥ª‰∏ãÁöÑÊñπÂêëÂêëÈáèÔºå (a‚ÇÇ,b‚ÇÇ) ÊòØ target Á≥ªÁöÑ y ËΩ¥Âú® origin Á≥ª‰∏ãÁöÑÊñπÂêëÂêëÈáè„ÄÇ ‰æãÂ¶ÇÔºå‰∏ãÂõæ‰∏≠ origin Á≥ªÊòØ worldÔºåtarget Á≥ªÊòØ camera:\nOriginal ÂùêÊ†á$[^x_y]$ÂÅöÁ∫øÊÄßÁªÑÂêàÔºåÂèòÊç¢Âà∞‰∫Ü Target ÂùêÊ†áÁ≥ª$[^{x_c}_{y_c}]$„ÄÇ Á´ñÁùÄÁúãÔºöÂàóÂêëÈáè (a‚ÇÅ,a‚ÇÇ) ÊòØ origin Á≥ªÁöÑ x ËΩ¥Âú® target Á≥ª‰∏ãÁöÑÊñπÂêëÂêëÈáèÔºå (b‚ÇÅ,b‚ÇÇ) ÊòØ origin Á≥ªÁöÑ y ËΩ¥Âú® target Á≥ª‰∏ãÁöÑÊñπÂêëÂêëÈáè„ÄÇ\nÁªº‰∏äÔºåÊóãËΩ¨Áü©Èòµ R (in w2c) Ê®™ÁùÄÁúãÂ∞±ÊòØ camera (target) Á≥ªÂú® world Á≥ª‰∏ãÁöÑË°®Á§∫ÔºåÁ´ñÁùÄÁúãÂ∞±ÊòØ world (original) Á≥ªÂú® camera Á≥ª‰∏ãÁöÑË°®Á§∫„ÄÇ\n(2023-11-05) ÊâÄ‰ª•ÈÄöËøáËΩ¨ÁΩÆÂ∞±ÂèØ‰ª•Êää R in w2c ÂàáÊç¢Êàê R in c2w„ÄÇ ‰ΩÜÊòØË¶ÅÊää w2c ÂèòÊàê c2wÔºåÈúÄË¶ÅÂØπ [R|t] Êï¥‰ΩìÊ±ÇÈÄÜ„ÄÇ Ê¨ßÂá†ÈáåÂæóÂèòÊç¢ [R T]ÔºöÊóãËΩ¨Áü©Èòµ R Âä†Âπ≥ÁßªÂêëÈáè TÔºåÊääÁÇπÂú® origin Á≥ªÁöÑÂùêÊ†áÂèòÊàêÂú® target Á≥ª‰∏ãÁöÑÂùêÊ†áÔºåÊàñËÄÖËØ¥Êää origin Á≥ªÂèòÊç¢Êàê target Á≥ª„ÄÇ\nFor example, w2c as below.\nÂÖàÊóãËΩ¨ÂêéÂπ≥ÁßªÔºöÁÇπÂú® origin (world) Á≥ª‰∏ãÁöÑÂùêÊ†áÂÖàÁªèËøá target (camera) Á≥ªÂú® origin Á≥ª‰∏ãÁöÑÊñπÂêëÂêëÈáè R ÁöÑÁ∫øÊÄßÁªÑÂêàÔºåÂç≥ ÊäïÂΩ±ÔºàÂÅöÂÜÖÁßØÔºâÂà∞‰∫Ü‰∏Ä‰∏™‰∏é target Á≥ªÂùêÊ†áËΩ¥ÈÉΩÂπ≥Ë°åÁöÑÊñ∞ÂùêÊ†áÁ≥ªÔºàËôöÁ∫øÔºâ‰∏ãÔºå ÂÜçÂä†‰∏ä‰∏ÄÊÆµÂπ≥ÁßªÂêëÈáè TÔºå‰ªéËÄå‰ΩøÊñ∞ÂùêÊ†áÊòØ‰ª• target Á≥ªÁöÑÂéüÁÇπÂºÄÂßãÔºåÊâÄ‰ª• T Â∞±ÊòØ origin Á≥ªÁöÑÂéüÁÇπÂú® target Á≥ªËßÜËßí‰∏ãÁöÑÂùêÊ†á„ÄÇ\nThis process is expressed by [R|t]. Conversely, [R|t] denotes rotation first then translation.\n(2024-01-09) ùê≠ is coordinate measured in the target space, because ùê≠ is simply added onto the target coordiantes without \u0026ldquo;recombination\u0026rdquo; of the elementary vectors in a basis. Therefore, ùê≠ is the original center seen from the target space.\nÊàñËÄÖÂÖàÂπ≥ÁßªÂêéÊóãËΩ¨Ôºöorigin (world) Á≥ª‰∏ãÁöÑÂùêÊ†áÂÖàÂáèÂéª target (camera) Á≥ªÂéüÁÇπÂú® origin Á≥ª‰∏ãÁöÑÂùêÊ†á CÔºå ÂèòÂà∞‰∫Ü‰∏Ä‰∏™Êñ∞ÂùêÊ†áÁ≥ª‰∏ãÔºàÂÖ∂ÂéüÁÇπ‰∏é target Á≥ªÂéüÁÇπÈáçÂêàÔºâÔºå ÂÜçÊóãËΩ¨Âà∞‰∏é target Á≥ªÂêÑËΩ¥ÈáçÂêàÔºõÊâÄ‰ª• C Â∞±ÊòØ camera ÂÖâÂøÉÂú® world Á≥ª‰∏ãÁöÑÂùêÊ†á„ÄÇ\nIn this case, R and T can\u0026rsquo;t be written as an augmented matrix, but separate matrices. If the given point\u0026rsquo;s coords are world coords, then apply [R|t]. While if point is already in camera space, only apply [R]. ÂèØÂÄüÂä© vanishing points (ÂæÖÂÆöÁ≥ªÊï∞)Êù•Ê±ÇÊóãËΩ¨Áü©ÈòµÔºàThis point is at infinite but finite in image.Ôºâ Camera Projection Matrix-UofMinnesota\nÈÄèËßÜÊäïÂΩ± (2022-05-08)\nÈÄèËßÜÊäïÂΩ±ÔºàÂÜÖÂèÇÁü©ÈòµKÔºâÊää camera space ‰∏ãÁöÑÂùêÊ†áÊäïÂΩ±Âà∞ÁÑ¶Âπ≥Èù¢‰∏äÔºåX Èô§‰ª• Z ‰πò‰ª• fÔºàÂç≥‰ª• f/z ‰∏∫Á≥ªÊï∞ÂØπ x,y ÂÅö‰∏Ä‰∏™Áº©ÊîæÔºâ„ÄÇ Â¶ÇÊûúÁÑ¶Ë∑ù(ÁÑ¶Âπ≥Èù¢Ë∑ùÁ¶ª) f ÊòØÂ∏∏Êï∞ÔºåÈÇ£Â∞±Áõ¥Êé•ÊòØ‰∏é z ÊàêÂèçÊØî„ÄÇ\n(2024-01-31) ÊâÄ‰ª• \u0026ldquo;z\u0026rdquo; ‰ª£Ë°®ÁöÑÊòØ \u0026ldquo;Zoom\u0026rdquo; Áº©ÊîæÔºöËøëÂ§ßËøúÂ∞è. Ray Marching for Dummies! - Ytb - The Art of Code ÔºàËøôÈáåÂùêÊ†áÈÉΩÊòØÁªùÂØπÂÄºÔºå‰∏çËÄÉËôëÂùêÊ†áÁ≥ªÁöÑÈÄâÂèñÔºâ\nB a c k p l a y n e f - / 0 p - i - n Z - h - o Y l e F r o n t Âõ†‰∏∫ÊôØÁâ©ÊòØ ÂÄíÁΩÆ ÁöÑÔºåÊâÄ‰ª•ÂÉèÁ¥†ÂùêÊ†áÁ≥ªÁöÑ y ËΩ¥ÊòØÂêë‰∏ãÁöÑÔºü Ëã•ÈááÁî®ÈΩêÊ¨°ÂùêÊ†áÔºàÁî®Áü©ÈòµË°®ËææÈô§Ê≥ïÔºâ, ÂØπ [X,Y,Z] ÂÅöÈÄèËßÜÊäïÂΩ±ÂæóÂà∞ÁöÑÊòØ [fX, fY, Z]ÔºåÂàô [u, v, 1] = [fX/Z, fY/Z, 1]„ÄÇ ÂÜç‰ª•ÂÉèÁ¥†Â∞∫ÂØ∏ dx,dy Áº©ÊîæÂπ∂Âä†‰∏ä(+)ÂÖâÂøÉÂùêÊ†á cx,cyÔºåÊääÂéüÁÇπ‰ªéÂÖâÂøÉÁßªÂà∞Â∑¶‰∏äËßíÔºàÂÉèÁ¥†Á≥ªÁöÑvËΩ¥ÊòØÊúù‰∏ãÁöÑÔºåÊâÄ‰ª•ËøòÈúÄË¶ÅÂä†Ë¥üÂè∑ÔºüÔºâÔºåÂ∞±ÂèòÂà∞‰∫ÜÂÉèÁ¥†ÂùêÊ†áÁ≥ª‰∏ãÔºö\n$$ \\begin{bmatrix} \\frac{f}{dx} \u0026amp; 0 \u0026amp; c‚Çì \\\\ 0 \u0026amp; \\frac{f}{dy} \u0026amp; c_y \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} = \\begin{bmatrix} f‚ÇìX + c‚ÇìZ \\\\ f_yY + c_yZ \\\\ Z \\end{bmatrix} = \\begin{bmatrix} \\frac{f‚ÇìX}{Z}+c‚Çì \\\\ \\frac{f_yY}{Z}+c_y \\\\ 1 \\end{bmatrix} $$\nÊäïÂΩ±ÂèòÊç¢ ÊäïÂΩ±ÂèòÊç¢ GL_PROJECTION ÊòØÊää Áõ∏Êú∫Á©∫Èó¥ ‰∏ãÁöÑÁÇπ (x‚Çë,y‚Çë,z‚Çë) ÂèòÊç¢Âà∞ Â±èÂπïÁ©∫Èó¥ ÁöÑ clip ÂùêÊ†á (xc,yc,zc,wc)Ôºö\nÂÖàÈÄèËßÜÊäïÂΩ± (1/ZÁº©Êîæ) Âà∞Áõ∏Êú∫ÁöÑ near plane (ÁÑ¶Ë∑ù‰∏∫-n)„ÄÇ\nÈÄèËßÜÈô§Ê≥ïÈúÄË¶ÅÈô§‰ª• -z‚ÇëÔºåÊâÄ‰ª•ÈΩêÊ¨°ÂùêÊ†áÁöÑ w‚Çë = -z‚Çë ÂÜçÊää x,y ÁöÑÂèñÂÄºËåÉÂõ¥Ôºö[top,bottom],[left,right] Á∫øÊÄßÂèòÊç¢Âà∞[-1,1]„ÄÇ\n‰ª§Áõ∏Êú∫Á©∫Èó¥‰∏ãÁöÑ [near, far] ÁöÑ NDC ÂùêÊ†áÁ≠â‰∫é [-1, 1]\nÂõ†‰∏∫ÊòØ‰ªé‰∏âÁª¥Âà∞‰∏âÁª¥ÔºåË¶ÅÊÉ≥Áî®Áü©ÈòµË°®ËææÈÄèËßÜÁº©ÊîæÂíåÂπ≥ÁßªÔºåÂ∞±ÈúÄË¶Å‰ΩøÁî®ÈΩêÊ¨°ÂùêÊ†á„ÄÇ\nclip ÂùêÊ†áÊòØ NDC ÁöÑÈΩêÊ¨°ÂΩ¢ÂºèÔºåÊâÄ‰ª•Ëøô‰∏™Áü©ÈòµÔºàProjection MatrixÔºâ ÂÆåÊàê‰∫Ü frustum culling Âíå NDC ÂèòÊç¢„ÄÇ\n$$ \\begin{bmatrix} \\frac{2}{r-l}‚ãÖn \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2}{t-b}‚ãÖn \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -\\frac{f+n}{f-n} \u0026amp; -\\frac{2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} x‚Çë \\\\ y‚Çë \\\\ z‚Çë \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{r-l}‚ãÖnx‚Çë + \\frac{r+l}{r-l}‚ãÖz‚Çë \\\\ \\frac{2}{t-b}‚ãÖny‚Çë + \\frac{t+b}{t-b}‚ãÖz‚Çë \\\\ -\\frac{f+n}{f-n}‚ãÖz‚Çë -\\frac{2fn}{f-n} \\\\ -z‚Çë \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{r-l}‚ãÖn\\frac{x‚Çë}{-z‚Çë} + \\frac{r+l}{r-l} \\\\ \\frac{2}{t-b}‚ãÖn\\frac{y‚Çë}{-z‚Çë} + \\frac{t+b}{t-b} \\\\ \\frac{f+n}{f-n} +\\frac{2fn}{(f-n)z‚Çë} \\ 1 \\end{bmatrix} $$\nviewing\u0026amp;project-utexas\nNDC Á©∫Èó¥ Â±èÂπïÊòæÁ§∫ÁöÑ‰∏ñÁïåÊ∑±Â∫¶ËåÉÂõ¥ÊòØ near,far ‰∏§‰∏™ÁÑ¶Âπ≥Èù¢‰πãÈó¥ÁöÑÂå∫Âüü„ÄÇ openGL ‰∏≠‰∏§ÁÑ¶Âπ≥Èù¢Èó¥ÁöÑ z interval Ë¢´Êò†Â∞ÑÂà∞ [-1,1]ÔºåÂç≥ Normalized Device CoordinatesÔºå ÂèòÊç¢Âà∞ NDC ÂêéÂ∞±ÂèØ‰ª•Ê†πÊçÆz_ndcÊääË∂ÖÂá∫ËåÉÂõ¥Â§ñÁöÑÁâ©‰ΩìË£ÅÂâ™Êéâ„ÄÇ\nÂ±èÂπïÁ™óÂè£ÊòæÁ§∫ÁöÑÊòØËøëÁÑ¶Âπ≥Èù¢„ÄÇÂèØ‰ª•Áî® fov ÊéßÂà∂ËøëÁÑ¶Âπ≥Èù¢ÁöÑËæπÈïøÔºåÂèØ‰ª•ÊääÂ±èÂπïÁöÑ t,b,l,r Êò†Â∞ÑÂà∞ËøëÁÑ¶Âπ≥Èù¢ÁöÑËæπÈïø [-1,1]ÔºåËøôÊ†∑Â±èÂπïÊòæÁ§∫ÁöÑÁ©∫Èó¥Â∞±ÊòØ‰∏Ä‰∏™Á´ãÊñπ‰Ωì„ÄÇ Êé¢Áßò‰∏âÁª¥ÈÄèËßÜÊäïÂΩ±-ÈΩêÊ¨°ÂùêÊ†áÁöÑÂ¶ôÁî® -Â•á‰πêbili\n(2023-11-30) NDC = Perspective projection to the near plane with depths kept + Scaling.\nÁõ∏Êú∫ÂèòÊç¢Áü©Èòµ Êé®ÂØºÁõ∏Êú∫ÂèòÊç¢Áü©Èòµ-csdn-ÊΩòÂÆè\n(‰∏çÂêåÂü∫Â∫ïÈó¥ÁöÑ) ÂùêÊ†áËΩ¨Êç¢ÂÖ¨Âºè: ùêØ=ùêê ùêØ\u0026rsquo;=ùêë ùêØ\u0026rsquo;\u0026rsquo; ‚áí ùêØ\u0026rsquo;\u0026rsquo;= ùêë‚Åª¬πùêê ùêØ\u0026rsquo;ÔºåÂÖ∂‰∏≠ùêê,ùêë ÊòØ‰∏çÂêåÁöÑÊ≠£‰∫§Áü©ÈòµÔºå‰ª£Ë°®ÂùêÊ†áÁ≥ªÔºåÂõ†‰∏∫Ê≠£‰∫§Áü©ÈòµÁöÑÈÄÜÁ≠â‰∫éËΩ¨ÁΩÆÔºåÊâÄ‰ª•ÂèØ‰ª•ÂÜô‰∏∫ÔºöùêØ\u0026rsquo;\u0026rsquo;= ùêë·µÄùêê ùêØ'\nUVNÁõ∏Êú∫Ê®°ÂûãÁî®ÂêëÈáèÂÆö‰πâÁõ∏Êú∫ÊúùÂêëÔºöN ÊòØÁõ∏Êú∫ËßÇÂØüÊñπÂêëÁöÑÂèçÊñπÂêëÔºåU Áî±ËæÖÂä©ÂêëÈáèup‰∏éNÂèâ‰πòÁ°ÆÂÆöÔºåËæÖÂä©ÂêëÈáèÁî®‰∫éËÆ©Áõ∏Êú∫‰∫ßÁîüÂÅèËΩ¨Ôºà‰∏çÊ≠™Â§¥‰∏ÄËà¨Âèñ(0,1,0)ÔºâÔºõV=N√óUÔºåV ËêΩÂú® up ‰∏é N ÂΩ¢ÊàêÁöÑÂπ≥Èù¢‰∏ä„ÄÇ\n‰æãÂ¶ÇnerfÁöÑÂáΩÊï∞viewmatrix() Áî®‰∫éÊûÑÂª∫Âπ≥ÂùáÁõ∏Êú∫‰ΩçÂßø poses_avg ÁöÑUVNÁõ∏Êú∫ÂùêÊ†áÁ≥ª [X|Y|Z]Ôºà‰∏ñÁïåÁ≥ªÂè™Êúâ‰∏Ä‰∏™ÔºåËÄåÁõ∏Êú∫Á≥ªÊúâÂ§ö‰∏™ÔºåÂèñÂπ≥ÂùáÁõ∏Êú∫Á≥ª‰Ωú‰∏∫\u0026rsquo;Êñ∞‰∏ñÁïåÁ≥ª\u0026rsquo;Ôºâ„ÄÇ\nView transformation: ÊääÁâ©‰ΩìÂùêÊ†á‰ªé‰∏ñÁïåÁ≥ªÂèòÊç¢Âà∞Áõ∏Êú∫Á≥ª‰∏ãÔºå‰πüÂ∞±ÊòØÂÅö‰∏ÄÊ¨°Áõ∏Êú∫ËøêÂä®ÁöÑÈÄÜÂèòÊç¢„ÄÇÂèòÊç¢ËøáÁ®ãÔºöÂàùÂßãÊó∂Áõ∏Êú∫Á≥ª‰∏é‰∏ñÁïåÁ≥ªÈáçÂêàÔºå(Âú®‰∏ñÁïåÁ≥ª‰∏ã)Áõ∏Êú∫ÂÅöÊóãËΩ¨„ÄÅÂÜçÂπ≥ÁßªÊé•ËøëÁâ©‰ΩìÔºåÁÑ∂ÂêéÁõ∏Êú∫‰∏éÁâ©‰Ωì‰∏ÄËµ∑ÂÅöÈÄÜÂπ≥Áßª„ÄÅÈÄÜÊóãËΩ¨ÔºåÁõ∏Êú∫ÂèàÂõûÂà∞ÂàùÂßã‰ΩçÁΩÆÔºåÁâ©‰ΩìÂ∞±ÂèòÂà∞‰∫ÜÁõ∏Êú∫Á≥ª‰∏ã„ÄÇ\nÈÄÜÂπ≥ÁßªÊòìÊ±Ç(ÂèñÂèç)ÔºåÈÄÜÊóãËΩ¨‰∏çÊòìÊ±Ç(Ê±ÇÈÄÜÁöÑÈ°∫Â∫è)Ôºõ‰ΩÜÊòØÂÅöÂÆåÈÄÜÂπ≥ÁßªÂêéÔºåÁõ∏Êú∫Á≥ª‰∏é‰∏ñÁïåÁ≥ªÁöÑÂéüÁÇπÈáçÂêà‰∫ÜÔºåÂè™ÊòØÂü∫Â∫ï‰∏çÂêåÔºåÂà©Áî®ÂùêÊ†áËΩ¨Êç¢ÂÖ¨ÂºèÂ∞±ÂèØ‰ª•Ê±ÇÂá∫Âú®Áõ∏Êú∫Á≥ª‰∏ãÁöÑÂùêÊ†á ùêØ\u0026rsquo;\u0026rsquo;= ùêë·µÄùêê ùêØ\u0026rsquo;ÔºåÂÖ∂‰∏≠ùêëÊòØUVNÁ≥ªÁªüÔºåùêê ÊòØ‰∏ñÁïåÁ≥ª(ÂØπËßíÈòµ)ÔºåùêØ\u0026rsquo;ÊòØÈÄÜÂπ≥ÁßªÂêéÁöÑÂêëÈáèùêì‚Åª¬πùêØÔºåÊïÖÊúÄÁªàÁöÑÂùêÊ†áÂèòÊç¢Áü©Èòµ(w2cÂ§ñÂèÇÁü©ÈòµExtrinsic matrix)Ôºöùêë·µÄùêê ùêì‚Åª¬π\ndoubt: ÊóãËΩ¨Áü©ÈòµÊ±ÇÈÄÜ DDG\n3‰∏™Áü©Èòµ Â§ñÂèÇÁü©ÈòµÊääÁÇπÁöÑ world space ÂùêÊ†á Xw ÂèòÊç¢Âà∞Áõ∏Êú∫Á≥ª‰∏ãÔºöXc=R‚ãÖXw+TÔºõ\nÂÜÖÂèÇÁü©ÈòµÊääÁÇπÁöÑ camera space ÂùêÊ†á Xc ÂèòÊç¢Âà∞ÁÑ¶Âπ≥Èù¢(ÂéüÁÇπÂú®ÂõæÁâá‰∏≠Â§Æ) ÔºàÂä†‰∏äÁº©ÊîæÂõ†Â≠êfx,fyÂíåÂÖâÂøÉÂùêÊ†á(cx,cy)ÂèØ‰ª•ÂèòÊç¢Âà∞ÂÉèÁ¥†ÂùêÊ†áÁ≥ªu,vÔºå ÂéüÁÇπÂú®ÂõæÁâáÂ∑¶‰∏äËßí,vËΩ¥Êúù‰∏ãÔºâ‰∏äÔºöP=K‚ãÖXcÔºõ\nÁõ∏Êú∫ÊäïÂΩ±Áü©Èòµ Camera projection matrixÔºöÊää‰∏ñÁïåÁÇπÁõ¥Êé•ÂèòÊç¢Âà∞ÂõæÂÉèÂπ≥Èù¢‰∏äÔºàÂÜÖÂèÇÁü©ÈòµK‚ÇÉ‚Çì‚ÇÉ ‚àó Â§ñÂèÇÁü©Èòµ[R T]‚ÇÉ‚Çì‚ÇÑ = P‚ÇÉ‚Çì‚ÇÑÔºâ„ÄÇ\nRef:\n11.1 Camera matrix-CMU SLAMÂÖ•Èó®‰πãËßÜËßâÈáåÁ®ãËÆ°(2)ÔºöÁõ∏Êú∫Ê®°ÂûãÔºàÂÜÖÂèÇÊï∞ÔºåÂ§ñÂèÇÊï∞Ôºâ Camera Calibration and 3D Reconstruction-openCV UVN Ê®°Âûã ‰ªéÂ§ñÂèÇÁü©ÈòµExtrinsic matrix‚ÇÑ‚Çì‚ÇÑ ÊèêÂèñÂá∫Áõ∏Êú∫ÁöÑ‰ΩçÁΩÆÂíåÊúùÂêë: (ÊúÄÂêé‰∏ÄË°åÊòØÈΩêÊ¨°ÂùêÊ†áÔºâÊúÄÂêé‰∏ÄÂàóÊòØ‰∏ñÁïåÁ≥ª‰∏≠ÂøÉÂú®Áõ∏Êú∫Á≥ª‰∏≠ÁöÑ‰ΩçÁΩÆÔºå Â∑¶‰∏ä3x3ÊòØÁõ∏Êú∫Âú®‰∏ñÁïåÁ≥ª‰∏ãÊóãËΩ¨ËøêÂä®RÁöÑËΩ¨ÁΩÆÔºàÂàóÂêëÈáèÊòØ‰∏ñÁïåÁ≥ªÔºåË°åÂêëÈáèÊòØÁõ∏Êú∫Á≥ªÔºâ„ÄÇÂ¶ÇÊûúÂÜçÁü•ÈÅìÁõ∏Êú∫ÁöÑËßÇÂØüÊñπÂêëÔºåÂÄüÂä©‰∏Ä‰∏™ËæÖÂä©ÂêëÈáèupÔºåÂ∞±ËÉΩÁ°ÆÂÆöUVNÁ≥ªÁªü„ÄÇ StackExchange\ndoubt: UVN Áõ∏Êú∫Ê®°Âûã Google Search\n(2024-04-02)\nEstablishing w2c from camera position and looking-at vector. Placing a Camera: the LookAt Function - Scratchapixel The \u0026ldquo;forward\u0026rdquo; direction is defined as From - To, because He marked the \u0026ldquo;out\u0026rdquo; of the screen as \u0026ldquo;forward\u0026rdquo;\nAfter determining the \u0026ldquo;forward\u0026rdquo; vector, specify a temporary \u0026ldquo;up\u0026rdquo; vector (usually (0,1,0)), which is not necessary to be perpendicular to the \u0026ldquo;forward\u0026rdquo;, to produce the \u0026ldquo;right\u0026rdquo; vector, accroding to \u0026ldquo;forward\u0026quot;√ó temporary \u0026ldquo;up\u0026rdquo;.\nOnce the \u0026ldquo;right\u0026rdquo; vector is obtained, re-construct the accurate \u0026ldquo;up\u0026rdquo; vector by \u0026ldquo;forward\u0026rdquo; cross \u0026ldquo;right\u0026rdquo;. Note: He define the Backward as \u0026ldquo;forward\u0026rdquo;.\nThe c2w in this post is row-major format. So, he writes each row is a direction.\nz = -1 is the camera-space coordinate of the ray direction vector.\nThe limitation of the looking-at method is the case that the \u0026ldquo;forward\u0026rdquo; direction is aligned with the temporary \u0026ldquo;up\u0026rdquo; (0,1,0), as the cross product of 2 parallel vectors is zero $\\vec{0}$. 10.4: The Cross Product - Mathematics LibreTexts\nA solution is using quaternion.\n(2024-04-04)\nNeRF builds RUB matrix (i.e., averaged c2w)Ôºöaverage up √ó back (z-axis) = right (x-axis) Code Cam Coords System (2024-03-21)\nÁ°ÆÂÆö‰∫ÜÁõ∏Êú∫ÂùêÊ†áÁ≥ª‰∏é‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÁõ∏ÂØπÂÖ≥Á≥ªÔºåÊâçËÉΩÊ≠£Á°ÆÂú∞Êää‰∏Ä‰∏™ 3D ÁÇπÁöÑ‰∏ñÁïåÂùêÊ†áÂèòÊàêÁõ∏Êú∫ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†á„ÄÇ\nÂÅáÂÆö‰∏ñÁïåÂùêÊ†áÁ≥ªÊòØÂè≥ÊâãÂùêÊ†áÁ≥ªÔºåÁõ∏Êú∫ÂùêÊ†áÁ≥ªÂèØËÉΩ‰∏ç‰∏é‰∏ñÁïåÁ≥ªÈáçÂêà„ÄÇ\nÁõ∏Êú∫ÂùêÊ†áÁ≥ªÊúâ 3 ‰∏™ËΩ¥ÔºöÂ∑¶ÔºàÂè≥ÔºâÔºå‰∏äÔºà‰∏ãÔºâÔºåÂâçÔºàÂêéÔºâÔºåÊñπÂêë‰∏çÂêåÂàôÔºöÁÇπÂú®ËØ•ÊñπÂêë‰∏äÁöÑÂùêÊ†áÁõ∏Â∑Æ 1 ‰∏™Ë¥üÂè∑„ÄÇËÄå‰∏î 3 ‰∏™ËΩ¥ÁöÑÊéíÂàóÊ¨°Â∫è‰πü‰∏çÁªü‰∏Ä„ÄÇ\nNeRF‰ª£Á†ÅËß£ËØª-Áõ∏Êú∫ÂèÇÊï∞‰∏éÂùêÊ†áÁ≥ªÂèòÊç¢ - ÈôàÂÜ†Ëã± - Áü•‰πé\nOpen3D\u0026rsquo;s camera coord. sys. is RDF. camera coordinate system of visualization #1347 Á°ÆÂÆö‰∫ÜÁõ∏Êú∫Âú® ‰∏ñÁïå ÂùêÊ†áÁ≥ª‰∏≠ÁöÑÊúùÂêëÂíå‰ΩçÁΩÆ: Up vector, viewing direction and positionÔºå ÊâçÂèØ‰ª•ÊìçÁ∫µÁõ∏Êú∫ (Camera Manipulation)ÔºöChanging roll, yaw, pitch, dollying (Slides - Hong Qin). An interactive example in LearnWebGL„ÄÇ Songho also explain camera manipulation OpenGL Camera.\nÊâÄ‰ª•ÂùêÊ†áÂèòÊç¢ÁöÑÈ°∫Â∫èÊòØÔºö‰∏ñÁïåÁ≥ª‰∏ãÁöÑÂùêÊ†á ‚û° Áõ∏Êú∫Á≥ª‰∏ãÁöÑÂùêÊ†á ‚û° Áõ∏Êú∫ÂÅö 6 DoF ËøêÂä®ÔºàÁ≠â‰ª∑‰∫éÁâ©‰ΩìÁöÑÁõ∏Êú∫Á≥ªÂùêÊ†áÂÅö inverse ËøêÂä®Ôºâ‚û° Âú®Áõ∏Êú∫ËøêÂä®ÂÆåÊàêÂêéÔºåÂÜçÊää 3D ÁÇπÂú®Áõ∏Êú∫Á≥ª‰∏ãÁöÑÂùêÊ†áÊäïÂΩ±Âà∞Áõ∏Êú∫Âπ≥Èù¢„ÄÇ\nÂØπ‰∫éÁü©Èòµ w2cÔºåÂâç 3 ÂàóÁöÑÊØè‰∏ÄÂàóÊòØ‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÊØè‰∏™ axis Âú®Áõ∏Êú∫ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†á„ÄÇ\n(2024-03-25)\nw2c ÊòØÊää‰∏Ä‰∏™ ÁÇπ ÁöÑ‰∏ñÁïåÂùêÊ†áËΩ¨Êç¢ÊàêÁõ∏Êú∫ÂùêÊ†á„ÄÇ‰∏Ä‰∏™ 3D ÁÇπÁöÑÂùêÊ†áÁ≠â‰∫é ‰∏Ä‰∏™Êï∞ÁªÑ‰πò‰ª•ÂùêÊ†áÁ≥ª„ÄÇ ÊâÄ‰ª• w2c Á≠â‰∫é rotation matrix ‰πò‰ª•‰∏ñÁïåÂùêÊ†áÁ≥ªÔºö\n$$ \\begin{bmatrix} c_{x_1} \u0026amp; c_{y_1} \u0026amp; c_{z_1} \\\\ c_{x_2} \u0026amp; c_{y_2} \u0026amp; c_{z_2} \\\\ c_{x_3} \u0026amp; c_{y_3} \u0026amp; c_{z_3} \\end{bmatrix} = \\begin{bmatrix} r‚ÇÅ‚ÇÅ \u0026amp; r‚ÇÅ‚ÇÇ \u0026amp; r‚ÇÅ‚ÇÉ \\\\ r‚ÇÇ‚ÇÅ \u0026amp; r‚ÇÇ‚ÇÇ \u0026amp; r‚ÇÇ‚ÇÉ \\\\ r‚ÇÉ‚ÇÅ \u0026amp; r‚ÇÉ‚ÇÇ \u0026amp; r‚ÇÉ‚ÇÉ \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nÂõ†‰∏∫ rotation matrix ÁöÑÁâπÊÄßÔºåÂÆÉÁöÑ‰∏ÄË°åÂ∞±ÊòØÁõÆÊ†áÂùêÊ†áÁ≥ªÁöÑ‰∏Ä‰∏™ËΩ¥Âú® Ê∫êÁ≥ª‰∏ãÁöÑÂùêÊ†á„ÄÇÂÆÉÁöÑ‰∏ÄÂàóÂ∞±ÊòØÊ∫êÁ≥ªÁöÑ‰∏Ä‰∏™ËΩ¥Âú®ÁõÆÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†á„ÄÇ\nEach row in rotation matrix for w2c is an axis of target camera coordinate system. This can be verified by the example below:\nPlotting script: Test_rotation_matrix.ipynb\nÂêåÊ†∑ÔºåÂØπ‰∫éÁü©Èòµ c2wÔºåÂâç 3 ÂàóÁöÑÊØè ‰∏ÄÂàó ÊòØÁõ∏Êú∫ÂùêÊ†áÁ≥ªÁöÑÊØè‰∏™ axis Âú®‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†á„ÄÇ ÊâÄ‰ª•Ë¶Å Ë∞ÉÊç¢ Áõ∏Êú∫ÂùêÊ†áÁ≥ªÂú®‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÁöÑÊúùÂêëÔºåÂØπ c2w ÁöÑ rot ÁöÑ Êüê‰∏ÄÂàó ‰πò‰∏ä‰∏Ä‰∏™Ë¥üÂè∑Âç≥ÂèØ„ÄÇ\nÁõ∏Êú∫ÂùêÊ†áÁ≥ªÁöÑÂÆö‰πâÂΩ±ÂìçÁöÑÊòØ 3D ÁÇπÂú®Áõ∏Êú∫ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂùêÊ†áÔºåÊîπÂèòÁõ∏Êú∫ÊúùÂêëÔºåÊúÄÁªà‰ΩìÁé∞Âú® 3D ÁÇπÂú®Áõ∏Êú∫Á≥ª‰∏ãÁöÑÂùêÊ†áÁöÑÊ≠£Ë¥ü„ÄÇ ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂØπ‰∫é w2c ‰∏≠ÁöÑÊóãËΩ¨Áü©ÈòµÔºåË¶Å Ë∞ÉÊç¢ Áõ∏Êú∫Á≥ªÁöÑ‰∏Ä‰∏™ËΩ¥ÁöÑÊñπÂêëÔºåÂ∫îÂØπ rotation matrix ‰∏≠ÂØπÂ∫îÁöÑ ‰∏ÄË°å Ê∑ªÂä†Ë¥üÂè∑„ÄÇ\nÂõ†‰∏∫ NeRF ‰ΩøÁî®ÁöÑÊòØ c2wÔºåÂÆÉÁöÑÊóãËΩ¨Áü©ÈòµÁöÑ ÊØè‰∏ÄÂàó ÊòØ‰∏Ä‰∏™Áõ∏Êú∫Á≥ªÁöÑËΩ¥ÔºåËÄå‰∏îÊ¨°Â∫èÊòØ DRB ÊâÄ‰ª•‰ª£Á†Å‰∏≠‰∫§Êç¢Á¨¨ 0 Âàó(Down) ÂíåÁ¨¨ 1 Âàó (Right)ÔºåÁÑ∂ÂêéÂØπ y ÊñπÂêë‰πò‰∏ä -1 ÂèòÊàê Up„ÄÇÊúÄÁªàÂèòÂà∞‰∫Ü OpenGL ÁöÑ RUB„ÄÇ Code\n(2024-03-24)\n\u0026ldquo;The view matrix transforms all world coordinates to camera-space coordinates.\u0026rdquo; \u0026ndash; LearnOpenGL - Camera\nTherefore, the view matrix (extrinsics) transforms the X,Y,Z axes of the world coordinates system to X,Y,Z axes of the camera coordinates system.\nLet the world X-Y-Z axes be the 3 unit column vectors, as shown in the below right matrix, they\u0026rsquo;re transformed to camera axes by a w2c:\n$$ \\begin{bmatrix} r‚ÇÅ‚ÇÅ \u0026amp; r‚ÇÅ‚ÇÇ \u0026amp; r‚ÇÅ‚ÇÉ \u0026amp; t‚ÇÅ \\\\ r‚ÇÇ‚ÇÅ \u0026amp; r‚ÇÇ‚ÇÇ \u0026amp; r‚ÇÇ‚ÇÉ \u0026amp; t‚ÇÇ \\\\ r‚ÇÉ‚ÇÅ \u0026amp; r‚ÇÉ‚ÇÇ \u0026amp; r‚ÇÉ‚ÇÉ \u0026amp; t‚ÇÉ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\end{bmatrix} $$\nThe extrinsics matrix transforms a world coordinates into camera-space coordinates. Next, the point will be applied with the projection matrix (scaling axes to preserve points whose $z_{clip}$ is larger than its $x_{clip},\\ y_{clip},\\ z_{clip}$, and performing intrinsics) for frustum clipping. The projection matrix requires a 4D homogeneous coordinates: $[x_{cam},y_{cam},z_{cam},1]^T$. So, the above extrinsic matrix has a 4-th row, that results in an additional $1$, which is reserved for storing the depth z value, for the final perspective division. After multiplied with the extrinsics (w2c), the X,Y,Z axes of the world system are still 3 columns, but values become their coordinates under the camera coordinate system. And the 3 rows in the R of w2c are the camera coordinate system.\nHowever, the camera coordinate system has many different matrix formats in different 3D applications. For example, OpenGL (Blender) uses RUB order.\nUsually, the world space is RUB as well. So, transforming world-space coordinates to OpenGL camera-space coordiantes doesn\u0026rsquo;t need to reverse axes.\nWhereas, OpenCV uses RDF camera coord. sys. Thus, the sign of y and z coordinates require flips in the camera space.\nOne of the differences between OpenCV and OpenGL is that in OpenGL, the z-axis has near and far boundary. Refer to Amy Tabb.\nThat post was found with searching \u0026ldquo;Converting camera poses from OpenCV to OpenGL can be easy\u0026rdquo; (DDG), that is a medium blog, which is found when searching \u0026ldquo;camera coordinates right front up\u0026rdquo; (DDG)\n(2024-03-25)\nExample with a 3D point p:\nZ Y O D o w n p F o r w a X r , d R i g h t The above figure shows the right-hand world coordinate system (X-Y-Z) and a OpenCV camera coordinate system (Right-Down-Forward).\nThose 2 coordinate systems have a common origin $O$. And the camera has no rotation.\nThe coordinates of a point p under the world space is $(2, 2, 1)$. However, the coordinates in the camera space is $(2, -2, -1)$.\nThis shows that when converting the world-space coordinate to OpenCV camera-space coordinates, there is a \u0026ldquo;sign matrix\u0026rdquo;:\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} $$\nWhen the camera shifts from the world origin by rotation and translation, i.e., the extrinsics matrix, which transforms the axes of world system. So, the result coordinates is measured in a transformed world coordinate system.\nThus, the \u0026ldquo;sign matrix\u0026rdquo; is required to convert the \u0026ldquo;transformed world\u0026rdquo; system to OpenCV (or other) camera coordinate system.\n$$ \\begin{bmatrix} x_{cam} \\\\ y_{cam} \\\\ z_{cam} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} r‚ÇÅ‚ÇÅ \u0026amp; r‚ÇÅ‚ÇÇ \u0026amp; r‚ÇÅ‚ÇÉ \u0026amp; t‚ÇÅ \\\\ r‚ÇÇ‚ÇÅ \u0026amp; r‚ÇÇ‚ÇÇ \u0026amp; r‚ÇÇ‚ÇÉ \u0026amp; t‚ÇÇ \\\\ r‚ÇÉ‚ÇÅ \u0026amp; r‚ÇÉ‚ÇÇ \u0026amp; r‚ÇÉ‚ÇÉ \u0026amp; t‚ÇÉ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x_{world} \\\\ y_{world} \\\\ z_{world} \\\\ 1 \\end{bmatrix} $$\nTherefore, the matrix transforming world coordinates to OpenCV camera coordiantes is:\n$$ \\begin{bmatrix} r‚ÇÅ‚ÇÅ \u0026amp; r‚ÇÅ‚ÇÇ \u0026amp; r‚ÇÅ‚ÇÉ \u0026amp; t‚ÇÅ \\\\ -r‚ÇÇ‚ÇÅ \u0026amp; -r‚ÇÇ‚ÇÇ \u0026amp; -r‚ÇÇ‚ÇÉ \u0026amp; -t‚ÇÇ \\\\ -r‚ÇÉ‚ÇÅ \u0026amp; -r‚ÇÉ‚ÇÇ \u0026amp; -r‚ÇÉ‚ÇÉ \u0026amp; -t‚ÇÉ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nSince w2c transforms world axes to camera-space coordinates, the coordinates performed by w2c must be a world coordinates of a point, instead of a camera coordinates.\nSo, the \u0026ldquo;sign matrix\u0026rdquo; must be applied after w2c. Otherwise, the world coordinate becomes a camera coordinate immediately, which doesn\u0026rsquo;t match w2c. Specifically, the below order is incorrect:\n$$ w2c \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x_{world} \\\\ y_{world} \\\\ z_{world} \\\\ 1 \\end{bmatrix} $$\nIn other words, the \u0026ldquo;sign matrix\u0026rdquo; should be applied on a camera-space coordinates.\nIn NeRF, the provided matrix is c2w, where each column of the rot is a camera axis, and the columns order is DRB. So, the first 2 columns need to switch, thus, becoming RDB. Then, to align the camera coordinate system of Blender: RUB, the second row (the U axis) needs to be multiplied with -1. Code\n(2024-03-26)\nNote: the rotation matrix in c2w and w2c are different. For the rot in c2w, each column is an axis of camera, so reversing the direction of a camera axis requires multiplying a column with -1.\nWhereas, for the rot in w2c, each row is an axis of a camera. Thus, to reverse a camera axis, a row needs to be negated.\n(2024-03-26)\nTest reversing a row and a column in a rotation matrix for w2c:\nOriginal Rot in w2c Flip 0th row Flip 0th column $$\\begin{bmatrix} 0.970 \u0026amp; 0.00747 \u0026amp; 0.241 \\\\ -0.0147 \u0026amp; 0.999 \u0026amp; 0.028 \\\\ -0.241 \u0026amp; -0.0309 \u0026amp; 0.969 \\end{bmatrix}$$ $$\\begin{bmatrix} -0.970 \u0026amp; -0.00747 \u0026amp; -0.241 \\\\ -0.0147 \u0026amp; 0.999 \u0026amp; 0.0282 \\\\ -0.241 \u0026amp; -0.0309 \u0026amp; 0.969 \\end{bmatrix}$$ $$\\begin{bmatrix} -0.970 \u0026amp; 0.00747 \u0026amp; 0.241 \\\\ 0.0147 \u0026amp; 0.999 \u0026amp; 0.0282 \\\\ 0.241 \u0026amp; -0.0309 \u0026amp; 0.969 \\end{bmatrix}$$ $p_{cam}=[2.18994, 0.99823, 0.45571]$ $p_{cam}=[-2.18994, 0.99823, 0.45571]$ $p_{cam}=[-1.69110, 1.05720, 1.42213]$ A row of the rotation matrix in w2c is the coordinates of a camera axis in the world space.\nA column is the coordinates of a world axis in the camera space.\nThe original rotation matrix transforms the world axes to a tilted coordinate system.\nFlip the 0th row of the rotation matrix: only the X axis entirely turns to the oppsite direction.\nFlip the 0th row of the rotation matrix: the x component of all the X-Y-Z axes are affected. Apparently, this is not desired result. When flipping a single axis, the other axes should be unchanged.\nFigure plotting script: Test_reverse_cam_axis.ipynb\nIdentify Cam Axes Âè™ÊúâÊóãËΩ¨Áü©Èòµ RÔºå‰ΩÜ‰∏çÁü•ÈÅìÁõ∏Êú∫Âú®‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏≠ÁöÑÊúùÂêëÔºà‰πü‰∏çÁü•ÈÅìÂêÑËΩ¥ÁöÑÊ¨°Â∫èÔºâ„ÄÇ\n@will Âú® 23-11-10 8:25 AM ËØ¥ÔºöÁîªÂá∫Êù•ÁúãÁúãÊ≠£‰∏çÊ≠£ÂØπÂú∫ÊôØ„ÄÇ @‰ªÄ‰πàÂäû Âú® 22-11-21 8:50 PM Â±ïÁ§∫Ëøá‰ªñÁî® plt ÁîªÁöÑÁõ∏Êú∫‰ΩçÂßø„ÄÇ (2024-03-27)\nThe pose1 does not face towards the object (MVSNet_testing/dtu/scan1/cams/00000000_cam.txt):\nR for w2c rect_001_0 (full) mod signs I have dragged the figure to make the z axis upside down. The camera position is $[-191.02, 3.28832, 22.5401 ]$ And I feel the pose should be $[3, 191, 22]$\n(2024-03-30) Camera position was wrong, but it\u0026rsquo;s not due to signs. The 4-th column in w2c is not the camera position in world.\nDragging z-axis to upside down is equivalent to negating the z coordinate and switching the x and y of points coordinates. Specifically, given a point (x,y,z), dragging z to flip equals: x=y; y=x; z=-z\nDrag manually Flip z and switch x,y Code for modifying axes for scan23 1 2 3 4 5 6 7 8 9 10 11 12 13 14 %matplotlib widget import numpy as np import matplotlib.pyplot as plt import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/mnt/data2_z/SampleSet/MVS Data/Points/stl/stl023_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vs = np.asarray(pcd.points) samples = vs[np.random.choice(vs.shape[0],1000)] x = samples[:,1] y = samples[:,0] z = -samples[:,2] fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(x, y, z, color=\u0026#39;black\u0026#39;) (204-03-28)\nI know DTU matches the setup of OpenCV because the function cv2.decomposeProjectionMatrix is used. But, I\u0026rsquo;m still confused about the scene visualization with matplotlib. (2024-03-30)\nThe 4-th column of the w2c is not the camera center position in world space!! The camera position in world should be the 4-th column of c2w, i.e., $-R_{w2c}^T t_{w2c}$.\nAnd the t returned by decomposeProjectionMatrix is the camera position as well.\nAfter correcting the camera center position, the camera geometry is correct:\nPlotting script: gist\nIdentifying the axes directions should be independent of camera geometry.\nOnly drawing one camera may not easily indicate if it\u0026rsquo;s facing the scene.\n(2024-03-31)\nOpen3D can set the window (visualizer) to be the specified camera pose.\nChange set_front to different row in the rotation mat:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([0, 0, 0]) view_ctl = vis.get_view_control() vis.add_geometry(pcd) view_ctl.set_front(w2c[0][:3]) vis.run() vis.destroy_window() set_front(w2c[2][:3]) set_front(w2c[0][:3]) Set the extrinsic to simulate a camera pose: Determining the Proper Camera Position #2338\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import open3d as o3d import numpy as np pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([0, 0, 0]) vis.add_geometry(pcd) view_ctl = vis.get_view_control() w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) cam = view_ctl.convert_to_pinhole_camera_parameters() cam.extrinsic = w2c view_ctl.convert_from_pinhole_camera_parameters(cam, True) current_param = view_ctl.convert_to_pinhole_camera_parameters() print(current_param.extrinsic) vis.run() vis.destroy_window() The argument allow_arbitrary=True is required (using 0.18.0), reminded by: How to you position camera and look at certain location in Open3D? #1483\nThis argument is added to free the limitation on pinhole camera models. ConvertFromPinholeCameraParameters() failed #834\nSimilar issues:\nconvert_from_pinhole_camera_parameters does not work #1343\nconvert_from_pinhole_camera_parameters allow_arbitrary=True modifies intrinsic matrix #5816\n\u0026ldquo;view matrix\u0026rdquo; means w2c. While \u0026ldquo;world transformation matrix\u0026rdquo; is c2w. 3D GEP\nEach column in c2w is a camera axis coordinates in world space. So, 3 columns represent directions, such as RDF, and the 4-th colmun is the camera center in world space.\nHe gave a code demo to show the matrix format in column-major memory accessing.\nOpenCV2OpenGL (2024-03-29)\nJust negate the 2nd and 3rd rows in the rotation matrix that transforms word coords to cam coords. Such that, RDF camera system becomes RUB. (And note OpenGL reads matrix by columns.) OpenCV to OpenGL coordinate system transform - SO\nThis process can be done with a \u0026ldquo;sign matrix\u0026rdquo;: $[[1,0,0,0],\\ [0,-1,0,0],\\ [0,0,-1,0],\\ [0,0,0,1]]$.\nThis \u0026ldquo;sign matrix\u0026rdquo; also appears in PixelNeRF to process DTU. Yu called it as \u0026ldquo;similarity transform\u0026rdquo;. (issue#2)\nSame as the function T_opencv_to_opengl() in camtools\n(2024-04-02)\nMapping a coordinate system to another has two transformations: rotation+translation [R|t] and sign matrix. Converting camera poses from OpenCV to OpenGL can be easy - readmedium (Found when searching \u0026ldquo;opencv to opengl transformation\u0026rdquo; DDG)\nSpecifically, change the source basis first, and then flip axes of the source basis (world) to target basis (camera).\nz y W o r l d x R | t R y o t a t e d z w o x r l d f s l i i g p n z ·∂ú C a m e r x a ·∂ú y ·∂ú I realized that the terminologies: w2c and c2w are siutable for the transition between world and the OpenGL camera coordinate syste, beacuse their axes are aligned, i.e., both RUB.\nOtherwise, for example, between world and OpenCV camera system, the world system is not directly becoming the target camera system after rotation and translation [R|t].\nThat post also takes the column-major memory access into account.\nDTU dataset Original (2024-02-21)\nDTU Homepage Each object scan is taken from 49 fixed camera positions.\nFor the SampleSet, the images dimensions are 1600x1200:\n1 2 yi@yi:~/Downloads/DTU_SampleSet$ identify MVS\\ Data/Rectified/scan1/rect_001_0_r5000.png MVS Data/Rectified/scan1/rect_001_0_r5000.png PNG 1600x1200 1600x1200+0+0 8-bit sRGB 2.85068MiB 0.000u 0:00.000 The camera projection matrix ùêè‚ÇÉ‚Çì‚ÇÑ from world to image, i.e. K@[R|t] (Casmvsnet and SO):\n1 2 3 4 yi@yi:~/Downloads/DTU_SampleSet$ cat MVS\\ Data/Calibration/cal18/pos_001.txt 2607.429996 -3.844898 1498.178098 -533936.661373 -192.076910 2862.552532 681.798177 23434.686572 -0.241605 -0.030951 0.969881 22.540121 (2024-03-27) The P is estimated by Matlab, and Matlab regards camera coordinates system as RDF (mentioned in Multi-cameras on same coordinate system - Dima found by Perplexity), which is the same as OpenCV camera model.\nThe P can be decomposed to K,R,t by decomposeProjectionMatrix(): 1 2 3 4 5 P_orig=np.array( [[2607.429996, -3.844898, 1498.178098, -533936.661373], [-192.076910, 2862.552532, 681.798177, 23434.686572], [-0.241605, -0.030951, 0.969881, 22.540121]]) K,R,t = cv2.decomposeProjectionMatrix(P_orig)[:3] The original intrinsic matrix K (performed K/K[2][2]) is:\n1 2 3 array([[ 2.89233051e+03, -2.48063349e-04, 8.23205273e+02], [ 0.00000000e+00, 2.88317528e+03, 6.19070918e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]) It\u0026rsquo;s aligned with the intrinsics in mvs_training (not the cams in train/ folder):\nmvs_training/dtu/Cameras/00000000_cam.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 z@homepc:~/Downloads/Datasets_life/mvs_training/dtu/Cameras$ cat 00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 2892.33 0 823.205 0 2883.18 619.071 0 0 1 425 2.5 The table lists focal length and image resolution correspondence:\nScale resolusion cropped f_x 1 1200x1600 2892.3 1/2 600x800 512x640 1446.1 1/4 300x400 723.08 1/8 150x200 361.5 (2024-03-27)\nIn Dima\u0026rsquo;s answer, he described RDF as the world space. That means the extrinsics has been applied by the \u0026ldquo;sign matrix\u0026rdquo;, which changes the world axes to camera axes. So, the R decomposed from P essentially corresponds to the RDF coordinates system.\nIn other words, the camera coordinate system is used as the world coord. sys.\nWhereas, the world system during visualization is usually RUB (Y-axis is Up), like OpenGL. So, the object is upside down when plotting the point cloud with matplotlib.\nAnd the ccs in Open3D also is RDF (relative to world space RUB), so its initial w2c has reverse the y-axis and z-axis of the world space:\n1 2 3 4 [[ 1. 0. 0. -0.] [-0. -1. -0. 0.] [-0. -0. -1. 0.] [ 0. 0. 0. 1.]] Code: Print current cam pose 1 2 3 4 5 6 7 8 import open3d as o3d vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() view_ctl = vis.get_view_control() current_param = view_ctl.convert_to_pinhole_camera_parameters() print(current_param.extrinsic) vis.run() vis.destroy_window() MVSNet (2024-02-22)\nTraining set: dtu_training.rar (19G) (\u0026ldquo;mvs_training/dtu/\u0026rdquo;)\nAs mentioned in the section 4.1 of the MVSNet paper, the training images are 1/2 * (1200,1600) = (600,800), which then cropped to (512,640).\nIn addition, because the camera is looking at feature maps, the focal lengths should be scaled with the ratio of the size of feature map to the input image size.\nAs feature map size (128,160) is 1/4 input image (512,640) mentioned in paper section 3.1, the focal_x should be: 2892.33 * 1/2 * 1/4 = 361.541. Issue\nNote: The already calculated trianing camera params are placed in \u0026ldquo;mvs_training/dtu/Cameras/train\u0026rdquo;. Code While the cameras displayed outside the \u0026ldquo;train/\u0026rdquo; are params corresponding to the original DTU images (1200,1600).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 z@lambda:~/Downloads/mvs_training/dtu$ identify Rectified/scan1_train/rect_001_0_r5000.png Rectified/scan1_train/rect_001_0_r5000.png PNG 640x512 640x512+0+0 8-bit sRGB 626KB 0.000u 0:00.000 z@lambda:~/Downloads/mvs_training/dtu$ cat Cameras/train/00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 361.54125 0.0 82.900625 0.0 360.3975 66.383875 0.0 0.0 1.0 425.0 2.5 Testing set (dtu.zip) has the full-size images:\nTesting images are\u0026rsquo;t downsized twice or cropped, so the focal lengths only times 1/4. Code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 z@lambda:~/Downloads/data2/MVSNet_testing/dtu$ identify scan1/images/00000000.jpg scan1/images/00000000.jpg JPEG 1600x1200 1600x1200+0+0 8-bit sRGB 705KB 0.000u 0:00.000 z@lambda:~/Downloads/data2/MVSNet_testing/dtu$ cat scan1/cams/00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 2892.33 0 823.205 0 2883.18 619.071 0 0 1 425 2.5 The factor adaptive_scaling is used for the requirement that image size must be evenly divisible by 32 (e.g., 864x1152) and reducing images for limited VRAM. So, this step will also change resolution, focals, and principle points: Code\nMVSNet-PyTorch Training cam: 1/8 focal of the original DTU Code\nTesting cam: 1/4 focal of the original DTU Code\nPixelNeRF (2023-08-17)\n\u0026ldquo;rs_dtu_4\u0026rdquo; follows the DVR format. Each object has 6 matrices. Take the object 0 as an example:\n1 2 3 4 5 6 [\u0026#39;scale_mat_0\u0026#39;, \u0026#39;scale_mat_inv_0\u0026#39;, \u0026#39;world_mat_0\u0026#39;, # Projection Matrix, 4x4 \u0026#39;world_mat_inv_0\u0026#39;, # Inverse Projection matrix, 4x4 \u0026#39;camera_mat_0\u0026#39;, # ??? \u0026#39;camera_mat_inv_0\u0026#39;] Use cv2.decomposeProjectionMatrix(P) to solve ùêä,ùêë,ùê≠ from ùêè‚ÇÉ‚Çì‚ÇÑ. Code in PixelNeRF:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 P = all_cam[\u0026#34;world_mat_0\u0026#34;] P = P[:3] # (3,4), projection: Intrinsics * Extrinsics * 3Dpoint K, R, t = cv2.decomposeProjectionMatrix(P)[:3] K = K / K[2, 2] # Not the camera_mat, # Xc = extrinsics*Xw extrinsics = np.eye(4, dtype=np.float32) extrinsics[:3,:3] = R # The 4th column is the rotated transVec extrinsics[:3,3] = -(R @ (t[:3]/t[3]))[:,0] print(extrinsics) # c2w equals inverse extrinsics c2w = np.eye(4, dtype=np.float32) c2w[:3, :3] = R.transpose() # The 4th column is the normalized t decomposed by cv2. c2w[:3, 3] = (t[:3] / t[3])[:, 0] c2w == np.linalg.inv(extrinsics) The K (focal) in pixelNeRF is about twice as large as the intrinsics of \u0026ldquo;dtu_training\u0026rdquo;, because the image size of pixelNeRF (300x400) is twice as small as MVSNet (or MVSNeRF) in each dimension, where (512x640) is cropped from (600x800). It\u0026rsquo;s like when you observe the scene from far away, the image captured gets smaller.\nSince the projection matrix computed from K@(R|Rt) is different, the decomposed intrinsics will be different.\n(2024-02-22) The above statement may be wrong. Take the 1st camera as an example:\n1 2 3 4 cams = np.load(\u0026#34;pixel-nerf/data/DTU_Dataset/rs_dtu_4/DTU/scan1/cameras.npz\u0026#34;) P = cams[\u0026#39;world_mat_0\u0026#39;][:3] K, R, t = cv2.decomposeProjectionMatrix(P)[:3] K / K[2, 2] The K equals 1/4 the intrinsics of the original DTU dataset:\n1 2 3 [[ 7.23082629e+02, -6.20158374e-05, 2.05801318e+02], [ 0.00000000e+00, 7.20793819e+02, 1.54767729e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]] Because camera is used to project 3D points ont feature maps, the focals should be scaled based on the ratio of the feat map to the original image (1600,1200).\nc2w (3x4) is not the Inverse Projection Matrix (4x4).\nInverse Projection Matrix = np.linalg.inv(Projection Matrix)\nProjection matrix converts a 3D world coords to 2D pixel coords;\nExtrinsics ( w2c = [R|t] = (R|Rt) ) converts 3D world coords to 3D camera coords.\nInverse Extrinsics ( c2w ) converts 3D camera coords to 3D world coords.\nGive Extrinsics and Intrinsics (of dataset \u0026ldquo;dtu_training\u0026rdquo; from MVSNet), the Projection matrix can be restored as implemented in MVSNeRF def build_proj_mats()\nThe translation vector also needs rotation. OpenCV Decompose projection matrix\nProjection matrix = Intrinsics@Extrinsics = K@[R|t] = K@(R|Rt) = (KR|KRt)\nDecomposed t needs normalization, and to be negated sometimes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 K = np.array([[631, 0, 384], [ 0, 631, 288], [ 0, 0, 1]]) R = np.array([[-0.30164902, 0.68282439, -0.66540117], [-0.63417301, 0.37743435, 0.67480953], [ 0.71192167, 0.6255351 , 0.3191761 ]]) t = np.array([ 3.75082481, -1.18089565, 1.06138781]) P = np.eye(4) P[:3, :3] = K @ R P[:3, 3] = K @ R @ t K1, R1, t1 = cv2.decomposeProjectionMatrix(P[:3, :])[:3] t == -(t1[:3]/t1[3]) The original t can be obtained directly from projection matrix: np.linalg.inv(P[:3,:3]) @ P[:3,3], i.e., use the inverse rotation to rotate the transVec back.\n(2024-03-29)\nThe t returned by cv2.decomposeProjectionMatrix is the position of a camera in the world space. (Docs) So, it\u0026rsquo;s actually the translation vector in c2w: $t_{c2w}$.\nBecause t (denoted as $t_{cv2}$ for later) is the camera center, its corresponding camera-space coordinates is 0. Thus, this is the relationship:\n$$t_{cv2} = R_{c2w} 0 + t_{c2w} \\\\ t_{cv2} = t_{c2w}$$\nTo get the $t_{w2c}$, i.e., the 4-th column in the w2c (extrinsics), the conversion formula is $t_{w2c} = - R_{w2c} t_{c2w}$.\nThis relationship can be derived from the transformation between camera-space coordinate X and world coordinate P:\n$$ P = R_{c2w} X + t_{c2w} \\\\ X = \\underbrace{R_{c2w}^T}_{R_{w2c}} P \\underbrace{- R_{c2w}^T t_{c2w}}_{t_{w2c}} $$\nConsidering the above example, t is not the 4-th column in extrinsics, but the -R @ (t/t[3])[:3] is.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;\u0026gt;\u0026gt; t array([[-0.99198397], [ 0.00603084], [-0.12611273], [-0.00519817]]) \u0026gt;\u0026gt;\u0026gt; t/t[3] array([[190.83346195], [ -1.16018638], [ 24.26100588], [ 1. ]]) \u0026gt;\u0026gt;\u0026gt; -R @ (t/t[3])[:3] array([[-191.01958721], [ 3.28830259], [ 22.54011993]]) Therefore, in PixelNeRF directly used t as the 4-th column of the c2w (named as pose)\nI was reminded by:\nOne question about cv2.decomposeProjectionMatrix #10\nQuestions on how to use PyTorch3D\nHow to find camera position and rotation from a 4x4 matrix? - SE (surfaced by \u0026ldquo;given camera extrinsics, how to determine right, up, front\u0026rdquo; DDG)\n$$0=RC+T \\\\ C=‚àíR^T T$$\nIn the folllwing posts, they all mentioned the 4-th column in Extrinsics is not the camera center, but $-R_{c2w}^T C$, where C is the camera center in world space:\nDissecting the Camera Matrix, Part 2: The Extrinsic Matrix - ksimek\nHe derived w2c from c2w, ie. w2c = (c2w)‚Åª¬π, and provided an interactive demo for visualizing camera intrinsic, extrinsic.\nCamera Pose \u0026amp; Pose Estimation - MiaoDX refered by Camera extrinsic matrix from camera location and rotation\nHow to plot the camera and image positions from camera calibration data? - SO\nThe OP cited wikipedia $C = -R^{-1} T = -R^T T$\nPlot Camera Trajectory - SO Understanding COLMAP\u0026rsquo;s Camera Poses and Depth Data #1476\ninv([R|t]) = [R'|-R'*t] Camera position in world coordinate from cv::solvePnP - SO\n","date":"2022-05-22T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/camera_matrices/","title":"memo: Vis | Camera Matrices"},{"content":"(2022-07-20)\nHyper-params N_rand Êàñ batch_rays ÊòØ‰∏Ä‰∏™ epoch/iteration Ë¶ÅËÆ≠ÁªÉÁöÑÂÖâÁ∫ø, no_batchingÊòØÊØèepochÂè™‰ªé‰∏ÄÂº†Âõæ‰∏≠ÈÄâÂÉèÁ¥†ËÆ≠ÁªÉÔºåFalseÂàôÊâì‰π±ÊâÄÊúâÂõæÂÉèÁöÑÂÉèÁ¥†ÔºàÈÉΩ‰∏étestÈò∂ÊÆµÊ∏≤ÊüìÂÖ®ÂõæÊó†ÂÖ≥Ôºâ„ÄÇ\nllff ‰ΩøÁî®‰∫ÜbatchingÔºåÊØè‰∏Ä‰∏™ epoch ‰ΩøÁî®ÁöÑ(4096)ÂÖâÁ∫øÊù•Ëá™ÊâÄÊúâËÆ≠ÁªÉÂõæÂÉèÔºõËÄåblender ‰∏çbatchingÔºåÊØèepochÂè™‰ªé‰∏ÄÂº†Âõæ‰∏≠ÂèñÂÖâÁ∫øÔºåÊâÄ‰ª•N_rand ‰πüËæÉÂ∞è(1024)ÔºåÊúâ‰∫∫ËØ¥ÊòØÂõ†‰∏∫ blender ÂåÖÂê´360Â∫¶ÁöÑÂõæÁâáÔºåÂèØËÉΩ‰ºöÈááÂà∞ÂÆåÂÖ®ÂØπÁ´ãÁöÑ‰∏§Êù°ÂÖâÁ∫øÔºà‰∏Ä‰∏™‰ªéÊ≠£Èù¢ÁúãÔºå‰∏Ä‰∏™‰ªéÂèçÈù¢ÁúãÔºâÔºåÂèØËÉΩÂΩ±ÂìçËÆ≠ÁªÉÔºàÂΩ±ÂìçdensityÂàÜÂ∏ÉÔºüÔºâ„ÄÇNeRFÊ∫êÁ†ÅËß£Êûê-‰ªÄÂ∫¶Â≠¶‰π†\nN_samplesÂÜ≥ÂÆöÊØèÊù°ÂÖâÁ∫ø‰∏äÁöÑÈááÊ†∑ÁÇπÊï∞ÈáèÂíåNeRFÁöÑÊ∏≤ÊüìË¥®Èáè;\nchunkÊòØmodel‰∏ÄÊ¨°ËÆ°ÁÆóÁöÑrays, netchunk ÊòØÂÆûÈôÖÊØèÊ¨°ËæìÂÖ•modelÁöÑ3D pointsÈáèÔºàÊ®°ÂûãÁöÑtrainÂíåtestÈÉΩ‰∏éÂÆÉ‰ª¨ÊúâÂÖ≥Ôºâ„ÄÇ chunk Âíå netchunk ÊòØ‰∏§Â±ÇÂæ™ÁéØ„ÄÇ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Select N_rand rays for training. # Or input all rays in an image (H,W) for rendering def render(rays): for each ray-chunk in rays: ret = render_rays(ray-chunk) def render_rays(): # Sampling 3D points and do PE for each point-netchunk in all points of this ray-chunk: raw.append(model(pts-netchunk)) return tf.concat(raw) # Collect ret for each ray-chunk ret_list.append[ret] # All rays are composed: all_ret = tf.concat(ret_list) # for N_rand or (H,W) rays chunkÔºàÂÖâÁ∫øÔºâÂíånetchunkÔºàÁÇπÔºâÂèØ‰æùÊòæÂ≠òÂ§ßÂ∞èË∞ÉÊï¥„ÄÇÂΩìN_randÁõ∏ÂêåÔºåËÄå netchunk ‰∏çÂêåÔºåÊõ≤Á∫øÂá†‰πéÈáçÂè†(~0.03)ÔºåÂè™ÂΩ±ÂìçËÆ≠ÁªÉÈÄüÂ∫¶„ÄÇ\nËôΩÈ¢ÑËÆæchunk=1024x32Ôºå‰ΩÜN_rand (batch_rays)È¢ÑËÆæÂè™Êúâ1024ÔºåÊâÄ‰ª•ÂÆûÈôÖËæìÂÖ•modelÁöÑÂÖâÁ∫øÊòØ1024ÔºåÂàôËæìÂÖ•coarse model ÁöÑÊï∞ÊçÆÁÇπÊúâ1024x64‰∏™ÔºàÁúüÊ≠£ÁöÑ\u0026rsquo;batch_size\u0026rsquo;ÔºâÔºåÂΩìÈÄöËøáattention layerÊó∂ÔºåË¶ÅÂàõÂª∫‰∏Ä‰∏™ (65536,128,128) Âº†ÈáèË¶ÅÁî®1.07G Ôºà‰∏çÔºåtfÂÖàÂàõÂª∫ËÆ°ÁÆóÂõæÔºåPEÂáΩÊï∞Âç†‰∫Ü8‰∏™GÔºåÂÖ∂‰ªñÁöÑÊòæÂ≠òÂ∫îËØ•ÊòØËøîÂõûÂÄº(tensor)Âç†ÁöÑÔºâ\nndc Âú®render()‰∏≠ÈªòËÆ§‰∏∫TrueÔºåÂè™ÊúâÂΩìÊï∞ÊçÆÈõÜ‰∏çÊòØllffÁ±ªÂûãÔºåÊàñÊåáÂÆö‰∏ç‰ΩøÁî®ndcÔºà--no_ndcÔºâÊó∂ÔºåÂèÇÊï∞Â≠óÂÖ∏‰ºöÊ∑ªÂä†‰∏ÄÈ°πndc=False„ÄÇ\nlindisp = False ÂàôÊòØÂØπÊ∑±Â∫¶Ôºà‰∏ñÁïåÁ©∫Èó¥:[near,far]ÊàñËÄÖndcÁ©∫Èó¥:[0,1]ÔºâÁ∫øÊÄßÈááÊ†∑Ôºõ‰∏∫True ÂàôÂØπ 1/depth Á∫øÊÄßÈááÊ†∑\ntestskip ‰∏çÂΩ±Âìçllff, ‰πü‰∏çÂΩ±Âìç train ÈõÜÔºàÈÄêÂº†ËÆ≠ÁªÉÔºâÔºåË∑ë val ÈõÜÂíå test ÈõÜ‰ºöË∑≥ËøáÂõæÁâá„ÄÇ\nfactorÊòØ llff Êï∞ÊçÆÈõÜÂú® load_llff_data() Êó∂ÁöÑ shrink ÂÄçÊï∞Ôºå half_resÂΩ±Âìç blender(ÂíådeepVoxel)Êï∞ÊçÆÈõÜÔºõ render_factorÊòØtestÈò∂ÊÆµ(render_path())ÁöÑÊ∏≤ÊüìÂàÜËæ®Áéá\nrandom_seed Âú®testÈò∂ÊÆµ‰∏çËµ∑‰ΩúÁî®(perturb=False,raw_noise_std=0)„ÄÇ Âú®train Êó∂Ôºå(use_batching) ‰ºöÊâì‰π±ÊâÄÊúâÂÉèÁ¥†Ôºå(no_batching)‰ºöÈöèÊú∫ÈÄâÂõæÁâáÂíåÂÉèÁ¥†, Âú®density‰∏äÂä†ÈöèÊú∫noiseÔºåÂú®ÂÖâÁ∫øÈááÊ†∑Êó∂Âä†ÂÖ•ÈöèÊú∫perturb„ÄÇ\nperturb Âíå raw_noise_std Âú® test (--render_only, render_path()) Ê∏≤ÊüìÈò∂ÊÆµÈÉΩÊòØ0„ÄÇ\nOOM 3 similar variables:\nN_rand is the #rays used for 1 \u0026ldquo;epoch\u0026rdquo; to train\nchunk is the #rays to do render_rays() in a for loop, where points will be sampled to do positional embedding (fine stage has double pts to do PE.),\nThe embedded_fn() in coarse stage takes 2G meomery; The embedded_dirs() in coarse stage takes 2G; And the fine stage takes another 4G. NeRF model takes 357 MB.\nDict all_ret contains all the returend values from model. In the 12th iteration of render_rays() after coarse stage, VRAM becomes 10763 MB from 8465 MB and then OOM, where I returned 3 additional tensors: pts, viewdirs, z_vals. So the return value seems to occupy memory as well.\nBut if keeping the original settings where fewer tensors are returned, it becomes 10763 MB when finishing the ray-chunk for loop (32768 ray x 23 chunks) in batchify_rays().\nMemory change: 485 MB ‚Æï 2277 MB ‚Æï 4369 MB ‚Æï 8465 MB\nnetchunk is the #pts fed into model.\nretraw is one of the reasons:\nWith setting --render_only and --factor=4, if retraw=True, OOM occurs at the line all_ret = {k: tf.concat(all_ret[k], 0) for k in all_ret} in batchify_ray(). 1 2 rgb, disp, acc, extras = render( H, W, focal, chunk=chunk, c2w=c2w[:3, :4], retraw=True, **render_kwargs) This shows that the returned values indeed take some memory. (2022-12-26)\nInverse transform sampling sample_pdf(z_vals_mid, color_weights[...,1:-1],N_importance,) code\nPDF is the proportion of the color-weight of a point to the sum of all weights on a ray. Each point has a ratio indicating how much of its color contributes to the pixel color. (They exclueded the start and end pts, and may only focus on the distribution of the middle part.)\nCDF tells how much the color has been rendered up to the current point. The input of CDF is the color-weight of a point, and its output is the cumulative weight from ro to that point. The weights will be accumulated to 1. So coarse net learns the weights CDF roughly. And fine sampling is based on the CDF increment.\n(When testing) N_importance points u are selected evenly from [0,1] and they will fall into buckets spaced apart by CDFs of coarse points. The percentage of the position of u between the coarse point below and above it determines the distance marching from its corresponding left-neighbor z_vals_mid. The marching distance of a fine pts z is proportional to the increment of cdf.\nWhen the CDF rises faster, there are more samples because the steep slope makes many us fall together.\nIn the above figure, the vertical coordiantes are the uniformly sampled u. Horizontal x-ticks are coarse samples on a ray. Green markers | are midpoints of coarse samples\u0026rsquo; z. Red points are fine samples. Red points are all start from a midpoint and march a distance which is proportional to the cdf increment of its corresponding u.\ntf.searchsorted(seq, values, side) returns the bin-index for each value. seq defines a series of bins, and values are assigned to bin-indices based on the edges listed in seq. side indicates whether the index of a bin is marked by its left or right edge. For example: edges = [-1, 3.3, 9.1, 10.0]; values = [0.0, 4.1, 12.0], return array([1,2,4]) more\n1 2 3 0 1 2 3 4 ‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚óè‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚óè‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚óè‚Äî‚Äî‚Äî‚Äî -1 0 3.3 4.1 9.1 10 12 (2022-06-15)\nPE has no œÄ (pi in positional embedding? #12)\n‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏ãÁöÑ zÔºàÈÄöËøá projectmatrix ÁöÑÁ¨¨3Ë°åÔºâ‰ªé (-near,-‚àû) ÁöÑËåÉÂõ¥Áº©ÊîæÂà∞‰∫ÜndcÂùêÊ†áÁöÑ[-1,1]Ôºåx,yÂπ∂Ê≤°ÊúâÂèÇ‰∏ézÁöÑÁº©ÊîæÔºåx,yÊòØ‰æùÊçÆÂ±èÂπïÂ§ßÂ∞èÁº©ÊîæÁöÑÔºåÂ¶ÇÊûú scene ‰∏çËÉΩË¢´‰∏Ä‰∏™Â±èÂπïË£Ö‰∏ãÔºà‰∏ÄÂπÖimageÊ≤°ÊúâÊääsceneÊãçÂÖ®ÔºâÔºåÂ±èÂπïÂ§ñÁöÑÁÇπÁöÑÂùêÊ†áÊòØÂú®[-1,1]ËåÉÂõ¥Â§ñÁöÑÔºöÊØîÂ¶Ç scene ÁöÑËåÉÂõ¥ÊòØ[-1.5, 1.5]ÔºåÁÑ∂ÂêéÂú®embedÊó∂‰πò‰∏äœÄÔºåÂàôÊúÄ‰ΩéÈ¢ëÊó∂ÁöÑÂÆö‰πâÂüü‰∏∫ [-1.5œÄ, 1.5œÄ]ÔºåÂõ†‰∏∫Âë®ÊúüÊÄßÔºåÂ§ö‰∏™xÂèØËÉΩÂØπÂ∫îÂêå‰∏Ä‰∏™yÔºåÂØºËá¥ÈáçÂ§çÂµåÂÖ•ÔºåÊâÄ‰ª•ÂÜçÁº©Êîæ‰∏Ä‰∏™œÄÔºåÂÆö‰πâÂüüÂèò‰∏∫[-1.5,1.5]ÔºåÂú®ËøôÊÆµÂå∫Èó¥‰∏äÊòØÂçïË∞ÉÁöÑÔºå‰∏çÂêåÁöÑxË¢´ÁºñÁ†ÅÂêéÁöÑÂÄºÊòØ‰∏çÂêåÁöÑ„ÄÇ nerf‰ª£Á†ÅÂàÜ‰∫´-Áà±Áù°ËßâÁöÑ‰∫∫‰∫∫-bili\n(2022-06-15)\nUse t to sample points ‰ΩøÁî®‰∏≠Èó¥Èáè t ÊòØ‰∏∫‰∫ÜÁªü‰∏Ä LLFF Âú∫ÊôØÂíå blender Âú∫ÊôØÂØπÂÖâÁ∫øÈááÊ†∑ÁöÑ‰ª£Á†ÅÔºàllffÁöÑÂÖâÁ∫øÈïøÂ∫¶z‚àà[0,1]ÔºåblenderÁöÑÂÖâÁ∫øÈïøÂ∫¶z‚àà[near,far]Ôºâ„ÄÇ\nblenderÂú∫ÊôØËæπÁïåÊúâÈôêÂèØ‰ª•Áõ¥Êé•ÂØπÂÖâÁ∫øÈïøÂ∫¶ÈááÊ†∑Ôºå‰ΩÜÊòØ llff Âú∫ÊôØÂÖâÁ∫øÊó†ÈôêÈïøÔºàÁõ¥Êé•ÈááÁÇπÊïàÊûú‰∏çÂ•ΩÔºüÔºâÔºåÈúÄË¶ÅÂÖàÂ∞ÜÂú∫ÊôØÁöÑ‰∏ñÁïåÁ≥ªÂèòÊç¢‰∏∫NDCÁ≥ªÔºàÂè™ÂØπllffÂú∫ÊôØÊúâÊïàÔºâÔºåÂ∑¶‰πò projection matrix ‰Ωø‰∏ñÁïåÁ≥ªÁöÑzÊñπÂêëËæπÁïåÂùêÊ†á[-near,-far]Âèò‰∏∫[-1,1]„ÄÇ\nÂàô‰∏ñÁïåÁ≥ª‰∏≠ÁöÑraysË°®ËææÂºè: ùê´=ùê®+tùêù ÂèòÊàê‰∫Ü ùê´=ùê®\u0026rsquo;+t\u0026rsquo;ùêù\u0026rsquo; (Ëµ∑ÁÇπùê®\u0026rsquo;ÂíåÊñπÂêëùêù\u0026rsquo;ÈÉΩÊòØnear,farÁöÑÂáΩÊï∞)Ôºõ ‰ª§ùê®\u0026rsquo;=ùê®ÔºåÂàô(‰∏ñÁïå)ÂÖâÁ∫øÈïøÂ∫¶t‚àà[0,‚àû] ÂèòÊàê(NDC)ÂÖâÁ∫øÈïøÂ∫¶ t\u0026rsquo;‚àà[0,1] ÈôÑÂΩïÂÖ¨Âºè15„ÄÇ ÂêéÈù¢Â∞±ÂèØ‰ª•ÈÄöËøáÂØπ t\u0026rsquo;‚àà[0,1] ÈááÊ†∑ÔºåÊù•ÂØπ‰∏ñÁïåÁ≥ª‰∏≠ÁöÑÂêÑÊù°ÂÖâÁ∫øÈááÊ†∑Ôºà‰∏çÊòØÂØπ‰∏ñÁïåÁ©∫Èó¥ÁöÑzËΩ¥ÈááÊ†∑ÔºåËÄåÊòØÂú®ÂêÑÊù°ÂÖâÁ∫ø‰∏äÈááÊ†∑Ôºâ„ÄÇ\nÂú®ÂØπ‰∏ñÁïåÂÖâÁ∫øÂÅö NDC ÂèòÊç¢ÂâçÔºåÊääÂêÑÊù°ÂÖâÁ∫øÁöÑËµ∑ÁÇπ ùê® ËΩ¨ÁßªÂà∞ÂÖ∂‰∏éËøëÂπ≥Èù¢ (z_world=-near) ÁöÑ‰∫§ÁÇπÂ§ÑÔºåËøôÊ†∑ÂèòÊç¢Âà∞NDCÂêéÔºåÂÖâÁ∫øÈïøÂ∫¶t\u0026rsquo;=0 Â∞±ÂØπÂ∫îÁùÄ -nearÔºåÂè™‰ºöÂØπÂêÑÊù°ÂÖâÁ∫øËêΩÂú®z_world‚àà[-near,-far]ÁöÑÈÉ®ÂàÜÈááÊ†∑„ÄÇÂú®‰ªé‰∏ñÁïåÁ≥ªÂèòÂà∞NDC‰∏≠ÂÖâÁ∫ø(ùê®\u0026rsquo;Âíåùêù\u0026rsquo;)Êó∂ÔºåËÆ§‰∏∫nearÂπ≥Èù¢Âú®z_world=1Â§Ñ(ÂáΩÊï∞ndc_rays()ÁöÑÂèÇÊï∞), farÂπ≥Èù¢Âú® z_world=‚àû Â§Ñ (ÂÖ¨Âºè23,24)„ÄÇ (‰ΩúËÄÖÂú®issue34Ëß£ÈáäËØ¥:) Âõ†‰∏∫NDCÁöÑzËΩ¥ÊòØÊåâ 1/depth Á∫øÊÄßÂèòÂåñÁöÑÔºåÊâÄ‰ª•‰∏ñÁïåÁ≥ª-far=‚àûÊ≤°ÂÖ≥Á≥ªÔºàÂèòÂà∞ndc‰∏≠Â∞±ÊòØfar=1ÔºâÔºåÂè™‰ºöÁ®çÂæÆÈôç‰ΩéÈááÊ†∑ÊïàÁéá„ÄÇ\nÁ°ÆÂÆöÂ•ΩndcÂÖâÁ∫øÂêéÔºåÊäänear,farÈáçÊñ∞ËµãÂÄº‰∏∫ÂÖâÁ∫øÈïøÂ∫¶ÁöÑÂèñÂÄºËåÉÂõ¥Ôºàllff ÁöÑNDC‰∏≠ÂÖâÁ∫øÈïøÂ∫¶ÁöÑÂèñÂÄºËåÉÂõ¥Âè™ËÉΩÊòØ[0,1]ÔºåËÄåblenderÂú∫ÊôØÁöÑÂÖâÁ∫øÈïøÂ∫¶ËåÉÂõ¥Â∞±ÊòØ‰∏ñÁïåÁ≥ªÁöÑ[near,far]ÔºâÔºåÁî®‰∫éÂú®ÂêÑÂÖâÁ∫ø‰∏äÈááÊ†∑ÔºåÂÜçÊ†πÊçÆÂÖâÁ∫øÂÖ¨Âºèùê®\u0026rsquo;+z‚ãÖùêù\u0026rsquo;ÂæóÂà∞ÈááÊ†∑ÁÇπÁöÑ3DÂùêÊ†á„ÄÇÔºànear,far‰∏çÊòØdepthÊ∑±Â∫¶ËåÉÂõ¥ÔºåËÄåÊòØÂ∞ÑÁ∫øÈïøÂ∫¶„ÄÇÔºâ nerf code\nnear,farÊú¨Êù•Âè™ÊòØÈïøÂ∫¶ÔºåÂú®nerf‰∏≠Ôºå‰∏ñÁïåÁ≥ª(avg_camera)‰∏éÁõ∏Êú∫Á≥ªzËΩ¥ÈáçÂêàÔºåsceneËêΩÂú®-zÊñπÂêëÔºåÊâÄ‰ª•ËøëËøúËæπÁïåÁöÑ ‰∏ñÁïåÂùêÊ†á ‰∏∫-near,-farÔºåËÄåndcÁöÑzËΩ¥ÊñπÂêëÊòØÁõ∏Êú∫ËßÇÊµãÊñπÂêëÔºåËøëËøúÂπ≥Èù¢ÁöÑndcÂùêÊ†á=-1,1„ÄÇ\n(2023-05-25)\nt or z sampling When sampling points:\n1 pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3] ËøôÈáårays_dË¶ÅÊòØÊ≤°ÊúâÂΩí‰∏ÄÂåñÔºåÁõ¥Êé•Â∞±ÊòØ KÈÄÜ*(u,v,1)ÔºåÈÇ£ËøôÈáåz_vals Â∞±ÊòØÂà∞ÊàêÂÉèÂπ≥Èù¢Ë∑ùÁ¶ªÔºõÂ¶ÇÊûúÂΩí‰∏ÄÂåñ‰∫ÜÔºåÈÇ£Â∞±ÊòØÂà∞ÂÖâÂøÉË∑ùÁ¶ª \u0026ndash; will. (QQ chat 2023-05-25T04:09:00) Âü∫‰∫éÊï∞ÊçÆÈõÜÁªôÂÆöÁöÑÊ∑±Â∫¶Áõ¥Êé•Êò†Â∞Ñ‰∏âÁª¥ÁÇπÔºåËøúÁ´Ø‰ºöËÜ®ËÉÄÔºåÊäïÂΩ±Âà∞zËΩ¥‰∏äÂá†‰ΩïÂ∞±Ê≠£Â∏∏‰∫ÜÔºåÊâÄ‰ª•NeRFÂá∫Êù•ÁöÑÊ∑±Â∫¶ÊòØËßÜÁÇπÂà∞‰∏âÁª¥ÁÇπÁöÑË∑ùÁ¶ª \u0026ndash; ÂìàÂìà (2022-06-15)\nRays in NeRF ÂØπÂÖâÁ∫øÁöÑÂ§ÑÁêÜÔºö(camera ‚ûî world (avg camera) ‚ûî ndc)\nÊûÑÈÄ†Ôºà‰∏ñÁïåÁ≥ª‰∏≠ÁöÑÔºâÂÖâÁ∫øÔºörays=get_rays_np(c2w) -\u0026gt; (ro+rd, 3)\nÂÉèÁ¥†Á≥ª(u,v) ‚ûî ÊàêÂÉèÂπ≥Èù¢Á≥ª(u-cx, v-cy) ‚ûî unproj2Áõ∏Êú∫Á≥ª(x=u-cx/f, y=-(v-cy)/f, z=-1) ‚ûî ‚àóc2wÔºà\u0026ldquo;‰∏ñÁïåÁ≥ª\u0026rdquo;ÔºâÔºåËøôÈáåÂèñÂπ≥ÂùáÁõ∏Êú∫Á≥ª‰Ωú‰∏∫‚Äú‰∏ñÁïåÁ≥ª‚Äù„ÄÇÂè™ÊúâÊääÂÖâÁ∫øÂèòÊç¢Âà∞‰∏Ä‰∏™ÂÖ±ÂêåÁöÑ\u0026quot;‰∏ñÁïåÁ≥ª\u0026quot;‰∏ãÊâçËÉΩÁî®‰∏çÂêåËßÜÂõæÂØπÂêå‰∏Ä 3D point ÂÅö‰ºòÂåñ„ÄÇ ‰º†ÂÖ•ÁöÑc2w=poses[:,:3,:4] ÊòØ‰ªé load_llff_data()ËøîÂõûÔºåÂÖ∂‰∏≠ÂáΩÊï∞recenter_poses()/spherify_poses()Ë∞ÉÊï¥ÂéüposesÔºåËÆ©ÂÆÉ‰ª¨ËÉΩÂ§üÊääÁÇπÂèòÊç¢Âà∞\u0026quot;Êñ∞‰∏ñÁïåÁ≥ª\u0026quot;‰∏ã„ÄÇÔºàNeRF‰ª£Á†Å‰∏≠ÁöÑposesÂÖ®ÊòØc2wÂèòÊç¢Ôºâ avgc_2w=poses_avg(poses)ÔºåÂπ≥ÂùáÁõ∏Êú∫Á≥ª‚ûî‰∏ñÁïåÁ≥ª poses=np.linalg.inv(avgc_2w)@posesÔºåÁõ∏Êú∫Á≥ª‚ûî‰∏ñÁïåÁ≥ª‚ûîÂπ≥ÂùáÁõ∏Êú∫Á≥ª(\u0026lsquo;Êñ∞‰∏ñÁïåÁ≥ª\u0026rsquo;) Ê∏≤ÊüìÔºà‰∏ñÁïåÁ≥ª‰∏≠ÁöÑÔºâÂÖâÁ∫øÔºörgb=render(batch_rays)\nro, rd = ndc_rays(near, ro, rd)ÔºåÂÖâÁ∫øÁöÑ‰∏ñÁïåÂùêÊ†áÂèòNDC all_ret = batchify_rays(rays,chunk)ÔºõÂú®render_rays()‰∏≠Ôºå‰ªéNDCÁöÑ [near,far] ÈááÊ†∑z ‰∏çÁõ¥Êé•‰ΩøÁî® colmap ÁîüÊàêÁöÑc2wÔºåÂõ†‰∏∫ colmap (sfm) ‰∏≠ÁöÑ‰∏ñÁïåÁ≥ªÊòØÁî±ËæìÂÖ•Â∫èÂàóÁöÑÁ¨¨‰∏ÄÂ∏ßÁ°ÆÂÆöÁöÑÔºå‰ΩÜÁ¨¨‰∏ÄÂ∏ßÁöÑÊñπÂêëÊòØÈöèÊú∫ÁöÑÔºå‰∏ç‰∏ÄÂÆöÊ≠£ÂØπsceneÔºåÂú∫ÊôØÂ∞±ÈáçÂª∫‰∏çÂÖ®ÔºåÂêéÈù¢Â∏ßÁöÑ poses (ÂàóÂêëÈáè)ÈÉΩÊòØÁõ∏Êú∫Âú®‰∏ñÁïåÁ≥ª‰∏≠ÁöÑË°®Á§∫ Áà±Áù°Ëßâ‰∫∫‰∫∫16:00„ÄÇ\n‰ªéÂÉèÁ¥†Âπ≥Èù¢ÊûÑÂª∫Âá∫ÁöÑ rays ÊòØÊ≤øÁùÄÁõ∏Êú∫Á≥ªÁöÑ-zËΩ¥ÁöÑÔºàËßÇÊµãÊñπÂêëÔºåÂç≥ÊôØÁâ©Âú®Áõ∏Êú∫zËΩ¥ÁöÑËÉåÈù¢ÔºâÔºå‰πüÂ∞±ÊòØNDCÁöÑZËΩ¥ÊñπÂêë(ndcÂÅáËÆæ)ÔºõÊääÂÖâÁ∫øÂèòÂà∞Âπ≥ÂùáÁõ∏Êú∫Á≥ª(‰∏ñÁïåÁ≥ª)‰∏ãÂêéÔºåÂÜçÂèòÊç¢Âà∞NDCÔºåÂàôsceneÂ∞±ËêΩÂú®NDCÁöÑÊ≠£ÂçäzËΩ¥‰∏äÔºà‰πãÂêéÈÉΩ‰ΩøÁî®ndcÂùêÊ†áÔºâ„ÄÇ Âõ†‰∏∫ NDC ÂÅáËÆæÁõ∏Êú∫ÊòØÊ≤ø -z ÊñπÂêëËßÇÂØüÔºåÊâÄ‰ª•ÂèñÂêÑÁõ∏Êú∫zËΩ¥ÂèçÊñπÂêëÁöÑÂπ≥ÂùáÔºå‰Ωú‰∏∫NDCÁöÑzËΩ¥ÊñπÂêë„ÄÇ Âõ†‰∏∫ÂÖâÁ∫øÂÖàË¢´ÂèòÊç¢Âà∞‰∏ñÁïåÁ≥ª‰∏ãÔºåÂÜçÊää‰∏ñÁïåÁ≥ªÂèò‰∏∫NDCÔºàËÄåOpenGLÊòØÊääÁõ∏Êú∫Á≥ªÂèòÊç¢Âà∞ndcÔºâÔºåÊâÄ‰ª•‰∏ñÁïåÁ≥ªÁöÑzËΩ¥‰∏éNDCÁöÑzËΩ¥‰∏ÄÊ†∑ÁúãÂêëscene„ÄÇ nerf-issue#34 LLFF data preprocessing\nÊ±ÇÂπ≥ÂùáÁõ∏Êú∫Á≥ª(Âú®‰∏ñÁïåÁ≥ª‰∏ãÁöÑË°®Á§∫) recenter_posesÔºö Áõ∏Êú∫ÂÖâÂøÉÁöÑÂùêÊ†áÁõ¥Êé•ÂØπÁ¨¨4ÂàóÂèñÂπ≥ÂùáÂæóÂà∞ÔºõÊóãËΩ¨Áü©ÈòµRÂàÜ‰∏∫3‰∏™ÂàóÂêëÈáèÔºöÂêÑÁõ∏Êú∫zËΩ¥ÂèñÂπ≥ÂùáÔºåÂÜç‰ΩøÁî®Âè≥ÊâãÂÆöÂàôÁ°ÆÂÆöx,y„ÄÇ Êää4‰∏™ÂàóÂêëÈáèx,y,zËΩ¥ÂíåÂÖâÂøÉ concat Ëµ∑Êù•Â∞±ÊòØÂπ≥ÂùáÁõ∏Êú∫Á≥ªÂú®‰∏ñÁïåÁ≥ª‰∏≠ÁöÑË°®Á§∫ÔºåÂ∞±ÊòØc2w„ÄÇc2w Ê±ÇÈÄÜÂæó w2c„ÄÇ Âéüposes ÊääÁõ∏Êú∫Á≥ª‰∏≠ÁÇπÂèòÊç¢Âà∞‰∏ñÁïåÁ≥ª‰∏ãÔºåÂÜçÁî±w2cÂèòÊç¢Âà∞Âπ≥ÂùáÁõ∏Êú∫Á≥ª(‚ÄòÊñ∞‰∏ñÁïåÁ≥ª‚Äô)‰∏ã„ÄÇ\n(Áü©ÈòµÁöÑÂàóÂêçÊòØÊ∫êÁ≥ª, Ë°åÂêçÊòØÁõÆÊ†áÁ≥ª, ÂèòÊç¢È°∫Â∫è‰∏∫‰ªé\u0026rsquo;Âàó\u0026rsquo;Âà∞\u0026rsquo;Ë°å\u0026rsquo;, ÊØèÂàóÁöÑÊÑè‰πâÊòØ\u0026rsquo;Ê∫ê\u0026rsquo; seen from \u0026lsquo;ÁõÆÊ†áÁ≥ª\u0026rsquo;)\nÂØπposes $\\begin{bmatrix} R \u0026amp; t \\\\ 0 \u0026amp; 1 \\end{bmatrix}$ Ê±ÇÈÄÜÊó∂ÔºåRËΩ¨ÁΩÆÔºåt ‰ºöÊúâ‰∏ÄÁÇπÂèòÂåñÔºåÊúÄÂêé‰∏ÄË°åËøòÊòØ0001„ÄÇ\n(2024-03-29)\nÂØπ c2w Ê±ÇÈÄÜÂêéÔºåw2c ‰∏≠ÁöÑ t Á≠â‰∫é $-R_{c2w}^T t_{c2w}$. For example, given a 3D point p, suppose its world-space coordinates is P. And its camera-space coordinates is X. So, there is:\n$$ \\begin{bmatrix} R_{c2w} \u0026amp; t_{c2w} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} P \\\\ 1 \\end{bmatrix} \\\\ R_{c2w} X + t_{c2w} = P \\\\ X = R_{c2w}^{-1} (P - t_{c2w}) \\\\ X = \\underbrace{R_{c2w}^T}_{R_{w2c}} P \\underbrace{- R_{c2w}^T t_{c2w}}_{t_{w2c}} $$\ndoubt: rays_d = dir@c2w.T ÊòØÊÄé‰πàÊé®ÂØºÁöÑ? (c2w ‚àó dir)·µÄ = dir·µÄ ‚àó c2w·µÄ nerf-pl (2022-05-22)\ndoubt: C2W ÊòØ K[R|T] ÁöÑÈÄÜÔºåcolmapÁõ¥Êé•Ê±ÇÂá∫‰∫ÜÔºüÂØπ\nc2w ‰∏≠ÁöÑ [R|T] ‰∏∫Ôºö $$ \\begin{array}{ccc} \\qquad \\rm Xcam \\quad\\quad Ycam \\quad\\quad Zcam \\quad cam\\ center\\\\ \\begin{bmatrix} 0.989 \u0026amp; -0.0224 \u0026amp; -0.142 \u0026amp; -3.67 \\\\ -0.0272 \u0026amp; -0.999 \u0026amp; -0.0318 \u0026amp; -1.603 \\\\ -0.141 \u0026amp; -0.0354 \u0026amp; -0.989 \u0026amp; -0.276 \\end{bmatrix} \\end{array} $$\nÊûÑÈÄ† world Á≥ª‰∏≠ÁöÑÂÖâÁ∫øÂç≥Á°ÆÂÆö ro Âíå rdÔºöroÂ∞±ÊòØ cam center Âú®world‰∏≠ÁöÑÂùêÊ†áÔºåÂ∞±Á≠â‰∫éc2wÁ¨¨4Âàó ro=c2w[:3,-1]Ôºård ÊòØÁî®ÂÉèÁ¥†(u,v)ÂØπÂ∫îÁöÑÁõ∏Êú∫Á≥ªÂùêÊ†á(u,v,1)ÂèòÊç¢Âà∞worldÁ≥ª‰∏ãÁöÑÂùêÊ†á(xw,yw,zw)Ë°®Á§∫ rd=c2w[:3,:3]@pixels_cam_coord bds.min Âú®loadÊï∞ÊçÆÊó∂ÔºåÁº©ÊîæÂú∫ÊôØ‰∏ãÁïåbds.min()ÂíåÂπ≥ÁßªÂèòÈáèTÔºàËá≥1./bd_factorÔºåR‰∏çÈúÄÁº©ÊîæÔºâÊòØ‰∏∫‰∫Ü‰ª•Èò≤Âú∫ÊôØÂ§™Â§ßÔºåÂèòÊç¢Âà∞ NDC Êó∂Ë∂ÖÂá∫ near Âπ≥Èù¢ÔºõËôΩÁÑ∂ËøôÈáåÁº©ÊîæÂêéÁöÑËæπÁïå‰∏çÁ≠â‰∫é nearÔºå‰ΩÜ‰πãÂêéÂú®ndc_rays()‰∏≠‰ºöË¢´Áº©ÊîæÂõû -near (-1)„ÄÇissue#34 (2022-06-03)\nget_rays() get_rays()‰ªéÂÉèÁ¥†Âπ≥Èù¢ÊûÑÂª∫rayÁöÑÊñπÂêëÂêëÈáèdirÔºö È¶ñÂÖàÊääÂÉèÁ¥†ÂùêÊ†á (u,v) ÂèçÊäïÂΩ±Âà∞Áõ∏Êú∫Á≥ª (RUB) ‰∏ã: (x‚Çö=(u-cx)/f, y‚Çö=-(v-cy)/f, z‚Çö=-1)ÔºåvËΩ¥‰∏éyËΩ¥ÂèçÂêëÔºåÂèñËøô‰∫õÁÇπÊ∑±Â∫¶ÂÄº=-1ÔºàÂ¶Ç‰∏ãÂõæÔºâ„ÄÇ ÂÜç‰πò‰ª• c2w ÂèòÊç¢Âà∞ world ÂùêÊ†áÁ≥ª‰∏ã„ÄÇ‰ªéËÄå‰∏ÄÊù°ÂÖâÁ∫øÂú®‰∏ñÁïåÁ≥ª‰∏≠Ë°®Á§∫‰∏∫Ôºöùê´=ùê®+tùêùÔºâdir ‰∏éÁêÉÂùêÊ†á œÜŒ∏ Ê≤°ÊúâÂÖ≥Á≥ª„ÄÇ nerf-issue#24: the direction in get_raysÔºõ Áà±Áù°Ëßâ‰∫∫‰∫∫04:07\nColmap/OpenCV‰∏≠ÁöÑÁõ∏Êú∫ÂùêÊ†áÁ≥ªÊòØ [right | down | forwards]Ôºå ÁªèËøáLLFFÂ≠òÂÇ®poses‰∏∫ [down|right|backwards]Ôºå NeRF Âú®_load_data()ËØªÂèñÂêéÁ´ãÂç≥ÂèòÊç¢‰∏∫ (OpenGL‰∏≠ÁöÑÁõ∏Êú∫Á≥ª) [right | up | backwards]„ÄÇ\nposes_bounds.npyÊñá‰ª∂‰∏≠ÊòØ camera-to-worldÔºÅ(ËÄå‰∏çÊòØÂ§ñÂèÇÁü©Èòµ!) ÊØè‰∏ÄË°å17‰∏™ÂÄºÔºå Ââç15‰∏™ÂÄºÊòØ3x5ÁöÑpose matrixÔºåÊúÄÂêé2‰∏™ÂÄºÊòØnear,farÂú∫ÊôØËæπÁïå„ÄÇ ‰∏Ä‰∏™ pose matrix Áî±5‰∏™ÂàóÂêëÈáèÁªÑÊàêÔºöcam down-axis, cam right-axis, cam back-axis, cam center, hwf(cam intrinsics)Ôºå Ââç3ÂàóÊòØ‰∏ñÁïåÁ≥ª‰∏ãÁöÑÁõ∏Êú∫Á≥ª„ÄÇ\n_load_data() ËøîÂõûÁöÑ poses ÊòØ reshape ÂêéÁöÑ(3,5,20), Á¥¢Âºï poses[:,1:2] ÂØπÂ∫îÂà∞matrix‰∏≠ÁöÑÁ¨¨2Âàó„ÄÇ ÊâÄ‰ª•Âú® #250Ë°å ÈáçÊñ∞Ë∞ÉÊï¥posesÁöÑ\u0026quot;Ë°å\u0026quot;È°∫Â∫èÔºå‰ΩøRÁöÑÂàóÈ°∫Â∫è(Áõ∏Êú∫Á≥ª)‰∏∫Ôºö[right,up,back]„ÄÇÂèàÂú®#251Ë°åÊääposesÂèòÂõû(20,3,5) Using your own poses without running COLMAP-LLFF\nÂú®ÂêéÁª≠Â∑•‰Ωú‰∏≠ÔºåÊäänerf‰ΩøÁî®ÁöÑposesÔºàÁªèload_llff_data()Â§ÑÁêÜÔºå‰ΩúÁî®‰∏∫cam-to-avg_camÔºâÔºåÂèàÂèòÂõûOpenCV‰∏ãÁöÑÁõ∏Êú∫Á≥ªÔºàÊØîÂ¶ÇIBRNet,GNTÔºâ„ÄÇ Â¶Ç‰∏ãÂõæ(from Zirui Wang-Twitter)\n(2022-11-14) The direction of the constructed world rays rays_d should not be normalized to 1 when used for sampling points (where in nerf-pl is wrong confess). Original nerf didn\u0026rsquo;t normalize in get_rays(). Because the points are sampled based on z, so they locate on parallel planes. If the directions are normalized, the plane will become a sphere.\nBut the viewdirs is normalized when it used as the model input (See run_nerf.py line#306). viewdir is the normalized rd.\ndist dists are intervals between [near,far] and used for transforming the density œÉ (activation) to opacity Œ± of each interval on the ray: Œ±=(1-e‚Åª ≥·µâÀ°·µò‚ÅΩ‚Å∂ ·ïÅ ·µà‚Å±À¢·µó‚Åæ), because the opacity is proportional to distance (\u0026ldquo;thickness\u0026rdquo;). The dists can be equal when no perturb, so if density is high like around the surface, the opacity is close to 1. And the color coefficient: w=(1-Œ±) ‚àè‚Çú‚Çå‚ÇÅ·µó‚Åª¬π Œ±‚Çú is monotonically decreasing, so the further a point is, the smaller its color coeff will be until 0.\nThe derivative of color weight at the surface point t‚àó is not zero: $dw(t^‚àó)/dt \u0026lt; 0$ (see Neus proof). Hence, the minia is not on the surface. Errors (2023-05-13)\nmogrify error A new environment is created on Ubuntu 22.04. The package imagemagick is installed in the env nerf by checking conda list. But when I debug the code in vscode, the shell /bin/sh cannot found the command mogrify:\n1 2 3 4 Fixing random seed 2201 Minifying 64 ./data/nerf_llff_data/fern mogrify -resize 1.5625% -format png *.JPG /bin/sh: 1: mogrify: not found Testing in a terminal, if the env is (nerf), executing the command: /bin/sh -c mogrify is okay and it will prompt the optional arguments.\nBut if the env is (base), executing /bin/sh -c mogrify, the same error occurs: /bin/sh: 1: mogrify: not found.\nI found a Chinese post for the same problem. He installed imagemagick again: sudo apt install imagemagick „ÄêNeRF„ÄëÂú®yenchenlin/nerf-pytorch‰∏äËøêË°åÊñ∞ÁöÑÊï∞ÊçÆÈõÜ\nFor this CalledProcessError \u0026quot;/bin/sh: 1: MY_COMMAND: not found\u0026quot; problem, some people suggest to create a link to /usr/bin, e.g., linux.org and ms-community\nmogrify doesn\u0026rsquo;t exist in /opt or /usr as the command find /opt /usr -name magick didn\u0026rsquo;t return anything. mogrify command not found (homebrew)\nmogrify only exits in:\n1 2 3 (nerf) w@homepc:~$ find ~/anaconda3 -name mogrify /home/w/anaconda3/pkgs/imagemagick-7.1.1_5-pl5321h211c493_1/bin/mogrify /home/w/anaconda3/envs/nerf/bin/mogrify Therefore, maybe the debugger of vscode will only search in the /usr/bin (without searching the newly created virtrual envs) ? But the interperter of VScode is indeed shown as Python 3.7.12 ('nerf') ~/anaconda3/envs/nerf/bin/python. I don\u0026rsquo;t know \u0026hellip;\nSolutions:\nCreat a symbolic link sudo ln -s ~/anaconda3/envs/nerf/bin/mogrify /usr/bin/mogrify, then that error disappeared. apt install imagemagick works too because it creates a binary in /usr/bin, which will be found by \u0026ldquo;/bin/sh\u0026rdquo;. (Didn\u0026rsquo;t try) Add the ~/anaconda3/envs/nerf/bin into PATH by adding the line: export PATH=$PATH:/home/zichen/anaconda3/envs/nerf/bin in ~/.bashrc imread_v2() error The version of imageio in this env is 2.28.1. And it\u0026rsquo;s 2.19.0 in my lab server. When reading images:\n1 2 3 def imread(f): if f.endswith(\u0026#39;png\u0026#39;): return imageio.imread(f, ignoregamma=True) 1 read() got an unexpected keyword argument \u0026#39;ignoregamma\u0026#39; Maybe I can remove that argument. But I chose to downgraded the package: conda install imageio=2.19.0.\n(2023-05-14)\nBlas GEMM launch failed Matrices cannot be multiplied at the 1st layer, but the input data and the coarse model both are correct.\n1 2 def ret(inputs): # inputs:(N_rays*N_samples, 90); ret: (n_inputs, 4) return tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0) Error message:\n1 tensorflow.python.eager.core._NotOkStatusException: InternalError: Blas GEMM launch failed : a.shape=(65536, 63), b.shape=(63, 256), m=65536, n=256, k=63 [Op:MatMul] Reboot doesn\u0026rsquo;t work. (Verified: nothing to with imagemagick.)\nnvidia-smi doesn\u0026rsquo;t show another process (notebooks, pycharm) using GPU besides Xorg and gnome-shell.\nNvidia suggested 30 series card to use CUDA 11.2 or newer. Error Internal: Blas GEMM launch failed 30Á≥ªÊòæÂç°‰∏çÂÖºÂÆπ?-Áü•‰πé; Problems Training on RTX3080 - DeepSpeech - Mozilla Discourse\nSolution is using tf1.15 maintained by Nvidia Dr.Donald-2020-12-09 ,referenced by this post Solved: Error with Tensorflow \u0026amp; GPU - Dataiku community And this package requires Python 3.8, but nerf is using 3.7, so there is an error: error: subprocess-exited-with-error, when installing pip install --user nvidia-tensorflow[horovod] issue#15 However, if I directly downgrade conda install python=3.8, there will be too many conflict error. Also modifying environment.yml and creating based on that doesn\u0026rsquo;t work neither.\nBase on the answer on SO, I create a new env:\n1 2 3 4 5 6 conda create -n nerf-nvtf python=3.8 conda activate nerf-nvtf pip install --user nvidia-pyindex # conda install -c conda-forge openmpi # for multi-GPU # export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/anaconda3/envs/nerf/lib/ pip install --user nvidia-tensorflow[horovod] Then I tried import tensorflow works. But once I installed other packages:\n1 conda install numpy matplotlib imageio imageio-ffmpeg configargparse imagemagick Then import tensorflow cannot find the module.\nIs this problem caused by the following modificaitons?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 The following packages will be REMOVED: libgomp-11.2.0-h1234567_1 The following packages will be UPDATED: libgcc-ng anaconda/pkgs/main::libgcc-ng-11.2.0-~ --\u0026gt; anaconda/cloud/conda-forge::libgcc-ng-12.2.0-h65d4601_19 libstdcxx-ng anaconda/pkgs/main::libstdcxx-ng-11.2~ --\u0026gt; anaconda/cloud/conda-forge::libstdcxx-ng-12.2.0-h46fd767_19 zlib anaconda/pkgs/main::zlib-1.2.13-h5eee~ --\u0026gt; anaconda/cloud/conda-forge::zlib-1.2.13-h166bdaf_4 The following packages will be SUPERSEDED by a higher-priority channel: _libgcc_mutex anaconda/pkgs/main::_libgcc_mutex-0.1~ --\u0026gt; anaconda/cloud/conda-forge::_libgcc_mutex-0.1-conda_forge _openmp_mutex anaconda/pkgs/main::_openmp_mutex-5.1~ --\u0026gt; anaconda/cloud/conda-forge::_openmp_mutex-4.5-2_kmp_llvm python anaconda/pkgs/main::python-3.8.16-h7a~ --\u0026gt; anaconda/cloud/conda-forge::python-3.8.16-0_73_pypy I tried to remove those packages, but there\u0026rsquo;s a long waitting.\nSo I create a new environment:\n1 2 3 4 5 conda create -n nerf-nvtf-1.15 python=3.8 conda activate nerf-nvtf-1.15 pip install --user nvidia-pyindex pip install --user nvidia-tensorflow[horovod] pip install \u0026#39;imageio==2.19.0\u0026#39; configargparse Then the code can run normally.\nLego ply (2024-04-03)\nThe points3d.ply opened in Meshlab is a cube.\n","date":"2022-05-17T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-nerf_code_notes/","title":"read: Render - NVS | NeRF Code"},{"content":"(2022-05-08)\nCondense math expression Homogeneous coord ÂèØ‰ª•Êää Â∏∏Êï∞È°π Âíå Èô§Ê≥ï ÂºïÂÖ•Áü©ÈòµËøêÁÆóÔºåÊòØ‰∏∫‰∫ÜÊääÂπ≥ÁßªÔºàÂä†Â∏∏Êï∞ÔºâÂíåÈÄèËßÜÊäïÂΩ±ÔºàÈô§Ê∑±Â∫¶ÔºâÂÜôÂà∞‰∏Ä‰∏™Áü©Èòµ‰∏≠„ÄÇ Áü©Èòµ‰πòÊ≥ïÂ∞±ÊòØÂÖà‰πòÂÜçÂä†ÔºåÂΩìÈΩêÊ¨°ÂùêÊ†á‰∏∫1ÔºåÁî®‰∫éÊ∑ªÂä†Â∏∏Êï∞È°πÔºõ ÂΩìÈΩêÊ¨°ÂùêÊ†á‰∏ç‰∏∫1ÔºåÂèØ‰ª•‰Ωú‰∏∫Á≥ªÊï∞Ë¢´Èô§ÊéâÔºåÂæóÂà∞ÂΩí‰∏ÄÁöÑxyz„ÄÇ\nRefer to Êé¢Áßò‰∏âÁª¥ÈÄèËßÜÊäïÂΩ±-ÈΩêÊ¨°ÂùêÊ†áÁöÑÂ¶ôÁî® -Â•á‰πêÁºñÁ®ãÂ≠¶Èô¢ bilibili (2023-12-20) ‰ΩøÁî®ÈΩêÊ¨°ÂùêÊ†áÔºåÂàôÈùûÁ∫øÊÄßÁöÑÈÄèËßÜÊäïÂΩ±ÂèØ‰ª•ÂÜôÊàêÁ∫øÊÄßÁöÑËøêÁÆó„ÄÇ\nA linear operation T() satisfies additivity $T(a+b) = T(a) + T(b)$ and homogeneity $T(ca) = c T(a)$, where c is a scalar.\nHowever, perspective projection ùêè for representing a 3D scene on a 2D plane requires x, y divided by depth: x/d, y/d. Intuitively, use scaling to indicate depth. Perspective projection will alter the shape, such as parallel lines are no longer parallel:\nParallel lines aren\u0026rsquo;t parallel after projection.\nSimilarly, the 2D projection of a 3D ellipsoid may not an ellipse with both ends of equal size, but a tapered oval, resembling the outline of an egg.\nEllipse vs oval\nPerspective projection of a 3D Gaussian does not result in a 2D Gaussian. \u0026ndash; Mathematical Supplement for the gsplat Library\nThus, ùêè is not linear.\nBut if we bypass the division by considering x, y are already divided by d, forming 2D plane coordiantes (x/d, y/d), and then multiplying by d again, the 2D coordinates (x/d, y/d) need to append an additional dimension, i.e., (x/d, y/d, 1), to record the multiplier d.\nSo, the 2D plane coordinates (x/d, y/d, 1) becomes (x,y,d) after multiplying with d.\n$$(u,v) = (u,v,1) = (x/d, y/d, 1) = (x, y, d)$$\nNote: Appending 1 is not trying to revert a 2D pixel to 3D space, but used to represent the one more operation for the 2D coordinates. So it still represents a 2D pixel after appending 1.\nThat means, during the intermediate computation, there is no need to calculate (x/d, v/d) to obtain (u,v) , but use (x, y, d) to refer a 2D plane coordiantes.\nIn summary, the division is skipped, and ùêè becomes a linear operation represented with a 3x3 matrix.\nWhen moving in 3D space, the 3D coordiantes (x,y,z) needs an extra dimension as (x,y,z,1) to combine rotation and translation together. And [R|t] is a linear operation.\n(2024-02-15) The projection matrix also applies to a 4D camera point (x,y,z,1), where the homogeneous coordinate 1 will store the z, after the camera point got multiplied by the projection matrix, where the 4-th row is [0 0 -1 0].\nAs the resulting clip coordinates are not the final transformation yet in the pipeline, the original depth $z$ in camera space requires to be recorded for the perspective division that is supposed to be the final step.\nAfter frustum clipping and the clip coordinates perform perspective division, ND coordinates are obtained and utilized in the ND space for image formation.\n(2024-01-01) Summarize again\nIn the perspective projection, homogeneous coordinates use 3D coordinates to represent a 2D pixel for temporarily storing the depth value, which will be divided at the very end to keep the intermediate computations as linear operations.\nMoreover, in the view transformation, homogeneous coordinates uses 4D coordinates to represent a 3D point for holding the translation vector.\n(2022-08-16)\nDistinguish point and vector ÈΩêÊ¨°ÂùêÊ†áÁî®‰∫éÂå∫ÂàÜ ÂêëÈáè Âíå ÁÇπ„ÄÇ\u0026lsquo;ÂêëÈáè\u0026rsquo;Âè™ÈúÄÂü∫ÂêëÈáèÁöÑÁ∫øÊÄßÁªÑÂêàÔºåËÄå\u0026rsquo;ÁÇπ\u0026rsquo;ÈúÄË¶ÅÂä†‰∏äÂéüÁÇπÔºåÊää\u0026rsquo;ÁÇπ\u0026rsquo;Ë°®Á§∫‰∏∫‰ªéÂéüÁÇπÂá∫ÂèëÁöÑÂêëÈáè„ÄÇ ÁªôÂÆö‰∏ÄÁªÑÂü∫ÂêëÈáèùê±,ùê≤,ùê≥ÔºåÂàô‰∏Ä‰∏™ÂêëÈáèùêØ = aùê±+bùê≤+cùê≥ÔºõËÄå‰∏Ä‰∏™ÁÇπ ùê© = ùê®+aùê±+bùê≤+cùê≥ÔºåÂÖ∂‰∏≠ùê®ÊòØÂéüÁÇπ„ÄÇÊâÄ‰ª•(a,b,c,0)ÊòØÂêëÈáèùêØÁöÑÂùêÊ†áÔºåËÄå(a,b,c,1) ÊòØÁÇπùê©ÁöÑÂùêÊ†á„ÄÇ\nÂπ≥ÁßªÂèòÊç¢ÈúÄË¶Å‰ΩøÁî®ÈΩêÊ¨°ÂùêÊ†áÔºåÊòØÂõ†‰∏∫Âè™Êúâ‚ÄòÁÇπ‚ÄôÈúÄË¶ÅÂπ≥ÁßªÔºåË¶ÅÊÉ≥Ë°®Á§∫ÁÇπÂ∞±ÂæóÁî®ÈΩêÊ¨°ÂùêÊ†á„ÄÇËÄåÂêëÈáèÊ≤°Êúâ‰ΩçÁΩÆÁöÑÊ¶ÇÂøµÔºåÂè™ÊúâÂ§ßÂ∞èÂíåÊñπÂêë\nRefer to Ê∑±ÂÖ•Êé¢Á¥¢ÈÄèËßÜÊäïÂΩ±ÂèòÊç¢ - popy007 -CSDN (2024-02-17) Tutorial 3 : Matrices - opengl-tutorial.org\n(x,y,z,w=1) is a position, while (x,y,z,w=0) is a direction.\n(2023-02-13)\nShow 3D world on a plane When the homogeneous coordinate w=1 is appended behind the Cartesian coordinates (u,v), the result (u,v,w=1) becomes the 3D point (x,y,depth) because u=x/depth, v=y/depth.\n2D world on plane\n3D world on plane\nFor example, the railroad tracks are parallel on the 2D ground plane. But when they\u0026rsquo;re observed in a (higher-dimension 3D) projective space (human eyes, camera, convex lens), the parallel lines would converge. Otherwise, if our eyes are plane mirror, we will never find the world is 3D.\nThis effect can be interpreted as that the coordinates (x,y) scale down as 3D points get further away. Hence, drawing a railroad onto canvas should follow th relationship: (x/depth, y/depth), where x,y are constants and the depth increases.\n2D plane can only represent 2 directions, so if we want to display 3D world on a 2D plane, the additional dimension (depth) has to be engaged implicitly.\nTherefore, the meaning of pixel (u,v) on plane is (x/depth, y/depth), which corresponds to the 3D point (x,y,depth), such that the picture mimics the scene looked at by human: x,y are inversely proportional to depth (Big near, small far: perspective).\nThe homogeneous coordinate w=1 is used to accommodate the depth: (u = x/depth, v = y/depth, w=1) ‚áí (x, y, depth).\nThe w is not specified arbitrarily. If the given 2D coordinates are (u,v), then the w should =1, waiting for the depth split from u,v.\nTherefore, an extra dimension is supplimented to adapt the observation from higher-dimension space.\nThen, the homogenous coordinates of a pixel (u,v) on the plane is (u,v,w). When analyzing it in 2D space, its coordinates are (u/w, v/w).\nFor example, two pixels represented by homogeneous coordinates are (1,2,1) and (1,2,0)\nThat is, the projection pixel (u,v) of a 3D point, when the point goes far away, the coordinates (u,v) are not constant but inversely changing with depth. This effect can be represented by an extra dimension to reflect the depth change.\n(2023-02-12)\nCompensate for Cartesian coord The homogeneous coordiante w is supplemented to adapting Cartesian coordiantes to represent projective space Homogeneous Coordinates - songho. (Cannot convince me)\nThe parallel lines should never intersect at infinity in Cartesian space (plane), but they have to converge in projective space (human eye/camera).\nTo use 2D planes to represent perspective, the homogeneous coordinate w is appended behind the Cartesian coordinates (x,y) of each point to adapt to the projective observation.\nThus, each point in projective space has 3 coordinates (x,y,w). Then, the 2D coordinates of each point are obtained by normalizing the 3rd dimension: (x/w, y/w, 1), such that two parallel lines would converge. In other words, it\u0026rsquo;s easier to analyze multiple points by scaling their w to 1.\nHomogeneous coordinate w is the auxiliary for the Cartesian space. Thus, the effect of depth can be represented on a plane (like projection).\nIf the point (1,2) from Cartesian space is combined with different w to make up the homogeneous coordinates (1,2,w), the corresponding 2D coordinates (1/w, 2/w) will form a line.\nIf the 3 coordinates change propotionally, like (1,2,3), (2,4,6), \u0026hellip; (n,2n,3n), these homogeneous coordinates corresponds to a common 2D coordinates (1/3, 2/3) on the plane. This means the homogeneous coordinates are scale invariant. Or inversely, a pixel on the plane corresponds to a line in the 3D space (homogeneous coordinates system).\nw is an attribute for each point in perspective space, where every point has 3 coordinates (x,y,w), while the points in Euclidean space don\u0026rsquo;t have this property.\nWhat we human perceived on our retina or captured on the camera plane are the projection: (x/w, y/w, 1).\nBecause each 3D point has differnt w, their projections are located on different position on the image plane. Thus,\nTherefore, given an image, the homogeneous coordinates for each pixel are (u/w, v/w, 1). If the w is known, then the homogeneous coordinates can be wrriten as (u, v, w).\nThe point in 3D space has the coordinates: (x,y,w) is divided by the w. homogeneous coordinates (x,y,w)\nA picture showing projective effect actually is a stack of different planes with different depth.\n(x/w, y/w), where the x,y are already divided by the dpeth w, so if we want to get the Cartisan coordiantes back, the w has to be separated: (x, y, w), then the first 2 number are 2D Cartesian coordinates.\nThat means the homogeneous coordiantes of 2D point (x,y) is just appending a w at the end, like (x,y,w).\nThe points with propotional homogeneous coordinates corresponds to the same 2D Cartesian point. For example, (1,2,3) and (2,4,6) With the extra dimenstion, the coordinates for a 2D pixel\nHomogeneous coordinates convert the non-homogeneous linear system to a homogeneous system.\nIf the homogeneous coordinate w added behind Cartesian coordinates is to represent the depth (x/w,y/w,1)·µÄ, then the homogeneous coordinate w=1 added behind a 3D points is to accommodate the translation (x/w, y/w, z/w, 1)·µÄ. Ref Ê∑±ÂÖ•Êé¢Á¥¢ÈÄèËßÜÊäïÂΩ±ÂèòÊç¢(Áª≠) - popy007 -CSDN Homogeneous coordinates - Wikipedia (Back to top)\n","date":"2022-05-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/homogeneous_coord/","title":"memo: Vis | Homogeneous Coordinates"},{"content":"P15\nIrradiance The power per (perpendicular/projected) unit area incident on a surface point. $E(x)‚â°\\frac{dŒ¶}{dA} cos\\theta$ Âçï‰ΩçÈù¢ÁßØ‰∏äÁöÑËÉΩÈáè(ÂûÇÁõ¥‰∏éË°®Èù¢ÁöÑÂàÜÈáè) ÂÖâÁ∫øÂú®Âçï‰ΩçÁêÉÈù¢ÁöÑËÉΩÈáè‰∏éË∑ùÁ¶ªÂëàÂπ≥ÊñπÂèçÊØîÔºå ","date":"2022-05-02T09:24:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/15-light_transport_global_illumination/","title":"watch: CG - Èó´‰ª§Áê™ 15 | Ray-Training-3"},{"content":"P14\nSpatial Partitioning ÂØπÁ©∫Èó¥ÂÅöÂàíÂàÜ tree ÂÖ´ÂèâÊ†ëOct-Tree Êää‰∏Ä‰∏™3Áª¥ÁöÑÂåÖÂõ¥ÁõíÂàáÊàê8‰ªΩ(2^3) È´òÁª¥Á©∫Èó¥Â∞±ÊòØ2^n ÂèâÊ†ëÔºå KD-Tree ÊØèÊ¨°ÂàíÂàÜÂè™Âú®1‰∏™Áª¥Â∫¶‰∏äÂàíÂàÜ(‰∫åÂèâÊ†ë) n‰∏™Áª¥Â∫¶‰æùÊ¨°Âæ™ÁéØË¢´ÂàíÂàÜÔºå‰øùËØÅÁ©∫Èó¥‰∏äÊòØÂùáÂåÄÁöÑ ÈóÆÈ¢òÔºöÂåÖÂõ¥ÁõíÊòØÂê¶ÂåÖÂõ¥‰∏âËßíÈù¢ÈúÄË¶ÅÂØπÂåÖÂõ¥Áõí‰∏é‰∏âËßíÈù¢Ê±Ç‰∫§Ôºå‰∏çÂ•ΩÂÜô; ‰∏Ä‰∏™Áâ©‰Ωì‰∏éÂ§ö‰∏™Âè∂Â≠êËäÇÁÇπÁõ∏‰∫§ÔºåÊâÄ‰ª•ÊØè‰∏™Âè∂Â≠êËäÇÁÇπ‰∏≠ÈÉΩÊúâÂ≠òÂÇ®Ëøô‰∏™Áâ©‰ΩìÔºàÂÜó‰ΩôÔºüÔºâÔºåÊúÄÂ•ΩÊòØ‰∏Ä‰∏™Áâ©‰ΩìÂè™Â≠òÂú®‰∫é‰∏Ä‰∏™Ê†ºÂ≠ê‰∏≠ÔºàÂü∫‰∫éÁâ©‰ΩìÂàíÂàÜÔºâ„ÄÇ BSP-Tree ÊØèÊ¨°ÂàíÂàÜÊñπÂêëÂπ∂‰∏ç‰∏éÂùêÊ†áËΩ¥Âπ≥Ë°å ËÆ°ÁÆóÈáèÂ§ß KD-Tree Preprocessing ÂØπÂú∫ÊôØÂª∫Á´ãKD-Tree Traversing a KD-Tree ‰ªéÊúÄÂ§ßÁöÑÂåÖÂõ¥ÁõíÂºÄÂßãÔºåÂà§Êñ≠ÂÖâÁ∫øÊòØÂê¶‰∏é‰πãÊúâ‰∫§ÁÇπÔºåÂ¶ÇÊûúÊúâ‰∫§ÁÇπÔºåÂàÜÂà´ÂØπÂÆÉÁöÑ‰∏§‰∏™Â≠êËäÇÁÇπÂà§Êñ≠ÊòØÂê¶‰∏éÂÖâÁ∫øÊúâ‰∫§ÁÇπÔºåÂ¶ÇÊûúÊ≤°‰∫§ÁÇπÂ∞±‰∏çÂà§Êñ≠ÂÖ∂Â≠êËäÇÁÇπ„ÄÇËµ∞Âà∞Âè∂Â≠êËäÇÁÇπÔºåÂ∞±ÂØπÂåÖÂõ¥Áõí‰∏≠ÁöÑÊâÄÊúâÁâ©‰ΩìÊ±Ç‰∫§„ÄÇ Object Partitions \u0026amp; Bounding Volume Hierarchy (BVH) Êää‰∏âËßíÂΩ¢ÂàÜÊàê‰∏§ÁªÑÔºåÂÜçÂàÜÂà´ÈáçÊñ∞Ê±ÇÂÆÉ‰ª¨ÁöÑÂåÖÂõ¥ÁõíÔºåÁü•ÈÅì‰∏Ä‰∏™ÁõíÂ≠ê‰∏≠ÊúÄÂ§öÂê´Êúâ5‰∏™‰∏âËßíÂΩ¢Â∞±ÂÅúÊ≠¢ÂàíÂàÜ\n‰∏Ä‰∏™Áâ©‰ΩìÂè™‰ºöÂ≠òÂú®‰∫é‰∏Ä‰∏™ÂåÖÂõ¥Áõí‰∏≠,(ÂêÑÁõíÂ≠ê‰ºöÊúâÈáçÂè†)\nHow to subdivide a node?\nChoose a dimension to split Heurstic #1: Always choose the longest axis in node (Ê≤øÊúÄÈïøËΩ¥ÂàíÂàÜ) Heurstic #2: Split node at location of median object (‰ª•‰∏≠Èó¥Áâ©‰ΩìÁöÑ‰ΩçÁΩÆÂàíÂàÜÔºå‰∏§‰æßÁâ©‰ΩìÊï∞ÈáèÁõ∏Á≠âÔºåÂπ≥Ë°°ÊÑèÂë≥ÁùÄÊúÄÂ§ßÊ∑±Â∫¶Â∞èÔºåÊêúÁ¥¢Ê¨°Êï∞Â∞ë) Data Structure for BVHs\nInternal nodes store: Bounding box and Pointers to its child nodes Leaf nodes store: Bounding box and Objects Nodes represent subset of primitives(Âü∫Á°ÄÂÖÉÁ¥†) in scene. All objects in subtree ÈÅçÂéÜÊñπÂºè‰∏éKD-Tree Áõ∏ÂêåÔºö\n1 2 3 4 5 6 7 8 9 10 11 Intersect(Ray ray, BVH node) { if (ray misses node.bbox) return; if (node is a leaf node) test intersection with all objs; return closest intersection; hit1 = Intersect(ray, node.child1); hit2 = Intersect(ray, node.child2); return the closer of hit1, hit2; Radiometry Termiology Radiant energy: ÁîµÁ£ÅËæêÂ∞ÑÁöÑËÉΩÈáè Q Radiant flux: Âçï‰ΩçÊó∂Èó¥ÁöÑËÉΩÈáè Œ¶=dQ/dt [Watt]ÔºàÂäüÁéáÔºöÂçï‰ΩçÊó∂Èó¥Â∞ÑÂá∫Â§öÂ∞ëÂÖâÂ≠êÔºâ Radiant intensity: ‰ªéÂÖâÊ∫êÂèëÂá∫ÁöÑÂÖâ Irradiance: Ë°®Èù¢Êé•Êî∂ÁöÑÂÖâ Radiance: Ê≤øÂÖâÁ∫ø‰º†Êí≠ÁöÑÂÖâ Radiant intensity The radiant (luminous) intensity I is the power Œ¶ per unit solid angle œâ emitted by a point light source. $I(w) ‚â° \\frac{dŒ¶}{dœâ}$ [candela]; $Œ¶ = \\int_S¬≤ I dw = 4œÄI$ Êüê‰∏ÄÊñπÂêë‰∏äÁöÑËÉΩÈáè ","date":"2022-05-01T18:28:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/14-ray_tracing2/","title":"watch: CG - Èó´‰ª§Áê™ 14 | Ray-Tracing-2"},{"content":"P13\nÂà§Êñ≠ÁÇπÊòØÂê¶Âú®Â§öËæπÂΩ¢ÂÜÖÈÉ®Ôºö‰ª•ËØ•ÁÇπ‰∏∫Ëµ∑ÁÇπÊ≤ø‰ªªÊÑèÊñπÂêëÂÅö‰∏ÄÊù°Â∞ÑÁ∫øÔºå‰∏éÂ§öËæπÂΩ¢ÁöÑ‰∫§ÁÇπ‰∏™Êï∞ÊòØÂ•áÊï∞ÔºàÁõ∏ÂàáÊÉÖÂÜµËÆ§‰∏∫ÊòØ‰∏§‰∏™Áõ∏ÂêåÁöÑÂÆûÊ†πÔºüÔºâÂèØ‰ª•Êé®ÂπøËá≥‰∏âÁª¥„ÄÇ\nÂ§πÊùø\n","date":"2022-05-01T14:24:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/13/","title":"watch: CG - Èó´‰ª§Áê™ 13"},{"content":"Multi-Layer Perceptron MLP consists of many layers of perceptrons. MLP Áî±Â§öÂ±ÇÊÑüÁü•Êú∫ÊûÑÊàê„ÄÇ Perceptrons Âè™ËÉΩËß£ÂÜ≥Á∫øÊÄß‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºåÊòØÂõ†‰∏∫ÂÆÉÈááÁî®ÁöÑÈò∂Ë∑ÉÊøÄÊ¥ªÂáΩÊï∞Ôºö $e(t)=\\{^{0,t\u0026lt;0}_{1,t\u0026gt;0}$ÔºåÂè™ËÉΩÁ°ÆÂÆöÂ§öÁª¥ËæìÂÖ•Á©∫Èó¥‰∏≠ÁöÑ‰∏Ä‰∏™Ë∂ÖÂπ≥Èù¢_„ÄÇ\nÊØîÂ¶ÇÂú®‰∫åÁª¥Á©∫Èó¥‰∏≠ÔºåÂÆÉÂ∞±Êó†Ê≥ïËß£ÂÜ≥ÂºÇÊàñÈóÆÈ¢ò„ÄÇË¶Å‰∫ßÁîüÈùûÁ∫øÊÄßÔºàÊñúÁéá‰∏çÂõ∫ÂÆöÔºâÁöÑÂÜ≥Á≠ñËæπÁïåÂèØ‰ª•Êç¢Áî®ÈùûÁ∫øÊÄßÁöÑÊøÄÊ¥ªÂáΩÊï∞Ôºåsigmoid function, tanh ÊàñËÄÖ ReLuÁ≠âÁ≠â„ÄÇÂÆÉ‰ª¨‰ΩøÂæóÊÑüÁü•Êú∫ÂèØ‰ª•Ë°®ËææÊõ≤Á∫øÔºå‰πüÂ∞±ÊòØÂç≥‰ΩøËæìÂÖ•Ë∂ÖËøáÈòàÂÄº‰∏Ä‰∫õÔºåËæìÂá∫Ê≤°ÊúâÂØπÂ∫îÁöÑÂèòÂåñÔºå‰æùÁÑ∂‰∏∫0„ÄÇÊ≠§Êó∂ËæìÂÖ•ËæìÂá∫‰∏çÂÜçÊòØÁ∫øÊÄßÂÖ≥Á≥ªÔºåËÄå‰∏çÂÉèÈò∂Ë∑ÉÂáΩÊï∞ÔºåË∂ÖËøáÈòàÂÄºÂ∞±ËæìÂá∫1„ÄÇÂºïÂÖ•ÈùûÁ∫øÊÄß‰πãÂêéÔºåÂ§öÂ±ÇÊÑüÁü•Êú∫ÊâçÂèØ‰ª•ÈÄºËøë‰ªªÊÑèÈùûÁ∫øÊÄßÁöÑÂÜ≥Á≠ñËæπÁïå„ÄÇÂê¶ÂàôÔºåMLPÂ∞±Áõ∏ÂΩì‰∫éÂ§öÊù°Áõ¥Á∫øÂä†ÊùÉÁõ∏Âä†ÔºåÊúÄÁªàÂæóÂà∞ÁöÑËøòÊòØ‰∏ÄÊù°Áõ¥Á∫ø4ÔºåÂè™ËÉΩÁªôÂá∫0-1ÂàÜÁ±ª„ÄÇ(ËøôÈáåÁöÑÂ§öÊù°Áõ¥Á∫øÂπ∂‰∏çÊòØËÅîÁ´ãÊ±ÇÊñπÁ®ãÁªÑÔºåÂèØ‰ª•ÈôêÂÆö‰∏ÄÂùóÂå∫ÂüüÔºåËÄåÊòØË¢´ÊÑüÁü•Êú∫Âä†ÊùÉÔºåÊúÄÂêéËøòÊòØ‰∏ÄÊù°Áõ¥Á∫ø: wn(\u0026hellip;(w2(w1+b1)+b2+\u0026hellip;+bn) Ôºâ„ÄÇExample with cubic function.6\nÂè¶‰∏ÄÊñπÈù¢ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÂèØ‰ª•Â∏ÆÂä©Ëá™Âä®Ë∞ÉÊï¥ÊùÉÈáç„ÄÇÂ§öÂ±ÇÊÑüÁü•Êú∫‰ºöÂ∏¶Êù•Â§ßÈáèÈúÄË¶ÅË∞ÉËäÇÁöÑÊùÉÈáç w 1ÔºåÊâãÂä®Ë∞ÉËäÇÊòØ‰∏çÂèØËÉΩÂÆûÁé∞ÁöÑ„ÄÇÈò∂Ë∑ÉÂáΩÊï∞ÁöÑÁ∫µËΩ¥ÊòØ0Êàñ1ÔºåÊ®™ËΩ¥ÊòØwxÔºàÊøÄÊ¥ªÂÄºÔºâÔºåwÁöÑÂæÆÂ∞èÂèòÂåñ‰ºöÂØºËá¥ÊøÄÊ¥ªÂÄºÂèëÁîüÂèòÂåñÔºåÂ¶ÇÊûúËæìÂá∫Â±ÇÈááÁî®Èò∂Ë∑ÉÂáΩÊï∞ÔºåoutputÂèØËÉΩ‰ºöË∑≥ÂèòÔºàÂú®Èõ∂ÁÇπÂ§Ñ‰∏çÂèØÂæÆÔºâÔºåÂ∞±Êó†Ê≥ïÁõ¥Êé•Ê±ÇÂØºÊâæÂà∞ËæìÂá∫‰∏éËæìÂÖ•‰πãÈó¥ÁöÑÂÖ≥Á≥ª2„ÄÇ‰∏∫‰∫ÜÈáèÂåñÂæÆÂ∞èÂèòÂåñŒîwÂ∏¶ÁªôËæìÂá∫ÁöÑÂÅèÂ∑ÆÔºåÈúÄË¶ÅÊääÂè∞Èò∂Èì∫ÂºÄÔºå‰πüÂ∞±ÊòØÂèòÊàêsigmoidÂáΩÊï∞„ÄÇ‰ªéÊ≠§ÁõÆÊ†áÂ∞±ÊòØË∞ÉÊï¥ w ‰ΩøMLPËæìÂá∫‰∏éÁõÆÊ†áÂÄº‰πãÈó¥ÁöÑËØØÂ∑Æe‰∏çÊñ≠Èôç‰Ωé„ÄÇËÄåwÁöÑÂèòÂåñÊñπÂêëÂ∫îËØ•ÊòØ‰ΩøËØØÂ∑Æ‰∏ãÈôçÊúÄÂø´ÁöÑÊñπÂêëÔºå‰πüÂ∞±ÊòØÊ¢ØÂ∫¶ÊñπÂêëÔºåÂ∞±ÈúÄË¶ÅeÂØπwÊ±ÇÂØºÊï∞Ôºàe ÂÖàÂØπ sigmoid Ê±ÇÂØºÔºåsigmoid ÂÜçÂØπ wÊ±ÇÂØºÔºâÔºåÂÜç‰πò‰ª•Ê≠•ÈïøÂ∞±ÊòØ w ÊØèÊ¨°ÈúÄË¶Å‰øÆÊ≠£ÁöÑÈáè„ÄÇ\nÂêåÊó∂ÔºåÁΩëÁªú‰∏≠ÂÖ∂‰ΩôÁöÑÊÑüÁü•Êú∫‰πüÈÉΩË¶Å‰ΩøÁî®sigmoidÂáΩÊï∞ÔºåÊâçËÉΩÊ±ÇÂæóÁΩëÁªú‰∏≠ÊâÄÊúâwÁöÑÂØºÊï∞„ÄÇ Âõ†‰∏∫ËØØÂ∑ÆeÊòØÂâç‰∏ÄÂ±ÇÊÑüÁü•Êú∫ËæìÂá∫ZÁöÑÂáΩÊï∞ÔºåÊØîÂ¶ÇÈááÁî®ÊúÄÂ∞è‰∫å‰πòÊ≥ïÔºåÂàô e = (œÉ(Z)-target)¬≤ÔºåÊ†πÊçÆÈìæÂºèÊ≥ïÂàôÔºåÈúÄË¶ÅÂØπÂ∑¶‰æß‰∏ÄÂ±ÇÁöÑÊøÄÊ¥ªÂáΩÊï∞Ê±ÇÂØºÊâçËÉΩÊ±ÇÂæó‰πãÂâç w ÁöÑÂØºÊï∞„ÄÇÂ¶ÇÊûúÂÆÉ‰ª¨‰ªçÈááÁî®Èò∂Ë∑ÉÊøÄÊ¥ªÂáΩÊï∞ÔºåÂØºÊï∞Âú®ÈùûÈõ∂Â§Ñ‰∏∫Èõ∂Ôºå‰ΩÜÂú®Èõ∂ÁÇπÂ§Ñ‰∏çÂèØÂØº3ÔºåÂ∞±Êó†Ê≥ï‰øÆÊ≠£‰πãÂâçÁöÑwÔºå‰πüÂ∞±Êó†Ê≥ïÂÆåÊàêËØØÂ∑ÆÁöÑÂèçÂêë‰º†Êí≠„ÄÇ\nMSE ÂØπ œÉ Ê±ÇÂØºÔºö((œÉ(z)-target)¬≤)\u0026rsquo; = 2œÉ(z)Ôºå œÉ ÂØπ z Ê±ÇÂØºÔºöœÉ(z)\u0026rsquo; = œÉ(z)(1-œÉ(z))Ôºå z ÂØπ w Ê±ÇÂØºÔºöxÔºàÊù•Ëá™‰∏ä‰∏ÄÂ±ÇÁöÑËæìÂá∫ÔºåÊàñÊòØÁΩëÁªúInputÔºâÔºå ÊâÄ‰ª•ËæìÂá∫Â±ÇÁöÑ w ÁöÑÂØºÊï∞‰∏∫Ôºö2œÉ(z)‚àóœÉ(z)(1-œÉ(z))‚àóxÔºàÁÆÄÂåñ‰∏∫ Œ¥‚àóxÔºâ\nË¶ÅÊ±ÇÂ∑¶‰æß‰∏ÄÂ±ÇÁöÑ w‚Çã‚ÇÅ ÁöÑÂØºÊï∞ÔºåÊ†πÊçÆ chain rule: ËøôÂ±ÇÁöÑzÂØπ x (‰πüÂ∞±ÊòØ œÉ‚Çã‚ÇÅ(z‚Çã‚ÇÅ)) ÁöÑÂØºÊï∞: wÔºå x ÂØπ z‚Çã‚ÇÅ ÁöÑÂØºÊï∞ÔºöœÉ(z‚Çã‚ÇÅ)(1-œÉ(z‚Çã‚ÇÅ))Ôºå z‚Çã‚ÇÅ ÂØπ w‚Çã‚ÇÅ ÁöÑÂØºÊï∞Ôºöx‚Çã‚ÇÅÔºå ÊâÄ‰ª•eÂØπÂ∑¶‰æß‰∏ÄÂ±ÇÁöÑ w‚Çã‚ÇÅ ÁöÑÂØºÊï∞‰∏∫ÔºöŒ¥‚àów‚àóœÉ(z‚Çã‚ÇÅ)(1-œÉ(z‚Çã‚ÇÅ))‚àóx‚Çã‚ÇÅÔºàÁÆÄÂåñ‰∏∫ Œ¥‚Çã‚ÇÅ‚àóx‚Çã‚ÇÅÔºâ\nÂÜçÂæÄÂ∑¶ÁöÑËØùÔºåÂç≥ Œ¥‚Çã‚ÇÇ‚àóx‚Çã‚ÇÇ , \u0026hellip;\nÂØπ‰∫éËæìÂá∫Â±ÇÊúâÂ§ö‰∏™Á•ûÁªèÂÖÉÔºåËØØÂ∑ÆÊòØÂ§ö‰∏™Á•ûÁªèÂÖÉËæìÂá∫‰πãÂíåÔºåÊâÄ‰ª•eÊ±ÇÂØºÁöÑÊó∂ÂÄôË¶ÅÂÖµÂàÜÂ§öË∑ØÔºåeÂàÜÂà´ÂØπÊØè‰∏ÄË∑ØÊ±ÇÂØºÊï∞ÔºåÁÑ∂ÂêéÂä†Ëµ∑Êù•„ÄÇ4\n(2022-12-25) ÊØîÂ¶Ç‰∏ãÂõæ‰∏≠ÔºåÊúâ‰∏§‰∏™ËæìÂá∫Á•ûÁªèÂÖÉ y‚ÇÅ^, y‚ÇÇ^ÔºåÊ±Ç e ÂØπ w‚ÇÅ ÁöÑÊ¢ØÂ∫¶‚àÇe/‚àÇw‚ÇÅÔºåÂ∞±ÊòØ‰∏§‰∏™ËæìÂá∫Á•ûÁªèÂÖÉÂàÜÂà´ÂØπ w‚ÇÅ Ê±ÇÊ¢ØÂ∫¶ÔºåÂÜçÂä†Ëµ∑Êù•„ÄÇ\nÂàò‰∫åÈÇ£‰∏™ÂèçÂêë‰º†Êí≠ÁöÑÂõæÔºåÂàÜ‰∏§Ë∑ØÔºö‰∏ÄË∑ØÂØπxÊ±ÇÂØºÔºå‰∏ÄË∑ØÂØπwÊ±ÇÂØº„ÄÇ\nÊØè‰∏™Â±ÄÈÉ®ËÆ°ÁÆóÁöÑÂØºÊï∞ÔºåÊòØÂú®forward ÊûÑÂª∫ËÆ°ÁÆóÂõæÊó∂Â∞±ÁÆóÂ•ΩÁöÑÔºåÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶Â≠òÂÇ®Âú®ÂêÑinputËäÇÁÇπ‰∏äÔºåËÄå‰∏çÊòØÂ≠òÂú®outputËäÇÁÇπ‰∏ä„ÄÇ\nsigmoid function Perceptron accepts several input signals and give one output, including 0 and 1. Each input times its corresponding weight and is added up. If the weighted summation is larger than threshold, the output is 1. Otherwise, output is 0.\nÊ∑±Â∫¶Â≠¶‰π†‰∏ÄÂÆöË¶ÅÊúâÂÅèÁΩÆÈ°πBiasÂêóÔºüÂ§ñÂä†‰∏ÄÊÆµÁÆÄÂçïÁöÑÁ•ûÁªèÁΩëÁªúÂèçÂêë‰º†Êí≠BPÁÆóÊ≥ïÊâãÂÜôÊé®ÂØºÔΩû ‰∏ÄËà¨Ë¶ÅÊúâ biasÔºåÂõ†‰∏∫Á∫øÊÄßÂáΩÊï∞ ax+bÔºåbËÆ©Ê®°ÂûãÂú®Á´ñËΩ¥‰∏äÁßªÂä®ÔºåÂÆÉÂú®Á∫øÊÄßÊ®°Âûã‰∏≠ÊòØÈúÄË¶ÅÁöÑ„ÄÇ‰ΩÜÂú®Ê∑±Â∫¶Â≠¶‰π†Á•ûÁªèÁΩëÁªú‰∏≠ÔºåÂπ∂‰∏çÊòØÊØè‰∏™Âú∞ÊñπÈÉΩË¶ÅËÆæÁΩÆ bias„ÄÇ Âú®Ê∏ÖÂçéÂ§ßÂ≠¶ÁöÑ cpm È°πÁõÆ‰∏≠ÔºåËÆ§‰∏∫Âú®‰∏Ä‰∫õÂú∞Êñπ‰∏çËÆæÁΩÆ biasÔºå‰ΩøÂæóÊ®°ÂûãËÆ≠ÁªÉÁ®≥ÂÆöÊÄß‰ºöÊõ¥Âº∫ÔºåËÆ°ÁÆóÈÄüÂ∫¶‰ª•ÂèäÊòæÂ≠òÊ∂àËÄóÊúâ‰ºòÂäø\nÂèÇËÄÉ 1. Lecture 10 - Neural Networks -Caltech (Learning from data)-ytb 2. NNDL Sigmoid Neurons 3. PyTorch: Introduction to Neural Network ‚Äî Feedforward / MLP 4. Á•ûÁªèÁΩëÁªúÊøÄÊ¥ªÂáΩÊï∞ÁöÑ‰ΩúÁî®ÂíåÂéüÁêÜÔºüÊúâÊ≤°ÊúâÂΩ¢Ë±°Ëß£ÈáäÔºü - È¢úÊ≤ÅÁùøÁöÑÂõûÁ≠î - Áü•‰πé 5. How Does Backpropagation Work? - kasperfred Why Neural Networks can learn (almost) anything - Emergent Garden - ytb ","date":"2022-02-14T23:10:00Z","permalink":"https://zichen34.github.io/writenotes/calc/dl_mlp_nonlinearity/","title":"memo: MLP Nonlinearity"},{"content":"‰∏ÄÁ∫ßÊ†áÈ¢ò ‰∫åÁ∫ßÊ†áÈ¢ò Ê≠£Êñá\nbold text ÔºàËÆæÁΩÆÂø´Êç∑ÂÅ• ,b\nitalic text (Êñú‰ΩìÂø´Êç∑ÈîÆ ,i)\nbold italic\nwasted text (Âà†Èô§Á∫ø ,s)\n3‰∏™` Êã¨Ëµ∑Êù•ÊòØ‰ª£Á†ÅÂùó(1Â∑¶ËæπÈÇ£‰∏™ÈîÆ)\nÂä†‰∏äpython‰ºöÊòæÁ§∫ÂØπÂ∫îÁöÑÈ´ò‰∫Æ\n1 print(\u0026#34;hello\u0026#34;) [Named Link](http://www.baidu.com/\"Named link title\u0026quot;)\nhttp://www.baidu.com/\nhttp://example.com/\nheading-1 (ËΩ¨Âà∞‰∏ÄÁ∫ßÊ†áÈ¢òÂ§ÑÔºâ\nFirst Header second Header Content Cell Content Cell content Cell content cell ‰∏ÄË°å‰ª£Á†Å Áî®` `Êã¨Ëµ∑Êù•\ncode\nBullet List Item1 (*Âè∑ÂíåÊñáÂ≠ó‰πãÈó¥ÊúâÁ©∫Ê†ºÔºâ Nested bullet Sub_nested bullet etc Sub_nested sub_nested Bullet List item 2 1.A number list 1. A nested numbered list 2, which is numbered 2.which is numberd\n[] An uncompleted task ( - Âíå [ ‰πãÈó¥ÊúâÁ©∫Ê†ºÔºâ A complete task [] A subtask ÂºïÁî®Âùó\nÂ≠êÂºïÁî®Âùó\n\u0026mdash; Ê∞¥Âπ≥Á∫ø\nTitle 1 ÊäòÂè†ÂÜÖÂÆπContent 1 Content 1 Content 1 Content 1 Content 1\nF\nÂ§öË°åÂÖ¨ÂºèÁã¨Á´ãÁºñÂè∑Ôºö(Êù•Ê∫êÔºöMarkdown‰∏ãLaTeXÂÖ¨Âºè„ÄÅÁºñÂè∑„ÄÅÂØπÈΩê) $$ \\begin{eqnarray*} x^n+y^n \u0026amp;=\u0026amp; z^n \\tag{1.4} \\ x+y \u0026amp;=\u0026amp; z \\tag{1.5} \\end{eqnarray*} $$\nÂçï‰∏™ÂÖ¨ÂºèÊç¢Ë°å:\nÂçï‰∏™ÂÖ¨ÂºèÂæàÈïøÁöÑÊó∂ÂÄôÈúÄË¶ÅÊç¢Ë°åÔºå‰ΩÜ‰ªÖÂÖÅËÆ∏ÁîüÊàê‰∏Ä‰∏™ÁºñÂè∑Êó∂ÔºåÂèØ‰ª•Áî®splitÊ†áÁ≠æÂåÖÂõ¥ÂÖ¨Âºè‰ª£Á†ÅÔºåÂú®ÈúÄË¶ÅËΩ¨Ë°åÁöÑÂú∞Êñπ‰ΩøÁî®\\\\ÔºåÊØèË°åÈúÄË¶Å‰ΩøÁî®1‰∏™\u0026amp;Êù•Ê†áËØÜÂØπÈΩêÁöÑ‰ΩçÁΩÆÔºåÁªìÊùüÂêéÂèØ‰ΩøÁî®\\tag{...}Ê†áÁ≠æÁºñÂè∑„ÄÇ $$ \\begin{split}a \u0026amp;= b \\c \u0026amp;= d \\e \u0026amp;= f \\end{split}\\tag{1.3} $$\nÂ≠óÊØçÂ§¥‰∏ä ^ 1 $\\^Œ∏$ Áî® vim Êèí‰ª∂ MarkdownPreview Âú®ÊµèËßàÂô®‰∏≠ÂèØ‰ª•Ê≠£Á°ÆÊ∏≤ÊüìËøô‰∏™ÂÖ¨ÂºèÔºå ‰ΩÜÊòØ Hugo ÁöÑKaTex ÈúÄË¶ÅÂ§öÊâì‰∏Ä‰∏™back slashÔºö\\\\^Œ∏ ÊâçËÉΩÊ≠£Á°ÆÊ∏≤ÊüìÔºå‰ΩÜËøôÊ†∑ÂÅö vim Êèí‰ª∂ÈáåÂ∞±Ê∏≤Êüì‰∏çÂØπ‰∫Ü„ÄÇ\nÂçïË°åÂÜôÂàÜÊÆµÂáΩÊï∞ For katex, there is one more \\\n1 \\\\{^{x=0}\\_{x=1} ‰∫§Êç¢ÂõæË°® MathJaxÂü∫Á°ÄÔºà10ÔºâÔºöCommutative diagrams\n$$ $\\require{AMScd}$ \\begin{CD} A @\u0026gt;a\u0026raquo; B\\ @V b V V= @VV c V\\ C @\u0026raquo;d\u0026gt; D \\end{CD} $$\nmermaid style %% node style classDef lyrs fill:#ff9 class lyr11,act1,lyr12,lyr21,act2,lyr22 lyrs; %% link linkStyle 9,10,11,12,16,17,18,19 stroke:#0af,stroke-width:3px %% fontsize classDef node font-size:20px; Anchor Uppercase -\u0026gt; lowercase; Spaces -\u0026gt; -\nGithub Markdown anchor only linking to top of the page\nImages side by side 1 2 3 4 5 \u0026lt;table\u0026gt;\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/DOG.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px;\u0026#34;\u0026gt;ÊòØÂõ†‰∏∫ËæπÁïåÊ≤°ÊúâÊûÅÂÄºÔºüÊâÄ‰ª•Âè™Áî®‰∏≠Èó¥Â±Ç\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/DOG_levels.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px\u0026#34;\u0026gt;Âõæ2.DOGÈáëÂ≠óÂ°îÁöÑÂÆûÈôÖÊòæÁ§∫ÊïàÊûú\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/SIFT_Features.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px\u0026#34;\u0026gt;Âõæ3.DOGÂõæÂÉèÂΩí‰∏ÄÂåñÁªìÊûú\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/table\u0026gt; Common Unicode U+2016\t‚Äñ\tDOUBLE VERTICAL LINE\nWhite square: ‚ñ° U+25A1 White Square Unicode Character - Compart\nThere is no superscript comma in unicode. Superscripting any Letter/Number for comma\nleft ceil, left floor, right ceil, right floor:\n‚åàx‚ÇÄ‚åâ |x‚ÇÅ| ‚åäx‚ÇÇ‚åã\ncircle:\n‚¶ø\nClipboard of symbl.cc\nŒµœÜŒºfùê±‚ã±‚ãÆ‚ãØ‚åä‚åãœÉ‚àö‚Üì‚Üë‚Üì‚ñº‚¨áŒªÀô‚ãÖ‚ñ°‚Üë‚áîŒ≥Œ≤‚ààŒ±‚àë√ó‚Äñ‚ëä ‚ä†‚àÇŒõŒ∏‚à≠‚àû¬∞Œ£‚ûî¬©‚Üí‚úÖ‚¨Ø‚¨≠œÑŒ¥‚äó‚à´ùö∫ùõçùêîùêàùêÆ‚ààùê±ùêåùêñùö´‚Ñ§œâ‚ÑÆ‚àè‚Äñ‚äô‚äõŒ∑œÅùêùùêúùêïùí¢ùê≠ùêâùê¶ùõà‚âà‚ùô‚ùò‚á§‚Ü≥‚ãÆ‚ãÜ‚≠ë‚ú¶‚òÖ‚Ä¢ùë≥ùêøùê≤ùêèùê©ùê™ùêöùêõùê¶ùêßùêäùêëùê≠ùêÄùêÅùêêùêÇùêáùêÖùêï|‚èê‚ñ≠‚úö‚ñ±ùêòùêóùê≥ùêÉùêÑ‚¨ØÀö‚àòŒîüòä‚â§‚ñ±‚ñ†‚å∑‚ç®‚ç©‚ç§‚ü®‚å©‚ü©‚¨Æ‚Üò‚àáùõÅ ‚ñ¶\nLess than \u0026lt; will be mis-parsed as a html syntax by vim-markdown, use ‚ùÆ (Heavy Left-Pointing Angle Quotation Mark Ornament, brackets), or $\u0026lt;$\n${}$ won\u0026rsquo;t render the curly braces { }, use ÔΩõ ÔΩù (Fullwidth Curly Brackets)\nOr $\\\\{ \\\\}$\nÁîªÊñúÁ∫øËßíÂ∫¶‰∏çÂ§üÔºåÂèØ‰ª•Áî® ellipsis ‚ã±‚ã∞ (Up Right Diagonal Ellipsis)\nË°®Ê†ºÂàóÂÆΩ ÂÖ¨Âºè ÁÆ≠Â§¥ÊåáÁ§∫ DDG search: \u0026ldquo;latex math equation block where a symbol is pointed by an upward arrow?\u0026rdquo;\nArrow to an equal symbol in a equation to justify it - SE\n$$ a+b\\underset{\\substack{\\color{red}\\uparrow \\\\ % or: \\rotatebox{90}{$\\longrightarrow$} \\mathclap{\\textup{\\tiny commutative}} \\\\ \\mathclap{\\textup{\\tiny property}}}}{=}b+a $$\nÁ¨¶Âè∑Âä†ÊñπÊ°Ü 1 \\boxed{x} $$ \\boxed{x} $$\nSmall text (2024-05-24)\nUse the attribute font-size of tag \u0026lt;p\u0026gt;, e.g., \u0026lt;p style=\u0026quot;font-size:10px\u0026quot;\u0026gt;text\u0026lt;/p\u0026gt;: text\nKeep the text inline without going to a new line: \u0026lt;span style=\u0026quot;font-size:10px; color:red;\u0026quot;\u0026gt;text\u0026lt;/span\u0026gt;: text\n\u0026lt;small\u0026gt;text\u0026lt;/small\u0026gt;: text\nDeprecated in HTML5: \u0026lt;font size=\u0026quot;3\u0026quot; color=\u0026quot;red\u0026quot;\u0026gt;text\u0026lt;/font\u0026gt;: text\niframe (2024-01-21)\nThe size of the content can be set as below:\n1 2 3 4 5 \u0026lt;iframe width=\u0026#34;560\u0026#34; height=\u0026#34;315\u0026#34; src=\u0026#34;https://www.youtube.com/embed/-P28LKWTzrI?si=szYmMkdL7Ixw-6_4\u0026#34; title=\u0026#34;YouTube video player\u0026#34; frameborder=\u0026#34;0\u0026#34; allow=\u0026#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\u0026#34; allowfullscreen\u0026gt;\u0026lt;/iframe\u0026gt; ","date":"2022-02-12T20:33:00Z","permalink":"https://zichen34.github.io/writenotes/lang/markdown_misc/","title":"memo: Markdown Syntax"},{"content":"Arxiv\nSummary (2022-01-08)\nThe input and output are the same as a volumetric model, but the loss function is based on the renderer error. A volumetric model maps something to the attributes of every volumetric grid. The output of a model is determined by the loss function. In NeRF, the loss is the rendering error between ground truth RGB image and the rendered image that is generated according to density (alpha channel) and RGB color at each point on the path of ray 1. Therefore, the output of the network should be a filed of density and color. Overview diagram\nOptimize Tricks Positional encoding: mapping the inputs to a higher dimensional space so that MLP could converge to a high-frequency function.\n(2023-10-27) PE is Fourier transform and then MLP re-composes Fourier coefficicent to the final value.\nwill. - QQ group:\nÂÇÖÈáåÂè∂Á∫ßÊï∞ÊòØÂØπ‰∏âËßíÂáΩÊï∞È°πÁöÑÁ∫øÊÄßÂä†ÊùÉÔºõÁ•ûÁªèÁΩëÁªúÁöÑÁ∫øÊÄßÂ±ÇÊòØÂØπËæìÂÖ•ÁöÑÁ∫øÊÄßÂä†ÊùÉ„ÄÇ density ÁΩëÁªúÂ∞±ÊòØÁî®ÂÇÖÈáåÂè∂Á∫ßÊï∞ÊãüÂêà‰∫Ü sigma Âú®‰∏âÁª¥Á©∫Èó¥‰∏≠ÁöÑÂàÜÂ∏É Áî®‰ºòÂåñÂô®‰ºòÂåñ‰∫ÜÂÇÖÈáåÂè∂Á∫ßÊï∞ÔºåÁÑ∂ÂêéÈÄöËøá‰ΩìÊ∏≤ÊüìÁ∫¶Êùü‰ΩìÂØÜÂ∫¶ÁöÑÊÄßË¥®„ÄÇ\nËµ§Â≠êÊ≥õËàüÔºö\nÊàëÁêÜËß£‰πüÊòØÁ≠âÊïàÂÅö‰∫Ü‰∏ÄÊ¨°ÂÇÖÈáåÂè∂ÂèòÊç¢ÂíåÂèçÂèòÊç¢ÔºåmlpÂÖ∂ÂÆûÂú®ÊãüÂêàÈ¢ëÂüüÁâπÂæÅ„ÄÇ ËôΩÁÑ∂Â£∞Áß∞Ê≤°Áî®Âç∑ÁßØÔºå‰ΩÜÂÖ∂ÂÆûÊï¥‰∏™ËøáÁ®ãÁõ∏ÂΩì‰∫éÂÅö‰∫Ü‰∏™Â§ßÁöÑÂç∑ÁßØ„ÄÇ\nÁæ§Èô§Êàë‰Ω¨ÔºöÂ∞±ÊòØ‰∏Ä‰∏™Êõ¥ÂêäÁöÑ3dËæìÂÖ•„ÄÇ\nHierarchical volume sampling: allocating more samples to where it is expected to have a significant contribution on the final rendering.\nviewdir is represented by Cartesian (x,y,z), rather than spherical coordinates (œÜ,Œ∏,r)\nœÉ is output of linear layer, so it lies in (-‚àû,+‚àû); but volume density is [0~‚àû), so œÉ(x) is activated by ReLU. Then it converts to alpha (the probability of terminating) by $(1-e^{-œÉ‚ãÖŒ¥})$, that is when œÉ=0, alpha=0 meaning no volume no block, while if œÉ‚Üí‚àû, the alpha‚Üí1 meaning this volume block light entirely. Therefore, the transmittance is (1-alpha)=$e^{-œÉŒ¥}$, so the decay coefficient (cumulative transimitance) of a RGB color is $Œ†^{i-1}e^{-œÉŒ¥}$\nMath derivation 2.3 Integration along Ray Segmetns\nSecond sampling is based on \u0026ldquo;color decay coeff (weights)\u0026rdquo;, rather than volume density. The bigger color weights means more volume here. œÉ(x) is proportional to alpha and weights\nCode ImportError: No module named \u0026lsquo;_pywrap_tensorflow_internal\u0026rsquo;\nCreate environment: conda env create -f environment.yml, and this error occured when executing import tensorflow.\nSolution: I didn\u0026rsquo;t creat the envrionment from the yml file but directly specify packages and version. conda create -n nerf python=3.7 cudatoolkit=10.0 tensorflow-gpu=1.15 -c conda-forge\nReference 1. Ray tracing volume densities ","date":"2022-01-08T00:00:00Z","image":"https://github.com/bmild/nerf/raw/master/imgs/pipeline.jpg","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-nerf_bmild/","title":"read: Render - NVS | NeRF"},{"content":"title: \u0026ldquo;3D Content Creation and Stylization with AI\u0026rdquo;\nSource link: GAMES Webinar 215-ËßÜËßâ‰∏ìÈ¢òÔºöÊô∫ËÉΩ‰∏âÁª¥ÂÜÖÂÆπÁîüÊàê\u0026ndash;ÈáçÂª∫‰∏éÂàõÈÄ†-Kangxue Yin\n3DÂÜÖÂÆπÂàõ‰ΩúÔºö Ê∏∏ÊàèÔºåÁîµÂΩ±ÁâπÊïàÔºåÂä®ÁîªÔºåVR/ARÔºå‰∏ì‰∏öÂåñÁ®ãÂ∫¶È´òÔºàÂª∫Ê®°ËΩØ‰ª∂Ôºâ\nÂä®ÁîªÁîµÂΩ±Âàõ‰ΩúÊµÅÁ®ã:\nÂâßÊú¨ ÂéüÁîª Layout: Áâ©‰ΩìÁõ∏Êú∫ÊëÜÊîæÔºå‰ΩçÁΩÆÂÖ≥Á≥ª ÁâπÊïàÊäÄÊúØÁ†îÂèë 2D Âª∫Ê®°‰∏∫3D Á∫πÁêÜ ÁªëÂÆöÈ™®È™ºÔºåÁöÆËÇ§ÔºåÁõ∏ÂØπËøêÂä®ÂÖ≥Á≥ª ‰∫∫Áâ©ËøêÂä®ÔºàËøêÂä®ÊçïÊçâÔºâ VFX ÁâπÊïàÊ®°ÊãüÔºàÁàÜÁÇ∏Ôºâ ÊâìÂÖâ Ê∏≤Êüì(Áâ©ÁêÜ‰ªøÁúü) Ê¢¶Â∑•ÂéÇpipelineÔºöhttps://www.youtube.com/watch?v=ru0tQRJ4qKs\n3D ÂÜÖÂÆπÊàêÊú¨È´òÔºö2009 ÈòøÂá°ËææÔºà3DÔºâ $ 2.37‰∫ø ÔºåËÄå 2010Âπ¥ÁöÑ2DÁîµÂΩ± ËÆ©Â≠êÂºπÈ£û Âè™Êúâ $ 0.18‰∫ø\nÂÖÉÂÆáÂÆôÈúÄË¶ÅÂ§ßÈáè3D ÂÜÖÂÆπ\nÈôç‰Ωé3DÂÜÖÂÆπÂà∂‰ΩúÊàêÊú¨ÔºöËÆ©ÊôÆÈÄöÁé©ÂÆ∂ÂèÇ‰∏éÂàõ‰ΩúÔºàÊàøÂ≠êÔºåÊ±ΩËΩ¶\u0026hellip;ÔºâNFTÔºåÊú™ÁªèËÆ≠ÁªÉÔºåÁªÜËäÇ‰∏çË∂≥ÔºåÈ£éÊ†º‰∏çÂ§üÂ§öÊ†∑ÔºåÁî®‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©„ÄÇ\nResearch works Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape\nËæìÂÖ• voxel Ê®°ÂûãÔºåÁΩëÁªúÂêàÊàêÁªÜËäÇ„ÄÇ\n3D represention ÈÄâÊã©\nDiscrete Repre\nImplicit Fields:\nÁîüÊàêÁΩëÁªúÁªÜËäÇ‰∏çË∂≥ÔºåËΩ¨Êç¢‰∏∫meshÊúâÈóÆÈ¢ò\ndifferentiable iso-surfacing ÊääÈöêÂºèÊñπÁ®ãËΩ¨Âåñ‰∏∫‰∏Ä‰∏™meshÔºåÁÑ∂ÂêéÁî® mesh Âíå ground truth ‰πãÈó¥ÁöÑÂ∑ÆÔºàlossÔºâÊù•‰ºòÂåñÁΩëÁªúÔºåÈôç‰Ωé iso-surfacing Á¶ªÊï£ÂåñÂ∏¶Êù•ÁöÑËØØÂ∑ÆÔºåÂÖ∂‰∏≠‰ΩøÁî®ÁöÑ‰∏çÊòØ Marching cube ËÄåÊòØ Marching Tetrahedra\n3DStyleNet: Creating\n3D Áâ©‰ΩìÁöÑÈ£éÊ†ºËøÅÁßª\n","date":"2022-01-07T22:22:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/215-%E8%A7%86%E8%A7%89%E4%B8%93%E9%A2%98%E6%99%BA%E8%83%BD%E4%B8%89%E7%BB%B4%E5%86%85%E5%AE%B9%E7%94%9F%E6%88%90--%E9%87%8D%E5%BB%BA%E4%B8%8E%E5%88%9B%E9%80%A0-kangxue-yin/","title":"watch: 215-Êô∫ËÉΩ‰∏âÁª¥ÂÜÖÂÆπÁîüÊàê"},{"content":"6-Â¶Ç‰ΩïÁêÜËß£‚ÄúÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï‚ÄùÔºü‰ªÄ‰πàÊòØ‚ÄúÂèçÂêë‰º†Êí≠‚ÄùÔºüÈÄöËøá‰∏Ä‰∏™ËßÜÈ¢ëÔºå‰∏ÄÊ≠•‰∏ÄÊ≠•ÂÖ®ÈÉ®ÊêûÊòéÁôΩ\nÊ≠£Âêë‰º†Êí≠ ËæìÂÖ•Êï∞ÊçÆÊ≤øÁùÄÁ•ûÁªèÁΩëÁªúÊ≠£Âêë‰º†ÈÄí ËæìÂÖ•Êï∞ÊçÆ‰∏éÂêÑ‰∏™ÊÑüÁü•Êú∫ÁöÑwÂíåbÁÇπ‰πòÔºåÂ∞ÜÁªìÊûú‰ª£ÂÖ•ÊøÄÊ¥ªÂáΩÊï∞ÔºåÁªôÂá∫Âà§Êñ≠ÁªìÊûú ÂèçÂêë‰º†Êí≠ ÊääÂà§Êñ≠ÁªìÊûúÁöÑÂÅèÂ∑Æ‰º†ÈÄíÁªôÂêÑ‰∏™wÂíåbÔºåÊ†πÊçÆÂèÇÊï∞ÂØπÂÅèÂ∑ÆË¥°ÁåÆÁöÑÂ§ßÂ∞èÔºåÊàêÊØî‰æãÁöÑË∞ÉÊï¥ Êú™ËÆ≠ÁªÉÂ•ΩÁöÑÁ•ûÁªèÁΩëÁªúÁöÑ$\\mathbf w,b$‰∏çÂáÜÁ°ÆÔºåÂØºËá¥Âà§Êñ≠ÁªìÊûúÊúâÂÅèÂ∑Æ„ÄÇÂ¶ÇÊûúÊüê $\\mathbf w/b$ ÂØπÊúÄÁªàÁöÑÂà§Êñ≠ÁªìÊûúÊúâÈáçÂ§ßÂΩ±ÂìçÔºåÂàôËØ•ÂèÇÊï∞ÂØπ‰∫éÂÅèÂ∑Æ‰πüÊòØÊúâÈáçÂ§ßÂΩ±ÂìçÁöÑ„ÄÇÊâÄ‰ª•Âú®ÂáèÂ∞èËØØÂ∑ÆÁöÑËøáÁ®ã‰∏≠ÔºåÂ∫î‰ºòÂÖàË∞ÉÊï¥dÂØπÂÅèÂ∑ÆÊúâÈáçÂ§ßÂΩ±ÂìçÁöÑÂèÇÊï∞„ÄÇ ËÑëË°•ËøáÁ®ãÔºö\nÁ•ûÁªèÁΩëÁªúËæìÂá∫Â±ÇÁöÑËæìÂá∫ÂÄºÊòØ $a^{[3]}$:\n$$ \\mathbf a^{[3]} = \\sigma(\\mathbf w^{[3]} \\cdot \\mathbf a^{[2]} + b^{[3]}) $$\nÂÖ∂‰∏≠ $\\sigma$ ÊòØÊøÄÊ¥ªÂáΩÊï∞Ôºå$\\mathbf w^{[3]}$ËæìÂá∫Â±ÇÊùÉÈáçÔºå$\\mathbf a^{[2]}$‰∏ä‰∏ÄÂ±ÇÁöÑËæìÂá∫ÂÄºÔºå$b^{[3]}$ÊòØËæìÂá∫Â±ÇÂÅèÁΩÆÁ≥ªÊï∞„ÄÇ\nÂèØ‰ª•ËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞Ôºà‰∫§ÂèâÁÜµÔºâÔºåÂæóÂà∞ÂÅèÂ∑ÆJÔºö\n$$ J = \\frac{1}{n} \\left( -‚àë_{i=1}^n (y^{(i)}) ‚ãÖ log_2 a^{[3](i)} + (1-y^{(i)} ‚ãÖ log_2(1-a^{[3](i)}) )\\right) $$\nÂÖ∂‰∏≠ $y_i$ ÊòØÁêÜÊÉ≥Á≥ªÁªü‰∏≠ËæìÂá∫yÁöÑÊ¶ÇÁéáÔºõ$log‚ÇÇ a^{[3](i)}$ÊòØÂà§Êñ≠ÁªìÊûúÁöÑ‰ø°ÊÅØÈáè„ÄÇ\nÂÅèÂ∑ÆÁõ¥Êé•Êù•Ëá™ËæìÂá∫Â±ÇÁöÑÊÑüÁü•Êú∫:\nÂÅèÂ∑ÆÊù•Ëá™‰∏âÈÉ®ÂàÜÔºöÂΩìÂâçÂ±ÇÁöÑ w Âíå bÔºå‰ª•Âèä‰∏ä‰∏ÄÂ±ÇÁöÑËæìÂá∫ $a^{[2]}$„ÄÇÂÖ∂‰∏≠ w Âíå b ÂèØ‰ª•Ê†πÊçÆÂç†ÊØîÁõ¥Êé•Ë∞ÉÊï¥ÔºåËÄå $a^{[2]}$ ÁöÑÂÅèÂ∑ÆÊù•Ëá™‰∫é‰∏ä‰∏ÄÂ±ÇÔºåÊåâÁÖßË¥°ÁåÆÂ§ßÂ∞èÂàÜÈÖçÂÅèÂ∑ÆÔºö\nÁÑ∂ÂêéÂâç‰∏ÄÂ±ÇÁöÑÊÑüÁü•Êú∫Âç†ÊúâÁöÑÂÅèÂ∑ÆÂèàÂèØÂàÜÊàê3ÈÉ®ÂàÜÔºåË∞ÉÊï¥ $w, b$ Âíå ‰∏ä‰∏ÄÂ±ÇÁöÑËæìÂá∫.\nÁ¨¨‰∏ÄÈöêËóèÂ±ÇÁöÑÊÑüÁü•Êú∫ÁöÑÂÅèÂ∑Æ‰∏éÁ¨¨‰∫åÈöêËóèÂ±ÇÊâÄÊúâÊÑüÁü•Êú∫Áõ∏ÂÖ≥Ôºö\nÁî±Ê≠§ÔºåÊï¥‰∏™Á•ûÁªèÁΩëÁªú‰∏≠ÁöÑÊØè‰∏™ $w$ Âíå b ÈÉΩËÉΩÂàÜÈÖçÂà∞ÂÅèÂ∑ÆÂç†ÊØî„ÄÇ\n‰ª•‰∏äÂà©Áî®ÁöÑÊòØÊï∞ÂÄºÁöÑÂä†Ê≥ïÂàÜÈÖçÂÅèÂ∑ÆÔºåËøòÂèØ‰ΩøÁî®ÂêëÈáèÁöÑÂä†Ê≥ïÊù•ÂàÜÈÖçÂÅèÂ∑ÆÔºå‰∏çËøáÈ¶ñÂÖàË¶ÅÁ°ÆÂÆöÂêëÈáèÁöÑÊñπÂêë„ÄÇ\nÊ¢ØÂ∫¶ÁöÑÂèçÊñπÂêëÂ∞±ÊòØË¶ÅÊâæÁöÑÂêëÈáèÊñπÂêëÔºöÊï∞ÂÄºÂáèÂ∞èÊúÄÂø´ÁöÑÊñπÂêë„ÄÇ Ê¢ØÂ∫¶ÂèØ‰ª•Âú®iÊñπÂêëÂíåjÊñπÂêë‰∏äÂàÜËß£ÔºåÂØπ‰∫éÁÇπ(x,y)Ê≤øÂèòÂåñÁéáÊúÄÂ§ßÁöÑÊñπÂêëÂèòÂåñÂ∞±ÊòØÂú®iÂíåjÊñπÂêë‰∏äÂêåÊ≠•ÂèòÂåñ„ÄÇ\nÂØπ(ËæìÂá∫Â±Ç)ÊçüÂ§±ÂáΩÊï∞ J Ê±ÇÊ¢ØÂ∫¶Ôºö\n$$ \\begin{aligned} \u0026amp;\\nabla J (\\mathbf w^{[3]}, a^{[2]}, b^{[3]}) \\\\ \u0026amp; = (\\alpha, \\beta, \\gamma) \u0026amp; \\text{‰∏â‰∏™ÂàÜÈáèÁöÑÁ≥ªÊï∞ÔºåÁÆÄÁï•Ë°®Á§∫} \\\\ \u0026amp; = \\alpha \\cdot \\mathbf i + \\beta \\cdot \\mathbf j + \\gamma \\cdot \\mathbf k \\end{aligned} $$\nËæìÂá∫Â±ÇÁöÑËæìÂá∫Ê≤øÊ¢ØÂ∫¶ÊñπÂêëÂèòÂåñ$\\eta$Ê≠•ÈïøÔºåÂêëÁõÆÊ†áÂÄºÈù†ËøëÔºö\n$$ \\begin{aligned} \\mathbf w^{[3]}{(\\rm target)} \u0026amp;= \\mathbf w^{[3]} - \\eta \\cdot \\alpha \\ b^{[3]}{(\\rm target)} \u0026amp;= b^{[3]} - \\eta \\cdot \\gamma \\ \\ a^{[2]}{(\\rm target)} \u0026amp;= a^{[2]} - \\eta \\cdot \\beta \u0026amp; ÈúÄË¶ÅÂèçÂêë‰º†ÈÄíÂàÜÈÖçÂà∞Á¨¨2ÈöêËóèÂ±Ç‰∏ä \\ \\cancel{\\eta \\cdot}\\ \\beta \u0026amp;= a^{[2]}{(\\rm target)} - a^{[2]}_{(\\rm now)} \\end{aligned} $$\nÊÑüÁü•Êú∫ËæìÂá∫ÂÄºaÁöÑ‰∏≠Èó¥‰º†ÈÄí‰∏çËÄÉËôë $\\eta$ÔºåÂÅèÂ∑ÆÊúÄÁªàÂàÜÈÖçÂú®ËæìÂÖ•Â±ÇÁöÑwÂíåb‰∏äÔºåÂÖ∂‰∏≠Âê´Êúâ$\\eta$„ÄÇ‰ªéËÄåÂèØ‰ª•ÁúãÂá∫ÈöêËóèÂ±Ç‰∏éËæìÂá∫Â±ÇÁ±ª‰ººÔºå‰πüÊòØÁõÆÊ†á‰∏éËæìÂá∫‰πãÈó¥ÁöÑÂ∑ÆÂÄº„ÄÇÂõ†Ê≠§ÂØπ‰∫éÁ¨¨2ÈöêËóèÂ±ÇÁöÑ\u0026quot;ÊçüÂ§±ÂáΩÊï∞\u0026quot;Ôºö $J^{[2]} = a^{[2]}{(\\rm target)} - a^{[2]}{(\\rm now)}$Ôºå$J^{[2]} (\\mathbf w^{[3]}, a^{[2]}, b^{[3]})$ ÂºÄÂêØ‰∏ã‰∏ÄËΩÆÔºöÊ±ÇÊ¢ØÂ∫¶,Ê±ÇÊçüÂ§±ÂáΩÊï∞\nÂêÑÂ±ÇÁöÑËøêÁÆóÂΩ¢ÂºèÁõ∏ÂêåÔºàÊçüÂ§±ÂáΩÊï∞ÔºâÔºåÂ∞±ÂèØ‰ª•Ëø≠‰ª£ÂàÜÈÖçÂÅèÂ∑Æ„ÄÇJÊ≤øÊ¢ØÂ∫¶ÊñπÂêëÂèòÂåñÂ∞±ÂèØ‰ª•ÊúÄÂø´ÁöÑÂáèÂ∞èÂÅèÂ∑ÆÔºåÂõ†Ê≠§‰ΩøÁî®‰∫ÜÂêëÈáèÁöÑÂä†Ê≥ïÂÅöÂÅèÂ∑ÆÂàÜÈÖç„ÄÇ\nÊ¢ØÂ∫¶ Ê±ÇÂÅèÂØºÂ∞±ÊòØÊ±ÇÔºàÂõ∫ÂÆöÂè¶‰∏Ä‰∏™Áª¥Â∫¶ÔºâÊõ≤Á∫øÁöÑÂàáÁ∫øÔºå‰∏§‰∏™ÂÅèÂØºÁªÑÂêàÂ∞±ÊòØ‰∏§Êù°ÂàáÁ∫øÁªÑÂêàÔºå‰∏§Êù°ÂàáÁ∫øÁ°ÆÂÆö‰∫ÜÂàáÈù¢„ÄÇÊâÄ‰ª•ÂØπÊõ≤Èù¢Ê±ÇÂÅèÂØºÔºåÂ∞±ÊòØÂú®Ê±ÇÂàáÈù¢ Êää‰∏§‰∏™ÂàáÁ∫øÂêàÊàê‰∏Ä‰∏™ÂêëÈáèÔºåÂ∞±ÊòØÊ¢ØÂ∫¶ $\\alpha, \\beta,\\gamma$ ÁöÑÂÖ∑‰ΩìË°®Á§∫ÔºàÊ±ÇÂÅèÂØºÔºâÔºö\n$$ \\begin{aligned} \\alpha \u0026amp;= \\frac{\\partial J}{\\partial \\mathbf w^{[3]}} \\ \\gamma \u0026amp;= \\frac{\\partial J}{\\partial b^{[3]}} \\ \\beta \u0026amp;= \\frac{\\partial J}{\\partial \\mathbf a^{[2]}} \\end{aligned} $$\nÂàÜÈÖçÂÅèÂ∑ÆÔºö\n$$ \\begin{aligned} \\mathbf w^{[3]} \u0026amp;= \\mathbf w^{[3]} - \\eta \\cdot \\frac{\\partial J}{\\partial \\mathbf w^{[3]}} \\ b^{[3]} \u0026amp;= b^{[3]} - \\eta \\cdot \\frac{\\partial J}{\\partial b^{[3]}} \\ \\frac{\\partial J}{\\partial \\mathbf a^{[2]}} \u0026amp;= a^{[2]}{(\\rm target)} - a^{[2]}{(\\rm now)} \u0026amp; \\text{‚ÄúÊñ∞ÊçüÂ§±ÂáΩÊï∞‚Äù} \\end{aligned} $$\n‰ª§$J^{[2]}=a^{[2]}{(\\rm target)} - a^{[2]}{(\\rm now)}$ ‰Ωú‰∏∫‰∏ã‰∏ÄËΩÆÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÂØπÊçüÂ§±ÂáΩÊï∞$J^{[2]}(\\mathbf w^{[2]}, \\mathbf a^{[1]}, b^{[2]}$)ÂàÜÈÖçÔºö\n$$ \\begin{aligned} \\mathbf w^{[2]} \u0026amp;= \\mathbf w^{[2]} - \\eta \\cdot \\frac{\\partial J^{[2]}}{\\partial \\mathbf w^{[2]}} \\ b^{[2]} \u0026amp;= b^{[2]} - \\eta \\cdot \\frac{\\partial J^{[2]}}{\\partial b^{[2]}} \\\n\\frac{1}{n} \\sum_{i=1}{n} \\frac{\\partial J^{[2]}}{\\partial \\mathbf a^{[1]}} \u0026amp;= a^{[1]}{(\\rm target)} - a^{[1]}{(\\rm now)} \u0026amp; \\text{‚Äú$a^{[1]}$ÁöÑÂÅèÂ∑ÆÊòØ4‰∏™$a^{[2]}$ÁöÑÂÅèÂ∑Æ‰πãÂπ≥Âùá‚Äù} \\end{aligned} $$\n‰ª§ ùë±‚ÅΩ¬π‚Åæ=a‚Çç‚Çú‚Çê·µ£·µ®‚Çë‚Çú‚Çé‚ÅΩ¬π‚Åæ - a‚Çç‚Çô‚Çí·µ•‚Çé‚ÅΩ¬π‚Åæ ‰Ωú‰∏∫‰∏ã‰∏ÄËΩÆÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÂØπÊçüÂ§±ÂáΩÊï∞$J^{[1]}(\\mathbf w^{[1]}, \\mathbf a^{[0]}, b^{[1]}$)ÂàÜÈÖçÔºåÂÖ∂‰∏≠ $a^{[0]}$ ÊòØËæìÂÖ•ÔºåÊó†Ê≥ï‰øÆÊîπÔºåÂè™Ë∞ÉÊï¥ w Âíå b„ÄÇ\nÂØπ‰∫éÁ¨¨ $l$ Â±ÇÁöÑÁ¨¨ $i$ ‰∏™ÊÑüÁü•Êú∫ÔºåÊé•Âèó‰∏ä‰∏ÄÂ±ÇÊâÄÊúâÁ•ûÁªèÂÖÉÁöÑËæìÂá∫ $a^{[l-1]}$Ôºå‰∏éÂÆÉÁöÑ $\\mathbf w_i^{[l]}$ Âíå $b_i^{[l]}$ ÂÅöÁ∫øÊÄßËøêÁÆóÂæóÂà∞ $z_{i}^{[l]}$ÔºåÊää z ÈÄÅÂÖ•ÊøÄÊ¥ªÂáΩÊï∞ÂæóÂà∞Ëøô‰∏™ÊÑüÁü•Êú∫ÁöÑËæìÂá∫Ôºö$\\sigma(z_i^{[l]})=a_i^{[l]}$\n$$ \\begin{aligned} z_i^{[l]} \u0026amp;= \\mathbf w_i^{[l]} \\cdot \\mathbf a^{[l-1]} + b_i^{[l]} \\ z_i^{[l]} \u0026amp;= \\begin{bmatrix} \\mathbf w_{i,1}^{[l]} \\ \\vdots \\\\mathbf w_{i,n}^{[l]} \\end{bmatrix}^T \\cdot \\begin{bmatrix} a_{1}^{[l-1]} \\ \\vdots \\ a_{n}^{[l-1]} \\end{bmatrix} + b_i^{[l]} \\end{aligned} $$\nÂØπ‰∏ÄÂ±ÇÊÑüÁü•Êú∫ËøõË°åË°®Ëø∞:\nÊ≤°Êúâ‰∏ãÊ†á $i$ Ë°®Á§∫Êï¥Â±ÇÔºåÁ¨¨$l$Â±ÇÂêÑÊÑüÁü•Êú∫ÁöÑËæìÂá∫Ôºö\n$$ \\begin{aligned} \\mathbf z^{l} \u0026amp;= \\mathbf w^{l} \\cdot \\mathbf a^{l-1} + b^{[l]} \\ \\mathbf z^{l} \u0026amp;= \\begin{bmatrix} z_1^{[l]} \\ \\vdots \\z_i^{[l]} \\end{bmatrix} = \\begin{bmatrix} \\mathbf w_{1,1}^{[l]} \u0026amp; \\cdot \u0026amp; \\mathbf w_{1,j}^{[l]} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\cdots \\ \\mathbf w_{i,1}^{[l]} \u0026amp; \\cdot \u0026amp; \\mathbf w_{i,j}^{[l]} \\ \\end{bmatrix} \\cdot \\begin{bmatrix} a_1^{[l-1]} \\ \\vdots \\ a_j^{[l-1]} \\end{bmatrix} + \\begin{bmatrix} b_1^{[l]} \\ \\vdots \\ b_i^{[l]} \\end{bmatrix} \\\na^{[l]} \u0026amp;=\\sigma\\left(z^{[l]}\\right)=\\sigma\\left(\\left[\\begin{array}{c} z_{1}^{[l]} \\ \\vdots \\ z_{i}^{[l]} \\end{array}\\right]\\right)=\\left[\\begin{array}{c} \\sigma\\left(z_{1}^{[l]}\\right) \\ \\vdots \\ \\sigma\\left(z_{i}^{[l]}\\right) \\end{array}\\right]=\\left[\\begin{array}{c} a_{1}^{[l]} \\ \\vdots \\ a_{i}^{[l]} \\end{array}\\right]\n\\end{aligned} $$\nËæìÂá∫Â±ÇÁöÑÊçüÂ§±ÂáΩÊï∞Ôºö$J(y,a^{[l]})$ÔºåyÊòØlabelÔºå(k)ÊòØÁ¨¨Âá†‰∏™Ê†∑Êú¨Ôºö\nÊØè‰∏™Ê†∑Êú¨$\\mathbf x$ÈÉΩÊúâ j ‰∏™Â±ûÊÄßÔºåÂØπÂ∫îÁ¨¨0Â±ÇÁöÑËæìÂá∫ $a^{[0]}$Ôºö\n$$ a^{[l]}=\\sigma\\left(z^{[l]}\\right)=\\sigma\\left(\\left[\\begin{array}{c} z_{1}^{[l]} \\ \\vdots \\ z_{i}^{[l]} \\end{array}\\right]\\right)=\\left[\\begin{array}{c} \\sigma\\left(z_{1}^{[l]}\\right) \\ \\vdots \\ \\sigma\\left(z_{i}^{[l]}\\right) \\end{array}\\right]=\\left[\\begin{array}{c} a_{1}^{[l]} \\ \\vdots \\ a_{i}^{[l]} \\end{array}\\right] $$\nÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºåÊúâÂ§ö‰∏™ËæìÂá∫ $a·µ¢^{l}$ÔºåÊ≠§Êó∂ÁöÑÊçüÂ§±ÂáΩÊï∞ÊòØÊääÊâÄÊúâËæìÂá∫ËäÇÁÇπÈÉΩËÄÉËôëËøõÊù•„ÄÇ‰∏çËÄÉËôëÂ∏∏Èáè: Ê†∑Êú¨xÂíåÊ†áÁ≠æyÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËæìÂá∫Â±ÇÊÑüÁü•Êú∫ÁöÑÂáΩÊï∞Ôºö\nÂèçÂêë‰º†Êí≠ÔºöÂØπJÊ±ÇÊ¢ØÂ∫¶ÔºåÁªôÂêÑÂèòÈáèÂàÜÈÖçÂÅèÂ∑Æ(ÂÅèÂØº)ÔºåËµ∞$eta$Ê≠•Èïø\nÊääÂØπËæìÂá∫Â±ÇÂêÑÊÑüÁü•Êú∫ÁöÑÂÅèÂØºÁúã‰ΩúÊòØÁ¨¨ $l+1$ Â±ÇÔºåÁ¨¨ l+1 Â±ÇÂè™ÂØπ Á¨¨lÂ±ÇÁöÑ‰∏Ä‰∏™ÊÑüÁü•Êú∫Êúâ‰ΩúÁî®Ôºå$J_1^{[l+1]}$ ÊòØ $\\mathbf w_1^{[l]},\\ a^{l-1},\\ b_1^{l}$ ÁöÑÂáΩÊï∞Ôºö\nÊúÄÂêé$\\nabla J$ÁöÑÂêÑÈ°πÈÉΩ‰ªéËæìÂá∫Â±Ç $a$ ÂºÄÂßãÊ±ÇÂØºÔºàÈìæÂºèÊ±ÇÂØºÔºâÔºö\n$$ \\nabla J_{1}^{[l+1]} = \\left( \\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial W_{1}^{[l]}}, \\quad \\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial a^{[l-1]}}, \\quad \\sqrt{\\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma}} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial b_{1}^{[l]}} \\right) $$\nÂÖ∂‰∏≠ $\\mathbf w$ ÊòØÂêëÈáèÔºåÂØπÂÖ∂Ê±ÇÂÅèÂØºË¶ÅÂØπÂÆÉÁöÑÊØè‰∏™ÂàÜÈáèÊ±ÇÂÅèÂØºÔºö\n$$ \\begin{array}{l} \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1,1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1,2}^{[l]}}, \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1, j}^{[l]}}\\right) \\ \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{1}^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{2}^{[l-1]}}, \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{j}^{[l-1]}}\\right) \\end{array} $$\nÊØèÂ±ÇÁöÑ $\\mathbf w$ Âíå $b$ Ê±ÇÂÅèÂØº‰πãÂêéÂèØÁõ¥Êé•‰øÆÊîπÔºö\n$$ \\begin{array}{c} W_{1}^{[l]}=W_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}} \\quad b_{1}^{[l]}=b_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}} \\ \\vdots \\ W_{i}^{[l]}=W_{i}^{[l]}-\\eta \\cdot \\frac{\\partial J_{i}^{[l+1]}}{\\partial W_{i}^{[l]}} \\quad b_{i}^{[l]}=b_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{i}^{[l+1]}}{\\partial b_{i}^{[l]}} \\end{array} $$\nÂØπ $a^{l-1}$ Ê±ÇÂÅèÂØºÂæóÂà∞ÁöÑÊòØÁ¨¨ $a^{[l-1]}$ Â±ÇÁöÑÂèòÂåñÈáèÔºå‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞Ôºö\n$$ J_{1}^{[l]}=\\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}} \\quad \\cdots \\quad J_{i}^{[l]}=\\frac{\\partial J·µ¢^{[l+1]}}{\\partial a^{[l-1]}} $$\n3‰∏™Á™óÂè£ÂèçÂêëÁßªÂä®ÔºåÂÅö‰∏ã‰∏ÄËΩÆ:\n‰ª•Á¨¨2ÈöêËóèÂ±Ç‰∏∫‰∏≠ÂøÉÔºåËøõË°åÂàÜÊûêÔºö\nÊää a Â±ïÂºÄÔºàÂØπaÁöÑÂêÑÂàÜÈáèÊ±ÇÂÅèÂØºÔºâÔºö\n$$ \\begin{array}{c} \\nabla J_{1}^{[l+1]}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\left.\\partial J_{1}^{[l+1]}\\right]}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\vdots \\ \\nabla J_{k}^{[l+1]}=\\left(\\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\end{array} $$\nÁ¨¨lÂ±ÇÁöÑÊØè‰∏™ÊÑüÁü•Êú∫ÁöÑÂÅèÂ∑ÆÁî±Á¨¨l+1 Â±ÇÁöÑÊâÄÊúâÂÅèÂ∑ÆÂÄºËµã‰∫àÔºö\nÁÑ∂ÂêéÁ¨¨lÂ±ÇÂêÑÂèÇÊï∞ÁöÑÂèòÂåñÈáèÔºö\n$$ \\left(\\Delta W_{1}^{[l]}, \\Delta a^{[l-1]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, \\Delta a^{[l-1]}, \\Delta b_{i}^{[l]}\\right) $$\n$\\delta a$ ‰Ωú‰∏∫‰∏ã‰∏ÄËΩÆÁöÑÊçüÂ§±ÂáΩÊï∞Ôºö\n$$ \\left(\\Delta W_{1}^{[l]}, J_{1}^{[l]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, J_{i}^{[l]} \\quad, \\Delta b_{i}^{[l]}\\right) $$\nÁ¨¨2Ê¨°Ëø≠‰ª£Ôºö\n$a^{[0]}$ ÊòØÂ∏∏ÈáèËæìÂÖ•ÔºåÊ±ÇÂØº‰∏∫Èõ∂ÔºåÊ≠§Êó∂ÁöÑ$J^{l-1}$Âè™‰∏é $\\mathbf w$ Âíå $b$ ÊúâÂÖ≥:\n$$ \\begin{array}{c} \\nabla J_{1}^{[l+1]}= \\left( \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}},\\ \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\vdots \\\n\\nabla J_{k}^{[l+1]}= \\left( \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\ \\left( \\Delta W_{1}^{[l]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, \\Delta b_{i}^{[l]}\\right) \\end{array} $$\n","date":"2022-01-04T23:29:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/06_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/","title":"watch: DL - ÁéãÊú®Â§¥ 06 | Gradient Descent"},{"content":"3 Á∫øÊÄßÂà§Âà´ÂàÜÊûê (FisherÂà§Âà´ÂàÜÊûê) - Ê®°ÂûãÂÆö‰πâ Source Video-P3\nData:\n$$ \\begin{aligned} \\text{Samples:}\\ \u0026amp; \\mathbf X = (\\mathbf x_1, \\mathbf x_2, \\cdots, \\mathbf x_N)^T_{N\\times p} = \\begin{pmatrix} \\mathbf x_1^T \\ \\mathbf x_2^T \\ \\vdots \\ \\mathbf x_N^T \\end{pmatrix}_{N \\times p} \\\\\n\\text{Samples:} \\ \u0026amp; \\mathbf Y = \\begin{pmatrix} y_1 \\ y_2 \\ \\vdots \\ y_N \\end{pmatrix}_{N \\times 1} \\\\\n\\text{Abbreviations:}\\ \u0026amp; \\left{ (\\mathbf x_i, y_i) \\right}_{i=1}^N, \\ \\mathbf x_i \\in \\R^p, \\ \\underset{(\\text{2class: } C_1,C_2)}{y_i \\in {+1, -1}}\\\\\n\\text{2 Group:}\\ \u0026amp;\\mathbf x_{C_1} = { \\mathbf x_i | y_i=+1} ; \\quad \\mathbf x_{C_2} = { \\mathbf x_i | y_i=-1} \\\\\n\\text{Number:}\\ \u0026amp; |\\mathbf x_{C_1}| = N_1, \\ |\\mathbf x_{C_2}|=N_2, \\ N_1 + N_2 = N \\end{aligned} $$\nFisher Á±ªÂÜÖÂ∞èÔºåÁ±ªÈó¥Â§ß\nÊääpÁª¥ÈôçÂà∞1Áª¥ÔºåÊäïÂΩ±Âà∞Êüê‰∏Ä‰∏™ËΩ¥‰∏äÂÜçÂÅöÂàÜÁ±ª„ÄÇ\nÂú®ÊäïÂΩ±ÊñπÂêë($\\mathbf w$)‰∏äÔºåÁ±ªÂÜÖÁÇπÁöÑÂùêÊ†áÊñπÂ∑ÆË∂≥Â§üÂ∞èÔºåÁ±ªÈó¥Ë∑ùÁ¶ªË¶ÅÂ§ß\nÊäïÂΩ±ÊñπÂêëÂ∞±ÊòØÂàÜÁ±ªË∂ÖÂπ≥Èù¢Ôºà$\\mathbf w^T \\mathbf x$ÔºâÁöÑÊ≥ïÂêëÈáè„ÄÇ\nËÆ∞Âè∑\nÈôêÂÆö $| \\mathbf w | =1$\nÊ†∑Êú¨ÁÇπ$\\mathbf x_i$ Âú®ÊäïÂΩ±ËΩ¥$\\mathbf w$‰∏äÁöÑÊäïÂΩ±ÈïøÂ∫¶: $z_i = \\mathbf w^T \\mathbf x_i$\n$$ \\begin{aligned}\nÂùáÂÄº: \\overline{z} =\u0026amp; \\frac{1}{N} \\sum_{i=1}^N z_i = \\frac{1}{N} \\sum_{i=1}^N \\mathbf w^T \\mathbf x_i \\\nÂçèÊñπÂ∑ÆÁü©Èòµ: S_{z} =\u0026amp; \\frac{1}{N} \\sum_{i=1}^N (z-\\overline z)(z-\\overline z)^T \\\n=\u0026amp; \\frac{1}{N} \\sum_{i=1}^N (\\mathbf w^T \\mathbf x_i-\\overline z)(\\mathbf w^T \\mathbf x_i-\\overline z)^T \\\nÁ±ªC_1:\\ \\overline{z_1} =\u0026amp; \\frac{1}{N_1} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\\nS_{z_1} =\u0026amp;\\frac{1}{N_1} \\sum_{i=1}^{N_1} (\\mathbf w^T \\mathbf x_i-\\overline{z_1})(\\mathbf w^T \\mathbf x_i-\\overline{z_1})^T \\\\ Á±ªC_2:\\ \\overline{z_2} =\u0026amp; \\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\\nS_{z_2} =\u0026amp; \\frac{1}{N_2} \\sum_{i=1}^{N_2} (\\mathbf w^T \\mathbf x_i-\\overline{z_2})(\\mathbf w^T \\mathbf x_i-\\overline{z_2})^T \\\\ Á±ªÂÜÖË∑ùÁ¶ª: \u0026amp; S_{z_1} + S_{z_2} \\ Á±ªÈó¥Ë∑ùÁ¶ª: \u0026amp; (\\overline{z_1} - \\overline{z_2})^2 \\\nÁõÆÊ†áÂáΩÊï∞: \u0026amp; J(\\mathbf w) = \\frac{(\\overline{z_1} - \\overline{z_2})^2}{S_{z_1} + S_{z_2}} \\\nÂàÜÂ≠êÔºö\u0026amp; \\left[ \\frac{1}{N_1} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i-\\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\right]^2 \\\n=\u0026amp; \\left[ \\mathbf w^T \\left( \\frac{1}{N_1} \\sum_{i=1}^{N_1}\\mathbf x_i-\\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf x_i \\right) \\right]^2\\\\ =\u0026amp; \\left[ \\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) \\right]^2\\\\ =\u0026amp; \\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}})^T \\mathbf w^T \\\\ S_{z_1}=\u0026amp; \\frac{1}{N_1} \\sum_{i=1}^{N_1} (\\mathbf w^T \\mathbf x_i - \\frac{1}{N_1} \\sum_{j=1}^{N_1} \\mathbf w^T \\mathbf x_j)(\\mathbf w^T \\mathbf x_i - \\frac{1}{N_1} \\sum_{j=1}^{N_1} \\mathbf w^T \\mathbf x_j)^T \\\n=\u0026amp; \\frac{1}{N_1}\\sum_{i=1}^{N_1} \\mathbf w^T (\\mathbf x_i - \\overline{\\mathbf x_{C_1}}) (\\mathbf x_i - \\overline{\\mathbf x_{C_1}})^T \\mathbf w\\\\ =\u0026amp; \\mathbf w^T \\left[ \\frac{1}{N}\\sum_{i=1}^{N_1} (\\mathbf x_i - \\overline{\\mathbf x_{C_1}}) (\\mathbf x_i - \\overline{\\mathbf x_{C_1}})^T \\right] \\mathbf w \\\\ =\u0026amp; \\mathbf w^T S_{C_1} \\mathbf w \\\\ ÂàÜÊØçÔºö\u0026amp; \\mathbf w^T S_{C_1} \\mathbf w + \\mathbf w^T S_{C_2} \\mathbf w\\ =\u0026amp; \\mathbf w^T (S_{C_1} + S_{C_2}) \\mathbf w \\\nJ(\\mathbf w) =\u0026amp; \\frac{\\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}})^T \\mathbf w^T} {\\mathbf w^T (S_{C_1} + S_{C_2}) \\mathbf w} \\ =\u0026amp; \\frac{\\mathbf w^T S_b \\mathbf w}{\\mathbf w^T S_{w} \\mathbf w} \\quad \\text{($S_b$: Á±ªÈó¥ÊñπÂ∑Æ; $S_w$: Á±ªÂÜÖÊñπÂ∑Æ)} \\ =\u0026amp; \\mathbf w^T S_b \\mathbf w \\cdot (\\mathbf w^T S_{w} \\mathbf w)^{-1}\n\\end{aligned} $$\nÊ±Ç: $\\hat{\\mathbf w} = \\underset{\\mathbf w}{\\operatorname{arg\\ max}}\\ J(\\mathbf w)$\n4 Á∫øÊÄßÂà§Âà´ÂàÜÊûê (FisherÂà§Âà´ÂàÜÊûê) - Ê®°ÂûãÊ±ÇËß£ Video-P4\n$$ \\begin{aligned} \\frac{\\partial J(\\mathbf w)}{\\partial \\mathbf w} \u0026amp;= 0 \\\n2 S_b \\mathbf w (\\mathbf w^T S_w \\mathbf w)^{-1} - \\mathbf w^T S_b \\mathbf w (\\mathbf w^T S_w \\mathbf w)^{-2} \\cdot 2S_w \\mathbf w \u0026amp;= 0 \\\nS_b \\mathbf w (\\mathbf w^T S_w \\mathbf w) - \\mathbf w^T S_b \\mathbf w S_w \\mathbf w \u0026amp;= 0 \\\n\\underbrace{\\mathbf w^T}{1\\times p} \\underset{\\in \\R} {\\underbrace{S_b}{p\\times p} \\underbrace{\\mathbf w}{p\\times 1}} S_w \\mathbf w \u0026amp;= S_b \\mathbf w \\underset{\\in \\R} {(\\underbrace{\\mathbf w^T}{1\\times p} \\underbrace{S_w}{p\\times p} \\underbrace{\\mathbf w}{p\\times 1})} \\\nS_w \\mathbf w \u0026amp;= \\frac{\\mathbf w^T S_w \\mathbf w}{\\mathbf w^T S_b \\mathbf w} S_b \\mathbf w \\\n\\mathbf w \u0026amp;= \\frac{\\mathbf w^T S_w \\mathbf w}{\\mathbf w^T S_b \\mathbf w} S_w^{-1} \\cdot S_b \\cdot \\mathbf w \\\nÂè™ÂÖ≥ÂøÉÊñπÂêëÔºö\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot S_b \\cdot \\mathbf w \\\n\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}}) \\underbrace{ (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})^T \\mathbf w }_{1\\times 1 \\in \\R} \\\n\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})\n\\end{aligned} $$\nÂ¶ÇÊûú$S_w^-1$ ÊòØÂØπËßíÁü©ÈòµÔºåËÄå‰∏îÊòØÂêÑÂêëÂêåÊÄßÔºåÂàô $S_w^{-1} \\propto Âçï‰ΩçÁü©ÈòµI$ÔºåÊâÄ‰ª• $\\mathbf w \\propto (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})$\n5 ÈÄªËæëÂõûÂΩí (Logistic Regression) Video-P5\nÁ°¨ËæìÂá∫Ôºö0 Âíå 1ÔºõÊàñËÄÖ +-1\nËΩØËæìÂá∫ÔºöÊ¶ÇÁéá\n9 Êú¥Á¥†Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô® (Naive Bayes Classifer) Video-P9\nÊ†∏ÂøÉÊÄùÊÉ≥ÔºöÊú¥Á¥†Ë¥ùÂè∂ÊñØÂÅáËÆæÔºåÂèàÂè´Êù°‰ª∂Áã¨Á´ãÊÄßÂÅáËÆæÔºåÊúÄÁÆÄÂçïÁöÑÊ¶ÇÁéáÊúâÂêëÂõæÊ®°Âûã\nÂú®ÁªôÂÆöÁ±ªÂà´ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ±ûÊÄßÔºàÁª¥Â∫¶Ôºâ‰πãÈó¥ÊòØÁõ∏‰∫íÁã¨Á´ãÁöÑ\nÈöèÊú∫ÂèòÈáè $y$ ÊòØÈöèÊú∫ÂèòÈáèÔºåÂØπÂ∫î p Áª¥Ëá™ÂèòÈáè\n‰ªéÊ¶ÇÁéáÂõæËßíÂ∫¶Êù•ÁúãÔºåÂú®ÁªôÂÆö y ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªé $x_1$ Âà∞ $x_2$ ÁöÑË∑ØÂæÑË¢´ y ÈòªÊñ≠‰∫ÜÔºåÊâÄ‰ª• $x_1$ Âíå $x_2$ Áã¨Á´ã„ÄÇ\nÊ¶ÇÁéáË°®ËææÂºèÔºö\n$$ P(\\mathbf x | y) = \\prod_{j=1}^p P(x_i | y) $$\nÂÅáËÆæÁöÑÂä®Êú∫Ôºö‰∏∫‰∫ÜÁÆÄÂåñËøêÁÆóÔºå ÂØπ‰∫é $\\mathbf x = (x_1, x_2, \\cdots x_p)^T$ÔºåÂøΩÁï•‰∫Ü $x_i$ ‰∏é $x_j$ ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂ¶ÇÊûúpÈùûÂ∏∏Â§ßÔºåÂØºËá¥ËÆ°ÁÆóÂõ∞Èöæ„ÄÇ\n$$ P(y|x) = \\frac{P(x,y)}{P(x)} = \\frac{P(y)\\cdot P(x|y)}{P(x)} \\propto P(y) \\cdot P(x | y) $$\nÂàÜÁ±ªÊï∞ÊçÆÔºö${(x_i, y_i)}_{i=1}^N, \\ x_i \\in \\R^p, \\ y_i \\in {0,1}$ÔºåÂØπ‰∫é‰∏Ä‰∏™ÁªôÂÆöÁöÑ xÔºåÂØπÂÆÉÂàÜÁ±ªÔºö\n$$ \\begin{aligned} \\hat y =\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ \\underset{ÂêéÈ™å}{\\underline{P{(y|x)}}} \\\n=\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ \\frac{P(x,y)}{P(x)}\\\\ =\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ P(y) \\cdot P(x|y) \\end{aligned} $$\n","date":"2021-12-24T13:58:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/04_%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/","title":"watch: ML - ÁôΩÊùø 04 | Linear Classification"},{"content":"Convolution ÂØπÂÉèÁ¥†ÈáçÊñ∞ËÆ°Êï∞ÔºåÂπ∂ËÆ°ÁÆóÊñ∞ÁöÑ‚ÄúÂÉèÁ¥†ÂÄº‚ÄùÁöÑËøáÁ®ã\nÂç∑ÁßØÊ†∏‰ªéÂ∑¶‰∏äËßíÂºÄÂßãÔºåÊØèÊ¨°ÂêëÂ∑¶ÊàñÂêë‰∏ãÊªëÂä®ÔºåÂπ∂‰∏éÂÖ∂ÈáçÂè†ÁöÑÈÉ®ÂàÜÂÅöÂÜÖÁßØÔºàÂØπÂ∫îÈ°πÁõ∏‰πòÂÜçÊ±ÇÂíåÔºâ\nÊèêÂèñÁâπÂæÅ\n‰∏çÂÅöÂ°´ÂÖÖÔºàvalid paddingÔºâÔºåÂç∑ÁßØÂêéÁöÑËæìÂá∫Â∞∫ÂØ∏‰∏∫ $\\lfloor\\frac{n-k}{s}\\rfloor+1$\nÂõæÂÉèÂ∞∫ÂØ∏Ôºön√ón\nÂç∑ÁßØÊ†∏Â∞∫ÂØ∏Ôºök√ók\nÊ≠•ÈïøÔºös\nÂç∑ÁßØÊ†∏‰ªéÂ∑¶‰∏äËßíÂºÄÂßãÔºåÊØèÊ¨°ÂêëÂ∑¶ÊªëÂä®‰∏ÄÂàóÔºåÊúÄÂêéÂÅúÈù†Âú®Âè≥ËæπÁºòÔºåËøôÊó∂Âç∑ÁßØÊ†∏Â∑¶‰æßÁöÑÂÉèÁ¥†Êï∞ÂÜçÂä†‰∏ä1ÔºàÂΩìÂâçÊ¨°ÔºâÔºåÂ∞±ÊòØËæìÂá∫ÁöÑÂ∞∫ÂØ∏ n-k+1„ÄÇ\nÊØîÂ¶Ç‰∏ãÂõæ‰∏ÄË°åÊúâ5‰∏™ÂÉèÁ¥†Ôºåk=2ÔºåÂç∑ÁßØÊ†∏ÂâçÈù¢Êúâ3‰∏™ÂÜçÂä†‰∏äÊúÄÂêé1‰∏™: 3+1 =4„ÄÇ\nÂ¶ÇÊûúÊ≠•Èïøs=2Ôºå‰∏çËÉΩÊ≠£Â•ΩÊªëÂà∞ÊúÄÂêéÔºåÂèØ‰ª•‰∏¢ÊéâÂ§ö‰ΩôÁöÑÈÉ®ÂàÜÊàñËÄÖÂ°´ÂÖÖÂÉèÁ¥†„ÄÇ2\nÂ¶ÇÊûúÊ≠•Èïøs=3ÔºåËÆ°ÁÆóÂºèÂ∫î‰∏∫Ôºö$\\frac{n-k}{s}+1 = \\frac{5-2}{3}+1 =2$\nÂ¶ÇÊûúÊ≠•Èïøs=4ÔºåËÆ°ÁÆóÂºèÂ∫î‰∏∫Ôºö$\\lfloor\\frac{n-k}{s}\\rfloor+1 = \\lfloor\\frac{5-2}{4}\\rfloor+1 =1$\nÂØπ‰∫é same padding, ËæìÂá∫Â∞∫ÂØ∏Ôºö$\\lfloor \\frac{(n+2\\times p-k)}{s} \\rfloor+1$\nÂ∞±ÊòØÂÖàÂØπÂéüÂßãÂõæÂÉèË°•ÂÖÖ p ÂúàÂÉèÁ¥†ÔºåÂÜçÂÅöÂç∑ÁßØ„ÄÇ\nPadding Âú®ÂõæÂÉèÂ§ñÂõ¥Â°´ÂÖÖ‰∏ÄÂúàÊàñÂá†ÂúàÂÉèÁ¥†ÔºåÂÉèÁ¥†ÂÄºÈÄöÂ∏∏‰∏∫0 ‰øùËØÅËæìÂá∫‰∏éËæìÂÖ•ÁöÑÂ∞∫ÂØ∏‰∏ÄËá¥„ÄÇ1 Â∏∏ËßÅ‰∏§ÁßçpaddingÔºö valid padding: ‰∏çÂ°´ÂÖÖÔºåÂè™‰ΩøÁî®ÂéüÂßãÂõæÂÉè same padding: Â°´ÂÖÖËæπÁºòÔºå‰ΩøÂç∑ÁßØÁªìÊûú‰∏éËæìÂÖ•Â∞∫ÂØ∏‰∏ÄËá¥„ÄÇ\n‰∏∫‰∫Ü‰ΩøËæìÂá∫Â∞∫ÂØ∏‰ªçÁ≠â‰∫énÔºåÂç≥Ôºö$\\frac{n-k+2*p}{s}+1 = n$ÔºåËß£ÂæóÔºö$p=\\frac{(n-1)*s+k-n}{2}$ÔºõÂ¶ÇÊûús=1ÔºåÂàô $p=\\frac{k-1}{2}$„ÄÇ Stride Âç∑ÁßØÊ†∏ÊªëÂä®ÁöÑÊ≠•Èïø s stride=1ÔºåÂàôÂç∑ÁßØÊ†∏ÊØèÊ¨°ÂêëÂ∑¶ÊªëÂä®‰∏ÄÂàóÊàñËÄÖÂêë‰∏ãÊªëÂä®‰∏ÄË°å ÂéãÁº©‰ø°ÊÅØÔºöÊàêÊØî‰æãÁº©Â∞èËæìÂá∫ÁöÑÂ∞∫ÂØ∏Ôºåstride=2ÔºåÂàôËæìÂá∫‰∏∫ËæìÂÖ•ÁöÑ1/2„ÄÇ1 Pooling ‰øùÁïôÁâπÂæÅÔºåÂπ∂ÂáèÂ∞ëËÆ°ÁÆóÈáè max-pooling: ËøëËßÜÁúºÔºåÂè™ËÉΩÁúãÂà∞ÊúÄÂ§ßÁöÑ;\naverage-pooling\n(2023-12-12)\nF.avg_pool3d Number of channels doesn\u0026rsquo;t change, and D, H, W shrink. Docs\n1 2 inp = torch.ones(1, 3, 7, 9, 13) F.avg_pool3d(inp, (4, 2, 3), stride=4, padding=1) # (1,3,2,3,4) C h D n = l 4 1 ‚ãÆ ‚ã± ‚ã± ‚ã± ‚ãÆ ‚ãÆ ‚ãÆ ' C h D n = l 4 2 ‚ãÆ ‚ã± ‚ã± ‚ã± ‚ãÆ ‚ãÆ ‚ãÆ ' C h D n = l 4 3 ‚ãÆ ‚ã± ‚ã± ‚ã± ‚ãÆ ‚ãÆ ‚ãÆ MVSNet uses AvgPool3d to compute the sum of every 4 depth-probability planes:\n1 2 3 4 5 avg4 = F.avg_pool3d( F.pad(prob_volume.unsqueeze(1),# (bs,C=1,D=192,H=128,W=160) pad=(0, 0, 0, 0, 1, 2)), # (bs,C=1,D=195,H=128,W=160) (4, 1, 1), stride=1, padding=0)# (bs,C=1,D=192,H=128,W=160) prob_volume_sum4 = 4 * avg4 Deconvolution Complexity of CNN 3\nConvTranspose2d() 1 torch.nn.ConvTranspose2d() Deconvolution visualization\n(2023-07-19)\ntorchvision.models.resnet34 ResNet - PyTorch | Source code\nlayers: [3,4,6,3] means that layer1 has 3 BasicBlock (resnet50 is Bottleneck) convolution blocks, and layer2 has 4 blocks, and layer3 has 6 blocks, and layer4 has 3 blocks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def _forward_impl(self, img) # img.shape: (3, 300, 400) x = self.conv1(img) # nn.Conv2d(), channels‚Üë and size‚Üì, (1, 64, 150, 200) x = self.bn1(x) # batch norm, shape doesn\u0026#39;t change, (1, 64, 150, 200) x = self.relu(x) # act, shape keeps the same, (1, 64, 150, 200) x = self.maxpool(x) # max-pooling, size‚Üì, (1, 64, 75, 100) # 3 identical \u0026#39;BasicBlock\u0026#39; x = self.layer1(x) # stride in the 1st block is 1, so shape doesn\u0026#39;t change, (1, 64, 75, 100) # 4 identical \u0026#39;BasicBlock\u0026#39; x = self.layer2(x) # stride in the 1st block is 2, so size‚Üì, (1, 128, 38, 50) # 6 identical \u0026#39;BasicBlock\u0026#39; x = self.layer3(x) # stride in the 1st block is 2, so size‚Üì, (1, 256, 19, 25) # 3 identical \u0026#39;BasicBlock x = self.layer4(x) # stride in the 1st block is 2, so size‚Üì, (1, 512, 10, 13) x = self.avgpool(x) # target output is (1,1), so (1, 512, 1, 1) x = torch.flatten(x,1) x = self.fc(x) # 512 -\u0026gt; num_classes (2023-09-12)\nF.pad Padding an image along width, or height, or depth directions. Docs\nThe order of dimensions should be arranged according to Width, Height, Depth, e.g., padding the last 3 dimensions: F.pad(x, (padding_left, padding_right, padding_top, padding_bottom, padding_front, padding_back) )\nSo the order of (l,r,t,b,f,b) is reverse against an image tensor: (Depth, H, W)\nnn.Conv3d Input: (B, Ch_in, D, H, W); Output: (B, Ch_out, D_out, H_out, W_out)\nFor example, a tensor with shape of (2, 3, 4, 224, 224) is 2 video clips with 3 frames and each frame is a 4-channel image with size 224x224.\nAfter convolution with a kernel of size (2, 4, 4), it can be transformed to (B=2, Ch_out=128, D=2, H=56, W=56)\nc o O a n u l v t l o ( l 2 c 4 v ( , h e 2 4 1 c , , = h 2 4 4 n , ) s l f 4 u s r ) o m a o f m f e c c h e h 1 a c 2 h s t e p f r a m e 3 Iterate each channel for D frames to convolve with a unique 3D kernel. Once every channel has multiplied by a kernel, all the 4 weighted channels are summed directly to form one of output channels.\nDepthwise Convolution date: 2023-07-25\nSeparate a convolution into two steps:\nShrink the size of the feature maps using 1-channel plane-wise kernel (Depthwise Conv);\nExpand the number of channels using 1x1 kernel (Pointwise Conv).\nFLOPs reduced, but the IO access increased resulting in slower inference. Depth-wise Convolution - Ê≤àÊôØÂÖµÁöÑÊñáÁ´† - Áü•‰πé\nExpanding channels process costs the equal amount of FLOPs in normal convolution and pointwise convolution. For example, when expanding 3 channels to 256 channels, each pixel performs multiplication 256 times.\nHowever, the depthwise convolution doesn\u0026rsquo;t multiply a kernel by each channel and sum them together, but only multiply a kernel by only one channel. A Basic Introduction to Separable Convolutions - Medium\nFewer parameters: 3x3x253 kernels are replaced with 1x1x256 kernels for every pixel on the resultant feature map.\nReference CNNÂü∫Á°ÄÁü•ËØÜ‚Äî‚ÄîÂç∑ÁßØÔºàConvolutionÔºâ„ÄÅÂ°´ÂÖÖÔºàPaddingÔºâ„ÄÅÊ≠•Èïø(Stride) - G-kdomÁöÑÊñáÁ´† - Áü•‰πé (accessed Dec. 22, 2021). Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂèÇÊï∞ËÆ°ÁÆóÂèäÂç∑ÁßØÂ±ÇËæìÂá∫Â∞∫ÂØ∏ËÆ°ÁÆó - Âá°Â§´‰øóÂ≠êÁöÑÊñáÁ´† - Áü•‰πé Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÁöÑÂ§çÊùÇÂ∫¶ÂàÜÊûê - Michael YuanÁöÑÊñáÁ´† - Áü•‰πé ","date":"2021-12-23T17:29:00Z","permalink":"https://zichen34.github.io/writenotes/calc/dl_conv_layers/","title":"memo: DL | Convolution Layers"},{"content":"‰ªé‚ÄúÂç∑ÁßØ‚Äù„ÄÅÂà∞‚ÄúÂõæÂÉèÂç∑ÁßØÊìç‰Ωú‚Äù„ÄÅÂÜçÂà∞‚ÄúÂç∑ÁßØÁ•ûÁªèÁΩëÁªú‚ÄùÔºå‚ÄúÂç∑ÁßØ‚ÄùÊÑè‰πâÁöÑ3Ê¨°ÊîπÂèò\nÂç∑ÁßØ $\\int_{-\\infin}^{+\\infin} f(\\tau) g(x-\\tau) d\\tau$\n‰∏Ä‰∏™‰∫∫‰∏ÄËæπËøõÈ£ü‰∏ÄËæπÊ∂àÂåñÔºåËøõÈ£üÂáΩÊï∞ f ÊòæÁ§∫‰∫ÜÂêÑÊó∂ÂàªÁöÑËøõÈ£üÈáèÔºö\nÊ∂àÂåñÂáΩÊï∞ g Ë°®Á§∫ËÇöÂ≠êÈáåÂâ©‰ΩôÈ£üÁâ©ÁöÑÊØî‰æãÈöèÊó∂Èó¥ÁöÑÂèòÂåñÔºåÂÆÉ‰∏éÂêÉÂ§öÂ∞ëÊó†ÂÖ≥Ôºö\nÊØîÂ¶ÇË¶ÅÊ±Ç‰∏Ä‰∏™‰∫∫Âú®‰∏ãÂçà2ÁÇπÔºåËÇöÂ≠êÈáåËøòÂâ©Â§öÂ∞ëÈ£üÁâ©ÔºåÂ∞±ÊòØ‰πãÂâçÂêÉÁöÑÊØè‰∏ÄÈ°øÈ•≠ÁªèËøá‰∫Ü‚ÄúÁã¨Á´ã‚ÄùÁöÑÊ∂àÂåñÂêéÂâ©‰ΩôÁöÑÈÉ®ÂàÜÂÜçÊ±ÇÂíåÔºö\n‰∏ÄËà¨ÊÉÖÂÜµÔºåÊ±ÇtÊó∂ÂàªÁöÑËÉÉ‰∏≠ËøòÂâ©Â§öÂ∞ëÈ£üÁâ©ÔºöxÊó∂ÂàªÂêÉÁöÑÈ£üÁâ©f(x)ÔºåÁªèËøá‰∫Ü(t-x)Êó∂Èó¥ÔºåËøòÂâ©‰∏ãÁöÑÊØî‰æãÊòØ$g(t-x)$Ôºå‰πò‰ª•ÊÄªÈáèÂ∞±ÊòØxÊó∂ÂàªÂêÉÁöÑÈ£üÁâ©Âà∞tÊó∂ÂàªËøòÂâ©Â§öÂ∞ë$f(x)\\cdot g(t-x)$ÔºåÂØπt‰πãÂâçÊØè‰∏ÄÊó∂ÂàªÂêÉÁöÑ‰∏úË•øÂà∞‰∫ÜtÊó∂ÂàªËøòÂâ©Â§öÂ∞ëÔºåÂä†Ëµ∑Êù•Ôºö$\\int_0^t f(x) g(t-x) dx$\nÂÖ∂‰∏≠‰∏§ÂáΩÊï∞ÁöÑËá™ÂèòÈáèÁõ∏Âä†ÂêéÔºåxÂ∞±Ë¢´Ê∂àÊéâ‰∫ÜÔºåËøôÊòØÂç∑ÁßØÁöÑ‰∏Ä‰∏™Ê†áÂøó„ÄÇ\nx ‰∏é (t-x) Âú®ÂõæÂÉè‰∏äÁöÑÂØπÂ∫îÔºö\n‰∏Ä‰∏™Á≥ªÁªüÔºåËæìÂÖ•(f)‰∏çÁ®≥ÂÆöÔºåËæìÂá∫(g)Á®≥ÂÆöÔºåÂ∞±ÂèØ‰ª•Áî®Âç∑ÁßØÊù•Ê±ÇËøô‰∏™Á≥ªÁªüÁöÑÂ≠òÈáè\nÂè¶‰∏ÄÁßçÁêÜËß£Ôºö‰πãÂâçÂèëÁîüÁöÑ‰∫ã‰ª∂ÂØπÂΩìÂâç‰∫ã‰ª∂ÁöÑÂΩ±ÂìçÔºåÊØè‰ª∂‰∫ãÁöÑÂΩ±ÂìçÂäõÈöèÊó∂Èó¥ÂèòÂåñÊòØgÔºåÊâÄ‰ª•ÂØºËá¥ t Êó∂Âàª‰∫ã‰ª∂ÁöÑÂèëÁîüÊòØ‰πãÂâçÂêÑÊó∂Âàª‰∫ã‰ª∂ÁöÑÂΩ±ÂìçÂú®tÊó∂ÂàªÁöÑÂíå„ÄÇ\nÂ¶ÇÊûúÂΩ±ÂìçÊòØÈöèË∑ùÁ¶ªÂèòÂåñÁöÑÂáΩÊï∞ÔºåÈÇ£‰πàÂú®Êüê‰ΩçÁΩÆÁöÑ‰∫ã‰ª∂Â∞±ÊòØ‰πãÂâçÂêÑ‰ΩçÁΩÆÂú®Ê≠§‰ΩçÁΩÆ‰∫ßÁîüÁöÑÂΩ±Âìç‰πãÂíå„ÄÇ\nÂßãÁöáÊó¢Ê≤°Ôºå‰ΩôÂ®ÅÈúá‰∫éÊÆä‰øó„ÄÇ \u0026ndash; Ë¥æË∞ä„ÄäËøáÁß¶ËÆ∫„Äã\nÂõæÂÉèÁöÑÂç∑ÁßØÊìç‰Ωú Âë®Âõ¥ÂÉèÁ¥†ÁÇπÂØπÂΩìÂâçÂÉèÁ¥†ÁÇπÁöÑÂΩ±Âìç\n3x3Âç∑ÁßØÊ†∏ËßÑÂÆöÂë®Âõ¥‰∏ÄÂúàÂÉèÁ¥†ÂØπÂΩìÂâçÂÉèÁ¥†ÁÇπÁöÑÂΩ±ÂìçÔºå5x5Â∞±ÊòØÁî®‰∫ÜÂë®Âõ¥2Âúà„ÄÇÂπ≥ÊªëÂç∑ÁßØÊìç‰ΩúÂ∞±ÊòØÊääÂΩìÂâçÂÉèÁ¥†ÂÄºÊõøÊç¢‰∏∫‰∏éÂë®Âõ¥ÂÉèÁ¥†ÁöÑÂπ≥ÂùáÂÄº„ÄÇ\nÂç∑ÁßØÊ†∏‰∏éÂõæÁâáÁöÑÊï∞Â≠¶ËøêÁÆóÔºö$f(x,y)\\star g(m,n) = \\sum f(x,y) \\cdot g(m-x, n-y)$„ÄÇÔºàx‰∏ém-xÁõ∏Âä†Âè™Ââ©mÔºåy‰∏én-yÁõ∏Âä†Âè™Ââ©nÔºå‰∏§‰∏™Áª¥Â∫¶‰∏äÈÉΩÊòØÂç∑ÁßØÔºâ\nÂ¶ÇÊûúÂè™ËÄÉËôëÁÇπ(x,y) Âë®Âõ¥Áõ∏ÈÇª1‰∏™ÂÉèÁ¥†ÂØπÂÆÉÁöÑÂΩ±ÂìçÔºåÂπ∂‰∏îÊØè‰∏™Áõ∏ÈÇªÂÉèÁ¥†ÂØπÂΩìÂâçÂÉèÁ¥†ÁöÑÂΩ±ÂìçÁî±Âç∑ÁßØÊ†∏gËßÑÂÆö„ÄÇÊØîÂ¶ÇË¶ÅÊ±ÇÂÉèÁ¥†f(x-1,y-1)ÂØπf(x,y)ÁöÑÂΩ±ÂìçÔºåÂ∞±ÊòØÂÆÉÊú¨Ë∫´‰πò‰ª•ÂØπÂ∫îÁöÑÊØî‰æãg„ÄÇ Á±ªÊØîÂêÉÈ•≠ÁöÑ‰æãÂ≠êÔºåf(x,y)ÊòØtÊó∂ÂàªÔºåf(x-1,y-1)ÊòØxÊó∂ÂàªÔºåxÊó∂ÂàªÂêÉÁöÑÈ£üÁâ©Âà∞‰∫ÜtÊó∂ÂàªËøòÂâ©‰∏ãÁôæÂàÜ‰πã$g(t-x)$ÔºåÊâÄ‰ª• $f(x-1,y-1)$ ÂØπÂ∫îÁöÑÊØî‰æã‰∏∫ $g(x-(x-1), y-(y-1)) = g(1,1)$ÔºåÂõæÁâáÂÉèÁ¥†‰ΩçÁΩÆf‰∏éÂç∑ÁßØÊ†∏‰∏≠gÁöÑ‰ΩçÁΩÆÂπ∂‰∏ç‰∏Ä‰∏ÄÂØπÂ∫îÔºåËÄåÊòØË¶ÅÊóãËΩ¨180Â∫¶„ÄÇ\ngÂáΩÊï∞‰∏çÁ≠â‰∫éÂç∑ÁßØÊ†∏Ôºö\nÂõæÂÉèÁöÑÂç∑ÁßØÊìç‰ΩúÁúÅÁï•‰∫ÜgÂáΩÊï∞ÁöÑÊóãËΩ¨ÔºåÁõ¥Êé•‰∏éÂç∑ÁßØÊ†∏Áõ∏‰πòÂÜçÁõ∏Âä†\nÂç∑ÁßØÂ±Ç Âç∑ÁßØÊ†∏ÊòØÂ±ÄÈÉ®ÁâπÂæÅÁöÑÊ®°Êùø ‰∏çËÄÉËôëÊüê‰ΩçÁΩÆÂ∞±Â∞ÜÂÖ∂Âç∑ÁßØÊ†∏ÂØπÂ∫î‰ΩçÁΩÆËÆæÁΩÆÊàêÈõ∂ÔºåËÄåË¶ÅÈáçÁÇπËÄÉËôëÊüê‰ΩçÁΩÆÔºåÂ∞±ËÆæÁΩÆÂæóÈ´ò‰∏Ä‰∫õ ‰øùÁïôÂ±ÄÈÉ®ÁâπÂæÅÔºåÂæóÂà∞feature map ÂûÇÁõ¥ËæπÁïåÊª§Ê≥¢Âô®ÂíåÊ∞¥Âπ≥ËæπÁïåÊª§Ê≥¢Âô® Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂèØËßÜÂåñ-github\n","date":"2021-12-23T07:46:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/01_%E5%8D%B7%E7%A7%AF/","title":"watch: DL - ÁéãÊú®Â§¥ 01 | Convolution"},{"content":"‰ªÄ‰πàÊòØ‚ÄúÊÑüÁü•Êú∫‚ÄùÔºåÂÆÉÁöÑÁº∫Èô∑‰∏∫‰ªÄ‰πàËÆ©‚ÄúÁ•ûÁªèÁΩëÁªú‚ÄùÈô∑ÂÖ•‰ΩéÊΩÆ\nÊÑüÁü•Êú∫ ÂàÜÁ±ªÂ∑•ÂÖ∑\nÂè™Êúâ‰∏§‰∏™ËæìÂá∫\n‰∏∫Á∫øÊÄß‰∫åÂàÜÁ±ªÈóÆÈ¢ò(ÂÆö‰πâÂüüÂÖÉÁ¥†Êó†Á©∑Â§öÔºåÂÄºÂüüÂè™Êúâ‰∏§‰∏™ÂÄº)Êèê‰æõ‰∫ÜÊ®°ÊùøÁ≠îÊ°à\n‰∏Ä‰∏™Á∫øÊÄßÂáΩÊï∞ÂÜçÂä†‰∏Ä‰∏™ÊøÄÊ¥ªÂáΩÊï∞\nÁ∫øÊÄßÂáΩÊï∞ÊòØÂØπÊ†áÂáÜÊ®°ÂûãÁöÑÊèèËø∞ÔºåÊøÄÊ¥ªÂáΩÊï∞ÊòØÂà§Êñ≠ËæìÂÖ•Êï∞ÊçÆÊòØÂê¶Á¨¶ÂêàÊ†áÂáÜÊ®°Âûã\nÊÑüÁü•Êú∫Áº∫Èô∑ ÂºÇÊàñËøêÁÆóÊó†Ê≥ïÂÆûÁé∞ ÂºÇÊàñÊ≤°ÂäûÊ≥ïÁ∫øÊÄßÂèØÂàÜÔºöÊó†Ê≥ïÂè™Áî®‰∏ÄÊù°Á∫øÊää0Âíå1Âå∫ÂàÜÂºÄÔºåÂøÖÈ°ªË¶ÅÁîª‰∏Ä‰∏™ÂúàÊâçËÉΩÂ∞Ü0Âíå1Âå∫ÂàÜÂºÄ Â¢ûÂä†ÊÑüÁü•Êú∫Êï∞ÈáèÔºöÈÄöËøá‰∏≠Èó¥Â±ÇÔºåÂêàÂπ∂‰∏§‰∏™Áõ∏ÂêåÁöÑÁä∂ÊÄÅ Ê†∏ÊñπÊ≥ïÂçáÁª¥ 3-‚ÄúÁ•ûÁªèÁΩëÁªú‚ÄùÊòØ‰ªÄ‰πàÔºüÂ¶Ç‰ΩïÁõ¥ËßÇÁêÜËß£ÂÆÉÁöÑËÉΩÂäõÊûÅÈôêÔºüÂÆÉÊòØÂ¶Ç‰ΩïÊó†ÈôêÈÄºËøëÁúüÁêÜÁöÑÔºü\nÁ•ûÁªèÁΩëÁªú Â§öÂ±ÇÊÑüÁü•Êú∫\nËæìÂÖ•Â±ÇÔºåÈöêËóèÂ±ÇÔºåËæìÂá∫Â±Ç\nÂÖ®ËøûÊé•ÁΩëÁªúÔºöÊØè‰∏™ËäÇÁÇπÈÉΩÂíå‰∏ã‰∏ÄÂ±ÇËäÇÁÇπÂÖ®ÈÉ®Áõ∏Ëøû ÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÔºöÊï∞ÊçÆÁöÑ‰º†ÈÄíÊñπÂêëÊòØÂçïÂêëÂêëÂâç‰º†Êí≠ ÊôÆÈÅçÈÄºËøëÂÆöÁêÜÔºöÂè™Ë¶ÅÁ•ûÁªèÁΩëÁªúÊúâÈöêËóèÂ±ÇÔºåÂÆÉÂ∞±ÂèØ‰ª•‰ªªÊÑèÈÄºËøë‰∏Ä‰∏™ËøûÁª≠ÂáΩÊï∞\n‰∏çÂÉèÊÑüÁü•Êú∫ÈÇ£Ê†∑ÔºåÁî®‰∏§‰æßÁöÑÊï∞ÊçÆÔºàÊòØÂê¶ÔºâÊääÂàÜÁïåÁ∫øÂ§πÈÄºÂá∫Êù•\nÂØπ‰∫éÂ§ö‰∏™Á±ªÂà´ÔºåÂõ†‰∏∫Êó†Ê≥ïÊòéÁ°ÆÁöÑÂÆö‰πâÊØè‰∏Ä‰∏™Á±ªÂà´(Áå´ÁöÑÂÆö‰πâÊòØ‰ªÄ‰πàÔºü)ÔºåÊâÄ‰ª•‰∏çÂêåÁ±ªÂà´‰πãÈó¥ÁöÑÁïåÈôê‰∏çÊòØÈªëÁôΩÂàÜÊòéÔºåÊ≤°ÊúâÊòéÁ°ÆÁöÑÊòØÈùûÔºåÂè™ËÉΩËØ¥ÊúÄÊúâÂèØËÉΩÊòØÂì™‰∏ÄÁ±ª„ÄÇÊøÄÊ¥ªÂáΩÊï∞ÈááÁî®sigmoidÂáΩÊï∞ÔºåËÄå‰∏çÊòØÊÑüÁü•Êú∫ÁöÑ0-1Èò∂Ë∑ÉÂáΩÊï∞ÔºåÊääÊòØÈùûÈóÆÈ¢òËΩ¨Âåñ‰∏∫Â•ΩÂùèÈóÆÈ¢ò„ÄÇ\n‰∫∫ÂíåÁ•ûÁªèÁΩëÁªúÈÉΩÁî®Ëá™Â∑±ÁöÑËÆ§Áü•(Ê†áÂáÜ)ÂéªÂà§Êñ≠ÊòØÈùûÔºåÁ•ûÁªèÁΩëÁªúÊääËá™Â∑±ÁöÑÁªìÊûú‰∏é‰∫∫ÁöÑÁªìÊûúËøõË°åÊØîÂØπÔºåË∞ÉÊï¥Ëá™Â∑±ÁöÑÊ®°ÂûãÔºå‰Ωø‰∏§‰∏™Ê®°Âûã‰πãÈó¥ÁöÑÂ∑ÆÂºÇ(ÊçüÂ§±ÂáΩÊï∞)ÊúÄÂ∞è\n","date":"2021-12-22T14:39:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/02-3/","title":"watch: DL - ÁéãÊú®Â§¥ 02 | Perceptron"},{"content":"1. ÊúÄÂ∞è‰∫å‰πòÊ≥ïÂèäÂÖ∂Âá†‰ΩïÊÑè‰πâ Source video: P1\n2. ÊúÄÂ∞è‰∫å‰πòÊ≥ï-Ê¶ÇÁéáËßÜËßí-È´òÊñØÂô™Â£∞-MLE 3. Ê≠£ÂàôÂåñ-Â≤≠ÂõûÂΩí 4. Ê≠£ÂàôÂåñ-Â≤≠ÂõûÂΩí-Ê¶ÇÁéáËßíÂ∫¶-È´òÊñØÂô™Â£∞È´òÊñØÂÖàÈ™å-MAP ","date":"2021-12-21T23:02:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/03-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"watch: ML - ÁôΩÊùø 03 | Linear Regression"},{"content":"1-ËÉåÊôØ\nÊäëÂà∂ËøáÊãüÂêà Â¢ûÂä†Ê†∑Êú¨Êï∞ÊçÆ Ê≠£ÂàôÂåñÔºöÂ¢ûÂä†Á∫¶ÊùüÈôêÂà∂ÂèÇÊï∞Á©∫Èó¥ ÈôçÁª¥ Áª¥Â∫¶ÁÅæÈöæ Êï∞Â≠¶ËßíÂ∫¶Ôºö ÊØîÂ¶ÇÊØèÂ¢ûÂä†‰∏Ä‰∏™‰∫åÂÄºÂ±ûÊÄßÔºåË¶ÅÊÉ≥ÂÆåÂÖ®coverÊ†∑Êú¨Á©∫Èó¥ÔºåÊâÄÈúÄÊ†∑Êú¨Êï∞‰ºö‰ª•2ÁöÑÊåáÊï∞Â¢ûÈïø\nÂá†‰ΩïÊÑè‰πâÔºö Âú®È´òÁª¥Á©∫Èó¥‰∏≠ÔºåÁ´ãÊñπ‰ΩìÁöÑÂÜÖÂàáÁêÉÁöÑ‰ΩìÁßØË∂ãËøë‰∫éÈõ∂Ôºå‰πüÂ∞±ÊòØËØ¥ÊääÁ´ãÊñπ‰ΩìÁöÑÂõõ‰∏™ËßíÂâäÊéâÔºåÂè™Ââ©‰∏ãÂÜÖÂàáÁêÉÔºåÂü∫Êú¨Â∞±‰∏ÄÁÇπ‰∏çÂâ©‰∫ÜÁü•‰πé:Êú∫Âô®Â≠¶‰π†‰∏≠ÁöÑÁª¥Â∫¶ÁÅæÈöæÔºåÂõõ‰∏™ËßíÊâÄÂç†ÊØî‰æã‰∏çÈ´òÔºåÂç¥Êã•ÊúâÂá†‰πéÂÖ®ÈÉ®ÁöÑ‰ΩìÁßØ„ÄÇ ÊâÄ‰ª•Â¶ÇÊûúÂú®È´òÁª¥Á©∫Èó¥‰∏≠Âèñ‰∏ÄË∂ÖÁ´ãÊñπ‰ΩìÔºåÂÖ∂‰∏≠Â≠òÂú®Ê†∑Êú¨ÁöÑÊ¶ÇÁéáÂæà‰ΩéÔºåÂõ†‰∏∫Ê†∑Êú¨Âè™Â≠òÂú®‰∫éÂõõ‰∏™Ëßí‰∏≠ÔºåËøôÂ∞±ÊòØÊï∞ÊçÆÁöÑÁ®ÄÁñèÊÄßÔºåÂπ∂‰∏îÂàÜÂ∏É‰∏çÂùáÂåÄ„ÄÇÂæàÈöæÂÅöÂàÜÁ±ª„ÄÇ\nÁª¥Â∫¶ Ë∂ÖÁ´ãÊñπ‰Ωì‰ΩìÁßØ Ë∂ÖÂÜÖÂàáÁêÉ‰ΩìÁßØ 2 1 œÄ (0.5)¬≤ 3 1 4/3 œÄ (0.5)¬≥ D 1 K(0.5)·¥∞; ÂΩì D‚Üí‚àû, V(Ë∂ÖÁêÉ‰Ωì)‚Üí0 Âá†‰ΩïÊÑè‰πâ2: ‰∏§‰∏™ÂêåÂøÉÂúÜÁöÑÂçäÂæÑÁõ∏Â∑Æ $\\varepsilon \\ (0\u0026lt;\\varepsilon\u0026lt;1)$ÔºåÂÜÖÂúÜÁöÑÂçäÂæÑ‰∏∫ $1-\\varepsilon$ÔºåÂ§ñË∂ÖÁêÉ‰ΩìÁöÑ‰ΩìÁßØ‰∏∫Ôºö$V_Â§ñ=K \\cdot 1^D = K$ÔºõÁéØÂΩ¢Â∏¶ÁöÑ‰ΩìÁßØÔºö$V_{ÁéØÂΩ¢Â∏¶} = V_Â§ñ-V_ÂÜÖ = K - K(1-\\varepsilon)^D$„ÄÇ\n‰∏§‰ΩìÁßØ‰πãÊØîÔºö$\\frac{V_ÁéØ}{V_Â§ñ} = \\frac{K- K(1-\\varepsilon)^D}{K} = 1-(1-\\varepsilon)^D$„ÄÇ ‰∏çËÆ∫$\\varepsilon$ÂèñÂ§öÂ∞èÔºåÂΩìÁª¥Â∫¶Ë∂ã‰∫éÊó†Á©∑Ôºå$\\underset{D\\rightarrow \\infin}{lim} (1-\\varepsilon)^D = 0$Ôºå‰πüÂ∞±ÊòØÊØîÂÄº‰∏∫1ÔºåÁéØÂΩ¢Â∏¶(Â£≥)‰ΩìÁßØÁ≠â‰∫éÂ§ñÁêÉÁöÑ‰ΩìÁßØ ÁêÉÂÜÖÁöÑÊ†∑Êú¨Âè™Â≠òÂú®‰∏éÁêÉÂ£≥‰∏ä\nÁª¥Â∫¶ÁÅæÈöæ‰ºöÂØºËá¥ËøáÊãüÂêà\nÈúÄË¶ÅÈôçÁª¥\nÈôçÁª¥ ÈÅøÂÖçËøáÊãüÂêàÔºåÂáèÂ∞èÊ≥õÂåñËØØÂ∑Æ Áõ¥Êé•ÈôçÁª¥/ÁâπÂæÅÈÄâÊã©: Âè™‰øùÁïôÈáçË¶ÅÁöÑÁª¥Â∫¶; LASSOÂ∏¶Êù•Á≥ªÊï∞ÁöÑÁ≥ªÊï∞ÊÄßÔºå‰ΩøÊüê‰∫õÂ±ûÊÄßÂØπÂ∫îÁöÑÁ≥ªÊï∞Á≠â‰∫é0„ÄÇ Á∫øÊÄßÈôçÁª¥: PCA, MDS ÈùûÁ∫øÊÄßÈôçÁª¥: ÊµÅÂΩ¢ÔºàISOmap, LLEÔºâ 2-Ê†∑Êú¨ÂùáÂÄº\u0026amp;Ê†∑Êú¨ÊñπÂ∑ÆÁü©Èòµ\nData:\n$$ \\mathbf X_{p\\times 1} = (\\mathbf x_1, \\mathbf x_2, \\cdots, \\mathbf x_N)^T_{N\\times p} = \\begin{pmatrix} \\mathbf x_1^T \\ \\mathbf x_2^T \\ \\vdots \\ \\mathbf x_N^T \\end{pmatrix}_{N \\times p},\\quad\n\\mathbf x_i \\in \\R^p,\\ i=1, 2, \\cdots, N $$\nSample Mean:\n$$ \\begin{aligned} \\bar{\\mathbf X} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\mathbf x_i \\ \u0026amp; = \\frac{1}{N} (\\mathbf x_1, \\mathbf x_2,\\cdots, \\mathbf x_N) \\begin{pmatrix} 1 \\ 1 \\ \\vdots \\ 1 \\end{pmatrix}_{N\\times 1} \\ \u0026amp; = \\frac{1}{N} \\ \\mathbf X^T \\ \\mathbf 1_N\n\\end{aligned} $$\nSample Covariance:\n$$ S = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X})^2 \\quad (‰∏ÄÁª¥Ê†∑Êú¨) $$\n$$ \\begin{aligned} S_{p\\times p} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X}) (\\mathbf x_i - \\bar{\\mathbf X})^T \\quad (pÁª¥Ê†∑Êú¨) \\\n\u0026amp; = \\frac{1}{N} (\\mathbf x_1 - \\bar{\\mathbf X} \\quad \\mathbf x_2 - \\bar{\\mathbf X}\\ \\cdots \\ \\mathbf x_N - \\bar{\\mathbf X}) \\begin{pmatrix} (\\mathbf x_1 - \\bar{\\mathbf X})^T \\ (\\mathbf x_2 - \\bar{\\mathbf X})^T \\ \\vdots \\ (\\mathbf x_N - \\bar{\\mathbf X})^T \\end{pmatrix} \\\n\u0026amp; = \\frac{1}{N} \\left[(\\mathbf x_1 \\ \\mathbf x_2 \\cdots \\mathbf x_N) - \\mathbf{\\bar{X}} \\ (1 \\ 1 \\cdots 1)\\right] (\\mathbf x_i - \\bar{\\mathbf X})^T \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T_{p\\times N} - \\frac{1}{N} \\mathbf{X}^T \\mathbf 1_N \\ \\mathbf 1_N^T \\ ]\\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) ]\\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\quad \\text{($I_N$ÊòØNxNÊñπÈòµ)} \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T \\underline{(I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T)} ] \\cdot [ \\underline{(I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T)^T} \\mathbf X] \\\n\u0026amp; = \\frac{1}{N} \\mathbf X^T H \\cdot H^T \\mathbf X \\ \u0026amp; = \\frac{1}{N} \\mathbf X^T H \\mathbf X\n\\end{aligned} $$\nH ÊòØ‰∏≠ÂøÉÁü©ÈòµÔºåÊääÊï∞ÊçÆÁöÑÂùáÂÄºÁßªÂä®Âà∞ÂéüÁÇπ(‰∏≠ÂøÉÂåñ).\n$$ \\begin{aligned} H \u0026amp;= (\\mathbf I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) \\ H^T \u0026amp;= (\\mathbf I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) =H \u0026amp; (ÂØπÁß∞ÊÄß)\\ H^2 \u0026amp;= H \\cdot H \u0026amp; (ÂπÇÁ≠âÊÄß)\\ \u0026amp;= (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) \\ \u0026amp;= I_N - \\frac{2}{N} \\mathbf 1_N \\mathbf 1_N^T + \\frac{1}{N^2} 1_N \\mathbf 1_N^T 1_N \\mathbf 1_N^T \\\n\u0026amp;= I_N - \\frac{2}{N} \\begin{pmatrix} 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{pmatrix} + \\frac{1}{N^2} \\begin{pmatrix} N \u0026amp; N \u0026amp; \\cdots \u0026amp; N \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ N \u0026amp; N \u0026amp; \\cdots \u0026amp; N \\\\ \\end{pmatrix} \\\\ \u0026amp;= I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T \\\\ \u0026amp;= H \\\\ H^n \u0026amp;= H \\end{aligned} $$\n3 PCA-ÊúÄÂ§ßÊäïÂΩ±ÊñπÂ∑Æ\nÁªèÂÖ∏PCA ‰∏Ä‰∏™‰∏≠ÂøÉÔºå‰∏§‰∏™Âü∫Êú¨ÁÇπ\nÊ†∏ÂøÉÔºöÂ∞Ü‰∏ÄÁªÑÂèØËÉΩÁ∫øÊÄßÁõ∏ÂÖ≥ÁöÑÂèòÈáèÔºåÈÄöËøáÊ≠£‰∫§ÂèòÊç¢ÔºåÂèòÊç¢Êàê‰∏ÄÁªÑÁ∫øÊÄßÊó†ÂÖ≥ÁöÑÂèòÈáè/Âü∫/ÊäïÂΩ±ÊñπÂêëÔºàÂØπÂéüÂßãÁâπÂæÅÁ©∫Èó¥ÁöÑÈáçÊûÑÔºâ\nÂü∫Êú¨ÁÇπÔºöÊúÄÂ§ßÊäïÂΩ±ÊñπÂ∑ÆÔºõÊúÄÂ∞èÈáçÊûÑË∑ùÁ¶ªÔºà‰∏§ÁßçËßíÂ∫¶,ÊïàÊûúÁõ∏ÂêåÔºâ\nÊúÄÂ§ßÊäïÂΩ±ÊñπÂ∑Æ ÊúÄËÉΩ‰ΩìÁé∞ÂéüÊù•Ê†∑Êú¨ÁöÑÂàÜÂ∏É Steps: ‰∏≠ÂøÉÂåñÔºöÊääÊ†∑Êú¨ÂùáÂÄºÁßªÂä®Âà∞ÂéüÁÇπ ($\\mathbf x_i - \\bar{\\mathbf X}$)\nÊ†∑Êú¨ÁÇπÂú® $\\mathbf u_1$ ÊñπÂêë‰∏äÁöÑÊäïÂΩ±Ôºå‰πüÊòØÂú®$\\mathbf u_1$ÊñπÂêë‰∏äÁöÑÂùêÊ†áÔºö\n$$ (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 $$\nÂÖ∂‰∏≠ $| \\mathbf u_1| = 1$ (Êàñ$\\mathbf u_1^T \\mathbf u_1 = 1$)ÔºåÊâÄ‰ª•ÂÜÖÁßØÁ≠â‰∫éÊäïÂΩ±„ÄÇ ‰∏§‰∏™ÂêëÈáèÁöÑÂÜÖÁßØÂÜôÊàê‰∏Ä‰∏™ÂêëÈáèÁöÑËΩ¨ÁΩÆ‰πò‰ª•Âè¶‰∏Ä‰∏™ÂêëÈáèÔºå$\\mathbf a_{p\\times 1} \\cdot \\mathbf b_{p \\times 1} = \\mathbf a^T_{1\\times p} \\ \\mathbf b_{p \\times 1} = n_{1\\times 1}$\nÊäïÂΩ±ÊñπÂ∑ÆÔºöÂõ†‰∏∫ÂùáÂÄºÂ∑≤Áªè‰∏∫0ÔºåÊäïÂΩ±Áõ¥Êé•Âπ≥Êñπ\n$$ \\begin{aligned} J \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\left( (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 \\right)^2 \\\n\u0026amp;= \\sum_{i=1}^N \\ \\frac{1}{N} \\ \\mathbf u_1^T (\\mathbf x_i - \\bar{\\mathbf X}) \\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 \\\n\u0026amp;= \\mathbf u_1^T \\left(\\frac{1}{N} \\ \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X}) \\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\right) \\mathbf u_1 \\\n\u0026amp;= \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 \\end{aligned} $$\nÊâæÂà∞‰Ωø J ÊúÄÂ§ßÁöÑÊñπÂêë $\\mathbf u_1$\n$$ \\begin{cases} \\hat \\mathbf u_1 = \\operatorname{arg\\ max}\\ \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 \\ s.t. \\quad \\mathbf u_1^T \\mathbf u_1 = 1 \\end{cases} $$\nÂ∏¶Á∫¶ÊùüÁöÑ‰ºòÂåñÈóÆÈ¢òÔºåÁî®ÊãâÊ†ºÊúóÊó•‰πòÂ≠êÊ≥ïÔºåÂÜôÂá∫ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Ôºö\n$$ L(\\mathbf u_1, \\lambda) = \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 + \\lambda (1 - \\mathbf u_1^T \\mathbf u_1) $$\nÊ±ÇÂØºÔºö\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf u_1} = 2 S \\cdot \\mathbf u_1 \u0026amp;- \\lambda \\cdot 2 \\mathbf u_1 = 0 \\ S \\underbrace{\\mathbf u_1}{\\text{Eigen vector}} \u0026amp;= \\underbrace{\\lambda}{\\text{Eigen value}} \\mathbf u_1 \\end{aligned} $$\n4-PCA-ÊúÄÂ∞èÈáçÊûÑ‰ª£‰ª∑\nÊúÄÂ∞è‰ª£‰ª∑ÈáçÊûÑ ‰ªéÈáçÊûÑÁ©∫Èó¥ÊÅ¢Â§çÂà∞ÂéüÂßãÁ©∫Èó¥Ôºå‰ª£‰ª∑ÊúÄÂ∞è Steps: ÂêëÈáè$\\mathbf x_i$Âú®Êñ∞ÁöÑÁâπÂæÅÁ©∫Èó¥‰∏≠ÁöÑË°®Á§∫Ôºö\n$$ \\begin{aligned} \\mathbf x_i \u0026amp;= ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1)\\cdot \\mathbf u_1 + ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_2)\\cdot \\mathbf u_2 + \\cdots + ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_p)\\cdot \\mathbf u_p \\ \u0026amp;= \\sum_{k=1}^p ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k \\end{aligned} $$\nÈôçÁª¥ÔºöÊ†πÊçÆÁâπÂæÅÂÄºÔºåÂèñÂâçq‰∏™ÊúÄÂ§ßÁöÑÁâπÂæÅÂêëÈáè(ÊñπÂêë)„ÄÇ\n$$ \\hat{\\mathbf x}i = \\sum{k=1}^q ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k $$\nÈáçÊûÑ‰ª£‰ª∑: $| \\mathbf x_i - \\hat{\\mathbf x}_i |^2$\nN‰∏™Ê†∑Êú¨ÁöÑÈáçÊûÑ‰ª£‰ª∑ÊúÄÂ∞èÔºö\n$$ \\begin{aligned} J \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N | \\mathbf x_i - \\hat{\\mathbf x}i |^2 \\ \u0026amp;= \\frac{1}{N} \\sum{i=1}^N | \\sum_{k=q+1}^p ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k |^2 \\\n\u0026amp;= \\frac{1}{N} \\sum_{i=1}^n \\sum_{k=q+1}^p \\left( (\\mathbf x_i - \\bar{\\mathbf x})^t \\mathbf u_k \\right)^2 \\quad \\text{(ÂêëÈáèÁöÑÊ®°Á≠â‰∫éÂùêÊ†áÁöÑÂπ≥ÊñπÂíå)} \\\n\u0026amp;= \\sum_{k=q+1}^p \\underline{ \\sum_{i=1}^n \\frac{1}{N} \\left( (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k \\right)^2 } \\\n\u0026amp;= \\sum_{k=q+1}^p \\mathbf u_k^T \\cdot S \\cdot \\mathbf u_k \\qquad\\ \\rm s.t.\\ \\mathbf u_k^T \\mathbf u_k = 1 \\\n\\end{aligned} $$\nÊúÄ‰ºòÂåñÈóÆÈ¢òÔºö\n$$ \\begin{cases} \\mathbf u_k = \\operatorname{arg\\ min} \\sum_{k=q+1}^p \\mathbf u_k^T \\cdot S \\cdot \\mathbf u_k \\ s.t. \\quad \\mathbf u_k^T \\mathbf u_k = 1 \\end{cases} $$\nÂõ†‰∏∫ÂêÑÁâπÂæÅÂêëÈáè‰∫í‰∏çÁõ∏ÂÖ≥ÔºåÊâÄ‰ª•ÂèØ‰ª•‰∏Ä‰∏™‰∏Ä‰∏™Ëß£Ôºå‰πüÂ∞±ÊòØÊ±ÇÂâ©‰ΩôÁöÑÊØè‰∏™ÁâπÂæÅÂêëÈáèÁöÑÊúÄÂ∞èÈáçÊûÑ‰ª£‰ª∑ÂØπÂ∫îÁöÑÁâπÂæÅÂÄº$\\lambda$\n$$ \\begin{cases} \\operatorname{arg\\ min} \\mathbf u_{q+1} \\cdot S \\cdot \\mathbf u_{q+1}\\ s.t. \\quad \\mathbf u_{q+1}^T \\ \\mathbf u_{q+1} = 1 \\end{cases} $$\n$$ J = \\sum_{i=q+1}^p \\lambda_i $$\nÂΩìJÊúÄÂ∞èÊó∂ÔºåÂØπÂ∫îÁöÑÂ∞±ÊòØÊúÄÂ∞èÁöÑÂá†‰∏™ÁâπÂæÅÂÄº\n5-SVDËßíÂ∫¶ÁúãPCAÂíåPCoA\nPCA ÊâæÊúÄÂ§ßÁöÑÊäïÂΩ±ÊñπÂêë(ÁâπÂæÅÂêëÈáè)ÔºåÂ∞±ÊòØ‰∏ªÊàêÂàÜ\nÊ±ÇËß£‰∏ªÊàêÂàÜÔºöÂØπÊñπÂ∑ÆÁü©ÈòµÂÅöÁâπÂæÅÂÄºÂàÜËß£Ôºö$S = G K G^T$ÔºàÂõ†‰∏∫SÊòØÂØπÁß∞Áü©ÈòµÔºåÊâÄ‰ª•ÂÆÉÁöÑÂ•áÂºÇÂÄºÂàÜËß£Â∞±ÊòØÁâπÂæÅÂÄºÂàÜËß£ÔºâÔºå ÂÖ∂‰∏≠ÁâπÂæÅÂêëÈáèÊòØÊ≠£‰∫§ÁöÑ: $G^T G = I$ÔºõKÊòØÂØπËßíÁü©ÈòµÔºåÂÖÉÁ¥†ÈÉΩÊòØÁâπÂæÅÂÄºÔºåÂÖ∂ÊéíÂàóÊª°Ë∂≥Ôºö $k_1 \u0026gt; k_2 \u0026gt; \\cdots \u0026gt; k_p$„ÄÇÈôçÂà∞qÁª¥ÔºåÂ∞±ÂèñÂâç q ‰∏™ÂÄºÔºå‰Ωú‰∏∫GÁöÑq‰∏™ÂàóÂêëÈáè„ÄÇ\n$$ K= \\begin{pmatrix} k_1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; k_2 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; k_p \\end{pmatrix} $$\nÊé¢Á¥¢‰∏Ä‰∏ãÔºö\nÂØπ‰∏≠ÂøÉÂåñ‰πãÂêéÁöÑÂéüÂßãÊï∞ÊçÆÂÅöSVDÂ•áÂºÇÂÄºÂàÜËß£Ôºö\n$$ H X = U \\Sigma V^T \\rightarrow SVD: \\begin{cases} U^T U = I \u0026amp; \\text{(ÂàóÊ≠£‰∫§)} \\ V^T V = V V^T = I \u0026amp; \\text{(Ê≠£‰∫§)} \\ \\Sigma \u0026amp; \\text{(ÂØπËßí)} \\end{cases} $$\nÁÑ∂Âêé‰ª£ÂÖ•ÂçèÊñπÂ∑ÆÁü©ÈòµÔºàÊé®ÂØºÁúÅÁï•Â∏∏Êï∞$\\frac{1}{N}$ÔºâÔºö\n$$ \\begin{aligned} S_{p\\times p} \u0026amp;= X^T H X \\ \u0026amp;= X^T H^T H X \u0026amp; \\text{(Á≠âÂπÇÊÄß)} \\ \u0026amp;= V \\Sigma \\underline{U^T \\cdot U} \\Sigma V^T \\ \u0026amp;= V \\Sigma I \\Sigma V^T \u0026amp; \\text{(UÂàóÊ≠£‰∫§)}\\ \u0026amp;= V \\Sigma^2 V^T \u0026amp; \\text{($\\Sigma$ÂØπËßíÈòµ)} \\end{aligned} $$\nÂ∞±Áõ∏ÂΩì‰∫éÂØπ S ÂÅö‰∫ÜÂ•áÂºÇÂÄºÂàÜËß£‰∫ÜÔºåÂØπÂ∫î‰∫é‰∏äÈù¢ S ÁöÑÁâπÂæÅÂÄºÂàÜËß£Ôºö\n$$ ÁâπÂæÅÂêëÈáèG = V, ÁâπÂæÅÂÄºK = \\Sigma^2 $$\nÊâÄ‰ª•Ôºå‰∏çÁî®Áõ¥Êé•ÂØπ SÂÅöÁâπÂæÅÂÄºÂàÜËß£ÔºåÁõ¥Êé•ÂØπÊï∞ÊçÆÂÅöÂÆå‰∏≠ÂøÉÂåñ‰πãÂêéÔºåÂÅöÂ•áÂºÇÂÄºÂàÜËß£ÔºåÂ∞±ÂèØ‰ª•ÂæóÂà∞ÁâπÂæÅÂêëÈáèV„ÄÇ\nÂÆö‰πâ‰∏Ä‰∏™Áü©Èòµ TÔºàSÂèçËøáÊù•ÔºåÂØπÊï∞ÊçÆÂÜÖÁßØÂàÜËß£Ôºâ:\n$$ \\begin{aligned} T_{N\\times N} \u0026amp;= H X X^T H^T \\ \u0026amp;= U \\Sigma V^T \\cdot V \\Sigma U^T \\ \u0026amp;= U \\Sigma^2 U^T \\end{aligned} $$\nT Âíå S ÊúâÁõ∏ÂêåÁöÑÁâπÂæÅÂÄº(eigen value): $\\Sigma^2$„ÄÇ\nPCAÔºöÂÖàÂØπ S ÂÅöÁâπÂæÅÂÄºÂàÜËß£ÔºåÊâæÂà∞‰∫Ü‰∏ªÊàêÂàÜÔºàÁâπÂæÅÂêëÈáè/ÊäïÂΩ±ÊñπÂêëÔºâÔºõÁÑ∂ÂêéÊ†∑Êú¨ÁÇπ $HX$ ‰πò‰ª•ÊñπÂêëÂêëÈáè$V$ÔºàÊäïÂΩ±ÔºâÔºåÂæóÂà∞ÂêÑÊñπÂêë‰∏äÁöÑÂùêÊ†á„ÄÇ ÂùêÊ†áÁü©ÈòµÔºö$HX \\cdot V = U \\Sigma \\underline{V^T \\cdot V} = U \\Sigma$\nËÄåÂØπTÂÅöÁâπÂæÅÂàÜËß£ÔºåÂèØ‰ª•Áõ¥Êé•ÂæóÂà∞ÂùêÊ†áÔºåËøôÂè´‰∏ªÂùêÊ†áÂàÜÊûêÔºàPCoAÔºâ\nÂØπT‰∏§ËæπÂ∑¶‰πò$U\\Sigma$ÔºàÂÅö‰∏Ä‰∏™Áº©ÊîæÔºâÔºö $$ \\begin{aligned} T \u0026amp;= U \\Sigma^2 U^T \\ T U \\Sigma \u0026amp;= U \\Sigma^2 \\underline{U^T U} \\Sigma \\ \u0026amp;= U \\Sigma^3 \\ T \\underbrace{U \\Sigma}{ÁâπÂæÅÂêëÈáè} \u0026amp;= U \\Sigma \\underbrace{\\Sigma^2}{ÁâπÂæÅÂÄº} \\end{aligned} $$\n‰πüÂ∞±ÊòØËØ¥ÔºåÂØπTÂÅöSVDÂ•áÂºÇÂÄºÂàÜËß£ÂêéÔºåÁõ¥Êé•ÂæóÂà∞ÁöÑÁâπÂæÅÂêëÈáèÂ∞±ÊòØÂùêÊ†á„ÄÇ\nÂ¶ÇÊûúÊï∞ÊçÆÁöÑÁª¥Â∫¶Â§™È´òÔºå$S_{p\\times p}$ ‰∏çÂ•ΩËÆ°ÁÆóÔºåÂèØ‰ª•ÂØπ$T_{N\\times N}$ÂàÜËß£„ÄÇ\n6-Probablistic PCA\n","date":"2021-12-16T13:33:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/05_%E9%99%8D%E7%BB%B4/","title":"watch: ML - ÁôΩÊùø 05 | Dimensionality Reduction"},{"content":"Video 7 2021-10-04\n[toc]\nDimensionality Reduction Avoids the curse of Dimensionality October 6, 2021\nCurse of Dimensionality When dimensionality increases, data becomes increasingly sparse Concepts become less meaningful: density and distance Subspace combinations grow very fast Dimentionality Reduction Eliminate irrelevant features and reduces noise\n$X$ is a set of $N$ features: $X={X_1, X_2, \\cdots X_N}$Ôºåa reduced set $X\u0026rsquo;$ is a transformation of $X$ and consists of $d$ features so that $d\u0026lt;N$:\n$$ X\u0026rsquo; = T(X) = { X_1\u0026rsquo;,\\ X_2\u0026rsquo;,\\ \\cdots,\\ X_d\u0026rsquo;} \\ T: \\R^N \\rightarrow \\R^d,\\ d\u0026lt;N $$\nAvoids the curse of dimensionality. Reduces time and space required for computations.\u0026gt;\nTwo ways:\nFeature Extraction: transformation to a lower dimension Wavelet transforms and PCA Feature Selection: transformation is limited to only selection from original features Filters, Wrappers, Embedded. 3 Features Relevant feature is neither irrelevant nor redundant to the target concept. Irrelevant feature is useless information for the learning task, and may causes greater computational cost and overfitting Redundant feature is duplication of information that has contained in other features. Feature Selection Assume a binary classification model, X ‚Üí Model ‚Üí Y ‚àà {0, 1}, where X consists of N different features, e.g., age, weight, temperature, blood pressure, etc. X = {X1, X2, X3 , . . . , XN } N could be small, or relatively large value, e.g., an image of size of 300 √ó 300.\nClass Separation Criterion Evaluation of data separation result based on selected features.\n$$ \\begin{aligned} ÂèòÊç¢ \\quad \u0026amp; T: \\R^N ‚Üí \\R^d, \\quad d\u0026lt;N \u0026amp; \\text{(ÊääÊï∞ÊçÆ‰ªéNÁª¥ÈôçÂà∞dÁª¥)} \\ ÂàÜÁ¶ªÁªìÊûú\\quad \u0026amp; X\u0026rsquo;= T(X) \\ ÂàÜÁ¶ªËØÑ‰ª∑\\quad \u0026amp; J(X\u0026rsquo;,Y) = 1-Œµ^* (X\u0026rsquo;,Y) \u0026amp; \\text{(Y: class distribution; ŒµÊòØX\u0026rsquo;‰∏éYÁöÑËØØÂ∑Æ)} \\end{aligned} $$\nÊ†πÊçÆÊúÄÂ§ßÂåñ J ÊàñËÄÖÊúÄÂ∞èÂåñŒµÊù•ËÆæËÆ°hÔºåÂÆûÁé∞Êõ¥Â•ΩÁöÑÊ†∑Êú¨ÂàÜÁ¶ª\nFeature Selection Find the optimal subset of d features $S^$ from N features, according to $S^ = \\underset{|S|=d}{\\operatorname{arg,max}}J(S)$\nOptimal number of features bring lowest error of model\nFSÂåÖÊã¨‰∏§ÈÉ®ÂàÜÔºö ÊêúÁ¥¢Á≠ñÁï• Search Strategy Âíå ÁõÆÊ†áÂáΩÊï∞ Objective Function\nSearch Strategy: Ëã• d Âõ∫ÂÆö‰ºöÊúâ$C_d^N$Ê¨°ËØÑ‰ª∑ÔºõËã• d ‰πüÈúÄ‰ºòÂåñÔºåÂ∞±Êúâ$2^N$Ê¨°ËØÑ‰ª∑ÊØîËæÉÔºå‰∏∫‰∫ÜÂú®ÊâÄÊúâÁöÑÁâπÂæÅÁªÑÂêà‰πã‰∏≠ÊêúÁ¥¢ÔºåÈúÄË¶Å‰∏Ä‰∏™ÊêúÁ¥¢Á≠ñÁï•Êù•ÂºïÂØº„ÄÇ Objective Function: ÈáèÂåñÁâπÂæÅÂ≠êÈõÜÁöÑÂ•ΩÂùèÔºå‰ª•‰æõsearch strategy Âà§Êñ≠ Importance:\nReduce computational complexity, run-time, required storage. FS can get meaningful and explainable rules from model based on raw data with real meaning. Building better generalizable model. FS can be applied for non-numerical features, which cannot be transformed easily. Á±ªÂàÜÁ¶ªËØÑ‰ª∑Ê†áÂáÜÔºö$J(X^S,Y) = J(S)$Ë∂äÂ§ßË∂äÂ•ΩÔºåÁî±Bayes error, classifier errorÊàñMahalanobis distanceÂÆö‰πâ„ÄÇËØØÂ∑ÆË∂äÂ∞èÔºåJË∂äÂ§ß„ÄÇ\nSearch Strategies Search sequence of N features Find the subset of d features from all possible combination of features quickly Three major categoriesÔºö Sequential algorithms: Add or remove features sequentially\nShortcoming: a tendency to become trapped in local minima\nSequential Forward Selection Sequential Backward Selection Plus-L Minus-R Selection Bidirectional Search Sequential Floating Selection Exponential algorithms: Used to evaluate a number of subsets that grows exponentially with the dimensionality of the search space.\nExhaustive Search Branch and Bound Approximate Monotonicity with a Branch and Bound Beam Search Randomized algorithms: Incorporate randomness into search procedure to escape from local minima\nRandom Generation plus Sequential Selection Objective Function Evaluation of subset goodness ‰∏âÂ§ßÁ±ªÔºö Filter Methods: statistical analysis without using predictive model statistical dependence, information gain, Chi square, log likelihood ratio, interclass distance or information-theoretic measures Wrapper Methods: pre-determined predictive models or classifiers Hybrid Methods: complement of wrapper and filter approaches Filters Approaches select d features greedy which are used for training a predictive model $h_M$ with M samples\n$$ X^N \\overset{FS}{\\longrightarrow} X^d \\overset{h_m}{\\longrightarrow}Y $$\nWhy is it?\nEvaluation is independent of the predictive models or classifiers. Objective function evaluate the information content and statistical measures of feature subsets Role: evaluate each feature individually or a batch of features\nMajor steps:\nEvaluating and ranking features Choosing the features with the highest ranks to induce models Advantages:\nFast Execution: non-iterative computation is faster than training session of predictive models Generality: evaluate intrinsic properties of the data, rather than their interactions with a particular predictive model. So the final subset is general for any subsequent predictive models Disadvantages:\nTendency to select large subsets: more features will make the monotonic objective functions larger Independence: ignore the performance on predictive models Wrapper Approches a predictive model $h_M$ is trained to find the best subset $S^*$\n‰∏Ä‰∏™È¢ÑÊµãÊ®°ÂûãË¢´ÂåÖÂú®‰∫Ü‚ÄúÈÄâÊã©Á≥ªÁªü‚ÄùÈáåÈù¢\nMaximize the separation criterion J or minimize the estimiated error$\\epsilon$\n‰∏çÈááÂèñÁ©∑Â∞ΩÊêúÁ¥¢ÔºåËÄåÊòØÊêúÁ¥¢ËæÉÂ∞ëÁöÑÁâπÂæÅÁ©∫Èó¥ÔºåÊâæÂà∞ sub-optimal Ëß£„ÄÇEvaluation Ê†áÂáÜ‰∏é predictive models Âíå classifiers Áõ∏ÂÖ≥„ÄÇÈÄöËøáÂú®ÊµãËØïÊï∞ÊçÆ‰∏äÈ™åËØÅÂáÜÁ°ÆÊÄßÊù•ËØÑÈÄâÁâπÂæÅÂ≠êÈõÜ„ÄÇ\nAdvantages:\nAccuracy: Âõ†‰∏∫Êõ¥ÂÖ≥Ê≥®ÁâπÂÆö predictive model ‰∏éÊï∞ÊçÆ‰πãÈó¥ÁöÑ‰∫§‰∫íÔºåÊâÄ‰ª•Ë¶ÅÊØî filter approaches Âú®È¢ÑÊµãÁªìÊûú‰∏äÊõ¥ÂáÜÁ°Æ„ÄÇ Generalization ability: Âõ†‰∏∫ÈÄöÂ∏∏ÈááÂèñ cross-validationÔºåÊâÄ‰ª•ÂèØ‰ª•ÈÅøÂÖçËøáÊãüÂêà Disadvantages:\nSlow execution: ËÆ≠ÁªÉpredictive model Ëä±Êó∂Èó¥ Lack of generality: ‰ªÖ‰ΩøÁî®‰∏Ä‰∏™ÁâπÂÆöÁöÑpredictive model Best Individual Features ÊúÄ‰Ω≥‰∏™‰ΩìÁâπÂæÅÔºö‰∏Ä‰∏™ÁâπÂæÅÁöÑÂçïÁã¨ÂàÜÁ±ªÊïàÊûúÂ•Ω‰∫éÂÖ∂‰ªñÁâπÂæÅÂçïÁã¨‰ΩúÁî®ÁöÑÊïàÊûú\n$$ J_{i*} = \\underset{i \\in N}{\\operatorname{arg\\ max}} J(X_i, Y) $$\nÂπ∂‰∏çËÉΩ‰øùËØÅÁªÑÂêàËµ∑Êù•ÁöÑÊïàÊûúËøòÊòØÊúÄÂ•ΩÁöÑ\nSequential Forward Search È°∫Â∫èÂâçÂêëÊêúÁ¥†ÔºöÁ¨¨‰∏Ä‰∏™ÁâπÂæÅÈÄâÊúÄ‰Ω≥Âçï‰ΩìÁâπÂæÅBIFÔºåÁÑ∂ÂêéÊØèÊ¨°ÈÉΩÂèñÂΩìÂâçÁöÑÊúÄ‰Ω≥ÁªÑÂêàÔºåÈÄâÂ§üd‰∏™ÁâπÂæÅÔºåÊàñËÄÖ J Âá∫Áé∞‰∏ãÈôçÔºåÂ∞±ÊòØËæìÂá∫\nÊúÄÁÆÄÂçïÁöÑË¥™ÂøÉÊêúÁ¥¢ÁÆóÊ≥ï (the simplest greedy search algorithm)\nÈÄÇÂêàÊåëÈÄâÂ∞èÂÆπÈáèÁöÑÁâπÂæÅÂ≠êÈõÜ\nFeature freeze: Once a feature added to the selection list can not be deleted. ÊâÄ‰ª•‰∏çËÉΩ‰øùËØÅÊúÄÁªàÁªìÊûúÊòØÊúÄÂ•ΩÁöÑ\nSequential Backward Search È°∫Â∫èÂêéÂêëÊêúÁ¥¢Ôºö‰ªéÁâπÂæÅÂÖ®ÈõÜÂºÄÂßãÔºåÈ°∫Â∫èÁßªÈô§ÈÇ£‰∏™‰Ωø$J(X_i,Y)$‰∏ãÈôçÊúÄÂ∞èÁöÑÁâπÂæÅÔºå ÈÄÇÂêàÊåëÈÄâÂ§ßÂÆπÈáèÁöÑÁâπÂæÅÂ≠êÈõÜ Limitation: Êó†Ê≥ïËØÑ‰ª∑Â∑≤ÁªèË¢´ÁßªÈô§ÁâπÂæÅÁöÑ‰ΩúÁî® General SFS and General SBS Âπø‰πâÈ°∫Â∫èÂâçÂêëÊêúÁ¥¢ÔºöÂÖÅËÆ∏ÊØèÊ¨°ÈÄâ‰∏≠Â§ö‰∏™ÔºàrÔºâÁâπÂæÅÔºåËøõË°åËØÑ‰º∞ÔºåÊ∑ªÂä†ÊàñÂà†Èô§\nÂáèÂ∞ëÂà§Êñ≠Ê¨°Êï∞\nGSFS:\nPLUS-L TAKE-R Âä† L Âáè R„ÄÇIn Plus-L Take away-R\nIf L\u0026gt;R: ‰ªéÁ©∫ÈõÜÂºÄÂßãÔºåÂÖàÊåâSFSÂä†ÂÖ•L‰∏™ÁâπÂæÅÔºåÂÜçÊåâSBSÁßªÈô§R‰∏™ÁâπÂæÅ„ÄÇ\nIf L\u0026lt;R: ‰ªéÂÖ®ÈõÜÂºÄÂßãÔºåÂÖàÊåâSBSÁßªÈô§R‰∏™ÁâπÂæÅÔºåÂÜçÊåâSFSÂä†ÂÖ•L‰∏™ÁâπÂæÅ\n‰ΩúÁî®Ôºöattemps to compensate for the weaknesses of SFS and SBS with some backtracking capabilities (ÂõûÊ∫ØËÉΩÂäõ)\nLimitation: Lack of theory to help predict the optimal values of L and R.\n‰æãÔºö‰ªé10‰∏™ÁâπÂæÅ‰∏≠ÈÄâ3‰∏™ÔºåL=3ÔºåR=2\n3\u0026gt;2ÔºåÊâÄ‰ª•‰ªéÁ©∫ÈõÜÂºÄÂßãÔºåÂÖàÂä†3‰∏™ÁâπÂæÅÔºåÂÜçÁßªÈô§2‰∏™ÁâπÂæÅ\nSequential Floating Selection È°∫Â∫èÊµÆÂä®ÈÄâÊã©ÔºöÊØèÊ¨°Ëø≠‰ª£Êó∂ÔºåÂèØ‰ª•Ë∞ÉÊï¥ L Âíå R\nÂú®PLTSÁöÑÂü∫Á°Ä‰∏äÔºå‰∏çÂõ∫ÂÆöL Âíå R„ÄÇ\n‰ºòÂåñLÂíåR\nTwo floating methods:\nSequential Floating Forward Selection (SFFS) È°∫Â∫èÊµÆÂä®ÂâçÂêëÈÄâÊã© Starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases.\nSequential Floating Backward Selection (SFBS) È°∫Â∫èÊµÆÂä®ÂêéÂêëÈÄâÊã© Starts from the full set. SFBS performs forward steps as long as the objective function increases\n","date":"2021-12-14T14:47:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/aml_feature-selection/","title":"watch: AML | Feature Selection"},{"content":"Video 17 Validation 2021-12-08\nOutline:\nThe validation set Model selection Cross validation Review of Lec12 (ÊúüÊú´‰∏çÊ∂âÂèä12)\nRegularization: add a overfit or complexity penalty termÔºå‰∏éÊ®°ÂûãÂ§çÊùÇÂ∫¶ÊúâÂÖ≥Ôºå‰ΩøÁî®Ëøô‰∏™\u0026quot;ÊÉ©ÁΩöÈ°π\u0026quot;‰º∞ËÆ°out-of-sample error\n‰∏§ÁßçÊ≠£ÂàôÂåñÊñπÊ≥ïÔºö\nconstrained regularization: select some type of hypotheses unconstrained regularization: ‰∏çÊòØÊúÄÂ∞èÂåñ$E_{out}$ÔºåËÄåÊòØÊúÄÂ∞èÂåñ $E_{\\rm augment}(\\mathbf w) = E_{in}(\\mathbf w) + \\underbrace{\\frac{\\lambda}{N} \\mathbf w^T \\bf w}_{\\text{penalty term}}$ ÈÄâÊã©‰∏Ä‰∏™ regularizer Âéª‰º∞ËÆ°penaltyÈ°π: $E_{\\rm augment}(\\mathbf w) = E_{in}(\\mathbf w) + \\frac{\\lambda}{N} \\Omega(h)$\nÂÖ∂‰∏≠ $\\Omega(h)$ ÊòØregularizerÔºå$\\lambda$ ÊòØÊ≠£ÂàôÂåñÂèÇÊï∞(regularization parameter)\n$\\Omega(h)$: ÂêØÂèëÂºèÂú∞ÈÄâÊã© heuristicÔºåÈÄöÂ∏∏‰ΩøÁî®weight decayÔºåÊâæÂà∞‰∏Ä‰∏™ smooth, simple $h$\n$\\lambda$ ÂÜ≥ÂÆö‰∫ÜÊ≠£ÂàôÂåñË¢´ÂºïÂÖ•ÁöÑÁ®ãÂ∫¶„ÄÇÂ¶ÇÊûúÈÄâ‰∫ÜÊ≠£Á°ÆÁöÑ$\\lambda$ÔºåÂèØ‰ª•ÂæàÂ•ΩÁöÑ‰º∞ËÆ°Êú™Áü•ÁõÆÊ†áÂáΩÊï∞„ÄÇValidation ‰πüË¶ÅÊâæÂà∞‰∏Ä‰∏™ÂêàÈÄÇÁöÑ$\\lambda$\nValidation vs Regularization Âú®learning ËøáÁ®ã‰∏≠Ôºå$E_{out}(h)$ Êú™Áü•ÔºàÂõ†‰∏∫ÁõÆÊ†áÂáΩÊï∞Êú™Áü•ÔºâÔºå‰ΩÜÊòØÂÆÉÁ≠â‰∫é $E_{in}(h)+$ overfit penaltyÔºåEin ÊòØÂ∑≤Áü•ÁöÑÔºàÈ¢ÑÊµãÂÄº‰∏éËÆ≠ÁªÉÊ†∑Êú¨ÁúüÂÆûÂÄºÁöÑËØØÂ∑ÆÔºâÔºåËøòÈúÄË¶ÅÁü•ÈÅìoverfit penalty„ÄÇ ÊâÄ‰ª•‰∏∫‰∫ÜËÆ°ÁÆó Eout Êúâ‰∏§ÁßçÊñπÊ≥ïÔºöRegularization ÊòØÂÖà‰º∞ËÆ°Âá∫ overfit penalty„ÄÇËÄåValidation ÊòØÁõ¥Êé•‰º∞ËÆ° Eout„ÄÇ $$ \\begin{aligned} \\rm Regularization: E_{out}(h) = E_{in}(h) + \\underbrace{\\text{overfit penalty}}{\\mathclap{\\text{regularization estimates this quantity}}} \\ \\ \\rm Validation: \\underbrace{E{out}(h)}{\\mathclap{\\text{validation estimates this quantity}}} = E{in}(h) + \\text{overfit penalty} \\end{aligned} $$\nAnalyzing the estimate Out-of-sample point ÊòØÊ≤°ÊúâÂú®ËÆ≠ÁªÉÈò∂ÊÆµ‰∏≠‰ΩøÁî®ÁöÑÁÇπÔºå\nÂú®‰∏Ä‰∏™out-of-sample ÁÇπ $(\\mathbf X,y)$ ‰∏äÁöÑËØØÂ∑ÆÊòØ $\\mathbf e(h(\\mathbf x),y)$„ÄÇÊ†πÊçÆË¶ÅËß£ÂÜ≥ÈóÆÈ¢òÁöÑ‰∏çÂêåÔºåËØØÂ∑ÆÂáΩÊï∞Êúâ‰∏çÂêåÁöÑÂΩ¢ÂºèÔºö\n$$ \\begin{aligned} \\text{ÂõûÂΩí, Squared error:} \u0026amp; (h(\\mathbf x)-y)^2 \\ \\text{ÂàÜÁ±ª, Binary error:} \u0026amp; [![ h(\\mathbf x)\\neq y]!] \\end{aligned} $$\n$h$ Âú® out-of-sampleÂàÜÂ∏É ‰∏äÁöÑËØØÂ∑ÆÁöÑÊúüÊúõÊòØ$E_{out}(h)$Ôºö $\\mathbb E[\\mathbf e(h(\\mathbf x),y)] = E_{out}(h)$\n$h$ Âú® out-of-sampleÂàÜÂ∏É ‰∏äÁöÑËØØÂ∑ÆÁöÑÊñπÂ∑ÆÊòØ$\\sigma^2$Ôºö $\\operatorname{var}[\\mathbf e(h(\\mathbf x),y)] = \\sigma^2$\n‰ªé1‰∏™ÁÇπÂà∞1ÁªÑÁÇπÔºö ‰ªétraining set ‰∏≠Áã¨Á´ãÂú∞ÈÄâÂá∫K‰∏™ÁÇπÁªÑÊàê‰∏Ä‰∏™È™åËØÅÈõÜ(validation set) $(\\mathbf x_1,y_1), \\cdots, (\\mathbf x_K, y_K)$ÔºåÈ™åËØÅÈõÜ‰∏äÁöÑËØØÂ∑ÆÊòØ $E_{\\text{val}}(h) = \\frac{1}{K} \\sum_{k=1}^K \\mathbf e(h(\\mathbf x_k), y_k)$\n‰∏çÂêåÈ™åËØÅÈõÜËØØÂ∑ÆÁöÑÊúüÊúõÔºö$\\mathbb E[E_{\\text{val}}(h)] = \\frac{1}{K} \\sum_{k=1}^K \\mathbb E[\\mathbf e(h(\\mathbf x_k), y_k)] = E_{out}(h)$ (ÊúüÊúõÊîæÈáåÈù¢ÔºåÂ∞±ÊòØ$E_{out}$)\n‰∏çÂêåÈ™åËØÅÈõÜËØØÂ∑ÆÁöÑÊñπÂ∑ÆÔºö$\\operatorname{var} [E_{\\text{val}}(h)] = \\frac{1}{K^2} \\sum_{k=1}^K \\operatorname{var}[\\mathbf e(h(\\mathbf x_k), y_k)] = \\frac{\\sigma^2}{K}$ (Âõ†‰∏∫ÂêÑÁÇπ‰∫íÁõ∏Áã¨Á´ãÔºåÊâÄ‰ª•ÂçèÊñπÂ∑ÆÁü©ÈòµÈô§‰∫ÜÂØπËßíÁ∫øÂÖ∂‰ªñ‰ΩçÁΩÆÈÉΩÊòØÈõ∂)\nÈ™åËØÅÈõÜÁöÑËØØÂ∑ÆÁ≠â‰∫éEout Âä†‰∏Ä‰∏™ $\\frac{1}{\\sqrt{K}}$ Èò∂ÔºàÊ†áÂáÜÂ∑ÆÔºâÁöÑÂÅèÁΩÆÈ°πÔºö\n$$ E_{\\text{val}}(h) = E_{\\text{out}}(h) \\pm O(\\frac{1}{\\sqrt{K}}) $$\nÂ¶ÇÊûúÂ¢ûÂä†È™åËØÅÈõÜÊ†∑Êú¨Êï∞Èáè KÔºåÂÅèÁΩÆÈ°πÂèòÂ∞èÔºåÈ™åËØÅÈõÜËØØÂ∑ÆÂ∞±Ë∂äÊé•ËøëEout„ÄÇ\nÂØπ‰∫éÊï∞ÊçÆÈõÜ $\\mathcal D = (\\mathbf x_1, y_1), \\cdots, (\\mathbf x_N, y_N)$\nÈÄâK‰∏™ÁÇπ‰Ωú‰∏∫È™åËØÅÈõÜÔºö$\\mathcal D_{\\rm val}$\nÂâ©‰∏ã N-K ‰∏™ÁÇπÊòØËÆ≠ÁªÉÈõÜÔºö$\\mathcal D_{\\rm train}$\nÂØπ‰∫éÂÅèÁΩÆÈ°πÔºö$O(\\frac{1}{\\sqrt{K}})$ÔºåÂ∞èKËÆ©Eval ‰∏é Eout Â∑ÆÁöÑËøúÔºåËÄåÂ§ßKËÆ©Ein ‰∏é Eout Â∑ÆÂæóËøú„ÄÇÊâÄ‰ª•KÈúÄË¶Åtradeoff\n‰ª•ÂâçÈÄöÂ∏∏Áî®ÂÖ®ÈÉ®ÁöÑÊï∞ÊçÆÈõÜÊù•ËÆ≠ÁªÉÔºåÂæóÂà∞gÔºåÁé∞Âú®Âè™Áî®‰∫Ü‰∏ÄÈÉ®ÂàÜÊï∞ÊçÆ (reduced dataset) Êù•ËÆ≠ÁªÉÔºåÂæóÂà∞$g^-$ÔºåÊâÄ‰ª•ÂÆÉÁöÑ EinÂíåEout ÈÉΩÊØîgÂ§ß„ÄÇÁÑ∂ÂêéËÆ°ÁÆó $g^-$ Âú®È™åËØÅÈõÜ‰∏äÁöÑËØØÂ∑Æ $E_{val}(g^-)$Ôºå‰Ωú‰∏∫Eout ÁöÑËøë‰ººÔºåÂ¶ÇÊûúKÂæàÂ§ßÔºåËøë‰ººÊïàÊûú‰ºöÂ∑Æ„ÄÇÁªèÈ™åÊ≥ïÂàôÔºö$K= \\frac{N}{5}$\nValidation set ‰∏çÊòØ test set\n$E_{val}(g^-)$ ‰πü‰∏çÊòØ $E_{out}$„ÄÇÊµãËØïÈõÜ‰∏éËÆ≠ÁªÉÊó†ÂÖ≥ (unbiased)ÔºåËÄåÈ™åËØÅÈõÜ‰ºöÂú®ËÆ≠ÁªÉÈò∂ÊÆµÂ∏ÆÂä©Êàë‰ª¨ÈÄâÊã©Ë∂ÖÂèÇÊï∞Ôºå‰ªéËÄåÂΩ±Âìç‰∫ÜÂ≠¶‰π†ËøáÁ®ã (optimistic bias)„ÄÇ\nÊØîÂ¶ÇÔºåÊúâ‰∏§‰∏™ÂÅáËÆæ $h_1$ Âíå $h_2$ÔºåÂÖ∂ÂÆûÂÆÉ‰ª¨ÁúüÊ≠£ÁöÑEoutÈÉΩÊòØ0.5Ôºö$E_{out}(h_1) = E_{out}(h_2) = 0.5$ Ôºå‰ΩÜÊòØÊú™Áü•„ÄÇÂÆÉ‰ª¨ÂàÜÂà´Âú®È™åËØÅÈõÜ‰∏äÁöÑËØØÂ∑Æ‰∏∫ $\\mathbf e_1,\\ \\mathbf e_2$ÔºåÁÑ∂ÂêéÊàë‰ª¨‰ºöÈÄâÊã©Áïô‰∏ãËØØÂ∑ÆÂ∞èÁöÑÈÇ£‰∏™Ôºö$\\mathbf e = min(\\mathbf{e_1,e_2})$, ÂÆÉÁöÑ Eout $\\mathbb E(\\mathbf e)$ Ë¶ÅÂ∞è‰∫éÁúüÂÆûÂÄº0.5ÔºåÂõ†‰∏∫ÂÆÉÁî®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂ∞ë‰∫éÂÖ®ÈÉ®Êï∞ÊçÆÈõÜÔºåÊâÄ‰ª•validataion ÁªôÂá∫ÁöÑËØØÂ∑ÆÊòØÂÅèÂêë‚Äú‰πêËßÇÁöÑ‚Äù\nModel selection ÊØîÂ¶ÇË¶ÅËß£ÂÜ≥‰∏Ä‰∏™ÂàÜÁ±ªÈóÆÈ¢òÔºåÊúâM‰∏™ÂÅáËÆæÁ©∫Èó¥Ôºö$\\mathcal H_1,\\cdots, \\mathcal H_M$ ÔºàÊØîÂ¶ÇsvmÁöÑÊ†∏ÂèØ‰ª•‰∏∫linear, polynomial, rbfÔºåÈÄâÂì™ÁßçÂ•ΩÂë¢ÔºüÔºâ„ÄÇ\nÊ†πÊçÆ (ÊúâÁº©ÂáèÁöÑreduced) ËÆ≠ÁªÉÈõÜÔºå‰ªéÊØè‰∏™ÂÅáËÆæÁ©∫Èó¥ÈÄâÂá∫‚ÄúÊúÄ‰Ω≥ÂÅáËÆæ‚Äù(finalists model ÂÜ≥ËµõÈÄâÊâã)„ÄÇÁÑ∂ÂêéÂàÜÂà´Âú®È™åËØÅÈõÜ‰∏äËÆ°ÁÆóEval„ÄÇÊ†πÊçÆËøô M ‰∏™EvalÔºåÈÄâÂá∫ÊúÄ‰Ω≥ $E_{val}$ ÂíåÊúÄ‰Ω≥ÂÅáËÆæÁ©∫Èó¥ $\\mathcal H_{m^}$„ÄÇÁÑ∂ÂêéÂÜç‰ΩøÁî®Êï¥‰∏™Êï∞ÊçÆÈõÜÂú®ÊúÄ‰Ω≥ÂÅáËÆæÁ©∫Èó¥‰∏≠ÊâæÂá∫ÊúÄ‰Ω≥ÂÅáËÆæ $g_{m^}$\n‰ΩøÁî®$\\mathcal D_{\\rm val}$ Âíå $E_{\\rm val}(g_{m^}^-)$ ÈÄâÊã©ÁöÑÊúÄ‰Ω≥ÂÅáËÆæÁ©∫Èó¥ $\\mathcal H_{m^}$ ÊòØ $E_{out}(g_{m^*}^-)$ ÁöÑ‰∏Ä‰∏™ biased estimateÔºåÂõ†‰∏∫Ê≤°Êúâ‰ΩøÁî®ÂÖ®ÈÉ®ÁöÑÊï∞ÊçÆÈõÜÔºåÊâÄ‰ª•Âè´biased„ÄÇ\n‰∏çÂêåÂÆπÈáèÁöÑÈ™åËØÅÈõÜ‰∏éÈ¢ÑÊúüÂÅèÂ∑ÆÁöÑÂÖ≥Á≥ªÂ¶Ç‰∏ãÂõæÔºö\nÈ™åËØÅÈõÜ‰∏≠Êï∞ÊçÆ K Ë∂äÂ§öÔºåÁî®‰∫éËÆ≠ÁªÉÁöÑÊ†∑Êú¨Ë∂äÂ∞ëÔºåEoutË∂äÂ∑ÆÔºå‰ΩÜÊòØÂêåÊó∂ $O(\\frac{1}{\\sqrt{K}})$ ÂáèÂ∞èÔºå$E_{\\rm val}$ ‰ºöË∂äÊé•Ëøë $E_{\\rm out}$„ÄÇ\nHow much bias ÂØπ‰∫é M ‰∏™ÂÅáËÆæÁ©∫Èó¥Ôºö$\\mathcal H_1, \\cdots ,\\mathcal H_M$Ôºå‰ªé‰∏≠ÈÄâÂá∫‰∫Ü M ‰∏™ finalists model $H_{\\rm val} = { g_1^-, g_2^-,\\cdots, g_M^- }$ÔºåÁÑ∂ÂêéÁî®È™åËØÅÈõÜ $\\mathcal D_{\\rm val}$ Âéª‚ÄúËÆ≠ÁªÉ‚ÄùÂÆÉ‰ª¨Ôºå‰πüÂ∞±ÊòØÂÜçÊâæÂá∫ÂÆÉ‰ª¨‰∏≠ÁöÑÊúÄ‰Ω≥ minus ÂÅáËÆæ $g_{m^\\star}^-$Ôºà$E_{\\rm val}$ÊúÄÂ∞èÔºâ„ÄÇ\nÂØπ‰∫é‰∏Ä‰∏™\u0026quot;ËÆ≠ÁªÉ\u0026quot;ËøáÁ®ãÔºåÂØπ‰∫éÂÅáËÆæ $g_{m^\\star}^-$ ÊúâHoeffding‰∏çÁ≠âÂºèÊàêÁ´ãÔºö\n$$ E_{out} (g_{m^\\star}^-) \\leq E_{val}(g_{m^\\star}^-) + O \\left( \\sqrt{\\frac{ln M}{K}} \\right) $$\nÂ¶ÇÊûúÊúâÊó†Á©∑Â§ö‰∏™ÂÅáËÆæÈõÜÔºàÊó†Á©∑Â§ö‰∏™Ê≠£ÂàôÂåñÂèÇÊï∞Ôºå$\\lambda$ ÊòØËøûÁª≠ÂÄºÔºâÔºåÊâÄ‰ª• $O \\left( \\sqrt{\\frac{ln M}{K}} \\right)$ Â∞±ÂèòÂæó‰∏çÂÜçÊúâÊïà\n‰∏∫‰∫ÜÁ∫¶Êùü MÔºåÂ∞±ÂÉè‰πãÂâçÈÇ£Ê†∑ÔºåÂºïÂÖ• VC Áª¥„ÄÇÊØîÂ¶ÇÔºåÊàë‰ª¨‰∏çÂÖ≥ÂøÉÊ≠£ÂàôÂåñÂèÇÊï∞ $\\lambda$ ËÉΩÂèñÂ§öÂ∞ëÂÄºÔºåËÄåÊòØÂÖ≥ÂøÉÊàë‰ª¨ÊúâÂá†‰∏™ÂèÇÊï∞ÔºàËá™Áî±Â∫¶ÔºâÔºåÊàë‰ª¨Âè™Êúâ1‰∏™ÂèÇÊï∞ $\\lambda$ÔºåÊâÄ‰ª•VCÁª¥ÊòØ1„ÄÇ\nData contamination Âú®ËÆ≠ÁªÉÈò∂ÊÆµÁî®‰∫ÜÂ§öÂ∞ëÊï∞ÊçÆÊ†∑Êú¨ $E_{in}ÔºåE_{out}(E_{test})ÔºåE_{\\rm val}$ Contamination: Optimistic (deceptive) bias in estimating Eout Training set: totally contaminated Validation set: slightly contaminated (Ëµ∑Âà∞‰∫Ü‚ÄúÊµãËØï‚ÄùÁöÑÊïàÊûúÔºå‰ΩÜ‰πüË¢´Áî®‰∫éËÆ≠ÁªÉ‰∫Ü) Test set: totally \u0026lsquo;clean\u0026rsquo; (ÂÆåÂÖ®Áî®‰∫éÊµãËØï) Cross validation Êäätrain set ÂàÜÊàênÊäòÔºåÊØèÊ¨°Âèñn-1ÊäòÂÅöËÆ≠ÁªÉÔºåËÆ°ÁÆóÂú®Ââ©‰∏ãÈÇ£Êäò‰∏äÁöÑÂáÜÁ°ÆÁéáÔºån‰∏™ÂáÜÁ°ÆÁéáÊ±ÇÂπ≥ÂùáÂ∞±ÊòØËØ•ÁªÑË∂ÖÂèÇÊï∞ÁöÑË°®Áé∞„ÄÇ\n‰∏ç‰ΩøÁî®test setÔºåÂç¥ÂèØ‰ª•‰º∞ËÆ°Âú®test set‰∏äÁöÑË°®Áé∞„ÄÇ\nÁõÆÁöÑÊòØÈÄâÊúÄ‰Ω≥ÁöÑË∂ÖÂèÇÊï∞Ôºõ‰∏çËÉΩÊ†πÊçÆÂú®train set‰∏äÁöÑÂáÜÁ°ÆÁéáÂà§Êñ≠Â•ΩÂùè„ÄÇ\nÈÄâÁî®‰∏çÂêåË∂ÖÂèÇÊï∞Êó∂ÔºåCVÂáÜÁ°ÆÁéáÁöÑÂèòÂåñË∂ãÂäø‰∏éÂú®test set‰∏äÁöÑÂèòÂåñË∂ãÂäøËøë‰ºº‰∏ÄËá¥„ÄÇ\nK ËøõÈÄÄ‰∏§Èöæ: $g^-$ÊòØÁî® reducedËÆ≠ÁªÉÈõÜÊâæÂá∫ÁöÑÊúÄ‰Ω≥ÔºåKË∂äÂ∞èÔºåÁî®‰∫éËÆ≠ÁªÉÁöÑÊï∞ÊçÆË∂äÂ§öÔºåË∂äÊé•ËøëÁúüÂÆûÁöÑEoutÔºåËÄåÊ†πÊçÆHoeffding‰∏çÁ≠âÂºèÔºå$E_{\\rm val}(g^-)$ÈúÄË¶ÅÂæàÂ§ßÁöÑKÔºåÊâçËÉΩËøë‰ºº$E_{out}(g^-)$\n$$ E_{\\rm out}(g) \\underset{\\mathclap{\\substack{\\ \\text{Â∞èKÊâçËøë‰ºº}}}}{\\approx} E_{\\rm out}(g^-) \\underset{\\mathclap{\\substack{\\ \\text{Â§ßKÊâçËøë‰ºº}}}}{\\approx} E_{\\rm val}(g^-) $$\n$E_{out}$ ÊòØÊúÄÁªàÁõÆÊ†áÔºå‰ΩÜÊòØÂè™Áü•ÈÅìÈ™åËØÅËØØÂ∑Æ $E_{\\rm val}(g^-)$\nhave K both small and large\n‰∏§Áßç‰∫§ÂèâÈ™åËØÅÊñπÊ≥ïÔºö\nLeave One Out\nK=1ÔºåÊØèÊ¨°Ëø≠‰ª£ÈÄâ1‰∏™Ê†∑Êú¨ÂÅöÈ™åËØÅÔºåÂâ©‰∏ãN-1‰∏™Ê†∑Êú¨ÂÅöËÆ≠ÁªÉ„ÄÇÂéªÈô§Á¨¨n‰∏™Ê†∑Êú¨ÁöÑËÆ≠ÁªÉÈõÜ$\\mathcal D_n:$\n$$ \\mathcal D_n = (\\mathbf x_1,y_1),\\cdots,(\\mathbf x_{n-1},y_{n-1}),\\sout{(\\mathbf x_n, y_n)},(\\mathbf x_{n+1},y_{n+1}),\\cdots,(\\mathbf x_N, y_N) $$\n‰ªé $\\mathcal D_n$ ‰∏≠Â≠¶Âà∞ÁöÑÂÅáËÆæÊòØ $g_n^-$ÔºåÈ™åËØÅËØØÂ∑Æ $\\mathbf e_n = E_{\\rm val}(g_n^-) = \\mathbf e(g_n^- (\\mathbf x_n),y_n)$\nÂØπÊØè‰∏™ÁïôÂá∫ÁöÑÊ†∑Êú¨ÁÇπÔºåËÆ°ÁÆóÈ™åËØÅËØØÂ∑ÆÔºåÁÑ∂ÂêéÂèñÂπ≥ÂùáÔºåÂ∞±ÊòØ‰∫§ÂèâÈ™åËØÅËØØÂ∑Æ (cross validation error): $$ E_{CV} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf e_n $$ ÂØπ‰∫é3‰∏™ÁÇπÔºåÊØèÊ¨°ÂèñÂá∫‰∏Ä‰∏™ÂÅöÈ™åËØÅÈõÜÔºåÂâ©‰∏ã‰∏§‰∏™ÂÅöËÆ≠ÁªÉÈõÜÔºåÁ∫øÊÄßÂõûÂΩíÈóÆÈ¢òÔºåÂØπ‰∫é‰∏§‰∏™Ê†∑Êú¨ÔºåËØØÂ∑ÆÊúÄÂ∞èÁöÑLinearÂÅáËÆæÔºåÂ∞±ÊòØËøá‰∏§ÁÇπÁöÑ‰∏ÄÊù°Áõ¥Á∫ø„ÄÇ\nÂØπ‰∫é Constant ÂÅáËÆæÔºö\nÂØπÊØî $E_{CV}$Ôºåconstant Ê®°ÂûãÁöÑ‰∫§ÂèâÈ™åËØÅËØØÂ∑ÆËæÉÂ∞èÔºåÊâÄ‰ª•ÊúÄÁªàÈÄâÊã©constantÊ®°Âûã\nN‰∏™Ê†∑Êú¨ÁöÑÊï∞ÊçÆÈõÜË¶ÅËø≠‰ª£ N Ê¨°ÔºåÊØèÊ¨°Âú® N-1 ‰∏™Ê†∑Êú¨‰∏äËÆ≠ÁªÉÔºåÂ¶ÇÊûúÊúâ1ÂçÉ‰∏™Ê†∑Êú¨Â∞±Ë¶ÅËø≠‰ª£1ÂçÉÊ¨°ÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶Â§™È´ò„ÄÇ\nLeave More Out\nÊääÊï∞ÊçÆÈõÜÂàíÂàÜÊàêÂ§ö‰ªΩÔºåÂàíÂàÜÊàê10‰ªΩÁöÑËØùÔºö$K = \\frac{N}{10}$ÔºåÂè™ÈúÄËø≠‰ª£10 ($\\frac{N}{K}$)Ê¨°ÔºåÊØèÊ¨°Âú®N-K‰∏™ÁÇπ‰∏äËÆ≠ÁªÉ„ÄÇ\nCross validation in action Êï∞Â≠óÂàÜÁ±ª‰ªªÂä°ÔºåÊää2‰∏™ÁâπÂæÅÔºàsymmetryÂíåAverage intensityÔºâÈùûÁ∫øÊÄßÂèòÊç¢Âà∞20Áª¥Á©∫Èó¥ÔºåÊúÄÈ´òÂπÇÊ¨°‰∏∫5ÁöÑÂ§öÈ°πÂºè\n$$ \\left(1, x_{1}, x_{2}\\right) \\rightarrow\\left(1, x_{1}, x_{2}, x_{1}^{2}, x_{1} x_{2}, x_{2}^{2}, x_{1}^{3}, x_{1}^{2} x_{2}, \\ldots, x_{1}^{5}, x_{1}^{4} x_{2}, x_{1}^{3} x_{2}^{2}, x_{1}^{2} x_{2}^{3}, x_{1} x_{2}^{4}, x_2^{5}\\right) $$\n‰ΩøÁî®ÁâπÂæÅÊï∞ÈáèË∂äÂ§öÔºåÊ®°ÂûãË∂äÂ§çÊùÇÔºå$E_{in}$ Ë∂äÂ∞èÔºàËø≠‰ª£‰∫ÜÂæàÂ§öÊ¨°ÔºâÔºå$E_{out}$ÂÖàÂáèÂ∞èÂêéÂ¢ûÂ§ßÔºåÂá∫Áé∞OverfittingÔºåËÄå$E_{CV}$ÁöÑË∂ãÂäø‰∏é$E_{out}$Áõ∏ÂêåÔºåÂõ†‰∏∫$E_{out}$Êú™Áü•Ôºå$E_{CV}$ÊòØ $E_{out}$ ÁöÑËøë‰ººÔºåÊâÄ‰ª•ÂèØ‰ª•Ê†πÊçÆ $E_{CV}$ Êù•ÂÜ≥ÂÆöËØ•ÈÄâÁî®Âá†‰∏™ÁâπÂæÅ„ÄÇEcv ÁöÑÊúÄÂ∞èÂÄºÂá∫Áé∞Âú®5 Âíå7ÔºåÊâÄ‰ª•ÂèØ‰ª•ÈÄâÁî®6‰∏™ÁâπÂæÅÁöÑÊ®°Âûã„ÄÇ\nÊ≤°Áî®validationÊó∂ÔºåÁõ¥Êé•‰ΩøÁî®20‰∏™ÁâπÂæÅÁöÑÊ®°ÂûãÂæàÂ§çÊùÇÔºåËÄå‰∏îËøáÊãüÂêàÔºàÂô™Èü≥ÔºâÔºåEin‰∏∫Èõ∂Ôºõ‰ΩøÁî®validationÂêéÔºåÂÜ≥ÂÆöÂè™Áî®6‰∏™ÁâπÂæÅÔºåÊ®°ÂûãÁõ∏ÂØπÁÆÄÂçïÔºåEoutËæÉÂ∞è„ÄÇ\n‰æãÈ¢ò Given three two-dimensional data examples $x_1 = (-1,1)Ôºåx_2=(0,2)$, and $x_3=(1,1)$, perform the leave-one-out cross validation for a linear fit using these data examples. What is $E_{CV}$?\n$$ E_{CV} = \\frac{1}{N} \\sum_{n=1}^N \\varepsilon_n $$\nwhere $\\varepsilon_n = (y_n - g(x_n))^2$\nNote: The line passing through two-dimensional data points $(x_1, y_1)$ and $(x_2,y_2)$ can be obtained as follows: $y-y_1 = \\frac{y_2 - y_1}{x_2-x_1} \\times (x-x_1)$\nGA answer:\nKeep $x_1$ as for the validation, while $x_2, x_3$ as for training:\n$g:\\ y-2 = \\frac{1-2}{1-0}(x-0) \\Rightarrow y=-x+2$\n$\\varepsilon_1 = (1-g(-1))^2 = (1-3)^2 = 4$\nKeep $x_2$ as for the validation:\n$g:\\ y-1 = \\frac{1-1}{1+1}(x+1) \\Rightarrow y=1$\n$\\varepsilon_2 = (2-g(0))^2 = (2-1)^2 = 1$\nKeep $x_3$ as for the validation:\n$g:\\ y-1 = \\frac{2-1}{0+1}(x+1) \\Rightarrow y=x+2$\n$\\varepsilon_3 = (1-g(1))^2 = (1-3)^2 = 4$\n$E_{CV} = \\frac{1}{3}(4+1+4) = 3$\n","date":"2021-12-14T01:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec13_validation/","title":"watch: AML 13 | Validation"},{"content":"Video 13 Neural Network 2021-11-10\nOutline:\nStochastic gradient descent Neural network model Backpropagation algorithm Gradient Descent Ê≤øÁùÄËØØÂ∑ÆÂáΩÊï∞ $\\mathbf e$ ÁöÑË¥üÊ¢ØÂ∫¶ÊñπÂêëÔºå‰∏ÄÊ≠•‰∏ÄÊ≠•ÊúÄÂ∞èÂåñ in-sample error„ÄÇ\nEin ÊòØÔºàÁ∫øÊÄß/ÈùûÁ∫øÊÄßÔºâÊ®°ÂûãÁöÑÊùÉÈáç $\\mathbf w$ ÁöÑÂáΩÊï∞Ôºö\n$$ E_{in}(\\mathbf w) = \\frac{1}{N} \\sum_{n=1}^N \\mathbf e(h(\\mathbf x_n), y_n) $$\n$\\mathbf e$ ÊòØËØØÂ∑ÆÂáΩÊï∞ÔºåËÆ°ÁÆóÂÅáËÆæÂÄº‰∏éÊ†∑Êú¨ÁúüÂÆûÂÄº‰πãÈó¥ÁöÑËØØÂ∑Æ„ÄÇÂ§çÊùÇÁöÑËØØÂ∑ÆÂáΩÊï∞Ë∂äÈöæ‰ºòÂåñ„ÄÇÊúâÊó∂‰∏çËÉΩÂêë linear regression ÈÇ£Ê†∑ one-shot Ê±ÇÂá∫ÊúÄ‰Ω≥w„ÄÇÂèØ‰ª•Áî®Ê¢ØÂ∫¶‰∏ãÈôçÔºå‰∏ÄÊ≠•‰∏ÄÊ≠•Âú∞‰ΩøËØØÂ∑Æ‰∏ãÈôç„ÄÇÁßªÂä®ÁöÑÊñπÂêëÊòØË¥üÊ¢ØÂ∫¶ÊñπÂêë$-\\nabla$ÔºåÊØèÊ¨°ÁßªÂä®ÁöÑÂ§ßÂ∞è‰∏é $-\\nabla E_{in}$ (Gradient error) ÊàêÊØî‰æãÔºà$\\eta$ÊòØÂ≠¶‰π†ÁéáÔºâÔºö\n$$ \\Delta \\mathbf w = - \\eta \\nabla E_{in}(\\mathbf w) $$\nËøôÈáå$\\nabla$ Ein ÊòØÂü∫‰∫éÊâÄÊúâÁöÑÊ†∑Êú¨ÁÇπ($\\mathbf x_n, y_n$)ÔºåÂè´ÂÅö\u0026quot;batch GD\u0026quot;Ôºå‰πüÂ∞±ÊòØ‰ΩøÁî®ÊâÄÊúâÁÇπÂÅö‰∫Ü‰∏ÄÊ¨°Ê¢ØÂ∫¶‰∏ãÈôç„ÄÇ\nStochastic gradient descent ‰∏ÄÊ¨°ÈöèÊú∫ÈÄâ‰∏Ä‰∏™ÁÇπÂÅöÊ¢ØÂ∫¶‰∏ãÈôç„ÄÇ\nPick one ($\\mathbf x_n, y_n$) at a time. Apply GD to $\\mathbf e(h(\\mathbf x_n),y_n)$\nN Ê¨°Ê¢ØÂ∫¶‰∏ãÈôçÁöÑ‚ÄúÂπ≥ÂùáÊñπÂêë‚Äù(Average direction)ËøòÊòØÁ≠â‰∫é$-\\nabla E_{in}$:\n$$ \\mathbb E_n [-\\nabla \\mathbf e(h(\\mathbf x_n), y_n)] = \\frac{1}{N} \\sum_{n=1}^N -\\nabla \\mathbf e(h(\\mathbf x_n),y_{n}) = -\\nabla E_{in} $$\nÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊòØÊ¢ØÂ∫¶‰∏ãÈôçÁöÑ randomized version\nSGD ÁöÑÂ•ΩÂ§ÑÔºö\nÁÆÄÂåñËÆ°ÁÆó (Cheaper computation): ÊØèÊ¨°Âè™Áúã‰∏Ä‰∏™Ê†∑Êú¨ÁÇπ ÈöèÊú∫Âåñ (Randomization): ÈÅøÂÖçÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄÂ∞èÊàñÈûçÁÇπÔºåÊó†Ê≥ïÁªßÁª≠‰ºòÂåñ„ÄÇÂ¶ÇÊûú‰ΩøÁî®‚Äúbatch GD‚ÄùÔºåÈÇ£‰πàÂàùÂßã‰ΩçÁΩÆÊúÄÂÖ≥ÈîÆÔºåÂõ†‰∏∫Âè™Ëµ∞‰∏ÄÊ≠•ÔºåÊâÄ‰ª•ÂÆπÊòìÈô∑ÂÖ•ÈôÑËøëÁöÑÂ±ÄÈÉ®ÊúÄ‰ºò„ÄÇ ÁÆÄÂçï (Simple) Rule of thumb (ÁªèÈ™åÊ≥ïÂàô): $\\eta = 0.1$ works\n‰æãÂ≠êÔºöÁîµÂΩ±ËØÑÂàÜ\nUser $u_i$ ÁöÑÂñúÂ•ΩÊúâK‰∏™Â±ûÊÄßÔºåMovie $v_j$ ÁöÑ‰πüÊúâÂØπÂ∫îÁöÑK‰∏™Â±ûÊÄß„ÄÇÊ†πÊçÆËøô‰∏™Áî®Êà∑‰ªñ‰πãÂâçËØÑ‰ª∑ËøáÁöÑÁîµÂΩ± $r_{ij}$ (rating)ÔºåË∞ÉÊï¥Áî®Êà∑ÁöÑÂêÑÂ±ûÊÄßÊùÉÈáçÔºåÊúÄÂ∞èÂåñËØØÂ∑Æ„ÄÇ\n$$ \\mathbf e_{ij} = \\left( \\underbrace{ r_{ij}}{\\text{actual}} - \\underbrace{\\sum{k=1}^K u_{ik}v_{jk}}_{\\text{predict}} \\right)^2 $$\nÂèçËøáÊù•ÔºåÊääÁî®Êà∑Â±ûÊÄßËæìÂÖ•Ê®°ÂûãÂ∞±ÂèØ‰ª•‰º∞ËÆ°ÊüêÁîµÂΩ±ÁöÑËØÑÂàÜ\n\u0026hellip;\n2D perceptron ÁöÑbreak point=4Ôºå‰πüÂ∞±ÊòØÊÑüÁü•Êú∫Êó†Ê≥ïËß£ÂÜ≥ÂºÇÊàñÈóÆÈ¢ò„ÄÇ\n\u0026hellip;\nNeural Network model ÂØπ‰∫é‰ªéÁ•ûÁªèÂÖÉ $i$ Âá∫ÂèëÔºåÊåáÂêëÁ¨¨ $l$ Â±ÇÁöÑÁ•ûÁªèÂÖÉ $j$ ÁöÑÊùÉÈáç $w_{ij}^{(l)}$\n$$ \\begin{cases} 1 \\leq l \\leq L \u0026amp; \\text{ÈöêËóèÂ±Ç/ËæìÂá∫Â±ÇÂ∫èÂè∑, ËæìÂÖ•Â±ÇÊòØ0} \\ 0 \\leq i \\leq d^{(l-1)} \u0026amp; \\text{wÂá∫ÂèëÁöÑÁ•ûÁªèÂÖÉ: 0‰ª£Ë°®‰ªébiasÂá∫Âèë}\\ 1 \\leq j \\leq d^{(l)} \u0026amp; \\text{wÊåáÂêëÁöÑÁ•ûÁªèÂÖÉ: Ëá≥Â∞ëÊúâ‰∏Ä‰∏™,ÊúÄÂ§öÊúâ$d^{l}$}\\ \\end{cases} $$\nÁ¨¨ $l$ Â±ÇÁöÑÊüêÁ•ûÁªèÂÖÉÔºåÊé•Âèó‰∫ÜÊù•Ëá™‰∏ä‰∏ÄÂ±ÇÊâÄÊúâÁ•ûÁªèÂÖÉÁöÑËæìÂÖ•ÔºàÂÜÖÁßØÔºâÔºö\n$$ x_j^{(l)} = \\theta(s_j^{(l)}) = \\theta \\left( \\sum_{i=0}^{d^{(l-1)}} w_{ij}^{(l)} x_i^{(l-1)} \\right) $$\n‰ªébias term ÂºÄÂßãÂä†Âà∞Á¨¨ $d^{(l-1)}$ ‰∏™ÔºåÊää‰ø°Âè∑ $s_j^{(i)}$ ‰º†ÂÖ• $\\theta$ ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞\n‰∏Ä‰∏™Ê†∑Êú¨ $\\mathbf x$ Êúâ$d^{(0)}$ ‰∏™Áª¥Â∫¶ÔºåÊâÄ‰ª•ËæìÂÖ•Â±ÇÂØπÂ∫îÊúâÔºö$x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)}$ÔºåÁªèËøá‰∏ÄÂ±Ç‰∏ÄÂ±Ç‰º†ÈÄíÔºåÁõ¥Âà∞ÊúÄÁªàËæìÂá∫‰∏Ä‰∏™ÂÄºÔºö$x_1^{(L)} = h(\\mathbf x)$\nBackpropagation algorithm Â∫îÁî®ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÔºåË∞ÉËäÇÁ•ûÁªèÁΩëÁªúÁöÑÊùÉÈáçÔºå‰ΩøËØØÂ∑ÆÂáΩÊï∞ÊúÄÂ∞è\nÁΩëÁªúÂÖ®ÈÉ®ÁöÑÊùÉÈáç $\\mathbf w = {w_{ij}^{(l)}}$ ÂÜ≥ÂÆö‰∫Ü‰∏Ä‰∏™ÂÅáËÆæ $h(\\mathbf x)$ (ËæìÂÖ•Âà∞ËæìÂá∫ÁöÑÊò†Â∞Ñ)\nÂØπ‰∫é‰∏Ä‰∏™Ê†∑Êú¨ $(\\mathbf x_n,\\ y_n)$ ‰∏äÁöÑËØØÂ∑ÆÔºö$\\mathbf e(h(\\mathbf X_n),\\ y_n) = \\mathbf e(\\mathbf w)$Ôºå‰ΩøÁî®SGDÔºåË∞ÉÊï¥ÊùÉÈáçÔºåÂáèÂ∞èËØØÂ∑Æ\nËØØÂ∑ÆÂáΩÊï∞ÂØπÊØè‰∏™ÊùÉÈáçÊ±ÇÊ¢ØÂ∫¶Ôºö\n$$ \\nabla \\mathbf e(\\mathbf w): \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}, \\quad \\text{for all } i,j,l $$\nËÆ°ÁÆóËØØÂ∑ÆÂáΩÊï∞ $\\mathbf e$ ÂØπÂêÑÊùÉÈáç w ÁöÑÊ¢ØÂ∫¶ $\\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}$\nËØØÂ∑Æ $\\mathbf e$ ÊòØÂÆûÈôÖËæìÂá∫ $\\theta(s)$ ÂáèÂéªÁúüÂÆûÂÄº $y$ÔºåÊâÄ‰ª•ËØØÂ∑ÆÂáΩÊï∞È¶ñÂÖàÊòØ $s_{j}^{(l)}$ ÁöÑÂáΩÊï∞ÔºåÁ¨¨ $l$ Â±ÇÁöÑÁ¨¨ $j$ ‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂÖ•‰ø°Âè∑$s_{j}^{(l)}$ ÊòØÊù•Ëá™‰∏ä‰∏ÄÂ±ÇÊâÄÊúâÁ•ûÁªèÂÖÉÁöÑËæìÂá∫Ë¥°ÁåÆ‰πãÂíåÔºåÊâÄ‰ª• $s_{j}^{(l)}$ ÊòØ $w_{ij}^{(l)}$ ÁöÑÂáΩÊï∞ÔºåÊ†πÊçÆÈìæÂºèÊ±ÇÂØºÊ≥ïÂàôÔºö\n$$ \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}} = \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_{j}^{(l)}} \\times \\frac{\\partial s_{j}^{(l)}}{\\partial w_{ij}^{(l)}} $$\nÂÖ∂‰∏≠Ôºö$\\frac{\\partial s_{j}^{(l)}}{\\partial w_{ij}^{(l)}} = x_i^{(l-1)}$Ôºå‰πüÂ∞±ÊòØÂâç‰∏ÄÂ±ÇÁöÑÁ•ûÁªèÂÖÉÁöÑËæìÂá∫„ÄÇ ÊääËØØÂ∑ÆÂØπËæìÂÖ•‰ø°Âè∑ÁöÑÂØºÊï∞Áß∞‰∏∫Ôºö$\\delta_j^{(l)} = \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_{j}^{(l)}}$„ÄÇ\nÊâÄ‰ª•ÔºàÊüêÁ•ûÁªèÂÖÉ‰∏äÁöÑÔºâËØØÂ∑ÆÂØπÂêÑÊùÉÈáç w ÁöÑÊ¢ØÂ∫¶Á≠â‰∫éÔºö$\\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}= \\delta_j^{(l)} x_i^{(l-1)}$„ÄÇ\nËÆ°ÁÆó$\\delta$:\n‰ªéÊúÄÂêé‰∏ÄÂ±ÇÔºàËæìÂá∫Â±Ç$l=L,\\ j=1$ÔºâÁöÑ $\\delta_1^{(L)}$ ÂºÄÂßãËÆ°ÁÆóÔºåËæìÂá∫Á•ûÁªèÂÖÉÁöÑËæìÂÖ•‰ø°Âè∑ÊòØ $s_1^{(L)}$:\n$$ \\begin{aligned} \\delta_1^{(L)} \u0026amp;= \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_1^{(L)}}\\ \\mathbf e(\\mathbf w) \u0026amp;= \\left ( x_1^{(L)}- y_n \\right)^2 \u0026amp; \\text{È¢ÑÊµãÂÄº-ÂÆûÈôÖÂÄº} \\ x_1^{(L)} \u0026amp;= \\theta(s_1^{(L)}) \u0026amp; \\text{Á•ûÁªèÂÖÉÁöÑËæìÂá∫ÊòØ$\\theta$ÁöÑËæìÂá∫} \\ \\theta\u0026rsquo;(s) \u0026amp;= 1 - \\theta^2(s) \u0026amp; \\text{for the tanh} \\end{aligned} $$\n‰πãÂâçÂ±Ç ($l-1$Â±Ç) Á•ûÁªèÂÖÉ‰∏äÁöÑËØØÂ∑ÆÂØπÂÖ∂ËæìÂÖ•‰ø°Âè∑ÁöÑÂØºÊï∞Ôºö\nÂõ†‰∏∫Á¨¨ $l-1$ Â±ÇÁöÑÊüê‰∏™Á•ûÁªèÂÖÉ‰ºöÂØπÁ¨¨ $l$ Â±ÇÁöÑÂÖ®ÈÉ®Á•ûÁªèÂÖÉÈÉΩÊúâË¥°ÁåÆÔºåÊâÄ‰ª•ÂÆÉÁöÑËØØÂ∑ÆÊù•Ëá™Á¨¨ $l$ Â±ÇÁöÑÂÖ®ÈÉ®Á•ûÁªèÂÖÉ $\\delta_j^{(l)}$ÔºåÊâÄ‰ª•ÈúÄË¶ÅÊ±ÇÂíå„ÄÇÊ†πÊçÆÈìæÂºèÊ≥ïÂàôÔºåËØØÂ∑Æ$\\mathbf e$ ‰ªé‰∏ä‰∏ÄÂ±Ç ($l$Â±Ç) ËøáÊù•ÔºåÊâÄ‰ª•È¶ñÂÖàÊòØ $s^{(l)}$ ÁöÑÂáΩÊï∞ÔºåÁÑ∂Âêé$s^{(l)}$ ÊòØÁ¨¨ $(l-1)$ Â±ÇÁ•ûÁªèÂÖÉ $x^{(l-1)}$ ÁöÑÂáΩÊï∞ÔºåÊúÄÂêé $x^{(l-1)}$ ÊâçÊòØÂÆÉËæìÂÖ•‰ø°Âè∑ $s^{(l-1)}$ ÁöÑÂáΩÊï∞Ôºö\n$$ \\begin{aligned} \\delta_i^{(l-1)} \u0026amp;= \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_i^{(l-1)}} \\ \u0026amp;=\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathbf{e}(\\mathbf{w})}{\\partial s_{j}^{(l)}} \\times \\frac{\\partial s_{j}^{(l)}}{\\partial x_{i}^{(l-1)}} \\times \\frac{\\partial x_{i}^{(l-1)}}{\\partial s_{i}^{(l-1)}} \u0026amp; \\text{Á¨¨$l$Â±ÇÊâÄÊúâÁ•ûÁªèÂÖÉËØØÂ∑ÆÊ±ÇÂíå}\\\n\u0026amp;=\\sum_{j=1}^{d^{(l)}} \\delta_{j}^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_{i}^{(l-1)}\\right) \\\n\\delta_{i}^{(l-1)} \u0026amp;=\\left(1-\\left(x_{i}^{(l-1)}\\right)^{2}\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)} \u0026amp; \\text{$\\theta$‰∏éjÊó†ÂÖ≥,Ê±ÇÂØºÊîæÂâçÈù¢; $x_i^{l-1}$‰πüÂ∞±ÊòØ$\\theta(s)$} \\end{aligned} $$\nÊâÄ‰ª•ÊúÄÂêé‰∏ÄÂ±Ç‰πãÂâçÂ±ÇÁöÑÁ•ûÁªèÂÖÉÁöÑ $\\delta$ Á≠â‰∫é 1 ÂáèÂéªËøô‰∏™Á•ûÁªèÂÖÉËæìÂá∫ÁöÑÂπ≥ÊñπÔºåÂÜç‰πò‰∏ä‰ªéÂÆÉÂá∫ÂèëÁöÑÂêÑÊùÉÈáç‰∏é‰∏ã‰∏ÄÂ±ÇÁöÑ$\\delta$ ÁöÑÂÜÖÁßØ‰πãÂíå„ÄÇÊúÄÂêé‰∏ÄÂ±ÇÁöÑ$\\delta^{(L)}$ÁÆóÂá∫Êù•‰∫ÜÔºåÊâçËÉΩÁÆóÂÄíÊï∞Á¨¨2Â±ÇÁöÑ$\\delta^{(L-1)}$Ôºå‰ªéËÄåÂèØ‰ª•ÂèçÂêëÂú∞‰∏ÄÂ±Ç‰∏ÄÂ±ÇÊ±ÇÂá∫ËØØÂ∑Æ$\\mathbf e$ ÂØπÂêÑ‰∏™ÊùÉÈáç w ÁöÑÊ¢ØÂ∫¶„ÄÇ\nBackpropagation algorithm:\nInitialize all weights $w_{ij}^{(l)}$ at random for t=0,1,2, \u0026hellip;, do //Âæ™ÁéØ Pick $n \\in { 1,2,\\cdots,N }$ //‰ªéN‰∏™Ê†∑Êú¨‰∏≠Êåë‰∏Ä‰∏™ Forward: Compute all $x_j^{(l)}$ //ËÆ°ÁÆóÊØè‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂá∫Ôºå‰ªéËÄåÂæóÂá∫È¢ÑÊµãÂÄº Backward: Compute all $\\delta_j^{l}$ //ËÆ°ÁÆóÊØè‰∏™Á•ûÁªèÂÖÉÁöÑËØØÂ∑Æ(Ë¥°ÁåÆ) Update the weights: $w_{ij}^{(i)} \\leftarrow w_{ij}^{l} - \\eta x_i^{(l-1)} \\delta_j^{(l)}$ //Ëø≠‰ª£Áõ¥Âà∞Êî∂Êïõ Iterate to the next step until it is time to stop Return the final weights $w_{ij}^{l}$ Final remark: hidden layers\nÈöêËóèÂ±ÇÊòØÂú®‚ÄúÊ®°‰ªø‚ÄùÈùûÁ∫øÊÄßÂèòÊç¢ÔºöÊääÈ´òÁª¥Ê†∑Êú¨ÔºàÁ∫øÊÄß‰∏çÂèØÂàÜÔºâÂèòÊç¢Âà∞Êñ∞ÁöÑÁª¥Â∫¶Á©∫Èó¥ÔºåÂè´ÂÅö‚Äúlearned nonlinear transform‚Äù„ÄÇÈöêËóèÂ±ÇÁöÑÊØè‰∏™Á•ûÁªèÂÖÉÊòØ \u0026ldquo;learned feature\u0026rdquo;„ÄÇ\nÁ•ûÁªèÂÖÉÊï∞ÈáèË∂äÂ§öÔºåËá™Áî±Â∫¶Ë∂äÂ§öÔºàÊúâÊïàÂèÇÊï∞Ë∂äÂ§öÔºâÔºåVCÁª¥Ë∂äÈ´òÔºåÊ®°ÂûãÂ§çÊùÇÂ∫¶Ë∂äÈ´òÔºåÈúÄË¶ÅÊõ¥Â§öÁöÑÊ†∑Êú¨ÔºåÊâçËÉΩ‰øùËØÅÂèØ‰ª•‰ªé $E_{in}$ Ê≥õÂåñÂà∞Eout.\nExampleÔºö Back PropagationÔºàÊ¢ØÂ∫¶ÂèçÂêë‰º†Êí≠ÔºâÂÆû‰æãËÆ≤Ëß£\n‰ª§ $x_1=1, x_2=0.5$ ÔºåÁÑ∂ÂêéÊàë‰ª¨‰ª§ $w_1, w_2, w_3, w_4$ ÁöÑÁúüÂÆûÂÄºÂàÜÂà´ÊòØ 1,2,3,4 Ôºå‰ª§ $w_5, w_6$ ÁöÑÁúüÂÆûÂÄºÊòØ $0.5, 0.6$ „ÄÇËøôÊ†∑Êàë‰ª¨ÂèØ‰ª•ÁÆóÂá∫ $y$ ÁöÑÁúüÂÆûÁõÆÊ†áÂÄºÊòØ $t=4$ „ÄÇ\nÈÇ£‰πà‰∏∫‰∫ÜÊ®°Êãü‰∏Ä‰∏™Back PropagationÁöÑËøáÁ®ãÔºåÊàë‰ª¨ÂÅáËÆæÊàë‰ª¨Âè™Áü•ÈÅì $x_1=1, x_2=0.5$ Ôºå‰ª•ÂèäÂØπÂ∫îÁöÑÁõÆÊ†á $t=4$ „ÄÇÊàë‰ª¨‰∏çÁü•ÈÅì $w_1,w_2,w_3,w_4,w_5,w_6$ ÁöÑÁúüÂÆûÂÄºÔºåÁé∞Âú®Êàë‰ª¨ÈúÄË¶ÅÈöèÊú∫‰∏∫‰ªñ‰ª¨ÂàùÂßãÂåñÂÄºÔºåÂÅáËÆæÊàë‰ª¨ÁöÑÈöèÊú∫ÂåñÁªìÊûúÊòØ $w_1=0.5, w_2=1.5, w_3=2.3, w_4=3, w_5=1, w_6=1$„ÄÇ\nForward: ËÆ°ÁÆó $h_1, h_2, y$ ÁöÑÈ¢ÑÊµãÂÄºÂíåËØØÂ∑ÆÈ°π EÔºåÂÖ∂‰∏≠ $E=\\frac{1}{2}(t-y)^2$\n$$ \\begin{aligned} h_1 \u0026amp;= w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0.5 \\cdot 1 + 1.5 \\cdot 0.5 = 1.25 \\ h_2 \u0026amp;= w_3 \\cdot x_1 + w_4 \\cdot x_2 = 2.3 \\cdot 1 + 3 \\cdot 0.5 = 3.8 \\ y \u0026amp;= w_5 \\cdot h_1 + w_6 \\cdot h_2 = 1 \\cdot 1.25 + 1 \\cdot 3.8 = 5.05 \\ E \u0026amp;= \\frac{1}{2} (y-t)^2 = 0.55125 \\end{aligned} $$\nBackward\nupdata $w_5$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_5} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_5} = (y-t)\\cdot h_1 = 1.05 \\cdot 1.25 =1.3125 \\\n\\frac{\\partial E}{\\partial y} \u0026amp;= (t-y)\\cdot -1 = y-t \\ \\frac{\\partial y}{\\partial w_5} \u0026amp;= \\frac{\\partial (w_5 h_1 + w_6 h_2)}{\\partial w_5} = h_1 \\ w_5^+ \u0026amp;= w_5 - \\eta \\cdot \\frac{\\partial E}{\\partial w_5} = 1-0.1\\cdot 1.3125 = 0.86875 \\end{aligned} $$\nupdata $w_6$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_6} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_6} = (y-t)\\cdot h_2 = 1.05 \\cdot 3.8 = 3.99 \\ w_6^+ \u0026amp;= w_6 -\\eta \\cdot \\frac{\\partial E}{\\partial w_6} = 1-0.1\\cdot 3.99 = 0.601 \\end{aligned} $$\n‰∏ãÈù¢Êàë‰ª¨ÂÜçÊù•Áúã $w_1, w_2, w_3, w_4$ ÔºåÁî±‰∫éËøôÂõõ‰∏™ÂèÇÊï∞Âú®Âêå‰∏ÄÂ±ÇÔºåÊâÄ‰ª•Ê±ÇÊ¢ØÂ∫¶ÁöÑÊñπÊ≥ïÊòØÁõ∏ÂêåÁöÑ\nupdata $w_1$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_1} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_1} = (y-t) \\cdot w_5 \\cdot x_1 = 1.05 \\cdot 1 \\cdot 1 = 1.05 \\\nw_1^+ \u0026amp;= w_1 - \\eta \\cdot \\frac{\\partial E}{\\partial w_1} = 0.5 - 0.1 \\cdot 1.05 = 0 .395 \\end{aligned} $$\nupdata $w_2$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_2} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_2} = (y-t) \\cdot w_5 \\cdot x_2 = 1.05 \\cdot 1 \\cdot 0.5 = 0.525 \\\nw_2^+ \u0026amp;= w_2 - \\eta \\cdot \\frac{\\partial E}{\\partial w_2} = 1.5 - 0.1 \\cdot 0.525 = 1.4475 \\end{aligned} $$\nupdata $w_3$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_3} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_3} = (y-t) \\cdot w_6 \\cdot x_1 = 1.05 \\cdot 1 \\cdot 1 = 1.05 \\\nw_3^+ \u0026amp;= w_3 - \\eta \\cdot \\frac{\\partial E}{\\partial w_3} = 2.3 - 0.1 \\cdot 1.05 = 2.195 \\end{aligned} $$\nupdata $w_4$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_4} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_4} = (y-t) \\cdot w_6 \\cdot x_2 = 1.05 \\cdot 1 \\cdot 0.5 = 0.525 \\\nw_4^+ \u0026amp;= w_4 - \\eta \\cdot \\frac{\\partial E}{\\partial w_4} = 3 - 0.1 \\cdot 0.525 = 2.9475 \\end{aligned} $$\nForward:\n$$ \\begin{aligned} h_1 \u0026amp;= w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0.395 \\cdot 1 + 1.4475 \\cdot 0.5 = 1.11875 \\ h_2 \u0026amp;= w_3 \\cdot x_1 + w_4 \\cdot x_2 = 2.195 \\cdot 1 + 2.9475 \\cdot 0.5 = 3.66875 \\ y \u0026amp;= w_5 \\cdot h_1 + w_6 \\cdot h_2 = 0.97191 + 2.204918 = 3.17683 \\ E \u0026amp;= \\frac{1}{2} (y-t)^2 = 0.338802 \\end{aligned} $$\n","date":"2021-12-14T00:53:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec10_neural_networks/","title":"watch: AML 10 | Neural Networks"},{"content":"Video 12 Bias-Variance Tradeoff 11-01-2021\nOutline:\nBias and Variance Learning Curves Review of Lec 7\n$d_{VC}(\\mathcal H)$: the most number of points $\\mathcal H$ can shatter $d_{VC}$ÊòØÊúâÈôêÂÄºÔºåËÆ© g Ëøë‰ºº $f$ Êàê‰∏∫ÂèØËÉΩ„ÄÇVCÁª¥‰ªÖÁî±ÂÅáËÆæÈõÜÂÜ≥ÂÆö„ÄÇ ‰∏∫‰∫ÜÈôç‰ΩéÂà∞Êüê‰∏ÄÊ¶ÇÁéáÔºå$d_{VC}$Ë∂äÂ§ßÔºåÊâÄÈúÄÊ†∑Êú¨ÁÇπÊï∞NË∂äÂ§ö„ÄÇ$N\\geq 10 d_{VC}$ $E_{out} \\leq E_{in}+\\Omega$ Generalization bound $\\Omega(N,\\mathcal H, \\delta))$: Bias and Variance Approximation-generalization tradeoff Ëøë‰ºº‰∏éÊ≥õÂåñÁöÑÊùÉË°°\nÂ∞èÁöÑEout ÊÑèÂë≥ÁùÄgÂú® out-of-sample ‰∏ä‰πüÊòØfÁöÑ‰∏Ä‰∏™Â•ΩÁöÑËøë‰ººÔºàÊ†∑Êú¨Â§ñËØØÂ∑Æ‰πüÂæàÂ∞èÔºâ„ÄÇ Ë∂äÂ§çÊùÇÁöÑÂÅáËÆæÈõÜ $\\mathcal H$ÔºàMË∂äÂ§ßÔºâÔºåÊúâÊõ¥Â•ΩÁöÑÊú∫‰ºöËøë‰ºº $f$ÔºàÊõ¥ÂèØËÉΩÂåÖÂê´ÊúÄ‰Ω≥ÂÅáËÆægÔºâ Ë∂äÁÆÄÂçïÁöÑÂÅáËÆæÈõÜ $\\mathcal H$ÔºåÊúâÊõ¥Â•ΩÁöÑÊú∫‰ºöÂú®out-of-sample‰∏äÊ≥õÂåñ„ÄÇ ÊúÄÁêÜÊÉ≥ÊÉÖÂÜµÔºöÂÅáËÆæÈõÜ‰∏≠Âè™ÂåÖÂê´‰∏Ä‰∏™Ê≠£Âú®ÂØªÊâæÁöÑ‚ÄúÊú™Áü•ÁöÑÁõÆÊ†áÂáΩÊï∞‚Äù $\\mathcal H={f}$Ôºåg ‰πüÂ∞±ÊòØf„ÄÇ Quantifying the tradeoff ‰πãÂâçÁöÑ VC analysis ÊòØ‰∏ÄÁßçËØÑ‰º∞ÊñπÊ≥ïÔºö$E_{out} \\leq E_{in}+\\Omega$ ‰∏é‰πãÁõ∏‰ººÔºåBias-variance analysis ÊòØÂè¶‰∏ÄÁßçËØÑ‰º∞ÊñπÊ≥ïÔºöÊää Eout ÂàÜËß£Êàê‰∏§È°πÔºö ÂÅáËÆæÈõÜ$\\mathcal H$ ËÉΩÊúâÂ§öËøë‰ºº $f$ ÔºàBiasÔºâ ËÉΩÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÁ°ÆÂÆö$\\mathcal H$‰∏≠ÁöÑÂ•ΩÁöÑÂÅáËÆæ ÔºàVarianceÔºâ ËøôÈáåÂàÜÊûêÁöÑÁõÆÊ†áÂáΩÊï∞ÊòØÂÆûÂÄºÁöÑ real-valued, Âπ∂‰∏î‰ΩøÁî®Âπ≥ÊñπËØØÂ∑Æ squared error Start with $E_{out}$ Eout ÊòØÂÅáËÆæÈõÜ‰∏≠ÁöÑÊúÄ‰Ω≥ÂÅáËÆæ $g$ ‰∏é Êú™Áü•ÁõÆÊ†áÂáΩÊï∞ $f$ Âú®ËæìÂÖ•Á©∫Èó¥ $\\mathcal X$ ÁöÑÂêÑ‰∏™ÁÇπ‰∏äÁöÑÂ∑ÆË∑ùÁöÑÊúüÊúõÔºõg ÊòØÊ†∑Êú¨ÈõÜ $\\mathcal D$ ÁöÑÂáΩÊï∞ÔºåÊ†∑Êú¨ÈõÜ‰∏çÂêåÔºåÈÄâÂá∫Êù•ÁöÑÊúÄ‰Ω≥ÂÅáËÆæ g ‰πü‰∏çÂêåÔºö\n$$ E_{out}(g^{(D)}) = \\mathbb E_{\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] $$\nÂõ†‰∏∫ÊØèÊ¨°‰ªéËæìÂÖ•Á©∫Èó¥ÊäΩÂá∫ÁöÑÊ†∑Êú¨ÈõÜ $\\mathcal D$ ‰∏ç‰∏ÄÊ†∑ÔºågÂ∞±‰∏ç‰∏ÄÊ†∑ÔºåEout ‰πüÂ∞±‰∏ç‰∏ÄÊ†∑ÔºåÊâÄ‰ª•ÊääÂêÑ‰∏™EoutÊ±Ç‰∏™ÊúüÊúõÔºå‰Ωú‰∏∫ÊúÄÁªàÁöÑEoutÔºö\n$$ \\begin{aligned} \\mathbb E_{\\mathcal D} \\left[ E_{out} \\left( g^{(\\mathcal D)} \\right) \\right] = \\mathbb E_{\\mathcal D} \\left[ \\mathbb E_{\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \\\n= \\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \u0026amp; \\text{(‰∫§Êç¢‰ΩçÁΩÆ)} \\ \\end{aligned} $$\nÂè™ÂÖ≥Ê≥®ÂÖ∂‰∏≠ \u0026ldquo;1‰∏™ÁÇπ‰∏äÁöÑÂπ≥ÂùáËØØÂ∑ÆÊúüÊúõ\u0026rdquo;Ôºö$\\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$\nÂÆö‰πâÂπ≥ÂùáÂÅáËÆæ (average hypothesis)Ôºö$\\bar{g}(\\mathbf x) = \\mathbb E_{\\mathcal D} \\left[ g^{(\\mathcal D)}(\\mathbf x) \\right]$ (the best thing you can do)Ôºå‰ªéÂêÑ‰∏çÂêåËÆ≠ÁªÉÈõÜ‰∏äÂæóÂá∫ÁöÑÊúÄ‰Ω≥ÂÅáËÆæÁöÑÂπ≥ÂùáÂÄº„ÄÇ\nÊØîÂ¶ÇÊúâ K ‰∏™ËÆ≠ÁªÉÈõÜÔºö$\\mathcal D_1, \\mathcal D_2, \\cdots, \\mathcal D_K$ÔºåÈÇ£‰πàÂπ≥ÂùáÂÅáËÆæÂ∞±ÊòØÔºö$\\bar{g}(\\mathbf x) \\approx \\frac{1}{K} \\sum_{k=1}^K g^{\\mathcal D_k}(\\mathbf x)$\nÊääÂπ≥ÂùáÂÅáËÆæ‰ª£ÂÖ•\u0026quot;1‰∏™ÁÇπ‰∏äÁöÑÂπ≥ÂùáËØØÂ∑ÆÊúüÊúõ\u0026quot;Ôºö\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\ \u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) -\\bar{g}(\\mathbf x) + \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\quad \\text{(Âáè‰∏Ä‰∏™Âä†‰∏Ä‰∏™)} \\\n\u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 + \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 + 2 \\left( g^{(D)}(\\mathbf x)-\\bar{g}(\\mathbf x) \\right) \\left( \\bar{g}(\\mathbf x) -f(\\mathbf x) \\right) \\right] \\quad \\text{(‰ª£ÂÖ•Êã¨Âè∑)} \\\n\u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] + \\underbrace{\\mathbb E_{\\mathcal D} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]}{‰∏éDÊó†ÂÖ≥,ÊúüÊúõËøòÊòØËá™Â∑±} + 2 \\left( \\underbrace{ \\mathbb E{\\mathcal D} \\left[ g^{(D)}(\\mathbf x) \\right] -\\bar{g}(\\mathbf x) }_{Áõ∏Á≠â, =0} \\right) \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right) \\\n\u0026amp; = \\underbrace{ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] }{\\rm var(\\mathbf x)} + \\underbrace{ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 }{\\rm bias(\\mathbf x)} \\end{aligned} $$\n$\\bar{g}(\\mathbf x)$ ÊòØ\u0026quot;ÊúÄ‰Ω≥ÂÅáËÆæ\u0026quot;ÔºåÂÆÉ‰∏éÁõÆÊ†áÊú™Áü•ÂáΩÊï∞ÁöÑÂ∑ÆÊòØÂ∏∏Êï∞ bias (‰∏éDÊó†ÂÖ≥)ÔºåËÄå $g^{\\mathcal D}(\\mathbf x)$ ÈöèËÆ≠ÁªÉÈõÜ‰∏çÂêåÔºå‰ºö‰∏ä‰∏ãÊ≥¢Âä®Ôºå‰∏é\u0026quot;Âπ≥ÂùáÂÄº\u0026quot;ÁöÑÂ∑ÆÁöÑÂπ≥ÊñπÔºåÂÜçÂèñÂπ≥ÂùáÂ∞±ÊòØÊñπÂ∑Æ„ÄÇ ÂêÑ‰∏™ÊúÄ‰Ω≥ÂÅáËÆæ‰∏éÁõÆÊ†áÊú™Áü•ÂáΩÊï∞ÁöÑÂπ≥ÊñπËØØÂ∑ÆÁöÑÊúüÊúõÔºåË¢´ÊãÜÊàê‰∫Ü‰∏§ÈÉ®ÂàÜÔºöÂêÑÊúÄ‰Ω≥ÂÅáËÆæ‰∏éÂπ≥ÂùáÂÅáËÆæÁöÑÊñπÂ∑ÆÔºåÂä†‰∏äÂπ≥ÂùáÂÅáËÆæ‰∏éÁõÆÊ†áÊú™Áü•ÂáΩÊï∞ÁöÑÂπ≥ÊñπËØØÂ∑Æ„ÄÇ\nÊâÄ‰ª• Eout Á≠â‰∫éÔºö\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D} \\left[ E_{out} (g^{(\\mathcal D)}) \\right] \\ \u0026amp; = \\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \\ \u0026amp; = \\mathbb E_{\\mathbf x} \\left[ \\rm bias(\\mathbf x) + var(\\mathbf x) \\right] \\ \u0026amp; = \\rm bias + var \\end{aligned} $$\nThe trade off between bias and var bias = $\\mathbb E_{\\mathbf x} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$\nvariance = $\\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] \\right]$\nÂ¶ÇÊûúÂÅáËÆæÈõÜ‰∏≠Âè™Êúâ‰∏Ä‰∏™ÂÅáËÆæ hÔºåÂÆÉ‰∏é $f$ ÁöÑË∑ùÁ¶ªÂ∞±ÊòØ biasÔºàÊñπÂ∑Æ‰∏∫0Ôºâ„ÄÇÂ¶ÇÊûúÊòØ‰∏Ä‰∏™Â§çÊùÇÁöÑÂÅáËÆæÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂæàÂ§öÂÅáËÆæÔºåÊõ¥ÊúâÂèØËÉΩÂõäÊã¨‰∫Ü fÔºåÂú®ÂÆÉÈôÑËøëÁöÑÈÉΩÊòØ‚ÄúÊúÄ‰Ω≥ÂÅáËÆæ‚ÄùÔºàÁ∫¢Ëâ≤ÔºâÔºåÂõ†‰∏∫Âπ≥ÂùáÂÅáËÆæ$\\bar g(\\mathbf x)$Â∞±Âú® f ÈôÑËøëÊâÄ‰ª•biasËæÉÂ∞èÔºåËÄåÊñπÂ∑ÆÊØîËæÉÂ§ßÔºàÂæàÂ§öÂ∞èÂÄºÂä†Ëµ∑Êù•‰πü‰ºöÂ§ßÔºâ„ÄÇ\nExample: sine target Ëøë‰ººÊ≠£Âº¶Êõ≤Á∫øÔºåÊú™Áü•ÁõÆÊ†áÂáΩÊï∞ $f(x) = sin(\\pi x)$ÔºåÊää f ‰ªéËæìÂÖ•Á©∫Èó¥‰ªé [-1,1] Êâ©Â±ïÂà∞ÂÆûÊï∞Âüü $f:[-1,1] \\rightarrow \\mathbb R$„ÄÇÂè™Êúâ‰∏§‰∏™Ê†∑Êú¨ÁÇπ N=2„ÄÇ\nÊúâ‰∏§‰∏™ÂÅáËÆæÈõÜÔºåËá™Áî±Â∫¶‰∏çÂêåÔºö\n$$ \\begin{aligned} \u0026amp; \\mathcal H_0 : h(x) = b \u0026amp;{\\text{ÂêÑÂÅáËÆæÂè™Êúâ‰∏Ä‰∏™ÂèÇÊï∞b (Â∏∏Êï∞)}} \\ \u0026amp; \\mathcal H_1 : h(x) = ax+b \u0026amp;{\\text{ÂêÑÂÅáËÆæÊúâ‰∏§‰∏™ÂèÇÊï∞a,b (Áõ¥Á∫ø)}} \\end{aligned} $$\nApproximation: ‰∏äÂ∏ùËßÜËßíÂèØ‰ª•ÁúãÂá∫‰∏§‰∏™ÂÅáËÆæÈõÜ‰∏≠ÁöÑÊúÄ‰Ω≥ÂÅáËÆæ(biasÊúÄÂ∞è)ÂàÜÂà´Â∫îËØ•‰∏∫Ôºö\nÈªÑËâ≤Âå∫ÂüüÊòØ bias (ÊàñËÄÖËØ¥Â∞±ÊòØ Eout, Âõ†‰∏∫Âçï‰∏™ÂÅáËÆæÁöÑÊñπÂ∑Æ‰∏∫0)„ÄÇÊâÄ‰ª•Âú®Ëøë‰ºº[-1,1]Âå∫Èó¥‰∏äÁöÑÊ≠£Âº¶ÂáΩÊï∞Êó∂Ôºå$\\mathcal H_1$ ÊØî $\\mathcal H_0$ ÁöÑ bias Êõ¥Â∞è„ÄÇ\nLearning: ‰∏§‰∏™Ê†∑Êú¨ÁÇπÁöÑ‰ΩçÁΩÆÊòØÈöèÊú∫ÁöÑ\nÂ¶ÇÊûú‰∏ÄÂºÄÂßã‰∏§‰∏™Ê†∑Êú¨ÁÇπ‰ΩçÁΩÆÂ¶ÇÂõæÔºö\n‰∏∫‰∫Ü‰Ωø bias ÊúÄÂ∞èÔºå$\\mathcal H_0$‰∏≠ÁöÑ‚ÄúÊúÄ‰Ω≥ÂÅáËÆæ‚ÄùÂ∫îËØ•Âú®‰∏§Ê†∑Êú¨ÁÇπ‰∏≠Èó¥Ôºå$\\mathcal H_1$ ‰∏≠ÁöÑ‚ÄúÊúÄ‰Ω≥ÂÅáËÆæ‚ÄùÂ∫îËØ•Á©øËøá‰∏§‰∏™Ê†∑Êú¨ÁÇπ„ÄÇ\n‰∏§‰∏™Ê†∑Êú¨ÁÇπÔºàËÆ≠ÁªÉÈõÜÔºâÊØèÊ¨°‰ªéËæìÂÖ•Á©∫Èó¥‰∏≠ÂèñÁöÑÈÉΩ‰∏ç‰∏ÄÊ†∑ÔºåÂØπÂ∫îÁöÑ‚ÄúÊúÄ‰Ω≥ÂÅáËÆæ‚Äù‰πüÊúâÂæàÂ§öÁßçÂèØËÉΩÔºö\nÂØπ‰∫éÂÅáËÆæÈõÜ $\\mathcal H_0$ÔºåÊúÄ‰Ω≥ÂÅáËÆæÁöÑÂàÜÂ∏ÉÂ¶ÇÂõæÔºö\nÂØπÂêÑ‰∏™‚ÄúÊúÄ‰Ω≥ÂÅáËÆæ‚ÄùÂèñÂπ≥ÂùáÔºåÂπ≥ÂùáÂÅáËÆæ‰Ωç‰∫é‚ÄùÂπ≥Ë°°‰ΩçÁΩÆ‚ÄúÔºåÁÅ∞Ëâ≤Âå∫ÂüüÊòØ variation.\nÂØπ‰∫éÂÅáËÆæÈõÜ $\\mathcal H_1$ÔºåÊúÄ‰Ω≥ÂÅáËÆæÔºàËøá‰∏§Ê†∑Êú¨ÁÇπÁöÑÁõ¥Á∫øÔºâÁöÑÂàÜÂ∏ÉÂ¶ÇÂõæÔºö\nÂπ≥ÂùáÂÅáËÆæÊòØÁ∫¢Ëâ≤Áõ¥Á∫øÔºåÁÅ∞Ëâ≤Âå∫ÂüüÊòØvariation„ÄÇ\nÂØπÊØî‰∏§‰∏™ÂÅáËÆæÈõÜÔºö\nÂè™Êúâ‰∏Ä‰∏™ÂèÇÊï∞ÁöÑÔºåÊúÄÁÆÄÂçïÁöÑÔºàÂ∏∏Êï∞ÔºâÂÅáËÆæÈõÜ $\\mathcal H_0$ ÁöÑ(Âπ≥ÂùáÂÅáËÆæ) biasÂ§ßÔºåÊñπÂ∑ÆÂ∞è„ÄÇËÄåÊØîËæÉÂ§çÊùÇÁöÑÔºàÁõ¥Á∫øÔºâÂÅáËÆæÈõÜ $\\mathcal H_1$ ÁöÑ bias Â∞èÔºåÊñπÂ∑ÆÂ§ß„ÄÇ\nÊúÄÁªàÔºå$\\mathcal H_0$ ÁöÑ Eout= 0.50+0.25 = 0.75Ôºå$\\mathcal H_1$ ÁöÑ Eout=0.21+1.69 = 1.9„ÄÇÊ†πÊçÆ EoutÔºåÁÆÄÂçïÁöÑÂÅáËÆæÈõÜ $\\mathcal H_0$ Â•Ω‰∫éÂ§çÊùÇÁöÑÂÅáËÆæÈõÜ $\\mathcal H_1$„ÄÇÂõ†‰∏∫Êàë‰ª¨ÊòØ‰ªé Ein \u0026ldquo;Ê≥õÂåñ\u0026rdquo; Âà∞ outÔºåÂ¶ÇÊûúEout Â§™Â§ßÔºåGeneralization bound Â§™Â§ßÔºåEin ‰∏é Eout Áõ∏Â∑ÆÂ§™Â§ßÔºå‰∫åËÄÖ‰∏çfollowÔºåÂ∞±‰∏çËÉΩÈÄöËøáEin Â≠¶‰π†Âà∞ Eout.\nÂ§çÊùÇÁöÑÔºàËá™Áî±Â∫¶Â§öÁöÑÔºâÂÅáËÆæÈõÜÊúâÂæàÂ•ΩËøë‰ººËÉΩÂäõÔºå‰ΩÜÊòØÂ≠¶‰π†ËÉΩÂäõÂæàÂ∑ÆÔºåÂõ†‰∏∫ÊñπÂ∑ÆÂ§™Â§ßÔºå‰∏ç‰∏ÄÂÆöËÉΩÂ≠¶Âà∞ÊúÄ‰Ω≥ÂÅáËÆæ„ÄÇ\nLesson learned: Ê®°ÂûãÁöÑÂ§çÊùÇÂ∫¶Â∫îËØ•ÂåπÈÖç Êï∞ÊçÆÔºàÊ†∑Êú¨ÂÜ≥ÂÆö‰∫ÜÊúÄÁªàÊâæÂá∫ÁöÑÂÅáËÆæÔºâÔºåËÄå‰∏çÂ∫îËØ•ÂåπÈÖçÁõÆÊ†áÂáΩÊï∞ÁöÑÂ§çÊùÇÂ∫¶„ÄÇÊØîÂ¶ÇÂè™Êúâ15‰∏™Ê†∑Êú¨ÔºåÂÅáËÆæÂ∑≤Áü•ÁõÆÊ†áÂáΩÊï∞ÊòØ10Èò∂ÁöÑ„ÄÇ‰Ω†ÂèØ‰ª•ÈÄâ1Èò∂Ôºå2Èò∂ÁöÑÊ®°ÂûãÔºåÂÆÉ‰ª¨ÂØπÂ∫îÁöÑÂèÇÊï∞Êúâ2‰∏™Ôºå3‰∏™ÔºåÊåâÁÖßÁªèÈ™åÊ≥ïÂàô $N\u0026gt;10 d_{VC}$ÔºåÂÆÉ‰ª¨Ëá≥Â∞ëÈúÄË¶Å20‰∏™Ôºå30‰∏™Ê†∑Êú¨„ÄÇ‰ΩÜÁé∞Âú®Âè™Êúâ15‰∏™ÔºåÂ¶ÇÊûú‰Ω†ËßâÂæó1Èò∂ÔºàÁõ¥Á∫øÔºâ‰∏çÂ§™ÂèØËÉΩÁöÑËØùÔºåÂèØ‰ª•Áî®2Èò∂Ôºà‰∫åÊ¨°ÂáΩÊï∞Ôºâ„ÄÇÂ¶ÇÊûúÁî®10Èò∂Ê®°ÂûãÔºåÂ∞±Âá∫Áé∞ËøáÊãüÂêà‰∫Ü„ÄÇ\nLearning Curves Ein ‰∏é Eout ÁöÑÊõ≤Á∫ø„ÄÇ\nÂØπ‰∫é‰ªªÊÑèÊúâN‰∏™(ËÆ≠ÁªÉ)Ê†∑Êú¨Êï∞ÊçÆÈõÜ $\\mathcal D$„ÄÇ\nExpected Eout = $\\mathbb E_{\\mathcal D} \\left[ E_{out} \\left( g^{(\\mathcal D)} \\right) \\right]$ (‰∏çÂêåÊµãËØïÈõÜ‰∏äÁöÑÊúÄ‰Ω≥ÂÅáËÆæ g ÁöÑÂπ≥Âùá)\nExpected Ein = $\\mathbb E_{\\mathcal D} \\left[ E_{in} \\left( g^{(\\mathcal D)} \\right) \\right]$ (‰∏çÂêå(ËÆ≠ÁªÉ)Êï∞ÊçÆÈõÜ‰∏äÁöÑÊúÄ‰Ω≥ÂÅáËÆæ g ÁöÑÂπ≥Âùá)\nÂÆÉ‰ª¨Èöè N Â¶Ç‰ΩïÂèòÂåñÔºü(How do they vary with N?)\nÂØπ‰∫éSimple ModelÔºàÁöÑÂÅáËÆæÈõÜÔºâÔºåÈöèÁùÄÊ†∑Êú¨Êï∞‰∏çÊñ≠Â¢ûÂä†ÔºåEoutË∂äÊù•Ë∂äÂ∞èÔºåË∂äÊù•Ë∂äËøë‰ºº fÔºåËÄå EinË∂äÊù•Ë∂äÂ§ßÔºåÂõ†‰∏∫ÊØè‰∏™Ê†∑Êú¨ÈÉΩÊúâËØØÂ∑ÆÔºåÊ†∑Êú¨Ë∂äÂ§öÂä†Ëµ∑Êù•Ë∂äÂ§ß„ÄÇNË∂äÂ§ßÔºåEin‰∏éEoutË∂äÊé•ËøëÔºåGeneralized boundË∂äÂ∞èÔºåÊî∂Êïõ‰∫é ‚ÄúÂπ≥ÂùáÂÅáËÆæ‚Äù (ÈªëËâ≤Ê∞¥Âπ≥Á∫ø)„ÄÇ\nÂØπ‰∫éComplex ModelÔºå‚ÄúÂπ≥ÂùáÂÅáËÆæ‚ÄùÊõ¥Èù†Ëøë fÔºåbiasËæÉÂ∞èÔºåÊâÄ‰ª•ÈªëËâ≤Ê∞¥Âπ≥Á∫øÂÆÉÊõ¥‰ΩéÔºåÂêåÊ†∑ÈöèÁùÄNÂ¢ûÂ§ßÔºåEout‰∏éEin ‰∏çÊñ≠Ë∂ãËøë‰∫é ‚ÄúÂπ≥ÂùáÂÅáËÆæ‚Äù„ÄÇ‰ΩÜÊòØÂΩìNÂæàÂ∞èÁöÑÊó∂ÂÄôÔºåEoutÂæàÂ§ßÔºàÊñπÂ∑ÆÂæàÂ§ßÔºâ„ÄÇEout ‰∏éEin Â∑ÆË∑ùÂæàÂ§ßÔºåÂ§çÊùÇÊ®°ÂûãÁõ∏ËæÉ‰∫éÁÆÄÂçïÊ®°ÂûãÁöÑ Generalization bound Êõ¥Â§ß„ÄÇÂ§çÊùÇÊ®°ÂûãÁöÑ Ein Âú®Ê†∑Êú¨ÂæàÂ∞ëÁöÑÊó∂ÂÄôÊòØÈõ∂ÔºåÂõ†‰∏∫Âú®Ê†∑Êú¨Êï∞Â∞è‰∫éVCÁª¥ÊàñËÄÖÂÅáËÆæÈõÜÁöÑeffectiveÂèÇÊï∞Ëá™Áî±Â∫¶Êó∂ÔºåÊ®°ÂûãÂèØ‰ª•ÊääÊâÄÊúâÁÇπÂÖ®ÈÉ®ÂàÜÂºÄ (shatter all the points)„ÄÇÂú®Ë∂ÖËøáVCÁª¥‰πãÂêéÔºåEinÂºÄÂßãÂ¢ûÂä†„ÄÇ\nVC vs Bias-variance Âú® VC ÂàÜÊûê‰∏≠Ôºå$E_{out} = E_{in} + \\text{Generalization error}$\nÂú® Bias-variance ‰∏≠Ôºå‰∏çÂÜçÂÖ≥Ê≥®$E_{in}$ÔºåÂõ†‰∏∫ $E_{out}=\\text{var + bias} = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] +\\mathbb E_{\\mathcal D} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$ÔºåEout Á≠â‰∫é‚ÄúÁ≤óÈªëÁ∫ø‚Äù(average hypothesis, bias)Âä†‰∏ävariance„ÄÇbiasÂèñÂÜ≥‰∏éÂÅáËÆæÈõÜËÄå‰∏éNÊó†ÂÖ≥ÔºåÊâÄ‰ª•biasÊòØÁõ¥Á∫ø„ÄÇ\nÈöèÁùÄNÂ¢ûÂ§ßÔºåÂÆÉ‰ª¨ÈÉΩË∂ãËøë‰∫éÂπ≥ÂùáÂÅáËÆæ„ÄÇÊâÄ‰ª•ÈÉΩÈúÄË¶ÅtradeoffÔºöÁÆÄÂçïÁöÑÊ®°ÂûãËøë‰ººËÉΩÂäõÂ∑ÆÔºå‰ΩÜÂÆÉ Generalization errorÂ∞èÔºõÂ§çÊùÇÁöÑÊ®°ÂûãËøë‰ººËÉΩÂäõÂº∫Ôºå‰ΩÜÂÆÉÈúÄË¶ÅÊõ¥Â§öÁöÑÊ†∑Êú¨ÔºåÊâçËÉΩÂáèÂ∞è Generalization bound„ÄÇÂØπ‰∫éÂ§çÊùÇÁöÑÊ®°ÂûãÔºåÊ†∑Êú¨Êï∞Ë∂äÂ∞ëÔºåEoutË∂äÂ§ß„ÄÇÊ†∑Êú¨Â∞ëÁöÑÊó∂ÂÄôÔºåÁÆÄÂçïÊ®°ÂûãÁöÑEout ÂèØËÉΩÊØîÂ§çÊùÇÊ®°ÂûãÁöÑ Eout ËøòË¶ÅÂ∞è„ÄÇÊâÄ‰ª•Ê†∑Êú¨Â∞ëÈÄâÁÆÄÂçïÊ®°ÂûãÔºåÊ†∑Êú¨Â§öÈÄâÂ§çÊùÇÊ®°Âûã„ÄÇ\nLinear Regression case Noisy target $y = \\mathbf w^{*T} \\mathbf x$ + noise ÔºàÁî®Á∫øÊÄßÊ®°Âûã, ‰ªéÊúâÂô™Â£∞ÁöÑÊ†∑Êú¨‰∏≠ÔºåÂ≠¶‰π†‰∏Ä‰∏™Á∫øÊÄßÁõÆÊ†áÂáΩÊï∞Ôºâ\nData set $\\mathcal D = { (\\mathbf x_1, y_1), \\cdots, (\\mathbf x_N, y_N) }$\nLinear regression solution: $\\mathbf w = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf y$\nIn-sample error vector = $\\bf X w - y$\n\u0026ldquo;Out-of-sample\u0026rdquo; error vector = $\\bf X w - y\u0026rsquo;$ Ôºà‰ΩøÁî®Áõ∏ÂêåÁöÑx, noise‰∏çÂêåÔºåÂæóÂà∞ÊµãËØïÈõÜÔºâ\nÊúâ‰∫Ü‰∏äÈù¢ÈùûÂ∏∏ÁâπÊÆäÁöÑÊÉÖÂÜµÔºåÊâçËÉΩÂæóÂà∞‰∏ãÈù¢ÁöÑÂÖ¨ÂºèÔºö\n$\\sigma^2$ ÊòØ energy of the noise„ÄÇ‰∏Ä‰∏™ zero-mean noiseÁöÑenergy ‰∏évariance ÊàêÊ≠£ÊØîÔºå\n","date":"2021-12-13T15:53:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec8_bias_variance/","title":"watch: AML 08 | Bias-Variance Tradeoff"},{"content":"Video 14 Overfitting 2021-11-17\nOutline:\nWhat is overfitting? The role of noise Deterministic noise Dealing with overfitting An Illustration of Overfitting on a Simple Example\nËìùËâ≤Êõ≤Á∫øÊòØ unknowen target function„ÄÇ‰ªéËøôÊú™Áü•ÂáΩÊï∞‰∏≠ÁîüÊàê5‰∏™Ê†∑Êú¨ÁÇπÔºåÂõ†‰∏∫ÊúâÂô™Â£∞ÔºåÊâÄ‰ª•ÂÆÉ‰ª¨ÂÅèÁ¶ª‰∫ÜÊõ≤Á∫ø„ÄÇ Ê†πÊçÆËøô5‰∏™ÁÇπÂéªËøë‰ººÊú™Áü•ÂáΩÊï∞„ÄÇÂõ†‰∏∫Âô™Â£∞ÁöÑÂ≠òÂú®ÔºåÂú®Áî®4Èò∂ÁöÑÂ§öÈ°πÂºèÔºàÊúâ5‰∏™ÂèÇÊï∞ÔºâÊãüÂêàËøôÁîüÊàêÁöÑ5‰∏™ÁÇπÊó∂ÔºåÂá∫Áé∞‰∫ÜËøáÊãüÂêàÔºö$E_{in}=0;\\ E_{out} \\gg 0$„ÄÇÔºàÂ¶ÇÊûúÊ≤°ÊúâÂô™Â£∞ÔºåÊúâÂèØËÉΩ‰ΩøÁî®Áõ∏ÂêåÈò∂Êï∞ÁöÑÊ®°ÂûãÂ∞±ËÉΩÂÆåÁæéÊãüÂêà„ÄÇ‰∏∫‰∫ÜÊãüÂêàÂ∏¶Âô™Â£∞ÁöÑÊï∞ÊçÆÔºåÂ∞±‰ΩøÁî®‰∫ÜË∂ÖËøáÊ†∑Êú¨ÊâÄË°®Ëææ‰ø°ÊÅØÁöÑÈ´òÈò∂Ê®°ÂûãÔºåÈÄ†ÊàêEout ÂæàÂ§ßÔºâ\nWhat is Overfitting? ÈöèÁùÄVC Áª¥ÁöÑÂ¢ûÂä†Ôºå$E_{in}$ ‰∏çÊñ≠‰∏ãÈôçÔºå$E_{out}$ÂèçËÄå‰∏äÂçá„ÄÇ\nÊãüÂêàÊï∞ÊçÆÁöÑÁ®ãÂ∫¶Ë∂ÖËøá‰∫ÜÊ†∑Êú¨Êï∞ÊçÆÂ∫îÊúâ(ÊîØÊåÅ)ÁöÑÁ®ãÂ∫¶ (Fitting the data more than is warranted)\nÈöèÁùÄ VC Áª¥ÁöÑÂ¢ûÂä†ÔºåÂÅáËÆæÈõÜ$\\mathcal H$‰∏≠ÁöÑÊ®°ÂûãÂ§çÊùÇÂ∫¶Â¢ûÂä†ÔºåÂØπÊú™Áü•ÂáΩÊï∞ÁöÑËøë‰ººËÉΩÂäõÂ¢ûÂº∫ÔºåËôΩÁÑ∂generalization errorÂú®Â¢ûÂä†Ôºå‰ΩÜ‰∏ÄÂºÄÂßãÔºå$E_{in}$ Âíå $E_{out}$ ÈÉΩÂú®‰∏ãÈôç„ÄÇË∂ÖËøáÊüê‰∏ÄÁÇπÂêéÔºå$E_{in}$ÁªßÁª≠ÂáèÂ∞èÔºå$E_{out}$ÂèçËÄå‰∏äÂçáÔºåGeneralization error ÂèòÊõ¥Â§ß:\nÂÆûÈ™å‰∏§ÁßçÊÉÖÂÜµÔºö‰∏Ä‰∏™10Èò∂ÁöÑÂáΩÊï∞ÊúâÂô™Â£∞ÈááÊ†∑15‰∏™Ê†∑Êú¨Ôºå‰∏Ä‰∏™50Èò∂ÁöÑÂáΩÊï∞Êó†Âô™Â£∞ÈááÊ†∑15‰∏™Ê†∑Êú¨Ôºö\nÂàÜÂà´Áî®2Èò∂Âíå10Èò∂ÁöÑÂ§öÈ°πÂºèÂéªÊãüÂêàÔºö\n‰ªé2Èò∂Ê®°ÂûãËΩ¨Âà∞10Èò∂Ê®°ÂûãÔºåÁ©øËøá‰∫ÜÊõ¥Â§öÁöÑÊ†∑Êú¨ÁÇπÔºåEinÊõ¥Â∞è‰∫ÜÔºå‰ΩÜEoutÂèòÂ§ß‰∫ÜÔºåËØ¥ÊòéÂá∫Áé∞‰∫ÜËøáÊãüÂêà„ÄÇÁÆÄÂçïÊ®°ÂûãÁîöËá≥Â•Ω‰∫éÊó†Âô™Â£∞ÁöÑÂ§çÊùÇÁõÆÊ†áÂáΩÊï∞„ÄÇnoise ÁöÑÈò∂Êï∞ÂèØËÉΩÂæàÈ´òÔºåÊâÄ‰ª•ÁÆÄÂçïÊ®°ÂûãÁöÑEin ÊØîËæÉÂ∑Æ„ÄÇ‰ΩÜÊòØÊ≤°ÊúâÂô™Â£∞ÁöÑÈ´òÈò∂ÁõÆÊ†áÂáΩÊï∞Ê†∑Êú¨‰πüÂ≠òÂú®ÊüêÁßç‚ÄúÂô™Â£∞‚ÄùÔºödeterminstic noise.\nÂÅáËÆæÈõÜ$\\mathcal H$ should match to the quantity and quality of the data, than the complexity of unknown target function.\n‰∏§‰∏™Â≠¶‰π†Âô®ÔºöO (overfitting) Âíå R (restricted)\n\u0026hellip;\u0026hellip;.\nÁªìÊûú:\nÂ∑¶ÂõæÔºåÂõ∫ÂÆöÁõÆÊØîÂáΩÊï∞ÁöÑÂ§çÊùÇÂ∫¶‰∏∫20th Èò∂Â§öÈ°πÂºèÔºåÈöèÁùÄÂ¢ûÂä†Âô™Â£∞ËÉΩÈáèÔºåÊ†∑Êú¨ÁöÑÊï∞ÈáèË∂äÊù•Ë∂ä‰∏çË∂≥‰ª•Ë°®ËææÂá∫ÁõÆÊ†áÂáΩÊï∞ÁöÑÂ§çÊùÇÂ∫¶ÔºåËøôÊó∂Â∏åÊúõ‰∏çÊñ≠ÂáèÂ∞èEinÔºåÈùûË¶ÅÊãüÂêàÂá∫Ë∂ÖÂá∫Êï∞ÊçÆË°®ËææËåÉÂõ¥ÁöÑÈÉ®ÂàÜÊù•ÂáèÂ∞èEout ‰ºö‰∫ã‰∏éÊÑøËøùÔºåÈÄÇÂæóÂÖ∂ÂèçÔºåÂçóËæïÂåóËæôÔºåÂØºËá¥ÈîôËØØÁöÑÂΩ¢ÂºèÔºåEoutÂèçËÄåË∂äÊù•Ë∂äÂ§ßÔºåÊâÄ‰ª•ËøáÊãüÂêàË∂äÊù•Ë∂ä‰∏•Èáç„ÄÇÂ¢ûÂä†Ê†∑Êú¨Êï∞ÈáèÔºåÂèØ‰ª•ÂáèÂ∞èËøáÊãüÂêà„ÄÇ\nÂè≥ÂõæÔºåÊó†Âô™Â£∞ÁöÑÈ´òÈò∂ÁõÆÊ†áÂáΩÊï∞Ê†∑Êú¨ÔºåÂõ∫ÂÆöÂô™Â£∞ËÉΩÈáè $\\sigma^2=0.1$„ÄÇÈöèÁùÄÂ¢ûÂä†ÁõÆÊ†áÂáΩÊï∞ÁöÑÂ§çÊùÇÂ∫¶ÔºåËøáÊãüÂêà‰πüË∂äÊù•Ë∂ä‰∏•Èáç„ÄÇÊâÄ‰ª•ËøôÈáåÂ≠òÂú®Âè¶‰∏ÄÁßç‰∏çÂêå‰∫éÂ∑¶ÂõæÈöèÊú∫Âô™Â£∞ÁöÑ‚ÄúÁ°ÆÂÆöÊÄßÂô™Èü≥‚Äù„ÄÇ\nDeterministic noise ÊòØÂÅáËÆæÈõÜ$\\mathcal H$ Êó†Ê≥ïÊçïÊçâÂà∞ÁöÑ f ÁöÑÈÉ®ÂàÜ„ÄÇ(The part of f that H cannot capture)\nÂõ†‰∏∫Ê®°ÂûãÂ§çÊùÇÂ∫¶ÊúâÈôêÔºåÊâÄ‰ª•ÂÅáËÆæÈõÜ‰∏≠ÁöÑÊúÄ‰Ω≥ÂÅáËÆæÊó†Ê≥ïÊãüÂêàÁõÆÊ†áÂáΩÊï∞ÁöÑÊüê‰∫õÈÉ®ÂàÜÔºå$h^\\star$ ‰∏éÁõÆÊ†áÂáΩÊï∞ f ‰πãÈó¥ÁöÑÈù¢ÁßØÂ∞±ÊòØÁ°ÆÂÆöÊÄßÂô™Èü≥: $f(\\mathbf x) - h^\\star(\\mathbf x)$„ÄÇÂÅáËÆæÈõÜÂ§™ÁÆÄÂçïÔºåÊó†Ê≥ïË°®ËææÂá∫ÁõÆÊ†áÂáΩÊï∞ÁöÑÂ§çÊùÇÂ∫¶ÔºåÂÆÉÊó†Ê≥ïÁêÜËß£ÁöÑÈÉ®ÂàÜ‰ºöÁªôÂÆÉÂ∏¶Êù•Âõ∞Êâ∞ÔºåÂΩìÂÆÉÂ∞ùËØïÊãüÂêàËøô‰∫õËÉΩÂäõËåÉÂõ¥‰πãÂ§ñÁöÑ‰∏úË•øÔºåÂ∞±‰ºöÂØºËá¥ÈîôËØØÁöÑÂΩ¢Âºè„ÄÇÈÇ£‰∫õÊó†Ê≥ïÁêÜËß£ÁöÑ‰∏úË•øÂØπÂÆÉÊù•ËØ¥Â∞±ÊòØÂô™Èü≥„ÄÇÂΩì‰ΩøÁî®Êõ¥Â§çÊùÇÁöÑÂÅáËÆæÈõÜÔºåÁ°ÆÂÆöÊÄßÂô™Â£∞Â∞±‰ºöÂáèÂ∞èÔºåÂõ†‰∏∫ÂÆÉÂèØ‰ª•ÊçïÊçâÂà∞Êõ¥Â§ö\nÁ°ÆÂÆöÊÄßÂô™Â£∞‰∏éÈöèÊú∫Âô™Â£∞ÁöÑ‰∏ªË¶ÅÂå∫Âà´:\nÁ°ÆÂÆöÊÄßÂô™Èü≥ÂèñÂÜ≥‰∫éÂÅáËÆæÈõÜ$\\mathcal H$ÔºåËÄåÈöèÊú∫Âô™Â£∞ÂØπÊâÄÊúâÁöÑÂÅáËÆæÈõÜÈÉΩ‰∏ÄÊ†∑ÔºåNothing can capture it, therefore it\u0026rsquo;s noise. ÂØπ‰∫é‰∏Ä‰∏™ÁªôÂÆöÁöÑÊ†∑Êú¨$\\mathbf x$ÔºåÁ°ÆÂÆöÊÄßÂô™Â£∞ÁöÑÈáèÊòØÂõ∫ÂÆöÁöÑÔºåÂ∞±ÊòØ $f(\\mathbf x) - h^\\star(\\mathbf x)$„ÄÇËÄåÈöèÊú∫Âô™Â£∞Ôºå‰∏§‰∏™Áõ∏ÂêåÁöÑxÔºå‰∫ßÁîüÁöÑÂô™Â£∞Â§ßÂ∞èÊòØÈöèÊú∫ÁöÑ„ÄÇ ‰ΩÜÊòØÂÆÉ‰ª¨‰∏§ËÄÖÂØπÊú∫Âô®Â≠¶‰π†ÈÄ†ÊàêÁöÑÂΩ±ÂìçÊòØÁõ∏ÂêåÁöÑÔºåÂõ†‰∏∫Êï∞ÊçÆÈõÜÊòØÁªôÂÆöÁöÑÔºà‰∏ÄÊ¨°‰ΩøÁî®ÔºâÔºåÂÅáËÆæÈõÜ‰∏ÄÊó¶Á°ÆÂÆöÂÆÉÁöÑÁ°ÆÂÆöÊÄßÂô™Â£∞‰πüÂõ∫ÂÆö‰∫Ü„ÄÇSo in a given learning situation, they behave the same. Impact on overfitting Á°ÆÂÆöÊÄßÂô™Â£∞ÈÄ†ÊàêÁöÑËøáÊãüÂêàÂú®10Èò∂‰ª•‰∏äÁöÑtarget complexity ÊâçÂá∫Áé∞ÔºåÂõ†‰∏∫ËøôÈáåÁöÑÂÅáËÆæÈõÜÊòØ10Èò∂ÁöÑÔºåË∂ÖÂá∫10Èò∂ÁöÑÈÉ®ÂàÜÂÆÉÊâçÊó†Ê≥ïËøë‰ºº„ÄÇ Âô™Â£∞ÈÄ†ÊàêËøáÊãüÂêàÊòØÂõ†‰∏∫ÊúâÈôêÁöÑÊ†∑Êú¨ÔºåËÆ©‰Ω†ËØØ‰ª•‰∏∫‰Ω†ÂèØ‰ª•ÂÆåÁæéÊãüÂêà„ÄÇ‰ΩÜÂÖ∂ÂÆûÈöèÊú∫Âô™Â£∞ÊòØÊçïÊçâ‰∏çÂà∞ÁöÑÔºåËÄå‰∏îÂΩìÁõÆÊ†áÂáΩÊï∞Â§çÊùÇÂ∫¶È´ò‰∫éÂÅáËÆæÈõÜÁöÑÊó∂ÂÄôÔºåÁ°ÆÂÆöÊÄßÂô™Â£∞‰πüÊòØÊçïÊçâÔºàÂ≠¶Ôºâ‰∏çÂà∞ÁöÑ„ÄÇ‰Ω†‰ª•‰∏∫‰Ω†Â≠¶Âà∞‰∫ÜÔºå‰ΩÜÂÖ∂ÂÆûÈÄ†Êàê‰∫ÜËøáÊãüÂêà„ÄÇ Noise and bias-variance $$ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] = \\underbrace{ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] }{\\rm var(\\mathbf x)} + \\underbrace{ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 }{\\rm bias(\\mathbf x)} $$\nEout Ë¢´ÂàÜÊàê‰∫Ü‰∏§ÈÉ®ÂàÜÔºåÂÖ∂‰∏≠ÁöÑ f ÊòØÊ≤°ÊúâÂô™Â£∞ÁöÑÔºåÂ¶ÇÊûúÁªôÊ†∑Êú¨Êï∞ÊçÆÂä†ÂÖ•Âô™Â£∞Ôºå‰∏äÂºè‰ºöÂ¶Ç‰ΩïÂèòÂåñÔºü\nÁªôÂÆöactual outputÔºö$y = f(\\mathbf x) + \\varepsilon(\\mathbf x)$ÔºåÂÅáËÆæÂô™Â£∞ÊúüÊúõÊòØ0Ôºö$\\mathbb E[\\varepsilon(\\mathbf x)] = 0$\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - y \\right)^2 \\right] \u0026amp; \\text{$\\varepsilon$ÂΩ±Âìç‰∫ÜyÔºå‰πüË¶ÅÂØπŒµÊ±ÇÊúüÊúõ}\\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) -\\varepsilon(\\mathbf x) \\right)^2 \\right] \u0026amp; \\text{Âä†‰∏Ä‰∏™Âáè‰∏Ä‰∏™} \\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) +\\bar g(\\mathbf x) - f(\\mathbf x) -\\varepsilon(\\mathbf x) \\right)^2 \\right] \\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) \\right)^2\n\\left( \\bar g(\\mathbf x) - f(\\mathbf x) \\right)^2 \\left( \\varepsilon(\\mathbf x) \\right)^2 + \\text{cross term} \\right] \\end{aligned} $$ Ê±ÇÊúüÊúõ‰πãÂêé cross term ÈÉΩÂèòÊàê0‰∫ÜÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂåÖÂê´ŒµÔºåŒµ ÁöÑÊúüÊúõ‰∏∫0„ÄÇÂâçÈù¢‰∏§È°πÂè™‰∏éÊï∞ÊçÆÈõÜÊúâÂÖ≥ÔºåËÄå‰∏éËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÂô™Â£∞ŒµÊó†ÂÖ≥ÔºåÊâÄ‰ª•ÂéüÊù•ÔºàÊ≤°Âä†Âô™Â£∞ÔºâÁ≠â‰∫é0ÁöÑÈ°πÁé∞Âú®ËøòÁ≠â‰∫é0„ÄÇEout Âè™Ââ©‰∏âÈ°πÔºö\n$$ E_{out} = \\underbrace{ \\mathbb E_{\\mathcal D,\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) \\right)^2 \\right] }_{var}\n\\underbrace{ \\mathbb E_{\\mathbf x} \\left[ \\left( \\bar g(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] }_{\\substack{bias \\‚Üë \\ \\text{deterministic noise}}} \\underbrace{ \\mathbb E_{Œµ, \\mathbf x} \\left[ \\left( \\varepsilon(\\mathbf x) \\right)^2 \\right] }_{\\substack{\\sigma^2 \\ ‚Üë \\ \\text{stochastic noise}}} $$ Á¨¨‰∏âÈ°πÊòØ target ‰∏é actual output‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÂ∞±ÊòØÈöèÊú∫Âô™Â£∞ÔºåÁ¨¨‰∫åÈ°πÊòØ\u0026quot;Âπ≥ÂùáÂÅáËÆæ\u0026quot;ÔºàÊúÄ‰Ω≥ÂÅáËÆæthe best thing you can doÔºâ‰∏étarget ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÂÆÉÊó†Ê≥ïÊãüÂêàÁöÑÈÉ®ÂàÜÂ∞±ÊòØÁ°ÆÂÆöÊÄßÂô™Â£∞„ÄÇ\n‰∏§‰∏™Âô™Â£∞ÊòØÂπ≥Á≠âÁöÑÔºåÂõ†‰∏∫Â¢ûÂä†Ê†∑Êú¨Êï∞ÈáèÔºåÊñπÂ∑ÆÁº©Â∞èÔºå‰ΩÜ‰∏§‰∏™Âô™Â£∞‰∏çÂèØÈÅøÂÖçÁöÑÔºàÂÅáËÆæÈõÜÁªôÂÆöÔºåÊï∞ÊçÆÈõÜÁªôÂÆöÔºåÊï¥‰ΩìÁöÑËøë‰ººÂ∞±Á°ÆÂÆö‰∫ÜÔºâ„ÄÇÂΩìÂÅáËÆæÂ∞ùËØïÊãüÂêàÂô™Â£∞Êó∂ÔºåÂ∞±‰∫ßÁîü‰∫ÜÊñπÂ∑ÆÔºàÂíåËøáÊãüÂêàÔºâ„ÄÇ\nTwo cures Regularization: putting the brakes Ë∏©ÂàπËΩ¶ÔºàÂ¢ûÂä†‰∏ÄÁÇπÈôêÂà∂; ÊèêÂâçÂÅúÊ≠¢Ôºâ\nValidation: Checking the bottom line ÊâæÂà∞Â∫ïÁ∫øÔºàÊâæÂà∞ÊØîEin Êõ¥Â•ΩÁöÑÂèçÊò†ÊãüÂêàË¥®ÈáèÁöÑËØØÂ∑Æ$E_{CV}$Ôºâ\n","date":"2021-12-13T13:55:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec11_overfitting/","title":"watch: AML 11 | Overfitting"},{"content":"Video 10 Training and testing 10-20-2021\nOutline\nFrom training to testing Illustrative examples Break point Puzzle Train multiple model / hypotheses: $$ \\mathbb P[|E_{in} - E_{out}| \u0026gt; \\varepsilon] \\leq \\underbrace{2 \\ M \\ e^{-2\\varepsilon^2 N}}_{\\text{union bound}} $$\nTest one model / hypothesis: (Hoeffding Inequality) $$ \\mathbb P[|E_{in} - E_{out}| \u0026gt; \\varepsilon] \\leq 2 e^{-2\\varepsilon^2 N} $$\nM: ÂÅáËÆæÈõÜ‰∏≠ÊâÄÊúâÂèØËÉΩÂÅáËÆæÁöÑ‰∏™Êï∞, ÂèØ‰ª•ÊòØÊó†Á©∑ÔºåÊâÄ‰ª•Ê¶ÇÁéáÂèØ‰ª•ÊòØÊó†Á©∑ÔºåÂ∞±‰∏çËÉΩ‰øùËØÅ $E_{out}\\approx E_{in}$\nLearning is to find the best hypothesis g which make the probability of \u0026quot;\u0026quot; ($\\mathcal B$ad event) minimum.\n\u0026ldquo;Learning\u0026rdquo; ÊòØÊâæÂà∞ÂÅáËÆæÈõÜ‰∏≠ÁöÑÊúÄ‰Ω≥ÂÅáËÆæ gÔºàËÆ© $\\mathcal B$ad event: $|E_{in}(h_m) - E_{out}(h_m)|\u0026gt;\\varepsilon$ ÂèëÁîüÁöÑÊ¶ÇÁéáÊúÄÂ∞èÔºâÔºåg ÂèØ‰ª•ÊòØÂÅáËÆæ1ÔºåÊàñËÄÖÂÅáËÆæ2Ôºå\u0026hellip;ÔºåÊàñËÄÖÂÅáËÆæMÔºåÂ∞±ÂØπÂ∫îÊ¶ÇÁéáÁõ∏Âä†ÔºåÂ∞±ÊòØunion boundÔºö $$ \\mathbb P [\\rm \\mathcal B_1 or \\mathcal B_2 or \\cdots \\mathcal B_M] \\leq \\underbrace{ \\mathbb P [\\mathcal B_1] + \\mathbb P [\\mathcal B_2] + \\cdots +\\mathbb P [\\mathcal B_M]}_{\\text{no overlaps: M terms}} $$\n‚ÄúÁ≠â‰∫é‚Äù ÂèëÁîüÂú®ÂÆÉ‰ª¨‰∫í‰∏çÈáçÂè†ÁöÑÊÉÖÂÜµÔºåÂ¶ÇÊûúÂêÑÂÅáËÆæÈó¥ÊúâÈáçÂè†ÔºåÂÆÉ‰ª¨ÁöÑÂíåÂ∞±ÂèòÂ∞è„ÄÇ\n‰∫ãÂÆû‰∏äÔºåBad Events are very overlapping.\nÁôΩËâ≤‰∏éÁÅ∞Ëâ≤ÂàÜÁïåÁ∫øÊòØ unknown target function„ÄÇ ËìùÁ∫øÂíåÁªøÁ∫øÊòØ‰∏§‰∏™ hypothesis„ÄÇ ÂÅáËÆæ h ‰∏éÁúüÂÆûÁïåÁ∫øÊâÄÂ∑ÆÁöÑÈù¢ÁßØÔºàÁ∫¢Ëâ≤Èò¥ÂΩ±ÔºâÊòØ$E_{out}$ÔºåÈªÑËâ≤Èù¢ÁßØÊòØ $\\Delta E_{out}$„ÄÇ$E_{in}$Êú™Âú®Âõæ‰∏≠‰ΩìÁé∞ÔºåÈúÄË¶ÅÂØπÊï¥‰∏™Èù¢ÁßØÁ©∫Èó¥ÈááÊ†∑ÔºåÊ†∑Êú¨ÁÇπËêΩÂú® $E_{out}$ ‰∏≠ÁöÑ‰∏™Êï∞ÊòØ$E_{in}$ÔºåÊ†∑Êú¨ÁÇπËêΩÂú®ÈªÑËâ≤Âå∫ÂüüÂÜÖÁöÑ‰∏™Êï∞ÊòØ$\\Delta E_{in}$„ÄÇ $h_1$ ‰∏é $h_2$ ÊúâÂæàÂ§ßÁöÑÈáçÂè†Ôºå$|E_{in}(h_1) - E_{out}(h_1)| \\approx |E_{in}(h_2)-E_{out}(h_2)|$\nÊîπÂèòhÔºå$E_{out}$ ÊúâÊòæËëóÂèòÂåñÔºå‰ΩÜ$E_{in}$‰∏ç‰ºöÊúâÊòæËëóÂèòÂåñÔºåÂõ†‰∏∫ËÆ≠ÁªÉÈõÜÊ†∑Êú¨ÁÇπÁöÑ‰∏™Êï∞ÂæàÊúâÈôê„ÄÇÊâÄ‰ª•ÂæàÂ§öÂÅáËÆæÁöÑÂàÜÁ±ªÊïàÊûúÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ\nReplace M with the number of dichotomies (ÊääÊ†∑Êú¨ÁÇπÊàêÂäü‰∫åÂàÜÁ±ªÁöÑÊñúÁ∫øÁöÑÊù°Êï∞).\nA hypothesis $\\rightarrow$ A dichotomy A dichotomy is a mini hypothesis\nÂØπÊØî:\nInput space: A hypothesis, ÂÖ®ËæìÂÖ•Á©∫Èó¥ÈÉΩÊòØËæìÂÖ• $h: \\mathcal X \\rightarrow{-1, +1}$ A dichotomy, Âè™Êúâ N ‰∏™ËÆ≠ÁªÉÁÇπ $h: {\\mathbf{x_1,x_2,\\cdots,x_N}}\\rightarrow {-1, +1}$ Number of hypotheses $|\\mathcal H|$ = M: ÂèØ‰ª•ÊòØÊó†Á©∑‰∏™ $|\\mathcal H(\\mathbf{x_1,x_2,\\cdots,x_N}})|$ = $m_{\\mathcal H}(N)$ ÊúÄÂ§ö $2^N$ ‰∏™ ÔºàN‰∏™$\\pm 1$ÁªÑÂêà) Áî® growth function $m_{\\mathcal H}(N)$ Êõø‰ª£ M\n$m_{\\mathcal H}(N)$ ÂøÖÈ°ªÊòØpolynomial in NÔºåËøôÊ†∑eÁöÑË¥üÊåáÊï∞ÂèØ‰ª•Ëµ∑‰ΩúÁî®ÊäµÊ∂à $m_{\\mathcal H}(N)$ÔºåËÆ©union boundÂèòÂæóÂ∞è„ÄÇ\nGrowth function $m_{\\mathcal H} (N)$ Áî± N ‰∏™Ê†∑Êú¨ÁÇπÁªÑÊàê‰ªªÊÑèÁöÑÔºåËÉΩÊàêÂäüË¢´ÂÅáËÆæÈõÜ $\\mathcal H$ ‰∫åÂàÜÁöÑ dichotomies ÊúÄÂ§öÁöÑ‰∏™Êï∞ (‰∏çÈôê‰ΩçÁΩÆ): $m_{\\mathcal H}(N) = \\underset{x_1,\\cdots,x_N \\in \\mathcal X}{max}\\ |\\mathcal H(x_1, \\cdots, x_N)|$. Ôºà\u0026ldquo;most dichotomies on any N points\u0026rdquo;Ôºâ $$ m_{\\mathcal H}(N) \\leq 2^N $$\nÊúâ‰∫õ config ÁöÑ dichotomies Ëææ‰∏çÂà∞ $2^N$ ÁßçÔºö\nÂØπ‰∫é 2D perceptron hypothesis / dichotomiesÔºö\n3‰∏™ÁÇπÔºåÊúÄÂ§öËÉΩÂàÜ8ÁßçÔºàËã•ÈôêÂÆöÁÇπÁöÑ‰ΩçÁΩÆÂèØËÉΩÊõ¥Â∞ëÔºâ\n4‰∏™ÁÇπÔºåÊúÄÂ§öËÉΩÂàÜ14ÁßçÔºàÊúâ2ÁßçÊÉÖÂÜµ (ÂºÇÊàñ)Ôºå2D perceptronÂàÜ‰∏çÂºÄÔºâ\n(2Ë°å2ÂàóÊëÜÊîæÊúÄÂ§öÔºâÊåâÁÖß4‰∏™oÔºå3‰∏™o (1‰∏™x)Ôºå2‰∏™oÔºå1‰∏™oÔºå0‰∏™o ÊéíÂàóÁªÑÂêà„ÄÇ\nÂ¶ÇÊûú4ÁÇπÂÖ±Á∫øÔºåÂè™ËÉΩÂàÜ8Áßç„ÄÇ\nÂ¶ÇÊûú3ÁÇπÂÖ±Á∫øÔºå‰Ωô1‰∏™ÂçïÁã¨Âú®‰∏ÄË°åÔºåÂè™ËÉΩÂàÜ12Áßç„ÄÇ\nÂØπ‰∫é Positive rays hypothesisÔºö\n‰ΩçÁΩÆÊëÜÊîæÊòØÂõ∫ÂÆöÁöÑÔºöÂÖ±Á∫ø„ÄÇÂØπ‰∫éN‰∏™ÁÇπÔºåÊ≠£Â∞ÑÁ∫øÂè™ËÉΩÂ§ÑÁêÜ N+1 ÁßçÈÖçÁΩÆÔºå‰πüÂ∞±ÊòØÂúÜÂúàÂÖ®Âú®Âè≥‰æßÔºåÂèØ‰ª•ÊòØ0‰∏™ÂúàÔºå1‰∏™ÂúàÔºå2‰∏™ÂúàÔºå\u0026hellip;ÔºåN‰∏™Âúà„ÄÇ\nÂØπ‰∫é Positive Intervals hypothesis\nÊ≠£Âå∫Èó¥Âè™ËÉΩÂ§ÑÁêÜ‰∏≠Èó¥Êúâ‰∏ÄÊÆµÊòØ+1Ôºå‰∏§‰æßÊòØ-1ÁöÑÈÖçÁΩÆ„ÄÇÈúÄË¶ÅÁî®‰∏§‰∏™ËæπÁïå (threshold) Êù•Á°ÆÂÆö‰∏Ä‰∏™Âå∫Èó¥Ôºö‰∏ÄÂÖ±ÊúâN‰∏™ÁÇπÔºåÂΩ¢ÊàêN+1‰∏™Á©∫ÔºåÂú®ËøôN+1‰∏™Á©∫ÈáåÈÄâ‰∏§‰∏™ÔºåÂ∞±ÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™Âå∫Èó¥ÔºåÊâÄ‰ª•ÊòØ $C_{N+1}^2$ÔºåÂÜçÂä†‰∏ä‰∏§‰∏™ËæπÁïåÂú®Âêå‰∏Ä‰∏™Á©∫ÁöÑÊÉÖÂÜµ„ÄÇ\nÂØπ‰∫é Convex sets hypothesis\nÂá∏ÈõÜÂèØ‰ª•Êää‰ªªÊÑèÈ¢úËâ≤ÈÖçÁΩÆÁöÑN‰∏™ÁÇπÂÆåÂÖ®ÂàÜÂºÄÔºåÂá∏ÈõÜÈáåÈù¢ÊòØ+1ÔºåÂ§ñÈù¢ÊòØ-1„ÄÇÊâÄ‰ª•ÂÆÉÁöÑgrowth function ÊòØ $2^N$ÔºåÂá∏ÈõÜÊ®°ÂûãÊ≤°Êúâbreak point„ÄÇ\nBreak point k Êó†Ê≥ïÁî®ÂÅáËÆæÈõÜ $\\mathcal H$ ÂÆåÂÖ®Êää‰∏§Á±ªÁÇπÂàÜÂºÄÁöÑÁÇπÁöÑ‰∏™Êï∞ k„ÄÇ‰πüÂ∞±ÊòØÂºÄÂßãÂá∫Áé∞ $m_{\\mathcal H}(k) \u0026lt; 2^k$ ËøôÁßçÊÉÖÂÜµÊó∂ÁöÑÁÇπÊï∞„ÄÇ\n‰æãÂ¶ÇÊ≤°Êúâ‰ªª‰Ωï‰∏ÄÁßç 4 ‰∏™ÁÇπÁöÑÈÖçÁΩÆÔºåËÉΩÁî®2D perceptron Â∞ÜÂÖ∂ÂÆåÂÖ®ÂàÜÂºÄÔºåÊâÄ‰ª• 2D perceptron ÁöÑ break point ÊòØ 4Ôºö$m_{\\text{2D percep}}(4) = 14 \u0026lt; 2^4$„ÄÇÂ§ß‰∫é4‰∏™ÁöÑÁÇπÈõÜ‰πü‰∏çËÉΩË¢´ÂÆåÂÖ®ÂàÜÂºÄ„ÄÇ\nÂ¶ÇÊûúÊ≤°Êúâ break point, growth function is $2^N$.\nÂ¶ÇÊûúÂ≠òÂú® break point, growth function is a polynomial function in N : $\\sum_{i=0}^{k-1} C_N^i$, whose maximum power is $N^{k-1}$.\n","date":"2021-12-12T23:23:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec5_traning_vs_testing/","title":"watch: AML 05 | Training vs Testing"},{"content":"Video 11 Theory of Generalization 10-25-2021\nOutline\nProof that the growth function is polynomial Proof that $m_{\\mathcal H}(N)$ can replace M VC dimension Examples of polynomial $m_{\\mathcal H}(N)$ Proof see the book\nKey quantity $B(N,k)$: Maximum number of dichotomies on N points, with break point $k$.\nTheorem: $B(N,k) \\leq \\sum_{i=0}^{k-1} \\left( \\begin{aligned} N\\ i \\end{aligned} \\right)$\nFor a given $\\mathcal H$, the break point $k$ is fixed:\n$$ m_{\\mathcal H}(N) \\leq \\underbrace{ \\sum_{i=0}^{k-1} \\left( \\begin{aligned} N\\ i \\end{aligned} \\right) }_{\\text{maximum power is} N^{k-1}} $$\nPositive ray (break point k=2):\n$$ m_{\\mathcal H}(N) = C_{N+1}^1 \\leq (=) \\sum_{i=0}^{k-1} C_N^i = N+1 $$\nPositive intervals (break point k=3):\n$$ m_{\\mathcal H}(N) = C_{N+1}^2 + 1 \\leq (=) \\sum_{i=0}^{k-1} C_{N}^i = \\frac{1}{2}N^2 + \\frac{1}{2}N + 1 $$\n2D perceptrons (break point k=4): $$ m_{\\mathcal H}(N)=? \\leq \\sum_{i=0}^{k-1} C_{N}^i = \\frac{1}{6}N^3 + \\frac{5}{6}N + 1 $$\nÂ¶ÇÊûúÂ≠òÂú® break point, growth function is polynomial function in N : $\\sum_{i=0}^{k-1} C_N^i$, whose maximum power is $N^{k-1}$ ($d_{VC}$).\nÂ¶ÇÊûúÊ≤°Êúâbreak point, growth function is $2^N$ÔºåÂ∞±‰∏çËÉΩ‰øùËØÅÂÆÉÊòØÂÖ≥‰∫éNÁöÑÂ§öÈ°πÂºèÔºàÊúÄÈ´òÊ¨°ÂπÇÊòØ $d_{VC}$ÔºåËÉΩË¢´eÁöÑË¥üÂπÇÊ¨°ÊäµÊ∂àÔºâÔºå‰πüÂ∞±‰∏çËÉΩ‰øùËØÅ Bad events ÁöÑÊ¶ÇÁéá‰∏äÁïåË∂≥Â§üÂ∞èÔºåÈÇ£Ê†∑Â∞±‰∏çËÉΩÈÄöËøá Ein Â≠¶Âà∞Eout„ÄÇ\nProof that $m_{\\mathcal H}(N)$ can replace $M$ Replace $M$ with $m_{\\mathcal H}(N)$ to narrow the upper bound. If M hypotheses are not overlapping, the probability upper bound is very lose.\n$$ \\begin{aligned} \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 2\\ M\\ e^{-2\\varepsilon^2 N} \\quad \\text{(Union bound)} \\ \u0026amp; \\Downarrow \\text{(Not quite)} \\ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 2\\ m_{\\mathcal H}(N)\\ e^{-2\\varepsilon^2 N} \\ \\end{aligned} $$\nÈïøÊñπÂΩ¢‰∏≠ÁöÑÊØè‰∏™ÁÇπ‰ª£Ë°®‰∏Ä‰∏™Ê†∑Êú¨Êï∞ÊçÆÈõÜ„ÄÇ The \u0026ldquo;flower\u0026rdquo; represents \u0026ldquo;bad events\u0026rdquo; probability (Ein Âíå Eout Áõ∏Â∑ÆË∂ÖËøá$\\varepsilon$ ÁöÑÊ¶ÇÁéá).\nFig (a) Ë°®Á§∫ÁöÑÊòØ‰∏Ä‰∏™ÂÅáËÆæÁöÑ‚ÄúÈ™åËØÅ‚Äù„ÄÇWhile Learning needs to pick the best hypothesis from M hypotheses based on \u0026ldquo;bad events\u0026rdquo; probability.\nIn Fig (b), every hypothesis is not overlapping each other. So their bad events probabilities are taking the whole space, resulting the upper bound maybe very large. ÊâÄ‰ª•‰∏çÂèØËÉΩÈÄöËøá $E_{in}$ Â≠¶Âà∞ $E_{out}$ÔºåÂõ†‰∏∫Â§ßÈÉ®ÂàÜÊÉÖÂÜµ‰∏ãÔºåÂùè‰∫ã‰ª∂ÂèëÁîüÔºåÂÆÉ‰ª¨Áõ∏Â∑ÆË∂ÖËøá‰∫Ü$\\varepsilon$„ÄÇ\nIn Fig ÔºàcÔºâ, many hypotheses overlap together in training data. VC bound ÊØîËæÉÂ∞èÔºåEin‰∏éEout Áõ∏Â∑ÆË∂ÖËøá $\\varepsilon$ ÁöÑÊ¶ÇÁéá‰∏çÂ§ß„ÄÇ\nWhat to do about $E_{out}$?\nTake another set of samples, called $E_{in}\u0026rsquo;(h)$. $E_{in}$ ‰∏é $E_{in}\u0026rsquo;$ Áã¨Á´ãÂêåÂàÜÂ∏ÉÔºåÊâÄ‰ª• Ein and Ein\u0026rsquo; are both following Eout, Ein and Ein\u0026rsquo; are following each other.\n$$ \\begin{aligned} \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 2\\ m_{\\mathcal H}(N)\\ e^{-2\\varepsilon^2 N} \\ \u0026amp; \\Downarrow \\text{(but rather)} \\ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 4\\ m_{\\mathcal H}(2N)\\ e^{-\\frac{1}{8} \\varepsilon^2 N} \\quad \\text{(VC bound)}\\ \\end{aligned} $$\n(Lec7)\nThe Vapnik-Chervonenkis Inequality: $$ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026gt; \\varepsilon] \\leq 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} $$\nVapnik-Chervonenkis Inequality ÂØπ‰ªª‰ΩïÊúâbreak pointÁöÑ hypothesis ÈÉΩÊàêÁ´ã„ÄÇ\nÁõ∏ËæÉ‰∫éÁõ¥Êé•Áî®$m_{\\mathcal H}(N)$ÊõøÊç¢MÔºåVCÁª¥ÂÖ∂ÂÆû make the bound worser, larger: $2 m_{\\mathcal H}(N) e^{-2\\varepsilon^2 N} \u0026lt; 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N}$\nVC dimension VCÁª¥ $d_{VC}$ ÊòØÊüêÂÅáËÆæÈõÜ $\\mathcal H$ ÊúÄÂ§öËÉΩÂÆåÂÖ®Ôºà‰∫åÂàÜÔºâÂàÜÂºÄÁöÑÁÇπÁöÑ‰∏™Êï∞„ÄÇ\n$$ d_{VC}(\\mathcal H) = k-1 $$\nÂÆåÂÖ®ÂàÜÂºÄÊÑèÂë≥ÁùÄÔºåÂÅáËÆæÈõÜ$\\mathcal H$ËÉΩÂàÜÂâ≤ÂºÄÁî±N‰∏™ÁÇπÁªÑÊàêÁöÑ‰ªªÊÑèÁöÑÈ¢úËâ≤ÈÖçÁΩÆÔºö$m_{\\mathcal H}(N) = 2^N$\nÊúâ‰∫Ü $d_{VC}$ ‰πãÂêéÔºö\n$N \\leq d_{VC}(\\mathcal H) \\Rightarrow \\mathcal H$ can shatter N points\n$k \\geq d_{VC}(\\mathcal H) \\Rightarrow k$ is a break point for $\\mathcal H$\n$m_{\\mathcal H}(N)$ ÁöÑÊúÄÈ´òÁª¥Â∫¶ÊòØ $d_{VC}$\nGrowth function Áî® break point k Ë°®Á§∫Ôºö$m_{\\mathcal H}(N) \\leq \\sum_{i=0}^{k-1} \\begin{pmatrix} N \\ i \\end{pmatrix}$\nGrowth function Áî® VC dimension $d_{VC}$ Ë°®Á§∫Ôºö$m_{\\mathcal H}(N) \\leq \\underbrace{ \\sum_{i=0}^{d_{VC}} \\begin{pmatrix} N \\ i \\end{pmatrix} }{\\text{minimum power is} N^{d{VC}}}$\nÂØπ‰∫é‰∏çÂêåÁ±ªÂûãÁöÑ hypothesis set:\n$\\mathcal H$ is positive rays: $d_{VC}=1$ $\\mathcal H$ is 2D perceptrons: $d_{VC}=3$ $\\mathcal H$ is convex sets: $d_{VC}=\\infin$ VC dimension and learning Âõ†‰∏∫ $d_{VC}(\\mathcal H)$ ÊòØÊúâÈôêÁöÑ, Â∞±ÂèØ‰ª•‰ªéÂÅáËÆæÈõÜ $\\mathcal H$ ‰∏≠Â≠¶‰π†Âà∞Ë∂≥Â§üÊé•ËøëÊú™Áü•ÁõÆÊ†áÂáΩÊï∞ $f$ ÁöÑÂÅáËÆæg„ÄÇÔºà‰øùËØÅ‰∫ÜEin‰∏éEoutÁõ∏Â∑ÆÂæàÂ§öÁöÑÊ¶ÇÁéá‰∏çÂ§ßÔºâ $d_{VC}$ Áã¨Á´ã‰∫é learing algorithm, input distribution, target function. ‰∏çÁÆ°ÂÆÉ‰ª¨ÊòØ‰ªÄ‰πàÊ†∑Ôºå$d_{VC}$Âè™Áî±ÂÅáËÆæÈõÜÂÜ≥ÂÆö„ÄÇ VC dimension of perceptrons $d_{VC} = d+1$, d is the working dimension of perceptrons (ËØÅÊòéËßÅ‰π¶) ‰æãÂ¶ÇÔºö2D perceptron ‰Ωç‰∫é 2D planeÔºåÊâÄ‰ª•$d_{VC}=3$; ËÄå 3D perceptron‰Ωç‰∫é3D spaceÔºåÊâÄ‰ª•$d_{VC}=4$ $d_{VC} \\ (d+1)$ ÊòØÊùÉÈáçÂèÇÊï∞ÁöÑ‰∏™Êï∞ ($w_0, w_1, \\cdots, w_d$)„ÄÇ1 ‰ª£Ë°®biasÈ°πÔºåd‰ª£Ë°®ÊØè‰∏™Ê†∑Êú¨ÁÇπÁöÑÁª¥Â∫¶Êï∞„ÄÇ Interpreting the VC dimension VCÁª¥ÊòØËá™Áî±Â∫¶ (Degrees of freedom)\n‰∏çÂêåÁöÑweightsÂ∞±ÊòØ‰∏çÂêåÁöÑ hypothesis„ÄÇEvery dimension of $d_{VC}$ is a tunable parameter, and parameters create degrees of freedom.\nNumber of parameters analog degrees of freedom; $d_{VC}$ equivalent \u0026ldquo;binary\u0026rdquo; degrees of freedom.\nThe usual suspects\npositive ray: $d_{VC}=1$Ôºå‰∏Ä‰∏™ÂèÇÊï∞ÂØπÂ∫î‰∏Ä‰∏™ËæπÁïå positive interval: $d_{VC}=2$Ôºå‰∏§‰∏™ÂèÇÊï∞ÂØπÂ∫î‰∏§‰∏™ËæπÁïåÔºåÁ°ÆÂÆö‰∏Ä‰∏™hypothesis Not just parameters\nParameters Âú®ÂÆûÈôÖÊÉÖÂÜµ‰∏≠ÂèØËÉΩ‰∏çÊòØËá™Áî±Â∫¶„ÄÇ ÊØè‰∏™ perceptron Êúâ 2 ‰∏™ parameters ($w_0, w_1$ ÂØπÂ∫î $b$ Âíå$x$). So 4 perceptron have 8 parameters those have an impact on a hypothesis, but the $d_{VC}$ maybe not 8. $d_{VC}$ measures the effective number of parameters Number of data points needed ÁªèÈ™åÊ≥ïÂàôÔºö $N \\geq 10 d_{VC}$\nVCÁª¥ÁöÑ‰∏§‰∏™ÈáèÂåñÂèÇÊï∞: $\\varepsilon, \\delta$:\n$$ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026gt; \\varepsilon] \\leq \\underbrace{ 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} }_\\delta $$\n$\\delta$ÊòØ N ÁöÑÂáΩÊï∞Ôºö$N^{d_{VC}} e^{-N}$ Ôºà$m_{\\mathcal H}(2N)$ÊòØNÁöÑÂ§öÈ°πÂºèÔºâ\nÊ®™ÂùêÊ†áÊòØ NÔºåÁ∫µÂùêÊ†áÊòØÊ¶ÇÁéá‰∏äÁïå $\\delta$ÔºåÊØèÊù°Êõ≤Á∫ø‰ª£Ë°®‰∏çÂêåÁöÑVCÁª¥:\n$d_{VC}$Ë∂äÂ§ßÔºåÊ¶ÇÁéá‰ºöÂÖà‰∏äÂçáÂæóË∂äÈ´òÔºå‰πãÂêéË¥üÊåáÊï∞take the controlÔºåÁ™ÅÁÑ∂ÊÄ•ÂâßÂáèÂ∞è„ÄÇÂõ†‰∏∫ËøôÊòØÊ¶ÇÁéáÔºåÊâÄ‰ª•Âè™ËÄÉËôëÂ∞è‰∫é1 ($10^0$ ‰ª•‰∏ã)ÁöÑÈÉ®ÂàÜÔºåÊ¶ÇÁéáË∂äÂ∞èË∂äÂ•Ω„ÄÇ\nÂú®Âêå‰∏ÄÊ¶ÇÁéá‰∏ãÔºåÂ¢ûÂä†VCÁª¥ÔºåNÁöÑÂÄº‰πüÂëàÁ∫øÊÄßÂ¢ûÂä†„ÄÇÊõ¥Â§ßÁöÑVCÁª¥ÔºàË¶ÅË∞ÉËäÇÊõ¥Â§öÁöÑÂèÇÊï∞ÔºâÈúÄË¶ÅÊõ¥Â§öÁöÑÊ†∑Êú¨ÁÇπ„ÄÇ‰∏∫‰∫ÜËææÂà∞ÂæàÂ∞èÁöÑ Bad eventsÊ¶ÇÁéáÔºåÂ∞±ÈúÄË¶ÅÊõ¥Â§öÁöÑÊ†∑Êú¨ÁÇπ„ÄÇ\nRule of thumb:(ÁªèÈ™å) $N \\geq 10 d_{VC}$\nRearranging things For VC inequalityÔºåÊää Bad events ÂèëÁîüÁöÑÊ¶ÇÁéáÁß∞‰∏∫ $\\delta$\n$$ \\mathbb P [ |E_{out}(g) - E_{in}(g)| \u0026gt; \\varepsilon] \\leq \\underbrace{ 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} }_\\delta $$\n‰ªé $\\delta$ ‰∏≠Êé®Âá∫ Œµ :\n$$ \\varepsilon = \\underbrace{ \\sqrt{\\frac{8}{N} ln \\frac{4 m_{\\mathcal H} (2N)}{\\delta}} }_\\Omega $$\n$\\Omega$ÊòØ$N, \\mathcal H, \\delta$ ÁöÑÂáΩÊï∞„ÄÇ\nGood events Ë°®Á§∫‰∏∫Ôºö$|E_{out} - E_{in}|\\leq \\Omega(N,\\mathcal H, \\delta)$ÔºåÂÆÉÁöÑÊ¶ÇÁéáÊòØ $P(good) \\geq 1-\\delta$„ÄÇ\nGeneralization bound ÈÄöÂ∏∏ Eout ÊØî Ein Â§ßÔºåÊâÄ‰ª•ÂèØ‰ª•ÂéªÊéâÁªùÂØπÂÄºÔºåÈÇ£‰πà Good events Â∞±ÊòØ: $E_{out}-E_{in} \\leq \\Omega(N,\\mathcal H, \\delta)$ÔºåÁß∞‰∏∫ Generalization error„ÄÇ\nÁÑ∂ÂêéÁßªÈ°πÂæóÂà∞ $E_{out}$:\n$$ E_{out} \\leq \\underbrace{ E_{in} + \\underbrace{\\Omega(N,\\mathcal H, \\delta)}{\\text{Generalization error}} }{\\text{Generalization bound}} $$\n$E_{in}$ ‰ªéËÆ≠ÁªÉÊ†∑Êú¨‰∏≠ÂæóÁü•ÔºåÂÜçÊ†πÊçÆ‰ª•‰∏äÂÖ≥Á≥ªÔºåÂ∞±ÂèØÂæóÁü•Eout„ÄÇ\n","date":"2021-12-12T20:01:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec6_7_generalization_dvc/","title":"watch: AML 06 | Genearalization-dVC"},{"content":"Video 6 Linear Models 9-29-2021\nOutline\nInput representation Linear Classificatin Linear Regression Nonlinear Transformation Review of Lecture 2\nLearning is feasible in a probabilistic sense.\nRed marble frequence $\\nu$ in the bin is unknow ‚Üí $E_{out}$ is unknown ‚Üí $E_{in}$: red marble frequence in the sample $\\mu$ ‚Üí ÊúÄ‰Ω≥ÂÅáËÆæ g ‰Ωø $E_{in}$ Âíå $E_{out}$ ÊúÄÊé•ËøëÔºåËøôÊó∂Bad events ÁöÑÊ¶ÇÁéá‰∏∫: Ein Âíå Eout Áõ∏Â∑ÆË∂ÖËøá $\\epsilon$ ÁöÑÊ¶ÇÁéá $|E_{in}(g) - E_{out}(g)| \u0026gt; \\epsilon$\nÂõ†‰∏∫ g ËÇØÂÆöÊòØÂÅáËÆæÈõÜ H ‰∏≠ M ‰∏™ÂÅáËÆæÁöÑÂÖ∂‰∏≠‰πã‰∏ÄÔºåÊØè‰∏™ÈÉΩÊúâÂèØËÉΩ:\n$$ |E_{in}(h_2) - E_{out}(h_2)| \u0026gt; \\epsilon\\ \\textbf{ or } |E_{in}(h_2) - E_{out}(h_2)| \u0026gt; \\epsilon\\ \\textbf{ or }\\ \\cdots |E_{in}(h_M) - E_{out}(h_M)| \u0026gt; \\epsilon $$\n‰πüÂ∞±ÊòØ M ‰∏™ÂÅáËÆæÂØπÂ∫îÁöÑBad eventsÂèëÁîüÊ¶ÇÁéá‰πãÂíåÔºàÂêÑÂÅáËÆæ‰πãÈó¥Êó†overlappingÔºå‰πüÂ∞±ÊòØÊúÄÂùèÁöÑÊÉÖÂÜµÔºå‰ΩÜÈÄöÂ∏∏ÂêÑÂÅáËÆæÈó¥ÊúâÁõ∏ÂÖ≥ÊÄßÔºâÔºåÊääMÂä†Âà∞Hoeffding InequalityÂè≥‰æßÔºåÁß∞‰∏∫Union BoundÔºö\n$$ P[ |E_{in}(g)-E_{out}(g)|\u0026gt; \\epsilon ] \\leq 2M e^{-2 \\epsilon^2 N} $$\nÂ¶ÇÊûúÊ†∑Êú¨Êï∞ÈáèNË∂≥Â§üÂ§ßÔºåBad event ÁöÑÊ¶ÇÁéáÂ∞±‰ºöÂèòÂ∞è.\nLinear model Weights are linear related.\nDigits pictures:\nraw input: $\\mathbf{x} = (x_0, x_1, \\cdots, x_{256})$ (x0ÊòØbias)\nlinear model: $(w_0, w_1, \\cdots, w_{256})$ (inputs constribution,w0=1)\nFeatures: Extracted useful information Ôºà257Áª¥Â§™È´ò,Á∫øÊÄßÊ®°Âûã‰∏çÂ§™Ë°åÔºâ\nIntensity and symmetry $\\mathbf x=(x_0, x_1, x_2)$Ôºå $x_0=$bias\nlinear model: $(w_0, w_1, w_2)$, $w_0=1$\nPLA is not so smart. It focus on one misclassified point and update the weight, then the in-sample error maybe better or worse. There are a lot of flactration during iterations. ÈîôËøá‰∫ÜÊúÄÂ∞èËØØÂ∑ÆÔºåÂ∞±Âõû‰∏çÂéª‰∫Ü„ÄÇ\n$E_{out}$ ÊòØÈ™åËØÅÈõÜ‰∏äÁöÑËØØÂ∑ÆÔºåerror of out-of-sample follows $E_{in}$\nPocket: Keep the best weights (in pocket). Replace it when finding better in the future.\nLinear Classification Outputs of a linear model are binary\n$$ h(\\mathbf x) = \\operatorname{sign} \\left( \\sum_{i=0}^d w_i x_i \\right) : { +1,-1} $$\n+1 or -1 (Approve or Deny)\nLinear Regression Outputs of a linear model are real-valued\n$$ h(\\mathbf x) = \\sum_{i=0}^d w_i x_i = \\mathbf{w}^T \\mathbf{x} \\quad \\text{(w0 for bias x0)} $$\n‰∏çÂÜç‰º†ÁªôsignÂáΩÊï∞ÔºåÂà§Êñ≠$\\pm 1$ÂàÜÁ±ª\ndata set: $\\rm (\\pmb x_1, y_1), (\\pmb x_2, y_2),\\cdots (\\pmb x_N, y_N)$\nÁî®Á∫øÊÄßÂõûÂΩíÂéªÂ§çÂà∂ data setÔºåÁÑ∂ÂêéÈ¢ÑÊµãÊú™Êù•xÁöÑy„ÄÇ\nÁî®ÂÅáËÆæ $h(\\mathbf x) = \\mathbf w^T \\mathbf x$ Ëøë‰ººÊú™Áü•ÁõÆÊ†áÂáΩÊï∞ $f(\\mathbf x)$:\n$$ \\left(h(\\mathbf x) - f(\\mathbf x) \\right)^2 $$\n(Square error will make solving linear regression problem easily one-shot.)\nIn-sample error:\n‰∏Ä‰∏™ÁâπÂæÅÊòØÁÇπÂà∞Áõ¥Á∫øË∑ùÁ¶ªÔºå‰∏§‰∏™ÁâπÂæÅÊòØÁÇπÂà∞Ë∂ÖÂπ≥Èù¢Ë∑ùÁ¶ª„ÄÇ\n$$ E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^N (h(\\mathbf x_n) - y_n)^2 $$\nIn-sample error ÊòØÊùÉÈáç $\\mathbf w$ ÁöÑÂáΩÊï∞ Ôºà$\\mathbf x$ÂíåyÈÉΩÊòØÂõ∫ÂÆöÁöÑËÆ≠ÁªÉÊ†∑Êú¨,Âè™Êúâ$\\mathbf w$ÊòØÂèòÈáèÔºâÔºåÁ∫øÊÄßÂõûÂΩíÁöÑÁõÆÊ†áÊòØÊâæÂà∞‰Ωø In-sample error ÊúÄÂ∞èÁöÑ$\\mathbf w$:\n$$ \\begin{aligned} E_{in}(\\mathbf w) \u0026amp;= \\frac{1}{N} \\sum_{n=1}^N (\\mathbf w^T \\mathbf x_n - y_n)^2 \\\\ \u0026amp;= \\frac{1}{N} | \\mathbf{Xw} - \\mathbf y |^2 \\end{aligned} $$\nÔºàÊääÊ±ÇÂíåÂèòÊàêÁü©ÈòµÔºåÊñπ‰æøÊ±ÇÂØºÊâæÊúÄÂÄºÔºâÂÖ∂‰∏≠Ôºö\n$$ \\mathbf X= \\begin{bmatrix} \\mathbf x_1^T \\\\ \\mathbf x_2^T \\\\ \\vdots \\\\ \\mathbf x_N^T \\end{bmatrix}, \\mathbf y= \\begin{bmatrix} y_1^T \\\\ y_2^T \\\\ \\vdots \\\\ y_N^T \\end{bmatrix} $$\nÊ±Ç $E_{in}$ ÁöÑÊúÄÂ∞èÂÄºÔºöÂØπ $\\mathbf w$ Ê±ÇÂØºÔºåÂπ∂‰ª§ÂÖ∂Á≠â‰∫é0:\n$$ \\begin{aligned} E_{in}\u0026rsquo;(\\mathbf w) \u0026amp;= 0 \\\\ \\frac{2}{N} \\mathbf X^T (\\mathbf {Xw} -y) \u0026amp;= 0 \\\\ \\mathbf X^T \\mathbf {Xw} \u0026amp;= \\mathbf X^T y \\\\ \\mathbf w \u0026amp;= \\mathbf X^{\\dagger} y \u0026amp; \\text{where } \\mathbf X^\\dagger = (\\bf X^T X)^{-1} X^T \\end{aligned} $$\nPerceptron is more similar to the learning process that you\u0026rsquo;re just trying to learn something from one iteration to the other iteration. Here it\u0026rsquo;s not iterative. Linear regression is a kind of one-shot learner that learns one iteration.\n$\\mathbf X^\\dagger$ is the pseudo-inverse of $\\mathbf X$:\n$$ \\underbrace{ \\begin{pmatrix} \\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{10} \u0026amp; \\cdots x_{N0} \\ x_{01} \u0026amp; x_{11} \u0026amp; \\cdots x_{N1} \\ \\vdots \\ x_{0d} \u0026amp; x_{1d} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }_{(d+1)\\times N}\n\\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{01} \u0026amp; \\cdots x_{0d} \\ x_{10} \u0026amp; x_{11} \u0026amp; \\cdots x_{1d} \\ \\vdots \\ x_{N0} \u0026amp; x_{N1} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }_{N\\times (d+1)}\n\\end{pmatrix}^{-1}\n\\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{10} \u0026amp; \\cdots x_{N0} \\ x_{01} \u0026amp; x_{11} \u0026amp; \\cdots x_{N1} \\ \\vdots \\ x_{0d} \u0026amp; x_{1d} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }{(d+1)\\times N} }{(d+1)\\times N} $$\n$\\mathbf w = \\mathbf X^\\dagger y = \\underbrace{[w_0\\ w_1\\ \\cdots w_{d}]}_{(d+1)\\times 1}$\nLinear regression algorithm ÊûÑÂª∫ the data matrix $\\mathbf X$ and the vector y from the data set $(\\mathbf X_1, y_1), \\cdots, (\\mathbf X_N, y_N)$\n$$ \\mathbf X = \\begin{bmatrix} \\cdots \\mathbf x_1^T \\cdots \\ \\cdots \\mathbf x_2^T \\cdots \\ \\vdots \\ \\cdots \\mathbf x_N^T \\cdots \\ \\end{bmatrix} , y= \\begin{bmatrix} y_1^T \\ y_2^T \\ \\vdots \\ y_N^T \\end{bmatrix} $$\nËÆ°ÁÆó‰º™ÈÄÜÁü©Èòµ $\\mathbf X^\\dagger = (\\bf X^T X)^{-1} X^T$\nËøîÂõû $\\mathbf w = \\mathbf X^\\dagger y$\nLinear regression for classification Âà©Áî®Á∫øÊÄßÂõûÂΩí‰∏ÄÊ¨°ÊÄßËß£Âá∫ $\\mathbf w$ÔºåÂ∞ÜÂÖ∂‰Ωú‰∏∫perceptionÁöÑÂàùÂÄºÔºåÂÜçËø≠‰ª£„ÄÇ Linear regression Â≠¶‰π†‰∏Ä‰∏™ÂÆûÂÄºÂáΩÊï∞ $y=f(x)$ ‰∫åÂàÜÁ±ªÂáΩÊï∞ÁöÑ $\\pm 1$ ‰πüÊòØÂÆûÊï∞ ‰ΩøÁî®linear regression ‚ÄúËÆ≠ÁªÉ‚ÄùÔºàÂ≠¶‰π†ÔºâÂà∞ÊúÄ‰Ω≥$\\mathbf w$Ôºà‰Ωø$E_{in}$(Âπ≥ÊñπËØØÂ∑Æ)ÊúÄÂ∞èÔºâ$\\mathbf w^T \\mathbf x_n \\approx y_n = \\pm 1$ Â∞ÜËøô‰∏™$\\mathbf w$ ‰Ωú‰∏∫perceptronÁöÑÂàùÂßãÂÄºÂºÄÂßãËÆ≠ÁªÉÔºåÈöèÊú∫ÂàùÂßãÂåñwÂèØËÉΩËø≠‰ª£ÂæàÂ§öÊ¨°‰πü‰∏ç‰ºöÊî∂Êïõ„ÄÇ Linear regression boundary Á∫øÊÄßÂõûÂΩí one-shot Ëß£Âá∫ÁöÑw ÂØπÂ∫î‰∏ÄÊù°Áõ¥Á∫ø. ÂõûÂΩíÊòØ‰∏∫‰∫Ü‰ΩøÊï¥‰ΩìÁöÑ $E_{in}$ (ÁÇπÂà∞Ë∂ÖÂπ≥Èù¢ÁöÑË∑ùÁ¶ª) ÊúÄÂ∞èÔºåÂΩì‰∏§Á±ªÊï∞ÊçÆÂàÜÂ∏É‰∏çÂùáÂåÄÊó∂ÔºåË∂ÖÂπ≥Èù¢‰ºöÂÅèÁßª‚ÄúÂàÜÁ±ªËæπÁïå‚Äù„ÄÇin-smaple Error (Âπ≥ÊñπËØØÂ∑Æ)‰∏çÊòØClassification error„ÄÇÂÜçÁî® perception ‰ºòÂåñÂàÜÁ±ªÁªìÊûú„ÄÇ Video 9\nNonlinear transformation Use $\\Phi$ to transform the non-linear input space $\\mathcal X$ to a linear space $\\mathcal Z$ (where there is linear relation between $\\mathbf w$s)\nAny point $\\mathbf x \\overset{\\Phi}{\\rightarrow} \\mathbf z$ preserves the linearity, so that points are linearly separable.\n$g(\\mathbf x) = \\tilde g(\\Phi(\\mathbf x)) = \\rm sign(\\tilde \\mathbf{w}^T \\Phi(\\mathbf x))$\nTransformation:\n$$ \\begin{aligned} \\mathbf x = (x_0, x_1, \\cdots, x_d)\\ \u0026amp;\\overset{\\Phi}{\\rightarrow} \\mathbf z = (z_0, z_1, \\cdots, z_{\\tilde d}) \u0026amp; \\text{Áª¥Â∫¶ÂèØ‰ª•‰∏çÂêå,$x_0$ÊòØbias} \\\n\\mathbf{x_1, x_2, \\cdots, x_N}\\ \u0026amp;\\overset{\\Phi}{\\rightarrow} \\mathbf{z_1, z_2, \\cdots, z_N} \u0026amp; \\text{n‰∏™ÁÇπÈÉΩÂÅöÂèòÊç¢}\\\ny_1, y_2, \\cdots, y_N \\ \u0026amp;\\overset{\\Phi}{\\rightarrow} y_1, y_2, \\cdots, y_N \u0026amp; \\text{Ê†áÁ≠æ‰∏çÂèò} \\\n\\text{No weights in } \\mathcal X \u0026amp; \\qquad \\widetilde \\mathbf w =(w_0, w_1,\\cdots, w_{\\tilde d}) \u0026amp; \\text{zÁ©∫Èó¥‰∏≠Âª∫Á´ãÁ∫øÊÄßÊ®°Âûã} \\\ng(\\mathbf x) \u0026amp;= \\rm sign (\\tilde \\mathbf w^T \\Phi(\\mathbf x)) \\end{aligned} $$\n","date":"2021-12-12T20:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec3_linear_model/","title":"watch: AML 03 | Linear Model"},{"content":"Video 16 Classification 2021-12-06\nsupervised learning practical diagram\nK-means ÂØπ‰∏âËû∫Á∫øÊï∞ÊçÆËÆ≠ÁªÉÔºåÂàÜÁ±ªÁªìÊûú‰∏çÂ•ΩÔºåÂõ†‰∏∫ÂÆÉÊòØÁî®‰∫éËÅöÁ±ªÁöÑ\nFuzzy C Means ÊòØÂè¶‰∏ÄÁßçÊîπËøõÁöÑËÅöÁ±ªÁÆóÊ≥ïÔºåÂàÜÁ±ªÊïàÊûú‰πü‰∏çÂ•Ω\nDecision Tree ÂàÜÁ±ªÂô®ÂØπ‰∏âËû∫Á∫øÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÂàÜÁ±ªÊïàÊûúÂæàÂ•Ω\nK nearest neighbor ÊòØÈùûÂ∏∏Âº∫Â§ßÁöÑ lazy learner\nMultilayer neural network Âº∫Â§ß\nPerformance evaluation ÂàÜÁ±ªÂô®È¢ÑÊµãÁ±ªÂà´Ê†áÁ≠æÊúâÂ§öÂáÜÁ°ÆÔºüÈÄâÊã©Âì™‰∏™ÂàÜÁ±ªÂô®ÔºàÊ®°ÂûãÔºâÊõ¥ÂêàÈÄÇÔºü Remember: The data have to be used both for training and testing. More training data ‚Üí better generalization. More test data ‚Üí better estimation for the classification error probability. Do not evaluate performance on training data ‚Üí the conclusion would be optimistically biased. (Âê¶Âàô‰ºöÂÅèÂêëËÆ≠ÁªÉÈõÜ) Methods for estimating a classifier\u0026rsquo;s accuracy: Holdout method (reserve 2/3 for training and 1/3 for testing) random subsampling (iterative holdout) Cross-validation (partition the data into k folders Stratified oversampling and undersampling (‰øùÊåÅÁ±ªÂà´ÊØî‰æã) Bootstrap (sampling with replacement) Comparing classifiers: Confidence intervals Cost-benefit analysis and ROC Curves Once evaluation is finished, all the available data can be used to train the final classifier. (Áü•ÈÅì‰∫ÜÊúÄ‰Ω≥ÂèÇÊï∞ÂêéÔºåÂÜçÁî®ÂÖ®ÈÉ®ÁöÑÊï∞ÊçÆËÆ≠ÁªÉÊúÄ‰Ω≥ÂÅáËÆæ) Hold out method Given data is randomly partitioned into two independent sets ÊØîÂ¶ÇÔºö2/3 ‰Ωú‰∏∫Training set ÂéªÊûÑÂª∫Ê®°ÂûãÔºå1/3‰Ωú‰∏∫Test set Âéª‰º∞ËÆ°ÂáÜÁ°ÆÁéá Random sampling: It is a variation of holdout method.\nRepeat the method k times, accuracy is estimated as average of obtained accuracies. Confusion matrix Represents the number of correct and incorrect predictions made by the classification model in comparison with the real outcomes (actual class).\nTP or True positive: # of tuples in class positive that were labeled by the classifier as class positive. FN or False negative: # of tuples in class positive that were labeled by the classifier as class negative FP or False positive: # of tuples in class negative that were labeled by the classifier as class positive. TN or True negative # of tuples in class negative that were labeled by the classifier as class negative. Evaluation measures\nMeasure Formula Accuracy, recognition rate (TP+TN)/all Error rate, misclassification rate (FP+FN)/all Sensitivity, true positive rate, recall TP/(TP+FN) Specificity, true negative rate TN/(TN+FP) Precision TP/(TP+FP) F, F1,F-score, Harmonic mean of precision and recall $\\frac{2 \\times \\rm Precision \\times recall}{\\rm Precision + recall}$ $F_\\beta$ where $\\beta$ is a none negative real number $\\frac{(1+\\beta)^2 \\times \\rm Precision \\times recall}{\\beta^2 \\times \\rm Precision +recall}$ Accuracy/recognition rate: the proportion of the total number of predictions that were correct. Error rate: 1- accuracy Precision: what % of tuples that the classifier labeled as positive are actually positive (Êü•ÂáÜÁéá) Recall: what % of positive tuples did the classifier label as positive? (Êü•ÂÖ®Áéá) ÂΩìÊï∞ÊçÆÂá†‰πéÊòØÂùáÂåÄÂàÜÂ∏ÉÊó∂ÔºöÂáÜÁ°ÆÊÄßÂèØ‰ª•Êàê‰∏∫‰∏Ä‰∏™ÂæàÂ•ΩÁöÑËØÑ‰º∞ÊåáÊ†á\nÊØîÂ¶Ç100‰∏™‰∫∫Ôºå99‰∏™Ê≤°ÊÇ£ÁôåÔºå1‰∏™ÊòØÁôåÁóáÊÇ£ËÄÖÔºå‰ΩÜÊòØÊ®°ÂûãÁªìÊûúÊòØ100‰∏™‰∫∫ÈÉΩÊòØÂÅ•Â∫∑ÔºåÂáÜÁ°ÆÁéá99%Ôºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂèØÈù†ÁöÑÊ®°Âûã„ÄÇ\nImbalanced data: There is an important class which is rare. e.g. cancerous patient Classifier may ignore the small class! Accuracy is not a good measurement as it does not consider FN rate that is so important in imbalanced data. In this case, classifier evaluation measures such sensitivity (or recall), specificity, precision, F-measure are better suited. Evaluation ‰πüÂèØ‰ª•ÂÖ≥Ê≥®ÂÖ∂‰ªñÁöÑÊåáÊ†áÔºö\nSpeed, Robustness, Scalability, Interpretability\nReceiving Operating Characteristic (ROC) Represent a relation between sensitivity and specificity for a given classifier.\nThe area under the curve is the measure of the accuracy of the classifier. The perfect accuracy is equal to one. The closer to red line, the less accurate model Â¶ÇÊûúÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéáÊòæËëó‰Ωé‰∫éÁ∫¢Á∫øÔºåÊòØ‰∏çËÉΩÊé•ÂèóÁöÑ(ÊúâÈîô)„ÄÇROC Êõ≤Á∫ø‰∏äÂçáË∂äÂø´ÔºåË∂äÊé•Ëøë1ÔºåË∂äÂ•Ω (Learner 1ÊúÄÂ•Ω)\nIt can be used for visual comparison of classification models.\nROC space: Two dimensional: FP rate on X axis ‚Üí FPR=FP/(TN+FP) TP rate on Y axis ‚Üí TPR=TP/(TP+FN) ÔºàÁÅµÊïèÂ∫¶Ôºâ FPR=1-SPC Ôºà= 1- ÁâπÂºÇÂ∫¶Ôºâ Model Selection Criteria Model selection criteria is always based on a compromise between the complexity of the model and its prediction accuracy on the training data\nGiven a dataset, basically we are looking for the simplest model that attains highest accuracy.\nModel 1 Model 2 Model 3 Complexity ‚úì‚úì ‚úì x (overfit) Training error xx ‚úì ‚úì‚úì Overall - ‚úì - Ensemble system - Strategies \u0026amp; components ÂêàÂ•èÁ≥ªÁªü\nÊØèÊ¨°ÊäΩÂèñ‰∏çÂêåÁöÑÊ†∑Êú¨(Â≠êÈõÜ)ÔºåËÆ≠ÁªÉÂ§ö‰∏™Ê®°ÂûãÔºåÁÑ∂ÂêéËÅöÂêàÔºàaggregationÔºâËµ∑Êù•ÔºåËØØÂ∑ÆÂèØËÉΩÊõ¥Â∞è\n‰πüÂèØ‰ª•ËÆ≠ÁªÉ‰∏çÂêåÁßçÁ±ªÁöÑÂàÜÁ±ªÂô®ÔºöÊÑüÁü•Êú∫ÔºåDT,kNN,SVM\u0026hellip;\nEnsemble Á≥ªÁªüÊúâ‰∏§Key Component: ÂàÜÁ±ªÁÆóÊ≥ïÔºàÊ≥®ÊÑèËÆ≠ÁªÉÈõÜÊ†∑Êú¨ÁöÑÂ§öÊ†∑ÊÄßÔºâÂíåËûçÂêàÊñπÊ≥ïÔºàÁÆÄÂçïÔºöÂ§öÊï∞Á•®Ôºâ\nEnsemble ÈÄÇÂêàÁî®‰∫éÂæàÂ§ßÂÆπÈáèÊï∞ÊçÆÔºå‰πüÂèØ‰ª•Áî®‰∫éÂæàÂ∞èÂÆπÈáèÊï∞ÊçÆ„ÄÇ\nLarge volume data:\nSmall size data:\nÊï∞ÊçÆÂæàÂ∞ëÔºåÁî®Â§çÊùÇÁöÑÊ®°ÂûãÂèØËÉΩÂØºËá¥ËøáÊãüÂêàÔºåÊâÄ‰ª•Á¨¨‰∏ÄÊ¨°‰ΩøÁî®ÊØîËæÉÂº±ÁöÑÊÑüÁü•Êú∫ÔºåÊúâ3‰∏™ÁÇπÂàÜÈîô‰∫ÜÔºåÂ¢ûÂä†ÂÆÉ‰ª¨ÁöÑÊùÉÈáçÔºå‰ΩøÂÆÉ‰ª¨Êõ¥ÂèØËÉΩË¢´ÊäΩÂèñÂà∞‰Ωú‰∏∫‰∏ã‰∏ÄÊ¨°ÁöÑËÆ≠ÁªÉÊ†∑Êú¨„ÄÇÁ¨¨‰∫åÊ¨°ÂàÜÁ±ªÂêéÔºåÂÜçÂº∫Ë∞ÉÂàÜÈîôÁöÑ2‰∏™ËìùÁÇπ„ÄÇÁ¨¨‰∏âÊ¨°ÂàÜÁ±ªÔºåÂ∞±Âè™Êúâ1‰∏™Á∫¢ÁÇπÂàÜÈîô‰∫Ü„ÄÇEnsemble ‰ΩøÂæóÊ®°Âûã‰∏çÂ§çÊùÇÔºåÊõ¥ÂáÜÁ°Æ\nÂÖ∂‰ªñ‰ºòÂäøÔºöÂ§ÑÁêÜÂ§çÊùÇÁöÑÂÜ≥Á≠ñËæπÁïåÔºåÈùûÁ∫øÊÄßÊÉÖÂÜµÔºåÂÆûÊó∂\u0026hellip;\n","date":"2021-12-12T19:58:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/aml_classification/","title":"watch: AML | Classification"},{"content":"Outline\nExample of machine Learning Components of Learning Perceptron Types of learning Puzzle Machine Learning Essence A pattern exists We cannot pin it down mathematically we have data on it. Perceptron $h(\\mathbf x) = \\operatorname{sign} \\left( \\sum_{i=0}^d w_i x_i \\right) = \\operatorname{sign} (\\mathbf w^T \\mathbf x)$ PLA steps: Given the training set: $(\\mathbf x_1, \\mathbf y_1),(\\mathbf x_2, \\mathbf y_2), \\cdots, (\\mathbf x_N, \\mathbf y_N)$ pick a misclassified point: $sign (\\mathbf w^T \\mathbf x_n) \\neq y$ update the weight vector: $\\mathbf w \\leftarrow \\mathbf w + y_n \\mathbf x_n$ Types of learning Supervised learning: input \u0026ldquo;correct output\u0026rdquo;. Unsupervised learning: no \u0026ldquo;correct output\u0026rdquo; input. Reinforcement learning: introduce the grade of output. ËøôÈó®ËØæÂú®ËØÅÊòé‰∏Ä‰ª∂‰∫ãÔºöÈÄöËøáfitÊï∞ÊçÆÔºåÂ∞±ÂèØ‰ª•‚Äùlearn‚ÄúÂà∞Êú™Áü•ÁõÆÊ†áÂáΩÊï∞„ÄÇ\n","date":"2021-12-12T19:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec1_learning_problem/","title":"watch: AML 01 | Learning Problem"},{"content":"Video 15 Clustering 2021-11-22\nOutline:\nClustering K-means K nearest neighbor Cluster Cluster is a set of data objects that similar to one another within the same group dissimilar to the objects in other groups High quality clusters: High intra-class similarity; Low inter-class similarity Cluster analysis an unsupervised learning (unlabeled) ÁªôÂÆö‰∏ÄÁªÑÊï∞ÊçÆÂØπË±° ÊâæÂà∞Êï∞ÊçÆÂØπË±°‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß ÊääÁõ∏‰ººÁöÑÊï∞ÊçÆÂØπË±°ÂΩíÂà∞ clusters ÂÖ∏ÂûãÂ∫îÁî®:\nAs a stand-alone tool to get insight into data distribution As a preprocessing step for other algorithms (classifier, regressor) ËÅöÁ±ªÊñπÊ≥ïÁöÑË¥®ÈáèÁöÑÁõ∏ÂÖ≥Âõ†Á¥†Ôºö\nthe similarity measure used by the method its implementation its ability to discover some or all of the hidden patterns ËÅöÁ±ªÂàÜÊûêÁöÑËÄÉËôëÂõ†Á¥†\nPartitioning criteria Single level vs. hierarchical partitioning (often, multi-level hierarchical partitioning is desirable) Separation of clusters Exclusive (e.g., one customer belongs to only one region) vs. non-exclusive (e.g., one document may belong to more than one class) ÊòØÂê¶‰∏ìÂ±û‰∫é1Á±ª Similarity measure Distance-based (e.g., Euclidian, road network, vector) vs. connectivity-based (e.g., density or contiguity) Clustering space Full space (often when low dimensional) vs. subspaces (often in high-dimensional clustering) ËÅöÁ±ªÁöÑÊåëÊàòÂíåË¶ÅÊ±Ç\nQuality Ability to deal with different type of attributes (‰∏çÂêåÂ±ûÊÄß) Discovery of clusters with arbitrary shape (‰ªªÊÑèÂΩ¢Áä∂) Ability to deal with noisy data (Âô™Â£∞) Interpretability and usability Constraint based clustering Scalability Constraint based clustering High dimensionality Incremental clustering and insensitivity to input order Similarity measure ÂØπ‰∫é‰∏§‰∏™Ê†∑Êú¨ÁÇπÁöÑÁ¨¨i‰∏™Áª¥Â∫¶: $x_i$Âíå$y_i$Ôºå‰∏§ËÄÖÁöÑÁõ∏‰ººÊÄßÂèØ‰ª•Áî®‰∏Ä‰∏™Ë∑ùÁ¶ªÂáΩÊï∞Ë°®ËææÔºö$\\rm d(x_i, y_i)$ Similarity measure are usually different based on type of data: interval-scaled, boolean, categorical, ordinal ratio, and vector variables. Ë∑ùÁ¶ªÁßçÁ±ªÔºö Euclidean: $\\sqrt{\\sum_{i=1}^{k}\\left(x_{i}-y_{i}\\right)^{2}}$ ‰∏§ÁÇπÊâÄÊúâÂ±ûÊÄßÈó¥ÁöÑË∑ùÁ¶ª Manhattan: $\\sum_{i=1}^{k}\\left|x_{i}-y_{i}\\right|$ Minkowski: $\\left(\\sum_{i=1}^{k}\\left(\\left|x_{i}-y_i\\right|\\right)^{2}\\right)^{1 / q}$ Major approaches Partitioning approaches (ÂàÜÂå∫): They create various partitions and then evaluate them by some criterion e.g., minimizing the sum of square errors typical methods: k-means, k-medoids, CLARANS\nHierarchical approaches (ÂàÜÂ±Ç): They create a hierarchical decomposition of the set of data (or objects) using some criterion typical methods: Diana, Agnes, BIRCH, CHAMELEON\nDensity-based approaches: They are based on connectivity and density functions typical methods: DBSACN, OPTICS, DenClue\nGrid-based approaches: They are based on a multiple-level granularity structure typical methods: STING, WaveCluster, CLIQUE\nModel-based approaches: A model is hypothesized for each of the clusters and then aim to find the best fit of that model to each other typical methods: EM, SOM, COBWEB\nFrequent pattern-based: They are based on the analysis of frequent patterns typical methods: p-Cluster\netc\u0026hellip;\nPartitioning method K-means ÂèØËßÜÂåñÔºöVisualizing K-Means Clustering\nK nearest neighbors supervised, efficient, clustering, classification and regression learning algorithm ","date":"2021-12-11T14:44:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/aml_clustering/","title":"watch: AML | Clustering"},{"content":"Video 9 - Error and noise 10-18-2021\nOutline\nError measures Noisy targets Preamble to the theory Review of Lec 3\nLinear Models\nUsing ‚Äúsignal‚Äù to classify and regress\nsignal:\n$$ \\sum_{i=0}^d w_i x_i = \\mathbf{w^T x} $$\nLinear Classification: $h(\\mathbf x) = \\rm sign(\\mathbf{w^T x})$ (Êää‰ø°Âè∑‰º†ÂÖ•threshold, PLA, Pocket)\nLinear Regression: $h(\\mathbf x) = \\mathbf{w^T x}$ (‰∏çÊää‰ø°Âè∑‰º†ÂÖ•threshold, one-shot learning)\n$\\mathbf w = \\mathbf{(x^T x)^{-1} x^T} y$\nError measures Quantify the dissimilarity between the output of hypothesis $h$ and the output of the unknown target function $f$.\nAlmost all error measures are pointwise\nCompute $h$ and $f$ on individual points $\\mathbf x$ using a pointwise error $e(h(\\mathbf x), f(\\mathbf x))$:\nBinary error: $e(h(\\mathbf x), f(\\mathbf x))= [![ h(\\mathbf x) \\neq f(\\mathbf x) ]!]$ Ôºà‰∏çÁõ∏Á≠âerror=1, Áõ∏Á≠âerror=0Ôºâ (Classification)\nSquared error: $e(h(\\mathbf x), f(\\mathbf x)) = (h(\\mathbf x) - f(\\mathbf x))^2$ (ÁúüÂÆûË∑ùÁ¶ª) (Regression)\nIn-sample error: $h(x)$ ‰∏é $f(x)$ Âú®ÂêÑÊ†∑Êú¨ÁÇπ‰∏äÁöÑÂ∑ÆÂºÇ\n$$ E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^N e(h(\\mathbf x_n), f(\\mathbf x_n)) $$\nOut-of-sample error: $h(x)$ ‰∏é $f(x)$ Âú®Á©∫Èó¥ÊâÄÊúâÁÇπ‰∏äÁöÑÂÅèÂ∑ÆÁöÑÊúüÊúõ\n$$ E_{out}(h) = \\mathbb E_x [e(h(\\mathbf x), f(\\mathbf x))] $$\nHow to choose the error measure\nFalse accept and False reject\nconfusion matrix (Ê∑∑Ê∑ÜÁü©Èòµ):\n$$ \\begin{array}{c|lcr} \u0026amp; \\qquad f (\\text{unknown}) \u0026amp; \\ h\u0026amp; +1 \u0026amp; -1 \\ \\hline +1 \u0026amp; \\text{no error} \u0026amp; \\text{false accept} \\ -1 \u0026amp; \\text{false reject} \u0026amp; \\text{no error} \\ \\end{array} $$\nThe error measure is pretty much related to the kind of application with different penalty.\nNoisy targets Á°ÆÂÆöÁöÑÁõÆÊ†áÂàÜÂ∏É $f(\\mathbf x) = \\mathbb E(y|\\mathbf x)$ + Âô™Â£∞ $y-f(\\mathbf x)$\nÊúâÊó∂Áõ∏ÂêåÁöÑËæìÂÖ•ÂØπÂ∫î‰∏çÂêåÁöÑÊ†áÁ≠æÔºåÊâÄ‰ª•ÊΩúÂú®ÂÖ≥Á≥ª‰∏çÊòØ‰∏Ä‰∏™\u0026quot;ÂáΩÊï∞\u0026quot; $y=f(\\mathbf x)$ÔºåËÄåÊòØ‰∏Ä‰∏™ÂàÜÂ∏É $P(y|\\mathbf x)$\n$\\mathbf x$ ÊåâÁÖßÊüêÁßçÊú™Áü•ÁöÑÂàÜÂ∏É $P(\\mathbf x)$ ‰ªéÁ©∫Èó¥$\\mathcal X$ ‰∏≠ÊäΩÂèñÂá∫Êù•„ÄÇÊ†áÁ≠æ $y$ Êúç‰ªéÂàÜÂ∏É $P(y|\\mathbf x)$„ÄÇÊâÄ‰ª•ËæìÂÖ• $(\\mathbf x,y)$ ÊòØÁî±ËÅîÂêàÂàÜÂ∏É $P(\\mathbf x) P(y|\\mathbf x) = P(\\mathbf x,y)$ ‰∫ßÁîü„ÄÇ\nDetermistic target ÊòØÂΩì P(y|x)=0 ÁöÑÁâπÊÆäÁöÑnoisy target, ÈÇ£Êó∂Âô™Â£∞=0Ôºå‰πüÂ∞±ÊòØ $y=f(\\mathbf x)$\nPreamble to the theory\nLearning is feasible in a probabilitstic sence: $E_{out}(g) \\approx E_{in}(g)$ We need $g\\approx f$, which means $E_{out}(g) \\approx 0$ $E_{out}(g) \\approx E_{in}(g)$ (Hoeffding Inequality) $E_{in}(g) \\approx 0$ (PLA, Pocket, Linear classification/regression) ","date":"2021-12-05T19:19:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec4_error_and_noise/","title":"watch: AML 04 | Error and Noise"},{"content":"Selecting dimensionality reduction with Pipeline and GridSearchCV (Âú®sklearnÁΩëÁ´ôÊêúÁ¥¢\u0026quot;gridsearchcv\u0026quot;ÂèëÁé∞ÁöÑ)\nPCAÈôçÁª¥ÂÆû‰æã(GridSearchCVÊ±ÇÊúÄ‰ºòÂèÇ)\nËã•‰∏çËøõË°åÈôçÁª¥ÔºåÈÄüÂ∫¶ÊÖ¢ÔºåÂáÜÁ°ÆÂ∫¶‰Ωé(ÈùûÂ∏∏‰Ωé‰∏çËÉΩÊé•Âèó)\n","date":"2021-11-22T03:24:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E9%99%8D%E7%BB%B4gridsearch/","title":"DR-GridSearch"},{"content":"Pytorch ‰øùÂ≠òÊ®°Âûã‰∏éÂä†ËΩΩÊ®°Âûã ‰øùÂ≠òÂíåÂä†ËΩΩÊ®°Âûã\n‰øùÂ≠òÊ®°Âûã‰∏éÂä†ËΩΩ\n‰ΩøÁî®ÁöÑ‰∏â‰∏™Ê†∏ÂøÉÂäüËÉΩÔºö\ntorch.save Â∞ÜÂ∫èÂàóÂåñÂØπË±°‰øùÂ≠òÂà∞Á£ÅÁõò„ÄÇÊ≠§ÂáΩÊï∞Ë∞ÉÁî®pickleÊ®°ÂùóÊääÔºàÊ®°Âûã„ÄÅtensor„ÄÅÂ≠óÂÖ∏Á≠âÔºâÂØπË±°Â∫èÂàóÂåñ„ÄÇ torch.load ÂáΩÊï∞Ë∞ÉÁî®pickleÁöÑunpicklingÂäüËÉΩÊääÊñá‰ª∂ÂèçÂ∫èÂàóÂåñÂà∞ÂÜÖÂ≠ò„ÄÇ torch.nn.Module.load_state_dict Ë∞ÉÁî®ÂèçÂ∫èÂàóÂåñÂáΩÊï∞state_dictÂä†ËΩΩÊ®°ÂûãÁöÑÂèÇÊï∞Â≠óÂÖ∏„ÄÇ 1. Áä∂ÊÄÅÂ≠óÂÖ∏ state_dict Ê®°ÂûãÁöÑstate_dictÂåÖÊã¨ÔºöÂêÑÂ±ÇÁΩëÁªúÁöÑÂèØÂ≠¶‰π†ÂèÇÊï∞ÔºàÊùÉÈáçÂíåÂÅèÁΩÆÔºâÔºå‰ºòÂåñÂô®ÁöÑstate_dictÂåÖÊã¨‰ºòÂåñÂô®ÁöÑÁä∂ÊÄÅÂíåË∂ÖÂèÇÊï∞„ÄÇ Ê®°Âûãtorch.nn.ModuleÁöÑÂèØÂ≠¶‰π†ÂèÇÊï∞ÔºåÁî®model.parameters()ËÆøÈóÆ 2. ‰∏§ÁßçÊñπÂºèÔºö Âè™‰øùÂ≠òÁΩëÁªú‰∏≠ÁöÑÂèÇÊï∞ÔºàÈÄüÂ∫¶Âø´ÔºåÂç†Á©∫Èó¥Â∞ëÔºåÊé®ËçêÔºâ\n1 2 3 4 5 6 7 #‰øùÂ≠òÂèÇÊï∞ torch.save(net1.state_dict(), \u0026#39;net_params.pt\u0026#39;) #Êàñ\u0026#39;.pth\u0026#39;,Êàñ\u0026#39;pkl\u0026#39; #Âä†ËΩΩ model = ModelClass(*args, **kwargs) #ÂÖàÂÆû‰æãÂåñ‰∏Ä‰∏™Ê®°ÂûãÂØπË±° model = model.load_state_dict(torch.load(PATH)) #ÊääÊñá‰ª∂ÂèçÂ∫èÂàóÂåñÊàêÂ≠óÂÖ∏ÂØπË±°ÔºåÊääÂèÇÊï∞‰º†ÁªôÊ®°Âûã model.eval() #ËÆæÁΩÆdropuout Âíå batch normalizationÂ±Ç‰∏∫ËØÑ‰º∞Ê®°ÂºèÔºåÂê¶ÂàôÂèØËÉΩÂØºËá¥Ê®°ÂûãÊé®Êñ≠ÁªìÊûú‰∏ç‰∏ÄËá¥„ÄÇ ‰øùÂ≠òÊï¥‰∏™ÁΩëÁªúÁöÑÁªìÊûÑÂíåÂèÇÊï∞‰∏éÂä†ËΩΩÔºö\n1 2 3 4 5 6 #‰øùÂ≠ò torch.save(net1, \u0026#39;net.pkl\u0026#39;) #Âä†ËΩΩ newmodel = torch.load(PATH) #‰∏çÈúÄÈáçÊûÑÊ®°ÂûãÔºåÁõ¥Êé•load newmodel.eval() 3. ‰øùÂ≠òÂíåÂä†ËΩΩCheckpoint Áî®‰∫éÊé®ÁêÜ/ÁªßÁª≠ËÆ≠ÁªÉ ‰øùÂ≠òËÆ≠ÁªÉÁä∂ÊÄÅÔºö\n1 2 3 4 5 6 torch.save({\u0026#39;epoch\u0026#39;: epoch+1, #‰øùÂ≠òÂΩìÂâçÁöÑËø≠‰ª£Ê¨°Êï∞ \u0026#39;model_state_dict\u0026#39;: model.state_dict(), #‰øùÂ≠òÊ®°ÂûãÂèÇÊï∞ \u0026#39;optimizer_state_dict\u0026#39;: optimizer.state_dict(), #‰øùÂ≠ò‰ºòÂåñÂô®ÂèÇÊï∞ \u0026#39;loss\u0026#39;: loss, #ÂÖ∂‰Ωô‰∏Ä‰∫õÊÉ≥‰øùÊåÅÁöÑÂèÇÊï∞ÈÉΩÂèØ‰ª•Ê∑ªÂä†ËøõÊù• ..., }, PATH) #ÂêéÁºÄÂèØ‰ª•Áî® \u0026#39;.pth.tar\u0026#39;Êàñ \u0026#39;.pth\u0026#39; Âä†ËΩΩÔºö\n1 2 3 4 5 6 7 8 9 10 11 12 model = ModelClass(*args, **kwargs) optimizer = OptimizerClass(*args, **kwargs) #** checkpoint = torch.load(PATH) model.load_state_dict(checkpoint[\u0026#39;model_state_dict\u0026#39;]) optimizer.load_state_dict(checkpoint[\u0026#39;optimizer_state_dict\u0026#39;]) epoch = checkpoint[\u0026#39;epoch\u0026#39;] loss = checkpoint[\u0026#39;loss\u0026#39;] model.eval() # - or - model.train() „ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇÈÉ®ÂàÜÂÜÖÂÆπÈúÄÂÖ¨‰ºóÂè∑È™åËØÅ\n4. Âú®‰∏Ä‰∏™Êñá‰ª∂‰∏≠‰øùÂ≠òÂ§ö‰∏™Ê®°Âûã 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # ‰øùÂ≠ò torch.save({ \u0026#39;modelA_state_dict\u0026#39;: modelA.state_dict(), \u0026#39;modelB_state_dict\u0026#39;: modelB.state_dict(), \u0026#39;optimizerA_state_dict\u0026#39;: optimizerA.state_dict(), \u0026#39;optimizerB_state_dict\u0026#39;: optimizerB.state_dict(), ... }, PATH) # Âä†ËΩΩ modelA = TheModelAClass(*args, **kwargs) modelB = TheModelBClass(*args, **kwargs) optimizerA = TheOptimizerAClass(*args, **kwargs) optimizerB = TheOptimizerBClass(*args, **kwargs) checkpoint = torch.load(PATH) modelA.load_state_dict(checkpoint[\u0026#39;modelA_state_dict\u0026#39;]) modelB.load_state_dict(checkpoint[\u0026#39;modelB_state_dict\u0026#39;]) optimizerA.load_state_dict(checkpoint[\u0026#39;optimizerA_state_dict\u0026#39;]) optimizerB.load_state_dict(checkpoint[\u0026#39;optimizerB_state_dict\u0026#39;]) modelA.eval() modelB.eval() # - or - modelA.train() modelB.train() ÂΩì‰øùÂ≠ò‰∏Ä‰∏™Ê®°ÂûãÁî±Â§ö‰∏™torch.nn.ModulesÁªÑÊàêÊó∂Ôºå‰æãÂ¶ÇGAN(ÂØπÊäóÁîüÊàêÁΩëÁªú)„ÄÅsequence-to-sequence (Â∫èÂàóÂà∞Â∫èÂàóÊ®°Âûã), ÊàñËÄÖÊòØÂ§ö‰∏™Ê®° ÂûãËûçÂêà, ÂèØ‰ª•ÈááÁî®‰∏é‰øùÂ≠òÂ∏∏ËßÑÊ£ÄÊü•ÁÇπÁõ∏ÂêåÁöÑÊñπÊ≥ï„ÄÇÊç¢Âè•ËØùËØ¥Ôºå‰øùÂ≠òÊØè‰∏™Ê®°ÂûãÁöÑ state_dict ÁöÑÂ≠óÂÖ∏ÂíåÁõ∏ÂØπÂ∫îÁöÑ‰ºòÂåñÂô®„ÄÇÂ¶ÇÂâçÊâÄËø∞ÔºåÂèØ‰ª•ÈÄö ËøáÁÆÄÂçïÂú∞Â∞ÜÂÆÉ‰ª¨ÈôÑÂä†Âà∞Â≠óÂÖ∏ÁöÑÊñπÂºèÊù•‰øùÂ≠ò‰ªª‰ΩïÂÖ∂‰ªñÈ°πÁõÆÔºåËøôÊ†∑ÊúâÂä©‰∫éÊÅ¢Â§çËÆ≠ÁªÉ„ÄÇ\nPyTorch ‰∏≠Â∏∏ËßÅÁöÑ‰øùÂ≠ò checkpoint ÊòØ‰ΩøÁî® .tar Êñá‰ª∂Êâ©Â±ïÂêç„ÄÇ\nË¶ÅÂä†ËΩΩÈ°πÁõÆÔºåÈ¶ñÂÖàÈúÄË¶ÅÂàùÂßãÂåñÊ®°ÂûãÂíå‰ºòÂåñÂô®ÔºåÁÑ∂Âêé‰ΩøÁî®torch.load()Êù•Âä†ËΩΩÊú¨Âú∞Â≠óÂÖ∏„ÄÇËøôÈáåÔºå‰Ω†ÂèØ‰ª•ÈùûÂ∏∏ÂÆπÊòìÁöÑÈÄöËøáÁÆÄÂçïÊü•ËØ¢Â≠óÂÖ∏Êù•ËÆøÈóÆ‰Ω†ÊâÄ‰øùÂ≠òÁöÑÈ°πÁõÆ„ÄÇ\nËØ∑ËÆ∞‰ΩèÂú®ËøêË°åÊé®ÁêÜ‰πãÂâçÔºåÂä°ÂøÖË∞ÉÁî®model.eval()ÂéªËÆæÁΩÆ dropout Âíå batch normalization ‰∏∫ËØÑ‰º∞„ÄÇÂ¶ÇÊûú‰∏çËøôÊ†∑ÂÅöÔºåÊúâÂèØËÉΩÂæóÂà∞‰∏ç‰∏ÄËá¥ÁöÑÊé®Êñ≠ÁªìÊûú„ÄÇ Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÊÅ¢Â§çËÆ≠ÁªÉÔºåËØ∑Ë∞ÉÁî®model.train()‰ª•Á°Æ‰øùËøô‰∫õÂ±ÇÂ§Ñ‰∫éËÆ≠ÁªÉÊ®°Âºè„ÄÇ\n5. ‰ΩøÁî®Âú®‰∏çÂêåÊ®°ÂûãÂèÇÊï∞‰∏ãÁöÑÁÉ≠ÂêØÂä®Ê®°Âºè 1 2 3 4 5 ‰øùÂ≠ò torch.save(modelA.state_dict(), PATH) Âä†ËΩΩ modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False) Âú®ËøÅÁßªÂ≠¶‰π†ÊàñËÆ≠ÁªÉÊñ∞ÁöÑÂ§çÊùÇÊ®°ÂûãÊó∂ÔºåÈÉ®ÂàÜÂä†ËΩΩÊ®°ÂûãÊàñÂä†ËΩΩÈÉ®ÂàÜÊ®°ÂûãÊòØÂ∏∏ËßÅÁöÑÊÉÖÂÜµ„ÄÇÂà©Áî®ËÆ≠ÁªÉÂ•ΩÁöÑÂèÇÊï∞ÔºåÊúâÂä©‰∫éÁÉ≠ÂêØÂä®ËÆ≠ÁªÉËøáÁ®ãÔºåÂπ∂Â∏åÊúõÂ∏ÆÂä©‰Ω†ÁöÑÊ®°ÂûãÊØî‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉËÉΩÂ§üÊõ¥Âø´Âú∞Êî∂Êïõ„ÄÇ\nÊó†ËÆ∫ÊòØ‰ªéÁº∫Â∞ëÊüê‰∫õÈîÆÁöÑ state_dict Âä†ËΩΩËøòÊòØ‰ªéÈîÆÁöÑÊï∞ÁõÆÂ§ö‰∫éÂä†ËΩΩÊ®°ÂûãÁöÑ state_dict , ÈÉΩÂèØ‰ª•ÈÄöËøáÂú®load_state_dict()ÂáΩÊï∞‰∏≠Â∞ÜstrictÂèÇÊï∞ËÆæÁΩÆ‰∏∫ False Êù•ÂøΩÁï•ÈùûÂåπÈÖçÈîÆÁöÑÂáΩÊï∞„ÄÇ\nÂ¶ÇÊûúË¶ÅÂ∞ÜÂèÇÊï∞‰ªé‰∏Ä‰∏™Â±ÇÂä†ËΩΩÂà∞Âè¶‰∏Ä‰∏™Â±ÇÔºå‰ΩÜÊòØÊüê‰∫õÈîÆ‰∏çÂåπÈÖçÔºå‰∏ªË¶Å‰øÆÊîπÊ≠£Âú®Âä†ËΩΩÁöÑ state_dict ‰∏≠ÁöÑÂèÇÊï∞ÈîÆÁöÑÂêçÁß∞‰ª•ÂåπÈÖçË¶ÅÂú®Âä†ËΩΩÂà∞Ê®°Âûã‰∏≠ÁöÑÈîÆÂç≥ÂèØ„ÄÇ\n6.ÈÄöËøáËÆæÂ§á‰øùÂ≠ò/Âä†ËΩΩÊ®°Âûã 6.1 ‰øùÂ≠òÂà∞ CPU„ÄÅÂä†ËΩΩÂà∞ CPU 1 2 3 4 5 6 ‰øùÂ≠ò torch.save(model.state_dict(), PATH) Âä†ËΩΩ device = torch.device(\u0026#39;cpu\u0026#39;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=device)) ÂΩì‰ªéCPU‰∏äÂä†ËΩΩÊ®°ÂûãÂú®GPU‰∏äËÆ≠ÁªÉÊó∂, Â∞Ütorch.device(\u0026lsquo;cpu\u0026rsquo;)‰º†ÈÄíÁªôtorch.load()ÂáΩÊï∞‰∏≠ÁöÑmap_locationÂèÇÊï∞.Âú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî® map_locationÂèÇÊï∞Â∞ÜÂº†Èáè‰∏ãÁöÑÂ≠òÂÇ®Âô®Âä®ÊÄÅÁöÑÈáçÊñ∞Êò†Â∞ÑÂà∞CPUËÆæÂ§á„ÄÇ\n6.2 ‰øùÂ≠òÂà∞ GPU„ÄÅÂä†ËΩΩÂà∞ GPU 1 2 3 4 5 6 7 8 ‰øùÂ≠ò torch.save(model.state_dict(), PATH) Âä†ËΩΩ device = torch.device(\u0026#34;cuda\u0026#34;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.to(device) # Á°Æ‰øùÂú®‰Ω†Êèê‰æõÁªôÊ®°ÂûãÁöÑ‰ªª‰ΩïËæìÂÖ•Âº†Èáè‰∏äË∞ÉÁî®input = input.to(device) ÂΩìÂú®GPU‰∏äËÆ≠ÁªÉÂπ∂ÊääÊ®°Âûã‰øùÂ≠òÂú®GPUÔºåÂè™ÈúÄË¶Å‰ΩøÁî®model.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))ÔºåÂ∞ÜÂàùÂßãÂåñÁöÑ model ËΩ¨Êç¢‰∏∫ CUDA ‰ºòÂåñÊ®°Âûã„ÄÇÂè¶Â§ñÔºåËØ∑ Âä°ÂøÖÂú®ÊâÄÊúâÊ®°ÂûãËæìÂÖ•‰∏ä‰ΩøÁî®.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))ÂáΩÊï∞Êù•‰∏∫Ê®°ÂûãÂáÜÂ§áÊï∞ÊçÆ„ÄÇËØ∑Ê≥®ÊÑèÔºåË∞ÉÁî®my_tensor.to(device)‰ºöÂú®GPU‰∏äËøîÂõûmy_tensorÁöÑÂâØÊú¨„ÄÇ Âõ†Ê≠§ÔºåËØ∑ËÆ∞‰ΩèÊâãÂä®Ë¶ÜÁõñÂº†ÈáèÔºömy_tensor= my_tensor.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))„ÄÇ\n6.3 ‰øùÂ≠òÂà∞ CPUÔºåÂä†ËΩΩÂà∞ GPU 1 2 3 4 5 6 7 8 ‰øùÂ≠ò torch.save(model.state_dict(), PATH) Âä†ËΩΩ device = torch.device(\u0026#34;cuda\u0026#34;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=\u0026#34;cuda:0\u0026#34;)) # Choose whatever GPU device number you want model.to(device) # Á°Æ‰øùÂú®‰Ω†Êèê‰æõÁªôÊ®°ÂûãÁöÑ‰ªª‰ΩïËæìÂÖ•Âº†Èáè‰∏äË∞ÉÁî®input = input.to(device) Âú®CPU‰∏äËÆ≠ÁªÉÂ•ΩÂπ∂‰øùÂ≠òÁöÑÊ®°ÂûãÂä†ËΩΩÂà∞GPUÊó∂ÔºåÂ∞Ütorch.load()ÂáΩÊï∞‰∏≠ÁöÑmap_locationÂèÇÊï∞ËÆæÁΩÆ‰∏∫cuda:device_id„ÄÇËøô‰ºöÂ∞ÜÊ®°ÂûãÂä†ËΩΩÂà∞ ÊåáÂÆöÁöÑGPUËÆæÂ§á„ÄÇÊé•‰∏ãÊù•ÔºåËØ∑Âä°ÂøÖË∞ÉÁî®model.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))Â∞ÜÊ®°ÂûãÁöÑÂèÇÊï∞Âº†ÈáèËΩ¨Êç¢‰∏∫ CUDA Âº†Èáè„ÄÇÊúÄÂêéÔºåÁ°Æ‰øùÂú®ÊâÄÊúâÊ®°ÂûãËæìÂÖ•‰∏ä‰ΩøÁî® .to(torch.device(\u0026lsquo;cuda\u0026rsquo;))ÂáΩÊï∞Êù•‰∏∫CUDA‰ºòÂåñÊ®°Âûã„ÄÇËØ∑Ê≥®ÊÑèÔºåË∞ÉÁî®my_tensor.to(device)‰ºöÂú®GPU‰∏äËøîÂõûmy_tensorÁöÑÊñ∞ÂâØÊú¨„ÄÇÂÆÉ‰∏ç‰ºöË¶ÜÁõñmy_tensor„ÄÇ Âõ†Ê≠§Ôºå ËØ∑ÊâãÂä®Ë¶ÜÁõñÂº†Èáèmy_tensor = my_tensor.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))„ÄÇ\n6.4 ‰øùÂ≠ò torch.nn.DataParallel Ê®°Âûã 1 2 3 4 ‰øùÂ≠ò torch.save(model.module.state_dict(), PATH) Âä†ËΩΩ # Âä†ËΩΩ‰ªª‰Ωï‰Ω†ÊÉ≥Ë¶ÅÁöÑËÆæÂ§á torch.nn.DataParallelÊòØ‰∏Ä‰∏™Ê®°ÂûãÂ∞ÅË£ÖÔºåÊîØÊåÅÂπ∂Ë°åGPU‰ΩøÁî®„ÄÇË¶ÅÊôÆÈÄö‰øùÂ≠ò DataParallel Ê®°Âûã, ËØ∑‰øùÂ≠òmodel.module.state_dict()„ÄÇ ËøôÊ†∑Ôºå‰Ω†Â∞±ÂèØ‰ª•ÈùûÂ∏∏ÁÅµÊ¥ªÂú∞‰ª•‰ªª‰ΩïÊñπÂºèÂä†ËΩΩÊ®°ÂûãÂà∞‰Ω†ÊÉ≥Ë¶ÅÁöÑËÆæÂ§á‰∏≠„ÄÇ\n","date":"2021-11-22T02:56:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E4%BF%9D%E5%AD%98%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"memo: PyTorch | Save Model"},{"content":"F1 score: F1 = 2* (precision * recall )/(precision+recall)\nsklearn.metrics.f1_score\nsklearn‰∏≠ F1-micro ‰∏é F1-macroÂå∫Âà´ÂíåËÆ°ÁÆóÂéüÁêÜ\n","date":"2021-11-22T02:16:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E8%AF%84%E4%BB%B7metrics/","title":"Metrics-sklearn"},{"content":"ÂÜ≥Á≠ñÊ†ëË∞ÉÂèÇ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import numpy as np import matplotlib.pyplot as plt ## Âä†ËΩΩirisÊï∞ÊçÆÈõÜ from sklearn.datasets import load_iris iris = load_iris() X,y = iris.data[:,:2], iris.target #Âè™ÂèñÂâç2‰∏™ÁâπÂæÅ ## ÂàÜÂâ≤ÊµãËØïÈõÜ‰∏éËÆ≠ÁªÉÈõÜ from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify = y, random_state= 42) ## ÈÄÇÈÖçÂÜ≥Á≠ñÊ†ëÔºåÂπ∂ËÆ°ÁÆóÂáÜÁ°ÆÁéá from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score dtc = DecisionTreeClassifier() dtc.fit(X_train, y_train) y_pred = dtc.predict(X_test) accuracy_score(y_test, y_pred) ## ÁîªÂá∫ÂÜ≥Á≠ñÊ†ëÁ§∫ÊÑèÂõæ from sklearn.tree import export_graphviz from io import StringIO from IPython.display import Image import pydot def create_png(clf): dot_iris = StringIO() export_graphviz(clf.out_file = dot_iris, feature_name = iris.feature_names[:2], filled = True) graphs = pydot.graph_from_dot_data(dot_iris.getvalue()) return graphs[0].create_png() Image(create_png(dtc)) ## Áî®GridSearchCVÊêúÁ¥¢ÊúÄ‰Ω≥ÂèÇÊï∞ from sklearn.model_selection import GridSearchCV dtc = DecisionTreeClassifier() grid = {\u0026#39;criterion\u0026#39;: [\u0026#39;gini\u0026#39;,\u0026#39;entropy\u0026#39;], \u0026#39;max_depth\u0026#39;: [3,5,7,9,20] } gs = GridSearchCV(dtc, param_grid=grid, cv=5) gs.fit(X_train, y_train) ## Êü•ÁúãÁΩëÊ†ºÊêúÁ¥¢ÂæóÂà∞ÁöÑÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéá accuracy_score(y_test, gs.predict(X_test)) ## Êü•ÁúãÊêúÁ¥¢ÁªìÊûú gs.cv_results_ gs.best_estimator_ ## Êü•ÁúãÁΩëÊ†ºÊêúÁ¥¢ÂæóÂà∞ÁöÑÂÜ≥Á≠ñÊ†ëÁ§∫ÊÑèÂõæ Image(create_png(gs.best_estimator_)) ## Êé¢Á¥¢ÊúÄÂ§ßÊ∑±Â∫¶ÂØπÂÜ≥Á≠ñÊ†ëÊÄßËÉΩÁöÑÂΩ±Âìç grid=[\u0026#39;max_depth\u0026#39;:range(3,50)] gs2 = GridSearchCV(dtc, param_grid=grid, cv=5) gs2.fit(X_train, y_train) gs2.cv_results_[\u0026#39;mean_test_score\u0026#39;] plt.plot(range(3,50), gs2.cv_results_[\u0026#39;mean_test_score\u0026#39;]) ÈöèÊú∫Ê£ÆÊûó-„ÄêÊú∫Âô®Â≠¶‰π†„Äë„Äêsklearn„ÄëÁΩëÊ†ºÊêúÁ¥¢GridSearchCV 1. Âä†ËΩΩÊï∞ÊçÆ from sklearn.datasets import load_wine\nwine = load_wine() X = wine.data y = wine.target\nfrom sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier()\n2. ÁΩëÊ†ºÊêúÁ¥¢ÊâæÂá∫ÊúÄ‰ºòÂèÇÊï∞ param_grid = {\u0026ldquo;n_estimator\u0026rdquo;:np.arange(10,201,10), \u0026ldquo;max_features\u0026rdquo;:np.arange(0.1, 1, 0.1), \u0026ldquo;max_depth\u0026rdquo;: np.arange(3,13), \u0026ldquo;bootstrap\u0026rdquo;: [True, False] } #ÂÆö‰πâÂ≠óÂÖ∏ÔºåËÆæÁΩÆÂèÇÊï∞ÁöÑÂèØÂèñÂÄº\nfrom sklearn.model_selection import GridSearchCV\nmyGrid = GridSearchCV(rfc, param_grid=param_grid, cv=5) #ÊûÑÈÄ†ÁΩëÊ†ºÊêúÁ¥¢ÔºåÂÜÖÁΩÆkÊäò‰∫§ÂèâÈ™åËØÅ\nmyGrid.fit(X,y) #ËÆ≠ÁªÉ\nprint( myGrid.best_params_, #ÊúÄ‰ºòÂèÇÊï∞ÁªÑÂêà myGrid.best_score_, myGrid.best_estimator_, #ÊúÄ‰ºòÊ®°Âûã myGrid.best_index_ )\nËæìÂá∫ÊúÄ‰ºòÂèÇÊï∞Ôºö\n1 2 3 4 5 6 7 8 9 10 11 from sklearn import metrics best_parameters = dict() best_parameters = grid_search.best_estimator_.get_params() for param_name in sorted(parameters.keys()): print \u0026#34;\\t%s: %r\u0026#34; % (param_name, best_parameters[param_name]) pipeline.set_params(clf__alpha = 1e-05, tfidf__use_idf = True, vect__max_df = 0.5, vect__max_features = None) pipeline.fit(X_train, y_train) pred = pipeline.predict(X_test) 3. ‰ΩøÁî®ÊúÄ‰ºòÊ®°ÂûãÂÅöÂàÜÁ±ª \u0026hellip;.\n","date":"2021-11-21T21:44:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9gridsearchcv/","title":"GridSearchCV-sklearn"},{"content":"ÊØèÂ§©‰∏ÄÁÇπsklearn‰πãKFold(9.8)\nKFodÊääÊï∞ÊçÆÈõÜÂàíÂàÜÊàêK‰ªΩÔºåËøîÂõû‰∏Ä‰∏™Á¥¢ÂºïÁîüÊàêÂô®ÔºåÂèØ‰ª•Áî®Âæ™ÁéØÈÅçÂéÜÂÆÉ„ÄÇ\nclass sklearn.model_selection.KFold(n_splits='warm', shuffle=False, random_state=None)\nn_splits: KÔºåÊääÊï∞ÊçÆÈõÜÂàíÂàÜÊàêk‰ªΩ shuffle: Êâì‰π±È°∫Â∫èÂÜçÂàíÂàÜ random_state: Áõ∏ÂΩì‰∫éÈöèÊú∫ÁßçÂ≠êÔºå‰∏ÄËà¨ÈÉΩË¶ÅÂíåshuffleÊê≠ÈÖç‰ΩøÁî®ÔºåÂè™ÊúâÂΩìshuffle=TrueÁöÑÊó∂ÂÄôÔºåÊâçÊúâÊÑè‰πâÔºåÊØèÊ¨°Êâì‰π±ÁöÑÁªìÊûúÊòØ‰∏ÄÊ†∑ÁöÑ 1 2 3 4 5 6 from sklearn.model_selection import KFold kf1 = KFold(n_splits=3, shuffle=True) #ÊääÊï∞ÊçÆÈõÜÂàíÂàÜÊàê3‰ªΩ for train_index, test_index in kf1.split(xtrain[:20]): print(\u0026#39;In KFold,test_index is:{}\u0026#39;.format(test_index)) #Á¨¨‰∏Ä‰ªΩÂÅöÈ™åËØÅÈõÜÔºåÂâ©‰∏ã‰∏§‰ªΩÂÅöËÆ≠ÁªÉÈõÜ print(\u0026#39;In KFold,train_index is:{}\u0026#39;.format(train_index)) ÂàÜÂ±ÇÊäΩÊ†∑ÔºåÈúÄË¶Å‰º†ÂÖ•labelÔºö\n1 2 3 4 5 6 7 8 9 from sklearn.model_selection import StratifiedKFold skf = StratifiedKFold(n_splits=3,shuffle=True, random_state=1) for train_index, test_index in skf.split(xtrain[:20],ytrain[:20]): print(\u0026#39;In StratifiedFold,test_index is:{}\u0026#39;.format(test_index)) #Á¨¨‰∏Ä‰ªΩÂÅöÈ™åËØÅÈõÜÔºåÂâ©‰∏ã‰∏§‰ªΩÂÅöËÆ≠ÁªÉÈõÜ print(ytest[test_index].value_counts()) #ÂêÑÁ±ª‰∏™Êï∞1:1 print(\u0026#39;In StratifiedFold,train_index is:{}\u0026#39;.format(train_index)) print(ytrain[test_index].value_counts()) ","date":"2021-11-21T15:28:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/kfold%E5%87%BD%E6%95%B0/","title":"KFoldÂáΩÊï∞-sklearn"},{"content":"RNN Cell ‰∏Ä‰∏™Á∫øÊÄßÂçïÂÖÉ ÊääËæìÂÖ•xÁöÑÁª¥Â∫¶ÔºàinputSizeÔºâÂèòÊç¢Âà∞ hiddenSize Â§ÑÁêÜÂ∏¶ÊúâÊó∂Èó¥Â∫èÂàóÁöÑÊï∞ÊçÆ x2 ‰∏é h1 ÂÖ±ÂêåÂÜ≥ÂÆö h2 tÊó∂ÂàªÁöÑËæìÂÖ• $x_t \\in \\mathbb R^3$ ÁªèËøá RNN cell ËΩ¨Êç¢ÔºåÂèòÊàê‰∫Ü‰∏Ä‰∏™ÈöêÁä∂ÊÄÅÂèòÈáè $h_t \\in \\mathbb R^5$Ôºå Âõ†Ê≠§RNN cellÊòØ‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÔºàÁü©ÈòµËøêÁÆóÔºâÊò†Â∞ÑÂà∞Âè¶Â§ñ‰∏Ä‰∏™Áª¥Â∫¶Á©∫Èó¥Ôºå‰∏çËøáËøô‰∏™Á∫øÊÄßÂ±ÇÊòØÂÖ±‰∫´ÁöÑ„ÄÇ\n$h_t$ ‰ºöÂèÇ‰∏é‰∏ãÊ¨° RNN cell ËÆ°ÁÆó $h_{t+1}$ÔºåÊää‰πãÂâçÁöÑ‰ø°ÊÅØÂêàÂπ∂„ÄÇ Âú®ËÆ°ÁÆó $h_1$ Êó∂ÔºåÈúÄË¶ÅËæìÂÖ•ÂÖàÈ™å $h_0$ÔºåÂ¶ÇÊûúÊ≤°ÊúâÂÖàÈ™åÔºåÂ∞±ËæìÂÖ•‰∏é $h_1, h_2\u0026hellip;$ ÂêåÁª¥Â∫¶ÁöÑÈõ∂ÂêëÈáè„ÄÇ\nÈÅçÂéÜÊï∞ÊçÆÈõÜ $x_t \\in \\mathbb R^{\\rm input_size}$ÔºåÂÅöÁ∫øÊÄßÂèòÊç¢\n$$ W^{\\rm hidden\\ size \\times input\\ size}_{hi} x‚Çú + b_{hi} $$\nÂèòÊç¢Âà∞‰∏Ä‰∏™ hidden_size√ó1 ÁöÑÂêëÈáèÔºõ\n‰∏ä‰∏ÄÂ±ÇÁöÑÈöêÂèòÈáè $h_{t-1}$ ‰πüËøõË°åÁ∫øÊÄßÂèòÊç¢ $W_{hh} h_{t-1} + b_{hh}$ ÂæóÂà∞‰∏Ä‰∏™ hidden_size√ó1ÁöÑÂêëÈáèÔºå ÂÆÉ‰∏é $x_t$ ÁöÑÁ∫øÊÄßÂèòÊç¢ËæìÂá∫Áõ∏Âä†ÔºåÂæóÂà∞ÁöÑÂêëÈáè‰ªç‰∏∫ hidden_size √ó 1Ôºå ÂÜçÂÅöÊøÄÊ¥ª tanhÔºåÁÆóÂá∫ÈöêÂêëÈáè $h_t \\in \\mathbb R^{\\rm hidden_size}$\n‰∏§‰∏™Á∫øÊÄßËøêÁÆóÂèØ‰ª•Âêà‰∏ÄËµ∑Ôºö\n$$ \\begin{aligned} \u0026amp; W_{hh} h_{t-1} + W_{hi} x_{t} \\\\ \u0026amp; = [W_{hh} \\ \\ W_{hi}]^{\\rm h\\ size \\times (h\\ size + i\\ size)} \\begin{bmatrix} h_{t-1} \\\\ x_t \\end{bmatrix}^{\\rm (h\\ size+i\\ size) \\times 1} \\\\ \u0026amp; = h_t^{\\rm h\\ size \\times 1} \\end{aligned} $$\n$h_t = \\rm tanh(W_{hi} x_t + b_{hi} + W_{hh}h_{t-1}+b_{hh})$\nÂú®Pytorch‰∏≠ÔºåÂèØ‰ª•Ëá™Â∑±ÊûÑÈÄ† RNN cellÔºåÂπ∂Â§ÑÁêÜÂ∫èÂàóÁöÑÂæ™ÁéØÔºõ‰πüÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®RNN„ÄÇ\nËá™Â∑±ÂàõÂª∫RNN Cell: cell = torch.nn.RNNCell (input_size=i_size, hidden_size = h_size)\nÁî® cell ËÆ°ÁÆó‰∏ã‰∏ÄÊó∂ÂàªÁöÑÈöêÂèòÈáèÔºöh_1 = cell (x_1, h_0)„ÄÇ\nËøô‰∏§‰∏™ËæìÂÖ•ÁöÑ shapeÔºöx(batch_size, input_size)Ôºåh(batch_size, hidden_size)Ôºå h_1ÁöÑÁª¥Â∫¶‰∏éh_0‰∏ÄÊ†∑„ÄÇbatchÊòØ‰∏ÄÊâπÁöÑÊ†∑Êú¨Êù°Êï∞„ÄÇ\nÊï∞ÊçÆÈõÜÂèØ‰ª•Ë°®ËææÊàê‰∏Ä‰∏™Âº†ÈáèÔºödataset.shape = (seqLen, batch_size, input_size)\n‰ΩøÁî®RNNCellÔºö\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch batch_size = 1 seq_len = 3 #Êúâ3‰∏™Ê†∑Êú¨:x1,x2,x3 input_size = 4 #xÈÉΩÊòØ4x1ÁöÑÂêëÈáè hidden_size =2 #hÈÉΩÊòØ2√ó1ÁöÑÂêëÈáè cell = torch.nn.RNNCell( input_size=input_size, hidden_size=hidden_size ) dataset = torch.randn(seq_len, batch_size, input_size) #ÂàùÂßãÂåñh0ÔºåÂÖ®Èõ∂ hidden = torch.zeros(batch_size, hidden_size) for idx, input in enumerate(dataset): hidden = cell(input, hidden) RNN class ÂÆû‰æãÂåñRNN: cell = torch.nn.RNN(input_size = input_size, hidden_size=hidden_size, num_layer=num_layers)\nRNNÁöÑËæìÂá∫Êúâ‰∏§‰∏™Âº†ÈáèÔºåË∞ÉÁî®Ôºöout, hidden = cell(inputs, hidden)„ÄÇ\ninputs ÊòØÊï¥‰∏™ËæìÂÖ•Â∫èÂàóÔºåËæìÂÖ•ÁöÑ hidden ÊòØ $h_0$Ôºåout ÊòØÊâÄÊúâÁöÑÈöêÂ±ÇËæìÂá∫ $h_1 \\cdots h_N$ÔºåËæìÂá∫ÁöÑ hidden ÊòØÊúÄÂêé‰∏Ä‰∏™ cell ÁöÑËæìÂá∫$h_N$„ÄÇ\ninputs ÁöÑÂΩ¢Áä∂ÊòØ (seqLen, batch, input_size)Ôºåhidden ÁöÑÂΩ¢Áä∂ÊòØ (numLayers, batch, hidden_size)ÔºånumLayers ÊòØRNNÁöÑÂ±ÇÊï∞ÔºåÊØèÂ±ÇÊúÄÁªàÈÉΩ‰ºöËæìÂá∫‰∏Ä‰∏™ÈöêÂèòÈáè„ÄÇ outputÁöÑÂΩ¢Áä∂‰∏∫(seqLen, batch, hidden_size)„ÄÇËæìÂá∫hidden‰∏éËæìÂÖ•hiddenÁöÑÂΩ¢Áä∂‰∏ÄÊ†∑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch batch_size = 1 seq_len = 3 input_size = 4 hidden_size = 2 num_layers = 1 cell = torch.nn.RNN(input_size = input_size, hidden_size= hidden_size, num_layers = num_layers) inputs = torch.randn(seq_len, batch_size, input_size) hidden = torch.zeros(num_layers, batch_size, hidden_size) out, hidden = cell(inputs, hidden) Example: Seq ‚ûî Seq ‰æãÂ≠êÔºö Â∫èÂàó ‚ûî Â∫èÂàóÔºåÊää‚Äúhello\u0026quot;ËΩ¨Êç¢Âà∞‚Äúohlol‚Äù\nÊääËæìÂÖ•‚Äúhello‚ÄùÂèòÊàêÁî±Êï∞Â≠óÊûÑÊàêÁöÑÂêëÈáèÔºö\nÂ≠óÁ¨¶Á∫ßÂà´Ôºå‰∏∫Âá∫Áé∞ËøáÁöÑÂ≠óÁ¨¶ÊûÑÈÄ†‰∏Ä‰∏™ËØçÂÖ∏Ôºõ ËØçÁ∫ßÂà´Ôºå‰∏∫Âá∫Áé∞ËøáÁöÑÂçïËØçÊûÑÈÄ†ËØçÂÖ∏„ÄÇ ËøôÈáå‰∏∫ÊØè‰∏™ÂÖÉÁ¥†ÂàÜÈÖçÁ¥¢ÂºïÔºåÁî®Á¥¢Âºï‰ª£ÊõøÂ≠óÊØç„ÄÇÂÜçÊääÊØè‰∏™Á¥¢ÂºïÂèòÊàê‰∏Ä‰∏™ÂêëÈáèÔºàÁã¨ÁÉ≠Á†ÅÔºâÔºåÂêëÈáèÁöÑÈïøÂ∫¶‰∏éËØçÂÖ∏ÁöÑÊù°Êï∞‰∏ÄÊ†∑\n$$ \\begin{aligned} \\begin{bmatrix} h \\\\ e \\\\ l \\\\ l \\\\ o \\end{bmatrix} \\underset{\\longrightarrow}{ËØçÂÖ∏Êúâ4Êù°Á¥¢Âºï} \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 2 \\\\ 3 \\end{bmatrix} \\rightarrow \\begin{matrix} [0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0] \\\\ [1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0] \\\\ [0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0] \\\\ [0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0] \\\\ [0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1] \\end{matrix} \\end{aligned} $$\n‰æùÊ¨°Êää 5 ‰∏™Áã¨ÁÉ≠ÂêëÈáèËæìÂÖ• RNNÔºåÂêëÈáèÈïøÂ∫¶Ôºàinput_sizeÔºâ = 4Ôºå output ÊòØ 5 ‰∏™ RNN CellÁöÑËæìÂá∫ÔºåÊØè‰∏™ËæìÂá∫ÂØπÂ∫î‰∫é 4 ‰∏™Â≠óÊØçÔºàh, e, l, oÔºâ‰∏≠ÁöÑ‰∏Ä‰∏™ÔºåÂõ†Ê≠§ÊòØ‰∏Ä‰∏™Â§öÂàÜÁ±ªÈóÆÈ¢ò„ÄÇ\nÊâÄ‰ª•ËÆæÁΩÆÊØè‰∏™ hidden ÊòØ‰∏Ä‰∏™ÈïøÂ∫¶‰∏∫ 4 ÁöÑÂêëÈáèÔºåËøô 4 ‰∏™Á∫øÊÄßËæìÂá∫ÂàÜÂà´ÂØπÂ∫îÔºàh,e,l,oÔºâ„ÄÇ ÊääËøô‰∏™ÂêëÈáè‰º†ÂÖ• softmaxÂ∞±ÂèòÊàê‰∫Ü‰∏Ä‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉÔºåË°®Á§∫ 4 ‰∏™Â≠óÊØçÂàÜÂà´ÂèØËÉΩÁöÑÊ¶ÇÁéá„ÄÇ\nÂÜç‰∏éÁúüÂÆûÊ†áÁ≠æÁöÑÁã¨ÁÉ≠ÂêëÈáèËÆ°ÁÆóÊçüÂ§±ÔºåÂç≥ÂèØÁî®ÂèçÂêë‰º†Êí≠+Ê¢ØÂ∫¶‰∏ãÈôç‰ºòÂåñÁΩëÁªúÂèÇÊï∞„ÄÇ\nËá™Â∑±ËÆæËÆ°RNN Cell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 import torch ## Step-1 ÂáÜÂ§áÊï∞ÊçÆ input_size = 4 # ÊØè‰∏™Ê†∑Êú¨ÁöÑÁã¨ÁÉ≠ÂêëÈáèÁöÑÈïøÂ∫¶‰∏∫4 hidden_size = 4 # ÈöêÂèòÈáèÁöÑÈïøÂ∫¶‰∏∫4ÔºåÂØπÂ∫î4‰∏™ËæìÂá∫Á±ªÂà´ batch_size = 1 # number of tokens idx2char = [\u0026#39;e\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;] # Êñπ‰æøÊâìÂç∞ÁªìÊûú x_data = [1, 0, 2, 2, 3] # Ê†∑Êú¨ \u0026#34;hello\u0026#34;ÔºàÁ¥¢ÂºïÔºâ y_data = [3, 1, 2, 3, 2] # ËæìÂá∫ \u0026#34;ohlol\u0026#34; ÁöÑÊ†áÁ≠æ(ÁúüÂÆûÁ±ªÂà´), oÊòØÁ¨¨4Á±ªÔºåhÊòØÁ¨¨2Á±ª one_hot_lookup =[[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]] # ÊääÊ†∑Êú¨Á¥¢ÂºïÂèòÊàêÁã¨ÁÉ≠ÂêëÈáèÔºåÂÖ∂ÂΩ¢Áä∂‰∏∫ (seqSize, input_size) = (5, 4) x_one_hot = torch.Tensor([one_hot_lookup[x] for x in x_data]) # ÂÖ®ÈÉ®Ê†∑Êú¨ÂàÜÊàêÂá†‰∏™ batchÔºå-1Ë°®Á§∫ batch ÁöÑ‰∏™Êï∞ num_batch Ëá™Âä®ËÆ°ÁÆó inputs = x_one_hot.view(-1, batch_size, input_size) # (5,1,4) # label vector (seqSize, 1)„ÄÇÂõ†‰∏∫ batch_size ÊòØ 1ÔºåÊâÄ‰ª•Á¨¨ 1 Áª¥Â∫¶ÊòØ 1 labels = torch.LongTensor(y_data).view(-1,1) # (5,1) ## Step-2 ËÆæËÆ°Ê®°Âûã class Model(torch.nn.Module): #ÁªßÊâøËá™Module def __init__(self, input_size, hidden_size, batch_size): super(Model, self).__init__() self.batch_size = batch_size self.input_size = input_size self.hidden_size = hidden_size self.rnncell = torch.nn.RNNCell( input_size=self.input_size, hidden_size=self.hidden_size ) # ÂÆû‰æãÂåñ RNN Cell def forward(self, input, hidden): # Module ÈáåÈù¢ÁöÑ __call__ÂáΩÊï∞Ë∞ÉÁî®‰∫ÜforwardÊñπÊ≥ï # Áî® input Âíå hidden state ËÆ°ÁÆó‰∏ã‰∏Ä‰∏™ hidden state hidden = self.rnncell(input, hidden) # input: (batchSize, inputSize) return hidden # (batchSize, hiddenSize) def init_hidden(self): #ÁîüÊàêÈªòËÆ§ÁöÑh0,ÂÖ®Èõ∂Áü©Èòµ return torch.zeros(self.batch_size, self.hidden_size) # ÂÆû‰æãÂåñÊ®°ÂûãÂØπË±°,batch_size Âè™ÊúâÂú®ÊûÑÈÄ†ÈªòËÆ§ h0 Êó∂Ë¢´Áî®Âà∞ net = Model(input_size, hidden_size, batch_size) ## Step-3 ÊçüÂ§±Âíå‰ºòÂåñÂô® criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.1) #Âü∫‰∫éSGDÁöÑÊîπËøõ‰ºòÂåñÂô® ## Step-4 ËÆ≠ÁªÉÂæ™ÁéØ n_iters = 15 for epoch in range(n_iters): loss = 0 optimizer.zero_grad() #Ê¢ØÂ∫¶ÂΩíÈõ∂ hidden = net.init_hidden() #ÁîüÊàê h0 print(\u0026#39;Predicted string: \u0026#39;, end=\u0026#39;\u0026#39;) # ÈÅçÂéÜÊØè‰∏™ batchÔºåÊØè‰∏™ batch ÈáåÊúâ batch_size ‰∏™ token„ÄÇ # inputs:Ôºànum_batch, batchSize, inputSizeÔºâ for input, label in zip(inputs, labels): # input: (batchSize, inputSize), label: (batchSize,) # zip ÊòØÊ≤øÁùÄÁü©ÈòµÁöÑÁ¨¨ 0 Áª¥Â∫¶ÊãºÊé•Ôºå‰πüÂ∞±ÊòØÊ≤øÁùÄ num_batch ÊñπÂêëÊãºÊé•ÔºåÊâÄ‰ª•‰∏Ä‰∏™Áü©ÈòµÂØπÂ∫î‰∏Ä‰∏™label„ÄÇ # ËÆ°ÁÆó‰∏ã‰∏ÄÊó∂ÂàªÁöÑ hiddenÔºåÁª¥Â∫¶‰∏∫ (batchSize, hiddenSize) hidden = net(input, hidden) # Á¥ØÂä†ÂêÑ batch ÁöÑÊçüÂ§±ÔºàÈúÄË¶ÅÊûÑÈÄ†ËÆ°ÁÆóÂõæ,‰∏çÂèØÁî®item()Ôºâ loss += criterion(hidden, label) # ÊåâÁª¥Â∫¶ 1 Êâæ hidden ÈáåÁöÑÊúÄÂ§ßÂÄºÁöÑ‰∏ãÊ†á,Â∞±Â±û‰∫éÈÇ£Á±ª _, idx = hidden.max(dim=1) # Áúã‰∏Ä‰∏ãËæìÂá∫‰∫Ü‰ªÄ‰πàÂ≠óÁ¨¶ÔºåËôΩÁÑ∂Â≠óÁ¨¶ÂØπ‰∫ÜÔºå‰ΩÜÂÆÉÁöÑÊ¶ÇÁéá‰∏çÊòØ1ÔºåÊâÄ‰ª•ËøòÊúâÊçüÂ§±„ÄÇ print(idx2char[idx.item()], end=\u0026#39;\u0026#39;) loss.backward() # ËÆ°ÁÆóÊ¢ØÂ∫¶ optimizer.step() # Êõ¥Êñ∞‰∏ÄÊ≠• print(f\u0026#39;, Epoch [{epoch+1}/{n_iters}], loss ={loss.item():.4f}\u0026#39;) #ÊâìÂç∞ÊçüÂ§± Ë∞ÉÁî®pytorchÁöÑRNNÁ±ª 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 import torch ## ÂáÜÂ§áÊï∞ÊçÆ input_size = 4 #ÂÖ±Êúâhelo 4ÁßçÂ≠óÁ¨¶ÔºåÈúÄË¶ÅÁî®4ÂÖÉÁ¥†ÁöÑÁã¨ÁÉ≠ÂêëÈáèÊù•Ë°®Á§∫ÊØèÁßçÂ≠óÁ¨¶ hidden_size = 4 #ËæìÂá∫ÂàÜÂà´Â±û‰∫é4Á±ªÁöÑÊ¶ÇÁéá num_layers = 1 batch_size = 1 #ÊØè‰∏™batchÊòØ1‰∏™Â≠óÁ¨¶ seq_len = 5 #ÂÖ®ÈÉ®Ê†∑Êú¨ÂàÜÊàê5‰∏™seq/batchÔºö\u0026#34;h\u0026#34;;\u0026#34;e\u0026#34;;\u0026#34;l\u0026#34;;\u0026#34;l\u0026#34;;\u0026#34;o\u0026#34; idx2char = [\u0026#39;e\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;] x_data = [1,0,2,2,3] y_data = [3,1,2,3,2] #ÂêÑÊ†∑Êú¨ÁöÑÁúüÂÆûÁ±ªÂà´ÔºåÁî®‰∫éÁ¥¢ÂºïÁúüÂÆûÁ±ªÂà´ÂØπÂ∫îÁöÑÈ¢ÑÊµãÂÄº one_hot_lookup =[[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]] x_one_hot = [one_hot_lookup[x] for x in x_data] #ÊääÊ†∑Êú¨ÂèòÊàêÁã¨ÁÉ≠ÂêëÈáèÔºåÂÖ∂ÂΩ¢Áä∂‰∏∫seq√óinput_size inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size) labels = torch.LongTensor(y_data) #Áª¥Â∫¶(seqSize√óbatchSize,1)Ôºötorch.Size([5])ÔºåÊâÄÊúâÊ†∑Êú¨ÁöÑÊ†áÁ≠æ(Á±ªÂà´) ## ËÆæËÆ°Ê®°Âûã class Model(torch.nn.Module): def __init__(self, input_size, hidden_size, batch_size, num_layers=1): super(Model, self).__init__() self.num_layers = num_layers self.batch_size = batch_size #Áî®‰∫éÂàõÂª∫ÈªòËÆ§h0 self.input_size = input_size self.hidden_size = hidden_size self.rnn = torch.nn.RNN(input_size= self.input_size, hidden_size = self.hidden_size, num_layers=num_layers) def forward(self, input): #ModuleË∞ÉÁî®Ê≠§ÊñπÊ≥ïÔºåËá™Âä®Ëø≠‰ª£Êï∞ÊçÆÈõÜÔºåÁªôÂá∫ÊúÄÁªàËæìÂá∫ hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size) #Â¶ÇÊûúÂú®Â§ñÈù¢ÂÆö‰πâÔºåËøôÂ∞±‰∏çÁî®ÂÜô out, _ = self.rnn(input, hidden) return out.view(-1, self.hidden_size) #ËæìÂá∫ÂèòÊàê‰∏§Áª¥ÁöÑÔºàseqLen√óbatchSize, hiddenSizeÔºâÔºå‰∫§ÂèâÁÜµÊçüÂ§±Âè™ËÉΩÊé•Âèó‰∫åÁª¥ÁöÑTensorÂíå‰∏ÄÁª¥ÁöÑlabelsÔºö‰∏ÄÊù°Ê†∑Êú¨ÁöÑÂêëÈáèÂØπÂ∫î‰∏Ä‰∏™label net = Model(input_size, hidden_size, batch_size, num_layers) ## ÊçüÂ§±Âíå‰ºòÂåñÂô® criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.05) #lr 0.1ÊØîËæÉÂ•Ω ## ËÆ≠ÁªÉÂæ™ÁéØ for epoch in range(15): optimizer.zero_grad() outputs = net(inputs) #inputsÁöÑÁª¥Â∫¶ÔºöSeqLen*BatchSize*InputSizeÔºåoutputsÁöÑÁª¥Â∫¶ÔºöSeqLen*BatchSize*HiddenSize loss = criterion(outputs, labels) #labelsÁª¥Â∫¶ÊòØSeqLen√óBatchSize√ó1Ôºå‰πüÂ∞±ÊòØÔºà5,1Ôºâ loss.backward() optimizer.step() _, idx = outputs.max(dim=1) idx = idx.data.numpy() print(\u0026#39;Predicted:\u0026#39;, \u0026#39;\u0026#39;.join([idx2char[x] for x in idx]), end=\u0026#39;\u0026#39;) #Êää4‰∏™Â≠óÁ¨¶ÊãºÊàê‰∏Ä‰∏™ÂçïËØç print(\u0026#39;,Epoch [%d/15] loss =%.3f\u0026#39; % (epoch+1, loss.item())) #ÊâìÂç∞ÊçüÂ§± Áã¨ÁÉ≠ÂêëÈáèÂú®ÁºñÁ†ÅËØç/Â≠óÁ¨¶Êó∂Ôºö\nÁã¨ÁÉ≠ÂêëÈáèÁª¥Â∫¶Â§™È´òÔºåÂ≠óÁ¨¶Á∫ßÂà´ÈúÄ128Áª¥ÔºåÂçïËØçÁ∫ßÂà´Áª¥Â∫¶Â§™È´ò ÂêëÈáèÂ§™Á®ÄÁñè ÊòØÁ°¨ÁºñÁ†ÅÁöÑ Â∏åÊúõÂêß ÂçïËØç/Â≠óÁ¨¶ ËÅîÁ≥ªÂà∞‰∏Ä‰∏™‰ΩéÁª¥„ÄÅÁ®†ÂØÜ„ÄÅ‰ªéÊï∞ÊçÆ‰∏≠Â≠¶‰π†Âà∞ÁöÑÂêëÈáèÔºåÊµÅË°åÂèàÂº∫Â§ßÁöÑÊñπÊ≥ïÊòØÂµåÂÖ•Â±ÇEmbedding„ÄÇÊääÈ´òÁª¥Á®ÄÁñèÊ†∑Êú¨Êò†Â∞ÑÂà∞‰ΩéÁª¥Á®†ÂØÜÁöÑÁ©∫Èó¥(Êï∞ÊçÆÈôçÁª¥)„ÄÇ\nÁã¨ÁÉ≠ÂêëÈáè$x_n$ÈÄöËøáÂµåÂÖ•Â±Ç Embed ÂèòÊàêÁ®†ÂØÜÁöÑË°®Á§∫ÔºåÁªèËøáRNNÁ∫øÊÄßÂèòÊç¢ÂêéÔºåÂÜçÁî®‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÔºåËÆ©ÊúÄÂêéÁöÑÈöêÂèòÈáèËæìÂá∫‰∏éÂàÜÁ±ªÁöÑÊï∞Èáè‰∏ÄËá¥\n4Áª¥ËΩ¨5Áª¥ÔºåÊûÑÂª∫‰∏Ä‰∏™Êü•ÊâæË°®Ôºö\nÊü•ÊâæË°®ÂÅöËΩ¨ÁΩÆÔºåÂÜç‰πò‰ª•ËæìÂÖ•ÁöÑÁã¨ÁÉ≠ÂêëÈáèÔºåÂ∞±ÂèØÂèñÂá∫‚ÄúÂØπÂ∫îË°å‚Äù\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 num_class = 4 #Á±ªÂà´ input_size = 4 #ËæìÂÖ•Ê†∑Êú¨ÊòØ4Áª¥ÁöÑ hidden_size = 8 #ÈöêÂèòÈáèËæìÂá∫ÊòØ8Áª¥ embedding_size = 10 #ÊääËæìÂÖ•‰ªé4Áª¥ÂµåÂÖ•Âà∞10Áª¥Á©∫Èó¥ num_layers = 2 #2Â±ÇRNN batch_size = 1 seq_len = 5 idx2char = [\u0026#39;e\u0026#39;,\u0026#39;h\u0026#39;,\u0026#39;l\u0026#39;,\u0026#39;o\u0026#39;] x_data = [[1,0,2,2,3]] y_data = [3,1,2,3,2] inputs = torch.LongTensor(x_data) labels = torch.LongTensor(y_data) class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.emb = torch.nn.Embedding(input_size, embedding_size) #ÂµåÂÖ•Â±ÇÊü•ÊâæË°®ÁöÑÂ§ßÂ∞èÔºöinputSize√óembeddingSizeÔºåÂØπËæìÂÖ•Êï∞ÊçÆÂÅöÁª¥Â∫¶ËΩ¨Êç¢ self.rnn = torch.nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #RNNËæìÂÖ•:(batchSize, seqLen, embeddingSize)ÔºåRNNËæìÂá∫:(batchSize, seqLen, hiddenSize) self.fc = torch.nn.Linear(hidden_size, num_class) #Áª¥Â∫¶ÂèòÊç¢: ‰ªéhidden_sizeÂà∞Á±ªÂà´Êï∞Èáè def forward(self, x): hidden = torch.zeros(num_layers, x.size(0), hidden_size) x = self.emb(x) #(batch,seqLen, embeddingSize) ÊääÈïøÊï¥ÂΩ¢ÁöÑÂº†ÈáèËΩ¨ÂèòÊàêÂµåÂÖ•Â±ÇÁ®†ÂØÜÁöÑÂêëÈáè x, _ = self.rnn(x, hidden) x = self.fc(x) return x.view(-1,num_class) #Áª¥Â∫¶Á≠â‰∫éÁ±ªÂà´Êï∞ net = Model() #ÂêéÈù¢ÈÉΩ‰∏ÄÊ†∑ criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.05) for epoch in range(15): optimizer.zero_grad() outputs = net(inputs) #inputsÁöÑÁª¥Â∫¶ÔºöSeqLen*BatchSize*InputSizeÔºåoutputsÁöÑÁª¥Â∫¶ÔºöSeqLen*BatchSize*HiddenSize loss = criterion(outputs, labels) #labelsÁª¥Â∫¶ÊòØSeqLen√óBatchSize√ó1Ôºå‰πüÂ∞±ÊòØÔºà5,1Ôºâ loss.backward() optimizer.step() _, idx = outputs.max(dim=1) #ÂèñÂá∫ÂÄºÊúÄÂ§ßÁöÑÁª¥Â∫¶ÁöÑÁ¥¢ÂºïÔºå‰πüÂ∞±ÊòØÊâÄÂ±ûÁöÑÁ¨¨Âá†Á±ª„ÄÇ idx = idx.data.numpy() #Á¥¢Âºï‰ªé0ÂºÄÂßã print(\u0026#39;Predicted:\u0026#39;, \u0026#39;\u0026#39;.join([idx2char[x] for x in idx]), end=\u0026#39;\u0026#39;) #Êää4‰∏™Â≠óÁ¨¶ÊãºÊàê‰∏Ä‰∏™ÂçïËØç print(\u0026#39;,Epoch [%d/15] loss =%.3f\u0026#39; % (epoch+1, loss.item())) #ÊâìÂç∞ÊçüÂ§± Áî®RNNÂØπMNISTÂõæÁâáÂàÜÁ±ª ÊÑüËßâÈáåÈù¢ÁöÑÊµÅÁ®ãÊå∫ËßÑËåÉÁöÑ„ÄÇ\nÂàùÂßãÂåñh0 ‰∏∫‰∏Ä‰∏™ÂØπËßíÁü©ÈòµÔºõ Áî®‰∫Ü3Â±ÇRNNÔºåÂè™Áî®10‰∏™epochÂ∞±ËææÂà∞96%ÁöÑaccuracy‰∫Ü„ÄÇ Â§ßÊ¶ÇÂéüÁêÜ ÊääMNIST‰∏≠ÊØèÂº†ÂõæÁâáÁúãÊàê‰∏Ä‰∏™Â∫èÂàóÔºåËøô‰∏™Â∫èÂàóÂê´Êúâ28‰∏™$x$ÔºàÂØπÂ∫îÊØè‰∏ÄË°åÂÉèÁ¥†ÔºâÔºåxÁöÑÁª¥Â∫¶ÊòØ28„ÄÇÊØèË°åÂÉèÁ¥†ÈÉΩÂèòÊç¢Âà∞‰∏Ä‰∏™hÔºåËøô‰∏™h‰ºöÂèÇ‰∏éËÆ°ÁÆó‰∏ã‰∏ÄË°åÁöÑhÔºåÊúÄÁªàÂæóÂà∞Á¨¨28Ë°åÁöÑhÔºåÂ∞ÜÊ≠§hËæìÂÖ•Á∫øÊÄßÂÖ®ËøûÊé•Â±ÇÔºåÂèòÊç¢Âà∞10Áª¥ÔºåÊØè‰∏ÄÁª¥ÂØπÂ∫îÁùÄ0-9ÁöÑÊØè‰∏ÄÁ±ª„ÄÇÁÑ∂ÂêéËøô‰∏™10Áª¥ÁöÑÂêëÈáèÈÄöËøásoftmaxÔºåÂ∞±ÊòØÂ±û‰∫éÊØèÁ±ªÁöÑÊ¶ÇÁéá‰∫Ü„ÄÇ\n‰ª£Á†Å 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 import torch import torch.nn as nn import torch.nn.functionall as F import torch.optim as optim import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt # 1. Âä†ËΩΩÊï∞ÊçÆÈõÜ MNIST batch_size = 128 train_dataset = torchvision.datasets.MNIST(root=\u0026#39;./\u0026#39;, train=True, transform=transforms.ToTensor(), download=True) #Âä†ËΩΩËÆ≠ÁªÉÈõÜ test_dataset = torchvision.datasets.MNIST(root=\u0026#39;./\u0026#39;, train=False, transform=transforms.ToTensor()) #Âä†ËΩΩÊµãËØïÈõÜ train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) #ÊûÑÈÄ†Êï∞ÊçÆÂä†ËΩΩÂô®ÔºåÊØèÊ¨°Ëø≠‰ª£‰ªéÊï∞ÊçÆÈõÜ‰∏≠ÂèñÂá∫batchSize‰∏™Ê†∑Êú¨ÔºåÊØè‰∏™epochÈÉΩ‰∏çÂêå test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # Êü•Áúã‰∏Ä‰∏ãÊï∞ÊçÆ def imshow(img): npimg = img.numpy() plt.imshow(np.transpose(npimg,(1,2,0))) dataiter = iter(train_loader) #ÂèñÂá∫‰∏Ä‰ªΩ128Âº†ÂõæÁâá images, labels = dataiter.next() imshow(torchvision.utils.make_grid(images,nrow=15)) #‰∏ÄÊéí15Âº†Âõæ # 2. ÂÆö‰πâÊ®°Âûã N_STEPS = 28 # 1Âº†ÂõæÁâáÊòØ1‰∏™Â∫èÂàóÔºåÊâÄ‰ª•1‰∏™Â∫èÂàóÂåÖÂê´28‰∏™Ê†∑Êú¨ N_INPUTS = 28 # ËæìÂÖ•Êï∞ÊçÆÁöÑÁª¥Â∫¶(1Ë°åÊúâ28‰∏™ÂÉèÁ¥†) N_NEURONS = 150 # RNN‰∏≠Èó¥ÁöÑÁâπÂæÅÁöÑÂ§ßÂ∞è(hiddenSize) N_OUTPUT = 10 # ËæìÂá∫Êï∞ÊçÆÁöÑÁª¥Â∫¶(ÂàÜÁ±ªÁöÑ‰∏™Êï∞) N_EPHOCS = 10 # epochÁöÑÂ§ßÂ∞è(ËÆ≠ÁªÉ10ËΩÆ) N_LAYERS = 3 # 3Â±ÇRNN class ImageRNN(nn.Module): #ÁªßÊâøËá™nn.Module def __init__(self, batch_size, n_inputs, n_neurons, n_outputs, n_layers): super(ImageRNN, self).__init__() self.batch_size = batch_size #ÊØèÊ¨°ËæìÂÖ•batchSizeÂº†ÂõæÁâá self.n_inputs = n_inputs # ËæìÂÖ•ÁöÑÁª¥Â∫¶ self.n_outputs = n_outputs # ÂàÜÁ±ªÁöÑÂ§ßÂ∞è self.n_neurons = n_neurons # RNN‰∏≠ËæìÂá∫ÁöÑÁª¥Â∫¶ self.n_layers = n_layers # RNN‰∏≠ÁöÑÂ±ÇÊï∞ self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons, num_layers=self.n_layers) #ÂÆû‰æãÂåñRNNÂØπË±°ÔºåinputSize,hiddenSize self.FC = nn.Linear(self.n_neurons, self.n_outputs) def init_hidden(self): #ÁîüÊàêh0ÊòØ‰∏Ä‰∏™ÂØπËßíÁü©Èòµ # (num_layers, batch_size, n_neurons) # initialize hidden weights with zero values # Ëøô‰∏™ÊòØnetÁöÑmemory, ÂàùÂßãÂåñmemory‰∏∫0 return (torch.zeros(self.n_layers, self.batch_size, self.n_neurons).to(device)) def forward(self,x): #ËÆ°ÁÆóRNN CellÁöÑËæìÂá∫ # transforms x to dimensions : n_step √ó batch_size √ó n_inputs x = x.permute(1,0,2) # ÈúÄË¶ÅÊään_stepÊîæÂú®Á¨¨‰∏Ä‰∏™Áª¥Â∫¶ self.batch_size = x.size(1) # ÊØèÊ¨°ÈúÄË¶ÅÈáçÊñ∞ËÆ°ÁÆóbatch_size, Âõ†‰∏∫ÂèØËÉΩ‰ºöÂá∫Áé∞‰∏çÂ§ü‰∏Ä‰∏™batchÁöÑÊÉÖÂÜµ self.hidden = self.init_hidden() # ÂàùÂßãÂåñhidden state rnn_out, self.hidden = self.basic_rnn(x,self.hidden) # ÂâçÂêë‰º†Êí≠ out = self.FC(rnn_out[-1]) # Ê±ÇÂá∫ÊØè‰∏ÄÁ±ªÁöÑÊ¶ÇÁéá return out.view(-1,self.n_outputs) # ÊúÄÁªàËæìÂá∫Â§ßÂ∞è : batch_size √ó n_output(10)„ÄÇÁÑ∂ÂêéÊé•softmaxÊâæÂá∫ÊâÄÂ±ûÁ±ªÂà´ÔºåËÆ°ÁÆóÊçüÂ§±„ÄÇ # ÊµãËØï‰∏Ä‰∏ã device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) #Êñπ‰æø‰πãÂêé‰ΩøÁî®gpuËøêÁÆó model = ImageRNN(batch_size, N_INPUTS, N_NEURONS, N_OUTPUT, N_LAYERS).to(device) #ÊûÑÈÄ†RNNÊ®°Âûã model.basic_rnn.weight_hh_l0.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) # ÂàùÂßãÂåñÊ®°ÂûãÁöÑweight‰∏∫ÂØπËßíÁü©Èòµ model.basic_rnn.weight_hh_l1.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) model.basic_rnn.weight_hh_l2.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) dataiter = iter(train_loader) # Âèñ1‰ªΩÊï∞ÊçÆ images, labels = dataiter.next() model.hidden = model.init_hidden() #ÂàùÂßãÂåñh0 logits = model(images.view(-1,28,28).to(device)) #ËæìÂÖ•Ê®°Âûã print(logits[0:2]) #ÊâìÂç∞Ââç2Âº†ÂõæÁöÑhiddenËæìÂá∫:10Áª¥ÂêëÈáè # 3. ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞Âíå‰ºòÂåñÂô® criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(),lr=0.001) def get_accuracy(logit, target, batch_size): #Áî®Êù•ÊúÄÂêéËÆ°ÁÆóÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéá corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum() accuracy = 100.0 * corrects/batch_size return accuracy.item() # 4. ËÆ≠ÁªÉ for epoch in range(N_EPHOCS): train_running_loss = 0.0 train_acc = 0.0 model.train() # trainging round for i, data in enumerate(train_loader): #ÊØèÊ¨°ÂèñÂá∫‰∏Ä‰∏™batch optimizer.zero_grad() #Ê¢ØÂ∫¶Ê∏ÖÈõ∂ # reset hidden states model.hidden = model.init_hidden() #ÂàùÂßãÂåñh0 # get inputs inputs, labels = data #ËæìÂÖ•‰∏Ä‰∏™batchÁöÑÊ†∑Êú¨ÂíåÊ†áÁ≠æ inputs = inputs.view(-1,28,28).to(device) labels = labels.to(device) # forward+backward+optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_running_loss = train_running_loss + loss.detach().item() train_acc = train_acc + get_accuracy(outputs, labels, batch_size) model.eval() print(\u0026#39;Epoch : {:0\u0026gt;2d} | Loss : {:\u0026lt;6.4f} | Train Accuracy : {:\u0026lt;6.2f}%\u0026#39;.format(epoch, train_running_loss/i, train_acc/i)) # 5. ËØÑ‰ª∑Ê®°ÂûãÔºöËÆ°ÁÆóÊµãËØïÈõÜÂáÜÁ°ÆÁéá est_acc = 0.0 for i,data in enumerate(test_loader,0): inputs, labels = data labels = labels.to(device) inputs = inputs.view(-1,28,28).to(device) outputs = model(inputs) thisBatchAcc = get_accuracy(outputs, labels, batch_size) print(\u0026#34;Batch:{:0\u0026gt;2d}, Accuracy : {:\u0026lt;6.4f}%\u0026#34;.format(i,thisBatchAcc)) test_acc = test_acc + thisBatchAcc print(\u0026#39;============Âπ≥ÂùáÂáÜÁ°ÆÁéá===========\u0026#39;) print(\u0026#39;Test Accuracy : {:\u0026lt;6.4f}%\u0026#39;.format(test_acc/i)) #96% # 6. ÂÆö‰πâhook, Êü•ÁúãÊ®°Âûã‰∏≠Èó¥ËøáÁ®ã kÊäò‰∫§ÂèâÈ™åËØÅ „ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„ÄãÁöÑpytorchÁâà\nËøîÂõûÁ¨¨iÊäòËÆ≠ÁªÉÂíåÈ™åËØÅÊï∞ÊçÆ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def get_k_fold_data(k,i,X,y): #Êàë‰∏çÊÉ≥ÊääÊ†áÁ≠æÂàÜÂá∫Êù•,DatasetÂØπË±°ÂèØ‰ª•ÂàÜ assert k\u0026gt;1 fold_size = X.shape[0] // k #ÂÖàÂÅöÈô§Ê≥ïÔºåÂÜçÂêë‰∏ãÂèñÊï¥(‰∏çÂ§ß‰∫é) X_train, y_train = None, None for j in range(k): idx = slice(j*fold_size, (j+1)*fold_size) #ÂàáÁâáÂáΩÊï∞slice(start, end, step) X_part, y_part = X[idx,:], y[idx] if j == i: X_valid, y_valid = X_part, y_part elif X_train is None: X_train, y_train = X_part, y_part else: X_train = torch.cat((X_train, X_part), dim=0) #Â¢ûÂä†Ë°åÊï∞ y_train = torch.cat((y_train, y_part), dim=0) return X_train, y_train, X_valid, y_valid ËÆ≠ÁªÉkÊ¨°\n1 2 3 4 5 def k_fold(k, X_train, num_epochs, learning_rate, weight_decay, batch_size): train_ls_sum, valid_ls_sum = 0, 0 for i in range(k): data = get_k_fold_data(k, i, X_train) .... ","date":"2021-11-21T10:26:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/12_rnn%E5%9F%BA%E7%A1%80/","title":"watch: PyTorch - Âàò‰∫å 12 | RNN Basics"},{"content":"Áî®pytorch Êèê‰æõÁöÑÂ∑•ÂÖ∑ÊûÑÂª∫Á∫øÊÄßÊ®°Âûã\nforward ÂâçÈ¶àÔºöÊ±Ç‰∏ÄÁªÑÊ†∑Êú¨ÁöÑÊçüÂ§± backward ÂèçÂêëÔºöÊ±ÇÊçüÂ§±ÂÖ≥‰∫éÂêÑwÁöÑÊ¢ØÂ∫¶ update Ê¢ØÂ∫¶‰∏ãÈôçÊõ¥Êñ∞w 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import torch ## 1. ÂáÜÂ§áÊï∞ÊçÆ x_data = torch.Tensor([[1.0],[2.0],[3.0]]) #3Êù°Êï∞ÊçÆ‰Ωú‰∏∫‰∏Ä‰∏™batchÔºåÊää3‰∏™Ê†∑Êú¨‰∏ÄÊ¨°ÊÄßÊ±ÇÂá∫‰∏Ä‰∏™ÊçüÂ§±ÔºåÂÆö‰πâÊàêÁü©Èòµ‰ª•‰æøÂà©Áî®numpyÁöÑÂπøÊí≠Êú∫Âà∂ y_data = torch.Tensor([[2.0],[4.0],[6.0]]) #y‰∏éx‰∏ÄÊ†∑‰πüÊòØ3x1ÁöÑÁü©Èòµ ## 2. ËÆæËÆ°Ê®°Âûã: ËÆ°ÁÆó y_pred = x*w +b class LinearModel(torch.nn.Module): #ÊääÊ®°ÂûãÂÆö‰πâ‰∏∫‰∏Ä‰∏™Á±ªÔºåÊâÄÊúâÁöÑÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÈÉΩÁªßÊâøËá™nn.ModuleÔºàÂåÖÂê´‰∫ÜÂæàÂ§öËÆ≠ÁªÉÊñπÊ≥ï(i.e.ÂèçÂêë‰º†Êí≠)Ôºâ def __init__(self): #ÂøÖÈ°ªÂÆûÁé∞ÊûÑÈÄ†ÂáΩÊï∞,ÂàùÂßãÂåñÂØπË±° super(LinearModel, self).__init__() #superË∞ÉÁî®Áà∂Á±ªÁöÑÊûÑÈÄ†ÂáΩÊï∞ self.linear = torch.nn.Linear(1,1) #ÊûÑÈÄ†Á∫øÊÄßÂçïÂÖÉÂØπË±°ÔºöËæìÂÖ•Ê†∑Êú¨ÂíåËæìÂá∫Ê†∑Êú¨ÁöÑÁâπÂæÅÔºàÁª¥Â∫¶ÔºâÈÉΩÊòØ1ÂàóÔºåÊ†∑Êú¨Êù°Êï∞ÊòØ‰∏Ä‰∏™batchÔºà3Êù°Ôºâ def forward(self, x): #ÂøÖÈ°ªÂÆûÁé∞ËÆ°ÁÆóÈ¢ÑÊµãÂÄºÊñπÊ≥ï y_pred = self.linear(x) #linear‰πüÊòØcallableÁöÑÂØπË±°ÔºåÂä†‰∏äÊã¨Âè∑()‰ºöË∞ÉÁî®nn.LinearÁöÑ call ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÂê´forwardÊñπÊ≥ï wx+b return y_pred model = LinearModel() #ÂÆû‰æãÂåñÊ®°ÂûãÂØπË±° ## 3. ÊûÑÈÄ†ÊçüÂ§±ÂáΩÊï∞Âíå‰ºòÂåñÂô® criterion = torch.nn.MSELoss(size_average = False) #ÊçüÂ§±ÂáΩÊï∞MSE, ÂæóÂà∞Ê†áÈáèÊçüÂ§±ÂÄºÔºåËøô‰∏™ËøáÁ®ã‰ºöÊûÑÂª∫ËÆ°ÁÆóÂõæÔºåÊâÄ‰ª•‰πüÂ∫îËØ•ÁªßÊâøËá™nn.Moudle optimizer = torch.optim.SGD(model.parameter(), lr=0.01) #ÂÆû‰æãÂåñ‰ºòÂåñÂô®ÂØπË±°ÔºåÂÆÉ‰∏ç‰ºöÊûÑÂª∫ËÆ°ÁÆóÂõæ, ‰º†ÂÖ•ÈúÄË¶Å‰ºòÂåñÁöÑÂèÇÊï∞Ôºålearning rateÂõ∫ÂÆö ## 4. ËÆ≠ÁªÉÂë®Êúü for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred, y_data)#ÂâçÈ¶àÔºöËÆ°ÁÆóÈ¢ÑÊµãÂÄºÂíåÊçüÂ§± print(epoch, loss) #lossÊòØÂØπË±°ÔºåÊâìÂç∞Êó∂‰ºöËá™Âä®Ë∞ÉÁî®__str__();‰∏ç‰ºö‰∫ßÁîüËÆ°ÁÆóÂõæ optimizer.zero_grad() #ÊâÄÊúâÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶ÂΩíÈõ∂ loss.backward() #ÂèçÂêë‰º†Êí≠ optimizer.step() #ËøõË°å‰∏ÄÊ¨°Êõ¥Êñ∞ÔºåÊ†πÊçÆÊâÄÊúâÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶ÂíåÊ≠•ÈïøÂÅöÊõ¥Êñ∞ ## ËæìÂá∫wÂíåb print(\u0026#39;w=\u0026#39;, model.linear.weight.item()) print(\u0026#39;b=\u0026#39;, model.linear.bias.item()) ## ÊµãËØïÊ®°Âûã x_test = torch.Tensor([[4.0]]) y_test = model(x_test) print(\u0026#39;y_pred =\u0026#39;, y_test.data) ËæìÂÖ•Êï∞ÊçÆÁöÑÊ†ºÂºè Áî®Áü©Èòµ‰∏ÄÊ¨°ÊÄßËÆ°ÁÆóÂá∫‰∏Ä‰∏™batchÁöÑy_pred Êàñ loss\nnumpyÁöÑÂπøÊí≠Êú∫Âà∂ÔºöÂÅöËøêÁÆóÁöÑ‰∏§‰∏™Êï∞ÁªÑÁª¥Â∫¶‰∏çÂêåÔºåÊääÂ∞èÁü©Èòµ(ÈáçÂ§ç)Êâ©ÂÖÖÂà∞‰∏éÂ§ßÁü©ÈòµÁõ∏ÂêåÁöÑÂ§ßÂ∞è\nËøêÁÆóÔºö\n$$ \\begin{aligned} \\begin{bmatrix} y_{pred}^{(1)} \\\\ y_{pred}^{(2)} \\\\ y_{pred}^{(3)} \\end{bmatrix}_{3\\times 1} = w \\cdot \\begin{bmatrix} x^{(1)} \\\\ x^{(2)} \\\\ x^{(3)} \\end{bmatrix}_{3\\times 1} + b \\end{aligned} $$\nÂÖ∂‰∏≠ w Âíå b ‰ºöËß¶ÂèëÂπøÊí≠Êú∫Âà∂ÔºåÂèòÊàê $[w\\ w\\ w]^T_{3\\times 1},\\ [b\\ b\\ b]^T_{3\\times 1}$\nËÆ°ÁÆóÊçüÂ§±Ôºö\n$$ \\begin{bmatrix} loss_1 \\\\ loss_2 \\\\ loss_3 \\end{bmatrix} = \\begin{pmatrix} \\begin{bmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ \\hat{y_3} \\end{bmatrix} - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} \\end{pmatrix}^2 $$\nloss ÈúÄË¶ÅÊòØ‰∏Ä‰∏™Ê†áÈáèÔºåÊâÄ‰ª•ÈúÄÊ±ÇÂíåÔºö$loss = (\\frac{1}{N}) \\sum_{i}^N loss_i$„ÄÇÂêëÈáèÊ≤°Ê≥ïbackward\nnn.LinearÁ±ª class torch.nn.Linear(in_features, out_features, bias=True) Docs-Linear\nÂØπÊï∞ÊçÆÊï∞ÊçÆÂ∫îÁî®‰∏Ä‰∏™Á∫øÊÄßÂèòÊç¢Ôºö$y_{1\\times n} = w^T_{1\\times 3} X_{3\\times n}+b$ÔºånÊù°Ê†∑Êú¨ÔºåxÊúâ3‰∏™ÁâπÂæÅÔºåyÊúâ‰∏Ä‰∏™ÁâπÂæÅ\n‰º†ÂÖ•ÂèÇÊï∞:\nin_feature: ÊØè‰∏™ËæìÂÖ•Ê†∑Êú¨ÁöÑÁª¥Â∫¶ÔºàÂàóÊï∞,ÁâπÂæÅÔºâ out_feature: ËæìÂá∫Ê†∑Êú¨ÁöÑÂàóÊï∞ bias: ÊòØÂê¶ÈúÄË¶ÅbÔºåÈªòËÆ§‰∏∫true Á∫øÊÄßÂçïÂÖÉÂåÖÊã¨‰∏§‰∏™ÊàêÂëòTensors(weight, bias)ÂèØ‰ª•ÂÆåÊàêw*x+bÁöÑËøêÁÆóÔºàÂêåÊ†∑ÁªßÊâøËá™moduleÔºåÂèØ‰ª•ÂèçÂêë‰º†Êí≠Ôºâ\nmagic method __call__() callable ÂØπË±°ÊòØ‰∏Ä‰∏™ÂèØ‰ª•Ë¢´Ë∞ÉÁî®ÊâßË°åÁöÑÂØπË±° Â¶ÇÊûú‰∏∫‰∏Ä‰∏™Á±ªÁºñÂÜô‰∫Ü__call__()ÊñπÊ≥ïÔºåÈÇ£‰πàÂú®ËØ•Á±ªÁöÑÂØπË±°ÂêéÈù¢Âä†Êã¨Âè∑ÔºåÂ∞±‰ºöË∞ÉÁî®ÊâßË°å __call__() ÊñπÊ≥ï 1 2 3 4 5 class Foobar: def __init__(self): pass def __call__(self, *args, **kwargs): #ÂÆûÁé∞Ê≠§ÊñπÊ≥ïÔºåËÆ©ÂØπË±°ÂèØË∞ÉÁî® *args ‰ª£Ë°®Ê≤°ÊúâÂõ∫ÂÆöÊï∞ÈáèÂíåÂèòÈáèÂêçÁöÑËæìÂÖ•ÂèÇÊï∞ÔºåÊòØ‰∏Ä‰∏™ÂÖÉÁªÑÔºõ**kwargsË°®Á§∫Â∏¶ÂèòÈáèÂêçÁöÑËæìÂÖ•ÂèÇÊï∞ÔºåÊòØ‰∏Ä‰∏™Â≠óÂÖ∏Ôºö\n1 2 def fun(*args, **kwargs): pass func(1,2,3, x=3, y=5)Ôºå*args‰∏∫(1,2,3)Ôºå**kwargs‰∏∫{'x':3, 'y':5}ÔºåÂ∞±ÂèØ‰ª•ÈÅçÂéÜÂêÑ‰∏™ÂèÇÊï∞‰∫Ü\nnn.MSELossÁ±ª class torch.nn.MSELoss(size_average=True, reduce=True)\nÁªßÊâøËá™nn.Module\n$\\hat{ùê≤} - ùê≤$ÔºåÂáèÂÆå‰πãÂêéÊ±ÇÂπ≥ÊñπÔºåÊ±ÇÂíå\nÂèÇÊï∞Ôºö\nsize_average=True ÊòØÂê¶ÊúÄÂêéÁöÑÊçüÂ§±Ë¶ÅÊ±ÇÂùáÂÄºÔºå$\\frac{1}{N}$Ê≤°‰ªÄ‰πàÁî®ÔºåÊ±ÇÂØº‰∏éwÊó†ÂÖ≥ reduce=True ÊòØÂê¶ÈôçÁª¥ SGDÁ±ª class torch.optim.SGD(params.lr=\u0026lt;object, object\u0026gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)\nparams ÊòØÈúÄË¶Å‰ºòÂåñÁöÑÂèÇÊï∞„ÄÇ‰ΩøÁî®model.parameters()ÔºåËøô‰∏™ÊàêÂëòÂáΩÊï∞‰ºöÊ£ÄÊü•model‰∏≠ÁöÑÊâÄÊúâÊàêÂëòÔºåÊúâÂì™‰∫õÂèÇÊï∞ÈúÄË¶ÅÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊõ¥Êñ∞ÔºåÂä†ÂÖ•ÈúÄË¶ÅËÆ≠ÁªÉÁöÑÂèÇÊï∞ÈõÜÂêà‰∏≠„ÄÇÊêúÁ¥¢model‰∏≠ÁöÑlinearÊàêÂëòÊó∂Ôºå‰ºöË∞ÉÁî®linear.params()ÔºålinearÊúâ2‰∏™ÂèÇÊï∞Ôºå\n","date":"2021-11-20T16:03:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/5_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92pytorch/","title":"watch: PyTorch - Âàò‰∫å 05 | Linear Regression"},{"content":"ÊØèÊ¨°Ëø≠‰ª£ÔºålossÁöÑÊ≥¢Âä®ÂæàÂ§ßÁöÑËß£ÂÜ≥ÂäûÊ≥ï\nÊØèÊ¨°Ëø≠‰ª£ÔºålossÁöÑÊ≥¢Âä®ÂæàÂ§ßÔºåÊúâÂ¶Ç‰∏ãÂá†ÊñπÈù¢Ôºö\nÂ≠¶‰π†ÁéáÈÄâÂèñËæÉÂ§ßÔºõ Âú®loss‰∏çÂÜç‰∏ãÈôçÁöÑÊó∂ÂÄôÈôç‰ΩéÂ≠¶‰π†ÁéáÔºõ ÊØè‰∏™epochËÆ≠ÁªÉ‰πãÂâçÔºåÂØπÊï∞ÊçÆËøõË°åÈáçÊñ∞Êâì‰π±ÔºåÂ¶ÇÊûú‰Ω†ÁöÑ batch ÁöÑÂÜÖÂÆπÂíåÈ°∫Â∫èÈÉΩÊòØÂõ∫ÂÆöÁöÑÔºåÂèØËÉΩÈÄ†ÊàêÊ®°Âûã overfit Ëøô‰∏™È°∫Â∫èÔºõ ÂêÑ‰∏™ batch ÁöÑ loss Êúâ‰∏çÂêåÊòØÊ≠£Â∏∏ÁöÑÔºå‰ΩÜÂ¶ÇÊûúÊ≥¢Âä®Â§™Â§ßÔºåÂèØËÉΩËØ¥Êòé‰Ω†ÁöÑÂêÑ‰∏™ batch ‰∏çÊòØ homogeneous ÁöÑÔºàÂç≥ÂÜÖÂÆπÂ∑ÆÂà´Â§™Â§ßÔºâÔºå‰∏çËÉΩ‰ª£Ë°®Êï¥‰ΩìÊï∞ÊçÆ„ÄÇÂèØ‰ª•ËØïËØïÂä†Â§ß batch size„ÄÇ ÊÄªÁªìÂ∞±ÊòØÔºö ÂΩìloss‰∏ç‰∏ãÈôçÊó∂ÔºåÈôç‰ΩéÂ≠¶‰π†ÁéáÔºå‰∏ÄËà¨Èôç‰ΩéÂà∞ÂéüÊù•ÁöÑ0.1ÔºåÂú®ÊØè‰∏™epochÂºÄÂßã‰πãÂâçÔºåÂùáÊâì‰π±‰∏ÄÊ¨°Êï∞ÊçÆÔºåÈÄÇÂΩìÂ¢ûÂ§ßbatch_sizeÁöÑÂ§ßÂ∞è„ÄÇ\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\nPytorch‰ΩøÁî®shuffleÊâì‰π±Êï∞ÊçÆ\n","date":"2021-11-20T11:43:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E6%AF%8F%E6%AC%A1%E8%BF%AD%E4%BB%A3loss%E6%B3%A2%E5%8A%A8%E5%BE%88%E5%A4%A7/","title":"lossÊ≥¢Âä®Â§ß"},{"content":"RBFÊ†∏ÂáΩÊï∏ÂèÉÊï∏Ëá™ÂãïÊåëÈÅ∏Ê≥ïPythonÂØ¶‰Ωú\n$x,z$ ÊòØÂéüÁ©∫Èó¥‰∏§‰∏™ÁÇπÁöÑÂùêÊ†áÔºå$\\gamma$ ÊòØÂèÇÊï∞Ôºö\n$$ \\kappa (x,z,\\gamma) = \\operatorname{exp} (-\\gamma | x-z |^2) $$\nrbfÊ†∏ÁöÑÁâπÊÆäÊÄßË¥®Ôºö\nÂú®ÁâπÂæÅÁ©∫Èó¥‰∏≠ÔºåËá™Â∑±ÂíåËá™Â∑±ÁöÑÂÜÖÁßØ‰∏∫1ÔºàËåÉÊï∞ÔºâÔºåÊâÄ‰ª•$\\phi(x)$ ÊòØÂú®‰∏Ä‰∏™Ë∂ÖÁêÉÈù¢‰∏ä\n‰∏§ÂêëÈáèÂ§πËßíÁ≠â‰∫éÂÜÖÁßØÔºö\n$$ cos(\\theta) = \\frac{\\kappa(x,z,\\gamma)}{\\sqrt{\\kappa(x,x,\\gamma)} \\sqrt{\\kappa(z,z,\\gamma)}} = \\frac{\\kappa(x,z,\\gamma)}{1} = \\kappa(x,z,\\gamma) $$\nÂΩì $x \\neq z$ Êó∂Ôºö\n$\\gamma \\rightarrow 0 \\ \\Rightarrow cos(\\theta)=\\kappa(x,z,\\gamma) \\rightarrow 1 \\ \\Rightarrow \\theta \\rightarrow 0^\\circ \\ \\Rightarrow \\phi(x)‰∏é\\phi(z)Ë∂äÁõ∏‰ºº$ $\\gamma \\rightarrow \\infin \\ \\Rightarrow cos(\\theta)=\\kappa(x,z,\\gamma) \\rightarrow 1 \\ \\Rightarrow \\theta \\rightarrow 90^\\circ \\ \\Rightarrow \\phi(x)‰∏é\\phi(z)Ë∂ä‰∏çÁõ∏‰ºº$ Ë∞ÉÂèÇ varying the parameter Âú®ÂéüÁ©∫Èó¥‰∏≠ÁöÑË∑ùÁ¶ªË∂äÂ§ßÂêëÈáèÔºåÂΩì$\\gamma$ÂèòÂ§ßÊó∂ÔºåËßíÂ∫¶ÊãâÂ§ßÁöÑÈÄüÂ∫¶Ë∂äÂø´„ÄÇ ÂΩì$\\gamma$ ÂæàÂ∞èÁöÑÊó∂ÂÄôÔºåÂêÑÂêëÈáèÂ§πËßíÂæàÂ∞èÔºåÁÇπÂæàÈõÜ‰∏≠Ôºå‰∏çÂ•ΩÂàÜÁ±ªÔºõ ÂΩì$\\gamma$ÂæàÂ§ßÊó∂ÔºåÂêÑÂêëÈáè‰∫íÁõ∏ÂûÇÁõ¥ÔºåÊØè‰∏™ÁÇπÈÉΩÂèò‰∏ÄÁ±ªÔºõÊâÄ‰ª•Ë¶ÅÂèñ‰∏çÂ§ß‰∏çÂ∞èÁöÑ$\\gamma$ÔºåÂêåÁ±ªÂêëÈáèÂ§πËßíÂ∞èÔºå‰∏çÂêåÁ±ªÂêëÈáèÁöÑÂ§πËßíÂ∑≤ÁªèÊãâÂà∞ÂæàÂºÄ„ÄÇ\nÁõÆÊ†áÔºö Ë∞ÉÊï¥gammaÔºåÂêåÁ±ªÁöÑÂÜÖÁßØË∂äÊé•Ëøë1Ë∂äÂ•ΩÔºå‰∏çÂêåÁ±ªÂÜÖÁßØË∂äÊé•Ëøë0Ë∂äÂ•Ω\ncode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # ÁîüÊàêÊ†∑Êú¨Êï∞ÊçÆÂèäÂÖ∂Ê†áÁ≠æ from sklearn import datasets Data,labels = datasets.make_circles(n_samples=400, noise=0.1, factor=0.1) #DataÊòØ400x2ÁöÑÁü©ÈòµÔºålabelsÊòØ1x400ÁöÑÂàóË°® # Áîª‰∏âÁª¥Êï£ÁÇπÂõæ import plotly.express as px fig = px.scatter(x=Data[:,0], y=Data[:,1], color=labels) fig.update_layout(yaxis=dict(scalenchor=\u0026#39;x\u0026#39;)) #yËΩ¥ÁöÑÂàªÂ∫¶‰∏éxËΩ¥‰∏ÄÊ†∑ from sklearn.metrics.pairwise import rbf_kernel import numpy as np def AutoRBF(gv, Data, labels): #ËÆ°ÁÆógvÂØπÂ∫îÁöÑÊåáÊ†áJ # ËÆ°ÁÆóKernel matrix K = rbf_kernel(Data, gamma=gv) #400x400 # ÁªÑÂÜÖÔºöÂØπËßíÁ∫ø‰∏äÁöÑblock n=[] #ÁªüËÆ°ÂêÑÁ±ªÊ†∑Êú¨ÁöÑ‰∏™Êï∞ w=0 #ÂêåÁªÑÂÜÖÁßØÁü©ÈòµÂíåÁ¥ØËÆ° for i in range(0, max(labels)+1): #ÈÅçÂéÜÊØè‰∏ÄÁ±ª idx = labels==i #ÊäälabelsÂàóË°®‰∏≠Á≠â‰∫éÊüêÁ±ªÁöÑÂÖÉÁ¥†ÂèòÊàê:True/FalseÔºåÂΩ¢ÊàêÂàóË°®idx #Within the Kernel matrixÔºåÂèñÂá∫ÂêåÁ±ªÁü©Èòµ KW = K[:,idx] #ÂÖàÂèñÂàóÂè∑Á≠â‰∫éidxÂàóË°®‰∏≠ÂÖÉÁ¥†‰∏∫TrueÁöÑindexÁöÑÂàó KW = KW[idx,:] #ÂÜçÂèñË°åÂè∑=idxÂàóË°®‰∏≠TrueÂÖÉÁ¥†ÁöÑindexÁöÑË°å n.append(sum(idx)) #ÁªüËÆ°ÂêÑÁ±ªÂÖÉÁ¥†‰∏™Êï∞ÔºåËøôÈáåÊòØ[200,200] w = w + np.sum(KW) #Á¨¨‰∏ÄÁ±ªÂÜÖÁßØÁü©ÈòµÊ±ÇÂíåÂä†‰∏äÁ¨¨‰∫åÁ±ªÂÜÖÁßØÁü©ÈòµÊ±ÇÂíå # ÁªÑÈó¥Ôºà‰∏çÂêåÁ±ªÊ†∑Êú¨ÂÜÖÁßØÁöÑblockÔºâ b = np.sum(K) - w #ÂÖ®ÈÉ®ÂÖÉÁ¥†‰πãÂíåÂáèÂéªÂêåÁ±ªÂÜÖÁßØÁü©Èòµ‰πãÂíåwÂ∞±ÊòØ‰∏çÂêåÁ±ªÂÜÖÁßØÁü©Èòµ‰πãÂíåb nw = sum(np.power(n,2)) #ÊØèÁ±ª200‰∏™ÔºåÂÖ±200x200 + 200x200=4‰∏á‰∏™ÂÄº nb = sum(n)**2 - nw w=w/nw #ÂèñÂπ≥Âùá b=b/nb #ÂèñÂπ≥Âùá J=(1-w)+b #wË∂äÊé•Ëøë1Ë∂äÂ•ΩÔºåbË∂äÊé•Ëøë0Ë∂äÂ•Ω return J # Á¨¨‰∏ÄÁßçÊñπÊ≥ïÊâæÊúÄ‰Ω≥gammaÔºögrid grid = np.linspace(0,50,1000) #0-50ÂàáÊàê1000‰ªΩ J=[] #Â≠òÂÇ®ÂêÑgvÂØπÂ∫îÁöÑJÂÄº for gv in grid: J.append(AutoRBF(gv,Data,labels)) px.line(x=grid, y=J) #ÊääJÁîªÊàêÊõ≤Á∫ø # Á¨¨‰∫åÁßçÊñπÊ≥ïÔºöÁî® minimize ÂáΩÊï∞ÊâæÊúÄ‰ΩéÁÇπÔºàÊØîgridÂø´ÂæàÂ§öÔºâ from scipy.optimize import minimize gv0=1/Data.shape[1] #ÂàùÂßãÂÄºÔºö1/Áª¥Â∫¶Êï∞ sol = minimize(AutoRBF,gv0,args=(Data,labels)) #solutionÊòØ‰ΩøÁõÆÊ†áÂáΩÊï∞AutoRBFÊúÄÂ∞èÁöÑgvÔºàÂàùÂÄº‰∏∫gv0ÔºâÔºåargsÊîæÂáΩÊï∞ÁöÑÂÖ∂‰ΩôÂèÇÊï∞ bestgv = sol.x #ÊúÄ‰ΩéÁÇπÁöÑxÂùêÊ†áÔºåÊúÄÂ•ΩÁöÑgv # Áî®kpcaÈôçÁª¥ÂêéÔºåËÉΩ‰∏çËÉΩÂàÜÂæóÂ•Ω from sklearn.decomposition import KernelPCA myKPCA = KernelPCA(n_components=2, kernel=\u0026#39;rbf\u0026#39;, gamma=bestgv) myKPCA.fit(Data) #ËÆ≠ÁªÉ reduced_Data = myKPCA.transform(Data) #ÊäïÂΩ±Âà∞‰ΩéÁª¥ fig = px.scatter(x=reduced_Data[:,0], y=reduced_Data[:,1],color=labels) fig.update_layout(yaxis=dict(scaleanchor=\u0026#39;x\u0026#39;)) ","date":"2021-11-19T17:14:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/rbf%E6%A0%B8%E5%8F%82%E6%95%B0%E8%87%AA%E5%8A%A8%E6%8C%91%E9%80%89/","title":"RBF-Kernel-Param-Select(Python)"},{"content":"ÊúÄÂü∫Êú¨ÁöÑÊï∞ÊçÆÁ±ªÂûã:TensorÔºåÂ≠òÂÇ®ÊâÄÊúâÁöÑÊï∞ÂÄºÔºåÊ†áÈáèÔºåÂêëÈáèÔºåÁü©ÈòµÔºåÈ´òÈò∂tensor\ntensorÊúâ‰∏§‰∏™ÊàêÂëòÔºö dataÔºàÊùÉÈáçÊï∞ÂÄºÊú¨Ë∫´wÔºâÔºå gradÔºà‰πüÊòØtensorÔºåÊçüÂ§±ÂÄºÔºàÊ†áÈáèÔºâÂØπÊùÉÈáçÁöÑÂØºÊï∞ ‚àÇloss/‚àÇwÔºâ\nÊûÑÂª∫ËÆ°ÁÆóÂõæ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import torch # ÂáÜÂ§áÊï∞ÊçÆÈõÜ x_data = [1.0, 2.0, 3.0] y_data = [2.0, 4.0, 6.0] w = torch.Tensor([1.0]) # ÂàõÂª∫ÊùÉÈáçÂàùÂßãÂÄºÔºåtensorÂèòÈáè‰∏≠Âè™Êúâ‰∏Ä‰∏™ÂÄº w.requires_grad = True # ËØ•ÂèòÈáèÈúÄË¶ÅËÆ°ÁÆóÊ¢ØÂ∫¶ÔºåÈªòËÆ§ÁöÑtensor‰∏çÈúÄË¶ÅËÆ°ÁÆóÊ¢ØÂ∫¶ # ËÆæËÆ°Ê®°Âûã def forward(x): return x* w # wÊòØ‰∏™tensorÔºå‰πòÊ≥ï*Ë¢´ÈáçËΩΩ‰∫Ü: tensor ‰∏étensor‰πãÈó¥ÁöÑÊï∞‰πòÔºåxË¢´Ëá™Âä®ËΩ¨Êç¢ÊàêtensorÔºåÊâÄ‰ª•‰πòÊ≥ïÁªìÊûú‰πüÊòØtensorÔºåÂπ∂‰∏î‰πü‰ºöÈúÄË¶ÅËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ # ËÆ°ÁÆóÊçüÂ§± def loss(x,y): y_pred = forward(x) # ËÆ°ÁÆóÈ¢ÑÊµãÂÄº return (y_pred - y) ** 2 # ÊØèË∞ÉÁî®‰∏ÄÊ¨°lossÂáΩÊï∞ÔºåËÆ°ÁÆóÂõæË¢´Âä®ÊÄÅÂú∞ÊûÑÂª∫Âá∫Êù• # ËÆ≠ÁªÉËøáÁ®ã print(\u0026#34;predict (before training)\u0026#34;, 4, forward(4).item()) for epoch in range(100): #ËÆ≠ÁªÉ100ËΩÆ for x,y in zip(x_data, y_data): # ÂØπÂ∫îÁªÑÂêàÊãºËµ∑Êù• # ÂâçÈ¶àÔºöËÆ°ÁÆóÊØè‰∏™Ê†∑Êú¨ÁöÑÊçüÂ§±(ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç)ÔºåÊòØ‰∏Ä‰∏™(Ê†áÈáè)tensorÔºåÂê´Êúâ1‰∏™ÂÄºÔºå # Â¶ÇÊûúÊòØ‰∏Ä‰∏™ÂêëÈáèÊ≤°Ê≥ïbackward l = loss(x,y) # ÂèçÂêë‰º†Êí≠ÔºöË∞ÉÁî®Âº†Èáè l ÁöÑÊàêÂëòÂáΩÊï∞ÔºåÊääÂõæ‰∏äÁöÑÊâÄÊúâÊ¢ØÂ∫¶Ê±ÇÂá∫Êù•ÔºåÂ≠òÂà∞ w ‰∏≠ÔºåÁÑ∂ÂêéËÆ°ÁÆóÂõæÂ∞±Ë¢´ÈáäÊîæ‰∫ÜÔºå # ‰∏ã‰∏ÄÊ¨°lossËÆ°ÁÆó‰ºöÂàõÂª∫Êñ∞ÁöÑËÆ°ÁÆóÂõæÔºàÂõ†‰∏∫ÊØèÊ¨°ËÆ°ÁÆóÂõæÂèØËÉΩ‰∏ç‰∏ÄÊ†∑Ôºâ l.backward() print(\u0026#39;\\t grad:\u0026#39;, x,y, w.grad.item()) #itemÊäägradÂèòÊàêint/float,Áõ¥Êé•ÊãøÂá∫ÂÆÉÁöÑÊï∞ÂÄº(Èò≤Ê≠¢‰∫ßÁîüËÆ°ÁÆóÂõæ) w.data = w.data - 0.01 * w.grad.data #Êõ¥Êñ∞Êï∞ÂÄºÔºåÊàêÂëògrad‰πüÊòØ‰∏Ä‰∏™tensorÔºåtensorÁöÑ‰πòÊ≥ï‰ºöÂª∫Á´ãËÆ°ÁÆóÂõæÔºåÊâÄ‰ª•Ë¶ÅÂèñÂÖ∂dataÂÜçÂÅö‰πòÊ≥ïÔºåÂ∞±‰∏ç‰ºöÂª∫Á´ãËÆ°ÁÆóÂõæÔºåÂè™ÊòØ‰øÆÊîπwÁöÑÊï∞ÂÄºÔºàÂπ∂‰∏çÊòØËØ¥‰ª•ÂêéËøòË¶ÅÂØπËøô‰∏™ËøêÁÆóÊ±ÇÊ¢ØÂ∫¶Ôºâ w.grad.data.zero_() #ÊùÉÈáçÁöÑÊ¢ØÂ∫¶(ÂØºÊï∞)ÁöÑÊï∞ÂÄºÊ∏ÖÈõ∂ÔºåÂê¶ÂàôÂêÑÊ¨°Áî±.backward()ËÆ°ÁÆóÂá∫ÁöÑÊ¢ØÂ∫¶ÂÄº‰ºöÁ¥ØÂä†ÔºàÊúâÁöÑÊó∂ÂÄôÈúÄË¶ÅÊ¢ØÂ∫¶Á¥ØÂä†Ôºâ print(\u0026#34;progress\u0026#34;,epoch, 1.item()) #ÊâìÂç∞ÊØèËΩÆËÆ≠ÁªÉÁöÑloss print(\u0026#34;predict (after training)\u0026#34;, 4, forward(4).item()) Â¶ÇÊûúË¶ÅÂú®Á®ãÂ∫è‰∏≠ÂØπÊçüÂ§± l Ê±ÇÂíåÂèñÂπ≥ÂùáÔºåÊ≥®ÊÑèË¶ÅÂèñÂá∫Êï∞ÂÄºÔºà‰ΩøÁî®int/floatËøêÁÆóÔºâÔºö sum + = l.item()ÔºåÂê¶ÂàôÂõ†‰∏∫ l ÊòØtensorÔºå‰∏ÄÁõ¥Âä†ÔºåËÆ°ÁÆóÂõæ‰∏ÄÁõ¥Âª∂ÈïøÔºåÂØºËá¥ÂÜÖÂ≠òÊ≥ÑÊºè„ÄÇ ÔºàËøôÂ∞±ÊòØ‰∏∫‰ªÄ‰πàË¶ÅÈÅøÂÖç in-place operation? 1)\nRef AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD ","date":"2021-11-17T21:47:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/4_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","title":"watch: PyTorch - Âàò‰∫å 04 | Backpropagation"},{"content":"Áî®‰∫éÂÅöÂàÜÁ±ª\nÁ∫øÊÄßÂõûÂΩíÔºö $y\\in \\mathbb R$ÊòØËøûÁª≠ÁöÑ\n$$ Affine Model(Linear unit): \\hat{y} = x*w + b \\ Loss function: loss = (\\hat{y} - y)^2 = (x\\cdot w - y)^2 $$\nÂàÜÁ±ªÔºöyÁöÑÂèñÂÄºÊòØ‰∏Ä‰∏™Á¶ªÊï£ÂÄºÈõÜÂêàÔºöMINIST 0-9\nËÆ°ÁÆóÂ±û‰∫éÊØè‰∏ÄÁ±ªÁöÑÊ¶ÇÁéáÔºåÂèñÊúÄÂ§ßÂÄºÂÅöÂà§Âà´\n","date":"2021-11-15T10:39:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/6_logistic%E5%9B%9E%E5%BD%92/","title":"watch: PyTorch - Âàò‰∫å 06 | Logistic Regression"},{"content":"Source video: „ÄäPyTorchÊ∑±Â∫¶Â≠¶‰π†ÂÆûË∑µ„ÄãÂÆåÁªìÂêàÈõÜ-02\nÂàòÊ¥™ÊôÆ Blog\nÁõëÁù£Â≠¶‰π†ËøáÁ®ãÔºö\nÂáÜÂ§áÊï∞ÊçÆÈõÜ Ê®°ÂûãËÆæËÆ° ËÆ≠ÁªÉ inferring Êï∞ÊçÆÈõÜÔºö\nx(hours) y(grades) 1 2 2 4 3 6 4 ? 1-3 training, 4 testing È¢ÑÊµã\nÁõëÁù£Â≠¶‰π†ÔºöËæìÂá∫ÂÄºÂ∑≤Áü•\nÊï∞ÊçÆÂàÜÊàê Training set Âíå Testing set ‰∏§ÈÉ®ÂàÜ(‰∏çËÉΩÂÅ∑ÁúãÊµãËØïÈõÜÁöÑlabel)\nTraining set Êé•Ëøë Êï∞ÊçÆÁöÑÁúüÂÆûËÅîÂêàÂàÜÂ∏É D(x,y)ÔºåÊ†πÊçÆÂ§ßÊï∞ÂÆöÂæãÈúÄË¶ÅÂ§ßÈáèÊï∞ÊçÆ\nTraining Set ‰∏≠ÂàÜÂá∫‰∏ÄÈÉ®ÂàÜÂÅöÂºÄÂèëÈõÜÔºåÈ™åËØÅÊ®°ÂûãÊÄßËÉΩ\nÊ®°ÂûãËÆæËÆ°Ôºö\nÊ®°ÂûãÔºöy = f(x)\nÂÖàÁî®Á∫øÊÄßÊ®°ÂûãÊòØÂê¶ÊúâÊïàÔºåÂÜçÊç¢ÂÖ∂‰ªñÁöÑÊ®°Âûã„ÄÇ\nÁ∫øÊÄßÊ®°ÂûãÔºö$\\hat{y}=f(x) = x*w +b$\nÈ¢ÑÊµãÂÄº $\\hat{y} = x * w$ (ÂÖà‰∏çËÄÉËôëb)\nÊâæÂà∞ÊúÄ‰ºòÁöÑÊùÉÈáçÔºåÂÖàÈöèÊú∫Êï∞ÔºåËÆ°ÁÆó‰∏éÊï∞ÊçÆÈõÜÁöÑËØØÂ∑ÆÔºàÂπ≥ÊñπÂíåÊúÄÂ∞èÔºâÔºå\nÊçüÂ§±ÂáΩÊï∞,ÈíàÂØπ‰∏Ä‰∏™Ê†∑Êú¨Ôºö$loss = (\\hat(y)-y)^2 = (x*w-y)^2$ÔºåÂøÖÈ°ªÊòØ‰∏Ä‰∏™Ê†áÈáèÔºåÊâçËÉΩËÆ©‰ªñÂèòÂæóÊõ¥Â∞èÔºå‰∏çÊñ≠‰ºòÂåñ„ÄÇ\nx(Hours) y(grades) y_predict(w=3) Loss(w=3) 1 2 3 1 2 4 6 4 3 6 9 9 mean = 14/3 ÈÄâÂèñwÔºåËÆ©Âπ≥ÂùáÊçüÂ§±ÈôçÂà∞ÊúÄ‰Ωé\nCost function, ÂØπ‰∫éÊï¥‰∏™training set, Âπ≥ÂùáÂπ≥ÊñπËØØÂ∑Æ(Mean Square Error): $cost = \\frac{1}{N} \\sum_{n=1}{N}(\\hat{y}_n - y_n)^2$\nx(Hours) Loss(w=0) Loss(w=1) Loss(w=2) Loss(w=3) Loss(w=4) 1 4 3 0 1 4 2 16 6 0 4 16 3 36 9 0 9 36 MSE 18.7 4.7 0 4.7 18.7 ‰∏ç‰øùËØÅËÉΩÊâæÂà∞0ÔºåÂú®ÁúüÂÆûÊúÄÂ∞èÂÄºÈôÑËøëÁ©∑‰∏æ„ÄÇÁªèËøáÊµãËØïÊúÄ‰ºòwÂ≠òÂú®‰∫é0-4‰πãÈó¥ÔºåÂØπ‰πãÈó¥ÊâÄÊúâÂèØËÉΩÂèñÂÄºÔºàÂØπÂÆûÊï∞ÂüüÈááÊ†∑ÔºâÈÉΩËÆ°ÁÆó‰∏Ä‰∏ãÊçüÂ§±ÔºåÁªòÂà∂Êõ≤Á∫øÔºåÊâæÊúÄ‰ΩéÁÇπ„ÄÇ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np import matplotlib.pyplot as plt x_data = [1.0, 2.0, 3.0] #Êï∞ÊçÆÁî®‰∏§‰∏™ÂàóË°®Ë°®Á§∫ y_data = [2.0, 4.0, 6.0] # ÂÆö‰πâÊ®°Âûã def forward(x): #ÂâçÈ¶à return x*w def loss(x,y): y_pred = forward(x) return (y_pred - y) * (y_pred - y) w_list = [] mse_list = [] for w in np.arange(0.0, 4.1, 0.1): #Èó¥Èöî0.1 print(\u0026#39;w=\u0026#39;, w) l_sum = 0 for x_val, y_val in zip(x_data, y_data): #ÊãºÊàêtraining pair y_pred_val = forward(x_val) loss_val = loss(x_val, y_val) #ÊØè‰∏™Ê†∑Êú¨ÁöÑloss l_sum += loss_val #cost functionÊòØloss function ÁöÑÂπ≥Âùá print(\u0026#39;\\t\u0026#39;,x_val, y_val, y_pred_val, loss_val) print(\u0026#39;MSE=\u0026#39;, l_sum/3) w_list.append(w) mse_list.append(l_sum/3) plt.plot(w_list, mse_list) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.xlabel(\u0026#39;w\u0026#39;) plt.show() Áî®ËÆ≠ÁªÉÁöÑËΩÆÊï∞ epoch ÂÅöÊ®™ÂùêÊ†áÔºåÊ£ÄÊü•Ë∂ÖÂèÇÊï∞ÔºåÂà§Âà´ÊòØÂê¶Êî∂Êïõ.\nÊâìÂç∞Êó•ÂøóËæìÂá∫ÔºåÂÆûÊó∂ÁîªÂõæÔºåVisdom\nË¶ÅÂ≠òÁõòÔºåÈÅøÂÖçÂ¥©Ê∫ÉÔºåËÆ°ÁÆóÁôΩË¥π\nÊ®°ÂûãÔºö$\\hat{y} = x*w +b$ Êúâ‰∏§‰∏™ÂèÇÊï∞ÔºåÊçüÂ§±ÂáΩÊï∞ÊòØÊõ≤Èù¢,ÊâæÊúÄ‰ΩéÁÇπ\n3dÂõæÁªòÂà∂‰ΩøÁî® np.meshgrid()\n","date":"2021-11-14T23:32:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/2_%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","title":"watch: PyTorch - Âàò‰∫å 02 | Linear Model"},{"content":"ËÆ≠ÁªÉÊ®°ÂûãÁöÑÊúÄÂ∏∏Áî®ÁÆóÊ≥ï\nwhat is best model for the data?\nlinear model, ÈöèÊú∫ÁåúÊµã‰∏Ä‰∏™ÊñúÁéáÔºàÊùÉÈáçÔºâ,ÊâæËÆ©ËØØÂ∑ÆÊúÄÂ∞èÁöÑÊúÄ‰ºòÊùÉÈáç„ÄÇ\nÈÄöÂ∏∏ÊçüÂ§±ÂáΩÊï∞ÊúâÂ§ö‰∏™ÂèÇÊï∞ÔºåÁ©∑‰∏æÊüê‰∏ÄÂå∫Èó¥‰∏≠ÊâÄÊúâÁÇπÔºåÊêúÁ¥¢ÊúÄ‰ºòÂèÇÊï∞ÔºåËÆ°ÁÆóÈáèÂ§™Â§ßÔºà1‰∏™ÂèÇÊï∞ÊêúÁ¥¢100‰∏™ÁÇπÔºå2‰∏™ÂèÇÊï∞Â∞±ÊòØ100^2Ôºâ\nÂàÜÊ≤ªÔºöÂÖàÂàÜÂ§ßÂùó(4x4)ÊêúÁ¥¢ÔºåÁ°ÆÂÆö‰∏Ä‰∏™Â∞èÂå∫ÂüüÔºåÂÜçÂú®Â∞èÂå∫Âüü‰∏≠ÂàÜÔºà4x4ÔºâÊêúÁ¥¢„ÄÇ‰ΩÜÊòØÂ¶ÇÊûúcost function ÂæàÁ≤óÁ≥ôÔºå„ÄÇ\nÂØªÊâæ‰Ωøcost funciton ÊúÄÂ∞èÁöÑÊùÉÈáçÔºåÊòØ‰∏Ä‰∏™‰ºòÂåñÈóÆÈ¢òÔºö\n$$ Mean Square Error: cost(w) = \\frac{1}{N} \\sum_{n=1}^N (\\hat{y_n}- y_n)^2 \\ w^* = \\underset{w}{argmin} cost(w) $$\nÊ¢ØÂ∫¶‰∏ãÈôçÔºö\nÂàùÂßãÊùÉÈáçÔºåÁ°ÆÂÆöÊªöÂä®ÊñπÂêëÔºåÂà∞ËææÊúÄ‰ΩéÁÇπ\n$w= w- \\alpha \\frac{\\partial cost}{\\partial w}$ (a ÊòØÂ≠¶‰π†ÁéáÔºâ\nÊ¢ØÂ∫¶‰∏ãÈôçÂè™ËÉΩÊâæÂà∞Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇÂÆûÈôÖ‰∏äÔºåÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÁöÑÊçüÂ§±ÂáΩÊï∞Âπ∂Ê≤°ÊúâÂæàÂ§öÂ±ÄÈÉ®ÊúÄ‰ºòÔºå‰ΩÜÊòØÂ≠òÂú®ÈûçÁÇπÔºåÂÆÉÁöÑÊ¢ØÂ∫¶‰∏∫Èõ∂ÔºåÂà∞Ëææ‰∫ÜÊåâÁÇπÊ≤°ÂäûÊ≥ïÁªßÁª≠Ëø≠‰ª£„ÄÇ\nÈöèÊú∫\n","date":"2021-11-14T22:36:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/3_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","title":"watch: PyTorch - Âàò‰∫å 03 | Gradient Descent"},{"content":"Automatic RBF Kernel Parameter Selection Method-ÊùéÊîøËΩ©\nKernel Method ‰ΩøÁî®kernel mapping $\\phi$ ÊääÊï∞ÊçÆÈÄÅÂà∞ feature space„ÄÇ‰ΩÜÈÄöÂ∏∏ $\\phi$ Êú™Áü•ÔºåÊâÄ‰ª•‰ΩøÁî® kernel function $\\kappa$ ÂÅöÂèòÊç¢„ÄÇ\nÊúÄÂ∏∏Áî® RBF kernel:\n$$ \\kappa (x,z,\\sigma) = \\operatorname{exp} \\left( -\\frac{| x-z |^2}{2\\sigma^2} \\right) $$\nË°®Á§∫ÂÜÖÁßØ $\\phi(x)^T \\phi(z)$\nÊÄßË¥®ËæÉÂ•ΩÔºö\nÂú®ÁâπÂæÅÁ©∫Èó¥‰∏≠ÔºåÊØè‰∏™Ê†∑Êú¨ÁöÑËåÉÊï∞ÊòØ +1Ôºõ ÊääÊ†∑Êú¨ÊäïÂΩ±Âà∞ÁêÉÈù¢‰∏äÔºàËÄå‰∏îÊòØÂú®Ê≠£ÁöÑÂç¶ÈôêÔºâ $$ $$\n","date":"2021-11-11T16:05:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/automatic_rbf_kernel_para_select/","title":"RBF-Kernel-Param-Select"},{"content":"P1-ËÉåÊôØ‰ªãÁªç\nKernel Method Êää‰ΩéÁª¥Á©∫Èó¥ÁöÑÈùûÁ∫øÊÄßÈóÆÈ¢òÔºåËΩ¨ÂåñÂà∞È´òÁª¥Á©∫Èó¥ÁöÑÁ∫øÊÄßÈóÆÈ¢òÊ±ÇËß£ Kernel trick ‰ΩøÁî®Ê†∏ÂáΩÊï∞ÂáèÂ∞ëËÆ°ÁÆóÈáèÔºåÈÅøÂÖçËÆ°ÁÆóÈ´òÁª¥ÁâπÂæÅÁ©∫Èó¥ÁöÑÂÜÖÁßØÔºàËÆ°ÁÆóËßíÂ∫¶Ôºâ Kernel Function ÊääËæìÂÖ•Á©∫Èó¥ùìß Êò†Â∞ÑÔºà‰ªªÊÑèÂΩ¢Âºè$œï(x)$ÔºâÂà∞È´òÁª¥ÁâπÂæÅÁ©∫Èó¥ùì©\n$$ K(x,x\u0026rsquo;)=\\phi(x)^T \\phi(x\u0026rsquo;) $$\n‰∏∫‰ªÄ‰πàÊòØÔºö\nÈùûÁ∫øÊÄßÂ∏¶Êù•È´òÁª¥ËΩ¨Êç¢:\nÁ∫øÊÄßÂàÜÁ±ªÊúÄÂÆåÁæéÁöÑÊÉÖÂÜµ:‰∫åÂàÜÁ±ªÈóÆÈ¢ò‰∏•Ê†ºÁ∫øÊÄßÂèØÂàÜÔºåÂç≥Â≠òÂú®‰∏Ä‰∏™ÊàñÂ§ö‰∏™Á∫øÊÄßË∂ÖÂπ≥Èù¢ÂèØ‰ª•Êää‰∏§Á±ªÊ≠£Á°ÆÂàÜÂºÄÔºå‰∏çÂêåÁöÑÂàùÂÄºÊúÄÁªàÊî∂ÊïõÁöÑÁªìÊûú‰∏çÂêå„ÄÇÂØπ‰∫éÈùûÁ∫øÊÄßËæìÂÖ•Êó†Ê≥ïÊî∂Êïõ„ÄÇ\nÂØπ‰∫é‰∏çÂêåÁöÑËæìÂÖ•Êï∞ÊçÆÔºåÈááÁî®‰∏çÂêåÁöÑÁÆóÊ≥ïÔºö\nÁ∫øÊÄßÂèØÂàÜ Á∫øÊÄß‰∏çÂèØÂàÜ Â≠òÂú®‰∏ÄÁÇπÁÇπÈùûÁ∫øÊÄß ‰∏•Ê†ºÈùûÁ∫øÊÄß PLA Pocket Algorithm Â§öÂ±ÇÊÑüÁü•Êú∫(ÈöêËóèÂ±ÇÊï∞‚â•1,ÈÄºËøë‰ªª‰∏ÄËøûÁª≠ÂáΩÊï∞);\nÈùûÁ∫øÊÄßËΩ¨Êç¢(Cover Therom: È´òÁª¥ÊØî‰ΩéÁª¥Êõ¥ÊòìÁ∫øÊÄßÂèØÂàÜ) Hard-margin SVM Soft-margin SVM Kernel SVM(ÂÖàÂÅöÈùûÁ∫øÊÄßËΩ¨Êç¢ÔºåÂÜçÂÅöSVM) ÂØπÂÅ∂Ë°®Á§∫Â∏¶Êù•ÂÜÖÁßØ:\nSVMÁöÑÊÄùÊÉ≥ÊòØÊúÄÂ§ßÈó¥ÈöîÂàÜÁ±ªÔºåÊòØ‰∏Ä‰∏™ÔºàÂá∏Ôºâ‰ºòÂåñÈóÆÈ¢ò„ÄÇÁÑ∂ÂêéÊ†πÊçÆÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ÊÄßÔºåËΩ¨Âåñ‰∏∫Ê±ÇËß£ÂéüÈóÆÈ¢òÁöÑÂØπÂÅ∂ÈóÆÈ¢ò:\n$$ \\begin{array}{c} \\begin{array}{cc} ÂéüÈóÆÈ¢ò\\ \\begin{cases} \\underset{\\mathbf w,b}{\\operatorname{min}}\\ \\frac{1}{2} \\mathbf w^T \\mathbf w \\ s.t. \\quad y_i(\\mathbf w^T x_i + b) \\geq 1 \\end{cases} \\end{array}\n\\Longrightarrow \\begin{array}{cc} ÂØπÂÅ∂ÈóÆÈ¢ò\\ \\begin{cases} \\underset{\\lambda}{\\operatorname{min}}\\ \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j \\mathbf x_i^T \\mathbf x_j - \\sum_{i=1}^N \\lambda_i \\ s.t. \\quad \\lambda_i \\geq 0, \\quad \\forall i=1,\u0026hellip;,N \\ \\ \\qquad \\sum_{i=1}^N \\lambda_i y_i =0 \\end{cases} \\end{array}\n\\end{array}\n$$\nDual Problem ÂåÖÂê´ÂÜÖÁßØ: $\\mathbf x_i^T \\mathbf x_j$ÔºåÂç≥ÈúÄË¶ÅÊ±ÇËß£‰ªªÊÑè‰∏§‰∏™Êï∞ÊçÆ‰πãÈó¥ÁöÑÂÜÖÁßØ\nËÄåÂØπ‰∫éÈùûÁ∫øÊÄßÂèØÂàÜÈóÆÈ¢òÔºåÂÅö‰∫ÜÈùûÁ∫øÊÄßËΩ¨Êç¢‰πãÂêéÔºåÂÜÖÁßØÂèò‰∏∫Ôºö$\\phi(x_i)^T \\phi(x_j)$Ôºå‰ΩÜÊòØÂØπ‰∫éÈ´òÁª¥Á©∫Èó¥ÁöÑ $\\phi(x)$ÔºåÁî±‰∫éÁª¥Â∫¶Â§™È´òÂæàÈöæÊ±Ç„ÄÇ ÊâÄ‰ª•Â∏åÊúõÊâæÂà∞‰∏Ä‰∏™ÂáΩÊï∞Áõ¥Êé•Ê±ÇÂÜÖÁßØ: $K(\\mathbf{x,x\u0026rsquo;})$ÔºåËÄåÈÅøÂÖçÊ±ÇÂçï‰∏™ÁâπÂæÅÁöÑ $\\phi(x)$\nÊï∞Â≠¶Ë°®Á§∫Ôºö\n$$ \\forall \\mathbf{x,x\u0026rsquo;} \\in \\mathcal X,\\quad \\exist \\phi: \\mathcal X \\rightarrow \\mathcal Z s.t. \\quad K(\\mathbf{x,x\u0026rsquo;}) = \\phi(\\mathbf x)^T \\phi(\\mathbf x\u0026rsquo;) = \u0026lt;\\phi(\\mathbf x) \\phi(\\mathbf x\u0026rsquo;)\u0026gt; ÂàôÁß∞ÔºöK(\\mathbf{x,x\u0026rsquo;}) ÊòØ‰∏Ä‰∏™Ê†∏ÂáΩÊï∞ $$\nÂ∞Ü(ËæìÂÖ•Á©∫Èó¥ÁöÑ)Ê†∑Êú¨‰ª£ÂÖ•Ê†∏ÂáΩÊï∞Âç≥ÂèØÁÆóÂá∫(È´òÁª¥Á©∫Èó¥ÁöÑ)ÂÜÖÁßØÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÈáè\n","date":"2021-11-10T13:36:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/07_%E6%A0%B8%E6%96%B9%E6%B3%95/","title":"watch: ML - ÁôΩÊùø 07 | Kernel Method"},{"content":"ÂêëÈáèÊäïÂΩ±Âà∞Âì™‰∏™ÊñπÂêëÔºåÂ∞±Áî®ËØ•ÊñπÂêëÁöÑÊñπÂêëÂêëÈáèÁöÑËΩ¨ÁΩÆ‰πò‰ª•ÂêëÈáè\nÊää$[-3,4]^T$ ÊäïÂΩ±Âà∞ $[0,1]^T$ ÊñπÂêë‰∏äÔºåÂ∞±ÊòØÔºö$[0, 1] \\begin{bmatrix} -3 \\ 4 \\end{bmatrix} = 4$\nÊää $[-4, 1]^T$ ÊäïÂΩ±Âà∞ $[1,2]^T$ ÊñπÂêë‰∏äÔºåÂÖàÂØπÊäïÂΩ±ÂêëÈáèÁöÑÊ®°ÈïøÂΩí‰∏Ä: $[\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}]^T$Ôºå ÁÑ∂Âêé $[\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}] \\begin{bmatrix} -4 \\ 1 \\end{bmatrix} =\\frac{-2}{\\sqrt{5}}$\n‰∏âÁª¥Á©∫Èó¥Ôºå$\\mathbf x$ Âú®$(\\mathbf x_1, \\mathbf x_2)$ ÁöÑÊäïÂΩ±ÂèòÊç¢Âà∞‰ª• $(\\mathbf e_1, \\mathbf e_2)$ ‰∏∫Âü∫ÔºàÁõ∏‰∫íÂûÇÁõ¥ÔºâÁöÑÂùêÊ†áÁ≥ª‰∏ã„ÄÇÂêëÈáèÊ≤°ÂèòÔºåÂùêÊ†áÂèò‰∫Ü„ÄÇ$\\mathbf x$ ÂàÜÂà´Âú®$\\mathbf{e_1,e_2}$ ÊñπÂêë‰∏äÊäïÂΩ±ÔºåÂæóÂà∞Êñ∞ÂùêÊ†áÔºö\n$$ \\begin{aligned} \\mathbf e_1^T \\mathbf x \u0026amp;= \\left[\\frac{1}{\\sqrt{2}}\\ \\frac{1}{\\sqrt{2}}\\ 0 \\right] \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\\n\\mathbf e_2^T \\mathbf x \u0026amp;= \\left[\\frac{1}{\\sqrt{2}}\\ -\\frac{1}{\\sqrt{2}}\\ 0\\right] \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\end{aligned}\n\\Longrightarrow\n\\underbrace{ \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; 0 \\\\ \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{2}} \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} }_{‰∏âÁª¥Âà∞‰∫åÁª¥ÔºåÈôçÁª¥}\n\\Rightarrow\n\\begin{bmatrix} \\mathbf e_1^T \\ \\mathbf e_2^T \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} $$\nÊäïÂΩ±Áü©Èòµ\n$$ A \\coloneqq [\\mathbf e_1 \\ \\mathbf e_2] = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{2}} \\ 0 \u0026amp; 0 \\end{bmatrix} $$\nÊâÄ‰ª•ÂùêÊ†áÂèòÊç¢ÔºåÁõ∏ÂΩì‰∫éÂ∑¶‰πòÊäïÂΩ±Áü©ÈòµÁöÑËΩ¨ÁΩÆ $A^T$\n$$ \\begin{bmatrix} \\mathbf e_1^T \\ \\mathbf e_2^T \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} = A^T \\begin{bmatrix} 1 \\ 0 \\ 2 \\end{bmatrix} $$\n‰∏ªÊàêÂàÜ‰πãÈó¥ÊòØÁõ∏‰∫íÁã¨Á´ãÁöÑÔºå‰∫íÁõ∏ÂûÇÁõ¥\nÂ¶Ç‰∏äÂõæÔºåÈÄâ $\\mathbf e_1$ ‰∏∫ÊãâÂæóÊúÄÂºÄÁöÑÊñπÂêëÔºåÊòØ‰∏∫‰∫ÜÊñπ‰æøÊâæ $\\mathbf e_2$ ÊñπÂêë‰∏äÁöÑÊúÄÂ§ßÊúÄÂ∞èÂÄº ","date":"2021-11-04T09:55:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/pca/","title":"PCA"},{"content":"Maximum Variance Unfolding\nÂÆÉÊâæÂà∞‰∏Ä‰∏™Â•ΩÁöÑÊ†∏ÂáΩÊï∞Ôºå‰Ωø \u0026ldquo;ÊâÄÊúâËΩ¨Êç¢ÂêéÁöÑÊï∞ÊçÆÁÇπ‰πãÈó¥ÁöÑË∑ùÁ¶ª‰πãÂíå \u0026ldquo;Âú® \u0026ldquo;ÈÇªÂüüÂõæ‰∏≠ÁöÑË∑ùÁ¶ª \u0026ldquo;‰∏çÂèòÁöÑÁ∫¶Êùü‰∏ãËææÂà∞ÊúÄÂ§ß„ÄÇÁõ¥ËßÇÂú∞ËØ¥ÔºåÂΩì‰∏Ä‰∏™ÊµÅÂΩ¢Ë¢´ÈÄÇÂΩìÂú∞Â±ïÂºÄÊó∂ÔºåÂêÑÁÇπÁöÑÊñπÂ∑ÆÊòØÊúÄÂ§ßÁöÑ„ÄÇÂõ†Ê≠§ÔºåÁõÆÊ†áÂáΩÊï∞Êó®Âú®Â±ïÂºÄÂèòÊç¢Á©∫Èó¥‰∏≠ÁöÑÊï∞ÊçÆÁÇπÔºàÁî±Ê†∏ÂáΩÊï∞ËØ±ÂØºÔºâÔºåÂêåÊó∂ÔºåÁ∫¶ÊùüÊù°‰ª∂‰øùËØÅÂú®Â±ïÂºÄÊó∂Êª°Ë∂≥Â±ÄÈÉ®Â±ûÊÄß„ÄÇ\nÁªôÂÆö n ‰∏™ÁÇπ $X={x_1, \\cdots, x_n}$ÔºåMVU È¶ñÂÖàÊûÑÂª∫‰∏Ä‰∏™ neighborhood graph GÔºåÂÖ∂‰∏≠ÊØè‰∏™ÁÇπ $x_i$ ÈÉΩÂíåÁ¶ªÂÆÉÊúÄËøëÁöÑ k ‰∏™ÁÇπÊúâËøûÊé•„ÄÇ\nÊâæ‰∏Ä‰∏™ÂõæÂΩ¢ $Y={y1, \\cdots,y_n}$ÔºåÂÖ∂Êª°Ë∂≥Ôºö$| y_i - y_j |^2 = |x_i - x_j |^2,\\ for \\forall (i,j) \\in G$ÔºåÁÑ∂ÂêéÂØπY‰∏≠ÂêÑÁÇπÈó¥ÁöÑË∑ùÁ¶ª‰πãÂíåÂèñÊúÄÂ§ßÔºö\n$$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} \\sum_{ij} | y_i -y_j |^2 \\\n\u0026amp; \\text{subject to}\\ |y_i - y_j |^2 = | x_i - x_j |^2 ,\\ \\text{for } \\forall (i,j) \\in G \\end{aligned} $$\nÂèØ‰ª•ÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™Ê†∏ÂáΩÊï∞($k: X \\times X \\rightarrow R$)ÊâæÂà∞‰∏Ä‰∏™ÂêàÁêÜËÄåÂèØË°åÁöÑYÁöÑËøë‰ºº„ÄÇ\nÂõ†Ê≠§ÈúÄË¶ÅÊâæÂà∞‰∏Ä‰∏™Â•ΩÁöÑÊ†∏ÂáΩÊï∞ÔºåÂú®ÂèòÊç¢ÂêéÁöÑÁ©∫Èó¥‰∏≠ÔºåÊâÄÊúâÂèòÊç¢ÂêéÁöÑÊï∞ÊçÆ‰πãÈó¥ÁöÑË∑ùÁ¶ª‰πãÂíåÊòØÊúÄÂ§ßÔºåÊª°Ë∂≥3‰∏™ÈôêÂà∂Êù°‰ª∂Ôºö\nÊ†∏ÂáΩÊï∞ÊúâÊïà; ÂèòÊç¢ÂêéÊï∞ÊçÆÁÇπÁöÑÂùáÂÄº‰∏∫Èõ∂;(‰æø‰∫éÂÜôÁõÆÊ†áÂáΩÊï∞) $|\\phi(x_i) - \\phi(x_j) |^2 = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G$ $$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} \\sum_{ij} |\\phi(x_i) - \\phi(x_j) |^2 \\\n\u0026amp; \\text{subject to} \\begin{cases} K \\text{ is positive semidefinite} \u0026amp; \\text{ÊúâÊïà}\\ | \\sum_i \\phi(x_i) |^2 = 0 \u0026amp; \\text{Èõ∂ÂùáÂÄº}\\ |\\phi(x_i) - \\phi(x_j) |^2 = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G \\end{cases} \\end{aligned} $$\nÈ¶ñÂÖàÊîπÂÜôÁ∫¶ÊùüÊù°‰ª∂2Ôºö\n$$ | \\sum_i \\phi(x_i) |^2 = \\left( \\sum_i \\phi(x_i) \\right)^T \\left( \\sum_j \\phi(x_j) \\right) = \\sum_{ij} \\phi(x_i)^T \\phi(x_j) = \\sum_{ij} k_{ij} = 0 $$\nÁ∫¶ÊùüÊù°‰ª∂3ÁöÑÂ∑¶ËæπÈÉ®ÂàÜÔºö\n$$ |\\phi(x_i) - \\phi(x_j) |^2 = \\left( \\phi(x_i) - \\phi(x_j) \\right)^T \\left( \\phi(x_i) - \\phi(x_j) \\right) = k_{ii} + k_{jj} - 2k_{ij} $$\nÊîπÂÜôÁõÆÊ†áÂáΩÊï∞Ôºö\n$$ \\sum_{ij} | \\phi(x_i) - \\phi(x_j) |^2 = \\sum_{ij} (k_{ii} + k_{jj} - 2 k_{ij} ) = \\sum_{ij} k_{ii} + \\sum_{ij} k_{jj} -2\\sum_{ij} k_{ij} = 2n \\sum_i k_{ii} - 2 \\cdot 0 = 2n \\cdot tr(K) $$\nÊúÄÁªàÔºå‰ºòÂåñÈóÆÈ¢òÊòØ‰∏Ä‰∏™ÂçäÊ≠£ÂÆöËßÑÂàíÈóÆÈ¢ò(SDP)Ôºö\n$$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} tr(K) \\\n\u0026amp; \\text{subject to} \\begin{cases} K \\text{ is positive semidefinite} \u0026amp; \\text{ÊúâÊïà}\\ \\sum_{ij} k_{ij} = 0 \u0026amp; \\text{Èõ∂ÂùáÂÄº} \\ k_{ii} + k_{jj} - 2k_{ij} = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G \\end{cases} \\end{aligned} $$\nSDP ÁöÑËß£K ÊòØÁî®‰∫éËæìÂÖ•Kernel PCAÁöÑÊ†∏Áü©Èòµ„ÄÇ\nstat946f10-uwaterloo\nMVU is a variation of Kernel PCA\n","date":"2021-11-03T12:37:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/mvu_notes/","title":"MVU notes"},{"content":"3 component of using Learning Pattern exists Pin down mathematically have data (most important) Learning set up Unkown target function Dataset: Learing algorithm picks $g\\appriox f from hypothesis set H Perceptron Learning Algorithm Feasibility of learning. In order to establish that learning setup and modifier the data, and in order to answer if that learning is feasible. We said that we gonna restart with specific __ and we went an example of bin. I just quickly review that.\nWe suppose to have a bin. In bin there are red marbles and green marbles as we see. And probility that if you pick a red marble is called $\\mu$, and a probability that you pick a\nBIN Model Bin with red and blue marbles; Pick a sample of N marble independently\n$\\mu$: probability to pick a red marbles from the bin (blue: 1-$\\mu$) $\\nu$: fraction of red marbles in the sample In a large sample (large N), ŒΩ is probably close to Œº (within tolerance Œµ)\nHoeffding\u0026rsquo;s Inequality:\n$$ \\begin{aligned} P[Bad] \u0026amp;= P[|ŒΩ-Œº|\u0026gt;Œµ]‚â§ 2e^{-2Œµ^2N}, \u0026amp; \\text{for any Œµ\u0026gt;0} \\ P[Good] \u0026amp;= P[|ŒΩ-Œº|‚â§Œµ]\u0026gt; 1-2e^{-2Œµ^2N},\u0026amp; \\text{for any Œµ\u0026gt;0} \\end{aligned} $$\nN is large, Œµ is large, the P[Bad] becomes small, ŒΩ and Œº are very close to each other.\nN=1000, Œµ=0.05, the probability of ŒΩ-Œµ ‚â§ Œº ‚â§ ŒΩ+Œµ is 0.986 N=1000, Œµ=0.1, the probability of ŒΩ-Œµ ‚â§ Œº ‚â§ ŒΩ+Œµ is 0.999 Œº‚àà[ŒΩ-Œµ, Œº+Œµ], error bar is ¬±Œµ learn from ŒΩ and reach outside the data (Œº). There still is a probability of getting wrong sample, but not often.\nŒº‚âàŒΩ is probably approximately correct (PAC-learning)\n\u0026ldquo;probably\u0026rdquo; : probabilty 2exp(-2 Œµ^2 N) \u0026ldquo;approximately\u0026rdquo; : tolerance Œµ\n","date":"2021-10-22T16:03:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec2_learning-feasible/","title":"watch: AML 02 | Learning Feasibility"},{"content":"SVM SVMÊú¨Ë¥®‰∏äÊòØ‰∏Ä‰∏™Âà§Âà´Ê®°ÂûãÔºåËß£ÂÜ≥‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºå‰∏éÊ¶ÇÁéáÊó†ÂÖ≥ Ë∂ÖÂπ≥Èù¢: ùê∞·µÄùê±+b=0 ÂàÜÁ±ªÊ®°ÂûãÔºöf(ùê∞) = sign(ùê∞·µÄùê±+b) SVMÊúâ3ÂÆùÔºåÈó¥ÈöîÂØπÂÅ∂Ê†∏ÊäÄÂ∑ß ‰∏âÂ§ßÂàÜÁ±ªÁÆóÊ≥ïÔºö Hard-margin SVM ÔºàÁ°¨Èó¥ÈöîÔºâ Soft-margin SVM Kernel SVM 1 Á°¨Èó¥ÈöîSVM-Ê®°ÂûãÂÆö‰πâÔºàÊúÄÂ§ßÈó¥ÈöîÂàÜÁ±ªÂô®Ôºâ Video-P1\nHard-margin SVM ÊúÄÂ§ßÈó¥ÈöîÂàÜÁ±ªÂô®Ôºàmax margin()Ôºâ\nÂá†‰ΩïÊÑè‰πâÔºöÂØπ‰∫éÂπ≥Èù¢‰∏äÁÇπÁöÑÂàÜÁ±ªÈóÆÈ¢òÔºå Â¶ÇÊûúÂàÜÁïåÁ∫ø‰∏§‰æßÁöÑÁÇπÂà∞Á∫øÁöÑË∑ùÁ¶ªÂæàÂ∞èÔºå‰ºöÂØπÂô™Â£∞ÂæàÊïèÊÑüÔºåÂ¶ÇÊûúÊúâ‰∏ÄÁÇπÂô™Â£∞ÂèØËÉΩÂ∞±Ë¢´ÂàÜÂà∞Âè¶‰∏Ä‰æßÂéª‰∫ÜÔºåÊ≥õÂåñËØØÂ∑ÆÂ§ß„ÄÇÊâÄ‰ª•ÊúÄÂ•ΩÁöÑÂàÜÁïåÁ∫øÊª°Ë∂≥ÂØπÊâÄÊúâÁÇπÁöÑË∑ùÁ¶ªÈÉΩË∂≥Â§üÂ§ß„ÄÇ\n‰ªéÊó†ÈôêÊù°ÂèØ‰ª•Ê≠£Á°ÆÂàÜÁ±ªÁöÑÁõ¥Á∫øÔºàË∂ÖÂπ≥Èù¢Ôºâ‰∏≠ÔºåÈÄâÊã©ÊúÄÂ•ΩÁöÑ\nÊï∞Â≠¶Ë°®Ëø∞Ôºö\n‰ΩøÈó¥ÈöîmarginÂáΩÊï∞ÊúÄÂ§ßÔºåÂÆûÁé∞ÂØπN‰∏™pÁª¥ÁöÑÊ†∑Êú¨ÁÇπ ${(x_i, y_i)}_{i=1}^{N}, \\quad x_i \\in \\R^p, y\\in{-1,1}$Ê≠£Á°ÆÂàÜÁ±ª\n$$ \\begin{aligned} \u0026amp; \\rm max , margin(\\mathbf w, b) \\\n\u0026amp; s.t. \\begin{cases} \\mathbf w^T x_i + b \u0026gt; 0, \u0026amp; y_i = 1 \\ \\mathbf w^T x_i + b \u0026lt; 0, \u0026amp; y_i = -1 \\end{cases}\n\\Rightarrow y_i(\\mathbf w^Tx_i +b )\u0026gt;0, \u0026amp; \\text{for $\\forall$ i=1,\u0026hellip;,N}ÔºàÂêåÂè∑Ôºâ \\end{aligned} $$\nÊØè‰∏™ÁÇπÂà∞Áõ¥Á∫øÁöÑË∑ùÁ¶ª: $\\rm distance(w,b,x_i) = \\frac{1}{| w |} |w^T x_i +b|$\n$$ \\begin{aligned} \\rm margin(\\mathbf w,b) \u0026amp; = \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, distance(\\mathbf w,b,x_i) \\ \u0026amp; = \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, \\frac{1}{| \\mathbf w |} |\\mathbf w^T x_i +b|\n\\end{aligned} $$\nÂõ†‰∏∫ÈôêÂà∂Êù°‰ª∂ $y_i(\\mathbf w^T x_i + b)\u0026gt;0$ÔºåËÄå‰∏î$y_i={-1,1}$ÔºåÊâÄ‰ª•ÂèØ‰ª•ÊõøÊç¢‰∏äÂºè‰∏≠ÁöÑÁªùÂØπÂÄº„ÄÇ\nÊâÄ‰ª•ÊúÄÂ§ßÈó¥ÈöîÂàÜÁ±ªÂô®ÂèØÂÜô‰∏∫Ôºö\n$$ \\rm \\underset{\\mathbf w,b}{max} \\frac{1}{| \\mathbf w|}; \\underset{x_i, i=1,\u0026hellip;,N}{min} ; |\\mathbf w^T x_i +b| $$\n(Âõ†‰∏∫minÂè™‰∏é$x_i$ÊúâÂÖ≥ÔºåËÄå$\\frac{1}{| \\mathbf w|}$‰∏é$x$Êó†ÂÖ≥ÔºåÊâÄ‰ª•ÁßªÂà∞‰∫ÜÂâçÈù¢)\nÂØπ‰∫éÈôêÂà∂Êù°‰ª∂ $y_i(\\mathbf w^T x_i + b)\u0026gt;0$ÔºåËØ¥ÊòéÂ≠òÂú®ÊúÄÂ∞èÂÄºÔºö\n$$ \\exist , r\u0026gt;0, \\quad s.t.; \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, y_i (\\mathbf w^T x_i +b) = r $$\nÊâÄ‰ª•ÂèØ‰ª•ÁªßÁª≠ÁÆÄÂåñÔºö\n$$ \\rm \\underset{\\mathbf w,b}{max} \\frac{1}{| \\mathbf w|} r $$\nÂõ†‰∏∫Ë∂ÖÂπ≥Èù¢ÂèØ‰ª•ÂêåÊØî‰æãÁº©ÊîæÔºà$\\mathbf{w^T x} + b = 2\\mathbf{w^T x} + 2b$ÔºâÔºåÊâÄ‰ª•ÂèØ‰ª•ËÆæÁΩÆÊúÄÂ∞èÂÄºr‰∏∫1ÔºåÁõ∏ÂΩì‰∫éÊääË∂ÖÂπ≥Èù¢Áº©ÊîæÂà∞1ÔºåÁ≥ªÊï∞‰πòÂú®ÂâçÈù¢ÁöÑ$\\frac{1}{| \\mathbf w|}$Èáå\nÊâÄ‰ª•ÈóÆÈ¢òÊúÄÁªàËΩ¨Âåñ‰∏∫‰∏Ä‰∏™Âá∏‰ºòÂåñÈóÆÈ¢òÔºö\n$$ \\begin{cases} \\rm \\underset{w,b}{max} \\frac{1}{| \\mathbf w |} \\ s.t. ; \\operatorname{min} y_i (\\mathbf w^T x_i + b) = 1 \\end{cases}\n\\Rightarrow\n\\begin{cases} \\rm \\underset{w,b}{min} \\frac{1}{2} \\mathbf{w^T w} \u0026amp; \\text{(ÁõÆÊ†áÂáΩÊï∞ÊòØ‰∫åÊ¨°)}\\ s.t. ; y_i (\\mathbf w^T x_i + b) ‚â• 1, ; i=1,\u0026hellip;,N \u0026amp; \\text{(N‰∏™Á∫øÊÄßÁ∫¶Êùü)} \\end{cases} $$\nÊ±ÇËß£QPÈóÆÈ¢ò (Âá∏‰∫åÊ¨°ËßÑÂàíQuadratic programmingÈóÆÈ¢ò)\nÂú®Áª¥Â∫¶‰∏çÈ´òÔºåÊ†∑Êú¨‰∏™Êï∞‰∏çÂ§öÁöÑÊÉÖÂÜµ‰∏ãÔºåÊØîËæÉÂ•ΩÊ±ÇËß£„ÄÇ ‰ΩÜÊòØÂØπ‰∫éÁª¥Â∫¶ÂæàÈ´òÔºåÊ†∑Êú¨ÂæàÂ§öÔºåÊàñËÄÖÂØπÊï∞ÊçÆ$x$ÂÅö$\\phi(x)$ÁöÑÁâπÂæÅËΩ¨Êç¢Âà∞‰∫ÜÊñ∞ÁöÑÁâπÂæÅÁ©∫Èó¥$Z$‰∏≠ÔºåËÄåZÁöÑÁª¥Â∫¶ÊØîÂéüÊï∞ÊçÆÁöÑÁª¥Â∫¶È´òÂæàÂ§öÔºåÂ∞±Ê≤°ÂäûÊ≥ïÁõ¥Êé•Ê±ÇËß£ÔºåËÆ°ÁÆóÈáèÂ§™Â§ß„ÄÇ ÂÄüÂä©ÊãâÊ†ºÊúóÊó•‰πòÂ≠êÔºåÂºïÂá∫ÂÆÉÁöÑÂØπÂÅ∂ÈóÆÈ¢òÔºåÊ±ÇËß£Áõ∏ÂØπÂÆπÊòìÁöÑÂØπÂÅ∂ÈóÆÈ¢ò„ÄÇ\nÊääÁõÆÊ†áÂáΩÊï∞ÂÜôÊàêÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ $L$ÔºåÊääÂ∏¶Á∫¶ÊùüÈóÆÈ¢òÂåñÊàêÊó†Á∫¶ÊùüÈóÆÈ¢òÔºöÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Âè™ÊúâÂØπ Œª ÁöÑÁ∫¶ÊùüÔºåÊ≤°ÊúâÂØπ $ùê∞$ Âíå $b$ ÁöÑÁ∫¶Êùü\n$$ L(\\mathbf w,b,Œª)= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} ; \\underbrace{Œª_i}{‚â•0} ; \\underbrace{(1-y_i(ùê∞^T x_i + b))}{‚â§0} $$\nÈóÆÈ¢òËΩ¨Âåñ‰∏∫Ôºö\n$$ \\begin{cases} \\rm \\underset{\\mathbf w,b}{min}\\ \\underset{Œª}{max}\\ L(\\mathbf w,b,Œª)\\ s.t. ; Œª_i ‚â•0 \\end{cases} $$\nÂéüÈóÆÈ¢òÁöÑÂØπÂÅ∂ÈóÆÈ¢òÔºöÂÖàÂØπùê∞,bÊ±ÇLÁöÑÊúÄÂ∞èÂÄºÔºåÂÜçÂØπ Œª Ê±Ç L ÁöÑÊúÄÂ§ßÂÄº\n$$ \\begin{cases} \\rm \\underset{Œª}{max}\\ \\underset{\\mathbf w,b}{min}\\ L(\\mathbf w,b,Œª)\\ s.t. ; Œª_i ‚â•0 \\end{cases} $$\n‰ªéÁõ¥ËßÇ‰∏äÁúãÔºå‰ªéÊúÄÂ§ßÂÄºÈáåÈù¢ÈÄâÁöÑÊúÄÂ∞èÂÄº‰∏ÄÂÆöÊòØÂ§ß‰∫é‰ªéÊúÄÂ∞èÂÄºÈáåÈù¢ÁöÑÊúÄÂ§ßÂÄºÔºàÁúÅÁï•ËØÅÊòéÔºâ,‰πüÂ∞±ÊòØÂº±ÂØπÂÅ∂ÂÖ≥Á≥ªÔºö\n$$ \\rm min, max, L ‚â• max , min, L $$\nÊàë‰ª¨ËøòÊÉ≥Ë¶ÅÂº∫ÂØπÂÅ∂ÂÖ≥Á≥ªÔºåÂç≥ $\\rm min, max, L = max , min, L$„ÄÇÂØπ‰∫éÂá∏‰∫åÊ¨°‰ºòÂåñÈóÆÈ¢òÔºåÂ§©ÁîüÊª°Ë∂≥Âº∫ÂØπÂÅ∂ÂÖ≥Á≥ªÔºàËØÅÊòéÁï•ÔºâÔºåÂéüÈóÆÈ¢òÂíåÂÆÉÁöÑÂØπÂÅ∂ÈóÆÈ¢òÊòØÂêåËß£ÁöÑ„ÄÇÊâÄ‰ª•Áõ¥Êé•Ê±ÇËß£ÂØπÂÅ∂ÈóÆÈ¢òÂç≥ÂèØ„ÄÇ\nÂÖàÂõ∫ÂÆö $Œª$ÔºåÂØπ $ùê∞,b$ Ê±ÇLÁöÑÊúÄÂ∞èÂÄºÔºåÂú®Ê≠§ÂØπÂÅ∂ÈóÆÈ¢ò‰∏≠Ê≤°ÊúâÂØπùê∞,bÁöÑÁ∫¶ÊùüÊù°‰ª∂ÔºåÊâÄ‰ª•ÊòØ‰∏Ä‰∏™Êó†Á∫¶ÊùüÁöÑ‰ºòÂåñÈóÆÈ¢òÔºåÁõ¥Êé•Ê±ÇÂØºÔºö\nÂÖàÂØπbÊ±ÇÂØºÔºö\n$$ \\begin{aligned} \\frac{‚àÇL}{‚àÇb} \u0026amp;= \\frac{‚àÇ}{‚àÇb} \\left[ \\cancel{\\sum_{i=1}^{N} Œª_i} - \\sum_{i=1}^{N} Œª_i y_i(\\cancel{\\mathbf w^T x_i} + b)\\right] \\ \u0026amp;= \\frac{‚àÇ}{‚àÇb} \\left[ -\\sum_{i=1}^{N} Œª_i y_i b \\right] \\ \u0026amp;= -\\sum_{i=1}^{N} Œª_i y_i =0 \\end{aligned} $$\nÊääËøô‰∏™ÁªìÊûúÂ∏¶ÂÖ•$L$\n$$ \\begin{aligned} L(\\mathbf w,b,Œª) \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} ; Œª_i ; (1-y_i(\\mathbf w^T x_i + b))\\ \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} \\ Œª_i - \\sum_{i=1}^{N} ; Œª_i y_i(\\mathbf w^T x_i + b)\\ \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} \\ Œª_i - \\sum_{i=1}^{N} ; Œª_i y_i\\mathbf w^T x_i\n\\cancel{\\sum_{i=1}^{N} ; Œª_i y_i b} \\ \\end{aligned} $$\nÂÜçÂØπ$ùê∞$Ê±ÇÂØºÔºåÂÆö‰πâÂØºÊï∞‰∏∫Á≠â‰∫é0\n$$ \\begin{aligned} \u0026amp; \\frac{‚àÇL}{‚àÇùê∞} = \\frac{1}{2} 2ùê∞ - \\sum_{i=1}^N Œª_i y_i x_i ‚âî 0 \\ \u0026amp; \\Rightarrow ùê∞^* = \\sum_{i=1}^{N} Œª_i y_i x_i \\end{aligned} $$\nÊääËøô‰∏™ÊúÄ‰ºò$ùê∞^*$ÁöÑË°®ËææÂºèÂÜçÂ∏¶ÂÖ•$L$ÔºåÂ∞±ÊòØLÁöÑÊúÄÂ∞èÂÄºÔºö\n$$ \\begin{aligned} L(\\mathbf w,b,Œª) \u0026amp; = \\frac{1}{2} \\underbrace{ \\left(\\sum_{i=1}^{N} Œª_i y_i ùê±_i \\right)^T}{\\sum{i=1}^N Œª_i y_i ùê±_i^T } \\sum_{j=1}^N Œª_j y_j ùê±_j\n\\sum_{i=1}^N \\ Œª_i y_i \\left(\\sum_{j=1}^{N} Œª_j y_j ùê±_j \\right)^T ùê±_i \\sum_{i=1}^N Œª_i \\ \u0026amp; = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N Œª_i Œª_j y_i y_j \\underline{ùê±_i^T ùê±_j} \\sum_{i=1}^N Œª_i y_i \\sum_{j=1}^N Œª_j y_j \\underline{ùê±_j^T ùê±_i} \\sum_{i=1}^N Œª_i \\ \u0026amp; = -\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N Œª_i Œª_j y_i y_j ùê±_i^T ùê±_j \\sum_{i=1}^N Œª_i \\ \\end{aligned} $$ ÊâÄ‰ª•ÂØπÂÅ∂ÈóÆÈ¢òÂèàÂåñ‰∏∫Ôºö\n$$ \\begin{cases} \\underset{Œª}{\\operatorname{min}} ; \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N Œª_i Œª_j y_i y_j ùê±_i^T ùê±_j\n\\sum_{i=1}^N Œª_i \\ s.t. \\ Œª_i ‚â•0; \\ \\sum_{i=1}^N Œª_i y_i =0 \\end{cases} $$ Âõ∫ÂÆö $\\mathbf w,b$, ÂØπ‰∫éÂêëÈáè$Œª$Ê±Ç$L$ÁöÑÊúÄÂ∞èÂÄº\nÊ≠§Êó∂ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞Âè™‰∏é $\\lambda$ ÊúâÂÖ≥\nÊ±ÇÂá∫ $\\lambda$ÔºåÊ†πÊçÆ $\\mathbf w = \\sum_{i=1}^{m} \\lambda_i y^{(i)} x^{(i)}$ Â∞±ÂèØÊ±ÇÂá∫ $\\mathbf w$ÔºåÂÜçÊ†πÊçÆ $b^* = -\\frac{\\rm max\\ x_{i:y^{(i)}=-1} \\mathbf w^{*T} x^{(i)} + min_{i:y^{(i)}} \\mathbf w^{*T} x^{(i)}}{2}$ÔºåÂ∞±ÂèØÊ±ÇÂá∫ $b$ÔºåÊúÄÁªàÂæóÂà∞ÂàÜÁ¶ªË∂ÖÂπ≥Èù¢ÂíåÂàÜÁ±ªÂÜ≥Á≠ñÂáΩÊï∞„ÄÇ\nKKTÊù°‰ª∂ ÂéüÈóÆÈ¢ò‰∏éÂØπÂÅ∂ÈóÆÈ¢òÂÖ∑ÊúâÂº∫ÂØπÂÅ∂ÂÖ≥Á≥ªÁöÑÂÖÖË¶ÅÊù°‰ª∂Â∞±ÊòØÊª°Ë∂≥KKTÊù°‰ª∂\n$$ \\begin{cases} \\frac{‚àÇL}{‚àÇùê∞} =0,\\frac{‚àÇL}{‚àÇb} =0,\\frac{‚àÇL}{‚àÇŒª} =0 \u0026amp;\\text{ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÂØπwÔºåÂØπbÔºåÂØπŒª Ê±ÇÂÅèÂØºÈÉΩÁ≠â‰∫é0}\\ Œª_i (1-y_i(ùê∞^T x_i + b)) = 0 \u0026amp; \\text{ÊùæÂºõ‰∫íË°•Êù°‰ª∂slackness complementary} \\ Œª_i ‚â• 0 \\ 1-y_i(ùê∞^T x_i + b)‚â§0 \\end{cases} $$\nÁî±Ëøô‰∫õÊù°‰ª∂Â∞±ÂèØ‰ª•Ê±ÇÂá∫ÊúÄ‰ºòËß£ÁöÑ$w^$Âíå$b^$\nÁî± ‚àÇL/‚àÇùê∞ ÂèØÊ±ÇÂá∫ùê∞‚àó=‚àë·µ¢‚Çå‚ÇÅ·¥∫ Œª·µ¢ y·µ¢ x·µ¢\n4 ËΩØÈó¥ÈöîSVM-Ê®°ÂûãÂÆö‰πâ Video-P4\nSoft-margin SVM ÂÖÅËÆ∏‰∏ÄÁÇπÁÇπÈîôËØØ loss\n$$ min \\frac{1}{2} w^T w + loss $$\nlossÂáΩÊï∞:\nËøùÂèçÁ∫¶ÊùüÊù°‰ª∂ÁöÑÁÇπÁöÑ‰∏™Êï∞Ôºö$loss = \\sum_{i=1}^N I \\left{ \\underbrace{y_i (w^T x_i + b)\u0026lt;1}_{ÂÖ≥‰∫éw‰∏çËøûÁª≠} \\right}$ ÔºàIÊòØÊåáÁ§∫ÂáΩÊï∞Ôºâ\n‰∏çËøûÁª≠ÊÄßÔºö ‰ª§$z=y(\\mathbf w^T \\mathbf x+b)$ÔºåÂàô $loss_{0/1} = \\begin{cases} 0, \u0026amp;\\text{z\u0026lt;1} \\\\ 0, \u0026amp;otherwise \\end{cases}$\nÂú®z=1Â§ÑÊúâË∑≥Ë∑ÉÔºå‰∏çËøûÁª≠ÂØºËá¥Ê±ÇÂØºÊúâÈóÆÈ¢ò„ÄÇ\nÁî®Ë∑ùÁ¶ª\nË∂ÖÂπ≥Èù¢ ËÉΩÊää n Áª¥Ê¨ßÂºèÁ©∫Èó¥ÂàÜÊàê‰∏§ÈÉ®ÂàÜÁöÑ n-1 Áª¥Â≠êÁ©∫Èó¥„ÄÇ\nn Áª¥Á©∫Èó¥ $\\R^n$ ÁöÑË∂ÖÂπ≥Èù¢ÊòØÁî±ÊñπÁ®ãÔºö$ùê∞^T ùê± + b = 0$ ÂÆö‰πâÁöÑÂ≠êÈõÜ„ÄÇÔºàùê∞ Âíå ùê± ÈÉΩÊòØnÁª¥ÂêëÈáèÔºâ\nÊ≥ïÂêëÈáè‰∏éË∂ÖÂπ≥Èù¢ÂÜÖ‰ªª‰∏ÄÂêëÈáèÂûÇÁõ¥„ÄÇÂÅáËÆæÂú®‰∏âÁª¥Á©∫Èó¥‰∏≠ÔºåÊ∞¥Âπ≥Èù¢ÂÜÖÁöÑ‰∏Ä‰∏™ÂêëÈáè $ùê±-ùê±\u0026rsquo;$ ‰∏éÊ≥ïÂêëÈáè $\\mathbf w$ ÂûÇÁõ¥ÔºåÂ¶Ç‰∏ãÂõæ(Ê∫êËá™Â¶Ç‰ΩïÁêÜËß£Ë∂ÖÂπ≥Èù¢Ôºü)Ôºö\nÊª°Ë∂≥Ôºö\n$$ \\begin{aligned} (ùê±-ùê±\u0026rsquo;)\\mathbf w \u0026amp;= 0 \\ (x_1 - x_1\u0026rsquo;, x_2 - x_2\u0026rsquo;, x_3 - x_3\u0026rsquo;) \\cdot (w_1, w_2, w_3) \u0026amp;= 0 \\ x_1 w_1 + x_2 w_2 + x_3 w_3 \u0026amp;= w_1 x_1\u0026rsquo; + w_2 x_2\u0026rsquo; + w_3 x_3\u0026rsquo; \\ \\mathbf w^T \\mathbf x \u0026amp;= \\mathbf w^T \\mathbf x' \\end{aligned} $$\nÁî±‰∫é $\\mathbf w^T \\mathbf x\u0026rsquo;$ ÊòØÂ∏∏Êï∞È°πÔºå$-\\mathbf w^T \\mathbf x\u0026rsquo; ‚âî b$Ôºå ÊâÄ‰ª•Ë∂ÖÂπ≥Èù¢ÁöÑÂÖ¨ÂºèÂèØÂÜô‰∏∫Ôºö\n$$ \\mathbf w^T \\mathbf x + b = 0 $$\nÁÇπÂà∞Ë∂ÖÂπ≥Èù¢Ë∑ùÁ¶ª ÁÇπÂà∞Ë∂ÖÂπ≥Èù¢ÁöÑÂáΩÊï∞Ë∑ùÁ¶ªÔºåÈô§‰ª•Ê≥ïÂêëÈáèÁöÑËåÉÊï∞\nÊ±ÇÂπ≥Èù¢Â§ñ‰∏ÄÁÇπ $\\mathbf x$ Âà∞Âπ≥Èù¢ÁöÑË∑ùÁ¶ª d„ÄÇ\nÊ†πÊçÆ‰∏âËßíÂáΩÊï∞Ôºö$cos \\theta = \\frac{d}{| \\mathbf x-\\mathbf x\u0026rsquo; |}$ (Á©∫Èó¥‰∏≠‰∏ÄÁÇπÂêëË∂ÖÂπ≥Èù¢‰ΩúÂûÇÁ∫øÔºå$\\theta$Âè™ËÉΩÊòØÈîêËßíÔºå‰∏çÂøÖÊãÖÂøÉÊ≠£Ë¥ü)\n$\\mathbf x-\\mathbf x\u0026rsquo;$ ‰∏é $\\mathbf w$ ÁöÑÂÜÖÁßØ‰∏∫Ôºö $|(\\mathbf x-\\mathbf x\u0026rsquo;) \\cdot \\mathbf w | = |\\mathbf x-\\mathbf x\u0026rsquo; | \\cdot | \\mathbf w | \\cdot cos \\theta$ ÔºàÂõ†‰∏∫Ê≥ïÂêëÈáèÂèØËÉΩÂèçÂêëÔºåÊâÄ‰ª•ÁªôÁ≠âÂºèÂ∑¶ËæπÂä†‰∏äÁªùÂØπÂÄºÔºâ\nËÅîÁ´ãÂèØÂæóÔºö\n$$ d = \\dfrac{|(ùê± - ùê±\u0026rsquo;) ùê∞ |}{| ùê∞ |} = \\dfrac{|ùê∞ùê± - ùê∞ùê±\u0026rsquo;|}{| ùê∞ |} $$\nÂõ†‰∏∫ $ùê±\u0026rsquo;$Âú®Ë∂ÖÂπ≥Èù¢ÂÜÖÔºå$ùê∞ùê±\u0026rsquo; = -b$Ôºå‰∫éÊòØÊúÄÂêéÂæóÂà∞ÁöÑ‰ªªÊÑèÁÇπÂà∞Ë∂ÖÂπ≥Èù¢ÁöÑË∑ùÁ¶ªÂÖ¨ÂºèÔºö\n$$ d = \\frac{|ùê∞ùê±+b|}{| ùê∞ |} $$\nÂá†‰ΩïË∑ùÁ¶ª‰∏éÂáΩÊï∞Ë∑ùÁ¶ª Âá†‰ΩïË∑ùÁ¶ªÔºöÁÇπÂà∞Áõ¥Á∫øÔºàË∂ÖÂπ≥Èù¢ÔºâË∑ùÁ¶ª ÂáΩÊï∞Ë∑ùÁ¶ªÔºö$Œîy$ÔºåÁõ¥Á∫øÔºàË∂ÖÂπ≥Èù¢Ôºâ‰∏äÁöÑÁÇπy=0ÔºåÊâÄ‰ª•‰∏çÂú®Áõ¥Á∫ø‰∏äÁöÑÁÇπÂà∞Áõ¥Á∫øÁöÑÂáΩÊï∞Ë∑ùÁ¶ªÂ∞±ÊòØÁÇπÁöÑyÂÄº„ÄÇ ","date":"2021-10-13T18:59:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/06_svm/","title":"watch: ML - ÁôΩÊùø 06 | SVM"},{"content":"P4\n3D Transformations 3D Scale $$ \\mathbf S(s_x, s_y, s_z) = \\begin{pmatrix} s_x \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_y \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_z \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} $$\n3D Translation $$ \\mathbf T(t_x,t_y,t_z) = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; t_x \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; t_y \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; t_z \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} $$\nRotaion around axises ÁªïxËΩ¥ÊóãËΩ¨Ôºö\nxÊñπÂêëÂùêÊ†á‰∏çÂèòÔºåÊâÄ‰ª•ÊóãËΩ¨Áü©ÈòµÂêÑÂàóÂêëÈáèÁöÑxÂàÜÈáèÈÉΩ‰∏çË¥°ÁåÆÔºåÊâÄ‰ª•Á¨¨‰∏ÄË°åÊòØ1 0 0Ôºõ\n‰∏∫‰∫Ü‰øùËØÅÈÄÜÊó∂ÈíàÊóãËΩ¨ÊòØÊ≠£ÂêëÊóãËΩ¨ÔºåÂÖ∂‰Ωô‰∏§ËΩ¥ÁöÑÈ°∫Â∫èÂ¶ÇÂõæÔºö\ny-zÂπ≥Èù¢ÊóãËΩ¨ÔºåÁî®ÂæÖÂÆöÁ≥ªÊï∞Ê≥ïÊàñÂ±ïÂºÄ‰∏âËßíÂáΩÊï∞ÔºåÂèØÂæóÊóãËΩ¨ÂÖ≥Á≥ª‰∏∫Ôºö $\\begin{cases} x\u0026rsquo;=x \\\\ y\u0026rsquo;=ycosŒ∏-zsinŒ∏ \\\\ z\u0026rsquo;=ysinŒ∏+zcosŒ∏ \\end{cases}$Ôºå\nÂàôÊóãËΩ¨Áü©Èòµ‰∏∫ $\\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ z\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix} 1\u0026amp;0\u0026amp;0\\\\ 0 \u0026amp; cosŒ∏ \u0026amp; -sinŒ∏ \\\\0 \u0026amp; sinŒ∏ \u0026amp; cosŒ∏ \\end{pmatrix} \\begin{pmatrix} x\\\\y\\\\z\\end{pmatrix}$Ôºõ\nÂÜôÊàêÈΩêÊ¨°ÂùêÊ†áÔºå‰∏çËÄÉËôëÂπ≥ÁßªÔºåÊâÄ‰ª•‰ªøÂ∞ÑÂèòÊç¢Áü©Èòµ‰∏∫Ôºö\n$$ R_x(Œ±)= \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; cosŒ∏ \u0026amp; -sinŒ∏ \u0026amp; 0 \\\\ 0 \u0026amp; sinŒ∏ \u0026amp; cosŒ∏ \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{pmatrix} $$\nÁªïyËΩ¥ÊóãËΩ¨Ôºö\nyÊñπÂêë‰∏çÂèòÔºåÊâÄ‰ª•Áü©ÈòµÁ¨¨2Ë°å‰∏∫Ôºö0 1 0Ôºõ\nx-zÂπ≥Èù¢ÊóãËΩ¨ÔºåÁî®ÂæÖÂÆöÁ≥ªÊï∞Ê≥ïÔºåÂèØÂæóÊóãËΩ¨ÂÖ≥Á≥ªÔºö $\\begin{pmatrix}z\u0026rsquo;\\\\x\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix}cosŒ∏\u0026amp; -sinŒ∏\\\\sinŒ∏ \u0026amp; cosŒ∏\\end{pmatrix} \\begin{pmatrix} z\\\\x \\end{pmatrix}$ (ÊàñËÄÖÂ±ïÂºÄ‰∏âËßíÂáΩÊï∞ÔºåÂèØÂæóÊóãËΩ¨ÂÖ≥Á≥ª‰∏∫Ôºö $\\begin{cases} x\u0026rsquo;=zsinŒ∏+xcosŒ∏ \\\\ z\u0026rsquo;=zcosŒ∏-xsinŒ∏\\end{cases}$) ÂÜôÊàêÊóãËΩ¨Áü©Èòµ‰∏∫Ôºö\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ z\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix} cosŒ∏ \u0026amp; 0 \u0026amp; sinŒ∏ \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ -sinŒ∏ \u0026amp; 0 \u0026amp; cosŒ∏ \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} $$\n‰∏â‰∏™ËΩ¥Âæ™ÁéØÂØπÁß∞Ôºö$x\\boxed{yzx}y\\boxed{zxy}z\\boxed{xyz}$ÔºåÂç≥ÁªôÂÆö‰ªªÊÑè‰∏§‰∏™ÂèØÂæóÂêéÈù¢‰∏Ä‰∏™ÔºåÊâÄ‰ª•YÊòØÁî±$Z√óX$ÁöÑÂà∞ÁöÑ $\\begin{pmatrix}Z\\\\ X\\\\ Y\\end{pmatrix}$ÔºåËøô‰∏éÈÄöÂ∏∏‰π¶ÂÜô Áü©ÈòµÈ°∫Â∫è$\\begin{pmatrix}X\\\\ Y\\\\ Z\\end{pmatrix}$‰∏çÂêåÔºåÂØºËá¥Á≥ªÊï∞Êòì‰ΩçÔºå$-sinŒ∏$‰∏çÂú®Â∑¶‰∏äËßíÔºåÁúãËµ∑Êù•‰∏éÁªïÂÖ∂‰ªñ‰∏§ËΩ¥ÁöÑÊóãËΩ¨Áü©Èòµ‰∏ç‰∏ÄËá¥„ÄÇ\nÁªïzËΩ¥ÊóãËΩ¨Ôºö\nzÊñπÂêë‰∏çÂèòÔºåÊâÄ‰ª•Áü©ÈòµÁ¨¨3Ë°å‰∏∫Ôºö0 0 1Ôºõ\nx-yÂπ≥Èù¢ÊóãËΩ¨ÔºåÊúâÊóãËΩ¨ÂÖ≥Á≥ª $\\begin{cases}x\u0026rsquo;=rcos(Œ±+Œ∏)\\\\y\u0026rsquo;=rcos(Œ±+Œ∏)\\end{cases}$ ‰ª•Âèä $\\begin{cases}x=rcosŒ±\\\\y=rsinŒ±\\end{cases}$ ÂèØÂæóÔºö$\\begin{cases}x\u0026rsquo;=xcosŒ∏-ysinŒ∏\\\\y\u0026rsquo;=ycosŒ∏+xsinŒ∏\\end{cases}$ ÂÜôÊàêÊóãËΩ¨Áü©Èòµ‰∏∫Ôºö\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\z\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix} cosŒ∏ \u0026amp; -sinŒ∏ \u0026amp; 0 \\\\ sinŒ∏ \u0026amp; cosŒ∏ \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\z \\end{pmatrix} $$\n‰ªªÊÑè‰∏âÁª¥ÊóãËΩ¨ Ê∏≤Êüì Áªô‰∏âÁª¥Áâ©‰ΩìÊãçÁÖßÁâá ÊëÜÂ•ΩÊ®°Âûã Model transformation ÊëÜÂ•ΩÁÖßÁõ∏Êú∫ View transformation ÊäïÂΩ±Âà∞Áõ∏Áâá Projection transformation View Transformation ÈÄöËøáÊóãËΩ¨ÂíåÂπ≥ÁßªÔºåÊää‰∏ñÁïåÂùêÊ†áÁ≥ª‰∏≠ÁöÑÁõ∏Êú∫Ë∞ÉÊï¥Âà∞Ê†áÂáÜÂßøÊÄÅ\nÁõ∏Êú∫ÁöÑÂßøÊÄÅÂåÖÊã¨Ôºö‰ΩçÁΩÆ„ÄÅÊúùÂêëÂíåÊóãËΩ¨\nÊ†áÂáÜÂßøÊÄÅÔºö\nÂÖâÂøÉÂú®ÂéüÁÇπÔºõ ÊúùÁùÄ-zÊñπÂêëÁúãÔºõ Âêë‰∏äÊòØyËΩ¥ ‰ªéÊ†áÂáÜÂßøÊÄÅÂèòÊç¢Âà∞ÂΩìÂâçÂßøÊÄÅÔºåÂÜçÂèñÈÄÜÔºåÂ∞±ÊòØËßÜÂõæÂèòÊç¢\nÊ≠£‰∫§ÊäïÂΩ± Âπ≥Ë°åÂÖâ ÊääÁ©∫Èó¥‰∏≠ÁöÑÈïøÊñπ‰ΩìÊò†Â∞ÑÂà∞‰∏Ä‰∏™ËæπÈïø‰∏∫2ÁöÑÁ´ãÊñπ‰ΩìÔºàÂ∑¶Âè≥‰∏ã‰∏äËøúËøëÁöÑËæπÁïåÈÉΩÊòØ1Ôºâ ‰∏≠ÂøÉÂπ≥ÁßªÂà∞ÂéüÁÇπ: $\\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; -\\frac{r+l}{2} \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; -\\frac{t+b}{2} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -\\frac{n+f}{2} \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}$ Áº©ÊîæÂà∞2: $\\begin{pmatrix}\\frac{2}{r-l} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2}{t-b} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{2}{n-f} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\end{pmatrix}$ ÈÄèËßÜÊäïÂΩ± ËßÜÈî• ËøëÂ§ßËøúÂ∞è ÊääÊ£±Âè∞Êå§ÂéãÊàêÈïøÊñπ‰ΩìÔºåÈïøÊñπ‰ΩìÂÜçÂÅöÊ≠£‰∫§ÊäïÂΩ± ËøúÂπ≥Èù¢Áº©ÊîæÂà∞‰∏éËøëÂπ≥Èù¢Áõ∏ÂêåÂ§ßÂ∞è\nÊ†πÊçÆÁõ∏‰ººÔºåÁ°ÆÂÆö‰∏çÂêåÊ∑±Â∫¶ÁöÑÁº©ÊîæÁ≥ªÊï∞\n$$ \\begin{cases} y\u0026rsquo;=\\frac{n}{z}y \\\\ x\u0026rsquo;=\\frac{n}{z}x \\\\ z\u0026rsquo;=unknow \\end{cases} $$\nËøúÂπ≥Èù¢Áº©Êîæ‰πãÂêéÂêÑÁÇπÂùêÊ†áÔºö\n$$ \\begin{pmatrix} \\frac{nx}{z} \\\\ \\frac{ny}{z} \\\\ unknown \\\\ 1 \\end{pmatrix} \\overset{‰πò‰ª•Ê∑±Â∫¶z}{==} \\begin{pmatrix} nx \\\\ ny \\\\ unkown \\\\ z \\end{pmatrix} $$\nÁ°ÆÂÆö$M_{persp‚Üíortho}$\n$$ M_{persp‚Üíortho}^{(4√ó4)} \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}= \\begin{pmatrix} nx \\\\ ny \\\\ unknown \\\\ z \\end{pmatrix} $$\n$$ M_{persp‚Üíortho}^{(4√ó4)} = \\begin{pmatrix} n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; n \u0026amp; 0 \u0026amp; 0 \\\\ ? \u0026amp; ? \u0026amp; ? \u0026amp; ? \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ \\end{pmatrix} $$\nËøëÂπ≥Èù¢‰∏äÁöÑÁÇπ‰ΩúÁî®‰πãÂêé‰∏çÂèòÔºö\n$$ \\begin{pmatrix} n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; n \u0026amp; 0 \u0026amp; 0 \\\\ ? \u0026amp; ? \u0026amp; ? \u0026amp; ? \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ n \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} x \\\\ y \\\\ n \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} nx \\\\ ny \\\\ n^2 \\\\ n \\end{pmatrix} $$\nÂõ†‰∏∫$n^2$‰∏éx,yÊó†ÂÖ≥ÔºåÊâÄ‰ª•Á¨¨‰∏âË°å‰∏∫Ôºö(0 0 A B), $An+B=n^2$\nËøúÂπ≥Èù¢ÁöÑ‰∏≠ÂøÉÁÇπ‰πüÊ≤°ÂèòÔºåÊª°Ë∂≥Ôºö\n$$ \\begin{pmatrix} n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; n \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; A \u0026amp; B \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ f \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ f \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ f^2 \\\\ f \\end{pmatrix} $$\nÊâÄ‰ª•$Af+B=f^2$\nËÅîÁ´ãÂèØËß£ÂæóA,B:\n$$ \\begin{cases} An+B=n^2 \\\\ Af+B=f^2 \\end{cases} ‚áí \\begin{cases} A=n+f \\\\ B=-nf \\end{cases} $$\nÈÄèËßÜÊäïÂΩ±Áü©ÈòµÔºö$M_{persp} = M_{ortho} M_{persp‚Üíortho}$\n","date":"2021-09-21T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/04-transform-cont/","title":"watch: CG - Èó´‰ª§Áê™ 04 | Transformation Cont."},{"content":"P5\nÂÖâÊ†ÖÂåñRasterize ÊääÁâ©‰ΩìÁîªÂú®Â±èÂπï‰∏ä Êää$[-1,1]^3$Ê†áÂáÜÁ´ãÊñπ‰Ωì(canonical cube)ÊäïÂΩ±Âà∞Â±èÂπï‰∏ä viewing frustum ÊëÑÂÉèÊú∫ÁöÑËßÜÈáé\nÂõæ4 ËßÜ‰Ωì ‰∏âÁª¥‰∏ñÁïå‰∏≠Â±èÂπï‰∏äÂèØËßÅÁöÑÂå∫Âüü\n‰ΩúÁî®ÔºöÁ°ÆÂÆöÂì™‰∫õÁâ©‰Ωì‰ºöË¢´Â±èÂπïÊòæÁ§∫\nËßÜ‰ΩìÂèØÁî®ËøëÂπ≥Èù¢ÁöÑÂÆΩÈ´òÊØî($\\frac{width}{height}$)ÂíåÂûÇÁõ¥ÂèØËßÜËßíÂ∫¶ÂÆö‰πâ\nÂ±èÂπï ‰∫åÁª¥Êï∞ÁªÑ ÊØè‰∏™ÂÖÉÁ¥†ÊòØ‰∏Ä‰∏™ÂÉèÁ¥† ","date":"2021-09-20T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/05-rasterization-1triangles/","title":"watch: CG - Èó´‰ª§Áê™ 05 | Rasterization-1(Triangles)"},{"content":"P3 Transformation\nÊóãËΩ¨Áü©Èòµ ‰πò‰ª•‰∏Ä‰∏™ÂêëÈáèÊó∂ÔºåÊîπÂèòÂêëÈáèÊñπÂêëÁöÑÁü©Èòµ\nË°®Á§∫‰∫Ü‰∏çÂêåÁª¥Â∫¶ÂùêÊ†áÁöÑÁ∫øÊÄßÂèòÊç¢\n‰ΩúÁî®ÔºöÊääÂêëÈáè(ÈªòËÆ§ÁªïÂéüÁÇπ„ÄÅÈÄÜÊó∂Èíà)ÁöÑÊóãËΩ¨Áî®Áü©Èòµ‰πòÊ≥ïË°®Á§∫\nÂØπ‰∫é‰∏Ä‰∏™‰∫åÁª¥ÂêëÈáèÔºåÊóãËΩ¨ÂÖ≥Á≥ªÁî®Áü©Èòµ‰πòÊ≥ïÂΩ¢Âºè‰∏∫Ôºö\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y' \\end{pmatrix} = M_{2√ó2} \\begin{pmatrix} x \\\\ y \\end{pmatrix} $$\nÁî®ÂæÖÂÆöÁ≥ªÊï∞Ê≥ïÂèØ‰ª•Á°ÆÂÆöÂêÑÂÖÉÁ¥†\nÂØπ‰∫é(a,0)ÁÇπÔºö\n$$ \\begin{pmatrix} acosŒ∏ \\\\ asinŒ∏ \\end{pmatrix} = \\begin{pmatrix} A \u0026amp; B \\\\ C \u0026amp; D \\end{pmatrix}\n\\begin{pmatrix} a \\\\ 0 \\end{pmatrix} $$\nËß£ÂæóÔºö$A=cosŒ∏, C=sinŒ∏$Ôºõ ÂêåÁêÜ‰ª£ÂÖ•(0,b)ÁÇπÔºåÂèØËß£ÂæóÔºö$B=-sinŒ∏, D=cosŒ∏$ÔºåÊâÄ‰ª•ÊóãËΩ¨Áü©Èòµ‰∏∫Ôºö\n$$ \\begin{pmatrix} cosŒ∏ \u0026amp; -sinŒ∏ \\\\ sinŒ∏ \u0026amp; cosŒ∏ \\end{pmatrix} $$\nÊóãËΩ¨Áü©ÈòµÁöÑÈÄÜ Á≠â‰∫éÂÆÉÁöÑËΩ¨ÁΩÆÔºà$R_Œ∏^{-1}=R_Œ∏^T$Ôºâ\nÂõ†‰∏∫ÊóãËΩ¨Áü©ÈòµÊòØ‰∏Ä‰∏™Ê≠£‰∫§Áü©Èòµ\nÈÄÜÊìç‰ΩúÂ∞±ÊòØÈ°∫Êó∂ÈíàÊóãËΩ¨Áõ∏ÂêåÁöÑËßíÂ∫¶Ôºå‰πüÂ∞±ÊòØÊ≠£ÂêëÊóãËΩ¨$-Œ∏$Ôºå‰ª£ÂÖ•ÂæóÔºö\n$$ \\begin{pmatrix} cosŒ∏ \u0026amp; sinŒ∏ \\\\ -sinŒ∏ \u0026amp; cosŒ∏ \\end{pmatrix} $$\nÂç≥‰∏∫ÊóãËΩ¨Áü©ÈòµÁöÑËΩ¨ÁΩÆ\nÈΩêÊ¨°ÂùêÊ†á ÂêëÈáèÊúÄÂêéÂä†‰∏™0ÔºåÁÇπÊúÄÂêéÂä†‰∏™1\n3D vector: $(x,y,z,0)^T$ 3D point: $(x,y,z,1)^T$\n‰ΩúÁî®ÔºöÂπ≥ÁßªÂèòÊç¢‰πüÂèØÂÜôÊàê‰∏Ä‰∏™Áü©Èòµ\nÂØπ‰∫éÂπ≥ÁßªÂÖ≥Á≥ªÔºö\n$$ \\begin{cases} x\u0026rsquo; = x + t_x \\\\ y\u0026rsquo; = y + t_y \\end{cases} $$\nx,yÊñπÂêëÈÉΩÊ≤°ÊúâÊóãËΩ¨ÔºåÊâÄ‰ª•ÊóãËΩ¨Áü©Èòµ‰∏∫Ôºö\n$$ \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} $$\nÂÜôÊàêÈΩêÊ¨°ÂùêÊ†áÔºö\n$$ \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \\end{pmatrix} $$\nÂè™ÊúâÂπ≥ÁßªÔºåÈôÑÂä†Âà∞ÂêéÈù¢Ôºö\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; t_x \\\\ 0 \u0026amp; 1 \u0026amp; t_y \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 0 \\end{pmatrix} $$\nÂêå‰∏ÄÁÇπÁöÑË°®Á§∫‰∏çÂîØ‰∏ÄÔºö\n$$ \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} kx \\\\ ky \\\\ kz \\\\ k \\end{pmatrix} $$\n‰ªøÂ∞ÑÂèòÊç¢ ÊóãËΩ¨ÂèòÊç¢ÂíåÂπ≥ÁßªÂèòÊç¢ÊãºÊàê‰∏Ä‰∏™Áü©Èòµ ","date":"2021-09-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/03-transformation/","title":"watch: CG - Èó´‰ª§Áê™ 03 | Transformation"},{"content":"ÁîªÁ´ñÁõ¥ÂíåÊ∞¥Âπ≥ÁöÑÂàÜÂâ≤Á∫ø 1 2 3 plt.vlines(x, ymin, ymax) plt.hlines(y, xmin, xmax) ‰æãÔºöplt.vlines(0, 0, 0.5, colors = \u0026quot;r\u0026quot;, linestyles = \u0026quot;dashed\u0026quot;)\nÁ∫µÂùêÊ†áÂàªÂ∫¶ÂèòÊñáÂ≠ó ÂèÇËÄÉMatplotlibÔºöËÆæÁΩÆÂùêÊ†áËΩ¥ËåÉÂõ¥ÔºåÂàªÂ∫¶Ôºå‰ΩçÁΩÆÔºåËá™ÂÆö‰πâÂàªÂ∫¶ÂêçÁß∞ÔºåÊ∑ªÂä†Êï∞ÊçÆÊ†áÁ≠æ\n1 2 3 # ÊääÁ∫µÂùêÊ†á-2Âèò‰∏∫ÊñáÂ≠ó‚Äúreally bad\u0026#34; plt.yticks([-2, -1.8, -1, 1.22, 3],[r\u0026#39;$really\\ bad$\u0026#39;, r\u0026#39;$bad$\u0026#39;, r\u0026#39;$normal$\u0026#39;, r\u0026#39;$good$\u0026#39;, r\u0026#39;$really\\ good$\u0026#39;]) ÂùêÊ†áÊòæÁ§∫ËåÉÂõ¥ 1 2 3 #ËÆæÁΩÆÂùêÊ†áËΩ¥ËåÉÂõ¥ plt.xlim((-5, 5)) plt.ylim((-2, 2)) ÂùêÊ†áËΩ¥ËÆæÁΩÆ 1 2 3 4 5 6 7 8 9 10 11 ax = plt.gca() # ËÆæÁΩÆ‰∏äËæπÂíåÂè≥ËæπÊó†ËæπÊ°Ü ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) # ËÆæÁΩÆxÂùêÊ†áÂàªÂ∫¶Êï∞Â≠óÊàñÂêçÁß∞ÁöÑ‰ΩçÁΩÆ ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) # ËÆæÁΩÆËæπÊ°Ü‰ΩçÁΩÆ ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;, 0)) Â∏¶ÁÆ≠Â§¥ÁöÑx-yÂùêÊ†áÁ≥ª 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import matplotlib.pyplot as plt import numpy as np import math # ÂºïÂÖ•axisartistÂ∑•ÂÖ∑ import mpl_toolkits.axisartist as axisartist # ÂàõÂª∫ÁîªÂ∏É # fig = plt.figure(figsize=(8,8)) fig = plt.figure(dpi = 100) # ‰ΩøÁî®axisartist.Subplot ÊñπÊ≥ïÂàõÂª∫‰∏Ä‰∏™ÁªòÂõæÂå∫ÂØπË±° ax ax = axisartist.Subplot(fig, 111) # Â∞ÜÁªòÂõæÂå∫ÂØπË±°Ê∑ªÂä†Âà∞ÁîªÂ∏É‰∏≠ fig.add_axes(ax) # ËÆæÁΩÆÁªòÂõæÂå∫ÂéüÊù•ÊâÄÊúâÂùêÊ†áËΩ¥ÈöêËóè ax.axis[:].set_visible(False) # Ê∑ªÂä†Êñ∞ÁöÑÂùêÊ†áËΩ¥ ax.axis[\u0026#34;x\u0026#34;] = ax.new_floating_axis(0,0) #Á¨¨‰∏Ä‰∏™0‰ª£Ë°®Ê∞¥Âπ≥Áõ¥Á∫øÔºåÁ¨¨‰∫å‰∏™0‰ª£Ë°®Áõ¥Á∫øÁªèËøá0ÁÇπ ax.axis[\u0026#34;y\u0026#34;] = ax.new_floating_axis(1,0) #1‰ª£Ë°®Á´ñÁõ¥Áõ¥Á∫øÔºå0‰ª£Ë°®Áõ¥Á∫øÁªèËøá0ÁÇπ # Áªô xËΩ¥,yËΩ¥ Âä†‰∏äÁÆ≠Â§¥ ax.axis[\u0026#34;x\u0026#34;].set_axisline_style(\u0026#34;-\u0026gt;\u0026#34;,size =1.0)#Á©∫ÂøÉÁÆ≠Â§¥ ax.axis[\u0026#34;y\u0026#34;].set_axisline_style(\u0026#34;-|\u0026gt;\u0026#34;,size=1.0)#ÂÆûÂøÉÁÆ≠Â§¥ # ËÆæÁΩÆÂàªÂ∫¶ÊòæÁ§∫ÊñπÂêë ax.axis[\u0026#34;x\u0026#34;].set_axis_direction(\u0026#34;top\u0026#34;) ax.axis[\u0026#34;y\u0026#34;].set_axis_direction(\u0026#34;right\u0026#34;) # ÁªòÂà∂Êõ≤Á∫øÔºö ‰∏§ÁÇπËøûÁ∫ø 1 plt.plot([0,6],[0,0.5]) ÁÆ≠Â§¥ 1 plt.arrow(0,0,6,0.5,head_width=0.1,head_length=0.1,overhang=1,ec=\u0026#34;deepskyblue\u0026#34;,linestyle=\u0026#34;:\u0026#34;) Áî®Ê≥®ÈáäÁîªÁÆ≠Â§¥ 1 2 3 ax=plt.gca() ax.annotate(\u0026#34;ÈöêÈÄùÊ≥¢\u0026#34;, xy=(0,0), xytext=(6,0.5), arrowprops=dict(arrowstyle=\u0026#34;\u0026lt;-\u0026#34;, color=\u0026#34;deepskyblue\u0026#34;, linestyle=\u0026#34;:\u0026#34;) ) ÈöêËóèÂùêÊ†áËΩ¥ 1 plt.xticks([]), plt.yticks([]) # ÈöêËóèxÂíåyËΩ¥ ÂùêÊ†áËΩ¥Ê†áÁ≠æ ÂáΩÊï∞ÂéüÂûãÂèäÂèÇÊï∞ matplotlib.pyplot.xlabel(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)\nxlabelÔºöÁ±ªÂûã‰∏∫Â≠óÁ¨¶‰∏≤ÔºåÂç≥Ê†áÁ≠æÁöÑÊñáÊú¨„ÄÇ labelpadÔºöÁ±ªÂûã‰∏∫ÊµÆÁÇπÊï∞ÔºåÈªòËÆ§ÂÄº‰∏∫NoneÔºåÂç≥Ê†áÁ≠æ‰∏éÂùêÊ†áËΩ¥ÁöÑË∑ùÁ¶ª„ÄÇ locÔºöÂèñÂÄºËåÉÂõ¥‰∏∫{‚Äòleft‚Äô, ‚Äòcenter‚Äô, ‚Äòright‚Äô}ÔºåÈªòËÆ§ÂÄº‰∏∫rcParams[‚Äúxaxis.labellocation‚Äù]Ôºà‚Äòcenter‚ÄôÔºâÔºåÂç≥Ê†áÁ≠æÁöÑ‰ΩçÁΩÆ„ÄÇ **kwargsÔºöText ÂØπË±°ÂÖ≥ÈîÆÂ≠óÂ±ûÊÄßÔºåÁî®‰∫éÊéßÂà∂ÊñáÊú¨ÁöÑÂ§ñËßÇÂ±ûÊÄßÔºåÂ¶ÇÂ≠ó‰Ωì„ÄÅÊñáÊú¨È¢úËâ≤Á≠â„ÄÇ ","date":"2021-06-30T10:57:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_matplot/","title":"memo: Python | Matplotlib"},{"content":"Books È¢ëÁéáÊ¥æ ÁªüËÆ°Êú∫Âô®Â≠¶‰π†\n„ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„ÄãÊùéËà™\n12Á´†Ôºö1Áª™ËÆ∫Ôºå12ÊÄªÁªìÔºå‰∏≠Èó¥10‰∏™Â∏∏Áî®ÁÆóÊ≥ïÔºöÊÑüKÊú¥ÂÜ≥ÈÄª ÊîØÊèêEÈöêÊù°\n„ÄäÊú∫Âô®Â≠¶‰π†„Äã‚ÄúË•øÁìú‰π¶‚Äù Âë®ÂøóÂçé\nÂæàÂ§öÂ≠¶‰π†ÊñπÊ≥ïÔºåÂÖ®Èù¢‰ΩÜ‰∏çÊ∑±ÂÖ•ÔºåÂü∫Êú¨Êé®ÂØºÂíåÂéüÁêÜÔºå\nThe Elements of Statistical LearningÔºàESLÔºâ\nË¥ùÂè∂ÊñØÊ¥æ Ê¶ÇÁéáÂõæÊ®°Âûã\nPattern Recognition And Machine Learning (PRML)\n12Á´†ÁÆóÊ≥ïÔºöÂõûÂàÜÁ•ûÊ†∏Êûê ÂõæÊ∑∑ËøëÈááËøû È°∫ÁªÑ\nMachine Learning-A Probabilistic Perspective (MLAPP)\nÁôæÁßëÂÖ®‰π¶ÔºåÂåÖÁΩó‰∏áË±°\nÊ∑±Â∫¶Â≠¶‰π† „ÄäDeepLearning„Äã‚ÄúÂú£Áªè‚Äù Âº†ÂøóÂçéËØë\nVideos shuhuai007 github\n„ÄäÊú∫Âô®Â≠¶‰π†Âü∫Áü≥„Äã‚Äî‚ÄîÂè∞Â§ß ÊûóËΩ©Áî∞\nÂü∫Êú¨ÁêÜËÆ∫ÔºöVC TheroyÔºåÊ≠£ÂàôÂåñÔºåÂü∫Á°ÄÁ∫øÊÄßÊ®°Âûã\n„ÄäÊú∫Âô®Â≠¶‰π†ÊäÄÊ≥ï„Äã‚Äî‚ÄîÂè∞Â§ß ÊûóËΩ©Áî∞\nÂêÑÁßçÊ®°ÂûãÔºöSVM(Â•Ω)ÔºåÂÜ≥Á≠ñÊ†ëÔºåÈöèÊú∫Ê£ÆÊûóÔºåÁ•ûÁªèÁΩëÁªúÔºåÊ∑±Â∫¶Â≠¶‰π†ÔºàÂâçÂêëÁΩëÁªúÔºâ\n„ÄäÊú∫Âô®Â≠¶‰π†ÂØºËÆ∫„Äã‚Äî‚ÄîÂº†ÂøóÂçé\nÈ¢ëÁéáÊ¥æËßíÂ∫¶ÔºåÊé®ÂØºËæÉÂ§ö\n„ÄäÁªüËÆ°Êú∫Âô®Â≠¶‰π†„Äã‚Äî‚ÄîÂº†ÂøóÂçé\n‰∏Ä‰∫õÁªüËÆ°ÁêÜËÆ∫ÔºåË¥ùÂè∂ÊñØËßíÂ∫¶ÔºöÂ¶ÇÊ¶ÇÁéá‰∏çÁ≠âÂºèÔºåÂÅèÊï∞Â≠¶ÔºåÊé®ÂØºËæÉÂ§ö\n„ÄäÊñØÂù¶Á¶èËØæÂ†Ç CS 229„Äã‚Äî‚ÄîÂê¥ÊÅ©Ëææ\nÂ§ßÈáèÊï∞Â≠¶Êé®ÂØºÔºåÂ•ΩÂÉèÊúâ2017Âπ¥ÁöÑÊñ∞ÁâàÊ∑ªÂä†deepLearningÂÜÖÂÆπ\nÊ¶ÇÁéáÊ®°Âûã‰∏ÄÁ≥ªÂàóËßÜÈ¢ë‚Äî‚Äî2015 Âæê‰∫¶Ëææ\nÊ∑±Â∫¶ËæÉÊ∑±ÔºåEMÔºåMCMCÔºåHMMÔºåÊª§Ê≥¢ÁÆóÊ≥ï„ÄÇÂú®github‰∏äÊúânotesÔºöÊ¶ÇÁéáÊ®°ÂûãÔºåDeepLearning\n„ÄäML„Äã(Êú∫Âô®Â≠¶‰π†)‚Äî‚Äî2017 Âè∞Â§ß ÊùéÂÆèÊØÖ\nCNNÔºåRNNÔºåLSTM\n„ÄäMLDS„Äã‚Äî‚Äî2018 Âè∞Â§ß ÊùéÂÆèÊØÖ\nÊ∑±Â∫¶Â≠¶‰π†ÈáåÁöÑ‰ºòÂåñÔºåÊ≠£ÂàôÂåñÔºåÂÆûË∑µÊñπÊ≥ïÔºåNLPÊ®°Âûã\nÊùéÂÆèÊØÖ2020Êú∫Âô®Â≠¶‰π†Ê∑±Â∫¶Â≠¶‰π†ÔºàÈôÑÂÆåÊï¥ËØæ‰ª∂ÂíåÊ∫êÁ†ÅÔºâ[bÁ´ô]\n","date":"2021-06-10T06:52:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/","title":"watch: ML - ÁôΩÊùø 00 | Learning Materials"},{"content":"ÂÖ•Èó®ÔºöC/C++‰∏≠ÁöÑÊÆµÈîôËØØÔºàSegmentation faultÔºâ„ÄêËΩ¨„Äë\nGDBË∞ÉËØï 1 2 gcc -g test.c -o test #ÊääË∞ÉËØï‰ø°ÊÅØÂä†ÂÖ•‰∫åËøõÂà∂‰ª£Á†Å‰∏≠ gdb test #ÂêØÂä®gdbË∞ÉËØï ‰∏Ä„ÄÅÊöÇÂÅú/ÊÅ¢Â§çÁ®ãÂ∫èËøêË°å 1. ÂêØÂä®gdb 1 2 3 gdb \u0026lt;program\u0026gt;\t#ÂΩìÂâçÁõÆÂΩï‰∏ãÁºñËØëÂ•ΩÁöÑ‰∫åËøõÂà∂‰ª£Á†Å gdb \u0026lt;program\u0026gt; core\t#ÂêåÊó∂Ë∞ÉËØïÁ®ãÂ∫èÂíåcoreÊñá‰ª∂Ôºàcore dumpÂêé‰∫ßÁîüÁöÑÊñá‰ª∂Ôºâ gdb \u0026lt;program\u0026gt; \u0026lt;PID\u0026gt;\t#Â¶ÇÊûúÊòØÊúçÂä°Á®ãÂ∫èÔºåÂèØ‰ª•ÊåáÂÆöËøõÁ®ãidÔºågdb‰ºöËá™Âä®attach‰∏äÂéªË∞ÉËØïÂÆÉÔºåprogramÂ∫îÂú®PATHÁéØÂ¢ÉÂèòÈáè‰∏≠ÊêúÂæóÂà∞ 2. ËÆæÁΩÆÊñ≠ÁÇπ Áî®breakÂëΩ‰ª§ËÆæÁΩÆÊñ≠ÁÇπÔºö\nbreak \u0026lt;function\u0026gt; ÔºöÂú®ËøõÂÖ•ÊåáÂÆöÂáΩÊï∞Êó∂ÂÅú‰Ωè\n1 2 break class::function break function(type, type) break \u0026lt;linenum\u0026gt; ÔºöÂú®ÊåáÂÆöË°åÂè∑ÂÅú‰Ωè\nbreak +offset / -offset Ôºö Âú®ÂΩìÂâçË°åÁöÑÂâç / ÂêéÁöÑoffsetË°åÂÅú‰Ωè\nbreak filename:linenum ÔºöÂú®Ê∫êÊñá‰ª∂filenameÁöÑlinenumË°åÂÅú‰Ωè\nbreak filename::function ÔºöÂú®Ê∫êÊñá‰ª∂filenameÁöÑfunctionÂáΩÊï∞ÂÖ•Âè£ÂÅú‰Ωè\nbreak *address ÔºöÂú®Á®ãÂ∫èËøêË°åÁöÑÂÜÖÂ≠òÂú∞ÂùÄÂ§ÑÂÅú‰Ωè\nbreak ÔºöÊ≤°ÊúâÂä†ÂèÇÊï∞Êó∂ÔºåË°®Á§∫Âú®‰∏ã‰∏ÄÊù°Êåá‰ª§Â§ÑÂÅú‰Ωè\nbreak \u0026hellip; if \u0026lt;condition\u0026gt; Ôºö\u0026hellip;ÂèØ‰ª•ÊòØ‰∏äËø∞ÂèÇÊï∞ÔºåconditionË°®Á§∫Êù°‰ª∂ÔºåÂú®Êù°‰ª∂ÊàêÁ´ãÊó∂ÂÅú‰Ωè ÔºàÊù°‰ª∂Êñ≠ÁÇπÔºâ\n1 break if i=100\t#ÂΩìi=100Êó∂ÔºåÂÅú‰ΩèÁ®ãÂ∫è Êü•ÁúãÊñ≠ÁÇπÔºåÂèØ‰ΩøÁî® info ÂëΩ‰ª§Ôºö\ninfo breakpoints [n] Ôºà n Ë°®Á§∫Êñ≠ÁÇπÂè∑Ôºâ info break [n] 3. ËÆæÁΩÆËßÇÂØüÁÇπ watchpoint ‰∏ÄËà¨Áî®Êù•ËßÇÂØüÊüê‰∏™Ë°®ËææÂºèÔºàÂèòÈáè‰πüÊòØ‰∏ÄÁßçË°®ËææÂºèÔºâÁöÑÂÄºÊòØÂê¶ÊúâÂèòÂåñ‰∫ÜÔºåÂ¶ÇÊûúÊúâÂèòÂåñÔºåÈ©¨‰∏äÂÅú‰ΩèÁ®ãÂ∫è„ÄÇÊàë‰ª¨Êúâ‰∏ãÈù¢ÁöÑÂá†ÁßçÊñπÊ≥ïÊù•ËÆæÁΩÆËßÇÂØüÁÇπ:\nwatch \u0026lt;expr\u0026gt; Ôºö‰∏∫Ë°®ËææÂºèÔºàÂèòÈáèÔºâexpr ËÆæÁΩÆ‰∏Ä‰∏™ËßÇÂØüÁÇπ„ÄÇ rwatch \u0026lt;expr\u0026gt; ÔºöÂΩìË°®ËææÂºèÔºàÂèòÈáèÔºâexpr Ë¢´ËØªÊó∂ÔºåÂÅú‰ΩèÁ®ãÂ∫è„ÄÇ awatch \u0026lt;expr\u0026gt; ÔºöÂΩìË°®ËææÂºèÔºàÂèòÈáèÔºâÁöÑÂÄºË¢´ËØªÊàñÂÜôÊó∂ÔºåÂÅú‰ΩèÁ®ãÂ∫è info watchpoints ÔºöÂàóÂá∫ÂΩìÂâçËÆæÁΩÆÁöÑÊâÄÊúâËßÇÂØüÁÇπ 4. ËÆæÁΩÆÊçïÊçâÁÇπ catchpoint Áî®Êù•ÊçïÊçâÁ®ãÂ∫èËøêË°åÊó∂ÁöÑ‰∏Ä‰∫õ‰∫ã‰ª∂„ÄÇÂ¶Ç:ËΩΩÂÖ•ÂÖ±‰∫´Â∫ì(Âä®ÊÄÅÈìæÊé•Â∫ì)ÊàñÊòØ C++ ÁöÑÂºÇÂ∏∏„ÄÇËÆæÁΩÆÊçïÊçâÁÇπÁöÑÊ†ºÂºè‰∏∫:\ncatch \u0026lt;event\u0026gt; ÔºöÂΩìeventÂèëÁîüÊó∂ÔºåÂÅú‰ΩèÁ®ãÂ∫è\n1 2 catch throw\t#‰∏Ä‰∏™C++ÊäõÂá∫ÁöÑÂºÇÂ∏∏ catch catch\t#‰∏Ä‰∏™C++ÊçïÊçâÂà∞ÁöÑÂºÇÂ∏∏ tcatch \u0026lt;event\u0026gt; ÔºöÂè™ËÆæÁΩÆ‰∏ÄÊ¨°ÊçïÊçâÁÇπÔºåÂΩìÁ®ãÂ∫èÂÅú‰ΩèÂêéÔºåÁÇπË¢´Ëá™Âä®Âà†Èô§\n5. Áª¥Êä§Êñ≠ÁÇπ GDB ‰∏≠ÁöÑÊñ≠ÁÇπ‰πüÂ∞±ÊòØ‰∏äËø∞ÁöÑ‰∏âÁ±ª„ÄÇÂú® GDB ‰∏≠,Â¶ÇÊûú‰Ω†ËßâÂæóÂ∑≤ÂÆö‰πâÂ•ΩÁöÑÂÅúÊ≠¢ÁÇπÊ≤°ÊúâÁî®‰∫Ü,‰Ω†ÂèØ‰ª•‰ΩøÁî® delete „ÄÅ clear „ÄÅ disable „ÄÅ enable ËøôÂá†‰∏™ÂëΩ‰ª§Êù•ËøõË°åÁª¥Êä§„ÄÇ\nclear ÔºöÊ∏ÖÈô§ÊâÄÊúâÁöÑÂ∑≤ÂÆö‰πâÁöÑÂÅúÊ≠¢ÁÇπ clear \u0026lt;function\u0026gt; Êàñ clear \u0026lt;filename:function\u0026gt; ÔºöÊ∏ÖÈô§ÊâÄÊúâËÆæÁΩÆÂú®ÂáΩÊï∞‰∏≠ÁöÑÂÅúÊ≠¢ÁÇπ clear \u0026lt;linenum\u0026gt; Êàñ clear \u0026lt;filename:linenum\u0026gt; ÔºöÊ∏ÖÈô§ÊâÄÊúâËÆæÁΩÆÂú®ÊåáÂÆöË°å‰∏äÁöÑÂÅúÊ≠¢ÁÇπ delete [breakpoints] [range\u0026hellip;] ÔºöÂà†Èô§ÊåáÂÆöÁöÑÊñ≠ÁÇπÔºàbreakpointsÊòØÊñ≠ÁÇπÂè∑ÔºåËã•‰∏çÊåáÂÆöÊñ≠ÁÇπÂè∑ÔºåË°®Á§∫Âà†Èô§ÊâÄÊúâÁöÑÊñ≠ÁÇπ„ÄÇrangeË°®Á§∫Êñ≠ÁÇπÂè∑ÁöÑËåÉÂõ¥ÔºàÂ¶ÇÔºö3-7Ôºâ„ÄÇÂÖ∂ÁÆÄÂÜôÂëΩ‰ª§‰∏∫ d disable [breakpoints] [range\u0026hellip;] ÔºöÊñ≠ÁÇπ‰∏ç‰ºöË¢´Âà†Èô§ÔºåÂΩìÂÜçÊ¨°ÈúÄË¶ÅÊó∂ÔºåenableÂç≥ÂèØ„ÄÇÂ¶ÇÊûú‰∏çÊåáÂÆöÊñ≠ÁÇπÔºå‰ºödisableÊâÄÊúâÊñ≠ÁÇπ enable [breakpoints] [range\u0026hellip;] enable [breakpoints] once range\u0026hellip; Ôºö‰ΩøËÉΩÊåáÂÆöÁöÑÊñ≠ÁÇπ‰∏ÄÊ¨°ÔºåÂΩìÁ®ãÂ∫èÂÅú‰ΩèÔºåËØ•Êñ≠ÁÇπÁ´ãÂàªË¢´GDBÁ¶ÅÁî®disable enable [breakpoints] delete range\u0026hellip; Ôºö‰ΩøËÉΩÊåáÂÆöÁöÑÊñ≠ÁÇπ‰∏ÄÊ¨°ÔºåÂΩìÁ®ãÂ∫èÂÅúÊ≠¢ÂêéÔºåËØ•Êñ≠ÁÇπÁ´ãÂàªË¢´Âà†Èô§ 6. Áª¥Êä§ÂÅúÊ≠¢Êù°‰ª∂ ‰∏ÄËà¨Êù•ËØ¥,‰∏∫Êñ≠ÁÇπËÆæÁΩÆ‰∏Ä‰∏™Êù°‰ª∂,Êàë‰ª¨‰ΩøÁî® if ÂÖ≥ÈîÆËØç,ÂêéÈù¢Ë∑üÂÖ∂Êñ≠ÁÇπÊù°‰ª∂„ÄÇÂπ∂‰∏î,Êù°‰ª∂ËÆæÁΩÆÂ•ΩÂêé,Êàë‰ª¨ÂèØ‰ª•Áî® condition ÂëΩ‰ª§Êù•‰øÆÊîπÊñ≠ÁÇπÁöÑÊù°‰ª∂„ÄÇ (Âè™Êúâ break Âíå watch ÂëΩ‰ª§ÊîØÊåÅ if, catch ÁõÆÂâçÊöÇ‰∏çÊîØÊåÅ if )\nconditon \u0026lt;bnum\u0026gt; \u0026lt;expression\u0026gt; Ôºö‰øÆÊîπÊñ≠ÁÇπÂè∑‰∏∫ bnum ÁöÑÂÅúÊ≠¢Êù°‰ª∂‰∏∫ expression condition \u0026lt;bnum\u0026gt; ÔºöÊ∏ÖÈô§Êñ≠ÁÇπÂè∑ bnum ÁöÑÂÅúÊ≠¢Êù°‰ª∂ ignore \u0026lt;bnum\u0026gt; \u0026lt;count\u0026gt; ÔºöÊåáÂÆöÁ®ãÂ∫èËøêË°åÊó∂ÔºåÂøΩÁï•Êñ≠ÁÇπÂè∑‰∏∫ bnum ÁöÑÂÅúÊ≠¢Êù°‰ª∂ count Ê¨° 7. ‰∏∫Êñ≠ÁÇπËÆæÂÆöËøêË°åÂëΩ‰ª§ ÂèØ‰ª•‰ΩøÁî® GDB Êèê‰æõÁöÑ command ÂëΩ‰ª§Êù•ËÆæÁΩÆÂÅúÊ≠¢ÁÇπÁöÑËøêË°åÂëΩ‰ª§„ÄÇ ‰πüÂ∞±ÊòØËØ¥,ÂΩìËøêË°åÁöÑÁ®ãÂ∫èÂú®Ë¢´ÂÅúÊ≠¢‰ΩèÊó∂,Êàë‰ª¨ÂèØ‰ª•ËÆ©ÂÖ∂Ëá™Âä®ËøêË°å‰∏Ä‰∫õÂà´ÁöÑÂëΩ‰ª§,ËøôÂæàÊúâÂà©Ë°åËá™Âä®ÂåñË∞ÉËØï„ÄÇÂØπÂü∫‰∫é GDB ÁöÑËá™Âä®ÂåñË∞ÉËØïÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑÊîØÊåÅ„ÄÇ\ncommands [bnum] \u0026hellip; command-list \u0026hellip; end\n‰∏∫Êñ≠ÁÇπÂè∑ bnum ÂÜô‰∏Ä‰∏™ÂëΩ‰ª§ÂàóË°®ÔºåÂΩìÁ®ãÂ∫èË¢´ËØ•Êñ≠ÁÇπÂÅú‰ΩèÊó∂Ôºågdb ‰ºö‰æùÊ¨°ËøêË°åÂëΩ‰ª§ÂàóË°®‰∏≠ÁöÑÂëΩ‰ª§„ÄÇ‰æãÂ¶ÇÔºö\n1 2 3 4 5 break foo if x\u0026gt;0\t#Êñ≠ÁÇπËÆæÁΩÆÂú®ÂáΩÊï∞foo‰∏≠ÔºåÊñ≠ÁÇπÊù°‰ª∂ÊòØx\u0026gt;0 commands printf \u0026#34;x is %d/n\u0026#34;,x\t#ÊâìÂç∞ x ÁöÑÂÄº continue\t#ÁÑ∂ÂêéÁªßÁª≠ËøêË°åÁ®ãÂ∫è end Â¶ÇÊûúË¶ÅÊ∏ÖÈô§Êñ≠ÁÇπ‰∏äÁöÑÂëΩ‰ª§Â∫èÂàó,ÈÇ£‰πàÂè™Ë¶ÅÁÆÄÂçïÁöÑÊâßË°å‰∏Ä‰∏ã commands ÂëΩ‰ª§,Âπ∂Áõ¥Êé•Âú®Ëæì‰∏™ end Â∞±Ë°å‰∫Ü„ÄÇ\n8. Êñ≠ÁÇπËèúÂçï Âú® C++ ‰∏≠ÔºåÂèØËÉΩ‰ºöÈáçÂ§çÂá∫Áé∞Âêå‰∏Ä‰∏™ÂêçÂ≠óÁöÑÂáΩÊï∞Ëã•Âπ≤Ê¨°ÔºàÂáΩÊï∞ÈáçËΩΩÔºâÔºåÂú®ËøôÁßçÊÉÖÂÜµ‰∏ã, break \u0026lt;function\u0026gt; ‰∏çËÉΩÂëäËØâ GDB Ë¶ÅÂÅúÂú®Âì™‰∏™ÂáΩÊï∞ÁöÑÂÖ•Âè£„ÄÇ ÂΩìÁÑ∂Ôºå‰Ω†ÂèØ‰ª•‰ΩøÁî® break \u0026lt;function(type)\u0026gt; ‰πüÂ∞±ÊòØÊääÂáΩÊï∞ÁöÑÂèÇÊï∞Á±ªÂûãÂëäËØâ GDBÔºå‰ª•ÊåáÂÆö‰∏Ä‰∏™ÂáΩÊï∞„ÄÇ Âê¶ÂàôÁöÑËØùÔºåGDB ‰ºöÁªô‰Ω†ÂàóÂá∫‰∏Ä‰∏™Êñ≠ÁÇπËèúÂçï‰æõ‰Ω†ÈÄâÊã©‰Ω†ÊâÄÈúÄË¶ÅÁöÑÊñ≠ÁÇπ„ÄÇ‰Ω†Âè™Ë¶ÅËæìÂÖ•‰Ω†ËèúÂçïÂàóË°®‰∏≠ÁöÑÁºñÂè∑Â∞±ÂèØ‰ª•‰∫Ü„ÄÇÂ¶Ç:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (gdb) b String::after [0] cancel [1] all [2] file:String.cc; line number:867 [3] file:String.cc; line number:860 [4] file:String.cc; line number:875 [5] file:String.cc; line number:853 [6] file:String.cc; line number:846 [7] file:String.cc; line number:735 \u0026gt; 2 4 6 Breakpoint 1 at 0xb26c: file String.cc, line 867. Breakpoint 2 at 0xb344: file String.cc, line 875. Breakpoint 3 at 0xafcc: file String.cc, line 846. Multiple breakpoints were set. Use the \u0026#34;delete\u0026#34; command to delete unwanted breakpoints. (gdb) ÂèØËßÅÔºåGDB ÂàóÂá∫‰∫ÜÊâÄÊúâ after ÁöÑÈáçËΩΩÂáΩÊï∞,‰Ω†ÂèØ‰ª•ÈÄâ‰∏Ä‰∏ãÂàóË°®ÁºñÂè∑Â∞±Ë°å‰∫Ü„ÄÇ 0 Ë°®Á§∫ÊîæÂºÉËÆæÁΩÆÊñ≠ÁÇπ, 1 Ë°®Á§∫ÊâÄÊúâÂáΩÊï∞ÈÉΩËÆæÁΩÆÊñ≠ÁÇπ„ÄÇ\n9. ÊÅ¢Â§çÁ®ãÂ∫èËøêË°åÂíåÂçïÊ≠•Ë∞ÉËØï ÂΩìÁ®ãÂ∫èË¢´ÂÅú‰Ωè‰∫ÜÔºåÂèØ‰ª•Áî®continueÂëΩ‰ª§ÊÅ¢Â§çÁ®ãÂ∫èÁöÑËøêË°åÁõ¥Âà∞Á®ãÂ∫èÁªìÊùüÔºåÊàñÂà∞‰∏ã‰∏Ä‰∏™Êñ≠ÁÇπÂ§Ñ„ÄÇ‰πüÂèØ‰ª•‰ΩøÁî®step ÊàñËÄÖ next ÂçïÊ≠•ÊâßË°å\ncontinue [ignore-count] ÊàñËÄÖ c [ignore-count] ÊàñËÄÖ fg [ignore-count] ÔºöÊÅ¢Â§çÁ®ãÂ∫èËøêË°åÁõ¥Âà∞ Basic:\n1 2 3 gcc -ggdb3 hello.c #gdb3‰ºöÁîüÊàêÊõ¥‰∏∞ÂØåÁöÑË∞ÉËØï‰ø°ÊÅØÔºåÂèØ‰ª•ÂíågdbÊõ¥Â•ΩÁöÑÂÆåÊàêÂÜÖËÅîÂäüËÉΩ gdb a.out (gdb) start #‰∏ªË¶ÅÁöÑ‰∏¥Êó∂Êñ≠ÁÇπ Output 1 2 3 4 5 Temporary breakpoint 1 at 0x1169: file hello.c, line 3. Starting program: /home/jack/backup/opencvTest/a.out Temporary breakpoint 1, main () at hello.c:3 3 { ÊòæÁ§∫‰ª£Á†ÅÁâáÊÆµÔºö\n1 (gdb) list\t#ÊòæÁ§∫Ê∫ê‰ª£Á†Å Output 1 2 3 4 5 6 7 8 9 10 1 #include\u0026lt;stdio.h\u0026gt; 2 int main(void) 3 { 4 int i = 0; 5 printf(\u0026#34;Hello, world\\n\u0026#34;); 6 printf(\u0026#34;i is %d\\n\u0026#34;, i); 7 i++; 8 printf(\u0026#34;i is now %d\\n\u0026#34;, i); 9 return 0; 10 } Êó†Ê≥ïÊòæÁ§∫‰∏≠Êñá GDB Ref:\nCppCon 2016: Greg Law ‚ÄúGDB - A Lot More Than You Knew\u0026quot;\n\u0026lsquo;Become a GDB Power User\u0026rsquo; - Greg Law [ ACCU 2016 ] (2024-01-19) ‰∏§‰∏™ËßÜÈ¢ëÂ∑Æ‰∏çÂ§öÂèàÊúâÂå∫Âà´ÔºåÁ¨¨‰∏Ä‰∏™ÁîªË¥®Â•ΩÂÜÖÂÆπÂ∞ë‰∏Ä‰∫õÔºåÁ¨¨‰∫å‰∏™ÁîªË¥®Â∑Æ‰ΩÜÂÜÖÂÆπÂ§ö‰∏Ä‰∫õ ÂÜ¨Áú†Ë¥ùÂ∞îÁÜä ÁöÑËØÑËÆ∫\n1. More than you knew Êåâ‰∏ã Ctrl+x+aÔºå‰ªé1979ËøõÂÖ•ÂÖ´ÂçÅÂπ¥‰ª£ÔºÅÁ±ª‰ººÂõæÂΩ¢ÁïåÈù¢ÁöÑTUIÔºåÂÜçÊåâ‰∏ÄÈÅçÂõûÂà∞ÂëΩ‰ª§Ë°å\nctrl + l : Âà∑Êñ∞Â±èÂπï\nctrl + p / ctrl+n : prev / next command\nctrl + x + 2 : Á¨¨2‰∏™Á™óÂè£Ôºåcycle though\nÊàñËÄÖËæìÂÖ•Ôºötui enableÔºåÊàñËÄÖ layout srcÔºåÊàñËÄÖ layout asmÔºàÊòæÁ§∫Ê±áÁºñ‰ª£Á†ÅÔºâÔºåËøõÂÖ• TUI\n2. GDB has Python Full Pyton interpreter with access to standard modules (unless your gdb installaion is messed up!)\nThe gdb python module gives most access to gdb\n1 2 3 (gdb) python gdb.execute()\t#ÊâßË°ågdbÂëΩ‰ª§ (gdb) python gdb.parse_and_eval()\t#to get data from inferior (gdb) python help(\u0026#39;gdb\u0026#39;)\t#to see online help Python Pretty Printers\n1 2 3 4 5 class MyPrinter(object) def __init__(self,val): self.val = val; def to_string(self): return (self.val[\u0026#39;member\u0026#39;]) 1 2 3 4 import gdb.printing pp = gdb.printing.RegexpCollectionPrettyPrinter(\u0026#39;mystruct\u0026#39;) pp.add_printer(\u0026#39;mystruct\u0026#39;, \u0026#39;^mystruct$\u0026#39;, MyPrinter) gdb.printing.register_pretty_printer( gdb.current_objfile(),pp) 3. In-built pretty printers for STL GDB will ( try to ) pretty-print most STL container classes ( std : : vector , std : string , etc ) , e.g.\n1 2 3 4 5 6 10\tvec.push_back(5); (gdb) next 12\treturn 0; (gdb) print vec $6 = std::vector of length 3, capacity 4 = {3, 4, 5} (gdb) Note that this relies on Python pretty printers installed on the target system\nCompiling / linking with a different version of libstdc + + ( e . g . executable built on a different host than the one beingused to debug ) , then pretty printing might give strange results.\nThere are many ( list with info pretty-printers ) , includingstd : string , std : bitset , std : list , std : multimap , std : queue , std : set , std : shared _ ptr ,std : stack , std : tuple , std : unique _ ptr , std : vector , std : weak _ ptr , and iterators.\n4. .gdbinit 5. GDB is built on ptrace and signals GDBÊòØÂª∫Á´ãÂú®ptrace‰πã‰∏äÁöÑ„ÄÇÂΩì‰∏Ä‰∏™Ê≠£Âú®Ë¢´Ë∑üË∏™ÁöÑÁ®ãÂ∫èÊî∂Âà∞‰∏Ä‰∏™‰ø°Âè∑ÔºåÂÆÉ‰ºöÊöÇÂÅúÂπ∂‰∏îtracer‰ºöÈÄöËøáwaitpid Ê≥®ÊÑèÂà∞ÔºåÊâÄ‰ª•ÂΩì the inferiorÊî∂Âà∞‰ø°Âè∑ÔºåÂÆÉÂ∞±‰ºöÂÅúÊ≠¢ÁÑ∂ÂêégbdËé∑ÂæóÊéßÂà∂„ÄÇÈÄöÂ∏∏gdb‰ºöÂõûÂà∞promptÔºå‰ΩÜÊòØÂÖ∑‰Ωì‰ºöÂÅö‰ªÄ‰πàÂèñÂÜ≥‰∫é‰ø°Âè∑ÂíåËÆæÁΩÆ„ÄÇ\n1 (gdb) info signals Êúâ‰∏§‰∏™‰ø°Âè∑ÂæàÁâπÂà´Ôºö\nSIGINT ÂΩìÂú®Êåâ‰∏ã Crtl+c ‰ºö‰∫ßÁîü SIGTRAP ÂΩì the inferior ÈÅáÂà∞Êñ≠ÁÇπÊàñËÄÖÂçïÊ≠•Ë∞ÉËØïÊó∂‰ºö‰∫ßÁîü„ÄÇ(ÊîπÂèò‰ª£Á†ÅÔºå0xCCÊìç‰ΩúÁ†ÅÁîüÊàêÈô∑Èò±) 1 2 3 4 5 6 (gdb) handle SIGINT stop pirnt pass Signal\tStop\tPrint\tPass to program\tDescription SIGINT\tYes\tYes\tYes\tInterrupt (gdb) handle SIGINT stop print nopass Signal\tStop\tPrint\tPass to program\tDescription SIGINT\tYes\tYes\tNo\tInterrupt 6. Breakpoints \u0026amp; watchpionts 1 2 3 4 5 watch foo #stop when foo is modified watch -l foo\t#watch location rwatch foo\t#stop when foo is read watch foo thread 3\t#stop when thread 3 modifies foo watch foo if foo \u0026gt; 10\t#stop when foo is\u0026gt;10 foo ÊòØ‰∏Ä‰∏™Â±ÄÈÉ®ÂèòÈáè\n7. thread apply 1 2 3 thread apply 1-4 print $sp thread apply all backtrace thread apply all backtrace full 8. Dynamic Printf ‰∏çÈúÄÊîπÂèò‰ª£Á†ÅÔºåÁî®printfÁöÑËØùÈúÄË¶ÅËÄÉËôëÂä†Âú®Âì™ÈáåÔºåÁÑ∂ÂêéÈáçÊñ∞ÁºñËØëËøêË°åÔºåÊü•ÁúãËæìÂá∫ÔºåÊ≤°Êü•Âà∞ÁªìÊûúÔºåÂèàË¶ÅÈáçÂ§ç‰∏ÄÈÅç\n9. Calling inferior functions call foo will call foo in your inferior\n10. Catchpoints ÂÉèÊñ≠ÁÇπ\n11. Remote debugging ÈÄöËøáserial/sockets Ë∞ÉËØïËøúÁ®ãÊúçÂä°Âô®„ÄÇ\ngdbserver localhost:2000 ./a.out\n12. Multiprocess Debugging Modern Source video: „ÄêGDBË∞ÉËØïÊïôÁ®ã„ÄëÂ¶Ç‰ΩïËÆæÁΩÆÊù°‰ª∂Êñ≠ÁÇπÔºüÂ¶Ç‰ΩïÂä®ÊÄÅ‰øÆÊîπÂèòÈáèÔºüPythonÂíåC++Ê∑∑ÂêàÊÄé‰πàË∞ÉËØïÔºüÂ¶Ç‰ΩïÈôÑÂä†Âà∞ËøõÁ®ãÔºü- ÂèåÁ¨ôÂ≠ê‰ΩØË∞¨ - bilibili\n(2024-01-19)\nË∞ÉËØï Python Á®ãÂ∫è\nDocs: DebuggingWithGdb\nÁî® gdb Ë∞ÉËØï pythonÔºåËøõÂÖ•‰πãÂêéÊääË¶ÅË∞ÉËØïÁöÑ test.py Êñá‰ª∂‰Ωú‰∏∫ÂèÇÊï∞‰º†ÂÖ•. vid\n1 2 3 4 5 6 7 gdb test.py # This trivial way won\u0026#39;t work. gdb python # debug python r test.py # i.e., shell cmd: `python test.py` # the appended arguments will be put into argv # Or, with `-ex` gdb python -ex \u0026#39;r test.py\u0026#39; ËøôÊ†∑ÔºåGDB Â∞±ÂèØ‰ª•ÊçïËé∑ python ÁöÑÂºÇÂ∏∏„ÄÇ\nTODO: Debugging Python C extensions with GDB - Redhat Developers\nTemporary breakpoint only break once.\n1 tb func ","date":"2021-04-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/gdb_cpp/","title":"memo: C++ | Debug with GDB"},{"content":"(Feature figure from g++ ‚Üí make ‚Üí cmake - ‰∫åÂúàÂ¶πÁöÑÊñáÁ´† - Áü•‰πé)\n„ÄäCMakeÂÆûË∑µ„Äã\n‰∏â„ÄÅcmakeÂ∑•Á®ã Âª∫Á´ãÂ∑•Á®ãÁõÆÂΩï t1\n1 2 3 mkdir t1 cd t1 touch main.c CMakeLists.txt ÁºñËæëÊ∫êÊñá‰ª∂ main.c\n1 2 3 4 5 6 # include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;Hello World from t1 Main!\\n\u0026#34;); return 0; } ÁºñËæëCMakeLists.txt (Âéü‰π¶ÊúÄÂêé‰∏ÄË°åÊúâËØØ)\n1 2 3 4 5 PROJECT(HELLO)\t#ÂÆö‰πâÂ∑•Á®ãÂêçÁß∞ SET(SRC_LIST main.c)\t#ÂÆö‰πâÂèòÈáèSRC_LIST, ÂÄº‰∏∫main.cÊ∫êÊñá‰ª∂ MESSAGE(STATUS \u0026#34;This is BINARY dir\u0026#34; ${HELLO_BINARY_DIR})\t#ÂêëÁªàÁ´ØËæìÂá∫Áî®Êà∑ÂÆö‰πâÁöÑ‰ø°ÊÅØ MESSAGE(STATUS \u0026#34;This is SOURCE dir\u0026#34; ${HELLO_SOURCE_DIR}) ADD_EXECUTABLE(hello ${SRC_LIST})\t#ÁºñËØëÊ∫êÊñá‰ª∂ÁîüÊàêÂèØÊâßË°åÊñá‰ª∂hello PROJECT(projectname [CXX] [C] [JAVA])\nÂÆö‰πâÂ∑•Á®ãÂêçÁß∞projectnameÔºåÂèØÊåáÂÆöÂ∑•Á®ãÊîØÊåÅÁöÑËØ≠Ë®Ä(ÊîØÊåÅÁöÑËØ≠Ë®ÄÂàóË°®ÂèØÁº∫ÁúÅ)ÔºåÈªòËÆ§ÊîØÊåÅÊâÄÊúâËØ≠Ë®Ä„ÄÇËøôÊù°projecctÊåá‰ª§ÈöêÂºèÂú∞ÂÆö‰πâ‰∫Ü2‰∏™cmakeÂèòÈáèÔºö\n1 2 \u0026lt;projectname\u0026gt;_BINARY_DIR\t#Ê≠§‰æã‰∏∫ÔºöHELLO_BINARY_DIR ‰∫åËøõÂà∂Êñá‰ª∂ÁõÆÂΩï \u0026lt;projectname\u0026gt;_SOURCE_DIR\t#Ê∫êÊñá‰ª∂ÁõÆÂΩï Âõ†‰∏∫ÈááÁî®ÁöÑÊòØÂÜÖÈÉ®ÁºñËØëÔºå2‰∏™ÂèòÈáèÁõÆÂâçÊåáÁöÑÈÉΩÊòØÂ∑•Á®ãÊâÄÂú®Ë∑ØÂæÑ /backup/cmake/t1\nÂêåÊó∂ cmake Á≥ªÁªü‰πüÂ∏ÆÊàë‰ª¨È¢ÑÂÆö‰πâ‰∫Ü2ÂèòÈáèÔºö\n1 2 PROJECT_BINARY_DIR PROJECT_SOURCE_DIR ‰ªñ‰ª¨ÁöÑÂÄºÂàÜÂà´Ë∑ü HELLO_BINARY_DIR ‰∏é HELLO_SOURCE_DIR ‰∏ÄËá¥„ÄÇ ‰∏∫‰∫ÜÁªü‰∏ÄËµ∑ËßÅ,Âª∫ËÆÆ‰ª•ÂêéÁõ¥Êé•‰ΩøÁî® PROJECT_BINARY_DIRÔºåPROJECT_SOURCE_DIRÔºåÂç≥‰Ωø‰øÆÊîπ‰∫ÜÂ∑•Á®ãÂêçÁß∞,‰πü‰∏ç‰ºöÂΩ±ÂìçËøô‰∏§‰∏™ÂèòÈáè„ÄÇ Â¶ÇÊûú‰ΩøÁî®‰∫Ü**\u0026lt;projectname\u0026gt;_SOURCE_DIR**Ôºå‰øÆÊîπÂ∑•Á®ãÂêçÁß∞Âêé,ÈúÄË¶ÅÂêåÊó∂‰øÆÊîπËøô‰∫õÂèòÈáè„ÄÇ SET (VAR [VALUE] [CACHE TYPE DOCSTRING [FORCE]])\nset Êåá‰ª§ÂèØ‰ª•Áî®Êù•ÊòæÂºèÁöÑÂÆö‰πâÂèòÈáè„ÄÇ\n1 2 SET(SRC_LIST main.c) SET(SRC_LIST main.c;t1.c;t2.c)\t#Ê∫êÊñá‰ª∂ÂàóË°®‰πüÂèØ‰ª•ÊòØÂ§ö‰∏™Êñá‰ª∂ MESSAG ([SEND_ERROR | STATUS | FATAL_ERROR] \u0026ldquo;message to display\u0026rdquo;\u0026hellip;)\nÂêëÁªàÁ´ØËæìÂá∫Áî®Êà∑ÂÆö‰πâÁöÑ‰ø°ÊÅØÔºåÂåÖÂê´‰∫Ü‰∏âÁßçÁ±ªÂûãÔºö\nSEND_ERROR ‰∫ßÁîüÈîôËØØÔºåÁîüÊàêËøáÁ®ãË¢´Ë∑≥Ëøá STATUS ËæìÂá∫ÂâçÁºÄ‰∏∫ - ÁöÑ‰ø°ÊÅØ FATAL_ERROR Á´ãÂç≥ÁªàÊ≠¢ÊâÄÊúâ cmake ËøáÁ®ã ADD_EXECUTABLE(hello ${SRC_LIST})\nÂÆö‰πâ‰∫ÜËøô‰∏™Â∑•Á®ã‰ºöÁîüÊàê‰∏Ä‰∏™Êñá‰ª∂Âêç‰∏∫ hello ÁöÑÂèØÊâßË°åÊñá‰ª∂ÔºåÁõ∏ÂÖ≥ÁöÑÊ∫êÊñá‰ª∂ÊòØ SRC_LIST‰∏≠ÂÆö‰πâÁöÑÊ∫êÊñá‰ª∂ÂàóË°®„ÄÇÁî® ${ } Êù•ÂºïÁî®ÂèòÈáè\nÂºÄÂßãÊûÑÂª∫\nÂú®Â∑•Á®ãÁõÆÂΩï‰∏ãÔºö\n1 cmake .\t#ÊûÑÂª∫ÂΩìÂâçÁõÆÂΩïÔºåÁîüÊàê‰∫ÜMakefile Ê†πÊçÆMakefileÁºñËØëÊ∫ê‰ª£Á†ÅÔºåËøûÊé•ÔºåÁîüÊàêÁõÆÊ†áÊñá‰ª∂„ÄÅÂèØÊâßË°åÊñá‰ª∂(hello)\n1 2 3 make #ÊàñËÄÖ make VERBOSE=1\t#ÂèØ‰ª•ÁúãÂà∞makeÊûÑÂª∫ÁöÑËØ¶ÁªÜËøáÁ®ã,‰ª•‰æøÊéíÊü•ÈîôËØØ ËøêË°åÂèØÊâßË°åÊñá‰ª∂(hello)\n1 ./hello cmakeÂü∫Êú¨ËØ≠Ê≥ï ÂèòÈáè‰ΩøÁî® ${ } ÊñπÂºèÂèñÂÄºÔºå‰ΩÜÊòØÂú®IFÊéßÂà∂ËØ≠Âè•‰∏≠ÊòØÁõ¥Êé•‰ΩøÁî®ÂèòÈáèÂêç\nÊåá‰ª§(ÂèÇÊï∞1 ÂèÇÊï∞2 \u0026hellip;)\nÂèÇÊï∞Áî®Êã¨Âè∑Êã¨‰ΩèÔºåÂèÇÊï∞‰πãÈó¥Áî®Á©∫Ê†ºÊàñÂàÜÂè∑ÈöîÂºÄÔºö\n1 2 ADD_EXECUTABLE(hello main.c func.c) ADD_EXECUTABLE(hello main.c;func.c) Êåá‰ª§ÊòØÂ§ßÂ∞èÂÜôÊó†ÂÖ≥ÁöÑÔºåÂèÇÊï∞ÂíåÂèòÈáèÊòØÂ§ßÂ∞èÂÜôÊïèÊÑüÁöÑ„ÄÇÔºàÊé®ËçêÂÖ®ÈÉ®Â§ßÂÜôÊåá‰ª§Ôºâ\nÂ∑•Á®ãÂêçHELLO Âíå ÂèØÊâßË°åÊñá‰ª∂Âêçhello ÊòØÊ≤°Êúâ‰ªª‰ΩïÂÖ≥Á≥ªÁöÑ\nÂ¶ÇÊûúÊñá‰ª∂Âêç‰∏≠ÊúâÁ©∫Ê†ºÔºå‰ΩøÁî®ÂèåÂºïÂè∑Êã¨‰ΩèÔºö\n1 SET(SRC_LIST \u0026#34;func.c\u0026#34;) Ê∏ÖÁêÜÂ∑•Á®ãÔºö\nÊ∏ÖÈô§‰∏äÊ¨°ÁöÑmakeÂëΩ‰ª§ÊâÄ‰∫ßÁîüÁöÑÁõÆÊ†á(object)Êñá‰ª∂ÔºàÂêéÁºÄ‰∏∫‚Äú.o‚ÄùÁöÑÊñá‰ª∂ÔºâÂèäÂèØÊâßË°åÊñá‰ª∂„ÄÇ\n1 make clean make disclean ÂØπcmakeÊó†ÊïàÔºåÊâÄ‰ª•ÈúÄË¶ÅÁî®Â§ñÈÉ®ÊûÑÂª∫(out-of-source)ÔºåÊù•‰ΩøÂ∑•Á®ãÁõÆÂΩïÊï¥Ê¥Å„ÄÇ\nÂÆâË£Ö\nÂ∞ÜÁºñËØëÊàêÂäüÁöÑÂèØÊâßË°åÊñá‰ª∂ÂÆâË£ÖÂà∞Á≥ªÁªüÁõÆÂΩï‰∏≠Ôºå‰∏ÄËà¨‰∏∫/usr/local/bin ÁõÆÂΩï‰∏≠\n1 make install ÁîüÊàêÂèëË°åÁâàËΩØ‰ª∂ÂåÖ\nÂ∞ÜÂèØÊâßË°åÊñá‰ª∂ÂèäÁõ∏ÂÖ≥Êñá‰ª∂ÊâìÂåÖÊàê‰∏Ä‰∏™tar.gzÂéãÁº©ÁöÑÊñá‰ª∂Áî®Êù•‰Ωú‰∏∫ÂèëÂ∏ÉËΩØ‰ª∂ÁöÑËΩØ‰ª∂ÂåÖ„ÄÇ\n1 make dist ÂÆÉ‰ºöÂú®ÂΩìÂâçÁõÆÂΩï‰∏ãÁîüÊàê‰∏Ä‰∏™ÂêçÂ≠óÁ±ª‰ºº‚ÄúPACKAGE-VERSION.tar.gz‚ÄùÁöÑÊñá‰ª∂„ÄÇPACKAGEÂíåVERSIONÔºåÊòØÊàë‰ª¨Âú®configure.in‰∏≠ÂÆö‰πâÁöÑAM_INIT_AUTOMAKE(PACKAGE, VERSION)„ÄÇ\nÊ£ÄÊü•ÂèëË°åËΩØ‰ª∂ÂåÖ\nÁîüÊàêÂèëÂ∏ÉËΩØ‰ª∂ÂåÖÂπ∂ÂØπÂÖ∂ËøõË°åÊµãËØïÊ£ÄÊü•Ôºå‰ª•Á°ÆÂÆöÂèëÂ∏ÉÂåÖÁöÑÊ≠£Á°ÆÊÄß„ÄÇ\n1 make distcheck Ëøô‰∏™Êìç‰ΩúÂ∞ÜËá™Âä®ÊääÂéãÁº©ÂåÖÊñá‰ª∂Ëß£ÂºÄÔºåÁÑ∂ÂêéÊâßË°åconfigureÂëΩ‰ª§ÔºåÂπ∂‰∏îÊâßË°åmakeÔºåÊù•Á°ÆËÆ§ÁºñËØë‰∏çÂá∫Áé∞ÈîôËØØÔºåÊúÄÂêéÊèêÁ§∫‰Ω†ËΩØ‰ª∂ÂåÖÂ∑≤ÁªèÂáÜÂ§áÂ•ΩÔºåÂèØ‰ª•ÂèëÂ∏É‰∫Ü„ÄÇ\nÂ§ñÈÉ®ÊûÑÂª∫Ôºö ÁºñËØë‰ºöÁîüÊàê‰∏Ä‰∫õÊó†Ê≥ïËá™Âä®Âà†Èô§ÁöÑ‰∏≠Èó¥Êñá‰ª∂ÔºåÊâÄ‰ª•Âú®Â∑•Á®ãÁõÆÂΩï‰∏ãÂª∫Á´ãbuildÁõÆÂΩïÔºåÁî®‰∫éÂ≠òÊîæ‰∏≠Èó¥Êñá‰ª∂ÔºåÁÑ∂Âêé cmake .. ÂØπ‰∏äÂ±ÇÁõÆÂΩïÁºñËØëÔºåÂú®buildÁõÆÂΩï‰∏≠ÁîüÊàê‰∫ÜmakeÈúÄË¶ÅÁöÑMakefileÂíåÂÖ∂‰ªñÁöÑ‰∏≠Èó¥Êñá‰ª∂„ÄÇËøêË°åmakeÁºñËØëÔºåÂ∞±‰ºöÂú®buildÁõÆÂΩï‰∏ãËé∑ÂæóÁõÆÊ†áÊñá‰ª∂ hello.o\nPROJECT_SOURCE_DIR ‰ªçÊåá‰ª£Â∑•Á®ãÁõÆÂΩïÔºåÂç≥ /backup/cmake/t1 PROJECT_BINARY_DIR ÂàôÊåá‰ª£ÁºñËØëÁõÆÂΩïÔºåÂç≥ /backup/cmake/t1/build Âõõ„ÄÅËßÑËåÉÁöÑÂ∑•Á®ã ËßÑËåÉË¶ÅÊ±Ç ‰∏∫Â∑•Á®ãÊ∑ªÂä†‰∏Ä‰∏™Â≠êÁõÆÂΩï srcÔºå Áî®Êù•ÊîæÁΩÆÂ∑•Á®ãÊ∫ê‰ª£Á†Å Ê∑ªÂä†‰∏Ä‰∏™Â≠êÁõÆÂΩï docÔºåÁî®Êù•ÊîæÁΩÆÂ∑•Á®ãÁöÑÊñáÊ°£ hello.txtÔºõ Âú®Â∑•Á®ãÁõÆÂΩïÊ∑ªÂä†ÊñáÊú¨Êñá‰ª∂ COPYRIGHTÔºåREADMEÔºõ Âú®Â∑•Á®ãÁõÆÂΩïÊ∑ªÂä†‰∏Ä‰∏™ runhello.sh ËÑöÊú¨ÔºåÁî®Êù•Ë∞ÉÁî®ÂèØÊâßË°åÊñá‰ª∂ hello Â∞ÜÊûÑÂª∫ÂêéÁöÑÁõÆÊ†á(object)Êñá‰ª∂ÊîæÂÖ•ÊûÑÂª∫ÁõÆÂΩïÁöÑ bin Â≠êÁõÆÂΩï ÊúÄÁªàÂÆâË£ÖËøô‰∫õÊñá‰ª∂ÔºöÂ∞ÜÂèØÊâßË°åÊñá‰ª∂ hello ‰∏é runhello.sh ÂÆâË£ÖËá≥ /usr/bin, Â∞Üdoc ÁõÆÂΩïÁöÑÂÜÖÂÆπ‰ª•Âèä COPYRIGHT/README ÂÆâË£ÖÂà∞ /usr/share/doc/cmake/t2, ÁºñËØë ÂáÜÂ§áÂ∑•‰Ωú\nÂú® /backup/cmake ÁõÆÂΩï‰∏ãÂª∫Á´ã /t2 ÁõÆÂΩïÔºåÂ∞Ü /t1 Â∑•Á®ãÁöÑ main.c Âíå CMakeLists.txt Êã∑Ë¥ùÂà∞ /t2 ÁõÆÂΩï‰∏ã„ÄÇ\nÊûÑÂª∫Â∑•Á®ã\nÊ∑ªÂä†Â≠êÁõÆÂΩï srcÔºö\n1 2 mkdir src mv main.c src\t#Ê∫ê‰ª£Á†ÅÊîæÂÖ• src ÁõÆÂΩï ËøõÂÖ• /t2/srcÔºåÁºñÂÜô CMakeLists.txt ÔºàÈúÄË¶Å‰∏∫‰ªª‰ΩïÂ≠êÁõÆÂΩïÂª∫Á´ã‰∏Ä‰∏™ CMakeLists.txtÔºâ\n1 ADD_EXECUTABLE(hello main.c)\t#Êäämainc.cÊ∫êÁ†ÅÁºñËØëÊàê‰∏Ä‰∏™Âêç‰∏∫helloÁöÑÂèØÊâßË°åÊñá‰ª∂ ‰øÆÊîπ /t2 Â∑•Á®ãÁõÆÂΩï‰∏ãÁöÑCMakeLists.txt ÔºåÊåáÂÆöÊ∫ê‰ª£Á†ÅÊñá‰ª∂Â§πÂíåÁºñËØëËæìÂá∫(ÂåÖÊã¨‰∏≠Èó¥ÁªìÊûúÔºöÊØè‰∏™Â≠êÊñá‰ª∂Â§π‰∏ãÈÉΩÊúâCMakeLists.txtÔºåÈÉΩ‰ºö‰∫ßÁîüÁºñËØëÁªìÊûú)Êñá‰ª∂Â§π\n1 2 PROJECT(HELLO) ADD_SUBDIRECTORY(src bin)\t#ÊåáÂÆöÊ∫êÁ†ÅÁõÆÂΩï/t2/srcÔºåÊåáÂÆömakeÁºñËØëÁªìÊûúÊîæÂÖ•/build/binÔºå ADD_SUBDIRECTORY(source_dir [binary_dir] [EXCLUDE_FORM_ALL])\nÊ≠§Êåá‰ª§Áî®‰∫éÂêëÂΩìÂâçÂ∑•Á®ãÊ∑ªÂä†Â≠òÊîæÊ∫êÊñá‰ª∂ÁöÑÂ≠êÁõÆÂΩï(source_dir)ÔºåÂπ∂ÂèØ‰ª•ÊåáÂÆöÁºñËØëËæìÂá∫Â≠òÊîæÁöÑ‰ΩçÁΩÆ(binary_dir)„ÄÇ[EXCLUDE_FROM_ALL] ÂèÇÊï∞ÁöÑÂê´‰πâÊòØÂ∞ÜËøô‰∏™ÁõÆÂΩï‰ªéÁºñËØëËøáÁ®ã‰∏≠ÊéíÈô§ÔºåÊØîÂ¶ÇÔºåÂ∑•Á®ãÁöÑexampleÔºåÂèØËÉΩÂ∞±ÈúÄË¶ÅÂ∑•Á®ãÁºñËØëÂÆåÊàêÂêéÔºåÂÜçËøõÂÖ•example ÁõÆÂΩïÂçïÁã¨ËøõË°åÊûÑÂª∫„ÄÇ\nÂ¶ÇÊûú‰∏çÊåáÂÆö bin ÁõÆÂΩïÔºåÁºñËØëÁªìÊûúÔºàÂåÖÊã¨‰∏≠Èó¥ÁªìÊûúÔºâÈÉΩÂ∞ÜÂ≠òÊîæÂú® build/src ÁõÆÂΩï‰∏ãÔºåÊåáÂÆö bin ÁõÆÂΩïÂêéÔºåÁõ∏ÂΩì‰∫éÂú®ÁºñËØëÊó∂Â∞Ü /build/src ÈáçÂëΩÂêç‰∏∫ /bin„ÄÇ\nÊç¢‰∏™Âú∞Êñπ‰øùÂ≠òÂèØÊâßË°åÊñá‰ª∂ÂíåÂ∫ìÊñá‰ª∂\n‰∏çËÆ∫ÊòØ SUBDIRS ËøòÊòØ ADD_SUBDIRECTORY Êåá‰ª§(‰∏çËÆ∫ÊòØÂê¶ÊåáÂÆöÁºñËØëËæìÂá∫ÁõÆÂΩï),Êàë‰ª¨ÈÉΩÂèØ‰ª•ÈÄöËøá SET Êåá‰ª§ÈáçÊñ∞ÂÆö‰πâ EXECUTABLE_OUTPUT_PATHÂíå LIBRARY_OUTPUT_PATH ÂèòÈáèÔºåÊù•ÊåáÂÆöÊúÄÁªàÁöÑÁõÆÊ†á‰∫åËøõÂà∂ÁöÑ‰ΩçÁΩÆÔºàÂç≥ÊúÄÁªàÁîüÊàêÁöÑÂèØÊâßË°åÊñá‰ª∂ hello ÊàñËÄÖÊúÄÁªàÁöÑÂÖ±‰∫´Â∫ì,‰∏çÂåÖÂê´ÁºñËØëÁîüÊàêÁöÑ‰∏≠Èó¥Êñá‰ª∂)\n1 2 SET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\t#Âç≥‰∏∫/build/bin SET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)\t#Âç≥‰∏∫/build/lib Âú®ADD_EXECUTABLEÊàñADD_LIBRARYÂêéÈù¢ÔºåÂÜôËøôÊù°Êåá‰ª§„ÄÇ\nÂª∫Á´ãbuildÔºåËøõÂÖ•buildÁõÆÂΩïÔºåËøõË°åÂ§ñÈÉ®ÁºñËØë\n1 2 3 cd build cmake .. make ÊûÑÂª∫ÂÆåÊàêÂêéÔºå‰Ω†‰ºöÂèëÁé∞ÁîüÊàêÁöÑÂèØÊâßË°åÊñá‰ª∂ hello ‰Ωç‰∫é build/bin ÁõÆÂΩï‰∏≠„ÄÇ\nINSTALLÊåá‰ª§ Êúâ‰∏§ÁßçÂÆâË£ÖÊñπÊ≥ïÔºå‰∏ÄÁßçÊòØ‰ªé‰ª£Á†ÅÁºñËØëÂêéÁõ¥Êé• make install ÂÆâË£ÖÔºå‰∏ÄÁßçÊòØÊâìÂåÖÊó∂ÁöÑÊåáÂÆöÁõÆÂΩïÂÆâË£Ö„ÄÇÂèØ‰ª•ÈÄöËøáÔºö\n1 make install\t#Â∞Ü hello Áõ¥Êé•ÂÆâË£ÖÂà∞ /usr/bin ÁõÆÂΩï ÊàñËÄÖÔºö\n1 make install DESTDIR=/tmp/test\t#ÂÆâË£ÖÂú®/tmp/test/usr/bin ÁõÆÂΩïÔºåÊâìÂåÖÊó∂Ëøô‰∏™ÊñπÂºèÁªèÂ∏∏Ë¢´‰ΩøÁî®„ÄÇ Á®çÂæÆÂ§çÊùÇ‰∏ÄÁÇπÁöÑÊòØËøòÈúÄË¶ÅÂÆö‰πâ PREFIXÔºå‰∏ÄËà¨autotoolsÂ∑•Á®ãÔºå‰ºöËøêË°åËøôÊ†∑ÁöÑÊåá‰ª§Ôºö\n./configure -prefix=/usr ÊàñËÄÖ **./configure --prefix=/usr/local**Êù•ÊåáÂÆö PREFIX„ÄÇ\nÂØπ‰∫écmakeÊù•ËØ¥Ôºå‰ΩøÁî®Ôºö\nINSTALL Êåá‰ª§Ôºö\nÁî®‰∫éÂÆö‰πâÂÆâË£ÖËßÑÂàôÔºåÂÆâË£ÖÁöÑÂÜÖÂÆπÂèØ‰ª•ÂåÖÊã¨‰∫åËøõÂà∂„ÄÅÂä®ÊÄÅÂ∫ì„ÄÅÈùôÊÄÅÂ∫ì‰ª•ÂèäÊñá‰ª∂„ÄÅÁõÆÂΩï„ÄÅËÑöÊú¨Á≠âÔºö\n1 2 3 4 5 6 INSTALL(TARGETS \u0026lt;target\u0026gt;... [...])\t#ÂÆâË£Ö‰∫åËøõÂà∂ INSTALL({FILES | PROGRAMS} \u0026lt;file\u0026gt;... [...]) INSTALL(DIRECTORY \u0026lt;dir\u0026gt;... [...]) INSTALL(SCRIPT \u0026lt;file\u0026gt; [...]) INSTALL(CODE \u0026lt;code\u0026gt; [...]) INSTALL(EXPORT \u0026lt;export-name\u0026gt; [...]) ÊúâÊó∂ÂÄôÔºå‰πü‰ºöÁî®Âà∞‰∏Ä‰∏™ÈùûÂ∏∏ÊúâÁî®ÁöÑÂèòÈáè**CMAKE_INSTALL_PREFIX**ÔºåÁî®‰∫éÊåáÂÆöcmake installÊó∂ÁöÑÁõ∏ÂØπÂú∞ÂùÄÂâçÁºÄ„ÄÇÁî®Ê≥ïÂ¶ÇÔºö\n1 cmake -DCMAKE_INSTALL_PREFIX=/usr .. CMAKE_INSTALL_PREFIXÂèòÈáèÁ±ª‰ºº‰∏éconfigure ËÑöÊú¨ÁöÑ -prefix„ÄÇ\nINSTALL Êåá‰ª§ÁöÑÂêÑÁßçÂÆâË£ÖÁ±ªÂûãÔºö\n1. ÁõÆÊ†áÊñá‰ª∂ÁöÑÂÆâË£ÖÔºö 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INSTALL(TARGETS targets ...\t#ÂêÑÁßçÁõÆÊ†áÊñá‰ª∂ [EXPORT \u0026lt;export-name\u0026gt;] [[ARCHIVE|LIBRARY|RUNTIME|OBJECTS|FRAMEWORK|BUNDLE| PRIVATE_HEADER|PUBLIC_HEADER|RESOURCE] #ÁõÆÊ†áÊñá‰ª∂Á±ªÂûã [DESTINATION \u0026lt;dir\u0026gt;]\t#ÊåáÂÆöÂêÑÊñá‰ª∂ÁöÑÂÆâË£ÖÁõÆÂΩï\u0026lt;dir\u0026gt; [PERMISSIONS permissions...]\t#Êñá‰ª∂ÁöÑÊùÉÈôê [CONFIGURATIONS [Debug|Release|...]]\t#ÊåáÂÆöÂÆâË£ÖËßÑÂàôÈÄÇÁî®ÁöÑÊûÑÂª∫ÈÖçÁΩÆÂàóË°®(DEBUGÊàñRELEASEÁ≠â) [COMPONENT \u0026lt;component\u0026gt;] [NAMELINK_COMPONENT \u0026lt;component\u0026gt;] [OPTIONAL] #Â¶ÇÊûúË¶ÅÂÆâË£ÖÁöÑÊñá‰ª∂‰∏çÂ≠òÂú®ÔºåÂàôÊåáÂÆö‰∏çÊòØÈîôËØØ„ÄÇ [EXCLUDE_FROM_ALL]\t#ÊåáÂÆöËØ•Êñá‰ª∂‰ªéÂÆåÊï¥ÂÆâË£Ö‰∏≠ÊéíÈô§Ôºå‰ªÖ‰Ωú‰∏∫ÁâπÂÆö‰∫éÁªÑ‰ª∂ÁöÑÂÆâË£ÖÁöÑ‰∏ÄÈÉ®ÂàÜËøõË°åÂÆâË£ÖÔºõ [NAMELINK_ONLY|NAMELINK_SKIP] ] [...] [INCLUDES DESTINATION [\u0026lt;dir\u0026gt; ...]] ) ÂèÇÊï∞‰∏≠ÁöÑTARGETÂèØ‰ª•ÊòØÂæàÂ§öÁßçÁõÆÊ†áÊñá‰ª∂ÔºåÊúÄÂ∏∏ËßÅÁöÑÊòØÈÄöËøáADD_EXECUTABLEÊàñËÄÖADD_LIBRARYÂÆö‰πâÁöÑÁõÆÊ†áÊñá‰ª∂ÔºåÂç≥ÂèØÊâßË°å‰∫åËøõÂà∂„ÄÅÂä®ÊÄÅÂ∫ì„ÄÅÈùôÊÄÅÂ∫ìÔºö‰ª•‰∏ãÊòØÈªòËÆ§ÁöÑÂÆâË£ÖË∑ØÂæÑ\nÁõÆÊ†áÊñá‰ª∂ ÂÜÖÂÆπ ÂÆâË£ÖÁõÆÂΩïÂèòÈáè ÈªòËÆ§ÂÆâË£ÖÊñá‰ª∂Â§π ARCHIVE ÈùôÊÄÅÂ∫ì ${CMAKE_INSTALL_LIBDIR} lib LIBRARY Âä®ÊÄÅÂ∫ì ${CMAKE_INSTALL_LIBDIR} lib RUNTIME ÂèØÊâßË°å‰∫åËøõÂà∂Êñá‰ª∂ ${CMAKE_INSTALL_BINDIR} bin PUBLIC_HEADER ‰∏éÂ∫ìÂÖ≥ËÅîÁöÑPUBLICÂ§¥Êñá‰ª∂ ${CMAKE_INSTALL_INCLUDEDIR} include PRIVATE_HEADER ‰∏éÂ∫ìÂÖ≥ËÅîÁöÑPRIVATEÂ§¥Êñá‰ª∂ ${CMAKE_INSTALL_INCLUDEDIR} include ‰∏∫‰∫ÜÁ¨¶Âêà‰∏ÄËà¨ÁöÑÈªòËÆ§ÂÆâË£ÖË∑ØÂæÑÔºåÂ¶ÇÊûúËÆæÁΩÆ‰∫ÜDESTINATIONÂèÇÊï∞ÔºåÊé®ËçêÈÖçÁΩÆÂú®ÂÆâË£ÖÁõÆÂΩïÂèòÈáè‰∏ãÁöÑÊñá‰ª∂Â§π„ÄÇ\n‰æãÂ¶ÇÔºö\n1 2 3 4 5 INSTALL(TARGETS myrun mylib mystaticlib RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} ) ‰∏äÈù¢ÁöÑ‰æãÂ≠ê‰ºöÂ∞ÜÔºöÂèØÊâßË°å‰∫åËøõÂà∂myrunÂÆâË£ÖÂà∞${CMAKE_INSTALL_BINDIR}ÁõÆÂΩïÔºåÂä®ÊÄÅÂ∫ìlibmylib.soÂÆâË£ÖÂà∞${CMAKE_INSTALL_LIBDIR}ÁõÆÂΩïÔºåÈùôÊÄÅÂ∫ìlibmystaticlib.aÂÆâË£ÖÂà∞${CMAKE_INSTALL_LIBDIR}ÁõÆÂΩï„ÄÇ\nINSTALLÂëΩ‰ª§ÁöÑÂÖ∂‰ªñ‰∏Ä‰∫õÂèÇÊï∞ÁöÑÂê´‰πâÔºö\nDESTINATIONÔºöÊåáÂÆöÁ£ÅÁõò‰∏äË¶ÅÂÆâË£ÖÊñá‰ª∂ÁöÑÁõÆÂΩïÔºõ PERMISSIONSÔºöÊåáÂÆöÂÆâË£ÖÊñá‰ª∂ÁöÑÊùÉÈôê„ÄÇÊúâÊïàÊùÉÈôêÊòØOWNER_READÔºåOWNER_WRITEÔºåOWNER_EXECUTEÔºåGROUP_READÔºåGROUP_WRITEÔºåGROUP_EXECUTEÔºåWORLD_READÔºåWORLD_WRITEÔºåWORLD_EXECUTEÔºåSETUIDÂíåSETGIDÔºõÔºà11ÁßçÊùÉÈôêÔºâ CONFIGURATIONSÔºöÊåáÂÆöÂÆâË£ÖËßÑÂàôÈÄÇÁî®ÁöÑÊûÑÂª∫ÈÖçÁΩÆÂàóË°®(DEBUGÊàñRELEASEÁ≠â)Ôºõ EXCLUDE_FROM_ALLÔºöÊåáÂÆöËØ•Êñá‰ª∂‰ªéÂÆåÊï¥ÂÆâË£Ö‰∏≠ÊéíÈô§Ôºå‰ªÖ‰Ωú‰∏∫ÁâπÂÆö‰∫éÁªÑ‰ª∂ÁöÑÂÆâË£ÖÁöÑ‰∏ÄÈÉ®ÂàÜËøõË°åÂÆâË£ÖÔºõ OPTIONALÔºöÂ¶ÇÊûúË¶ÅÂÆâË£ÖÁöÑÊñá‰ª∂‰∏çÂ≠òÂú®ÔºåÂàôÊåáÂÆö‰∏çÊòØÈîôËØØ„ÄÇ Ê≥®ÊÑè‰∏Ä‰∏ãCONFIGURATIONSÂèÇÊï∞ÔºåÊ≠§ÈÄâÈ°πÊåáÂÆöÁöÑÂÄº‰ªÖÈÄÇÁî®‰∫éÊ≠§ÈÄâÈ°π‰πãÂêéÂàóÂá∫ÁöÑÈÄâÈ°πÔºö‰æãÂ¶ÇÔºåË¶Å‰∏∫Ë∞ÉËØïÂíåÂèëÂ∏ÉÈÖçÁΩÆËÆæÁΩÆÂçïÁã¨ÁöÑÂÆâË£ÖË∑ØÂæÑÔºåËØ∑ÊâßË°å‰ª•‰∏ãÊìç‰ΩúÔºö\n1 2 3 4 5 6 INSTALL(TARGETS target CONFIGURATIONS Debug RUNTIME DESTINATION Debug/bin) INSTALL(TARGETS target CONFIGURATIONS Release RUNTIME DESTINATION Release/bin) ‰πüÂ∞±ÊòØËØ¥ÔºåDEBUGÂíåRELEASEÁâàÊú¨ÁöÑDESTINATIONÂÆâË£ÖË∑ØÂæÑ‰∏çÂêåÔºåÈÇ£‰πàDESTINATIONÂøÖÈ°ªÂú®CONFIGUATIONSÂêéÈù¢„ÄÇ\n2. ÊôÆÈÄöÊñá‰ª∂ÁöÑÂÆâË£Ö 1 2 3 4 5 6 INSTALL(\u0026lt;FILES|PROGRAMS\u0026gt; files... TYPE \u0026lt;type\u0026gt; | DESTINATION \u0026lt;dir\u0026gt; [PERMISSIONS permissions...] [CONFIGURATIONS [Debug|Release|...]] [COMPONENT \u0026lt;component\u0026gt;] [RENAME \u0026lt;name\u0026gt;] [OPTIONAL] [EXCLUDE_FROM_ALL]) FILES|PROGRAMSËã•‰∏∫Áõ∏ÂØπË∑ØÂæÑÁªôÂá∫ÁöÑÊñá‰ª∂ÂêçÔºåÂ∞ÜÁõ∏ÂØπ‰∫éÂΩìÂâçÊ∫êÁõÆÂΩïËøõË°åËß£Èáä„ÄÇÂÖ∂‰∏≠ÔºåFILES‰∏∫ÊôÆÈÄöÁöÑÊñáÊú¨Êñá‰ª∂ÔºåPROGRAMSÊåáÁöÑÊòØÈùûÁõÆÊ†áÊñá‰ª∂ÁöÑÂèØÊâßË°åÁ®ãÂ∫è(Â¶ÇËÑöÊú¨Êñá‰ª∂)„ÄÇ\nÂ¶ÇÊûúÊú™Êèê‰æõPERMISSIONSÂèÇÊï∞ÔºåÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÊôÆÈÄöÁöÑÊñáÊú¨Êñá‰ª∂Â∞ÜÂÖ∑ÊúâOWNER_WRITEÔºåOWNER_READÔºåGROUP_READÂíåWORLD_READÊùÉÈôêÔºåÂç≥644ÊùÉÈôêÔºõËÄåÈùûÁõÆÊ†áÊñá‰ª∂ÁöÑÂèØÊâßË°åÁ®ãÂ∫èÂ∞ÜÂÖ∑ÊúâOWNER_EXECUTE, GROUP_EXECUTE,ÂíåWORLD_EXECUTEÔºåÂç≥755ÊùÉÈôê„ÄÇ\nÂÖ∂‰∏≠Ôºå‰∏çÂêåÁöÑTYPEÔºåcmake‰πüÊèê‰æõ‰∫ÜÈªòËÆ§ÁöÑÂÆâË£ÖË∑ØÂæÑÔºåÂ¶Ç‰∏ãË°®Ôºö\nTYPEÁ±ªÂûã ÂÆâË£ÖÁõÆÂΩïÂèòÈáè ÈªòËÆ§ÂÆâË£ÖÊñá‰ª∂Â§π BIN ${CMAKE_INSTALL_BINDIR} bin SBIN ${CMAKE_INSTALL_SBINDIR} sbin LIB ${CMAKE_INSTALL_LIBDIR} lib INCLUDE ${CMAKE_INSTALL_INCLUDEDIR} include SYSCONF ${CMAKE_INSTALL_SYSCONFDIR} etc SHAREDSTATE ${CMAKE_INSTALL_SHARESTATEDIR} com LOCALSTATE ${CMAKE_INSTALL_LOCALSTATEDIR} var RUNSTATE ${CMAKE_INSTALL_RUNSTATEDIR} /run DATA ${CMAKE_INSTALL_DATADIR} INFO ${CMAKE_INSTALL_INFODIR} /info LOCALE ${CMAKE_INSTALL_LOCALEDIR} /locale MAN ${CMAKE_INSTALL_MANDIR} /man DOC ${CMAKE_INSTALL_DOCDIR} /doc ËØ∑Ê≥®ÊÑèÔºåÊüê‰∫õÁ±ªÂûãÁöÑÂÜÖÁΩÆÈªòËÆ§ÂÄº‰ΩøÁî®DATAROOTÁõÆÂΩï‰Ωú‰∏∫ÂâçÁºÄÔºå‰ª•CMAKE_INSTALL_DATAROOTDIRÂèòÈáèÂÄº‰∏∫ÂÜÖÂÆπ„ÄÇ\nËØ•ÂëΩ‰ª§ÁöÑÂÖ∂‰ªñ‰∏Ä‰∫õÂèÇÊï∞ÁöÑÂê´‰πâÔºö\nDESTINATIONÔºöÊåáÂÆöÁ£ÅÁõò‰∏äË¶ÅÂÆâË£ÖÊñá‰ª∂ÁöÑÁõÆÂΩïÔºõ PERMISSIONSÔºöÊåáÂÆöÂÆâË£ÖÊñá‰ª∂ÁöÑÊùÉÈôê„ÄÇÊúâÊïàÊùÉÈôêÊòØOWNER_READÔºåOWNER_WRITEÔºåOWNER_EXECUTEÔºåGROUP_READÔºåGROUP_WRITEÔºåGROUP_EXECUTEÔºåWORLD_READÔºåWORLD_WRITEÔºåWORLD_EXECUTEÔºåSETUIDÂíåSETGIDÔºõ CONFIGURATIONSÔºöÊåáÂÆöÂÆâË£ÖËßÑÂàôÈÄÇÁî®ÁöÑÊûÑÂª∫ÈÖçÁΩÆÂàóË°®(DEBUGÊàñRELEASEÁ≠â)Ôºõ EXCLUDE_FROM_ALLÔºöÊåáÂÆöËØ•Êñá‰ª∂‰ªéÂÆåÊï¥ÂÆâË£Ö‰∏≠ÊéíÈô§Ôºå‰ªÖ‰Ωú‰∏∫ÁâπÂÆö‰∫éÁªÑ‰ª∂ÁöÑÂÆâË£ÖÁöÑ‰∏ÄÈÉ®ÂàÜËøõË°åÂÆâË£ÖÔºõ OPTIONALÔºöÂ¶ÇÊûúË¶ÅÂÆâË£ÖÁöÑÊñá‰ª∂‰∏çÂ≠òÂú®ÔºåÂàôÊåáÂÆö‰∏çÊòØÈîôËØØÔºõ RENAMEÔºöÊåáÂÆöÂ∑≤ÂÆâË£ÖÊñá‰ª∂ÁöÑÂêçÁß∞ÔºåËØ•ÂêçÁß∞ÂèØËÉΩ‰∏éÂéüÂßãÊñá‰ª∂‰∏çÂêå„ÄÇ‰ªÖÂΩìÂëΩ‰ª§ÂÆâË£Ö‰∫ÜÂçï‰∏™Êñá‰ª∂Êó∂ÔºåÊâçÂÖÅËÆ∏ÈáçÂëΩÂêç„ÄÇ 3. ÁõÆÂΩïÁöÑÂÆâË£Ö 1 2 3 4 5 6 7 8 9 10 INSTALL(DIRECTORY dirs... TYPE \u0026lt;type\u0026gt; | DESTINATION \u0026lt;dir\u0026gt; [FILE_PERMISSIONS permissions...] [DIRECTORY_PERMISSIONS permissions...] [USE_SOURCE_PERMISSIONS] [OPTIONAL] [MESSAGE_NEVER] [CONFIGURATIONS [Debug|Release|...]] [COMPONENT \u0026lt;component\u0026gt;] [EXCLUDE_FROM_ALL] [FILES_MATCHING] [[PATTERN \u0026lt;pattern\u0026gt; | REGEX \u0026lt;regex\u0026gt;] [EXCLUDE] [PERMISSIONS permissions...]] [...]) ËØ•ÂëΩ‰ª§Â∞Ü‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÁõÆÂΩïÁöÑÂÜÖÂÆπÂÆâË£ÖÂà∞ÊåáÂÆöÁöÑÁõÆÁöÑÂú∞ÔºåÁõÆÂΩïÁªìÊûÑË¢´ÈÄê‰∏™Â§çÂà∂Âà∞ÁõÆÊ†á‰ΩçÁΩÆ„ÄÇÊØè‰∏™ÁõÆÂΩïÂêçÁß∞ÁöÑÊúÄÂêé‰∏Ä‰∏™ÁªÑÊàêÈÉ®ÂàÜÈÉΩÈôÑÂä†Âà∞ÁõÆÊ†áÁõÆÂΩï‰∏≠Ôºå‰ΩÜÊòØÂèØ‰ª•‰ΩøÁî®ÂêéË∑üÊñúÊù†Êù•ÈÅøÂÖçËøôÁßçÊÉÖÂÜµÔºåÂõ†‰∏∫ÂÆÉÂ∞ÜÊúÄÂêé‰∏Ä‰∏™ÁªÑÊàêÈÉ®ÂàÜÁïôÁ©∫„ÄÇËøôÊòØ‰ªÄ‰πàÊÑèÊÄùÂë¢Ôºü\nÊØîÂ¶ÇÔºåDIRECTORYÂêéÈù¢Â¶ÇÊûúÊòØabcÊÑèÂë≥ÁùÄabcËøô‰∏™ÁõÆÂΩï‰ºöÂÆâË£ÖÂú®ÁõÆÊ†áË∑ØÂæÑ‰∏ãÔºåabc/ÊÑèÂë≥ÁùÄabcËøô‰∏™ÁõÆÂΩïÁöÑÂÜÖÂÆπ‰ºöË¢´ÂÆâË£ÖÂú®ÁõÆÊ†áË∑ØÂæÑ‰∏ãÔºåËÄåabcÁõÆÂΩïÊú¨Ë∫´Âç¥‰∏ç‰ºöË¢´ÂÆâË£Ö„ÄÇÂç≥ÔºåÂ¶ÇÊûúÁõÆÂΩïÂêç‰∏ç‰ª•\u0026quot;/\u0026ldquo;ÁªìÂ∞æÔºåÈÇ£‰πàËøô‰∏™ÁõÆÂΩïÂ∞ÜË¢´ÂÆâË£Ö‰∏∫ÁõÆÊ†áË∑ØÂæÑ‰∏ãÁöÑabcÔºåÂ¶ÇÊûúÁõÆÂΩïÂêç‰ª•/ÁªìÂ∞æÔºå‰ª£Ë°®Â∞ÜËøô‰∏™ÁõÆÂΩï‰∏≠ÁöÑÂÜÖÂÆπÂÆâË£ÖÂà∞ÁõÆÊ†áË∑ØÂæÑÔºå‰ΩÜ‰∏çÂåÖÊã¨Ëøô‰∏™ÁõÆÂΩïÊú¨Ë∫´„ÄÇ\nFILE_PERMISSIONSÂíåDIRECTORY_PERMISSIONSÈÄâÈ°πÊåáÂÆöÂØπÁõÆÊ†á‰∏≠Êñá‰ª∂ÂíåÁõÆÂΩïÁöÑÊùÉÈôê„ÄÇÂ¶ÇÊûúÊåáÂÆö‰∫ÜUSE_SOURCE_PERMISSIONSËÄåÊú™ÊåáÂÆöFILE_PERMISSIONSÔºåÂàôÂ∞Ü‰ªéÊ∫êÁõÆÂΩïÁªìÊûÑ‰∏≠Â§çÂà∂Êñá‰ª∂ÊùÉÈôê„ÄÇÂ¶ÇÊûúÊú™ÊåáÂÆöÊùÉÈôêÔºåÂàôÂ∞Ü‰∏∫Êñá‰ª∂Êèê‰æõÂú®ÂëΩ‰ª§ÁöÑFILESÂΩ¢Âºè‰∏≠ÊåáÂÆöÁöÑÈªòËÆ§ÊùÉÈôê(644ÊùÉÈôê)ÔºåËÄåÁõÆÂΩïÂ∞ÜË¢´Ëµã‰∫àÂú®ÂëΩ‰ª§ÁöÑPROGRAMSÂΩ¢Âºè‰∏≠ÊåáÂÆöÁöÑÈªòËÆ§ÊùÉÈôê(755ÊùÉÈôê)„ÄÇ\nÂèØ‰ª•‰ΩøÁî®PATTERNÊàñREGEXÈÄâÈ°π‰ª•Á≤æÁªÜÁöÑÁ≤íÂ∫¶ÊéßÂà∂ÁõÆÂΩïÁöÑÂÆâË£ÖÔºåÂèØ‰ª•ÊåáÂÆö‰∏Ä‰∏™ÈÄöÈÖçÊ®°ÂºèÊàñÊ≠£ÂàôË°®ËææÂºè‰ª•ÂåπÈÖçËæìÂÖ•ÁõÆÂΩï‰∏≠ÈÅáÂà∞ÁöÑÁõÆÂΩïÊàñÊñá‰ª∂„ÄÇPATTERN‰ªÖÂåπÈÖçÂÆåÊï¥ÁöÑÊñá‰ª∂ÂêçÔºåËÄåREGEXÂ∞ÜÂåπÈÖçÊñá‰ª∂ÂêçÁöÑ‰ªª‰ΩïÈÉ®ÂàÜÔºå‰ΩÜÂÆÉÂèØ‰ª•‰ΩøÁî®/Âíå$Ê®°ÊãüPATTERNË°å‰∏∫„ÄÇ\nÊüê‰∫õË∑üÈöèPATTERNÊàñREGEXË°®ËææÂºèÂêéÁöÑÂèÇÊï∞Ôºå‰ªÖÂ∫îÁî®‰∫éÊª°Ë∂≥Ë°®ËææÂºèÁöÑÊñá‰ª∂ÊàñÁõÆÂΩï„ÄÇÂ¶ÇÔºöEXCLUDEÈÄâÈ°πÂ∞ÜË∑≥ËøáÂåπÈÖçÁöÑÊñá‰ª∂ÊàñÁõÆÂΩï„ÄÇPERMISSIONSÈÄâÈ°πÂ∞ÜË¶ÜÁõñÂåπÈÖçÊñá‰ª∂ÊàñÁõÆÂΩïÁöÑÊùÉÈôêËÆæÁΩÆ„ÄÇ\n‰æãÂ¶ÇÔºö\n1 2 3 4 5 6 INSTALL(DIRECTORY icons scripts/ DESTINATION share/myproj PATTERN \u0026#34;CVS\u0026#34; EXCLUDE PATTERN \u0026#34;scripts/*\u0026#34; PERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ) ËøôÊù°ÂëΩ‰ª§ÁöÑÊâßË°åÁªìÊûúÊòØÔºöÂ∞ÜiconsÁõÆÂΩïÂÆâË£ÖÂà∞share/myprojÔºåÂ∞Üscripts/‰∏≠ÁöÑÂÜÖÂÆπÂÆâË£ÖÂà∞share/myprojÔºå‰∏§‰∏™ÁõÆÂΩïÂùá‰∏çÂåÖÂê´ÁõÆÂΩïÂêç‰∏∫CVSÁöÑÂ≠êÁõÆÂΩïÔºåÂØπ‰∫éscripts/*ÁöÑÊñá‰ª∂ÊåáÂÆöÊùÉÈôê‰∏∫OWNER_EXECUTEÔºåOWNER_WRITEÔºåOWNER_READÔºåGROUP_EXECUTEÔºåGROUP_READ„ÄÇ\n4. ÂÆâË£ÖÊó∂ËÑöÊú¨ÁöÑËøêË°å ÊúâÊó∂ÂÄôÈúÄË¶ÅÂú®installÁöÑËøáÁ®ã‰∏≠ÊâìÂç∞‰∏Ä‰∫õËØ≠Âè•ÔºåÊàñËÄÖÊâßË°å‰∏Ä‰∫õcmakeÊåá‰ª§Ôºö\n1 2 INSTALL([[SCRIPT \u0026lt;file\u0026gt;] [CODE \u0026lt;code\u0026gt;]] [COMPONENT \u0026lt;component\u0026gt;] [EXCLUDE_FROM_ALL] [...]) SCRIPTÂèÇÊï∞Â∞ÜÂú®ÂÆâË£ÖËøáÁ®ã‰∏≠Ë∞ÉÁî®ÁªôÂÆöÁöÑCMakeËÑöÊú¨Êñá‰ª∂(Âç≥.cmakeËÑöÊú¨Êñá‰ª∂)ÔºåÂ¶ÇÊûúËÑöÊú¨Êñá‰ª∂ÂêçÊòØÁõ∏ÂØπË∑ØÂæÑÔºåÂàôÂ∞ÜÁõ∏ÂØπ‰∫éÂΩìÂâçÊ∫êÁõÆÂΩïËøõË°åËß£Èáä„ÄÇCODEÂèÇÊï∞Â∞ÜÂú®ÂÆâË£ÖËøáÁ®ã‰∏≠Ë∞ÉÁî®ÁªôÂÆöÁöÑCMake‰ª£Á†Å„ÄÇÂ∞Ü‰ª£Á†ÅÊåáÂÆö‰∏∫ÂèåÂºïÂè∑Â≠óÁ¨¶‰∏≤ÂÜÖÁöÑÂçï‰∏™ÂèÇÊï∞„ÄÇ\n‰æãÂ¶ÇÔºö\n1 INSTALL(CODE \u0026#34;MESSAGE(\\\u0026#34;Sample install message.\\\u0026#34;)\u0026#34;) ËøôÊù°ÂëΩ‰ª§Â∞Ü‰ºöÂú®installÁöÑËøáÁ®ã‰∏≠ÊâßË°åcmake‰ª£Á†ÅÔºåÊâìÂç∞ËØ≠Âè•„ÄÇ\nÂÆâË£Ö Â∞±ÊòØÊääÊñá‰ª∂Â§çÂà∂Âà∞Âà∞ÊåáÂÆöÁõÆÂΩï‰∏ã\nÊ∑ªÂä† doc ÁõÆÂΩïÂèäÊñá‰ª∂Ôºö\n1 2 3 cd /backup/cmake/t2 mkdir doc\t#Â≠òÂÇ®Â∑•Á®ãÊñáÊ°£ touch doc/hello.txt **Ê∑ªÂä†ËÑöÊú¨Ôºö**Âú®Â∑•Á®ãÁõÆÂΩïÊ∑ªÂä† runhello.shÔºåÂÜÖÂÆπ‰∏∫Ôºö\n1 2 cd /home/jack/backup/cmake/t2/build/bin ./hello\t#Ë∞ÉÁî®ÂèØÊâßË°åÊñá‰ª∂ **Ê∑ªÂä†Êñá‰ª∂Ôºö**Â∑•Á®ãÁõÆÂΩï‰∏≠ÁöÑ COPYRIGHT Âíå README\n1 touch COPYRIGHT README ‰øÆÊîπ CMakeLists.txt Ôºå‰Ωø‰πãÂèØ‰ª•ÊîØÊåÅÂêÑÁßçÊñá‰ª∂ÁöÑÂÆâË£Ö\nÂÆâË£ÖÊñáÊ°£Ôºå‰øÆÊîπÂ∑•Á®ãÁõÆÂΩï‰∏ãÁöÑ CMakeLists.txt\n1 2 3 4 5 # ÂÆâË£Ö COPYRIGHT/README Âà∞ /\u0026lt;prefix\u0026gt;/share/doc/cmake/t2 INSTALL(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2) # ÂÆâË£Ö runhello.sh Âà∞ /\u0026lt;prefix\u0026gt;/bin INSTALL(PROGRAMS runhello.sh DESTINATION bin) ÂÆâË£Ö doc ‰∏≠ÁöÑ hello.txt , ‰∏§ÁßçÊñπÂºèÔºö\nÈÄöËøá doc ÁõÆÂΩïÂª∫Á´ã CMakeLists.txt Âπ∂Â∞Ü doc ÁõÆÂΩïÈÄöËøá ADD_SUBDIRECTORY Âä†ÂÖ•Â∑•Á®ãÊù•ÂÆåÊàê„ÄÇ\nÁõ¥Êé•Âú®Â∑•Á®ãÁõÆÂΩïÈÄöËøá INSTALL(DIRECTORY Êù•ÂÆåÊàê)Ôºö\nÂõ†‰∏∫ hello.txt Ë¶ÅÂÆâË£ÖÂà∞ /\u0026lt;prefix\u0026gt;/share/doc/cmake/t2,ÊâÄ‰ª•Êàë‰ª¨‰∏çËÉΩÁõ¥Êé•ÂÆâË£ÖÊï¥‰∏™ doc ÁõÆÂΩïÔºåËøôÈáåÈááÁî®ÁöÑÊñπÂºèÊòØÂÆâË£Ö doc ÁõÆÂΩï‰∏≠ÁöÑÂÜÖÂÆπÔºå‰πüÂ∞±ÊòØ‰ΩøÁî® \u0026quot; doc/ \u0026ldquo;„ÄÇ Âú®Â∑•Á®ãÁõÆÂΩï‰∏ãÁöÑCMakeLists.txt ‰∏≠Ê∑ªÂä†Ôºö\n1 INSTALL(DIRECTORY doc/ DESTINATION share/doc/cmake/t2) ÁºñËØëÂπ∂ÂÆâË£Ö\nËøõÂÖ•build ÁõÆÂΩïËøõË°åÂ§ñÈÉ®ÁºñËØëÔºåÊ≥®ÊÑè‰ΩøÁî® CMAKE_INSTALL_PREFIX ÂèÇÊï∞ÔºåËøôÈáåÂ∞ÜÂÆÉÂÆâË£ÖÂà∞‰∫Ü /tmp/t2 ÁõÆÂΩï:\n1 2 3 cmake -DCMAKE_INSTALL_PREFIX=/tmp/t2/usr .. make make install cd ËøõÂÖ• /tmp/t2 ÁõÆÂΩïÁúã‰ª•‰∏ãÂÆâË£ÖÁªìÊûúÔºö\n1 2 3 4 5 6 7 8 9 10 11 ./usr ./usr/share ./usr/share/doc ./usr/share/doc/cmake ./usr/share/doc/cmake/t2 ./usr/share/doc/cmake/t2/hello.txt ./usr/share/doc/cmake/t2/README ./usr/share/doc/cmake/t2/COPYRIGHT ./usr/bin ./usr/bin/hello ./usr/bin/runhello.sh Â¶ÇÊûúË¶ÅÁõ¥Êé•ÂÆâË£ÖÂà∞Á≥ªÁªüÔºåÂèØ‰ª•‰ΩøÁî®Â¶Ç‰∏ãÊåá‰ª§Ôºö\n1 cmake -DCMAKE_INSTALL_PREFIX=/usr .. Â¶ÇÊûúÊ≤°ÊúâÈ¢ùÂ§ñÂÆö‰πâÔºåCMAKE_INSTALL_PREFIX ÁöÑÈªòËÆ§ÂÆö‰πâÊòØ /usr/local\n‰∫î„ÄÅÈùôÊÄÅÂ∫ì‰∏éÂä®ÊÄÅÂ∫ìÊûÑÂª∫ Âä®ÊÄÅÂ∫ì„ÄÅÈùôÊÄÅÂ∫ì‰∏éÂèØÊâßË°åÊñá‰ª∂ÁöÑÂå∫Âà´Ôºö\nÂä®ÊÄÅÈìæÊé•Â∫ìÔºàDynamic Link LibraryÔºåÁº©ÂÜô‰∏∫DLLÔºâÊòØÂú®Á®ãÂ∫èËøêË°åÊó∂Âä®ÊÄÅË∞ÉÁî®ÁöÑÔºåÂèØ‰ª•Ë¢´ÂÖ∂ÂÆÉÂ∫îÁî®Á®ãÂ∫èÂÖ±‰∫´ÁöÑÁ®ãÂ∫èÊ®°ÂùóÔºåÂÖ∂‰∏≠Â∞ÅË£Ö‰∫Ü‰∏Ä‰∫õÂèØ‰ª•Ë¢´ÂÖ±‰∫´ÁöÑ‰æãÁ®ãÂíåËµÑÊ∫ê„ÄÇÂä®ÊÄÅÈìæÊé•Â∫ìÊñá‰ª∂ÁöÑÊâ©Â±ïÂêç‰∏ÄËà¨ÊòØdllÔºå‰πüÊúâÂèØËÉΩÊòØdrv„ÄÅsysÂíåfonÔºåÂÆÉÂíåÂèØÊâßË°åÊñá‰ª∂ÔºàexeÔºâÈùûÂ∏∏Á±ª‰ººÔºåÂå∫Âà´Âú®‰∫éDLL‰∏≠ËôΩÁÑ∂ÂåÖÂê´‰∫ÜÂèØÊâßË°å‰ª£Á†ÅÂç¥‰∏çËÉΩÂçïÁã¨ÊâßË°åÔºåËÄåÂ∫îÁî±WindowsÂ∫îÁî®Á®ãÂ∫èÁõ¥Êé•ÊàñÈó¥Êé•Ë∞ÉÁî®„ÄÇ\nLibÁß∞‰∏∫ÈùôÊÄÅÈìæÊé•Â∫ì(static link library)ÔºåÊòØÂú®ÁºñËØëÁöÑÈìæÊé•ÊúüÈó¥‰ΩøÁî®ÁöÑÔºå‰ªñÈáåÈù¢ÂÖ∂ÂÆûÂ∞±ÊòØÊ∫êÊñá‰ª∂ÁöÑÂáΩÊï∞ÂÆûÁé∞„ÄÇLibÂè™ÊòØDllÁöÑÈôÑÂ∏¶ÂìÅÔºåÊòØDLLÂØºÂá∫ÁöÑÂáΩÊï∞ÂàóË°®Êñá‰ª∂ËÄåÂ∑≤„ÄÇ\nDllÂÖ∂ÂÆûÂíåExeÊòØÂá†‰πéÂÆåÂÖ®‰∏ÄÊ†∑ÁöÑÔºåÂîØ‰∏ÄÁöÑ‰∏ç‰∏ÄÊ†∑Â∞±ÊòØExeÁöÑÂÖ•Âè£ÂáΩÊï∞ÂºèWinMainÂáΩÊï∞ÔºàconsoleÁ®ãÂ∫èÊòØmainÂáΩÊï∞ÔºâÔºåËÄåDllÊòØDllMainÂáΩÊï∞ÔºåÂÖ∂‰ªñÂÆåÂÖ®ÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇÊâÄ‰ª•Êúâ‰∫∫‰πüÊàèÁß∞DllÊòØ‰∏çËÉΩËá™Â∑±ËøêË°åÁöÑExe„ÄÇ\nÈùôÊÄÅÈìæÊé•ÊòØÊåáÊääË¶ÅË∞ÉÁî®ÁöÑÂáΩÊï∞ÊàñËÄÖËøáÁ®ãÈìæÊé•Âà∞ÂèØÊâßË°åÊñá‰ª∂‰∏≠ÔºåÊàê‰∏∫ÂèØÊâßË°åÊñá‰ª∂ÁöÑ‰∏ÄÈÉ®ÂàÜ(Êã∑Ë¥ùÂáΩÊï∞) Âä®ÊÄÅÈìæÊé•ÊâÄË∞ÉÁî®ÁöÑÂáΩÊï∞‰ª£Á†ÅÂπ∂Ê≤°ÊúâË¢´Êã∑Ë¥ùÂà∞Â∫îÁî®Á®ãÂ∫èÁöÑÂèØÊâßË°åÊñá‰ª∂‰∏≠ÂéªÔºåËÄåÊòØ‰ªÖ‰ªÖÂú®ÂÖ∂‰∏≠Âä†ÂÖ•‰∫ÜÊâÄË∞ÉÁî®ÂáΩÊï∞ÁöÑÊèèËø∞‰ø°ÊÅØÔºàÂæÄÂæÄÊòØ‰∏Ä‰∫õÈáçÂÆö‰Ωç‰ø°ÊÅØÔºâ„ÄÇ‰ªÖÂΩìÂ∫îÁî®Á®ãÂ∫èË¢´Ë£ÖÂÖ•ÂÜÖÂ≠òÂºÄÂßãËøêË°åÊó∂ÔºåÂú®Êìç‰ΩúÁ≥ªÁªüÁöÑÁÆ°ÁêÜ‰∏ãÔºåÊâçÂú®Â∫îÁî®Á®ãÂ∫è‰∏éÁõ∏Â∫îÁöÑDLL‰πãÈó¥Âª∫Á´ãÈìæÊé•ÂÖ≥Á≥ª„ÄÇÂΩìË¶ÅÊâßË°åÊâÄË∞ÉÁî®DLL‰∏≠ÁöÑÂáΩÊï∞Êó∂ÔºåÊ†πÊçÆÈìæÊé•‰∫ßÁîüÁöÑÈáçÂÆö‰Ωç‰ø°ÊÅØÔºåÊìç‰ΩúÁ≥ªÁªüÊâçËΩ¨ÂéªÊâßË°åDLL‰∏≠Áõ∏Â∫îÁöÑÂáΩÊï∞‰ª£Á†Å„ÄÇ ÂèØÊâßË°åÊñá‰ª∂ÂíåÂä®ÊÄÅÂ∫ì‰πãÈó¥ÁöÑÂå∫Âà´ÔºöÂèØÊâßË°åÊñá‰ª∂‰∏≠ÊúâmainÂáΩÊï∞ÔºåÂä®ÊÄÅÂ∫ì‰∏≠Ê≤°ÊúâmainÂáΩÊï∞ÔºåÂèØÊâßË°åÊñá‰ª∂ÂèØ‰ª•Ë¢´Á®ãÂ∫èÊâßË°åÔºåÂä®ÊÄÅÂ∫ìÈúÄË¶Å‰æùËµñÁ®ãÂ∫èË∞ÉÁî®ËÄÖ„ÄÇ\nÊú¨ËäÇ‰ªªÂä°Ôºö\nÂª∫Á´ã‰∏Ä‰∏™ÈùôÊÄÅÂ∫ìÂíåÂä®ÊÄÅÂ∫ìÔºåÊèê‰æõ HelloFunc ÂáΩÊï∞‰æõÂÖ∂‰ªñÁ®ãÂ∫èÁºñÁ®ã‰ΩøÁî®ÔºåHelloFunc ÂêëÁªàÁ´ØËæìÂá∫ Hello World Â≠óÁ¨¶‰∏≤„ÄÇ ÂÆâË£ÖÂ§¥Êñá‰ª∂‰∏éÂÖ±‰∫´Â∫ì„ÄÇ 1. ÂáÜÂ§áÂ∑•‰Ωú Âú® /back/cmake ÁõÆÂΩï‰∏ãÂª∫Á´ã t3 ÁõÆÂΩïÔºåÁî®‰∫éÂ≠òÊîæÊú¨ËäÇÂ∑•Á®ã\n1 2 3 4 5 6 cd /backup/cmake/t3 touch CMakeLists.txt\t#Âª∫Á´ãÂ∑•Á®ãÊñá‰ª∂ mkdir lib\t#Âª∫Á´ãÂ∫ìÊñá‰ª∂Â§π cd lib touch hello.c hello.h\t#Âú®libÁõÆÂΩï‰∏ãÂª∫Á´ã‰∏§‰∏™Ê∫êÊñá‰ª∂ touch CMakeLists.txt 2. Âª∫Á´ãÂÖ±‰∫´Â∫ì ÁºñËæëÂ∑•Á®ãÁõÆÂΩï‰∏ãÁöÑ CMakeLists.txt Êñá‰ª∂ÂÜÖÂÆπ‰∏∫Ôºö\n1 2 PROJECT(HELLOLIB)\t#ÂÆö‰πâÂ∑•Á®ãÂêç ADD_SUBDIRECTORY(lib)\t#ÊåáÂÆö Â∫ì ÁöÑÊñá‰ª∂Â§π ÁºñËæë libÊñá‰ª∂Â§π‰∏ãÁöÑ hello.h ÂÜÖÂÆπÂ¶Ç‰∏ãÔºö( #if„ÄÅ#ifdef„ÄÅ#ifndef Âå∫Âà´ )\n1 2 3 4 5 #ifndef HELLO_H\t//Â¶ÇÊûúÂΩìÂâçÁöÑÂÆèÊú™Ë¢´ÂÆö‰πâÔºåHELLO_HÊòØÂÆèÂêç #define HELLO_H\t//ÂÆö‰πâÂÆè #include \u0026lt;stdio.h\u0026gt;\t//ÂºïÂÖ•Â§¥Êñá‰ª∂ void HelloFunc();\t//Â£∞ÊòéÂáΩÊï∞ #endif ÁºñËæë libÊñá‰ª∂Â§π‰∏ãÁöÑ hello.c ÂÜÖÂÆπ‰∏∫Ôºö\n1 2 3 4 5 #include \u0026#34;hello.h\u0026#34;\t//\u0026#34;\u0026#34;Êã¨‰ΩèË°®Á§∫: È¢ÑÂ§ÑÁêÜÁ®ãÂ∫èÂÖàÂà∞ÂΩìÂâçÁõÆÂΩï‰∏ãÂØªÊâæÊñá‰ª∂ÔºåÂÜçÂà∞È¢ÑÂÆö‰πâÁöÑÁº∫ÁúÅË∑ØÂæÑ(ÈÄöÂ∏∏Áî±INCLUDEÁéØÂ¢ÉÂèòÈáèÊåáÂÆö)‰∏ãÂØªÊâæÊñá‰ª∂ void HelloFunc() { printf(\u0026#34;Hello world\\n\u0026#34;); } ÁºñËæë /lib/CMakeLists.txt :\n1 2 SET(LIBHELLO_SRC hello.c)\t#ÂÆö‰πâÂèòÈáè ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})\t#Â∞ÜÊåáÂÆöÁöÑÊ∫êÊñá‰ª∂ÁºñËØëÊàêÂ∫ì ÈááÁî®Â§ñÈÉ®ÁºñËØëÔºåÂú®Â∑•Á®ãÁõÆÂΩï‰∏ãÂª∫Á´ã‰∏Ä‰∏™ build ÁõÆÂΩïÔºå\n1 2 3 4 mkdir build cd build cmake .. make ËøôÊó∂ÔºåÂú® /build/lib ÁõÆÂΩï‰∏ãÂæóÂà∞‰∏Ä‰∏™ libhello.soÔºåËøôÂ∞±ÊòØÊàë‰ª¨ÊúüÊúõÁöÑÂÖ±‰∫´Â∫ì„ÄÇ\nÂ¶ÇÊûúË¶ÅÊåáÂÆö libhello.so ÁîüÊàêÁöÑ‰ΩçÁΩÆÔºåÂèØ‰ª•ÈÄöËøáÂú®‰∏ªÂ∑•Á®ãÊñá‰ª∂ CMakeLists.txt ‰∏≠‰øÆÊîπ ADD_SUBDIRECTORY(lib) Êåá‰ª§Êù•ÊåáÂÆö‰∏Ä‰∏™ÁºñËØëËæìÂá∫‰ΩçÁΩÆÊàñËÄÖÂú® lib/CMakeLists.txt ‰∏≠Ê∑ªÂä†Ôºö\n1 SET(LIBRARY_OUTPUT_PATH \u0026lt;Ë∑ØÂæÑ\u0026gt;)\t#ÊåáÂÆöÊñ∞ÁöÑ‰ΩçÁΩÆ ADD_LIBRARY ‰∏ªË¶Å‰ΩúÁî®Â∞±ÊòØÂ∞ÜÊåáÂÆöÁöÑÊ∫êÊñá‰ª∂ÁîüÊàêÈìæÊé•Êñá‰ª∂ÔºåÁÑ∂ÂêéÊ∑ªÂä†Âà∞Â∑•Á®ã‰∏≠Âéª„ÄÇ\n1 2 3 4 ADD_LIBRARY(libname #ÁîüÊàêÁöÑÂ∫ìÊñá‰ª∂ÁöÑÂêçÂ≠ó [SHARED|STATIC|MODULE]\t#Â∫ìÊñá‰ª∂Á±ªÂûã [EXCLUDE_FROM_ALL]\t#ÊåáÂÆöËøô‰∏™Â∫ì‰∏ç‰ºöË¢´ÈªòËÆ§ÊûÑÂª∫,Èô§ÈùûÊúâÂÖ∂‰ªñÁöÑÁªÑ‰ª∂‰æùËµñÊàñËÄÖÊâãÂä®ÊûÑÂª∫ source1 source2 ... sourceN)\t#ÂêÑ‰∏™Ê∫êÊñá‰ª∂ SHARED Â∫ìÔºö‰ºöË¢´Âä®ÊÄÅÈìæÊé•ÔºàÂä®ÊÄÅÈìæÊé•Â∫ìÔºâÔºåÂú®ËøêË°åÊó∂‰ºöË¢´Âä†ËΩΩ„ÄÇ STATIC Â∫ìÔºöÊòØÁõÆÊ†áÊñá‰ª∂ÁöÑÂΩíÊ°£Êñá‰ª∂ÔºåÂú®ÈìæÊé•ÂÖ∂ÂÆÉÁõÆÊ†áÁöÑÊó∂ÂÄô‰ΩøÁî®„ÄÇ MODULE Â∫ìÔºöÊòØ‰∏ÄÁßç‰∏ç‰ºöË¢´ÈìæÊé•Âà∞ÂÖ∂ÂÆÉÁõÆÊ†á‰∏≠ÁöÑÊèí‰ª∂Ôºå‰ΩÜÊòØÂèØËÉΩ‰ºöÂú®ËøêË°åÊó∂‰ΩøÁî®dlopen-Á≥ªÂàóÁöÑÂáΩÊï∞„ÄÇ 3. Ê∑ªÂä†ÈùôÊÄÅÂ∫ì ÂêåÊ†∑‰ΩøÁî®‰∏äÈù¢ÁöÑÊåá‰ª§ÔºåÊàë‰ª¨Âú®ÊîØÊåÅÂä®ÊÄÅÂ∫ìÁöÑÂü∫Á°Ä‰∏äÂÜç‰∏∫Â∑•Á®ãÊ∑ªÂä†‰∏Ä‰∏™ÈùôÊÄÅÂ∫ìÔºåÊåâÁÖß‰∏ÄËà¨ÁöÑ‰π† ÊÉØÔºåÈùôÊÄÅÂ∫ìÂêçÂ≠óË∑üÂä®ÊÄÅÂ∫ìÂêçÂ≠óÂ∫îËØ•ÊòØ‰∏ÄËá¥ÁöÑÔºåÂè™‰∏çËøáÂêéÁºÄÊòØ.a ÁΩ¢‰∫Ü„ÄÇ\nÊ∑ªÂä†ÈùôÊÄÅÂ∫ìÁöÑÊåá‰ª§Ôºö\n1 ADD_LIBRARY(hello STATIC ${LIBHELLO_SRC}) ÁÑ∂ÂêéÂÜçÂú® build ÁõÆÂΩïËøõË°åÂ§ñÈÉ®ÁºñËØëÔºåÊàë‰ª¨‰ºöÂèëÁé∞ÔºåÈùôÊÄÅÂ∫ìÊ†πÊú¨Ê≤°ÊúâË¢´ÊûÑÂª∫Ôºå‰ªçÁÑ∂Âè™ÁîüÊàê‰∫Ü ‰∏Ä‰∏™Âä®ÊÄÅÂ∫ì„ÄÇÂõ†‰∏∫ hello ‰Ωú‰∏∫‰∏Ä‰∏™ target ÊòØ‰∏çËÉΩÈáçÂêçÁöÑÔºåÊâÄ‰ª•ÔºåÈùôÊÄÅÂ∫ìÊûÑÂª∫Êåá‰ª§Êó†Êïà„ÄÇ\nÂ¶ÇÊûúÊàë‰ª¨Êää‰∏äÈù¢ÁöÑ hello ‰øÆÊîπ‰∏∫ hello_static:\n1 ADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) Â∞±ÂèØ‰ª•ÊûÑÂª∫‰∏Ä‰∏™ libhello_static.a ÁöÑÈùôÊÄÅÂ∫ì‰∫Ü„ÄÇ\n‰ΩÜÊòØËøôÁßçÁªìÊûúÊòæÁ§∫‰∏çÊòØÊàë‰ª¨ÊÉ≥Ë¶ÅÁöÑ,Êàë‰ª¨ÈúÄË¶ÅÁöÑÊòØÂêçÂ≠óÁõ∏ÂêåÁöÑÈùôÊÄÅÂ∫ìÂíåÂä®ÊÄÅÂ∫ì,Âõ†‰∏∫ target Âêç Áß∞ÊòØÂîØ‰∏ÄÁöÑ,ÊâÄ‰ª•,Êàë‰ª¨ËÇØÂÆö‰∏çËÉΩÈÄöËøá ADD_LIBRARY Êåá‰ª§Êù•ÂÆûÁé∞‰∫Ü„ÄÇËøôÊó∂ÂÄôÊàë‰ª¨ÈúÄË¶ÅÁî®Âà∞ Âè¶Â§ñ‰∏Ä‰∏™Êåá‰ª§:\n1 2 3 4 SET_TARGET_PROPERTITES(target1 target2 ... PROPERTIES prop1 value1 prop2 value2 ...) ËøôÊù°Êåá‰ª§ÂèØ‰ª•Áî®Êù•ËÆæÁΩÆËæìÂá∫ÁöÑÂêçÁß∞ÔºåÂØπ‰∫éÂä®ÊÄÅÂ∫ìÔºåËøòÂèØ‰ª•Áî®Êù•ÊåáÂÆöÂä®ÊÄÅÂ∫ìÁâàÊú¨Âíå API ÁâàÊú¨„ÄÇ\nÂú®Êú¨‰æã‰∏≠,Êàë‰ª¨ÈúÄË¶Å‰ΩúÁöÑÊòØÂêë lib/CMakeLists.txt ‰∏≠Ê∑ªÂä†‰∏ÄÊù°:\n1 SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \u0026#34;hello\u0026#34;) ËøôÊ†∑ÔºåÊàë‰ª¨Â∞±ÂèØ‰ª•ÂêåÊó∂ÂæóÂà∞ libhello.so Âíå libhello.a ‰∏§‰∏™Â∫ì‰∫Ü„ÄÇ\n‰∏éSET_TARGET_PROPERTIESÂØπÂ∫îÁöÑÊåá‰ª§ÊòØÔºö\n1 GET_TARGET_PROPERTY(VAR target property)\t#ÂæóÂà∞Â±ûÊÄßÂÄº ÂÖ∑‰ΩìÁî®Ê≥ïÂ¶Ç‰∏ãÔºöÊàë‰ª¨Âêë lib/CMakeLists.txt ‰∏≠Ê∑ªÂä†Ôºö\n1 2 GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME) MESSAGE(STATUS \u0026#34;This is the hello_static OUTPUT_NAME:\u0026#34;${OUTPUT_VALUE}) Â¶ÇÊûúÊ≤°ÊúâËøô‰∏™Â±ûÊÄßÂÆö‰πâÔºåÂàôËøîÂõû NOTFOUND„ÄÇ\n4. Âä®ÊÄÅÂ∫ìÁâàÊú¨Âè∑ ÊåâÁÖßËßÑÂàôÔºåÂä®ÊÄÅÂ∫ìÊòØÂ∫îËØ•ÂåÖÂê´‰∏Ä‰∏™ÁâàÊú¨Âè∑ÁöÑÔºåÊàë‰ª¨ÂèØ‰ª•Áúã‰∏Ä‰∏ãÁ≥ªÁªüÁöÑÂä®ÊÄÅÂ∫ìÔºå‰∏ÄËà¨ÊÉÖÂÜµÊòØÔºö\n1 2 3 libhello.so.1.2 libhello.so -\u0026gt;libhello.so.1 libhello.so.1-\u0026gt;libhello.so.1.2 ‰∏∫‰∫ÜÂÆûÁé∞Âä®ÊÄÅÂ∫ìÁâàÊú¨Âè∑,Êàë‰ª¨‰ªçÁÑ∂ÈúÄË¶Å‰ΩøÁî® SET_TARGET_PROPERTIES Êåá‰ª§„ÄÇ ÂÖ∑‰Ωì‰ΩøÁî®ÊñπÊ≥ïÂ¶Ç‰∏ãÔºö\n1 SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1) VERSION ÔºöÊåá‰ª£Âä®ÊÄÅÂ∫ìÁâàÊú¨Ôºå SOVERSIONÔºöÊåá‰ª£API ÁâàÊú¨ Â∞Ü‰∏äËø∞Êåá‰ª§Âä†ÂÖ• lib/CMakeLists.txt ‰∏≠,ÈáçÊñ∞ÊûÑÂª∫ÁúãÁúãÁªìÊûú„ÄÇ Âú® build/lib ÁõÆÂΩï‰ºöÁîüÊàê: libhello.so.1.2 libhello.so.1-\u0026gt;libhello.so.1.2 libhello.so -\u0026gt;libhello.so.1\n5. ÂÆâË£ÖÂÖ±‰∫´Â∫ìÂíåÂ§¥Êñá‰ª∂ ‰ª•‰∏äÈù¢ÁöÑ‰æãÂ≠êÔºåÊàë‰ª¨ÈúÄË¶ÅÂ∞Ü libhello.aÔºå libhello.so.x ‰ª•Âèä hello.h ÂÆâË£ÖÂà∞Á≥ªÁªüÁõÆÂΩïÔºåÊâçËÉΩÁúüÊ≠£ËÆ©ÂÖ∂‰ªñ‰∫∫ÂºÄÂèë‰ΩøÁî®ÔºåÂú®Êú¨‰æã‰∏≠Êàë‰ª¨Â∞Ü hello ÁöÑÂÖ±‰∫´Â∫ìÂÆâË£ÖÂà∞ \u0026lt;prefix\u0026gt;/lib ÁõÆÂΩïÔºåÂ∞Ü hello.h ÂÆâË£ÖÂà∞**\u0026lt;prefix\u0026gt;/include/hello** ÁõÆÂΩï„ÄÇ\nÂà©Áî®‰∏ä‰∏ÄËäÇ‰∫ÜËß£Âà∞ÁöÑ INSTALL Êåá‰ª§ÔºåÊàë‰ª¨Âêë lib/CMakeLists.txt ‰∏≠Ê∑ªÂä†Â¶Ç‰∏ãÊåá‰ª§:\n1 2 3 4 5 INSTALL(TARGETS hello hello_static LIBRARY DESTINATION lib ARCHIVE DESTINATION lib)\t#Ê≥®ÊÑè,ÈùôÊÄÅÂ∫ìË¶Å‰ΩøÁî® ARCHIVE ÂÖ≥ÈîÆÂ≠ó INSTALL(FILES hello.h DESTINATION include/hello) ÁªàÁ´ØËæìÂÖ•Ôºö\n1 2 3 cmake -DCMAKE_INSTALL_PREFIX=/usr .. make make install\t#Permission denied Êàë‰ª¨Â∞±ÂèØ‰ª•Â∞ÜÂ§¥Êñá‰ª∂ÂíåÂÖ±‰∫´Â∫ìÂÆâË£ÖÂà∞Á≥ªÁªüÁõÆÂΩï/usr/lib Âíå/usr/include/hello ‰∏≠‰∫Ü„ÄÇ\nÂÆåÊï¥‰ª£Á†ÅÔºö lib/CMakeLists.txt\n1 2 3 4 5 6 7 8 9 10 11 12 SET(LIBHELLO_SRC hello.c) ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})\t#Â∞ÜÊ∫êÁ†ÅÁºñËØëÊàêÂä®ÊÄÅÂ∫ì ADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC})\t#ÁºñËØëÊàêÈùôÊÄÅÂ∫ì SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \u0026#34;hello\u0026#34;) #ÈáçÂëΩÂêç GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME) #ËØªÂèñÂêçÂ≠óËæìÂá∫ÊòæÁ§∫ MESSAGE(STATUS \u0026#34;This is the hello_static OUTPUT_NAME:\u0026#34;${OUTPUT_VALUE}) SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1) #ÁâàÊú¨Âè∑ INSTALL(TARGETS hello hello_static\t#ÂÆâË£ÖÂä®ÊÄÅÂ∫ìÂíåÈùôÊÄÅÂ∫ì LIBRARY DESTINATION lib ARCHIVE DESTINATION lib)\t#Ê≥®ÊÑè,ÈùôÊÄÅÂ∫ìË¶Å‰ΩøÁî® ARCHIVE ÂÖ≥ÈîÆÂ≠ó INSTALL(FILES hello.h\t#ÂÆâË£ÖÂ§¥Êñá‰ª∂ DESTINATION include/hello) ÂÖ≠„ÄÅÂ¶Ç‰Ωï‰ΩøÁî®Â§ñÈÉ®ÂÖ±‰∫´Â∫ìÂíåÂ§¥Êñá‰ª∂ ‰∏ä‰∏ÄËäÇÊàë‰ª¨Â∑≤ÁªèÂÆåÊàê‰∫Ü libhello Âä®ÊÄÅÂ∫ìÁöÑÊûÑÂª∫‰ª•ÂèäÂÆâË£ÖÔºåÊú¨ËäÇÊàë‰ª¨ÁöÑ‰ªªÂä°ÂæàÁÆÄÂçïÔºö ÁºñÂÜô‰∏Ä‰∏™Á®ãÂ∫è‰ΩøÁî®Êàë‰ª¨‰∏ä‰∏ÄËäÇÊûÑÂª∫ÁöÑÂÖ±‰∫´Â∫ì„ÄÇ\n1. ÂáÜÂ§áÂ∑•‰Ωú Âª∫Á´ãÂ∑•Á®ãÊñá‰ª∂Â§πÔºöÂú®/backup/cmake ÁõÆÂΩïÂª∫Á´ã t4 ÁõÆÂΩï,Êú¨ËäÇÊâÄÊúâËµÑÊ∫êÂ∞ÜÂ≠òÂÇ®Âú® t4 ÁõÆÂΩï„ÄÇ\n1 2 3 4 5 cd t4 touch CMakeLists.txt mkdir src cd src touch main.c CMakeList.txt 2. ÁºñÂÜôÊ∫êÊñá‰ª∂ Âª∫Á´ã src ÁõÆÂΩïÔºåÁºñÂÜôÊ∫êÊñá‰ª∂ main.c Ôºö\n1 2 3 4 5 #include \u0026lt;hello.h\u0026gt; int main() { HelloFunc(); } ÁºñËæëÂ∑•Á®ãÁõÆÂΩï‰∏ãÁöÑ CMakeLists.txtÔºö\n1 2 PROJECT(NEWHELLO) ADD_SUBDIRECTORY(src) ÁºñËæë /src/CMakeLists.txtÔºö\n1 ADD_EXECUTABLE(main main.c) Â¶ÇÊûúÁõ¥Êé•ÁºñËØë‰ºöÂá∫ÈîôÔºö/backup/cmake/t4/src/main.c:1:19: error: hello.h: Ê≤°ÊúâÈÇ£‰∏™Êñá‰ª∂ÊàñÁõÆÂΩï\n3. ÂºïÂÖ•Â§¥Êñá‰ª∂ÊêúÁ¥¢Ë∑ØÂæÑ hello.h ‰Ωç‰∫é /usr/include/hello ÁõÆÂΩï‰∏≠ÔºåÂπ∂Ê≤°Êúâ‰Ωç‰∫éÁ≥ªÁªüÊ†áÂáÜÁöÑÂ§¥Êñá‰ª∂Ë∑ØÂæÑÔºå‰∏∫‰∫ÜËÆ©Êàë‰ª¨ÁöÑÂ∑•Á®ãËÉΩÂ§üÊâæÂà∞ hello.h Â§¥Êñá‰ª∂ÔºåÊàë‰ª¨ÈúÄË¶ÅÂºïÂÖ•‰∏Ä‰∏™Êñ∞ÁöÑÊåá‰ª§ÔºöINCLUDE_DIRECTORIESÔºåÂÖ∂ÂÆåÊï¥ÁöÑËØ≠Ê≥ïÊòØÔºö\n1 2 3 INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...) ËøôÊù°Êåá‰ª§ÂèØ‰ª•Áî®Êù•ÂêëÂ∑•Á®ãÊ∑ªÂä†Â§ö‰∏™ÁâπÂÆöÁöÑÂ§¥Êñá‰ª∂ÊêúÁ¥¢Ë∑ØÂæÑÔºåË∑ØÂæÑ‰πãÈó¥Áî®Á©∫Ê†ºÂàÜÂâ≤ÔºåÂ¶ÇÊûúË∑ØÂæÑ‰∏≠ÂåÖÂê´‰∫ÜÁ©∫Ê†ºÔºåÂèØ‰ª•‰ΩøÁî®ÂèåÂºïÂè∑Â∞ÜÂÆÉÊã¨Ëµ∑Êù•ÔºåÈªòËÆ§ÁöÑË°å‰∏∫ÊòØËøΩÂä†Âà∞ÂΩìÂâçÁöÑÂ§¥Êñá‰ª∂ÊêúÁ¥¢Ë∑ØÂæÑÁöÑÂêéÈù¢Ôºå‰Ω†ÂèØ‰ª•ÈÄöËøá‰∏§ÁßçÊñπÂºèÊù•ËøõË°åÊéßÂà∂ÊêúÁ¥¢Ë∑ØÂæÑÊ∑ªÂä†ÁöÑÊñπÂºè:\nCMAKE_INCLUDE_DIRECTORIES_BEFOREÔºåÈÄöËøá SET Ëøô‰∏™ cmake ÂèòÈáè‰∏∫ onÔºåÂèØ‰ª• Â∞ÜÊ∑ªÂä†ÁöÑÂ§¥Êñá‰ª∂ÊêúÁ¥¢Ë∑ØÂæÑÊîæÂú®Â∑≤ÊúâË∑ØÂæÑÁöÑÂâçÈù¢„ÄÇ ÈÄöËøá AFTER ÊàñËÄÖ BEFORE ÂèÇÊï∞Ôºå‰πüÂèØ‰ª•ÊéßÂà∂ÊòØËøΩÂä†ËøòÊòØÁΩÆÂâç„ÄÇ Áé∞Âú®Êàë‰ª¨Âú® src/CMakeLists.txt ‰∏≠Ê∑ªÂä†‰∏Ä‰∏™Â§¥Êñá‰ª∂ÊêúÁ¥¢Ë∑ØÂæÑÔºåÊñπÂºèÂæàÁÆÄÂçïÔºåÂä†ÂÖ•:\n1 INCLUDE_DIRECTORIES(/usr/include/hello) ËøõÂÖ• build ÁõÆÂΩïÔºåÈáçÊñ∞ËøõË°åÊûÑÂª∫ÔºåËøôÊòØÊâæ‰∏çÂà∞ hello.h ÁöÑÈîôËØØÂ∑≤ÁªèÊ∂àÂ§±Ôºå‰ΩÜÊòØÂá∫Áé∞‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈîôËØØÔºö main.c:(.text+0x12): undefined reference to HelloFunc\u0026rsquo;`\nËøôÊòØÂõ†‰∏∫Êàë‰ª¨Âπ∂Ê≤°Êúâ link Âà∞ÂÖ±‰∫´Â∫ì libhello ‰∏ä„ÄÇ\n4. ‰∏∫ target ÈìæÊé•ÂÖ±‰∫´Â∫ì Êàë‰ª¨Áé∞Âú®ÈúÄË¶ÅÂÆåÊàêÁöÑ‰ªªÂä°ÊòØÂ∞ÜÁõÆÊ†áÊñá‰ª∂(target)ÈìæÊé•Âà∞ libhelloÔºåËøôÈáåÊàë‰ª¨ÈúÄË¶ÅÂºïÂÖ•‰∏§‰∏™Êñ∞ÁöÑÊåá‰ª§**LINK_DIRECTORIES** Âíå TARGET_LINK_LIBRARIES\n1 LINK_DIRECTORIES(directory1 directory2 ...) Ëøô‰∏™Êåá‰ª§ÈùûÂ∏∏ÁÆÄÂçïÔºåÊ∑ªÂä†ÈùûÊ†áÂáÜÁöÑÂÖ±‰∫´Â∫ìÊêúÁ¥¢Ë∑ØÂæÑÔºåÊØîÂ¶ÇÔºåÂú®Â∑•Á®ãÂÜÖÈÉ®ÂêåÊó∂Â≠òÂú®ÂÖ±‰∫´Â∫ìÂíåÂèØÊâßË°å‰∫åËøõÂà∂,Âú®ÁºñËØëÊó∂Â∞±ÈúÄË¶ÅÊåáÂÆö‰∏Ä‰∏ãËøô‰∫õÂÖ±‰∫´Â∫ìÁöÑË∑ØÂæÑ„ÄÇËøô‰∏™‰æãÂ≠ê‰∏≠Êàë‰ª¨Ê≤°ÊúâÁî®Âà∞Ëøô‰∏™Êåá‰ª§„ÄÇ\n1 2 3 TARGET_LINK_LIBRARIES(target library1 \u0026lt;debug | optimized\u0026gt; library2 ...) Ëøô‰∏™Êåá‰ª§ÂèØ‰ª•Áî®Êù•‰∏∫ target Ê∑ªÂä†ÈúÄË¶ÅÈìæÊé•ÁöÑÂÖ±‰∫´Â∫ìÔºåÊú¨‰æã‰∏≠ÊòØ‰∏Ä‰∏™ÂèØÊâßË°åÊñá‰ª∂Ôºå‰ΩÜÊòØÂêåÊ†∑ÂèØ‰ª•Áî®‰∫é‰∏∫Ëá™Â∑±ÁºñÂÜôÁöÑÂÖ±‰∫´Â∫ìÊ∑ªÂä†ÂÖ±‰∫´Â∫ìÈìæÊé•„ÄÇ\n‰∏∫‰∫ÜËß£ÂÜ≥Êàë‰ª¨ÂâçÈù¢ÈÅáÂà∞ÁöÑ HelloFunc Êú™ÂÆö‰πâÈîôËØØ,Êàë‰ª¨ÈúÄË¶Å‰ΩúÁöÑÊòØÂêë src/CMakeLists.txt ‰∏≠Ê∑ªÂä†Â¶Ç‰∏ãÊåá‰ª§Ôºö\n1 2 3 TARGET_LINK_LIBRARIES(main hello)\t#hello ÊòØÂÖ±‰∫´Â∫ì libhello.so # ‰πüÂèØ‰ª•ÂÜôÊàê TARGET_LINK_LIBRARIES(main libhello.so) ËøõÂÖ• build ÁõÆÂΩïÈáçÊñ∞ËøõË°åÊûÑÂª∫ÔºåÂ∞±ÂæóÂà∞‰∫Ü‰∏Ä‰∏™ÈìæÊé•Âà∞ libhello ÁöÑÂèØÊâßË°åÁ®ãÂ∫è mainÔºå‰Ωç‰∫é build/src ÁõÆÂΩïÔºåËøêË°å main ÁöÑÁªìÊûúÊòØËæìÂá∫ÔºöHello world\nËÆ©Êàë‰ª¨Êù•Ê£ÄÊü•‰∏Ä‰∏ã main ÁöÑÈìæÊé•ÊÉÖÂÜµ:\n1 ldd src/main\t#ÂèØ‰ª•Áî®‰æÜÂ∞ãÊâæÊ≠§Âü∑Ë°åÊ™îÈèàÊé•‰∫ÜÂì™‰∏Ä‰∫õÂáΩÂºèÂ∫´ 1 2 3 4 linux-gate.so.1 =\u0026gt; (0xb7ee7000) libhello.so.1 =\u0026gt; /usr/lib/libhello.so.1 (0xb7ece000) libc.so.6 =\u0026gt; /lib/libc.so.6 (0xb7d77000) /lib/ld-linux.so.2 (0xb7ee8000) ÂèØ‰ª•Ê∏ÖÊ•öÁöÑÁúãÂà∞ main Á°ÆÂÆûÈìæÊé•‰∫ÜÂÖ±‰∫´Â∫ì libhelloÔºåËÄå‰∏îÈìæÊé•ÁöÑÊòØÂä®ÊÄÅÂ∫ì libhello.so.1\nÈÇ£Â¶Ç‰ΩïÈìæÊé•Âà∞ÈùôÊÄÅÂ∫ìÂë¢? ÊñπÊ≥ïÂæàÁÆÄÂçï: Â∞Ü TARGET_LINK_LIBRRARIES Êåá‰ª§‰øÆÊîπ‰∏∫:\n1 TARGET_LINK_LIBRARIES(main libhello.a) ÈáçÊñ∞ÊûÑÂª∫ÂêéÂÜçÊù•Áúã‰∏Ä‰∏ã main ÁöÑÈìæÊé•ÊÉÖÂÜµÔºö\n1 ldd src/main 1 2 3 linux-gate.so.1 =\u0026gt; (0xb7fa8000) libc.so.6 =\u0026gt; /lib/libc.so.6 (0xb7e3a000) /lib/ld-linux.so.2 (0xb7fa9000) ËØ¥Êòé,main Á°ÆÂÆûÈìæÊé•Âà∞‰∫ÜÈùôÊÄÅÂ∫ì libhello.a\nÁâπÊÆäÁöÑÁéØÂ¢ÉÂèòÈáè: CMAKE_INCLUDE_PATH Âíå CMAKE_LIBRARY_PATH Âä°ÂøÖÊ≥®ÊÑè,Ëøô‰∏§‰∏™ÊòØÁéØÂ¢ÉÂèòÈáèËÄå‰∏çÊòØ cmake ÂèòÈáè„ÄÇ ‰ΩøÁî®ÊñπÊ≥ïÊòØË¶ÅÂú® bash ‰∏≠Áî® export ÊàñËÄÖÂú® csh ‰∏≠‰ΩøÁî® set ÂëΩ‰ª§ËÆæÁΩÆÊàñËÄÖ CMAKE_INCLUDE_PATH=/home/include cmake ..Á≠âÊñπÂºè„ÄÇ\nËøô‰∏§‰∏™ÂèòÈáè‰∏ªË¶ÅÊòØÁî®Êù•Ëß£ÂÜ≥‰ª•Ââç autotools Â∑•Á®ã‰∏≠ --extra-include-dir Á≠âÂèÇÊï∞ÁöÑÊîØÊåÅÁöÑ„ÄÇ‰πüÂ∞±ÊòØ,Â¶ÇÊûúÂ§¥Êñá‰ª∂Ê≤°ÊúâÂ≠òÊîæÂú®Â∏∏ËßÑË∑ØÂæÑ (/usr/include, /usr/local/include Á≠â)ÔºåÂàôÂèØ‰ª•ÈÄöËøáËøô‰∫õÂèòÈáèÂ∞±Ë°åÂº•Ë°•„ÄÇ\nÊàë‰ª¨‰ª•Êú¨‰æã‰∏≠ÁöÑ hello.h ‰∏∫‰æãÔºåÂÆÉÂ≠òÊîæÂú®**/usr/include/hello** ÁõÆÂΩï,ÊâÄ‰ª•Áõ¥Êé•Êü•ÊâæËÇØÂÆöÊòØÊâæ‰∏çÂà∞ÁöÑ„ÄÇÂâçÈù¢Êàë‰ª¨Áõ¥Êé•‰ΩøÁî®‰∫Ü**ÁªùÂØπË∑ØÂæÑ****INCLUDE_DIRECTORIES(/usr/include/hello)**ÂëäËØâÂ∑•Á®ãËøô‰∏™Â§¥Êñá‰ª∂ÁõÆÂΩï„ÄÇ\n‰∏∫‰∫ÜÂ∞ÜÁ®ãÂ∫èÊõ¥Êô∫ËÉΩ‰∏ÄÁÇπ,Êàë‰ª¨ÂèØ‰ª•‰ΩøÁî® **CMAKE_INCLUDE_PATH**Êù•ËøõË°åÔºå‰ΩøÁî® bash ÁöÑÊñπÊ≥ïÂ¶Ç‰∏ãÔºö\n1 export CMAKE_INCLUDE_PATH=/usr/include/hello ÁÑ∂ÂêéÂú®Â§¥Êñá‰ª∂‰∏≠Â∞Ü **INCLUDE_DIRECTORIES(/usr/include/hello)**ÊõøÊç¢‰∏∫Ôºö\n1 2 3 4 FIND_PATH(myHeader hello.h) IF(myHeader) INCLUDE_DIRECTORIES(${myHeader}) ENDIF(myHeader) ‰∏äËø∞ÁöÑ‰∏Ä‰∫õÊåá‰ª§Êàë‰ª¨Âú®ÂêéÈù¢‰ºö‰ªãÁªç„ÄÇËøôÈáåÁÆÄÂçïËØ¥Êòé‰∏Ä‰∏ãÔºåFIND_PATH Áî®Êù•Âú®ÊåáÂÆöË∑ØÂæÑ‰∏≠ÊêúÁ¥¢Êñá‰ª∂ÂêçÔºåÊØîÂ¶Ç: FIND_PATH(myHeader NAMES hello.h PATHS /usr/include /usr/include/hello)\nËøôÈáåÊàë‰ª¨Ê≤°ÊúâÊåáÂÆöË∑ØÂæÑÔºå‰ΩÜÊòØÔºåcmake ‰ªçÁÑ∂ÂèØ‰ª•Â∏ÆÊàë‰ª¨ÊâæÂà∞ hello.h Â≠òÊîæÁöÑË∑ØÂæÑÔºåÂ∞±ÊòØÂõ†‰∏∫Êàë‰ª¨ËÆæÁΩÆ‰∫ÜÁéØÂ¢ÉÂèòÈáè CMAKE_INCLUDE_PATH„ÄÇ\nÂ¶ÇÊûú‰Ω†‰∏ç‰ΩøÁî® FIND_PATHÔºå**CMAKE_INCLUDE_PATH**ÂèòÈáèÁöÑËÆæÁΩÆÊòØÊ≤°Êúâ‰ΩúÁî®ÁöÑÔºå‰Ω†‰∏çËÉΩÊåáÊúõÂÆÉ‰ºöÁõ¥Êé•‰∏∫ÁºñËØëÂô®ÂëΩ‰ª§Ê∑ªÂä†ÂèÇÊï∞ -I\u0026lt;CMAKE_INCLUDE_PATH\u0026gt;„ÄÇ\n‰ª•Ê≠§‰∏∫‰æãÔºåCMAKE_LIBRARY_PATH ÂèØ‰ª•Áî®Âú® FIND_LIBRARY ‰∏≠„ÄÇ\nÂêåÊ†∑,Âõ†‰∏∫Ëøô‰∫õÂèòÈáèÁõ¥Êé•‰∏∫ FIND_Êåá‰ª§ÊâÄ‰ΩøÁî®,ÊâÄ‰ª•ÊâÄÊúâ‰ΩøÁî® FIND_Êåá‰ª§ÁöÑ cmake Ê®°ÂùóÈÉΩ‰ºöÂèóÁõä„ÄÇ\n","date":"2021-01-06T00:00:00Z","image":"https://picx.zhimg.com/v2-42db546bd0f2ffdd462636ddd87ecdc3_1440w.jpg?source=172ae18b","permalink":"https://zichen34.github.io/writenotes/lang/cmake_practice/","title":"memo: CMakeÂÆûË∑µ"},{"content":"Frequency folding\nThe sampled signal\u0026rsquo;s frequency spectrum is mirrored by the sample frequency. 1-Steve\nRef Shannon Nyquist Sampling Theorem ","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aliasing/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/img/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/splat/img/","title":"image"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/imgs/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/mvs/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/img/","title":"img"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/diffrender/img/","title":"img"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/img/","title":"img"},{"content":"Source Video: Reflection laws proof using Huygen\u0026rsquo;s principle | Wave optics | Physics | Khan Academy\n","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/waveoptics/khan_waveoptics/","title":"watch: WaveOptics - Khan Academy India"}]