[{"content":"camtools (2024-03-28)\nyxlao/camtools - Github\nPlot the 49 camera poses in DTU:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import numpy as np import os def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min import camtools as ct import open3d as o3d Ks, Ts= [], [] for i in range(49): intrinsics, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan1\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) Ks.append(intrinsics) Ts.append(extrinsics) cameras = ct.camera.create_camera_frames(Ks, Ts) o3d.visualization.draw_geometries([cameras]) Result Demo in README Don\u0026rsquo;t know how to add camera frames like his. 1 2 3 4 5 6 0 1 2 3 4 10 9 8 7 6 5 11 12 13 14 15 16 17 18 27 26 25 24 23 22 21 20 19 28 29 30 31 32 33 34 35 36 37 48 47 46 45 44 43 42 41 40 39 38 CameraViewer (2024-03-28)\nxt4d/CameraViewer found by DDG when searching \u0026ldquo;how to visualize camera pose\u0026rdquo;\nIt uses plotly to visualize cameras internally.\n(2024-03-28)\nTest the poses in DTU.\nThe extrinsics (w2c) don\u0026rsquo;t appear on the canvas if keeping the translation vectors.\nI set the translation (camera pose), i.e., 4th column in extrinsics to all 0, then the rotations are shown.\n1 2 3 4 5 # /mnt/data2_z/MVSNet_testing/dtu/scan1/cams/{0:08d}_cam.txt array([[ 0.970263 , 0.00747983, 0.241939 , 0. ], [-0.0147429 , 0.999493 , 0.0282234 , 0. ], [-0.241605 , -0.030951 , 0.969881 , 0. ]], dtype=float32) The reason could be that the translation vector is too large: [-191.02, 3.28832, 22.5401].\nBy reducing it by 1/100 times (extrinsics[:,3] = extrinsics[:,3]/100): [-1.9102, 0.0328832, 0.225401], the camera appears.\nThe w2c (extrinsics) can be prepared as npy files:\n1 2 3 4 5 for i in range(49): intrinsics, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan1\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) extrinsics = extrinsics[:3] extrinsics[:,3] = extrinsics[:,3]/100 np.save(f\u0026#39;/mnt/data2_z/Poses_CamViewer/obj/poses/{i:03d}\u0026#39;, extrinsics) BTW, writing json file manually is a time black hole.\nAs long as the filenames of poses and images are the same, it\u0026rsquo;s okay. The indexing doesn\u0026rsquo;t matter.\n1 ~/Downloads/CameraViewer$ python app.py --root /mnt/data2_z/Poses_CamViewer/obj/ --type w2c --image_size 128 If omitting the argument --type, the program will use poses.json. Otherwise, the program will read directories: poses/ and images/. 49 cameras for scan1\nThe above figure shows the original poses. And the principal axis is facing away from the object. pytransform3d (2024-03-29)\nPlot mesh and cameras:\nVisualizing camera trajectory in Open3D #148 (Found when searching \u0026ldquo;open3d visualize camera poses\u0026rdquo; DDG)\nThe mesh in the image is produced by Meshroom. And then use Figure.plot_camera()\npytransform3d.camera.plot_camera() Example\nCode for plotting pose 1 of DTU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import matplotlib.pyplot as plt import pytransform3d.camera as pc import pytransform3d.transformations as pt w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) c2w = np.linalg.inv(w2c) intrinsics = np.array([ [ 2.89233051e+03, -2.48063349e-04, 8.23205273e+02], [ 0.00000000e+00, 2.88317528e+03, 6.19070918e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]) sensor_size = np.array([1600, 1200]) # image size virtual_image_distance = 1 ax = pt.plot_transform(A2B=c2w, s=0.2) ax.set_xlim(186, 188) ax.set_ylim(3, 5) ax.set_zlim(20, 22) pc.plot_camera( ax, cam2world=c2w, M=intrinsics, sensor_size=sensor_size, virtual_image_distance=virtual_image_distance) plt.show() pytransform3d matplotlib In the matplotlib code, I have corrected the camera position to -extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1]. The 2 results are the same. Plot basis and camera plane:\nCamera Extrinsic Matrix with Example in Python - Part2\n1 2 3 4 5 6 7 # plot the global basis and the transformed camera basis ax = pr.plot_basis(ax) ax = pr.plot_basis(ax, R, offset) # plot the original and transformed image plane ax.plot_surface(xx, yy, Z, alpha=0.75) ax.plot_surface(xxt, yyt, Zt, alpha=0.75) Matplotlib (2024-03-30)\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 %matplotlib widget import os import numpy as np def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min poses_list = [] for i in range(49): _, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan23\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) poses_list.append({ # \u0026#34;position\u0026#34;: extrinsics[:,-1][:-1], # Wrong \u0026#34;position\u0026#34;: - extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1], \u0026#34;rotation\u0026#34;: extrinsics[:-1][:,:-1], }) import matplotlib.pyplot as plt from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection def plot_camera_poses(poses, axis_length=0.1): fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # Plot each camera pose for pose in poses: # Extract camera position and orientation cam_position = pose[\u0026#39;position\u0026#39;] cam_orientation = pose[\u0026#39;rotation\u0026#39;] # Plot camera position ax.scatter(cam_position[0], cam_position[1], cam_position[2], c=\u0026#39;r\u0026#39;, marker=\u0026#39;.\u0026#39;) axes_endpoints = cam_position + axis_length * cam_orientation # Plot camera canvas determined by 4 corners (forming 2 triangles) corner_back_1 = axes_endpoints[2] + 0.3*axis_length * cam_orientation[1] + 0.3*axis_length * cam_orientation[0] corner_back_2 = axes_endpoints[2] - 0.3*axis_length * cam_orientation[1] + 0.3*axis_length * cam_orientation[0] corner_back_3 = axes_endpoints[2] + 0.3*axis_length * cam_orientation[1] - 0.3*axis_length * cam_orientation[0] corner_back_4 = axes_endpoints[2] - 0.3*axis_length * cam_orientation[1] - 0.3*axis_length * cam_orientation[0] verts = np.array([[ corner_back_1, corner_back_2, corner_back_4]]) tri = Poly3DCollection(verts, alpha=0.3, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) verts = np.array([[ corner_back_1, corner_back_3, corner_back_4]]) tri = Poly3DCollection(verts, alpha=0.3, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) # Hull of the camera ax.plot3D([cam_position[0], corner_back_1[0]], [cam_position[1], corner_back_1[1]], [cam_position[2], corner_back_1[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_2[0]], [cam_position[1], corner_back_2[1]], [cam_position[2], corner_back_2[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_3[0]], [cam_position[1], corner_back_3[1]], [cam_position[2], corner_back_3[2]], \u0026#39;gray\u0026#39;) ax.plot3D([cam_position[0], corner_back_4[0]], [cam_position[1], corner_back_4[1]], [cam_position[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) # Camera plane edges ax.plot3D([corner_back_1[0], corner_back_2[0]], [corner_back_1[1], corner_back_2[1]], [corner_back_1[2], corner_back_2[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_3[0], corner_back_1[0]], [corner_back_3[1], corner_back_1[1]], [corner_back_3[2], corner_back_1[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_3[0], corner_back_4[0]], [corner_back_3[1], corner_back_4[1]], [corner_back_3[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) ax.plot3D([corner_back_2[0], corner_back_4[0]], [corner_back_2[1], corner_back_4[1]], [corner_back_2[2], corner_back_4[2]], \u0026#39;gray\u0026#39;) lines = [] for idx in range(len(poses)-1): lines.append([poses[idx][\u0026#39;position\u0026#39;], poses[idx+1][\u0026#39;position\u0026#39;]]) # Create a line collection lc = Line3DCollection(lines, colors=\u0026#39;b\u0026#39;, linewidths=1, label=\u0026#39;Camera Trajectory\u0026#39;) ax.add_collection3d(lc) # Set plot limits and labels ax.set_box_aspect([1, 1, 1]) ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) plt.show() return ax # Plot camera poses ax_prev = plot_camera_poses(poses_list, axis_length=60) # Plot point cloud import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/mnt/data2_z/SampleSet/MVS Data/Points/stl/stl023_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vs = np.asarray(pcd.points) samples = vs[np.random.choice(vs.shape[0],100)] x = samples[:,0] y = samples[:,1] z = samples[:,2] ax_prev.scatter(x, y, z, color=\u0026#39;gray\u0026#39;, alpha=0.4) The Line3DCollection usage is seen from an chatGPT-generated anwser: How to visualize colmap export that Camera parameters -SO open3d camera hull (2024-03-29)\ncreate_camera_visualization()\nSample code: Is there a way to draw a camera in a visualizer? #3876 (Found when searching \u0026ldquo;open3d draw cameras\u0026rdquo; DDG)\ncamera moves (2024-03-31)\nSetting the extrinsic matrix for ViewControl #2121 - Open3D (surfaced by DDG searching \u0026ldquo;open3d camera extrinsic set_extrinsic\u0026rdquo;)\nIterate multiple camera poses (extrinsics):\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np import os def read_cam_file(filename): with open(filename) as f: lines = [line.rstrip() for line in f.readlines()] # extrinsics: line [1,5), 4x4 matrix extrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[1:5]), dtype=np.float32, sep=\u0026#39; \u0026#39;) extrinsics = extrinsics.reshape((4, 4)) # intrinsics: line [7-10), 3x3 matrix intrinsics = np.fromstring(\u0026#39; \u0026#39;.join(lines[7:10]), dtype=np.float32, sep=\u0026#39; \u0026#39;) intrinsics = intrinsics.reshape((3, 3)) # depth_min \u0026amp; depth_interval: line 11 depth_min = float(lines[11].split()[0]) return intrinsics, extrinsics, depth_min poses_list = [] for i in range(49): _, extrinsics, _ = read_cam_file(os.path.join(\u0026#39;/mnt/data2_z/MVSNet_testing/dtu\u0026#39;,\u0026#39;scan23\u0026#39;, f\u0026#39;cams/{i:08d}_cam.txt\u0026#39;)) poses_list.append({ \u0026#34;position\u0026#34;: - extrinsics[:-1][:,:-1].T @ extrinsics[:,-1][:-1], \u0026#34;rotation\u0026#34;: extrinsics[:-1][:,:-1], \u0026#34;extrinsics\u0026#34;: extrinsics, }) import time import itertools import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window(window_name=\u0026#34;Playback\u0026#34;, visible=True) vis.get_render_option().background_color = np.asarray([0, 0, 0]) vis.add_geometry(pcd) ctr = vis.get_view_control() cam = ctr.convert_to_pinhole_camera_parameters() for view_idx, camParams in zip(itertools.count(), poses_list): cam.extrinsic = camParams[\u0026#39;extrinsics\u0026#39;] ctr.convert_from_pinhole_camera_parameters(cam, True) vis.poll_events() vis.update_renderer() time.sleep(0.1) vis.destroy_window() Note: The argument allow_arbitrary=True is required in convert_from_pinhole_camera_parameters(cam, True) (using 0.18.0), Custom Animation Customized visualization - Open3D Docs\nCode from View Control No Effect in 0.17 #6098\n1 2 3 4 5 6 7 8 9 import open3d as o3d def rotate(vis): ctr = vis.get_view_control() ctr.rotate(5, 0) return False frame = o3d.geometry.TriangleMesh.create_coordinate_frame() o3d.visualization.draw_geometries_with_animation_callback([frame], rotate) Others demul/extrinsic2pyramid\nOpenCV has example code. How to plot the camera and image positions from camera calibration data? pytorch3d/docs/tutorials/utils/camera_visualization.\nsxyu/nerfvis: NeRF visualization library under construction\nBlender add-on: Photogrammetry-Importer How to visualize colmap export \u0026lsquo;images.txt\u0026rsquo; in blender? -SO\nkaolin.render.camera — Kaolin documentation - Read the Docs\nWebGL Visualizing the Camera\nOpenCV (2024-04-01)\nOpenCV: cv::viz::WCameraPosition Class Reference\nDraw coordinates axes\nHow to draw 3D Coordinate Axes with OpenCV for face pose estimation? - SO\n1 2 scale = 0.1 img = cv2.drawFrameAxes(img, K, distortion, rotation_vec, translation_vec, scale) ","date":"2024-03-28T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/camera_plots/","title":"memo: Vis | Camera Poses Visualization"},{"content":"plyfile (2024-03-27)\nRead ply file (Polygon File Format): Docs\n1 2 3 4 5 6 7 import numpy from plyfile import PlyData, PlyElement with open(\u0026#39;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#39;, \u0026#39;rb\u0026#39;) as f: plydata = PlyData.read(f) np.array(plydata.elements[0].data)[0] Output:\n1 (49.720848, -54.11675, 672.04956, 0.9649841, -0.08213623, -0.24911714, 102, 70, 44) The returned tuple is a single data, whose datatype has 9 members. I want to only take the first 3 values: x,y,z. But it\u0026rsquo;s not allowed to use syntax like [:3] to slice it.\nWrite a ply file\nExample in 3DGS:\nopen3d (2024-03-27)\nCompare open3d, plyfile, pyntcloud, and meshio\nopen3d: good docs. How To Read and Write ply File in Python - Jdhao plyfile: lightweight. Could be slow when reading a large ply file. Python plyfile vs pymesh - SO open3d read .ply file Docs\n1 2 3 import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) Convert pcd to np.array and visualize it: Docs\n1 2 3 4 import numpy as np xyz_load = np.asarray(pcd.points) # (2880879, 3) print(f\u0026#39;xyz_load:\\n {xyz_load}\u0026#39;) o3d.visualization.draw_geometries([pcd]) Setting camera directions:\n1 2 3 4 lookat = np.array([[500.],[500.], [500.]]) up = np.array([[0.853452],[-0.447425], [0.267266]]) front = np.array([[0.417749],[0.893913],[0.162499]]) o3d.visualization.draw_geometries([pcd], width=500, height=500, lookat=lookat, up=up, front=front, zoom=1.0) open3d.visualization.draw_geometries\nlookat is the window center. set_lookat(), set_front(), set_up() usage of VisualControl #2139\nVisualize point cloud from a specified camera pose:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([1,1,1]) vis.add_geometry(pcd) view_ctl = vis.get_view_control() w2c = np.array([[-0.636298, -0.727666, 0.25618, -143.534], [0.0315712, 0.307237, 0.951109, -579.42], [-0.770797, 0.613276, -0.172521, 759.831], [ 0.0, 0.0, 0.0, 1.0]]) # cam 38 cam = view_ctl.convert_to_pinhole_camera_parameters() cam.extrinsic = w2c view_ctl.convert_from_pinhole_camera_parameters(cam, True) vis.run() vis.destroy_window() Others pyminiply claims that it\u0026rsquo;s faster than open3d.\nply-parser\nStitch (2024-04-01)\nStitching point clouds from multiple cameras - camcalib\nWis3D (2024-04-07)\nzju3dv/Wis3D\n","date":"2024-03-27T18:14:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pointcloud_libs/","title":"memo: Libs for Point Clouds Data"},{"content":"NeRF Synthetic The dataset nerf/data/nerf_synthetic/lego composes of 3 things:\nRGB images (train/),\npoint cloud (points3d.ply),\nassociated camera poses (transforms_train.json), where each frame contains 3 attributes: file_path, rotation, and c2w transform_matrix\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \u0026#34;camera_angle_x\u0026#34;: 0.6911112070083618, \u0026#34;frames\u0026#34;: [ { \u0026#34;file_path\u0026#34;: \u0026#34;./train/r_0\u0026#34;, \u0026#34;rotation\u0026#34;: 0.012566370614359171, \u0026#34;transform_matrix\u0026#34;: [ [ -0.9999021887779236, 0.004192245192825794, -0.013345719315111637, -0.05379832163453102 ], [ -0.013988681137561798, -0.2996590733528137, 0.95394366979599, 3.845470428466797 ], [ -4.656612873077393e-10, 0.9540371894836426, 0.29968830943107605, 1.2080823183059692 ], [ 0.0, 0.0, 0.0, 1.0 ] ] }, Colmap Sparse (2024-03-14)\nBlender Add-On (2024-03-20)\nmaximeraafat/BlenderNeRF - Github Found by DDG searching \u0026ldquo;how to make nerf blender\u0026rdquo; AI RENDERING from Blender to NeRF | BlenderNeRF Tutorial - Youtube NeRFStudio Nerfstudio-Docs Creating VFX with NeRFs - Nerfstudio Blender Add-On Tutorial - Youtube DTU (2024-03-21)\nAdapt the DTU dataset to the NeRF synthetic style dataset format. Concretely, the extrinsics (w2c) in DTU (aligned with OpenCV) are required to be modified to c2w of OpenGL.\nDTU dataset already provides point cloud (ply) and multi-view images.\n(2024-04-02)\nThe extrinsics in DTU is w2c. So, c2w = np.linalg.inv(extrinsics)\nAnd because the camera coordinate system in OpenCV (used by DTU) is RDF, the 4 columns in the c2w are [Right | Down | Front | CamCenter]\nHowever, in OpenGL, the c2w matrix should be: [Right | Up | Back | CamCenter]\n1 2 3 4 5 6 7 8 9 Trsfm = np.array([[ 9.70263e-01, 7.47983e-03, 2.41939e-01, -1.91020e+02], [-1.47429e-02, 9.99493e-01, 2.82234e-02, 3.28832e+00], [-2.41605e-01,-3.09510e-02, 9.69881e-01, 2.25401e+01], [ 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00]]) c2w_opencv = np.linalg.inv(Trsfm) print(c2w_opencv) c2w_opengl = c2w_opencv c2w_opengl[:,1] = - c2w_opencv[:,1] c2w_opengl[:,2] = - c2w_opencv[:,2] The other way is adjusting w2c: Each row in the rotation matrix of w2c is a camera axes. And the 4-th column is $-R_{c2w}^{-1} t_{c2w} = - R_{w2c} C$, where the C is the camera center.\nTo align with the OpenGL camera coord. system (from RDF to RUB), the 2nd and 3rd rows in w2c need flips:\n1 2 3 4 5 w2c_opencv = Trsfm w2c_opencv[1] *=-1 w2c_opencv[2] *=-1 c2w_opengl_ = np.linalg.inv(w2c_opencv) print((c2w_opengl_ ==c2w_opengl).all()) The c2w_opengl_ equals to the c2w_opengl above.\n","date":"2024-03-20T12:31:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-test-make_dataset/","title":"test: NeRF | Make Datasets"},{"content":"A sparse reconstruction model consists of 3 .txt files: cameras.txt, images.txt, points3D.txt Docs\nCode for writing data: colmap / scripts / python / read_write_model.py\ncameras.txt\nEach line is the intrinsic parameters of a camera.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def write_cameras_text(cameras, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::WriteCamerasText(const std::string\u0026amp; path) void Reconstruction::ReadCamerasText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; HEADER = ( \u0026#34;# Camera list with one line of data per camera:\\n\u0026#34; + \u0026#34;# CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\\n\u0026#34; + \u0026#34;# Number of cameras: {}\\n\u0026#34;.format(len(cameras)) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, cam in cameras.items(): to_write = [cam.id, cam.model, cam.width, cam.height, *cam.params] line = \u0026#34; \u0026#34;.join([str(elem) for elem in to_write]) fid.write(line + \u0026#34;\\n\u0026#34;) images.txt\nOne camera has two lines. The 1-st line includes extrinsic parameters. The 2-nd line lists the coordinates of 2D keypoints.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def write_images_text(images, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::ReadImagesText(const std::string\u0026amp; path) void Reconstruction::WriteImagesText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; if len(images) == 0: mean_observations = 0 else: mean_observations = sum( (len(img.point3D_ids) for _, img in images.items()) ) / len(images) HEADER = ( \u0026#34;# Image list with two lines of data per image:\\n\u0026#34; + \u0026#34;# IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\\n\u0026#34; + \u0026#34;# POINTS2D[] as (X, Y, POINT3D_ID)\\n\u0026#34; + \u0026#34;# Number of images: {}, mean observations per image: {}\\n\u0026#34;.format( len(images), mean_observations ) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, img in images.items(): image_header = [ img.id, *img.qvec, *img.tvec, img.camera_id, img.name, ] first_line = \u0026#34; \u0026#34;.join(map(str, image_header)) fid.write(first_line + \u0026#34;\\n\u0026#34;) points_strings = [] for xy, point3D_id in zip(img.xys, img.point3D_ids): points_strings.append(\u0026#34; \u0026#34;.join(map(str, [*xy, point3D_id]))) fid.write(\u0026#34; \u0026#34;.join(points_strings) + \u0026#34;\\n\u0026#34;) points3D.txt\nEach line is a 3D point.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def write_points3D_text(points3D, path): \u0026#34;\u0026#34;\u0026#34; see: src/colmap/scene/reconstruction.cc void Reconstruction::ReadPoints3DText(const std::string\u0026amp; path) void Reconstruction::WritePoints3DText(const std::string\u0026amp; path) \u0026#34;\u0026#34;\u0026#34; if len(points3D) == 0: mean_track_length = 0 else: mean_track_length = sum( (len(pt.image_ids) for _, pt in points3D.items()) ) / len(points3D) HEADER = ( \u0026#34;# 3D point list with one line of data per point:\\n\u0026#34; + \u0026#34;# POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\\n\u0026#34; + \u0026#34;# Number of points: {}, mean track length: {}\\n\u0026#34;.format( len(points3D), mean_track_length ) ) with open(path, \u0026#34;w\u0026#34;) as fid: fid.write(HEADER) for _, pt in points3D.items(): point_header = [pt.id, *pt.xyz, *pt.rgb, pt.error] fid.write(\u0026#34; \u0026#34;.join(map(str, point_header)) + \u0026#34; \u0026#34;) track_strings = [] for image_id, point2D in zip(pt.image_ids, pt.point2D_idxs): track_strings.append(\u0026#34; \u0026#34;.join(map(str, [image_id, point2D]))) fid.write(\u0026#34; \u0026#34;.join(track_strings) + \u0026#34;\\n\u0026#34;) Problems I didn\u0026rsquo;t manage to realize the method as the following problems:\nEach image entry requires specify 2D keypoints.\nNot sure how to obtain them yet.\nAre the X-Y-Z axes of DTU camera aligned with right-bottom-front in COLMAP ?\nThis may require visulization to verify.\nInstall Ubuntu 22.04 (2024-04-17)\nOS: Ubuntu 22.04.4 LTS x86_64, Kernel: 6.5.0-27-generic\nInstall dependecies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 sudo apt-get install \\ git \\ cmake \\ ninja-build \\ build-essential \\ libboost-program-options-dev \\ libboost-filesystem-dev \\ libboost-graph-dev \\ libboost-system-dev \\ libeigen3-dev \\ libflann-dev \\ libfreeimage-dev \\ libmetis-dev \\ libgoogle-glog-dev \\ libgtest-dev \\ libsqlite3-dev \\ libglew-dev \\ qtbase5-dev \\ libqt5opengl5-dev \\ libcgal-dev \\ libceres-dev Error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 The following packages have unmet dependencies: libqt5opengl5 : Depends: qtbase-abi-5-15-3 qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) E: Unable to correct problems, you have held broken packages. Some posts suggests that the libqt5opengl5 has already been included into qt\nReference:\ncannot install qtbase-abi-5-5-1 on ubuntu 17.10 -SO\n1 2 (base) zichen@homepc:~$ apt-cache search qtbase-abi libqt5core5a - Qt 5 core module ubuntu22.04安装软件出现qtbase错误_qtbase-abi-5-15-3-CSDN博客\n这依赖捞不着啊- Community - Deepin Technology\nI want to skip those 2 packages and compile directly. However, got error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 (base) zichen@homepc:~/Downloads/colmap/build$ cmake .. -GNinja -- The C compiler identification is GNU 9.5.0 -- The CXX compiler identification is GNU 9.5.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found version \u0026#34;1.74.0\u0026#34;) found components: filesystem graph program_options system CMake Error at cmake/FindFLANN.cmake:89 (message): Could not find FLANN Call Stack (most recent call first): cmake/FindDependencies.cmake:17 (find_package) CMakeLists.txt:96 (include) -- Configuring incomplete, errors occurred! Found this issue: Colmap cmake .. error #1451\nI indeed have not installed the libmetis-dev and other packages except for the above 2 missing packages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sudo apt-get install \\ git \\ cmake \\ ninja-build \\ build-essential \\ libboost-program-options-dev \\ libboost-filesystem-dev \\ libboost-graph-dev \\ libboost-system-dev \\ libeigen3-dev \\ libflann-dev \\ libfreeimage-dev \\ libmetis-dev \\ libgoogle-glog-dev \\ libgtest-dev \\ libsqlite3-dev \\ libglew-dev \\ libcgal-dev \\ libceres-dev Linking failed:\n1 2 3 4 5 6 7 8 9 /home/zichen/anaconda3/lib/libQt5OpenGL.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Widgets.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Gui.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2 -lcudadevrt -lcudart_static -lrt -lpthread -ldl \u0026amp;\u0026amp; : /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldTag@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldName@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldReadCount@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldPassCount@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `TIFFFieldDataType@LIBTIFF_4.0\u0026#39; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libfreeimage.so: undefined reference to `_TIFFDataSize@LIBTIFF_4.0\u0026#39; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. Uninstall libtiff\nAccording to this issue: /usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/libfreeimage.so: undefined reference to `_TIFFDataSize@LIBTIFF_4.0\u0026rsquo; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. #1803\n1 2 3 4 ninja clean conda uninstall libtiff cmake .. -GNinja ninja I have libtiff:\n1 2 3 4 5 (base) zichen@homepc:~/Downloads/colmap/build$ conda list libtiff # packages in environment at /home/zichen/anaconda3: # # Name Version Build Channel libtiff 4.5.0 h6a678d5_2 https://repo.anaconda.com/pkgs/main Another error about libQt5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 [213/213] Linking CXX executable src/colmap/exe/colmap FAILED: src/colmap/exe/colmap : \u0026amp;\u0026amp; /usr/bin/c++ -Wno-maybe-uninitialized -Wall -O3 -DNDEBUG src/colmap/exe/CMakeFiles/colmap_main.dir/feature.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/sfm.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/colmap.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/database.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/gui.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/image.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/model.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/mvs.cc.o src/colmap/exe/CMakeFiles/colmap_main.dir/vocab_tree.cc.o -o src/colmap/exe/colmap -L/usr/local/cuda-11.6/targets/x86_64-linux/lib/stubs -L/usr/local/cuda-11.6/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-11.6/targets/x86_64-linux/lib:/home/zichen/anaconda3/lib: src/colmap/controllers/libcolmap_controllers.a src/colmap/retrieval/libcolmap_retrieval.a src/colmap/scene/libcolmap_scene.a src/colmap/sfm/libcolmap_sfm.a src/colmap/util/libcolmap_util.a src/colmap/util/libcolmap_util_cuda.a src/colmap/mvs/libcolmap_mvs_cuda.a src/colmap/ui/libcolmap_ui.a src/colmap/util/libcolmap_util_cuda.a src/colmap/controllers/libcolmap_controllers.a src/colmap/retrieval/libcolmap_retrieval.a src/colmap/sfm/libcolmap_sfm.a src/colmap/mvs/libcolmap_mvs.a src/thirdparty/PoissonRecon/libcolmap_poisson_recon.a /usr/lib/x86_64-linux-gnu/libgmpxx.so /usr/lib/x86_64-linux-gnu/libmpfr.so /usr/lib/x86_64-linux-gnu/libgmp.so src/colmap/estimators/libcolmap_estimators.a src/colmap/feature/libcolmap_feature.a /usr/lib/x86_64-linux-gnu/libflann.so /usr/lib/x86_64-linux-gnu/liblz4.so src/thirdparty/SiftGPU/libcolmap_sift_gpu.a /usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-11.6/targets/x86_64-linux/lib/libcurand.so /usr/lib/x86_64-linux-gnu/libGLEW.so src/colmap/optim/libcolmap_optim.a /usr/lib/x86_64-linux-gnu/libboost_program_options.so.1.74.0 src/colmap/image/libcolmap_image.a src/colmap/scene/libcolmap_scene.a src/colmap/feature/libcolmap_feature_types.a src/colmap/geometry/libcolmap_geometry.a src/colmap/math/libcolmap_math.a /usr/lib/x86_64-linux-gnu/libmetis.so /usr/lib/x86_64-linux-gnu/libboost_graph.so.1.74.0 /usr/lib/x86_64-linux-gnu/libboost_regex.so.1.74.0 src/colmap/sensor/libcolmap_sensor.a src/colmap/util/libcolmap_util.a /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.74.0 /usr/lib/x86_64-linux-gnu/libsqlite3.so /usr/lib/x86_64-linux-gnu/libGLX.so /usr/lib/x86_64-linux-gnu/libOpenGL.so /usr/lib/libceres.so.2.0.0 /usr/lib/x86_64-linux-gnu/libglog.so.0.4.0 /usr/lib/x86_64-linux-gnu/libunwind.so /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.2 -lpthread src/thirdparty/VLFeat/libcolmap_vlfeat.a /usr/lib/gcc/x86_64-linux-gnu/9/libgomp.so /usr/lib/x86_64-linux-gnu/libpthread.a /usr/lib/x86_64-linux-gnu/libfreeimage.so src/thirdparty/LSD/libcolmap_lsd.a /home/zichen/anaconda3/lib/libQt5OpenGL.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Widgets.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Gui.so.5.15.2 /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2 -lcudadevrt -lcudart_static -lrt -lpthread -ldl \u0026amp;\u0026amp; : /usr/bin/ld: warning: libicui18n.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libicuuc.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libicudata.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found (try using -rpath or -rpath-link) /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_errorName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_setMillis_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getAlias_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_inDaylightTime_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_strToLower_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getStandardName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_strToUpper_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_setSubstChars_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getMaxCharSize_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getTimeZoneDisplayName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_fromUnicode_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getDefaultName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getDefaultTimeZone_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_clone_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_getDSTSavings_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_strcoll_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_countAvailable_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openCountryTimeZones_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_compareNames_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_getAvailableName_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openTimeZoneIDEnumeration_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_open_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_setAttribute_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_openTimeZones_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `uenum_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_countAliases_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_close_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucol_getSortKey_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucal_get_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `uenum_next_58\u0026#39; /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `ucnv_toUnicode_58\u0026#39; collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. Reference:\nLinkage against libQt5Core - SO\nBuild from source error libQt5Core.so.5.15.2: undefined reference to #3829\nI already have libicu-dev installed.\nInstall qtbase5-dev\nI noticed the error said: \u0026ldquo;warning: libicudata.so.58, needed by /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2, not found\u0026rdquo;.\nAnd the binary couldn\u0026rsquo;t find symbols: \u0026ldquo;/home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to u_errorName_58'\u0026rdquo;\nI didn\u0026rsquo;t have Qt? Is this why there is the suggestion?\n1 sudo aptitude install qtbase5-dev Someone used aptitude to install the above 2 packages. Ubuntu20.04安装colmap从零开始全过程记录（包括CUDA/CUDNN/ceres/anaconda） - CSDN I forgot to check Qt before installing qtbase5: How to find Version of Qt? - SO\n1 2 3 (base) zichen@homepc:~/Downloads/colmap$ QT_SELECT=5 qmake -v QMake version 3.1 Using Qt version 5.15.2 in /home/zichen/anaconda3/lib (2024-04-19) I accepted the first solution provided by the aptitude. But, I didn\u0026rsquo;t notice that the 1st solution is to keep everything unchanged and give up installing qtbase5. Qt Pkgs Matter (2024-04-19)\nThe \u0026ldquo;undefined reference\u0026rdquo; errors persist:\n1 /usr/bin/ld: /home/zichen/anaconda3/lib/libQt5Core.so.5.15.2: undefined reference to `u_errorName_58\u0026#39; I uninstalled anaconda (removed ~/anaconda3 to /mnt), based on this answer: OpenCV undefined references for libQt5Core.so.5 - raggot (Found by search the above error).\nI tested later that rename the ~/anaconda3 works as well. 【超详细】安装了anaconda后，Ubuntu18+COLMAP配置疯狂踩坑踩至魔怔的记录 - CSDN Then cmake .. -GNinja. It could not find Qt5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CMake Error at cmake/FindDependencies.cmake:150 (find_package): By not providing \u0026#34;FindQt5.cmake\u0026#34; in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \u0026#34;Qt5\u0026#34;, but CMake did not find one. Could not find a package configuration file provided by \u0026#34;Qt5\u0026#34; (requested version 5.4) with any of the following names: Qt5Config.cmake qt5-config.cmake Add the installation prefix of \u0026#34;Qt5\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;Qt5_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;Qt5\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): CMakeLists.txt:96 (include) Qt5 has been existed on the system:\n1 2 3 zichen@homepc:~/Downloads/colmap$ QT_SELECT=5 qmake -v QMake version 3.1 Using Qt version 5.15.3 in /usr/lib/x86_64-linux-gnu Specifing path to Qt5 doesn\u0026rsquo;t work:\n1 2 3 cmake .. -DQt5_DIR=/usr/lib/x86_64-linux-gnu/ -GNinja cmake .. -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/ -GNinja cmake -B ./build -DCMAKE_PREFIX_PATH=\u0026#34;/usr/lib/x86_64-linux-gnu/qt5;/usr/lib/qt5;/usr/share/qt5\u0026#34; -GNinja I really don\u0026rsquo;t have Qt?\n\u0026ldquo;ubuntu check qt version\u0026rdquo; DDG\n1 2 zichen@homepc:~/Downloads/colmap$ qmake -v qmake: could not find a Qt installation of \u0026#39;\u0026#39; The following packages already existed on the system: (qt4 - How to find Version of Qt? - SO)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 zichen@homepc:~/Downloads/colmap$ dpkg -l | grep qt ri libfcitx-qt5-1:amd64 1.2.7-1.2build1 amd64 Free Chinese Input Toy of X - D-Bus client libraries for Qt5 ri libfcitx-qt5-data 1.2.7-1.2build1 all Free Chinese Input Toy of X - data files for Qt5 integration ii libgsettings-qt1:amd64 0.2-4 amd64 library to access GSettings from Qt (shared libraries) ii libqt5core5a:amd64 5.15.8.1-1+dde amd64 Qt 5 core module ii libqt5core5a:i386 5.15.8.1-1+dde i386 Qt 5 core module ii libqt5dbus5:amd64 5.15.8.1-1+dde amd64 Qt 5 D-Bus module ii libqt5dbus5:i386 5.15.8.1-1+dde i386 Qt 5 D-Bus module ii libqt5gui5:amd64 5.15.8.1-1+dde amd64 Qt 5 GUI module ii libqt5network5:amd64 5.15.8.1-1+dde amd64 Qt 5 network module ri libqt5positioning5:amd64 5.15.8-1+dde amd64 Qt Positioning module ri libqt5printsupport5:amd64 5.15.8.1-1+dde amd64 Qt 5 print support module ii libqt5qml5:amd64 5.15.8.1-1+dde amd64 Qt 5 QML module ii libqt5qmlmodels5:amd64 5.15.8.1-1+dde amd64 Qt 5 QML Models library ii libqt5quick5:amd64 5.15.8.1-1+dde amd64 Qt 5 Quick library ii libqt5quickwidgets5:amd64 5.15.8.1-1+dde amd64 Qt 5 Quick Widgets library ri libqt5webchannel5:amd64 5.15.8-1+dde amd64 Web communication library for Qt ii libqt5widgets5:amd64 5.15.8.1-1+dde amd64 Qt 5 widgets module ri libqt5x11extras5:amd64 5.15.8-1+dde amd64 Qt 5 X11 extras ii qt5-qmake:amd64 5.15.3+dfsg-2ubuntu0.2 amd64 Qt 5 qmake Makefile generator tool ii qt5-qmake-bin 5.15.3+dfsg-2ubuntu0.2 amd64 Qt 5 qmake Makefile generator tool — binary file ii qtchooser 66-2build1 amd64 Wrapper to select between Qt development binary versions ii qttranslations5-l10n 5.15.3-1 all translations for Qt 5 zichen@homepc:~/Downloads/colmap$ How to make sure that Qt5.4.2 is installed properly\n1 2 zichen@homepc:~/Downloads/colmap$ whereis qt5 qt5: /usr/lib/x86_64-linux-gnu/qt5 /usr/lib/qt5 /usr/share/qt5 I have qmake:\n1 2 3 4 zichen@homepc:~/Downloads/colmap$ export QT_SELECT=qt5-x86_64-linux-gnu zichen@homepc:~/Downloads/colmap$ qmake -v QMake version 3.1 Using Qt version 5.15.3 in /usr/lib/x86_64-linux-gnu Only 1 binary qmake exists:\n1 2 3 4 5 6 zichen@homepc:~/Downloads/colmap$ tree /usr/lib/qt5 /usr/lib/qt5 └── bin └── qmake 1 directory, 1 file There should be other libraries or binaries for development. apt can find qtbase5-dev, but I can\u0026rsquo;t install: How do you install qt on ubuntu22.04\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 zichen@homepc:~/Downloads/colmap$ apt-cache search qt zichen@homepc:~/Downloads/colmap$ sudo apt-get install qtbase5-dev Reading package lists... Done Building dependency tree... Done Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) Recommends: libqt5opengl5-dev (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed E: Unable to correct problems, you have held broken packages. qt5-default can\u0026rsquo;t be installed neither:\n1 2 3 4 5 6 7 8 9 10 zichen@homepc:~/Downloads/colmap$ sudo apt-get install qt5-default [sudo] password for zichen: Reading package lists... Done Building dependency tree... Done Reading state information... Done Package qt5-default is not available, but is referred to by another package. This may mean that the package is missing, has been obsoleted, or is only available from another source E: Package \u0026#39;qt5-default\u0026#39; has no installation candidate Others packages related qtbase5:\n1 2 3 4 5 zichen@homepc:~/Downloads/colmap$ apt-cache search qtbase5-dev qtbase5-dev - Qt 5 base development files qtbase5-dev-tools - Qt 5 base development programs qtbase5-gles-dev - Qt 5 base development files — OpenGL ES variant qtbase5-private-gles-dev - Qt 5 base private development files — OpenGL ES variant Some of them requires qtbase-abi-5-15-3\nBut, I can\u0026rsquo;t install it:\n1 E: Package \u0026#39;qtbase-abi-5-15-3\u0026#39; has no installation candidate Select the second solution offered by aptitude:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 zichen@homepc:~/Downloads/colmap$ sudo aptitude install qtbase5-dev The following NEW packages will be installed: qtbase5-dev{b} 0 packages upgraded, 1 newly installed, 0 to remove and 15 not upgraded. Need to get 1,135 kB of archives. After unpacking 15.7 MB will be used. The following packages have unmet dependencies: qtbase5-dev : Depends: libqt5concurrent5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5core5a (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5dbus5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5gui5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5network5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5printsupport5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5sql5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable Depends: libqt5test5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable Depends: libqt5widgets5 (= 5.15.3+dfsg-2ubuntu0.2) but 5.15.8.1-1+dde is installed Depends: libqt5xml5 (= 5.15.3+dfsg-2ubuntu0.2) but it is not going to be installed Depends: qtbase5-dev-tools (= 5.15.3+dfsg-2ubuntu0.2) but it is not installable The following actions will resolve these dependencies: Keep the following packages at their current version: 1) qtbase5-dev [Not Installed] Accept this solution? [Y/n/q/?] . The following actions will resolve these dependencies: Remove the following packages: 1) libqt5core5a:i386 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now)] 2) libqt5dbus5:i386 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now)] 3) libqt5positioning5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] 4) libqt5webchannel5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] 5) libqt5x11extras5 [5.15.8-1+dde (\u0026lt;NULL\u0026gt;, now)] Install the following packages: 6) libqt5sql5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 7) libqt5sql5-odbc [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 8) libqt5test5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 9) libqt5xml5 [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 10) qtbase5-dev-tools [5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] Downgrade the following packages: 11) libqt5concurrent5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 12) libqt5core5a [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 13) libqt5dbus5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 14) libqt5gui5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 15) libqt5network5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 16) libqt5printsupport5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] 17) libqt5qml5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 18) libqt5qmlmodels5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 19) libqt5quick5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 20) libqt5quickwidgets5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3-1+dde (\u0026lt;NULL\u0026gt;)] 21) libqt5widgets5 [5.15.8.1-1+dde (\u0026lt;NULL\u0026gt;, now) -\u0026gt; 5.15.3+dfsg-2ubuntu0.2 (jammy-updates)] Accept this solution? [Y/n/q/?] Y The following packages will be DOWNGRADED: libqt5concurrent5 libqt5core5a libqt5dbus5 libqt5gui5 libqt5network5 libqt5printsupport5 libqt5qml5 libqt5qmlmodels5 libqt5quick5 libqt5quickwidgets5 libqt5widgets5 The following NEW packages will be installed: libqt5sql5{a} libqt5sql5-odbc{a} libqt5test5{a} libqt5xml5{a} qtbase5-dev qtbase5-dev-tools{a} The following packages will be REMOVED: libqt5core5a:i386{a} libqt5dbus5:i386{a} libqt5positioning5{a} libqt5webchannel5{a} libqt5x11extras5{a} The following packages are RECOMMENDED but will NOT be installed: libqt5svg5 qt5-gtk-platformtheme 0 packages upgraded, 6 newly installed, 11 downgraded, 5 to remove and 15 not upgraded. Need to get 15.1 MB of archives. After unpacking 6,725 kB will be used. Do you want to continue? [Y/n/?] After installation, there are more binary files:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 zichen@homepc:~/Downloads/colmap$ tree /usr/lib/qt5/bin/ /usr/lib/qt5/bin/ ├── fixqt4headers.pl ├── moc ├── qdbuscpp2xml ├── qdbusxml2cpp ├── qlalr ├── qmake ├── qvkgen ├── rcc ├── syncqt.pl ├── tracegen └── uic 0 directories, 11 files The Qt5OpenGL is required as well:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CMake Error at /usr/lib/x86_64-linux-gnu/cmake/Qt5/Qt5Config.cmake:28 (find_package): Could not find a package configuration file provided by \u0026#34;Qt5OpenGL\u0026#34; with any of the following names: Qt5OpenGLConfig.cmake qt5opengl-config.cmake Add the installation prefix of \u0026#34;Qt5OpenGL\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;Qt5OpenGL_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;Qt5OpenGL\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): cmake/FindDependencies.cmake:150 (find_package) CMakeLists.txt:96 (include) Similarly, use aptitude to install it:\n1 2 3 4 5 6 zichen@homepc:~/Downloads/colmap$ sudo aptitude install libqt5opengl5-dev The following NEW packages will be installed: libqt5opengl5{a} libqt5opengl5-dev 0 packages upgraded, 2 newly installed, 0 to remove and 27 not upgraded. Need to get 195 kB of archives. After unpacking 927 kB will be used. Do you want to continue? [Y/n/?] Y Finally, compilation succeeded:\n1 2 3 zichen@homepc:~/Downloads/colmap$ cmake -B ./build -GNinja cd build ninja (2024-04-18)\nCUDA requires GCC setup under Ubuntu 22.04. Docs\n1 2 3 4 # sudo apt-get install gcc-10 g++-10 export CC=/usr/bin/gcc-10 export CXX=/usr/bin/g++-10 export CUDAHOSTCXX=/usr/bin/g++-10 Installing nvidia-cuda-toolkit failed via apt install. I don\u0026rsquo;t install it since I already have CUDA-11.6 installed by downloading package manually.\nVerified Practice (2024-04-19)\nRename anaconda3 folder, e.g., anaconda3_1. Otherwise, the qt5 called by header and cmake mismatch. issue-sofa-fredroy\nUse aptitude to install unavailable dependecies, as apt install cannot install qtbase5-dev and libqt5opengl5-dev on Ubuntu 22.04.\nSelect an alternative solutions provided by aptitude.\nSpecify gcc and g++ versions for Ubuntu 22.04:\n1 2 3 4 # sudo apt-get install gcc-10 g++-10 export CC=/usr/bin/gcc-10 export CXX=/usr/bin/g++-10 export CUDAHOSTCXX=/usr/bin/g++-10 Compile colmap:\n1 2 3 4 5 # cd colmap cmake -B ./build -GNinja cd build ninja sudo ninja install The colmap is installed in /usr/local\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 [0/1] Install the project... -- Install configuration: \u0026#34;Release\u0026#34; -- Installing: /usr/local/share/applications/COLMAP.desktop -- Installing: /usr/local/lib/libcolmap_controllers.a -- Installing: /usr/local/lib/libcolmap_estimators.a -- Installing: /usr/local/lib/libcolmap_exe.a -- Installing: /usr/local/lib/libcolmap_feature_types.a -- Installing: /usr/local/lib/libcolmap_feature.a -- Installing: /usr/local/lib/libcolmap_geometry.a -- Installing: /usr/local/lib/libcolmap_image.a -- Installing: /usr/local/lib/libcolmap_math.a -- Installing: /usr/local/lib/libcolmap_mvs.a -- Installing: /usr/local/lib/libcolmap_optim.a -- Installing: /usr/local/lib/libcolmap_retrieval.a -- Installing: /usr/local/lib/libcolmap_scene.a -- Installing: /usr/local/lib/libcolmap_sensor.a -- Installing: /usr/local/lib/libcolmap_sfm.a -- Installing: /usr/local/lib/libcolmap_util.a -- Installing: /usr/local/lib/libcolmap_lsd.a -- Installing: /usr/local/lib/libcolmap_poisson_recon.a -- Installing: /usr/local/lib/libcolmap_vlfeat.a -- Installing: /usr/local/lib/libcolmap_ui.a -- Installing: /usr/local/lib/libcolmap_util_cuda.a -- Installing: /usr/local/lib/libcolmap_mvs_cuda.a -- Installing: /usr/local/lib/libcolmap_sift_gpu.a -- Installing: /usr/local/share/colmap/colmap-config.cmake -- Installing: /usr/local/share/colmap/colmap-config-version.cmake -- Installing: /usr/local/share/colmap/colmap-targets.cmake -- Installing: /usr/local/share/colmap/colmap-targets-release.cmake -- Installing: /usr/local/include/colmap -- Installing: /usr/local/include/colmap/estimators -- Installing: /usr/local/include/colmap/estimators/utils.h -- Installing: /usr/local/include/colmap/estimators/pose.h -- Installing: /usr/local/include/colmap/estimators/coordinate_frame.h -- Installing: /usr/local/include/colmap/estimators/two_view_geometry.h -- Installing: /usr/local/include/colmap/estimators/generalized_absolute_pose.h -- Installing: /usr/local/include/colmap/estimators/bundle_adjustment.h -- Installing: /usr/local/include/colmap/estimators/affine_transform.h -- Installing: /usr/local/include/colmap/estimators/generalized_absolute_pose_coeffs.h -- Installing: /usr/local/include/colmap/estimators/fundamental_matrix.h -- Installing: /usr/local/include/colmap/estimators/euclidean_transform.h -- Installing: /usr/local/include/colmap/estimators/alignment.h -- Installing: /usr/local/include/colmap/estimators/homography_matrix.h -- Installing: /usr/local/include/colmap/estimators/absolute_pose.h -- Installing: /usr/local/include/colmap/estimators/similarity_transform.h -- Installing: /usr/local/include/colmap/estimators/triangulation.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix_poly.h -- Installing: /usr/local/include/colmap/estimators/cost_functions.h -- Installing: /usr/local/include/colmap/estimators/generalized_relative_pose.h -- Installing: /usr/local/include/colmap/estimators/generalized_pose.h -- Installing: /usr/local/include/colmap/estimators/translation_transform.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix_coeffs.h -- Installing: /usr/local/include/colmap/estimators/essential_matrix.h -- Installing: /usr/local/include/colmap/controllers -- Installing: /usr/local/include/colmap/controllers/feature_matching_utils.h -- Installing: /usr/local/include/colmap/controllers/automatic_reconstruction.h -- Installing: /usr/local/include/colmap/controllers/bundle_adjustment.h -- Installing: /usr/local/include/colmap/controllers/hierarchical_mapper.h -- Installing: /usr/local/include/colmap/controllers/incremental_mapper.h -- Installing: /usr/local/include/colmap/controllers/image_reader.h -- Installing: /usr/local/include/colmap/controllers/feature_extraction.h -- Installing: /usr/local/include/colmap/controllers/feature_matching.h -- Installing: /usr/local/include/colmap/controllers/option_manager.h -- Installing: /usr/local/include/colmap/math -- Installing: /usr/local/include/colmap/math/math.h -- Installing: /usr/local/include/colmap/math/matrix.h -- Installing: /usr/local/include/colmap/math/graph_cut.h -- Installing: /usr/local/include/colmap/math/polynomial.h -- Installing: /usr/local/include/colmap/math/random.h -- Installing: /usr/local/include/colmap/exe -- Installing: /usr/local/include/colmap/exe/gui.h -- Installing: /usr/local/include/colmap/exe/image.h -- Installing: /usr/local/include/colmap/exe/database.h -- Installing: /usr/local/include/colmap/exe/feature.h -- Installing: /usr/local/include/colmap/exe/model.h -- Installing: /usr/local/include/colmap/exe/mvs.h -- Installing: /usr/local/include/colmap/exe/vocab_tree.h -- Installing: /usr/local/include/colmap/exe/sfm.h -- Installing: /usr/local/include/colmap/optim -- Installing: /usr/local/include/colmap/optim/sampler.h -- Installing: /usr/local/include/colmap/optim/random_sampler.h -- Installing: /usr/local/include/colmap/optim/least_absolute_deviations.h -- Installing: /usr/local/include/colmap/optim/support_measurement.h -- Installing: /usr/local/include/colmap/optim/combination_sampler.h -- Installing: /usr/local/include/colmap/optim/ransac.h -- Installing: /usr/local/include/colmap/optim/progressive_sampler.h -- Installing: /usr/local/include/colmap/optim/sprt.h -- Installing: /usr/local/include/colmap/optim/loransac.h -- Installing: /usr/local/include/colmap/sensor -- Installing: /usr/local/include/colmap/sensor/models.h -- Installing: /usr/local/include/colmap/sensor/database.h -- Installing: /usr/local/include/colmap/sensor/bitmap.h -- Installing: /usr/local/include/colmap/sensor/specs.h -- Installing: /usr/local/include/colmap/util -- Installing: /usr/local/include/colmap/util/types.h -- Installing: /usr/local/include/colmap/util/ply.h -- Installing: /usr/local/include/colmap/util/string.h -- Installing: /usr/local/include/colmap/util/threading.h -- Installing: /usr/local/include/colmap/util/sqlite3_utils.h -- Installing: /usr/local/include/colmap/util/base_controller.h -- Installing: /usr/local/include/colmap/util/endian.h -- Installing: /usr/local/include/colmap/util/eigen_alignment.h -- Installing: /usr/local/include/colmap/util/logging.h -- Installing: /usr/local/include/colmap/util/version.h -- Installing: /usr/local/include/colmap/util/timer.h -- Installing: /usr/local/include/colmap/util/cuda.h -- Installing: /usr/local/include/colmap/util/controller_thread.h -- Installing: /usr/local/include/colmap/util/cache.h -- Installing: /usr/local/include/colmap/util/testing.h -- Installing: /usr/local/include/colmap/util/opengl_utils.h -- Installing: /usr/local/include/colmap/util/misc.h -- Installing: /usr/local/include/colmap/util/cudacc.h -- Installing: /usr/local/include/colmap/mvs -- Installing: /usr/local/include/colmap/mvs/mat.h -- Installing: /usr/local/include/colmap/mvs/meshing.h -- Installing: /usr/local/include/colmap/mvs/cuda_rotate.h -- Installing: /usr/local/include/colmap/mvs/cuda_texture.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat_ref_image.h -- Installing: /usr/local/include/colmap/mvs/image.h -- Installing: /usr/local/include/colmap/mvs/cuda_flip.h -- Installing: /usr/local/include/colmap/mvs/patch_match_cuda.h -- Installing: /usr/local/include/colmap/mvs/cuda_transpose.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat_prng.h -- Installing: /usr/local/include/colmap/mvs/depth_map.h -- Installing: /usr/local/include/colmap/mvs/normal_map.h -- Installing: /usr/local/include/colmap/mvs/workspace.h -- Installing: /usr/local/include/colmap/mvs/fusion.h -- Installing: /usr/local/include/colmap/mvs/patch_match.h -- Installing: /usr/local/include/colmap/mvs/model.h -- Installing: /usr/local/include/colmap/mvs/gpu_mat.h -- Installing: /usr/local/include/colmap/mvs/consistency_graph.h -- Installing: /usr/local/include/colmap/image -- Installing: /usr/local/include/colmap/image/undistortion.h -- Installing: /usr/local/include/colmap/image/warp.h -- Installing: /usr/local/include/colmap/image/line.h -- Installing: /usr/local/include/colmap/sfm -- Installing: /usr/local/include/colmap/sfm/incremental_mapper.h -- Installing: /usr/local/include/colmap/sfm/incremental_triangulator.h -- Installing: /usr/local/include/colmap/retrieval -- Installing: /usr/local/include/colmap/retrieval/utils.h -- Installing: /usr/local/include/colmap/retrieval/vote_and_verify.h -- Installing: /usr/local/include/colmap/retrieval/inverted_file_entry.h -- Installing: /usr/local/include/colmap/retrieval/inverted_file.h -- Installing: /usr/local/include/colmap/retrieval/geometry.h -- Installing: /usr/local/include/colmap/retrieval/inverted_index.h -- Installing: /usr/local/include/colmap/retrieval/visual_index.h -- Installing: /usr/local/include/colmap/scene -- Installing: /usr/local/include/colmap/scene/synthetic.h -- Installing: /usr/local/include/colmap/scene/point2d.h -- Installing: /usr/local/include/colmap/scene/two_view_geometry.h -- Installing: /usr/local/include/colmap/scene/reconstruction_manager.h -- Installing: /usr/local/include/colmap/scene/reconstruction_io.h -- Installing: /usr/local/include/colmap/scene/image.h -- Installing: /usr/local/include/colmap/scene/scene_clustering.h -- Installing: /usr/local/include/colmap/scene/database.h -- Installing: /usr/local/include/colmap/scene/database_cache.h -- Installing: /usr/local/include/colmap/scene/camera.h -- Installing: /usr/local/include/colmap/scene/track.h -- Installing: /usr/local/include/colmap/scene/reconstruction.h -- Installing: /usr/local/include/colmap/scene/correspondence_graph.h -- Installing: /usr/local/include/colmap/scene/projection.h -- Installing: /usr/local/include/colmap/scene/visibility_pyramid.h -- Installing: /usr/local/include/colmap/scene/camera_rig.h -- Installing: /usr/local/include/colmap/scene/point3d.h -- Installing: /usr/local/include/colmap/tools -- Installing: /usr/local/include/colmap/feature -- Installing: /usr/local/include/colmap/feature/utils.h -- Installing: /usr/local/include/colmap/feature/types.h -- Installing: /usr/local/include/colmap/feature/matcher.h -- Installing: /usr/local/include/colmap/feature/sift.h -- Installing: /usr/local/include/colmap/feature/extractor.h -- Installing: /usr/local/include/colmap/geometry -- Installing: /usr/local/include/colmap/geometry/pose.h -- Installing: /usr/local/include/colmap/geometry/gps.h -- Installing: /usr/local/include/colmap/geometry/sim3.h -- Installing: /usr/local/include/colmap/geometry/rigid3.h -- Installing: /usr/local/include/colmap/geometry/homography_matrix.h -- Installing: /usr/local/include/colmap/geometry/triangulation.h -- Installing: /usr/local/include/colmap/geometry/essential_matrix.h -- Installing: /usr/local/include/colmap/ui -- Installing: /usr/local/include/colmap/ui/qt_utils.h -- Installing: /usr/local/include/colmap/ui/image_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/colormaps.h -- Installing: /usr/local/include/colmap/ui/bundle_adjustment_widget.h -- Installing: /usr/local/include/colmap/ui/feature_extraction_widget.h -- Installing: /usr/local/include/colmap/ui/model_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/thread_control_widget.h -- Installing: /usr/local/include/colmap/ui/point_viewer_widget.h -- Installing: /usr/local/include/colmap/ui/project_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_options_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_manager_widget.h -- Installing: /usr/local/include/colmap/ui/license_widget.h -- Installing: /usr/local/include/colmap/ui/point_painter.h -- Installing: /usr/local/include/colmap/ui/media -- Installing: /usr/local/include/colmap/ui/undistortion_widget.h -- Installing: /usr/local/include/colmap/ui/render_options.h -- Installing: /usr/local/include/colmap/ui/movie_grabber_widget.h -- Installing: /usr/local/include/colmap/ui/reconstruction_stats_widget.h -- Installing: /usr/local/include/colmap/ui/shaders -- Installing: /usr/local/include/colmap/ui/match_matrix_widget.h -- Installing: /usr/local/include/colmap/ui/feature_matching_widget.h -- Installing: /usr/local/include/colmap/ui/options_widget.h -- Installing: /usr/local/include/colmap/ui/main_window.h -- Installing: /usr/local/include/colmap/ui/log_widget.h -- Installing: /usr/local/include/colmap/ui/database_management_widget.h -- Installing: /usr/local/include/colmap/ui/triangle_painter.h -- Installing: /usr/local/include/colmap/ui/automatic_reconstruction_widget.h -- Installing: /usr/local/include/colmap/ui/line_painter.h -- Installing: /usr/local/include/colmap/ui/dense_reconstruction_widget.h -- Installing: /usr/local/include/colmap/ui/render_options_widget.h -- Installing: /usr/local/include/colmap/thirdparty -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/LiteWindow.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/GlobalUtil.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidGL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidCL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/CuTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftPyramid.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/GLTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCG.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftMatchCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftMatch.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/SiftGPU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/CLTexImage.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramCL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/FrameBufferObject.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ShaderMan.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramGLSL.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/PyramidCU.h -- Installing: /usr/local/include/colmap/thirdparty/SiftGPU/ProgramGPU.h -- Installing: /usr/local/include/colmap/thirdparty/LSD -- Installing: /usr/local/include/colmap/thirdparty/LSD/lsd.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/fisher.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/qsort-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/generic.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/sift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop_sse2.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/heap-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/gmm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/ikmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/slic.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/stringop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/quickshift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/hog.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/lbp.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/float.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/kmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop_avx.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/host.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/dsift.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/homkermap.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/array.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/pgm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/svmdataset.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/shuffle-def.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mathop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/svm.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/mser.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/liop.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/scalespace.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/imopv_sse2.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/rodrigues.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/vlad.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/covdet.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/imopv.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/aib.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/hikmeans.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/getopt_long.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/random.h -- Installing: /usr/local/include/colmap/thirdparty/VLFeat/kdtree.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SurfaceTrimmer.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Polynomial.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BinaryNode.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.IsoSurface.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Geometry.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BSplineData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/CmdLineParser.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Allocator.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.Evaluation.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Factor.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/BSplineData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Polynomial.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PPolynomial.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SparseMatrix.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/FunctionData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MarchingCubes.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/SparseMatrix.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Array.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/FunctionData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Array.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MAT.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PoissonRecon.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MAT.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Geometry.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Octree.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.SortedTreeNodes.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MemoryUsage.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.WeightedSamples.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Hash.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PPolynomial.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Octree.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PointStream.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/CmdLineParser.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MultiGridOctreeData.System.inl -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/Ply.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/MyTime.h -- Installing: /usr/local/include/colmap/thirdparty/PoissonRecon/PointStream.h -- Installing: /usr/local/share/colmap/cmake -- Installing: /usr/local/share/colmap/cmake/FindLZ4.cmake -- Installing: /usr/local/share/colmap/cmake/FindMetis.cmake -- Installing: /usr/local/share/colmap/cmake/FindFLANN.cmake -- Installing: /usr/local/share/colmap/cmake/FindGlew.cmake -- Installing: /usr/local/share/colmap/cmake/FindFreeImage.cmake -- Installing: /usr/local/share/colmap/cmake/FindGlog.cmake -- Installing: /usr/local/share/colmap/cmake/FindDependencies.cmake -- Installing: /usr/local/bin/colmap -- Set non-toolchain portion of runtime path of \u0026#34;/usr/local/bin/colmap\u0026#34; to \u0026#34;\u0026#34; Ubuntu 20.04 (2024-04-18)\nSystem info:\nOS: Ubuntu 20.04.6 LTS x86_64; Kernel: 5.15.0-101-generic Host: Alienware Aurora R8 1.0.6 CPU: Intel i7-9700 (8) @ 4.700GHz, GPU: Intel UHD Graphics 630 GPU: NVIDIA GeForce GTX 1050 Ti Memory: 16GB gcc version 9.4.0 Cuda compilation tools, release 11.6, V11.6.55; Build cuda_11.6.r11.6/compiler.30794723_0 (nvcc -V) Nvidia Driver Version: 545.23.06 Dependecies: Docs\nThe installation has no error reported:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 (base) yi@yi-Alienware-Aurora-R8:~/Downloads$ sudo apt-get install \\ \u0026gt; git \\ \u0026gt; cmake \\ \u0026gt; ninja-build \\ \u0026gt; build-essential \\ \u0026gt; libboost-program-options-dev \\ \u0026gt; libboost-filesystem-dev \\ \u0026gt; libboost-graph-dev \\ \u0026gt; libboost-system-dev \\ \u0026gt; libeigen3-dev \\ \u0026gt; libflann-dev \\ \u0026gt; libfreeimage-dev \\ \u0026gt; libmetis-dev \\ \u0026gt; libgoogle-glog-dev \\ \u0026gt; libgtest-dev \\ \u0026gt; libsqlite3-dev \\ \u0026gt; libglew-dev \\ \u0026gt; qtbase5-dev \\ \u0026gt; libqt5opengl5-dev \\ \u0026gt; libcgal-dev \\ \u0026gt; libceres-dev [sudo] password for yi: Reading package lists... Done Building dependency tree Reading state information... Done libboost-filesystem-dev is already the newest version (1.71.0.0ubuntu2). libboost-filesystem-dev set to manually installed. libboost-program-options-dev is already the newest version (1.71.0.0ubuntu2). libboost-program-options-dev set to manually installed. libboost-system-dev is already the newest version (1.71.0.0ubuntu2). libboost-system-dev set to manually installed. libboost-graph-dev is already the newest version (1.71.0.0ubuntu2). libboost-graph-dev set to manually installed. libeigen3-dev is already the newest version (3.3.7-2). libglew-dev is already the newest version (2.1.0-4). build-essential is already the newest version (12.8ubuntu1.1). build-essential set to manually installed. git is already the newest version (1:2.25.1-1ubuntu3.11). The following additional packages will be installed: cmake-data googletest libaec-dev libatlas3-base libblas-dev libbtf1 libceres1 libcxsparse3 libflann1.9 libfreeimage3 libgflags-dev libgflags2.2 libgmp-dev libgmpxx4ldbl libgoogle-glog0v5 libgraphblas3 libhdf5-mpi-dev libhdf5-openmpi-dev libjxr0 libklu1 liblapack-dev libldl2 liblz4-dev libmongoose2 libmpfr-dev libqt5opengl5 librbio2 librhash0 libspqr2 libsuitesparse-dev qt5-qmake qt5-qmake-bin qtbase5-dev-tools qtchooser Suggested packages: cmake-doc liblapack-doc libmpfi-dev libntl-dev gmp-doc libgmp10-doc libhdf5-doc libmpfr-doc sqlite3-doc default-libmysqlclient-dev firebird-dev libpq-dev unixodbc-dev The following NEW packages will be installed: cmake cmake-data googletest libaec-dev libatlas3-base libblas-dev libbtf1 libceres-dev libceres1 libcgal-dev libcxsparse3 libflann-dev libflann1.9 libfreeimage-dev libfreeimage3 libgflags-dev libgflags2.2 libgmp-dev libgmpxx4ldbl libgoogle-glog-dev libgoogle-glog0v5 libgraphblas3 libgtest-dev libhdf5-mpi-dev libhdf5-openmpi-dev libjxr0 libklu1 liblapack-dev libldl2 liblz4-dev libmetis-dev libmongoose2 libmpfr-dev libqt5opengl5 libqt5opengl5-dev librbio2 librhash0 libspqr2 libsqlite3-dev libsuitesparse-dev ninja-build qt5-qmake qt5-qmake-bin qtbase5-dev qtbase5-dev-tools qtchooser 0 upgraded, 46 newly installed, 0 to remove and 47 not upgraded. Need to get 39.1 MB of archives. After this operation, 323 MB of additional disk space will be used. Do you want to continue? [Y/n] Compile\nSame error pertain to libtiff as above.\n1 2 3 4 5 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/colmap/build$ conda list libtiff # packages in environment at /home/yi/anaconda3: # # Name Version Build Channel libtiff 4.5.1 h6a678d5_0 Uninstall libtiff\nHowever, conda uninstall libtiff hangs forever.\n","date":"2024-03-14T23:40:00Z","permalink":"https://zichen34.github.io/writenotes/lib/colmap_notes/","title":"memo: COLMAP | Install and Usages"},{"content":"Code | Arxiv\nNotes The editing feature is based on Embedded deformation for shape manipulation\nThe transformation of each Gaussian in the entire point cloud is an expaction of the transformations of the K nearest control points.\nPlay Environment (2024-03-11)\nUbuntu 20.04, cuda-11.6\n1 2 3 4 5 conda create -n SC-GS python=3.10 conda activate SC-GS pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 # I also change the torch version to cu116 in requirements.txt pip install -r requirements.txt If I directly run pip install -r requirements.txt, the following error about pip compiling occurs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 (SC-GS) yi@yi:~/Downloads/SC-GS-comments$ pip install -r requirements.txt ... Collecting git+https://github.com/facebookresearch/pytorch3d.git (from -r requirements.txt (line 14)) Cloning https://github.com/facebookresearch/pytorch3d.git to /tmp/pip-req-build-2ndb6zwl Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-2ndb6zwl Resolved https://github.com/facebookresearch/pytorch3d.git to commit 7566530669203769783c94024c25a39e1744e4ed Preparing metadata (setup.py) ... error error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─\u0026gt; [6 lines of output] Traceback (most recent call last): File \u0026#34;\u0026lt;string\u0026gt;\u0026#34;, line 2, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;pip-setuptools-caller\u0026gt;\u0026#34;, line 34, in \u0026lt;module\u0026gt; File \u0026#34;/tmp/pip-req-build-2ndb6zwl/setup.py\u0026#34;, line 15, in \u0026lt;module\u0026gt; import torch ModuleNotFoundError: No module named \u0026#39;torch\u0026#39; [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed × Encountered error while generating package metadata. ╰─\u0026gt; See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Refer issue 10 and issue 15\nCompilin Pillow requires: sudo apt-get install libjpeg-dev. Otherwise, error occus:\n1 2 3 4 5 6 7 8 9 10 11 The headers or library files could not be found for jpeg, a required dependency when compiling Pillow from source. Please see the install instructions at: https://pillow.readthedocs.io/en/latest/installation.html note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for Pillow Running setup.py clean for Pillow Failed to build Pillow ERROR: Could not build wheels for Pillow, which is required to install pyproject.toml-based projects PIL error: no attribute 'ANTIALIAS'\n1 2 3 File \u0026#34;/home/yi/anaconda3/envs/SC-GS/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py\u0026#34;, line 486, in make_image image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS) AttributeError: module \u0026#39;PIL.Image\u0026#39; has no attribute \u0026#39;ANTIALIAS\u0026#39; ChatGPT: The api has changed, the function should be called like:\nDidn't try 1 2 3 from PIL import Image, ImageFilter # ... image = image.resize((scaled_width, scaled_height), ImageFilter.ANTIALIAS) This error is solved by reinstalling the conda environment with python=3.8, pip will download the packages compatible (cp) with python 3.8. And Pillow cp38 still have the same api.\nTrain Train on 1050Ti (4GB)\n1 2 3 4 5 6 # Train with terminal only (for the resolution of 400*400 with best PSNR) CUDA_VISIBLE_DEVICES=0 python train_gui.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512 \\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 80000 iterations cost 1:47:24.\nDeformation Once the model has been trained, launch GUI with the output dir (just add --gui):\n1 2 3 4 5 CUDA_VISIBLE_DEVICES=0 python train_gui.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512\\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 --gui Unlike LBS, the model can be teared intio pieces.\nAnd the deformation may be anti-physical.\nAfter adjusting the poses, clik play to watch animation.\nTo debug the deformation code, once selected keypoints (A+Left Clik), Pause the debugger first, and then step by step inspect.\nPress D + Right-click drag will trigger callback_keypoint_drag\nClick Init Graph will tigger callback_animation_initialize to assign the attribute animate_tool by instantiating the class LapDeform\n1 dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Right, callback=callback_keypoint_drag) The command for eval only change train_gui.py to render.py:\n1 2 3 4 5 CUDA_VISIBLE_DEVICES=0 python render.py \\ --source_path /home/yi/Downloads/Dataset_life/DNeRF_data/jumpingjacks \\ --model_path outputs/jumpingjacks --deform_type node --node_num 512 \\ --is_blender --eval --gt_alpha_mask_as_scene_mask \\ --local_frame --resolution 2 --W 800 --H 800 ","date":"2024-03-11T00:00:00Z","image":"https://arxiv.org/html/2312.14937v2/x2.png","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-sc-gs/","title":"read: SC-GS"},{"content":"remap Docs - OpenCV: Remapping\nUse cv2.remap to do image warpping:\nHomography mapping determines the projection location on the source image for a reference image.\nr e ( ( ( f 0 1 2 , , , p 0 0 0 i ) ) ) x e ( ( ( l 0 1 2 , , , c 1 1 1 o ) ) ) o r ( ( ( d 0 2 2 s , , , 2 2 2 ) ) ) p o ( ( ( i x x x n y y y t z z z s ) ) ) w ( ( ( o x x x r y y y l z z z d ) ) ) c ( ( ( o x x x o y y y r z z z d ) ) ) s ( ( ( p x x x r , , , o y y y j ) ) ) s ( ( ( o x x x n , , , y y y s ) ) ) r c ( ( ( x x x i , , , m y y y g ) ) ) By fetching the pixels of the source image to the grid, according to the projection coordinates, a warpped source image is obtained.\nTherefore, remap requires the (x,y) \u0026ldquo;new arrange pattern\u0026rdquo; of the source image as input, along with the source image.\nExample in CasMVSNet-pl:\n1 2 3 4 5 6 7 8 9 xy_ref = np.mgrid[:img_wh[1],:img_wh[0]][::-1].astype(np.float32) xy_src = xy_ref2src(xy_ref, depth_ref, P_world2ref, depth_src, P_world2src, img_wh) # Sample the depth of xy_src using bilinear interpolation depth_src2ref = cv2.remap(depth_src, xy_src[0].astype(np.float32), xy_src[1].astype(np.float32), interpolation=cv2.INTER_LINEAR) resize (2024-03-14)\n1 2 3 4 5 6 7 8 9 10 img_ref = cv2.imread(os.path.join(root_dir, # channel is BGR f\u0026#39;Rectified/{scan}/rect_{vid+1:03d}_3_r5000.png\u0026#39;)) # scale to specified size img_ref = cv2.resize(img_ref, tuple(args.img_wh), interpolation=cv2.INTER_LINEAR)[:,:,::-1] # to RGB # scaling with factor, h and w both increase 4 times proba_ref = cv2.resize(proba_ref, None, fx=4, fy=4, interpolation=cv2.INTER_LINEAR) ","date":"2024-03-07T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/opencv/","title":"memo: OpenCV"},{"content":"Neural Depth GPU-Accelerated 3D Point Cloud Processing with Hierarchical GMM\nCode\nVideo: GTC Silicon Valley-2019: GPU-Accelerated 3D Point Cloud Processing with Hierarchical Gaussian Mixtures\nFound by DDG (image) with searching the paper: \u0026ldquo;GPU-Accelerated LOD Generation for Point Clouds\u0026rdquo;\nVisualization GPU-Accelerated LOD Generation for Point Clouds\nCode | Bernhard Kerbl (3DGS) | CoolPapers\nCudaLOD Multi-level details for point cloud Refinement (2024-03-15)\nJourney when searching: \u0026ldquo;point cloud refinement research\u0026rdquo; DDG\nReal-Time Point Cloud Refinement\nRSGate (2024-03-16)\nUpsampiling point cloud by inserting new point set Splatting A voxel-based multiview point cloud refinement method via factor graph optimization\nNo Code | Paper not accessible\nIt\u0026rsquo;s compared with ICP, ICP-Normal, GICP, LUM, BALM\nSearch the title of above paper 1 DDG\nPR-GCN: A Deep Graph Convolutional Network With Point Refinement for 6D Pose Estimation (ICCV 2021)\nSemantic (2024-03-16)\nKNN and Graph Convolution Network Edge-aware The paper-1 is cited by:\nEc-net: an edge-aware point set consolidation network\nCode\n","date":"2024-03-06T17:25:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/c-symp-pc_pro/","title":"sympo: Point Cloud Processing"},{"content":"Parallel programming model - Wikipedia\nReduction 归约 (2024-03-01)\nSource video: CUDA编程模型系列八(原子操作 / 规约 / 向量元素求和) - Ken He Code The parallism design depends on the operations to be performed. For example, given a task N-number summation, the operation executed on each thread in parallel is addition.\nThreads reduce by half every time.\nAs the \u0026ldquo;plus\u0026rdquo; operation computes 2 numbers, the data sequence is bisected.\n1 2 3 4 5 6 7 8 9 10 11 source[8]: 0 1 2 3 4 5 6 7 step 1: thread 0: source[0] + source[4] -\u0026gt; source[0] thread 1: source[1] + source[5] -\u0026gt; source[1] thread 2: source[2] + source[6] -\u0026gt; source[2] thread 3: source[3] + source[7] -\u0026gt; source[3] step 2: thread 0: source[0] + source[2] -\u0026gt; source[0] thread 1: source[1] + source[3] -\u0026gt; source[1] step 3: thread 0: source[0] + source[1] As shown above, the number of inital threads allocated is a half of the total data items. And in the following steps, the number of launched threads is a half of the number of threads used last time.\nSpecificaly, in the 1st round, 4 threads for 8 items, and the 2nd round only uses 2 threads for 4 results of the last step, and the final round only uses 1 thread.\n8 4 4 2 2 1 r e i t i t i t s t h t h t h u e r e r e r l m e m e m e t s a s a s a : : d : d : d s s : : : 0 0 0 0 1 1 1 2 2 3 3 4 5 6 7 If using CPU, there will be 7 plus operation, however, on GPU, there are only 3 steps.\nWhen the total number of operations is larger than the allocated threads, the Grid stride loop trick can be used.\nFor example, there are 32 operations need to be executed, but only 8 threads are allocated. Therefore, each thread has to be reused 4 times.\nBased on this fact, accumulate the 4 loops at first and then perform summation within a block (8 threads).\nOnly consider the behavior of one thread: what values will it use?\nFor a thread in a block, the sum of values assigned to it during 4 loops is computed as:\n3 A 8 r 2 c e c t s e h u l l r l e o e t : o d p : 0 0 0 l 1 1 o o 2 2 p ⋯ ⋯ 1 7 7 8 l 9 o o 1 p 0 2 ⋯ 1 5 1 6 l 1 o 7 o p 1 8 3 ⋯ 2 3 2 4 2 l 5 o o 2 p 6 4 ⋯ 3 1 In this way, multiple steps are compressed into a single block (8 threads).\nShared memory is very fast, so it can be used for those memory that is frequently accessed.\nAs the accumulated sums of 4 loops for each thread requires summation across the BLOCK_SIZE at the end, they can be stored in shared memory for later frequent reading.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // grid loop: accumulate loops first // allocate the same size as a block __shared__ int acc_tmp[BLOCK_SIZE]; int shared_tmp = 0; // Necessary to get correct result // Each thread adds the thread after n_thrd_cur for(int ele_id=blockDim.x * blockIdx.x + threadIdx.x; ele_id\u0026lt;num_items; ele_id+=blockDim.x*gridDim.x){ shared_tmp += d_in[ele_id]; } // __syncthreads(); // Sometimes lead to wrong result acc_tmp[threadIdx.x] = shared_tmp; // assign shared mem __syncthreads(); // Necessary Note:\nIf directly using the shared memory to do accumulation like: acc_tmp[threadIdx.x] += d_in[ele_id];, the result could be wrong because it\u0026rsquo;s in a loop, where mutliple thread may access the same memory at the same time, due to shared memory is accessible for all threads in a block.\nHowever, the local variable (shared_tmp) reside in register is private for a thread, and other threads can\u0026rsquo;t access it. So modifying the shared_tmp is safe and necessary.\nThe last __syncthreads(); cannot be put right after the for loop, and must be after shared memory assignment (on 1050Ti). Otherwise, the result could be wrong. (1080Ti is ok.)\nSo far, only the \u0026ldquo;block\u0026rdquo; of shared memory acc_tmp needs to compute the sum.\nThe threads reduction is performed through a for loop to adjust the number of threads step-by-step:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Sum numbers in the shared memory with size of BLOCK_SIZE // Threads reduce by half each step // Initial number of threads is a half of the total data for (n_thrd_cur=BLOCK_SIZE/2; n_thrd_cur\u0026gt;=1; n_thrd_cur/=2){ // Let a thread to do an operations: plus // Temporary variable is necessary for memory safety: int sum_tmp = 0; // Only use threads required if (threadIdx.x \u0026lt; n_thrd_cur){ sum_tmp = acc_tmp[threadIdx.x] + acc_tmp[threadIdx.x + n_thrd_cur]; } __syncthreads(); // Necessary // as write after read for the same memory // Can\u0026#39;t reside in if or other brach syntax // Write result back to memory if (threadIdx.x \u0026lt; n_thrd_cur){ acc_tmp[threadIdx.x] = sum_tmp; } __syncthreads(); // Necessary for 1050Ti, 1080Ti } Finally, the sum of a block (shared memory) is stored in acc_tmp[0].\n__syncthreads() is used when a memory is read followed by writing/modification to avoid data Hazard-wiki (Race condition-wiki, Memory safety-wiki)\n__syncthreads() can\u0026rsquo;t reside in if because it\u0026rsquo;s a branch. Otherwise, when multiple threads run in parallel, threads may go different branches, consequently, leading to errors.\natomicAdd guarantees the read/write to an address won\u0026rsquo;t be disrupted by other threads.\nWhen adding the summation of each block acc_tmp[0] (shared memory) upto d_out (global memory), multiple threads access the same global memory d_out, so atomicAdd is applied:\n1 2 3 4 5 6 7 8 // Accumulate all blocks in the grid // The sum of each block was stored in acc_tmp[0] // Each block uses 1 thread to add its sum to the total sum of all blocks if (blockIdx.x * blockDim.x \u0026lt; num_items){ if (threadIdx.x == 0){ atomicAdd(d_out, acc_tmp[0]); } } Full code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 #include \u0026lt;stdio.h\u0026gt; #define N 10000000 // number of data. No = and column ; #define BLOCK_SIZE 256 // blockDim.x #define GRID_SIZE 32 // gridDim.x, the total threads allocated: 8192 __managed__ int source[N]; // allocate memory for input data __managed__ int result_gpu[1] = {0}; // output the sum of N items // Remeber to declare types for args __global__ void sum_gpu(int* d_in, int num_items, int* d_out){ // grid loop __shared__ int acc_tmp[BLOCK_SIZE]; int shared_tmp = 0; for (int ele_id=blockIdx.x * blockDim.x + threadIdx.x; ele_id \u0026lt; num_items; ele_id+=BLOCK_SIZE*GRID_SIZE){ shared_tmp += d_in[ele_id]; } acc_tmp[threadIdx.x] = shared_tmp; __syncthreads(); // threads reduction int sum_tmp = 0; for (int thd_cur=BLOCK_SIZE/2; thd_cur\u0026gt;=1; thd_cur/=2){ if (threadIdx.x \u0026lt; thd_cur){ sum_tmp = acc_tmp[threadIdx.x] + acc_tmp[threadIdx.x + thd_cur]; } __syncthreads(); if (threadIdx.x \u0026lt; thd_cur){ acc_tmp[threadIdx.x] = sum_tmp; } __syncthreads(); } // accumulate all blocks if (blockIdx.x * blockDim.x \u0026lt; num_items){ if (threadIdx.x == 0){ // d_out[0] += acc_tmp[0]; atomicAdd(d_out, acc_tmp[0]); } } } int main(){ // Initialize source data // Can\u0026#39;t use: for(int\u0026amp; i :sourace) for (int i=0; i\u0026lt;N; i++) source[i] = rand()%10; // record time cudaEvent_t start, stop_gpu, stop_cpu; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop_gpu); cudaEventCreate(\u0026amp;stop_cpu); cudaEventRecord(start); cudaEventSynchronize(start); for (int i=0; i\u0026lt;20; i++){ // avg time for 20 rounds result_gpu[0] = 0; // clear value sum_gpu\u0026lt;\u0026lt;\u0026lt;GRID_SIZE, BLOCK_SIZE\u0026gt;\u0026gt;\u0026gt;(source, N, result_gpu); cudaDeviceSynchronize(); // wait gpu } cudaEventRecord(stop_gpu); cudaEventSynchronize(stop_gpu); // cpu execution int result_cpu = 0; for (int i=0; i\u0026lt;N; i++) result_cpu += source[i]; cudaEventRecord(stop_cpu); cudaEventSynchronize(stop_cpu); float time_gpu, time_cpu; cudaEventElapsedTime(\u0026amp;time_gpu, start, stop_gpu); cudaEventElapsedTime(\u0026amp;time_cpu, stop_gpu, stop_cpu); cudaEventDestroy(start); cudaEventDestroy(stop_gpu); cudaEventDestroy(stop_cpu); printf(\u0026#34;Time on gpu: %.2f, Time on cpu: %.2f\\n\u0026#34;, time_gpu/20, time_cpu); printf(\u0026#34;%s\\n\u0026#34;, (result_gpu[0] == result_cpu) ? \u0026#34;Equal\u0026#34; : \u0026#34;Error\u0026#34;); printf(\u0026#34;Sum on gpu: %d, Sum on cpu: %d\\n\u0026#34;, *result_gpu, result_cpu); return 0; } Output:\n1 2 3 4 5 (base) yi@yi:~/Downloads/CUDA_Study$ nvcc Tut_KenHe/8_reduction.cu (base) yi@yi:~/Downloads/CUDA_Study$ ./a.out Time on gpu: 1.15, Time on cpu: 26.45 Equal Sum on gpu: 45011704, Sum on cpu: 45011704 ","date":"2024-03-01T18:20:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/tut_%E4%BD%95%E7%90%A8/","title":"watch: CUDA - 何琨 | CUDA Programming Model"},{"content":"CUDA Impl (2024-03-05)\nExample from simple-knn:\nUse thread block to divide the sorted points\nGiven 100K points, using only 1D grid and 1D block which length is 1024 (SO), the number of blocks is $\\frac{100K + 1024-1}{1024} = 97$\nTherefore, the kernel launch configuration parameters are \u0026lt;\u0026lt;\u0026lt;97, 1024\u0026gt;\u0026gt;\u0026gt;\nConvert 3D cartesian coordinates to Morton code\nMake up key-value pairs, where Morton code is key and points indices are values.\nSuch that, the points (indices) are sorted and similar points will gather together.\n1 2 3 4 5 6 7 8 thrust::device_vector\u0026lt;uint32_t\u0026gt; indices(P);\t// indices for points thrust::sequence(indices.begin(), indices.end());\t// indices: 0, 1, 2, 3. ... P thrust::device_vector\u0026lt;uint32_t\u0026gt; indices_sorted(P);\t// the sorted indices cub::DeviceRadixSort::SortPairs(nullptr, temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); temp_storage.resize(temp_storage_bytes); cub::DeviceRadixSort::SortPairs(temp_storage.data().get(), temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); Once the points are spread onto blocks, points within a block are similar to each other.\nFind the min and max coordinates combinations in each block\n1 boxMinMax \u0026lt;\u0026lt; \u0026lt;num_boxes, BOX_SIZE \u0026gt;\u0026gt; \u0026gt; (P, points, indices_sorted.data().get(), boxes.data().get()); Each thread has a point index. The indices indices_sorted of points are already sorted based on the Morton (z-order) code. So points within a block are similar to each other, and their distances are supposed to be small.\nG r a i i B n d l p d o o e c i x k n 1 t 0 0 2 m 4 i n t , h r m e a ⋯ a x d s B l o c k 1 1 0 2 m 4 i n t , h r m e a ⋯ a x d s ⋯ B l o m c i k n , 9 7 m a x The vector boxes records each block\u0026rsquo;s min and max coordinates:\nb 2 l x k y 0 z b 2 l x k y 1 z b 2 l x k y 2 z b 2 l x k y n z Based on the min and max coordinates, a point can quickly determine whether a block contains potential nearest neighbors.\nSpecifically, if the distance of a point to the min or max of a block is larger than reject, the block won\u0026rsquo;t be searched for possible nearest neighbors.\n1 2 3 4 5 for (int b = 0; b \u0026lt; (P + BOX_SIZE - 1) / BOX_SIZE; b++){ MinMax box = boxes[b]; float dist = distBoxPoint(box, point); if (dist \u0026gt; reject || dist \u0026gt; best[2]) continue; The reject value is the distance of the 3-rd nearest neighbor based on the initially sorted points sequence.\n1 2 3 4 5 6 7 8 9 int idx = cg::this_grid().thread_rank(); float3 point = points[indices[idx]]; float best[3] = { FLT_MAX, FLT_MAX, FLT_MAX }; for (int i = max(0, idx - 3); i \u0026lt;= min(P - 1, idx + 3); i++){ if (i == idx) continue; updateKBest\u0026lt;3\u0026gt;(point, points[indices[i]], best); } In this way, quiet amount of points are filtered out to save computation.\nIterate every points in the potential block\n1 2 3 4 5 6 7 8 9 for (int b = 0; b \u0026lt; (P + BOX_SIZE - 1) / BOX_SIZE; b++){ ... for (int i = b * BOX_SIZE; i \u0026lt; min(P, (b + 1) * BOX_SIZE); i++){ if (i == idx) continue; updateKBest\u0026lt;3\u0026gt;(point, points[indices[i]], best); } } dists[indices[idx]] = (best[0] + best[1] + best[2]) / 3.0f; Compute distance every two points:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 __device__ void updateKBest(const float3\u0026amp; ref, const float3\u0026amp; point, float* knn) { float3 d = { point.x - ref.x, point.y - ref.y, point.z - ref.z };\t// diff float dist = d.x * d.x + d.y * d.y + d.z * d.z;\t// sequare error for (int j = 0; j \u0026lt; K; j++)\t// K-nearest { if (knn[j] \u0026gt; dist)\t// closer than j { float t = knn[j];\t// tmp for previous j knn[j] = dist;\t// replace the previous j dist = t;\t// the previous j is compared with remaining neighbors } } } Test (2024-03-06)\nUbuntu 20.04, 1050Ti, cuda-11.6 (nvcc -V)\nClone: git clone https://gitlab.inria.fr/bkerbl/simple-knn.git\nEnvironment: conda env create -f environment.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 name: PCRefine channels: - pytorch - conda-forge - defaults dependencies: - python=3.10 - pip - pip: - torch==1.12.1+cu116 - torchvision==0.13.1+cu116 - --extra-index-url https://download.pytorch.org/whl/cu116 - ipykernel - plyfile Compile based on the steup.py: pip install .\nCall from python:\n1 2 3 4 5 6 7 import numpy as np import torch from simple_knn._C import distCUDA2 from utils import fetchPly pcd = fetchPly(\u0026#34;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/points3d.ply\u0026#34;) meanDist = distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()) Output the mean distance from neighbors to each point:\n1 2 (PCRefine) yi@yi:~/Downloads/simple-knn-comments$ python test_knn.py tensor([0.0013, 0.0020, 0.0008, ..., 0.0023, 0.0017, 0.0007], device=\u0026#39;cuda:0\u0026#39;) Indices (2024-03-08)\nModify the original code to return the neighbors\u0026rsquo; indices:\n1 2 3 4 5 6 7 8 (casmvsnet_pl) yi@yi:~/Downloads/CasMVSNet_pl-comments/submodules/simple-knn-comments$ python test_knn.py tensor([[41057, 23085, 16403], [21674, 59181, 31901], [18699, 99481, 15716], ..., [17352, 51604, 48154], [90929, 45350, 94932], [ 6797, 62182, 78410]], device=\u0026#39;cuda:0\u0026#39;, dtype=torch.int32) pytorch3d (2024-03-12)\nSet up environment to compile pytorch3d:\n1 2 3 4 5 conda create -n casmvsnet_pl python=3.8 pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 pip install git+https://github.com/facebookresearch/pytorch3d.git pip install -r requirements.txt Example from SC-GS\n1 nn_dist, nn_idxs, _ = pytorch3d.ops.knn_points(init_pcl[None], init_pcl[None], None, None, K=K+1) Use pytorch3d to find KNN in the lego point cloud:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np from plyfile import PlyData from typing import NamedTuple import torch import pytorch3d.ops class BasicPointCloud(NamedTuple): points : np.array colors : np.array normals : np.array # Code from: https://github.com/graphdeco-inria/gaussian-splatting def fetchPly(path): plydata = PlyData.read(path) vertices = plydata[\u0026#39;vertex\u0026#39;] # coordinates (x,y,z), normal (nx,ny,nz), r,g,b of 100K points positions = np.vstack([vertices[\u0026#39;x\u0026#39;], vertices[\u0026#39;y\u0026#39;], vertices[\u0026#39;z\u0026#39;]]).T # xyz: (3, 100K) -\u0026gt; (100K,3) colors = np.vstack([vertices[\u0026#39;red\u0026#39;], vertices[\u0026#39;green\u0026#39;], vertices[\u0026#39;blue\u0026#39;]]).T / 255.0 # rgb: (3, 100K) -\u0026gt; (100K, 3) normals = np.vstack([vertices[\u0026#39;nx\u0026#39;], vertices[\u0026#39;ny\u0026#39;], vertices[\u0026#39;nz\u0026#39;]]).T # normal xyz: (100K,3) return BasicPointCloud(points=positions, colors=colors, normals=normals) # NamedTuple pcd = fetchPly(\u0026#34;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/points3d.ply\u0026#34;) init_pcl = torch.from_numpy(np.asarray(pcd.points)).float().cuda() K=3 nn_dist, nn_idxs, _ = pytorch3d.ops.knn_points(init_pcl[None], init_pcl[None], None, None, K=K+1) # Both are (1,100k,4) print(nn_dist[0][:3]) print(nn_idxs[0][:3]) pytorch3d\u0026rsquo;s function can return nearest neighbors distance and indices at the same time. And the results are the same as the above cuda implementation:\n1 2 3 4 5 6 tensor([[0.0000, 0.0008, 0.0016, 0.0017], [0.0000, 0.0012, 0.0021, 0.0026], [0.0000, 0.0006, 0.0008, 0.0009]], device=\u0026#39;cuda:0\u0026#39;) tensor([[ 0, 41057, 23085, 16403], [ 1, 21674, 59181, 31901], [ 2, 18699, 99481, 15716]], device=\u0026#39;cuda:0\u0026#39;) I also tested the point cloud predicted by casmvsnet_pl, but this time the pytorch3d is so much slower than the cuda code (1h). Dont\u0026rsquo;t know why.\nResults are the same: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 scan1 contains 9.03 M points # cuda result nn indices: tensor([[2654648, 181848, 2654649], [1825888, 2654649, 182085], [1825889, 2025037, 182085], ..., [1788118, 5669911, 1996297], [2195795, 2814578, 1568896], [2814579, 2597015, 2195796]], device=\u0026#39;cuda:0\u0026#39;, dtype=torch.int32) # pytorch3d result nn indices: (1, 9030200, 4) tensor([[[ 0, 2654648, 181848, 2654649], [ 1, 1825888, 2654649, 182085], [ 2, 1825889, 2025037, 182085], ..., [9030197, 1788118, 5669911, 1996297], [9030198, 2195795, 2814578, 1568896], [9030199, 2814579, 2597015, 2195796]]], device=\u0026#39;cuda:0\u0026#39;) print(nn_dist) tensor([[[0.0000, 0.0137, 0.0167, 0.3456], [0.0000, 0.2516, 0.3126, 0.3652], [0.0000, 0.0262, 0.0991, 0.2446], ..., [0.0000, 0.0775, 0.0991, 0.1228], [0.0000, 0.0577, 0.0749, 0.0777], [0.0000, 0.0449, 0.1124, 0.1350]]], device=\u0026#39;cuda:0\u0026#39;) In simple-knn, the distance dist is a square error:\n1 2 float3 d = { point.x - ref.x, point.y - ref.y, point.z - ref.z }; float dist = d.x * d.x + d.y * d.y + d.z * d.z; ","date":"2024-02-29T11:50:00Z","permalink":"https://zichen34.github.io/writenotes/algo/knn/","title":"memo: algo | KNN"},{"content":"Explore (2024-02-24)\nDDG search: \u0026ldquo;point cloud fusion\u0026rdquo;, found the wikipedia page of \u0026ldquo;Point cloud\u0026rdquo;, where the caption of the illustration 3 mentioned: \u0026ldquo;reconstructing 3D shapes from multi-view depth map\u0026rdquo;. That picture is from:\nSynthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes with Deep Generative Networks ~ CVPR 2017\nCode | Amir A. Soltani, Jiajun Wu\nThen I searched the paper\u0026rsquo;s title in DDG , and I found:\nRender4Completion: Synthesizing Multi-View Depth Maps for 3D Shape Completion ~ ICCV 2019 workshop\nArxiv | CoolPapers | Tao Hu, Zhizhong Han, Matthias Zwicker\nData: Render multi-view depth maps for incomplete object\nTrain: 8-view incomplete depth map -\u0026gt; NN -\u0026gt; 8-view completed depth map\n3D shape: Unproject 8 depth maps\nLearning Implicit Fields for Generative Shape Modeling ~ CVPR 2019\nArxiv | Zhiqin Chen, Hao Zhang, SFU\nImproved Modeling of 3D Shapes with Multi-view Depth Maps ~ 3DV 2020\nCode | CoolPapers | Kamal Gupta, Matthias Zwicker\nsingle view GAN ","date":"2024-02-24T20:25:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/c-symp-cpltpc/","title":"sympo: Complete Point Clouds"},{"content":" Depth Completion Depth Completion - Paper with code\nLearning Guided Convolutional Network for Depth Completion\nArxiv | Code | CoolPapers | Jie Tang, Ping Tan\n(2024-02-21)\nDynamic convolution kernel Depth Super-Resolution PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation\nCode | ProjPage | Zhenyu Li, Peter Wonka\n(2024-02-23)\nJoint Implicit Image Function for Guided Depth Super-Resolution ~ ACM MM 2021\nCode | Arxiv | Jiaxiang Tang\n(2024-02-22)\n","date":"2024-02-21T12:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/c-symp-dmapest/","title":"sympo: Point Cloud from Depth Maps"},{"content":"Restoring point cloud: Fuse depth maps of multi-views, and then unproject the depth map to 3D points.\nMVSNet-PyTorch Source code: MVSNet-PyTorch\nDownload pretrained model checkpoint\n1 gdown 1j2I_LNKb9JeCl6wdA7hh8z1WgVQZfLU9 -O ./checkpoints/pretrained/MVSNet-Pytorch_model_000014.ckpt Download preprocessed testing data of MVSNet\n1 2 gdown 135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ -O /data2/zichen/MVSNet_testing_dtu.zip unzip /data2/zichen/MVSNet_testing_dtu.zip -d /data2/zichen/MVSNet_testing Modify data paths in eval.sh, and uncomment the line#302: save_depth() in \u0026ldquo;eval.py\u0026rdquo; to generate depth map first. And then run the script: ./eval.sh.\nThis program requires 15GB VRAM. so 1080Ti can\u0026rsquo;t run it.\nThe resolution of the test images (3x1184x1600; Feat map: 32x296x400) is too high to be processed by 1080Ti with 11 GB VRAM.\nWhen processing the 2nd source view, OOM occurs at:\n1 2 3 warped_src_fea = F.grid_sample( src_fea, grid.view(batch, num_depth * height, width, 2), mode=\u0026#39;bilinear\u0026#39;, padding_mode=\u0026#39;zeros\u0026#39;) (2024-02-23) If executing the eval.py with the training images, the Dataset data_yao_eval may be mismatched with the training data folder.\nVisualization: \u0026ldquo;outputs/mvsnet001_l3.ply\u0026rdquo;\nThere are noise point around the round edge of the bowl.\nUse Matlab to compute the quantitative Metrics:\nThe DTU dataset (\u0026ldquo;DTU_SampleSet.zip\u0026rdquo;) provides the \u0026ldquo;Matlab evaluation code/\u0026rdquo; to assess the quality of point clouds.\nBased on the SampleSet directory structure:\n1 2 3 4 5 6 7 8 9 10 11 (base) yi@yi:~$ tree /mnt/data2_zichen/SampleSet/ -d -L 2 /mnt/data2_zichen/SampleSet/ ├── Matlab evaluation code │ └── MeshSupSamp_web └── MVS Data ├── Calibration ├── Cleaned ├── ObsMask # 3D parts used for evaluation ├── Points # ground truth point clouds for each scan ├── Rectified └── Surfaces # Poisson reconstruction To evaluate point clouds of all scans, the folder \u0026ldquo;Points\u0026rdquo; needs to be the full version. Modify the paths and specify the scan to be evaluated in the matlab code:\n1 2 3 4 5 6 7 8 % ground-truth data: dataPath=\u0026#39;/mnt/data2_zichen/SampleSet/MVS Data\u0026#39;; % the dir storing testing .ply files: plyPath=\u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained\u0026#39;; % .mat fill will be saved here: resultsPath=\u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained\u0026#39;; - - - UsedSets=[1] Run BaseEvalMain_web in matlab:\nOutput 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt;\u0026gt;\u0026gt; BaseEvalMain_web cSet = 1 DataInName = \u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained/mvsnet001_l3.ply\u0026#39; EvalName = \u0026#39;/mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrainedmvsnet_Eval_1.mat\u0026#39; /mnt/Server/Downloads/MVSNet_pytorch-comments/outputs/pretrained/mvsnet001_l3.ply ans = 19 14 Elapsed time is 58.298360 seconds. downsample factor: 1.6901 Elapsed time is 18.930595 seconds. Computing Data 2 Stl distances Elapsed time is 83.304838 seconds. Computing Stl 2 Data distances Distances computed Elapsed time is 42.517009 seconds. Saving results Elapsed time is 42.864935 seconds. Elapsed time is 52.741538 seconds. ans = 19 18 mean/median Data (acc.) 0.254528/0.180807 mean/median Stl (comp.) 0.254594/0.218734 The samller the values, the better. CasMVSNet-pl (2024-02-23)\nSource code: CasMVSNet_pl - AIkui\nEnvironment Depth_raw:\nThe MVSNeRF inherited some codes of this repo, thus, in mvsnerf\u0026rsquo;s training dataset, the directory \u0026ldquo;Depths/\u0026rdquo; under \u0026ldquo;mvs_training/dtu/\u0026rdquo; is also replaced with the folder \u0026ldquo;Depths/\u0026rdquo; unzipped from \u0026ldquo;Depth_raw.zip\u0026rdquo;\nCreate environment:\n(2024-04-04)\n1 2 3 4 conda create -n casmvsnet_pl python=3.8 conda activate casmvsnet_pl pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 pip install -r requirements.txt Python version: Environment casmvsnet_pl requires python 3.7 for pytorch 1.4.0, and the packages listed in requirements.txt. If using python 3.10, the available pytorch version is higher than 1.11. (2024-04-07)\nEnv on Ubuntu 22.04 ctk-11.6 in China:\n1 2 3 4 conda create -n casmvsnet_pl python=3.8 # pip always breaks when downloading torch. conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.6 -c pytorch -c conda-forge pip install -r requirements.txt # comment torch entries I cannot compile inplace-abn via pip with erros:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/utils/cpp_extension.py:820: UserWarning: There are no g++ version bounds defined for CUDA version 11.6 warnings.warn(f\u0026#39;There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}\u0026#39;) building \u0026#39;inplace_abn._backend\u0026#39; extension gcc -pthread -B /home/zichen/anaconda3/envs/casmvsnet_pl/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn.cpp -o build/temp.linux-x86_64-cpython-38/src/inplace_abn.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14 cc1plus: warning: command-line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ gcc -pthread -B /home/zichen/anaconda3/envs/casmvsnet_pl/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn_cpu.cpp -o build/temp.linux-x86_64-cpython-38/src/inplace_abn_cpu.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14 cc1plus: warning: command-line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ /usr/local/cuda-11.6/bin/nvcc -DWITH_CUDA=1 -I/home/zichen/Downloads/inplace_abn/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda-11.6/include -I/home/zichen/anaconda3/envs/casmvsnet_pl/include/python3.8 -c src/inplace_abn_cuda.cu -o build/temp.linux-x86_64-cpython-38/src/inplace_abn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1013\\\u0026#34; -DTORCH_EXTENSION_NAME=_backend -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -std=c++14 /home/zichen/anaconda3/envs/casmvsnet_pl/lib/python3.8/site-packages/torch/include/c10/core/SymInt.h(84): warning #68-D: integer conversion resulted in a change of sign /usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with ‘...’: 435 | function(_Functor\u0026amp;\u0026amp; __f) | ^ /usr/include/c++/11/bits/std_function.h:435:145: note: ‘_ArgTypes’ /usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’: 530 | operator=(_Functor\u0026amp;\u0026amp; __f) | ^ /usr/include/c++/11/bits/std_function.h:530:146: note: ‘_ArgTypes’ error: command \u0026#39;/usr/local/cuda-11.6/bin/nvcc\u0026#39; failed with exit code 1 Then, I tried to install it through compiling the source code. Unfortunately, the same error ocurred.\nI compiled the source code on Ubuntu 20.04 and ctk-11.6 (I had created a same conda env using the above 2 lines. When using the pip install manner, it will find the previous cache of the inplace-abn package on the machine.). Superisingly, the compilation suceeded.\nThe crucial difference between two trials could be the gcc version gcc --version. The return values are:\ngcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 After I changed to gcc-9 on Ubuntu 22.04, the compilation for inplace-abn successed.\nI was reminded to check gcc due to chatGPT (prompted with the error), and the issuse CUDA version (12.1) #232 and this issue asked gcc: failed install #143\nThere is another solution: inplace_abn安装报错？来看看这篇避坑指南吧！ But I didn\u0026rsquo;t try it.\nHow to change gcc version:\nRef: How to switch between multiple GCC and G++ compiler versions on Ubuntu 22.04 LTS Jammy Jellyfish\nThere are multiple gcc versions under /usr/bin/.\nBut, they didn\u0026rsquo;t show up:\n1 2 (casmvsnet_pl_) z@homepc:~/Downloads/inplace_abn$ update-alternatives --list gcc update-alternatives: error: no alternatives for gcc Install GCC 9: sudo apt-get install gcc-9 g++-9.\nIt\u0026rsquo;s said the 2 lines below are creating list for multiple gcc and g++:\n1 2 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 9 But once I exectuted them, the gcc --version has changed to gcc 9.5.0\nThe gcc and g++ will revert to 11.4 by executing:\n1 2 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 11 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 11 Later, the versions can be chosen from a list:\nsudo update-alternatives --config gcc\nNot found GPU:\nAlthough the pytorch 1.4.0 (installed by pip) was compiled with cuda 10.1 (download), the cudatoolkit (nvcc -V) on the PATH is not neccessary to match the cuda version.\nFor example, with using the same conda environemnt, Alien-PC (Ubuntu 20.04) using cuda-11.6 and the lambda server (Ubuntu 18.04) using cuda-10.2 both can detect gpu and run.\nThe error: torch.cuda.is_available() returns Falsehere is not due to cuda, but my debug settings!!! Fuck!! the CUDA_VISIBLE_DEVICES was 6 for server, which should be 0 on my PC.\n(2024-04-13)\nCreate an environment again on Lambda server: Ubuntu 18.04, cudatoolkit 11.6 (nvcc -V), and gcc, g++ both are 7.5.0 (gcc -v)\n1 2 3 4 5 6 7 8 9 10 (casmvsnet_pl_py38) z@lambda-server:~/Downloads/CasMVSNet_pl-comments$ g++ -v Using built-in specs. COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper OFFLOAD_TARGET_NAMES=nvptx-none OFFLOAD_TARGET_DEFAULT=1 Target: x86_64-linux-gnu Configured with: ../src/configure -v --with-pkgversion=\u0026#39;Ubuntu 7.5.0-3ubuntu1~18.04\u0026#39; --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu Thread model: posix gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04) The pytorch is not install separately, just using the requirements.txt as below. Git commit\n1 2 3 conda create -n casmvsnet_pl_py38 python=3.8 conda activate casmvsnet_pl_py38 pip install -r requirements.txt Evaluation Evaluation and Depth Map Fusion: evalutions/\nDownload pretrained model checkpoint\n1 2 3 4 mkdir -p ckpts wget https://github.com/kwea123/CasMVSNet_pl/releases/download/v1.0/exp2.zip -P ckpts/ unzip ckpts/exp2.zip -d ckpts/ wget https://github.com/kwea123/CasMVSNet_pl/releases/download/v1.0/_ckpt_epoch_10.ckpt -P ckpts/exp2 Specify testing data\nI only left scan9 in \u0026ldquo;lists/test.txt\u0026rdquo; for a quick peek.\nThe code has some path templates that don\u0026rsquo;t match the DTU testing data directory structure.\nChanges datasets/dtu.py, build_metas() (line #40-42):\n1 2 3 pair_file = \u0026#34;pair.txt\u0026#34; # Make 49 * 22 testing pairs for scan in self.scans: # add `scan` into path with open(os.path.join(self.root_dir, scan, pair_file)) as f: datasets/dtu.py, build_proj_mats (Line #58-59):\n1 2 3 # all scans have the same cams, so hardcard \u0026#39;scan1\u0026#39; here: proj_mat_filename = os.path.join(self.root_dir, \u0026#39;scan1\u0026#39;, f\u0026#39;cams/{vid:08d}_cam.txt\u0026#39;) datasets/dtu.py, __getitem__() (Line #165-166):\n1 2 3 # Add `scan` into path: img_filename = os.path.join(self.root_dir, scan, f\u0026#39;images/{vid:08d}.jpg\u0026#39;) eval.py, read_image() (line #87):\n1 2 if dataset_name == \u0026#39;dtu\u0026#39;: return cv2.imread(os.path.join(root_dir, scan, f\u0026#39;images/{vid:08d}.jpg\u0026#39;)) eval.py, (Line #352): set name format to align the matlab code (of the MVSNet_pytorch).\n1 PlyData([el]).write(f\u0026#39;{point_dir}/scan{int(scan[4:]):03d}_l3.ply\u0026#39;) Execute \u0026ldquo;eval.py\u0026rdquo;. (1080Ti is okay.)\n1 CUDA_VISIBLE_DEVICES=5 python eval.py --root_dir /data2/zichen/MVSNet_testing/dtu Visualization:\nImport the result \u0026ldquo;results/dtu/points/scan9.ply\u0026rdquo; into MeshLab:\ncasmvsnet mvsnet Some floaters will be exposed when looking at the point cloud from novel views.\nCasMVSNet-pl has less noise points than MVSNet-pytorch.\nModify the paths in \u0026ldquo;BaseEvalMain_web.m\u0026rdquo; (of the MVSNet_pytorch):\n1 2 3 4 5 6 dataPath=\u0026#39;/mnt/data2_zichen/SampleSet/MVS Data\u0026#39;; % GT plyPath=\u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points\u0026#39;; % pred resultsPath=\u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results\u0026#39;; % store .mat method_string=\u0026#39;scan\u0026#39;; - - - UsedSets=[9] Output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cSet = 9 DataInName = \u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points/scan009_l3.ply\u0026#39; EvalName = \u0026#39;/mnt/Server/Downloads/CasMVSNet_pl/resultsscan_Eval_9.mat\u0026#39; /mnt/Server/Downloads/CasMVSNet_pl/results/dtu/points/scan009_l3.ply ans = 0 35 Elapsed time is 412.622045 seconds. downsample factor: 5.5621 Elapsed time is 285.285496 seconds. Computing Data 2 Stl distances Elapsed time is 101.764399 seconds. Computing Stl 2 Data distances Distances computed Elapsed time is 106.225475 seconds. Saving results Elapsed time is 106.982512 seconds. Elapsed time is 126.089258 seconds. ans = 0 52 mean/median Data (acc.) 0.357586/0.229884 mean/median Stl (comp.) 0.304593/0.177679 Evaluate all testing scans\u0026rsquo; point clouds in matlab and save outputs into write.txt for calculating average performance:\n1 2 3 4 5 6 7 8 9 10 11 12 import re import numpy as np means_acc = [] means_comp = [] with open(\u0026#34;/mnt/Server/Downloads/MVSNet_pytorch-comments/evaluations/dtu/write.txt\u0026#34;, \u0026#34;r\u0026#34;) as file: for line in file.readlines(): if \u0026#39;acc.\u0026#39; in line: means_acc.append( eval(re.findall(\u0026#39;\\d+\\.\\d+|\\d+\u0026#39;,line)[0]) ) elif \u0026#39;comp.\u0026#39; in line: means_comp.append( eval(re.findall(\u0026#39;\\d+\\.\\d+|\\d+\u0026#39;,line)[0]) ) print(np.mean(means_acc), np.mean(means_comp)) Output: 0.36399536, 0.36997940\nResults of each scan scans accuracy Completness num pts 1 0.235012/0.177113 0.219228/0.176128 29.40M 4 0.267118/0.198705 0.376820/0.192314 23.06M 9 0.312725/0.196551 0.204539/0.169905 25.38M 10 23.63M 11 19.57M 12 21.47M 13 22.35M 15 26.06M 23 29.97M 24 24.95M 29 19.67M 32 22.06M 33 16.67M 34 28.18M 48 15.82M 49 19.90M 62 22.97M 75 18.60M 77 7.46 M 110 27.34M 114 0.223317/0.169992 0.249666/0.178679 31.52M 118 0.243320/0.183991 0.403758/0.195397 30.18M avg 22 (2024-04-03)\nVisualization for scan1:\nThe Moiré pattern (摩尔纹) resulted from insufficint sampling frequency is wave interference. Eval LLFF Data (2024-04-15)\nConvert LLFF dataset to the format of MVSNet.\nUse Colmap to solve the LLFF images: Colmap Tutorial; ytb (The camera poses in NeRF are also recovered by Colmap with imgs2poses), and then use the script colmap2mvsnet.py provided by MVSNet to obtain matched format.\nDepth map fusion (2024-03-06)\nCode-CasMVSNet_pl\n\\begin{algorithm} \\caption{Depth map fusion} \\begin{algorithmic} \\FOR{view=1 \\TO 49} \\STATE Read reference image \\FOR{src=1 \\TO 10} \\STATE Warp src depth map to be seen from the ref view \\STATE Reproject the warped dMap\\_src onto dMap\\_ref \\IF {Pixel diff \u003c 1 pix \\AND depth error \u003c 0.01} \\STATE mask\\_geo = 1 \\ENDIF \\ENDFOR \\STATE Average reprojected ref depth maps and images \\STATE Unproject the mean ref depth map to point cloud in world space \\ENDFOR \\STATE Output 49 point clouds \\end{algorithmic} \\end{algorithm} Procedure description refer to paper MVSNet sec 4.2 The fused depth map of a ref view is the average of refined depth maps after applying the photometric and geometric constraints on each pair of the ref and a src view\nApply the final mask on the depth_est_averaged, then unproject pixels on dpeth map to world space.\nBecause there are 49 views for a scan, the output .ply file is a combination of 49 point clouds.\nCasMVSNet depth map fusion process: With substituing the predicted depth into homography, the warped source image aligns with the reference image pretty much.\nSeen from the valid-depth mask, some highlight spots are missed.\nCasMVSnet-pl averaged the color of 10 source views that have performed homography transformation. (desc)\n1 2 image_refined_ = \\ np.sum(image_src2refs, 0)/np.expand_dims((mask_geo_sum+1), -1) Whereas, MVSNet-pytorch only casts the color of the refernce view.\n1 color = ref_img[1:-16:4, 1::4, :][valid_points] # hardcoded for DTU dataset ","date":"2024-02-21T09:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/mvs/b-test-mvs_ply/","title":"test: MVSNets | Restore Point Cloud"},{"content":"Conduct 3D reconstruction through depth map fusion.\nSolution categories\nExplicitly optimization\nDepp learning-based methods\nJourney when searching \u0026ldquo;Depthmap fusion with depth and normal consistency check\u0026rdquo; DDG\nDepthmap fusion with depth and normal consistency check\nMassively Parallel Multiview Stereopsis by Surface Normal Diffusion (ICCV 2015)\nPaper | GScholar\nCode for Gipuma kysucix/gipuma Code for Section 4.3 Fusion: kysucix/fusibile (2024-03-16)\n4.3 Fusion\nObtain depth map for each view Iterate each depth map to be treated as the reference image and perform reprojection. Check if the disparity between the reference and source depth maps is lower than a threshold, so as to determine valid 3D points. Check if the estimated surface normal direction consistent among all views. Check the if reprojected depth on other view is consistent with the reference view. Accuracy and completeness is a trade-off.\nRoutedFusion: Learning Real-time Depth Map Fusion ~ CVPR 2020\nArxiv\n(2024-03-15)\n2D depth routing network + 3D depth fusion network Less training data A Review of Depth and Normal Fusion Algorithms\npmc | Sensors MDPI\nDepth (and Normal) Map Fusion Algorithm - GitHub\nCode\nMulti-view stereo via depth map fusion: A coordinate decent optimization method (CoD-Fusion)\nNeurocomputing 2015 | Zhaoxin Li, HIT\n(2024-03-15)\nGoal of depth map fusion is recovering the surface.\nUse coordinate descent to decompose the energy functional (泛函) for each voxel.\n3D total variance TV is the loss function. So the Objective function = TV + regularizer L. Naming refers to Fast Optimization Methods for L1 Regularization - ECML07 and MIT courseware\nScene Representation: TSDF for surface reconstruction.\nDepth maps are generated based on binocular stereo matching using library libelas.\nLeverage TSDFs to fuse depth maps\nEq (1) is an expection of all-view TSDFs on a voxel. It fails for nosiy depth maps due to lack of spatial regularizer.\n$$ min \\ \\int_B ∑_{i∈ Φ(𝐱)} w_i (u - f_i)² d𝐱 $$\nIntegral over all voxels in the bounding box $B$ $fᵢ$ is a TSDF set generated from the i-th depth map. Each TSDF corresponds a weight $wᵢ$ representing its reliability. $Φ$ is the filtered TSDF set for a voxel $𝐱=B(i,j,k)$. And the original set is $F$ that contains TSDFs on 𝐱 from all depth maps. u is the resultant TSDF. It will finally be the weighted mean of depth values on all dpeth maps. ","date":"2024-02-20T23:54:00Z","permalink":"https://zichen34.github.io/writenotes/model/mvs/c-symp-dmapfuse/","title":"sympo: MVS | Depth Maps Fusion"},{"content":"Create Matrix mat3 (2024-02-01)\nType: typedef mat3x3 mat3 API - GLM: Types\nPassing a single scalar will initialize the diagonal. OpenGL Mathematics (GLM)\nOpenGL矩阵运算——GLM库的使用 - CSDN\nExample: 3DGS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;glm/glm.hpp\u0026gt; #include\u0026lt;glm/gtx/string_cast.hpp\u0026gt; #include \u0026lt;iostream\u0026gt; int main(){ glm::mat3 S = glm::mat3(1.0f); // identity matrix // Make scaling matrix S[0][0] = 2; S[1][1] = 3; S[2][2] = 4; std::cout \u0026lt;\u0026lt; glm::to_string(S) \u0026lt;\u0026lt; std::endl; return 0; } Output 1 2 3 (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ g++ test_glm.cpp (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ ./a.out mat3x3((2.000000, 0.000000, 0.000000), (0.000000, 3.000000, 0.000000), (0.000000, 0.000000, 4.000000)) Refer to How do I print vector values of type glm::vec3 that have been passed by reference? - SO\nUse glm::to_string(). Episode 18 - OpenGL Math - Introduction to the GLM Library - Modern OpenGL - Mike Shah\n","date":"2024-02-01T14:50:00Z","permalink":"https://zichen34.github.io/writenotes/lib/glm/","title":"memo: glm | Basics"},{"content":"Introducing CUDA UnBound (CUB) - Microway\nInclusiveSum Docs\n(2024-01-27)\n1 2 d_in: [8 , 6 , 7 , 5 , 3 , 0 , 9 ] d_out: [8 , 14 , 21 , 26 , 29 , 29 , 38 ] Example from 3DGS\n1 2 3 4 5 6 cub::DeviceScan::InclusiveSum( nullptr, // No memory is allocated geom.scan_size, // num of bytes needed to reserve geom.tiles_touched, // input sequence geom.tiles_touched, // becomes prefix sums P) // number of items in the sequence, 100K points Given a block of P bytes of memory pointed to by geom.tiles_touched, obtain geom.scan_size indicating the number of bytes for temporay storage required to reserve for executing the function: SortPairs Docs\n(2024-02-03)\nSort key-value pairs according to keys\nExample in diff-rast-gass\n1 cub::DeviceRadixSort::SortPairs (2024-03-05)\nExample in simple-KNN of 3DGS\nClustering points (indices) based on morton codes morton:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Determine the number of bytes for temp_storage cub::DeviceRadixSort::SortPairs( nullptr, temp_storage_bytes, morton.data().get(), // in: keys morton_sorted.data().get(), // out: sorted keys indices.data().get(), // in: values indices_sorted.data().get(), // out: sorted values P); // Allocate memory for temp_storage temp_storage.resize(temp_storage_bytes); // Sort the pairs based on keys cub::DeviceRadixSort::SortPairs( temp_storage.data().get(), temp_storage_bytes, morton.data().get(), morton_sorted.data().get(), indices.data().get(), indices_sorted.data().get(), P); Reduce min (2024-02-28)\nTo execute this function, temp_storage_bytes is required to be reserved. Therefore, this function needs to run twice:\nGet the number of required temporary bytes, with specifying d_temp_storage as nullptr;\nExecute the function again given the correct address d_temp_storage\nA simplest code: \u0026ldquo;cub_reduce.cu\u0026rdquo;\nFull code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include \u0026lt;cub/cub.cuh\u0026gt; // CustomMin functor struct CustomMin { template \u0026lt;typename T\u0026gt; __device__ __forceinline__ T operator()(const T \u0026amp;a, const T \u0026amp;b) const { return (b \u0026lt; a) ? b : a; } }; // Declare, allocate, and initialize device-accessible pointers for // input and output int N = 7; // num of items int d_in[7] = {8, 6, 7, 5, 3, 1, 9}; int d_out[1] = {0}; // the min value CustomMin min_op; int init = INT_MAX; // To be compared initially int main(){ int *in, *out; cudaMalloc(\u0026amp;in, sizeof(int)*N); cudaMalloc(\u0026amp;out, sizeof(int)); cudaMemcpy(in, d_in, sizeof(int)*N, cudaMemcpyHostToDevice); // Determine temporary device storage requirements void *d_temp_storage = NULL; size_t temp_storage_bytes = 0; cub::DeviceReduce::Reduce( d_temp_storage, temp_storage_bytes, in, out, N, min_op, init); cudaDeviceSynchronize(); // Allocate temporary storage cudaMalloc(\u0026amp;d_temp_storage, temp_storage_bytes); // Run reduction cub::DeviceReduce::Reduce( d_temp_storage, temp_storage_bytes, in, out, N, min_op, init); cudaDeviceSynchronize(); int min; cudaMemcpy(\u0026amp;min, out, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;The min is: %d\\n\u0026#34;, min); // 1 // Free allocated memory cudaFree(d_temp_storage); return 0; } Notes:\nIf the suffix is .cpp and when compiling with nvcc cub_reduce.cpp, there will be errors about Debug:\n1 2 3 4 5 6 /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/block/specializations/../../block/../util_debug.cuh:101:43: error: ‘Debug’ is not a member of ‘cub’ 101 | #define CubDebug(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__) | ^~~~~ /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/util_allocator.cuh:695:17: note: in expansion of macro ‘CubDebug’ 695 | if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error; | ^~~~~~~~ __host__ __forceline function cannot be called, need __device__\n1 2 3 4 5 6 /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/device/dispatch/dispatch_reduce.cuh(145): error: calling a __host__ function(\u0026#34;T1 CustomMin::operator ()\u0026lt;int\u0026gt; (const T1 \u0026amp;, const T1 \u0026amp;) const\u0026#34;) from a __global__ function( \u0026#34;cub::DeviceReduceSingleTileKernel\u0026lt; ::cub::DeviceReducePolicy\u0026lt;int, int, int, ::CustomMin\u0026gt; ::Policy600, int *, int *, int, ::CustomMin, int\u0026gt; \u0026#34;) is not allowed /usr/local/cuda-11.6/bin/../targets/x86_64-linux/include/cub/device/dispatch/dispatch_reduce.cuh(145): error: identifier \u0026#34;CustomMin::operator ()\u0026lt;int\u0026gt; const\u0026#34; is undefined in device code The customize min_op (or CustomMin()) can be changed to cub::Sum() to solve the sum of input data.\nRef:\ncub::DeviceReduce — CUB 104.0 documentation\nCUDA高性能计算经典问题（一）—— 归约（Reduction） - Will Zhang的文章 - 知乎\nCUDA编程模型系列八(原子操作 / 规约 / 向量元素求和) - 扫地的小何尚 (何琨) Found by DDG searching \u0026ldquo;cuda cub 例子\u0026rdquo;\nsum (2024-03-04)\nMove the input data onto GPU first! Otherwise, it won\u0026rsquo;t be processed and the output variable won\u0026rsquo;t update and remains the initialized value, e.g., 0.\nMove the output data back to host for printing! Otherwise, the gpu memory is not allowed to access and return segment fault.\nFull code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #include \u0026lt;cub/cub.cuh\u0026gt; int N=7; // num of items int d_in[7] = {8, 6, 7, 5, 3, 0, 9}; int d_out[1]={0}; int main(){ int *in, *out; cudaMalloc(\u0026amp;in, sizeof(int)*N); // allocate memory cudaMalloc(\u0026amp;out, sizeof(int)); cudaMemcpy(in, d_in, sizeof(int)*N, cudaMemcpyHostToDevice); // Determine temporary device storage requirements void *d_temp_storage = NULL; size_t temp_storage_bytes = 0; cub::DeviceReduce::Sum( d_temp_storage, temp_storage_bytes, in, out, N); // Allocate temporary storage cudaMalloc(\u0026amp;d_temp_storage, temp_storage_bytes); // Run sum-reduction cub::DeviceReduce::Sum( d_temp_storage, temp_storage_bytes, in, out, N); int sum=0; cudaMemcpy(\u0026amp;sum, out, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;%d\\n\u0026#34;, sum); } Ref:\nDocs-Snippest\nExample Code Reference by this SO question: Sum reduction with CUB surfaced by DDG with searching: \u0026ldquo;cub::DeviceReduce::Reduce example tutorial\u0026rdquo;\n","date":"2024-01-27T15:35:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/cub/","title":"memo: CUDA | CUB"},{"content":"White background Code from 3DGS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pathlib import Path from PIL import Image image_path = Path(path) / cam_name # \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego/./train/r_0.png\u0026#39; image_name = Path(cam_name).stem # r_0 image = Image.open(image_path) # \u0026lt;class \u0026#39;PIL.PngImagePlugin.PngImageFile\u0026#39;\u0026gt; im_data = np.array(image.convert(\u0026#34;RGBA\u0026#34;)) # (800,800,4) bg = np.array([1,1,1]) if white_background else np.array([0, 0, 0]) # Normalize to [0,1] norm_data = im_data / 255.0 # Re-composite img_arr = norm_data[...,:3] * norm_data[...,-1:] + bg * (1-norm_data[...,-1:]) # Convert to image img_rgb = Image.fromarray(np.array(img_arr * 255.0, dtype=np.byte), \u0026#39;RGB\u0026#39;) Scaling image Zooming image requires changing the focal lengths together, while cropping image doesn\u0026rsquo;t need.\nDownsize h and w, the focal also downscales, see NeRF:\n1 2 3 4 5 6 factor 4 # shrink raw image to 1/4 args = \u0026#39; \u0026#39;.join([\u0026#39;mogrify\u0026#39;, \u0026#39;-resize\u0026#39;, f\u0026#39;{100./factor}%\u0026#39;, \u0026#39;-format\u0026#39;, \u0026#39;png\u0026#39;, \u0026#39;*.{}\u0026#39;.format(ext)]) ... # the 5th column is hwf poses[:2, 4, :] = np.array(sh[:2]).reshape([2, 1]) # hw poses[2, 4, :] = poses[2, 4, :] * 1./factor # focal A b l e t f o s e e 4 b l k s . C l o s e r : o ½ n f l y 1 t i l e f i l l s i n e y e s Example: CasMVSNet has 3 levels of feature maps, so the first 2 rows of the camera intrinsics are scaled up along with the image size increases:\n1 2 for l in reversed(range(self.levels)): intrinsics[:2] *= 2 # 1/4-\u0026gt;1/2-\u0026gt;1 Crop a patch doesn\u0026rsquo;t affect focals referring to GNT\nToTensor 1 from torchvision import transforms fov Code from 3DGS\ni m g f p l a n e l e f t z n _ e n a e r a r p l a n e r i g h t Field of view: fovX = $2* arctan(\\frac{width}{2f})$ Near plane\u0026rsquo;s right boundary: $z_{near} * tan(fovX)$ Convert fov to focal\n1 2 def fov2focal(fovX, width): # 1111.11103, 800 return width / (2 * math.tan(fov / 2)) Near plane computed from fov\n1 2 3 4 5 6 7 tanHalfFovY = math.tan((fovY / 2)) tanHalfFovX = math.tan((fovX / 2)) top = tanHalfFovY * znear bottom = -top right = tanHalfFovX * znear left = -right Pixel Coords (2024-03-14)\nnp.mgrid. Example from casmvsnet_pl\n1 2 3 xy_ref = np.mgrid[:args.img_wh[1],:args.img_wh[0]][::-1] # (2, args.img_h, args.img_w) # restore depth for (x,y): xyz_ref = np.vstack((xy_ref, np.ones_like(xy_ref[:1]))) * depth_refined[ref_vid] # (3:xyz, h,w) np.meshgrid. Example form MVSNet_pytorch\n1 2 3 4 5 xx, yy = np.meshgrid(np.arange(0, width), np.arange(0, height)) print(\u0026#34;yy\u0026#34;, yy.max(), yy.min()) yy = yy.reshape([-1]) xx = xx.reshape([-1]) X = np.vstack((xx, yy, np.ones_like(xx))) torch.meshgrid. Example form MVSNet_pytorch\n1 2 3 4 5 6 y, x = torch.meshgrid([torch.arange(0, height), torch.arange(0, width)]) y, x = y.contiguous(), x.contiguous() y, x = y.view(height * width), x.view(height * width) xyz = torch.stack((x, y, torch.ones_like(x))) # [3, H*W] xyz = torch.unsqueeze(xyz, 0).repeat(batch, 1, 1) # [B, 3, H*W] torch.cartesian_prod referred by Docs\n1 2 3 h, w = ref.shape[:2] vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed Write Image (2024-04-02)\nExample of cv2 in casmvsnet_pl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from PIL import Image # pillow import cv2 import numpy as np root_dir = \u0026#34;/mnt/data2_z/MVSNet_testing/dtu\u0026#34; img_path = f\u0026#39;{root_dir}/scan1/images/00000000.jpg\u0026#39; img = np.array(Image.open(img_path)) # RGB fig, ax = plt.subplots(1,2) ax[0].imshow(img) cv2.imwrite(f\u0026#39;1.png\u0026#39;, img[:,:,::-1]) # save ax[1].imshow(cv2.imread(img_path)) # nd array, BGR img_read = cv2.imread(img_path)[:,:, ::-1] # RGB print((img_read == img).all()) ","date":"2024-01-22T17:25:00Z","permalink":"https://zichen34.github.io/writenotes/vis/image_rw/","title":"memo: Vis | Image Read/Write"},{"content":"(Feature image credits: GS big picture, please say thanks to GS.#419 - yuedajiong)\ngaussian-splatting python Source code\nDefine Params (2024-01-22)\nCreate PyTorch tensors for Gaussians\u0026rsquo; parameters at GaussianModel() by __init__ them with torch.empty(0):\n_xyz (Gaussian center), _features_dc and _features_rest (SH coeffs), _scaling (vec3), _rotation(quaternion), sh_degree (color), _opacity (unweighted alpha)\nEach covariance matrix 𝚺 is built from a scaling vector and a quaternion:\ns q c u a a l t i e n r g n i v o e n c S R t o r t e a c t h i i o n n g m m a a t t r r i i x x 𝐑 𝐒 ↗ ↘ 𝚺 = 𝐑 𝐒 𝐒 ᵀ 𝐑 ᵀ t l r o i w a e n r g ? l e ` s y m m ` symm is [Σ₀₀, Σ₀₁, Σ₀₂, Σ₁₁, Σ₁₂, Σ₂₂]\nRead point cloud and cameras at Scene() by calling __init__\nSceneInfo:\nBasic point cloud: points(x,y,z), normals (nx,ny,nz), colors (r,g,b)\nCameraInfo: extrinsics (R,T), fov (FovX,FovY), gt images (image, image_name, image_path, width, height)\nnerf_normalization (avg_cam_center (translate), max displacement from the avg cam (radius)),\nCameras list train_cameras of 300 Camera() is made up by repeatly loadCam()\nResize GT image for each camera (i.e., 300 views).\nViewing transform (world➡camera): self.world_view_transform\nProjection matrix (camera➡clip): self.projection_matrix\n$$ 𝐏 = \\begin{bmatrix} \\frac{2n}{r-l} \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f}{f-n} \u0026amp; \\frac{-fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $$\nBy denoting sight width as $W$, the above $r=\\frac{W}{2},\\ l=-\\frac{W}{2}$, so $(r-l)=W$. The sight center (principle center) (cx,cy) is (r,l), i.e., (W/2, H/2) The clip coordinate of $x$ produced by multiplying 𝐏 will result in $x_{NDC} ∈ [-1,1]$\nBy using this 𝐏, the final $z$ axis of ND coordinate ranges in [0,1], unlike the usual NDC cube of [-1,1]\n$$ near_{NDC} = \\frac{ \\frac{f * near}{f-n} + \\frac{-fn}{f-n} }{n} = 0 \\\\ far_{NDC} = \\frac{ \\frac{f*far}{f-n} + \\frac{-fn}{f-n} }{f} = 1 $$\nTherefore, this 𝐏 leads to a cuboid NDC space, rather than a cube NDC space. Figuratively, in the clip space, the points satisfying $0\u0026lt;z_{clip} \u0026lt; w_{clip}$ and $-w_{clip} \u0026lt; x_{clip}, y_{clip} \u0026lt; w_{clip}$ will be excluded from rendering. After clipping those point, the remaining space is the NDC space.\nThe transformation from world to clip space (P@w2c)ᵀ for each camera: self.full_proj_transform\nInitialize Gaussians\u0026rsquo; parameters create_from_pcd\n_xyz is set as the basic piont cloud _features_dc and _features_rest are converted from the color of the basic point cloud. The 0th-order SH coeffs is the rgb. And the rest of coeffs are initialized as 0. spatial_lr_scale is set to the max displacement from the avg cam center. _scaling is initialized with simple_KNN. _rotation is preset with 0 and 1 _opacity is initilized as the inverse of sigmoid(0.1) Training optimization settings, gaussians.training_setup()\nlr for each parameter Render (2024-01-23)\nrender()\nscreenspace_points: Gaussian centers on the 2D screen are initialized as 0. diff_gauss_raster (2023-11-18)\nCode | Explaination: kwea123/gaussian_splatting_notes\nPython Invokes The Python program gaussian_renderer/__init__.py calls GaussianRasterizer.forward() to render an image tile-by-tile.\nInternally, program goes to the submodule\u0026rsquo;s _RasterizeGaussians.forward() in __ini__.py, analogous to the \u0026ldquo;test.py\u0026rdquo; that calls the cuda-extension package\u0026rsquo;s methods, which are wrapped in PyTorch\u0026rsquo;s forward and backward.\n_C is a class of the diff_gaussian_rasterization package, as set in setup.py\nThe names of package\u0026rsquo;s methods are set in PYBIND11_MODULE()\nforward Steps RasterizeGaussiansCUDA() in rasterize_points.cu\nDefine geomBuffer, binningBuffer, and imgBuffer for storing data of point cloud, tiles, and the rendered image. Their memory will be allocated through a Lambda function resizeFunctional(), where tensor\u0026rsquo;s resize_ method is called. ➔ CudaRasterizer::Rasterizer::forward() in cuda_rasterizer/rasterizer_impl.cu\n➔ FORWARD::preprocess() in cuda_rasterizer/forward.cu\n➔ preprocessCUDA() in forward.cu\n\\begin{algorithm} \\begin{algorithmic} \\STATE RasterizeGaussiansCUDA \\STATE $\\quad$ CudaRasterizer::Rasterizer::forward \\STATE $\\qquad$ geomState, imgState \\COMMENT{Allocate memory} \\STATE $\\qquad$ FORWARD::preprocess \\STATE $\\qquad$ cub::DeviceScan::InclusiveSum \\COMMENT{Count tiles} \\STATE $\\qquad$ binningState \\STATE $\\qquad$ duplicateWithKeys \\STATE $\\qquad$ getHigherMsb \\STATE $\\qquad$ cub::DeviceRadixSort::SortPairs \\STATE $\\qquad$ identifyTileRanges \\STATE $\\qquad$ FORWARD::render \\end{algorithmic} \\end{algorithm} preprocessCUDA Frustum culling\nDelete points whose w (equal to depth) is larger than x, y, z in clip space. However, in_frustum() is based on coordinates in the camera space.\nConstruct 3D covariance matrix. Code\nGiven a quaternion $q = [r, \\ x, \\ y, \\ z]$, the rotation matrix is: wiki\n$$ 𝐑 = \\begin{bmatrix} 1 - 2(y² + z²) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 2(xy + rz) \u0026amp; 1 - 2(x² + z²) \u0026amp; 2(yz - rx) \\\\ 2(xz - ry) \u0026amp; 2(yz + rx) \u0026amp; 1 - 2(x²+y²) \\end{bmatrix} $$\nThe stretching matrix is diagonal and represented as a 3D vector $𝐒 = [x, \\ y, \\ z]$\nCovariance matrix (3x3) of 3D Gaussian cov3D: 𝚺 = 𝐑𝐒𝐒ᵀ𝐑ᵀ\nCovariance matrix (3x3) is projected onto 2D screen cov: 𝚺\u0026rsquo; = 𝐉 𝐖 𝚺 𝐖ᵀ𝐉ᵀ. Code\nTake the inverse of 𝚺\u0026rsquo; (𝚺\u0026rsquo;⁻¹), conic, to evaluate the 2D Gaussian PDF. Code\nThe extent of a point (Gaussian center) is a bounding square, where the \u0026ldquo;radius\u0026rdquo; from each side to the projected pixel is my_radius: 3σ. Code\nThe radius of the circumscribed circle for a 2D Gaussian is the max eigenvalue of the 2D covariance matrix $𝐀 = [^{a \\ b}_{b \\ c}]$. The eigenvalues can be solved from:\n$$ det (𝐀 - λ⋅𝐈) = 0 \\\\ \\begin{vmatrix} a-λ \u0026amp; b \\\\ b \u0026amp; c-λ\\end{vmatrix} = 0 \\\\ (a-λ)(c-λ) - b^2 = 0 \\\\ λ^2 - (a+c)λ + ac-b^2 =0 $$\nTwo roots: $λ₁,\\ λ₂ = \\frac{(a+c)}{2} ± \\sqrt{\\frac{(a+c)²}{4}-(ac-b²)}$\n1 2 3 4 5 float det = (cov.x * cov.z - cov.y * cov.y); // determinant - - - float mid = 0.5f * (cov.x + cov.z); float lambda1 = mid + sqrt(max(0.1f, mid * mid - det)); float lambda2 = mid - sqrt(max(0.1f, mid * mid - det)); Pixel coordinates: Scale the ND coordinates of x,y ∈ [-1,1] to image-grid coordiantes point_image [0,W] through viewport transformation: $x = \\frac{W⋅(x-1)}{2} -0.5,\\ y=\\frac{H⋅(y-1)}{2} -0.5$\nPrepare Tiles (2024-02-02)\nObtain the total sum of touched tiles for all Gaussian centers by inclusive sum\n1 2 3 4 5 6 cub::DeviceScan::InclusiveSum(geomState.scanning_space, geomState.scan_size, geomState.tiles_touched, geomState.point_offsets, P) - - - int num_rendered; // total number CHECK_CUDA(cudaMemcpy(\u0026amp;num_rendered, geomState.point_offsets + P - 1, sizeof(int), cudaMemcpyDeviceToHost), debug); InclusiveSum produces a sequence of prefix sum for tiles_touched (the number of touched tiles) of 100k points.\nThe pointer point_offsets points to the first byte of the 100k-byte sequence.\nn t p s u i r u m l e m e f : o s i f : x p p o 1 9 9 i n t p 1 1 _ 2 0 o f p 2 3 f 3 5 5 s e ⋯ 1 5 t 6 1 s 3 8 6 7 4 9 1 4 1 9 4 0 ⋯ ⋯ ⋯ ⋯ 8 1 6 4 p p 4 2 o 1 . i 0 5 n 0 e t K 6 _ o f f s e t s + 9 9 9 9 9 P = 100k bytes (char) is the number of points (Gaussian center).\nSo, point_offsets + P - 1 is the last element in the prefix-sum sequence. Specifically, it\u0026rsquo;s the total number of touched tiles for all 100k points.\nEvaluate each Gaussian to pair every touched tile\u0026rsquo;s index and Gaussian\u0026rsquo;s depth: Code\n1 duplicateWithKeys \u0026lt;\u0026lt; \u0026lt;(P + 255) / 256, 256 \u0026gt;\u0026gt; \u0026gt; ( Iterate each tile that a Gaussian overlaps, and combine the tile index and the Gaussian\u0026rsquo;s depth to form a key\n1 2 3 4 5 for (int y = rect_min.y; y \u0026lt; rect_max.y; y++) { for (int x = rect_min.x; x \u0026lt; rect_max.x; x++) { gaussian_keys_unsorted[off] = key; 100x100 image are divided to tiles with size of 16x16, so the thread gridDim: (7,7,1)\n0 1 2 3 4 5 6 0 ▦ 1 2 r e c t 3 _ m i n 4 5 6 r e c t 3 _ σ m a x For example, the above small Gaussian covers 9 tiles at: (0,1), (1,1), (2,1), (0,2), (1,2), (2,2), (0,3), (1,3), (2,3)\nThe tile index is computed as y*grid.x + x. Therefore, the 9 tiles\u0026rsquo; indices are: 8,9,10, 15,16,17, 22,23,24\nSuppose the tile id is 24 and the Gaussian\u0026rsquo;s depth is 15.7 (cast to int 15), the corresponding pair\u0026rsquo;s key is:\n1 2 3 tile id 24: 0000_0000_0000_0000_0000_0000_0001_1000 Shift left 32 bits: 0000_0000_0000_0000_0000_0000_0001_1000_0000_0000_0000_0000_0000_0000_0000_0000 combine depth (15): 0000_0000_0000_0000_0000_0000_0001_1000_0000_0000_0000_0000_0000_0000_0000_1111 Because each Gaussian is processed by a thread in parallel, the prefix sum point_offsets is referenced to locate the starting cell for each Gaussian to store keys into corresponding area of the array gaussian_keys_unsorted:\nP t r s k h e u o e r f m f y e i s f s a x : : : d : 0 0 1 2 G a 3 u 1 s s 4 5 6 7 8 G a 9 9 u 2 s s 1 1 0 0 G a 3 - u ⋯ s s ⋯ ⋯ ⋯ ⋯ G 1 a 0 u 0 s k s 2 2 . . 5 5 e e 6 6 The value is the Gaussian\u0026rsquo;s index ranging in [0, 100k-1], i.e., the thread index idx:\n1 2 3 auto idx = cg::this_grid().thread_rank(); --- gaussian_values_unsorted[off] = idx With performing SortPairs, the tile-Gaussian pairs sequence (binningState.point_list_keys) is arranged based on the tile ID, and for keys with identical tile id, the depths are then sorted.\n1 2 3 4 5 CHECK_CUDA(cub::DeviceRadixSort::SortPairs( ... binningState.point_list_keys_unsorted, binningState.point_list_keys, binningState.point_list_unsorted, binningState.point_list, ... G K a e u i y s d s s : : 8 9 4 7 0 t 3 8 i 0 5 3 l e 7 5 6 0 9 0 3 6 5 1 9 4 7 2 6 7 2 0 7 8 0 t 3 5 i 9 6 8 l e 6 8 5 5 3 1 3 1 8 8 1 8 2 9 4 2 3 4 5 2 4 1 5 - 6 0 9 ⋯ ⋯ t 8 5 i 2 1 3 l e 1 0 1 4 3 4 8 2 0 2 3 5 Evaluate each pair to identify the boundaries ranges of each tile for rendering. Code\n1 identifyTileRanges \u0026lt;\u0026lt; \u0026lt;(num_rendered + 255) / 256, 256 \u0026gt;\u0026gt; \u0026gt; ( B t ( o K h p u e r a r n y e i a d s a r n s : d s g : : ) e | s [ t 0 i ] l . e x 0 r r a a n n g g e e s s [ [ 1 0 ] ] t . . i x y l e 1 r r a a n n g g e e s s [ [ - 2 1 ⋯ ] ] . . x y ⋯ t i l e r a 4 n 8 g e s [ 4 8 ] . y Render Pixel (2024-02-05)\nA block for a tile; A thread for a Gaussian; A pixel is an iterative composition of multiple Gaussian. Ultimately, in the end, a thread in a block corresponds to a pixel.\nA thread syntheses a single pixel by performing alpha compositing Gaussian-by-Gaussian.\n1 2 renderCUDA\u0026lt;NUM_CHANNELS\u0026gt; \u0026lt;\u0026lt; \u0026lt;grid, block \u0026gt;\u0026gt; \u0026gt; ( // grid: (7,7,1), block: (16,16,1), 49*256 threads for 100x100 pix A pixel\u0026rsquo;s color is rendered within the tile encompassing it, based on the obtained sorted sequence of Gaussians\u0026rsquo; indices.\nAll pixels in a tile correspond to the same Gaussians, but with different weights, as the weights depend on distance from the pixel coordinates to the 2D Gaussian center.\n1 6 A 1 6 ■ t i ■ l e G a u a ▨ ▨ s s G a u b ▨ ▨ s s G a u c ▨ ▨ s s ⋯ ⋯ G a u j ▨ ▨ s s ⋯ ⋯ G r a a u n ▨ ▨ s g s e 1 2 3 4 5 6 7 for (int j = 0; !done \u0026amp;\u0026amp; j \u0026lt; min(BLOCK_SIZE, toDo); j++) { ... float2 xy = collected_xy[j]; // pixel coords float2 d = { xy.x - pixf.x, xy.y - pixf.y }; // dist float4 con_o = collected_conic_opacity[j]; // cov float power = -0.5f * (con_o.x * d.x * d.x + con_o.z * d.y * d.y) - con_o.y * d.x * d.y; The coordinates of the pixel to be rendered by a thread is obtained as followes:\n1 p 6 i x p e i l x s _ A A m i p t n t i h i x r □ l e e e l a : d 1 s d ( : 6 h b ( a m l p ( p r e o i b i e m c x l x k _ o e ) m c l i k s p ( n . i 2 . t x , x h _ 1 + r m ) t e a h a x r d G G G e _ a a a a i u u u d n s s s - d s s s i e i i i n x a a a d ( n n n - ) x . i x c , x n y o , d v p e i b x m x l a _ o t m c i k a n . n . t d y h + r o t e p h a a r d c e _ i a i t d n y - d i e n x d ( - ) y . ) y ) pix_min = (2 * BLOCK_X, 1 * BLOCK_Y) = (32, 16) pix_max = (max(32+BLOCK_X, W), max(16+BLOCK_Y, H)) = (48, 32) pix = (48+block.thread_index().x, 32+block.thread_index().y) pix_id = 100*pix.y + pix.x tile id = 1*7 + 2 = 9 range = ranges[9], number of Gaussians for this tile. After the pixel is fully rendered as the accumulated transmittance T is close to 0.0, this thread is assigned to transfer data (Gaussian properties) from the global memory to shared memory.\n(2023-02-06) A 16×16 thread block works repeatedly for one tile.\nIn NeRF, a rendering batch comprises multiple rays emitted from pixels, while in 3DGS, a rendering batch is a tile (16x16) of pixels.\nDifferent tile has different number of tile-Gaussian pairs (range). Thus, each tile requires varying computations.\nEach pixel in a tile results from multiple (range) tile-Gaussian pairs. Each tile-Gaussian pair is a \u0026ldquo;filter composition\u0026rdquo; and requires a thread to execute once. As only a single 16x16 thread block is assigned to render the tile. To fulfill all the tile-Gaussian pairs composition, the equal amount (range) of thread executions are needed. Thus, the computation for a tile has to re-use the 16x16 thread block several times repeatedly, i.e., rounds times.\n1 2 3 uint2 range = ranges[block.group_index().y * horizontal_blocks + block.group_index().x]; // given tile index, read (starting pair, ending tile-Gaussian pair) const int rounds = ((range.y - range.x + BLOCK_SIZE - 1) / BLOCK_SIZE);\t// the number of 16x16 blocks are allocated based on the number of tile-Gaussian pairs int toDo = range.y - range.x;\t// number of tile-Gaussian pairs to render the tile For example, if the tile 0 corresponds to 158 tile-Gaussian pairs in the sorted list: point_list_keys, the 16x16 thread block runs only once to render pixels in tile 0. And if tile 1\u0026rsquo;s range is 768 tile-Gaussian pairs, 3-round executions are needed.\n1 ⋮ ⋮ H 0 5 0 b l C t 1 k o i ( m l r 0 p e o , u u 0 t 0 n ) e d 1 5 1 6 b l C t 3 k o i ( m l r 1 p e o , u u 0 t 1 n ) e d 3 s 1 3 2 b l C t k o i ( m l 2 p e , u 0 t 2 ) e 4 7 4 8 b l k ( 3 , 0 ) 6 4 ⋯ ⋯ 9 6 b l k W ( 7 , 1 0 1 ) 2 Every Gaussian gets added onto every pixel (color += color * alpha * T) until a pixel gets fully rendered, i.e., T is close to 0:\n1 2 3 4 5 6 7 8 9 10 11 12 // Iterate tiles for (int i = 0; i \u0026lt; rounds; i++, toDo -= BLOCK_SIZE) { // if pixels in the tile are all rendered, break ... // Iterate Gaussians for (int j = 0; !done \u0026amp;\u0026amp; j \u0026lt; min(BLOCK_SIZE, toDo); j++) { ... // Add a Gaussian to every pixel with specific weight for (int ch = 0; ch \u0026lt; CHANNELS; ch++) C[ch] += features[collected_id[j] * CHANNELS + ch] * alpha * T; The alpha compositing can be calculated in parallel, as the cumulative summation is definite and independent of the adding sequence.\nBackward Looking up definitions: \u0026ldquo;backward.cu\u0026rdquo; ← \u0026ldquo;rasterizer_impl.cu\u0026rdquo; ← \u0026ldquo;rasterize_points.cu\u0026rdquo; ← \u0026ldquo;__init__.py\u0026rdquo;\nback render (2024-02-09) Calling entry\nThe rendered pixel is derived from color (R,G,B) and opacity of each Gaussian.\nDerivative flows backward:\nm e a n 2 D o w p 𝚺 e a ' i c ⁻ g i ¹ h t t y o G G a c G u o a a s l u l s o s p i r s h a i a n a ₖ n ₖ ∑ c α T P c i o x l e o l r L o s s The partial derivatives of Loss w.r.t. each Gaussian can be computed in parallel similar to forward rendering, as the contribution is a summation?\nIncoming upstream gradient: dL_dpixels Accumulated transmittance over all Gaussians is known, so accumulated transmittance of each Gaussian can be accquired with: $T_{curr} = \\frac{T_{curr+1}}{(1-α_{curr})} $ (2024-02-10)\nFor color scalar (R,G,B) in each channel k:\nPartial derivative of Loss w.r.t. each Gaussian\u0026rsquo;s color: d\n$$\\rm \\frac{∂C_{pixel}}{∂C_{k}} = α_{curr}⋅ T_{curr}$$\n1 2 3 4 5 6 7 8 9 T = T / (1.f - alpha); const float dchannel_dcolor = alpha * T; for (int ch = 0; ch \u0026lt; C; ch++) { ... const float dL_dchannel = dL_dpixel[ch]; // add to previous dL_dcolors atomicAdd(\u0026amp;(dL_dcolors[global_id * C + ch]), dchannel_dcolor * dL_dchannel); } Partial derivative of Loss w.r.t. each Gaussian\u0026rsquo;s alpha: d\nl a s t - 1 ✚ f l o a r s w t a r ✚ d c c o u m r p r o s ✚ i t ⋯ i n g = p i x $$ \\begin{aligned} \\rm C_{pix} \u0026amp;= \\rm C_{last-1} α_{last-1} T_{curr}(1-α_{curr})(1-α_{last}) + C_{last}α_{last}T_{curr}(1-α_{curr}) + C_{curr} α_{curr} T_{curr} + ⋯ \\\\ \\rm \\frac{∂C_{pix}}{∂α_{curr}} \u0026amp;= \\rm - C_{last-1} α_{last-1} T_{curr}(1-α_{last}) - C_{last}α_{last}T_{curr} + C_{curr} T_{curr} \\\\ \u0026amp;= \\rm (C_{curr} - \\underbrace{C_{last-1} α_{last-1}}_{\\text{accum rec}} (1-α_{last}) - C_{last}α_{last}) ⋅ T_{curr} \\\\ \\rm \\frac{∂L}{∂α_{curr}} \u0026amp;= \\rm \\frac{∂L}{∂C_{pix}}⋅ \\frac{∂C_{pix}}{∂α_{curr}} \\end{aligned} $$\n1 2 3 4 5 6 7 8 9 10 11 for (int ch = 0; ch \u0026lt; C; ch++) // R,G,B 3 chnls { const float c = collected_colors[ch * BLOCK_SIZE + j]; accum_rec[ch] = last_alpha * last_color[ch] + (1.f - last_alpha) * accum_rec[ch]; last_color[ch] = c; const float dL_dchannel = dL_dpixel[ch]; dL_dalpha += (c - accum_rec[ch]) * dL_dchannel; ... } dL_dalpha *= T; Old notes $$ \\rm \\frac{∂C_{pixel}}{∂ α_{curr}} = C_{k} T_{curr} - \\frac{∑_{m\u0026gt;curr}cₘ⋅αₘ⋅ Tₘ}{1-α_{curr}} $$\nA summation of subsequent Gaussians which are based on the current Gaussian.\nAs the solving process starts from the rare Gaussian that depends on all previous Gaussians to, when calculating the current Gaussian, the summation of Gaussians affected by it is ready to use.\nConsider the contribution of background color to the pixel color finally:\n$$ \\begin{aligned} \\rm C_{pix} \u0026amp;= \\rm C_{gs} + T_{final}C_{bg} \\\\ \\rm \\frac{∂ L}{∂ α_{gs}} \u0026amp;= \\rm \\frac{∂ L}{∂ C_{pix}} ⋅ ( \\frac{∂C_{pix}}{∂α_{gs}} + \\frac{∂C_{pix}}{∂T_{final}} ⋅ \\frac{∂T_{final}}{∂α_{gs}} ) \\\\ \u0026amp;= \\rm \\frac{∂L}{∂α_{gs}} + \\frac{∂ L}{∂ C_{pix}} ⋅ C_{bg} ⋅ (-\\frac{T_{final}}{(1-α_{gs})}) \\end{aligned} $$\n1 2 3 4 float bg_dot_dpixel = 0; for (int i = 0; i \u0026lt; C; i++) bg_dot_dpixel += bg_color[i] * dL_dpixel[i]; dL_dalpha += (-T_final / (1.f - alpha)) * bg_dot_dpixel; Partial derivative of Loss w.r.t. 2D means:\n$$ \\begin{aligned} G \u0026amp;= e^{-½ (\\bm μ-𝐱)ᵀ \\bm Σ⁻¹ (\\bm μ-𝐱)} \\\\ \u0026amp;= exp(-½ ⋅ [μₓ-x \\ μ_y-y] ⋅ [^{a \\ b}_{d \\ c}] ⋅ [^{μₓ-x} _{μ_y-y}])\\\\ \u0026amp;= exp(-½⋅ (a⋅Δₓ² + (b+d)⋅ΔₓΔ_y + c⋅Δ_y²)) \\\\ α \u0026amp;= o ⋅ G \\\\ \\frac{∂α}{∂Δₓ} \u0026amp;= o⋅G⋅ (-aΔₓ - bΔ_y) \\\\ \\frac{∂L}{∂μ_{x(flim)}} \u0026amp;= \\frac{∂L}{∂α}⋅ \\frac{∂α}{∂Δₓ}⋅ \\frac{∂Δₓ}{∂μₓ} ⋅ \\frac{∂μₓ}{∂μ_{x (film)}} \\\\ \u0026amp;= \\frac{∂L}{∂α} ⋅o⋅G⋅ (-aΔₓ - bΔ_y) ⋅ 1 ⋅ \\frac{W}{2} \\end{aligned} $$\nAlthough b=d, to be consistent with the derivative ∂L/∂b in the Jacobian matrix, the b and d keep separated instead of using b+d = 2b. 1 2 3 4 5 6 7 8 9 10 11 const float ddelx_dx = 0.5 * W; // viewport transform const float ddely_dy = 0.5 * H; const float dL_dG = con_o.w * dL_dalpha; const float gdx = G * d.x; const float gdy = G * d.y; const float dG_ddelx = -gdx * con_o.x - gdy * con_o.y; const float dG_ddely = -gdy * con_o.z - gdx * con_o.y; // Update gradients w.r.t. 2D mean position of the Gaussian atomicAdd(\u0026amp;dL_dmean2D[global_id].x, dL_dG * dG_ddelx * ddelx_dx); atomicAdd(\u0026amp;dL_dmean2D[global_id].y, dL_dG * dG_ddely * ddely_dy); Partial derivative of Loss w.r.t. 3 elements a,b,c in the inverse of 2D covariance matrix, based on $α = o⋅G = o⋅exp(-\\frac{1}{2}Δₓᵀ[^{a\\ b}_{d\\ c}]Δₓ)$:\nNote: Here $[^{a\\ b}_{d\\ c}]$ represents the inverse cov 𝚺⁻¹, not the original cov2D 𝚺. $$ \\frac{∂L}{∂a} = \\frac{∂L}{∂α}⋅ \\frac{∂α}{∂a} = \\frac{∂L}{∂α}⋅ o ⋅ G ⋅ (-½Δₓ²) $$\n1 2 3 atomicAdd(\u0026amp;dL_dconic2D[global_id].x, -0.5f * gdx * d.x * dL_dG); atomicAdd(\u0026amp;dL_dconic2D[global_id].y, -0.5f * gdx * d.y * dL_dG); atomicAdd(\u0026amp;dL_dconic2D[global_id].w, -0.5f * gdy * d.y * dL_dG); Partial derivative of Loss w.r.t. opacity based on $α=oG$\n$$ \\frac{∂L}{∂o} = \\frac{∂L}{∂α} ⋅ \\frac{∂α}{∂o} = \\frac{∂L}{∂α}⋅ G $$\nbw cov2D The cov2D $\\bm Σ\u0026rsquo;_{ray}$ is originated from the 3D covariance matrix 𝚺 and the mean vector 𝐱 (ie, 𝛍) in world space. So the $\\bm Σ\u0026rsquo;_{ray}$ is the starting point of the derivative of Loss w.r.t. the 𝚺 and 𝐱 of each Gaussian.\nTo align notations in previous posts, the covariance matrix in ray space is dentoed as 𝚺\u0026rsquo;, and the cov matrix in world space is 𝚺.\nw o 𝐱 r l d 𝐑 c 𝐭 a m 𝐉 𝐉 𝐑 c w o o 𝚺 v r 𝐓 3 l D d 𝐓 𝚺 𝐓 ᵀ r a 𝚺 2 y ' D s p c c 𝚺 o ' v ⁻ 2 ¹ D Note:\nThe clip coordinates aren\u0026rsquo;t used in the conversion for the 3D covariance matrix from world space to ray space. This is aligned with the EWA splatting. As the Gaussian center\u0026rsquo;s coordiantes are used in the Jacobian 𝐉, which is an approximation of perspective division, where the camera-space coordinates are supposed to be used, the 𝐭 in 𝐉 is coordinates of Gaussian center in camera space. So, only the viewmatrix \u0026ldquo;w2c\u0026rdquo; 𝐑 is applied to the world coordinates 𝐱.\nHowever, the 2D Gaussian center on the screen indeed comes from clip coordinates, because points outside the frustrum don\u0026rsquo;t need to be rendered. Therefore, the projection matrix projmatrix is applied to the world coordinates 𝐱.\nThe above two branches both are derived from the world coordinates 𝐱, so the derivative of Loss w.r.t. 𝐱 ($\\frac{∂L}{∂𝐱}$) comprises of two portions.\ndΣ\u0026rsquo;⁻¹/dΣ' (2024-02-11)\nSo far, the derivative of loss w.r.t. the inverse (conics, $(\\bm Σ\u0026rsquo;)⁻¹$) of a 2D-plane covariance matrix $\\bm Σ\u0026rsquo;$ (i.e., the 3D cov in ray space with 3rd row and column omitted) has been obtained. So, the derivative of loss w.r.t. the original 2D cov matrix 𝚺\u0026rsquo; is $\\frac{∂L}{∂\\bm Σ\u0026rsquo;⁻¹}$ appended with the derivative of 𝚺\u0026rsquo;⁻¹ w.r.t. 𝚺'.\nConsider 4 variables a, b, c, d form the matrix $\\bm Σ\u0026rsquo; = \\begin{bmatrix}a \u0026amp; b \\\\ d \u0026amp; c \\end{bmatrix}$, its inverse matrix is: $\\bm Σ\u0026rsquo;⁻¹ = \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix}$, where b=d.\nThe partial derivatives of the inverse matrix w.r.t. each variable: a,b,c,d are:\n$$ \\begin{aligned} \\frac{∂ \\bm Σ\u0026rsquo;⁻¹}{∂a} \u0026amp;= \\frac{-c² + bc + dc - bd}{(ac-bd)²}, \u0026amp; \\frac{∂ \\bm Σ\u0026rsquo;⁻¹}{∂b} \u0026amp;= \\frac{cd-ac-d²+ad}{(ac-bd)²} \\\\ \\frac{∂ \\bm Σ\u0026rsquo;⁻¹}{∂c} \u0026amp;= \\frac{-bd+ba+da-a²}{(ac-bd)²}, \u0026amp; \\frac{∂ \\bm Σ\u0026rsquo;⁻¹}{∂d} \u0026amp;= \\frac{cb-b²-ac+ab}{(ac-bd)²} \\\\ \\end{aligned} $$\n1 2 3 4 5 6 7 float denom = a * c - b * b; // detminant float dL_da = 0, dL_db = 0, dL_dc = 0; float denom2inv = 1.0f / ((denom * denom) + 0.0000001f); ... dL_da = denom2inv * (-c * c * dL_dconic.x + 2 * b * c * dL_dconic.y + (denom - a * c) * dL_dconic.z); dL_dc = denom2inv * (-a * a * dL_dconic.z + 2 * a * b * dL_dconic.y + (denom - a * c) * dL_dconic.x); dL_db = denom2inv * 2 * (b * c * dL_dconic.x - (denom + 2 * b * b) * dL_dconic.y + a * b * dL_dconic.z); dΣ\u0026rsquo;/dΣ (2024-02-12)\nDerivative of 3D covariance matrix from ray space to camera space:\nAs the above 2D covariance matrix on plane is the 3D cov matrix 𝚺\u0026rsquo; in the ray space with the 3rd row and column omitted. Thus, the next step to be calculated is the conversion from 𝚺\u0026rsquo; in the 3D ray space to 𝚺 in the world space.\nThe 3D covariance matrix 𝚺\u0026rsquo; in the ray space is obtained after performing viewing transform and projective transform:\n$$ \\begin{aligned} \\bm Σ\u0026rsquo;_{ray} \u0026amp;= 𝐉⋅ 𝐑_{w2c} ⋅\\bm Σ_{world}⋅ 𝐑ᵀ_{w2c} ⋅𝐉ᵀ \\\\ \\begin{bmatrix} a \u0026amp; b \u0026amp; ∘ \\\\ b \u0026amp; c \u0026amp; ∘ \\\\ ∘ \u0026amp; ∘ \u0026amp; ∘ \\end{bmatrix} \u0026amp;= \\underbrace{ \\begin{bmatrix} \\frac{f_x}{t_z} \u0026amp; 0 \u0026amp; -\\frac{f_x t_x}{t_z^2} \\\\ 0 \u0026amp; \\frac{f_y}{t_z} \u0026amp; -\\frac{f_y t_y}{t_z^2} \\\\ \\frac{t_x}{‖𝐭‖} \u0026amp; \\frac{t_y}{‖𝐭‖} \u0026amp; \\frac{t_z}{‖𝐭‖} \\end{bmatrix} \\begin{bmatrix} R_{11} \u0026amp; R_{12} \u0026amp; R_{13} \\\\ R_{21} \u0026amp; R_{22} \u0026amp; R_{23} \\\\ R_{31} \u0026amp; R_{32} \u0026amp; R_{33} \\end{bmatrix} }_{T} \\begin{bmatrix} v_0 \u0026amp; v_1 \u0026amp; v_2 \\\\ v_1 \u0026amp; v_3 \u0026amp; v_4 \\\\ v_2 \u0026amp; v_4 \u0026amp; v_5 \\\\ \\end{bmatrix} \\underbrace{ \\begin{bmatrix} R_{11} \u0026amp; R_{21} \u0026amp; R_{31} \\\\ R_{12} \u0026amp; R_{22} \u0026amp; R_{32} \\\\ R_{13} \u0026amp; R_{23} \u0026amp; R_{33} \\end{bmatrix} \\begin{bmatrix} \\frac{f_x}{t_z} \u0026amp; 0 \u0026amp; \\frac{t_x}{‖𝐭‖} \\\\ 0 \u0026amp; \\frac{f_y}{t_z} \u0026amp; \\frac{t_y}{‖𝐭‖} \\\\ -\\frac{f_x t_x}{t_z^2} \u0026amp; -\\frac{f_y t_y}{t_z^2} \u0026amp; \\frac{t_z}{‖𝐭‖} \\end{bmatrix} }_{Tᵀ} \\\\ \u0026amp;= \\begin{bmatrix} T_{00} \u0026amp; T_{01} \u0026amp; T_{02} \\\\ T_{10} \u0026amp; T_{11} \u0026amp; T_{12} \\\\ T_{20} \u0026amp; T_{21} \u0026amp; T_{22} \\\\ \\end{bmatrix} \\begin{bmatrix} v_0 \u0026amp; v_1 \u0026amp; v_2 \\\\ v_1 \u0026amp; v_3 \u0026amp; v_4 \\\\ v_2 \u0026amp; v_4 \u0026amp; v_5 \\\\ \\end{bmatrix} \\begin{bmatrix} T_{00} \u0026amp; T_{10} \u0026amp; T_{20} \\\\ T_{01} \u0026amp; T_{11} \u0026amp; T_{21} \\\\ T_{02} \u0026amp; T_{12} \u0026amp; T_{22} \\\\ \\end{bmatrix} \\end{aligned} $$\nThe derivatives of 𝚺\u0026rsquo; w.r.t. each element in the cov matrix $\\bm Σ_{world}$ in world are:\n$$ \\begin{aligned} \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂v_0} = \\begin{bmatrix} T_{00}T_{00} \u0026amp; T_{00}T_{10} \\\\ T_{10}T_{00} \u0026amp; T_{10}T_{10} \\end{bmatrix} \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂v_1} = \\begin{bmatrix} T_{00}T_{01} \u0026amp; T_{00}T_{11} \\\\ T_{10}T_{01} \u0026amp; T_{10}T_{11} \\end{bmatrix} \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂v_2} = \\begin{bmatrix} T_{00}T_{02} \u0026amp; T_{00}T_{12} \\\\ T_{10}T_{02} \u0026amp; T_{10}T_{12} \\end{bmatrix} \\\\ \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂v_3} = \\begin{bmatrix} T_{01}T_{01} \u0026amp; T_{01}T_{11} \\\\ T_{11}T_{01} \u0026amp; T_{11}T_{11} \\end{bmatrix} \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂v_4} = \\begin{bmatrix} T_{01}T_{02} \u0026amp; T_{01}T_{12} \\\\ T_{11}T_{02} \u0026amp; T_{11}T_{12} \\end{bmatrix} \\\\ \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂v_5} = \\begin{bmatrix} T_{02}T_{02} \u0026amp; T_{02}T_{12} \\\\ T_{12}T_{02} \u0026amp; T_{12}T_{12} \\end{bmatrix} \\end{aligned} $$\nTrick: Just find out the terms involving $v_0, v_1, v_2, v_3, v_4, v_5$\nFor example, in the first matmul, the coefficients applied on v₀ are $T_{00},\\ T_{10},\\ T_{20}$ (a row). Then, in the outer matmul, the coefficients that multiplis with $v_0$ are $T_{00},\\ T_{10},\\ T_{20}$ (a column).\nSo, the row and the column form a matrix:\n$$ \\begin{bmatrix} T_{00} \\\\ T_{10} \\\\ T_{20} \\end{bmatrix} \\begin{bmatrix} T_{00} \u0026amp; T_{10} \u0026amp; T_{20} \\end{bmatrix} → \\begin{bmatrix} T_{00}T_{00} \u0026amp; T_{00}T_{10} \u0026amp; T_{00}T_{20} \\\\ T_{10}T_{00} \u0026amp; T_{10}T_{10} \u0026amp; T_{10}T_{20} \\\\ T_{20}T_{00} \u0026amp; T_{20}T_{10} \u0026amp; T_{20}T_{20} \\end{bmatrix} $$\nThe 3-rd row and column are omitted to yield the 2D covariance matrix on plane.\nSince the upstream derivative of loss w.r.t. $\\bm Σ\u0026rsquo;_{ray}$ is a matrix as well, they need to do element-wise multiplication:\n$$ \\begin{aligned} \\frac{∂L}{∂v_0} \u0026amp;= \\begin{bmatrix} \\frac{∂L}{∂a} \u0026amp; \\frac{∂L}{∂b} \\\\ \\frac{∂L}{∂b} \u0026amp; \\frac{∂L}{∂c} \\end{bmatrix} ⊙ \\begin{bmatrix} T_{00}T_{00} \u0026amp; T_{00}T_{10} \\\\ T_{10}T_{00} \u0026amp; T_{10}T_{10} \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{∂L}{∂a} T_{00}T_{00} \u0026amp; \\frac{∂L}{∂b} T_{00}T_{10} \\\\ \\frac{∂L}{∂b} T_{10}T_{00} \u0026amp; \\frac{∂L}{∂c} T_{10}T_{10} \\end{bmatrix} \\end{aligned} $$\nAs loss is a scalar, the derivative of the loss w.r.t. $v_0$ should be the summation of all elements in the matrix (Need reference❗):\n$$ \\frac{∂L}{∂v_0} = \\frac{∂L}{∂a} T_{00}T_{00} + 2× \\frac{∂L}{∂b}T_{00}T_{10} + \\frac{∂L}{∂c} T_{10}T_{10} $$\nDetails for v₄ Similarly, the coefficients related to $v_4$ are the 3rd row of the right matrix 𝐓ᵀ, and the 2nd column of the left matrix 𝐓.\n$$ \\begin{bmatrix} T_{01} \\\\ T_{11} \\\\ T_{21} \\end{bmatrix} \\begin{bmatrix} T_{02} \u0026amp; T_{12} \u0026amp; T_{22} \\end{bmatrix} → \\begin{bmatrix} T_{01}T_{02} \u0026amp; T_{01}T_{12} \u0026amp; T_{01}T_{22} \\\\ T_{11}T_{02} \u0026amp; T_{11}T_{12} \u0026amp; T_{11}T_{22} \\\\ T_{21}T_{02} \u0026amp; T_{21}T_{12} \u0026amp; T_{21}T_{22} \\\\ \\end{bmatrix} $$\nThe derivative of loss w.r.t. $v_1$\n$$ \\begin{aligned} \\frac{∂L}{∂v_1} \u0026amp;= \\begin{bmatrix} \\frac{∂L}{∂a} \u0026amp; \\frac{∂L}{∂b} \\\\ \\frac{∂L}{∂b} \u0026amp; \\frac{∂L}{∂c} \\end{bmatrix} ⊙ \\begin{bmatrix} T_{01}T_{02} \u0026amp; T_{01}T_{12} \\\\ T_{11}T_{02} \u0026amp; T_{11}T_{12} \\end{bmatrix} \\\\ \u0026amp;= T_{01}T_{02} \\frac{∂L}{∂a} + (T_{01}T_{12} + T_{11}T_{02})\\frac{∂L}{∂b} + T_{11}T_{12}\\frac{∂L}{∂c} \\end{aligned} $$\nBecause the covariance matrix is symmatric (cov[1][2]=cov[2][1]), the effect attributed to the change of $v_1$ should double.\n$$ \\frac{∂L}{∂v_4} = 2×T_{01}T_{02} \\frac{∂L}{∂a} + 2×(T_{01}T_{12} + T_{11}T_{02})\\frac{∂L}{∂b} + 2×T_{11}T_{12}\\frac{∂L}{∂c} $$\nIn the 3DGS code, dL_db had doubled.\n1 2 3 4 5 6 7 8 glm::mat3 T = W * J; ... dL_dcov[6 * idx + 0] = (T[0][0] * T[0][0] * dL_da + T[0][0] * T[1][0] * dL_db + T[1][0] * T[1][0] * dL_dc); dL_dcov[6 * idx + 3] = (T[0][1] * T[0][1] * dL_da + T[0][1] * T[1][1] * dL_db + T[1][1] * T[1][1] * dL_dc); dL_dcov[6 * idx + 5] = (T[0][2] * T[0][2] * dL_da + T[0][2] * T[1][2] * dL_db + T[1][2] * T[1][2] * dL_dc); dL_dcov[6 * idx + 1] = 2 * T[0][0] * T[0][1] * dL_da + (T[0][0] * T[1][1] + T[0][1] * T[1][0]) * dL_db + 2 * T[1][0] * T[1][1] * dL_dc; dL_dcov[6 * idx + 2] = 2 * T[0][0] * T[0][2] * dL_da + (T[0][0] * T[1][2] + T[0][2] * T[1][0]) * dL_db + 2 * T[1][0] * T[1][2] * dL_dc; dL_dcov[6 * idx + 4] = 2 * T[0][2] * T[0][1] * dL_da + (T[0][1] * T[1][2] + T[0][2] * T[1][1]) * dL_db + 2 * T[1][1] * T[1][2] * dL_dc; dΣ\u0026rsquo;/dT (2024-02-13)\nTo solve the derivative of Loss w.r.t. $𝐭$ (in camera space), the chain is:\n$$\\frac{∂L}{∂t} = \\frac{∂L}{\\bm Σ\u0026rsquo;_{ray}} \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂T} \\frac{∂T}{∂J} \\frac{∂J}{∂t}$$\nThe derivative of Loss w.r.t. 2D $\\bm Σ\u0026rsquo;_{ray}$ has been obtained earlier. The next step is to calculate $\\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂T}$, where all variables except for T are treated as constants.\nBased on the relationship:\n$$ \\bm Σ\u0026rsquo;_{ray} = 𝐓⋅\\bm Σ_{world}⋅𝐓ᵀ \\\\ = \\begin{bmatrix} T_{00} \u0026amp; T_{01} \u0026amp; T_{02} \\\\ T_{10} \u0026amp; T_{11} \u0026amp; T_{12} \\\\ T_{20} \u0026amp; T_{21} \u0026amp; T_{22} \\end{bmatrix} \\begin{bmatrix} v_{00} \u0026amp; v_{01} \u0026amp; v_{02} \\\\ v_{10} \u0026amp; v_{11} \u0026amp; v_{12} \\\\ v_{20} \u0026amp; v_{21} \u0026amp; v_{22} \\end{bmatrix} \\begin{bmatrix} T_{00} \u0026amp; T_{10} \u0026amp; T_{20} \\\\ T_{01} \u0026amp; T_{11} \u0026amp; T_{21} \\\\ T_{02} \u0026amp; T_{12} \u0026amp; T_{22} \\end{bmatrix} $$\nRepresent the matrix $\\bm Σ\u0026rsquo;_{ray}$ as a single generic term with varying indices: (inspired by David Levin surfaced by perplexity)\nan element in the result of $\\bm Σ_{world}⋅𝐓ᵀ$ is $q$; an element in the $\\bm Σ\u0026rsquo;_{ray}$ is $p$ $$ \\begin{aligned} q_{ij} \u0026amp;= ∑ₖ₌₀² v_{ik} T_{jk} \\\\ p_{mn} \u0026amp;= ∑ₛ₌₀² T_{ms} q_{sn} = ∑ₛ₌₀² T_{ms} ∑ₖ₌₀² v_{sk} T_{nk}\\\\ \\bm Σ\u0026rsquo;_{ray} \u0026amp;= \\begin{bmatrix} p_{00} \u0026amp; p_{01} \u0026amp; p_{02} \\\\ p_{10} \u0026amp; p_{11} \u0026amp; p_{12} \\\\ p_{20} \u0026amp; p_{21} \u0026amp; p_{22} \\end{bmatrix} \\end{aligned} $$\nExpand each $p_{mn}$:\n$$ \\begin{array}{ccc} \\begin{aligned} p_{00} =\u0026amp; T_{00}(v_{00}T_{00} + v_{01}T_{01} + v_{02}T_{02}) \\\\ +\u0026amp; T_{01}(v_{10}T_{00} + v_{11}T_{01} + v_{12}T_{02}) \\\\ +\u0026amp; T_{02}(v_{20}T_{00} + v_{21}T_{01} + v_{22}T_{02}) \\\\ \\end{aligned} \u0026amp; \\begin{aligned} p_{01} =\u0026amp; T_{00}(v_{00}T_{10} + v_{01}T_{11} + v_{02}T_{12}) \\\\ +\u0026amp; T_{01}(v_{10}T_{10} + v_{11}T_{11} + v_{12}T_{12}) \\\\ +\u0026amp; T_{02}(v_{20}T_{10} + v_{21}T_{11} + v_{22}T_{12}) \\\\ \\end{aligned} \u0026amp; \\begin{aligned} p_{02} =\u0026amp; T_{00}(v_{00}T_{20} + v_{01}T_{21} + v_{02}T_{22}) \\\\ +\u0026amp; T_{01}(v_{10}T_{20} + v_{11}T_{21} + v_{12}T_{22}) \\\\ +\u0026amp; T_{02}(v_{20}T_{20} + v_{21}T_{21} + v_{22}T_{22}) \\\\ \\end{aligned} \\\\ \\begin{aligned} p_{10} =\u0026amp; T_{10}(v_{00}T_{00} + v_{01}T_{01} + v_{02}T_{02}) \\\\ +\u0026amp; T_{11}(v_{10}T_{00} + v_{11}T_{01} + v_{12}T_{02}) \\\\ +\u0026amp; T_{12}(v_{20}T_{00} + v_{21}T_{01} + v_{22}T_{02}) \\\\ \\end{aligned} \u0026amp; p_{11} \u0026amp; p_{12} \\\\ \\begin{aligned} p_{20} =\u0026amp; T_{20}(v_{00}T_{00} + v_{01}T_{01} + v_{02}T_{02}) \\\\ +\u0026amp; T_{21}(v_{10}T_{00} + v_{11}T_{01} + v_{12}T_{02}) \\\\ +\u0026amp; T_{22}(v_{20}T_{00} + v_{21}T_{01} + v_{22}T_{02}) \\\\ \\end{aligned} \u0026amp; p_{21} \u0026amp; p_{22} \\end{array} $$\nThe 3-rd row and column are omitted to become the 2x2 $\\bm Σ\u0026rsquo;_{ray}$. So, only 4 elements affect the derivative of the $\\bm Σ\u0026rsquo;_{ray}$. For example, the derivative of $\\bm Σ\u0026rsquo;_{ray}$ w.r.t. $T₀₀$ only related to components containing $T_{00}$.\nCalculate the derivative of each element $p_{mn}$ in the $\\bm Σ\u0026rsquo;_{ray}$ w.r.t. $T_{00}$. A matrix is assembled:\n$$ \\begin{bmatrix} \\frac{∂p_{00}}{∂T_{00}} \u0026amp; \\frac{∂p_{01}}{∂T_{00}} \\\\ \\frac{∂p_{10}}{∂T_{00}} \u0026amp; \\frac{∂p_{11}}{∂T_{00}} \\end{bmatrix} = \\begin{bmatrix} \\substack{2T_{00}v_{00} + v_{01}T_{01} + v_{02}T_{02} \\\\ + T_{01}v_{10} + T_{02}v_{20} } \u0026amp; v_{00}T_{10} + v_{01}T_{11} + v_{02}T_{12} \\\\ T_{10}v_{00} + T_{11}v_{10} + T_{12}v_{20} \u0026amp; 0 \\end{bmatrix} $$\nTherefore, the derivative of the scalar Loss w.r.t. T₀₀, i.e., $\\frac{∂L}{∂T₀₀}$, is also a matrix:\n$$ \\frac{∂L}{∂T_{00}} = \\frac{∂L}{∂\\bm Σ\u0026rsquo;_{ray}}⊙ \\frac{∂\\bm Σ\u0026rsquo;_{ray}}{∂T_{00}} = \\begin{bmatrix} \\frac{∂L}{∂a} (2(T_{00}v_{00}+T_{01}v_{01}+T_{02}v_{02})) \u0026amp; \\frac{∂L}{∂b} (T_{10}v_{00}+T_{11}v_{01}+T_{12}v_{02} ) \\\\ \\frac{∂L}{∂b} (T_{10}v_{00}+T_{11}v_{10}+T_{12}v_{20} ) \u0026amp; 0 \\end{bmatrix} $$\nAs the loss $L$ is a scalar, the derivative of $L$ w.r.t. a certain variable (e.g., T₀₀) is the total quantity of changes in all elements of a matrix caused by that variable\u0026rsquo;s unit perturbation. Thus, $\\frac{∂L}{∂T₀₀}$ is a sum of all elements in the final \u0026ldquo;derivative matrix\u0026rdquo; of Loss w.r.t. T₀₀:\n$$ \\frac{∂L}{∂T_{00}} = \\frac{∂L}{∂a}(2(T_{00}v_{00}+T_{01}v_{01}+T_{02}v_{02}) ) + \\frac{∂L}{∂b}(2(T_{10}v_{00}+T_{11}v_{01}+T_{12}v_{02} ) ) $$\nTricks for identifying necessary terms Only focus on the terms that includes $T_{00}$:\nFor the first matmul between $\\bm Σ_{world}$ and 𝐓ᵀ, the full 1st row will be kept as it will times a $T_{00}$ during the second matmul, and additional 2 terms containing $T_{00}$ are left:\n$$\\begin{matrix} v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02} \u0026amp; v₀₀T_{10} + v_{01}T_{11} + v_{02}T_{12} \u0026amp; v₀₀T_{20} + v_{01}T_{21} + v_{02}T_{22} \\\\ v_{10} T_{00} \\\\ v_{20} T_{00} \\end{matrix}$$\nI disregard unrelated terms, to which the derivatives with respect are 0. Then, applying the left 𝐓\n$$ \\begin{aligned} \\bm Σ\u0026rsquo;_{ray}\u0026amp;= \\begin{bmatrix} T_{00} \u0026amp; T_{01} \u0026amp; T_{02} \\\\ T_{10} \u0026amp; T_{11} \u0026amp; T_{12} \\\\ T_{20} \u0026amp; T_{21} \u0026amp; T_{22} \\end{bmatrix} \\begin{bmatrix} v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02} \u0026amp; v₀₀T_{10} + v_{01}T_{11} + v_{02}T_{12} \u0026amp; v₀₀T_{20} + v_{01}T_{21} + v_{02}T_{22} \\\\ v_{10} T_{00} \\\\ v_{20} T_{00} \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} T_{00} (v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{01} v_{10} T_{00} + T_{02} v_{20} T_{00} \u0026amp; T_{00} (v₀₀T_{10} + v_{01}T_{11} + v_{02}T_{12}) \u0026amp; T_{00} (v₀₀T_{20} + v_{01}T_{21} + v_{02}T_{22}) \\\\ T_{10} (v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{11} v_{10} T_{00} + T_{12} v_{20} T_{00} \\\\ T_{20} (v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{21} v_{10} T_{00} + T_{22} v_{20} T_{00} \\end{bmatrix} \\end{aligned} $$\nAs the 3rd row and column don\u0026rsquo;t appear in the 2D plane cov matrix $\\bm Σ\u0026rsquo;_{ray}$, only 4 terms contribute to the derivative of L w.r.t. $T₀₀$:\n$$\\begin{bmatrix} T_{00} (v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{01} v_{10} T_{00} + T_{02} v_{20} T_{00} \u0026amp; T_{00} (v₀₀T_{10} + v_{01}T_{11} + v_{02}T_{12}) \\\\ T_{10} (v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02}) + T_{11} v_{10} T_{00} + T_{12} v_{20} T_{00} \\end{bmatrix}$$\nCompute derivative of each element w.r.t. $T_{00}$:\n$$\\begin{bmatrix} 2(v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02}) \u0026amp; v₀₀T_{10} + v_{01}T_{11} + v_{02}T_{12} \\\\ T_{10}v₀₀ + T_{11} v_{10} + T_{12} v_{20} \\end{bmatrix}$$\nCombine with $\\frac{∂L}{∂\\bm Σ\u0026rsquo;_{ray}}$ by element-wise product to get the ultimate \u0026ldquo;derivative matrix\u0026rdquo; of Loss w.r.t T₀₀.\n$$\\frac{∂L}{∂T₀₀} = \\begin{bmatrix} \\frac{∂L}{∂a} ⋅ 2(v₀₀T_{00} + v_{01}T_{01} + v_{02}T_{02}) \u0026amp; \\frac{∂L}{∂b} (v₀₀T_{10} + v_{01}T_{11} + v_{02}T_{12}) \\\\ \\frac{∂L}{∂b} (T_{10}v₀₀ + T_{11} v_{10} + T_{12} v_{20}) \\end{bmatrix}$$\nFinally, as L is a scalar, the derivative of L w.r.t. T₀₀ is the sum of all derivatives of each element in the \u0026ldquo;derivative matrix\u0026rdquo;.\nThe dL_db in 3DGS Code had doubled:\n1 2 float dL_dT00 = 2 * (T[0][0] * Vrk[0][0] + T[0][1] * Vrk[0][1] + T[0][2] * Vrk[0][2]) * dL_da + (T[1][0] * Vrk[0][0] + T[1][1] * Vrk[0][1] + T[1][2] * Vrk[0][2]) * dL_db; Similarly, the derivatives of Loss w.r.t. T₀₁, T₀₂, T₁₀, T₁₁, T₁₂ can be obtained. As T₂₀, T₂₁, T₂₂ are not involved in the 2D $\\bm Σ\u0026rsquo;_{ray}$, they don\u0026rsquo;t contribute the derivative of $\\bm Σ\u0026rsquo;_{ray}$.\n$$ \\frac{∂L}{∂𝐓} = \\begin{bmatrix} \\frac{∂L}{∂T_{00}} \u0026amp; \\frac{∂L}{∂T_{01}} \u0026amp; \\frac{∂L}{∂T_{02}} \\\\ \\frac{∂L}{∂T_{10}} \u0026amp; \\frac{∂L}{∂T_{11}} \u0026amp; \\frac{∂L}{∂T_{12}} \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\ndT/dJ (2024-02-14)\nBased on the relationship:\n$$ 𝐓 = 𝐉𝐖 = \\begin{bmatrix} J₀₀ \u0026amp; J₀₁ \u0026amp; J₀₂ \\\\ J₁₀ \u0026amp; J₁₁ \u0026amp; J₁₂ \\\\ J₂₀ \u0026amp; J₂₁ \u0026amp; J₂₂ \\end{bmatrix} \\begin{bmatrix} W₀₀ \u0026amp; W₀₁ \u0026amp; W₀₂ \\\\ W₁₀ \u0026amp; W₁₁ \u0026amp; W₁₂ \\\\ W₂₀ \u0026amp; W₂₁ \u0026amp; W₂₂ \\end{bmatrix} $$\nThe derivative of 𝐓 w.r.t. $J₀₀$\n$$ \\frac{∂T}{∂J_{00}} = \\begin{bmatrix} W₀₀ \u0026amp; W₀₁ \u0026amp; W₀₂ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe \u0026ldquo;derivative matrix\u0026rdquo; of Loss w.r.t. $J₀₀$\n$$ \\frac{∂L}{∂T} ⊙ \\frac{∂T}{∂J_{00}} = \\begin{bmatrix} \\frac{∂L}{∂T_{00}} \u0026amp; \\frac{∂L}{∂T_{01}} \u0026amp; \\frac{∂L}{∂T_{02}} \\\\ \\frac{∂L}{∂T_{10}} \u0026amp; \\frac{∂L}{∂T_{11}} \u0026amp; \\frac{∂L}{∂T_{12}} \\end{bmatrix} ⊙ \\begin{bmatrix} W₀₀ \u0026amp; W₀₁ \u0026amp; W₀₂ \\\\ 0 \u0026amp; 0\u0026amp;0 \\end{bmatrix} \\\\ = \\begin{bmatrix} \\frac{∂L}{∂T_{00}} W₀₀ \u0026amp; \\frac{∂L}{∂T_{01}} W₀₁ \u0026amp; \\frac{∂L}{∂T_{02}} W₀₂ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe derivative of Loss w.r.t. J₀₀ is the sum of all elements in the \u0026ldquo;derivative matrix\u0026rdquo;:\n$$ \\frac{∂L}{∂J₀₀} = \\frac{∂L}{∂T_{00}} W₀₀ + \\frac{∂L}{∂T_{01}} W₀₁ + \\frac{∂L}{∂T_{02}} W₀₂ $$\nIn 3DGS code, the 3-rd row of the Jacobian matrix is set to 0 (as opacity isn\u0026rsquo;t obtained by integration along z-axis of ray space, but learned), thus no contribution to the derivative of Loss. Only compute the derivative of Loss w.r.t. J₀₀, J₀₂, J₁₁, J₁₂\n$$ \\frac{∂L}{∂𝐉} = \\begin{bmatrix} \\frac{∂L}{∂J₀₀} \u0026amp; 0 \u0026amp; \\frac{∂L}{∂J₀₂} \\\\ 0 \u0026amp; \\frac{∂L}{J₁₁} \u0026amp; \\frac{∂L}{∂₁₂} \\end{bmatrix} $$\n1 float dL_dJ00 = W[0][0] * dL_dT00 + W[0][1] * dL_dT01 + W[0][2] * dL_dT02; dJ/dt The 3D mean 𝐭 in 𝐉 is the coordinates of a Gaussian center in camera space, Not the clip space, whereas the 2D mean 𝛍 indeed originates from the clip coordinates. Clip space is the scaled camera space for frustum clipping, and it\u0026rsquo;ll lead to 2D-plane projection coordinates (x,y) ranging in [-1,1]. However, the 𝐭 in 𝐉 must be camera-space coordinates, instead of the clip coordinates, because the perspective division approximated by the Jacobian 𝐉 uses coordinates of the 3D mean in camera space.\nThe 𝐉 is the projective transform that transforms a point to 3D ray space (screen is the ray space with z-axis omitted). 𝐉 is an approximation of the non-linear perspective division with the 1st-order Taylor expansion evaluated at the 3D Gaussian center 𝐭 in camera space.\nIn 3DGS, the 3rd row of 𝐉 is set to 0, as the opacity of each Gaussian is learned through gradient descent, instead of an integral over the viewing ray in the ray space. Thus, the z-axis of the ray space is useless in 3DGS.\nBased on the relationship:\n$$ 𝐉 = \\begin{bmatrix} f_x/t_z \u0026amp; 0 \u0026amp; -f_x t_x / t_z² \\\\ 0 \u0026amp; f_y/t_z \u0026amp; -f_y t_y/t_z² \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe derivatives of 𝐉 w.r.t. $t_x,\\ t_y,\\ t_z$:\n$$ \\begin{aligned} \\frac{∂𝐉}{∂t_x} \u0026amp;= \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; -f_x / t_z² \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{∂𝐉}{∂t_y} \u0026amp;= \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -f_y / t_z²\\end{bmatrix} \\\\ \\frac{∂𝐉}{∂t_z} \u0026amp;= \\begin{bmatrix} -f_x⋅t_z^{-1} \u0026amp; 0 \u0026amp; 2f_x t_x ⋅t_z^{-3} \\\\ 0 \u0026amp; -f_y⋅t_z^{-1} \u0026amp; 2f_y t_y ⋅t_z^{-3} \\\\ \\end{bmatrix} \\end{aligned} $$\nCombine upstream coefficients:\n$$ \\begin{aligned} \\frac{∂L}{∂t_x} \u0026amp;= \\frac{∂L}{∂𝐉}⊙ \\frac{∂𝐉}{∂t_x} = \\frac{∂L}{∂J_{02}} ⋅ -f_x/t_z^2 \\\\ \\frac{∂L}{∂t_y} \u0026amp;= \\frac{∂L}{∂𝐉}⊙ \\frac{∂𝐉}{∂t_y} = \\frac{∂L}{∂J_{12}} ⋅ -f_y/t_z^2 \\\\ \\frac{∂L}{∂t_z} \u0026amp;= \\frac{∂L}{∂𝐉}⊙ \\frac{∂𝐉}{∂t_z} = \\begin{bmatrix} \\frac{∂L}{∂J₀₀} \u0026amp; 0 \u0026amp; \\frac{∂L}{∂J₀₂} \\\\ 0 \u0026amp; \\frac{∂L}{J₁₁} \u0026amp; \\frac{∂L}{∂₁₂} \\end{bmatrix} ⊙ \\begin{bmatrix} -f_x⋅t_z^{-2} \u0026amp; 0 \u0026amp; 2f_x t_x ⋅t_z^{-3} \\\\ 0 \u0026amp; -f_y⋅t_z^{-2} \u0026amp; 2f_y t_y ⋅t_z^{-3} \\\\ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{∂L}{∂J₀₀} ⋅-f_x⋅t_z^{-2} \u0026amp; 0 \u0026amp; \\frac{∂L}{∂J₀₂}⋅ 2f_x t_x ⋅t_z^{-3} \\\\ 0 \u0026amp; \\frac{∂L}{J₁₁} ⋅-f_y⋅t_z^{-2} \u0026amp; \\frac{∂L}{∂₁₂} ⋅2f_y t_y ⋅t_z^{-3} \\end{bmatrix} \\end{aligned} $$\nCode\n1 2 3 4 // h_x and h_y are f_x and f_y float dL_dtx = x_grad_mul * -h_x * tz2 * dL_dJ02; float dL_dty = y_grad_mul * -h_y * tz2 * dL_dJ12; float dL_dtz = -h_x * tz2 * dL_dJ00 - h_y * tz2 * dL_dJ11 + (2 * h_x * t.x) * tz3 * dL_dJ02 + (2 * h_y * t.y) * tz3 * dL_dJ12; dt/dx After the projective transform, the derivative of $𝐭$ (Gaussian center coordinates) in camera space w.r.t. $𝐱$ in world space follows.\nBased on the relationship of w2c viewmatrix:\n$$ \\begin{aligned} 𝐭_{cam} \u0026amp;= 𝐑_{w2c}⋅𝐱_{world} + 𝐓_{w2c} \\\\ \u0026amp;=\\begin{bmatrix} R₀₀ \u0026amp; R₀₁ \u0026amp; R₀₂ \\\\ R₁₀ \u0026amp; R₁₁ \u0026amp; R₁₂ \\\\ R₂₀ \u0026amp; R₂₁ \u0026amp; R₂₂ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} + \\begin{bmatrix} T_0 \\\\ T_1 \\\\ T_2 \\end{bmatrix} \\\\ \u0026amp;=\\begin{bmatrix} R₀₀x + R₀₁y + R₀₂z + T_0 \\\\ R₁₀x + R₁₁y + R₁₂z + T_1 \\\\ R₂₀x + R₂₁y + R₂₂z + T_2 \\end{bmatrix} \\end{aligned} $$\nThe derivative of $𝐭_{cam}$ w.r.t. $x, y, z$ of $𝐱_{world}$ are:\n$$ \\frac{∂𝐭}{∂x} = \\begin{bmatrix} R₀₀\\\\ R₁₀\\\\ R₂₀ \\end{bmatrix}, \\frac{∂𝐭}{∂y} = \\begin{bmatrix} R₀₁\\\\ R₁₁\\\\ R₂₁ \\end{bmatrix}, \\frac{∂𝐭}{∂z} = \\begin{bmatrix} R₀₂\\\\ R₁₂\\\\ R₂₂ \\end{bmatrix} $$\nThe derivative of $L$ w.r.t. $x, y, z$ of $𝐱_{world}$ are:\n$$ \\frac{∂L}{∂𝐭} \\frac{∂𝐭}{∂x} = \\frac{∂L}{∂t_x}\\begin{bmatrix} R₀₀\\\\ R₁₀\\\\ R₂₀ \\end{bmatrix}, \\frac{∂L}{∂𝐭} \\frac{∂𝐭}{∂y} = \\frac{∂L}{∂t_y}\\begin{bmatrix} R₀₁\\\\ R₁₁\\\\ R₂₁ \\end{bmatrix}, \\frac{∂L}{∂𝐭} \\frac{∂𝐭}{∂z} = \\frac{∂L}{∂t_z}\\begin{bmatrix} R₀₂\\\\ R₁₂\\\\ R₂₂ \\end{bmatrix} $$\nAs the x, y, z are individual variables to be optimized, the derivatives of L w.r.t. each of them shouldn\u0026rsquo;t add up.\n1 2 3 4 5 6 7 8 9 __forceinline__ __device__ float3 transformPoint4x3(const float3\u0026amp; p, const float* matrix) { float3 transformed = { matrix[0] * p.x + matrix[4] * p.y + matrix[8] * p.z + matrix[12], matrix[1] * p.x + matrix[5] * p.y + matrix[9] * p.z + matrix[13], matrix[2] * p.x + matrix[6] * p.y + matrix[10] * p.z + matrix[14], }; return transformed; } bw preprocess (2024-02-15)\nThe preprocess performed conversions from SH to RGB, from quaternion to a 3x3 rotation matrix, and from a stretching vector to the 3x3 covariance matrix in the world space.\nTherefore, the inputs to the function BACKWARD::preprocessCUDA are the derivatives of Loss w.r.t. upstream variables: $\\frac{∂L}{∂(RGB)},\\ \\frac{∂L}{∂\\bm Σ_{world}},\\ \\frac{∂L}{∂𝐱_{world}},\\ \\frac{∂L}{∂\\bm μ_{2D}}$. to solve the derivatives of Loss w.r.t. stretching vector, quaternion, and SH coeffs.\nd_pix/d_world (2024-02-16)\nGaussian center transformation from world space to 2D screen:\n( μ s ₓ c 𝛍 , r μ e _ e y n ) D i v c 𝛍 l ᶜ i p P r o j M a t ( x w , o 𝐱 y r , l z d ) The derivative of Loss w.r.t. the 3D Gaussian mean 𝐱 in world space comprises of 2 streams:\n$$\\frac{∂L}{∂𝐱} = \\frac{∂L}{∂𝐉} \\frac{∂𝐉}{∂𝐱} + \\frac{∂L}{∂\\bm μ} \\frac{∂\\bm μ}{∂𝐱} $$\nThe first term resulted from 𝐉 has been obtained above, and the relationship between the 2D mean 𝛍 on screen and 3D 𝐱 is the full_proj_transform , i.e., projection matrix @ w2c\n$$ \\begin{aligned} \\bm μᶜ \u0026amp;= 𝐏 [𝐑|𝐓] 𝐱 \\\\ \\begin{bmatrix} μᶜ_x \\\\ μᶜ_y \\\\ μᶜ_z \\\\ μᶜ_w \\end{bmatrix} \u0026amp;= \\begin{bmatrix} \\frac{2n}{r-l} \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f}{f-n} \u0026amp; \\frac{-f n}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} R₀₀ \u0026amp; R₀₁ \u0026amp; R₀₂ \u0026amp; T₀ \\\\ R₁₀ \u0026amp; R₁₁ \u0026amp; R₁₂ \u0026amp; T₁ \\\\ R₂₀ \u0026amp; R₂₁ \u0026amp; R₂₂ \u0026amp; T₂ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{2n}{r-l} R₀₀ + \\frac{r+l}{r-l} R₂₀ \u0026amp; \\frac{2n}{r-l} R₀₁ + \\frac{r+l}{r-l} R₂₁ \u0026amp; \\frac{2n}{r-l} R₀₂ + \\frac{r+l}{r-l} R₂₂ \u0026amp; \\frac{2n}{r-l} T₀ + \\frac{r+l}{r-l} T₂ \u0026amp; \\\\ \\frac{2n}{t-b} R₁₀ + \\frac{t+b}{t-b} R₂₀ \u0026amp; \\frac{2n}{t-b} R₀₁ + \\frac{t+b}{t-b} R₂₁ \u0026amp; \\frac{2n}{t-b} R₀₂ + \\frac{t+b}{t-b} R₂₂ \u0026amp; \\frac{2n}{t-b} T₀ + \\frac{t+b}{t-b} T₂ \u0026amp; \\\\ \\frac{f}{f-n} R₂₀ \u0026amp; \\frac{f}{f-n} R₂₁ \u0026amp; \\frac{f}{f-n} R₂₂ \u0026amp; \\frac{f}{f-n} T₂ + \\frac{-f n}{f-n} \\\\ R₂₀ \u0026amp; R₂₁ \u0026amp; R₂₂ \u0026amp; T₂ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} (\\frac{2n}{r-l} R₀₀ + \\frac{r+l}{r-l} R₂₀)x + (\\frac{2n}{r-l} R₀₁ + \\frac{r+l}{r-l} R₂₁)y + (\\frac{2n}{r-l} R₀₂ + \\frac{r+l}{r-l} R₂₂)z + (\\frac{2n}{r-l} T₀ + \\frac{r+l}{r-l} T₂ ) \\\\ (\\frac{2n}{t-b} R₁₀ + \\frac{t+b}{t-b} R₂₀)x + (\\frac{2n}{t-b} R₀₁ + \\frac{t+b}{t-b} R₂₁)y + (\\frac{2n}{t-b} R₀₂ + \\frac{t+b}{t-b} R₂₂)z + (\\frac{2n}{t-b} T₀ + \\frac{t+b}{t-b} T₂ ) \\\\ (\\frac{f}{f-n} R₂₀)x + (\\frac{f}{f-n} R₂₁)y + (\\frac{f}{f-n} R₂₂)z + (\\frac{f}{f-n} T₂ + \\frac{-f n}{f-n}) \\\\ R₂₀x + R₂₁y + R₂₂z + T₂ \\end{bmatrix} \\end{aligned} $$\nPerform perspective division to get the 2D-plane pixel coordinates:\n$$ \\begin{aligned} \\bm μ \u0026amp;= \\frac{1}{μᶜ_w} \\bm μᶜ \\\\ \\begin{bmatrix} μ_x \\\\ μ_y \\\\ μ_z \\\\ 1 \\end{bmatrix} \u0026amp;= \\begin{bmatrix} μᶜ_x/μᶜ_w \\\\ μᶜ_y/μᶜ_w \\\\ μᶜ_z/μᶜ_w \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} μᶜ_x/(R₂₀x + R₂₁y + R₂₂z + T₂) \\\\ μᶜ_y/(R₂₀x + R₂₁y + R₂₂z + T₂) \\\\ μᶜ_z/(R₂₀x + R₂₁y + R₂₂z + T₂) \\\\ 1 \\end{bmatrix} \\end{aligned} $$\nwhere $(μ_x,\\ μ_y)$ are the pixel coordinates, and $μ_z$ is similar to the z in ND space, but ranges in [0,1]. As $μ_z$ doesn\u0026rsquo;t involve into the image formation in 3DGS, it has no contribution to the Loss, and won\u0026rsquo;t participate the derivative of Loss w.r.t the world coordinates. The derivative of $\\bm μ$ w.r.t. $𝐱$:\n$$ \\begin{aligned} \\frac{∂μ_x}{∂x} \u0026amp;= \\frac{(\\frac{2n}{r-l} R₀₀ + \\frac{r+l}{r-l} R₂₀)(R₂₀x + R₂₁y + R₂₂z + T₂) - μᶜ_x R₂₀ }{(R₂₀x + R₂₁y + R₂₂z + T₂)^2} \\\\ \u0026amp;+ \\frac{ (\\frac{2n}{t-b} R₁₀ + \\frac{t+b}{t-b} R₂₀) (R₂₀x + R₂₁y + R₂₂z + T₂) - μᶜ_y R₂₀ }{(R₂₀x + R₂₁y + R₂₂z + T₂)^2} \\end{aligned} $$\nThe element indices in the float array proj is column-major: $\\begin{bmatrix} 0 \u0026amp; 4 \u0026amp; 8 \u0026amp; 12 \\\\ 1 \u0026amp; 5 \u0026amp; 9 \u0026amp; 13 \\\\ 2 \u0026amp; 6 \u0026amp; 10 \u0026amp; 14 \\\\ 3 \u0026amp; 7 \u0026amp; 11 \u0026amp; 15 \\end{bmatrix}$\n1 2 3 4 5 6 7 8 float m_w = 1.0f / (m_hom.w + 0.0000001f); glm::vec3 dL_dmean; float mul1 = (proj[0] * m.x + proj[4] * m.y + proj[8] * m.z + proj[12]) * m_w * m_w; float mul2 = (proj[1] * m.x + proj[5] * m.y + proj[9] * m.z + proj[13]) * m_w * m_w; dL_dmean.x = (proj[0] * m_w - proj[3] * mul1) * dL_dmean2D[idx].x + (proj[1] * m_w - proj[3] * mul2) * dL_dmean2D[idx].y; dL_dmean.y = (proj[4] * m_w - proj[7] * mul1) * dL_dmean2D[idx].x + (proj[5] * m_w - proj[7] * mul2) * dL_dmean2D[idx].y; dL_dmean.z = (proj[8] * m_w - proj[11] * mul1) * dL_dmean2D[idx].x + (proj[9] * m_w - proj[11] * mul2) * dL_dmean2D[idx].y; d_RGB/d_SH d_cov3D/d_M (2024-02-17)\nGoal: Compute the derivative w.r.t. the entire matrix 𝐌 (not per element).\nGiven a quaternion: $q = [r,x,y,z]$, the rotation matrix is:\n$$ 𝐑 = \\begin{bmatrix} 1 - 2(y² + z²) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 2(xy + rz) \u0026amp; 1 - 2(x² + z²) \u0026amp; 2(yz - rx) \\\\ 2(xz - ry) \u0026amp; 2(yz + rx) \u0026amp; 1 - 2(x²+y²) \\end{bmatrix} $$\nGiven a scaling vector $s=[s_x, s_y, s_z]$, the stretching matrix is:\n$$ 𝐒 = \\begin{bmatrix} s_x \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_z \\end{bmatrix} $$\nThe covariance matrix can be decomposed by SVD as:\n$$ \\bm Σ = 𝐑𝐒𝐒ᵀ𝐑ᵀ $$\nUse 𝐌 to represent 𝐑𝐒:\n$$ 𝐌 = 𝐑𝐒 \\\\ = \\begin{bmatrix} s_x(1 - 2(y² + z²)) \u0026amp; s_y(2(xy - rz)) \u0026amp; s_z(2(xz + ry)) \\\\ s_x(2(xy + rz)) \u0026amp; s_y(1 - 2(x² + z²)) \u0026amp; s_z(2(yz - rx)) \\\\ s_x(2(xz - ry)) \u0026amp; s_y(2(yz + rx)) \u0026amp; s_z(1 - 2(x²+y²)) s_x\\end{bmatrix} $$\nTherefore, $\\bm Σ = 𝐌 𝐌ᵀ$\n$$ \\bm Σ = \\begin{bmatrix} m₀₀ \u0026amp; m₀₁ \u0026amp; m₀₂ \\\\ m₁₀ \u0026amp; m₁₁ \u0026amp; m₁₂ \\\\ m₂₀ \u0026amp; m₂₁ \u0026amp; m₂₂ \\\\ \\end{bmatrix} \\begin{bmatrix} m₀₀ \u0026amp; m₁₀ \u0026amp; m₂₀ \\\\ m₀₁ \u0026amp; m₁₁ \u0026amp; m₂₁ \\\\ m₀₂ \u0026amp; m₁₂ \u0026amp; m₂₂ \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} m₀₀²+m₀₁²+m₀₂² \u0026amp; m₀₀m₁₀+m₀₁m₁₁+m₀₂m₁₂ \u0026amp; m₀₀m₂₀+m₀₁m₂₁+m₀₂m₂₂ \\\\ m₁₀m₀₀+m₁₁m₀₁+m₁₂m₀₂ \u0026amp; m₁₀²+m₁₁²+m₁₂² \u0026amp; m₁₀m₂₀+m₁₁m₂₁+m₁₂m₂₂ \\\\ m₂₀m₀₀+m₂₁m₀₁+m₂₂m₀₂ \u0026amp; m₂₀m₁₀+m₂₁m₁₁+m₂₂m₁₂ \u0026amp; m₂₀²+m₂₁²+m₂₂² \\\\ \\end{bmatrix} $$\nThe partial derivative of covariance matrix w.r.t. the element m₀₀:\n$$ \\frac{∂\\bm Σ}{∂m₀₀} = \\begin{bmatrix} 2m₀₀ \u0026amp; m₁₀ \u0026amp; m₂₀ \\\\ m₁₀ \u0026amp; 0 \u0026amp; 0 \\\\ m₂₀ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nThe contribution of m₀₀ to the 𝚺 is the summation of all elements. Similarly, other elements\u0026rsquo; contiribution to d𝚺 are as follows:\n$$ \\begin{aligned} \\frac{∂\\bm Σ}{∂m₀₀} = 2(m₀₀+m₁₀+m₂₀)\\ \u0026amp; \\frac{∂\\bm Σ}{∂m₀₁} = 2(m₀₁+m₁₁+m₂₁) \u0026amp; \\frac{∂\\bm Σ}{∂m₀₂} = 2(m₀₂+m₁₂+m₂₂) \\\\ \\frac{∂\\bm Σ}{∂m₁₀} = 2(m₀₀+m₁₀+m₂₀)\\ \u0026amp; \\frac{∂\\bm Σ}{∂m₁₁} = 2(m₀₁+m₁₁+m₂₁) \u0026amp; \\frac{∂\\bm Σ}{∂m₁₂} = 2(m₀₂+m₁₂+m₂₂) \\\\ \\frac{∂\\bm Σ}{∂m₂₀} = 2(m₀₀+m₁₀+m₂₀)\\ \u0026amp; \\frac{∂\\bm Σ}{∂m₂₁} = 2(m₀₁+m₁₁+m₂₁) \u0026amp; \\frac{∂\\bm Σ}{∂m₂₂} = 2(m₀₂+m₁₂+m₂₂) \\end{aligned} $$\n$$ \\frac{∂\\bm Σ}{∂𝐌 } = 2 \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} m₀₀ \u0026amp; m₀₁ \u0026amp; m₀₂ \\\\ m₁₀ \u0026amp; m₁₁ \u0026amp; m₁₂ \\\\ m₂₀ \u0026amp; m₂₁ \u0026amp; m₂₂ \\\\ \\end{bmatrix} $$\nConcat with the upstream coefficient to get the derivative of Loss w.r.t. the matrix 𝐌: $\\frac{∂L}{∂𝐌 } = \\frac{∂L}{∂\\bm Σ} \\frac{∂\\bm Σ}{∂𝐌 }$.\nIn the code, however, $\\frac{∂\\bm Σ}{∂𝐌 }$ is on the left: $\\frac{∂L}{∂𝐌 } = \\frac{∂\\bm Σ}{∂𝐌 } \\frac{∂L}{∂\\bm Σ}$\nBecause the $\\frac{∂L}{∂𝐌 }$ to be solved is a derivative w.r.t. the entire matrix 𝐌, not for each element, the 2 \u0026ldquo;derivative matrices\u0026rdquo; should perform matrix multiplication to get all terms mixtured completely, rather than an elementise product:\n$$ \\frac{∂L}{∂𝐌 } = \\frac{∂\\bm Σ}{∂𝐌 } \\frac{∂L}{∂\\bm Σ} \\\\ =\\begin{bmatrix} 2(m₀₀+m₁₀+m₂₀) \u0026amp; ⋯ \u0026amp; ⋯\\\\ 2(m₀₀+m₁₀+m₂₀) \u0026amp; ⋯ \u0026amp; ⋯\\\\ ⋯ \u0026amp; ⋯ \u0026amp; ⋯ \\end{bmatrix} \\begin{bmatrix} \\frac{∂L}{∂v_0} \u0026amp; \\frac{∂L}{∂v_1} \u0026amp; \\frac{∂L}{∂v_2} \\\\ \\frac{∂L}{∂v_1} \u0026amp; \\frac{∂L}{∂v_3} \u0026amp; \\frac{∂L}{∂v_4} \\\\ \\frac{∂L}{∂v_2} \u0026amp; \\frac{∂L}{∂v_4} \u0026amp; \\frac{∂L}{∂v_5} \\end{bmatrix} $$\n1 glm::mat3 dL_dM = 2.0f * M * dL_dSigma; glm performs matrix multiplication according to the same rules as in the math textbook, and the only difference is that the row and column indices for an elements are reversed: mat[col][row] for an element; mat[col] for a column vector. Matrices and Quaternions FAQ - opengl-tutorial Tutorial 3 : Matrices - opengl-tutorial dM/ds Goal: Compute derivative of loss w.r.t. each element in the scaling vector.\nIn the code, the 𝐌 =𝐒 𝐑, not 𝐑𝐒,\n$$ 𝐌 =\\begin{bmatrix} s_x \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_z \\end{bmatrix} \\begin{bmatrix} 1 - 2(y² + z²) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 2(xy + rz) \u0026amp; 1 - 2(x² + z²) \u0026amp; 2(yz - rx) \\\\ 2(xz - ry) \u0026amp; 2(yz + rx) \u0026amp; 1 - 2(x²+y²) \\end{bmatrix} \\\\ =\\begin{bmatrix} s_x(1 - 2(y² + z²)) \u0026amp; s_x(2(xy - rz)) \u0026amp; s_x(2(xz + ry)) \\\\ s_y(2(xy + rz)) \u0026amp; s_y(1 - 2(x² + z²)) \u0026amp; s_y(2(yz - rx)) \\\\ s_z(2(xz - ry)) \u0026amp; s_z(2(yz + rx)) \u0026amp; s_z(1 - 2(x²+y²)) \\end{bmatrix} $$\nCalculate the partial derivative of 𝐌 w.r.t. the scaling vector 𝐒 by element:\n$$ \\frac{∂𝐌 }{∂s_x} = \\begin{bmatrix} 1-2(y² + z²) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp;0 \\end{bmatrix} $$\nAs here it\u0026rsquo;s computing the derivative of the loss w.r.t. each element, the multiplication with the upstream derivative part is an element-wise product:\n$$ \\frac{∂L}{∂s_x} = \\frac{∂L}{∂𝐌 }\\frac{∂𝐌 }{∂s_x} = \\\\ \\begin{bmatrix} s_x(1 - 2(y² + z²)) \u0026amp; s_x(2(xy - rz)) \u0026amp; s_x(2(xz + ry)) \\\\ ⋯ \u0026amp; ⋯ \u0026amp; ⋯ \\\\ ⋯ \u0026amp; ⋯ \u0026amp; ⋯ \\\\ \\end{bmatrix} ⊙ \\begin{bmatrix} 1-2(y² + z²) \u0026amp; 2(xy - rz) \u0026amp; 2(xz + ry) \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp;0 \\end{bmatrix} $$\nBecause glm indexes elements by columns, to easily access a row in the above 2 matrices, their transposed matrices are used:\n1 2 3 4 5 6 glm::mat3 Rt = glm::transpose(R); glm::mat3 dL_dMt = glm::transpose(dL_dM); // Gradients of loss w.r.t. scale glm::vec3* dL_dscale = dL_dscales + idx; dL_dscale-\u0026gt;x = glm::dot(Rt[0], dL_dMt[0]);\t// scalar mat[0] takes out a column, as the indices in glm is column-major. 9. Matrix columns dM/dq Goal: Compute derivative of loss w.r.t. each element in the quaternion vector (r,x,y,z).\nBased on the realtionship:\n$$ 𝐌 =\\begin{bmatrix} s_x(1 - 2(y² + z²)) \u0026amp; s_x(2(xy - rz)) \u0026amp; s_x(2(xz + ry)) \\\\ s_y(2(xy + rz)) \u0026amp; s_y(1 - 2(x² + z²)) \u0026amp; s_y(2(yz - rx)) \\\\ s_z(2(xz - ry)) \u0026amp; s_z(2(yz + rx)) \u0026amp; s_z(1 - 2(x²+y²)) \\end{bmatrix} $$\n$$ \\frac{∂𝐌 }{∂r} = \\begin{bmatrix} 0 \u0026amp; -2s_x z \u0026amp; 2s_x y \\\\ 2s_y z \u0026amp; 0 \u0026amp; -2s_y x \\\\ -2s_z y \u0026amp; 2s_z x \u0026amp; 0 \\end{bmatrix} $$\nConcat with the obtained upstream derivative of Loss w.r.t. 𝐌 through element-wise (Hadamard) product:\n$$ \\frac{∂L}{∂r} = \\frac{∂L}{∂𝐌 } \\frac{∂𝐌 }{∂r} = \\\\ \\begin{bmatrix} \\frac{∂L}{∂m_{00}} \u0026amp; \\frac{∂L}{∂m_{01}} \u0026amp; \\frac{∂L}{∂m_{02}} \\\\ \\frac{∂L}{∂m_{10}} \u0026amp; \\frac{∂L}{∂m_{11}} \u0026amp; \\frac{∂L}{∂m_{12}} \\\\ \\frac{∂L}{∂m_{20}} \u0026amp; \\frac{∂L}{∂m_{21}} \u0026amp; \\frac{∂L}{∂m_{22}} \\\\ \\end{bmatrix} ⊙ \\begin{bmatrix} 0 \u0026amp; -2s_x z \u0026amp; 2s_x y \\\\ 2s_y z \u0026amp; 0 \u0026amp; -2s_y x \\\\ -2s_z y \u0026amp; 2s_z x \u0026amp; 0 \\end{bmatrix} \\\\ = \\begin{bmatrix} 0 \u0026amp; \\frac{∂L}{∂m_{01}} (-2s_x z) \u0026amp; \\frac{∂L}{∂m_{02}} 2s_x y \\\\ \\frac{∂L}{∂m_{10}} 2s_y z \u0026amp; 0 \u0026amp; \\frac{∂L}{∂m_{12}} (-2s_y x) \\\\ \\frac{∂L}{∂m_{20}} (-2s_z y) \u0026amp; \\frac{∂L}{∂m_{21}} 2s_z x \u0026amp; 0 \\\\ \\end{bmatrix} $$\n$$ $$\nSum all element in the result matrix $\\frac{∂L}{∂r}$:\n1 2 3 4 5 6 7 8 9 dL_dMt[0] *= s.x; dL_dMt[1] *= s.y; dL_dMt[2] *= s.z; // Gradients of loss w.r.t. normalized quaternion glm::vec4 dL_dq; dL_dq.x = 2 * z * (dL_dMt[0][1] - dL_dMt[1][0]) + 2 * y * (dL_dMt[2][0] - dL_dMt[0][2]) + 2 * x * (dL_dMt[1][2] - dL_dMt[2][1]); Finally, calculate the derivative of loss for the normalization process of the quaternion.\nBased on the normalization operation:\n$$ 𝐪_{unit} = \\frac{1}{‖𝐪‖} ⋅ 𝐪 = \\frac{1}{\\sqrt{r²+x²+y²+z²}} \\begin{bmatrix} r \\\\ x \\\\ y \\\\ z \\end{bmatrix} \\\\ \\frac{∂𝐪}{∂r} = \\frac{1}{r²+x²+y²+z²} - \\frac{r²}{(r²+x²+y²+z²)^{3/2}} $$\nIt seems that the derivatives of the normalization step contributions are not considered in the code:\n1 2 3 4 // Gradients of loss w.r.t. unnormalized quaternion float4* dL_drot = (float4*)(dL_drots + idx); *dL_drot = float4{ dL_dq.x, dL_dq.y, dL_dq.z, dL_dq.w }; //dnormvdv(float4{ rot.x, rot.y, rot.z, rot.w }, float4{ dL_dq.x, dL_dq.y, dL_dq.z, dL_dq.w }); ","date":"2024-01-22T00:00:00Z","image":"https://user-images.githubusercontent.com/52232153/283030293-f9f3589c-fa59-431f-8bd0-478337426933.png","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-code/","title":"read: 3DGS | Code Understanding"},{"content":"CUDA Tutorials I Profiling and Debugging Applications - NVIDIA Developer\n(2024-01-20)\nSource video: GPU L16: Support: cuda-gdb - YouTube - HPC Education (Rupesh Nasre 2021)\nIt\u0026rsquo;s a gdb extension for real hardware (not a simulator). Comparing with Nsight having GUI, CUDA-GDB is CLI. Regretfully, cuda-gdb doesn\u0026rsquo;t have TUI. Capture Last Error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Filename: test_cuda-gdb.cu #include \u0026lt;cuda_runtime.h\u0026gt; // to synchronize #include \u0026lt;cstdio\u0026gt; __global__ void kernel(int* x) { *x = 0; printf(\u0026#34;%d\\n\u0026#34;, *x); } int main() { int* x; kernel\u0026lt;\u0026lt;\u0026lt;2, 10\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); // Capture error cudaError_t err = cudaGetLastError(); printf(\u0026#34;err=%d, %s, %s\\n\u0026#34;, err, cudaGetErrorName(err), cudaGetErrorString(err) ); return 0; } Build: nvcc test_cuda-gdb.cu. Execution: ./a.out\nNothing is printed out, although 0 is supposed to show.\nAnd no error is reported, because the CPU sometimes isn\u0026rsquo;t aware of the error (e.g., SegFault) that happens on the GPU.\nTo identify whether the error occurred on the GPU, cudaGetLastError()\n1 2 yi@yi-Alien:~/Downloads/CUDA_Study/Debug_CUDA$ ./a.out err=700, cudaErrorIllegalAddress, an illegal memory access was encountered x requires GPU memory allocated:\n1 2 3 4 5 6 7 8 9 10 11 int main() { int* x; cudaMalloc( (void**)\u0026amp;x, 1*sizeof(int) ); kernel\u0026lt;\u0026lt;\u0026lt;2,2\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); cudaFree(x); cudaError_t err = cudaGetLastError(); printf(\u0026#34;err=%d, %s, %s\\n\u0026#34;, err, cudaGetErrorName(err), cudaGetErrorString(err) ); return 0; } Output 1 2 3 4 5 6 yi@yi-Alien:~/Downloads/CUDA_Study/Debug_CUDA$ ./a.out 0 0 0 0 err=0, cudaSuccess, no error cudaError Homework: Write programs to invoke these errors.\nRef:\nProper CUDA Error Checking - Lei Mao\u0026rsquo;s Log Book\nCUDA DEBUGGING - Bob Crovella, 9/14/2021\nNVIDIA CUDA Library: cudaError\nCUDA-GDB CLI Set flags to include the symbol information (variable name, function name) into the binary file:\nNames of variables and functions are used only for programming, as execution is instructed by memory addresses. So, symbols will be discarded for efficiency after compilation by default. 1 nvcc -g -G main.cu -g is for __host__ functions, compiled by gcc.\n-G is for __device__ functions, compiled by nvcc.\nDisable optimizations (preventing remove unused code) for debugging line-by-line.\nDebugging with cuda-gdb:\n1 cuda-gdb a.out Given the erroneous code:\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;cuda.h\u0026gt; __global__ void kernel(int* x) { *x = 0; printf(\u0026#34;%d\\n\u0026#34;, *x); } int main() { int* x; kernel\u0026lt;\u0026lt;\u0026lt;2, 2\u0026gt;\u0026gt;\u0026gt;(x); cudaDeviceSynchronize(); return 0; } Build: nvcc test_cuda-gdb.cu. Debug: cuda-gdb a.out.\nrun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (cuda-gdb) run Starting program: /home/yi/Downloads/CUDA_Study/Debug_CUDA/a.out [Thread debugging using libthread_db enabled] Using host libthread_db library \u0026#34;/lib/x86_64-linux-gnu/libthread_db.so.1\u0026#34;. [New Thread 0x7ffff5d9b000 (LWP 2434197)] [New Thread 0x7ffff4ab1000 (LWP 2434198)] [Detaching after fork from child process 2434199] [New Thread 0x7fffeef3d000 (LWP 2434215)] [New Thread 0x7fffed533000 (LWP 2434216)] CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x100002ede48 Thread 1 \u0026#34;a.out\u0026#34; received signal CUDA_EXCEPTION_14, Warp Illegal Address. [Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] 0x00000100002ede78 in kernel(int*)\u0026lt;\u0026lt;\u0026lt;(2,1,1),(2,1,1)\u0026gt;\u0026gt;\u0026gt; () LWP: Light weight process Switching focus to a specific thread info cuda kernels Intro to GPU: 06 Debugging on GPU - YouTube - NERSC\n","date":"2024-01-20T17:20:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/debug_gdb/","title":"memo: CUDA | Debug with CUDA-GDB"},{"content":" Source video: 【并行计算】CUDA在现代C++中如何运用？看这一个就够了！- 双笙子佯谬 - bilibili parallel101/course - Github Testing repo Textbook; pdf Enable CUDA in CMake 1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.10) set(CMAKE_CXX_STANDARD 17) set(CMAKE_BUILD_TYPE Release) # add CUDA project(hellocuda LANGUAGES CXX CUDA) add_executable(main main.cu) CUDA syntax is compatible with C++, so nvcc can compile a C++ project by chaning all .cpp files renamed to .cu. The nvcc can compile CPU and GPU code jointly. CPU-GPU Asyncronous For the sake of efficiency, after CPU tells GPU to run the kernel function (decorated by __global__), CPU proceeds to the next line of code without waiting for the GPU to finish the computation.\nTherefore, in the following code, the printf won\u0026rsquo;t be executed because programs returns directly after CPU pushes the task to GPU execution queue. However, the GPU didn\u0026rsquo;t have time to execute and return results.\n1 2 3 4 5 6 7 8 9 10 11 // Filename: test_async.cu #include \u0026lt;cstdio\u0026gt; __global__ void kernel() { printf(\u0026#34;Hello World!\\n\u0026#34;); } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(); return 0; } Compile: nvcc test_async.cu. Execute application: ./a.out. Set the program to wait for GPU completing all the tasks in its queue:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;iostream\u0026gt; __global__ void kernel() { printf(\u0026#34;Hello World!\\n\u0026#34;); // std::cout \u0026lt;\u0026lt; \u0026#34;out\u0026#34; \u0026lt;\u0026lt; std::endl; } int main(){ kernel\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); return 0; } std::cout and std::endl are \u0026ldquo;host (CPU) functions\u0026rdquo;, which can\u0026rsquo;t be executed on GPU.\n1 test_async.cu(7): error: calling a __host__ function (\u0026#34;std::basic_ostream\u0026lt;char, st...\u0026#34;) from a __global__ function(\u0026#34;kernel\u0026#34;) is not allowed __host__ functions are compiled to callable only for other host functions. NV Forums\nFunction types Docs - Sec 7.1 Function Execution Space Specifiers\n__global__ function: called from the host or other devices, and executed on the device.\n__device__ function: called from other __device__ (or __global__) functions and executed on device.\n__host__ function: called from __host__ functions and executed on CPU.\nA function without decorated by any execution space specifier is compiled as a __host__ function.\nCalling a __device__ function (from other devices) doesn\u0026rsquo;t need \u0026lt;\u0026lt;\u0026lt; \u0026gt;\u0026gt;\u0026gt;, as it\u0026rsquo;s called on the GPU interally:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;cuda_runtime.h\u0026gt; #include \u0026lt;cstdio\u0026gt; __device__ void say_hello() { printf(\u0026#34;hello\\n\u0026#34;); } __global__ void kernel() { say_hello(); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); return 0; } Since __gloabl__ functions are asyncronous and won\u0026rsquo;t return immediately, their return type must be void.\nHowever, the __device__ can have return value, like a normal function.\nA function can be called from either GPU or CPU with using both specifier: __host__ __device__\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; __host__ __device__ void say_hello() { printf(\u0026#34;hello~\\n\u0026#34;); } __global__ void kernel() { say_hello(); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); // gpu version cudaDeviceSynchronize(); say_hello(); // cpu version } Wil the computation in say_hello executed both by CPU and GPU?\nThe constexpr keyword can be replaced with __host__ __device__ by nvcc compiler to enable a constexpr function (e.g., math function) can be called from either a host or a device.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; constexpr const char* cuthead(const char* p) { return p + 1; } __global__ void kernel() { printf(cuthead(\u0026#34;Hello World!\\n\u0026#34;)); } int main() { kernel\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceSynchronize(); print(cuthead(\u0026#34;ABC\\n\u0026#34;)); return 0; } By decorating with __host__ __device__, the constexpr function will be inlined automatically.\nEnable the nvcc flag --expt-relaxed-constexpr with a \u0026ldquo;CMake的生成器表达式来实现只对 .cu 文件有效，而不会在 gcc 编译 .c 文件时生效，不然给到 gcc 就出错了\u0026rdquo; (?):\n1 2 add_executable(main main.cu foo.cpp) target_compile_options(main PUBLIC $\u0026lt;$\u0026lt;COMPILE_LANGUAGE:CUDA\u0026gt;:--expt-relaxed-constexpr\u0026gt;) However, on the contrary, __host__ __device__ can\u0026rsquo;t be replaced with constexpr, because constexpr function cannot call printf and GPU-specific functions, like _syncthreads. inline device function Docs - Sec 7.1.5\nIf appropriate, the compiler will inline __device__ functions automatically.\nWhen the function body is too big, the compiler may won\u0026rsquo;t insert code. __noinline__ declares a function that won\u0026rsquo;t be inserted into the place where it\u0026rsquo;s called.\nAnd __forceinline__ is the opposite.\n","date":"2024-01-19T14:58:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/tut_%E5%BD%AD%E4%BA%8E%E6%96%8C/","title":"watch: Parallel101 - 彭于斌 | CUDA Programming"},{"content":"(2024-02-14)\nDerivative is the amount of change in a target object caused by a variable\u0026rsquo;s change.\nA row a matrix consists of the coefficient of each term in a linear equation. And based on the \u0026ldquo;sum rule\u0026rdquo; of derivative ($(f+g)\u0026rsquo;=f\u0026rsquo;+g\u0026rsquo;$), the derivative of the linear equation w.r.t. a variable is the summation of the derivative of each element in the row w.r.t. the variable.\nd Ax (2024-01-13)\nSource video: Derivative of a Matrix : Data Science Basics - ritvikmath\nMatrix 𝐀() stands for a linear transformation (function). And only the derivative of a function (𝐀𝐱) makes sense.\nMatrix is a representation of linear systems. $$ \\begin{aligned} f(x) \u0026amp;= 𝐀𝐱 \\\\ \u0026amp;= \\begin{bmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\end{bmatrix} \\begin{bmatrix} x₁ \\\\ x_2 \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} x₁ + 2 x₂ \\\\ 3x₁ + 4x₂ \\end{bmatrix} ⇒ \\begin{bmatrix} f₁(x₁,x₂) \\\\ f₂(x₁,x₂) \\end{bmatrix} \\end{aligned} $$\n$$ \\frac{d𝐀𝐱}{d𝐱} = \\begin{bmatrix} ∂f₁/∂x₁ \u0026amp; ∂f₁/∂x₂ \\\\ ∂f₂/∂x₁ \u0026amp; ∂f₂/∂x₂ \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\end{bmatrix} $$\nThe derivative of the linear transformation 𝐀𝐱 w.r.t. x is A. It analog to single-variable function.\nA matrix $A$ is a \u0026ldquo;scalar\u0026rdquo;. More concretely, it\u0026rsquo;s a collection of scalars in a box.\nTherefore, the derivative of A means the derivative of a constant, which would be 0. So, it doesn\u0026rsquo;t make any sense.\nThereby, we are not calculating the derivative of a matrix, but the derivative of a linear transformation 𝐀𝐱 w.r.t. 𝐱.\nd xᵀAx $$ \\begin{aligned} 𝐱ᵀ𝐀𝐱 \u0026amp;= \\begin{bmatrix} x₁ \u0026amp; x₂ \\end{bmatrix} \\begin{bmatrix} a₁₁ \u0026amp; a₁₂ \\\\ a₂₁ \u0026amp; a₂₂ \\end{bmatrix} \\begin{bmatrix} x₁ \\\\ x₂ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} x₁ \u0026amp; x₂ \\end{bmatrix} \\begin{bmatrix} a₁₁x₁+ a₁₂x₂ \\\\ a₂₁x₁ + a₂₂x₂ \\end{bmatrix} \\\\ \u0026amp;= a₁₁x₁²+ a₁₂x₁x₂ + a₂₁x₁x₂ + a₂₂x₂² ⇒ f(x₁,x₂) \\end{aligned} $$\nConsider 𝐀 is a symmetric matrix, so a₂ = a₃. Then, $𝐱ᵀ𝐀𝐱 = a₁₁x₁²+ a₁₂x₁x₂ + a₂₁x₁x₂ + a₂₂x₂² = f(x₁,x₂)$\nThe derivative of the linear transformation 𝐱ᵀ𝐀𝐱:\n$$ \\begin{aligned} \\frac{d𝐱ᵀ𝐀𝐱}{d𝐱} \u0026amp;= \\begin{bmatrix} ∂f/∂x₁ \\\\ ∂f/∂x₂ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} 2a₁₁x₁+2a₁₂x₂ \\\\ 2a₁₂x₂ + 2a₂₂x₂ \\end{bmatrix} \\\\ \u0026amp;= 2 \\begin{bmatrix} a₁₁ \u0026amp; a₁₂ \\\\ a₁₂ \u0026amp; a₂₂ \\end{bmatrix} \\begin{bmatrix} x₁ \\\\ x₂ \\end{bmatrix} \\\\ \u0026amp;= 2𝐀𝐱 \\end{aligned} $$\nIt\u0026rsquo;s an analog to quadratic of matrix operations.\n3 cases Source article: The derivative matrix - Math Insight\nA matrix 𝐀 contains elements that are functions of a scalar x.\nThe $\\frac{d𝐀}{dx}$ is a matrix of the same size as 𝐀.\nRefer to Definition 5 in Matrix Differentiation - Department of Atmospheric Sciences\nThe derivative of a multi-variable scalar-valued function $f$ is a matrix of partial derivatives of each function with respect to each variable.\nDerivative of 𝐟 w.r.t. each coordinate axis. $\\frac{df}{d𝐱} = [ \\frac{∂f}{∂x₁}\\ \\frac{∂f}{∂x₂}\\ ⋯ \\ \\frac{∂f}{∂xₙ} ]$ A matrix 𝐀 contains elements that are functions of a vector 𝐱.\n$𝐀(𝐱) = 𝐟(𝐱) = (f_1(𝐱),\\ f_2(𝐱),\\ \u0026hellip;, f_m(𝐱)) = \\begin{bmatrix} f_1(𝐱) \\\\ f_2(𝐱) \\\\ ⋮ \\\\ f_m(𝐱) \\end{bmatrix}$\nThe $\\frac{d𝐀}{d𝐱}$ is a matrix with the size of mxn:\n$$ \\frac{d𝐀}{d𝐱} = \\begin{bmatrix} \\frac{f_1}{x_1} \u0026amp; \\frac{f_1}{x_2} \u0026amp; ⋯ \u0026amp; \\frac{f_1}{xₙ} \\\\ \\frac{f_2}{x_1} \u0026amp; \\frac{f_2}{x_2} \u0026amp; ⋯ \u0026amp; \\frac{f_2}{xₙ} \\\\ ⋮ \u0026amp; ⋮ \u0026amp; ⋮ \u0026amp; ⋮ \\\\ \\frac{f_m}{x_1} \u0026amp; \\frac{f_m}{x_2} \u0026amp; ⋯ \u0026amp; \\frac{f_m}{xₙ} \\\\ \\end{bmatrix} $$\nMatrix derivative (2023-02-12)\nMatrix derivatie is in terms of the whole matrix, instead of each element. Whereas partial derivatives of a matrix\nGiven a matrix $[^{a\\ b}_{d\\ c}]$,the derivative of its inverse matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$ w.r.t. the original matrix is the \u0026ldquo;coefficient\u0026rdquo; in their relation:\n$$ \\underbrace{ \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} }_{\\text{Coefficient}} \\begin{bmatrix} a \u0026amp; b \\\\ d \u0026amp; c \\end{bmatrix} = \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} $$\nThis transformation can be understood as that the original matrix first times its inverse $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$ to become the identity matrix $[^{1\\ 0}_{0\\ 1}]$, which gets multiplied by $[^{\\ c\\ -b}_{-d\\ a}]$ to yield the inverse matrix.\nTherefore, the coefficient is:\n$$ \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} = \\frac{1}{ac-bd} \\begin{bmatrix} c² + bd \u0026amp; -bc-ab \\\\ -cd-ad \u0026amp; bd+a²\\end{bmatrix} $$\nIn this case, is the optimizing objective the whole matrix $[^{a\\ b}_{d\\ c}]$, with its coefficient serving as the gradient?\nperplexity\nOn the other hand, the partial derivatives of the inverse matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$ with respect to each element a, b, c, d can be conceptualized as:\nhow does changes in the 4 \u0026ldquo;variables\u0026rdquo; $a,\\ b,\\ c,\\ d$ affect the matrix $\\frac{1}{ac-bd}[^{\\ c\\ -b}_{-d\\ a}]$\n$$ \\begin{aligned} \\frac{ ∂\\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix}}{∂a} \u0026amp;= \\begin{bmatrix} \\frac{∂}{∂a} (\\frac{c}{ac-bd} ) \u0026amp; \\frac{∂}{∂a} (\\frac{-b}{ac-bd}) \\\\ \\frac{∂}{∂a} (\\frac{-d}{ac-bd}) \u0026amp; \\frac{∂}{∂a} (\\frac{a}{ac-bd} ) \\\\ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\frac{-c²}{(ac-bd)²} \u0026amp; \\frac{bc}{(ac-bd)²} \\\\ \\frac{dc}{(ac-bd)²} \u0026amp; \\frac{-bd}{(ac-bd)²} \\\\ \\end{bmatrix} \\end{aligned} $$\nThe total change of the matrix magnitude caused by moving $a$ by one unit would be:\n$$\\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{∂a} = \\frac{-c² + bc + dc - bd}{(ac-bd)²} $$\nParticularly, with this derivative, $a$ can be optimized via gradient descent. Similarly, the partial derivatives of the matrix w.r.t. $b,\\ c,\\ d$ are:\n$$ \\begin{aligned} \\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{∂b} \u0026amp;= \\frac{cd-ac-d²+ad}{(ac-bd)²} \\\\ \\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{∂c} \u0026amp;= \\frac{-bd+ba+da-a²}{(ac-bd)²} \\\\ \\frac{∂ (\\frac{1}{ac-bd} [^{\\ c\\ -b}_{-d\\ a}] )}{∂d} \u0026amp;= \\frac{cb-b²-ac+ab}{(ac-bd)²} \\\\ \\end{aligned} $$\n(2024-02-13)\nMatrix Derivatives: What\u0026rsquo;s up with all those transposes ? - David Levin\nGradient: Matrix form -\u0026gt; indices form -\u0026gt; matrix form\nMatrix Calculus - Online\nXᵀwX (2024-04-06)\n拆分成：向量函数 + 多元函数\n空间的基可以是多项式函数, 幂函数, 所以线性方程可以表示非线性函数\n【微积分和线性代数碰撞的数学盛宴：最小二乘法公式推导！】-晓之车高山老师 - bilibili\n","date":"2024-01-13T09:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/matrix_derivative/","title":"memo: Calc | Derivative of \"Matrix\""},{"content":"Code | arXiv | Vickie Ye\nSurfaced by NeRF\u0026amp;Beyond 12.5日报(CustomNeRF，VideoRF，网格引导编辑，SANeRF-HQ，GPS-Gaussian，两个GaussianAvatar，SpalTAM，gsplat） - Jason陪你练绝技的文章 - 知乎 Feature image from: Understanding the Covariance Matrix - Janakiev Projection Old Notes on 2023-12-05 NDC is the de-homogeneous clip coordinates and ranges [-1,1].\nDe-homogeneous means the z value has been divided.\nMapping the coordinates of point (x,y,z) in camera space to clip coordinates, i.e., a cube of [-1,1] can be decomposed to two operations: perspective projection and scaling ranges, and then compound them.\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nThe individual perspective projection:\n$$ \\begin{bmatrix} fₓ \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} $$\nAfter that, the plane coordinates are (u, v), wher $u = \\frac{fₓx}{z}, v=\\frac{f_yy}{z}$.\nThen scaling the ranges:\nScale the range of u from [-w/2,w/2] to [-1,1] through a linear mapping: α u + β\nα and β be solved based on two points.\n$$ \\begin{array}{cc} \\begin{cases} α (-w/2) + β = -1 \\\\ α w/2 + β = 1 \\end{cases} ⇒ \\begin{cases} α = 2/w \\\\ β = 0 \\end{cases} \\end{array} $$\nSimilarly, scale the range of v from [-h/2,h/2] to [-1,1] through a linear mapping: α v + β\nThus, $α= 2/h, β=0$\nSo far, the first 2 rows are determined:\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2fₓ/w \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2f_y/h \u0026amp; 0 \u0026amp; 0 \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nWhen scaling z, it has nothing to do with x and y. Thus, the 3rd row is 0 0 □ □.\nBeacuse NDC is the de-homogeneous clip coordinates, which requires divide by z to become NDC. Therefore, the 4-th row is 0 0 1 0.\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2fₓ/w \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2f_y/h \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; A \u0026amp; B \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nWith denoting the two unknowns as A and B, the NDC of z dimension is: $\\frac{A z + B}{z}$.\nAccording to the range constraint [-1,1], the A and B can be solved from:\n$$ \\begin{array}{cc} \\begin{cases} \\frac{A n + B }{n} = -1 \\\\ \\frac{A f + B }{f} = 1 \\end{cases} ⇒ \\begin{cases} A = (f+n)/(f-n) \\\\ B = -2fn/(f-n) \\end{cases} \\end{array} $$\nFinally, the mapping from the camera coordinates of a point to corresponding clip coordinates is:\n$$ \\begin{array}{ccc} \\begin{bmatrix}x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{bmatrix} = \\begin{bmatrix} 2fₓ/w \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2f_y/h \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} \\end{array} $$\nNDC Mapping (2024-01-01)\nThe perspective division (a 3D pixel coordinates are divided by the 3rd dimension) should be performed as the final step in the transformation pipeline, as it\u0026rsquo;s a non-linear operation.\nGiven a 3D point (x,y,z)ᵀ located in the camera space, the perspective projection and scaling are carried out in sequence to obtain its clip coordinates (not NDC yet).\n$$ \\begin{array}{c} \\text{[Scaling Matrix] [Perspective Projection] [Camera space] = [Clip space]} \\\\ \\\\ \\begin{bmatrix} □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\end{bmatrix} \\begin{bmatrix} fₓ \u0026amp; 0 \u0026amp; cₓ \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ w_c \\end{bmatrix} \\end{array} $$\nThe scaling matrix is built with the goal of mapping the view frustum to a [-1,1] NDC-space cube encompassing only valid points, whose clip coordinates satisfy: $-w_c \u0026lt; x_c,y_c,z_c \u0026lt; w_c$. Specifically, the projected coordinates are scaled and then perform perspective division to become the NDC.\nPerspective projection:\n$$ \\begin{bmatrix} fₓ \u0026amp; 0 \u0026amp; cₓ \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} fₓx + cₓ z \\\\ f_y y + c_y z \\\\ z \\\\ 1 \\end{bmatrix} $$\nScaling projected coordinates u and v to [-1, 1]\nTo scale u, the 1st row of the scaling matrix is A 0 B 0.\nPerform scaling first, followed by perspective division, to obtain the x in NDC space: $\\frac{A (fₓ x + cₓ z) + Bz}{z} = A (\\frac{fₓ x}{z} + cₓ) + B ∈ [-1,1]$\nSince $(\\frac{fₓ x}{z} + cₓ) = u ∈ [0,W]$\n$$ \\begin{cases} A0 + B = -1 \\\\ AW + B = 1 \\end{cases} ⇒ \\begin{cases} A = \\frac{2}{W} \\\\ B = -1 \\end{cases} $$\nTherefore, the first 2 rows are:\n$$ \\begin{bmatrix} 2/W \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 2/H \u0026amp; -1 \u0026amp; 0 \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\\\ □ \u0026amp; □ \u0026amp; □ \u0026amp; □ \\end{bmatrix} \\begin{bmatrix} fₓx + cₓ z \\\\ f_y y + c_y z \\\\ z \\\\ 1 \\end{bmatrix} $$\nScaling frustum $z ∈ [n, f]$ to [-1,1]\nz is independent to x and y, so the 3rd row only has 2 unknows: 0 0 A B.\nScaling first, then perspective division, thereby the z in NDC space is: $\\frac{A z + B}{z} ∈ [-1, 1]$\nSubstituting z = n and f:\n$$ \\begin{cases} \\frac{A n + B}{n} = -1 \\\\ \\frac{A f + B}{f} = 1 \\end{cases} ⇒ \\begin{cases} A = \\frac{f+n}{f-n} \\\\ B = \\frac{-2fn}{f-n} \\end{cases} $$\nFinally, since the denominators are z (i.e., the w of a point\u0026rsquo;s clip coordinates is z), the 4th row is 0 0 1 0:\n$$ \\begin{bmatrix} 2/W \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 2/H \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} fₓx + cₓ z \\\\ f_y y + c_y z \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2 (fₓx + cₓ z)}{W} - z \\\\ \\frac{2 (f_y y + c_y z) }{H} - z \\\\ \\frac{f+n}{f-n} z - \\frac{2fn}{f-n} \\\\ z \\end{bmatrix} $$\nThe result coordinates are in the clip space, which will become ND coordinates after perspective division, and only points within the cube of [-1,1] in NDC space will be rendered.\nIn summary, the Projection Matrix (GL_PROJECTION) transforming camera-space (x,y,z)ᵀ to clip coordinates is: $$ P = \\begin{bmatrix} \\frac{2}{W} \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2}{H} \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} fₓ \u0026amp; 0 \u0026amp; cₓ \u0026amp; 0 \\\\ 0 \u0026amp; f_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2fₓ}{W} \u0026amp; 0 \u0026amp; (\\frac{2cₓ}{W}) -1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2f_y}{H} \u0026amp; (\\frac{2c_y}{H}) -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $$\nWhen cx and cy are W/2 and H/2, it\u0026rsquo;s in the form of Ye\u0026rsquo;s article.\nWhen cx and cy are r and t, it\u0026rsquo;s in the form of songho.\n(2024-01-02)\nProject mean vector 𝛍 A mean vector 𝛍 in world space is changed to pixel space as follows:\nW C o o 𝛍 r o l r d d V t 𝐓 i r e a w n s f C C a o m o 𝐭 e r r d a P + e r S 𝐏 s c p a l p i r n o g j C C l o i o 𝐭 p r ' d C P ↑ l e d i ϕ r i p ₖ s v ( p i 𝐭 e s ' c i → ) t o i n v e N C ( D o C C o u r b d e S ) c a l e P C i o 𝛍 x o ' e r l d 𝐭 refers to coordinates of 𝛍 in the camera space as: $𝐭 = \\begin{bmatrix} 𝐑_{w2c} \u0026amp; 𝐭_{w2c} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} \\bm μ \\\\ 1 \\end{bmatrix}$\nWhile the translation vector is denoted as $𝐭_{w2c}$.\nAnd the extrinsics is represented as $𝐓_{w2c}$.\nThe clip coordinates of 𝛍 is $𝐭\u0026rsquo; = 𝐏𝐭$\nThe nonlinear perspective division is approximated by the projective transformation $ϕₖ(𝐭\u0026rsquo;)$.\nPoints\u0026rsquo; coordinates conversion from world space to clip space:\n$$ \\begin{aligned} 𝐭\u0026rsquo; \u0026amp;= 𝐏⋅ 𝐓_{w2c}⋅ [^{\\bm \\mu}_1] \\\\ \u0026amp;=\\begin{bmatrix} \\frac{2fₓ}{W} \u0026amp; 0 \u0026amp; (2cₓ/W) -1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2f_y}{H} \u0026amp; (2c_y/H) -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{f+n}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} R_{w2c} \u0026amp; t_{w2c} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} μₓ \\\\ μ_y \\\\ μ_z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} tₓ\u0026rsquo; \\\\ t_y\u0026rsquo; \\\\ t_z\u0026rsquo; \\\\ t_w\u0026rsquo; \\end{bmatrix} \\end{aligned} $$\n$t_w\u0026rsquo;$ is the point\u0026rsquo;s camera-space depth $t_z$, which is \u0026gt; 0. Frustum clipping (clip-space culling) filters points that won\u0026rsquo;t appear in the frustum based on clip coordinates before perspective division:\nc C - n c e a 1 e a n m ⋅ m t - a e s r p a r c f e a r i f c n r o u o s r 1 d s ➔ N C C D l a N i m D c p e o r c o c a o r o o d o c r : D r P o d i d r o i ( v : o r n x j d a ₙ b ( : t , y x + e _ ( o s y w c s x z u ₙ _ , c , ₙ t , c a y \u0026gt; y l , ❌ 1 o z _ e z f ₙ c , , , 1 f ) r 1 z u ) _ ∈ s c t ∈ , [ u 0 m [ w , 0 _ , c ∞ ) ] ∞ ] ∈ → [ [ 0 - ↑ ↑ , 1 , D C ∞ 1 i l ] ] v i p w ❗ View Frustum Clipping - UofTexas - Lec9\nView frustum clipping aims to reduce computation. Essentially, it filters points by comparing the clip coordinate $w_c$ with $x_c, y_c, z_c$ for each point.\nIn the previous derivation, clip coordinates are the scaled projection coordinates: (A*ProjCoord+B). NDC space is defined by $w$, as the NDC cube is constrained by bounds where the clip coordinates divided by 𝑤 equals 1 (𝑤 is the benchmark), such as $\\frac{A (f_x x+c_x z) + Bz}{w} = 1$\nIn other words, the NDC planes wrap around points whose clip coordinates $x_c,y_c,z_c$ less than or equal to $w_c$. In addition, $w_c$ must be the camera-space depth z for the final perspective division. Thus, if the clip coordinate of a point is bigger than w or less than $-w$, the \u0026ldquo;quotient\u0026rdquo; will be outside of [-1,1], i.e., the point is not located in the camera-space view frustum or the NDC-space cube.\nFrustum clipping retains points that satisfy: $-w_c \\leq x_c, y_c, z_c \\leq w_c$. Conversely, those points whose w (equals camera-space depth) is smaller than x,y,z will be filtered out.\nAlthough ND Coordinates are also able to identify points for clipping, to reduce the number of perspective divisions (executed at final), clipping is performed in the clip space for efficiency.\nOn the other hand, since the ND Coordinates of the Left, Right, Bottom, Top, Near, and Far frustum planes are -1 and 1, the clip coordinates of points located within the Frustum satisfy the relation: $-1 \u0026lt; \\frac{x_c}{w_c}, \\frac{y_c}{w_c}, \\frac{z_c}{w_c} \u0026lt;1$\nConsequently, only points in the frustum (i.e., NDC-space cube: $-1 \u0026lt; xₙ, yₙ, zₙ \u0026lt; 1$) are survived.\n(2024-01-03) It is the view frustum clipping that makes ND coordinates can be regarded as a cube space, because out-of-the-cube points have been disregared.\nPerforming perspective division on the clip coordinates resulting in NDC, whose all components ranges in [-1,1]:\n$$ NDC = \\begin{bmatrix} tₓ\u0026rsquo;/t_w\u0026rsquo; \\\\ t_y\u0026rsquo;/t_w\u0026rsquo; \\\\ t_z\u0026rsquo;/t_w\u0026rsquo; \\\\ 1 \\end{bmatrix} ∈ [-1,1] $$\n(2024-02-08) NDC is 3D, including zₙ coord besides the 2D pixel coords. Scaling NDC to obtain pixel coordinates 𝛍\u0026rsquo; (viewport transformation): songho\n$$ [-1, 1] \\overset{×W}{→} [-W, W] \\overset{+1}{→} [-W+1,W+1] \\overset{÷2}{→} [\\frac{-W+1}{2}, \\frac{W+1}{2}] \\\\ \\overset{+c_x}{→} [0.5, W+0.5] (\\text{ if $c_x =\\frac{W}{2}$}) $$\nTherefore, the final pixel coordinate 𝛍\u0026rsquo; of a world-space mean vector 𝛍 is: $$ \\bm \\mu\u0026rsquo; = \\begin{bmatrix} (W⋅tₓ\u0026rsquo;/t_w\u0026rsquo; + 1) /2 + c_x \\\\ (H⋅t_y\u0026rsquo;/t_w\u0026rsquo; + 1) / 2 + c_y \\end{bmatrix} $$\nProject Covariance (2024-01-03)\nBecause the perspective projection is not a linear operation due to division, the 2D projection of a 3D Gaussian is not a 2D Gaussian:\nsnowball splat diffuse Is it a 2D Gaussian? Conic sections EWA Splatting doesn\u0026rsquo;t scale the projected coordinates to the [-1,1] NDC-space cube. It directly transforms points from camera space onto screen (or viewing-ray space) by dividing z. The result coordinates range in [0,W] and [0,H].\nProjective transformation ϕ(𝐭) in EWA Splatting: Converting an arbitrary point\u0026rsquo;s camera-space coordinates $𝐭=(tₓ,t_y,t_z)ᵀ$ to the coordinates in 3D ray space (pixel coordinates x0,x1 + \u0026ldquo;new depth\u0026rdquo; x2) has 2 steps:\nPixel coords = perspective projection + perspective division. The new depth is set to the L2 norm of the point\u0026rsquo;s camera-space coordinates. $$ ϕ(𝐭) = \\begin{bmatrix} \\frac{fₓ tₓ}{t_z} + cₓ \\\\ \\frac{f_y t_y}{t_z} + c_y \\\\ \\sqrt{t_x^2 + t_y^2 + t_z^2} \\end{bmatrix} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} $$\nBecause EWA Splatting doesn\u0026rsquo;t consider frustum clipping, the approximation is based on the camera coordinates 𝐭.\n(2024-02-16) The clip coordinates shouldn\u0026rsquo;t be used in EWA splatting because the Gaussian center is used in the Jacobian that approximates the perspective projection, where the camera-space coordinates are supposed to be used.\nWhereas, 3DGS (or gsplat) requires to determine whether the point (Gaussian center) is in the frustrum to be rendered, the clip coordinates are utilized for frustum clipping. Therefore, the world coordinates 𝐱 are involved into 2 procedures: the covertion for covariance matrix from world space to ray space (JW𝚺WᵀJᵀ), and the projection for Gaussian center from world space onto the screen.\nConsequently, the derivative of Loss w.r.t. the world coordinates ($\\frac{∂L}{∂𝐱}$) has 2 portions.\nHowever, gsplat uses the clip space to filter points outside the camera frustum. So, after perspective projection and scaling for NDC with matrix 𝐏, points are transferred into clip space as 𝐭\u0026rsquo; for clipping.\nAfter clipping, the nonlinear perspective division and x₂ reassignment in ϕ(𝐭), are approximated with an affine transformation based on the clip coordinates 𝐭\u0026rsquo;. Therefore, the projective transformation $ϕ(𝐭)$ that maps camera space to ray space becomes a mapping from clip space to the ray space $ϕ(𝐭\u0026rsquo;)$.\n$$ \\begin{aligned} 𝐭 \u0026amp;→ 𝐭\u0026rsquo;=𝐏𝐭 = \\begin{bmatrix} \\frac{2 (fₓ t_x + cₓ t_z)}{W} - t_z \\\\ \\frac{2 (f_y t_y + c_y t_z) }{H} - t_z \\\\ \\frac{f+n}{f-n} z - \\frac{2fn}{f-n} \\\\ t_z \\end{bmatrix} = \\begin{bmatrix}𝐭_x\u0026rsquo; \\\\ t_y\u0026rsquo; \\\\ t_z\u0026rsquo; \\\\ t_z\\end{bmatrix} \\\\ \u0026amp;→ ϕ(𝐭\u0026rsquo;) = \\begin{bmatrix} tₓ\u0026rsquo;/t_z\u0026rsquo; \\\\ t_y\u0026rsquo;/t_z\u0026rsquo; \\\\ ‖𝐭\u0026rsquo;‖₂ \\end{bmatrix} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} \\end{aligned} $$\nThe affine approximation is the first 2 terms of $ϕ(𝐭\u0026rsquo;)$\u0026rsquo;s Taylor expansion evaluated at a Gaussian\u0026rsquo;s mean vector $𝐭ₖ\u0026rsquo; = (t_{k,x}\u0026rsquo;, t_{k,y}\u0026rsquo;, t_{k,z}\u0026rsquo;)ᵀ$ in the clip space:\n$$ \\begin{aligned} \u0026amp; ϕ(𝐭\u0026rsquo;) ≈ ϕₖ(𝐭\u0026rsquo;) = ϕ(𝐭ₖ\u0026rsquo;) + 𝐉_{𝐭ₖ\u0026rsquo;} ⋅ (𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;) \\\\ \u0026amp; = \\begin{bmatrix} t_{k,x}\u0026rsquo;/t_{k,z}\u0026rsquo; \\\\ t_{k,y}\u0026rsquo;/t_{k,z}\u0026rsquo; \\\\ ‖𝐭ₖ\u0026rsquo;‖²\\end{bmatrix} + \\begin{bmatrix} 1/t_{k,z}\u0026rsquo; \u0026amp; 0 \u0026amp; -t_{k,x}/{t_{k,z}\u0026rsquo;}^2 \\\\ 0 \u0026amp; 1/t_{k,z}\u0026rsquo; \u0026amp; -t_{k,y}/{t_{k,z}\u0026rsquo;}^2 \\\\ t_{k,x}\u0026rsquo;/‖𝐭ₖ\u0026rsquo;‖₂ \u0026amp; t_{k,y}\u0026rsquo;/‖𝐭ₖ\u0026rsquo;‖₂ \u0026amp; t_{k,z}\u0026rsquo;/‖𝐭ₖ\u0026rsquo;‖₂ \\end{bmatrix} (𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;) \\end{aligned} $$\nIf using the camera-space coordinates 𝐭 to express the projective transformation as ϕ(𝐭), focal lengths will be exposed:\n$$ \\begin{aligned} ϕ(𝐭) ≈ ϕₖ(𝐭) \u0026amp;= ϕ(𝐭ₖ)+ 𝐉_{𝐭ₖ} ⋅ (𝐭 - 𝐭ₖ) \\\\ \u0026amp;=\\begin{bmatrix} 2(fₓ t_{k,x}/t_{k,z} + cₓ)/W -1 \\\\ 2(f_y t_{k,y}/t_{k,z} + c_y)/H -1 \\\\ ‖𝐭ₖ‖₂ \\end{bmatrix} + 𝐉_{𝐭ₖ} ⋅ (𝐭 - 𝐭ₖ) \\end{aligned} $$\nIf $c_x=W/2,\\ c_y=H/2$, then $ϕₖ(𝐭) = \\begin{bmatrix} \\frac{2f_x t_{k,x}}{W⋅ t_{k,z}} \\\\ \\frac{2f_y t_{k,y}}{H⋅ t_{k,z}} \\\\ ‖𝐭ₖ‖₂ \\end{bmatrix}$\nand the Jacobian $𝐉_{𝐭ₖ}$ will be:\n$$ 𝐉_{𝐭ₖ} = \\begin{bmatrix} (2/W)⋅fₓ/t_{k,z} \u0026amp; 0 \u0026amp; (2/W)⋅-fₓ t_{k,x} / {t_{k,z}}^2 \\\\ 0 \u0026amp; (2/H)⋅f_y/t_{k,z} \u0026amp; (2/H)⋅-f_y t_{k,y} / {t_{k,z}}^2 \\\\ t_{k,x}/‖𝐭ₖ‖₂ \u0026amp; t_{k,y}/‖𝐭ₖ‖₂ \u0026amp; t_{k,z}/‖𝐭ₖ‖₂ \\end{bmatrix} $$\nIf the camera-film coords x∈[0,W] and y∈[0,H] are not scaled to [-1,1], the scaling factors 2/W and 2/H won\u0026rsquo;t exist.\nThus, the Jacobian evaluated at center 𝐭ₖ in the camera space is:\n$$ 𝐉_{𝐭ₖ} = \\begin{bmatrix} fₓ/t_{k,z} \u0026amp; 0 \u0026amp; -fₓ t_{k,x} / {t_{k,z}}^2 \\\\ 0 \u0026amp; f_y/t_{k,z} \u0026amp; -f_y t_{k,y} / {t_{k,z}}^2 \\\\ t_{k,x}/‖𝐭ₖ‖₂ \u0026amp; t_{k,y}/‖𝐭ₖ‖₂ \u0026amp; t_{k,z}/‖𝐭ₖ‖₂ \\end{bmatrix} $$\nBy expressing the projective transformation ϕₖ() with the camera coordinates 𝐭, the relation between ray-space coordinates and camera-space coordinates is constructed. Thereby, the derivative of Gaussian center ϕₖ(𝐭ₖ) in the ray space is derived from camera-space coordinates 𝐭ₖ, with the clip coordinates 𝐭ₖ\u0026rsquo; skipped.\nFor the case of 2D projection, only x and y dimensions of the covariance matrix need consideration, with the 3rd row and column are omitted. Because of the affine approximation, the projective transformation ϕ(𝐭\u0026rsquo;) becomes a linear operation. Thereby, a 3D Gaussian after perspective division is a 2D Gaussian.\nFiguratively, points surrounding the 3D Gaussian center in clip space will fall into an ellipse on the 2D screen. In the 3DGS code, the 2D ellipse is further simplified as a circle to count the overlapped tiles. The covariance matrix 𝚺ₖ\u0026rsquo; of the 2D Gaussian in the pixel space corresponding to the 3D Gaussian (𝚺ₖ) in the world space can be derived based on properties of Gaussians as:\n$$\\bm Σₖ\u0026rsquo; = 𝐉ₖ 𝐑_{w2c} \\bm Σₖ 𝐑_{w2c}ᵀ 𝐉ₖᵀ$$\nBecause covariance matrix 𝚺\u0026rsquo; is symmetric, it can be decomposed to a stretching vector (diagonal matrix) and a rotation matrix by SVD, analogous to describing the configurations of an ellipsoid ^3DGS. (Essentially, the covariance matrix depicts a data distribution.)\nThe rotation matrix is converted to a quaternion during optimization.\nIn summary: A 3D Gaussian (𝛍ₖ,𝚺ₖ) in world (object) space is transformed into 3D ray space (or the screen with the 3rd dim omitted), resulting in:\nMean vector: $\\bm μₖ\u0026rsquo; = ϕ(𝐭ₖ\u0026rsquo;)$, where $𝐭ₖ\u0026rsquo; = 𝐏⋅ [^{𝐑_{w2c} \\ 𝐭_{w2c}}_{0 \\quad\\ 1}] ⋅[^{\\bm μₖ}_1]$ is clip coordinates.\nCovariance matrix: $\\bm Σₖ\u0026rsquo; = 𝐉ₖ⋅ 𝐑_{w2c}⋅ \\bm Σₖ⋅ 𝐑_{w2c}ᵀ⋅ 𝐉ₖᵀ$\nA point 𝐭\u0026rsquo; in clip space within the Gaussian distribution is projected to ray space: $ϕₖ(𝐭\u0026rsquo;) = ϕ(𝐭ₖ\u0026rsquo;) + 𝐉ₖ⋅(𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;)$. The discrepancy between the approximated and the real projected locations is $ϕₖ(𝐭\u0026rsquo;)-ϕ(𝐭\u0026rsquo;)$.\nS c r e ⬯ 𝛍 e 𝐱 ' n , 𝚺 ' C l i 𝛍 p 𝐭 ᶜ ' , s 𝚺 p ᶜ a c e O s b p j a e c ⬯ 𝐗 𝛍 c e , t 𝚺 Rasterizing Sorting Kernels (2024-01-09)\nSort Gaussians within each 16x16 tile based on depth\nEvery pixel has a perpendicular dot line, and the discs intersected with the line are visible to the pixel.\nThe depths of those discs are $x₂ = ‖𝐭\u0026rsquo;‖₂ = \\sqrt{ {t₀\u0026rsquo;}² + {t₁\u0026rsquo;}² + {t₂\u0026rsquo;}²}$, L2 norm of the clip coordinates.\nThe disc closer to the screen is more prominent.\nDifferent images are obtained given different viewing rays, as the opacities of the discs change in tandem with viewing rays. (Specifically, the opacity of a disc is an integral for the Gaussian in the 3D ray space along a viewing ray.)\nIn contrast, volume rendering method changes point-wise colors on different viewing rays.\n(2024-01-22) In splatting, image formation still relies on alpha compositing. Distinct from NeRF where a pixel-ray originates from the camera optical center, in splatting, a pixel emits a perpendicular ray from the screen. And it is the incoming viewing rays determine varying alpha (opacity) of the filters on the pixel-ray path. Such that the screen displays diverse colors with various viewing rays.\n(2024-04-20)\nI think the \u0026ldquo;screen\u0026rdquo; is indeed the camera film. The \u0026ldquo;viewing ray\u0026rdquo; doesn\u0026rsquo;t hit the screen\u0026quot;. The viewing ray is just required to pass through 3D Gaussians to calculate each Gaussian\u0026rsquo;s opacity by integrating the 3D Gaussian over the intersecting segment.\nRendering a pixel in the splatting method also emitting a ray from a pixel and compositing discs on the ray. The difference is that the opacity has been precomputed by splatting procedure: integrating the viewing ray.\nThus, it is the screen, i.e., pixels that shoot lines.\nIn the implementation of 3DGS, the splatting process is omitted, since the opacity of each Gaussian is accquired by optimizing it iteratively.\nSplatting is just one of the ways to get the opacity. As long as the opacities are obtained, any rendering method can be applied to form an image, e.g., rasterization, ray marching/tracing.\nAlpha Blending (2024-01-05)\nEWA splatting equation for N kernels existing in the space:\n$$ \\underset{\\substack{↑\\\\ \\text{Pixel}\\\\ \\text{color}}}{C} = ∑_{k∈N} \\underset{\\substack{↑\\\\ \\text{Kernel}\\\\ \\text{weight}}}{wₖ}⋅ \\underset{\\substack{↑\\\\ \\text{Kernel} \\\\ \\text{color}}}{cₖ}⋅ \\underset{\\substack{↑\\\\ \\text{Accumulated} \\\\ \\text{transmittance}}}{oₖ}⋅ (\\underset{\\substack{↑\\\\ \\text{Kernel}\\\\ \\text{opacity}}}{qₖ}⊗ \\underset{\\substack{↑\\\\ \\text{Loss-pass}\\\\ \\text{filter}}}{h}) (\\underset{\\substack{↑\\\\ \\text{2D} \\\\ \\text{coords}}}{𝐱}) $$\nIn 3DGS, each 3D Gassian in the object space has 4 learnable parameters:\nColor (cₖ): SH for \u0026ldquo;directional appearance component of the radiance field\u0026rdquo;\nOpacity ($qₖ⊗ h$). It results in accu. transmittance oₖ as $∏_{m≤k}(1-\\text{opacity}ₘ)$.\nPosition 𝛍: determines the kernel\u0026rsquo;s weight wₖ, as wₖ is an evaluation of the projected kernel (2D Gaussian distribution) at the pixel.\nCovariance matrix: the stretching matrix and rotaion matrix (quaternion) jointly determine wₖ as well.\nThe splatting equation can be reformulated as alpha compositing used in 3DGS:\n$$C = ∑_{n≤N} Tₙ⋅αₙ⋅cₙ, \\text{ where } Tₙ = ∏_{m≤n}(1-αₘ)$$\nAlpha can be expressed with sigma, which is an exponent of e, akin to NeRF.\n$$αₙ = oₙ ⋅ exp(-σₙ), \\text{where } σₙ = ½\\bm Δₙᵀ \\bm Σ\u0026rsquo;⁻¹ \\bm Δₙ$$\noₙ is a kernel\u0026rsquo;s opacity, i.e., the above qₖ⊗ h. And the negative exponential term is a scalar scaling factor. σₙ is Mahalanobis distance.\n(2024-02-16)\nGaussian\u0026rsquo;s opacity oₙ is fixed after splatting with a specific viewing ray, so during alpha compositing, the variation in alpha among different Gaussian results from the different positions of a target rendering pixel relative to various Gaussians.\nWhen performing alpha compositing, the alpha of a Gaussian is the Gaussian\u0026rsquo;s opacity scaled by the \u0026ldquo;probability\u0026rdquo; for the position of the rendering pixel in the Gaussian distribution.\nHowever, the alpha value in NeRF is $αᵢ = 1- exp(-σᵢδᵢ)$ and serves as the opacity in $∑ᵢ₌₁ᴺ Tᵢ αᵢ cᵢ$. Alpha is a converted point opacity ranging in [0,1].\nIn other words, alpha $αₙ$ equals a kernel\u0026rsquo;s opacity oₙ scaled by a weight Gₙ (the above wₖ), i.e., the evaluation of 2D Gaussian Gₙ at the viewing pixel:\n$$αₙ = oₙ ⋅ Gₙ, \\text{where } Gₙ = e^{-\\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σ\u0026rsquo;}}$$\noₙ is the opacity (the footprint qₙ) of the n-th 3D Gaussian. qₙ is an integral of the Gaussian in the 3D ray space along the viewing ray: $qₙ(𝐱) = ∫rₙ\u0026rsquo;(𝐱,x₂)dx₂$. oₙ will get optimized directly via gradient descent during training. Gₙ is a 2D Gaussian with the normalization factor omitted. Its mean and covariance matrix will get optimized. Δₙ is the displacement of a pixel center from the 2D Gaussian\u0026rsquo;s mean. Initially, the opacity of an arbitrary 3D location in the object space is considered as an expectation of the contributions from all 3D Gaussians on that location.\nAfter ❶ substituting the perspectively projected kernel within the 3D ray space into the rendering equation, ❷ switching the sequence of integral and expectation, ❸ and applying simplifying assumptions, the opacity becomes a 1-D (depth) integral along the viewing ray for each kernel in the 3D ray space.\nIn summary, the changes of opacity before and after perspective projection:\nAspect Original form Post-projection Venue Object space 3D ray space, or screen Intuition Opacity combination Discs stack Scope Location-wise Ellipsoid-wise Operation Expectation Integral Formula $f_c(𝐮) = ∑_{k≤N} wₖ rₖ(𝐮)$ Footprint $qₖ(𝐱) = ∫_{x₂=0}^L rₖ\u0026rsquo;(𝐱,x₂) dx₂$ Basis Gauss. mixture Scene locality Locality: 3D positions are grouped into different ellipsoids. (2024-01-06) A 3D Gaussian (datapoint) in the camera space (or clip space) is \u0026ldquo;perspectively\u0026rdquo; projected (thrown) onto the screen (or the ray space, as its x2 is independtly assigned beside the screen coordinates x0,x1), and results in a 2D Gaussian (with applying the Taylor expansion to approximate the nonlinear perspective effects).\nThe viewing ray in camera space will be projected into the 3D ray space remaining a straight line segment (due to the linear approximation), and then the 3D line is projected onto the screen orthogonally.\nOrthogonal projection is because the 3D Gaussians have already been projected onto the screen (the location has been determined as x,y divided by z and covariance matrix 𝚺\u0026rsquo;= 𝐉𝐖 𝚺𝐖ᵀ 𝐉ᵀ) yielding 2D Gaussians, so each pixel is only derived from those projected kernels (2D Gaussians) that overlaps with it (\u0026ldquo;Overlapping\u0026rdquo; refer to 3DGS.), like a stack of filters in alpha compositing. That implies the alpha compositing is performed in the screen space, or the ray sapce, as the ray space is equivalent with screen (EWA paper: \u0026ldquo;the transformation from volume-data to ray space is equivalent to perspective projection.\u0026rdquo;).\nA p i ■ x e → l = = d k S i e u s ■ ↑ ⬭ n m c l 1 1 o f ✚ 2 d k D i e s ■ ↑ ⬯ n G c l a 2 2 u s s ✚ ⋯ i a n d k s i e s ■ ↑ ⬭ n O c l v N N e r l a o p ( m ( p 2 i 3 i D t D n ↑ g S \" R c d a w r e y i e p t e t s h n h p ) \" a i c t x e ₂ ) With the \u0026ldquo;orthogonal correspondence\u0026rdquo; between the ray space and the screen, the ray integral (footprint function, or kernel\u0026rsquo;s opacity) in the 3D ray space becomes (??Not sure) an integral on the 2D screen plane, i.e., an integral of a 2D Gaussian.\nAnd the ray in the 3D ray space corresponds to a line on the screen, as rays in the 3D ray space are parallel (i.e., orthogonal projection). Thus, the opacity is an\nAlpha of an arbitrary point in screen (or 3D ray space) is a 2D-Gaussian mixture over all kernels. (Not the screen, object space is opacities combination, whereas the screen space is filter stacking.)\nThe alpha of a 3D location is calculated in each 3D Gaussian based on the distance to the center. And the final alpha on the location is the expectation of all the evaluations. (No location-wise alpha was calculated.)\nThe color of a pixel on the screen is a 2D-Gaussian mixture:\nG a i 2 u a D s n s s S - c r e e n ∫ D V e i V p d e o t i w l h f i . \" u f n ~ m h e g e e r r e r d e n a a t y t i a s v 3 e D h ‖ i • a 𝐭 w ( s ' i V ‖ n i b ₂ g e e w e f r i n o a n r y g ~ t s ) . \" h c . r a R o l C a w c o y n u v l s o a m p . n t a a \" • \" t i t c . o n r e g i t x h o e p i P p a s e r s c r o c i 𝚺 s j r t ' p e y ₃ e e ₓ c n g ₃ C S . i . a p ⬭ v m a e e c n r e a ⬯ The alpha compositing process for a pixel is illustrated below:\nOpacities (o₁,o₂,o₃) of different kernels are various-length integral along the viewing ray in the 3D ray space.\nNot sure whether the integral in 3D ray space equal the integral on screen. Weight (w₁,w₂,w₃) of a kernel\u0026rsquo;s opacity is its evaluation at the pixel.\nAlpha (α₁,α₂,α₃) of a kernel equals its opacity multiplied with its weight.\nAccumulated transmittance (T₁,T₂,T₃) equals the product of previously passed kernels\u0026rsquo; transmittance.\nPixel color is the sum of each kernel\u0026rsquo;s color scaled by alpha.\nThere is no volume rendering as there is no sampling points on the viewing ray. Pixel is a summation of visible 2D discs (referring to 3DGS). Only alphas of the explicitly existent discs require to be computed, unlike volume rendering where every sampling location need to compute alpha.\nGradient wrt Composite (2023-01-07)\nS P k 3 p r e D l o r a j n r t e e a t ⋯ c l y i t s n ⋯ e s g ⋯ d i p : ⋯ ⋯ n a ⋯ c U ⋯ ⋯ e s c ⋯ ⋯ e o ⋯ m ⋯ ⋯ ⋯ V p ⋯ i u ⋯ ⋯ e t w e i n o g p a r c a i y t i t e o s F B d 𝛍 o a i ' c r c s ■ ₁ ₁ w k c , , a 1 𝚺 o r ' ₁ d ✚ ₁ : d r i 𝛍 e s ■ ' c n F c ₂ ₂ d r 2 , , e o 𝚺 o r n ' ₂ t ✚ ⋯ ₂ p i ( x f d e i i 𝛍 l r s ■ ' c s c ₙ ₙ c t N , , o 𝚺 o l b = ' ₙ o e p ₙ r i ■ s x e e n ) The dot lines in the left figure represent orthogonal correspondence, not projection. A pixel can only see 2D Gaussians located on its perpendicular line. Those visible 2D Gaussians to a pixel are sorted based on depth, and then their colors are composited from near to far with multiplying with their opacities that computed as an integral along the viewing ray. A synthesized pixel is a weighted sum of the related (overlapping) kernels\u0026rsquo; color in the whole space:\n$$C_{pred} = ∑_{n≤N} Tₙ⋅αₙ⋅cₙ, \\quad \\text{where } αₙ=oₙ⋅e^{-\\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σ\u0026rsquo;}}$$\nLoss:\n$$L = ‖C_{targ} - C_{pred}‖₂$$\nA Frobenius inner product is like a linear layer:\n$$⟨𝐗,𝐘⟩ = \\operatorname{vec}(𝐗)ᵀ \\operatorname{vec}(𝐘)$$\n1 2 3 fc = nn.Linear(3, 10) # fc.weight is (10,3) x = torch.range(2,3) fc(x) # X⋅Wᵀ = (2,3)⋅(3,10) = (2,10) is ⟨Xᵀ,Wᵀ⟩ Chain rule:\nx A ₘ ₓ ₚ ( x ) Y ₚ ₓ ₙ ( x ) X ₘ ₓ ₙ f $$\\begin{aligned} \\frac{∂f}{∂x} \u0026amp;= \\frac{∂f}{∂X}⋅\\frac{∂X}{∂AY}⋅\\frac{∂AY}{∂x} \\\\ \u0026amp;= \\frac{∂f}{∂X}⋅\\frac{∂X}{∂AY}⋅(\\frac{∂A}{∂x}Y + A\\frac{∂Y}{∂x})\\\\ \\end{aligned}$$\nSince the passed kernels on the ray path (starting from a pixel) have influences on the next kernel\u0026rsquo;s contribution, which is scaled by the previous accumulated transmittance, the order of solving derivatives should start from the most rear kernel, and then sequentially calculate the derivatives of front kernels in the reverse order of the forward pass.\nS G p r s l F B a o a o a d l t r N c ⬮ v t w k - f e i a - i d n r ⬮ r g d - s : : - t ⬮ V P - i i - e x ⬮ w e - T i i l - h n n ⬮ e g r F - t e r - t h r n 1 o ⬮ ↑ o e a d n p y e t = p c r e o ↘ i ■ s l n t o h g r i c t ← o s s l t o a s r c c k r e e n B ⬮ N D a a - e l c ✚ - r l k ⬮ i w - v d a ✚ - a o r ⬮ t w d - i n : ✚ - n v - ⬮ + e s s - 1 t u - c r m ⬮ n o e - m m g - e r ⬮ s c a o d = f l i r o e ■ o r n m s t s The viewing ray travels from back to front and hits the screen. But the color nearest to the screen is the first to be seen by (or shown on) the camera (or eye).\nThe toppest color is based on downstream colors, so, its a function of all the preceding colors.\nIn the color stack, the color above depends on color below. Thus, the derivatives at the bottom should be sovled first.\n(2024-01-16) The toppest color is the base of all downstream colors, so, its derivative is contributed by all the behind colors.\nColor, Opacity (2024-01-08)\nGiven $\\frac{∂L}{∂Cᵢ(k)}$, the partial derivatives of the predicting pixel color $Cᵢ$ w.r.t. each parameter of a Gaussian Gₙ (in the ray space) that contributes to the pixel are:\nThe parital derivative of Cᵢ w.r.t. the kernel Gₙ\u0026rsquo;s color cₙ, based on the forward pass: $Cᵢ = T₁⋅α₁⋅c₁+ T₂⋅α₂⋅c₂ + ⋯ Tₙ⋅αₙ⋅cₙ+ Tₙ₊₁⋅αₙ₊₁⋅cₙ₊₁ +⋯ + T_N⋅α_N⋅c_N$:\n$$\\frac{∂Cᵢ(k)}{∂cₙ(k)} = αₙ⋅Tₙ$$\nk represents one channel of RGB.\nThe furthest $T_N$ from the screen is saved at the end of the forward pass. And then the $T_{N-1}$ in front of it is calculated as $T_{N-1} = \\frac{T_N}{1-α_{N-1}}$. The points in front follow this relation.\n3DGS Code\n1 2 T = T / (1.f - alpha); const float dchannel_dcolor = alpha * T; Alpha αₙ\nTo solve the partial derivative of pixel color $C_i$ w.r.t. the kernel Gₙ\u0026rsquo;s αₙ, only consider the kernels that follow Gₙ, as the transmittances of all the subsequent kernels rely on the currenct kernel Gₙ: $$Tₙ₊₁ = (1-αₙ)Tₙ$$\nThereby, the behind kernels will provide derivatives to the current kernel Gₙ\u0026rsquo;s alpha.\nFor example, the color of the next kernel, Gₙ₊₁, behind Gₙ is:\n$$\\begin{aligned} Cₙ₊₁ \u0026amp;= cₙ₊₁⋅αₙ₊₁⋅Tₙ₊₁ \\\\ \u0026amp;= cₙ₊₁⋅αₙ₊₁⋅ (1-αₙ)Tₙ \\\\ \u0026amp;= cₙ₊₁⋅αₙ₊₁⋅(1-αₙ)⋅ \\frac{ Tₙ₊₁}{1-αₙ} \\\\ \\end{aligned}$$\nThus, the color $Cₙ₊₁$ contributes to the total partial derivative $\\frac{∂Cᵢ}{∂αₙ}$ with the amount: $-\\frac{cₙ₊₁⋅αₙ₊₁⋅Tₙ₊₁}{1-αₙ}$ .\nContinuously, the following color Cₙ₊₂ can be represented with αₙ:\n$$ Cₙ₊₂ = cₙ₊₂⋅αₙ₊₂⋅Tₙ₊₂ \\\\ = cₙ₊₂⋅αₙ₊₂ ⋅\\cancel{(1-αₙ₊₁)} (1-αₙ) \\frac{Tₙ₊₂}{ \\cancel{(1-αₙ₊₁)} (1-αₙ)}$$\nThus, $\\frac{∂Cₙ₊₂}{∂αₙ} = -\\frac{cₙ₊₂⋅αₙ₊₂⋅Tₙ₊₂}{1-αₙ}$ Similarly, the subsequent kernel Gₘ, with m\u0026gt;n, also contribute to the overall partial derivative $\\frac{∂Cᵢ}{∂αₙ}$.\nThereby, the ultimate partial derivatives of the pixel color $Cᵢ$ w.r.t. the Gₙ\u0026rsquo;s alpha αₙ is:\n$$\\frac{∂Cᵢ}{∂αₙ} = cₙ⋅Tₙ - \\frac{∑_{m\u0026gt;n}cₘ⋅αₘ⋅ Tₘ}{1-αₙ}$$\nOpacity oₙ, mean 𝛍\u0026rsquo;, and covariance 𝚺':\nAccording to $αₙ = oₙ e^{-σₙ}$, where $σₙ = \\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σₙ\u0026rsquo;}$ (a scalar), and 𝚫ₙ is the offset from the pixel center to the 2D Gaussian Gₙ\u0026rsquo;s mean $\\bm μ\u0026rsquo;$, such as $\\bm Δₙ = \\bm μₙ\u0026rsquo; - 𝐱ᵢ$.\nPartial derivative of αₙ w.r.t. opacity oₙ:\n$$ \\frac{∂αₙ}{∂oₙ} = e^{-\\frac{\\bm Δₙᵀ \\bm Δₙ}{2\\bm Σₙ\u0026rsquo;}} $$\nPartial derivative of αₙ w.r.t. the \u0026ldquo;exponent\u0026rdquo; sigma $σₙ$:\n$$ \\frac{∂αₙ}{∂σₙ} = -oₙ e^{-σₙ}$$\nPartial derivative of sigma σₙ w.r.t. 2D mean 𝛍ₙ':\nBecause $\\bm Δₙ$ is a function of 𝛍ₙ\u0026rsquo;, computing derivaties w.r.t. 𝛍ₙ\u0026rsquo; is equivalent to $\\bm Δₙ$.\nThe Jacobian of σₙ is:\n$$\\frac{∂σₙ}{∂\\bm μₙ\u0026rsquo;} = \\frac{∂σₙ}{∂\\bm Δₙ\u0026rsquo;} = \\frac{∂(½\\bm Δₙ\u0026rsquo;ᵀ \\bm Σ\u0026rsquo;⁻¹ \\bm Δₙ\u0026rsquo;)}{∂\\bm Δₙ\u0026rsquo;} = \\bm Σₙ\u0026rsquo;⁻¹ \\bm Δₙ\u0026rsquo;$$\nPartial derivative of sigma σₙ w.r.t. 2D covariance matrix 𝚺':\n$$\\frac{∂σₙ}{∂\\bm Σₙ\u0026rsquo;} =$$\nGradient wrt Projection (2024-01-10)\nS 2 c D r e ⬯ e n 𝐱 ⋯ O 𝛁 | ⋯ m 𝓛 i R ( ⋯ t b a ϕ 𝛍 a y ₖ ₖ ⋯ d c 3 ( ' k D s 𝐭 , ⬮ p p ' 𝚺 r a ) ₖ o c ' ϕ + ( p e ) ₖ D a ( 𝐉 i g 𝐭 ⋅ v a ' ( t ) 𝐭 a e | = ' p ϕ p C ( r l ( 𝛍 𝛍 o i 𝛍 ₖ ₖ x p ₖ ᶜ ᶜ . 𝐭 ᶜ ) ) ) S ' , ⬮ p 𝚺 a ₖ c ᶜ e ) | P C a m e ( r 𝐭 a ₖ 𝐭 , ⬮ s 𝚺 p ₖ a ) c ⌈ ⌊ e 𝐑 0 ʷ | ² ᶜ O ' b 𝐭 1 j ʷ e ² c ( ᶜ t 𝛍 ⌉ ⌋ 𝐗 ₖ s , p 𝚺 ⬮ a ₖ c ) e | A 3D Gaussian distribution centered at 𝛍ₖ with a covariance matrix 𝚺ₖ in the object space will be projected onto 2D screen through the splatting step, resulting in a 2D Gaussian distribution, with applying the affine transformation approximation for the nonlinear perspective division.\nThe 3D ray space (or the screen) is constructed based on the perspective division (x,y divided by z), which however is non-linear. Therefore, the projective transformation $ϕ(𝐭\u0026rsquo;)$ (i.e., perspective division + new depth) converting the clip space to the 3D ray space is approximated by a linear mapping: $ϕₖ(𝐭\u0026rsquo;) = ϕ(𝐭ₖ\u0026rsquo;) + 𝐉ₖ⋅(𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;)$, where 𝐭ₖ\u0026rsquo; = 𝛍ₖᶜ, the mean vector in clip space.\nThe effects of the approximated affine mapping ϕₖ(𝐭\u0026rsquo;) are as follows:\nThe transformed 2D Gaussian\u0026rsquo;s center 𝛍ₖ\u0026rsquo; is the exact projective transformation ϕ(𝛍ₖᶜ), i.e., ϕₖ(𝐭ₖ\u0026rsquo;)=ϕ(𝐭ₖ\u0026rsquo;), without any error, with the 3rd dimension omitted.\n$$ \\underset{(3D)}{\\bm μₖ\u0026rsquo;} = ϕ(\\bm μₖᶜ) = \\begin{bmatrix} f_x⋅μ_{k,x}ᶜ/μ_{k,z}ᶜ + c_x \\\\ f_y⋅μ_{k,y}ᶜ/μ_{k,z}ᶜ + c_y \\\\ \\sqrt{ {μ_{k,x}ᶜ}^2 + {μ_{k,y}ᶜ}^2 + {μ_{k,z}ᶜ}^2} \\end{bmatrix} \\overset{\\text{Omit 3rd dim}}{\\longrightarrow} \\underset{(2D)}{\\bm μₖ\u0026rsquo;}= \\begin{bmatrix} \\frac{f_x⋅μ_{k,x}ᶜ}{μ_{k,z}ᶜ} + c_x \\\\ \\frac{f_y⋅μ_{k,y}ᶜ}{μ_{k,z}ᶜ} + c_y \\end{bmatrix} $$\nHowever, the approximated transformation ϕₖ(𝐭\u0026rsquo;) of an arbitrary point 𝐭\u0026rsquo; around the clip-space 3D Gaussian center 𝐭ₖ' will deviate from the precise perspective projections ϕ(𝐭\u0026rsquo;) gradually, as the (𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;) increases in the approximated mapping:\n$$ \\begin{aligned} ϕ(𝐭\u0026rsquo;) ≈ ϕₖ(𝐭\u0026rsquo;) \u0026amp;= ϕ(𝐭ₖ\u0026rsquo;) + 𝐉ₖ⋅(𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;) \\\\ \u0026amp;= \\bm μₖ\u0026rsquo; + 𝐉ₖ⋅(𝐭\u0026rsquo; - 𝐭ₖ\u0026rsquo;) \\end{aligned} $$\nThe projected 2x2 covariance matrix on screen is the 3x3 matrix in the ray space: $\\bm Σₖ\u0026rsquo; = 𝐉ₖ⋅ 𝐑_{w2c}⋅ \\bm Σₖ⋅ 𝐑_{w2c}ᵀ⋅ 𝐉ₖᵀ$, with the 3rd row and column omitted.\n(2024-01-11)\n⭐Note: The following $𝐭ₖ$ is the coordinates of a Gaussian center in the camera space: $$𝐭ₖ = [^{𝐑_{w2c} \\ 𝐭_{w2c}}_{0 \\quad\\ 1}] ⋅[^{\\bm μₖ}_1]$$\nwhere 𝛍ₖ³ᕽ¹ is the coordinates of the mean vector in world space.\n$𝐭\u0026rsquo;$ is the clip coordinates, which is camera coordinates times the projection matrix 𝐏: $𝐭\u0026rsquo; = 𝐏𝐭$\n𝐏 maps the camera-space coordinates to camera film (homogeneous) and scales for letting the ND Coordinates of points located within the camera frustum range in [-1,1]. With using clip coordinates, points whose w (i.e., z) is smaller than x,y,z are deleted.\nThe approximated projective transformation $ϕₖ(𝐭\u0026rsquo;)$ fulfills perspective division after frustum clipping. Therefore, the 2D screen coordinates $\\bm μₖ\u0026rsquo;_{(2D)} = ϕₖ(𝐭ₖ\u0026rsquo;)_{(2D)}$ are NDC ∈ [-1,1].\nThen the Gaussian center\u0026rsquo;s ND coordinates are scaled back to the screen size, yielding pixel coordinates 𝛍ₖ\u0026rsquo;₍ₚᵢₓ₎ represented with clip coordinates as:\n$$ \\bm μₖ\u0026rsquo;_{(\\text{pix})} = \\begin{bmatrix} (W⋅t_{k,x}\u0026rsquo;/t_{k,w}\u0026rsquo; + 1) /2 + c_x \\\\ (H⋅t_{k,y}\u0026rsquo;/t_{k,w}\u0026rsquo; + 1) / 2 + c_y \\end{bmatrix} $$\nThis relationship enables propagating gradients from pixel coordinates $\\bm μₖ\u0026rsquo;₍ₚᵢₓ₎$ to the clip coordinates $𝐭ₖ\u0026rsquo;$ directly, without the 2D screen coordinates $\\bm μₖ\u0026rsquo;_{(2D)}$ involved.\nCenter (2024-01-12)\nBecause 𝛍ₖ\u0026rsquo;₍ₚᵢₓ₎ and 𝚺ₖ\u0026rsquo; on the 2D screen both are functions of 3D Gaussian center 𝐭ₖ, the partial derivatives of the loss w.r.t. 𝐭ₖ³ᕽ¹ is a sum:\n$$ \\frac{∂L}{∂𝐭ₖ} = \\frac{∂L}{∂ \\bm μₖ\u0026rsquo;₍ₚᵢₓ₎} \\frac{∂\\bm μₖ\u0026rsquo;₍ₚᵢₓ₎}{∂𝐭ₖ} + \\frac{∂L}{∂\\bm Σₖ\u0026rsquo;_{(2D)}} \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂𝐭ₖ} \\\\ $$\nThe partial derivative of 2D Gaussian\u0026rsquo;s mean $\\bm μₖ\u0026rsquo;₍ₚᵢₓ₎$ w.r.t. the camera coordinates of 3D Gaussian center 𝐭ₖ:\n$$ \\begin{aligned} \\frac{∂\\bm μₖ\u0026rsquo;₍ₚᵢₓ₎}{∂𝐭ₖ} \u0026amp;= \\frac{∂\\bm μₖ\u0026rsquo;₍ₚᵢₓ₎}{∂\\bm μₖ\u0026rsquo;_{(2D)}} ⋅ \\frac{∂ϕₖ(𝐭ₖ\u0026rsquo;)_{(2D)}}{𝐭ₖ\u0026rsquo;} ⋅ \\frac{∂𝐭ₖ\u0026rsquo;}{∂𝐭ₖ} \\qquad \\text{(Full process)} \\\\ \u0026amp;= \\frac{∂\\bm μₖ\u0026rsquo;₍ₚᵢₓ₎}{∂𝐭ₖ\u0026rsquo;} ⋅ \\frac{∂𝐭ₖ\u0026rsquo;}{∂𝐭ₖ} \\qquad \\text{(Clip→Pix, skip screen coords)} \\\\ \u0026amp;= \\frac{1}{2} \\begin{bmatrix} W/t_{k,w}\u0026rsquo; \u0026amp; 0 \u0026amp; 0 \u0026amp; -W⋅t_{k,x}\u0026rsquo;/{t_{k,w}\u0026rsquo;}^2 \\\\ 0 \u0026amp; H/t_{k,w}\u0026rsquo; \u0026amp; 0 \u0026amp; -H⋅t_{k,y}\u0026rsquo;/{t_{k,w}\u0026rsquo;}^2 \\end{bmatrix}⋅ 𝐏 \\end{aligned} $$\nBased on the properties of the Frobenius inner product, eq. (23) is obtained.\nThe partial derivative of the 2D Gaussian\u0026rsquo;s covariance 𝚺ₖ\u0026rsquo; w.r.t. the camera coordinates of 3D Gaussian center 𝐭ₖ:\n$$ \\frac{∂\\bm Σₖ\u0026rsquo;}{∂𝐭ₖ} = \\frac{∂(𝐉ₖ⋅ 𝐑_{w2c}⋅ \\bm Σₖ⋅ 𝐑_{w2c}ᵀ⋅ 𝐉ₖᵀ)_{2D} }{∂𝐭ₖ} $$\n(2024-01-13) Derivation refers to 3D Gaussian Splatting中的数学推导 - 八氨合氯化钙的文章 - 知乎\nLetting $𝐔 = 𝐉ₖ⋅ 𝐑_{w2c}$ (3DGS code refers to it as T.), the Gaussian covariance 𝚺ₖ\u0026rsquo; in the 3D ray space derived from the projective transformation ϕₖ(𝐭\u0026rsquo;) is:\n$$\\bm Σₖ\u0026rsquo; = 𝐔 ⋅ \\bm Σₖ⋅𝐔ᵀ = \\\\ \\begin{bmatrix} U₁₁ \u0026amp; U₁₂ \u0026amp; U₁₃ \\\\ U₂₁ \u0026amp; U₂₂ \u0026amp; U₂₃ \\\\ U₃₁ \u0026amp; U₃₂ \u0026amp; U₃₃ \\end{bmatrix} \\begin{bmatrix} σ₁₁ \u0026amp; σ₁₂ \u0026amp; σ₁₃ \\\\ σ₂₁ \u0026amp; σ₂₂ \u0026amp; σ₂₃ \\\\ σ₃₁ \u0026amp; σ₃₂ \u0026amp; σ₃₃ \\end{bmatrix} \\begin{bmatrix} U₁₁ \u0026amp; U₂₁ \u0026amp; U₃₁ \\\\ U₁₂ \u0026amp; U₂₂ \u0026amp; U₃₂ \\\\ U₁₃ \u0026amp; U₂₃ \u0026amp; U₃₃ \\end{bmatrix} = \\\\ \\begin{bmatrix} U₁₁ \u0026amp; U₁₂ \u0026amp; U₁₃ \\\\ U₂₁ \u0026amp; U₂₂ \u0026amp; U₂₃ \\\\ U₃₁ \u0026amp; U₃₂ \u0026amp; U₃₃ \\end{bmatrix} \\begin{bmatrix} \\boxed{σ₁₁}U₁₁+σ₁₂U₁₂+σ₁₃U₁₃ \u0026amp; \\boxed{σ₁₁}U₂₁+σ₁₂U₂₂+σ₁₃U₂₃ \u0026amp; \\boxed{σ₁₁}U₃₁+σ₁₂U₃₂+σ₁₃U₃₃ \\\\ σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃ \u0026amp; σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃ \u0026amp; σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃ \\\\ σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃ \u0026amp; σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃ \u0026amp; σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃ \\end{bmatrix} = \\\\ \\Big[ \\begin{array}{c|c|c} U₁₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₁₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₁₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026amp; U₁₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₁₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₁₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \u0026amp; U₁₁(σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃) + U₁₂(σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃) + U₁₃(σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃) \\\\ U₂₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₂₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₂₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026amp; U₂₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₂₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₂₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \u0026amp; U₂₁(σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃) + U₂₂(σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃) + U₂₃(σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃) \\\\ U₃₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₃₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₃₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026amp; U₃₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₃₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₃₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \u0026amp; U₃₁(σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃) + U₃₂(σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃) + U₃₃(σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃) \\end{array} \\Big] $$\nThe 3rd row and column in 𝚺ₖ\u0026rsquo; are omitted due to the orthogonal correspondence between the 3D ray space and the 2D screen. Thus, $\\bm Σₖ\u0026rsquo;_{(2D)}$ is only the upper-left 2×2 elements of the 3D 𝚺ₖ\u0026rsquo;, contributing to the gradient of 2D loss $L(𝐜ₖ, oₖ, \\bm μₖ\u0026rsquo;_{(2D)}, \\bm Σₖ\u0026rsquo;_{(2D)})$, while the remaining 5 elements of 𝚺ₖ\u0026rsquo;₍₃ₓ₃₎ make no contributions.\n$$ \\bm Σₖ\u0026rsquo;_{(2D)} = \\\\ \\Big[ \\begin{array}{c|c} U₁₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₁₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₁₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026amp; U₁₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₁₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₁₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \\\\ U₂₁(σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃) + U₂₂(σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃) + U₂₃(σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃) \u0026amp; U₂₁(σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃) + U₂₂(σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃) + U₂₃(σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃) \\end{array} \\Big] $$\nEach element of $\\bm Σₖ\u0026rsquo;_{(2D)}$ is a \u0026ldquo;sub-\u0026rdquo; function, which is taken derivative w.r.t. each variable: σ₁₁, σ₁₂, σ₁₃, σ₂₂, σ₂₃, σ₃₃, to backpropagate the gradient $\\frac{∂L}{∂\\bm Σₖ\u0026rsquo;_{(2D)}}$ to 𝚺ₖ. (Only these 6 elements of 𝚺ₖ need computation as 𝚺ₖ₍₃ₓ₃₎ is symmetric.)\nIt\u0026rsquo;s not proper to think of the derivative of a \u0026ldquo;matrix\u0026rdquo; w.r.t. a matrix. Instead, it\u0026rsquo;s better to consider the derivative of a function w.r.t. variables, as essentially a matrix stands for a linear transformation.\nThe partial derivative of $\\bm Σₖ\u0026rsquo;_{(2D)}$ w.r.t. $\\bm Σₖ$ (the 3D covariance matrix in world space):\n$$ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₁₁} = \\begin{bmatrix} U₁₁U₁₁ \u0026amp; U₁₁U₂₁ \\\\ U₂₁U₁₁ \u0026amp; U₂₁U₂₁ \\end{bmatrix} = \\begin{bmatrix} U₁₁ \\\\ U₂₁ \\end{bmatrix} \\begin{bmatrix} U₁₁ \u0026amp; U₂₁ \\end{bmatrix} \\\\ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₁₂} = \\begin{bmatrix} U₁₁U₁₂ \u0026amp; U₁₁U₂₂ \\\\ U₂₁U₁₂ \u0026amp; U₂₁U₂₂ \\end{bmatrix} = \\begin{bmatrix} U₁₁ \\\\ U₂₁ \\end{bmatrix} \\begin{bmatrix} U₁₂ \u0026amp; U₂₂ \\end{bmatrix} \\\\ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₁₃} = \\begin{bmatrix} U₁₁U₁₃ \u0026amp; U₁₁U₂₃ \\\\ U₂₁U₁₃ \u0026amp; U₂₁U₂₃ \\end{bmatrix} = \\begin{bmatrix} U₁₁ \\\\ U₂₁ \\end{bmatrix} \\begin{bmatrix} U₁₃ \u0026amp; U₂₃ \\end{bmatrix} \\\\ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₂₂} = \\begin{bmatrix} U₁₂U₁₂ \u0026amp; U₁₂U₂₂ \\\\ U₂₂U₁₂ \u0026amp; U₂₂U₂₂ \\end{bmatrix} = \\begin{bmatrix} U₁₂ \\\\ U₂₂ \\end{bmatrix} \\begin{bmatrix} U₁₂ \u0026amp; U₂₂ \\end{bmatrix} \\\\ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₂₃} = \\begin{bmatrix} U₁₂U₁₃ \u0026amp; U₁₂U₂₃ \\\\ U₂₂U₁₃ \u0026amp; U₂₂U₂₃ \\end{bmatrix} = \\begin{bmatrix} U₁₂ \\\\ U₂₂ \\end{bmatrix} \\begin{bmatrix} U₁₃ \u0026amp; U₂₃ \\end{bmatrix} \\\\ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₃₃} = \\begin{bmatrix} U₁₃U₁₃ \u0026amp; U₁₃U₂₃ \\\\ U₂₃U₁₃ \u0026amp; U₂₃U₂₃ \\end{bmatrix} = \\begin{bmatrix} U₁₃ \\\\ U₂₃ \\end{bmatrix} \\begin{bmatrix} U₁₃ \u0026amp; U₂₃ \\end{bmatrix} $$\nσ₁₁, σ₂₂, σ₃₃ are on the diagonal, while σ₁₂, σ₁₃, σ₂₃ are off-diagonal. The partial derivative of the loss L w.r.t. each element of 𝚺ₖ:\n$$ \\begin{aligned} \\frac{∂L}{∂\\bm Σₖ\u0026rsquo;_{(2D)}} \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₁₁} \u0026amp;= ∑_{row}∑_{col}{ \\begin{bmatrix} \\frac{∂L}{∂a} \u0026amp; \\frac{∂L}{∂b} \\\\ \\frac{∂L}{∂b} \u0026amp; \\frac{∂L}{∂c} \\end{bmatrix} ⊙ \\begin{bmatrix} U₁₁U₁₁ \u0026amp; U₁₁U₂₁ \\\\ U₂₁U₁₁ \u0026amp; U₂₁U₂₁ \\end{bmatrix} } \\\\ \u0026amp;= \\frac{∂L}{∂a} U₁₁U₁₁ + 2× \\frac{∂L}{∂b} U₁₁U₂₁ + \\frac{∂L}{∂c} U₂₁U₂₁ \\\\ \\frac{∂L}{∂\\bm Σₖ\u0026rsquo;_{(2D)}} \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₁₂} \u0026amp;= ∑_{row}∑_{col}{ \\begin{bmatrix} \\frac{∂L}{∂a} \u0026amp; \\frac{∂L}{∂b} \\\\ \\frac{∂L}{∂b} \u0026amp; \\frac{∂L}{∂c} \\end{bmatrix} ⊙ \\begin{bmatrix} U₁₁U₁₂ \u0026amp; U₁₁U₂₂ \\\\ U₂₁U₁₂ \u0026amp; U₂₁U₂₂ \\end{bmatrix} } \\\\ \u0026amp;= \\frac{∂L}{∂a} U₁₁U₁₂ + \\frac{∂L}{∂b} U₁₁U₂₂ +\\frac{∂L}{∂b}U₂₁U₁₂ + \\frac{∂L}{∂c} U₂₁U₂₂ \\end{aligned} $$\n⊙ is Hadamard product (element-wise product). $∑_{row}∑_{col}$ means summation of all elements in the matrix.\n(2024-02-17) In this step, the derivative w.r.t. a matrix is determined by calculating the derivative w.r.t. each element individually, rather than the entire matrix. Thus, the multiplication between two \u0026ldquo;derivative matrices\u0026rdquo; is hadamard product, as essentially it\u0026rsquo;s the derivative w.r.t. a single scalar (in contrast to vector or matrix). However, for example, if $\\frac{∂\\bm Σ}{∂𝐌}$ is the derivative of 𝚺 w.r.t. the matrix 𝐌, the multiplication with the incoming upstream \u0026ldquo;derivative matrix\u0026rdquo; should be a normal matmul.\nWithin a chain of differentiation, the 2 manners of solving derivative for a matrix by computing the derivative for the entire matrix or calculating the derivative for each element can coexist simultaneously.\nNote: The $\\frac{∂L}{∂b}$ in the 3DGS code has been doubled. And each off-diagonal element is multiplied by 2, as the symmetrical element has the same gradient contribution.\nIn 3DGS code, the derivative of loss w.r.t. each element of 3D covariance matrix 𝚺ₖ₍₃ₓ₃₎ in the world space is computed individually:\n1 2 3 4 5 6 7 8 9 10 11 dL_da = denom2inv * (...); dL_dc = denom2inv * (...); dL_db = denom2inv * 2 * (...); dL_dcov[0] = (T[0][0]*T[0][0]*dL_da + T[0][0]*T[1][0]*dL_db + T[1][0]*T[1][0]*dL_dc); dL_dcov[3] = (T[0][1]*T[0][1]*dL_da + T[0][1]*T[1][1]*dL_db + T[1][1]*T[1][1]*dL_dc); dL_dcov[5] = (T[0][2]*T[0][2]*dL_da + T[0][2]*T[1][2]*dL_db + T[1][2]*T[1][2]*dL_dc); dL_dcov[1] = 2*T[0][0]*T[0][1]*dL_da + (T[0][0]*T[1][1] + T[0][1]*T[1][0])*dL_db + 2*T[1][0]*T[1][1]*dL_dc; dL_dcov[2] = 2*T[0][0]*T[0][2]*dL_da + (T[0][0]*T[1][2] + T[0][2]*T[1][0])*dL_db + 2*T[1][0]*T[1][2]*dL_dc; dL_dcov[4] = 2*T[0][2]*T[0][1]*dL_da + (T[0][1]*T[1][2] + T[0][2]*T[1][1])*dL_db + 2*T[1][1]*T[1][2]*dL_dc; The 2D covariance matrix $\\bm Σₖ\u0026rsquo;_{(2D)}$ is Not equivalent to the calculation where the 3rd row and column of 𝚺ₖ are omitted from the beginning, because σ₁₃, σ₂₃, σ₃₃ are also involved in the projected covariance 𝚺ₖ\u0026rsquo;. However, the derivatives w.r.t. them ($\\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₁₃},\\ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₂₃},\\ \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂σ₃₃}$) can\u0026rsquo;t be derived from the following expression:\n$$\\begin{aligned} \\bm Σₖ\u0026rsquo;_{(2D)} \u0026amp;= \\begin{bmatrix} U₁₁ \u0026amp; U₁₂ \\\\ U₂₁ \u0026amp; U₂₂ \\end{bmatrix} \\begin{bmatrix} σ₁₁ \u0026amp; σ₁₂ \\\\ σ₂₁ \u0026amp; σ₂₂ \\end{bmatrix} \\begin{bmatrix} U₁₁ \u0026amp; U₂₁ \\\\ U₁₂\u0026amp; U₂₂ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} (U₁₁σ₁₁ + U₁₂σ₂₁)U₁₁ + (U₁₁σ₁₂ + U₁₂σ₂₂) U₁₂ \u0026amp; (U₁₁σ₁₁ + U₁₂σ₂₁)U₂₁ + (U₁₁σ₁₂ + U₁₂σ₂₂) U₂₂ \\\\ (U₂₁σ₁₁ + U₂₂σ₂₁)U₁₁ + (U₂₁σ₁₂ + U₂₂σ₂₂) U₁₂ \u0026amp; (U₂₁σ₁₁ + U₂₂σ₂₁)U₂₁ + (U₂₁σ₁₂ + U₂₂σ₂₂) U₂₂ \\end{bmatrix} \\end{aligned} $$\nThe partial derivative of the 2D Gaussian covariance matrix $\\bm Σₖ\u0026rsquo;_{(2D)}$ w.r.t. the 3D Gaussian center 𝐭ₖ in the camera space:\n$$ \\begin{aligned} \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂𝐔ₖ} \\frac{∂𝐔ₖ}{∂𝐭ₖ} = \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂𝐔ₖ} \\frac{∂(𝐉ₖ⋅ 𝐑_{w2c})}{∂𝐭ₖ} \\\\ \\end{aligned} $$\n$\\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂𝐔ₖ}$ (corresponding to $\\frac{∂\\bm Σₖ\u0026rsquo;}{∂T}$ inside $\\frac{∂L}{∂T}$ of eq.(25) in the gsplat paper.)\n$$ \\begin{aligned} \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₁₁} \u0026amp;= \\begin{bmatrix} 2σ₁₁U₁₁+(σ₁₂+σ₂₁)U₁₂+(σ₁₃+σ₃₁)U₁₃ \u0026amp; σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃ \u0026amp; σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃ \\\\ σ₁₁U₂₁ + σ₂₁U₂₂ + σ₃₁U₂₃ \u0026amp; 0 \u0026amp; 0 \\\\ σ₁₁U₃₁ + σ₂₁U₃₂ + σ₃₁U₃₃ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₁₂} \u0026amp;= \\begin{bmatrix} U₁₁σ₁₂+σ₂₁U₁₁+2σ₂₂U₁₂+σ₂₃U₁₃ +U₁₃σ₃₂ \u0026amp; σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃ \u0026amp; σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃ \\\\ U₂₁σ₁₂ + U₂₂σ₂₂+U₂₃σ₃₂ \u0026amp; 0 \u0026amp; 0 \\\\ U₃₁σ₁₂ + U₃₂σ₂₂+U₃₃σ₃₂ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₁₃} \u0026amp;= \\begin{bmatrix} U₁₁σ₁₃+U₁₂σ₂₃+σ₃₁U₁₁+σ₃₂U₁₂+2σ₃₃U₁₃ \u0026amp; σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃ \u0026amp; σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃ \\\\ U₂₁σ₁₃+U₂₂σ₂₃+U₂₃σ₃₃ \u0026amp; 0 \u0026amp; 0 \\\\ U₃₁σ₁₃+U₃₂σ₂₃+U₃₃σ₃₃ \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₂₁} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; U₁₁σ₁₁+U₁₂σ₂₁+U₁₃σ₃₁ \u0026amp; 0 \\\\ σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃ \u0026amp; 2σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃+U₂₂σ₂₁+U₂₃σ₃₁ \u0026amp; σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃ \\\\ 0 \u0026amp; U₃₁σ₁₁+U₃₂σ₂₁+U₃₃σ₃₁ \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₂₂} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; U₁₁σ₁₂+U₁₂σ₂₂+U₁₃σ₃₂ \u0026amp; 0 \\\\ σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃ \u0026amp; U₂₁σ₁₂+σ₂₁U₂₁+2σ₂₂U₂₂+σ₂₃U₂₃+U₂₃σ₃₂ \u0026amp; σ₂₁U₃₁+σ₂₂U₃₂+σ₂₃U₃₃ \\\\ 0 \u0026amp; U₃₁σ₁₂+U₃₂σ₂₂+U₃₃σ₃₂ \u0026amp; 0 \\end{bmatrix} \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₂₃} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; U₁₁σ₁₃+U₁₂σ₂₃+U₁₃σ₃₃ \u0026amp; 0 \\\\ σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃ \u0026amp; U₂₁σ₁₃+U₂₂σ₂₃+σ₃₁U₂₁+σ₃₂U₂₂+2σ₃₃U₂₃ \u0026amp; σ₃₁U₃₁+σ₃₂U₃₂+σ₃₃U₃₃ \\\\ 0 \u0026amp; U₃₁σ₁₃+U₃₂σ₂₃+U₃₃σ₃₃ \u0026amp; 0 \\end{bmatrix} \\\\ \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₃₁} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; U₁₁σ₁₁+U₁₂σ₂₁+U₁₃σ₃₁ \\\\ 0 \u0026amp; 0 \u0026amp; U₂₁σ₁₁+U₂₂σ₂₁+U₂₃σ₃₁ \\\\ σ₁₁U₁₁+σ₁₂U₁₂+σ₁₃U₁₃ \u0026amp; σ₁₁U₂₁+σ₁₂U₂₂+σ₁₃U₂₃ \u0026amp; 2σ₁₁U₃₁+σ₁₂U₃₂+σ₁₃U₃₃+ U₃₂σ₂₁ + U₃₃σ₃₁ \\\\ \\end{bmatrix} \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₃₂} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; U₁₁σ₁₂+U₁₂σ₂₂+U₁₃σ₃₂ \\\\ 0 \u0026amp; 0 \u0026amp; U₂₁σ₁₂+U₂₂σ₂₂+U₂₃σ₃₂ \\\\ σ₂₁U₁₁+σ₂₂U₁₂+σ₂₃U₁₃ \u0026amp; σ₂₁U₂₁+σ₂₂U₂₂+σ₂₃U₂₃ \u0026amp; U₃₁σ₁₂+ σ₂₁U₃₁+2σ₂₂U₃₂+σ₂₃U₃₃+ U₃₃σ₃₂ \\\\ \\end{bmatrix} \\\\ \\frac{∂𝐔ₖ {\\bm Σₖ}_{(3D)} 𝐔ₖᵀ}{∂U₃₃} \u0026amp;= \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; U₁₁σ₁₃+U₁₂σ₂₃+U₁₃σ₃₃ \\\\ 0 \u0026amp; 0 \u0026amp; U₂₁σ₁₃+U₂₂σ₂₃+U₂₃σ₃₃ \\\\ σ₃₁U₁₁+σ₃₂U₁₂+σ₃₃U₁₃ \u0026amp; σ₃₁U₂₁+σ₃₂U₂₂+σ₃₃U₂₃ \u0026amp; U₃₁σ₁₃ + U₃₂σ₂₃ + σ₃₁U₃₁+σ₃₂U₃₂+2σ₃₃U₃₃ \\\\ \\end{bmatrix} \\end{aligned} $$\n$\\frac{∂𝐔ₖ}{∂𝐭ₖ} = \\frac{∂(𝐉ₖ⋅ 𝐑_{w2c})}{∂𝐭ₖ} = \\frac{∂𝐉ₖ}{∂𝐭ₖ}⋅ 𝐑_{w2c} + \\cancel{ 𝐉ₖ⋅\\frac{∂𝐑_{w2c}}{∂𝐭ₖ} }$,\n(2024-01-16)\nThe derivative of 𝐉ₖ w.r.t. the camera-space center 𝐭ₖ could be obtained from the representation of 𝐉ₖ in terms of 𝐭ₖ, which includes focals fx,fy more than the representation with clip coordinates 𝐭ₖ\u0026rsquo;. In this way, the projection matrix P isn\u0026rsquo;t involved as 𝐭ₖ\u0026rsquo;=𝐏𝐭ₖ.\n$$ 𝐉_{𝐭ₖ} = \\begin{bmatrix} fₓ/t_{k,z} \u0026amp; 0 \u0026amp; -fₓ t_{k,x} / {t_{k,z}}^2 \\\\ 0 \u0026amp; f_y/t_{k,z} \u0026amp; -f_y t_{k,y} / {t_{k,z}}^2 \\\\ t_{k,x}/‖𝐭ₖ‖ \u0026amp; t_{k,y}/‖𝐭ₖ‖ \u0026amp; t_{k,z}/‖𝐭ₖ‖ \\end{bmatrix} $$\nThe derivative of $𝐉_{𝐭ₖ}$ w.r.t. each component of 𝐭ₖ:\n$$ \\begin{aligned} \\frac{∂𝐉_{𝐭ₖ}}{∂t_{k,x}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; -f_x/t_{k,z}² \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 1/‖𝐭ₖ‖ - t_{k,x}^2/‖𝐭ₖ‖^3 \u0026amp; -t_{k,y}t_{k,x}/‖𝐭ₖ‖^3 \u0026amp; -t_{k,z}t_{k,x}/‖𝐭ₖ‖^3 \\end{bmatrix} \\\\ \\frac{∂𝐉_{𝐭ₖ}}{∂t_{k,y}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -f_y/t_{k,z}^2 \\\\ -t_{k,x}t_{k,y}/‖𝐭ₖ‖^3 \u0026amp; 1/‖𝐭ₖ‖ - t_{k,y}^2/‖𝐭ₖ‖^3 \u0026amp; -t_{k,z}t_{k,y}/‖𝐭ₖ‖^3 \\end{bmatrix} \\\\ \\frac{∂𝐉_{𝐭ₖ}}{∂t_{k,z}} = \\begin{bmatrix} -f_x/t_{k,z}^2 \u0026amp; 0 \u0026amp; 2 f_x t_{k,x}/t_{k,z}^3 \\\\ 0 \u0026amp; -f_y/t_{k,z}^2 \u0026amp; 2 f_y t_{k,y}/t_{k,z}^3 \\\\ -t_{k,x}t_{k,z}/‖𝐭ₖ‖^3 \u0026amp; -t_{k,y}t_{k,z}/‖𝐭ₖ‖^3 \u0026amp; 1/‖𝐭ₖ‖ - t_{k,z}^2/‖𝐭ₖ‖^3 \\end{bmatrix} \\end{aligned} $$\nThe derivative of 𝚺ₖ\u0026rsquo; w.r.t. 𝐭ₖ\n(2024-01-17)\n$$ \\begin{aligned} \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂𝐔ₖ} \\frac{∂𝐔ₖ}{∂t_{k,x}} \\end{aligned} $$\nBased on the projective projection $ϕ(𝐭) ≈ ϕ(𝐭ₖ) + 𝐉ₖ⋅(𝐭 - 𝐭ₖ)$,\nwhere\nThe extrinsics of camera: $𝐓_{w2c} = [^{𝐑_{w2c} \\ 𝐭_{w2c}}_{0 \\quad\\ 1}]$\n𝐭 is the mean vector represented in the camera space: $𝐭 = 𝐓_{w2c} ⋅[^{\\bm μₖ}_1]$\nThe Jacobian of the projective transformation evaluated at 𝛍ₖ:\n$$𝐉ₖ = \\begin{bmatrix} fₓ/μ_{k,z} \u0026amp; 0 \u0026amp; -fₓ μₓ / μ_{k,z}^2 \\\\ 0 \u0026amp; f_y/μ_{k,z} \u0026amp; -f_y μ_y / μ_{k,z}^2 \\\\ μₖₓ/‖\\bm μₖ‖₂ \u0026amp; μ_{k,y}/‖\\bm μₖ‖₂ \u0026amp; μ_{k,z}/‖\\bm μₖ‖₂ \\end{bmatrix} $$\nTherefore, the partial derivatives of the loss w.r.t.\nThe partial derivative of the loss 𝓛 w.r.t. the 3D Gaussian center 𝐭 in the world space:\n$$ \\frac{∂L}{∂𝐭} = \\frac{∂L}{∂\\bm μ\u0026rsquo;} \\frac{∂\\bm μ\u0026rsquo;}{∂𝐭} + \\frac{∂L}{∂\\bm Σₖ\u0026rsquo;_{(2D)}} \\frac{∂\\bm Σₖ\u0026rsquo;_{2D}}{∂𝐭} $$\nThe partial derivative of the loss 𝓛 w.r.t. the 3D Gaussian covariance matrix 𝚺ₖ in the world space:\n$$ \\frac{∂L}{∂\\bm Σₖ} = \\frac{∂L}{∂\\bm Σₖ\u0026rsquo;_{(2D)}} \\frac{∂\\bm Σₖ\u0026rsquo;_{(2D)}}{∂\\bm Σₖ} $$\nCovariance Because covariance matrix is symmetric (𝚺ₖ = 𝚺ₖᵀ), it\u0026rsquo;s a square matrix, so it\u0026rsquo;s diagonalizable\n\u0026ldquo;Diagonalizable matrix 𝐀 can be represented as: 𝐀 = 𝐏𝐃𝐏⁻¹.\u0026rdquo;\n\u0026ldquo;A diagonalizable matrix 𝐀 may (?) be decomposed as 𝐀=𝐐𝚲𝐐ᵀ\u0026rdquo;\n\u0026ldquo;Quadratic form can be regarded as a generalization of conic sections.\u0026rdquo; Symmetric matrix\nSince the covariance matrix 𝚺 is a symmetric matrix, its eigenvalues are all real. By arranging all its eigenvectors and eigenvalues into matrices, there is:\n$$\\bm Σ 𝐕 = 𝐕 𝐋$$\nwhere each column in 𝐕 is an eigenvector, which are orthogonal to each other.\n𝐋 is a diagonal matrix. For example:\n$$ \\bm Σ 𝐕 = 𝐕 𝐋 = \\begin{bmatrix} a \u0026amp; d \u0026amp; g \\\\ b \u0026amp; e \u0026amp; h \\\\ c \u0026amp; f \u0026amp; j \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 3 \\end{bmatrix} = \\begin{bmatrix} a \u0026amp; 2d \u0026amp; 3g \\\\ b \u0026amp; 2e \u0026amp; 3h \\\\ c \u0026amp; 2f \u0026amp; 3j \\end{bmatrix} $$\nIf 𝐕 is invertible, 𝚺 can be represented as 𝚺=𝐕𝐋𝐕⁻¹\nThe eigenvectors matrix 𝐕 and eigenvalues matrix 𝐋 corresponds to the rotation matrix 𝐑 and the stretching matrix 𝐒 squared, which are solved from SVD: 𝚺=𝐑𝐒𝐒ᵀ𝐑ᵀ.\nThe rotation matrix rotates the original space to a new basis, where each axis points in the direction of the highest variance, i.e., the eigenvectors. And the stretching matrix indicates the magnitude of variance along each axis, i.e., the square root of the eigenvalues $\\sqrt{𝐋}$. janakiev-Blog\nAfter obtaining the magnitude of the variance in each direction, the extent range on each axis can be calculated based on the standard deviation according to the 3-sigma rule.\nTo optimize the 3D Gaussians in the world space based on rendered image, the derivative chain is like:\nI l m o a s g s e s s c p r a 𝚺 e c ' e e n ( 2 D ) R s a p y a 𝚺 c ' e ( 3 D ) c s a p m a e c 𝚺 r e a w s o p 𝚺 r a l c d e ","date":"2024-01-01T12:45:00Z","image":"https://janakiev.com/assets/covariance-matrix_files/covariance_visualization.jpg","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-math/","title":"read: 3DGS | Math Derivation"},{"content":"Code | Arxiv | Oral | GScholar | Rui Chen, Hao Su\nNotes (2023-12-24)\nAbs \u0026amp; Intro Use point cloud to refine depth map rather than regularizing (\u0026ldquo;squashing\u0026rdquo;) cost volumes.\nRefine the coarse point cloud by iteratively estimating the residual from the current depth to the target depth.\nfeature volume ➔ feature-augmented point cloud ➔ depth residual ➔ point displacement\nOutput: a dense point cloud, as opposed to sparse point cloud from SfM used by 3DGS.\nDepth map yielded from point cloud representation doesn\u0026rsquo;t suffer from the problem of resolution limitation.\nOptimization objective: minimize the distance from point to the surface, with the supervision of depth map\nDepth estimation doesn\u0026rsquo;t involve opacity for rendering, so can the RGB appearance be used as loss function? PointFlow: Move point towards the surface. 3DGS split or clone a Gaussian along the position\u0026rsquo;s gradient with some hyper-parameters setting.\nHow is the generalizability of this method?\nPipeline Point cloud initialization from a small coarse depth map for reference image.\nS m a i l n l r C e o f s t c a V m o e l r u a m e 3 c D l a U s N s e i t f y d C e o p a t r h s e m a U p n p r o j P w o o i r n l t d c s l p o a u c d e 48 homographies (depths) for warping a source feature map with the 1/8 size of the original image. Assign 2D and 3D context feature to each point\n2D feature: project each point onto 3-level feature maps of each view with camera focals and cx,cy scaled.\nW o r l d s U p P p a o d c i a e n t t i s n g F i x f e e d a t 3 p u - y r s r e c a a m m l i a e d p s The retrieved feature vectors of N views are merged into a variance for a certain level j of the feature maps:\n$$𝐂ʲ = \\frac{\\sum (𝐅ʲ - \\bar{𝐅ʲ}) }{N}$$\nOnly the feature vectors at the projection locations are taken, rather than processing the entire feature map, thereby improving efficiency. 3D feature: normalized point coordinates $𝐗ₚ$\nConcatenate features: $Cₚ = 𝐂¹ ⊕ 𝐂² ⊕ 𝐂³ ⊕ 𝐗ₚ$\nDefine displacement steps with point hypotheses.\nThe unprojected 3D point cloud from 2D depth map is determined. The displacement direction has a lot of freedom. Thus, each point is confined to move along the ray emitted from the reference camera. Such that the per-pixel depth can be refined.\n→ R - e □ f ↓ ↑ □ □ c - a ← m In the above figure, • is a real point, and o is a point hypothesis $\\tilde{𝐩ₖ}$ associated with a read point $𝐩ₖ$:\n$$\\tilde{𝐩ₖ} = 𝐩ₖ + k s 𝐭, \\quad k = -m, …, m$$\nwhere k is the number of steps, s is the step size along the ray direction 𝐭.\nm = 1, so\nAggreate features of the n nearest neighbors\nUse Dynamic Graph CNN to aggreate neighboring points\nUse a MLP to map the aggregated feature to probabilities of point hypotheses.\nThe PointFlow module requires iterations to approach the surface.\nThe predicted depth residual against the target depth is a probabilistic weighted sum (expectation) of all predefined hypothetical steps.\n$$Δdₚ = 𝐄(ks) = ∑_{k=-m}^m ks × Prob( \\tilde{𝐩ₖ} )$$\nUpsampling the refined depth map and shrink the interval between point hypotheses.\nLoss: Accumulated absolute error between refined depth map and the ground-truth depth map over all previous iterations.\n$$L = ∑_{i=0}^l \\Big( \\frac{1}{s^{(i)}} ∑_{p∈𝐏_{valid}} \\| 𝐃_{GT}(p) - 𝐃^{(i)}(p) \\|₁ \\Big)$$\nMethod Feature concatenation is similar to PixelNeRF: point ocoordinates + feature vectors. But here the points position isn\u0026rsquo;t consistent resulting in that different feature vectors are sampled on the feature pyrimid.\nAs the 2D CNN also requires optimization, the feature pyrimid is changingas well.\nPlay Environment (2023-12-26)\nInstallation on Lambda server (Ubuntu 18.04.6 LTS, nvcc -V return 10.2, Driver Version: 470.103.01):\n1 2 bash install_dependencies.sh bash compile.sh Error: No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\nTraceback 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 No CUDA runtime is found, using CUDA_HOME=\u0026#39;/usr/local/cuda\u0026#39; running build_ext Traceback (most recent call last): File \u0026#34;setup.py\u0026#34;, line 20, in \u0026lt;module\u0026gt; \u0026#39;build_ext\u0026#39;: BuildExtension File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/setuptools/__init__.py\u0026#34;, line 153, in setup return distutils.core.setup(**attrs) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/core.py\u0026#34;, line 148, in setup dist.run_commands() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/dist.py\u0026#34;, line 955, in run_commands self.run_command(cmd) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/dist.py\u0026#34;, line 974, in run_command cmd_obj.run() File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/setuptools/command/build_ext.py\u0026#34;, line 79, in run _build_ext.run(self) File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/distutils/command/build_ext.py\u0026#34;, line 339, in run self.build_extensions() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 404, in build_extensions self._check_cuda_version() File \u0026#34;/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.6/site-packages/torch/utils/cpp_extension.py\u0026#34;, line 777, in _check_cuda_version torch_cuda_version = packaging.version.parse(torch.version.cuda) File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/pkg_resources/_vendor/packaging/version.py\u0026#34;, line 49, in parse return Version(version) File \u0026#34;/home/zichen/.local/lib/python3.6/site-packages/pkg_resources/_vendor/packaging/version.py\u0026#34;, line 264, in __init__ match = self._regex.search(version) TypeError: expected string or bytes-like object Search the error with DDG.\ntorch.cuda.is_available() returns False. NV forums\npytorch is of cpu version. issue\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; conda list cpuonly 2.0 0 pytorch pytorch 1.10.2 py3.6_cpu_0 pytorch pytorch-mutex 1.0 cpu pytorch torchvision 0.11.3 py36_cpu [cpuonly] pytorch Reinstall with newer packages:\n\u0026ldquo;install_dependencies.sh\u0026rdquo;:\n1 2 3 4 5 6 #!/usr/bin/env bash conda create -n PointMVSNet python=3.10 source activate PointMVSNet conda install pytorch==1.12.1 torchvision==0.13.1 cudatoolkit=10.2 -c pytorch conda install -c anaconda pillow pip install -r requirements.txt Error: identifier \u0026quot;AT_CHECK\u0026quot; is undefined.\nFull message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 building \u0026#39;dgcnn_ext\u0026#39; extension creating build creating build/temp.linux-x86_64-cpython-310 creating build/temp.linux-x86_64-cpython-310/csrc /usr/local/cuda/bin/nvcc -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/TH -I/home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/zichen/anaconda3/envs/PointMVSNet/include/python3.10 -c csrc/gather_knn_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/gather_knn_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1011\\\u0026#34; -DTORCH_EXTENSION_NAME=dgcnn_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++14 /home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign /home/zichen/anaconda3/envs/PointMVSNet/lib/python3.10/site-packages/torch/include/ATen/Context.h(25): warning: attribute \u0026#34;__visibility__\u0026#34; does not apply here csrc/gather_knn_kernel.cu(34): error: identifier \u0026#34;AT_CHECK\u0026#34; is undefined csrc/gather_knn_kernel.cu(106): error: identifier \u0026#34;AT_CHECK\u0026#34; is undefined csrc/gather_knn_kernel.cu(125): error: identifier \u0026#34;THArgCheck\u0026#34; is undefined csrc/gather_knn_kernel.cu(145): error: identifier \u0026#34;THCudaCheck\u0026#34; is undefined csrc/gather_knn_kernel.cu(84): error: identifier \u0026#34;TH_INDEX_BASE\u0026#34; is undefined 5 errors detected in the compilation of \u0026#34;/tmp/tmpxft_00002cb5_00000000-6_gather_knn_kernel.cpp1.ii\u0026#34;. error: command \u0026#39;/usr/local/cuda/bin/nvcc\u0026#39; failed with exit code 1 Perplexity (GPT4): Identifiers have been deprecated after PyTorch 1.0.\nIn PyTorch 1.5.0 and later, AT_CHECK has been replaced with TORCH_CHECK. Similarly, THArgCheck and THCudaCheck are no longer used in newer versions of PyTorch. The identifier TH_INDEX_BASE is also undefined because it\u0026rsquo;s no longer used in PyTorch 1.0 and later.\nDowngrade pytorch:\nAI葵 used torch 1.4. Issue: \u0026ldquo;TH_INDEX_BASE\u0026rdquo; is undefined #1\n1 2 3 4 5 6 #!/usr/bin/env bash conda create -n PointMVSNet python=3.8 source activate PointMVSNet conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch conda install -c anaconda pillow pip install -r requirements.txt torchvision older than v0.6.1 doesn\u0026rsquo;t exist in conda:\nFull message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels: - torchvision==0.5.0 Current channels: - https://conda.anaconda.org/pytorch/linux-64 - https://conda.anaconda.org/pytorch/noarch - https://conda.anaconda.org/conda-forge/linux-64 - https://conda.anaconda.org/conda-forge/noarch - https://repo.anaconda.com/pkgs/main/linux-64 - https://repo.anaconda.com/pkgs/main/noarch - https://repo.anaconda.com/pkgs/r/linux-64 - https://repo.anaconda.com/pkgs/r/noarch To search for alternate channels that may provide the conda package you\u0026#39;re looking for, navigate to https://anaconda.org and use the search bar at the top of the page. Use pip to install torch 1.4. (perplexity)\n1 2 3 4 5 conda create -n PointMVSNet python=3.8 source activate PointMVSNet conda install -c anaconda pillow pip install torch==1.4.0 torchvision==0.5.0 pip install -r requirements.txt And modify TH_INDEX_BASE -\u0026gt; 0 in \u0026ldquo;gather_knn_kernel.cu\u0026rdquo;.\nIt can be compiled successfully.\n3DGS also has knn code Train \u0026amp; Eval Place the dataset (DTU) in the specified directory:\n1 2 mkdir data ln -s /home/zichen/Downloads/mvs_training/dtu/ ./data Training for 16 epochs cost 3 days approximately:\n1 2 export CUDA_VISIBLE_DEVICES=4,5,6,7 python pointmvsnet/train.py --cfg configs/dtu_wde3.yaml Evaluate\nThe \u0026ldquo;Recified\u0026rdquo; dataset can\u0026rsquo;t be download from the official site by clicking the link. It may be due to broswer limitations as the zip file is 123 GB (analyzed by Perplexity).\nWget works: wget http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip\nDownload message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (base) zichen@lambda-server:/data/zichen$ wget http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip wget: /home/zichen/anaconda3/envs/GNT/lib/libuuid.so.1: no version information available (required by wget) --2023-12-30 19:30:31-- http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Resolving roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)... 130.225.69.128 Connecting to roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)|130.225.69.128|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip [following] --2023-12-30 19:30:32-- https://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip Connecting to roboimagedata2.compute.dtu.dk (roboimagedata2.compute.dtu.dk)|130.225.69.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 129593443783 (121G) [application/zip] Saving to: ‘Rectified.zip’ Rectified.zip 100%[===========================\u0026gt;] 120.69G 3.48MB/s in 14h 18m 2023-12-31 09:48:33 (2.40 MB/s) - ‘Rectified.zip’ saved [129593443783/129593443783] Unzip it\n","date":"2023-12-16T21:30:00Z","image":"http://hansf.me/projects/PMVSNet/images/network.png","permalink":"https://zichen34.github.io/writenotes/model/mvs/b-note-point-mvsnet/","title":"read: Point-MVSNet"},{"content":"Code | Arxiv | GScholar | mvsnet pytorch\nNotes Task The model predicts a single depth map at once instead of an entire scene:\nMVSNet can reconstruct a large scene by performing per-view depth map estimation repeatedly.\n\u0026ldquo;Per-view\u0026rdquo; also appears in Point-Based Neural Rendering with Per-View Optimization - Inria.\nThe estimated depth map of the reference view is a weighted sum of multiple predefined depth values, where the weights are regressed from the variance of multi-view cost volumes by a 3D UNet-like network.\nA cost volume is a structure-fixed container (file holder) in the reference camera space, with each compartment holding a source view\u0026rsquo;s feature map that requires warping to align with the camera pose and depth before being inserted into each slot.\nC a r c o r e a s e f m t e e i r r V n e a o n l t c s u h e p m e a e c S s e a m e d e p t h - s l i c i n g A s f o o b u r u r n c o d e t l h e f e e r o a f t V u o w r l a e u r m p m e e a s d p s (2023-12-13) In contrast, NeRF lacks a unified spatial structure to facilitate multi-view geometry consistency.\nPer-ray sampling \u0026 Scene-specific Each view samples depths and maintains a point cloud in its own frustum for rendering (not for reconstructing the global geometry), and the fine-stage sampling results in each ray having different sampled points.\n(2024-02-19) The sampled points don\u0026rsquo;t have standardized geometric structure, because NeRF is a \u0026ldquo;continuous\u0026rdquo; filed, whose continuity is achieved by an MLP. NeRF encompassed multi views into a Radiance filed, so the scene geometry is not directly modeled and constrained. Consequently, multi-view (geometry) inconsistency (floater) is incurred.\nv i 1 e w v i e w 2 v i 3 e w NeRF samples per ray rather than the whole space because it employs the volume rendering method to generate view-dependent images. The primary goal is achieving accurate pixels by optimizing the alignment of volume density σ and voxel color c on an arbitrary piont due to the volume rendering equaiton, while the overall geometry is not focused specifically.\nIn other words, since what NeRF learned is the match between density and voxel color, even if the densities (geometry) are wrong, with colors compensated, the composition could be plausible. Despite the multi-view consistency constraint during training, the geometry estimation (surface) is inherently biased. (NeuS)\nOn the other hand, because the correspondence between volume density and voxel color is different for various scenes, the MLP network of NeRF is scene-specific.\nSimilarly, PixelNeRF regress densities and colors from pixel-wise image features in the frustum of each viewpoint as well, instead of the world space. And multiple views\u0026rsquo; results get aggregated by taking average finally.\nPer-ray and fine-stage sampling decides NeRF is a scene-specific representation because each region is not reconstructed equally. Although some generalizable NeRFs predict density and color from image features, the precision won\u0026rsquo;t be high if only image features are input without providing explicit geometry structures.\n(2023-12-15) The whole space to be reconstructed is split (divide-and-conquer strategy) with some assumption instead of estimating from null. CVPR2023|基于深度学习MVS的最新进展 GeoMVSNet - 3D视觉工坊\nThe resolution of a cost volume is fixed (point-mvsnet). A cost volume is a voxel grid to some extent.\nThe pipeline of MVSNet:\nR i f e m e f g a S v f t S v f r i e r i e c e a c e a w t w t R 1 e 2 p l i c C a o s W s a t r p V e o ✚ d l u v i f m a n e e r a : i r V v t o e o a s w u f l r a s - u i r c m a p d a e n e e m c d p e t s f h p + e s a s a c 3 U o t e D N f s e t t m a a t x R e V 1 f o 9 ' l 2 s u m d p e e r p o s b D E e m x p a p t p e h c r e f i n e Implicit Camera Pose \u0026ldquo;Arbitrary camera geometries\u0026rdquo; prevent the direct comparision among all multi-view feature maps (due to the enormous disparity in observations?).\nIn contrast, 2-view stereo is free from that problem because after pre-rectification, the matching cost of a pair of feature maps is only attributed to the disparity between the pair of images. (?)\n\u0026ldquo;Arbitrary camera geometries\u0026rdquo; means that the epipolar constraint served as the base of two-view stereo could fail, e.g. when no overlapping regions exist in the 2 views. In such a case, however, homography still holds. homography - Carleton University\nTherefore, MVSNet transforms all feature maps into the reference-view camera space.\nHomography warping injects camera geometries into the 2D CNN implicitly.\nBecause the convolution and homography (warping) both are linear operations, they can be switched. Thus, warping the feature map is equivalent to warp the source images followed by convolution. Consequently, the convolution layer (feature extractor) is trained on an image set that implies the camera geometries.\n(2023-12-19) Note: The source image (or feature map) is not mapped onto the depth plane. They\u0026rsquo;re mapped into the reference camera viewpoint to watch the scene.\nBy applying different homographies solved with the predefined depth planes, the source image is warped uniquely. Thus, the features at the same location vary.\nOverall, as depth increases, the source images are go backward.\n(2024-03-17) The above effect is like that a source image is seen from the reference view. It\u0026rsquo;s like watching TV from another angle. This is because when restoring depth values for all pixels, they are assigned with a common depth value to become 3D points (on a depth plane) Code. During training, there are 192 assumed depth values to be assigned on pixels, and then the network is required to predict which depth each pixel is, resulting in a correct pixel-wise depth.\nWith the correct depth map, applying homography will lead to overlapping for the common area between src and ref images.\nThis differentiates homography from epipolar geometry, where a point is projected onto different views to retrieve feature vectors. (2023-12-20) As ray marches, for epipolar geometry, the projected pixel is moving along the epipolar line. However, for homography, the source image keeps warping using the increasing depth, resulting in the same effect that the projected location moves along the epipolar line.\nAnd the directions of data flow are opposite. Epipolar is from 3D points at different depths to a source image, whereas homography is from different warped source images to a common 3D point.\nThe reason of performing homography warping on feature map rather than source images could be reducing indexing time with a small resolution?\n(2023-12-15) Maybe to avoid performing too many convolutions. A source image does convolution once and the feature map is warped 192 times.\n(2023-12-23) Maybe the author got inspired by the CNN-based 2-view stereo matching, where camera parameters are disregarded when applying CNN onto 2 pre-rectified images. In contrast, MVSNet incorporates camera poses implicitly after CNN. In other words, \u0026ldquo;rectification\u0026rdquo; is performed on feature maps instead of original images, so as to encode the camera poses into the 2D CNN.\nIf warping the source image first, many black empty areas will appear and disrupt the following convolution. Thus, perform CNN on source images first and then sample feature maps.\nBecause the cost volume is built in the reference camera frustum rather than the world space, the coordinates transformation applied is homography $𝐊₁[𝐑₁|𝐭₁](𝐊ᵢ[𝐑ᵢ|𝐭ᵢ])⁻¹$ rather than only unprojection $(𝐊ᵢ[𝐑ᵢ|𝐭ᵢ])⁻¹$. The \u0026ldquo;implicit\u0026rdquo; camera pose embedding manner is proposed to generalize the 2-view stereo to multi-view stereo.\nOur 3D cost volume is built upon the camera frustum instead of the regular Euclidean space.\nConvolution fuses the neighboring pixels into one feature vector for dense matching. A feature vector is a compact representation of a patch of pixels.\nIn this way, image size reduced but without losing context information required by matching. Hence, the efficiency is higher than matching the original images directly. The high-dimensional descriptor can lead to correct matching if they\u0026rsquo;re well-optimized.\nEvery source view passes the identical 2D CNN, so the differences between cost volumes indicate the probabilities of each depth for each pixel.\nEach \u0026ldquo;depth slot\u0026rdquo; within a cost volume stores a 32-channel feature map, which will be regularized to a 1-channel probability map by 3D UNet and softmax.\nEstimate by Variance The matching cost is measured by element-wise (pixel-wise, depth-wise, and channel-wise) variance of cost volumes, such that the number of source views is not limited.\nA property can be inferred from the mean of multiple views, serving as a synthesized representation. For example, PixelNeRF used the average of projected features from all source views to regress rgbσ per point. However, MVSNet maps the explicit difference (measured by variance) among the same-position feature vectors to a property, i.e., probability of each hypothetical depth for a pixel.\nThe feature vectors at the same location on the warped feature maps from different source images don\u0026rsquo;t correspond to the same 3D point, because a 3D point projected onto different camera films gets different pixel coordinates.\nThus, the homologous feature vectors (\u0026ldquo;allele\u0026rdquo;) at the same relative position from the 3 input images are distinct evidently.\nCyan squares are all at the same location [100:150, 165:215].\nThe ground truth depth is around 776:\n(2023-12-15) The variance for each channel of each matched feature vector from 3 cost volumes, for 192 depths is calculated as:\nF A e c a R r t e o s f s s f o S 3 r r c C d 1 o e s p t t S h r V 1 c o 2 l u m F e e s a t R s e f f o S r r c d 1 e p t S h r 2 c 2 ⋯ ⋯ F e a t R s e f f o S r r c d 1 e p 1 S 9 r 2 c 2 The 192 32-channel variance vectors will be fused to 1-D scalars by a 3D CNN.\nConv along the depth dimension, kernel size=2 and stride=1: aggregate every two variance vector at adjacent depths.\nOutput channel is 1: combine the covered vectors by channel-wise weighted summation only once.\nIn a kernel, weighted sum the same channels, and then sum up all channels to produce one of the output channels.\no 1 □ u V t c a □ : h r v a ↓ a t ⋯ r ⋯ s 1 D u 1 m □ V a r a s t t ⋯ r ⋯ D i 2 d e = 1 ⋯ o 1 V □ u a t c r □ : h a v t ↓ a ⋯ r D ⋯ s 1 1 u 9 9 m 2 1 □ V a r a t D ⋯ 1 9 2 192 variance scalars will be normalized by softmax to 192 probabilities for each depth value.\nDepth map is a summation of 192 preset depths weighted by probabilities.\nThe variance of the \u0026ldquo;position-matched\u0026rdquo; feature vectors from a pair of source feature maps at each depth is interpreted as each depth\u0026rsquo;s probability for a pixel:\nA source image is warped using a homography, solved with a certain depth $dₙ$, to supplement the reference view with information from a lateral perspective, although the scale doesn\u0026rsquo;t match:\nIf the scale matches, the pixels on the ref and source views projected from a common 3D point will overlapped, and the depth of that 3D point is found. (Refer to the test result in another post.)\nThe feature map of the source image is sampled into a warped feature map.\nFeature vectors at the same location are matched? do not match a common 3D point.\nThe variance of the feature vectors pertaining to a depth is calculated\nHigh variance means the patches that a point projected onto each view are distinct, leading to different feature vectors.\n3D CNN aggregates adjacent variance vectors along 3 directions into a scalar variance.\n3D CNN is a smooth constraint to filter the noise in the cost volume caused by non-Lambertian surfaces and object occlusions.\nNormalize 192 variance by Softmax to obtain the probability distribution of depths.\nWhy does the correct depth has the highest variance?\nWhy use variance? high variance means high probability.\n为什么高方差意味着高概率呢？？？\n把一组照片的 homography 效果做出来看看 Read: Rethink depth esti and IS-MVSNet Google search: \u0026ldquo;why does MVSNet use the variance to regress the probability of depths?\u0026rdquo; Because the exact depth for a pixel results in other views\u0026rsquo; \u0026ldquo;deny\u0026rdquo; since the matching pixel on other source images doesn\u0026rsquo;t correspond to that depth due to the viewpoint shifts.\nchanging viewpoint will change distance from the observing point to the camera as well. a feature vector represents a patch of pixels, the area with high rgb-variance means geometry changes.\n(2023-12-16) Each variance vector is attributed to 3 feature vectors:\nepipolar homography The epipolar reminds me the view fusion in GNT, where multi-view features are fused by \u0026quot; subtraction attention\u0026quot;.\n(2023-12-20) However, MVSNet is not epipolar. The feature 3 vectors are at the same location with the underlying feature maps changing.\nBut essentially, homography is epipolar in terms of variance vectors ( not actual 3D points). The feature vectors on the epipolar line are shifted to the location of the reference feature.\nSuccinctly, in epipolar, the projection changes (moves), while in homography, the feature map changes (warps).\nI probably thought of (on 2023-12-16) that after warping, the feature vectors at the same location correspond to a same 3D point, like eipolar. But that\u0026rsquo;s not right even for the warping with the accurate depth value (776 mm) as shown in the above demo. The image (or feature map) moved indeed after warping, but not got the exact position corresponding to the common 3D point.\nR f s f □ e e s f o e f a o e u a t u a r t 1 r t 2 c c e m e m a 3 a p 2 p d c 1 h n l s 3 □ □ 2 d 2 c □ □ h n l s □ □ d 3 1 2 9 2 c h n l s However, if the epipoles are at infinite (no overlapping observation), the variance matching is poor (under-determined system has inifinitely many solutions normally), and a reference pixel is falsely matched resulting in false probabilities distribution: Multiple depth values are plausible and gain similar probabilities. The correct warping correspond to the correct depth, and vice versa.\n(2023-12-20) 3D CNN is a classifier to find the most possible depth value among 192 hypotheses based on the input variance, which is produced from a 2D CNN.\nTherefore, the job of the 2D CNN is to make the features cooresponding to the correct depth having the biggest variance. Such that the aggregated variance will still be the most salient after softmax. And finally, it takes the highest weight.\n(2023-12-20) If the above analysis that the homography is equivalent to epipolar geometry for building variance vectors is true, then the reference pixel should find the matched pixel on the epipolar line projected from the correct depth.\nThe source feature map is warped differently according to various depth values. The feature vectors at a common location of all the input images differs. Their variance is the matching cost for a hypothetical depth. Since a feature vector indicates the context around a pixel, the high variance means the behind pixels are not similar. So they should not be matched. But they use softmax to identify the highest variance.\ndoubt: I still believe the variance should be minimized at the correct depth. 计算机视觉中cost-volume的概念具体指什么？ - 知乎\n(2024-02-19) The PointMVSNet paper said MVSNet used \u0026ldquo;soft argmin\u0026rdquo; in sec3.1.\nCost Volume A cost volume 𝐕ᵢ corresponding to a source view\u0026rsquo;s feature map 𝐅ᵢ constitutes multiple warped feature maps 𝐕ᵢ(d), which can be obtained by specifying a depth and a [𝐑|𝐭] that transforms the reference camera to the source view camera.\nAs shown below, a cost volume for a source feature map contains 192 warped feature maps at corresponding depths, while the cost volume for the reference feature map is a replication of itself 192 times.\nF o M S s f z N R F 𝐅 e f a a o e o e e ₁ a p m u a a r f a t r p p r t x m t r e e l c i a i e m f D d e e 𝐕 s l m m p a e d ᵢ g a l p v p g ( o v ' p i i t r d f e s c 𝐅 e 𝐊 h i ) c a ₁ w ᵢ : d R t s [ e o 𝐑 1 ▦ f r ᵢ | c o 𝐭 a f ᵢ 2 ▦ m ] e p ( r l 𝐊 a a ₁ 3 ▦ n [ e 𝐑 s ₁ ⋯ | : t : 𝐭 1 ( o ( ₁ 9 C C w C C ] 2 o = a o = ) s 3 r s 3 ⁻ t 2 d t 2 ¹ , s , V V o N c o N l _ a l _ u d m u d m e e m e e p r e p t a t o h o h f = f = 1 1 v 9 R 9 i 2 e 2 e , f , w H v H i , i , W e W ) w ) A feature map has 32 channels, so a cost volume is a 4-D tensor: (C=32, D=192, H=128, W=160).\n(2023-12-12) Homography isn\u0026rsquo;t used to map the pixel of reference-view feature map to source-view feature map, but instead warp the source feat to reference feat via sampling cooresponding matching points.\nCode Understand A complete demo of homography warping: gist\nAccording to the principle of homography, pixels on the reference view\u0026rsquo;s feature map are mapped linearly onto each source view. Thus, a warped source feature map is sampled (F.grid_sample) from the original source feature map at the mapped pixels.\nConstruct coordinates of pixels:\n1 2 3 4 h, w = 128, 160 vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed uv1 = torch.cat([uv, torch.ones(len(uv), 1)], dim=-1) # (hw,3) Map coordinates on the reference image to a source image.\nThe mapping matrix is 𝐊ᵢ[𝐑ᵢ|𝐭ᵢ] (𝐊₁[𝐑₁|𝐭₁])⁻¹. Therefore, each view calculates its own proj 𝐊ᵢ[𝐑ᵢ|𝐭ᵢ] in advance.\nUse $[\\^𝐑|\\^𝐭]$ to represent the mapping for a pixel from the reference image to a source image, so that the pixel-wise (u,v,1) and depth-wise (d) matmul is:\n$$ \\begin{bmatrix} u\u0026rsquo; d\u0026rsquo; \\\\ v\u0026rsquo; d\u0026rsquo; \\\\ d\u0026rsquo; \\\\ 1 \\end{bmatrix}= \\begin{bmatrix} \\^𝐑 \u0026amp;| \\^𝐭 \\\\ 0 \u0026amp;| 1 \\end{bmatrix} \\begin{bmatrix} ud \\\\ vd \\\\ d \\\\ 1 \\end{bmatrix} = \\^𝐑_{3×3} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}⋅d + \\begin{bmatrix} t₁ \\\\ t₂ \\\\ t₃ \\\\ 1 \\end{bmatrix} $$\nwhere [ud, vd, d,1]ᵀ is homogeneous coordinates for translation, and [u, v, 1]ᵀ is homogeneous coordinates for perspective division to present a 3D scene on a 2D image.\nThe translation $\\^𝐭$ is not affected by d, so it\u0026rsquo;s separated.issue 10\n1 2 3 4 5 6 7 8 src_KRt = intrisics @ extrinsics[:3,:4] proj = src_KRt @ torch.inverse(ref_KRt) rot, trans = torch.split(proj, [3,1], dim=-1) # (4,3), (4,1) rot_uv1 = rot[:3] @ uv1.t() # (3, hw) # depth_values: d = 425. + 1.06*2.5* torch.arange(192).view(1,-1,1) # (1,192,1) rot_uvd = rot_uv1.unsqueeze(1).expand(3,192,-1) *d # (3,192,hw) pix_proj = rot_uvd + trans[:3].unsqueeze(1).expand(3,192,1) The above procedures of determining projection locations are not involving learnable feature vectors. Thus, with torch.no_grad(): runtime context isn\u0026rsquo;t actually effective. And the \u0026ldquo;differentiable\u0026rdquo; is achieved by F.grid_sample(). Sample the source feature map at the mapped coordinates\n1 2 3 4 u_src = 2*(pix_proj[0] / pix_proj[2]) / (w-1) - 1 # (192, hw) v_src = 2*(pix_proj[1] / pix_proj[2]) / (h-1) - 1 uv_src = torch.stack([u_src, v_src], dim=-1) # (192, hw, 2) warped_feat = F.grid_sample(src_feat, uv_src.view(bs, 192*h, w, 2)) Merge all feature maps\u0026rsquo; cost volume into their variance:\n$$ \\begin{aligned} \\frac{ ∑_{i=1}^N (𝐕ᵢ - \\bar{𝐕})² }{N} \u0026amp;= \\frac{∑_{i=1}^N (𝐕ᵢ² - 2𝐕ᵢ \\bar{𝐕} + \\bar{𝐕}² ) }{N} \\\\ \u0026amp;= \\frac{∑_{i=1}^N 𝐕ᵢ²}{N} - \\frac{∑_{i=1}^N 2𝐕ᵢ \\bar{𝐕} }{N} + \\frac{∑_{i=1}^N \\bar{𝐕}² }{N} \\\\ \u0026amp;= \\frac{∑_{i=1}^N 𝐕ᵢ²}{N} - \\frac{ 2\\bar{𝐕} ∑_{i=1}^N 𝐕ᵢ }{N} + \\frac{N \\bar{𝐕}²}{N} \\\\ \u0026amp;= \\frac{∑_{i=1}^N 𝐕ᵢ²}{N} - \\bar{𝐕}² \\end{aligned} $$\n1 c = volume_sq_sum / n_src - (volume_sum / n_src)**2 Compress the 3D 32-channel volume variance to 1-channel scalar for each depth through a 3D UNet:\n3 2 → 8 ↓ 1 6 → 1 ↓ 3 6 2 → 3 ↓ 6 2 4 ✚ ✚ + → 3 6 2 ↑ 4 = 3 1 ↑ 2 6 = 1 8 ↑ 6 = 8 → 1 Note: The original UNet concats the feature maps on the same level, whereas here the feature vectors are added up, like skip connections in restnet. Then, the logits will be normalized by softmax to become a vector of probabilities.\nThe depth of one pixel is a weighted sum (expectation) of 192 depth values.\nA d d e e p p p i t t x h h e s l = : ' s p 4 1 2 5 ✚ p 4 2 2 7 ✚ . 6 p 3 4 3 ✚ 0 ⋯ ✚ p 9 1 3 9 1 2 . 1 Kind of like \u0026ldquo;alpha compositing\u0026rdquo;, where the opacity corresponds to the probability and the color of each filter is the depth value here.\nOr it can be interpreted as a linear interpolation for 192 depth values.\nEach pixel has a distinct depth-probability distribution derived from feature maps.\nPhotometric consistency\nprob_volume → prob_volume_sum4: 4 * average of every 4 depths\u0026rsquo; prob → retrieve the \u0026ldquo;4 times average prob\u0026rdquo; from prob_volume_sum4 according to the depth_index, which is a weighted sum (expectation) of 0 ~ 192 using the prob_volume.\n1 9 2 p r A T o ⋯ v i b g m e e s v e 4 r . * y [ 0 4 p s - ; r u 1 o m 9 b 4 2 ] D e p t h p R e e ⋯ p r t l r a p i n i e e x v e e i l n d e x P c h o o n t f o i m d e e t n r c i e c The quality of depth estimation is measured by the sum of 4 nearby probabilities for a predicted depth. If this probability sum is high, the estimation is reliable. Refine Network\nd ⊕ e p r ↓ 4 t g h b 3 2 3 2 3 2 1 ✚ = 1 T\u0026amp;T dataset doesn\u0026rsquo;t provide normal information to generate mesh surface, and render images for each viewpoint, so the depth map cannot be finetuned by the reference image. Point Cloud Filter Depth Map based on photometric (probability map\u0026gt;0.8) and geometric (re-projection error less than 1 pixel) consistencies\nr □ e f □ s r c Fuse multi-view depth maps based on visibility, and then unproject the unified depth map.\nPlay Profiler The training is slow, almost as slow as NeRF. Why?\nThere is a profile() function provided.\n(2023-12-09)\nDTU The preprocessed DTU includes 79 training scans (scenes) and 22 testing scans. Each scan has the same 49 camera poses with 7 different light conditions.\nTherefore, each scan has 79×49×7 = 27097 pictures serving as reference images. Each reference image is assigned with 10 images taken with nearest poses under the same light condition.\nDuring training, 3 source images are used to estimate the depth map of the corresponding reference image, while 5 source images are used during testing.\nThe depth range is determined based on the depth_min (425.0) and depth_interval * interval_scale (2.5 × 1.06) between two adjacent fronto-parallel planes. For example, if there are 192 planes, the max depth is 425 + (2.5 × 1.06) × (192-1) = 931.15\n1 2 |------|------|------ ... ---| depth: 425.0 427.65 430.3 931.1488 Code warps the source feat map in a backward way by reversing the target coordinates back to the source feat map and then sampling, rather than computing the target coordinates directly from the homography represented with planes.\nThe pixel transferring from the (target) reference plane to a source plane is performed by 3 steps: rotate first, then assign depths, finally add translation.\n1 2 rot@ *d +t (u,v,1) -\u0026gt; (u\u0026#39;,v\u0026#39;,w\u0026#39;) -\u0026gt; (u\u0026#39;d, v\u0026#39;d, w\u0026#39;d) -\u0026gt; (u\u0026#39;d, v\u0026#39;d, w\u0026#39;d) + t $$ \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} → \\begin{bmatrix} u\u0026rsquo; \\\\ v\u0026rsquo; \\\\ w\u0026rsquo; \\end{bmatrix} → \\begin{bmatrix} u\u0026rsquo;d \\\\ v\u0026rsquo;d \\\\ w\u0026rsquo;d \\end{bmatrix} → \\begin{bmatrix} u\u0026rsquo;d+t \\\\ v\u0026rsquo;d+t \\\\ w\u0026rsquo;d+t \\end{bmatrix} → \\begin{bmatrix} \\frac{u\u0026rsquo;d+t}{w\u0026rsquo;d+t} \\\\ \\frac{v\u0026rsquo;d+t}{w\u0026rsquo;d+t} \\\\ 1 \\end{bmatrix} $$\nRefer to eq.(11) in Multi-View Stereo中的平面扫描(plane sweep) - ewrfcas的文章 - 知乎\n$$ R $$\nRead GT Depth (2023-12-19)\nFind the ground-truth depth map for an input image (512, 640).\nThe preprocessced DTU (\u0026ldquo;dtu_training.rar\u0026rdquo;) only contains 1/4 depth maps with size 128x160, aligned with the size of feature maps. So, each file (\u0026ldquo;DTU_Depths_raw/scan1_train/depth_map_0000.pfm\u0026rdquo;) is 80K.\nWhile the folder \u0026ldquo;DTU_Depths_raw/\u0026rdquo; (\u0026ldquo;Depth_raw.zip\u0026rdquo; used in MVSNeRF) includes full-size pfm with the same resolution 1200x1600 as the DTU images. An example: \u0026ldquo;DTU_Depths_raw/scan1/depth_map_0000.pfm\u0026rdquo;: 7.32M\nRead depth map with function read_pfm():\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import imageio import matplotlib.pyplot as plt from pathlib import Path from datasets.data_io import read_pfm depth_path = Path(\u0026#34;../DTU_Depths_raw/scan4_train/depth_map_0008.pfm\u0026#34;) depth_pfm, scale = read_pfm(depth_path) # (128,160), 1.0 depth_png = imageio.imread(\u0026#34;../DTU_Depths_raw/scan4_train/depth_visual_0008.png\u0026#34;) orig_rgb = imageio.imread(\u0026#34;../mvs_training/dtu/Rectified/scan4_train/rect_009_3_r5000.png\u0026#34;) orig_rgb = orig_rgb[::4,::4,:] # reduce 4 times q_x, q_y = int(190/4), int(125/4) # query point plt.text(x=q_x, y=q_y, s=f\u0026#39;{depth_pfm[q_x, q_y]:.2f}\u0026#39;) plt.vlines(x=q_x, ymin=q_y-12.5, ymax=q_y+12.5) plt.hlines(y=q_y, xmin=q_x-12.5, xmax=q_x+12.5) plt.imshow(depth_pfm) # plt.imshow(depth_png, alpha=1) plt.imshow(orig_rgb, alpha=0.2) plt.title(str(depth_path).split(\u0026#39;/\u0026#39;)[2:]) ### LLFF\nAnalyses:\nWhat\u0026rsquo;s the key of its generalizability?\nEnvironment (2024-02-21)\nReferring to this issue#101 and issue#149\n1 2 conda create --name MVSNet python=3.7 cudatoolkit=9.0 cudnn=7.6.4 -c conda-forge pip install -r requirements.txt # tf==1.15.0 Download pretrained model: 1 gdown 1-1JyFT9ClqPO0kz0d_5I1_IHX05paS4h Validate:\n1 2 3 4 python mvsnet/validate.py --regularization \u0026#39;3DCNNs\u0026#39; --validate_set dtu --max_w 640 --max_h 512 --max_d 128 \\ --pretrained_model_ckpt_path \u0026#39;tf_model_dtu/3DCNNs/model.ckpt-150000.data-00000-of-00001\u0026#39; \\ --dtu_data_root \u0026#39;~/Downloads/mvs_training/dtu\u0026#39; \\ --validation_result_path \u0026#39;validation\u0026#39; Related Mutli-view stereo多視角立體重建技術介紹 - AI葵\nMVSNet系列 - MEGVII 黄百川\n【代码精读】开山之作MVSNet PyTorch版本超详细分析 - doubleZ的文章 - 知乎\n【论文精读5】MVSNet系列论文详解-Point-MVSNet - CSDN博客\n多视图几何三维重建实战系列- Cascade-MVSNet - 哔哩哔哩\nFollow-up 2019-2020. 基于深度学习的三维重建——MVSNet系列论文解读 - 机器人3D感知的文章 - 知乎\n","date":"2023-12-03T00:49:00Z","image":"https://ar5iv.labs.arxiv.org/html/1804.02505/assets/x1.png","permalink":"https://zichen34.github.io/writenotes/model/mvs/b-note-mvsnet/","title":"read: MVSNet"},{"content":" 换元法 The area of a cube in orthogonal coordinate system and the spherical coordinate system.\n直角坐标与极坐标的互化中，为什么 dxdy=rdrdθ？ - 予一人的回答 - 知乎\n立体角\n如何推导柱坐标，球坐标的体积元和极坐标的面积元？ - 半个冯博士的回答 - 知乎\nChange of variables | MIT 18.02SC Multivariable Calculus, Fall 2010\nChange of variables (single integral and substitution) | Lecture 30 | Vector Calculus for Engineers\nChange of Variables and the Jacobian - Serpentine Integral\nAn infinitesimal area varies in different coordinate system because shapes of a unit area are distinct. In other words, different coordinate systems have varous scales.\nIn polar coordinate system:\n$$ x = r⋅cosθ \\\\ y = r⋅sinθ $$\nJacobian matrix is a function that depicts how much each dimension of the source space should be scaled to align with another coordinate system at an arbitrary position.\nThe scale factor is the ratio between a target dimension and a source dimension. Thus, by multiplying those factors, the source space will be scaled to the target space.\n$$ 𝐉 = \\begin{bmatrix} \\frac{dx}{dr} \u0026amp; \\frac{dx}{dθ} \\\\ \\\\ \\frac{dy}{dr} \u0026amp; \\frac{dy}{dθ} \\end{bmatrix} $$\nBecause an infinitesimal area $dx⋅dy$ only has magnitude (without direction), the area scaling factor should be just a positive real number.\nTo make the area in the source space to be the same in the target space, the scaling factor should be the ratio of two unit areas.\n$$ \\frac{d(x,y)}{d(r,θ)} $$\nThat is $f(x,y) = g(r,θ) dx dy$, so the integrated areas equals:\n$$ ∬_D f(x,y) dx dy = ∬_E g(r,θ) dx dy = ∬_E g(r,θ) ∂(x,y)/∂(r,θ) drdθ $$\nAnd the calculation of an area is just the cumulative product of every dimensions.\nTherefore, the abosolute value of the determinant of the Jacobian matrix is taken.\n$$ |det(𝐉)| = |dx/dr * dy/dθ - dx/dθ * dy/dr | = |cosθrcosθ - rsinθ(-sinθ)| = |r| $$\n11.9: Change of Variables - Mathematics LibreTexts\n(2023-11-28)\nThe determinant of Jacobian matrix is responsible for the infinitesimal area, not the integrand.\nThe new integrand is just substituting the old variable with the tranformation with new variable.\nf(x,y) -\u0026gt; g(rcosθ, rsinθ)\ndxdy -\u0026gt; r drdθ\nf(x,y) = g(rcosθ, rsinθ) * r\nThe change of variables theorem has two aspects:\nf ( x , y ) C M h u a l n t g i i p n l g y v | a d r e i t a ( b J l ) e | g ( r , θ ) ","date":"2023-11-27T19:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/change_of_variables/","title":"memo: Calc | Change of Variables Theorem"},{"content":"Source video: Continuous time convolution with impulses - ProfKathleen\nConvolution For example, there is a man keep eating all the time. $f(t)$ is the food eaten at time t, and $h(t)$ is the food mass changing function for a period of digestion time t. For example, after the duration $(t-τ)$ of digestion, the amount of food eaten at time $τ$ becomes $h(t-τ)$.\nTherefore, the total food reamined in stomach at time t is f convolving with h:\n$$ y(t) = f(t) * h(t) = ∫_{-∞}^{+∞}f(τ)⋅h(t-τ) dτ $$\nIntuitively, the value $x(τ)$ is scaled by a factor from τ time ago. Thus, the function $h(t)$ looks like in reverse.\nGiven $f = x(t)$ and $h = δ(t)$, the existing amount at time t is:\n$$ y(t) = f * h = ∫_{-∞}^{+∞} x(τ) h(t-τ) dτ $$\nDelta function $δ(τ)$:\n0 1 τ $δ(τ-3)$: Right shift $δ(τ)$ by 3.\nWhere the 0 was becomes -3. Therefore, all the coordinates minus 3, i.e., τ becomes τ-3.\n0 3 1 τ $δ(-τ-3)$: Rverse $δ(τ-3)$:\n- 3 1 0 τ $δ(t-τ-3)$: Shift $δ(τ-3)$ to t, which is a constant.\nWhere the τ-3 = 0 becomes t, so all coordinate plus t:\nt - 3 1 t τ Convolve with Impulse Given $x(t)$\nt The existing amount of a system containing two functions x(t) and δ(t-3) at time t is:\n$$ y(t) = x(t) * δ(t-3) = ∫_{-∞}^{+∞} x(τ) δ(t-τ-3) dτ $$\nMultiply x(τ) by the reversed impluse:\nt - 3 t τ Delta function is 0 except for τ = t-3, therefore, only x(t-3) will be computed:\n$$ y(t) = x(t) * δ(t-3) = ∫_{-∞}^{+∞} x(t-3) δ(t-τ-3) dτ $$\nAnd in that integral, x(t-3) has nothing to do with τ, so it can be pulled outside the integral:\n$$ y(t) = x(t) * δ(t-3) = x(t-3) ∫_{-∞}^{+∞} δ(t-τ-3) dτ \\\\ = x(t-3)*1 \\\\ = x(t-3) $$\nBy convolving with an impulse function, x(t) is shifted (based on the origin) to where the impulse is.\nThis conclusion can be generalized to any f.\nBox function 0 1 f ( t ) 3 t ✶ 0 1 δ ( t - 1 ) t = 0 1 y 2 ( t ) 4 t Two impulses Convolution is linear. Compute separately and sum together.\n0 1 f ( 1 t ) 3 t ✶ 0 1 1 δ ( t - 1 ) 1 4 / 2 t = 1 2 1 y ( 4 t ) 1 6 / 2 t Impulse train Convolving with a infinite sum of delta function: $Σ_{n=-∞}^{+∞} δ(t-n)$\nThe replicas of the signal overlaps:\n0 0 1 1 f y 2 ( 2 ( t t 3 ) 3 ) t t ✶ s u m - 1 3 2 0 0 δ 1 ( t 1 2 - 1 2 y 3 ) ( 3 t ) t = t ","date":"2023-11-20T11:40:00Z","permalink":"https://zichen34.github.io/writenotes/calc/convol_impulses/","title":"memo: Calc | Convolution with Impulses"},{"content":"(Feature image from 3D Gaussian Splatting中的数学推导 - 八氨合氯化钙的文章 - 知乎)\nEWA Splatting - UMD\n(2023-11-20)\nAbs \u0026amp; Intro (2023-11-29)\nThis work combines elliptical splat kernels with a low-pass filter to avoid aliasing without introducing much blurring.\nFootprint function is an integral across a 3D reconstruction kernel along the line of sight, so it computes the contribution of an individual 3D kernel to a screen point. Those contributions form a splat, which can be precomputed, thus improving efficiency of volume rendering.\nTwo types of volume rendering approaches can be distinguished based on the availability of volume data.\nR ( a b S p y a a o c m i C k p n a w l t p s a e s i t r B x i d a e n ) c l g k ( f a r ) B V d a o a c l t k u a m S ( e p f l o a r p t w r t a o i r j n d e g ) c t S ⬯ ⬯ c ⬯ ⬯ r e F e r n o n t In NeRF, each sampled point is queried out from a MLP (smpling an implicit field), and then a pixel is synthesized by alpha-compositing along the viewing ray. Thus, there isn\u0026rsquo;t physical volume data in NeRF.\nIn splatting, each volume data (as a continuous distribution) is projected onto the screen perspectively yielding various splats (滩涂) before rendering (i.e., A pixel emits a ray). Splats don\u0026rsquo;t result in pixels directly as they still need to do combination (to produce the final weight of the 3D point corresponding to the pixel).\nWhen rendering from an arbitrary viewpoint, each splat is integrated along the viewing ray, and then the expectation of all splats\u0026rsquo; contributions is calculated, forming a function that maps a position to an rendered attribute (color).\n(2024-01-21) Their comparison is somewhat like that of a CPU and GPU:\nTwo ways of rendering a point dataset:\nPoints ➔ triangle meshes ➔ \u0026ldquo;surface rendering\u0026rdquo; ➔ image Points ➔ \u0026ldquo;splatting\u0026rdquo; ➔ image P o i n t s T l r a i t a i n o g n u S p l a m t e t s i h n e g s R e d u c t i o n I m a g e Gaussian is a universal rendering primitive for volume splatting and surface splatting.\nPrevious Work Slice-based volume rendering\nProgressive\nfor an arbitrary position in 3D, the contributions from all kernels must sum up to one.\n(2023-11-22)\nSplatting A point cloud (data set) is a discrete sampling of a continuous scene function.\nIf a point is considered as a continuous function, then the point cloud corresponds to a continuous representation on the 2-D screen space corresponding to the 3D scene.\nThus, an image (pixel grid) reflecting the scene comes from sampling the intemediate reconstructed continuous screen, instead of directly sampling the scene, that is, \u0026ldquo;resampling\u0026rdquo; (sampling again).\n(2024-03-16) After reading the code, I figured out the splatting and alpha compositing (forward) are 2 separate procedure. The point cloud is a sample of the scene. By performing splatting, each point is converted into a disc in the (viewing-) ray space. Such than, the opacities of each Gaussian have been determined. When rendering images, given a camera, each pixel emits a ray, and then the sampling points on a ray query their opacities from the ray sapce, to perform alpha compositing. In this way, an image is formed. Therefore, an image is a sampling of the ray space. By considering each point in dataset as a reconstruction kernel, which is a continuous attribute-weight function (e.g., opacity: the weight of color) in the object space, the continuous scene function can be constructed by aggregating all kernels.\nFormally, the weight assigned to a 3D point 𝐮 in the object space for a certain attribute (e.g., color) $f_c(𝐮)$ is a mixture (weighted sum) of weights calculated from each reconstruction kernel:\n$$f_c(𝐮) = ∑_{k∈IN} wₖ rₖ(𝐮)$$\n$rₖ$ is a reconstruction kernel centered at a data point $𝐮ₖ$. A kernel is a function mapping a position to a weight (opacity) for a property (color).\nIn this work, rₖ is chosen to be a 3D Gaussian whose mean vector is the position of a data point 𝐮ₖ, and\u0026hellip; what\u0026rsquo;s the variance?\n$rₖ(𝐮)$ is the \u0026ldquo;weight of an attribute\u0026rdquo; (opacity of a color) of an arbitrary voxel 𝐮 inside the kernel $rₖ$.\nA kernel center 𝐮ₖ will perform transformations as follows:\n⌈ ⌊ P e c x x r c e ₀ ₁ o t n ⌉ ⌋ S j i t c - o e ⬯ r n r e e ★ n ` F p i ⋅ o r n o i t x ` t n e ₂ - t g ⋅ r a ⬯ l L f o i g w l - t ⊗ p e a r h s s ⌈ | ⌊ R s ( s x x x 𝐱 a p N c ₀ ₁ ₂ ₖ y a D a ⌉ | ⌋ c C l e i w n 𝐭 ϕ t / g + r o ) 𝐉 p a ⋅ r n ∆ o s 𝐭 j f ⌈ | ⌊ C s ( t t t 𝐭 a p P ₀ ₁ ₂ ₖ m a e ⌉ | ⌋ e c r r e s a p 𝐖 φ t ( e r E c 𝐮 v a y t + i n e i 𝐝 e s ) v w f e ) ⌈ | ⌊ u u u O s ₀ ₁ ₂ 𝐮 b p ⌉ | ⌋ ₖ j a e c c e t Prime $\u0026rsquo;$ indicates the ray space.\nx₂ is the Euclidean distance (L2 norm) from the 3D point 𝐱 to the projection center for integrating the total opacity given by a kernel.\nRay space is a \u0026ldquo;plane\u0026rdquo; because the x₂ is not a practical depth anymore and only used for object culling. And the screen is just a footprint table:\nThus essentially, ray space is the projection plane, while screen is another space recording the contributions of a 3D kernel to each 2D point.\nRoughly, the main focus on the ray space is on the near plane. For example, the pixel covered by a kernel projection is determined on the near plane.\n(2023-11-24)\nResampling \u0026ldquo;Ideal sampling\u0026rdquo; means no aliasing and the original continuous signal can be reconstucted from the sampled signal exactly.\nAntialiasing Spatial signal (Screen function) ➔ Samples (Image) ➔ Frequency response (Coefficient of each basis) given by Fourier Transform.\n0 f ₁ f ₂ f ₃ T f ₙ x 2 π ⁄ T ⋮ w Sampling in time (or spatial) domain: the continuous signal is multiplied by an impulse train with a period T:\n$$ y(t) = x(t) ⋅ ∑_{k=-∞}^{∞} δ(t-kT) = ∑_{k=-∞}^{∞} x(t) ⋅ δ(t-kT) $$\nMultiplication in time domain corresponds to convolution ⊗ in frequency domain.\n$$ \\begin{aligned} Y(ω)\u0026amp;= \\frac{1}{2π} \\left(X(ω) ⊗ \\frac{2π}{T} ∑_{k=-∞}^{∞} δ(ω-k \\frac{2π}{T}) \\right)\\\\ \u0026amp;= \\frac{1}{T} ∫_{-∞}^{∞} X(λ)⋅∑_{k=-∞}^{∞} δ(ω-k \\frac{2π}{T}-λ) dλ \\\\ \u0026amp;= \\frac{1}{T} ∑_{k=-∞}^{∞} X(ω-k\\frac{2π}{T}) \\end{aligned} $$\nThe frequency spectrum Y(ω) of the sampled signal is a sum of the original signal\u0026rsquo;s spectrum X(ω) shifted according to the corresponding impulse train in the frequcy domain.\nRefer to: Fourrier transform of an impulsion train - Inria\nSampling with an impulse train: Lecture 12 - Stanford University\n$2π/T$ is the frequency of the sampling impulse. So, the space between 2 spectrum replicas is $\\frac{2π}{T}$.\nDenoting the highest frequency of the temporal signal as $w_m$, to fully separate 2 neighbor spectrums, the interval $2π/T$, i.e., the sampling frequency $w_s$, must be larger than (no equal) $2w_m$. Otherwise, spectrum replicas will overlap and an individual intact spectrum can\u0026rsquo;t be selected out by a box function to reconstruct the temporal signal.\nConversely, given a temporal impulse train with a sampling frequency $w_s$, the highest frequency of the continuous signal to be sampled shouldn\u0026rsquo;t exceed $\\frac{w_s}{2}$, which is called the Nyquist frequency.\nTo match the Nyquist frequency of the discrete desired grid, the time-domin signal can pass a low-pass filter before sampling, in contrast to increasing sampling frequency causing more computation cost.\nMoreover, the width of the low-pass filter to be convolved is a tradeoff for computation efficiency. Thus, aliasing is inevitable.\nMip-Splatting (231127) adds a constrains to the size of kernels based on the $w_s$ induced by the input views, and changes the 2D Gaussian low-pass filter to a 2D Mip filter.\nThus, alliviate the dilation effect when zooming out and the high frquency artifacts when zooming in.\nCont. Screen The \u0026ldquo;weights-mixture\u0026rdquo; function $f_c(𝐮)$ determinating an attribute\u0026rsquo;s weight at an arbitrary position 𝐮 in source space are transformed as follows:\nI D m i a s g c e r e t A C M I T e l o u m r p m l p a h p t u i a o i l n s p s i l e t y 2 C e g D o _ n c S t ' c i ( r n 𝐱 e u ) e o I k n u n e s t r e n g e r l a s t e L f o i w l - t p e a r s s ⦿ ⦿ ⦿ ⦿ g 3 S ⦿ _ D p ⦿ c a ⦿ ( R c ⦿ 𝐱 a e ) y P P e r r o s j p e e c c t t i v e ⦿ ⦿ ⦿ 3 C ⦿ f D o ⦿ _ n ⦿ c s t ⦿ ( c . ⦿ 𝐮 e ) n e The coordinates 𝐱 in the 3D ray space contain the 2D perspectively-projected screen coordinates (t0/t2, t1/t2) and the 3rd dimension is set to the L2 norm of 𝐭, which is the coordinates in the camera space.\n𝐱 is also used to refer to the corresponding screen point.\nThree operations in splatting equation:\nFor splatting approach, a scene consists of reconstruction kernels spreading out the space. Thus, a volume at 𝐮 in the source space is a combination of contributions from all reconstruction kernels:\n$$ f_c(𝐮) = ∑_{k∈ N} wₖ rₖ(𝐮) $$\n𝐮 is a position in the source (object) space; $rₖ$ is a reconstruction kernel centered at 𝐮ₖ. $rₖ(𝐮)$ is the weight for an attribute stored in the location 𝐮 computed according to $rₖ$; $wₖ$ is the coefficient of each weight to produce a unified weight for an attribute on the location 𝐮; An attribute-weight function $f_c$ in the source space will be finally projected onto the screen as a 2D continuous function that produces the weight for an attribute on any screen position 𝐱:\n$$ g_c(𝐱) = \\{ P( f_c ) \\}(𝐱) = \\left\\{ P \\left( ∑_{k∈ N} wₖ rₖ \\right) \\right\\}(𝐱) $$\n𝐱 is not a pixel, because the screen is still continuous. A discrete image is sampled from the screen via an impulse train: $g(𝐱) = g_c(𝐱) i(𝐱)$ I use curly braces to imply a function. Because a non-affine projection operation can be approximated as a linear transformation through the 1st-order Taylor expansion, the weighted sum and linear projecting operations can be flipped:\n$$ g_c(𝐱) = ∑_{k∈ N} wₖ⋅ P( rₖ) ) (𝐱) $$\nThereby, each kernel performs projection first, and then combined together. Commutative: switching operators means switching the sequence of operations. To avoid aliasing, the screen function before being sampled to an image needs to pass a low-pass filter $h$ to meet the Nyquist frequency:\n$$ \\begin{aligned} \\^g_c(𝐱) \u0026amp;= g_c(𝐱) ⊗ h(𝐱) \\\\ \u0026amp;= ∫_{-∞}^{∞} g_c(\\bm η)⋅h(𝐱- \\bm η) d \\bm η \\\\ \u0026amp;= ∫_{-∞}^{∞} ∑_{k∈ N} wₖ⋅ P(rₖ)(\\bm η)⋅h(𝐱-\\bm η) d\\bm η \\\\ \u0026amp;= ∑_{k∈ N} wₖ ∫_{-∞}^{∞} P(rₖ) (\\bm η) ⋅ h(𝐱-\\bm η) d\\bm η \\\\ \u0026amp;= ∑_{k∈ N} wₖ ⋅ P(rₖ) (𝐱) ⊗ h(𝐱) \\end{aligned} $$\nEvery projected kernel (mapping a position to a weight) is filtered by h. The weighted sum is done after projection and convolution. An ideal resampling kernel: $$ ρₖ(𝐱) = ( P(rₖ) ⊗ h )(𝐱) $$ Therefore, any location in the 2D continuous screen space is a combination of the projected and filtered reconstruction kernels $ρₖ$ evaluated at that location.\n(2023-11-25)\nRendering (In NeRF,) Using opacity (self-occlusion) $αₜ$ to compute the color based on alpha compositing:\n$$\\rm c = ∫_{t=n}^f Tₜ αₜ cₜ dt, \\quad Tₜ = ∏_{s=n}^{t-1} (1-αₛ) ds $$\nIf using volume density σ to compute, the pixel color is:\n$$ \\rm c = ∫_{t=n}^f Tₜ (1 - e^{-σₜ δ}) cₜ dt, \\quad Tₜ = e^{- ∫_{s=n}^{t-1} σₜ δ ds} $$\nAlpha = $1 - e^{-σδ}$, when the density σ is 0, the alpha (opacity) is 0: The filter itself doesn\u0026rsquo;t show its color. It\u0026rsquo;s as if there were no particles there, and the rays pass through without any changes.\nδ is a unit interval on the ray. (The width of a filter.)\nNeRF can use 1 pixel to observe all points in the space, while splatting use all points to composite 1 pixel.\nC a m e r N a e □ R s F p a c e R a E y y S e ⦿ p s ⦿ - l p - a a ⦿ ⦿ t c - t e S - - - - i c ❚ ❚ ❚ ❚ □ n r g e P e r n o j I e n c t t e i g o r n a t c e e n t e r This paper uses the low albedo approximation of the volume rendering, so that the intensity (a 1D attribute) of a screen point 𝐱 corresponding to a ray with length 𝐿 is computed as:\n$$ I(𝐱) = ∫_{ξ=0}^L c(𝐱,ξ) ⋅f_c\u0026rsquo;(𝐱,ξ) ⋅ e^{-∫_{μ=0}^ξ f_c\u0026rsquo;(𝐱,μ)dμ}dξ $$\nL is the distance from eye to the projection center on screen.\n▹ $c(𝐱,ξ)$ is the intensity on a 3D point (𝐱,ξ) in ray space. 𝐱 is (x0,x1), ξ is x2.\n▹ $f_c\u0026rsquo;(𝐱,ξ)$ is the point\u0026rsquo;s opacity (weight for color) in the ray space. ▹ The exponential term is the accumulated transmittance prior to the point at ξ. Why is it in this form? Is a transmittance $e^{-f_c\u0026rsquo;(𝐱,ξ)}$?\nK e r n ( I l s e x 3 n i i l ₀ D t n g , e e h r x R g t ₖ ₁ a r o , ' , y a f ( ξ t L 𝐮 ) ℮ S e ) ᵀ y p ℮ a ⦿ c - e ⦿ - - - ⦿ - - - - - S | • ❚ ❚ ❚ | c ↙ r P 𝐱 e r : e o ( n j c S x e e c ₀ c n V r , t t i e x i e e e ₁ o r w n ) n i ᵀ n c g o o r r a d y i n a t e s The 3D ray space is not perspective because depth has been divided and then reset to the distance from the projection center. Thus, kernels in ray space are projected onto the screen orthogonally by just omitting their depth.\nAnd the probing line L from eye to the projection center are projected orthogonally onto the screen as well.\nTherefore, integrating along L in 3D space corresponds to integrating the viewing ray between (x₀,x₁) and the projection center on 2D screen.\nRewrite the volume rendering equation using projected kernels in the ray space.\nSince the position 𝐱 in the ray space is the projection of position 𝐮 in the source space after a viewing transformation φ (w2c) and a projective transformation ϕ (Note: NDC= projection with depths kept + scaling. Here is NDC without scaling), there are:\n$$ \\begin{aligned} 𝐱 \u0026amp;= ϕ(φ(𝐮)) \u0026amp; \\text{Project a location to ray space} \\\\ 𝐮 \u0026amp;= φ⁻¹(ϕ⁻¹(𝐱)) \\\\ f_c(𝐮) \u0026amp;= ∑_{k∈IN} wₖ rₖ(𝐮) \u0026amp;\\text{Combination in object space} \\\\ f_c\u0026rsquo;(𝐱) \u0026amp;= ∑_{k∈IN} wₖ rₖ(φ⁻¹(ϕ⁻¹(𝐱))) \u0026amp;\\text{Change of variable} \\\\ rₖ\u0026rsquo;(𝐱) \u0026amp;= rₖ(φ⁻¹(ϕ⁻¹(𝐱))) \u0026amp; \\text{Kernel k in the ray space} \\\\ f_c\u0026rsquo;(𝐱) \u0026amp;= ∑_{k∈IN} wₖ rₖ\u0026rsquo;(𝐱) \u0026amp;\\text{Written in a consistent form} \\end{aligned} $$\nWeights-mixture function $f_c\u0026rsquo;(𝐱)$ is the corresponding representation of $f_c(𝐮)$ in the ray space, merely substituting variables without scaling the axes.\nThus, the weight for an attribute on 𝐱 is a summation of all projected kernels. For example, the assigned opacity at 𝐱 in the ray space is a weighted sum of all opacities evaluated from each kernel. (Gassian mixture model)\nSubstituting the opacity $f_c\u0026rsquo;(𝐱)$ into the rendering equation:\n$$ \\begin{aligned} I(𝐱) \u0026amp;= ∫_{ξ=0}^L c(𝐱,ξ)⋅f_c\u0026rsquo;(𝐱,ξ) ⋅ e^{-∫_{μ=0}^ξ f_c\u0026rsquo;(𝐱,μ)dμ}dξ \\\\ \u0026amp;= ∫_{ξ=0}^L c(𝐱,ξ)⋅ ∑_{k∈IN} wₖ ⋅rₖ\u0026rsquo;(𝐱,ξ) ⋅ e^{-∫_{μ=0}^ξ ∑_{j∈IN} wⱼ rⱼ\u0026rsquo;(𝐱,μ) dμ} ⋅dξ \\\\ \u0026amp;= ∑_{k∈IN} wₖ \\left( ∫_{ξ=0}^L c(𝐱,ξ)⋅ rₖ\u0026rsquo;(𝐱,ξ) ⋅ e^{-∑_{j∈IN} wⱼ ⋅∫_{μ=0}^ξ rⱼ\u0026rsquo;(𝐱,μ) dμ} ⋅dξ \\right) \\\\ \u0026amp;= ∑_{k∈IN} wₖ \\left( ∫_{ξ=0}^L c(𝐱,ξ)⋅ rₖ\u0026rsquo;(𝐱,ξ) ⋅ ∏_{j∈IN} e^{- wⱼ⋅∫_{μ=0}^ξ rⱼ\u0026rsquo;(𝐱,μ) dμ} ⋅dξ \\right) \\end{aligned} $$\nA point (𝐱,ξ) in ray space indicates the 3rd dim (x₂) is ξ and the first 2 dims (x₀,x₁) is 𝐱.\nThe rendered color is a composition of each point\u0026rsquo;s color c(𝐱,ξ) multiplied by a weight given by the weights-mixture function $f_c\u0026rsquo;$ at the point on the line of sight 𝐿. And the weights-mixture function $f_c\u0026rsquo;$ is a combination of the evaluations of all kernels $rₖ$ in the ray space.\nCommutative: Sum all kernels evaluated at a point followed by integrating all points on a ray = integrate all the points in each kernel followed by summation for all kernels.\nThe integral for the evaluations at all ray points, considering rₖ as the kernel to be evaluated: $qₖ(𝐱) = ∫₀ᴸ rₖ\u0026rsquo;(𝐱,ξ)dξ$ is called the footprint function of rₖ, which is used as the approximate opacity for all voxels in the support of a kernel.\nAnd the exponential product term can be approximated as $∏ⱼᵏ⁻¹(1-wⱼqⱼ(𝐱))$ by applying Taylor expansion (see below), such that it can be regarded as the accumulated transmittance very harmoniously.\n(2023-12-31) In this way, the rendering equation aligns with the alpha compositing:\nequation opacity transmittance volume rendering α (1-α) splatting qₖ (1-wₖqₖ) A projected rendered attribute in the screen space, such as the above $I(𝐱)$, is collectively referred to as $g_c(𝐱)$.\nApproximations To simplify the computation of $g_c(𝐱)$, 4 approximations are applied.\nLocal support: the regime of each 3D kernel is range-limited and not overlapped with other kernels on the ray.\nlocal support\nL s 3 i i D N n g k o e h e R t r a O o , n y v f L e l s e l S o u r s E p c p l y a a p a e c l o p ⦿ e r t ⦿ S - c • ❚ ❚ ❚ ❚ ❚ ❚ r e P c E e r e x n o n t j t e e e n c r t t i o n The attribute (e.g., color) in the local support region of a 3D kernel along a ray is assumed to be constant. Specifically, the $c(𝐱,ξ)$ over the line of sight (for all ξ) is constant, while other rays (i.e., 𝐱) passing through the kernel can have different colors. Hence, c(𝐱,ξ) can be put outside of the ray integral.\n$$g_c(𝐱) = ∑_{k∈IN} wₖ cₖ(𝐱) \\left( ∫_{ξ=0}^L rₖ\u0026rsquo;(𝐱,ξ) ⋅ ∏_{j∈IN} e^{- wⱼ⋅∫_{μ=0}^ξ rⱼ\u0026rsquo;(𝐱,μ) dμ} ⋅dξ \\right)$$\nAssume the attenuation factor of voxels in a kernel along a ray is constant, i.e., the transmittance isn\u0026rsquo;t dependent on previous voxels passed through by the ray, but equals the accumulation of all voxels. Thus, the upper bound of the integral becomes L:\n$$\\rm exp(-∫_{μ=0}^{ξ} f_c\u0026rsquo;(𝐱, μ) dμ) → exp(-∫_{μ=0}^{L} f_c\u0026rsquo;(𝐱, μ) dμ)$$\nIn this way, the transmittance isn\u0026rsquo;t restricted by ξ of the outer rendering integral and can be pulled outside.\n$$g_c(𝐱) = ∑_{k∈IN} wₖ cₖ(𝐱) ( ∫_{ξ=0}^L rₖ\u0026rsquo;(𝐱,ξ)dξ ) ⋅ ∏_{j∈IN} e^{- wⱼ⋅∫_{μ=0}^L rⱼ\u0026rsquo;(𝐱,μ) dμ} $$\nThe integral of opacities of all voxels rₖ\u0026rsquo;(𝐱,ξ) on the line of sight L belonging to a kernel is denoted as:\n$$ qₖ(𝐱) = ∫_{x₂=0}^L rₖ\u0026rsquo;(𝐱,x₂) dx₂ $$\nqₖ(𝐱) is the footprint function for a 3D kernel $rₖ\u0026rsquo;(𝐱,x₂)$ in the ray space.\nqₖ(𝐱) is a 2D function that specifies the contribution (total opacity) of a 3D kernel to a screen location.\nR a y s e I p y n a e t c e e ⋯ g r a 3 t D e s ⋯ r u ₖ p ⋯ ( p 𝐱 o ⦿ ⋯ , r ξ t ⋯ ) o ⋯ a f l o r n ₖ - g ⋯ t h e s l c i r ⌈ ⌊ n e x x e e ₀ ₁ n ⌉ ⌋ o f s i g h t The 3rd dimension (x₂) of a kernel has been integrated out. Thus, inputting a 2D coordinates, it returns an attribute\u0026rsquo;s weight resulting from the corresponding kernel.\n(2023-12-31) The objects to be blended differ between EWA and NeRF.\na r e p r o P j o e c i c a n t m t e e s d r a ( i 1 n v e r s e l y ) L p V i i r i n n o s e e j c w 3 e r i D s c e ∫ f n e t e l o g r g e n i o a m d n t r y e e p a n o r y s t n s i s p s t e n a o g t i c a n e r s i t e c s e r g e r e a n l s (2024-01-04) In NeRF, points on a ray are composited, whereas EWA blends \u0026ldquo;line segments\u0026rdquo;. (refer to 八氨)\nIn EWA, scene voxels are grouped into different ellipsoids, and then rendering for ellipsoids, instead of volume-wise compositing. The scene element is a line integral of an ellipsoid, analogous to a particle in the volume rendering, thereby boosting efficiency.\nIn this ellipsoidal scenario, the alpha of each ellipsoid is the footprint function (opacity integral over the line of sight).\nAnd 3D kernels in the dataset can be preintegrated before rendering.\nWith the opacity precomputed, the alpha compositing is a 2D convolution over the line of sight L for splatting, whereas ray-casting needs 3D convolutions. Thus, splatting is more efficient.\nWith this approximation, volume rendering equation becomes:\n$$g_c(𝐱) = ∑_{k∈IN} wₖ ⋅cₖ(𝐱)⋅ qₖ(𝐱) ⋅ ∏_{j∈IN} e^{- wⱼ⋅qⱼ(𝐱)} $$\nAssume all kernels are ordered back to front (statements inconsistent in the paper), so that when computing the transmittance, only the opacities of kernels prior to the current kernel need to be accumulated.\ne y e b a c r k ' ₖ ⦿ ₋ ₁ ( r 𝐱 ' ⦿ ⋅ , ₖ ξ ) r ' ₖ ⦿ ₊ f ₁ r o n t s c r e e n $$g_c(𝐱) = ∑_{k∈IN} wₖ ⋅cₖ(𝐱)⋅ qₖ(𝐱) ⋅ ∏_{j=0}^{k-1} e^{- wⱼ⋅qⱼ(𝐱)} $$\nThe exponential term is approximated by its 1st-order Taylor expansion based on $e^{-x} = 1-x$ evaluated at x=0:\n$$ e^{- wⱼ⋅qⱼ(𝐱)} \\approx 1-wⱼ qⱼ(𝐱) \\\\ g_c(𝐱) = ∑_{k∈IN} wₖ ⋅cₖ(𝐱)⋅ qₖ(𝐱) ⋅ ∏_{j=0}^{k-1} (1-wⱼ qⱼ(𝐱)) $$\n$g_c(𝐱)$ is the splatting equation representing the continuous screen.\nConsequently, with the above 4 assumptions, the point-based splatting becomes the same form as the NeRF-style volumetric rendering, because they\u0026rsquo;re both based on alpha compositing (image formation model). (Refer to 3DGS)\nCombine Filter Screen is a continuous 2D representation of the scene. A discrete image grid can be obtained by sampling it with an impulse train. To avoid aliasing when sampling the screen function, each projected 2D splat needs to be filtered to the Nyquist frequency of the output image by passing a proper loss-pass filter $h(𝐱)$.\n$$ \\begin{aligned} \u0026amp;\\^g_c(𝐱) = g_c(𝐱) ⊗ h(𝐱) \\\\ \u0026amp;= ∫_{η=-∞}^{∞} ∑_{k∈IN} wₖ ⋅ cₖ(\\bm η) ⋅ qₖ(\\bm η) ⋅ ∏_{j=0}^{k-1} (1-wⱼ qⱼ(\\bm η)) ⋅ h(𝐱-\\bm η) d\\bm η \\\\ \u0026amp;= ∑_{k∈IN} wₖ ⋅ ∫_{η=-∞}^{∞} cₖ(\\bm η) ⋅ qₖ(\\bm η) ⋅ ∏_{j=0}^{k-1} (1-wⱼ qⱼ(\\bm η)) ⋅ h(𝐱-\\bm η) d\\bm η \\\\ \\end{aligned} $$\ncₖ(𝛈) is the color (emission coefficient) of the ray point (𝛈,ξ) calculated relative to the kernel rₖ. It\u0026rsquo;s a function of rays 𝛈.\nqₖ(𝛈) is the contribution (total opacity) of the kernel rₖ to the screen point 𝛈.\nThe cumulative product term is the transmittance (attenuation) of each voxel (𝛈,ξ) in the kerenel rₖ.\nBecause the color cₖ(𝛈) and the transmittance of a voxel (𝛈,ξ) in a kernel have no explicit formula to be integrated, two approximations are introduced to reach an analytical expression to compute.\nColor of any voxel on any ray 𝛈 in the support region of a 3D kernel rₖ is a constant cₖ:\n$$cₖ(\\bm η) = cₖ$$\nTransmittance of each voxel in the kernel rₖ is a constant.\n$$∏_{j=0}^{k-1} (1-wⱼ qⱼ(\\bm η)) \\approx oₖ $$\nThe transmittance variation inside a 3D kernel is omitted, so the sole splatting equation can\u0026rsquo;t avoid edge aliasing, which needs to be solved by other techniques. Therefore after filtering, the footprint function becomes band-limited in frequency domain:\n$$∫_{η=-∞}^{∞} qₖ(\\bm η) h(𝐱-\\bm η)d\\bm η$$\nAnd the antialiased splatting equation (screen function):\n$$ \\begin{aligned} \\^g_c(𝐱) \u0026amp;= g_c(𝐱) ⊗ h(𝐱) \\\\ \u0026amp;≈ ∑_{k∈IN} wₖ⋅cₖ⋅oₖ⋅∫_{η=-∞}^{∞} qₖ(\\bm η) ⋅ h(𝐱-\\bm η) d\\bm η \\\\ \u0026amp;= ∑_{k∈IN} wₖ⋅cₖ⋅oₖ⋅ (qₖ ⊗ h)(𝐱) \\end{aligned} $$\nThe formula can be interpreted as a weighted sum (combination, expectation) of footprint function in the 2D screen space. Thus, the primitives are the projected, prefiltered reconstruction kernels, so called ideal volume resampling filter:\n$$ρₖ(𝐱) = cₖ⋅oₖ⋅(qₖ ⊗ h)(𝐱)$$\nInstead of band limiting the output function $g_c(𝐱)$ directly, we band limit each footprint function qₖ separately.\n(2023-11-26)\nEWA Splats The ideal volume resampling filter (splat primitive) is obtained after 3 steps: projection (viewing + projective transformation), footprint function, and convolving with a Gaussian loss-pass filter.\nGaussian will yields a Gaussian after affine transformations, convolving with another Gaussian, and integrating along one of its dimensions.\nTherefore, 3D Gaussian is used as 3D reconstruction kernels (in object space) to produce an analytical expression of the 2D Gaussian resampling filter $ρₖ(𝐱)$ in screen space. So that splatting equation is an elliptical weighted average (EWA).\nA reconstruction kernel centered at datapoint 𝐮ₖ in the object space is defined as the 3D Gaussian distribution with a mean vector 𝐮ₖ and a covariance matrix 𝐕ₖ:\n$$rₖ(𝐮) = \\frac{1}{(2π)^{3/2} det(𝐕ₖ)^{1/2}} e^{-½(𝐮-𝐮ₖ)^T 𝐕ₖ^{-1} (𝐮-𝐮ₖ)}$$\n(2023-11-28)\nProject Kernels Project a 3D Gaussian distribution from object space to ray space through a viewing transformation and a projective transformation.\nViewing transformation φ Transform a Gaussian distribution from source (object) space to camera space through an affine mapping: $𝐭=𝐖 𝐮+𝐝$. If 𝐖 is invertible, there is $𝐮 = 𝐖⁻¹(𝐭-𝐝)$.\nGiven a probability density function $f_𝐮$ of the variable 𝐮, and a linear mapping 𝐭=𝐖 𝐮+𝐝, by substituting the 𝐮 of $f_𝐮(𝐮)$ with 𝐖⁻¹(𝐭-𝐝), the result expression is a new distribution $f_𝐭$ represented in 𝐭\u0026rsquo;s space:\n$$f_𝐭(𝐭) = \\frac{1}{(2π)^{3/2} det(𝐕ₖ)^{1/2}} e^{-½(𝐖⁻¹(𝐭-𝐝)-𝐮ₖ)^T 𝐕ₖ^{-1} (𝐖⁻¹(𝐭-𝐝)-𝐮ₖ)}$$\nClarify: Subscripts indicate which variable\u0026rsquo;s distribution the function is depicting. The argument in parentheses is the variable building the function (in the associate space). On the other hand, to represent the PDF $f_𝐮$ in another space with a new variable, e.g., 𝐭 associating with $f_𝐮(𝐭)$, rather than the generic $f_𝐮(𝐮)$. After substituting variable, the result representation in 𝐭\u0026rsquo;s space is $f_𝐭(𝐭)$ as stated above.\nBut the scale is mismatched. According to the Change of Variables theorem, the \u0026ldquo;absolute value of the determinant of the Jacobian matrix\u0026rdquo; must be multiplied as a scaling factor to align different units between 𝐭 and 𝐮.:\n$$f_𝐮(𝐭) = f_𝐭(𝐭) ⋅|det(\\frac{∂𝐮}{∂𝐭})| = f_𝐭(𝐭)⋅ |det(𝐖⁻¹)|$$\nAnd the new variable 𝐭\u0026rsquo;s PDF can be written as: $$f_𝐭(𝐭) = \\frac{1}{|det(𝐖⁻¹)|} f_𝐮(𝐭)$$\n, corresponding to the equation (21) in the paper: $G_𝐕^n (ϕ⁻¹(𝐮) - 𝐩) = \\frac{1}{|𝐌⁻¹|} G_{𝐌 𝐕 𝐌ᵀ}^n(𝐮-ϕ(𝐩))$\n(2023-11-28) doubt: I\u0026rsquo;m confused in eq. (21), is the LHS the $f_𝐭(𝐭)$? Why do they care the unscaled distribution?\n(2023-12-02) If the |det(J)| is missing and just substituting variable, the new distribution $f_𝐭(𝐭)$ can\u0026rsquo;t be written as a Gaussian (?). So, the author used the transformed \u0026ldquo;non-\u0026ldquo;Gaussian divided by the factor, such that the new distribution is a Gaussian as well.\n(2023-12-31) I forgot the meaning of the above comment on 2023-12-02. The eq. (21) can be understood as:\nGiven an affine mapping: 𝐮 = 𝐌 𝐱+c = ϕ(𝐱), and define its inverse as: 𝐱 = 𝐌⁻¹(𝐮-c) = ϕ⁻¹(𝐮)\nGiven the distribution of 𝐱 is $G_𝐕^n (𝐱 - 𝐩)$, after applying the affine mapping 𝐌 𝐱+c, the mean and variance will become: ϕ(𝐩) and 𝐌 𝐕 𝐌ᵀ. So the new distribution is represented as $G_{𝐌 𝐕 𝐌ᵀ}^n (𝐮 - ϕ(𝐩))$.\nAccording to Change of Variables, the relations are:\n$$ Gⁿ_𝐕(𝐱-𝐩) \\overset{substitute}{→} Gⁿ_𝐕(ϕ⁻¹(𝐮) - 𝐩) \\overset{times |det(J)|}{→} G_𝐕^n(ϕ⁻¹(𝐮) - 𝐩) |𝐌⁻¹| = G_{𝐌 𝐕 𝐌ᵀ}^n (𝐮 - ϕ(𝐩)) $$\nSo the eq.(21) is indeed the unscaled transformed Gaussian in another space.\ndoubt: The unscaled, merely variable-changed distribution $Gⁿ_𝐕(ϕ⁻¹(𝐮) - 𝐩)$ is also a Guassian (?), as the 3 axes are stretched by the affine mapping linearly and separately, so the overall shape of Gaussian will be kept. And the |det(J)| is just a scalar coefficient ensuring consistent area quantity.\nSimilarly, the case of projecting a kernel from source space into camera space ($𝐭=𝐖 𝐮+𝐝$) is changing mean vector and covariance matrix:\n$rₖ(𝐭)$ is the transformed representation of the kernel $k$ in the camera space, matching the above $f_𝐮(𝐭)$, so it equals the variable-changed $rₖ(𝐮)$ multiplied with $|det(∂𝐮/∂𝐭)|$:\n$$ \\begin{aligned} \u0026amp;rₖ(𝐭) = rₖ(𝐖⁻¹(𝐭-𝐝)) ⋅ |det(𝐖⁻¹)| \\\\ \u0026amp;= \\frac{1}{(2π)^{3/2} \\sqrt{det(𝐕ₖ)} |det(𝐖)|} e^{-½(𝐖⁻¹(𝐭-𝐝)-𝐮ₖ)ᵀ𝐕ₖ⁻¹(𝐖⁻¹(𝐭-𝐝)-𝐮ₖ )} \\\\ \u0026amp;= \\frac{1}{(2π)^{3/2} \\sqrt{det(𝐕ₖ)} |det(𝐖)|} e^{-½(𝐖⁻¹(𝐭-𝐝-𝐖𝐮ₖ))ᵀ𝐕ₖ⁻¹(𝐖⁻¹(𝐭-𝐝-𝐖𝐮ₖ) )} \\\\ \u0026amp;= \\frac{1}{(2π)^{3/2} \\sqrt{det(𝐕ₖ)} |det(𝐖)|} e^{-½(𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)))ᵀ𝐕ₖ⁻¹(𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)) )} \\\\ \u0026amp;= \\frac{1}{(2π)^{3/2} ⋅ \\sqrt{det(𝐕ₖ)} ⋅ |det(𝐖ᵀ)|^½ ⋅ |det(𝐖)|^½} \\\\ \u0026amp;\\quad ⋅ e^{-½(𝐭-(𝐖𝐮ₖ+𝐝))ᵀ(𝐖⁻¹)ᵀ𝐕ₖ⁻¹𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)) } \\\\ \u0026amp;= \\frac{1}{(2π)^{3/2} \\sqrt{|det(𝐖 𝐕ₖ 𝐖ᵀ)}|} e^{-½(𝐭-(𝐖𝐮ₖ+𝐝))ᵀ(𝐖ᵀ)⁻¹𝐕ₖ⁻¹𝐖⁻¹(𝐭-(𝐖𝐮ₖ+𝐝)) } \\\\ \u0026amp;= N(𝐖𝐮ₖ+𝐝, 𝐖 𝐕ₖ 𝐖ᵀ) \\end{aligned} $$\nHence, after performing an affine transformation, the distribution in the new space has a new mean vector 𝐖𝐮ₖ+𝐝, i.e., the original mean 𝐮ₖ is shifted by the affine mapping, and the variance matrix becomes 𝐖 𝐕ₖ 𝐖ᵀ.\nDerivation refers to: Linear Transformation of Gaussian Random Variable (Found by DDG with keywords: \u0026ldquo;affine transform for Gaussian distribution\u0026rdquo;)\nFact: $(Wᵀ)⁻¹ = (W⁻¹)ᵀ$\nAffine property for x~N(0,1): STAT 830 The Multivariate Normal Distribution - SFU\nThe Multivariate Gaussian Distribution - Stanford\nProjective transformation ϕ (2024-01-04) Summary: The non-linear projective transformation is approximated via Taylor expansion. In this paper, a 3D point (x,y,z) is projected onto a plane perspectively as follows: the x, y coordinates are divided by z, and the z value is then reset to the Euclidean norm $‖𝐭‖$ for object culling.\nIn this way, points in the camera space are transformed into the 3D ray space.\n$$ \\begin{bmatrix}x₀ \\\\ x₁ \\\\ x₂ \\end{bmatrix} = ϕ(\\begin{bmatrix}t₀ \\\\ t₁ \\\\ t₂ \\end{bmatrix}) = \\begin{bmatrix}t₀/t₂ \\\\ t₁/t₂ \\\\ \\|(t₀,t₁,t₂)ᵀ\\| \\end{bmatrix} $$\nAfter perspective division with pixel coords obtained, x₂ is supposed to be 1, but it\u0026rsquo;s assigned with ‖𝐭‖₂.\nDenote the L2 norm of 𝐭 as l: $‖(t₀,t₁,t₂)ᵀ‖ = l = \\sqrt{t₀²+t₁²+t₂²}$, i.e., the magnitude of the line connecting a point 𝐭 to the projection center on the screen.\nThe dpeth is not confined to [-1,1] like NDC. The points on a single ray all have the same (t₀,t₁)ᵀ, thus, the depth $x₂=\\sqrt{t₀²+t₁²+t₂²}$ only depends on t₂.\nAlthough $y=\\sqrt{1+x²}$ is not linear, it\u0026rsquo;s approaching linear: $y=x$ as x grows.\n(plot by Perplexity-Wolfram)\nBy using the L2 norm of the camera-space coordinates 𝐭 as the depth, the evenly sampled points on the line of sight in the ray space will almost remain evenly spaced after this non-linear projection (due to perspective division) from 𝐭 to 𝐱, close to directly using the t₂ of camera space as the depth:\n⌈ ⌊ S x x c ₀ ₁ r ⌉ ⌋ e e n M a ↓ i n ↓ t R a a ↓ i y n ℮ s e p v a x e c ₂ n e ⦿ k w t s ( r e a h p x ₖ r t e a ₀ ' n c c , e h r i x l i a n ₁ n y g , i g x s s ₂ p ) a ᵀ c e (2024-01-01) Unlike directly using t₂ as the depth in the ray space, with which the uniform intervals can be exactly kept (as perspective projection with homogeneous coords is linear), this approach considers both t₀ and t₁. After viewing transformation, it is the kernels that are viewing the screen. Thus, the screen seen by a kernel is perspective (i.e., near large, far small, as shown below Fig.3 bottom) after projective transformation. Each ray emitted from the kernel is parallel to each other in the ray space.\nMap a kernel from camera space to ray space.\n(2024-01-06)\nThe transformation ϕ from camera space to ray space should be understood as the datapoints are projected into the ray space (or the screen by disregarding the x₂) perspectively, because we want to display 3D world in a 2D plane. (Note: x₂ is independtly assigned beside the screen coordinates x0,x1, so ray space = screen + x₂.)\nShortly, the near-large-far-small effect is desired. But it\u0026rsquo;s a nonlinear function, so its linear approximation is applied to simulate those \u0026ldquo;curves\u0026rdquo;. As shown in Fig. 9, the correct perspective curving effects (bottom-right) aren\u0026rsquo;t accurately represented by an linear projection (bottom-left). Intuitively, one might think that all points are using a common depth value, but for the exact perspective projection, each point should use its individual depth.\nIn addition, the x₂ (‖𝐭‖₂, depth in the ray space) can be disregarded, because the datapoints have already been projected onto the screen, i.e., (x₀,x₁). The existence of ray space may just for introducing the footprint function (ray integral). The \u0026ldquo;projective transformation ϕ\u0026rdquo; is solely assigning x₂ based on the perspective projection.\nThe statement that \u0026ldquo;rays are parallel\u0026rdquo; may be misleading, as the perspective projection (x,y divided by z) has been performed, and one can focus on the screen directly. \u0026ldquo;Parallel\u0026rdquo; doesn\u0026rsquo;t mean that the kernel is projected onto the screen orthogonally, because they already are on the screen. The ray space is set for defining the footprint function of each kernel as an integral along a certain viewing ray, which remains a straight line in the 3D ray space after perspective projection due to the linear approximation.\nAnd it is the viewing ray that will be orthogonally projected onto the screen. Thereby, the ray integral in the 3D ray space becomes a line integral on the 2D screen plane, i.e., an integral for a 2D Gaussian.\n(2024-01-07) The 3D Gaussian integral in the ray space equals the 2D Gaussian integral in the screen space (?), because I think that the \u0026ldquo;voxels\u0026rdquo; perpendicular to a pixel at different depths are replicas of that pixel, which is projected by a certain kernel. S p c i r x e e ■ □ e l n ( v ■ 1 ( □ R v ■ a 2 y □ v ■ s 3 p □ a ⋯ c e v ■ ⋯ N ) □ ) K K e e r r n n e e l l 1 2 The depth dimension x₂ is ‖𝐭‖₂, which is independent to x0, x1. So ray space is \u0026ldquo;orthogonal\u0026rdquo; to the screen. In other words, a screen pixel is a summation of only kernels that overlaps with it, because object-space 3D Gaussian have already been projected perspectively onto the screen (corresponding to 2D center and covariance matrix), and the ray space is constructed after that projection as \u0026ldquo;rays space = screen + x₂\u0026rdquo;. In short, ray space is for viewing rays, not for datapoints.\n(2024-01-07)\nThe viewing ray is only used to compute each kernel\u0026rsquo;s opacity, which is the contribution coefficient to the target pixel\u0026rsquo;s color.\nKernels contributing to the target pixel are determined by whether a kernel overlaps with the pixel after being thrown onto the screen. In essence, the contributing kernel in the ray space is located on the perpendicular line to the target pixel, and its squashed flat disc covers the target pixel.\nThe role of viewing rays in splatting differs from volume rendering, where a viewing ray yields a pixel color, but in splatting, color is generated from kernels and a viewing ray determines the weights of kernels.\n(2024-01-22)\nThe 2D location (x₀,x₁) of a Gaussian center on the screen and the 2D covariance matrix are determined by perspective projection, while the opacity of a Gaussian is determined by computing the integral over x₂ along the viewing ray in the 3D ray space. However, this perspective projection is not affine (linear) because the depth t₂ is divided and x₂ is reassigned. Therefore, the first-order Taylor expansion of this transformation matrix is used as its linear approximation.\n(2024-01-05) The reassignment of x₂ is not the reason for the approximation. x₂ is just used for integration along the viewing ray. And after approximation, the shape of the 2D Gaussian doesn\u0026rsquo;t depend on depth, which (the 3rd row, col) will be omitted to obtain the 2D covariance matrix of the projected 2D ellipse. The Jacobian matrix of ϕ(𝐭) is:\n$$ (\\begin{bmatrix}t₀/t₂ \\\\ t₁/t₂ \\\\ ‖(t₀,t₁,t₂)ᵀ‖ \\end{bmatrix})\u0026rsquo;= 𝐉 = \\begin{pmatrix} 1/t₂ \u0026amp; 0 \u0026amp; -t₀/t₂² \\\\ 0 \u0026amp; 1/t₂ \u0026amp; -t₁/t₂² \\\\ t₀/l \u0026amp; t₁/l \u0026amp; t₂/l \\end{pmatrix} $$\nThe first-order Taylor expansion evaluated at the kernel center point 𝐭ₖ is called the local affine approximation:\n$$ \\begin{aligned} \u0026amp; ϕₖ(𝐭) = ϕ(𝐭ₖ) + 𝐉ₖ ⋅ (𝐭-𝐭ₖ) \\\\ \\\\ \u0026amp; 𝐉ₖ = \\frac{∂ϕ}{∂𝐭}(𝐭ₖ) = \\begin{pmatrix} 1/tₖ,₂ \u0026amp; 0 \u0026amp; -tₖ,₀/tₖ,₂² \\\\ 0 \u0026amp; 1/tₖ,₂ \u0026amp; -tₖ,₁/tₖ,₂² \\\\ tₖ,₀/‖𝐭ₖ‖ \u0026amp; tₖ,₁/‖𝐭ₖ‖ \u0026amp; tₖ,₂/‖𝐭ₖ‖ \\end{pmatrix} \\end{aligned} $$\n(2024-01-10) If the 𝐭 is 𝐭ₖ, (𝐭-𝐭ₖ). So, there is no approximation, the projection of 𝐭ₖ is exact ϕ(𝐭ₖ). This is what the following sentence in the paper means.\n\u0026ldquo;As illustrated in Fig. 9, the local affine mapping is exact only for the ray passing through tk or xk, respectively.\u0026rdquo;\nAnd for the point far away from the Guassian center, the approximation result has a noticeble deviation from the actual case, as (𝐭-𝐭ₖ) is large.\nBy concatenating the viewing transform φ(𝐱) and projective transform ϕ(𝐭), the conversion for a point 𝐮 in source space to 𝐱 in ray space is an affine mapping 𝐦(𝐮):\n$$𝐭 = φ(𝐮) = 𝐖 𝐮+𝐝 \\\\ 𝐱 = 𝐦ₖ(𝐮) = ϕₖ(𝐭) = ϕ(𝐭ₖ) + 𝐉ₖ ⋅ (𝐭-𝐭ₖ) \\\\ = 𝐱ₖ + 𝐉ₖ ⋅ (𝐖 𝐮+𝐝 -𝐭ₖ) = 𝐉ₖ⋅𝐖 𝐮 + 𝐱ₖ + 𝐉ₖ⋅(𝐝-𝐭ₖ)$$\nTherefore, for this compound affine mapping, the multiplier is $𝐉ₖ⋅𝐖 $, and the bias is $𝐱ₖ + 𝐉ₖ⋅(𝐝-𝐭ₖ)$.\nAccording to the aforementioned viewing transformation derivation, the kernel in the 3D ray sapce rₖ\u0026rsquo;(𝐱) has a shifted mean 𝐮ₖ and a scaled variance 𝐕ₖ:\n$$ rₖ\u0026rsquo;(𝐱) ∼ N(𝐦(𝐮ₖ), 𝐉ₖ𝐖 𝐕ₖ 𝐖 ᵀ𝐉ₖᵀ) $$\n(2023-12-02) doubt: Does the eq. (30) mean that the unscaled representation in ray space is the desired projected kernel?\nThe normalization factor of the transformed Gaussian is canceled, so that its integral isn\u0026rsquo;t a unit. Is that they want?\nIntegrate Kernels The footprint function is an integral over the voxels in a 3D kernel along the ray.\nGiven a 3D kernel in the ray space rₖ\u0026rsquo;(𝐱) as above, the footprint function is the integral over depth:\nConvolve with Filter The anti-aliased splatting equation is achieved by using the band-limited footprint function under 2 assumptions described above.\nConvolving the footprint function with a Gaussian low-pass filter:\n","date":"2023-11-18T21:52:00Z","image":"https://pic2.zhimg.com/80/v2-7cbe3b0c3b67ce80593fad0d73a814b5_720w.webp","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-ewa_splatting/","title":"read: EWA Splatting"},{"content":"Example Source video: 【cmake教程】为你的项目引入外部第三方库(以真实项目partio为例) - 只喝白开水\nPrepare: Download the code of partio.\n1 git clone https://github.com/wdas/partio.git --depth=1 Directory structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (base) yi@yi:~/Downloads/partio$ tree -L 2 . ├── build_wheels.sh ├── CMakeLists.txt ├── Dockerfile ├── LICENSE ├── Makefile ├── README.md ├── setup.cfg ├── setup.py └── src ├── data ├── doc ├── lib ├── Makefile ├── py ├── tests └── tools 7 directories, 9 files Four steps for testing the introduced library:\nCopy codes\nOnly the codes partio/src/lib/ are needed in this example., so copy it to your own project, and place it under the directory example_cmake/external/partio/ as shown below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (base) yi@yi:~/Downloads/example_cmake$ tree . ├── CMakeLists.txt ├── external │ └── partio │ ├── CMakeLists.txt # partio/src/lib/CMakeLists.txt │ ├── core │ ├── io │ ├── PartioAttribute.h │ ├── Partio.h │ ├── PartioIterator.h │ ├── PartioVec3.h │ └── test │ └── test_helloWorld.cpp └── src ├── CMakeLists.txt └── main.cpp Inherit the original CMakeListst settings:\nCopy all contents of the top-level CMakeLists.txt of the original partio project to the front of the copied lib\u0026rsquo;s external/partio/CMakeLists.txt, i.e., the previous partio/src/lib/CMakeLists.txt, as it\u0026rsquo;s included first.\nSo as to retain some macro settings and variables for compiling. And then delete unnecessary settings.\nThe top-level CMakeLists.txt of your own project is:\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.20) project(learn-cmake LANGUAGES CXX) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) # each sub-directory has its own CMakeLists.txt: add_subdirectory(external/partio) add_subdirectory(src) The src/ folder stores your own code, which will be built to an executable program. So, the executable in the src/CMakeLists.txt is your own main.cpp:\n1 add_executable(main main.cpp) An example of src/main.cpp:\n1 2 3 int main(int argc, char const *argv[]){ return 0; } Write an test executable program (test_helloWorld.cpp) inside external/partio.\nIn this case, test_helloWorld.cpp calls functions in external/partio \u0026ldquo;natively\u0026rdquo;. However, this isn\u0026rsquo;t the case for calling its functions from another project.\n1 2 3 4 #include \u0026#34;Partio.h\u0026#34; int main(){ Partio::ParticlesDataMutable* data = Partio::read(\u0026#34;bgeoFile\u0026#34;); std::cout \u0026lt;\u0026lt; \u0026#34;Number of \u0026#34; \u0026lt;\u0026lt; std::endl;} Set the external/partio/CMakeLists.txt to compile it as an executable application:\n1 2 3 4 add_library(partio ${PARTIO_LIBRARY_TYPE} ${io_cpp} ${core_cpp}) # Setting target_include_directories .... add_executable(test_helloWorld ${CMAKE_CURRENT_LIST_DIR}/test/test_helloWorld.cpp) target_link_libraries(test_helloWorld PRIVATE partio) After compiling, a binary file test_helloWorld will be generated under \u0026ldquo;external/partio\u0026rdquo;.\nThis test code has nothing to do with the other subdirectories\u0026rsquo; \u0026ldquo;src/\u0026rdquo;, where the main application will be build based on its own CMakeLists.txt.\nThe CMakeLists.txt of different projects are independent to each other.\nThe most commonly used rule is \u0026ldquo;One CMakeLists.txt per target\u0026rdquo;. cmake: add_subdirectory() vs include() - SO\nTest DiffRast I want to debug the library diff-gaussian-rasterization.\nI can create a project, and make the diffRast as an external library to call its methods.\n1 2 3 4 mkdir debug_diff_rast \u0026amp;\u0026amp; cd debug_diff_rast git init mkdir external \u0026amp;\u0026amp; cd external git submodule add https://github.com/graphdeco-inria/diff-gaussian-rasterization Directory structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 (base) yi@yi:~/Downloads/debug_diff_rast$ tree . ├── CMakeLists.txt ├── main.cpp ├── README.md └── external └── diff-gaussian-rasterization ├── CMakeLists.txt ├── cuda_rasterizer │ ├── auxiliary.h │ ├── backward.cu │ ├── backward.h │ ├── config.h │ ├── forward.cu │ ├── forward.h │ ├── rasterizer.h │ ├── rasterizer_impl.cu │ └── rasterizer_impl.h ├── diff_gaussian_rasterization │ └── __init__.py ├── ext.cpp ├── LICENSE.md ├── rasterize_points.cu ├── rasterize_points.h ├── README.md ├── setup.py └── third_party ├── glm └── stbi_image_write.h The original diff-gaussian-rasterization/CMakeLists.txt would build the project to a library (CudaRasterizer), not an executable application.\nThe command add_subdirectory() is used to add an external project, which will automatically build according to its own CMakeLists.txt when the root CMakeLists.txt starts building.\nIf without add_subdirectory, an error would occur: CMake Error at CMakeLists.txt:17 (target_include_directories): Cannot specify include directories for target which is not built by this project. Hence, the library CudaRasterizer is linked as a static library through target_link_libraries() to an executable program MyApp based on the main.cpp, and debug it.\nIn summary, the top-level (root) CMakeLists.txt needs to link 2 libraries: libtorch and CudaRasterizer, as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cmake_minimum_required(VERSION 3.20 FATAL_ERROR) project(MyApp) # ${PROJECT_NAME} find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_subdirectory(external/diff-gaussian-rasterization) add_executable(MyApp main.cpp) set_property(TARGET MyApp PROPERTY CXX_STANDARD 17) target_link_libraries(MyApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) target_link_libraries(MyApp CudaRasterizer) # rasterization_points.cu and ~.h aren\u0026#39;t in their CMakeLists.txt, so need: target_sources(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization/rasterize_points.cu) target_include_directories(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization) In the diff-gaussian-rasterization/CMakeLists.txt, the header files inside the folder cuda_rasterizer/ are included and set to PUBLIC, such that when the library is linked to the target, those headers will be appended to the target\u0026rsquo;s INCLUDE_DIRECTORIES. So they can be included via their filenames, for example #include \u0026quot;forward.h\u0026quot;, without relative path. (Not sure. 2023-11-16)\nHowever, the rasterize_point.h isn\u0026rsquo;t included into the library (CudaRasterizer), so it has to be included using its relative path.\nSpecifically, in the main.cpp:\n1 2 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026#34;external/diff-gaussian-rasterization/rasterize_points.h\u0026#34; With adding that header file (declaration) to executable target, the compilation of main.cpp can pass. However, if the function body rasterize_points.cu (function implementation) isn\u0026rsquo;t added to the executable in CMakeLists.txt, there will be a linking error:\n1 2 /usr/bin/ld: CMakeFiles/MyApp.dir/main.cpp.o: in function `main\u0026#39;: main.cpp:(.text.startup+0x383): undefined reference to `RasterizeGaussiansCUDA(at::Tensor const\u0026amp;, ...)\u0026#39; Two modifications for source code of diff-gaussian-rasterization:\nrasterize_point.cu isn\u0026rsquo;t included into the CudaRasterizer library, because it\u0026rsquo;s not specified within the add_library() function in the diff-gaussian-rasterization/CMakeLists.txt.\nTherefore, when the project diff-gaussian-rasterization is built by executing cmake -B ./build, the .cu file rasterize_point.cu isn\u0026rsquo;t compiled.\nThus, the following error of missing Python.h won\u0026rsquo;t be tiggered.\nrasterize_point.cu includes header \u0026lt;torch/extension.h\u0026gt;, which is used for interacting with Python. Based on that, the library CudaRasterizer can be called in Python scripts as a PyTorch extension.\nTherefore, the Python development headers must be present to compile it. Otherwise, an error would occur:\n1 2 3 4 5 6 7 8 9 In file included from /usr/local/libtorch/include/torch/csrc/Device.h:3, from /usr/local/libtorch/include/torch/csrc/api/include/torch/python.h:8, from /usr/local/libtorch/include/torch/extension.h:6, from /home/yi/Downloads/diff-gaussian-rasterization/rasterize_points.h:13, from /home/yi/Downloads//diff-gaussian-rasterization/test_DiffRast.cpp:2: /usr/local/libtorch/include/torch/csrc/python_headers.h:10:10: fatal error: Python.h: No such file or directory 10 | #include \u0026lt;Python.h\u0026gt; | ^~-~~-~~-~~-~~ compilation terminated. Previously, my trivial experiments only used LibTorch tensors without interacting with python. Therefore, no error was reported when building without specifying the path of Python.\nBut the Python development headers is already installed:\n1 2 sudo apt-get install python3-dev apt list python3-dev A possible solution is to include python in CMakeLists.txt: Similar issure\nSince here I just want to debug it as a C++ project, instead of for python, and I have LibTorch installed.\nSo I changed #include \u0026lt;torch/extension.h\u0026gt; to #include \u0026lt;torch/torch.h\u0026gt; in the 2 files: \u0026ldquo;rasterize_points.cu\u0026rdquo; and \u0026ldquo;rasterize_points.h\u0026rdquo;.\nBuild the top-level project \u0026ldquo;debug_diff_rast\u0026rdquo;:\n1 2 3 4 5 6 cd ~/Downloads/debug_diff_rast # configure: cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja # build: cmake --build ./build ./build/MyApp Ref:\n【公开课】现代CMake高级教程（持续更新中）- 双笙子佯谬 - bilibili\nInclude Python cmake (2024-01-29)\nThe direct solution for Python.h: No such file or directory could be include Python in the CMakeLists.txt,\nThis is inspired by Example debugging mixed Python C++ in VS Code - Nadiah Pardede Kristensen, where #include \u0026lt;Python.h\u0026gt; will be reported the error by Intellisense, if without specifying \u0026ldquo;includePath\u0026rdquo; for Python in the .vscode/c_cpp_properties.json.\nCMake built-in module: FindPython3 mentioned in How do I get cmake to find python3.9 instead of python3.10 on ubuntu22.04\ng++ CLI (2024-01-31)\nCheck if the Python Development Libraries has been installed: dpkg -l | grep python3-dev. devicetests Install it: sudo apt-get install python3-dev\nFind where it is: find / -type f -iname 'python.h' 2\u0026gt;/dev/null Debian / Ubuntu: Fatal error: Python.h: No such file or Directory\nThe directory of Python is also required to be included: g++ -I/usr/include/python3.8 main.cpp. GeeksforGeeks\nA complete example command is:\n1 2 3 4 5 6 7 8 9 10 11 12 (AIkui) yi@yi-Alien:~/Downloads/CppCudaExt_PT_Tut_AIkui$ g++ \\ -g -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ -I/home/yi/anaconda3/envs/AIkui/include/python3.10 \\ -I./include \\ # Custom functions declarations -L/usr/local/libtorch/lib \\ -Wl,-rpath,/usr/local/libtorch/lib \\ test_None_tensor.cpp \\ # main function definition interpolation.cpp \\ # Custom functions definitions -ltorch -ltorch_cpu -lc10 A demo Refer to Nathan Sebhastian\n1 2 3 4 5 6 7 8 #include\u0026lt;Python.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } Compile: g++ test_python.cpp, an error occurs:\n1 2 3 4 5 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/Cpp_Study$ g++ test_python.cpp test_python.cpp:1:9: fatal error: Python.h: No such file or directory 1 | #include\u0026lt;Python.h\u0026gt; | ^~~~~~~~~~ compilation terminated. Locate header file:\n1 2 3 sudo apt install mlocate sudo updatedb locate Python.h Include Python dir: gcc test_python.cpp -I/usr/include/python3.8\n","date":"2023-11-16T11:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cmake_3rdparty/","title":"memo: CMake | Include 3rd-Party Library"},{"content":"Build add_executalbe() is for building a executable program. While add_library() is for building a library.\nBoth the source files and header files need to be added to executable target.\nOld fasion: How can i include header files from a directory into cmake\nMordern style: parallel101/course - 彭于斌\n1 2 3 add_executable(MyApp) file(GLOB sources *.cpp *.h) target_source(main PUBLIC ${sources}) (2023-11-16)\nCcache Speed up recompilation. Docs\nUsage refer to parallel101/course\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.15) project(hellocmake) find_program(CCACHE_PROGRAM ccache) if (CCACHE_PROGRAM) message(STATUS \u0026#34;Found CCache: ${CCACHE_PROGRAM}\u0026#34;) set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM}) set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK ${CCACHE_PROGRAM}) endif() ","date":"2023-11-15T22:05:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cmake_misc/","title":"memo: CMake | Misc"},{"content":"Debug Config Open \u0026ldquo;launch.json\u0026rdquo; ➡ Clik blue button \u0026ldquo;Add Configuration\u0026hellip;\u0026rdquo; at right-bottom corner ➡ Select \u0026ldquo;C/C++: (gdb) Launch\u0026rdquo;.\nThen, a snippest of configuration with name \u0026quot;(gdb) Launch\u0026quot; is inserted into the \u0026ldquo;configurations\u0026rdquo; field in the config file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(gdb) Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/test\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;stopAtEntry\u0026#34;: false, \u0026#34;cwd\u0026#34;: \u0026#34;${fileDirname}\u0026#34;, \u0026#34;environment\u0026#34;: [], \u0026#34;externalConsole\u0026#34;: false, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true }, { \u0026#34;description\u0026#34;: \u0026#34;Set Disassembly Flavor to Intel\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-gdb-set disassembly-flavor intel\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;g++ compile\u0026#34; }, ] } This newly created configuration can be selected in the drop-down box beside the gear.\nAdd a task in tasks.json.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;g++ compile\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;g++\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;${workspaceFolder}/test.cpp\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;test\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, ] } The code \u0026ldquo;test.cpp\u0026rdquo; to be debugged is as follows:\n1 2 3 4 5 6 7 8 9 10 11 int main(){ int x = 1; int exampleArray[5]; for (int i=0; i\u0026lt;5; i++) exampleArray[i] = i; int* exampleArray_heap = new int[5]; for (int i=0; i\u0026lt;5; i++) exampleArray_heap[i] = i; delete[] exampleArray_heap } View Memory After debugging started,\nFind the address of a variable x:\nAdd expression \u0026amp;x into \u0026ldquo;Watch\u0026rdquo; panel. SO\nUse VS Code\u0026rsquo;s generic memory view\nInstall extention Hex Editor; Clik the binary data icon $[^{01}_{10}]$ following the address (not variables). Another ways to view memory\nUsing extension Memmory View\nPress F1, MemoryView: Add new memory view (for debugger)\nType the address of x: 0x7fffffffdbf0. Then, \u0026ldquo;MEMORY\u0026rdquo; section appers in the bottom panel.\nUse GDB command -exec x/64xb 0x7fffffffdc00\n1 2 3 -exec x/64xb 0x555558854e20 0x555558854e20:\t0x03\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x555558854e28:\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 The byte at 0x555558854e20 is 3. Inspect array Array on stack is the pointer to the array. The pointer is stored in: \u0026amp;exampleArray= 0x7fffffffdc00\nAfter initializing, memory at exampleArray is 0,1,2,3,4:\n1 2 00007fffffffdc00 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 00007fffffffdc10 04 00 00 00 ff 7f 00 00 00 50 58 2b fa da fc 53 This array takes 16 bytes. Array on heap is the address of pointer, which is stored in: \u0026amp;exampleArray_heap= 0x7fffffffdbf8\n1 00007fffffffdbf0 01 00 00 00 04 00 00 00 c0 b2 56 55 55 55 00 00 That\u0026rsquo;s an address: 00 00 55 55 55 56 b2 c0 (0x000055555556b2c0), because the endianness of computer is in reverse order.\nAfter initializing by the for loop, heap memory of the array is:\n1 2 000055555556b2c0 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 000055555556b2d0 04 00 00 00 Read Address Pointer\u0026rsquo;s value is an address.\nUse for loop to read 8 bytes one-by-one:\n1 2 3 4 5 6 7 8 // Read the \u0026#34;address\u0026#34; (0x55555556b2c0) from memory (0x7fffffffdbf0) in bytes unsigned char* bytePointer = (unsigned char*)\u0026amp;exampleArray_heap; const int numBytesToRead = 8; unsigned char buffer[numBytesToRead]; for (int i=0; i\u0026lt;numBytesToRead; i++) buffer[i] = *(bytePointer + i); for (int i=0; i\u0026lt;numBytesToRead; i++) std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; static_cast\u0026lt;int\u0026gt;(buffer[i]) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; Output:\n1 c0 b2 56 55 55 55 0 0 Another way is converting the type of pointer to (long long*)\n1 std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; *(long long*)\u0026amp;exampleArray_heap \u0026lt;\u0026lt; std::endl; And it will automatically reverse the number:\n1 55555556b2c0 Python Calls Cpp (2024-01-29)\nAttach the GDB used for C++ application to the running Python debugger, following steps in Example debugging mixed Python C++ in VS Code - Nadiah Pardede Kristensen\nPreparation: Compile the C++ code to a python package:\nMake a folder for compiling the cpp package:\n1 2 3 4 5 6 7 8 9 10 11 (AIkui) yi@yi-Alien:~/Downloads/Cpp_Study$ tree . ├── cpp_ext_myadd | ├── debug_w_cpp.py │ ├── myAdd.cpp │ ├── setup.cfg │ └── setup.py └── .vscode ├── c_cpp_properties.json ├── launch.json └── settings.json where the myAdd.cpp file refers to code\nThe \"setup.py\" looks like: 1 2 3 4 5 6 7 8 from distutils.core import setup, Extension def main(): setup(name=\u0026#34;myadd\u0026#34;, ext_modules=[Extension(\u0026#34;myadd\u0026#34;, [\u0026#34;myAdd.cpp\u0026#34;])], ) if __name__ == \u0026#34;__main__\u0026#34;: main() Install package:\n1 (AIkui) yi@yi-Alie:~/Downloads/Cpp_Study$ pip install ./cpp_ext_myadd/ The customize package myadd will be installed in the current environment. So, VS Code requires \u0026ldquo;Python:Select Interpreter\u0026rdquo; by pressing F1 to match the envrionment.\nPyhon code: 1 2 3 4 import myadd print(\u0026#34;going to ADD SOME NUMBERS\u0026#34;) x = myadd.myadd(5,6) print(x) Add a breakpoint at line 2 before getting into package functions. And add a breakpoint (at z = x + y;) in the cpp file. Otherwise, the debugger won\u0026rsquo;t pause.\nThe method that needs to manually find the process ID:\nAdd 2 configurations for \u0026ldquo;Python\u0026rdquo; and \u0026ldquo;C/C++: gdb (Attach)\u0026rdquo; separately in the launch.json.\nDefault contents generated after clicking the blue button \"Add Configuration...\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(gdb) Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;/home/yi/anaconda3/envs/AIkui/bin/python3.10\u0026#34;, \u0026#34;processId\u0026#34;: \u0026#34;${command:pickProcess}\u0026#34;, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true }, { \u0026#34;description\u0026#34;: \u0026#34;Set Disassembly Flavor to Intel\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-gdb-set disassembly-flavor intel\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Python: Current File\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${file}\u0026#34;, \u0026#34;console\u0026#34;: \u0026#34;integratedTerminal\u0026#34;, \u0026#34;justMyCode\u0026#34;: true }, ] } With focusing on debug_w_cpp.py, select the dubugger \u0026ldquo;Python: Current File\u0026rdquo;, and then click the green start button to kick off the debugging.\nWhen the debugger hits the breakpoint in debug_w_cpp.py, find its process ID:\n1 2 3 4 5 6 7 8 9 10 (base) yi@yi-Alien:~$ ps aux | grep python root 905 0.0 0.0 42744 1680 ? Ss 2023 0:00 /usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggers root 1278 0.0 0.0 121200 1436 ? Ssl 2023 0:00 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal yi 1605315 0.0 0.1 38840 26336 pts/5 S+ 16:17 0:04 /usr/bin/python3.8 -O /usr/bin/ranger yi 1698762 2.9 0.9 1179385796 150624 ? Sl 18:42 0:04 /usr/share/code/code --ms-enable-electron-run-as-node /home/yi/.vscode/extensions/ms-python.vscode-pylance-2023.12.1/dist/server.bundle.js --cancellationReceive=file:bad7d71dedf58b5bb22a36398d8eb2bcf4447338c7 --node-ipc --clientProcessId=1698586 yi 1700442 0.4 0.1 840012 18472 ? Sl 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter yi 1700450 0.3 0.1 250260 18376 pts/6 Sl 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher 45763 -- /home/yi/Downloads/Cpp_Study/cross_lang_debug_host.py yi 1700456 3.1 0.1 414788 30468 pts/6 Sl+ 18:44 0:00 /home/yi/anaconda3/envs/AIkui/bin/python /home/yi/.vscode/extensions/ms-python.python-2023.22.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy --connect 127.0.0.1:55977 --configure-qt none --adapter-access-token 2bb19c19a6a1592229fd27c7c8e3e0a37c002b320568e3b3c8fe450a3d343404 /home/yi/Downloads/Cpp_Study/cross_lang_debug_host.py yi 1700778 0.0 0.0 12116 2776 pts/7 S+ 18:45 0:00 grep --color=auto python The desired process is the one attached with a token: 1700456\nSwitch the debugger to \u0026ldquo;gdb (Attach)\u0026rdquo;, and then click the green start button again, it will ask the process ID just found.\nThen, a prompt pops:\n1 2 (base) yi@yi-Alienware:~/Downloads/Cpp_Study$ /usr/bin/env /bin/sh /tmp/Microsoft-MIEngine-Cmd-00cvveuw.5cg Superuser access is required to attach to a process. Attaching as superuser can potentially harm your computer. Do you want to continue? [y/N] However, after I input \u0026ldquo;y\u0026rdquo;, the terminal said executing GDB requires elevated permission, but it didn\u0026rsquo;t prompt me to enter password.\n1 2 3 4 5 ==== AUTHENTICATING FOR org.freedesktop.policykit.exec === Authentication is needed to run `/usr/bin/gdb\u0026#39; as the super user Authenticating as: Yi Cao,,, (yi) Password: [1] + Stopped (tty output) /usr/bin/pkexec \u0026#34;/usr/bin/gdb\u0026#34; --interpreter=mi --tty=${DbgTerm} 0\u0026lt;\u0026#34;/tmp/Microsoft-MIEngine-In-tkwd0bog.wdj\u0026#34; 1\u0026gt;\u0026#34;/tmp/Microsoft-MIEngine-Out-n1p4gbjg.vvb\u0026#34; You have stopped jobs. Extension: Python C++ Debugger eliminates the need of manually entering process ID.\nIts default configuration in launch.json is:\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Python C++ Debug\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;pythoncpp\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;pythonConfig\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;cppConfig\u0026#34;: \u0026#34;default (gdb) Attach\u0026#34;, } ] } However, the authentication error persists.\nBypass the authentication:\n1 echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope Refer to: Debugging mixed Python C++ in VS Code. Can\u0026rsquo;t enter sudo password - SO\n\u0026ldquo;This solution doesn\u0026rsquo;t require reboot, but it\u0026rsquo;s not permanent.\u0026rdquo; , as explained in Attaching gdb (C++ debugger) to remote python process in VSCode - SO (OP is Mark Harris) (Found by Perplexity)\nOriginal answer 1 2 3 4 5 6 7 $ gdb --pid=30428 ... Attaching to process 30428 Could not attach to process. If your uid matches the uid of the target process, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try again as the root user. For more details, see /etc/sysctl.d/10-ptrace.conf ptrace: Operation not permitted. In this example, I found that the debugger cannot step downward after entering the cpp file, instead, it gets stuck at the line following the breakpoint.\nHowever, the method works in the project of AIkui\u0026rsquo;s CUDA extension tutorial, where the debugger moves line-by-line after jumping into \u0026ldquo;interpolation_kernel.cu\u0026rdquo; files (function: trilinear_fw_cu()), whereas it can\u0026rsquo;t step into kernel functions, which may require CUDA-GDB, rather than GDB.\nIt seems that after GDB concludes the execution of C++ programm, it will stay at the last line, and won\u0026rsquo;t automatically return to the Python debugger. So, I have to click the \u0026ldquo;disconnect\u0026rdquo; button on the control bar to free the GDB. Other problems:\nIf the file currently open in the editor is not the Python file \u0026ldquo;debug_w_cpp.py\u0026rdquo;, but rather \u0026ldquo;myAdd.cpp\u0026rdquo;, an error pops upon clicking the start button:\nThe following command for installing cpp packege, referring How to pass \u0026ndash;debug to build_ext when invoking setup.py install?, requires the path to source file relative path to workspaceFolder in the \u0026ldquo;setup.py\u0026rdquo;: ext_modules=[Extension(\u0026quot;myadd\u0026quot;, [\u0026quot;./cpp_ext_myadd/myAdd.cpp\u0026quot;])]\n1 (AIkui) yi@yi-Alienware:~/Downloads/Cpp_Study$ python3.10 ./cpp_ext_myadd/setup.py build_ext --debug install (2024-04-18)\nWill\u0026rsquo;s practice: 使用PythonCppDebugger联合调试Python与Cpp，以及应用到3DGS的若干尝试 - will的文章 - 知乎\n利用vscode的插件PythonCppDebugger，gdb选用cuda拓展的cuda-gdb； 相比于：“利用vscode的插件PythonCppDebugger，gdb选用cpp拓展的cppdbg”， 程序会突然崩溃，(chatGPT)怀疑是gs本身显存管理过于复杂，长时间的debug容易崩溃所致。 最终由于实在不稳定，笔者放弃使用该方法，选用笨办法： 把python端数据存成txt，然后另起一个c++项目调试cuda代码。 另可见笔者的issue： Question aboud cuda-gdb extension in attach process · Issue #32 · benibenj/vscode-pythonCpp\n","date":"2023-11-13T11:49:00Z","permalink":"https://zichen34.github.io/writenotes/lang/c++_debug/","title":"memo: C++ | Debug with VS Code"},{"content":"C++ Extensions - Docs\nInstall LibTorch libtorch is the C++ API for PyTorch. Guide: INSTALLING C++ DISTRIBUTIONS OF PYTORCH\nDownload the binary files with modified URL for specified version:\n1 2 3 4 5 6 7 # Download here (cxx11 ABI): wget https://download.pytorch.org/libtorch/cu116/libtorch-cxx11-abi-shared-with-deps-1.12.1%2Bcu116.zip upzip libtorch-cxx11-abi-shared-with-deps-1.12.1+cu116.zip # Optional sudo mv libtorch/ /usr/local/ Create folder \u0026ldquo;example-app\u0026rdquo;, and write code into \u0026ldquo;example-app.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;iostream\u0026gt; int main(){ torch::Tensor tensor = torch::rand({2,3}); std::cout \u0026lt;\u0026lt; tensor \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size of Tensor type: \u0026#34;\u0026lt;\u0026lt; sizeof(torch::Tensor) \u0026lt;\u0026lt; std::endl; } Set dependence for LibTorch via CMakeLists.txt:\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.18 FATAL_ERROR) project(MyLibTorchApp) # name find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_executable(MyLibTorchApp main.cpp) target_link_libraries(MyLibTorchApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) set_property(TARGET MyLibTorchApp PROPERTY CXX_STANDARD 17) Build (externally) inside the dir \u0026ldquo;example-app/build\u0026rdquo;\n1 2 3 4 # mkdir build # cd build cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch .. cmake --build . --config Release (Optional) For C/C++ extension, set up includePath in \u0026ldquo;c_cpp_properties.json\u0026rdquo; for correct intellisense:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/libtorch\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], \u0026#34;defines\u0026#34;: [], \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/bin/g++\u0026#34;, \u0026#34;cStandard\u0026#34;: \u0026#34;c17\u0026#34;, \u0026#34;cppStandard\u0026#34;: \u0026#34;gnu++14\u0026#34;, \u0026#34;intelliSenseMode\u0026#34;: \u0026#34;linux-gcc-x64\u0026#34; } ], \u0026#34;version\u0026#34;: 4 } Debug LibTorch (2023-11-12)\nlaunch.json Modify launch.json in .vscode to debug C/C++ binary file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;LibTorch Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/build/MyLibTorchApp\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, } ] } program is the compiled binary file, not the main.cpp file. Debug C++ in Visual Studio Code - Docs General attributes: Options for Debugging task.json Set up \u0026ldquo;tasks.json\u0026rdquo; for building project with cmake:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;cmake-configure\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-DCMAKE_BUILD_TYPE=Debug\u0026#34;, \u0026#34;-DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch\u0026#34;, \u0026#34;..\u0026#34;, ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;cmake-build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--build\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;--config\u0026#34;, \u0026#34;Debug\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;dependsOn\u0026#34;: [\u0026#34;cmake-configure\u0026#34;, \u0026#34;cmake-build\u0026#34;] } ] } Multiple tasks can be combined to one, which then is able tobe passed to preLaunchTask in \u0026ldquo;launch.json\u0026rdquo;. (Can\u0026rsquo;t pass 2 tasks at once.)\nIntegrate with External Tools via Tasks - Docs\nThe above 2 tasks both are custom tasks executed in shell.\nCMake requires two steps, so 2 tasks are needed.\nThe tag -DCMAKE_BUILD_TYPE=Debug is needed for generating debug info. Otherwise, the breakpoints won\u0026rsquo;t take effect:\n1 Module containing this breakpoint has not yet loaded or the breakpoint address could not be obtained. However, sometimes, camke build failed:\nExpand the error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 * Executing task: cmake --build . --config Debug -- Caffe2: CUDA detected: 11.6 -- Caffe2: CUDA nvcc is: /usr/local/cuda-11.6/bin/nvcc -- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.6 CMake Error at /usr/local/libtorch/share/cmake/Caffe2/public/cuda.cmake:88 (message): Caffe2: Couldn\u0026#39;t determine version from header: Change Dir: \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Run Build Command(s): /usr/local/bin/cmake -E env VERBOSE=1 /usr/bin/make -f Makefile cmTC_a6e2f/fast make[1]: Entering directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; /usr/bin/make -f CMakeFiles/cmTC_a6e2f.dir/build.make CMakeFiles/cmTC_a6e2f.dir/build make[2]: Entering directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Building CXX object CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o /usr/bin/c++ -I/usr/local/cuda-11.6/include -o CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o -c /home/yi/Downloads/example-app/build/detect_cuda_version.cc Assembler messages: Fatal error: can\u0026#39;t create CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o: No such file or directory make[2]: *** [CMakeFiles/cmTC_a6e2f.dir/build.make:78: CMakeFiles/cmTC_a6e2f.dir/detect_cuda_version.cc.o] Error 1 make[2]: Leaving directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; make[1]: *** [Makefile:127: cmTC_a6e2f/fast] Error 2 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/example-app/build/CMakeFiles/CMakeTmp\u0026#39; Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include) /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:4 (find_package) -- Configuring incomplete, errors occurred! make: *** [Makefile:179: cmake_check_build_system] Error 1 * The terminal process \u0026#34;/usr/bin/bash \u0026#39;-i\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cmake --build . --config Debug\u0026#39;\u0026#34; terminated with exit code: 2. I tried g++ command, but I had problem with linking torch libraries (-L) and including header files (-I).\n1 2 3 4 5 6 7 * Executing task: C/C++: g++ build active file Starting build... /usr/bin/g++ -fdiagnostics-color=always -g /home/yi/Downloads/example-app/main.cpp -o /home/yi/Downloads/example-app/main /home/yi/Downloads/example-app/main.cpp:1:10: fatal error: torch/torch.h: No such file or directory 1 | #include \u0026lt;torch/torch.h\u0026gt; | ^~~-~~-~~-~~-~~-~~ Ref:\nCMake+VSCode编译运行C++程序简单教程 - 迷楼的文章 - 知乎\nLinux环境下使用VScode调试CMake工程 - Colorful的文章 - 知乎\nlibtorch 常用api函数示例（史上最全、最详细）- 无左无右 - 博客园\nInspect Tensor (2023-11-19)\nSave tensors to a file.\n(LibTorch -\u0026gt; PyTorch tensors) How to save a C++ Libtorch Tensor and load it into a Python Pytorch project? In GDB: *(float_t*)x.data_ptr()\n1 2 int main(){ const torch::Tensor bkg = torch::full({3}, 0., torch::device(torch::kCUDA)); GDB: *(float_t*)bkg.data_ptr() return 0.\nIs it possible to view values of a at::Tensor in Visual codes debug variable view (linux)?\n","date":"2023-11-09T20:04:00Z","permalink":"https://zichen34.github.io/writenotes/lib/libtorch_debug/","title":"memo: LibTorch | Debug"},{"content":"Create Tensor shape (2024-01-24)\n.sizes() is an \u0026ldquo;vector-like\u0026rdquo; object of class: IntArrayRef. It can be created with curly braces, e.g., {5.2}, or an std::vector\u0026lt;int64_t\u0026gt;{1,2,4}.\n1 2 3 4 5 6 7 #include \u0026lt;cassert\u0026gt; std::vector\u0026lt;int64_t\u0026gt; myVec = {5,2}; assert(torch::ones({5,2}).sizes() == myVec); // pass std::cout \u0026lt;\u0026lt; \u0026#34;Equal\u0026#34; \u0026lt;\u0026lt; std::endl; c10::IntArrayRef myArrRef = {5,2}; assert(myVec == myArrRef); // pass \u0026ldquo;Create vector out of the IntArrayRef constructor, , otherwise the vector is destroyed immediately afterward.\u0026rdquo; How to compare a torch::tensor shape against some other shapes? - SO Use tensor.size(i) (better than tensor.sizes()[i]) to access one of dimensions. Docs\n1 2 3 std::cout \u0026lt;\u0026lt; torch::ones(5).sizes() \u0026lt;\u0026lt; std::endl; // [5] std::cout \u0026lt;\u0026lt; torch::ones({5,2}).sizes() \u0026lt;\u0026lt; std::endl; // [5, 2] std::cout \u0026lt;\u0026lt; myTensor.size(1) \u0026lt;\u0026lt; std::endl; // 2 Use a Lambda function to reshape a tensor and return the updated shape:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 std::function\u0026lt;c10::IntArrayRef(c10::IntArrayRef newSize)\u0026gt;getResizedShape(torch::Tensor\u0026amp; t) { auto lambda = [\u0026amp;t](c10::IntArrayRef newSize){ t.resize_(newSize); return t.sizes(); }; return lambda; } int main() { torch::Tensor myTensor = torch::rand({1,2,3}); std::cout \u0026lt;\u0026lt; myTensor \u0026lt;\u0026lt; std::endl; auto getNewSize = getResizedShape(myTensor); std::cout \u0026lt;\u0026lt; getNewSize({3,2}) \u0026lt;\u0026lt; std::endl; } Output: 1 2 3 4 5 (1,.,.) = 0.9838 0.7854 0.6991 0.8325 0.1196 0.3780 [ CPUFloatType{1,2,3} ] [3, 2] Create from factory func Tensor Creation API — PyTorch main documentation Test repo General schema:\n1 torch::\u0026lt;factory-func-name\u0026gt; (\u0026lt;func-specific-args\u0026gt;, \u0026lt;sizes\u0026gt;, \u0026lt;tensor-opt\u0026gt;) \u0026lt;factory-func-name\u0026gt; e.g., arange, empty, \u0026hellip; Create a tensor from the factory function torch::rand()\n1 2 3 4 5 6 7 #include \u0026lt;torch/torch.h\u0026gt; // unzipped to /usr/local/libtorch int main(){ const torch::Tensor a = torch::randint(1, 9, {1,2,3}); std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt;\u0026#34;size:\u0026#34; \u0026lt;\u0026lt; a.sizes() \u0026lt;\u0026lt; std::endl; } CMakeLists.txt\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.18 FATAL_ERROR) project(MyLibTorchApp) # name find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_executable(${PROJECT_NAME} main.cpp) target_link_libraries(${PROJECT_NAME} \u0026#34;${TORCH_LIBRARIES}\u0026#34;) set_property(TARGET ${PROJECT_NAME} PROPERTY CXX_STANDARD 17) Build:\n1 2 3 4 mkdir -p build cd build cmake -DCMAKE_PREFIX_PATH=/usr/local/libtorch .. cmake --build . --config Release Or in a modern way (under the workspace; no need cd to ./build):\n1 2 cmake -B build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja cmake --build build # build in ./build Execute it: ./MyLibTorchApp\nOutput: 1 2 3 4 5 (1,.,.) = 8 7 6 2 7 2 [ CPUFloatType{1,2,3} ] size: [1, 2, 3] Create with 4 Properties Pass an instance TensorOptions to the factory function:\n1 2 3 4 5 6 7 8 torch::TensorOptions options = torch::TensorOptions().dtype(torch::kFloat32) .layout(torch::kStrided) .device(torch::kCUDA, 0) .requires_grad(true); torch::Tensor a = torch::full({3,4}, 123, options); std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; a.device() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; a.requires_grad() \u0026lt;\u0026lt; std::endl; Only float and complex can .requires_grad. full(...) is not implemented for sparse layout Output: 1 2 3 4 5 6 123 123 123 123 123 123 123 123 123 123 123 123 [ CUDAFloatType{3,4} ] cuda:0 1 Omitting torch::TensorOptions(), which will be pre-configured and returned if calling the 4 properties directly from torch:: namespace.\n1 torch::Tensor a = torch::arange(1,9, torch::dtype(torch::kInt32).device(torch::kCUDA, 0)); If only one property needs to be specified, its property name (torch::dtype()) can be omitted even further.\n1 torch::Tensor a = torch::arange(8, torch::kInt32); Convert tensor by .to Use TensorOptions and .to() to create a new tensor on new memory based on a source tensor.\nConvert dtype:\n1 2 3 4 5 6 7 8 torch::Tensor src_tensor = torch::randn({3,2}); torch::Tensor a = src_tensor.to(torch::kInt32); // combinational torch::Tensor a = src_tensor.to(torch::dtype(torch::kInt32).device(torch::kCUDA,0)); auto opts = a.options(); std::cout \u0026lt;\u0026lt; opts \u0026lt;\u0026lt; std::endl; What does \u0026ldquo;new\u0026rdquo; mean? Options Alteration 1 2 3 4 5 6 torch::Tensor a = torch::randn(3); // change the property of dtype in the TensorOptions object auto int_opts = a.options().dtype(torch::kInt32); auto float_opts = a.options().dtype(torch::kFloat32); size of a tensor (2024-01-24)\nLibTorch sizeof tensor - SO\n1 2 3 4 5 6 torch::Tensor myTensor = torch::rand({1,2,3}, torch::kFloat32); int sizeOfFloat = torch::elementSize(torch::typeMetaToScalarType(myTensor.dtype())); std::cout \u0026lt;\u0026lt; \u0026#34;size of the kFloat32 type: \u0026#34; \u0026lt;\u0026lt; sizeOfFloat \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Number of elements in the tensor: \u0026#34; \u0026lt;\u0026lt; myTensor.numel() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Bytes occupied by the tensor: \u0026#34; \u0026lt;\u0026lt; myTensor.numel() * sizeOfFloat \u0026lt;\u0026lt; std::endl; Output: 1 2 3 size of the kFloatt32 type: 4 Number of elements in the tensor: 6 Bytes occupied by the tensor: 24 Manipulate Tensor ATen means \u0026ldquo;A Tensor Library\u0026rdquo;. The Tensor class under its namespace at:: lays the base for all tensor operations. ezyang\u0026rsquo;s blog\nResize (2023-11-12)\nAPI: Class Tensor in Namespace ATen - Docs\nReshape a tensor in place:\n1 2 3 4 torch::Tensor t = torch::arange(6).resize_({1,2,3}); std::cout \u0026lt;\u0026lt; t \u0026lt;\u0026lt; std::endl; t.resize_({6}); std::cout \u0026lt;\u0026lt; t \u0026lt;\u0026lt; std::endl; Output: 1 2 3 4 5 6 7 8 9 10 11 (1,.,.) = 0 1 2 3 4 5 [ CPULongType{1,2,3} ] 0 1 2 3 4 5 [ CPULongType{6} ] It can be resized to more than its elements:\n1 2 3 4 5 6 7 torch::Tensor t = torch::arange(6).resize_({1,2,3}); t.resize_({10}); std::cout \u0026lt;\u0026lt; \u0026#34;Allocated bytes:\u0026#34; \u0026lt;\u0026lt; t.numel() * torch::elementSize(torch::typeMetaToScalarType(t.dtype())) \u0026lt;\u0026lt; std::endl; for (size_t i = 0; i \u0026lt; t.numel(); ++i) { std::cout \u0026lt;\u0026lt; t[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Output 1 2 3 4 5 6 7 8 9 10 11 Allocated bytes:80 0 [ CPULongType{} ] 1 [ CPULongType{} ] 2 [ CPULongType{} ] 3 [ CPULongType{} ] 4 [ CPULongType{} ] 5 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] 0 [ CPULongType{} ] Flatten Reshape a tensor to 1D and return the pointer to it. Code from 3DGS\nUse a lambda function to resize the tensor and return the data pointer.\n.data_ptr() points to data of the tensor x, while x doesn\u0026rsquo;t point to data directly.\nreinterpret_cast\u0026lt;char*\u0026gt; converts the tensor-type pointer .data_ptr() to a char-type pointer pResizedX, which will read memory byte by byte.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdio\u0026gt; std::function\u0026lt;char*(size_t N)\u0026gt; resizeFunctional(torch::Tensor\u0026amp; t){ std::cout \u0026lt;\u0026lt; \u0026#34;size of the reference of the input tensor: \u0026#34; \u0026lt;\u0026lt; sizeof(t) \u0026lt;\u0026lt; std::endl; auto lambda = [\u0026amp;t](size_t N){ // Number of elements t.resize_({ (long long) N}); // shape: {N} std::cout \u0026lt;\u0026lt; \u0026#34;N is: \u0026#34; \u0026lt;\u0026lt; N \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size of t: \u0026#34; \u0026lt;\u0026lt; sizeof(t) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;dtype of t: \u0026#34; \u0026lt;\u0026lt; t.dtype() \u0026lt;\u0026lt; std::endl; return reinterpret_cast\u0026lt;char*\u0026gt;(t.contiguous().data_ptr()); // read memory byte by byte }; return lambda; } int main(){ torch::Tensor a = torch::arange(33,40, torch::kByte).resize_({1,2,3}); std::cout \u0026lt;\u0026lt; \u0026#34;Test tensor: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Tensor is a ptr, so its size is: \u0026#34; \u0026lt;\u0026lt; sizeof(torch::Tensor) \u0026lt;\u0026lt; std::endl; auto resizer = resizeFunctional(a); // lambda expression char* pTensor = resizer(a.numel()); // pointer to tensor\u0026#39;s data // Memory address printf(\u0026#34;char*: %p \\n\u0026#34;, pTensor); std::cout \u0026lt;\u0026lt; \u0026#34;size of pointer of a char: \u0026#34; \u0026lt;\u0026lt; sizeof(pTensor) \u0026lt;\u0026lt; std::endl; // The return address is the data_ptr() printf(\u0026#34;data_ptr(): %p \\n\u0026#34;, a.data_ptr()); // Print out the data stored in the returned address // Since a data is only 1 byte, the 1st byte is the 1st data. char data = *pTensor; // the first byte. // Note: unicode of 0-31 are invisible, so I test char 33-40 printf(\u0026#34;The first byte: %c \\n\u0026#34;, data); std::cout \u0026lt;\u0026lt; data \u0026lt;\u0026lt; std::endl; // Convert value (char) to integer printf(\u0026#34;Decimal: %d \\n\u0026#34;, data); // 33 std::cout \u0026lt;\u0026lt; \u0026#34;Convert 1st byte to int: \u0026#34; \u0026lt;\u0026lt; static_cast\u0026lt;int\u0026gt;(*pTensor) \u0026lt;\u0026lt; std::endl; // Indexing elements like an array: std::cout \u0026lt;\u0026lt; \u0026#34;Use [0]: \u0026#34; \u0026lt;\u0026lt; pTensor[0] \u0026lt;\u0026lt; std::endl; // ! for (size_t i = 0; i \u0026lt; 6; ++i) { std::cout \u0026lt;\u0026lt; static_cast\u0026lt;char\u0026gt;(pTensor[i]) \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; } Output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 (base) yi@yi:~/Downloads/LibTorch_Study$ ./build/MyLibTorchApp Test tensor: (1,.,.) = 33 34 35 36 37 38 [ CPUByteType{1,2,3} ] Tensor is a ptr, so its size is: 8 size of the reference of the input tensor: 8 N is: 6 size of t: 8 dtype of t: unsigned char char*: 0x55f202301640 size of pointer of a char: 8 data_ptr(): 0x55f202301640 The first byte: ! ! Decimal: 33 Convert 1st byte to int: 33 Use [0]: ! ! \u0026#34; # $ % \u0026amp; N is the total number of elements in the tensor t.\nresize_ requires the shape argument size to be c10::IntArrayRef type, which is an array of int64_t, i.e., signed 8-byte integer.\nTherefore, from the unsigned long int size_t (N) to a signed int64_t is a narrowing conversion.\nlong is at least 32-bit. In my computer, long is 8-byte. And long long is at least 64-bit. Because the signedness modifier is omitted, both long and long long are signed. Thus, the type casting (long long) N is equivalent to (int64_t) N\nint64_t is exact 8 bytes for all compilers, unlike long somewhere is 4-bytes. Definition of int64_t - SO\n1 2 3 4 5 6 std::cout \u0026lt;\u0026lt; sizeof(size_t) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(signed long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(unsigned long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(long long) \u0026lt;\u0026lt; std::endl; // 8 std::cout \u0026lt;\u0026lt; sizeof(int64_t) \u0026lt;\u0026lt; std::endl; // 8 Attributes of tensor x:\n--- title: tensor x --- classDiagram direction RL class T[\"at::TensorBase\"]{ + c10::intrusive_ptr impl_ } class P[\"c10::intrusive_ptr\"]{ + c10::TensorImpl* target_ } note for P \"0x555557729610\" P --\u003e T View the memory via GDB command -exec:\n1 2 3 -exec x/64xb 0x555557729610 0x555557729610:\t0x60\t0x35\t0xfb\t0xf7\t0xff\t0x7f\t0x00\t0x00 0x555557729618:\t0x01\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00\t0x00 0x7FFFF7FB3560 is not the address storing x\u0026rsquo;s data. Char pointer pResizedX = 0x555557729500 points to the memory storing the x\u0026rsquo;s data:\n1 2 3 4 5 6 7 8 9 -exec x/64b 0x555557729500 0x555557729500:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729508:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729510:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729518:\t3\t0\t0\t0\t0\t0\t0\t0 0x555557729520:\t0\t0\t0\t0\t0\t0\t0\t0 0x555557729528:\t81\t0\t0\t0\t0\t0\t0\t0 0x555557729530:\t0\t0\t0\t0\t0\t0\t0\t0 0x555557729538:\t16\t80\t87\t85\t85\t85\t0\t0 There are four 3. A tensor takes 8-byte integer? DEBUG CONSOLE panel:\n1 2 x.data_ptr {void *(const at::TensorBase * const)} 0x55555555b392 \u0026lt;at::TensorBase::data_ptr() const\u0026gt; Don\u0026rsquo;t know what that address is? Get value (2023-11-12)\nReturn the pointer to data: Tensor.data\u0026lt;T\u0026gt;(), which is deprecated and changed to Tensor.data_ptr\u0026lt;T\u0026gt;() internally. Source code\n1 2 3 4 5 int main(){ torch::Tensor x = torch::full({1,3}, 2, torch::dtype(torch::kFloat)); std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x.contiguous().data\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x.contiguous().data_ptr\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl;} Output: 1 2 3 4 5 ~/l/build$ ./MyLibTorchApp 2 2 2 [ CPUFloatType{1,3} ] 0x557d9beab500 0x557d9beab500 .item\u0026lt;dtype\u0026gt;() can get scalar data, not vector. Torch C++: Getting the value of a int tensor by using *.data() - SO\n1 2 3 4 5 6 int main(){ torch::Tensor x = torch::randn({1,3}); std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x[0][0].item\u0026lt;int\u0026gt;() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; x[0][0].item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; } Output: 1 2 3 4 5 ~/l/build$ ./MyLibTorchApp -0.6926 -0.2304 1.2920 [ CPUFloatType{1,3} ] 0 -0.692582 Use a vector to hold result tensor after inference: Part-2 Garry\u0026rsquo;s Blog\n1 2 3 4 5 6 7 8 9 // Extract size of output (of the first and only batch) // and preallocate a vector with that size auto output_size = output.sizes()[1]; auto output_vector = std::vector\u0026lt;float\u0026gt;(output_size); // Fill result vector with tensor items using `Tensor::item` for (int i = 0; i \u0026lt; output_size; i++) { output_vector[i] = output[0][i].item\u0026lt;float\u0026gt;(); } Copy cv::Mat to a tensor: Part-3 Garry\u0026rsquo;s Blog\n1 2 3 4 torch::Tensor tensor = torch::empty({mat.row, mat.cols, mat.channels()}, torch::TensorOptions().dtype(torch::kByte).device(torch::kCPU)); std::memcpy(tensor.data_ptr(), reinterpret_cast\u0026lt;void*\u0026gt;(mat.data), tensor.numel() * sizeof(at::kByte)); A more detailed post: Data Transfer to and from PyTorch - SimonWenkel.com libtorch 常用api函数示例（史上最全、最详细） - 博客园\nAllentDan/LibtorchTutorials\nFrom PyTorch to Libtorch: tips and tricks - Marc Lalonde - Medium\nAnnouncing a series of blogs on PyTorch C++ API - Kushashwa Ravi Shrimali\nempty tensor (2024-01-28)\nIn 3DGS, the project diff-gaussian-rasterization is built as an cpp extension according to setup.py, which is called in Python program. Whereas the CMakeList.txt serves for building the project as a static library (.so) to be inserted into the C++ executable application.\nOriginally, I want to debug the diff-gaussian-rasterization as a static library, so I need to construct input tensors that mimic those passed from Python, where some tensors are assigned as None, such as cov3D_precomp.\nHowever, I don\u0026rsquo;t know how to create a \u0026ldquo;None\u0026rdquo; tensor in the C++ program (Perplexity said: \u0026ldquo;You can\u0026rsquo;t directly set a tensor to NULL as you would do in Python by setting a variable to None.\u0026rdquo;).\nI have tried torch::empty({0}), but its data_ptr() is not the desired nullptr. Consequently, a if judge statement later won\u0026rsquo;t enter into the branch that would happened when the extension is called by Python.\n(2024-01-31) It turns out that I forgot the re-build and make the application again. So, CUDA-GDB still steps through the old application.\nThe .data_ptr() of torch::empty({0}) and torch::full({0},0) both are nullptr.\n(2024-01-30)\nJust found the None Python tensors in 3DGS are reassigned with torch.Tensor([]):\n1 2 if cov3D_precomp is None: cov3D_precomp = torch.Tensor([]) The torch.Tensor([]) will be passed into the C++ package function: _C.rasterize_gaussians() (i.e., the forward method RasterizeGaussiansCUDA)\nA demo where Python calls C++ package referring to AIkui\u0026rsquo;s CUDA extension tutorial:\nExpand codes The code can be evaluated by commands: chmod +x test.sh and ./test.sh P t y o t r h c o h n . T e n s o r ( [ ] ) p a s s t o r L c i h b : T : o e r m c p h t y ( { 0 } ) r e t u r n t o r P c y h t . h T o e n n s o r ( [ ] ) ","date":"2023-11-09T19:40:00Z","permalink":"https://zichen34.github.io/writenotes/lib/libtorch_tensor/","title":"memo: LibTorch | Tensor APIs and Examples"},{"content":"Pointer (2023-11-09) Review:\nA pointer variable and a regular variable are 2 ways of indexing memory.\nA pointer variable stores the address of the start byte of a variable. And its type indicates how many bytes the complete data takes.\nPointer variable is an address, a unsigned integer, whose size depends on system architecture (32-/64-bit addr). While a variable represents the whole data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // \u0026#34;pointer.c\u0026#34; #include \u0026lt;stdio.h\u0026gt; int main(){ int a = 1; int* p = \u0026amp;a; printf(\u0026#34;a = %d \\n\u0026#34;, a); printf(\u0026#34;p = %p \\n\u0026#34;, p); printf(\u0026#34;address of a = %p \\n\u0026#34;, \u0026amp;a); printf(\u0026#34;#bytes of a = %ld \\n\u0026#34;, sizeof(a)); printf(\u0026#34;#bytes of p = %ld \\n\u0026#34;, sizeof(p)); int b[2][2] = {1,2,3,4}, *ptr_b = \u0026amp;b[2][2]; printf(\u0026#34;#bytes of b = %ld \\n\u0026#34;, sizeof(b)); printf(\u0026#34;#bytes of ptr_b = %ld \\n\u0026#34;, sizeof(ptr_b)); } Build gcc pointer.c -o pointer and run ./pointer.\nOutput: 1 2 3 4 5 6 7 a = 1 p = 0x7ffe45d7c88c address of a = 0x7ffe45d7c88c #bytes of a = 4 #bytes of p = 8 #bytes of b = 16 #bytes of ptr_b = 8 The *p (dereferenced p) and i are equivalent at all time.\n1 2 3 int i = 1; int* p = \u0026amp;i; bool cf_res = (*p == i); // 1 (2023-11-11) char* indicates the direction: from pointer * to char. Similarly, char** means from a pointer * to another pointer *, then to char.\nPOINTERS in C++ - YouTube - The Cherno\nvoid* ptr = 0. Address 0 is NULL or nullptr. void means dismissing the type of the data it points to.\nReference Note: C doesn\u0026rsquo;t have reference. SO\nA reference variable is an alias. Compared to pointer, it\u0026rsquo;s an already dereferenced address.\n1 2 3 4 5 int i = 10 int* ptr = \u0026amp;i; // an address int\u0026amp; ref = i; // an alias Declaration and initialization must be performed at the same time.\n1 2 3 4 5 int\u0026amp; ref; // incorrect ref = i; int* ptr; ptr = \u0026amp;i; // ok Reference variable cannot be reassigned.\n1 2 3 4 5 6 int i =0, j =1; int\u0026amp; ref1 = i; int\u0026amp; ref1 = j; // error: redeclaration of ‘int\u0026amp; ref1’ int\u0026amp; ref2 = ref1; // ok std::cout \u0026lt;\u0026lt; ref2 \u0026lt;\u0026lt; std::endl; // 0 Reference shares the same memory as the variable, not occupying another memory.\nPass reference into function to modify the source data directly. Pointer needs deferencing to access the data.\nPython only has reference without pointer.\n1 2 3 4 5 6 7 8 9 10 11 #include\u0026lt;iostream\u0026gt; int main(){ int i = 0; std::cout \u0026lt;\u0026lt; \u0026amp;i \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e74 int* ptr = \u0026amp;i; std::cout \u0026lt;\u0026lt; \u0026amp;ptr \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e78 int\u0026amp; ref = i; std::cout \u0026lt;\u0026lt; \u0026amp;ref \u0026lt;\u0026lt; std::endl; // 0x7ffd18889e74 } Use reference for function parameters and return types.\nUse references when you can, and pointers when you have to. C++ FAQ\nIteration:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; int main(){ vector\u0026lt;int\u0026gt; myVec; myVec = {1,2,3,4,5}; // for (int i:myVec) // will copy item to i for (int\u0026amp; i : myVec) // use reference to avoid copy data std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } Pass reference to function to avoid copying:\n1 2 3 4 5 6 7 8 9 void Function(const std::vector\u0026lt;int\u0026gt;\u0026amp; myVec){ for (int i : myVec) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt;std::endl; } int main(){ std::vector\u0026lt;int\u0026gt; myVec={1,2,3}; Function(myVec); } A pointer to a class/struct uses -\u0026gt; to access its members, whereas a reference uses . (Same as python.) What are the differences between a pointer variable and a reference variable? - SO\nRef:\nPointers vs References in C++ - GeeksforGeeks Raw Array (2023-11-11)\nThe variable name exampleArray is an address of the starting byte, so it\u0026rsquo;s a pointer.\n1 f = Array is a row of contiguous memory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #include \u0026lt;iostream\u0026gt; int main(){ // Create an array on stack, which // will be destoryed when getting out of the scope int exampleArray[4]; for (int i=0; i\u0026lt;5; i++) exampleArray[i] = i; cout \u0026lt;\u0026lt; exampleArray \u0026lt;\u0026lt; \u0026#34; is the address of the array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026amp;(exampleArray[0]) \u0026lt;\u0026lt; \u0026#34; is the address of the first element\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; *exampleArray \u0026lt;\u0026lt; \u0026#34; is the first element by dereferencing the pointer\u0026#34; \u0026lt;\u0026lt; endl; // Get the 3rd elements can do arithmatic on pointer, // where the size of the type will be multiplied automatically. cout \u0026lt;\u0026lt; *(exampleArray + 2) \u0026lt;\u0026lt; \u0026#34; is the 3rd element\u0026#34;\u0026lt;\u0026lt; endl; // If access the 2nd elements by calculating in bytes, // cast integer pointer to char pointer with is 1-byte type. // After locating the address, cast back to integer pointer for assigning a integer value. *(int*)((char*)exampleArray + 8) = 30; cout \u0026lt;\u0026lt; exampleArray[2] \u0026lt;\u0026lt; \u0026#34; is the modified 3rd element\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; sizeof(exampleArray) \u0026lt;\u0026lt; \u0026#34; bytes taken by the entire array\u0026#34;\u0026lt;\u0026lt; endl; // For array created on stack, the number of elements can be known because stack pointer contains offset: int num_elements = sizeof(exampleArray) / sizeof(int); cout \u0026lt;\u0026lt; num_elements \u0026lt;\u0026lt; \u0026#34; elements in the array on stack\u0026#34; \u0026lt;\u0026lt; endl; // Create an array on heap with the `new` keyword, // This array is the same as exampleArray, except for its lifetime that lasts until calling `delete[] arrayname`. // So if an array created inside function needs to be returned, it must be created on heap int* exampleArray_heap = new int[4]; // is a pointer (address 0x55555556b2c0) to heap, not the 1st element 0, for (int i=0; i\u0026lt;4; i++) exampleArray_heap[i] = i; // Read the \u0026#34;address\u0026#34; from memory in bytes std::cout \u0026lt;\u0026lt; std::hex \u0026lt;\u0026lt; *(long long*)\u0026amp;exampleArray_heap \u0026lt;\u0026lt; std::endl; cout \u0026lt;\u0026lt; \u0026amp;exampleArray_heap \u0026lt;\u0026lt; \u0026#34; stores the 8-byte address (the pointer) to the array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; exampleArray_heap \u0026lt;\u0026lt; \u0026#34; is the address (pointer) to the array on heap\u0026#34; \u0026lt;\u0026lt; endl; // cast type to integer pointer cout \u0026lt;\u0026lt; *(exampleArray_heap+0) \u0026lt;\u0026lt; \u0026#34; is the first element in the array\u0026#34; \u0026lt;\u0026lt; endl; // one more jump will reduce performance // exampleArray_heap is a pointer of pointer (memory indirection). An 32-bit address taking 4 bytes cout \u0026lt;\u0026lt; sizeof(exampleArray_heap) \u0026lt;\u0026lt; \u0026#34; bytes for the pointer of the array on heap\u0026#34; \u0026lt;\u0026lt; endl; // delete[] exampleArray_heap; // If array is not created on stack, or only has the pointer to an array, // the number of elements can\u0026#39;t be computed as sizeof(array)/sizeof(int). So it must be maintained. static const int exampleSize = 5; // must be known at compile-time int myArray[exampleSize]; } 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 b f 5 6 Create an array on heap.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Entity{ public: int* exampleArray_obj = new int[4]; Entity(){ // construction function for (int i=0; i\u0026lt;4; i++) exampleArray_obj[i] = i; } }; int main(){ Entity e; cout \u0026lt;\u0026lt; e.exampleArray_obj \u0026lt;\u0026lt; \u0026#34; is the address (pointer) to array\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; *(e.exampleArray_obj) \u0026lt;\u0026lt; \u0026#34; is the 1st element of array\u0026#34; \u0026lt;\u0026lt; endl; // 0 } std::array has .size().\n1 2 3 4 5 6 7 8 9 #include \u0026lt;iostream\u0026gt; #include \u0026lt;array\u0026gt; int main(){ std::array\u0026lt;int, 4\u0026gt; myArray; for(int i=0; i\u0026lt;myArray.size(); i++) myArray[i] = i; } Ref:\nArrays in C++ - The Cherno Pointer Alignment (2023-11-14)\n(Unsure about my naming for this.)\nBecause the algorithm specifies processing 128 data at a time, i.e., the factor alignment = 128. Thus, each time 128*sizeof(dtype) memory will be accessed.\nTherefore, the pointers to data should all be multiples of 128.\nThe way to round a number to a multiple of 128 is setting its last 8 bits to 1000_0000 (128).\nThe type of pointer affects its arithmetic:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(){ // int-type pointer: int* chunk = nullptr; // 8-byte address: 0 0 0 0 0 0 0 0 int* ptr = chunk + 127; // 0 + 127*sizeof(int) = 508 = 0x1fc std::cout \u0026lt;\u0026lt; ptr \u0026lt;\u0026lt; std::endl; // 0x1fc // char-type pointer: char* ptr2 = (char*) chunk + 127; // 0 + 127*sizeof(char) = 127 = 0x7f std::cout \u0026lt;\u0026lt; static_cast\u0026lt;void*\u0026gt;(ptr2) \u0026lt;\u0026lt; std::endl; // 0x7f } Convert a pointer to an integer: SO\n1 2 char* chunk = nullptr; uint var = reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(chunk); // ok Ensure offset a multiple of 128:\nIf the currect pointer chunk is not a multiple of 128, add another 127, and then truncate the last 8 bits to 1000_000 to make it a multiple of 128.\n1 2 3 4 5 6 int main(){ char* chunk = (char*)130; std::size_t alignment = 128; // uint // 130 + 127 \u0026amp; ~127 = 256 std::size_t offset = (reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(chunk) + alignment - 1) \u0026amp; ~(alignment - 1); // uint } Bitwise operation:\n1 2 3 4 5 chunk: 0000_0000_1000_0010 alignment-1: 0000_0000_0111_1111 ~(alignment-1): 1111_1111_1000_0000 chunk + (alignment-1): 0000_0001_0000_0001 (257) (257)\u0026amp;~(127) = 256: 0000 0001 0000 0000 Code from 3DGS\nPossible related articles:\nWhat exactly is an \u0026lsquo;aligned pointer\u0026rsquo;? size_t (2024-01-24)\nsize_t is the unsigned integer type. For example, if it\u0026rsquo;s 8 bytes (64-bit unsigned long), its value ranges in [0, 2⁶⁴-1]. In other words, if a number is larger than 2⁶⁴-1, it can\u0026rsquo;t be declared as the size_t type.\nUsage: Given a variable that stores the number of elements of an array, or number of bytes of an object, it can be set as the size_t type, as the name indicated that size_t is used for representing a \u0026ldquo;size\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;cstddef\u0026gt; // For size_t #include \u0026lt;typeinfo\u0026gt; int main() { size_t arraySize = 10; // unsigned int (16 bits at least) int myArray[arraySize]; // a 10-integer array size_t sizeOfArray = sizeof(myArray); // 40 bytes // The type of `arraySize` is unsigned integer const std::type_info\u0026amp; typeInfo = typeid(arraySize); std::cout \u0026lt;\u0026lt; typeInfo.name() \u0026lt;\u0026lt; std::endl; // return: m } m refers to unsigned long integer. The returned strings are defined differently across various compilers. So, the correct meaning should be found in the implementation. Strange output of std::typeid::name() - SO Do not use typeid The returned string is not consistent; Need to check the documents.\nRef: typeid(xxxx).name() \u0026ldquo;m\u0026rdquo; returned? - C++ Forum\ncppreference.com Returning the Name and Value of a C Type Using typeid() Reserve Memory (2024-01-26)\nPad the lastCount value to a multiple of the alignment 128, and then allocate the following count bytes, which is the number of elements of the target data pointed by dataPtr:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // \u0026#34;reserve_memory.cpp\u0026#34; #include \u0026lt;torch/torch.h\u0026gt; template \u0026lt;typename T\u0026gt; // T can be int, float..., deduced when compiling void accum(char*\u0026amp; lastCount, T*\u0026amp; dataPtr, size_t count, size_t alignment) { // Decimal arithmetic size_t offset = (reinterpret_cast\u0026lt;std::uintptr_t\u0026gt;(lastCount) + alignment-1) \u0026amp; ~(alignment-1); // Update the pointer to the data to be filled dataPtr = reinterpret_cast\u0026lt;T*\u0026gt;(offset); // Accumulate the number of bytes of the data lastCount = reinterpret_cast\u0026lt;char*\u0026gt;(dataPtr+count); } int main(){ char* size = nullptr; // Count from 0x0 float* depth; // sizeof(float) = 4 bytes float* color; accum(size, depth, 100, 128); // 0 + 100*4 = 400 bytes = 0x190 bytes printf(\u0026#34;%p \\n\u0026#34;, size); // 0x190 accum(size, color, 100*3, 128); // 512 + 300*4= 1712 bytes = 0x6b0 bytes printf(\u0026#34;%p \\n\u0026#34;, size); // 0x6b0 return 0; } char*\u0026amp; size = nullptr: the variable size is an alias of the nullptr, sharing the same memory.\nSuch that the pointer (lastCount and dataPtr) is changed directly.\nCompilation. Ref: Troubles while compiling C++ program with PyTorch, HElib and OpenCV - SO\n1 2 3 4 5 6 7 8 g++ -g -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ -L/usr/local/libtorch/lib \\ -Wl,-rpath,/usr/local/libtorch/lib \\ reserve_memory.cpp \\ -ltorch -ltorch_cpu -lc10 # Order matters: After .cpp file doubt: If I add -O2 option, the libraries -ltorch -ltorch_cpu -lc10 can be omitted. Don\u0026rsquo;t know why. With -O2 optimization, debugging will become difficult.\nThe original anwser: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 g++ -g -O2 -std=c++17 \\ -pthread \\ -march=native \\ -I/home/lulu/helib_install/helib_pack/include \\ -I/usr/include/opencv4 \\ -I/home/lulu/libtorch/include \\ -I/home/lulu/libtorch/include/torch/csrc/api/include \\ -I/home/lulu/libtorch/include/torch \\ -L/home/lulu/helib_install/helib_pack/lib \\ -L/usr/include/opencv4 \\ -L/home/lulu/libtorch/lib \\ -Wl,-rpath,/home/lulu/libtorch/lib \\ prova.cpp \\ -lopencv_core -lopencv_highgui -lopencv_imgcodecs \\ -lhelib -lntl -lgmp -lm \\ -ltorch -ltorch_cpu -lc10 \\ -o prova ","date":"2023-11-09T18:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/c++_pointer/","title":"memo: C++ | Pointer Usages"},{"content":"auto Compiler will evaluate the expression and use the result\u0026rsquo;s type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // \u0026#34;auto1.cpp\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;typeinfo\u0026gt; using namespace std; int main(){ int a = 1, b = 2; auto s = a + b; // compiler will compute a+b and assign the type of result to s. cout \u0026lt;\u0026lt; \u0026#34;Sum = \u0026#34; \u0026lt;\u0026lt; sum \u0026lt;\u0026lt;endl; cout \u0026lt;\u0026lt; typeid(s).name() \u0026lt;\u0026lt; endl; return 0; } Build: g++ auto1.cpp -o auto1\nMultiple variables followed by auto should be initialized with a common type.\n1 2 3 4 auto i = 0, *p = \u0026amp;i; // pointer variable p stores the address of i, // And the type of i is deduced from the value 0, auto* p = \u0026amp;i; // ok Reference:\nCodes from The auto Type Specifier in C++ - Neso Academy Placeholder type specifiers (since C++11) - cppreference Vector (2023-11-10)\n1 2 3 4 5 6 7 8 9 #include \u0026lt;vector\u0026gt; void main(){ std::vector\u0026lt;int\u0026gt; myVec; myVec = {1,2,3,4,5}; for (int i:myVec) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } std::vector is a class template (class maker), from which the specific class is derived with additional specifications.\nTemplate in C++ is like the parent class in Python to some extent: compile-time polymorphism v.s. runtime polymorphsim. But templates are not functions or classes.\nSpecifications follow the template name and are enclosed by angle brackets.\nA std::vector is a container for a sequence of objects.\nA std::vector\u0026rsquo;s definition doesn\u0026rsquo;t need number of elements, because it\u0026rsquo;s a dynamic array with adjustable size, achieved by allocating new memory and copying data as needed.\nPass a customized type to std::vector\u0026lt;custom_type\u0026gt;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; struct Vertex{ float x,y,z; }; // Overload the \u0026lt;\u0026lt; operator for Vertex struct std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; stream, const Vertex\u0026amp; v){ stream \u0026lt;\u0026lt; v.x \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; v.y \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; v.z; return stream; } int main(){ std::vector\u0026lt;Vertex\u0026gt; vertices; vertices.push_back({1,2,3}); vertices.push_back({4,5,6}); // Range-based for loop // for (Vertex v : vertices) // will copy each vertex for (const Vertex\u0026amp; v : vertices) // reference won\u0026#39;t copy std::cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; std::endl; // Delete the 2nd element vertices.erase(vertices.begin() + 1); for (int i=0; i \u0026lt; vertices.size(); i++) std::cout \u0026lt;\u0026lt; vertices[i] \u0026lt;\u0026lt; std::endl; // Clear the vector vertices.clear(); // size = 0 std::cout \u0026lt;\u0026lt; vertices.size() \u0026lt;\u0026lt; std::endl; } Reference:\nThe Vector Type in C++ - Youtube - Neso Academy Dynamic Arrays in C++ (std::vector) - Youtube - The Cherno vector insert() Function in C STL - GeeksforGeeks Iterator Ref:\nITERATORS in C++ - The Cherno Raw Function Pointer A representation for a function with an alias. For example, myFunc(int i) is represented as: void (*alias)(int)\nA similar thing in Python:\n1 2 3 4 5 6 norm = torch.nn.LayerNorm myModel = nn.Sequential( nn.Linear(3,5), norm(5) ) The class name is assigned to a variable, which is an alias (reference). And objects are instantiated later by specifying arguments.\nThe instantiation is deferred.\nFuntion pointer stores a function\u0026rsquo;s address in the compiled binary code.\nIn C++, to define a function pointer variable, its type can be deduced by auto:\n1 2 3 4 5 6 7 8 9 10 #include\u0026lt;iostream\u0026gt; void MyFunc(int a){ std::cout \u0026lt;\u0026lt; \u0026#34;Hello\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; } int main(){ auto pFunc = MyFunc; // No (), otherwise calling. pFunc(1); } The \u0026ldquo;Function Pointer\u0026rdquo; type is void(*)(args). And a function pointer variable is void(*var_name)(args).\n1 2 3 4 int main(){ void(*pFunc)(int) = MyFunc; pFunc(1); } And it can be rewritten as a type by typedef:\n1 2 3 4 5 int main(){ typedef void(*MyFunc_type)(int); MyFunc_type pFunc = MyFunc; pFunc(1); } When passing a function to another function, the function pointer parameter should be defined as void (*func_name)(args_list):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; void PrintInt(int i){ std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; } void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, void(*func)(int) ){ # Apply func to each element in the vector for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,2,3}; ForEach(myVec, PrintInt); } For compactness, the function PrintInt can be written as a one-line (anonymous) lambda function :\n1 2 3 4 5 6 7 8 9 void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, void(*func)(int) ){ for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,2,3}; ForEach(myVec, [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;} ); } Pass an outside variable a into the lambda function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;string\u0026gt; void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; v, const std::function\u0026lt;void(int)\u0026gt;\u0026amp; func){ for (int i : v) func(i); } int main(){ std::vector\u0026lt;int\u0026gt; myVec = {1,3,5,6,0}; std::string a = \u0026#34;Current element:\u0026#34;; ForEach(myVec, [\u0026amp;a](int i){ std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}); } [\u0026amp;a] captures the outside variable a by reference without copying data.\nOutput 1 2 3 4 5 6 7 yi@yi:~/Downloads/Cpp_Study$ g++ function_pointer.cpp yi@yi:~/Downloads/Cpp_Study$ ./a.out Current element:1 Current element:3 Current element:5 Current element:6 Current element:0 Ref:\nFunction Pointers in C++ - YouTube - The Cherno Lambda expression (2023-11-11)\n[](){}:\nSquare brackets [] is for variables outside the scope of {} lambda function\n[\u0026amp;] all variables are captures by references. [=] all variables are captured by values. Parentheses () enclose parameter to be used in {} lambda function\nCurly braces {} enclose operations.\nUsage:\nIf there is a function pointer, lambda function can be in place of it. Example using lambda expression as a bool:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; int main(){ std::vector\u0026lt;int\u0026gt; myVec = {2,0,5,4,3}; auto it = std::find_if(myVec.begin(), myVec.end(), [\u0026amp;](int val){return val \u0026gt; 3;} ); std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; std::endl; // Output: 5 } std::find_if examins myVec to find the fisrt element that is larger than 3 and returns an iterator it.\nIterator object it is the address of the first element in the container. And it+1 points to the 2nd element.\nSo, it needs to be dereferenced to get the value.\nRef:\nLambdas in C++ - The Cherno Lambda expressions - cppreference std::find, std::find_if, std::find_if_not - cppreference.com std::function (2023-11-11)\nA representation for a kind of callable objects, such as normal function, functor, lambda expression, struct.\n\u0026ldquo;Same kind\u0026rdquo; refers to having the same return type and input arguments, as indicated by the function signature.\n1 2 3 4 5 6 7 8 9 // Normal function can be represented as a std::function: void MyFunc(int x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; }; std::function\u0026lt;void(int)\u0026gt; f = MyFunc; // Lambda expression is assigned to a variable: std::function\u0026lt;void(int)\u0026gt; fl = [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}; // Equivalent to a function pointer: auto pf = [](int i){std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}; The std::function\u0026lt;void(int)\u0026gt; represents any callable object that has no return value and only one int input argument.\nTherefore, it can be used as the type for a formal parameter when defining a function that takes as input a specific kind of function.\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; void funcsPrintNum(int x, std::function\u0026lt;void(int)\u0026gt; func){ func(x); } int main(){ auto myLambda = [](int i){ std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; }; funcsPrintNum(1, myLambda); } A vector storing multiple callable objects that have an identical function signature.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;vector\u0026gt; void aNormalFunc(int i){ std::cout \u0026lt;\u0026lt; i + 5 \u0026lt;\u0026lt; std::endl;} int main(){ std::vector\u0026lt;std::function\u0026lt;void(int)\u0026gt;\u0026gt; vf; vf.push_back([m=5](int i){std::cout \u0026lt;\u0026lt; m \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl;}); vf.push_back(aNormalFunc); vf[0](1); // Call the 1st function with the argument 1 vf[1](4); // Call the 2nd function } Output 1 2 51 9 Calling a function from a std::function is slower than calling it natively. This is the cost of unifying representation for callable objects with the same input and output.\nRef:\nC++ std::function Next Level Function Polymorphism - The Builder Template (2023-11-14)\nThe template argument typename T (a type) will be deduced when compiling and the corresponding function will be created to link.\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; template \u0026lt;typename T\u0026gt; // before the return type of the func void Print(T value){ std::cout \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; } int main(){ Print(5); Print(5.5f); Print(\u0026#34;A string\u0026#34;); } When calling a template in program, the teamplate argument \u0026lt;T\u0026gt; can be omitted as it can be deduced by compiler.\nOutput 1 2 3 5 5.5 A string The following int N is required at compile time, because an array created on stack needs size known when compiling. And it can be deduced when evaluating the template:\n1 2 3 4 5 6 7 8 9 10 11 12 template \u0026lt;typename T, int N\u0026gt; class ArrayClass{ private: // visible inside the class T m_Array[N]; public: // visible outside the class int GetSize() const {return N;} }; int main(){ ArrayClass\u0026lt;int, 5\u0026gt; myArray; std::cout \u0026lt;\u0026lt; myArray.GetSize() \u0026lt;\u0026lt; std::endl; } Ref:\nTemplates in C++ - YouTube - The Cherno Class vs Struct \u0026ldquo;Struct is the same as class.\u0026rdquo; \u0026ndash; CLASSES vs STRUCTS in C++ - YouTube - The Cherno\nDefault members (without setting visibility) of a class are all private.\nIn contrast, members in a struct are all public by default.\nStruct also can set members (variables and methods) to private.\nStruct can do inheritance as well, but not common. Struct is often used to group variables.\ndoubt: Why does a struct can include itself? (2024-01-27) Not itself. fromChunk is a function and its return type is the stuct. The code is from 3DGS.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct GeometryState { size_t scan_size; float* depths; char* scanning_space; bool* clamped; int* internal_radii; float2* means2D; float* cov3D; float4* conic_opacity; float* rgb; uint32_t* point_offsets; uint32_t* tiles_touched; static GeometryState fromChunk(char*\u0026amp; chunk, size_t P); }; Bitwise Operator (2023-11-14)\n~\n1 2 3 4 5 6 7 8 9 #include \u0026lt;iostream\u0026gt; #include \u0026lt;bitset\u0026gt; int main(){ std::size_t alignment = 128; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;(alignment) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;(alignment-1) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::bitset\u0026lt;32\u0026gt;( ~ (alignment-1)) \u0026lt;\u0026lt; std::endl; } Output:\n1 2 3 00000000000000000000000010000000 00000000000000000000000001111111 11111111111111111111111110000000 Ref:\nhow to output an int in binary? - SO #define (2023-11-15)\nShort for a for loop\n1 2 3 4 5 #define fori(x) for(int i=0; i\u0026lt;x; i++) fori(20){ ... } Ref: Why use #define instead of a variable\n#define 100_000 won\u0026rsquo;t work. #define 100000 is ok.\nHeader files (2023-11-15)\nFunctions\u0026rsquo; signature must be written in the file to use them.\nThose declarations can be stored in header files.\nThe preprocess directive #include will copy and paste a header file to there.\nHeader files can form a chain, whereas a potential probelm is repeated difinations. (e.g. structs need unique names.)\nThe header guide #pragma once marks a header file won\u0026rsquo;t be included multiple times into a single translation unit (cpp file).\nOld fasion is wrapping the entire header file with #ifndef symbol and #endif\nSuffix .h is used to differentiate \u0026ldquo;c standard library\u0026rdquo; (\u0026lt;stdio.h\u0026gt;) and \u0026ldquo;c++ standard libray\u0026rdquo; (\u0026lt;iostream\u0026gt;)\nAngular brackets \u0026lt;fname\u0026gt; wll search the \u0026ldquo;standard include directories\u0026rdquo; (of compiler) for the file named fname.\nWhile double qutoes \u0026quot;../myHeader.h\u0026quot; normaly enclose a relative path, although it can enclose a file that resides in \u0026ldquo;standard include directories\u0026rdquo; as well (e.g., \u0026quot;iostream\u0026quot;).\nDo not use namespace in a header file. Why I don\u0026rsquo;t \u0026ldquo;using namespace std\u0026rdquo; - The Cherno Ref:\nC++ Header Files - The Cherno Source file inclusion - cppreference.com Compiling \u0026amp; Linking Compiling:\nConvert cpp files (translation units) to binary object file.\nPreprocess statements (directives) will be done when compiling.\n#include just copy and paste to where it is. #define replace symbol #if 1 (or 0) and #endif to use a code snippest or not. Once all preprocess statements are finished, the preprocessed \u0026ldquo;full\u0026rdquo; code will be tranformed to an object file\nMultiple cpp files can be combined into a single cpp file by #include, and then only one translation unit will be generated.\nHowever, if every cpp file doesn\u0026rsquo;t include others, each cpp file will have an translation unit.\nEach translation unit yields a object file\nconstant folding: constant arithmetics will be solved at compile time, such that there is only 1 asm instruction: put a number into a register.\nRef: How the C++ Compiler Works\nLinking:\nFind the binary code of functions, symbols and entry point main, according to their \u0026ldquo;function signatures\u0026rdquo;, and then form a executable file.\nCompiling is to preprocess and generate object file. Linking is organizing binary code to a executable fiel. Build = Compiling + Linking Link error and compile error\nError code starts with \u0026ldquo;LNK\u0026rdquo; means it\u0026rsquo;s an linking error:\nFunction signature mismatch with the definition (name, return type, parameters), thus the binary code can\u0026rsquo;t be found: LNK2019 Unresolve externel symbol\nNo entry point definition (e.g., main()) for Application(.exe).\nDuplicate symbols (function signature, variables) are found in the project\u0026rsquo;s obj files. LNK1169 One or more funcname defined symbols found\nEven if a function isn\u0026rsquo;t called by main(), it could be called in other cpp files.\nTherefore, linker also needs to link those \u0026ldquo;uncalled\u0026rdquo; functions, unless it has static keysword representing it will only be called internally in the cpp file it exists.\nError code starts with \u0026ldquo;C\u0026rdquo; means it\u0026rsquo;s an compiling error:\nSyntax errors, e.g., missing ; (C2143) No declaration or definition for functions used in a cpp file. Multiple definitions in a single file. Multiple definition if a function is defined in a header file without static or inline limitation, because #include is just pasting header file.\n\u0026ldquo;Log.h\u0026rdquo;:\n1 2 3 4 5 #pragma once #include \u0026lt;iostream\u0026gt; void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } \u0026ldquo;Log.cpp\u0026rdquo;\n1 2 3 4 #include \u0026#34;Log.h\u0026#34; // Log definition will be here void InitLog(){ Log(\u0026#34;Initialization\u0026#34;); } \u0026ldquo;main.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 #include \u0026#34;Log.h\u0026#34; // Log definition will be here int Multiply(int\u0026amp; a, int\u0026amp; b){ Log(\u0026#34;Multiplication:\u0026#34;); return a*b; } int main(){ int c = Multiply(5, 2); } There won\u0026rsquo;t be compiling error, since each cpp file knows functions to be used. But this project cannot be linked since there two Log function with the same signature, and the linker don\u0026rsquo;t know which one should be used.\nThere are 3 soulutions:\nstatic will limit the function used only in the file it resides in and won\u0026rsquo;t be visible to any other obj files.\n1 2 3 static void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } That means even though \u0026ldquo;Log.cpp\u0026rdquo; and \u0026ldquo;main.cpp\u0026rdquo; both have the definition of void Log(const char*), they use their own Log. The linker won\u0026rsquo;t be confused.\ninline will replace the calls of the function with its body:\n1 2 3 inline void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } Define the function in one of translation units. And header file only contain declarations.\nMove the definition into \u0026ldquo;Log.cpp\u0026rdquo;:\n1 2 3 void Log(const char* x){ std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } Ref: How the C++ Linker Works - The Cherno\nFind Next MSB (2023-11-17)\nFind the next-highest bit of the MSB (Most Significant Bit)\nRight shift:\n1 2 3 4 5 6 7 #include \u0026lt;iostream\u0026gt; #include \u0026lt;bitset\u0026gt; int main(){ uint16_t x = 32767; // 01111111_11111111 std::cout \u0026lt;\u0026lt; (x \u0026gt;\u0026gt; 16) \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; (x \u0026gt;\u0026gt; 12) \u0026lt;\u0026lt; std::endl; // 0000000_0000111 } Parse Args (2023-11-03)\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(int argc, char **argv) { std::cout \u0026lt;\u0026lt; \u0026#34;Number of input arguments: \u0026#34; \u0026lt;\u0026lt; argc \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt;= argc-1; i++) { std::cout \u0026lt;\u0026lt; argv[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } return 0; } Scope Resolution (2024-02-01)\nScope resolution operator: :: | Microsoft Learn :: : Scope resolution operator in C++ - GeeksforGeeks\nDifferentiate global and locatl variables with the same name; Identify classes with the same name in different namespace. Define a member function outside the class Access a class\u0026rsquo;s static member Distinguish members with the same name reside in multiple classes with inheritances. Refer to nesting class. . is used for member of object. Member access operators - cppreference\na-\u0026gt;b, where a is a pointer, equals to ((*a).b). In contrast, a.b where a is an object. member access operators - C++ Forum\nType Casting (2024-02-03)\n(uint32_t) is forceful casting in terms of the bits reading. whereas static_cast\u0026lt;uint32_t\u0026gt; cannot be applied on data with bits interpretion mismatched. Type conversions and type safety - Microsoft Learn\n1 2 3 4 5 6 7 #include \u0026lt;iostream\u0026gt; int main(){ float depth = 15.7; std::cout \u0026lt;\u0026lt; (uint32_t)depth; // 15 std::cout \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(depth); // 15 std::cout \u0026lt;\u0026lt; reinterpret_cast\u0026lt;uint32_t\u0026gt;(depth); //error } ","date":"2023-11-09T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/lang/c++_misc/","title":"memo: C++ | Misc"},{"content":" The 2 submodules are CUDA projects that should be debugged separately from the python project, because the python debugger can\u0026rsquo;t access the PyTorch CUDA extensions. Debug Settings (2023-11-08)\nIntellisense To enable intellisense, the configuration of \u0026ldquo;C/C++ extension\u0026rdquo; must be put in the \u0026ldquo;.vscode/\u0026rdquo; of (top-level) current working directory (.vscode/c_cpp_properties.json), not in submodule\u0026rsquo;s \u0026ldquo;.vscode/\u0026rdquo; (submodules/diff-gaussian-rasterization/.vscode/c_cpp_properties.json).\nOtherwise, the settings won\u0026rsquo;t be loaded.\nCUDA syntax (e.g.,__global__) and header (\u0026lt;cooperative_groups/reduce.h\u0026gt;, min) won\u0026rsquo;t be recognized, if includePath and compiler are incorrect,\nDoubtful Attempts Put the includes paths of torch before cuda\u0026rsquo;s include. Otherwise #include \u0026lt;cooperative_groups/reduce.h\u0026gt; in \u0026ldquo;forward.cu\u0026rdquo; can\u0026rsquo;t be found.\nSet the includePath in \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo; as:\n1 2 3 4 5 6 7 \u0026#34;includePath\u0026#34;: [ \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34;, \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/cuda-11.6/include\u0026#34; ], Error in \u0026ldquo;rasterizer_impl.cu\u0026rdquo;: cannot open source file glm/glm.hpp (dependency of cub/cub.cuh)\n1 sudo apt-get install libglm-dev Why can\u0026rsquo;t C++ find GLM headers? - SO\nError in \u0026ldquo;backward.cu\u0026rdquo;: namespace \u0026quot;cooperative_groups\u0026quot; has no member \u0026quot;this_grid\u0026quot;\nChange CMakelist: SO\nDidn\u0026rsquo;t fix\nError: identifier \u0026quot;min\u0026quot; is undefinedC/C++(20)\nDidn\u0026rsquo;t fix\nSolution: Use nvcc as compiler. Verified \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/include/python3.7m\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], \u0026#34;defines\u0026#34;: [], \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/local/cuda-11.6/bin/nvcc\u0026#34;, \u0026#34;cStandard\u0026#34;: \u0026#34;c17\u0026#34;, \u0026#34;cppStandard\u0026#34;: \u0026#34;gnu++14\u0026#34;, \u0026#34;intelliSenseMode\u0026#34;: \u0026#34;linux-gcc-x64\u0026#34; } ], \u0026#34;version\u0026#34;: 4 } Debug diffRast (2023-11-18)\nAction: Write a \u0026ldquo;main.cpp\u0026rdquo; that calls the methods of the compiled library \u0026ldquo;CudaRasterizer\u0026rdquo; for debugging. Repo for debugging DiffRast: debug_diff_rust\n\u0026ldquo;CMakeLists.txt\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cmake_minimum_required(VERSION 3.20 FATAL_ERROR) project(MyApp) # ${PROJECT_NAME} find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\u0026#34;) add_subdirectory(external/diff-gaussian-rasterization-comments) add_executable(MyApp main.cpp) set_property(TARGET MyApp PROPERTY CXX_STANDARD 17) target_link_libraries(MyApp \u0026#34;${TORCH_LIBRARIES}\u0026#34;) target_link_libraries(MyApp CudaRasterizer) target_sources(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization-comments/rasterize_points.cu) target_include_directories(MyApp PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/external/diff-gaussian-rasterization-comments) \u0026ldquo;launch.json\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DiffRast Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/build/MyApp\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, } ] } \u0026ldquo;tasks.json\u0026rdquo;\nopen 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;cmake-configure\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-DCMAKE_BUILD_TYPE=Debug\u0026#34;, \u0026#34;-DCMAKE_PREFIX_PATH=/usr/local/libtorch\u0026#34;, \u0026#34;..\u0026#34;, ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;cmake-build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;cmake\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--build\u0026#34;, \u0026#34;.\u0026#34;, // \u0026#34;--config\u0026#34;, // \u0026#34;Debug\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Set the build directory }, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;Build with cmake\u0026#34;, \u0026#34;dependsOn\u0026#34;: [\u0026#34;cmake-configure\u0026#34;, \u0026#34;cmake-build\u0026#34;] } ] } Step into CUDA Kernels Building project as above won\u0026rsquo;t allow to step into CUDA kernels, e.g., preprocessCUDA.\n(2024-01-20)\nEnable CUDA language in CMAKE and set for debugging:\n1 2 3 4 5 project(MyApp LANGUAGES CXX CUDA) set(CMAKE_BUILD_TYPE Debug) if(CMAKE_BUILD_TYPE STREQUAL \u0026#34;Debug\u0026#34;) set(CMAKE_CUDA_FLAGS \u0026#34;${CMAKE_CUDA_FLAGS} -g -G\u0026#34;) endif() Build to binary file:\n1 2 3 # cd ~/Downloads/debug_diff_rast cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -GNinja cmake --build ./build Launch cuda-gdb: cuda-gdb ./build/MyApp. It\u0026rsquo;s convenient to use cuda-gdb in the terminal of vscode, where I can jump to the code by clicking the path.\nAdd breakpoint inside preprocessCUDA: (cuda-gdb) b forward.cu:182 Not sure if it worked.\n(2024-01-26) Nsight (CUDA-GDB)\nGenerate Makefile with cmake:\n1 2 cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; cmake --build ./build To let cmake produce Makefile, do not use -GNinja. Why isn\u0026rsquo;t the command \u0026ldquo;cmake .\u0026rdquo; generating a makefile? - SO (2024-04-23)\nError: \u0026ldquo;Caffe2: Cannot find cuDNN library\u0026rdquo;\nMessage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 (gaussian_splatting) yi@yi-Alien:~/Downloads/debug_diff_rast$ cmake -B ./build -DCMAKE_PREFIX_PATH=/usr/local/libtorch -G\u0026#34;Unix Makefiles\u0026#34; -- The CXX compiler identification is GNU 9.4.0 -- The CUDA compiler identification is NVIDIA 11.6.55 -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Detecting CUDA compiler ABI info -- Detecting CUDA compiler ABI info - done -- Check for working CUDA compiler: /usr/local/cuda-11.6/bin/nvcc - skipped -- Detecting CUDA compile features -- Detecting CUDA compile features - done -- Found CCache: /usr/local/bin/ccache -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found CUDA: /usr/local/cuda-11.6 (found version \u0026#34;11.6\u0026#34;) -- Caffe2: CUDA detected: 11.6 -- Caffe2: CUDA nvcc is: /usr/local/cuda-11.6/bin/nvcc -- Caffe2: CUDA toolkit directory: /usr/local/cuda-11.6 -- Caffe2: Header version is: 11.6 -- Could NOT find CUDNN (missing: CUDNN_LIBRARY_PATH CUDNN_INCLUDE_PATH) CMake Warning at /usr/local/libtorch/share/cmake/Caffe2/public/cuda.cmake:120 (message): Caffe2: Cannot find cuDNN library. Turning the option off Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include) /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:19 (find_package) -- /usr/local/cuda-11.6/lib64/libnvrtc.so shorthash is 280a23f6 -- Autodetected CUDA architecture(s): 6.1 -- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61 CMake Error at /usr/local/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:96 (message): Your installed Caffe2 version uses cuDNN but I cannot find the cuDNN libraries. Please set the proper cuDNN prefixes and / or install cuDNN. Call Stack (most recent call first): /usr/local/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package) CMakeLists.txt:19 (find_package) -- Configuring incomplete, errors occurred! I didn\u0026rsquo;t encounter this error before.\nThere is no libcudnn.so under directory: /usr/local/cuda-11.6/lib64/\nlibcudnn.so does\u0026rsquo;t apper in the output of python -m torch.utils.collect_env as shown in issue#30\nDownload cuDNN Library for CUDA11 Ubuntu 20.04. Referring to this answer: spconv - issues#277\n1 2 3 4 5 wget https://developer.download.nvidia.com/compute/cudnn/9.1.0/local_installers/cudnn-local-repo-ubuntu2004-9.1.0_1.0-1_amd64.deb sudo dpkg -i cudnn-local-repo-ubuntu2004-9.1.0_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu2004-9.1.0/cudnn-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cudnn-cuda-11 The libcudnn and cudnn are installed under:\n1 2 3 4 5 6 7 8 9 10 11 12 13 (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ whereis libcudnn libcudnn: /usr/lib/x86_64-linux-gnu/libcudnn.so (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ whereis cudnn cudnn: /usr/include/cudnn.h (gaussian_splatting) yi@yi:~/Downloads/debug_diff_rast$ dpkg -l | grep cudnn ii cudnn-local-repo-ubuntu2004-9.1.0 1.0-1 amd64 cudnn-local repository configuration files ii cudnn9-cuda-11 9.1.0.70-1 amd64 NVIDIA cuDNN for CUDA 11 ii cudnn9-cuda-11-8 9.1.0.70-1 amd64 NVIDIA cuDNN for CUDA 11.8 ii libcudnn9-cuda-11 9.1.0.70-1 amd64 cuDNN runtime libraries for CUDA 11.8 ii libcudnn9-dev-cuda-11 9.1.0.70-1 amd64 cuDNN development headers and symlinks for CUDA 11.8 ii libcudnn9-static-cuda-11 9.1.0.70-1 amd64 cuDNN static libraries for CUDA 11.8 After installation, the cmake configuration works fine.\nI didn\u0026rsquo;t copy it to /usr/local/cuda/lib64 or /usr/local/cuda/include, like: CuDNN not found while compiling PyTorch C++ extension - Forum\nI didn\u0026rsquo;t set environment variable neither.\nCreate tasks.json\nFollowing this tutorial: Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;label\u0026#34;: \u0026#34;CUDA Make\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;make dbg=1\u0026#34;, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true }, \u0026#34;problemMatcher\u0026#34;: [ \u0026#34;$nvcc\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34; // Makefile }, }, And then click the menu bar in vscode \u0026ldquo;Terminal\u0026rdquo; -\u0026gt; \u0026ldquo;Run Build Task \u0026hellip;\u0026rdquo; -\u0026gt; select \u0026ldquo;CUDA Make\u0026rdquo;.\nCreate launch.json:\n1 2 3 4 5 6 { \u0026#34;name\u0026#34;: \u0026#34;CUDA: Debug with CUDA-GDB\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/MyApp\u0026#34;, }, Select the configuration: \u0026ldquo;CUDA: Debug with CUDA-GDB\u0026rdquo; beside the \u0026ldquo;Run\u0026rdquo; button. Then, click \u0026ldquo;Run\u0026rdquo; to debug.\nThe program can hit the breakpoint in the kernel function preprocessCUDA, for example: float3 p_view; at \u0026ldquo;diff-gaussian-rasterization/cuda_rasterizer/forward.cu#192\u0026rdquo;\n","date":"2023-11-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-debug/","title":"read: 3DGS | Debug Code"},{"content":"Source video: 21. Eigenvalues and Eigenvectors - Gilbert Strang - MIT OpenCourseWare\n(2023-11-06) The 1st-time recitation note. Haven\u0026rsquo;t re-check.\nOut direction same as in Matrix multiply vector acting like MLP, where all current dimensions are combined in different weights to produce each dimension in another space.\n$$ 𝐀 𝐗 \\\\ \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} = 𝐀 \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} $$\nMatrix 𝐀 is like a function that takes as input 𝐗 and output 𝐀𝐗. Typically, 𝐀𝐗 points in different direction from the input 𝐗. Eigenvectors are those 𝐗 make 𝐀𝐗 parallels to 𝐗. Parallel means scaling: 𝐀𝐗 = λ𝐗, where the multiplier λ is eigenvalue. Given a matrix 𝐀, how to solve its eigenvalues λ and eigenvectors 𝐗?\nIf λ = 0, 𝐗 solved from 𝐀𝐗 = 0 by elimination is 𝟎, which is useless. Some hints can be found through the following examples\nProjection matrix Given a projection matrix 𝐏, a 3D vector is projected on to a plane.\nz x p l a n e y Eigenvectors 𝐗 should parallel to the projected vectors 𝐏𝐗. So the vectors located in the plane all are eigenvectors, as $𝐏𝐗=𝐗$, with λ=1.\nVectors perpendicular to the plane statisfy: $𝐏𝐗=0$, i.e., λ=0.\nThose two sets of eigenvectors are perpendicular.\nPermutation matrix Given the 2D permutation matrix $𝐀=[^{0 \\ 1}_{1 \\ 0}]$, which vector can make 𝐀𝐗=𝐗, i.e., identical after permutation.\n𝐗 = $[^1_1]$, with λ=1.\nAs 𝐀 is a 2D matrix, there should be 2 eigenvalues. Which vector can statisfy λ=-1 ?\n𝐗 = $[^{-1}_1]$\nThe 2 sets of eigenvectors 𝐗 are perpendicular as well.\nFact: The sum of eigenvalues equals to the sum of elements on diagonal of 𝐀.\nHere, there is 1 + (-1) = 0 + 0.\nHow to solve λ, 𝐗 Trick: Rearrange to 𝐀𝐗 - λ𝐗 = 0.\nIf there is $(𝐀 - λ𝐈)𝐗 = 0$ and 𝐗, a non-zero vector, becomes 0 after multiplied by (𝐀 - λ𝐈), the (𝐀 - λ𝐈) must be sigular: determinant is 0.\nThe formula $𝐀 - λ𝐈 = 0$ doesn\u0026rsquo;t include 𝐗, so eigenvalues λ can be solved first, as in the example below.\nPlus multiple identity Given a matrix 𝐀 = $[^{3 \\ 1}_{1 \\ 3}]$, to calculate λ, solve:\n$$ \\begin{aligned} | 𝐀 - λ𝐈 | = 0 \\\\ \\begin{bmatrix} 3-λ \u0026amp; 1 \\\\ 1 \u0026amp; 3-λ \\end{bmatrix} = 0 \\\\ (3-λ)^2 -1 = 0 \\\\ λ^2 - 6λ + 8 = 0 \\\\ (λ-4)(λ-2) = 0 \\end{aligned} $$\nTwo roots: λ₁=4, λ₂=2. Then solve the 2 sets of eigenvectors.\nFor λ₁=4,\n$$ \\begin{aligned} (𝐀 - λ𝐈)𝐗 = 0 \\\\ \\begin{bmatrix} -1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix} 𝐗 = 0 \\\\ \\end{aligned} $$\nBy letting the free variable to 1, 𝐗 can be solved as $[^1_1]$.\nHence, one of eigenvectors is $[^1_1]$\nFor λ₂=2,\n$$ \\begin{aligned} (𝐀 - λ𝐈)𝐗 = 0 \\\\ \\begin{bmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \\end{bmatrix} 𝐗 = 0 \\\\ \\end{aligned} $$\nHence, one of eigenvectors is $[^{-1}_1]$\nThe sum of the two eigenvalues is the trace of 𝐀, and their product is the determinant of 𝐀.\nComparing the matrix $[^{3 \\ 1}_{1 \\ 3}]$ and the permutation matrix $[^{0 \\ 1}_{1 \\ 0}]$, there is:\nA $[^{3 \\ 1}_{1 \\ 3}]$ $[^{0 \\ 1}_{1 \\ 0}]$ λ 4 and 2 1 and -1 𝐗 $[^1_1]$ and $[^{-1}_1]$ $[^1_1]$ and $[^{-1}_1]$ If $𝐀𝐗 =λ𝐗$, then $(𝐀+3𝐈)𝐗= 𝐀𝐗+3𝐈𝐗 = (λ+3𝐈)𝐗$\nThat means, if 𝐀 plus 3𝐈, then eigenvalues λ will plus 3𝐈, while eigenvectors doesn\u0026rsquo;t change.\nCan\u0026rsquo;t generalize However, th above property (for multiple identity) can\u0026rsquo;t be generalized to plusing an arbitrary matrix 𝐁.\nMatrix addition doesn\u0026rsquo;t imply eigenvalues addition, because the eigenvector of 𝐁 is typically not 𝐗. Therefore, the following addition can\u0026rsquo;t be performed:\n$$ 𝐀𝐗 =λ𝐗 \\\\ 𝐁𝐗 =α𝐗 \\\\ (𝐀+𝐁)𝐗 = (λ+α)𝐗 $$\nComplex eigenvalues Considering rotation matrix:\n$$ \\begin{aligned} \\begin{bmatrix} cosθ \\\\ sinθ \\end{bmatrix} = \\begin{bmatrix} cosθ \u0026amp; -sinθ \\\\ sinθ \u0026amp; cosθ \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{aligned} $$\nLet θ be 90°, the rotation matrix is $[^{0 \\ -1}_{1 \\ 0}]$.\nHowever, according to the theory \u0026ldquo;eigenvector outcomes in the direction that it went in\u0026rdquo;, there seems to be no eigenvector intuitively, as any vector gets in will rotate 90 degree by this matrix.\nSolve its eigenvalues:\n$$ \\begin{aligned} | 𝐀 - λ𝐈 | = 0 \\\\ |\\begin{bmatrix} 0 \u0026amp; -1 \\\\ 1 \u0026amp; 0 \\end{bmatrix} - \\begin{bmatrix} λ \u0026amp; 0 \\\\ 0 \u0026amp; λ \\end{bmatrix}| = \\begin{vmatrix} -λ \u0026amp; -1 \\\\ 1 \u0026amp; -λ \\end{vmatrix} = 0 \\\\ λ^2 + 1 = 0 \\end{aligned} $$\nTwo roots: λ₁=i, λ₂=-i\nAnti-symmetric matrix has imaginary eigenvalues which are always in pairs, as they\u0026rsquo;re complex conjugate. While a symmetrix matrix\u0026rsquo;s eigenvalues are all real numbers.\nSolve eigenvectors 𝐗:\nFor λ₁=i,\n$$ \\begin{aligned} (𝐀 - λ𝐈) 𝐗 = 0 \\\\ \\begin{bmatrix} 0-λ₁ \u0026amp; -1 \\\\ 1 \u0026amp; 0-λ₁ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\begin{bmatrix} -i \u0026amp; -1 \\\\ 1 \u0026amp; -i \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\end{aligned} $$\nLet the free variable b=1, then $[^a_b] = [^i_1]$\nFor λ₁= -i,\n$$ \\begin{aligned} \\begin{bmatrix} i \u0026amp; -1 \\\\ 1 \u0026amp; i \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\end{aligned} $$\nLet the free variable b=1, then $[^a_b] = [^{-i}_1]$\nRepeated eigenvalues Eigenvalues of a triangular matrix is obvious: the diagonal, as the determinant is directly factorized.\nFor example, given matrix $𝐀 = [^{3 \\ 1}_{0 \\ 3}]$:\nSolve its eigenvalues:\n$$ \\begin{aligned} | 𝐀 - λ𝐈 | = 0 \\\\ \\begin{vmatrix} 3-λ \u0026amp; 1 \\\\ 0 \u0026amp; 3-λ \\end{vmatrix} = 0 \\\\ (3-λ)(3-λ) \\end{aligned} $$\nTwo roots: λ₁=3, λ₂= 3\nSolve eigenvectors:\nFor λ₁=3,\n$$ \\begin{aligned} (𝐀 - λ𝐈) 𝐗 = 0 \\\\ \\begin{bmatrix} 3-λ₁ \u0026amp; 1 \\\\ 0 \u0026amp; 3-λ₁ \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\\\ \\begin{bmatrix} 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = 0 \\end{aligned} $$\nSo $[^a_b]$ can be $[^1_0]$\nFor λ₂= 3, same formulas appear. There isn\u0026rsquo;t the 2nd (set of) independent eigenvectors.\nBut there are supposed to be two eigenvalues. It\u0026rsquo;s incomplete yet.\n","date":"2023-11-06T13:51:00Z","permalink":"https://zichen34.github.io/writenotes/calc/eigenvalues_vectors/","title":"watch: LA - G.S. 21 | Eigenvalues \u0026 Eigenvectors"},{"content":"vars() Create a class ParamGroup to store args into instance variables, then use vars(self) to read memebers from self.__dict__.\nAdd each argument into parser through parser.add_argument().\nSet shorthand to the initial character, e.g., --source_path use -s\nExample from gaussian-splatting:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class ParamGroup: def __init__(self, parser: ArgumentParser, name : str, fill_none = False): group = parser.add_argument_group(name) for key, value in vars(self).items(): shorthand = False if key.startswith(\u0026#34;_\u0026#34;): shorthand = True key = key[1:] t = type(value) value = value if not fill_none else None if shorthand: if t == bool: group.add_argument(\u0026#34;--\u0026#34; + key, (\u0026#34;-\u0026#34; + key[0:1]), default=value, action=\u0026#34;store_true\u0026#34;) else: group.add_argument(\u0026#34;--\u0026#34; + key, (\u0026#34;-\u0026#34; + key[0:1]), default=value, type=t) else: if t == bool: group.add_argument(\u0026#34;--\u0026#34; + key, default=value, action=\u0026#34;store_true\u0026#34;) else: group.add_argument(\u0026#34;--\u0026#34; + key, default=value, type=t) class ModelParams(ParamGroup): def __init__(self, parser, sentinel=False): self.sh_degree = 3 self._source_path = \u0026#34;\u0026#34; self._model_path = \u0026#34;\u0026#34; self._images = \u0026#34;images\u0026#34; self._resolution = -1 self._white_background = False self.data_device = \u0026#34;cuda\u0026#34; self.eval = False super().__init__(parser, \u0026#34;Loading Parameters\u0026#34;, sentinel) (2024-04-03)\nIn this way, there won\u0026rsquo;t be a long list of parser.add_argument() declaiming all arguments.\nInstead, related arguments are arranged into a group.\nCustomize Parsing (2023-09-27)\nAnalyse string manually\nRefer to Match-NeRF for an example.\n","date":"2023-11-04T13:40:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_args_parse/","title":"memo: Python | Parse Input Args"},{"content":"From C to CUDA Source article: CUDA 01 | 第一个程序 - Master KangKang的文章 - 知乎\nCUDA extends Cpp to GPU.\nC lang: \u0026ldquo;add.c\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 #include\u0026lt;stdio.h\u0026gt; int add(int a, int b){ int c = a + b; return c; } int main(){ int c = add(2, 3); printf(\u0026#34;c = %d\\n\u0026#34;, c); return 0; } CUDA: \u0026ldquo;add.cu\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include\u0026lt;stdio.h\u0026gt; __global__ void add(int a, int b, int *c){ *c = a + b; } int main(){ int c; // Allocate Unified Memory - accessible from CPU or GPU int* c_cuda; // a memory for a int data cudaMalloc((void**)\u0026amp;c_cuda, 1*sizeof(int)); //return pointer // Launch kernel on 1 block containing 1 thread add\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(2, 3, c_cuda); // Transfer data between GPU VRAM and CPU RAM cudaMemcpy(\u0026amp;c, c_cuda, sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;c=%d\\n\u0026#34;, c); // Free the allocated unified memory cudaFree(c_cuda); return 0; } Adapt C to CUDA (kernel function):\nThe function add is declared as a __global__ function, and then it becomes a kernel, which is called from CPU and executed on GPU.\nKernels have no return value, as results are left in memory.\nThus, the GPU memory for output data must be allocated (by cudamalloc) and passed into the kernel. And free it (with cudaFree) at the end. A kernel needs execution configuration: \u0026lt;\u0026lt;\u0026lt;blocks, threads, shared_mem, stream\u0026gt;\u0026gt;\u0026gt; to sequentialize the host code before host compilation. Docs\nCompile\n1 2 3 4 5 6 7 8 # For C project gcc add.c -o add_c # For CUDA project nvcc add.cu -o add_cuda # Profiler nvprof ./add_cuda Blocks \u0026amp; Threads CUDA locates a thread via blocks.\nNumber of threads in a block is a multiple of 32, e.g., 256 threads. An Even Easier Introduction to CUDA - Nvidia Blog - Mark Harris, Jan25, 2017\nTotal threads can be reshaped to 2D or 3D, and accordingly the kernel needs to modify the threads indexing.\nNumber of blocks must be larger than the total elements.\nGiven N elements, there needs n = (N + blkDim.x - 1)/blkDim.x\nd a t a 0 b l 1 k 1 2 3 b l 4 k 2 5 ⋯ ⋯ ⋯ b l k n - 1 N - 1 b l ⋯ ⋯ k ⋯ ⋯ ⋯ ⋮ ⋯ n ⋯ ⋯ - ⋯ ⋯ ⋮ ' Index of a thread is blockIdx.x * blockDim.x + threadIdx.x.\nAnd a grid includes all the threads = gridDim.x * blockDim.x. (grdiDim in the following figure may be wrong.)\nRef\nTutorial 01: Say Hello to CUDA - CUDA Tutorial - Read the Docs\nTriple chevrons: How is the CUDA\u0026laquo;\u0026lt;\u0026hellip;\u0026raquo;\u0026gt;() kernel launch syntax implemented - SO\nExample Source article: An Easy Introduction to CUDA C and C++ - Nvidia Blog - Mark Harris, Oct31, 2012\nEach thread handles a single element in an array. Glossary SM: Streaming Multiprocessor Docs\ndim3: An integer vector based on uint3. Programming Guide\nTo do\nGetting Started With CUDA for Python Programmers\ncudaMemcpy (2024-02-02)\nDoc: NVIDIA CUDA Library | Example from 3DGS\nSpecify dest and src pointers, number of bytes to be transferred, and direction of copy.\n1 2 3 4 5 int num_rendered; // Declare and Allocate memory on the host cudaMemcpy(\u0026amp;num_rendered, // points to destination area geomState.point_offsets + P - 1, // points to source area sizeof(int), // copy an signed int (4 bytes) cudaMemcpyDeviceToHost) // direction Shared Memory Docs\n(2024-02-05)\nShared by threads within a thread block. Synchronization Docs-#7.6\n__syncthreads is a \u0026ldquo;checkpoint\u0026rdquo; to wait all thread arrive at this point.\nNVIDIA CUDA Tutorial 8: Intro to Shared Memory -Ytb - Creel\n","date":"2023-11-03T12:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/basic/","title":"memo: CUDA | Basics"},{"content":"Docs of Nsight: Getting Started with the CUDA Debugger\nDebug Demo (2023-11-03)\nEnvironment: Ubuntu 20.04, cuda-11.6 (in /usr/local/cuda-11.6), GPU 1050Ti.\nnvcc\n1 2 3 4 5 6 (base) yi@yi-Alienware:~/Downloads/CUDA_Study/Debug_CUDA$ nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Tue_Mar__8_18:18:20_PST_2022 Cuda compilation tools, release 11.6, V11.6.124 Build cuda_11.6.r11.6/compiler.31057947_0 Prerequisite:\nInstall 2 extensions: Nsight and C/C++\nCreate 2 debugging configuration files: launch.json and tasks.json under \u0026ldquo;.vscode/\u0026rdquo;\nSelect debugger: CUDA C++ (CUDA-GDB) Example with nvcc Testing repo\n\u0026ldquo;test.cu\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int main(int argc, char **argv) { std::cout \u0026lt;\u0026lt; \u0026#34;Number of input arguments: \u0026#34; \u0026lt;\u0026lt; argc \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt;= argc-1; i++) { std::cout \u0026lt;\u0026lt; argv[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } return 0; } \u0026ldquo;launch.json\u0026rdquo; for debug configurations:\nSet program as the output binary program to be debugged:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;CUDA C++: Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}/test.bin\u0026#34;, // binary file \u0026#34;preLaunchTask\u0026#34;: \u0026#34;mynvcc\u0026#34; }, // no need to change this: { \u0026#34;name\u0026#34;: \u0026#34;CUDA C++: Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cuda-gdb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34; } ] } \u0026ldquo;tasks.json\u0026rdquo; for building configurations:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;mynvcc\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;nvcc\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;${file}\u0026#34;, \u0026#34;-g\u0026#34;,\u0026#34;-G\u0026#34;, \u0026#34;-o\u0026#34;,\u0026#34;${fileDirname}/test.bin\u0026#34;, ] } ] } Error: /usr/bin/bash: nvcc: command not found\nAdd command on PATH before running startup scripts. tasks - VSCode - Docs\nTried add export PATH=\u0026quot;$PATH:/usr/local/cuda-12.3/bin\u0026quot; into \u0026ldquo;/etc/environment\u0026rdquo;, \u0026ldquo;/etc/profile\u0026rdquo;, \u0026ldquo;/etc/xprofile\u0026rdquo;, \u0026ldquo;/etc/bash.bashrc\u0026rdquo; all doesn\u0026rsquo;t work. How to permanently set $PATH on Linux/Unix -SO\nSolution: Set integrated terminal in user settings.json (My vscode version: 1.83.1, 2023-11-03)\n1 2 3 4 5 6 7 8 9 \u0026#34;terminal.integrated.profiles.linux\u0026#34;: { \u0026#34;bash\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;bash\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-i\u0026#34; ] } }, \u0026#34;terminal.integrated.defaultProfile.linux\u0026#34;: \u0026#34;bash\u0026#34;, Ref: VSCode tasks error: /bin/bash: npm: command not found (Found by DDG with searching \u0026ldquo;vscode debug tasks.json /usr/bin/bash: nvcc: command not found\u0026rdquo;)\nThen, with the \u0026ldquo;test.cu\u0026rdquo; file opening in the editor, click the start button to initiate debugging.\n(2024-01-26) I still don\u0026rsquo;t know how to include headers for libtorch in the CLI of nvcc. So, I didn\u0026rsquo;t manage to compile the 3DGS project with nvcc as above.\nPotentially useful:\nAn example: Include path problems for GPU library - SO Docs of nvcc: NVIDIA CUDA Compiler Driver NVCC (2024-01-27)\nBased on Troubles while compiling C++ program with PyTorch, HElib and OpenCV - SO, and reminded by perplexity, -Wl and -rpath are used in GCC for linking and specifying runtime library search path. In contrast, nvcc has -Xlinker for linking during compilation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Compile CUDA source files into object files with nvcc nvcc --compile -g -G -std=c++17 \\ -I/usr/local/libtorch/include \\ -I/usr/local/libtorch/include/torch/csrc/api/include \\ -I/usr/local/libtorch/include/torch \\ main_copy.cu \\ -o main.o # Link object files into an executable with g++ g++ main.o \\ -L/usr/local/libtorch/lib \\ -L/usr/local/cuda/lib64 \\ -Wl,-rpath,/usr/local/libtorch/lib \\ -ltorch -ltorch_cpu -lc10 -lcudart \\ -o my_executable Compiling is OK. But linking reports error:\n1 2 3 /usr/bin/ld: main.o: in function `main\u0026#39;: /home/yi/Downloads/debug_diff_rast/main_copy.cu:31: undefined reference to `RasterizeGaussiansCUDA(at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, float, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, at::Tensor const\u0026amp;, float, float, int, int, at::Tensor const\u0026amp;, int, at::Tensor const\u0026amp;, bool, bool)\u0026#39; collect2: error: ld returned 1 exit status Not successful yet. Need to compile the library with nvcc.\nExample with Makefile Nvidia Tutorial Clip: Debugging CUDA kernels with VS Code\nMicrosoft VS CUDA Support in Visual Studio Code with Julia Reid\n(2024-01-26)\nUse cmake to produce Makefile, otherwise, error occurs: make *** no targets specified and no makefile found. stop\nAnd then edit the launch.json and tasks.json following this article: Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\nRefer to my repo for debugging 3DGS.\nExample with CMake Debugging CUDA kernels with VS Code\nRef\nSource articles: CUDA 番外篇 | Visual Studio Code的CUDA环境 - Master KangKang的文章 - 知乎 Adapted demo: vscode远程调试Linux CUDA程序- oushaojun2 - CSDN Debug Cuda Samples (2023-11-02)\nDownload sample project: NVIDIA/cuda-samples for 12.3\n1 git clone https://github.com/NVIDIA/cuda-samples.git Make 12.3 failed with 11.6\n1 2 cd ./cuda-samples make dbg=1 Error: /usr/bin/ld: cannot find -lglut\nNeed: sudo apt-get install freeglut3 freeglut3-dev\nError:\n1 2 3 4 5 6 /usr/bin/ld: simpleCUFFT_callback.o: in function `main\u0026#39;: /home/yi/Downloads/cuda-samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu:103: undefined reference to `cudaGetDeviceProperties_v2\u0026#39; collect2: error: ld returned 1 exit status make[1]: *** [Makefile:373: simpleCUFFT_callback] Error 1 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback\u0026#39; make: *** [Makefile:45: Samples/4_CUDA_Libraries/simpleCUFFT_callback/Makefile.ph_build] Error 2 cudaGetDeviceProperties_v2 is not existed in cuda 11.x, but appear in cuda 12.2. SO\ncuda-sample-11.6 Download zip: Release pkg 11.6; Git tag-11.6\nmake\n1 2 3 4 5 6 7 (base) yi@yi-Alienware-Aurora-R8:~/Downloads/cuda-samples-11.6$ make dbg=1 make[1]: Entering directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/3_CUDA_Features/ptxjit\u0026#39; /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -g -G --threads 0 --std=c++11 -gencode arch=compute_35,code=compute_35 -o ptxjit.o -c ptxjit.cpp nvcc fatal : Unsupported gpu architecture \u0026#39;compute_35\u0026#39; make[1]: *** [Makefile:396: ptxjit.o] Error 1 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/3_CUDA_Features/ptxjit\u0026#39; make: *** [Makefile:45: Samples/3_CUDA_Features/ptxjit/Makefile.ph_build] Error 2 Devices with compute capacity (cc) 3.x have been dropped by cuda 12.x. Solution is removing the requests of compute_35 in the make file. Forum Nv\nCuda Toolkit is compatible the devices with lower cc than it supports. CUDA 11.x supports a maximum cc of 8.x. CSDN\n(2023-11-02) Remove cc of 35 and 37 SO:\nReplace all the pattern SMS ?= 35 37 with SMS ?= through VSCode. Replace all the pattern compute_35 with compute_61 Make failed:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 /usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 -g -G --std=c++11 --threads 0 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o reduction_kernel.o -c reduction_kernel.cu reduction_kernel.cu(558): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(558): error: expected an identifier __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(558): warning #1835-D: attribute \u0026#34;__shared__\u0026#34; does not apply here __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ Remark: The warnings can be suppressed with \u0026#34;-diag-suppress \u0026lt;warning-number\u0026gt;\u0026#34; reduction_kernel.cu(558): error: expected a \u0026#34;;\u0026#34; __attribute__((shared)) cg::experimental::block_tile_memory\u0026lt;sizeof(T), BlockSize\u0026gt; scratch; ^ reduction_kernel.cu(561): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name auto cta = cg::experimental::this_thread_block(scratch); ^ reduction_kernel.cu(561): error: identifier \u0026#34;scratch\u0026#34; is undefined auto cta = cg::experimental::this_thread_block(scratch); ^ reduction_kernel.cu(563): error: name followed by \u0026#34;::\u0026#34; must be a class or namespace name auto multiWarpTile = cg::experimental::tiled_partition\u0026lt;MultiWarpGroupSize\u0026gt;(cta); ^ 6 errors detected in the compilation of \u0026#34;reduction_kernel.cu\u0026#34;. make[1]: *** [Makefile:358: reduction_kernel.o] Error 255 make[1]: Leaving directory \u0026#39;/home/yi/Downloads/cuda-samples-11.6/Samples/2_Concepts_and_Techniques/reduction\u0026#39; make: *** [Makefile:45: Samples/2_Concepts_and_Techniques/reduction/Makefile.ph_build] Error 2 VSCode didn\u0026rsquo;t find header with red underlines:\n1 2 3 #include errors detected. Please update your includePath. Squiggles are disabled for this translation unit (/home/yi/Downloads/cuda-samples-11.6/Samples/2_Concepts_and_Techniques/reduction/reduction_kernel.cu).C/C++(1696) cannot open source file \u0026#34;cooperative_groups/reduce.h\u0026#34;C/C++(1696) Edit \u0026ldquo;c_cpp_properties.json\u0026rdquo; as:\n1 2 3 4 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/usr/local/cuda-11.6/include\u0026#34; ], Didn\u0026rsquo;t solve. And the header is there and can be found.\n(2023-11-03) Intellisense erros for CUDA syntax.\nEnsure selecting lang as \u0026ldquo;CUDA C++\u0026rdquo; rather than \u0026ldquo;C++\u0026rdquo;. VSCode Nsight Intellisense not detecting functions and datatypes for *cu; *cpp works - NV forum\nRed underlines disappeared after the C/C++ extension got disabled and only \u0026ldquo;Nsigh\u0026rdquo; extension left. But these 2 extensions both are installed in every tutorial.\n(2023-11-03)No error in a folder contains only .cu files. Thus, Python code can\u0026rsquo;t co-exist with CUDA code?\n(2023-11-08) ✅ Refering the \u0026ldquo;.vscode/c_cpp_properties.json\u0026rdquo; in the CUDA sample: \u0026ldquo;Samples/0_Introduction/matrixMul\u0026rdquo;, the compiler should be nvcc, not \u0026ldquo;/usr/bin/gcc\u0026rdquo;.\n1 \u0026#34;compilerPath\u0026#34;: \u0026#34;/usr/local/cuda/bin/nvcc\u0026#34;, Getting Started with the CUDA Debugger :: NVIDIA Nsight VSCE Documentation\nSame error about reduction here issue#201. But he was 11.8.\nRe-install cuda toolkit 11.8 and test samples of 11.8.\nNote: the final line of the installing scripts provided on official site should be: sudo apt-get -y install cuda-11-8 instead of sudo apt-get -y install cuda\nError persists at reduction_kernel.cu.\nBuild with CMake (2023-11-17)\nThe library \u0026ldquo;diff-gaussian-rasterization-comments/cuda_rasterizer\u0026rdquo; is built according to CXX standard.\nHowever, I cannot step into the CUDA kernels when debugging.\nMaybe using nvcc to build can enable debugging.\nCreate a CMakeLists.txt using nvcc?\n","date":"2023-11-02T17:44:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/debug/","title":"memo: CUDA | Debugging"},{"content":"CosAnelWrmRst Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import torch from torch import nn from torch.utils.data import TensorDataset, DataLoader from matplotlib import pyplot as plt import numpy as np inps = torch.arange(50.).expand(10,-1).reshape(100, 5) tgts = torch.arange(50.).expand(10,-1).reshape(100, 5) dataset = TensorDataset(inps, tgts) loader = DataLoader(dataset, batch_size=1, pin_memory=True) model = nn.Sequential(nn.Linear(5,5)) criterion = torch.nn.MSELoss() optimizer = torch.optim.AdamW(model.parameters()) scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( optimizer, T_0=6, # First cycle is 6 epochs T_mult=2, # next cycle will be 2x epochs (int) eta_min=1e-5, # minimum lr last_epoch=-1, # set when resuming verbose=False) # print lr eps = np.arange(100) lrs = [] for idx, batch in enumerate(loader): x = batch[0] y = batch[1] y_pred = model(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step() lrs.append(scheduler.get_last_lr()) scheduler.step() plt.plot(eps, lrs) plt.title(\u0026#34;CosineAnnealingWarmRestarts\u0026#34;) scheduler.step(0) will set lr to the value at epoch 0. WarmUp + CosAnelWrmRst Ref: firstelfin/WarmUpLR\nThe original CosineAnnealingWarmRestarts doesn\u0026rsquo;t have warmup. WarmUpLR followed by CosineAnnealingWarmRestarts\n1 2 cosine = CosineAnnealingWarmRestarts(**param) warm_up_lr = WarmUpLR(cosine) The first 9 epochs use WarmUpLR, and the following use CosineAnnealingWarmRestarts.\nGallery A Visual Guide to Learning Rate Schedulers in PyTorch - Medium - Leonie Monigatti\n(2023-10-30)\nMax_lr Decay qu-gg/pytorch-cosine-annealing-with-decay-and-initial-warmup\nFound by github searching \u0026ldquo;CosineAnnealingWarmRestart\u0026rdquo;. Results\n","date":"2023-10-24T17:53:52Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_lr_sched/","title":"memo: PyTorch | LR \u0026 Scheduler"},{"content":"Code | Arxiv(2303) | ProjPage\nBriefs Author\u0026rsquo;s Talk Source Video: 【Talk | ICLR'23 Oral 美国东北大学马旭：图像亦是点集（Image as Set of Points）】\nInsights:\nUse clustering to extract image features.\nFusing cluster members through Collapse followed by Reconstruction\nPaper Explain Source Video: 【ICLR 2023】Image as Set of Points.计算机视觉新范式，利用聚类的思想实现图像建模。在多个下游任务上不输ViT和ConvNets】\nImage features are refined through multiple aggregation and dispatching using attention for pixels. Paper Notes Abstract Is the picture natively well-clustered?\nIf so, this method essentially is same as convolution, which extracts features by fusing pixels in the kernel.\nBut Cluster is discreate and sparse, can it capture high-level feature?\nClusters appeals to segmentation and interpretability.\nIntro Is this method designed for a single image?\nHowever, MVS can directly leverage epipolar lines on source views more concisely, without fusing neighbor pixels features..\nPixels with same color can imply fundamentally difference?\n\u0026ldquo;Similar pixels are grouped, but they are fundamentally different.\u0026rdquo; ?\nI think same color means rays hit same spatial geometry.\nMethod Pipeline:\nInitial feature of raw pixels are 5-D, including color (r,g,b) ∈ [0,255] and normalized pixel center (x,y) ∈ [0, 1]-0.5\nPixels are reduced to 𝑟 times at each stage\u0026rsquo;s start by fusing k (=4 or 9) neighbors around the 𝑟 evenly distributed anchors across the image, i.e., concatenating their channels and projecting to specified dimension by FC, unlike maxpooling.\nConv layer can perform points reduction as well if points\u0026rsquo; organization aligned with the raw image.\nIn their implementation, function PointReducer uses a conv layer instead of FC.\nFor model coc_base_dim64, before stage-0, image is reduced to 1/16 with kernels of size=4, i.e., 16 points are fused to 1. and 5-D features are mixed to 64-D.\n1 2 # Point Reducer is implemented by a layer of conv since it is mathmatically equal. nn.Conv2d(5, 64, kernel_size=4, stride=4, padding=0) Output points require reording for downstream pixel-wise tasks, like segmentation.\nCoC Block:\nEach stage repeats Context Cluster block several time. A blocks processes points set in 3 steps:\nPixels clustered $c$ groups in the feature space\nc centers are evenly distributed and form voronoi based on the cosine similarity of each point to each center feature.\nCenters are chose through nn.AdaptiveAvgPool2d((proposal_w, proposal_h)), which will set kernel and stride automatically for AvgPool2d.\nCenter feature is the average of k nearest neighbors, after placing the c centers.\nIntial features are 5-D including RGB and position (x,y).\nAggregation: Add features of all members to an aggregated feature g:\n$$g = \\frac{1}{C} (v_c + ∑_{i=1}^m sig(α s_i + β) * v_i )$$\nThe aggregated feature g is computed by plusing center\u0026rsquo;s feature and the weighted sum of feature vectors of all m points vᵢ in a cluster, scaled by a tunable factor $sig(α sᵢ + β)$ ∈ (0,1), where s is similarity to the center feature and α,β are nn.parameters.\nThe denominator $C = 1+ ∑_{i=1}^m sig(α sᵢ+ β)$ aims to limit the magnitude.\nDispatching: Each member updates its feature from the aggregated feature, so as to fuse all other points and realize spatial interaction.\n$$p_i\u0026rsquo; = p_i + FC(sig (α sᵢ+ β) * g )$$\nThe amount of g assigned to a member is determined by the adaptive similarity again, inversing the summation.\nCommunication between pixel in a cluster is like server-client in a centralized network.\nCenters\u0026rsquo;s positions are fixed for efficiency, so it emphasizes locality.\ndoubt: Advanced postional embedding could be applied.\ndoubt: Will different selection strategies affect model performance? They mentioned Farthest Point Sampling (FPS) mehtod in appx.D\nArchitecture:\nContext Cluster is a hierarchical model composed 4 stages and points are reduced to 1/4 (ie, h/2, w/2) after each stage.\nPlay Model can be comprehended by debugging the file \u0026ldquo;context_cluster\u0026rdquo;, using environment of \u0026ldquo;AIM\u0026rdquo;.\n\\begin{algorithm} \\begin{algorithmic} \\STATE PointReducer: Conv2d(x), downsample 16 times, 256 dim \\STATE Partition feature maps: rearrange(x) \\STATE Centers feature from x: AdaptiveAvgPool2d((2,2))(x) \\STATE Simlarity matrix: vectors' inner product with multi-head \\STATE Clustering: .scatter\\_ \\STATE Aggregate feature $g$: sum members' feat based on similarity \\STATE Dispatch $g$ to members \\STATE Reverse partition \\STATE Project to out\\_dim: Conv2d \\STATE FFN: Mlp, out\\_dim → hidden → out\\_dim \\end{algorithmic} \\end{algorithm} Similarity and points\u0026rsquo; features are optimized separately:\nSimilarity: x → center → sim\nFeatures: x → value → val_center → aggregated feature out\nsim and out are decoupled.\n","date":"2023-10-15T13:40:00Z","image":"https://pic2.zhimg.com/80/v2-2aac08c8a30f4726f3eb32b608a95589_720w.webp","permalink":"https://zichen34.github.io/writenotes/model/misc/b-note-image_as_points/","title":"read: Image as Set of Points"},{"content":" (Discussed in QQ group 706949479) Code | Arxiv | ProjPage\nAuthor\u0026rsquo;s blog: ICCV 2023 NeRF提点的Magic Loss 即插即用 —— S3IM随机结构相似性 - Summer Clover的文章 - 知乎 Notes Abs Previous NeRF didn\u0026rsquo;t utilize structural information on image level, but train and predict point-wise.\nMethod = 3 1 = 6 1 9 = 2 3 = = 2 5 = 3 2 S 5 - 4 = 3 - 1 3 ‖ ‖ ‖ ‖ ‖ ‖ ‖ ⬇ I - = - M 8 - 0 = 9 - 4 ₁ 7 - = 1 - = = 2 7 = 4 6 0 = 6 8 R e o r d e r = 9 7 = 6 2 1 = 8 0 = = 0 4 = 4 8 S - 6 = - 7 3 ‖ ‖ ‖ ‖ ‖ ‖ ‖ ⬇ I - = - M 2 - 2 = 6 - ₂ 1 - 5 = 2 - 3 = = 5 3 = 1 1 4 3 = 9 3 R e o r d e r Steps:\nApply SSIM on the randomly selected training pixel patch with a kernel size $K$ (=2) and stride size S (=K).\nRepeatedly reorder the predicted and target pixel patchs, and calculate S3IM multiple ($M$=10) times.\nThe final loss term is the average of them multiplied with a weight factor (hyperparameter) $λ$.\n$$ \\rm L_{S3IM} = λ ⋅ (1 - \\frac{1}{M} \\sum_{m=1}^M SSIM(Patch_{rendered}, Patch_{target}) ) $$\nCompare with SSIM:\nS3IM applied on random pixel patches significantly outperforms SSIM applied on local continuous patches.\nThe authors explain this as the SSIM can only capture the local similarity, whereas S3IM can compare the nonlocal structural similarity over all training images.\nTraining NeRF with local continuous patches will hurt the performance (as stated at the end of section 3.1).\nPlay Code\n1 2 3 4 5 6 7 8 9 s3im_func = S3IM(kernel_size=args.s3im_kernel, stride=args.s3im_stride, repeat_time=args.s3im_repeat_time, patch_height=args.s3im_patch_height, patch_width=args.s3im_patch_width).cuda() if args.s3im_weight \u0026gt; 0: s3im_pp = args.s3im_weight * s3im_func(rgb_map, rgb_train) total_loss += s3im_pp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class S3IM(torch.nn.Module): r\u0026#34;\u0026#34;\u0026#34;Implements Stochastic Structural SIMilarity(S3IM) algorithm. Arguments: kernel_size (int): kernel size in ssim\u0026#39;s convolution(default: 4) stride (int): stride in ssim\u0026#39;s convolution(default: 4) repeat_time (int): repeat time in re-shuffle virtual patch(default: 10) patch_height (height): height of virtual patch(default: 64) patch_width (height): width of virtual patch(default: 64) \u0026#34;\u0026#34;\u0026#34; def __init__(self, kernel_size=4, stride=4, repeat_time=10, patch_height=64, patch_width=64): super(S3IM, self).__init__() self.kernel_size = kernel_size self.stride = stride self.repeat_time = repeat_time self.patch_height = patch_height self.patch_width = patch_width self.ssim_loss = SSIM(window_size=self.kernel_size, stride=self.stride) def forward(self, src_vec, tar_vec): r\u0026#34;\u0026#34;\u0026#34; src_vec: (ray_batch_size=4096=64*64, 3) \u0026#34;\u0026#34;\u0026#34; loss = 0.0 index_list = [] for i in range(self.repeat_time): if i == 0: tmp_index = torch.arange(len(tar_vec)) # (4096) index_list.append(tmp_index) else: ran_idx = torch.randperm(len(tar_vec)) index_list.append(ran_idx) res_index = torch.cat(index_list) # (M * ray_bs = 10*4096) tar_all = tar_vec[res_index] # (10*4096, 3) src_all = src_vec[res_index] tar_patch = tar_all.permute(1, 0).reshape(1, 3, self.patch_height, self.patch_width * self.repeat_time) src_patch = src_all.permute(1, 0).reshape(1, 3, self.patch_height, self.patch_width * self.repeat_time) loss = (1 - self.ssim_loss(src_patch, tar_patch)) return loss ","date":"2023-10-14T10:30:00Z","image":"https://miro.medium.com/v2/resize:fit:1400/1*SBVkh54RJZrMsozG-vQdCQ.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-s3im/","title":"read: S3IM: Stochastic SSIM Loss for NeRF"},{"content":"(2023-10-15)\nTalk-221121 Source video: 丁霄汉：结构重参数化是怎么来的【深度学习】【直播回放】-bilibili\nRepVGG: (2021)\nVGG has bad performance with fast inference because single stream can run in parallel efficientlly.\nAnd multiple branches means multiple sets of parameters, which help achieve better precision.\nIf a set of parameters can be transformed equivalently to another set of parameters, the corresponding structure would changed naturally.\nTherefore, the multi-branches architecture during training can be transformed to a single branch model in inference period.\nMethodology: Kernel size can vary while computation remains, e.g., a 1×1 kernel can be reshaped to 3×3\nThus, 3 branches with a 3×3 kernel, a 1×1 kernel, and a 3×3 identity kernel can be added to a single 3x3 kernel based on the property of linearity of convolution: $x * K_a + x * K_b = x * (K_a+K_b)$ Centripetal SGD: (2017)\nInception: Compressing (pretrained?) models by pruning redundent channels in feature maps. To create identical channels, let optimizer (SGD) guide some channels to become similar. Two same channels merged to a comprehensive channel, model gets concise while performance unchanged. Linear Redundancy Unit (Obsolete)\nMerge 2 feature maps: Training with two 3×3 kernels and merge the 2 kernels after training. This method brought marginal improvement though.\nThis indicates that two models with the same final structure, but experienced different training processes in different architectures, have different performances.\nAsymmetic Convolution Block (2019)\nBranches are different: 3×3 + 1×3 + 3×1 Research on Simple Models: (2020)\nUse identity branch to eliminate some shortcuts in ResNet.\nHow to make a ultimate simple yet powerful model without shortcut? ▶ RepVGG Multiple branches like InceptionNet in just a single kernel.\nWhy can it work in an arbitrary model? ▶ Diverse Branch Block (DBB) RepMLP: (2022)\nInject locality into MLP (CNN is a special MLP) by transforming an arbitrary conv kernel to a FC kerenel. RepLKNet: (2022)\nLarge kernel: 31x31 + 5x5 Misc:\nResRep for channel pruning.\nOutput channels can be controled through a 1x1 kernel after the original 3x3 kernel. Such that channel pruning can be performed on the 1x1 kernel. RepOptimizer: generalize to gradient reparameterization for fast training.\nIncorperating the prior knowledge (inductive bias) into optimizer instead of model structure. RepVGGplus: principles behind RepVGG Ideas moving forward:\nConnect Structural Rep with every element in a general vision model:\nTopology (RepVGG), Component (ACNet, DBB), Width (ResRep), Globality v.s. locality (RepMLP), Kernel size (RepLKNet), Optimizer (RepOptimizer)\nRethink classical problems.\nSimple model, like VGG, doesn\u0026rsquo;t work? (RepVGG) Can\u0026rsquo;t train a super deep model without shortcut? (RepVGGplus) Inception Net is too complex to be abandoned? (DBB) MLP can\u0026rsquo;t handle image tasks? (RepMLP) Large kernels are less effective? (RepLKNet) Related works:\nNon-deep Network; RepNAS, YOLO v6\u0026amp;v7, DyRep, Scaling up Kernels in 3D GNNs, RepUNet, RepSR (superres), De-IReps. Talk-220426 Source video: 【论文连讲：用重参数化赋予MLP网络局部性、超大卷积核架构【CVPR2022】【基础模型】】- bilibili\n(2023-10-16)\nRepMLPNet \u0026ldquo;一种采用重参数化技术引入局部性的分层 MLP 网络\u0026rdquo;\nCode | Arxiv MLP has no locality, only global capacity, thus it\u0026rsquo;s not favorable to do linear projection on 2D images.\nLocality means the surrounding pixels of a input pixel should have larger contributions due to stronger correlation compared to distant pixels.\nHowever, MLP treats all pixels on the image equally without considering relative positions, resulting in that MLP is difficult to converge for images data due to high dimensionailty and individually training for each pixel.\nCNN perserves this inductive bias through kernels. But CNN doesn\u0026rsquo;t have long-range dependencies because different regions share the same parameters: the kernel. Thus, a CNN stacks multiple layers for a large receptive field. In contrast, MLP is a function of positions, sensitive to location.\nHence, one approach to inject locality is by creating parallel branches with various conv kernels (for different dimensions) alongside the fc layer.\nBy supplementing conv kernels, the model is competent both at long-range dependency and locality for 2D images.\nThe side effect is the mutiple disunified branches will hinder computation parallelism, and impair the inference efficiency finally.\nThe solution to maintain the inference efficiency and perserve conv branches is Structural Reparameterization:\nMerging multiple auxiliary branches to a single FC stream can be realized by transforming their parameters after training into one FC kernel, such that the inference speed and precision are unchanged.\n通过参数的等价转换实现结构的等价转换。\nGeneric CNNs with conv kernels include massive parameters. And multiple branch of conv kernels may be unfeasible if without reducing parameters.\nThere are three branches: FC3, 3x3 \u0026amp; 1x1 kernels, and \u0026ldquo;Identity\u0026rdquo; Identity branch performs FC1+FC2 after maxpooling shrinks (H,W) to only (1,1).\nThus, the FC layer only need 1 parameter. Plusing 4 parameters in BatchNorm (mean, std, scale factor, bias), this branch only has 5 parameters.\nThis branch functions like a SE block (Squeeze-Excitation) providing channel-wise \u0026ldquo;overall scaling\u0026rdquo;.\n3x3 and 1x1 conv layer perform \u0026ldquo;set-sharing\u0026rdquo; (depth-wise conv + group convolution), where total of C channels are split to S groups.\nC S c g h r n o l u s p s 0 1 2 3 4 5 6 7 8 9 1 0 1 1 Then the number of parameters in a conv layer reduced from (C×H×W)² to S×(H×W)².\nThe main branch performs FC3 after depth-wise convolution for input feature maps.\nThe equivalent FC layer for a conv layer is required for adding conv layers to FC layer.\nFC kernel is the 2D weight matrix $W_{dₒ×dᵢ}$ in a linear layer.\nA 3D Conv kernel is a special FC kernel represented as a Toeplitz matrix, containing lots of shared parameters, so its associated FC kernel must exist.\nThen, 2 FC kernels can add up directly based on linearity.\nA FC layer processes a feature map through 4 steps: (n,c,h,w) ➔ (n, c×h×w) ➔ FC kernel ➔ (n, o×h×w) ➔ (n,o,h,w), denoted as: $\\rm MMUL(featmap, W_{dₒ×dᵢ})$, where dᵢ = c×h×w.\nA Conv layer with a 3D conv kernel $F$ and padding $p$ processes the feature map is denoted as $\\rm CONV(featmap, F,p)$\nThus, the problem is how to convert a 3D kernel to a 2D kernel.\nGiven the corresponding FC kernel of a conv kernel $W^{(F,p)}$, two operations are equivalent: $\\rm MMUL(featmap, W^{(F,p)}) = CONV(featmap,F,p)$\nConsidering a linear layer, it projects vectors: $\\rm V_{n×dₒ} = V_{n×dᵢ} ⋅W^{(F,q)\\ T}$\nInsert an identity matrix I:\n$$ V_{n×dₒ} = V_{n×dᵢ} ⋅I ⋅ W^{(F,q)\\ T} = V_{n, dᵢ} ⋅(I_{dᵢ×dᵢ} ⋅ W^{(F,q)\\ T}) $$\nThen, the term $(I⋅W^{(F,q)\\ T})$ can be regarded as a convolution operation.\nA conv operation must be a Mat-Mul, but a Mat-Mul may not be a conv operation.\nWhat kind of Mat-Mul (FC layer) is a conv operation? It\u0026rsquo;s when the weight matrix is a Toeplitz matrix transformed from a conv kernel.\nBecause $W^{(F,p)}$ is transformed indeed from conv kernel, the Mat-Mul $\\rm I⋅W^{(F,p)}$ is a convolution operation for sure.\n$$\\rm I_{dᵢ×dᵢ}⋅W^{(F,p)} ⇔ CONV(F,p,featmap)$$\nIn the convolution $I_{dᵢ×dᵢ}⋅W^{(F,p)}$, $I_{dᵢ×dᵢ}$ is convoled. Thus, it\u0026rsquo;s supposed to be the featmap in CONV(). i.e., the $I_{dᵢ×dᵢ}$ is reshaped from featmap $I_{(c×h×w, c, h, w)}$\nAdditional reshaping is needed to match the dimensionality:\n$$\\rm I_{dᵢ×dᵢ}⋅W^{(F,p)} = CONV(F,p,featmap).reshape(chw, c, h, w)$$\nFrom the above equation, the desired FC kernel $W^{(F,p)}$ is the result feature map of convolving the kernel F with a blank featmap:\n$$\\rm W^{(F,p)} = CONV(F,p,I_{(c×h×w, c, h, w)}).reshape(chw, c, h, w)$$\nFor example, if the conv kernel F is (c, o, (3,3)), then the corresponding FC kernel $W^{(F,p)}$ has shape: (o, h-3+2×p+1, w-3+2×p+1) = (c×h×w, o,h,w).\nThis \u0026ldquo;3D FC kernel\u0026rdquo; has finished the \u0026ldquo;sum\u0026rdquo; computation and gets waiting for Mat-Mul with the input feature maps.\nTo align with the squashed 2D input feature maps (n, c×h×w), it needs to be reshaped to 2D: (c×h×w, o×h×w).\nFinally, a 3D conv kernel becomes a 2D kernel.\nThe equivalent FC kernel of a conv kernel is the result of convolution on an identity matrix with proper reshaping.\nFuse the parameters (μ,σ,γ,β) of BatchNorm into convolution layer based on linearity.\n$$M\u0026rsquo; = γ⋅[(MF -μ)/σ] + β = γ⋅(MF)/σ + (β - γ⋅μ/σ)$$\nSo new kernel and bias: $F\u0026rsquo; = γ⋅F/σ, \\quad b\u0026rsquo; = (β - γ⋅μ/σ)$\nAfter that, bias-added conv kernels are converted to 2D kernels, which can be added up the main stream: FC3 kernel for inference with only MLP layers.\nResMLP-Net\nHierarchical design mimic popular vision models\nRepMLPBlock and FFN alternate.\nCan be used as the backbone for downstream tasks.\nAdjust the amount of parameters in each stage through \u0026ldquo;set-sharing\u0026rdquo;.\nNo need for large datasets (JFT300M) or many epochs (300~400) to train. (IN for 100 epochs).\nThroughput is higher than conventional CNN models. Speed has not much relation with the number of FLOPs.\nRepMLP is suitable for highly parallelized devices (GPU) rather than devices with lower computation capacity, like mobile.\n\u0026ldquo;Identity\u0026rdquo; branch is necessary for the performance with providing information in different scale and dimensions.\n\u0026ldquo;set-sharing\u0026rdquo; increase the number of groups will bring precision.\nLocality can be observed on the feature maps.\nRepMLPNet is robust for discontinuity between split patches from big images.\nThe resolution of Cityscapes dataset doesn\u0026rsquo;t match the pretrained model. They devided an entire image to small patches.\n1 2 3 4 5 class RepMLPNet: RepMLPNetUnit RepMLPBlock RepMLPBlock cannot resume training after model.locality_injection() because sub-modules have been deleted. Therefore, .locality_injection should be called with a new model before inference.\nBlog-210426 Source: 结构重参数化：利用参数转换解耦训练和推理结构 - 丁霄汉的文章 - 知乎\nBlog-210517 解读模型压缩6：结构重参数化技术：进可暴力提性能，退可无损做压缩 - 科技猛兽的文章 - 知乎\n矩阵乘法可以看做卷积：一个 2D 数据矩阵乘以 $W^{(F,p)}$，相当于这个数据矩阵先 reshape 成 4D 的 feature map 做卷积，结果再 reshape 成 2D. Papers FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization\nCode\n","date":"2023-10-13T20:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/misc/c-symp-struct_reparam/","title":"sympo: Structural Reparameterization"},{"content":"watch: CppExt - AI葵 01 | Cpp Bridges PyTorch \u0026amp; CUDA\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 1 - English CC -\nCode Instructions The pure purpose of CUDA extensions is to make PyTorch programs faster.\nCUDA extensions are more efficient than PyTorch in two scenarios:\nProcedures can\u0026rsquo;t be executed in parallel, e.g., each ray has different numbers of points.\nMany sequential computations, like a nn.Sequential module including lots of conv layers. C++ can fuse multiple layers to a single function.\nRelations: PyTorch will call a C++ function, which will call the CUDA extension.\nP y T o r c h \" B C r p i p d g e \" C U D A Environment conda create -n cppcuda python=3.8\nLatest PyTorch: conda install pytorch==1.12.1 cudatoolkit=10.2 -c pytorch\nVersion of the (compiled) PyTorch needs to match the local CUDA version (checked by nvcc -V).\nUpgrade pip for building cpp programs: python -m pip install pip -U\nPybind11 The code: \u0026ldquo;interpolation.cpp\u0026rdquo; acts like the main function that calls the C++ function, and python will call the \u0026ldquo;main\u0026rdquo; function. The \u0026ldquo;main\u0026rdquo; function receives input tensors from PyTorch and return output tensors from CUDA code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Declare PyTorch #include \u0026lt;torch/extension.h\u0026gt; // Define starts with the type of return values torch::Tensor trilinear_interpolate( torch::Tensor features, // 8 corners torch::Tensor point // target point coord. No comma at the end ){ return features; } // API for Python PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ // Function name in python and the cpp function m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); } (2023-10-18) Didn\u0026rsquo;t update the includePath for PyTorch as follows because I didn\u0026rsquo;t find the entry \u0026ldquo;C/C++: Edit Configurations (JSON)\u0026rdquo; after pressing F1. It seems like VSCode finds PyTorch automatically.\n1 2 3 4 5 6 \u0026#34;includePath\u0026#34;: [ \u0026#34;${workspaceFolder}/**\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/include/python3.10\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include\u0026#34;, \u0026#34;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\u0026#34; ], (2023-10-27) However, error intellisense occurs after I installed the \u0026lsquo;C/C++ Extension Pack\u0026rsquo; for VSCode. So setting includePath is necessary.\npybind11 connects Python and C++11 codes.\n1 2 pip install pybind11 pip install ninja Pip compile Build the cpp codes to a python package.\nCreate a \u0026ldquo;setup.py\u0026rdquo; for building settings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from setuptools import setup from torch.utils.cpp_extension import BuildExtension, CppExtension setup( name=\u0026#34;my_cppcuda_pkg\u0026#34;, # python package name version=\u0026#34;0.1\u0026#34;, description=\u0026#34;cppcuda example\u0026#34;, long_description=\u0026#34;cpp-cuda extension\u0026#34;, author=\u0026#34;z\u0026#34;, author_email=\u0026#34;luckily1640@gmail.com\u0026#34;, ext_modules=[ CppExtension( name=\u0026#39;my_cppcuda_pkg\u0026#39;, sources=[\u0026#34;interpolation.cpp\u0026#34;,]) # code files ], cmdclass={ # commands to be executed \u0026#34;build_ext\u0026#34;:BuildExtension } ) Build and install the package:\n1 2 3 4 pip install . # setup.py path is cwd (since pip 21.3) # Or adding an arg to avoid the deprecation warning: pip install . --use-feature=in-tree-build (2024-03-06) Pybind11 module also can be compiled with cmake: 如何在Python中调用C++代码？pybind11极简教程 - HexUp\nPyTorch Call \u0026ldquo;test.py\u0026rdquo; will call the cpp program.\nPackage torch must to be imported before the cuda extensions. 1 2 3 4 5 6 7 8 9 10 import torch import my_cppcuda_pkg features = torch.ones(8,1) point = torch.zeros(1,2) out = my_cppcuda_pkg.trilinear_interpolate(features, point) print(out) title: \u0026ldquo;watch: CppExt - AI葵 02 | Kernel Function\u0026rdquo; date: 2023-10-23T20:20:00\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 2 - English CC -\nDocs: CUDA C++ Programming Guide\nGPU Parallsiam Kernel → Grid → Block → Thread\nC P K n U e e r l d a t a ⋮ G P G U r i B d l T o c h k r 0 e a d B l T o c h k r 1 e a d ⋅ ⋅ ⋅ A thread is the smallest computation unit that executes element arithmatic independently.\nThe number of threads in a block is limited up to 1024. To multiply the amount of threads, many Block are placed together in a Grid. Docs\nThe number of Blocks can be $(2^{31}-1) × 2^{16} × 2^{16}$\nIntroducetion to GPUs - NYU\nTrilinear Interpolate Each corner is sumed up with a weight which is the product of normalized distance from the point to the opposite side.\nAnalogy to Bilinear interpolation:\nf f ₁ ₃ = = = = = u = = 1 = - 1 v = v ‖ 1 - u f 1 f ₂ ₄ $$\\rm f(u,v) = (1-u)(1-v)⋅f₁ + u(1-v)⋅f₂ + (1-u)v⋅f₃ +uv⋅f₄$$\nFor Trilinear interpolation, each weight is the product of 3 normalized distances to the opposite plane.\n$$ \\begin{aligned} \\rm f(u,v,w) =\u0026amp; (1-u)(1-v)(1-w)f₁ + u(1-v)(1-w)f₂ + (1-u)v(1-w)f₃ + uv(1-w)f₄ \\\\ \u0026amp;+ (1-u)(1-v)w f₅ + u(1-v)w f₆ + (1-u)vw f₇ + uvwf₈ \\\\ \u0026amp; \\\\ =\u0026amp;\\rm (1-u) [ (1-v)(1-w)f₁ + v(1-w)f₃ +(1-v)wf₅ +vw f₇ ] \\\\ \u0026amp;\\rm + u [ (1-v)(1-w)f₂ + v(1-w)f₄ + (1-v)w f₆ + vwf₈] \\end{aligned} $$\nf f ₁ ₃ f f ₅ ₇ u w V f f ₄ ₂ f f ₆ ₈ Input-Output Input: features (N, 8, F) and points coordinates in each cube (N, 3)\nOutput: features at points (N, F).\nOperations can be performed in parallel\nEach point can be computed individually; Each feature can be computed individually. Code Notes:\nIf input variables of CUDA kernel are torch.Tensor, they must be checked whether they\u0026rsquo;re on cuda and contiguous, because threads needs to read/write data without jumping.\nWhile if input variables are not tensor, the checking is not required.\ncpp Cpp: \u0026ldquo;trilinear_interpolate.cpp\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;torch/extension.h\u0026gt; #include \u0026#34;utils.h\u0026#34; torch::Tensor trilinear_interpolate( const torch::Tensor features, const torch::Tensor points ){ // Check input tensors for building successfully CHECK_INPUT(features); CHECK_INPUT(points); // Call the cuda kernel return trilinear_fw_cu(features, points); } PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); } header Header: \u0026ldquo;include/utils.h\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;torch/extension.h\u0026gt; // \u0026#34;one-line functions\u0026#34; // Any tensor must reside on cuda device. #define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \u0026#34; must be a CUDA tensor\u0026#34;) // Next element in x corresponds 1 step for R/W head, // thus, a multi-dim tensor is indexed like a flatten tensor. // Workers are contiguous, so tensor must be as well. #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \u0026#34; must be contiguous\u0026#34;) // Combine two conditions: #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x) // Declear the cuda kernel torch::Tensor trilinear_fw_cu( const torch::Tensor feats, const torch::Tensor points ); cu CUDA kernel: \u0026ldquo;interpolation_kernel.cu\u0026rdquo;\nSource video: part-3 Source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 #include \u0026lt;torch/extension.h\u0026gt; // kernel function template \u0026lt;typename scalar_t\u0026gt; // for type of scalar_t __global__ void trilinear_fw_kernel( // no return value // input variables are packed_accessor const torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; feats, const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; points, torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; feat_interp ){ // index thread along x for samples: const int n = blockIdx.x * blockDim.x + threadIdx.x; // index thread along y for features: const int f = blockIdx.y * blockDim.y + threadIdx.y; // Terminate exceeded threads without input data if (n \u0026gt;= feats.size(0) || f \u0026gt;= feats.size(2)) return; // Put results into output variable // normalized coordinates in each cell, [-1,1] -\u0026gt; [0,1] const scalar_t u = (points[n][0]+1)/2; const scalar_t v = (points[n][1]+1)/2; const scalar_t w = (points[n][2]+1)/2; // factors const scalar_t a = (1-v)*(1-w); const scalar_t b = v*(1-w); const scalar_t c = (1-v)*w; const scalar_t d = v*w; // Each thread will perform: feat_interp[n][f] = (1-u) * (a*feats[n][0][f] + b*feats[n][1][f] + c*feats[n][2][f] + d*feats[n][3][f]) + u * (a*feats[n][4][f] + b*feats[n][5][f] + c*feats[n][6][f] + d*feats[n][7][f]); } // foward pass torch::Tensor trilinear_fw_cu( torch::Tensor feats, // (N=20, 8, F=10) torch::Tensor points // (N=20, 3) ){ const int N = points.size(0); const int F = feats.size(2); // Initialize the output data residing on the same devices // as the input data torch::Tensor feat_interp=torch::empty({N,F}, feats.options()); // Allocate threads and blocks // #Threads per block: 256 (Rule of thumb). // Threads can be 3-D (cube) at most, where each dim can be set as proportional as the data\u0026#39;s shape. // Two dimensions will run in parallel: N (20) and F (10) const dim3 threads(16, 16, 1); // total 256. // #Blocks is determined by repeating `threads` to sufficiently cover the output data. const dim3 blocks( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ); // Launch threads to compute for each \u0026#34;voxel\u0026#34; in the \u0026#34;cube\u0026#34; of block AT_DISPATCH_FLOATING_TYPES(feats.type(), \u0026#34;trilinear_fw_cu\u0026#34;, ([\u0026amp;] { // call kernel function with passing input and output trilinear_fw_kernel\u0026lt;scalar_t\u0026gt;\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;( feats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;(), points.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), feat_interp.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;() ); } ) ); return feat_interp; } Since 2 dimensions of the output tensor both require parallism, 256 threads in a block are resized to a square of (16, 16).\nTo ensure each element of the output tensor assigned with a thread, 2 by 1 (2,1) blocks are required.\nSuch that each element will be computed by a thread (\u0026ldquo;box\u0026rdquo;) individually.\n2 - 0 ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' = = □ □ □ □ □ = □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ = 1 = □ □ □ □ □ = □ □ □ □ □ = 0 = □ □ □ □ □ = □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ = 1 = □ □ □ □ □ = □ □ □ □ □ = 6 = □ □ □ □ □ = □ □ □ □ □ = □ □ □ = = □ □ □ □ □ = □ □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ □ = = □ □ □ □ □ = □ □ □ □ □ □ = . ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' 1 6 I ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' s = o = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = l = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = a T = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = t h = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = e r = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = e = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = 1 U a = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = 6 n d = □ □ □ □ □ = □ □ ⊠ ⊠ ⊠ = u s ⊠ ⊠ ⊠ = s = ⊠ ⊠ ⊠ ⊠ ⊠ = ⊠ ⊠ ⊠ ⊠ ⊠ ⊠ = e = ⊠ ⊠ ⊠ ⊠ ⊠ = ⊠ ⊠ ⊠ ⊠ ⊠ ⊠ = d = ⊠ ⊠ ⊠ ⊠ ⊠ = ⊠ ⊠ ⊠ ⊠ ⊠ ⊠ = . ‖ ‖ ‖ ‖ ‖ ⋅ ‖ ‖ ‖ ‖ ‖ ‖ ' 1 6 Notes:\nthreads and blocks are not assigned with tuples:\n1 2 const dim3 threads = (16, 16, 1); // total 256. const dim3 blocks = ( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ) They\u0026rsquo;re object instantiated from classes:\n1 2 const dim3 threads(16, 16, 1); // total 256. const dim3 blocks( (N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y ) If multiple tensors need return, the return type of the func should be std::vector\u0026lt;torch::Tensor\u0026gt;. And the end syntax: return {feat_interp, points};\npytorch.org/cppdocs/ Docs: CUSTOM C++ AND CUDA EXTENSIONS Kernel func Source video: P4\nAT_DISPATCH_FLOATING_TYPES got passed data type and a name for error prompt.\nscalar_t is used to allow various float types of input data to kernel function trilinear_fw_kernel, as AT_DISPATCH_FLOATING_TYPES can recieve float16, float32, float64.\nSpecify sepcific dtype rather than scalar_t and size_t:\n1 2 3 4 5 6 trilinear_fw_kernel\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;( feats.packed_accessor\u0026lt;float, 3, torch::RestrictPtrTraits\u0026gt;(), points.packed_accessor\u0026lt;float, 2, torch::RestrictPtrTraits\u0026gt;(), feat_interp.packed_accessor\u0026lt;float, 2, torch::RestrictPtrTraits\u0026gt;(), var_not_tensor // packed_accessor is only for tensor ) packed_accesor indicates how to index elements by stating \u0026ldquo;datatype\u0026rdquo; (scalar_t) and \u0026ldquo;number of dimensions\u0026rdquo; (3) for each input. And size_t means shape of an index aligned with scalar_t.\ntorch::RestrictPtrTraits: Memory is independent to any other variables.\nKernel trilinear_fw_kernel doesn\u0026rsquo;t return any value (void), with directly changing the memory of output data. Thus, output must be passed.\n__global__ means kernel function is called on cpu and excecuted on cuda devices.\n__host__ for functions called on cpu and run on cpu. __device for functions called and run both on cuda device. Indexing samples by n and indexing features by f.\nIf threads accessed empty area, program returns.\nsetup.py Building CudaExtension: \u0026ldquo;setup.py\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from setuptools import setup from torch.utils.cpp_extension import CUDAExtension, BuildExtension from pathlib import Path ROOT_DIR = Path.cwd() exts = [\u0026#34;.cpp\u0026#34;, \u0026#34;.cu\u0026#34;] sources = [str(p) for p in ROOT_DIR.rglob(\u0026#39;*\u0026#39;) if p.suffix in exts] include_dirs = [ROOT_DIR / \u0026#34;include\u0026#34;] setup( name=\u0026#34;my_cppcuda_pkg\u0026#34;, version=\u0026#34;0.1\u0026#34;, description=\u0026#34;cppcuda example\u0026#34;, long_description=\u0026#34;cpp-cuda extension\u0026#34;, author=\u0026#34;z\u0026#34;, author_email=\u0026#34;luckily1640@gmail.com\u0026#34;, ext_modules=[ CUDAExtension( name=\u0026#39;my_cppcuda_pkg\u0026#39;, sources=sources, # code files include_dirs=include_dirs, extra_compile_args={\u0026#39;cxx\u0026#39;: [\u0026#39;-O2\u0026#39;], \u0026#39;nvcc\u0026#39;: [\u0026#39;-O2\u0026#39;]} ) ], cmdclass={ # commands to be executed \u0026#34;build_ext\u0026#34;:BuildExtension } ) Build and install: pip install . Delete failed building history manually: \u0026ldquo;/home/yi/anaconda3/envs/AIkui/lib/python3.10/site-packages/my_cppcuda_pkg-0.1.dist-info\u0026rdquo; test.py Python function: \u0026ldquo;test.py\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 import torch from my_cppcuda_pkg import trilinear_interpolate N = 65536; F = 256 feats = torch.rand(N,3, F, device=\u0026#39;cuda\u0026#39;) points = torch.rand(N,3, device=\u0026#39;cuda\u0026#39;)*2-1 # [0,1] -\u0026gt; [-1,1] out = trilinear_interpolate(feats, points) print(out.shape) title: \u0026ldquo;watch: CppExt - AI葵 05 | Validate\u0026rdquo; date: 2023-10-28T12:05:00\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 5 - English CC - Source code To validate if cuda kernel yields correct results, impelement a PyTorch version.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch import my_cppcuda_pkg import time def trilinear_interpolate_py(feats, points): r\u0026#34;\u0026#34;\u0026#34; feats: (N, 8, F), features on 8 vertices points: (N, 3) , coordinates [-1,1] \u0026#34;\u0026#34;\u0026#34; u,v,w = (points[:,0:1]+1)/2, (points[:,1:2]+1)/2, (points[:,2:3]+1)/2 a,b,c,d = (1-v)*(1-w), v*(1-w), (1-v)*w, v*w feat_interp = (1-u) * (a*feats[:,0] + b*feats[:,1] + c*feats[:,2] + d*feats[:,3]) \\ + u*(a*feats[:,4] + b*feats[:,5] + c*feats[:,6] + d*feats[:,7]) return feat_interp # (N,F) if __name__ == \u0026#34;__main__\u0026#34;: N=65536; F=256 feats = torch.rand(N,8,F, device=\u0026#34;cuda\u0026#34;).requires_grad_(True) points = torch.rand(N,F, device=\u0026#34;cuda\u0026#34;)*2-1 t = time.time() out_cuda = my_cppcuda_pkg.trilinear_interpolate(feats, points) torch.cuda.synchronize() print(f\u0026#39;CUDA time: {time.time()-t} s\u0026#39;) t = time.time() out_py = trilinear_interpolate_py(feats, points) torch.cuda.synchronize() print(f\u0026#39;PyTorch time: {time.time()-t} s\u0026#39;) print(f\u0026#34;fw all close? {torch.allclose(out_cuda, out_py)}\u0026#34;) print(f\u0026#34;Cuda has grad? {out_cuda.requires_grad}\u0026#34;) title: \u0026ldquo;watch: CppExt - AI葵 06 | Backward\u0026rdquo; date: 2023-10-28T16:40:00\nSource video: Pytorch+cpp/cuda extension 教學 tutorial 6 反向傳播 - English CC -\nSource code Compute Partial Derivatives When loss L comes, the partial derivatives of L w.r.t. every trainable input variable of the function are required.\nTrilinear interpolation:\n$$ \\begin{aligned} f(u,v,w) = (1-u) * [ \u0026amp; (1-v)(1-w)f₁ + v(1-w)f₃ + (1-v)wf₅ + vw f₇ ] \\\\ + u * [ \u0026amp; (1-v)(1-w)f₂ + v(1-w)f₄ + (1-v)w f₆ + vwf₈ ] \\end{aligned} $$\nu,v,w are coordinates, which are constant (requires_grad is False). So only vertices features f₁, f₃, f₅, f₇, f₂, f₄, f₆, f₈ need optimizing.\nGiven interpolated result f, their gradients for this operation are:\n$$ \\begin{aligned} \u0026amp;\\frac{∂f}{∂f₁} = (1-u)(1-v)(1-w); \u0026amp;\\frac{∂f}{∂f₂} \u0026amp;= u(1-v)(1-w); \\\\ \u0026amp;\\frac{∂f}{∂f₃} = (1-u)v(1-w); \u0026amp;\\frac{∂f}{∂f₄} \u0026amp;= uv(1-w); \\\\ \u0026amp;\\frac{∂f}{∂f₅} = (1-u)(1-v)w; \u0026amp;\\frac{∂f}{∂f₆} \u0026amp;= u(1-v)w; \\\\ \u0026amp;\\frac{∂f}{∂f₇} = (1-u)vw \u0026amp;\\frac{∂f}{∂f₈} \u0026amp;= uvw \\end{aligned} $$\nThe derivatives of L w.r.t. features f₁, f₂, f₃, f₄, f₅, f₆, f₇, f₈ are:\n$$ \\frac{∂L}{∂f} \\frac{∂f}{∂f₁}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₂}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₃}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₄}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₅}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₆}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₇}; \\quad \\frac{∂L}{∂f} \\frac{∂f}{∂f₈} $$\nBw Kernel Write host function trilinear_bw_cu based on trilinear_fw_cu in \u0026ldquo;interpolation_kernel.cu\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 torch::Tensor trilinear_bw_cu( const torch::Tensor dL_dfeat_interp, // Inputs const torch::Tensor feats, const torch::Tensor points ){ const int N = points.size(0); const int F = feats.size(2); torch::Tensor dL_dfeats=torch::empty({N,8,F}, feats.options()); // output data const dim3 threads(16,16); const dim3 blocks((N+threads.x-1)/threads.x, (F+threads.y-1)/threads.y); // Launch kernel function AT_DISPATCH_FLOATING_TYPES(feats.type(), \u0026#34;trilinear_bw_cu\u0026#34;, ([\u0026amp;] { trilinear_bw_kernel\u0026lt;scalar_t\u0026gt;\u0026lt;\u0026lt;\u0026lt;blocks, threads\u0026gt;\u0026gt;\u0026gt;( dL_dfeat_interp.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), feats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;(), points.packed_accessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt;(), dL_dfeats.packed_accessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt;() ); } ) ); return dL_dfeats; } Write kernel function trilinear_bw_kernel based on trilinear_fw_kernel in \u0026ldquo;interpolation_kernel.cu\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 template \u0026lt;typename scalar_t\u0026gt; __global__ void trilinear_bw_kernel( const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; dL_dfeat_interp, const torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; feats, const torch::PackedTensorAccessor\u0026lt;scalar_t, 2, torch::RestrictPtrTraits, size_t\u0026gt; points, torch::PackedTensorAccessor\u0026lt;scalar_t, 3, torch::RestrictPtrTraits, size_t\u0026gt; dL_dfeats ){ const int n = blockIdx.x * blockDim.x + threadIdx.x; const int f = blockIdx.y * blockDim.y + threadIdx.y; if (n \u0026gt;= points.size(0) || f\u0026gt;= feats.size(2)) return; // Define helper variables const scalar_t u = (points[n][0]+1)/2; const scalar_t v = (points[n][1]+1)/2; const scalar_t w = (points[n][2]+1)/2; const scalar_t a = (1-v)*(1-w); const scalar_t b = v*(1-w); const scalar_t c = (1-v)*w; const scalar_t d = v*w; // Compute derivatives dL_dfeats[n][0][f] = dL_dfeat_interp[n][f]*(1-u)*a; dL_dfeats[n][1][f] = dL_dfeat_interp[n][f]*(1-u)*b; dL_dfeats[n][2][f] = dL_dfeat_interp[n][f]*(1-u)*c; dL_dfeats[n][3][f] = dL_dfeat_interp[n][f]*(1-u)*d; dL_dfeats[n][4][f] = dL_dfeat_interp[n][f]*u*a; dL_dfeats[n][5][f] = dL_dfeat_interp[n][f]*u*b; dL_dfeats[n][6][f] = dL_dfeat_interp[n][f]*u*c; dL_dfeats[n][7][f] = dL_dfeat_interp[n][f]*u*d; } Add the function signature into header file \u0026ldquo;include/utils.h\u0026rdquo;\n1 2 3 4 5 torch::Tensor trilinear_bw_cu( const torch::Tensor dL_dfeat_interp, const torch::Tensor feats, const torch::Tensor points ); Add a cpp function to call the backward method trilinear_bw_cu in \u0026ldquo;interpolation.cpp\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 torch::Tensor trilinear_interpolate_bw( const torch::Tensor dL_dfeat_interp, const torch::Tensor feats, const torch::Tensor points ){ CHECK_INPUT(dL_dfeat_interp); CHECK_INPUT(feats); CHECK_INPUT(points); return trilinear_bw_cu(dL_dfeat_interp, feats, points); } Give the function trilinear_interpolate_bw a name in PYBIND as a method of the package:\n1 2 3 4 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){ m.def(\u0026#34;trilinear_interpolate\u0026#34;, \u0026amp;trilinear_interpolate); m.def(\u0026#34;trilinear_interpolate_bw\u0026#34;, \u0026amp;trilinear_interpolate_bw); } Encapsulate Wrap forward and backward by a subclass inherited from torch.autograd.Function in \u0026ldquo;test.py\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class trilinear_interpolate_cuda(torch.autograd.Function): @staticmethod def forward(ctx, feats, points): feat_interp = my_cppcuda_pkg.trilinear_interpolate(feats, points) ctx.save_for_backward(feats, points) return feat_interp @staticmethod def backward(ctx, dL_dfeat_interp): # The number of input vars corresponds to return values of forward pass. # i.e., inputs are gradients of Loss w.r.t the forward\u0026#39;s outcomes. feats, points = ctx.saved_tensors dL_dfeats = my_cppcuda_pkg.trilinear_interpolate_bw( dL_dfeat_interp.contiguous(), feats, points) return dL_dfeats, None # return gradients of Loss w.r.t each input data forward Notes:\nThe nubmer of return values needs to match the input to forward pass. If some input doesn\u0026rsquo;t require grad, return a None.\nctx is mandatory for storing intermeidate data.\nVerify Graident Test the gradient of backward:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # test.py import torch import my_cppcuda_pkg import time def trilinear_interpolate_py(feats, points): r\u0026#34;\u0026#34;\u0026#34; feats: (N, 8, F), features on 8 vertices points: (N, 3) , coordinates [-1,1] \u0026#34;\u0026#34;\u0026#34; u,v,w = (points[:,0:1]+1)/2, (points[:,1:2]+1)/2, (points[:,2:3]+1)/2 a,b,c,d = (1-v)*(1-w), v*(1-w), (1-v)*w, v*w feat_interp = (1-u) * (a*feats[:,0] + b*feats[:,1] + c*feats[:,2] + d*feats[:,3]) \\ + u*(a*feats[:,4] + b*feats[:,5] + c*feats[:,6] + d*feats[:,7]) return feat_interp # (N,F) if __name__==\u0026#34;__main__\u0026#34;: N = 1024; F=256 feats = torch.rand(N,8,F, device=\u0026#34;cuda\u0026#34;) feats_py = feats.clone().requires_grad_() feats_cu = feats.clone().requires_grad_() points = torch.rand(N,3, device=\u0026#34;cuda\u0026#34;)*2-1 t = time.time() out_py = trilinear_interpolate_py(feats_py, points) torch.cuda.synchronize() print(f\u0026#34;py: {time.time() - t}\u0026#34;) t = time.time() out_cuda = trilinear_interpolate_cuda.apply(feats_cu, points) torch.cuda.synchronize() print(f\u0026#34;cu: {time.time() - t}\u0026#34;) loss_py = out_py.sum() loss_cuda = out_cuda.sum() loss_py.backward() loss_cuda.backward() print(f\u0026#34;Grad all close? {torch.allclose(feats_py.grad, feats_cu.grad)}\u0026#34;) ","date":"2023-10-11T16:23:00Z","image":"https://img.youtube.com/vi/l_Rpk6CRJYI/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/lang/cuda/tut_ai%E8%91%B5/","title":"watch: CppExt - AI葵 | CUDA Extension for PyTorch"},{"content":"Yannic Source video: Retentive Network: A Successor to Transformer for Large Language Models (Paper Explained)\nRemove softmax outside the attention scores, then no all the results have to be hold and wait for softmax.\nT p r a a r n a i l n l L T f g i i r o s n a r m e n m a s e r - r T R r I L e a n o t n f w N s e e f r C t o e o r n s m c t e e r R N e e c t p u w S e r o t r r r r f e k o o n n r t g m a n c e RetNet is a kind of linear transforemr, like RWKV.\nRecurrent network each time train only 1 token because once the next work has been predicted, the backpropagation has to be done to optimize previous hidden states.\nw h s o i t r d a d d t s e e : n s b p a r c o k p Recurrent network cannot be trained parallelly because the non-linearity activation function\nG(c( G(b( G(ax+γ)+γ )+γ) +γ) )\nHidden state is a shared buffer. The hidden state contains all the previous information, so the memory cost is consistent during training.\n$$ \\begin{aligned} ax+\\gamma = \\gamma \\\\ by + \\gamma = \\gamma \\\\ cz + \\gamma = \\gamma \\\\ \\end{aligned} $$\nTransformer can\u0026rsquo;t be recurrent because the existence of softmax, which requires all the attention scores (\u0026ldquo;hidden states\u0026rdquo;) not to be abandoned.\nRetNet achieved training parallism through matrix multiplication, like a Linear layer.\nTime-scaling mask replaces causal mask (blocking the subsequent words when doing attention in parallel)\nRetNet by chunks is a trade-off between recurrent and parallel.\n秋刀鱼 Source video: 【论文速览】 RetNet: A Successor to Transformer for Large Language Models2307.08621\nEquentions explaination and code walkthrough.\nA global state is maintained like recurrent network. With that, expand the equation of attention: Q K V Apply singular decomposition \u0026hellip;. ","date":"2023-10-10T23:41:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/d-vid-retnet/","title":"watch: RetNet"},{"content":" Source video: 第一次在美國直播（講解Gaussian Splatting的cuda code）- AI葵\nkwea123/gaussian_splatting_notes\nSelected comments:\n\u0026ldquo;3D Gaussian Mixture Model.\u0026rdquo; GMM?!\n\u0026ldquo;EWA Splatting paper (2001) contains all the necessary derivations and math.\u0026rdquo; - Matias Turkulainen (gsplat contributor).\nforward.cu Trade-off:\nThe 2D-projections of 3D ellipsoids are circles rather than ellipses to reduce shading tiles.\nDetermine visibility of tiles instead of pixels (shaded by circle shadows) for fast rasterization.\nMain steps:\nDetermine the radii of circles shadows (preprocessCUDA)\nProject a 3D ellipsoid will yield a 2x2 covariance matrix\nSolve the 2 Eigenvalues for the covariance matrix, and the bigger one is the length of the major axis.\nUse the major axis as the radius of the circle.\nDetermine pixels covered by the projected circles.\nIf the distance betwen a pixel to the circle center is smaller than the circle radius, the pixel is visible to the circle (a disc corresponding to a 3D Gaussian in the ray space).\nGetRect Sort 3D Gaussians (discs) by depths\nEach tile is shaded by multiple ellipsoids, i.e., visible to multiple Gaussians.\nPair each pixel with each contributing ellipsoid, and form a 64-bit identifier for each pair.\ni.e., stitching the index (32-bit) of a pixel and the depth (32-bit) of a ellipsoid.\nSort all the identifier, and obtain a sequence\nFor example:\n0 2 c a b 1 3 tile-0 has 2 pairs: 0-a, 0-c tile-1 has 3 pairs: 1-a, 1-b, 1-c Suppose the depths of 3 Gaussians are b \u0026gt; a \u0026gt; c, the sequence of pairs is as follows:\n1 2 3 4 5 0-c 0-a 1-c 1-a 1-b Alpha compositing for each pixel\u0026rsquo;s color (renderCUDA)\nImplementation tricks:\nA tile is a Block, in which each pixel is a worker. Every pixel in a tile uses the same Gaussian distributions, so those data are stored in __share__ memory. Docs Calculate alpha, which is proportional to the probability in a Gaussian distribution.\nThe probability is calculated according to the expression of 2D Gaussian.\nBlend alpha * color (SH) of each ellipsoid front-to-back.\nbackward.cu output input pixels\u0026rsquo; color (p,3) Gaussians\u0026rsquo; color (g,3) Gaussians\u0026rsquo; alpha Gaussians\u0026rsquo; position (g,3) Gaussians\u0026rsquo; rotation (g,4) Gaussians\u0026rsquo; length of axis (g,3) p is number of pixels; g is number of 3D Gaussians Loss = color + SSIM\nA full image is produced at once, so image metrics, like SSIM, can be added. Write parital derivative for each input tensor.\n","date":"2023-10-10T16:43:00Z","image":"https://img.youtube.com/vi/1buFrKUaqwM/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/model/splat/d-vid-3dgs-explain-%E8%91%B5/","title":"watch: 3DGS | AI葵 Cuda Code Walkthrough"},{"content":"Metapost ChatGPT - code interpreter Data visualization\nProcesson Networks Architecture\n你的科研能力从什么时候开始突飞猛进的？ - 平凡的回答 - 知乎\n","date":"2023-10-07T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/latex_draw/","title":"memo: Plotting for Academics"},{"content":"Nvidia apex An example project using it is AIM.\ntorch amp An example: Automatic Mixed Precision recipe\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 use_amp = True net = make_model(in_size, out_size, num_layers) opt = torch.optim.SGD(net.parameters(), lr=0.001) scaler = torch.cuda.amp.GradScaler(enabled=use_amp) for epoch in range(epochs): for input, target in zip(data, targets): with torch.autocast(device_type=\u0026#39;cuda\u0026#39;, dtype=torch.float16, enabled=use_amp): output = net(input) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(opt) scaler.update() opt.zero_grad() # set_to_none=True here can modestly improve performance 【pytorch distributed】amp 原理，automatic mixed precision 自动混合精度\n","date":"2023-09-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_amp/","title":"memo: PyTorch | Automatic Mixed Precision"},{"content":"Source video: CLIP 论文逐段精读【论文精读】- 跟李沐学AI ~ Bilibili 2022-02-10\nCLIP (Contrastive Language-Image Pre-Training) Code\nFeatures Large-scale dataset: 4e8 pairs of image and caption.\nSelf-supervised learning strategy (pretext task): Given an image, find the matched text vector from candidates\nContrastive learning needs positive and negative samples.\nThere is only one correct text vector for an image, while the remaining text vectors are served as negative samples.\nLoosen the target: pairing rather than predicting next word\nGood transferability: Able to generalize to unseen classes based on the text prompts.\nLeverage text to enhance image features with semantic understanding \\begin{algorithm} \\caption{CLIP} \\begin{algorithmic} \\STATE If = ImageEncoder(I) $\\quad$ \\COMMENT{(n,h,w,c)→(n, di)} \\STATE Tf = TextEncoder(T) $\\quad$ \\COMMENT{(n,l)→(n,dt)} \\STATE Ie = Linear projection (If) $\\quad$ \\COMMENT{(n,de)} \\STATE Te = Linear projection (Tf) $\\quad$ \\COMMENT{(n,de)} \\STATE logits = Inner Product (Ie, Te.T) \\STATE labels = np.arange(n) \\STATE lossᵢ = CrossEntropy(logits, labels, axis=0) \\STATE lossₜ = CrossEntropy(logits, labels, axis=1) \\STATE loss = (lossᵢ + lossₜ)/2 \\end{algorithmic} \\end{algorithm} Experiments Backbone model: The image encoder can be ResNet or ViT, text encoder is a transformer\nZero-shot transfer: No downstream task adaptation, apply the pre-trained model directly onto the unseen data.\nFew-shot transfer: Given a few images, fine-tune or linearly probe the pre-trained model. CLIP outperforms all the previous pre-trained models supervised by labels.\nFull-data transfer: Better than other zero-shot model.\nThe features extracted by previous pre-trained models only have the image modality, while the image features of CLIP are learned under the instructions of text description, so the image features have fused with text modality and guided to semantic understanding.\nMix precision training can save half of memory without losing performance.\nPrompt engineering: Fit the label into a sentence by putting it into prompt templates to close gap with the training set, i.e., image-caption pairs.\nThey made 80 templates for describing different situations in images, such that more specific context is confined to help find the solution in a small possible range.\nUnrealistic and abstract datasets, like MNIST, counting number of objects, are difficult for CLIP because they are hard to describe with language. Otherwise, as long as the describable object exists in the image, CLIP can recognize it.\nLimitations CLIP is not the SOTA on ImageNet, but only in the zero-shot task.\nCannot understanding abstract concepts: \u0026ldquo;abnormal\u0026rdquo;, \u0026ldquo;safe\u0026rdquo;\nOut-of-distribution when performing zero-shot inference will ruin the generaliability of CLIP: MNIST (different from natural images) isn\u0026rsquo;t included in the training set.\nZero-shot inference of CLIP requires the \u0026ldquo;new label\u0026rdquo; is provided in the candidates to do a multiple choice question.\nBy contrast, let model generate caption from image will make the data loop. But that is infesible because massive computation with low-efficient training techinics.\nData utilization is inefficient with too many training images. Dataloader spitting image one-by-one needs long time.\nDatasets bias: Hyperparameter tunning is based on ImageNet; The testing performance is based on chosen 27 datasets.\nTraining set is from internet without filtering, so the model may learned malicious information.\nPerformance of few-shot learning sometimes is inferior to zero-shot scenario weirdly.\nFooter:\nThe pre-trained method isn\u0026rsquo;t open-source. But the model is open source. Code Repo\nInstall CLIP:\n1 2 3 conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0 pip install ftfy regex tqdm pip install git+https://github.com/openai/CLIP.git Zero-shot classification:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch import clip from PIL import Image device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) image = preprocess(Image.open(\u0026#34;CLIP.png\u0026#34;)).unsqueeze(0).to(device) text = clip.tokenize([\u0026#34;a diagram\u0026#34;, \u0026#34;a dog\u0026#34;, \u0026#34;a cat\u0026#34;]).to(device) with torch.no_grad(): image_features = model.encode_image(image) text_features = model.encode_text(text) logits_per_image, logits_per_text = model(image, text) probs = logits_per_image.softmax(dim=-1).cpu().numpy() print(\u0026#34;Label probs:\u0026#34;, probs) # prints: [[0.9927937 0.00421068 0.00299572]] ","date":"2023-08-29T12:12:00Z","permalink":"https://zichen34.github.io/writenotes/model/transfer/d-vid-clip_paper/","title":"watch: CLIP Paper Walkthrough"},{"content":"Code | Arxiv | ProjPage | OpenReview | Ytb\nNote Abs Transfer between different modalities: image classification and video understanding\nAdd 3 new layers inside each transformer block, not at the very end of the model.\n3 new layers:\nSpatial adapter is after self-attention; Temporal adapter is after two self-attention, Joint adapter is a bypass branch of the MLP layers. Frozen pre-trained parameters and optimize only the new layers to transfer the pre-trained model onto another task.\nIntro Two directions: adding temporal module onto or inflating an image transformer model both have drawbacks: heavy-computation full fine-tunning is required. Related work Pre-trained image models have good transferability. Fully fine-tuning a transformer-based image model is uneconomical. Parameter-efficient finetuning was applied on LLM for downstream tasks. Method ViT consists of 12 encoder blocks (MSA and MLP).\nAn image is split into N pathes, which will be projected to D channels;\nThe input to MSA is each patch attached class token channel and added positional encoding.\nSpace-only model (baseline, no temporal modelong): Apply pre-trained frozen ViT onto video by processing each frame independently.\nEach frame will be represented by the final class token.\nThe token of each frames are averaged to form a vector for predicting\nSpatial adaptation is adding an adapter after the self-attention (pre-trained MSA) fuses N+1 patches.\nAn adapter is a bottleneck, i.e, Reduce-Act-Expand with skip connection.\nThis can achieve comparable performance compared with space-only baseline, because image model learns spatial feature well.\nTemporal modeling reused the self-attention parameters again, whereas the T frames got fused by reshapeing the tensor.\nAnother adapter is appended for adapting the generated temporal features.\nTemporal modeling is performed ahead of spatial modeling, so the adapter is removed skip connection and initialized as zero to avoid disrupting the perfomance of the original model.\nBy reusing the MSA, the number of parameters is maintained.\nJoint adapation jointly fits the temporal features and spatial features.\nThis adapter also doesn\u0026rsquo;t has skip connection.\nAverage the final class token of each frame and pass it to classification head.\nExperiments Task: classification video?\n8 frames Memory: AIM based on Swin-B pre-trained with IN-21K occupies 9GB. Underperform on temporal-heavy video because the temporal modeling is simply reusing the spatial modeling parameters. Discussion Deeper layer needs adaptation for task-specific features, while shallow layer may not. Conclusion Transfer models trained with other sequence data, like text and audio for video action recognition. flowchart TD input(\"Image (224,224,3)\") --\u003e cls(\"Class token (1,768)\") \u0026 pe(\"Position Embedding (197,768)\") input --\u003e feat(\"Conv2d (16x16,s16) (14,14)\") cls \u0026 feat --\u003e Cat pe \u0026 Cat --\u003e add1(\"Add\") add1 --\u003e msa1(\"MSA\") --\u003e Tadap --\u003e msa2(\"MSA\") --\u003e Sadap Sadap --\u003e ineck(\"Inverse bootleneck\") Sadap --\u003e Jadap add1 \u0026 ineck \u0026 Jadap --\u003e add2(\"Add\") --\u003e x Play Debug code with experiment settings in \u0026ldquo;run_exp.sh\u0026rdquo;\nEnvironment 1 2 3 4 5 6 7 8 9 10 11 12 13 conda env create -f ./environment.yml conda activate AIM # install CLIP pip install git+https://github.com/openai/CLIP.git # install mmaction2 python setup.py develop # install apex git clone https://github.com/NVIDIA/apex cd apex pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\u0026#34;--cpp_ext\u0026#34; --global-option=\u0026#34;--cuda_ext\u0026#34; ./ Dataset diving48 To prepare the dataset diving48 , I downloaded the repo MMAction2 Documentaions\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 conda create --name openmmlab python=3.8 -y conda activate openmmlab conda install pytorch torchvision -c pytorch # Step 1: pip install -U openmim mim install mmengine mim install mmcv mim install mmdet mim install mmpose # Step 2: git clone https://github.com/open-mmlab/mmaction2.git cd mmaction2 pip install -v -e . Following \u0026ldquo;Download from Official Source\u0026rdquo; section.\nDownload annotations using their shell script. bash download_annotations.sh Download videos \u0026ldquo;Diving48_rgb.tar.gz\u0026rdquo; (9.6G) Only extract the rgb frames: bash extract_rgb_frames_opencv.sh Generate file list using program: bash generate_videos_filelist.sh Make a symbolic link to \u0026ldquo;mmaction2/data\u0026rdquo; in \u0026ldquo;adapt-image-models\u0026rdquo;: ln -s /home/zichen/Downloads/mmaction2/data/ ./\nFormat\nannotation file \u0026ldquo;data/diving48/diving48_train_list_videos.txt\u0026rdquo; includes: filename and class label of each video Config for 1080Ti Train with 1 video cannot make the acc increase\nDefault configs (8 videos, 32 frames) will cause 1 1080Ti OOM. (\u0026ldquo;configs/recognition/vit/vitclip_large_diving48.py\u0026rdquo;)\nOverride the number of videos in config file with args:\n1 2 3 \u0026#34;args\u0026#34;:[ \u0026#34;--cfg-options\u0026#34;, \u0026#34;data.videos_per_gpu=1\u0026#34; ] But the top1_acc didn\u0026rsquo;t grow:\n1 2 3 4 5 2023-08-31 12:22:11,768 - mmaction - INFO - Epoch [1][4180/15027] lr: 6.003e-05, eta: 6 days, 8:30:03, time: 0.709, data_time: 0.001, memory: 5659, top1_acc: 0.0500, top5_acc: 0.3500, loss_cls: 3.4383, loss: 3.4383 \u0026quot;data.videos_per_gpu=2\u0026quot; will OOM.\nReduce num_frames\n.vscode/launch.json is made based on \u0026ldquo;run_exp.sh\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34;, \u0026#34;data.videos_per_gpu=8\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, // The follwings cannot change // \u0026#34;train_pipeline[1].clip_len=3\u0026#34;, // \u0026#34;val_pipeline[1].clip_len=3\u0026#34; \u0026#34;--train_clip_len\u0026#34;, \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; ] (2023-09-06) The cfg.data.train['pipeline']['clip_len'] didn\u0026rsquo;t changed, which still equals 32. Consequently, the images x passed to forward(self, x) of model ViT_CLIP has the shape (256, 197, 768)\nHowever, the instance variable self.num_frames of the backbone model ViT_CLIP was changed to 3.\nThen, the einops.rearrange cannot parse the dimensionality in: x = rearrange(x, '(b t) n d -\u0026gt; (b n) t d', t=self.num_frames)\n1 2 einops.EinopsError: Shape mismatch, can\u0026#39;t divide axis of length 256 in chunks of 3 Dataset is built based the key cfg.data.train, thus, its values are also required to update:\n1 2 3 cfg.merge_from_dict(dict(train_pipeline=args.train_clip_len, val_pipeline=args.train_clip_len)) update_option = {\u0026#39;data\u0026#39;: {\u0026#39;train\u0026#39;: {\u0026#39;pipeline\u0026#39;: args.train_clip_len}, \u0026#39;val\u0026#39;: {\u0026#39;pipeline\u0026#39;: args.train_clip_len}}} cfg.merge_from_dict(update_option) Start training:\n1 2 3 4 5 6 7 8 9 10 11 12 13 export CUDA_VISIBLE_DEVICES=4 python -m torch.distributed.launch \\ --nproc_per_node=1 --master_port=29500 \\ tools/train.py \\ \u0026#34;configs/recognition/vit/vitclip_base_diving48.py\u0026#34; \\ --launcher=\u0026#34;pytorch\u0026#34; \\ --test-last \\ --validate \\ --cfg-options model.backbone.pretrained=\u0026#34;openaiclip\u0026#34; \\ work_dir=\u0026#34;work_dirs_vit/diving48/debug\u0026#34; \\ data.videos_per_gpu=8 \\ model.backbone.num_frames=3 \\ --train_clip_len \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; Optimization Souce code\nAdamW: lr=3e-4, weight_decay=0.05, LR scheduler: CosineAnnealing Pseudocode With backbone: ViT_CLIP\n\\begin{algorithm} \\caption{main()} \\begin{algorithmic} \\PROCEDURE{Config}{cfg, args} \\STATE args = parse\\_args() \\PROCEDURE{Config.fromfile}{args.config} \\STATE model settings \\STATE dataset settings: ann\\_file, train\\_pipeline,... \\STATE optimizer settings \\STATE learning policy \\STATE runtime settings \\ENDPROCEDURE \\ENDPROCEDURE \\STATE $\\newline$ \\PROCEDURE{build-model}{cfg.model} \\COMMENT{Construct ViT with Adapters added} \\PROCEDURE{build-localizer}{cfg} \\PROCEDURE{LOCALIZERS.build}{cfg} \\PROCEDURE{BaseRecognizer}{} \\STATE $\\newline$ \\PROCEDURE {builder.build-backbone}{backbone} \\STATE BACKBONES.build(cfg) \\ENDPROCEDURE \\STATE $\\newline$ \\PROCEDURE {init-weights}{} \\STATE self.backbone.init\\_weights() \\COMMENT{Load pretrained state\\_dict} \\ENDPROCEDURE \\STATE $\\newline$ \\ENDPROCEDURE \\ENDPROCEDURE \\ENDPROCEDURE \\ENDPROCEDURE \\STATE $\\newline$ \\STATE datasets = [build\\_dataset(cfg.data.train)] \\STATE $\\qquad$ build\\_from\\_cfg(cfg, DATASETS) \\STATE $\\qquad$ 11 transforms operations \\STATE Freeze params.requires\\_grad=False \\STATE $\\newline$ \\PROCEDURE{train-model}{model,datasets,cfg,...} \\STATE dataloader\\_settings \\STATE data\\_loaders = build\\_dataloader(dataset, dataloader\\_setting) \\STATE optimizer = build\\_optimizer(model, cfg.optimizer) \\STATE amp settings \\STATE fp16 settings \\STATE register DistOptimizerHook \\STATE build validation dataset and dataloader \\STATE $\\newline$ \\PROCEDURE{runner.run}{data\\_loaders, cfg.workflow, cfg.total\\_epochs,**runner\\_kwargs} \\STATE DistOptimizerHook.before\\_run(self, runner): \\STATE $\\qquad$ runner.optimizer.zero\\_grad() \\STATE BaseRecognizer.train\\_step(self, data\\_batch,) \\STATE losses = self(imgs, label) \\PROCEDURE {Recognizer3D.forward-train}{img, label} \\STATE x = BaseRecognizer.extract\\_feat(imgs) \\STATE $\\qquad$ self.backbone(imgs) \\COMMENT{ViT\\_CLIP.forward()} \\ENDPROCEDURE \\STATE $\\qquad$ self.forward\\_test(img, label) \\ENDPROCEDURE \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} Debug VideoSwin The pretrained weights of ViT_CLIP are obtained from an initialized clip model:\n1 2 3 4 5 clip_model, preprocess = clip.load(\u0026#34;ViT-B/16\u0026#34;, device=\u0026#34;cpu\u0026#34;) pretrain_dict = clip_model.visual.state_dict() # param del clip_model del pretrain_dict[\u0026#39;proj\u0026#39;] msg = self.load_state_dict(pretrain_dict, strict=False) Source code\nHowever, the weights of Swin Transformer needs to be loaded from file. Source code\nReminded by this issue MMCV load pretrained swin transformer\nPretrained Swin Transformer (Swin-B 224x224, \u0026ldquo;swin-base_3rdparty_in21k.pth\u0026rdquo;) of open-mmlab (mmpretrain) doesn\u0026rsquo;t have the key: \u0026lsquo;model\u0026rsquo;, so it mismatches the code.\n1 2 3 def inflate_weights(self, logger): checkpoint = torch.load(self.pretrained, map_location=\u0026#39;cpu\u0026#39;) state_dict = checkpoint[\u0026#39;model\u0026#39;] While the pretrained swin from microsoft can be successfully loaded. The Swin Transformer has not been trained with CLIP, only on ImageNet21K.\nThe author adds adapters to \u0026ldquo;Swin-B_IN-21K\u0026rdquo; SwinTransformer2D (\u0026ldquo;swin2d.py\u0026rdquo;) in \u0026ldquo;mmaction/models/backbones/ swin2d_adapter.py\u0026rdquo; as clarified in issue18.\nThe \u0026ldquo;swin2d_adapter\u0026rdquo; is compared with SwinTransformer3D (VideoSwin, \u0026ldquo;swin_transformer.py\u0026rdquo;) in Table 6. And most of their experiments are based on ViT_CLIP and compared with TimeSformer.\nSwinTransformer2D is adapted by settings: \u0026ldquo;configs/recognition/swin/ swin2d_adapter_patch244_window7_kinetics400_1k.py\u0026rdquo;.\nWhereas, the config file: \u0026ldquo;configs/recognition/swin/ swin_base_patch244_window877_kinetics400_1k.py\u0026rdquo; is for the original VideoSwin SwinTransformer3D.\nArguments pretrained: str and pretrained2d: bool of class SwinTransformer3D originate in VideoSwin, which adapted pretrained 2D swin transfromer to 3D.\nAIM codes are based on VideoSwin.\nFollowing VideoSwin, pretrained is supposed to be a path to the pretrained model, which should be downloaded in advance. An example is KeyError: \u0026lsquo;patch_embed.proj.weight\u0026rsquo; #22\nBased on the above, the args in launch.json should be set as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Swin-B settings \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;1\u0026#34;, // GPUs \u0026#34;--master_port\u0026#34;, \u0026#34;29600\u0026#34;, \u0026#34;tools/train.py\u0026#34;, \u0026#34;configs/recognition/swin/swin2d_adapter_patch244_window7_kinetics400_1k.py\u0026#34;, \u0026#34;--launcher\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;--test-last\u0026#34;, \u0026#34;--validate\u0026#34;, \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=work_dirs_swin/swin_base_patch4_window7_224_22k.pth\u0026#34;, \u0026#34;work_dir=work_dirs_swin/K400/debug\u0026#34;, \u0026#34;data.videos_per_gpu=8\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, \u0026#34;--train_clip_len\u0026#34;, \u0026#34;{\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: 3}}\u0026#34; ] Dataset SSv2 AIM-Swin only has configuration file for K400 and ssv2 datasets. K400 has 240K training videos, which are massive. So I choose the smaller one, SSv2, which has 169K training videos.\nRefer to the guide of SSv2 - mmaction2\nAnnotations: Once signed in your Qualcomm account, download \u0026ldquo;Labels\u0026rdquo; into \u0026ldquo;data/sthv2/annotations/\u0026rdquo; from homepage (Need to acknowledge the agreement before jumping to the download page)\n1 2 3 4 5 6 7 unzip 20bn-something-something-download-package-labels.zip # Rename to match the python code \u0026#34;parse_file_list.py\u0026#34; mv data/sthv2/annotations/labels/train.json data/sthv2/annotations/something-something-v2-train.json mv data/sthv2/annotations/labels/validation.json data/sthv2/annotations/something-something-v2-validation.json mv data/sthv2/annotations/labels/test.json data/sthv2/annotations/something-something-v2-test.json mv data/sthv2/annotations/labels/labels.json data/sthv2/annotations/something-something-v2-labels.json Videos: Download 20 files into \u0026ldquo;mmaction2/data/sthv2/\u0026rdquo;.\nBy executing the following 2 commands, 220847 webm videos (19G) are extracted into the folder: \u0026ldquo;sthv2/20bn-something-something-v2\u0026rdquo;\n1 2 3 4 5 unzip 20bn-something-something-v2-\\??.zip cat 20bn-something-something-v2-?? | tar zx # Rename to match the script below and configs in AIM mv 20bn-something-something-v2/ videos/ Split: Generate list\n1 2 cd mmaction2/tools/data/sthv2/ bash generate_videos_filelist.sh Two .txt files \u0026ldquo;sthv2_train_list_videos.txt\u0026rdquo; and \u0026ldquo;sthv2_val_list_videos.txt\u0026rdquo; are created under \u0026ldquo;data/sthv2/\u0026rdquo;.\nTo debug AIM-swin with SSv2, specify the config file as \u0026ldquo;configs/recognition/swin/swin2d_adapter_patch244_window7_sthv2_1k.py\u0026rdquo; in \u0026ldquo;launch.json\u0026rdquo;.\n2023-09-12 15:41:48,166 - mmaction - INFO - Epoch [1][28160/84457]\tlr: 6.601e-05, eta: 21 days, 10:31:39, time: 0.365, data_time: 0.001, memory: 1420, loss_cls: 4.3897, loss: 4.3897\nForward swin \\begin{algorithm} \\caption{SwinTransformer2d\\_Adapter} \\begin{algorithmic} \\PROCEDURE{forward}{x: (B,T,D,H,W)} \\STATE Conv3d extracts feat maps: (B, C, num\\_Ttokens, H', W') \\STATE $\\newline$ \\PROCEDURE{SwinTransformer2d-Adapter}{B*num\\_Ttokens, H*W, C} \\STATE 2 SwinTransformerBlock \\STATE $\\quad$ rearrange \\STATE $\\quad$ LN1 \\STATE $\\quad$ Temporal MSA mix \"num\\_Ttokens\" of feat maps \\COMMENT{even blks} \\STATE $\\quad$ Temporal Adapter \\STATE $\\quad$ rearrange back \\STATE $\\newline$ \\STATE $\\quad$ LN1 \\STATE $\\quad$ Shift window rows and cols \\STATE $\\quad$ window\\_partition \\COMMENT{reshape} \\STATE $\\quad$ WindowAttention mix \"pixels\" in each window \\STATE $\\quad$ Spatial Adapter \\STATE $\\quad$ window\\_reverse \\STATE $\\quad$ Shift window rows and cols \\STATE $\\newline$ \\STATE $\\quad$ Squash feat maps to 1D \\STATE $\\quad$ Skip connect with the features before S\\_adap \\STATE $\\quad$ LN2 \\STATE $\\quad$ MLP + Joint Adapter \\STATE PatchMerging: (B*num\\_Ttokens, H'/2*W'/2, 2*C) \\STATE $\\newline$ \\STATE 2 SwinTransformerBlock \\STATE PatchMerging: (B*num\\_Ttokens, H'/4*W'/4, 4*C) \\STATE $\\newline$ \\STATE 18 SwinTransformerBlock \\STATE PatchMerging: (B*num\\_Ttokens, H'/8*W'/8, 8*C) \\STATE $\\newline$ \\STATE 2 SwinTransformerBlock \\ENDPROCEDURE \\STATE $\\newline$ \\STATE LN \\STATE rearrange to (B,C,T,H,W) \\STATE cls\\_head, i.e. I3DHead (A linear layer) \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} The reason of setting window_size to 7 may be that the resolution of feature maps is (56,56), which can shrink gradually to (7,7).\nAdapter: Pass the attended features to a bottleenck (2-layer MLP) for adapting them.\nAdapted Swin Differences of the adapted Swin (\u0026ldquo;swin2d_adapter.py\u0026rdquo;) from the baseline model SwinTransformer2D (\u0026ldquo;swin_transformer.py\u0026rdquo;):\n1 2 diff mmaction/models/backbones/swin2d_adapter.py \\ mmaction/models/backbones/swin2d.py swin2d has a temporal adapter more than swin_transformer\nswin2d_adapter has\nNo joint adapter\n","date":"2023-08-23T00:00:00Z","image":"https://adapt-image-models.github.io/method.JPG","permalink":"https://zichen34.github.io/writenotes/model/transfer/b-note-aim-video/","title":"read: AIM adapt image model for video"},{"content":"Code-pytroch | Arxiv | OpenReview\nQ\u0026amp;A Condition image vs target image? Abstract A img2img diffusion model is conditioned with pose and a single source view to generate multiviews.\nStochastic conditioning: Randomly select a view from avaliable views as condition image at each denoising step during sampling?, rather than using only the given view.\nReconstruct a NeRF to measure 3D consistency of multi-views.\nNeRF is not their ultimate objective. Intro Regressive methods for NVS from sparse views based on NeRF are still not generalizable enough or able to produce high-quality completion for the occluded parts.\nRegularized NeRF (RegNeRF) suffer from artifacts when only few views are given because they and didn\u0026rsquo;t apply the features of commen prior of multiple scenes.\nRegressing a NeRF from image feataures (pixel-NeRF) tend to get blurred images.\nGeometry-free methods for NVS obtain colors that aren\u0026rsquo;t directly derived from volume rendering.\nLight field network Scene Representation Transformer EG3D combines StyleGAN and volume rendering 3D diffusion model is a generative and geometry-free method.\nUse pairs of images of the same scene to train a diffusion model. During training, one of them serves as the original, and the other is the condition image. The trained model can produce a multi-view set of a scene given one condition image. Model They consider multiple views from a scene are not independent, but follow the distribution of the training views, to enhance multi-view consistency.\nThe distributions of different views, given a scene with a total observation set 𝐒, $p(𝐱|S)$ are conditionally independent (different).\nNeRF solves NVS under an even strict condition: each ray in the scen is conditionally independent.\nHowever, with this nature, the diffusion model cannot guarantee the samplings (generated images), conditioned with different source view, follow a common distribution, i.e., the diffusion model needs a unique distribution to learn.\nIdeally, the common distribution should be p(S), but it\u0026rsquo;s difficult to approximate the entire scene based on sparse views. (Not sure, my guess.)\nThat\u0026rsquo;s why they reused the generated views previously for later condition.\nPose-conditioned Given the data distribution p(𝐱₁, 𝐱₂), diffusion model learns the distribution of one of the two images conditioned on the other image and both poses.\nNoise schedule involving signal-to-noise ratio λ. Loss function of DDPM Stochastic condition Figure 3: Stochastic conditioning sampler\nMarkovian model didn\u0026rsquo;t perform well, where the next image is conditioned on (k) previously generated views. Thus, a scene can be represented as $p(𝐗) = ∏ᵢp(𝐱ᵢ|𝐱_{\u0026lt;i})$.\nUsing all previous sampled views is imfesible due to the limited memory.\nThey found k=2 can achieve 3D consistency, but more previous states impair the sampling quality.\nAnd instead of conditioning on the last few samplings as in the Markovian model, 2 views are stochastically selected as condition images at each denoising step.\nGenerating a new view $𝐱ₖ₊₁$ needs 256 denoising steps, where each time the condition image 𝐱ᵢ is randomly chosen from the current views set 𝜲 = {𝐱₁,\u0026hellip;,𝐱ₖ}.\nIn a denoising step, noise in the intermediate image $\\hat 𝐱ₖ₊₁$ will be subtracted from $𝐳ₖ₊₁^{(λₜ)}$, which follows a forward noising distribution 𝒒, given the noisy image $𝐳ₖ₊₁^{(λₜ₋₁)}$ of last step and the denoised image $\\hat 𝐱ₖ₊₁$.\n$$ \\hat 𝐱ₖ₊₁ = \\frac{1}{\\sqrt{σ(λₜ)}} \\left( 𝐳ₖ₊₁^{(λₜ)} - \\sqrt{σ(-λₜ)}\\ ε_θ(𝐳ₖ₊₁^{(λₜ)}, 𝐱ᵢ) \\right) \\\\ \\ \\\\ 𝐳ₖ₊₁^{(λₜ₋₁)} \\sim q(𝐳ₖ₊₁^{λₜ₋₁};\\ 𝐳ₖ₊₁^{(λₜ)}, \\hat 𝐱ₖ₊₁ ) $$\n( I guess 𝒒 gets \u0026ldquo;reversed\u0026rdquo; after applying Bayes rule)\nThe first noisy image $𝐳ₖ₊₁^{(λ_T)}$ is Gaussian N(0,𝐈).\nAfter 256 steps finished, add the result image 𝐱ₖ₊₁ to set 𝜲.\n256 can be larger to cover all the existing views.\nThis scheme approximate the true autoregressive sampling.\nAutoregressive model always use all previous states to predict the next state, unlike Markov chain only considering limited recent outputs,\nTherefore, to train an autoregressive model, a sequence, i.e. multi-view training data here, is needed.\n\u0026ldquo;True autoregressive sampling needs a score model of the form $log\\ q(𝐳ₖ₊₁^{(λ)} | 𝐱₁,\u0026hellip;,𝐱ₖ)$ and multi-view training data.\u0026rdquo;\nBut they\u0026rsquo;re not interesting in multiple source views here.\nX-UNet Figure 4: X-UNet Architecture\nUNet with only self-attention fails to generate images with multi-view consistency, given limited training images.\nDifferent frame has a different noise-level Positional encoding of pose is the same size of feature maps Use cross-attention to make two images attend to each other. Inputs:\nConcat two images? such that the weights of Conv2d and self-attention layers are shared for the noisy image and condition image\nExperiments Dataset: SRN ShapeNet (synthetic cars and chairs) github\nfile size cars_train.zip 3.26GB chairs_train.zip 60.3GB Use instant-NGP without view dependent modules\n","date":"2023-08-12T09:40:00Z","image":"https://ar5iv.labs.arxiv.org/html/2210.04628/assets/figures/training.png","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/b-note-nvs-dm-posecond/","title":"read: NVS with Pose-conditioned Diffusion Models"},{"content":"Code | Arxiv (2307) | ProjPage\nAbs \u0026amp; Intro Fine-tune the pre-trained text-to-image diffusion model (SD) Insert cross-attention blocks between UNet blocks; Generate multiple views in parallel using a SD, and fuse multi views by attention; Freeze pre-trained weights while training the attention blocks Solving problems:\nGenerating panorama Extrapolate one perspective image to a full 360-degree view Preliminary MVDiffusion derives from LDM (Latent Diffusion Model¹), which contains 3 modules:\nVAE for transfering the generation process to a latent space, denoising model (UNet) for sampling from the distribution of the inputs\u0026rsquo; latent codes, condition encoder for providing descriptors. Loss function is similar to original diffusion model: the MSE between original noise and predicted noise, which conditioned on noisy latents, timestep, and featues.\n$$L_{LDM} = ∑$$\nConvolution layers are insert in each UNet block:\nFeature maps at each level will be added into UNet blocks. Figure 2\nPipeline pixel-to-pixel correspondences enable the multi-view consistent generation of panorama and depth2img, because they have homography matrix and projection matrix to determine the matched pixel pairs in two images.\nPanorama Text-conditioned model: generate 8 target images from noise conditioned by per-view text prompts.\nThe final linear layer in CAA blocks are initialized to zero to avoid disrupt the SD\u0026rsquo;s original capacity.\nMultiple latents will be predict by UNet from noise images and then restored to images by the pre-trained VAE\u0026rsquo;s decoder.\nImage\u0026amp;Text-conditioned model: generate 7 target images based on 1 condition image and respective text prompts.\nBased on SD\u0026rsquo;s impainting modle as it takes 1 condition image.\nDon\u0026rsquo;t impaint the condition image by concatenating the noise input with a all-one mask (4 channels in total)\nCompeletly regenerate the input image by concatenating the noise input with a all-zero mask (4 channels in total)\nConcatenating all-one and all-zero channel during training make the model learn to apply different processes to condition image and target image.\ndepth2img S d \u0026amp; T e e e q p p x u t o t e h s n e p c m s r e a o p m o s p f t I A m n a T y g M S k e e i u e x t \u0026amp; d b y t m w i T m d s - o o m e o l e f c d a x d e t r o e c g t e a n l o e - l i o m d n s c m f e i s o a s t e n g i c d e o u i n t t e i i d v o e n e d k e i y m - a f g r e a s m e This two-stage design is because SD\u0026rsquo;s impainting modle doesn\u0026rsquo;t support depth map condition.\nSo the Text-conditioned model is reused to generate condition images. Then the Image\u0026amp;Text-conditioned model interpolate the two condition images. Correspondence-aware attention Aggregate the features of KxK neighbor pixels on every target feature maps to each pixel their own feature vector.\nThe source pixel $s$ perform positional encoding γ(0)\nThe neighbor pixel $t_*^l$ around the corresponding pixel $t^l$ on the target image $l$ perform position encoding $γ(s_*^l-s)$, which means the neighbor pixel $t_*^l$ need to be warpped back to source feature map to find the distance from $s_*^l$ to the source pixel $s$.\nFigure 3\nRef High-Resolution Image Synthesis with Latent Diffusion Models - Robin Rombach ","date":"2023-08-10T20:40:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/b-note-mvdiffusion-scene/","title":"read: MVDiffusion generates multi-view images"},{"content":"Arxiv\nAbstract Embed diffusion model into stereo matching network Adopt multi-level network for high-resolution input Fuse generated depth map to reconstruct 3D human model. Introduction Sparse-view methods, which predict geometry based on appearance, cannot produce detailed human model because of lacking sufficient multiview stereo matching.\nContinuous models are basically obtained from traditional stereo methods based on a continuous varitional formulation, which can solved by diffusion model.\nPipeline:\nReconstruct coarse field first by using DoubleField; Render depth maps from multiple viewpoints Compute disparity flow masks Refine disparity flow with diffusion model Level 1: Use CNN to extract feature maps of disparity flow masks Level 2: Condition diffusion model with feature maps Fuse 3D points through interpolation. ","date":"2023-08-10T18:40:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/b-note-diffustereo-human/","title":"read: DiffuStereo reconstruct 3D human"},{"content":" (2023-07-28)\ntaku Source video: 你必须拥有RWKV，以及其他模型为何拉胯，NLP杂谈 - taku的交错电台 - bilibili\nSequentially generating mimics the human speaking behavior naturally;\nNo need to perform Positional Encoding, RNN won\u0026rsquo;t mess up the order of tokens,\nbecause the next token always derives from the previous hidden state; RNN internally has time order. Vanilla transformer doesn\u0026rsquo;t include Positional Encoding. Cannot generate a sentence parallelly, but only word-by-word,\nbecause the \u0026ldquo;fusion matrix multiplication\u0026rdquo;, i.e., attn @ V has simplied to a RNN; Less multiplication: RNN module only consider previous two hidden states;\nAnd the matmul of Q*V and feedfoward module are kept. Locally optimizing can be done by giving a hidden state.\nTransformer has the ultimate precision because it attends all the token in the sequence, but it\u0026rsquo;s not necessary if the required performance can be met in some way.\nRWKV may forget former tokens along inputting. RWKV is good for inference and Transformer is good for training.\nRWKV incorporates RNN into transformer;\nSuch that RNN is combined with residual connection: hidden_state = hidden_state + attention(hidden_state)\nPrevious RNN is combine with attention but without residual connection;\nTransformer is attention + residual\nRWKV has a consistent memory cost at inference,\nbecause each generation only attends to the last two hidden states. That means it can accept infinite-long sequence when inference.?\nHowever, the memory cost grows up linearly when tranining, because the intermediate hidden states are required to store for calculating gradients.\nLarge model ranking\nSu, Jianlin Source article: Google新作试图“复活”RNN：RNN能否再次辉煌？- 苏剑林\nOnly if the sequence length is significantly longer than hidden size, the standard attention will become slower quickly because its quadratic complexity. Otherwise, it\u0026rsquo;s almost linear complexity. So, it\u0026rsquo;s not necessary to make attention linear.\nOn LM (Language Model) tasks, RNN underperform attention may suffer from the hidden size.\nYannic Kilcher Source video: RWKV: Reinventing RNNs for the Transformer Era (Paper Explained) - Yannic Kilcher\n","date":"2023-07-28T17:59:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/c-sum-rwkv/","title":"sum: RWKV"},{"content":"ResNet Deep Residual Learning for Image Recognition ~ 2015 MSRA CVPR arxiv\nNetwork Architectures: Bottleneck Block: Figure 5: A deeper residual function ℱ for ImageNet. Left: a building block (on 56×56 feature maps) as in Fig. 3 for ResNet-34. Right: a “bottleneck” building block for ResNet-50/101/152.\nCode: torchvision\nMobileNet MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications ~ 2017 Google arxiv\nFigure 3: Left: Standard convolutional layer with batchnorm and ReLU. Right: Depthwise Separable convolutions with Depthwise and Pointwise layers followed by batchnorm and ReLU.\nDepthwise + Pointwise convolution reduces FLOPs and parameters. Accuracy is slightly inferior to fully CNN MobileNet V3 Searching for MobileNetV3 ~ 2019 Google ICCV arxiv\nArchitecture of MobileNetV3-Large:\nTable 1 Specification For Mobilenetv3-Large. SE Denotes Whether there Is A Squeeze-and-Excite In That Block. NL Denotes the Type of Nonlinearity Used. Here, HS Denotes H-Swish and RE Denotes Relu. NBN Denotes No Batch Normalization. S Denotes Stride.\nTutorial-bili\nCode: torchvision\nEfficientNet EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ~ 2019 Google ICML arxiv\nFigure 2:Model Scaling. (a) is a baseline network example; (b)-(d) are conventional scaling that only increases one dimension of network width, depth, or resolution. (e) is our proposed compound scaling method that uniformly scales all three dimensions with a fixed ratio.\nSummary:²\nScale proportionally the resolution and channels of feature maps, and number of blocks in a model. Use NAS (Neural Architecture Search) to search a structure for smaller models. Architecture:\nMBConv block is similar to MobileNetV3 InvertedResidualBlock. 8\nCode: torchvision\nVision Transformer An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ~ 2020 Google ICLR arxiv\nImage from 7\nConvert an image (224x224) to a vector using a 16x16 kernel with a stride of 16 and then flattening the result feature map (14x14) into a sequence (196).\nBy performing 728 times convolution for getting 728 vectors, an image is represented by a matrix (196, 728).\nTo encode this image\u0026rsquo;s information into a vector for classification, the class label are concatenated onto each vector and Positional encoding is added element-wise onto each vector.\nThen this matrix (197, 728) gets passed through Multi-head Self-Attention so that each token (728) will obtain a vector recording the similarity (mutual information?) between it and each other token.\nOnly taking out the vector belonging to the class label, it will be projected to the number of target categories for classifing.\nThe hybrid model didn\u0026rsquo;t downsample the image by a 16x16 kernel, but use ResNet50 to shrink the 224x224 image to 14x14.\nWhen training fewer epochs (7), hybrid model has higher accuracy than standard ViT. However, more epochs will make ViT better than hybrid model. ViT needs pre-traine on a large dataset (Google JFT) to perform better on ImageNet. However, if it\u0026rsquo;s trained on ImageNet-1K directly, the result won\u0026rsquo;t be good.\nSwin Transformer Swin Transformer: Hierarchical Vision Transformer using Shifted Windows ~ 2021 MSRA ICCV arxiv\nA unit component of swin transformer has two blocks: a Window MSA and a Shifted Window MSA.\nPatch merging\nImage from 5\nWindow MSA\nInstead of performing MSA for the all patches (sequence) of a feature map, a feature map is divided into finer grid, where severl patches is a group and a group of patches perform MSA.\nThis way reduces computation.\nDisadvantage is that the context between different group isn\u0026rsquo;t built.\nShifted Window MSA\nMove the grid (H/2, W/2) patches to the bottom right, then different groups can be fused through MSA.\nTo enhence parallelization, top row of patches are moved to the bottom and the left-most column of patches are moved the right-most.\nTo avoid fuse non-neighbor patches that are not adjacent in the original feature maps, masked MSA is used.\nThe masks are added onto the q-k weights corresponding to the non-neighbor patches for the current patch (q).\n(2023-09-28)\nSplit windows 1 image, 2 channels, H=4, W=6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 a = torch.arange(48).reshape(1,2,4,6) [[[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]] # a.is_contiguous() == True b = rearrange(a, \u0026#34;B C (nh H) (nw W) -\u0026gt; B C nh H nw W\u0026#34;, nh=2, nw=2) # b.is_contiguous() == True c = rearrange(b, \u0026#34;B C nh H nw W -\u0026gt; B nh nw C H W\u0026#34;) # c.is_contiguous() == False d = rearrange( c, \u0026#34;B nh nw C H W -\u0026gt; (B nh nw) C H W\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 b (1,2,2,2,2,3) c (1,2,2,2,2,3) d (4,2,2,3) [[[[[[ 0, 1, 2], [[[[[[ 0, 1, 2], [[[[ 0, 1, 2], [ 3, 4, 5]], [ 6, 7, 8]], [ 6, 7, 8]], [[ 6, 7, 8], [[24, 25, 26], [[24, 25, 26], [ 9, 10, 11]]], [30, 31, 32]]], [30, 31, 32]]], [[[12, 13, 14], [[[ 3, 4, 5], [[[ 3, 4, 5], [15, 16, 17]], [ 9, 10, 11]], [ 9, 10, 11]], [[18, 19, 20], [[27, 28, 29], [[27, 28, 29], [21, 22, 23]]]], [33, 34, 35]]]], [33, 34, 35]]], [[[[24, 25, 26], [[[[12, 13, 14], [[[12, 13, 14], [27, 28, 29]], [18, 19, 20]], [18, 19, 20]], [[30, 31, 32], [[36, 37, 38], [[36, 37, 38], [33, 34, 35]]], [42, 43, 44]]], [42, 43, 44]]], [[[36, 37, 38], [[[15, 16, 17], [[[15, 16, 17], [39, 40, 41]], [21, 22, 23]], [21, 22, 23]], [[42, 43, 44], [[39, 40, 41], [[39, 40, 41], [45, 46, 47]]]]]] [45, 46, 47]]]]]] [45, 46, 47]]]] The above 3 steps are equivalent to: e = rearrange(a, \u0026quot;B C (nh H) (nw W) -\u0026gt; (B nh nw) H W, nh=2, nw=2)\n= 1 1 0 6 0 6 = 2 8 = = 1 1 1 7 1 7 = 3 9 = = 1 2 2 8 2 - 8 = 4 - 0 ‖ ‖ ‖ ‖ ‖ ‖ ‖ - = 1 - 2 2 3 3 - 9 = 5 - 1 4 0 = 1 = 1 2 2 3 4 0 = 6 2 5 1 = 1 = 1 2 2 3 5 1 = 7 3 6 3 3 9 4 1 2 3 = 3 4 0 4 0 = 6 2 = 5 1 2 3 = 3 4 1 5 1 = 7 3 = 2 3 2 3 = 3 4 7 3 6 - 2 = 8 - 4 ‖ ‖ ‖ ‖ ‖ ‖ ‖ 2 3 2 - 3 = 3 - 4 8 4 7 - 3 = 9 - 5 = 2 3 2 3 = 4 4 9 5 8 4 = 0 6 = 1 1 2 3 = 4 4 2 8 9 5 = 1 7 1 1 3 9 1 2 4 0 3 4 6 2 3 4 7 3 3 4 8 4 1 2 5 1 1 2 6 2 1 2 7 3 3 4 9 5 4 4 0 6 4 4 1 7 Restore feat maps .permute() changed .stride(), which can\u0026rsquo;t return to the structure that matches with the tensor\u0026rsquo;s original shape anymore.\nTherefore, .contiguous() is necessary before tensor .view() to the original size.\n1 2 e = d.view(1, 2, 2, 2, 2, 3) # (B, nh, nw, C, H, W) merge = e.permute(0, 3, 1, 4, 2, 5).contiguous().view(1, 2, 4, 6) # [B, C, H, W] Code from MatchNeRF.\nConvNeXt A ConvNet for the 2020s ~ 2022 FAIR CVPR arxiv\nModify ResNet50 according to Swin Transformer: 4\nStages: [3, 4, 6, 3] ➡ [3, 3, 9, 3] (Tiny) ³ Stem: First conv1 (kernel=7x7, stride=2, pad=3) and maxpool (stride=2) ➡ Conv2dNormActivation (kernel=4x4, stride=4) Depthwise Conv: groups=1 ➡ groups=input_channels Expand Input Chanls: Stage 0~4=(64,256,512,1024,2048) ➡ Stage 0~4=(96,192,384,768) Expand Middle Chanls: Bottleneck (256➞64➞64➞256) ➡ Inverted Bottleneck (96➞384➞384➞96) Conv First: fc + conv + fc ➡ conv + fc + fc. Because in a transformer block, attention is ahead of fc. Large Kernel: 3 ➡ 7 Activation: ReLU ➡ GELU Fewer Activation: After each Conv2d ➡ After 1st 1x1 conv Fewer Norms: After each Conv2d ➡ After 1st 7x7 conv Norms: BatchNorm ➡ LayerNorm Downsample Layer: Conv(stride=2) ➡ LayerNorm + Conv2d(k=2,s=2) A ConvNext block mimics a transformer block: attention + feedforward (MLP), so Multi-Head Self-Attention corresponds to Depthwise Conv, and feedforward corresponds to 1x1 conv + activation. 6\nNetwork Architecture:\nCode flowchart from 4\nCodes: torchvision | csdn-AI浩 | official\nReference ConvNeXt实战：使用ConvNeXt实现植物幼苗分类（自创，非官方）- AI浩 细品EfficientNet - 沈景兵的文章 - 知乎 ConvNext | Less is More (Found this under the Images section of DDG with searching \u0026ldquo;convnext model\u0026rdquo;)\nConvNeXt网络详解 - 太阳花的小绿豆 - CSDN Swin-Transformer网络结构详解 - 太阳花的小绿豆 - csdn A Basic Introduction to Separable Convolutions - Medium Vision Transformer详解 - 太阳花的小绿豆 - csdn ","date":"2023-07-25T16:00:00Z","image":"https://pic2.zhimg.com/80/v2-35966819d100014901f3c819b7252c65_720w.webp","permalink":"https://zichen34.github.io/writenotes/model/misc/c-symp-vision_champions/","title":"sum: Champions on Vision"},{"content":"Source video: PyTorch Hooks Explained - In-depth Tutorial - Elliot Waite\nHooks for tensor 1 a.register_hook(hook_func) This will add a property _backward_hooks for the tensor a. And hooks for tensors only take effect when back-propagating (when the gradient is calculated).\nhook_func can be normal function or a lambda function, which takes as input the gradient grad for this tensor a coming from the last node, and pass the current gradient to the later backwards graph.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def hook_func(grad: Tensor): print(grad) return grad + 1 # the grad passed to the next operation increased by 1. a.register_hook(hook_func) # Second hook function a.register_hook(lambda grad: print(grad)) # Third hook function: changed grad a.register_hook(lambda grad: grad * 100) # Fourth hook function: save gradient for an intermediate node a.retain_grad() _backward_hooks is an OrderDict so it can contain multiple functions, and they\u0026rsquo;ll be executed according to their definition sequence.\nRegistering a hook for an intermediate node tensor will notify the corresponding tensor in the backwards graph.\nInside the associated node on the backwards graph, there will be an additional property: pre_hooks list, which will call the hook property a._backward_hooks of that tensor, before the grad getting into the method backward.\nSo the hook will be executed ahead of backward property during the back-propagating. backward will use the gradients returned from the hooks.\nHowever, when setting a hook for a leaf node, the hook function will only add the hook func into the _backward_hooks OrderDict of that leaf node.\nAnd the associated AccumulateGrad node of that leaf node will check if the leaf node has hook function needed to be executed before assigning grad from previous calculations.\nEach hook function has a handle index, which will be returned after executing the hook function, e.g., h = c.register_hook(hook_func) A hook can be removed via the handle index: h.remove()\nCaveat: Change grad in-place in the hook functions may affect other tensors\u0026rsquo; gradients, so later backward pass will be mess up.\nFor example, as for the grad_fn of operation e = c+d, the output gradients for tensor c and d are supposed to be the same. If the grad of d has changed, the grad of c will also changed.\ne.g. Gradient clipper ¹ Clamp the gradient of each tensor in a certain range by registering a hook for each parameter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torchvision import models def gradient_clipper(model: nn.Module, val: float) -\u0026gt; nn.Module: for parameter in model.parameters(): # in-place changing the gradient parameter.register_hook(lambda grad: grad.clamp_(-val, val)) return model resnet_clipped = gradient_clipper(models.resnet50(), val=0.01) dummy_input = torch.ones(1, 3, 224, 224) pred = resnet_clipped(dummy_input) loss = pred.log().mean() loss.backward() print(resnet_clipped.fc.bias.grad[:25]) Hooks for modules Hooks registered for modules can be automatically triggered before or after a nn.module.forward is called (even if a layer), so a hook can modify the input and output tensors of a nn.module\nHooks before forward register_forward_pre_hook(hook_func), where the hook_func can access the module and its positional input.\n1 2 3 4 5 def hook_func_pre_forward(module: nn.Module, inputs: Tensor): a, b = inputs return a+2, b myModel.register_forward_pre_hook(hook_func_pre_forward) Hooks after forward: register_forward_hook(hook_func), where the hook_func will recieve 3 arguments: the module, its input, and its output.\n1 2 3 4 def hook_func_forward(module: nn.Module, inputs: Tensor, output: Tensor): return output + 10 myModel.register_forward_hook(hook_func_forward) Hooks after backward: register_backward_hook() has been deprecated in favor of register_full_backward_hook()\ne.g. Inspect a model ¹ Printing the shape of output tensors after each layer by registering a hook for each layer in an external wrapper, rather than adding print inside the model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch from torch import nn, Tensor from torchvision import models class VerboseExecution(nn.Module): def __init__(self, model: nn.Module): super().__init__() self.model = model # Register a hook for each layer for name, module in self.model.named_children(): # conv1, bn1, relu, maxpool, layer1, ... module.__name__ = name module.register_forward_hook(self.print_shape()) def print_shape(self): def hook_func(module, inputs, output): print(f\u0026#34;{module.__name__}: {output.shape}\u0026#34;) return hook_func def forward(self, x: Tensor) -\u0026gt; Tensor: return self.model(x) # Print intermediate shape in ResNet50 resnet_verbose = VerboseExecution(models.resnet50()) dummy_input = torch.ones(1, 3, 224, 224) _ = resnet_verbose(dummy_input) e.g. Extract feature maps ¹ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import torch from torch import nn, Tensor from torchvision import models from typing import Dict, Iterable, Callable class FeatureExtractor(nn.Module): def __init__(self, model: nn.Module, layer_ls: Iterable[str]): super().__init__() self.model = model self.layers_ex = layer_ls # Define a dict to store feature maps self._features = {layer: torch.empty(0) for layer in layer_ls} for name in layer_ls: # Pick out the selected layers by their names from a dictionary layer = dict([*self.model.named_modules()])[name] # Register a hook for each layer layer.register_forward_hook(self.save_outputs(name)) def save_outputs(self, layer_name: str) -\u0026gt; Callable: def hook_func(module, inputs, output): self._features[layer_name] = output return hook_func def forward(self, x: Tensor) -\u0026gt; Dict[str, Tensor]: _ = self.model(x) return self._features # Extract feature maps at each level before \u0026#34;avgpool\u0026#34; and \u0026#34;fc\u0026#34; resnet50 = models.resnet50() resnet_features = FeatureExtractor( resnet50, layer_ls = list(resnet50._modules)[:-2] ) dummy_input = torch.ones(1, 3, 224, 224) feature_maps = resnet_features(dummy_input) print({name: output.shape for name, output in feature_maps.items()}) Reference How to Use PyTorch Hooks ","date":"2023-07-22T15:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_hooks/","title":"memo: PyTorch | Hooks"},{"content":"How to extract features of an image from a trained model - PyTorch Forum\nHow can l load my best model as a feature extractor/evaluator?\nLoad and call torchvision.models | ResNet50 Docs\n1 2 3 4 5 6 7 8 9 10 11 import torch from torch import nn from torchvision import models, transforms import PIL resnet50 = models.resnet50(weights=\u0026#39;DEFAULT\u0026#39;) resnet50.eval() im_tensor = transforms.ToTensor()(PIL.Image.open(\u0026#39;data/nerf_llff_data/fern/images_4/image000.png\u0026#39;)) output = resnet50(im_tensor[None, ...]) Or using the weights object:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from torchvision import models from torchvision.io import read_image im = read_image(\u0026#34;data/nerf_llff_data/fern/images_4/image000.png\u0026#34;) # Step 1: Initialize model with the best available weights weights = models.ResNet50_Weights.DEFAULT model = models.resnet50(weights=weights) model.eval() # Step 2: Initialize the inference transforms preprocess = weights.transforms() # Step 3: Apply inference preprocessing transforms batch = preprocess(im).unsqueeze(0) # Step 4: Use the model and print the predicted category prediction = model(batch).squeeze(0).softmax(0) category_id = prediction.argmax().item() score = prediction[category_id].item() category_name = weights.meta[\u0026#34;categories\u0026#34;][category_id] print(f\u0026#34;{category_name}: {100*score:.1f}%\u0026#34;) triceratops: 11.5%\nBut the prediction seems not to be accurate.\nAdjust image input example\n1 2 3 4 5 6 # Initialize the Weight Transforms weights = ResNet50_Weights.DEFAULT preprocess = weights.transforms() # Apply it to the input image img_transformed = preprocess(img) (2023-07-23)\nInspect model\u0026rsquo;s modules Use hook to print layer name and the shape of their outputs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import torch from torch import nn, Tensor from torchvision import models class InspectModel(nn.Module): def __init__(self, model: nn.Module): super().__init__() self.model = model self.hook_handles = [] # print layers and their outputs\u0026#39; shape for name, module in self.model.named_children(): module.__name__ = name handle = module.register_forward_hook( lambda module, inputs, output: print(f\u0026#34;{module.__name__}: {output.shape}; Op: {module._get_name()}\u0026#34;)) self.hook_handles.append(handle) def forward(self, x: Tensor): self.model(x) for handle in self.hook_handles: handle.remove() Another ugly way is using a for loop:\n1 2 for name, module in self._modules.items(): print(name) Extract \u0026amp; Intrpl feature maps Retrieve specified layers\u0026rsquo;s feature maps and interpolate them to the same size.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import torch from torch import nn, Tensor from torchvision import models from typing import Callable, Iterable, Tuple import torch.nn.functional as F class ExtractIntrplFeatures(nn.Module): def __init__(self, model: nn.Module, layer_names: Iterable[str] = None, chnl_dim: int = 1) -\u0026gt; None: super().__init__() self.model = model self.layer_names = layer_names self._features = {} # Register a hook for each layer if layer_names is None: layer_names = list(model._modules) for layerName in layer_names: layer = dict([*self.model.named_children()])[layerName] layer.register_forward_hook(self.save_features( layerName, chnl_dim)) def save_features(self, layerName, chnl_dim) -\u0026gt; Callable: # print(layerName) def hook_func(module, inputs, output): if chnl_dim != 1: perm_order = list(range(len(output.shape))) perm_order.remove(chnl_dim) perm_order = [0, chnl_dim] + perm_order[1:] output = output.permute(*perm_order) self._features[layerName] = output return hook_func def forward(self, x: Tensor, size: Tuple[int] = None) -\u0026gt; Tensor: self.model(x) # Interpolate to the same size as the first conv feature for i in range(0, len(self._features)): layerName = list(self._features.keys())[i] if size is None: size = list(self._features.values())[0].shape[-2:] self._features[layerName] = F.interpolate( input=self._features[layerName], size=size, mode=\u0026#34;bilinear\u0026#34;, align_corners=True, ) return torch.cat(list(self._features.values()), dim=1) (2023-08-08)\nFeature extraction Get feature map at certain layers through create_feature_extractor() torchvision.models.feature_extraction — Torchvision 0.11.0 documentation I guess it cannot realize fine-tuning the pre-trained model.\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from torchvision import models from torchvision.models.feature_extraction import get_graph_node_names from torchvision.models.feature_extraction import create_feature_extractor model = models.resnet50( weights=\u0026#34;DEFAULT\u0026#34;) train_nodes, eval_nodes = get_graph_node_names(model) print(train_nodes) featExtractor = create_feature_extractor(model, return_nodes={\u0026#39;layer4.2.relu_2\u0026#39;:\u0026#39;layer4_feat\u0026#39;}) inp = torch.ones(2, 3,224,224) with torch.no_grad(): out = featExtractor(inp) # dict (2023-08-08)\n.modules vs ._modules convnext.modules is a method. Its output content is in a sepcific format.\nconvnext.modules() is a generator. Docs - nn.Module(); Docs - Modules\nHowever, if I traverse it like for _ in convnext.modules(): print(_), it will repeatly print all the modules in the model.\nAnd chatGPT answer:\nmodules() function also iterates through sub-modules of each module, resulting in duplicate prints. To avoid this, you can use the children(). Using children() will only give you the immediate sub-modules of the features module\n1 2 3 4 5 import torchvision.models as models convnext = models.convnext_tiny(weights=\u0026#39;DEFAULT\u0026#39;) for module in convnext.features.children(): print(module) convnext._modules is an OrderedDict\nlist(convnext._modules) only has the keys (name of the modules).\nTruncate pre-trained model Question for chatGPT:\n\u0026ldquo;Given a pre-trained multiple-layer neural network in PyTorch, how to run a part of it and stop at certain layer?\u0026rdquo;\nJust extract features up to a certain layer, without performing classification or regresion steps.\n1 2 3 4 5 6 7 8 # 1. Load model import torch.nn as nn from torchvision import models pre_model = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) # 2. Create a new model sliced_model = nn.Sequential(*list(pre_model.features.children())[:5]) sliced_model.eval() The new model will inherit the pre-trained weights.\nAlexNet Source code\nAlexNet.features contains 13 modules\n1 2 3 4 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) alexnet_debug = InspectModel(alexnet.features) dummy_input = torch.ones(1,3,224, 224) alexnet_debug(dummy_input) Get pre-logits vector Retrieve the feature vector before it gets compressed to 1_000 categories, i.e., removing the last Linear layer of \u0026lsquo;classifier\u0026rsquo; module in alexnet and making a new model\n1 2 3 4 5 6 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) # drop the last \u0026#39;linear layer\u0026#39; in classifier module new_classifier = nn.Sequential(*list(alexnet.classifier.children())[:-1]) alexnet.classifier = new_classifier Get feature maps Make a new model that stops at a certain feature map\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class AlexNetConv4(nn.Module): def __init__(self, original_alexnet): super().__init__() # stop at conv4 layer_list = list(original_alexnet.features.children())[:-3] self.features = nn.Sequential(*layer_list) def forward(self, x): x = self.features(x) return x model = AlexNetConv4(models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;)) dummy_input = torch.ones(1, 3, 224, 224) features = model(dummy_input) # (1, 256, 13, 13) Retrieve the feature maps after ReLU each time:\n1 2 3 4 5 6 alexnet = models.alexnet(weights=\u0026#39;DEFAULT\u0026#39;) alexnet_features = ExtractIntrplFeatures( alexnet.features, layer_names=[\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;11\u0026#39;] ) dummy_input = torch.ones(1, 3, 224, 224) feat_alexnet = alexnet_features(dummy_input) # (1, 1152, 55, 55) ResNet Get feat map of resnet34 Pixel-NeRF obtains the feature map by copying the forward method until the layer4 and concat feature maps of each layer along the channels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class SpatialEncoder(nn.Module): def __init__(self, backbone): super().__init__() self.model = getattr(torchvision.model, backbone)(weights=\u0026#39;DEFAULT\u0026#39;) # Modifying model here doesn\u0026#39;t affect forward method. self.model.avgpool = nn.Sequential() self.model.fc = nn.Sequential() def forward(self, img): # DTU img: (1, 3, H=300, W=400) ... x = self.model.conv1(x) # (B, 64, H/2, W/2) x = self.model.bn1(x) x = self.model.relu(x) latents = [x] # store feature maps after different times of convlution if self.num_layers \u0026gt; 1: if self.use_first_pool: x = self.model.maxpool(x) # (B, 64, H/4, W/4) x = self.model.layer1(x) # (B, 64, H/4, W/4) latents.append(x) if self.num_layers \u0026gt; 2: x = self.model.layer2(x) # (B, 128, H/8, W/8) latents.append(x) if self.num_layers \u0026gt; 3: x = self.model.layer3(x) # (B, 256, H/16, W/16) latents.append(x) if self.num_layers \u0026gt; 4: x = self.model.layer4(x) # (B, 512, H/32, W/32) latents.append(x) self.latents = latents align_corners = None if self.index_interp == \u0026#34;nearest \u0026#34; else True latent_sz = latents[0].shape[-2:] # (H/2, W/2) # expand the feature maps to the original size for i in range(len(latents)): latents[i] = F.interpolate( input=latents[i], size=latent_sz, # (H/2, W/2) mode=self.upsample_interp, # bilinear align_corners=align_corners, ) self.latent = torch.cat(latents, dim=1) # (B, 64+64+128+256, H/2, W/2) self.latent_scaling[0] = self.latent.shape[-1] # W, 200 self.latent_scaling[1] = self.latent.shape[-2] # H, 150 self.latent_scaling = self.latent_scaling / (self.latent_scaling - 1) * 2.0 # tensor([200., 150.]) -\u0026gt; tensor([2.0101, 2.0134]) return self.latent # (B, 512, H/2, W/2) There is a flatten step in the forward method (Source code), so even the avgpool and fc are canceled as self.model.fc = nn.Sequential(), the output after calling the modified model will still become a vector, but not a feature map (planes).\nThe solution is to create a model whose forward method doesn\u0026rsquo;t contain the torch.flatten operation.\n1 2 3 4 5 resnet34 = models.resnet34(weights=\u0026#39;DEFAULT\u0026#39;) resnet_feat = nn.Sequential(*list(resnet34.children())[:-2]) im_tensor = transforms.ToTensor()(PIL.Image.open(\u0026#39;path/to/png\u0026#39;))[None,...] resnet_feat(im_tensor) However, if want to keep the feature map at every level, the forward method has to be rewritten. hooks for modules can realize this.\n(2023-07-23)\nHook feat maps 1 2 3 4 5 6 7 8 resnet34 = models.resnet34(weights=\u0026#39;DEFAULT\u0026#39;) # save feature maps after: relu, layer1, layer2, layer3. resnet_features = ExtractIntrplFeatures( resnet34, [\u0026#34;relu\u0026#34;, *list(resnet34._modules)[4:-3]] ) im_tensor = transforms.ToTensor()( PIL.Image.open(\u0026#39;data/nerf_llff_data/fern/images_4/image000.png\u0026#39;)) feat = resnet_features(im_tensor.unsqueeze(0)) The result is identical to pixel-NeRF\u0026rsquo;s SpatialEncoder, which I copied its definition to a ipynb and instantiate it:\n1 2 3 pixelNeRFEncoder = SpatialEncoder() feature_maps = pixelNeRFEncoder(im_tensor.unsqueeze(0)) torch.eq(feat, feature_maps).detach().numpy().all() (2023-07-21)\nMobileNet v3 MobileNetV3 contains 3 components: features, avgpool, and classifier.\nThe MobileNetV3.features starts with a Conv2dNormActivation layer followed by 15 InvertResidual blocks, and ends with a Conv2dNormActivation layer.\nAn InvertResidual module is a nn.Sequential model of 3 Conv2dNormActivation() layers, corresponding to \u0026ldquo;expand\u0026rdquo;, \u0026ldquo;depthwise\u0026rdquo;, and \u0026ldquo;project\u0026rdquo;. Source code\nInspect mobilenet 1 2 3 4 mobilenetv3 = models.mobilenet_v3_large(weights=\u0026#39;DEFAULT\u0026#39;) mobilenet_inspect = InspectModel(mobilenetv3.features) dummy_input = torch.ones(1, 3, 224, 224) mobilenet_inspect(dummy_input) Hook feature maps 1 2 3 4 5 6 mobilenetv3 = models.mobilenet_v3_large(weights=\u0026#39;DEFAULT\u0026#39;) mobilenet_feats = ExtractIntrplFeatures( mobilenetv3.features, layer_names=list(mobilenetv3.features._modules) ) feat_mbnet = mobilenet_feats(dummy_input) # (1,1224,112,112) faster-rcnn Obj Detect Everything you need to know about TorchVision’s MobileNetV3 implementation - PyTorch blog\nDocs | Faster RCNN Code\nMobileNet feature maps \u0026ndash;\u0026gt; Feature Pyramid Network \u0026ndash;\u0026gt; FasterRCNN detector for locating bounding box\n1 2 3 4 high_res = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) high_res.eval() out = high_res([im_tensor[0]])[0] out.keys() 1 2 3 4 5 6 7 low_res = models.detection.fasterrcnn_mobilenet_v3_large_320_fpn( weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT) low_res.eval() x = [im_tensor, torch.rand(3, 500, 400)] predictions = low_res(x) print(predictions[0].keys()) lraspp segment Outputs are masks for different classes. I guess these masks may be not suitable to serve as feature maps.\n1 2 3 4 5 6 lraspp = models.segmentation.lraspp_mobilenet_v3_large(pretrained=True) # input: (B, C, H, W); output: (B, C\u0026#39;, H, W) output = lraspp(im_tensor) out = output[\u0026#39;out\u0026#39;].detach().numpy() plt.imshow(out[0][3]) deeplabv3 segment 1 2 deeplabv3 = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True) deeplabv3(im_tensor) (2023-07-22)\nReplace forward of predefined model How can I replace the forward method of a predefined torchvision model with my customized forward function?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import torchvision.models as models import torch def new_forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) return x # define a resnet instance resent = models.resnet18() # add new_forward function to the resnet instance as a class method bound_method = new_forward.__get__(resent, resent.__class__) setattr(resent, \u0026#39;forward\u0026#39;, bound_method) (2023-07-22)\nExtract \u0026amp; interpolate feature maps Question: I want to extract and interpolate all the features maps after each stage of convolution to the same size form a pytorch pretrained model, for example, mobilenet v3. Please give the code.\nBing chat Here is an example code that extracts and interpolates all the feature maps after each stage of convolution to the same size from a PyTorch pre-trained model, MobileNet V3.\nClick to expand the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torch import torch.nn as nn import torch.nn.functional as F from torchvision.models.mobilenetv3 import mobilenet_v3_large class FeatureExtractor(nn.Module): def __init__(self): super(FeatureExtractor, self).__init__() self.features = nn.ModuleList([ nn.Sequential( m.conv, m.bn, nn.ReLU(inplace=True), m.hs, m.se, m.conv_pw, m.bn_pw, nn.ReLU(inplace=True), m.hs_pw, ) for m in mobilenet_v3_large().features[:-1] ]) def forward(self, x): out = [] for f in self.features: x = f(x) out.append(x) return out model = FeatureExtractor() x = torch.randn(1, 3, 224, 224) out = model(x) This code creates a FeatureExtractor class that extracts all the feature maps after each stage of convolution to the same size from a PyTorch pre-trained model, MobileNet V3. The forward method takes an input tensor x and returns a list of feature maps. The out variable contains all the feature maps after each stage of convolution to the same size.\nI hope this helps! Let me know if you have any other questions.\nRun it:\nAttributeError: \u0026lsquo;Conv2dNormActivation\u0026rsquo; object has no attribute \u0026lsquo;conv\u0026rsquo;\nChatGPT To extract and interpolate all the feature maps after each stage of convolution in a PyTorch pre-trained model like MobileNetV3, you can use the intermediate_layers functionality to access intermediate outputs and the nn.Upsample module for interpolation. Here\u0026rsquo;s a step-by-step guide on how to achieve this:\nLoad the pre-trained MobileNetV3 model. Modify the model to store intermediate feature maps during forward pass. Use the nn.Upsample module to interpolate the feature maps to a common size. First, make sure you have PyTorch and torchvision installed: pip install torch torchvision\nNow, let\u0026rsquo;s create the code:\nClick to expand the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import torch import torch.nn as nn import torchvision.models as models class MobileNetV3WithIntermediateOutputs(nn.Module): def __init__(self, model_name=\u0026#39;mobilenet_v3_large\u0026#39;, pretrained=True): super(MobileNetV3WithIntermediateOutputs, self).__init__() self.model = models.__dict__[model_name](pretrained=pretrained) # Register hooks to save intermediate outputs self.intermediate_outputs = [] self.hook_handles = [] for module in self.model.modules(): handle = module.register_forward_hook(self.hook_fn) self.hook_handles.append(handle) def hook_fn(self, module, input, output): self.intermediate_outputs.append(output) def forward(self, x): self.intermediate_outputs = [] # Clear previous intermediate outputs _ = self.model(x) return self.intermediate_outputs def interpolate_to_common_size(feature_maps, target_size): _, _, H, W = feature_maps[-1].size() if (H, W) == target_size: return feature_maps interpolated_feature_maps = [] for fmap in feature_maps: interpolated_feature_maps.append(nn.functional.interpolate(fmap, size=target_size, mode=\u0026#39;bilinear\u0026#39;, align_corners=False)) return interpolated_feature_maps # Example usage: model_name = \u0026#39;mobilenet_v3_large\u0026#39; input_size = 224 target_size = (56, 56) # Specify the target size you want to interpolate the feature maps to # Load the MobileNetV3 model with intermediate outputs model = MobileNetV3WithIntermediateOutputs(model_name=model_name, pretrained=True) # Set the model to evaluation mode model.eval() # Create a random input tensor (batch size 1, 3 channels, height=input_size, width=input_size) input_tensor = torch.rand(1, 3, input_size, input_size) # Get the intermediate feature maps intermediate_feature_maps = model(input_tensor) # Interpolate the feature maps to the common size interpolated_feature_maps = interpolate_to_common_size(intermediate_feature_maps, target_size) # The \u0026#39;interpolated_feature_maps\u0026#39; now contain all the intermediate feature maps, each resized to the \u0026#39;target_size\u0026#39;. In the code above, we create a MobileNetV3WithIntermediateOutputs class that inherits from nn.Module. This class will hold the pre-trained MobileNetV3 model and includes a hook function to store intermediate feature maps during forward pass.\nThe interpolate_to_common_size function takes a list of feature maps and resizes each one to the specified target_size using bilinear interpolation. If the size of the feature maps already matches the target size, they are left unchanged.\nFinally, we load the MobileNetV3 model with intermediate outputs, perform a forward pass with a random input tensor, and then interpolate all the intermediate feature maps to the common size specified by target_size.\nPlease note that the example assumes you want to interpolate the feature maps to a target size of (56, 56). You can modify the target_size to your desired dimensions. Additionally, you can change the model_name parameter to use other versions of MobileNetV3, such as \u0026lsquo;mobilenet_v3_small\u0026rsquo;, etc.\nRun it:\n1 2 3 4 5 6 7 /home/zichen/Downloads/GNT-comments/temptest.ipynb Cell 4 in interpolate_to_common_size(feature_maps, target_size) 25 def interpolate_to_common_size(feature_maps, target_size): ---\u0026gt; 26 _, _, H, W = feature_maps[-1].size() 27 if (H, W) == target_size: 28 return feature_maps ValueError: not enough values to unpack (expected 4, got 2) It used hook, but it registered a hook for each module. But it didn\u0026rsquo;t stop before torch.flatten when forwarding, so the last feature_maps is a vector. Maybe that\u0026rsquo;s my bad in prompting.\n(2023-07-24)\nConvNeXt-tiny Print the shape of output after each layer:\n1 2 3 4 convnext = models.convnext_tiny(weights=\u0026#39;DEFAULT\u0026#39;) convnext_inspect = InspectModel(convnext) dummy_input = torch.ones(1,3,754,1008) convnext_inspect(dummy_input) Extract specified feature maps and interpolate them to specified size:\n1 2 3 convnext_features = ExtractIntrplFeatures(convnext.features, [\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;]) H_maps, W_maps = dummy_input.size()[-2:] features = convnext_features(dummy_input, (H_maps, W_maps)) # (1,192,754,1008) ","date":"2023-07-18T19:25:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_pretrained_models/","title":"memo: PyTorch | Pre-trained models"},{"content":" Source Video: Diffusion Models | Paper Explanation | Math Explained - Outlier Code: dome272/Diffusion-Models-pytorch (2023-08-02)\nIdea \u0026amp; Theory Diffusion model is a generative model, so it learns the distribution of data 𝐗. (Discrimitive model learns labels. And MLE is a strategy to determine the distribution through parameters 𝚯)\nThe essential idea is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. [1]\nForward diffusion process:\nSample noise from a normal distribution¹ and add it to an image iteratively, until the original distribution of the image has been completely destroyed, becoming the same as the noise distribution.\nThe noise level (mean, variance) of each timestep is scaled by a schedule to avoid the variance explosion along with adding more noise.\nThe image distribution should be destroyed slowly, and the noise redundency at the end stage should be reduced.\nOpenAI ³ proposed Cosine schedule in favor of the Linear schedule in DDPM².\nReverse diffusion process:\nPredict the noise of each step.\nDo not predict the full image in one-shot because that\u0026rsquo;s intractable and results in worse results¹.\nPredicting the mean of the noise distribution and predicting the noise in the image directly are equivalent, just being parameterized differently ².\nPredict noise directly, so it can be subtracted from image.\nThe variance σ² of the normal distribution followed by the noise can be fixed². But optimizing it together with the mean, the log-likehood will get improved ³.\nArchitecture DDPM used UNet like model:\nMulti-level downsample + resnet block ➔ Low-res feature maps ➔ Upsample to origianl size Concatenate RHS feature maps with the LHS feature maps of the same resolution to supplement the location information for features at each pixel. Attend some of LHS and RHS feature maps by attention blocks to fuse features further. Time embedding is added to each level of feature maps for \u0026ldquo;identifying\u0026rdquo; the consistent amount of noise to be predicted during a forward pass at a timestep. OpenAI 2nd paper(4) made improvement by modifying the model in:\nIncrease levels, reduce width; More attention blocks, more heads; BigGAN residual block when downsampling and upsampling; Adaptive Group Normalization after each resnet block: GroupNorm + affine transform ( Time embedding * GN + Label embedding) Classifier guidance is a separate classifier that helps to generate images of a certain category. Math Derivation (2024-04-17)\nVAE and diffuseion model both follows MLE strategy to find the parameter corresponding to the desired data distribution. VAE solves the dataset distribution P(𝐗) by approximating ELBO; While diffusion model solves the dataset distribution P(𝐗) by minimizing the KL Divergence. (2023-08-04)\nVAE VAE also wants to get the distribution of dataset P(𝐗), and an 𝐱 is generated by a latent variable 𝐳.\nTherefore, based on Bayes theorem, p(𝐱) = p(𝐳) p(𝐱|𝐳) / p(𝐳|𝐱), where p(𝐳) is the prior.\nAnd p(𝐳|𝐱) is intractable because in p(𝐳|𝐱) = p(𝐳)p(𝐱|𝐳) / p(𝐱), the p(𝐱) can\u0026rsquo;t be computed through ∫f(𝐱,𝐳)d𝐳 since 𝐳 is high dimensional continuous.\nBy introducing an approximated posterior q(𝐳|𝐱), log p(𝐱) = ELBO + KL-divergence.\n$log p(𝐱) = E_{q(𝐳|𝐱)} log \\frac{p(𝐱,𝐳)}{q(𝐳|𝐱)} + ∫ q(𝐳|𝐱) log \\frac{q(𝐳|𝐱)}{p(𝐳)} d𝐳$\nThe KL-divergence can be integrated analytically.\nELBO is an expectation w.r.t q(𝐳|𝐱), which can technically be estimated using Monte Carlo sampling directly.\nBut when sampled q(𝐳ⁱ|𝐱) is around 0, the variance of $log\\ q(𝐳|𝐱)$ would be high and make its gradint unstable, then cause the optimization difficult. And the function to be estimated $log(\\frac{p_θ(𝐱,𝐳)}{q_φ(𝐳|𝐱)})$ involves two approximate models containing a lot error.\nThus, it\u0026rsquo;s not feasible to approximate ELBO directly.\nTo approximate ELBO, we analyse the generative model (Decoder) p(𝐱|𝐳).\nBase on Bayes theorem, p(𝐱|𝐳) = p(𝐱) p(𝐳|𝐱)/p(𝐳).\nBy introducing the posterior approximation q(𝐳|𝐱), p(𝐱|𝐳) can derive: E(log p(𝐱|𝐳)) = ELBO + KL-divergence, i.e.,\n$E_{q(𝐳|𝐱)}[ log p(𝐱|𝐳)] = E_{q(𝐳|𝐱)}[ log(\\frac{p(𝐳|𝐱) p(𝐱)}{q(𝐳|𝐱)})] + ∫ q(𝐳|𝐱) log \\frac{q(𝐳|𝐱)}{p(𝐳)} d𝐳$\nGiven 𝐳, the likelihood p(𝐱|𝐳) is supposed to be maximized. (The probiblity that the real 𝐱 is sampled should be maximum.)\nTherefore, the parameters θ of generative model p(𝐱|𝐳) should be optimized via MLE (cross-entropy) loss.\nNow since ELBO = E(log p(𝐱|𝐳)) - KL-divergence and KL-div is known, ELBO will be obtained by just computing E(log p(𝐱|𝐳)).\nE(log p(𝐱|𝐳)) can be estimated by MC: sample a 𝐳 then compute log p(𝐱|𝐳), and repeat N times, take average.\nThe approximated E(log p(𝐱|𝐳)) should be close to the original 𝐱, so there is a MSE loss to optimize the parameters ϕ of the distribution of 𝐳.\n(2023-10-30) 𝐳\u0026rsquo;s distribution needs to be learned as well for sampling 𝐱. But MC sampling is not differentiable, so ϕ cannot be optimized through gradient descent.\nTherefore, reparameterization considers that 𝐳 comes from a differentiable determinstic transform of ε, a random noise, i.e., 𝐳 = μ + σε.\nThen, parameters (μ, σ²) of 𝐳\u0026rsquo;s distribution (Encoder) will be optimized by MSE.\nForward process The forward diffusion process is like the \u0026ldquo;Encoder\u0026rdquo; p(𝐳|𝐱) in VAE:\n$$q(𝐳|𝐱) ⇒ q(𝐱ₜ | 𝐱ₜ₋₁)$$\nThe distribution of image 𝐱ₜ at timestep t is determined by the image 𝐱ₜ₋₁ at the previous timestep, where smaller t means less noise.\nSpecifically, 𝐱ₜ follows a normal distribution with a mean of $\\sqrt{1-βₜ}𝐱ₜ₋₁$ and a variance of $\\sqrt{βₜ}𝐈$:\n$$q(𝐱ₜ | 𝐱ₜ₋₁) = N(𝐱ₜ; \\sqrt{1-βₜ} 𝐱ₜ₋₁, \\sqrt{βₜ}𝐈)$$\n𝐱ₜ is similar to 𝐱ₜ₋₁ because its mean is around 𝐱ₜ₋₁.\nAn image 𝐱 is a \u0026ldquo;vector\u0026rdquo;, and each element of it is a pixel.\nAs timestep t increase, βₜ increases and (1-βₜ) decreases, which indicates the variance gets larger and the mean value gets smaller.\nIntuitively, the value of the original pixel xₜ₋₁ is fading and more pixels become outliers resulting in a wider range of variation around the mean.\nBy introducing a notation $α = 1-βₜ$, the t-step evolution from 𝐱₀ to 𝐱ₜ can be simplied to a single expression instead of sampling t times iteratively.\nReplace (1-βₜ) with α, the distribution becomes:\n$q(𝐱ₜ | 𝐱ₜ₋₁) = N(𝐱ₜ; \\sqrt{αₜ} 𝐱ₜ₋₁, (1-αₜ)𝐈)$\nBased on the reparameterization trick, a sample from the distribution is:\n$𝐱ₜ = \\sqrt{αₜ} 𝐱ₜ₋₁ + \\sqrt{1-αₜ} ε$\nSimilarly, $𝐱ₜ₋₁ = \\sqrt{αₜ₋₁} 𝐱ₜ₋₂ + \\sqrt{1-αₜ₋₁} ε$, and plug it into 𝐱ₜ.\nThen $𝐱ₜ = \\sqrt{αₜ₋₁} ( \\sqrt{αₜ} 𝐱ₜ₋₂ + \\sqrt{1-αₜ₋₁} ε ) + \\sqrt{1-αₜ} ε$. Now, the mean becomes $\\sqrt{αₜαₜ₋₁} 𝐱ₜ₋₂$\nGiven variance = 1 - (mean/𝐱)² in the above normal distribution $N(𝐱ₜ; \\sqrt{αₜ} 𝐱ₜ₋₁, (1-αₜ)𝐈)$, and here mean = $\\sqrt{αₜαₜ₋₁} 𝐱ₜ₋₂$,\nthe standard deviation should be $\\sqrt{1 - αₜαₜ₋₁}$, then 𝐱ₜ becomes:\n$𝐱ₜ = \\sqrt{αₜαₜ₋₁} 𝐱ₜ₋₂ + \\sqrt{1 - αₜαₜ₋₁} ε$\nRepeatedly substituting intermediate states, the 𝐱ₜ can be represented with 𝐱₀ :\n$𝐱ₜ = \\sqrt{αₜαₜ₋₁ \u0026hellip; α₁} 𝐱₀ + \\sqrt{1 - αₜαₜ₋₁ \u0026hellip; α₁} ε$\nDenote the cumulative product \u0026ldquo;αₜαₜ₋₁ \u0026hellip; α₁\u0026rdquo; as $\\bar aₜ$, the 𝐱ₜ can be reached in one-shot.\n$𝐱ₜ = \\sqrt{\\bar aₜ} 𝐱₀ + \\sqrt{1 - \\bar aₜ} ε$\nThe distribution of 𝐱ₜ given 𝐱₀ is:\n$q(𝐱ₜ | 𝐱₀) = N(𝐱ₜ; \\sqrt{\\bar aₜ} 𝐱₀, (1 - \\bar aₜ)𝐈)$\nWith this expression, the deterministic forward process is ready-to-use and only the reverse process needs to be learned by a network.\nThat\u0026rsquo;s why in the formula below, they \u0026ldquo;reverse\u0026rdquo; the forward q(𝐱ₜ|𝐱ₜ₋₁) to q(𝐱ₜ₋₁|𝐱ₜ) resulting in the equation only containing \u0026ldquo;reverse process\u0026rdquo;: 𝐱ₜ₋₁|𝐱ₜ, which then can be learned by narrowing the gap between q(𝐱ₜ₋₁|𝐱ₜ) and p(𝐱ₜ₋₁|𝐱ₜ). Reverse process The reverse diffusion process is like the Decoder in VAE.\n$$p(𝐱|𝐳) ⇒ p(𝐱ₜ₋₁𝐱ₜ₋₂..𝐱₀ | 𝐱ₜ)$$\nGiven a noise image 𝐱ₜ, the distribution of less-noise image 𝐱ₜ₋₁ is\n$p(𝐱ₜ₋₁ | 𝐱ₜ) = N(𝐱ₜ₋₁; μ_θ(𝐱ₜ, t), Σ_θ(𝐱ₜ, t))$\nwhere the variance can be a fixed schedule as βₜ, so only the mean $μ_θ(𝐱ₜ, t)$ needs to be learned with a network.\nVLB VLB is the loss to be minimized. VLB gets simplied by:\nApplying Bayes rule to \u0026ldquo;reverse\u0026rdquo; the direction of the forward process, which becomes \u0026ldquo;forward denoising\u0026rdquo; steps q(𝐱ₜ₋₁ | 𝐱ₜ), because it\u0026rsquo;s from a noise image to a less-noise image;\nAdding extra conditioning on 𝐱₀ for each \u0026ldquo;forward denosing\u0026rdquo; step q(𝐱ₜ₋₁ | 𝐱ₜ, 𝐱₀).\nDerivation by step:\nDiffusion model wants a set of parameter 𝛉 letting the likelihood of the original image 𝐱₀ maximum.\n$$\\rm θ = arg max_θ\\ log\\ p_θ(𝐱₀)$$\nWith adding a minus sign, the objective turns to find the minimum:\n-log p(𝐱₀) = -ELBO - KL-divergence\n$$ \\begin{aligned} \u0026amp; -log p(𝐱₀) \\left( = -log \\frac{p(𝐱₀, 𝐳)}{p(𝐳|𝐱₀)} \\right) \\\\ \u0026amp;= -log \\frac{p(𝐱_{1:T}, 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)} \\\\ \u0026amp; \\text{(Introduce \u0026ldquo;approximate posterior\u0026rdquo; q :)} \\\\ \u0026amp;= -(log \\frac{ p(𝐱_{1:T}, 𝐱₀) }{ q(𝐱_{1:T} | 𝐱₀)} \\ + log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)}) ) \\\\ \\end{aligned} $$\nNote that $q(𝐱_{1:T} | 𝐱₀)$ represents a joint distribution of N conditional distributions 𝐱ₜ and 𝐱ₜ₋₁.\nIt is the step-by-step design that makes training a network to learn the data distribution possible. Meanwhile, the sampling process also has to be step-by-step.\nCompute expection w.r.t. $q(𝐱_{1:T} | 𝐱₀)$ for both side.\n$$ E_{q(𝐱_{1:T} | 𝐱₀)} [ -log p(𝐱₀) ] \\\\ \\ = E_{q(𝐱_{1:T} | 𝐱₀)} \\left[-log \\frac{ p(𝐱_{0:T}) }{ q(𝐱_{1:T} | 𝐱₀)}\\right] \\ + E_{q(𝐱_{1:T} | 𝐱₀)} \\left[-log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right] $$\nExpectation is equivalent to integration.\n$$ \\begin{aligned} \u0026amp; \\text{LHS:} ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * (-log p(𝐱₀)) d𝐱_{1:T} = -log p(𝐱₀) \\\\ \u0026amp; \\text{RHS:} \\ = E_{q(𝐱_{1:T} | 𝐱₀)} \\left[-log \\frac{ p(𝐱_{0:T}) }{ q(𝐱_{1:T} | 𝐱₀)}\\right] \\\\ \u0026amp; + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * \\left(-log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right) d𝐱_{1:T} \\end{aligned} $$\nSince KL-divergence is non-negative, there is:\n-log p(𝐱₀) ≤ -log p(𝐱₀) + KL-divergence =\n$$ \\begin{aligned} \u0026amp; -log p(𝐱₀) + D_{KL}( q(𝐱_{1:T} | 𝐱₀) || p(𝐱_{1:T} | 𝐱₀) ) \\\\ \u0026amp;= -log p(𝐱₀) \\ + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * \\left(log (\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right) d𝐱_{1:T} \\end{aligned} $$\nBreak apart the denominator $p(𝐱_{1:T} | 𝐱₀)$ of the argument in the KL-divergence\u0026rsquo;s logarithm based on Bayes rule:\n$$p(𝐱_{1:T} | 𝐱₀) = \\frac{p(𝐱_{1:T}, 𝐱₀)}{p(𝐱₀)} = \\frac{p(𝐱_{0:T})}{p(𝐱₀)}$$\nPlug it back to KL-divergence:\n$$ \\begin{aligned} \u0026amp;∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * \\left( log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{1:T} | 𝐱₀)})\\right) d𝐱_{1:T} \\\\ \u0026amp;= ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log (\\frac{q(𝐱_{1:T} | 𝐱₀) p(𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T} \\\\ \u0026amp;= ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * [ log(p(𝐱₀) + log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})})] d𝐱_{1:T}\\\\ \u0026amp;= ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log(p(𝐱₀) d𝐱_{1:T} \\\\ \u0026amp;\\quad + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T} \\\\ \u0026amp;= log p(𝐱₀) + ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀) * log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T} \\end{aligned} $$\nPlug this decomposed KL-divergence into the above inequality, and the incomputable log-likelihood (-log p(𝐱₀)) can be canceled, resulting in the Variational Lower Bound (VLB):\n$$-log p(𝐱₀) ≤ ∫_{𝐱_{1:T}} q(𝐱_{1:T} | 𝐱₀)\\ log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_{0:T})}) d𝐱_{1:T}$$\nThe argument of log is a ratio of the forward process and the reverse process.\nThe numerator is the distribution of $𝐱_{1:T}$ given the starting point 𝐱₀. To make the numerator and denominator have symmetric steps, the starting point of the reverse process $p(𝐱_T)$ can be separated out.\nSeparate out $p(𝐱_T)$ from the denominator by rewriting the conditional probability as a cumulative product:\n$$ p(𝐱_{0:T}) = p(𝐱_T) Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ) $$\nPlug it back into the logarithm of the VLB, and break the numerator joint distribution as a product of N-1 steps as well:\n$$ log(\\frac{q(𝐱_{1:T} | 𝐱₀)}{p(𝐱_T) Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ)}) = log \\frac{ Π_{t=1}^T q(𝐱ₜ|𝐱ₜ₋₁)}{ p(𝐱_T) Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ)} \\\\ = log \\frac{ Π_{t=1}^T q(𝐱ₜ|𝐱ₜ₋₁)}{ Π_{t=1}^T p(𝐱ₜ₋₁|𝐱ₜ)} - log p(𝐱_T) \\\\ = ∑_{t=1}^T log (\\frac{q(𝐱ₜ|𝐱ₜ₋₁)}{p(𝐱ₜ₋₁|𝐱ₜ)}) - log\\ p(𝐱_T) $$\nThis form includes every step rather than only focusing on the distribution of the all events $𝐱_{1:T}$.\n(2023-08-11) DM wants the data distribution, but it doesn\u0026rsquo;t rebuild the distribution transformation directly from Gaussian to data distribution, but approachs the corruption process step-by-step to reduce the difficulty (variance).\nSeparate the first item (first step, t=1) from the summation, so that the other terms can be conditioned on 𝐱₀, thus reducing the variance:\n$$ log \\frac{q(𝐱₁|𝐱₀)}{p(𝐱₀|𝐱₁)} + ∑_{t=2}^T log (\\frac{q(𝐱ₜ|𝐱ₜ₋₁)}{p(𝐱ₜ₋₁|𝐱ₜ)}) - log\\ p(𝐱_T) $$\nReformulate the numerator $q(𝐱ₜ|𝐱ₜ₋₁)$ based on Bayes rule:\n$$ q(𝐱ₜ|𝐱ₜ₋₁) = \\frac{q(𝐱ₜ₋₁|𝐱ₜ)q(𝐱ₜ)}{q(𝐱ₜ₋₁)} $$\nIn this form, forward adding noise $q$ and reverse denoising $p$ become the same process from 𝐱ₜ to 𝐱ₜ₋₁. Such that, in one pass, the model can both perform forward process and reverse process once.\nMake each step conditioned on 𝐱₀ to reduce the variance (uncertainty).\n$$ q(𝐱ₜ|𝐱ₜ₋₁) = \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)q(𝐱ₜ| 𝐱₀)}{q(𝐱ₜ₋₁| 𝐱₀)} $$\nAnd this distribution $q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)$ has a closed-form solution.\nHere is why the first step is separated out: If t=1, the $q(𝐱₁|𝐱₀)$ conditioned on 𝐱₀ is:\n$$ q(𝐱₁|𝐱₀) = \\frac{q(𝐱₀|𝐱₁, 𝐱₀)q(𝐱₁|𝐱₀)}{q(𝐱₀|𝐱₀)} $$\nThere is a loop of $q(𝐱₁|𝐱₀)$ if 𝐱₀ exists, and other terms $q(𝐱₀|𝐱₁, 𝐱₀)$ and $q(𝐱₀|𝐱₀)$ don\u0026rsquo;t make sense.\nPlug the newly conditioned numerator back to the fraction, and break it apart based on log rule:\n$$ ∑_{t=2}^T log \\frac{q(𝐱ₜ|𝐱ₜ₋₁)}{p(𝐱ₜ₋₁|𝐱ₜ)} \\ = ∑_{t=2}^T log \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)q(𝐱ₜ| 𝐱₀)}{p(𝐱ₜ₋₁|𝐱ₜ)q(𝐱ₜ₋₁| 𝐱₀)} \\\\ \\ = ∑_{t=2}^T log \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)}{p(𝐱ₜ₋₁|𝐱ₜ)} + ∑_{t=2}^T log \\frac{q(𝐱ₜ| 𝐱₀)}{q(𝐱ₜ₋₁| 𝐱₀)} \\\\ $$\nThe second term will be simplied to $log \\frac{q(𝐱_T| 𝐱₀)}{q(𝐱₁| 𝐱₀)}$\nThen, the variational lower bound becomes:\n$$ D_{KL}(q(𝐱_{1:T}|𝐱₀) || p(𝐱_{0:T})) = \\\\ log \\frac{q(𝐱₁|𝐱₀)}{p(𝐱₀|𝐱₁)} \\ + ∑_{t=2}^T log \\frac{q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)}{p(𝐱ₜ₋₁|𝐱ₜ)} \\ + log \\frac{q(𝐱_T| 𝐱₀)}{q(𝐱₁| 𝐱₀)} \\ - log\\ p(𝐱_T) \\\\ \\ \\\\ \\ = ∑_{t=2}^Tlog \\frac{ q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) }{ p(𝐱ₜ₋₁|𝐱ₜ)} \\ + log \\frac{q(𝐱_T| 𝐱₀)}{p(𝐱₀|𝐱₁)} \\ - log\\ p(𝐱_T) \\\\ \\ \\\\ \\ = ∑_{t=2}^T log \\frac{ q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) }{ p(𝐱ₜ₋₁|𝐱ₜ)} \\ + log \\frac{q(𝐱_T| 𝐱₀)}{p(𝐱_T)} \\ - log p(𝐱₀|𝐱₁) $$\nWrite this formula as KL-divergence, so that a concrete expression can be determined later.\nHow are those two fractions written as KL-divergence? $$ \\begin{aligned} \u0026amp; ∑_{t=2}^T D_{KL} (q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) || p(𝐱ₜ₋₁|𝐱ₜ)) \\\\ \u0026amp; + D_{KL} (q(𝐱_T| 𝐱₀) || p(𝐱_T)) \\\\ \u0026amp; - log\\ p(𝐱₀|𝐱₁) \\end{aligned} $$\nLoss function The VLB to be minimized is eventually derived as a MSE loss function between the actual noise and the predicted noise.\n$D_{KL} (q(𝐱_T| 𝐱₀) || p(𝐱_T))$ can be ignored.\n$q(𝐱_T| 𝐱₀)$ has no learnable parameters because it just adds noise following a schedule. And $p(𝐱_T)$ is the noise image sampled from normal distribution. Since $q(𝐱_T| 𝐱₀)$ is the eventual image which is supposed to follow the normal distribution, this KL-divergence should be small. Then, the loss only contains the other two terms:\n$$L = ∑_{t=2}^T D_{KL} (q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) || p(𝐱ₜ₋₁|𝐱ₜ)) - log\\ p(𝐱₀|𝐱₁)$$\n$D_{KL} (q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) || p(𝐱ₜ₋₁|𝐱ₜ))$ is the MSE between the actual noise and the predicted noise.\nFor the reverse pass, the distribution of the denoised image $p(𝐱ₜ₋₁|𝐱ₜ)$ has a parametric expression:\n$$p(𝐱ₜ₋₁|𝐱ₜ) = N(𝐱ₜ₋₁; μ_θ(𝐱ₜ,t), Σ_θ(𝐱ₜ,t)) \\\\ = N(𝐱ₜ₋₁; μ_θ(𝐱ₜ,t), β𝐈)$$\nwhere Σ is fixed as βₜ𝐈, and only the mean $μ_θ(𝐱ₜ,t)$ will be learned and represented by a network (output) through the MSE loss of noise as below.\nFor the (\u0026ldquo;reversed\u0026rdquo;) forward pass, the distribution of noise-added image $q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀)$ has a closed-form solution, which can be written as a similar expression as p(𝐱ₜ₋₁|𝐱ₜ): What\u0026rsquo;s the derivation?\n$$ q(𝐱ₜ₋₁|𝐱ₜ, 𝐱₀) = N(𝐱ₜ₋₁; \\tilde μₜ(𝐱ₜ,𝐱₀), \\tilde βₜ𝐈) \\\\ \\ \\\\ \\tilde βₜ = \\frac{1- \\bar αₜ₋₁}{1-\\bar αₜ} ⋅ βₜ \\\\ \\ \\\\ \\tilde μ(𝐱ₜ,𝐱₀) = \\frac{\\sqrt{αₜ} (1-\\bar αₜ₋₁) }{1-\\bar αₜ} 𝐱ₜ \\ + \\frac{\\sqrt{\\bar αₜ₋₁} βₜ}{1-\\bar αₜ} 𝐱₀ \\\\ \\ \\\\ \\rm Is there \\sqrt{αₜ} or \\sqrt{\\bar αₜ} ? $$\nwhere the $\\tilde βₜ$ is fixed, so only consider the $\\tilde μ(𝐱ₜ,𝐱₀)$, which can be simplified by the one-step forward process expression: $𝐱ₜ = \\sqrt{\\bar αₜ} 𝐱₀ + \\sqrt{1 - \\bar αₜ} ε$\n$$ 𝐱₀ = \\frac{𝐱ₜ - \\sqrt{1 - \\bar αₜ} ε}{\\sqrt{\\bar αₜ}} $$\nPlug 𝐱₀ into $\\tilde μ(𝐱ₜ,𝐱₀)$, then the mean of the noise-added image doesn\u0026rsquo;t depend on 𝐱₀ anymore:\n$$ \\begin{aligned} \\tilde μ(𝐱ₜ,𝐱₀) \u0026amp; = \\frac{\\sqrt{αₜ} (1-\\bar αₜ₋₁) }{1-\\bar αₜ} 𝐱ₜ \\ + \\frac{\\sqrt{\\bar αₜ₋₁} βₜ}{1-\\bar αₜ} \\ \\frac{𝐱ₜ - \\sqrt{1 - \\bar αₜ} ε}{\\sqrt{\\bar αₜ}} \\\\ \\ \\\\ \u0026amp; = ???\\ How \\ to \\ do? \\ ??? \\\\ \u0026amp; = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε) \\end{aligned} $$\nThe mean of the distribution from which the noise-added image (𝐱ₜ,𝐱₀) at timestep t get sampled out is subtracting some random noise from image 𝐱ₜ.\n𝐱ₜ is known from the forward process schedule, and the $\\tilde μ(𝐱ₜ,𝐱₀)$ is the target for the network to optimize weights to make the predicted mean $μ_θ(𝐱ₜ,t)$ same as $\\tilde μ(𝐱ₜ,𝐱₀)$.\nSince network only output μ, the KL-divergence in the loss function can be simplified in favor of using MSE:\n$$ Lₜ = \\frac{1}{2σₜ²} \\| \\tilde μ(𝐱ₜ,𝐱₀) - μ_θ(𝐱ₜ,t) \\|² $$\nThis MSE indicates that the noise-added image in the forward process and the noise-removed image in the reverse process should be as close as possible.\nSince the actual mean $\\tilde μ(𝐱ₜ,𝐱₀) = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε)$, where 𝐱ₜ is known, as it\u0026rsquo;s the input to the network. So the model is essentially estimating the actual $ε$ (random noise) every time.\nHence, the predicted mean $μ_θ(𝐱ₜ,t)$ by the model can be written in the same form as $\\tilde μ(𝐱ₜ,𝐱₀)$, where only the noise $ε_θ$ has parameters:\n$$μ_θ(𝐱ₜ,t) = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ((𝐱ₜ,t)))$$\nTherefore, the loss term becomes:\n$$ Lₜ = \\frac{1}{2σₜ²} \\| \\tilde μ(𝐱ₜ,𝐱₀) - μ_θ(𝐱ₜ,t) \\|² \\\\ \\ = \\frac{1}{2σₜ²} \\left\\| \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε) \\ - \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t)) \\right\\|² \\\\ \\ = \\frac{βₜ²}{2σₜ² αₜ (1-\\bar αₜ)} \\|ε - ε_θ(𝐱ₜ,t) \\|² $$\nDisregarding the scaling factor can bring better sampling quality and easier implementation, so the final loss for the KL-divergence is MSE between actual noise and predicted noise at time t:\n$$\\|ε - ε_θ(𝐱ₜ,t) \\|²$$\nOnce the mean $μ_θ(𝐱ₜ,t)$ has predicted out based on 𝐱ₜ and t, a \u0026ldquo;cleaner\u0026rdquo; image can be sampled from the distribution:\n$$ N(𝐱ₜ₋₁; μ_θ(𝐱ₜ,t), \\sigma_θ(𝐱ₜ,t)) = N(𝐱ₜ₋₁; \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t), βₜ𝐈) $$\nBy using reparameterization trick, this sampled image is: $$ 𝐱ₜ₋₁ = μ_θ(𝐱ₜ,t) + σε \\ = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t) + \\sqrt{βₜ}ε $$\nThe last term $log p(𝐱₀|𝐱₁)$ in the VLB is the predicted distribution for the original image 𝐱₀. Its goodness is measured by a probability that the original image $𝐱₀$ gets sampled from the estimated distribution $N(x; μ_θⁱ(𝐱₁,1), β₁)$.\nThe probability of an image should be a product of total D pixels. And the probability a pixel should be an integral over an interval [δ₋, δ₊] of the PDF curve:\n$$ p_θ(𝐱₀|𝐱₁) = ∏_{i=1}^D ∫_{δ₋(x₀ⁱ)}^{δ₊(x₀ⁱ)} N(x; μ_θⁱ(𝐱₁,1), β₁) dx $$\nwhere $x₀$ is the pixel\u0026rsquo;s ground-truth. $N(x; μ_θⁱ(𝐱₁,1), β₁)$ is the distribution to be integrated. This interval is determined based on the actual pixel value as:\n$$ δ₊(x) = \\begin{cases} ∞ \u0026amp; \\text{if x = 1} \\\\ x+\\frac{1}{255} \u0026amp; \\text{if x \u0026lt; 1} \\end{cases}, \\quad δ₋(x) = \\begin{cases} -∞ \u0026amp; \\text{if x = -1} \\\\ x-\\frac{1}{255} \u0026amp; \\text{if x \u0026gt; -1} \\end{cases} $$\nThe original pixel range [0,255] has been normalized to [-1, 1] to align with the standard normal distribution $p(x_T) \\sim N(0,1)$\nIf the actual value is 1, the integral upper bound in the distribution is ∞, and the lower bound is 1-1/255 = 0.996, the width of the interval is from 0.996 to infinity.\nIf the actual value is 0.5, the upper bound is 0.5+1/255, and the lower bound is 0.5-1/255, the width of the interval is 2/255.\npic: area of the true pixel region in two predicted distributions.\nIf the area around the actual pixel value under the predicted distribution PDF curve is large, the predicted distribution is good. Howerver, if the area around real pixel value is small, the estimated mean is wrongly located.\nHence, this probability (log-likelihood) should be maximized, and by condering the minus sign in front of it, the corresponding loss term comes.\nHowever, the authors got rid of this loss term $-log p(𝐱₀|𝐱₁)$ when training the network. And the consequense is at inference time, the final step from 𝐱₁ to 𝐱₀ doesn\u0026rsquo;t add noise, because this step wasn\u0026rsquo;t get optimized. Therefore, The difference from other sampling steps is that the predicted 𝐱₀ doesn\u0026rsquo;t plus random noise.\n$$ \\begin{aligned} \\text{t\u0026gt;1:}\\quad 𝐱_{t-1} \u0026amp;= \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t) + \\sqrt βₜ ε) \\\\ \\text{t=1:}\\quad 𝐱_{t-1} \u0026amp;= \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{βₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t)) \\end{aligned} $$\nA simple reason is that we don\u0026rsquo;t want to add noise to the final denoised clear output image 𝐱₀. Otherwise, the generated image is low-quality.\nThe complete loss function is MSE:\n$$ \\begin{aligned} \\rm L_{simple} \u0026amp;= E_{t,𝐱₀,ε} [ || ε - ε_θ(𝐱ₜ,t)||² ] \\\\ \u0026amp;= E_{t,𝐱₀,ε} [ || ε - ε_θ( \\sqrt{\\bar aₜ} 𝐱₀ + \\sqrt{1 - \\bar aₜ} ε, t) ||² ] \\end{aligned} $$\nt is sampled from a uniform distribution between 1 and t; 𝐱ₜ is the one-step forward process. Algorithms DDPM paper\nTraining a model:\n\\begin{algorithm} \\caption{Training} \\begin{algorithmic} \\REPEAT \\STATE Sample a t from U(0,T) \\STATE Select an input image 𝐱₀ from dataset \\STATE Sample a noise from N(0,𝐈) \\STATE Perform gradient descent with loss: \\\\\\\\ $||ε - ε_θ(\\sqrt{\\bar aₜ} 𝐱₀ + \\sqrt{1 - \\bar aₜ} ε, t)||²$ \\UNTIL{converge} \\end{algorithmic} \\end{algorithm} Sampling from the learned data distribution by means of reparameterization trick:\n\\begin{algorithm} \\caption{Sampling} \\begin{algorithmic} \\STATE Sample a noise image $𝐱_T \\sim N(0,𝐈)$ \\FOR{t = T:1} \\COMMENT{Remove noise step-by-step} \\IF{t=1} \\STATE ε=0 \\ELSE \\STATE ε ~ N(0,𝐈) \\ENDIF \\STATE $𝐱ₜ₋₁ = \\frac{1}{\\sqrt{αₜ}} (𝐱ₜ - \\frac{1-αₜ}{\\sqrt{1 - \\bar αₜ}} ε_θ(𝐱ₜ,t) + \\sqrt{σₜ}ε$ \\COMMENT{Reparam trick} \\ENDFOR \\RETURN 𝐱₀ \\end{algorithmic} \\end{algorithm} In this reparametrization formula change βₜ and $\\sqrt{βₜ}$ to 1-αₜ and σₜ, which are different from the above equation.\nTraining and Sampling share the common pipeline:\n𝐱 t ₜ ‖ U ε N ⋮ - e ε t _ θ ‖ ² ε _ θ ⋯ ▶ μ _ θ ( 𝐱 ₜ , t ) ⋯ ▶ 𝐱 ₜ ₋ ₁ Improvements Improvements from OpenAI\u0026rsquo;s 2021 papers.\nLearn a scale factor for interpolating the upper and lower bound to get a flexible variance:\n$$Σ_θ(xₜ,t) = exp(v\\ log βₜ +(1-v)\\ log(1- \\tilde{βₜ}))$$\nv is learned by adding an extra loss term $λ L_{VLB}$, and λ=0.001.\n$$L_{hybrid} = E_{t,𝐱₀,ε} [ || ε - ε_θ(𝐱ₜ,t)||² ] + λ L_{VLB}$$\nUse cosine noise schedule $f(t)=cos(\\frac{t/T+s}{1+s}⋅π/2)²$ in favor of linear schedule.\nReference Deep Unsupervised Learning using Nonequilibrium Thermodynamics, 2015 Denoising Diffusion Probabilistic Models, 2020 Improved Denoising Diffusion Probabilistic Models, 2021 Feb Diffusion Models Beat GANs on Image Synthesis, 2021 May ","date":"2023-07-14T21:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/d-vid-outlier/","title":"watch: Diffusion - Outlier | Explain 4 papers"},{"content":"Explicit Correspondence Matching for Generalizable Neural Radiance Fields\nCode | Arxiv\nNotes Idea Take the difference between reference image features of a 3D point as geometry prior.\n(2023-10-25) Method is like a simplified GPNR: feature differences between views measued by cosine similarity (dot product). However, GPNR is more ultimate, while MatchNeRF only computes differences for each two views and differences are taken averaged as the input feature.\n(2023-12-09) Image features alone cannot constrain geometry of the unseen scenes well. In contrast, MVSNet preset depths explicitly.\nAnd MVSNet used the variance of features to measure the depth misalignment among all feature maps, whereas MatchNeRF used differences of each 2 feature maps.\nPipeline CNN extract image feature (1/8 reso), which will be upsampled to (1/4 reso), for each reference view.\nSelect a pair of reference views and Mix their feature maps by cross-attention (GMFlow).\nProject a 3D point onto this pair of interacted feature maps.\nMeasuring the difference of feature vectors by cosine similarity (dot prodcut)\nThe feature difference indicates whether the 3D point is at surface, so that it provides a geometry prior. Dot product of two feature vectors is a scalar, which will lost much information. So they divided the total channel into many groups to do \u0026ldquo;group dot product\u0026rdquo;. And concatenate the dot product of each group as a vector 𝐳.\nAlso, for the 1/4 feature map, there is a \u0026ldquo;dot-products\u0026rdquo; vector $\\^𝐳$ for a pair of reference views.\nGiven 𝑁 reference views, there are 𝑁(𝑁-1)/2 pairs of reference views, corresponding to 𝑁(𝑁-1)/2 \u0026ldquo;difference\u0026rdquo; vectors, which will merge together by taking their element-wise average as a single 𝐳\nThis \u0026ldquo;feature difference\u0026rdquo; vector 𝐳 (geometry prior) is fed along with the 3D point\u0026rsquo;s position and viewdir into decoder (MLP and ray-transformer), which regresses the color and volume density.\nExperiments Settings are following MVSNeRF.\nDatasets:\nStage Data Contents Resolution N_views Train DTU 88 scenes 512x640 49 Test DTU 16 scenes 3 Test NeRF real 8 scenes 640x960 4 Test Blender 8 scenes 800x800 4 Device: 16G-V100 Play (2023-08-24)\nCompare with GNT The architectures of Match-NeRF and GNT are similar.\n(2023-12-09) Overview: Souce images\u0026rsquo; features are extracted, mixed and regressed to rgbσ. Match-NeRF is trained only on DTU dataset, while GNT can be trained on multiple datasets (gnt_full).\nGNT merges multiple source views via subtract attention, while Match-NeRF fuses multi-view feature maps before getting into the model.\nMatch-NeRF mixes the entire feature maps for each two reference views, and then project 3D points onto the fused feature maps to index feature vectors.\nHowever, GNT directly mixes point\u0026rsquo;s feature vectors coming from each feature maps.\nDifferent training settings:\nHyper-params GNT MatchNeRF #rays for grad-descnt 2048 1024 #source views 8~10 3 1080Ti only supports --nerf.rand_rays_train=512 for MatchNeRF.\nThe opts.batch_size will be divided evenly to each gpu (self.opts.batch_size // len(self.opts.gpu_ids)), so bs (num of images)=1 cannot be split to multiple GPUs.\nAnd if setting bs=2, each card still have to process 1024 rays selected from an image.\nTesting with 1 1080Ti:python test.py --yaml=test --name=matchnerf_3v --nerf.rand_rays_test=10240\n(2023-09-27)\nCode Details GMFlow uses 6 transformer blocks consisting of self_attn and cross_attn for fusing windows, where the 1st and odd blocks perform window shift.\nMatchNeRF fully-finetuned the pre-trained GMFlow.\nDoes Inner product of a pair of features come from GMFlow?\n(2023-10-25) I guess it\u0026rsquo;s a simplified attention only for pairs, instead of among all views.\n(2023-12-09) Inferring geometry from the difference in high-dimensional features may have been present even earlier than MVSNet.\nSelf-attn and cross-attn for two samples data1 and data2 can be done in a single transformer block of GMFlow by concating 2 samples in the batch dimension twice in different order, i.e., source=[\u0026lsquo;data1\u0026rsquo;,\u0026lsquo;data2\u0026rsquo;] and target=[\u0026lsquo;data2\u0026rsquo;,\u0026lsquo;data1\u0026rsquo;].\nSuch that self-attn is performed on source and source. And cross-attn is source and target. If fused source returned after a block, the order requires reverse again to form the new target.\nviewdir didn\u0026rsquo;t perform positional embedding (same as PixelNeRF).\nRay transformer (MHA) in decoder mixes 16-dim feature vectors. (Unexpectedly tiny)\nTake the nearest views.\nCoordinates of 3D points are projected onto the image plane of the source view 0 to do positional embedding. Code\nThe encoder is supposed to provide the overall (geometry) prior, so they emphasized in the paper:\ndo not tune encoder for per-scene fine tuning\n","date":"2023-07-14T14:59:00Z","image":"https://i.imgur.com/rQN8Eg5.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-matchnerf/","title":"read: Match-NeRF"},{"content":"Source video: 英伟达高俊: AI高质量三维内容生成（内容生成系列【一】） 北京智源大会2023 视觉与多模态大模型\nThe representation of 3D objects Implicit field is in favor of neural network, where it can be optimized by gradient. mesh can achieve real-time rendering and is handy for downstream creation, and good topology. Marching cube is not fully differentiable DMTet: A differentiable iso-surfacing is an implict field, and also a mesh.\nAn field where only the location at surface has value? a field only has one mesh? Diff-render 2D images supervise 3D generation 2D GAN advantages:\nvarious discriminator architecture powerful generator GAN3D\nThe latent codes of geometry and texture are sampled from 3D gaussian as prior 3D generator: Tri-plane consistute the implicit field. Get a mesh by DMTet from the generated geometry and texture, then render it to 2D image Use GAN to discriminate if the render is real and backward the gradient of loss Limitation: class label conditioned. One model can only can generate 1 category of objects. Text prompts generate 3D objects 2D diffusion used socre function to encourage high-fadality images score function needs a full image, but NeRF are trained batch-by-batch of rays, not a full image. Dream fusion can only render 64x64 images, so its geometry is low-quality. Coarse to fine: Use instant-ngp generate a rough geometry based on low-resolution diffusion model， then use DMTet convert the geometry to mesh; So that a highe-resolution image can be rendered, which can offer a strong gradient for fine geometry Future work a universal model can generate any category of objects. composite objects to form a scene dynamic objects ","date":"2023-07-13T18:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/%E9%AB%98%E8%B4%A8%E9%87%8F3d%E5%86%85%E5%AE%B9%E5%88%9B%E9%80%A0-%E9%AB%98%E4%BF%8A/","title":"watch: Jun Gao | ML for 3D content generation"},{"content":"Course page\nSampling Steps:\nSample a random noise image from normal distribution;\nUse the trained network to predict the noise as opposed to the meaningful object for one step;\nUse DDPM algorithm to compute noise-level scaling factors given a timestep: s1, s2, s3 = ddpm_scaling(t)\nSubtract the predicted noise from noise image and add extra noise, sample = s1 * (sample - s2 * predicted_noise) + s3 * extra_noise\nRepeat steps 2 to 4 to remove noise progressively.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 samples = torch.randn(N_imgs, 3, height, height) timesteps = 500 for i in range(timesteps, 0, -1): # reshape to match input image (N_imgs, 3, height, height) t = torch.tensor( [i/timesteps] )[:, None, None, None] # extra noise (except for the first step) z = torch.randn_like(samples) if i \u0026gt; 1 else 0 # predict noise eps = nn_model(samples, t) # remove noise and add extra noise samples = denoise_add_noise(samples, i, eps, z) Adding extra noise before removing noise in the next step avoids collapsing to the average thing of the training dataset.\nUNet UNet can output images of the same size as the input, and assign the image feature onto each pixel.\nCompress image for compact representation; Down sampling once, number of channel doubles Also UNet allows incorporating addtional information during the decoding period.\nEach time up-sampling, the sample is multiplied with context embeddings and plus time embeddings.\nTime embedding indicates timestep of the feature vector, so with that, the \u0026ldquo;time-dependent\u0026rdquo; noise level can be determined.\nContext embedding can be text description, so the UNet will be guided to generate specific output.\nf e ( h a 4 i t d u c d e h e r n n l m s a ) p s u a p m - p l e ( e e 2 C m m u o b T b p c n e i e 1 h t d m d n e d e d l x i i s n n ) g g ⨂ ⨁ s u a p m - p l e ( 1 u p c 2 h n l ) 1 2 3 4 5 # embed context and timestep cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1) temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1) up2 = self.up(cemb1 * up1 + temb1, down2) (2023-07-10)\nTraining Train the UNet to identify the noise that was applied to the image.\nUNet can segment image (classify each pixel), so here is it used to identify whether every pixel is noise or not?\nNo, it\u0026rsquo;s used to make each pixel carried with extracted or introduced features. Training steps:\nSample a random timestep (noise-level) to make noise; Add the known noise onto a random training image; UNet takes as input the noise image and predicts the applied noise as output; Loss is the difference between the true noise and the predicted noise 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for ep in range(n_epoch): for x in dataloader: # perturb data t = torch.randint(1, timesteps + 1, (x.shape[0],) ).to(device) noise = torch.randn_like(x) x_pert = perturb_input(x, t, noise) # use network to recover noise pred_noise = nn_model(x_pert, t / timesteps) # loss is MSE loss = F.mse_loss(pred_noise, noise) optim.zero_grad() loss.backward() optim.step() (2023-07-11)\nControling Use embedding vector to control the predicted noise.\nEmbedding is a vector (a set of numbers) that is a representation for something in another space.\nEmbeddings can perform arithmetic operations.\nParis : - France : + England : = London : Noise is what should be removed from the image.\nOnce the noise is fully subtracted out, what left is the generated image. By injecting context embeddings into decoder, the output feature vector of the predicted noise becomes specific for that given context.\nFor example, the noise corresponding to \u0026ldquo;A ripe avocado\u0026rdquo; is pixels that are not \u0026ldquo;A ripe avocado\u0026rdquo;, and they\u0026rsquo;ll be removed eventually.\n' a A v o r c i n i a m o p d g i e o ' ⨁ E C m o b n e U t d - e d N x i e n t g p n r o e i d l o s s And because the embedding vectors can be mixed, once the mixed noise is removed, what is left is the combination of two objects, i.e. the thing that the context embedding stands for.\nFor example, an embedding vector of \u0026ldquo;Avocado armchair\u0026rdquo; has the information of both \u0026ldquo;avocado\u0026rdquo; and \u0026ldquo;armchair\u0026rdquo;, so its context will lead the model to predict the noise that is neither \u0026ldquo;avocado\u0026rdquo; nor \u0026ldquo;armchair\u0026rdquo;.\n' a A r v m o c n c h o a a i d i o r ' C E U o m - n b N t e e e d t x d i n g s t n e o p i ⊖ i m g Context can be one-hot encoded vector for indicating categories, which will result in a specific class of images.\nSpeeding Up DDIM skips some timesteps, so it breaks Markov chain process, where each timestep is probablisticly dependent on the previous one.\nThere is a hyper-parameter step_size to decide how many timesteps are skipped.\nDDIM performs better than DDPM under 500 timesteps. The quality of images from DDIM may differs as opposed to DDPM.\nDenoising Diffusion implict model predicts a \u0026ldquo;rough sketch\u0026rdquo; of the final output, and then it refines it with the denoising process.\n","date":"2023-07-09T17:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/d-vid-dlai/","title":"watch: DM - DLAI | How Diffusion Models Work"},{"content":"Code-matlab | Code-python | paper\nIntuitive from Shakil Goal Find the true matched keypoints and identify the outliers.\nAffinity matrix Measure the difference between any two edges in two graphs.\nG r a 1 2 p h - P G r a a b c p h - Q c is the outlier. Nodes can be SIFT feature points.\n$$W_{1a,2b} = 12 - ab$$\nThe index of each element is a notation representing if i was a and if j was b.\nW 1a 1b 1c 2a 2b 2c 1a 0 1b 0 1c 0 2a 0 2b 0 2c 0 Random walk Iterations will not walk out of the area, i.e., the middle area has higher probability.\na b c 1 1 0 0 2 0 1 0 probability node 0.75 a 0.1 b 0.05 c 0.25 a 0.5 b 0.25 c The initial probility for each point is the same: 1/6:\nprobability node 0.16 a 0.16 b 0.16 c 0.16 a 0.16 b 0.16 c Associate graph (2023-07-09)\nAbstract Association graph consistitue of nodes representing correspondence between two graphs. Random walk select nodes to enforce real correspondence on the association graph. Introduction Previous work didn\u0026rsquo;t make a objective function\nIQP is NP hard, so its solution needs approximation.\nRandom walk view: rank the nodes on the association graph.\nPrevious methods\nTensor eigen decomposition Problem Each graph have a set of nodes (vertices) 𝐕, edges 𝐄, and attributes 𝐀.\nnode is local appearance of feature edge is geometrical relationship of two nodes. Matching two graphs is to find the correspondence of nodes in two graphs\nAffinity matrix 𝐖 is recording the compatibility of pairs of edges\ndiagonal is unary affinity, e.g. $𝐖_{ia;ia}$, between one correspondence and itself. non-diagonal $𝐖_{ia;jb}$ is the affinity of a pair of correspondence: $(vᵢᴾ,\\ vₐ^Q)$ and $(vⱼᴾ,\\ v_b^Q)$ W ia ib ic ja jb jc ia ib ic ja jb jc Each row is fixing one correspondence in one pair of correspondences, and changing the other correspondence.\nFor example, 1 corresponds to a is fixed, then the neighbor of 1 and a is changing:\nf i 1 1 x c h a 1 2 n g e f i a a a x c h a a b c n g e The correspondence assignment is stored in assignment matrix 𝐗;\nwhere 1 means matched correspondence, while 0 means the two nodes are not matched. x a b c i j Column vector 𝐱 is the reshape of matrix 𝐗 with length of $nᴾ×n^Q$\nx_ia x_ib x_ic x_ja x_jb x_jc Indicator vector 𝐱* is the target by maximizing the score of 𝐱ᵀ𝐖𝐱\nRandom Walks for Graph Matching Convert the affinity matrix to an association graph for random walk\nAssociation graph $G^{rw}$ is made up by nodes that represents a correspondence between $Gᴾ$ and $G^Q$.\nFor example, the correspondence $(vᵢᴾ,\\ vₐ^Q)$ is node $v_{ia}$ on the association graph.\nso the edge attributes are the elements of affinity matrix 𝐖 ;\nFor example, the edge $e_{ia; jb}$ on the association graph is the affinity $𝐖_{ia;jb}$\nRanking the nodes of association graph by random walk process\nAffinity preserving random walk Random walk process:\nA walker starts off with an arbitrary node and select as the next step one of its out-going edges based on the Markov transition kernel.\nInternet democracy:\nTotal vote that every webpage has is 1. This is realized by dividing the weight of its every out-going edge by the total number of its out-going edges.\nRow stochastic:\nOn to the association graph, its edges set 𝐖 is supposed to be normalized by 𝐃. That is each row is divided by the sum of the affinity values in that row. Then the normalized affinity matrix is row stochastic matrix 𝐏 = 𝐃⁻¹, where 𝐃 is a diagonal matrix.\nHowever, because there are outliers on the association graph, which are suppose to have small weight, the Internet democracy doesn\u0026rsquo;t suit here.\nOutliers on the association graph are the mismatched correspondence.\nFor example, the actual corespondences are $(v_1^P, v_1^Q)$, and $(v_2^P, v_2^Q)$, which are nodes $v_{11}$ and $v_{22}$ on the association graph.\nTherefore, on the association graph, the outliers are nodes other than $v_{11}$ and $v_{22}$, i.e., $v_{12},\\ v_{13},\\ v_{23},\\ v_{21}$\nG ^ 1 2 P 1 G ^ Q 2 3 A s s o 1 c 3 i a t i o n g r a p h ","date":"2023-07-06T00:00:00Z","image":"https://cv.snu.ac.kr/research/~RRWM/paper_teaser.jpg","permalink":"https://zichen34.github.io/writenotes/model/misc/b-note-rrwm/","title":"read: RRWM"},{"content":"Source video: Lesson 3: Complex conjugates and dividing complex numbers - Khan Academy\nGiven a complex number\n$$ z = \\underset{↑}{2} + \\underset{↑}{\\underline{3}} i \\\\ \\quad \\quad Re(z) \\ Im(z) $$\nwhere $2$ is a real number, $3i$ is an imaginary number. The real part of z $Re(z)$ is 2, while the imaginary part of z $Im(z)$ is 3.\nComplex conjugate The complex conjugate of z is\n$$ \\bar z \\text{ or } z^* = 2 - 3i = \\overline{2+3i} $$\nwhere the real part Re(z*) remains the same, while the imaginary part Im(z*) has the opposite sign.\n- I b 0 b m a z z * R e This act like mirror reflecting over Real axis.\nThe sum of a complex number and its complex conjugate is two times of its real part, 2Re(z):\n$$ z + \\bar z = a + \\cancel bi + a - \\cancel bi = 2a = 2 Re(z) = 2 Re(\\bar z) $$\nGraphically, vector addition\n- I b 0 b m a z z * 2 ( a z + z * ) R e The product of a complex number and its complex conjugate is a real number, and it\u0026rsquo;s equal to the magnitude of the complex number squared:\n$$ z ⋅ \\bar z = (a+bi) ⋅ (a-bi) = a² - (bi)² = a² + b² = |z|² $$\nwhich is useful in the division of comple numbers: multiply the numerator and denominator by the conjugate of the denominator to convert the division to one complex number\nFor example:\n$$ \\begin{aligned} \u0026amp; \\frac{1+2i}{4-5i} \\\\ \u0026amp; = \\frac{1+2i}{4-5i} ⋅ \\frac{4+5i}{4+5i} \\\\ \u0026amp; = \\frac{(1+2i)(4+5i)}{4²-(5i)²} \\\\ \u0026amp; = \\frac{4+5i + 8i-10}{16+25} \\\\ \u0026amp; = \\frac{-6}{41} + \\frac{13i}{41} \\end{aligned} $$\nFactoring sum of squares We can factor a difference of squares as:\n$$ x² - y² = (x-y)(x+y) $$\nBut the sum of squares x² + y² cannot be factorized if without considering imaginary unit i.\n$$ \\begin{aligned} x² + y² \u0026amp; = x² - (-y²) \\\\ \u0026amp; = x² - (- 1 y²) \\\\ \u0026amp; = x² - (i² y²) \\\\ \u0026amp; = x² - (iy)² \\\\ \u0026amp; = (x-iy) (x+iy) \\end{aligned} $$\nModulus of complex value Source page Lesson 5\nThe absolute value (modulus) of a number is the distance away from zero.\nA complex number $3-4i$ plotted on the complex plane:\n4 I m 3 | 3 - 4 i | R e The absolute value of 3-4i is the hypotenuse of the right triangle.\nBased on the Pythagorean theorem, |3-4i|= √(3²+4²) = 5. (Because distance is positive, so only take the positive square root).\npolar \u0026amp; rectangular forms Source page: Lesson 5\nA complex number can be represented in two forms:\nRead number + imaginary number: a + bi Exponential form Both of them have the same diagram, and just are described in different coordinates:\nb I m r φ a z R e The two arguments of complex number z can be (r, φ) or (a,b)\nMagnitude: r = |z| = √(a²+b²)\nArgument (Polar angle): φ = arctan(b/a)\na = r⋅cosφ\nb = r⋅sinφ\nz = a+bi = r⋅cosφ + r⋅sinφi = r(cosφ + sinφ i) = $r e^{iφ}$\nwhere $cosφ + sinφ i = e^{iφ}$ can be derived by using Taylor series.\nMultiplying complex number Source page Lesson 7\nGiven z,\n3z has the same direction as z, but three times it\u0026rsquo;s magnitude;\n-3z is in the opposite direction, and has three times modulus of z\n-3iz = $1e^{iπ} ⋅ 3e^{i⋅π/2} ⋅ re^{iφ} = 3r e^{i(π+π/2+φ)}$ which turns into the opposite direction then rotates another 90 degrees in counter-clock wise.\nOr this can be derived from the loop of 1 -\u0026gt; i -\u0026gt; -1 -\u0026gt; -i with multiplying i each time.\nz⋅(-1-i) : the angle and modulus of $-1-i$ take effect on z separately.\nThe angle of -1-i is 225 degrees (observed from the complex plane), so it will rotate the z by 225 degrees.\nThe magnitude of -1-i is √2, so z will be scaled by √2.\n(I think it as a vector addition: z⋅(-1-i) = -z-zi,, but there\u0026rsquo;re too many steps).\nOperations in polar form Source page: Lesson 8\nMultiplication Given: $$ w₁ = 3(cos(330°) + i sin(330°)) \\\\ w₂ = 2(cos(120°) + i sin(120°)) $$\nWhat is w₁⋅w₂ ?\n$3 e^{i⋅330°} ⋅ 2 e^{i⋅120°} = 6 e^{i(450°)}$\nw₁⋅w₂ can be viewed as w₁ transforming w₂, i.e., w₂ is transformed by multiplying w₁\nThe modulus of w₂ is scaled by the modulus of w₁ (2), so |w₁⋅w₂| = 3⋅2 = 6;\nThe argument (angle) of w₁ is rotated by the argument of w₁ (330°), so arg(w₁⋅w₂) = 120° + 330° = 450° = 90°\nSo w₁⋅w₂ = 6 (cos(90°) + i sin(90°)) = 6i\nDivision $$ w₁ = 8(cos( \\frac{4π}{3} ) + i sin( \\frac{4π}{3} )) \\\\ w₂ = 2(cos( \\frac{7π}{6} ) + i sin( \\frac{7π}{6} )) \\\\ $$\nWhat is $\\frac{w₁}{w₂}$ ?\n$8e^{ i\\frac{4π}{3} } / (2e^{ i\\frac{7π}{6} }) = 4 e^{i(π/6)}$\nAnother way to think about it is that w₁ is transformed by w₂:\nthe modulus of w₁ is divided by the modulus of w₂;\nthe argument of w₁ is rotated clock-wise by the argument of w₂.\nmodulus argument |w₁|=8 arg(w₁)=4π/3 |w₂|=2 arg(w₂)=7π/6 |w₁/w₂|=4 arg(w₁/w₂) = 4π/3 - 7π/6 = π/6 So $\\frac{w₁}{w₂} = 4(cos(π/6) + i sin(π/6)) = 4(√3/2 + i/2)$\nPowers of complex number Consider the complex number $z = -1 + i \\sqrt 3$. Find $z^4$ in polar and rectangular form.\nModulus of z is 2, so its modulus is times itself four times: 2⁴ = 16 Argument of z is φ = $\\rm arctan(\\sqrt 3)$=60°=120°, so its angle rotate by 4 times of its argument: φx4 = 480° = 120° The polar form is $z⁴ = 16 (cos(120°) + i sin(120°))$, so the rectangular form is $z⁴ = 16(1/2 + i √3/2) = 8 + i8√3 $\nComplex number equations Given equation x³=1, find all of the real and/or complex roots of this equation.\nFor a real number: $z= 1 = 1 + 0i = 1e^{i0°}$, its argument arg(z) can be 0, 2π, 4π, \u0026hellip;, i.e., $1 = e^{i0} = e^{i2π} = e^{i4π} = e^{i6π}$ \u0026hellip;\nPlug these exponential form into x³=1:\nx³=1 x³=$e^{i2π}$ x³=$e^{i4π}$ x³=$e^{i6π}$ cube root x=1 x=$e^{i2π/3}$ x=$e^{i4π/3}$ x=$e^{i6π/3}$ modulus 1 1 1 1 angle 0 2π/3 = 120° 4π/3 = 240° 2π root x₁ x₂ x₃ redundant a+bi 1 -1/2 + i√3/2 -1/2 - i√3/2 1 x x ₂ ₃ I m x 1 ₁ R e Fundamental theorem of Algebra Source page: Lesson 9\nThe Fundamental theorem of Algebra: a n-th degree polynomial has n roots.\n$P(x) = ax^n + bx^{n-1} + \u0026hellip; + K$\n","date":"2023-06-26T11:29:00Z","permalink":"https://zichen34.github.io/writenotes/calc/complex_conjugates/","title":"watch: Khan | Complex Number"},{"content":"Real Python Tutorial\nConstruct path Make the filename an object instead of strings.\n1 2 3 4 5 6 7 from pathlib import Path # Instantiate an object: datadir = Path(\u0026#34;./datadir\u0026#34;) # Joining Paths new_path = datadir / \u0026#34;sampled.npy\u0026#34; Iterate dir .iterdir() method iterates over all the files in the given directory\n1 2 3 4 5 from pathlib import Path from collections import Counter # count number of files of different types Counter(path.suffix for path in Path.cwd().iterdir()) Return: Counter({'.md': 2, '.txt': 4, '.pdf': 2, '.py': 1})\n.glob(\u0026quot;*.txt\u0026quot;) returns all the files with a .txt suffix in the current directory.\n1 2 \u0026gt;\u0026gt;\u0026gt; Counter(path.suffix for path in Path.cwd().glob(\u0026#34;*.p*\u0026#34;)) Counter({\u0026#39;.pdf\u0026#39;: 2, \u0026#39;.py\u0026#39;: 1}) .rglob() recursively find all the files in both the directory and its subdirectories.\n1 2 3 4 5 6 def tree(Path(directory)): print(f\u0026#34;+ {directory}\u0026#34;) for path in sorted(directory.rglob(\u0026#34;*\u0026#34;)): depth = len(path.relative_to(directory).parts) spacer = \u0026#34; \u0026#34; * depth print(f\u0026#34;{spacer}+ {path.name}\u0026#34;) glob two patterns python - How to glob two patterns with pathlib? - Stack Overflow\n1 2 3 4 5 6 7 8 9 10 11 12 13 from pathlib import Path exts = [\u0026#34;.jl\u0026#34;, \u0026#34;.jsonlines\u0026#34;] mainpath = \u0026#34;/path/to/dir\u0026#34; # Same directory files = [p for p in Path(mainpath).iterdir() if p.suffix in exts] # Recursive files = [p for p in Path(mainpath).rglob(\u0026#39;*\u0026#39;) if p.suffix in exts] # \u0026#39;files\u0026#39; will be a generator of Path objects, to unpack into strings: list(files) ","date":"2023-06-25T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_pathlib/","title":"memo: Python | pathlib"},{"content":"Overview Source video: Fourier Analysis: Overview\nMost major breakthrough started with coordinate transformation. (e.g., Theory of relativity)\nFourier transform is another coordinate transformation\nFourier derived Fourier series as a way of approximating the solutions of partial differential equations, e.g., Heat equations.\nA rectangular metal has a temperature distribution u(x,y,t) in 2 dimensions, which is governed by the heat equation:\n$$ \\frac{∂u}{∂t} = α (\\frac{∂²u}{∂x²} + \\frac{∂²u}{∂y²}) = α ∇²u $$\nFourier Transform diagonalize the laplacian operator in the heat equation. That means the laplacian operator has eigen values and eigen functions, just like other linear operators do.\nThe eigen functions are sines and cosines with a particular frequency determined by the boundary conditions and geometry of this object. The eigen values are those spatial frequencies. An arbitrary function can be approximated by a sum of sines and cosines of increasing frequencies\nf s ( i x n ) ( w t ) ‖ ✚ ✚ ⋮ These sines and cosines form an orthogonal basis for the space of possible functions. (读作“一个基”，而不是“一组基”) For example, in a vector space, the x-axis and y-axis form the basis for 2-D vector space.\nFast Fourier Transform (FFT) computes Fourier series efficiently to analyze and process data.\nFourier Series-1 Source video: Fourier Series: Part 1\n(2023-11-19) Series means a list of number, where the meaning of each term is implied. Thus in Fourier Series, each term stands for a basis, which is a trigonometric functions of a increasingly high frequence.\nWhile Fourier Transform is a function that gives an aribitrary number in the \u0026ldquo;series\u0026rdquo;.\nFourier series approximates arbitrary functions f(x) as a infinite sum of sines and cosines of increasingly high frequency.\n(2023-11-19) For 1D signal, sine (or cosine) is enough. For 2D signal, sine and cosine are both needed as they\u0026rsquo;re orthogonal. Considering f(x) is 2π-periodic:\nf ( x ) = - π π The function f(x) can be represented as a sum from k=1 to infinity of 2π-periodic sines and cosines of increasingly high frequency:\n$$ f(x) = A₀/2 + ∑ₖ₌₁^∞ (Aₖcos(kx) + Bₖsin(kx) ) $$\nwhere\nAₖ, Bₖ are Fourier coefficients, which indicate how much of each sine and cosine is needed to add up to recover f(x);\nk stands for k-th frequency.\nWhen k=0, cos0=1, sin0=0, so there\u0026rsquo;s a constant A₀\nThese sine and cosine waves are 2π periodic.\nCoefficients are inner products These Fourier coefficients can be computed as inner products (in Hilbert space) of the function f(x) with particular wave:\n$$ Aₖ = 1/π ∫_{-π}^π f(x) cos(kx) dx\\\\ Bₖ = 1/π ∫_{-π}^π f(x) sin(kx) dx $$\nMore explicitly, Aₖ, Bₖ are the inner products of f(x) with the k-th wave function normalized by the magnitude of (co)sine function\ndoubt: Isn\u0026rsquo;t the magnitude should be square-rooted? Why is it divided by the norm squared? (2023-07-11) 傅里叶系数的归一化除以了两个基向量的模长，其中一个来自“求傅里叶系数”时，f 要与 单位 基向量做点积， 第二个是在“重建”信号时，傅里叶系数要与 单位 基向量相乘以得到某一方向上的分量。 $$ Aₖ = \\frac{1}{‖cos(kx)‖²} ⋅ ⟨ f(x), cos(kx) ⟩ \\\\ \\ \\\\ Bₖ = \\frac{1}{‖sin(kx)‖²} ⋅ ⟨ f(x), sin(kx) ⟩ $$\nWhen the function f(x) is projected into the direction cos(kx), the unit length of that direction is needed, so the norm of cos(kx) is divided.\n‖cos(kx)‖² can be computed as the inner product of cos(kx) with itself, which is π.\nProjection \u0026amp; reconstruction 𝐮 ` ` ` ` ` 𝐲 , 𝐟 , , , , 𝐯 𝐱 A test vector 𝐟 can be represented separately in two sets of orthogonal basis (2D vector space, 2D coordinate system: 𝐱-𝐲 and 𝐮-𝐯):\n$$ 𝐟 = ⟨𝐟, 𝐱⟩ ⋅ \\frac{𝐱}{‖𝐱‖²}\\ + ⟨𝐟, 𝐱⟩ ⋅ \\frac{𝐲}{‖𝐲‖²} \\\\ \\ \\\\ 𝐟 = ⟨𝐟, 𝐮⟩ ⋅ \\frac{𝐮}{‖𝐮‖²}\\ + ⟨𝐟, 𝐯⟩ ⋅ \\frac{𝐯}{‖𝐯‖²} $$\nThe definition of Fourier Series is the same as projecting the vector 𝐟 on an orthogonal basis in a 2D vector space.\nThe coefficients are the projections of 𝐟 in each basis direction (i.e., inner products), and then they\u0026rsquo;re multiplied with the unit vectors, and add them up.\nThat means, in Fourier Series, the sines and cosines are the orthogonal basis functions. Coefficients indicate how much 𝐟 is in each direction.\nFourier Series-2 Source video: Fourier Series: Part 2\nArbitrary period L Generalize the period from (-π, π) to (0, L)\n$$ f(x) = \\frac{A₀}{2} \\ + ∑ₖ₌₁^∞ (Aₖcos( \\frac{2πkx}{L} ) + Bₖsin( \\frac{2πkx}{L} ) ) $$\nwhere:\ncos( 2πkx/L ) and sin( 2πkx/L ) are periodic between 0 and L.\nThat is when x=0, 2πkx/L=0, and when x=L, 2πkx/L=2πk, i.e., the wave returns to the start.\nThese cos( 2πkx/L ) and sin( 2πkx/L ) are orthogonal basis for Hilbert space of function f.\nThe Fourier coefficients change their bounds from (-π, π) to (0, L):\n$$ Aₖ = \\frac{2}{L} ∫_0^L f(x) cos(2πkx/L) dx\\\\ Bₖ = \\frac{2}{L} ∫_0^L f(x) sin(2πkx/L) dx $$\nApproximation is periodic The approximation $\\^f(x)$ is L-periodic as follows, because the f(x) is defined from 0 to L and all sines and cosines are L-periodic\n⋅ ⋅ ⋅ - L 0 L 2 L ⋅ ⋅ ⋅ The Fourier approximation $\\^f(x)$ is periodic just repeating forever the pattern of the target function $f(x)$, which is defined on an interval.\nInner Products in Hilbert Space Source video: Inner Products in Hilbert Space\nInner products of functions It\u0026rsquo;s consistent with the definition of inner products of vectors.\nGiven two functions f(x) and g(x),\nf g ₁ : ₁ : x a ₁ f g ₂ : ₂ : x ₂ f : g : : : x ₃ ₃ ₃ f : g : : : : x ₙ ₙ ₙ b f g ( ( x x x ) ) The inner product between these two functions is defined as:\n$$ ⟨f(x), g(x)⟩ = ∫ₐᵇ f(x) g(x) dx $$\nIf they\u0026rsquo;re complex-valued functions (like $e^{iwx}$), then this inner product would use the complex conjugate (same real part, opposite imaginary part) of g(x): $⟨f(x), g(x)⟩ = ∫ₐᵇ f(x) \\bar g(x) dx$\nThis inner product indicates how similar these two functions are, just like the inner product of vectors.\nBy sampling these two functions at n evenly distributed x locations, a function is represented by a data vector.\nThe interval between 2 sampling x is Δx = (b-a)/(n-1).\nAs n increases, more points are sampled and the interval is getting infinitely small, the continuous function can be recovered by a series of points.\n$$ \\underline 𝐟 = \\begin{bmatrix} f₁ \\\\ f₂ \\\\ ⋮ \\\\ fₙ \\end{bmatrix} ; \\quad \\underline 𝐠 = \\begin{bmatrix} g₁ \\\\ g₂ \\\\ ⋮ \\\\ gₙ \\end{bmatrix} $$\nCompute the inner product of these two vectors:\n$$ ⟨\\underline 𝐟, \\underline 𝐠⟩ = \\underline 𝐠ᵀ ⋅ \\underline 𝐟 \\ = ∑ₖ₌₁ⁿ fₖ gₖ $$\nwhere 𝐠ᵀ is a row vector, 𝐟 is a column vector.\nIf data is complex-valued, 𝐠ᵀ would be complex conjugate transpose 𝐠* and gₖ will be complex conjugate $\\bar gₖ$:\n$⟨\\underline 𝐟, \\underline 𝐠⟩ = \\underline 𝐠^* ⋅ \\underline 𝐟 = ∑ₖ₌₁ⁿ fₖ \\bar gₖ$\nIf the number of samples (resolution) is doubled, this sum will get twice as large, which is not correct because the \u0026ldquo;similarity\u0026rdquo; of two functions doesn\u0026rsquo;t change.\nSo the inner product should be normalized by Δx. (Expectation)\n$$ ⟨\\underline 𝐟, \\underline 𝐠⟩ Δx = ∑ₖ₌₁ⁿ f(xₖ) \\bar g(xₖ) Δx $$\nThe righ-hand term is the Riemann approximation of the continuous integral.\nIf take the limit as Δx goes to 0, the resolution becomes infinitely fine corresponding to infinitely tall vectors, and the Riemann approximation becomes continuous integral:\n$$ ⟨f(x), g(x)⟩ = ∫ₐᵇ f(x) \\bar g(x) dx $$\n再往后是先看一遍，然后默写\n(2023-06-27)\nComplex Fourier Series Source video: Complex Fourier Series\nFourier series uses a infinite sum of sines and cosines functions of increasingly high frequencies to approximate an arbitrary periodic function.\nIf the function is complex-valued, the Fourier series should be reformulated to accomondate imaginary numbers.\nBy leveraging to Euler\u0026rsquo;s formula $e^{ikx} = cos(kx) + i sin(kx)$, the complex Fourier series can be written as:\n$$ f (x) = ∑_{-∞}^{+∞} Cₖ eⁱᵏˣ \\\\ \\ = ∑_{-∞}^{+∞} (α+βi) (cos(kx) + i sin(kx)) $$\nwhere Cₖ is a complex coefficient.\nReal number is a subset of complex number, so this formula also adapt to read-valued functions, with subjecting to a constraint\nif f(x) is real-valued, $Cₖ = \\bar C₋ₖ$.\nThis can be derived by expanding and kill all the imaginary terms:\n$$ f(x) = ∑_{-∞}^{+∞} (α+βi) (cos(kx) + i sin(kx)) \\\\ \\ = ∑_{-∞}^{+∞} αcos(kx) + \\cancel{ αi sin(kx) } + \\cancel{ βicos(kx)} - βsin(kx) \\\\ \\ = ∑_{-∞}^{+∞} αcos(kx) - βsin(kx) \\\\ \\ = ∑_{-∞}^{+∞} (α-βi) (cos(-kx) + i sin(-kx) ) \\\\ $$\neⁱᵏˣ are Orthogonal basis eⁱᵏˣ with taking different integer k stands for different basis function. And these basis functions are orthogonal, which are unique directions and form an infinite-dimensional function space.\nOrthogonal means the inner products against each other are 0, but the inner product with itself is its norm squared.\nLet eⁱᵏˣ be defined as Ψₖ (eⁱᵏˣ ≔ Ψₖ), then the inner product in the Hilbert space of any two basis functions can be computed as an integral (the period is [-π,π]):\n$$ ⟨Ψⱼ, Ψₖ⟩ = ∫_{-π}^π Ψⱼ ⋅ \\bar Ψₖ dx \\\\ \\ = ∫_{-π}^π eⁱʲˣ ⋅ e⁻ⁱᵏˣ dx \\\\ \\ = ∫_{-π}^π eⁱ⁽ʲ⁻ᵏ⁾ˣ dx \\\\ \\ = \\frac{1}{i(j-k)} ⋅ [eⁱ⁽ʲ⁻ᵏ⁾ˣ ]_{-π}^π $$\nif (k-j) is non-zero, but an integar d, and because eⁱᵈ$^π$ = eⁱᵈ$^{-π}$, the inner product (definite integral) is 0.\nWhile if (k-j)=0, the inner product is 0/0:\n$$ \\frac{ [eⁱ⁽ʲ⁻ᵏ⁾ˣ]_{-π}^π }{ i(j-k) } = \\frac{0}{0} $$\nAccording to Loptial\u0026rsquo;s Rule, \u0026quot; the limit of the ratio of two functions is the same after we take the derivative of each function.\u0026quot;\nSo take the derivative of numerator and denominator w.r.t. (j-k):\n$$ \\begin{aligned} \u0026amp; lim_{(j-k)→0} \\frac{ [eⁱ⁽ʲ⁻ᵏ⁾ˣ ]_{-π}^π }{ i(j-k) } \\\\ \u0026amp;= lim_{(j-k)→0} \\frac{e^{i(j-k)π} - e^{i(j-k)(-π)} }{i(j-k)} \\\\ \u0026amp;= lim_{(j-k)→0} \\frac{ iπ e^{i(j-k)π} - (-πi) e^{i(j-k)(-π)}}{i} \\\\ \u0026amp;= 2π \\end{aligned} $$\nActually, it\u0026rsquo;s not necessary to take this limit.\nSince $⟨Ψⱼ, Ψₖ⟩ = ∫_{-π}^π eⁱ⁽ʲ⁻ᵏ⁾ˣ dx$, this integral = 2π when (j-k) = 0.\nTherefore, the inner product is\n$$ ⟨Ψⱼ, Ψₖ⟩ = \\frac{1}{i(j-k)} ⋅ eⁱ⁽ʲ⁻ᵏ⁾ˣ |_{-π}^π \\\\ \\ = \\begin{cases} 0 \u0026amp; \\text{if j $\\neq$ k} \\\\ 2π \u0026amp; \\text{if j=k} \\end{cases} $$\nThe norm squared of a basis function Ψⱼ (‖Ψⱼ‖²) is 2π.\nAnalogy to regular 2D vector space above, a function can be written as a sum of projections that the function is projected onto each basis function:\n$$ 𝐟(x) = ∑_{-∞}^{+∞} Cₖ eⁱᵏˣ = ∑_{k=-∞}^{+∞} \\frac{⟨𝐟(x), Ψₖ⟩}{2π} ⋅Ψₖ $$\ndoubt: Is Ψₖ unit length? Otherwise, Ψₖ needs normalization?, because the 1/2π is for energy measurement instead of getting the unit length of Ψₖ?\nThe intuitive application of Fourier series is that: given a function, the approximated version of it can be obtained by truncating the summation of basis direction weighted by coefficients. And the coefficients is the projection (inner product) of the function onto each basis.\n(2023-06-28)\nFourier Transform Double integral for x then for w\n还是内积（投影），x的范围是负无穷到正无穷，投影到一个特定频率 w 的基函数上，得到 Fourier coefficient，这就是 Fourier Transform\nFourier Series\u0026rsquo;s counterpart is inverse Fourier Transform\n","date":"2023-06-20T21:23:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ddse-steven/02_fourier/","title":"watch: Steven | Fourier Analysis"},{"content":"Source video: 傅立叶变换如何理解？美颜和变声都是什么原理？李永乐老师告诉你\n变换 向量可以从图形“变换”成数，也可以从数“逆变换”回图形\ny B A ( ( 1 2 , , C - 4 2 ) ( ) 4 , 2 ) x 变 换 = A B C ( ( 2 1 ( , , 3 , 4 - ) 2 2 ) ) 向量的图像与数组是一一对应的。向量相加对应数组相加。\n向量 A 被表示为 (2,4)，其含义为 A 在 x 方向上有 2 个单位长度 ex，在 y 方向上有 4 个单位长度 ey。\n因为：\n𝒆x 和 𝒆y 的长度（自己与自己的内积）都等于 1 ： 𝒆x⋅ex = ey⋅ey = 1\nex 与 ey 正交（垂直），两者内积等于 0： ex ⋅ ey = |ex|⋅|ey|⋅cos90° = 0\n所以 ex 和 ey 是一组标准正交基。 空间中的任何一个向量都可以变成标准正交基的组合。\n傅里叶级数 任何一个周期性的函数 f(t) 都可以变换为一系列正余弦函数的和。 正弦余弦函数就是一组正交基。\n它的逆变换是显而易见的，因为 sin 和 cos 都是周期性的，它们组合出来的函数也一定是周期性的。\n对于一个周期性函数，按照不同频率 w 分解出不同的正余弦函数（波）：\n如上图，在频率方向上，一个周期性波可以分解成一个频率为 w 的正弦函数，与一个频率为 2w 的分量（振幅比较大），再与一个频率为 3w 的分量（振幅比较小）相加。\n从后方观察，以时间为坐标系，只能看到信号随时间的变化； 从侧面观察，以频率为坐标系，一个信号是多个信号的累加。\n因为分量是有限个，所以频谱图上是一条条竖线，不同的高度代表振幅的大小。 不同频率的分量，除了振幅不同，起始点也不同，所以还需要一个轴表示相位：\n时域里的信号 f 变换到频域里的三个维度：频率 w，振幅 F(f)，相位 φ。\n如果已知频域中的 w, F(f), φ，就可以把它们组合起来，逆变换到时域中的信号。\n$$ f(t) = \\frac{a_0}{2} + \\sum_i a_n sin(n w t + φ_n) \\\\ \\quad = \\frac{a_0}{2} + \\sum_i a_n sin(n w t) + \\sum_i b_n cos(n w t) $$\nn 是整数，nw 是不同的频率，aₙ 是振幅，φₙ 是相位。 按照第2行的写法，标准正交基有 3 个：1， sin(nwt), cos(nwt)\n任何一个周期性的函数都可以分解成以上 3 个标准正交基的线性组合。 （sin(nwt) 与 cos(nwt) 是一对，拆不开？）\n连续傅里叶变换 非周期信号可以看作是周期无穷大的周期性信号。\n欧拉公式 用一个平面坐标系表示虚数，则横轴表示实部，纵轴表示虚部。 横轴的单位为1，纵轴的单位为虚数单位 i，则单位圆上任何一点 A 可以表示为： cosθ + i⋅sinθ。\n令 θ = wt，w 取不同的频率，对应 A 点在圆上逆时针转动不同的角度\n根据欧拉公式：\n$$ cosθ + i⋅sinθ = e^{iθ} \\\\ cos(wt) + i⋅sin(wt) = e^{iwt} $$\n所以 A 在逆时针旋转时，每时每刻都代表了两个正交基 cosθ, sinθ。 如果是 $e^{-iwt}$ 就是顺时针旋转。\nFT: 用正交基\u0026quot;摘\u0026quot;分量 一个非周期性的时域信号肯定也包含各种频率的信号分量， 又因为标准正交基与自己的内积为 1，与其他正交基的内积为 0， 所以可以用各个正交基与该信号内积，则留下的就只是在这个基向量上的分量。\n非周期性时域信号的频率分量有无穷个，所以傅里叶变换是用积分：\n$$ \\^F_T(w) = ∫_{-∞}^{+∞} f(t) e^{-jwt} dt \\\\ \\ \\\\ f(t) e^{-jwt} \\begin{cases} =0 \u0026amp; \\text{f(t) 不含 sin(wt),cos(wt)} \\\\ \\neq 0 \u0026amp; \\text{f(t) 含有 sin(wt),cos(wt)} \\end{cases} $$\n其中 $e^{-jwt}$ 是一组正交基 sin(wt) 和 cos(wt)。\n$\\^F_T(w)$ 是个复数（带 i），实部表示振幅，虚部是相位，而且是连续的 ，意味着每个频率的正（余）弦函数都有分量，\nIFT: 正交基加权和 $F_T(w)$ 是各正交基（正弦余弦函数）的”系数“，即在各正交基上的”长度“。\n从频域信号逆变换回时域信号，就把系数乘到基向量上，再加起来：\n$$ f(t) = ∫_{-∞}^{+∞} F_T(w) e^{-jwt} dw $$\n应用 声音信号是时域，横坐标是时间；图像信号是空间域，横坐标是空间位置\n低频分量是图像的轮廓，高频分量是细节\n","date":"2023-06-20T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/d-vid-ft-%E6%9D%8E%E6%B0%B8%E4%B9%90/","title":"watch: FT - 李永乐 | Fourier Transform"},{"content":"NO code | arxiv\nFound by google scholar with searching: \u0026ldquo;fourier transform neural radiance field\u0026rdquo; 3. Generalized PlenOctree Fusion silhouettes 剪影 Pipeline generalized NeRF Ψ: averaging the density σ and\nΨ are queried at every position with several view directions to predicted density and color\nPlenOctree stores the averaged density and color\nFilter out leaves having low density ($σ \u0026lt; 1e-3$) by \u0026ldquo;averaging\u0026rdquo; the queried results of 100 rendered views at the points, where the (cumulated) transmittance $T \u0026gt; 1e-3$ (close to the camera?).\n\u0026ldquo;Coarse\u0026rdquo; means sparse adjacent views of a target view, while \u0026ldquo;fine\u0026rdquo; means dense adjacent views\n4. Fourier PlenOctree \u0026ldquo;adopt PlenOctree to dynamic scenes by compressing time-variant information in the frequency domain\u0026rdquo;\n4D Scene Representation: position and time (x,y,z,t)\nhigh dimensional frequency domain: Mapping the position to Fourier Transform coefficients of the density σ(t) and each SH coefficient 𝐳(t)\n$$Φ(x,y,z) = 𝐤^σ, 𝐤^𝐳$$\nwhere $𝐤^σ ∈ ℝ^{n₁}$ (a sigma corresponds to n₁ DFT coefficients),\nand $𝐤^𝐳 ∈ ℝ^{n₂ × (l_{max} + 1)^2 × 3 }$ (a point has $(l_{max} + 1)^2 × 3$ SH coefficients and each SH coefficient has n₂ DFT coefficients.)\nReconstruct density σ at time t by summing n₁ DFT coefficients of the Fourier PlenOctree with orthogonal basis:\n$$\\rm σ(t; 𝐤^σ) = ∑ᵢ₌₀^{n₁-1} 𝐤^σᵢ ⋅ IDFTᵢ(t)$$\nwhere IDFTᵢ(t) = $\\{^{cos(\\frac{i\\pi}{T} t) \\quad \\text{if i is even}} _{sin(\\frac{(i+1)\\pi}{T} t) \\quad \\text{if i is odd}}$\nReconstruct each SH coefficient at time t by summing n₂ DFT coefficients of the Fourier PlenOctree:\n$$\\rm z_{m, l}(t; 𝐤^𝐳) = ∑ᵢ₌₀^{n₂-1} 𝐤^𝐳_{m,l,i} ⋅ IDFTᵢ(t)$$\nGeneralized PlenOctree Fusion Aggregating the PlenOctrees of different frames at T times.\nLeaves of different PlenOctrees at the same position are stacked, so the density σ and SH coefficients 𝐳(t) are stacked along the time axis.\nThe stacked vector performs DFT becoming Fourier coefficients $𝐤^σ, 𝐤^𝐳$, which are stored in Fourier PlenOctree.\nFourier PlenOctree Fine-tunning Based on PlenOctree fusion \u0026ldquo;training\u0026rdquo;, the Fourier PlenOctree can be continuous optimizing via gradient descent.\n","date":"2023-06-19T12:16:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-fourier_plenoctrees/","title":"read: Fourier PlenOctrees for Dynamic"},{"content":"(2023-11-22)\nDiscrete Convolve Source video: 【官方双语】那么……什么是卷积？- 3B1B\nTwo list of numbers are combined and form a new list: Make a table, calculate pairwise product for each cell and sum up all anti-diagonals.\n$(a * b)_n = ∑_{\\substack{i,j \\\\ i+j=n}} a_i * b_j$\nThe sum of indecies of the 2 number in each pair to be summed up is the same, just distinct combinations. Reverse the second list, slide it from left to righ, and sum the products of each pair aligned vertically.\nConvolution for image is for smoothing by averaging nearby pixels.\nThe multiplication of two polynomials is convolution, because multiply each two term and collect term with the same order.\n(2024-01-12) 父亲寿命 和 儿子的年龄做卷积，等于一起生活的岁月\n【官方双语】卷积的两种可视化|概率论中的X+Y既美妙又复杂 - 3B1B\n【官方双语】但是什么是中心极限定理？- 3B1B\n【官方双语】为什么正态分布里会有一个π？（不止是积分技巧）- 3B1B\n(2023-11-07)\nCovariance Matrix is PSD Variance is the average of the squared distance between the mean and point for a single dimension.\n方差是偏离期望的平方的期望。\nVariance is a scalar: the averaged squared mangitude of the distance vector between a certain vector and the mean vector in a data space.\n$$Var(x) = \\frac{∑(x-μ)²}{n}$$\nCovariance is the product of 2 distances betwee mean and point for the 2 dimensions respectively.\n$$Covar(x,y) = \\frac{∑(x-μ_x)(y-μ_y)}{n}$$\nCovariance is a real number and indicates positive- or negtive correlation of 2 dimensions by its sign.\nCovariance matrix is a symmatric matrix recording covariance for each pair of dimensions.\n$$ \\begin{array}{c|ccc} 𝐱 \u0026amp; d₁ \u0026amp; d₂ \u0026amp; d₃ \\\\ \\hline \\\\ d₁ \u0026amp; \\frac{∑(x_{d₁}-μ_{d₁})(x_{d₁}-μ_{d₁})}{n} \u0026amp; \\frac{∑(x_{d₁}-μ_{d₁})(x_{d₂}-μ_{d₂})}{n} \u0026amp; \\frac{∑(x_{d₁}-μ_{d₁})(x_{d₃}-μ_{d₃})}{n} \\\\ \\\\ d₂ \u0026amp; \\frac{∑(x_{d₂}-μ_{d₂})(x_{d₁}-μ_{d₁})}{n} \u0026amp; \\frac{∑(x_{d₂}-μ_{d₂})(x_{d₂}-μ_{d₂})}{n} \u0026amp; \\frac{∑(x_{d₂}-μ_{d₂})(x_{d₃}-μ_{d₃})}{n} \\\\ \\\\ d₃ \u0026amp; \\frac{∑(x_{d₃}-μ_{d₃})(x_{d₁}-μ_{d₁})}{n} \u0026amp; \\frac{∑(x_{d₃}-μ_{d₃})(x_{d₂}-μ_{d₂})}{n} \u0026amp; \\frac{∑(x_{d₃}-μ_{d₃})(x_{d₃}-μ_{d₃})}{n} \\end{array} $$\nThus, element on the main diagonal is the variance of each variable.\nCovariance matrix is always positive semi-definite, symmetric, square.\nThe eigenvalues λ and eigenvectors 𝐯 of covariance matrix 𝐄(𝐱𝐱ᵀ) can be solved from:\n𝐄(𝐱𝐱ᵀ) ⋅ 𝐯 = λ𝐯\nSince 𝐄(𝐱𝐱ᵀ) is a symmetric matrix, λs are all real numbers.\nλ = 𝐄(𝐱𝐱ᵀ) = $\\frac{∑(xᵢ-μᵢ)²}{n}$ ≥ 0,\nwhere n is the number of datapoints in the dataset. xᵢ is one of dimensions of 𝐱. xᵢ is a column vector containing n points.\nCheck the definition of positive semi-definite:\n𝐯ᵀ𝐄 𝐯 = λ𝐯ᵀ𝐯 = λ[a b c] $[^a_{^b_c}]$ = λ(a²+b²+c²) ≥ 0.\nThus, covariance matrix 𝐄 is positive semi-definite.\ntodo: This could be wrong, need to watch Dr. Strang\u0026rsquo;s lecture.\nThe following proof is from Is a sample covariance matrix always symmetric and positive definite? - SE\nCovariance matrix E(𝐱𝐱ᵀ) = (𝐱-𝛍)(𝐱-𝛍)ᵀ/n\nCheck the definition of positive semi-definite:\n𝐯ᵀ E(𝐱𝐱ᵀ) 𝐯 = $\\frac{1}{n}$ 𝐯ᵀ (𝐱-𝛍)(𝐱-𝛍)ᵀ 𝐯 = ((𝐱-𝛍)ᵀ 𝐯)²/n ≥ 0. A positive scalar.\nOther Proofs:\nProof: Positive semi-definiteness of the covariance matrix Since covariance matrix is positive semi-definite, there is global minima for all axis:\nHeatmap of a covariance matrix - Gowri Shankar\nCategorizing Quadratic Forms - Ximera, OSU - Categorizing Quadratic Forms\n(2023-11-08)\nRadius of 3D Gaussian If a dataset scattered as an ellipse following 2D Gaussian is circumscribed by a circle, to cover the most points, the radius of the circle could be $r = 3σ$, 3 times the standard deviation.\nThe standard deviation σ is the square root of the variance, which is the element on the main diagonal of the covariance matrix.\nAnd variances are the eigenvalues of the covariance matrix.\nGiven a covariance matrix $𝐂=[^{a \\ b}_{b \\ c}]$, its eigenvalue λ and eigenvector 𝐯 satisfy: 𝐂 𝐯 = λ𝐯.\nEigenvalues λs can be solved from: $\\rm det(𝐂 - λ𝐈) = 0$:\n$$ \\begin{vmatrix} a-λ \u0026amp; b \\\\ b \u0026amp; c-λ\\end{vmatrix} = 0 \\\\ (a-λ)(c-λ) - b^2 = 0 \\\\ λ^2 - (a+c)λ + ac-b^2 =0 $$\nλ₁ = $\\frac{(a+c) + \\sqrt{(a+c)^2 - 4(ac-b^2)} }{2}$, λ₂ = $\\frac{(a+c) - \\sqrt{(a+c)^2 - 4(ac-b^2)} }{2}$\n","date":"2023-06-17T22:14:00Z","permalink":"https://zichen34.github.io/writenotes/calc/gaussian/","title":"memo: Calc | Gaussian"},{"content":"FNet: Mixing Tokens with Fourier Transforms - NAACL 2022\nCode: arxiv appendix | official-jax; | keras code\n(2023-07-05) Other implementations found by asking bing chat: \u0026ldquo;Could you give its pytorch code?\u0026rdquo;\nrishikksh20/FNet-pytorch erksch/fnet-pytorch1 jaketae/fnet vgundecha/fnet-google-pytorch (2023-06-16)\nVideo Intro Source video: FNet: Mixing Tokens with Fourier Transforms (Machine Learning Research Paper Explained) - Yannic Kilcher\n(2023-07-07)\nAbstract Use linear transformations replace self-attention sublayers resulting in speeding up;\nUse unparameterized Fourier Transform replace self-attention sublayers achieving over 90% accuracy of BERT counterparts.\nFNet has a light memory footprint (because it doesn\u0026rsquo;t have parameters?)\nIntroduction Attention connects each token by the relevance weights of every other token in the input.\nAnd more complex mixing help capture the relationship between tokens.\nCan attention, the relevance-based \u0026ldquo;token-mixer\u0026rdquo;, be replaced by simpler linear transformation (𝐗𝐖⁻¹+𝐛)?\nDecent results are gived by replacing attention with twice parametrized (optimizable) matrix multiplications, which are mixing the sequence dimension and then mixing hidden dimension.\nA sequence containing 5 tokens, which are 4-dimensional.\nS e q u e n c e d i m e n t i o n h d i i d m d e e n n s i o n Use the faster, structured linear transformation FFT without parameters, yielding similar performance of dense layer mixing and good scalability.\nContributions:\nattention may not be a necessary component. Hence, seeking new mixing mechanisms is valuable. FNet uses FFT to mix token speeding up the training while losing some accuracy. Attention do help increase accuracy to some extent. FNet scales well to long inputs. Code from: rishikksh20/FNet-pytorch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import torch from torch import nn class FeedForward(nn.Module): def __init__(self, dim, hidden_dim, dropout = 0.): super().__init__() self.net = nn.Sequential( nn.Linear(dim, hidden_dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout) ) def forward(self, x): return self.net(x) class PreNorm(nn.Module): def __init__(self, dim, fn): super().__init__() self.norm = nn.LayerNorm(dim) self.fn = fn def forward(self, x, **kwargs): return self.fn(self.norm(x), **kwargs) class FNetBlock(nn.Module): def __init__(self): super().__init__() def forward(self, x): # \u0026#34;2-D fft\u0026#34;? row-wise first, then column-wise. x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real return x class FNet(nn.Module): def __init__(self, dim, depth, mlp_dim, dropout = 0.): super().__init__() self.layers = nn.ModuleList([]) for _ in range(depth): self.layers.append(nn.ModuleList([ PreNorm(dim, FNetBlock()), PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)) ])) def forward(self, x): for attn, ff in self.layers: x = attn(x) + x x = ff(x) + x return x ","date":"2023-06-16T16:23:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/b-note-fnet/","title":"read: FNet"},{"content":"Arxiv | Code-keras | Code-cvae\nAnother paper: An Introduction to Variational Autoencoders Recognition model with parameters φ, $q_φ(𝐳|𝐱)$, approximates the intractable posterior distribution;\nGenerative model with parameters θ, $p_θ(𝐳)p_θ(𝐱|𝐳)$, maps a latent variable to a \u0026ldquo;sample\u0026rdquo;.\nAbstract directed probabilistic model\nThe distribution of an i.i.d. dataset with latent variables P(𝐗,𝐙)\ncontinuous latent variables with intractable posterior distributions\nThe latent variable 𝐳 per datapoint is continuous, so its posterior distribution $p_θ(𝐳|𝐱)$ cannot be computed explicitly.\nreparameterization of the variational lower bound yields a lower bound estimator\nLower bound estimator can be optimized using SGD,\nIntroduction Variational Bayesian approach involves the optimization of an approximation to the intractable posterior.\nUse q(𝐳) to approximate $p_θ(𝐳|𝐱)$\nMean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior\n𝔼_{q(𝐳)} [ ]\nq(𝐳) is also intractable\nMethod Problem scenario Each sample is generated by two steps:\nsample a 𝐳 from prior distribution $p_{θ^*}(𝐳)$ Sample an 𝐱 from the conditional distribution $p_{θ^*}(𝐱|𝐳)$ where θ* is the true parameters.\nIntractability:\nThe integral of the marginal likelihood $p_θ(𝐱) = ∫ p_θ(𝐳) p_θ(𝐱|𝐳) d𝐳 = ∫ p_θ(𝐱,𝐳) d𝐳$ cannot be computed, because 𝐳 is a high-dimensional continuous variable.\nSuch that the true posterior density $p_θ(𝐳|𝐱) = \\frac{ p_θ(𝐱|𝐳) p_θ(𝐳) }{ p_θ(𝐱) }$ is also intractable, because the denominator marginal likelihood cannot be computed.\nThat means the EM algorithm cannot be used, because in each iteration the introduced prior distribution q(𝐳) needs to be equal to $p_θ(𝐳|𝐱)$, which however is intractable.\nAnd any reasonable mean-field variational bayesian algorithms are intractable, where the $p_θ(𝐳|𝐱)$ is used as the objective of approximating by q(𝐳), but it doesn\u0026rsquo;t work for $p_θ(𝐳|𝐱)$ that cannot be computed.\nA large dataset:\nSampling-based methods, e.g. Monte Carlo EM, would be too slow, because sampling is performed on every datapoint.\nThree meaningful problems:\nEstimating the parameter θ of the distribution via MLE or MAP can allow mimicking the data-generating process: Sample 𝐳 first then sample the x from the conditional distribution\nMLE try to find the θ that makes the probability of dataset X given Z maximum. MLE is to estimate parameter θ, while VAE is to estimate the distribution of X.\nMAP believe that the θ having the maximum posterior probability given a dataset likelihood p(X|θ) and prior probability p(θ) according to, $p(θ|X) = \\frac{p(X|θ) p(θ)}{p(X)}$, is the most possible.\nEM algorithm uses MLE to find the parameter θ of a probabilistic model involving latent variable, where assume the prior probability q(𝐳) = posterior probability $p_θ(𝐳|𝐱)$ of the latent variable, then apply MLE find optimal θ.\nApproximating the posterior probability $p_θ(𝐳|𝐱)$ given an observed value 𝐱 and a choice of parameters θ is useful for encoding data.\nThe latent variable 𝐳 can be regarded as a latent representation of a datapoint 𝐱.\nApproximating the marginal likelihood $p_θ(𝐱)$ enable to perform those inference tasks where the prior p(𝐱) is required, e.g. image denoising and inpainting.\nThe above three goals can be tackled by a recognition model $q_φ(𝐳|𝐱)$, which is an approximation to the intractable posterior probability $p_θ(𝐳|𝐱)$\nThe recognition model $q_φ(𝐳|𝐱)$ is a probabilistic encoder, which produces a distribution over all possible values of 𝐳 with given a datapoint 𝐱.\n(Each datapoint 𝐱 corresponds to a distribution $q_φ(𝐳|𝐱)$ with some parameters (e.g., μ,σ).)\np ( 𝐳 D | i ( 𝐱 s b f ) t y r r o i g m b i u v E t e n i n c o o n a d e o 𝐱 r f ) 𝐳 𝐳 p ( 𝐱 | D 𝐳 i ( ) s b f t y r r o i g m b i u v D t e e i n c o o n a d e o 𝐳 r f ) 𝐱 𝐱 A 𝐳 produces a conditional distribution p(𝐱|𝐳), from which a datapoint 𝐱 is generated. So the generative model $p_θ(𝐱|𝐳)$ is called probabilistic decoder.\n(2023-07-09)\nSo a training step is: sampling a 𝐳 from distribution p(𝐳|𝐱), then with this 𝐳, find the parameter θ that makes the probability p(𝐱|𝐳) largest based on MLE. That\u0026rsquo;s why the loss funciton has a cross-entropy term that measures the likelihood of output 𝐱\u0026rsquo;, i.e., log p(𝐱|𝐳).\nAccording to KL-divergence $-q(𝐳) log \\frac{p(𝐳|𝐱)}{q(𝐳)}$, the approximated posterior q(𝐳) is supposed to equal the real posterior p(𝐳|𝐱), which, however, is intractable. So the authors use network and reparameterization to estimate the parameters (mean, variance) of the posterior.\n(I think) 𝐳 sampled from posterior can be one value or multiple times and doing average.\n2.2 The Variational bound The marginal likelihood of the dataset with N datapoints is a sum over the marginal likelihoods of individual datapoints:\n$log\\ p_θ(𝐱⁽¹⁾, \u0026hellip;, 𝐱⁽ᴺ⁾ ) = ∑ᵢ₌₁ᴺ log\\ p_θ(𝐱⁽ⁱ⁾)$\nRewrite it by introducing posterior approximation $q_φ(𝐳|𝐱⁽ⁱ⁾)$ to make up KL divergence:\n$$ \\begin{aligned} \u0026amp;log\\ p_θ(𝐱⁽ⁱ⁾) \\\\ \u0026amp;= log\\ (\\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{p_θ(𝐳|𝐱⁽ⁱ⁾)} ) \\\\ \u0026amp;= log\\ (p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)) - log\\ p_θ(𝐳|𝐱⁽ⁱ⁾) \\\\ \u0026amp;= log\\ (p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)) - log\\ p_θ(𝐳|𝐱⁽ⁱ⁾) \\\\ \u0026amp;\\quad + log\\ q_φ(𝐳|𝐱⁽ⁱ⁾) - log\\ q_φ(𝐳|𝐱⁽ⁱ⁾) \\\\ \u0026amp;= log\\ \\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} - log\\ \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)}\\\\ \\end{aligned} $$\nCompute expectations w.r.t. the approximate posterior $q_φ(𝐳|𝐱)$ for both side:\n$$ ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log\\ p_θ(𝐱⁽ⁱ⁾) d𝐳 = \\\\ ∫q_φ(𝐳|𝐱⁽ⁱ⁾) \\left[log\\ \\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} - log\\ \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} \\right] d𝐳 $$\nLeft-hand side remains marginal likelihood $log\\ p_θ(𝐱⁽ⁱ⁾)$ (i.e. $𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [ log\\ p_θ(𝐱⁽ⁱ⁾) ]$) because p(x) has nothing to do with z.\nWhile right-hand side is the lower bound plus KL divergence: $$ \\begin{aligned} \u0026amp; log\\ p_θ(𝐱⁽ⁱ⁾) = \\\\ \u0026amp; ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log\\ \\frac{p_θ(𝐱⁽ⁱ⁾|𝐳) p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} d𝐳 \\\\ \u0026amp;\\quad - ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log\\ \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} d𝐳 \\\\ \u0026amp; = ℒ(θ,φ; 𝐱⁽ⁱ⁾) + D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳|𝐱⁽ⁱ⁾ ) \\\\ \u0026amp; \\\\ \u0026amp; = 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾,𝐳) - log\\ q_φ(𝐳|𝐱⁽ⁱ⁾)] \\\\ \u0026amp;\\quad + D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳|𝐱⁽ⁱ⁾ ) \\quad (2) \\end{aligned} $$\nIn another way, the lower bound can also be written as eq.(3): 𝓛$(θ,φ; 𝐱⁽ⁱ⁾) = -D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳) ) + 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾|𝐳)]$\nwhose derivation starts from the conditional probability $p_θ(𝐱⁽ⁱ⁾|𝐳)$:\nThe likelihood of the i-th datapoint: $$ \\begin{aligned} \u0026amp; log(p_θ(𝐱⁽ⁱ⁾|𝐳)) \\\\ \u0026amp;= log( \\frac{p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾))}{p_θ(𝐳)} )\\\\ \u0026amp;= log( \\frac{ p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾)* q_φ(𝐳|𝐱⁽ⁱ⁾) ) }{ p_θ(𝐳)*q_φ(𝐳|𝐱⁽ⁱ⁾) } ) \\\\ \u0026amp;= log( \\frac{ p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} ) - log( \\frac{p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} ) \\end{aligned} $$\nCompute the expectation w.r.t. approximate posterior $q_φ(𝐳|𝐱⁽ⁱ⁾)$:\n$$ \\begin{aligned} \u0026amp; ∫q_φ(𝐳|𝐱⁽ⁱ⁾)\\ log(p_θ(𝐱⁽ⁱ⁾|𝐳)) d𝐳 = \\\\ \u0026amp; ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log( \\frac{ p_θ(𝐳|𝐱⁽ⁱ⁾)p_θ(𝐱⁽ⁱ⁾)}{q_φ(𝐳|𝐱⁽ⁱ⁾)})d𝐳\\\\ \u0026amp; \\quad - ∫q_φ(𝐳|𝐱⁽ⁱ⁾) log( \\frac{p_θ(𝐳)}{q_φ(𝐳|𝐱⁽ⁱ⁾)} ) d𝐳 \\\\ \u0026amp; \\\\ \u0026amp; 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾|𝐳))] = \\\\ \u0026amp; \\quad ℒ(θ,φ; 𝐱⁽ⁱ⁾) + D_{KL} ( q_φ(𝐳|𝐱⁽ⁱ⁾ || p_θ(𝐳)) \\end{aligned} $$\nHowever, using Monte Carlo to estimate gradient of 𝓛 (an expectation) will bring high variance.\n2.3 SGVB estimator and AEVB algo A practical estimator of the lower bound 𝓛 of the likelihood and its derivatives w.r.t. the parameters.\n(With some mild conditions for a selected approximate posterior distribution $q_φ(𝐳|𝐱)$,)\nConsider a variable $\\~𝐳$ that comes from $\\~𝐳 = g_φ$(𝛆,𝐱) with 𝛆 ~ p(𝛆), follows the posterior distribution $\\~𝐳 \\sim q_φ(𝐳|𝐱)$ (or not conditioned distribution $q_φ(𝐳)$). And $g_φ$(𝛆,𝐱) is a deterministic differentiable transformation of an (auxiliary) noise variable 𝛆.\nSince $\\~𝐳$ is a transformation of 𝛆, $\\~𝐳$ follows the distribution p(𝛆) as well.\nMonte Carlo estimation\nTherefore, using Monte Carlo (i.e., averaging the L sampled values) to approximate an expectation of some function $f(𝐳)$ w.r.t. the posterior approximation $q_φ(𝐳|𝐱⁽ⁱ⁾)$ becomes:\n$$ 𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [f(𝐳)] = 𝔼_{p(ε)} [f( g_φ(ε, 𝐱⁽ⁱ⁾) )] \\\\ ≃ \\frac{1}{L} ∑ₗ₌₁ᴸ f( g_φ(ε⁽ˡ⁾, 𝐱⁽ⁱ⁾) ), $$ where 𝛆⁽ˡ⁾~ p(𝛆).\nApproximate lower bound with eq. (2)\nThe version A of SGVB estimator comes from eq. (2), the lower bound 𝓛 should be finally equal to that ELBO expectation, 𝓛ᴬ (𝛉,𝛗,𝐱⁽ⁱ⁾) ≃ 𝓛 (𝛉,𝛗,𝐱⁽ⁱ⁾) i.e., the KL divergence=0.\nAnd that expectation can be approximated via Monte Carlo (sampling), so the lower bound is approximated as:\n$$ \\~\\mathcal L^A (θ,φ,𝐱⁽ⁱ⁾) = \\\\ \\frac{1}{L} ∑ₗ₌₁ᴸ \\left[ log\\ p_θ(𝐱⁽ⁱ⁾, 𝐳⁽ⁱ\u0026rsquo;ˡ⁾) - log\\ q_φ(𝐳⁽ⁱ\u0026rsquo;ˡ⁾| 𝐱⁽ⁱ⁾) \\right] \\\\ $$\nwhere $𝐳⁽ⁱ\u0026rsquo;ˡ⁾= g_φ(ε⁽ⁱ\u0026rsquo;ˡ⁾, 𝐱⁽ⁱ⁾))$ and 𝛆⁽ˡ⁾~ p(𝛆)\nApproximate lower bound with eq. (3)\nSince the KL-divergence $D_{KL}( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳) )$ of eq. (3) can be integrated analytically, it can be substituted into eq. (3), then only that expectation (\u0026ldquo;expected reconstruction error\u0026rdquo; $𝔼_{q_φ(𝐳|𝐱⁽ⁱ⁾)} [log\\ p_θ(𝐱⁽ⁱ⁾|𝐳))]$) is approximated, so the second version of lower bound approximation: 𝓛ᴮ (𝛉,𝛗,𝐱⁽ⁱ⁾) ≃ 𝓛 (𝛉,𝛗,𝐱⁽ⁱ⁾) is more relatively accurate than the version A.\n$$ \\tilde{\\mathcal L^B} = -D_{KL}( q_φ(𝐳|𝐱⁽ⁱ⁾) || p_θ(𝐳) ) \\\\ \\qquad + \\frac{1}{L} ∑ₗ₌₁ᴸ log\\ p_θ(𝐱⁽ⁱ⁾ | 𝐳⁽ⁱ\u0026rsquo;ˡ⁾) $$\nwhere $𝐳⁽ⁱ\u0026rsquo;ˡ⁾= g_φ(ε⁽ⁱ\u0026rsquo;ˡ⁾, 𝐱⁽ⁱ⁾))$ and 𝛆⁽ˡ⁾~ p(𝛆).\nThe KL-divergence there can be interpreted as regularizer, and the optimization objective is enlarging the \u0026ldquo;expected negative reconstruction error\u0026rdquo;, i.e., recover original 𝐱 from code 𝐳.\nTrain with minibatches\nGiven a dataset 𝐗 with N datapoints, each time M datapoints are drawn as a minibatch, then the marginal likelihood lower bound of the full dataset batch-by-batch is estimated as:\n$$ \\mathcal L(\\pmb{θ,φ},𝐱) \\simeq \\mathcal L^M (\\pmb{θ,φ},𝐗ᴹ) = \\frac{N}{M} ∑ᵢ₌₁ᴹ \\tilde{\\mathcal L} (\\pmb{θ,φ},𝐱⁽ⁱ⁾) $$\n2.4 Reparameterization trick Previously, 𝐳 is sampled directly, then input to model $p_θ(𝐱|𝐳)$, but the parameter φ of the distribution of 𝐳 is not able to be optimized via gradient descent, since Monte Carlo isn\u0026rsquo;t differentiable.\np ( z z ) p r i o r , q ᵩ ( z ) s a z M m . p z C l . i n g z - θ - x Instead of sampling the 𝐳 directly, but the 𝐳 is derived from the sampled 𝛆, based on the deterministic differentiable transformation: 𝐳 = gᵩ(𝛆, 𝐱).\np ( 𝛆 ) 𝛆 d i s t r i b u t i o s n a M m . p 𝛆 𝛆 C l . i n g z = μ + σ 𝛆 z - θ - x (The non-differentiable operation (M.C.) is put ahead of the leave nodes on the computational graph.)\nThen the parameters (e.g. μ,σ²) of 𝐳\u0026rsquo;s distribution can be learned by a network with parameters φ.\nx - φ l μ o g σ ² z p ( = 𝛆 ) 𝛆 μ d + i s σ s t m r 𝛆 a i p b l u i t n i g o n L z - 𝛆 θ - x R e c c l o t o n i s s o s t n r u - Such that the Monte Carlo estimate of the expectation is differentiable w.r.t. φ.\nThe reasoning is as follows:\nFor infinitesimals, there is $$q_φ(𝐳|𝐱)∏ᵢdzᵢ = p(\\pmb ε)∏ᵢdεᵢ$$; where zᵢ is one of dimensions, d𝐳 = ∏ᵢ dzᵢ\nTherefore, $∫ q_φ(𝐳|𝐱) f(𝐳) d𝐳$ = ∫p(𝛆) f(𝐳) d𝛆 = ∫ p(𝛆) $f(g_φ$(𝛆,𝐱)) d𝛆\nUse Monte Carlo sampling approximation: $∫ q_φ(𝐳|𝐱) f(𝐳) d𝐳$ ≃ 1/L ∑ₗ₌₁ᴸ f( gᵩ(𝛆⁽ˡ⁾, 𝐱)), where 𝛆⁽ˡ⁾ ~ p(𝛆)\nThe transformation (φ) from 𝛆 to 𝐳 can be learned by back-propagation and gradient descent from the reconstruction loss, since the transformation differentiable.\nSo the parameters (e.g. μ,σ) of the approximate posterior distribution $q_φ(𝐳|𝐱)$ of 𝐳 can be optimized through φ, and the parameter θ of the generative model is trained jointly.\nThis\u0026rsquo;s just a trick, the essence of the algorithm is still the coordinate ascent:\nSample a 𝐳 by sampling an 𝛆:\n1 2 3 4 def reparameterize(mu, logvar): # logvar is log𝛔² std = torch.exp(0.5 * logvar) # (e^{log𝛔²})^½ eps = torch.randn_like(std) return mu + eps * std Use 𝐳 to generate 𝐱 with model $p_θ(𝐱|𝐳)$,\n1 2 3 def decode(z): result = nn.linear(latent_dim, hidden_dims)(z) return result then use 𝐱 to produce 𝐳 with model $q_φ(𝐳|𝐱)$\n1 2 3 4 5 def encode(input): result = nn.linear(input_dim, hidden_dim)(input) mu = nn.Linear(hidden_dim, latent_dim)(result) logvar = nn.Linear(hidden_dim, latent_dim)(result) return [mu, logvar] With using reparameterization trick, both the two models can be optimized by gradient descent.\n(2024-04-16)\nReparameterization trick is a sampling method. It\u0026rsquo;s similar to inverse transform sampling: using a known distribution (uniform) to sample an unknown distribution.\nReparameterization trick makes the sampling from an unknown distribution differentiable, and enables the parameters of the distribution to be optimized.\nLoss func Loss function contains two parts: KL divergence and reconstruction error.\nKL divergence (i.e., cross entropy) can be computed with given the prior and assumed posterior of 𝐳.\nFor example, let the prior $\\rm p_θ(𝐳) = N(𝐳,𝟎,𝐈)$ and assume the posterior qᵩ(𝐳|𝐱) = N(𝐳; 𝛍, 𝛔²𝐈), then cross entropy can be derived by plugging them into Gaussian expression.\nThe reconstruction error is the log likelihood of the input datapoint log p(𝐱|𝐳).\nFor a datapoint obeying multivariate Bernoulli (Yes or No), the log likelihood of 𝐱 is\n$$ log p(𝐱|𝐳) = log ∏ᵢ₌₁ᴰ yᵢˣⁱ (1-yᵢ)¹⁻ˣⁱ \\\\ \\ = ∑ᵢ₌₁ᴰ [ xᵢlog yᵢ + (1-xᵢ)log (1-yᵢ) ] $$\nwhere yᵢ should be like a probability (need to do sigmoid activation). So in this case, this loss term is a cross entropy.\nFor a datapoint following multivariate Gaussian distribution N(𝐱; 𝛍, 𝛔²𝐈), refer to Su, Jianlin\n$$ \\begin{aligned} \u0026amp;p(𝐱|𝐳) = \\frac{1}{∏ᵢ₌₁ᴰ \\sqrt{2πσᵢ²(𝐳)}} \\rm exp(-\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖²) \\\\ \\ \\\\ \u0026amp;log\\ p(𝐱|𝐳) \\\\ \u0026amp;= log \\frac{1}{∏ᵢ₌₁ᴰ \\sqrt{2πσᵢ²(𝐳)}} \\ -\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\ \u0026amp;= -∑ᵢ₌₁ᴰ log \\sqrt{2πσᵢ²(𝐳)} -\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\ \u0026amp;= -∑ᵢ₌₁ᴰ [\\frac{1}{2} (log2π + logσᵢ²(𝐳))] - \\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\ \u0026amp;= -\\frac{D}{2} log2π -∑ᵢ₌₁ᴰlogσᵢ²(𝐳) -\\frac{1}{2} ‖\\frac{x-\\pmb μ(𝐳)}{\\pmb σ(𝐳)}‖² \\\\ \\end{aligned} $$\nNormally, the variance 𝛔² will be fixed, so this loss term is only related with mean 𝛍(𝐳):\n-log p(𝐱|𝐳) ~ $\\frac{1}{2\\pmb σ²}\\| 𝐱-\\pmb μ(𝐳) \\|²$\nTherefore, this loss term is MSE.\n","date":"2023-06-07T10:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/b-note-vae/","title":"read: VAE"},{"content":"Code | Arxiv\nNotes Abstract (2023-05-24)\nModel the radiance field as a 4D tensor.\nOptimize a 4D tensor? Optimize a 4D matrix? MLP (network) is a 2D matrix? 𝐗 @ 𝐖ᵀ = 𝐘; Nonlinear regession cannot be solved by linear algebra? Gauss-Newton algorithm (2023-05-29)\nTerminology:\n\u0026ldquo;4D scene tensor\u0026rdquo;: (x, y, z, feature). Each voxel is allocated with a sigma and appearance feature. (Alike a feature field?).\n(2023-10-19) Features on each point derived from the coefficients of each vector in each basis. \u0026ldquo;factorize\u0026rdquo;: Encode the data into \u0026ldquo;coordinates\u0026rdquo; (coefficients) in the principle components;\n\u0026ldquo;multiple compact\u0026rdquo;: Principle components are orthogonal between each other;\n\u0026ldquo;low-rank\u0026rdquo;: The number of principle components is small. Rank=1 is a vector, rank\u0026gt;1 will depend on the rank of the matrix;\n\u0026ldquo;component\u0026rdquo;: The most \u0026ldquo;iconic sample\u0026rdquo;, which can be reshaped (reconstructed) to a data that has the original dimensionality,\nIn other words, it\u0026rsquo;s a \u0026ldquo;space\u0026rdquo; constituted by several directions; e.g., an image space has X and Y 2 directions.\nIn TensoRF, a vec + a mat is a component.\nFor a scene placed in a cube, one of components is an equal-sized cube, and the 3 directions are the X,Y,Z axes.\n(2023-10-19) Because the scene is the reconstruction goal, it served as the template of each basis. If the scene isn\u0026rsquo;t be reconstructed directly, for example, reconstring each ray (ro,rd,l,s), what\u0026rsquo;s the basis then?\nA 1D signal\u0026rsquo;s basis is sin+cos.\n(2023-10-19) Voxels are discrete samples from a continuous scene function. Coefficients on vectors of a basis are inner product between the scene function and each basis. Given some sample points (voxels), their coefficients are the coordinates of their projection onto the basis.\n\u0026ldquo;mode\u0026rdquo;: A principle component, a coordinate system, a space, a pattern;\nIntroduction \u0026ldquo;CAN-DECOMP/PARA-FAC\u0026rdquo;: Every direction in a component is a 1-dimensional vector.\n\u0026ldquo;Vector-matrix decomposition\u0026rdquo;: two directions out of 3 are jointly represented by a \u0026ldquo;frame\u0026rdquo; (plane),\nSo the obtained factors for 1 component are 1 vector and 1 matrix.\nVM decomposition spends more computation for more expressive components.\nThe \u0026ldquo;similarity\u0026rdquo; is computed \u0026ldquo;frame-by-frame\u0026rdquo;, so it needs more calculation. And the original structure is more kept than 2 dimensions are analyzed individually, so the components could be more representitive and less components are needed.\nBetter space complexity: O(n) with CP or O(n²) with VM,\nComparing with optimizing each voxel directly, which is O(n³), optimizing factors takes less memory.\nGradient descent\nThey\u0026rsquo;re not encoding the radiance field into factors because the feature grid/tensor is unknown. They decompose the feature grid to simplify the optimization, which then turns to optimizing factors.\nImplementation A scene is a radiance field.\nRadiance field (4D tensor) = Volume density grid (3D tensor: X,Y,Z) + Appearance feature grid (4D tensor: X,Y,Z,feat)\nRadiance field is a tensor of (X,Y,Z,σ+feature), where volume density σ is 1D, appearance feature is 27D;\n1D Volume density (feature) is decomposed to 3 vectors (CP) or 3 vector-matrix combo (VM) for each component.\n27D Appearance feature is amortized into 3 vector-matrix combos for 16 components.\nThese components are coefficients to fuse the data \u0026ldquo;basis vectors\u0026rdquo; in different ways, which is acting like a network.\n1D density and 27D features are optimized jointly with coefficients.\n(2023-10-28) Features and coefficients are optimized simultaneously. R a d i a n c e f i e l d = V o l u m e d e n s i t y ✚ S c S ( o ( m a f p p 2 e o p 7 a n D t e f n e t a ✶ t u r e 3 R s D G ✶ B c c o o = m m p p o o n n e e n n t t ) ⟩ where S is two-layer MLP: 150 → 128 → 128 → 3\n(2023-10-28) I guess the authors came up with tensor decomposition because they realized that Positional Encoding is Fourier decomposition, And NeRF then used MLP to learn the coefficients for the decomposed \u0026ldquo;Fourier basis\u0026rdquo;: sin and cos. Interpolate vector \u0026amp; matrix Instead of performing tri-linear interpolation at a point by computing 8 corners, a point at arbitrary position is computed by interpolating th vector and matrix.\nt r i - l i n e a r V . S . l i n e a r × b i - l i n e a r Code Notes Steps overview Dataset includes all_rgbs and corresponding ro, and normalized rd under world space all_rays, (N_imgs×H×W, 3)\nSplit the bounding box $[^{[-1.5, -1.5,\\ -1.5]}_{[1.5,\\quad 1.5,\\quad 1.5]}]$ into a voxel grid of the given resolution [128,128,128] determined by the number of voxels args.N_voxel_init\n1 reso_cur = N_to_reso(args.N_voxel_init, aabb) - 1 . 5 , - 1 . 5 , - 1 . 5 V ) o x ╰ e ╌ l ╌ ╌ s ╌ i ╌ z ╌ e ╌ ╌ 3 3 ╌ / z ╌ 1 ╌ 2 ╌ 8 ╌ , y ╌ ╌ s ╯ t e p S i z ( x e 1 . i 5 s , a 1 . h 5 a , l f 1 . 5 ) Sampling number nSamples = voxel grid diagnoal ∛(128²+128²+128²) / stepSize\nLearnable Parameters init_svd_volume() creates basis vectors and matrices. A vector and a matrix are orthogonal because they\u0026rsquo;re a side and a plane of a cube.\nParameters to be optimized: 3 Vectors and 3 Matrices\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def init_one_svd(self, n_component, gridSize, scale, device): plane_coef, line_coef = [], [] # 3 kinds of vec-mat combo, each combo has 16 components; for i in range(len(self.vecMode)): # vecMode: [2, 1, 0] vec_id = self.vecMode[i] # matMode: [(0,1),(0,2),(1,2)] mat_id_0, mat_id_1 = self.matMode[i] # (1, 16, 128, 128) plane_coef.append(torch.nn.Parameter( scale * torch.randn((1, n_component[i], gridSize[mat_id_1], gridSize[mat_id_0])))) # (1, 16, 128, 1) line_coef.append(torch.nn.Parameter( scale * torch.randn((1, n_component[i], gridSize[vec_id], 1)))) return torch.nn.ParameterList(plane_coef).to(device), torch.nn.ParameterList(line_coef).to(device) Given a 3D tensor, it can be decomposed as Vector-Matrix combo in three directions:\n= ✚ ✚ Each direction has 16 components. In other words, an observation of the cube from a direction can be reconstructed by summing those components up.\ndoubt: These 3 directions are orthogonal because the cube is viewed from distinct directions, but how are those 16 channels guaranteed to be orthogonal?\n(2023-10-17) Based on the theory of PCA? (2023-10-28) Is it possible that 16 components are parallel instead of orthogonal? They\u0026rsquo;re summed directly, similar to an FC layer with 16 neurons representing 16 ways of combining features. doubt: Are those components parallel to each other? Do they have different importance or priority?\n(2023-06-22) I guess no. They\u0026rsquo;re just added together simply. A scene is decomposed to a set of components, then a scene can be reconstructed using a set of coefficients of those components.\nSepcificlly, each voxel is a summation of the products for corresponding projections on vector and matrix in 3 directions and 16 components.\nBased on those vector-matrix components, with the help of interpolation, the value at any location can be obtained.\nFiltering rays Filter the effective rays based on the ratio (deviation) betweem the direction of rays and the direction of bounding box corners.\nMask those rays inside bbox by compareing the ratio of the rd to the direction of the two bbox corners:\n1 2 3 4 5 6 7 8 9 10 11 12 13 if bbox_only: # Avoid denominator of 0, normalized rd, (chunk, 3) vec = torch.where(rays_d == 0, torch.full_like(rays_d, 1e-6), rays_d) # ratio of the direction of bbox corner xyz_min to testing ray rate_a = (self.aabb[1] - rays_o) / vec # (chunk, 3) # ratio of the direction of bbox corner xyz_max to testing rd rate_b = (self.aabb[0] - rays_o) / vec t_min = torch.minimum(rate_a, rate_b).amax(-1) # [chunk] t_max = torch.maximum(rate_a, rate_b).amin(-1) # [chunk] # rays located inside the bbox mask_inbbox = t_max \u0026gt; t_min An effective ray should end up inside the bounding box,\nr o v e c ` x y ` z _ r ` m d i ✩ n x y z _ m a x ✩ is an effective ray, while ◯ is a non-effective ray because it\u0026rsquo;s out of the bbox.\nReconstruct sigma The sigma value on each voxel is the summation of 16 components of all 3 directions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1D scalar: (args.batch_size*N_samples) sigma_feature = torch.zeros((xyz_sampled.shape[0],), device=xyz_sampled.device) # traverse 3 modes for idx_plane in range(len(self.density_plane)): # interpolate tensor (1,16,128,128) at coordinates (batch_size*N_samples, 1, 2) plane_coef_point = F.grid_sample(self.density_plane[idx_plane], coordinate_plane[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1]) # (16, batch_size*N_samples) # interpolate tensor (1,16,128,1) at coordinates:(bs*N_samples, 1, 2) line_coef_point = F.grid_sample(self.density_line[idx_plane], coordinate_line[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1]) # (16, batch_size*N_samples) # accumulate 16 components of 3 directions for each voxel, (batch_size*N_samples) sigma_feature = sigma_feature + torch.sum(plane_coef_point * line_coef_point, dim=0) The factors grid is composed of 3 planes self.density_plane and 3 vectors self.density_line\nThe factors corresponding to the sampled 3D points are obtained by F.grid_sample():\np l a n e \" 0 d - e 1 s \" n i t ⬣ 1 y 6 p c l ⬣ h a a n n e n c ⬣ e ( o l 1 o s 2 r 8 d ⬣ , i 1 n 2 a 8 t ) e _ 0 p l a n 2 e \" 1 1 - 2 \" c \" o 0 o \" r , d \" i 1 n \" a , t d e o e _ r n ( l s 1 i \" i 2 n 2 t 8 e \" y , 1 l ) i n e Retrieve the \u0026ldquo;factor in the vector direction\u0026rdquo; for each sample voxel is like the above figure right.\ns f i e g a m t a u r e = p v r ( e o 1 V c j 6 e t e c o c c r t o - i m 2 o ＋ ⋮ p x × n o n p e m r n M a o t a t j s t r e ) i c x t - i 0 o 1 n ✚ p v r e o ( V c j 1 e t e 6 c o c r t c - i ＋ o 1 o ⋮ m x × n p o p n m r e M a o n a t j t t r e s i c ) x t - i 0 o 2 n ✚ p v r e o ( V c j 1 e t e 6 c o c r t ＋ c - i o 0 o ⋮ m x × n p o p n m r e M a o n a t j t t r e s i c ) x t - i 1 o 2 n doubt: Were the coefficients not multiplied with basis vector, but simply summed up together as the reconstruction?\n(2023-06-29) TensoRF is not projecting voxel onto each basis vector (matrix), but retrieving the coefficients from the factor grid.\nWhat the TensoRF retrieved is the coefficient * vector because it samples the factor grid directly. The factor grid satifies orthogonality naturally, so a coefficient inside is equivalent to having already multiplied with basis vectors.\nReconstruct appear. feature For appearance feature of each voxel, the vector projection and matrix projection in 3 directions are concatenated together, then multiply together:\n1 2 3 4 5 6 7 8 9 10 11 12 plane_coef_point, line_coef_point = [],[] for idx_plane in range(len(self.app_plane)): plane_coef_point.append(F.grid_sample(self.app_plane[idx_plane], coordinate_plane[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1])) line_coef_point.append(F.grid_sample(self.app_line[idx_plane], coordinate_line[[idx_plane]], align_corners=True).view(-1, *xyz_sampled.shape[:1])) plane_coef_point, line_coef_point = torch.cat(plane_coef_point), torch.cat(line_coef_point) return self.basis_mat((plane_coef_point * line_coef_point).T) 1 1 l 4 p 4 i 4 l 4 f n a e 2 e c n c a a 7 _ h e h p t c a _ a p u d o n c n r i e n o n e m f e e e = = l = f l s s ( 4 ( ❲ p 8 p 4 v r m r 8 V e o c a o 1 e c j ⋮ o t j ⋮ c 4 c t e m r e o 4 o c p i c m × x r t o x t p d - i n - i o i M 2 o e 0 o n m a n n 1 n e t t n s t ❳ ) s ) × ⊕ ⊕ ( 1 4 ( 4 p 8 m p 4 4 v r a r 8 x e o c t o 2 c j ⋮ o r j ⋮ c 7 t e m i e o o c p x c m b r t o - t p a - i n 0 i o s 1 o e 2 o n i n n n e s t n s t m ) s a ) t ⊕ ⊕ ( 4 ( p 8 m p 4 v r a r 8 e o c t o c j ⋮ o r j ⋮ c t e m i e o o c p x c m r t o - t p - i n 1 i o 0 o e 2 o n n n n e t n s t ) s ) Then RGB is mapped from the appearance featrue:\nr g 3 b = M L P ❲ f e a a 2 p t 7 p u r e ⊕ v d i i 3 e r w ⊕ a p p e p f 1 e 0 ⊕ a 8 t v i p 1 e e 2 w ❳ Optimizing The learnable parameters includes:\n16 vectors and 16 matrices for density in 3 directions; 48 vec and 48 mat for app feature in 3 directions; linear layer transforming 48x3 dim appearance feat to 27D; linear layer mapping 27D feat+viewdir to 3-dim rgb. flowchart LR x(\"Sample a point\") y(\"Reconstruct its sigma and rgb by aggregating its components\") b(\"Use BP+GD (Adam) to optimize those parameters\") x --\u003e y --\u003e b Losses: L2 rendering loss + L1 norm loss + Total Variation loss.\nTV loss benefits real datasets with few input images, like LLFF. Q\u0026amp;A How does it ensure that the components are orthogonal during training? (2023-06-10) Reference {论文代码阅读}TensoRF: Tensorial Radiance Fields - 爱睡觉的人人\n","date":"2023-05-29T18:18:00Z","image":"https://apchenstu.github.io/TensoRF/img/pipeline.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-tensorf/","title":"read: TensoRF"},{"content":"Repo\nThis diagram is supported by Hugo.\nAscii-art Archive\nTrees 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 Overlaps Line Decorations Line Ends Dot Grids · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · Large Nodes A 1 2 3 4 B 5 C 6 8 7 D Small Grids a b a b a b a b Big Grids A B A B A B Complicated \u0026amp; A M S i o i q f b B x u j o e a ( - x d r a \u0026gt; e f R \u0026gt; c o C n u o b ( n r ) ) d n e e J d r o s i n N o R D t o i u a a n g d d l o i t n e D i a g o n a l s C V u e r r v t e i d c a l n o t A N C : o u l d r r i A a / I v n s i n e e h s t d - e - t r l B i h i i s i o n ' s r e q n . u * o o b t t o e l a s d ' * l i n e D o n S e e ? a r c 3 h Ascil art cube , 1 2 8 9 6 9 6 ` ` . . ` ` ` ` . . ` ` . . ` . ` ` . ` . Coordinates z x p l a n e y f g ₁ : ₁ : x a ₁ f g ₂ : ₂ : x ₂ f : g : : : x ₃ ₃ ₃ f : g : : : : x ₙ ₙ ₙ b f g ( ( x x x ) ) f ₁ x ₁ f f ( ₂ x x ₂ ) f x ∑ ₃ ₃ _ { k ∈ ℤ } δ ( f x t ₙ ₙ - k T ) x w w ( x ₀ , x S ₁ c ) r ᵀ e e n I k n e x t r ₂ e n g e ( P c ( r l O r e x a s r o n ₀ l s t j t , ϕ R p h e e x ( a a o c r ₁ φ y c g t , ( e o i r x 𝐮 n o ' ₂ ) a n ( ) ) l 𝐱 ᵀ ϕ t ) ) 𝐖 r N a 𝐮 D n + C s ( b f P r φ C s o ( a p s 𝐮 m a p ) e c e r e c a t φ t i 𝐌 r v v a e 𝐮 i n ) + e s c w f K O e b r j n e e c l t P i s f r n p e a ⊗ f c i e h l t e r 𝐮 NeRF vs EWA:\nP r o j c e a c m t e r p a o i 1 n t s P r o j e c t R p r a a a l s y r y i c s a n r l s e e ∫ i a l p e L s r e a s n i e l c e n f e g e o i s o n s t e p g r i n t PixelNeRF:\nP r ◸ o j e c v t i e p w o i 1 v n i t e s w 2 Ellipses . \" ~ • - - - o ~ . \" \" - _ \" - - - • - - - ∘ - . - _ \" - \" . \" ∘ • \" . Splat\nG a i 2 u a D s n s s S - c r e e n . \" • \" . V D o e l p u t m h e h . \" d e ~ a r t e a i h s a s ‖ 𝐭 b ‖ • e ₂ e . n C t o h v r o m ~ w a . \" n t r o i n x t o i s t h 𝚺 e ' . s c r e e n ","date":"2023-05-29T09:58:00Z","permalink":"https://zichen34.github.io/writenotes/lang/go_goat/","title":"memo: GoAT Ascii Diagram"},{"content":"Course page: AM301\nSingular Value Decomposition Source video: Lecture: The Singular Value Decomposition (SVD) - AMATH301\nlinalg: Matrix acts on vector All what matrix multiplication does is it rotates and stretch the vectors.\nPrincipal Component Analysis Source video: Lecture: Principal Componenet Analysis (PCA)\nPhysical example Considering a physical system as the following, a rod suspends a mass via a spring.\nr o d m s p k r i | n g f ( z ) Pull the mass down and let it oscillate up and down.\nThe displacement (position) of the mass oscillation can be measured by a function f(z). Then f(z) can be solved based on Newton\u0026rsquo;s second law:\n$$ \\begin{aligned} F \u0026amp;= ma = m \\frac{d²f(z)}{dz²} \\\\ \\ \\\\ -w²f(z) \u0026amp;= m \\frac{d²f(z)}{dz²} f(z) = Acos(wz + w₀) \\end{aligned} $$\nData-driven approach can solve it too.\nSuppose the law $F=ma$ is unknown, infer the F=ma from the data alone. First of all, the complexity of this system should be figured out.\nMeasure this system using cameras\nc a m e r a 1 r o d m s p k r i | n g f ( z ) c a c m a e m r e a r a 3 2 Every camera records the mass coordinates on their projection plane:\nCamera 1: (𝐱$_a$, 𝐲$_a$); Camera 1: (𝐱$_b$, 𝐲$_b$); Camera 1: (𝐱$_c$, 𝐲$_c$).\nArrange them into a data matrix 𝐗 (6-rows):\n$$ 𝐗 = \\begin{bmatrix}𝐱_a \\\\ 𝐲_a \\\\ 𝐱_b \\\\ 𝐲_b \\\\ 𝐱_c \\\\ 𝐲_c \\end{bmatrix} $$\nTwo fundamental issuses associated with these data need to be addressed.\nNoise\nData with noise on top of it is not a good representation of the system.\nRedundancy\nMeasurements are not independent to each other, i.e., x and y are related. Different cameras take the similar infomation just from different angles.\nThe movement is only one degree of freedom, but the observed data has six sets.\nOne doesn\u0026rsquo;t know how to take the perfect observation ahead of time. PCA will reveal which camera at which angle is enough to describe the whole system.\nVariance and Covariance Assumption: big variance score means that vector is chaning a lot. It has a lot stuff happening.\nIf the diagonal terms that are big in the covariance matrix , those vectors are matter. Vectors having small variance don\u0026rsquo;t change much. MAKE DIAGNOAL (SVD) is remove all the redundancy\nPCA for Face Recognition Source video: Lecture: PCA for Face Recognition\nA 2D image are flattened into a vector. So each column in the U returned from SVD is an image. So a column essentially contains two directions: x and y. ","date":"2023-05-27T22:20:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/02_svd/","title":"watch: Nathan 02 | SVD"},{"content":"Jacobian matrix Khan Academy\nPrerequisite Matrix is a linear transformation of space by moving the basis vectors to new landing spot.\n$$ \\begin{bmatrix} 2 \u0026amp; -3 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\rightarrow \\begin{bmatrix} 2x + (-3)y \\\\ 1x+1y \\end{bmatrix} $$\n\u0026ldquo;linear\u0026rdquo;: grid lines remain parallel, evenly spaced and straight lines after transformation.\nTransform\nThe two basis vectors $[^1_0]$ and $[^0_1]$ are moved to $[^2_1]$ and $[^{-3}_{1}]$, which are the columns of the matrix.\n$$ \\begin{array}{ccc} \\begin{bmatrix} 2 \u0026amp; -3 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2+0 \\\\ 1+0 \\end{bmatrix} \\\\ \\\\ \\begin{bmatrix} 2 \u0026amp; -3 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0-3 \\\\ 0+1 \\end{bmatrix} \\end{array} $$\nReconstruct\nThe basis vectors $𝐢 = [i₁,i₂], 𝐣 = [j₁,j₂]$ of the new space are completely made up by original basis vectors $[^1_0]$ and $[^0_1]$:\n$$ \\begin{array}{ccc} 𝐢 = \\begin{bmatrix} i_1 \\\\ i_2 \\end{bmatrix} = \\begin{bmatrix} 2×1 + (-3)×0 \\\\ 1×1 + 1×0 \\end{bmatrix} \\begin{matrix} \\text{first component}\\\\ \\text{second component}\\end{matrix} \\\\ \\\\ 𝐣 = \\begin{bmatrix} j_1 \\\\ j_2 \\end{bmatrix} = \\begin{bmatrix} 2×0 + (-3)×1 \\\\ 1×0 + 1×1 \\end{bmatrix} \\begin{matrix} \\text{first component}\\\\ \\text{second component}\\end{matrix} \\end{array} $$\nRecord\nMatrix records synthesising factor for transforming each original basis vector to the new basis vector:\nThe first row of the matrix is the combination factors for the original first basis vector to form the first component of the new first basis vector.\nThe second row corresponds to the second component of the new first basis vector, by re-combining the original first basis vector. And so forth.\nLiner Operator\nMatrix operator 𝑳 takes in a vector and spit out a vector.\nBased on the properties of linearity: scaling and adding, i.e.,\nL(a𝐯) = aL(𝐯) L(𝐯+𝐰) = L(𝐯) + L(𝐰) Applying the operator on a vector $[^x_y]$ can be represented as:\n$$ \\begin{aligned} L(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) \u0026amp;= L(x \\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} + y \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}) \\\\ \u0026amp;= x L(\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}) + y L(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}) \\end{aligned} $$\nLet $[^x_y]$ equals to $[^2_1]$, the result vector should be 2 times of $L([^1_0])$ plus 3 times of $L([^0_1])$, i.e., $[^1_3]$\nThat means matrix transforms the space by modifying the original basis vectors. The coordinates of the given vector have no change either in the original space or the new space.\nSo the transformation can be found by determining the coefficient of the change of each component.\nThe first column in the transformation matrix is landing spot of the first basis vector (i.e., x axis)\nlocal linearity Nonlinear function $f([^x_y]) = \\rm [^{x+sin(y)}_{y+sin(x)}]$ has local linearity. Therefore, the local linear transformation can be represented by a 2x2 matrix\nJacobian matrix There are multiple functions and multiple variables. Jacobian records the effects of each variable on each function. Its organization comes down to local linearity, which allows the nonlinear transformation to be represented as a linear transformation.\nJacobian determinant Jacobian determinant is the factor of the neighbor area around a point scratched or squished by the local \u0026ldquo;linear transformation\u0026rdquo;.\ntodo: What is Jacobian? | The right way of thinking derivatives and integrals - Mathemaniac\nGauss-Newton algorithm 最优化算法之高斯牛顿法-path-int\n用雅克比矩阵的乘积 J(𝐱) J(𝐱)ᵀ 代替 Hessian 矩阵\n-拼凑梦想-\n初值会影响能否走到全局最优\n视觉slam 十四讲第6章\nGauss-Newton algorithm for solving non linear least squares explained\n（干货）《雅可比矩阵是什么东西》3Blue1Brown，搬自可汗学院。 【自制中文字幕】\n(2023-12-24) \u0026ldquo;In [8], an LSTM [18] is used to model the Levenberg-Marquardt (LM) algorithm and predicts the update at each step directly.\u0026rdquo; mentioned in Fast-MVSNet.\nLevenberg Marquadt code example\npaper METHODS FOR NON-LINEAR LEAST SQUARES PROBLEMS\n","date":"2023-05-26T13:58:00Z","image":"https://img.youtube.com/vi/Vnga_psnCAo/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/calc/nonlinear_least_squares/","title":"memo: Calc | Nonlinear Least Squares"},{"content":"Book site; book\nⅠ. Curve Fitting Sec 4.1: Classic Curve Fitting and Least-Squares Regression\nRegression ≈ curve fitting ≈ 𝐀𝐱=𝐛 (Least-square fitting), where 𝐀 is the data, 𝐱 is the parameters of the model, 𝐛 is the target.\nRegression: Fitting a model to some data with some parameters\nOver- and Under-determined Models could be Over- and Under-determined.\nOver-determined system normally has No solution.\nThere\u0026rsquo;re massive constraints (equations, samples) given, but the complexity (#variables) of the system is not enough to describe the existing data.\nAn over-determined system can be \u0026ldquo;No solution\u0026rdquo; or \u0026ldquo;Infinitely many solution\u0026rdquo; Over~ wiki\nHomogeneous case: (no bias)\nThe coefficient matrix is a tall, skinny matrix, the \u0026ldquo;all-zero solution\u0026rdquo; always holds. If there\u0026rsquo;re enough equations are dependent, and after eliminating #non-zero row $\u0026lt;$ #cols in the coefficient matrix in row-echelon form, this over-determined homogeneous system has \u0026ldquo;infinitely many solutions\u0026rdquo; (including all-0 solution). Otherwise, \u0026ldquo;all-zero\u0026rdquo; is the only solution. Non-homogeneous case:\nIf the last non-zero row of the augmented matrix in row echelon form is only having the constant entry of the last column (giving an equaltion 0=c), this system has \u0026ldquo;No solution\u0026rdquo;.\nSince it\u0026rsquo;s a tall matrix, this case is likely to happen:\n$$ \\left[ \\begin{array}{cc|c} 2 \u0026amp; 1 \u0026amp; -1 \\\\ -3 \u0026amp; 1 \u0026amp; -2 \\\\ -1 \u0026amp; 1 \u0026amp; 1 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc|c} 2 \u0026amp; 1 \u0026amp; -1 \\\\ 0 \u0026amp; 5/2 \u0026amp; -7/2 \\\\ 0 \u0026amp; 3/2 \u0026amp; 1/2 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc|c} 2 \u0026amp; 1 \u0026amp; -1 \\\\ 0 \u0026amp; 5/2 \u0026amp; -7/2 \\\\ 0 \u0026amp; 0 \u0026amp; 13/5 \\\\ \\end{array} \\right] $$\nSince the equations are way more than unknowns, the coefficient matrix in row echelon form is most likely not having: #non-zero rows = #cols. So the case \u0026ldquo;single unique solution\u0026rdquo; is almost impossible, but there\u0026rsquo;ll be \u0026ldquo;Infinite many solutions\u0026rdquo;.\nUnless, enough rows can be eliminated (lines overlap), and the remaining coefficient matrix has the same rank as the augmented matrix, and also the #non-zero rows = #cols in the coefficient matrix, this system has \u0026ldquo;single unique solution\u0026rdquo;.\nUnder-determined system normally has infinitely many solutions.\nBecause the #parameters is more than equations (samples), some variables are not restricted, which caused the infinitude of solutions. In other words, there\u0026rsquo;re not enough equations to uniquely determine each unknown. So the problems is how to determine which solution is the best.\nAn underdetermined linear system has either No solution or Infinitely many solutions.Under~ wiki\nHomogeneous case: The coefficient matrix is a short, fat matrix, #non-zero rows $\u0026lt;$ #cols. So the system always has \u0026ldquo;Infinitely many solutions\u0026rdquo; (including all-0 solution). Non-homogeneous case: Rank of augmented matrix \u0026gt; Rank of coefficient matrix: No solution Rank of augmented matrix = Rank of coefficient matrix: There must be some solutions. But since the #row is already less than #cols for the coefficient matrix, the single unique solution (except for all-0) is impossible. So this system indeed has infinitely many potential solutions. $$ \\left[ \\begin{array}{ccc|c} 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{ccc|c} 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \\end{array} \\right] $$ How to solve If there\u0026rsquo;re infinite optional solutions, then the best model should be selected according to other constraints, i.e., regularizers g(𝐱).\nFor instance, the penalty for L2 norm of the parameters vector ‖𝐱‖₂ can lead to the model with smallest mse; And the penalty for L1 norm of the parameters vector ‖𝐱‖₁ can lead to the model with sparse number of parameters.\nTherefore, the objective is:\n$argmin_𝐱 g(𝐱)$ subject to ‖𝐀𝐱-𝐛‖₂=0.\nIf some error can be tolerated to just find the model that has the minimum L2-norm params, the constraint can be relaxed by a small error, like: ‖𝐀𝐱-𝐛‖₂≤ε\nIf there\u0026rsquo;s no solution, the expected model should have the minimum error.\nTherefore, the objective is: $argmin_𝐱$ (‖𝐀𝐱-𝐛‖₂).\nAdding regularizers can confine the optimizer to find the right model, so the objective can be\n$argmin_𝐱$ (‖𝐀𝐱-𝐛‖₂ + λg(𝐱))\nOrdinary least squares gives the approximate solutions (when no exact solution exists) or the exact solution (when it exists) by minimizing the square error:\n$argmin_𝐱$ ‖𝐀𝐱-𝐛‖\nIts solution is 𝐱 = (𝐀ᵀ𝐀)⁻¹𝐀ᵀ𝐛. And using QR factorization of A to solve the least squares can achieve good numerical accuracy.\nMore broadly, this generic architecture can be generalized to non-linear regressions by replacing ‖𝐀𝐱-𝐛‖₂ to non-linear constraints f(𝐀,𝐱,𝐛):\n$$ argmin_𝐱 ( f(𝐀,𝐱,𝐛) + λg(𝐱)) \\quad \\text{(Over-determined) or} \\\\ argmin_𝐱 g(𝐱) \\ \\text{subject to } f(𝐀,𝐱,𝐛) ≤ ε \\quad \\text{(Under-determined)} $$\nAn over-determined non-linear system can be solved by Gauss-Newton iteration\nOver- and Under-fitting Two canonical issues:\nOver-fitting: With enhencing the model complexity, the training error keeps dropping, but the evaluating error on withhold data goes up.\nUnder-fitting: No matter how you increase the model complexity (parameters), the training error doesn\u0026rsquo;t drop, either that\u0026rsquo;s a bad model or there\u0026rsquo;s not enough data.\nRegression framework Generic Regression: 𝐘 = 𝑓(𝐗,𝛃), input 𝐗 into model 𝑓, then target 𝐘 can be obtained.\nRegression needs 2 things: select a model 𝑓 and find its parameters beta that can map X to Y.\nDifferent norms are used to measure the error: L∞ norm, L1 norm, L2 norm. And the selected norm (error metric) has big impact on the \u0026ldquo;Goodness of Fit\u0026rdquo;.\nⅡ. Nonlinear Regression Sec 4.2: Nonlinear Regression and Gradient Descent\nⅢ. Regularization Sec 4.3: Over- and Under-determined Systems; Code demo\nⅣ. Optimization Sec 4.4: Optimization for regression\nOptimization is the cornerstone of regression.\nOptimization is to minimize some objective function. Regression is finding the parameters of the given model that maps the input to the output. Regularization is Critical\nRegularizers are what determine the type solutions obtained. Simple Example The data is generated from parabola + noise $f(x) = x^2 + 𝐍(0,σ)$. However, the model in practice is unknown, and regularization can help find better models.\nGiven 100 Realizations of data. And the model is represented as $𝐘 = 𝑓(𝐗,β)$\nSmall noise results complex predicted models.\nDifferent regression 1 2 3 f = (x.^2 + 0.2 * rand(1,n)).\u0026#39;; lambda = 0.1; phi pinv(): Pseudo-inverse (Least square with the minimum L2 norm) \\ (bashslash): QR decomposition lasso() robustfit() () Least-square fit pinv get very different model for samll pertubation every time and all the coefficients are functioning. The uncertainty of this solver is high.\nThe high-dgree terms are highly volatile, which makes the model unstable.\nParsimony (Pareto front) Keep adding higher degree polynomials to the model, the error will no longer decrease at some point and instead even rise up.\nPareto front Sec 4.7: The Pareto front and Lex Parsimoniae\nSmall number of parameters can bring interperity\nConnect NNs Sec Neural Networks: 1-Layer Networks\n","date":"2023-05-23T17:24:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/10_regression_model_selection/","title":"watch: Nathan 10 | Regression and Model Selection"},{"content":"Course page: AM584\nDescovered by Ytb search: CANDECOMP/PARAFAC (CP) decomposition\nTensor Decomposition Source video Applied Linear Algebra: Tensor Decompositions\nTensor is the generalization of matrix. Tensor decomposition is doing multiple times SVD to extract the corelation in each dimension.\nOne piece of data could be a matrix, e.g., the temperture distribution in a region of latitude and longitude.\nBy stacking the data matrices of different time points, the obtained data forms a cube.\nX ⱼ ` ` . . ` ` X ` ` . . Vectorizing Vectorize a (n-dimensional) array is to reshape it into a vector.\nStacking the columns on top of each other.\nX ⱼ : X ⱼ O r ` ` . . ` ` ` ` . . : X ⱼ Vectorizing doesn\u0026rsquo;t bother the similarity against each smaples? because the total amount of difference is fixed? No matter using which kind of metric, the normalized distinctions between each other are fixed?\nSVD By flattening each sample, and arrange them into a matrix, then SVD can be applied.\nReconstruct the matrix A with low-rank structures?\nm n A = σ ₁ u 1 v 1 + σ ₂ u ₂ v ₂ + σ ₃ u ₃ v ₃ + ⋯ ⋯ To avoid vectorization (flattening) messes up or mixes together the independent attributes (temperture, pressure, and humidity) and keep them in their own dimensions separately, SVD needs to be applied for every dimension.\nDecompose a data cube 𝓜 by performing SVD in 3 directions:\nc σ ⱼ ₁ a ₁ a ⱼ b ₁ c ₁ b ⱼ = + σ ₂ a ₂ b ₂ c ₂ + σ ₃ a ₃ b ₃ c ₃ + ⋯ ⋯ A dominant vector is responsible for its homologous vectors, having the same dimensionality.\nFor example, the above 5-dimensional vector is representing for the vectors parallel to it.\nAs the following hand illustration shows, Align the fingers with the vector and move towards the direction that the palm is facing. The vectors on the hand path will be represented.\n(image comes from here)\nReconstruct The r-rank approximation to the data cube M is the sum of the outer product of the dominant vectors in 3 directions.\n𝓜 = ∑ⱼ₌₁ʳ σⱼ∘aⱼ∘bⱼ∘cⱼ\nAll individual dominant vectors aⱼ constitute 𝓐ᵣ. And 𝓑ᵣ is the collection of all the bⱼ, and so do 𝓒ᵣ.\n𝓜 = 𝓐ᵣ ∘ 𝓑ᵣ ∘ 𝓒ᵣ\nInner and Outer product: Given two column vectors: $𝐮=[^{{u_1}}_{^{u_2}_{u_3}}]$ and $𝐯=[^{v_1}_{^{v_2}_{v_3}}]$,\nInner product (equals dot product):\n$$ \\rm uᵀv = (u₁,u₂,u₃) [^{v_1}_{^{v_2}_{v_3}}] = u₁v₁ +u₂v₂ + u₃v₃ \\text{= scalar} $$\nOuter prodcut: $$ \\rm 𝐮⊗𝐯 = uvᵀ = [^{{u_1}}_{^{u_2}_{u_3}}] (v₁,v₂,v₃) \\\\ = \\begin{bmatrix} \\rm u₁v₁ \u0026amp; \\rm u₁v₂ \u0026amp; \\rm u₁v₃ \\\\ \\rm u₂v₁ \u0026amp; \\rm u₂v₂ \u0026amp; \\rm u₂v₃ \\\\ \\rm u₃v₁ \u0026amp; \\rm u₃v₂ \u0026amp; \\rm u₃v₃ \\end{bmatrix} $$\nAny two vectors (with difference lengths) can perform outer product:\n$$ \\rm (^{v₁}_{v₂}) ⊗ (^{w₁}_{^{w₂}_{w₃}}) = \\begin{bmatrix} \\rm v₁w₁ \u0026amp; \\rm v₁w₂ \u0026amp; \\rm v₁w₃ \\\\ \\rm v₂w₁ \u0026amp; \\rm v₂w₂ \u0026amp; \\rm v₂w₃ \\end{bmatrix} $$\nEach entry in the matrix is a scalar and can be indexed by: (𝐯 ⊗ 𝐰)ᵢⱼ= 𝐯ᵢ⋅𝐰ⱼ, the product of an element in the first vector with an element in the second vector.\nVector-matrix outer product\n1 2 3 4 5 6 7 8 9 \u0026gt;\u0026gt;\u0026gt; v = torch.tensor([0,1]) # [2] \u0026gt;\u0026gt;\u0026gt; m = torch.tensor([[2,0],[1,3]]) # [2, 2] \u0026gt;\u0026gt;\u0026gt; torch.einsum(\u0026#39;p,qr-\u0026gt; pqr\u0026#39;,v,m) tensor([[[0, 0], [0, 0]], [[2, 0], [1, 3]]]) Pytorch batch matrix vector outer product - SO\nThe SVD decomposition can be interpreted as the sum of outer products of each left (𝐮ₖ) and right (𝐯ₖ) singular vectors, scaled by the corresponding nonzero signular value σₖ wiki\n𝐀 = 𝐔 𝚺 𝐕ᵀ = $∑_{k=1}^{rank(A)}$ (𝐮ₖ ⊗ 𝐯ₖ)σₖ\nCode Demo Source video: Applied Linear Algebra: Implementing Tensor Decompositions\nDataset The data cube is generated by a spatial temporal function. It\u0026rsquo;s like a video having x and y directions and chaning in time.\ny x t = σ ₁ m o t d ₁ e 1 x ₁ y ₁ + σ ₂ m o a d ₂ e 2 x ₂ y ₂ The function mixes two fundamental features (modes) together. So the decomposition method should pull out the the two features back.\nFeature 1: $f₁(x,y,t) = e^{-(x²+0.5*y²)} cos(2t)$, the spatial structure is a elliptical shaped gaussian, and this bump oscillate up and down in time. Feature 2: $f₂(x,y,t) = sech(x)tanh(x) e^{-0.2y^2} sin(t)$, x direction is the shape -∿-, y direction is gaussian bell, and it oscillates with sine t Data matrix: F = f₁ + f₂ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 clear all; close all; clc % Basic domains (each frame is a rectangle) x = -5:0.1:5; y = -6:0.1:6; t = 0:0.1:10*pi; [X, Y, T] = meshgrid(x, y, t); % Data cube f1 = exp(-(X.^2 + 0.5*Y.^2)).*(cos(2*T)); % (Y,X,t): (121, 101, 315) f2 = (sech(X).*tanh(X).*exp(-0.2*Y.^2)).*sin(T) A = f1 + f2; % Tensor % Visualizing the shapes for j = 1:length(t): surfl(x, y, f1(:,:,j); %surfl(x, y, f2(:,:,j); %axis([-5,5], [-6,6], [-1,1]); colormap(hot) shading interp; drawnow end % Vectorizing (for performing standard SVD) nx = length(x); ny = length(y); for j = 1:length(t): % Every flattened column vector is appended to the matrix Af Af(:,j) = reshape(A(:,:,j), nx*ny, 1); % (nx*ny, t) end % Make matrix Af back to a tensor for j = 1:length(t) At(:,:,j) = reshape(Af(:,j), nx, ny) end % n-way array tensor decomposition (parallel factorization algorithm) % Given a hypercube of data, it will find the dominant factors in each direction. % Input tensor and rank (# of dominant components truncated) model = parafac(A,2); % Determine factors in each directions from each low-rank projections % If A is a 10-dim tensor, A1,A2,...,A10 directions are expected. % A1 has 2 factors, A2 has 2 factors, ... [A1, A2, A3] = fac2let(model); % Plot the 2 factors (vectors) in each component. subplot(3,1,1), plot(y, A1, \u0026#39;Linewidth\u0026#39;, [2]) % subplot(3,1,2), plot(x, A2, \u0026#39;Linewidth\u0026#39;, [2]) subplot(3,1,3), plot(t, A3, \u0026#39;Linewidth\u0026#39;, [2]) In y direction has two gaussian: $e^{-0.5y^2}$ and $e^(-0.2y^2)$\nIn x direction, there are a gaussian $e^(-x^2)$ and $sech(x)*tanh(x)$\nIn t direciton, cos(2t) and sin(t) are pulled out.\n","date":"2023-05-22T18:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/09_tensor_decompositions/","title":"watch: Nathan 09 | Tensor Decompositions"},{"content":"Code | Arxiv(2308) | ProjPage\nCode: wanmeihuali/taichi_3d_gaussian_splatting\nVideo Explain (2023-09-11)\nSource video: 【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！- 意の茗\nCG technique: Splatting\nEach point is an anisotropic 3D Gaussian distribution:\nmean is point location (xyz), covariance matrix determined the shape of the 3D Gaussian Optimize points\u0026rsquo; distribution:\nLocations are at mean, which are needless to learn; Co-variance matrix is converted to quaternion 3D Gaussian distribution can do clone and split to fit complex geometry.\nTile-based fast rendering rather volume rendering.\n3D Gaussian (2023-10-28)\n1D Gaussian distribution:\nGiven a scalar $x \\sim N(μ,σ²)$, its PDF:\n$$ p(x) = \\frac{1}{\\sqrt{2πσ²}} e^{-\\frac{(x-μ)²}{2σ²}} $$\n3D Gaussian composed of 3 independent 1D Gaussian in 3 directions can be represented as below.\nGiven a vector 𝐯: [a,b,c], its PDF: p(𝐯) = p(a) p(b) p(c)\n$$ \\begin{aligned} p(𝐯) \u0026amp;= \\frac{1}{(2π)^{3/2}σₐ σ_b σ_c} exp(-\\frac{(a-μₐ)²}{2σₐ²} -\\frac{(b-μ_b)²}{2σ_b²} -\\frac{(c-μ_c)²}{2σ_c²}) \\\\ (\\text{vectorize}) \u0026amp;= \\frac{1}{(2π)^{3/2} |Σ|^½} exp(-½⋅(𝐯-\\bm μ)ᵀ(𝐯-\\bm μ) Σ⁻¹) \\end{aligned} $$\nWhere the $σₐ σ_b σ_c$ is the square root of the determinant of covariance matrix Σ:\n$$ |Σ|^{½} = \\begin{vmatrix} σₐ² \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; σ_b² \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; σ_c² \\end{vmatrix}^{1/2} = σₐ σ_b σ_c $$\nThe exponent can be derived as:\n$$ \\begin{array}{ccc} \\begin{bmatrix} a-μ_a \\\\ b-μ_b \\\\ c-μ_c \\end{bmatrix} \\begin{bmatrix} a-μ_a \u0026amp; b-μ_b \u0026amp; c-μ_c \\end{bmatrix} \\begin{bmatrix} 1/σₐ² \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1/σ_b² \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1/σ_c² \\end{bmatrix} \\\\ = \\frac{(a-μ_a)²}{σₐ²} + \\frac{(b-μ_b)²}{σ_b²} + \\frac{(c-μ_c)²}{σ_c²} \\end{array} $$\nIf those 3 1D Gaussian are all $N(μ=0, σ²=1)$, the 3D Gaussian becomes simpler:\n$$\\rm p(𝐯) = \\frac{1}{(2π)^{3/2}} exp(-\\frac{a²+b²+c²}{2})$$\nDerivation for arbitrary 3D Gaussians:\n3D Gaussian is a product of three 1D Gaussian. If each 1D Gaussian is N(0,1), then 3D Gaussian is:\n$$ \\begin{aligned} p(𝐱) \u0026amp;= p(x)p(y)p(z) \\\\ \u0026amp;= \\frac{1}{(2π)^{3/2}} exp(-\\frac{x²+y²+z²}{2}) = \\frac{1}{(2π)^{3/2}} exp(-\\frac{𝐱ᵀ𝐱}{2}) \\\\ \\end{aligned} $$\nFor an arbitrary vector 𝐱 = [x,y,z], where 3 Gaussians could be in various shapes. it will be moved to the origin of world space by subtracting the mean vecotr, and the 3 directions (covariance matrix) will be scaled to 1 (identity matrix) through a transformation matrix 𝐀.\n$$\\rm 𝐱\u0026rsquo; = 𝐀(𝐱-\\bm μ)$$\nBecause the mean vector is point\u0026rsquo;s position. This step means all points are moved to world\u0026rsquo;s origin and reshaped into a unit space for later optimization.\nPlug this \u0026ldquo;normalized\u0026rdquo; 𝐱\u0026rsquo; into 3D Gaussian :\n$$p(𝐱\u0026rsquo;) = \\frac{1}{(2π)^{3/2}} exp(-\\frac{(𝐱-\\bm μ)ᵀ𝐀ᵀ𝐀(𝐱-\\bm μ)}{2})$$\nTo reach a form about 𝐱, integrate p(𝐱\u0026rsquo;):\n$$1 = ∭_{-∞}^{+∞} \\frac{1}{(2π)^{3/2}} exp(-\\frac{(𝐱-\\bm μ)ᵀ𝐀ᵀ𝐀(𝐱-\\bm μ)}{2}) d𝐱\u0026rsquo;$$\nSubstitude $d𝐱\u0026rsquo;$ with $d𝐀(𝐱-\\bm μ) = |𝐀|d𝐱$\n$$1 = ∭_{-∞}^{+∞} \\frac{|𝐀|}{(2π)^{3/2}} exp(-\\frac{(𝐱-\\bm μ)ᵀ𝐀ᵀ𝐀(𝐱-\\bm μ)}{2}) d𝐱$$\nThe function being integrated is p(𝐱). Since 𝐱 follows Gaussian, it can be rewritten with a mean vector 𝛍 and a covariance matrix 𝚺.\nThe covariance matrix 𝚺 is a symmetric matrix, which can be decomposed by SVD:\n$$ \\begin{aligned} Σ \u0026amp;= U Λ Uᵀ \\\\ \u0026amp;= UΛ^{½} Λ^{½T} Uᵀ \\\\ \u0026amp;= UΛ^{½} (UΛ^{½})ᵀ \\end{aligned} $$\nOn a 2D plane, SVD is strecting and rotating, represented separately by a stretch matrix (diagnoal) $S = [^{s₁ \\ 0}_{0\\ s₂}]$, and a rotate matrix (UUᵀ=1) $R = [^{cosθ \\ -sinθ}_{sinθ \\ cosθ}]$.\nEach column of U is orthogonal to each other and of magnitude 1. For example, when U is R:\n⌈ ⌊ c s x o i s n θ θ - R s c y i o n s θ θ ⌉ ⌋ ⌈ ⌊ p 1 0 P ₁ ⌉ ⌋ t s ⌈ ⌊ p 0 1 ₂ ⌉ ⌋ = ⌈ ⌊ c s R p o i o ₁ s n t ' θ θ a ⌉ ⌋ t e ⌈ ⌊ d - c p s o P ₂ i s t ' n θ s θ ⌉ ⌋ In 3D space, matrices are 3×3.\n𝐔 is a basis (a component, a coordinate system). By multiplying it with a diagnoal matrix, $UΛ^½$ is a linear transformation, denoted as the transformation matrix 𝐀. Thus, 𝚺 = 𝐀𝐀ᵀ.\nIn other words, an identity matrix (basis) will be transformed to another basis $UΛ^½$ by 𝐀.\nTherefore, 𝐀ᵀ𝐀 = 𝚺⁻¹, which will reverse an arbitrary covariance matrix 𝚺 to identity matrix 𝐈, i.e., putting the ellipsoid into the \u0026ldquo;unit\u0026rdquo; space $[^{100}_{^{010}_{001}}]$, where the modulus of each axis is 1.\nSubstitute 𝐀ᵀ𝐀 with 𝚺⁻¹, and $|𝐀| = |Σ|^½$, the 3D Gaussian is the function being integrated:\n$$\\frac{1}{(2π)^{3/2}|Σ|^½} exp(-\\frac{(𝐱-\\bm μ)ᵀΣ⁻¹(𝐱-\\bm μ)}{2})$$\nThe 3D Gaussian used in this work is simplified as:\n$$ G(x) = exp(-\\frac{(𝐱)ᵀΣ⁻¹(𝐱)}{2}) \\tag{4} $$\nOmit the mean vector 𝛍, becuase it\u0026rsquo;s 0. Every 3D Gaussian distribution\u0026rsquo;s center has been shifted to the origin. Once the optimization finished, ellipsoids will be reverted to the world space for rasterization.\nThe front fraction is omitted as the integral (\u0026ldquo;volume\u0026rdquo;) of 3D Gaussian isn\u0026rsquo;t limited to 1 to be a probability distribution, and considering a 3D Gaussian can be any size.\n𝐀 is for rotating and stretching an ellipsoid. Thus, 𝐀=𝐑𝐒. And then 𝚺 = 𝐑𝐒𝐒ᵀ𝐑ᵀ\nThe covariance matrix 𝚺 gets optimized during training, such that the shape and direction of ellipsoids get adjusted to fit the scene.\nOptimization 3D Gaussian representation based on point cloud from SfM.\nEach 3D Gaussian contains properties: 3D position (xyz), shape (transform matrix 𝐀), SH coeffs (color), opacity (α).\nOptimize rotate matrix 𝐑 using Quaternion instead of 3×3 matrix\nPoint cloud optimization\nRemove points whose opacity lower than threshold after a certain epochs;\nHigh positional gradients are inferred as a point is hard to reconstruct the geometry. So, small Gaussian do clone (for faster training), while large Gaussian do split (for recovering bkg).\nReset opacity to 0 periodically to remove floater around camera\nTile-based rasterization.\nAn image is split into 16×16 patches. Sorting 3D ellipses observed by a patch based on depth. Terminate alpha compositing on a pixel when opactiy reaches 1. Each tile has a CUDA block, and each pixel has a CUDA thread. Read Notes (2023-05-14)\nThree elements:\nBased on sparse points (colmap) and 3D Gaussians representation Point cloud optimization Fast rendering algorithm with GPU sorting (tile-based rasterization) How much meomery does the 1-5 million (1e6) Gaussians (for all scenes tested) cost?\nComparing with Mip-NeRF360 of 8.6MB, 3DGS has 523MB after 7K iterations on dataset \u0026ldquo;Mip-NeRF360\u0026rdquo;. Chinese translation: 3DGS笔记 - bo233的文章 - 知乎\nAbs (2023-10-29)\nNVS based on radiance field without neural network.\nSfM point cloud and 3D Gaussian representation. Point cloud adjustment: add and remove based on gradients. Tile-based rasterizer leveraging depth and cuda. Intro Points cloud format is chosen for rasterization. Then, 3D Gaussian is chosen to make point cloud a continuous field.\nEfficient method based on continuous representation (MLP) most performed interpolation.\nNeural nets representation is convinent to be optimized, but hinder fast rendering.\nRelated Works Splatting made point-based rendering more efficient by extending the rasterization beyond a single pixel to cover a spread-out area.\nSome methods used CNN to render.\nNeRF requires extensive sampling around the entire space.\nPulsar is \u0026ldquo;order-independent\u0026rdquo;, whereas alpha-blending for a pixel is performed based on visibility order.\ndiffuse - diffusion model\nOverview The key to the efficiency of our method is our tile-based rasterizer.\nExplicit scene representation appeals fast rendering without inferencing neural network.\nGradients are backpropagated to concrete 3D Gaussian.\nDifferentiable 3DGS \u0026ldquo;Unstructured\u0026rdquo; is opposite to \u0026ldquo;regular volume grid\u0026rdquo;. 3DGS is unstructured but able to do volume rendering. Point cloud data + 3DGS primitive -\u0026gt; scene representation\nComparing with small planar circle representation for each point, 3D Gaussian doesn\u0026rsquo;t need normal. And normals are intractable for a sparse (SfM) point cloud.\ncovariance matrices have physical meaning only when they are positive semi-definite\nThus, they use a decomposed, equivalent form: 𝚺 = 𝐑𝐒𝐒ᵀ𝐑ᵀ , i.e., stretching and rotating an ellipsoid.\nScaling matrix 𝐒 is represented by a 3D vector; Rotation 𝐑 is represented by a quaternion. Reparameterization is changing spaces. For example, spherical coords (θ,φ) of viewdir is recombinded to cartisian coords (x,y,z) in NeRF\u0026rsquo;s code. However, SVD doesn\u0026rsquo;t alter the number of dimensions, as a single space is split into 2 spaces. Optimization Create, delete or move points (3D Gaussian) to fit geometry.\nThe quality of covariance of 3D Gaussians is critical for the compactness.\nSGD performed by CUDA. Alpha comes from sigmoid. Covariance has done exponential activation.\nThe fast rasterization is critical in the efficiency of our optimization.\nLoss function: 𝓛 = (1-λ)𝓛₁ + λ𝓛_dssim\nSince the output is a complete image, SSIM is measured holistically between the gt image and predicted image. In contrast, S3IM is for random-ray patches.\nUse absolute error of rgbs, instead of MSE.\nAdaptive Density Control Densify to better represent scene and remove transparent 3D Gaussians (points) every 100 iterations.\nTotal volume?\nReset alpha value close to zero every 3k iterations to reduce floaters near cameras.\nTile-based Gaussian Split image into 16x16 patches ➔ Cull 3D Gaussians intersected with frustum ➔ Project 3D Gaussians to 2D ➔ Assign depth observed by a patch to each 3D Gaussian ➔ Sorting Gaussians based on depth for each patch.\nA block is assigned to a patch and loads packets of Gaussians into shared memory.\nEach pixel has a thread to do alpha compositing.\nReuse the per-tile lists for each pixel in the tile.\nImplementation Central-object photos taken from the entire hemisphere without angular regions missing result in good SH coefficients.\nHowever, for incomplete observation, e.g., corners or \u0026ldquo;inside-out\u0026rdquo; photos, the 0th order SH prone to be corrupt during optimization.\nSo they first optimize only the 0-order coefficient for 1K iterations, then add 1 more coeff every 1K iters until all 4 orders are supplemented.\nResults As fast as instantNGP, and as good as MipNeRF360. Synthetic dataset can use 100K random intialized point cloud in the bbox. More compact than other point-based methods. Limitations Hight memory cost due to point cloud representation. Tradeoff of model size for speed.\nCompress point cloud. Defects occurs in the scene not well observed.\nPlay (2023-11-01)\nEnvironment Ubuntu 18.04 Lambda Server: nvcc 10.2, Ubuntu 18.04, driver 470.103.01 with support for CUDA 11.4 at highest.\nClone needs \u0026ndash;recursive: git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive\nOtherwise, there will be an error when running conda env create --file environment.yml:\n1 ERROR: Directory \u0026#39;submodules/diff-gaussian-rasterization\u0026#39; is not installable. Neither \u0026#39;setup.py\u0026#39; nor \u0026#39;pyproject.toml\u0026#39; found. Or fetch and install submodules individually:\n1 2 3 4 conda activate gaussian_splatting git submodule update --init --recursive pip install submodules/diff-gaussian-rasterization pip install submodules/simple-knn Submodule install error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Pip subprocess error: error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─\u0026gt; [12 lines of output] Traceback (most recent call last): File \u0026#34;\u0026lt;string\u0026gt;\u0026#34;, line 36, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;pip-setuptools-caller\u0026gt;\u0026#34;, line 34, in \u0026lt;module\u0026gt; File \u0026#34;/home/z/Downloads/gaussian-splatting/submodules/diff-gaussian-rasterization/setup.py\u0026#34;, line 13, in \u0026lt;module\u0026gt; from torch.utils.cpp_extension import CUDAExtension, BuildExtension File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/__init__.py\u0026#34;, line 201, in \u0026lt;module\u0026gt; _load_global_deps() File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/__init__.py\u0026#34;, line 154, in _load_global_deps ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL) File \u0026#34;/home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/ctypes/__init__.py\u0026#34;, line 364, in __init__ self._handle = _dlopen(self._name, mode) OSError: /home/z/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torch/lib/../../../../libcublas.so.11: symbol cublasLtGetStatusString version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference [end of output] Submodules (\u0026ldquo;diff-gaussian-rasterization\u0026rdquo;) needs compilation locally with CUDA Toolkit. issue#45\nAlternative steps refer to issue#406\n(2024-04-05)\nI previously thought that the driver 470 is insufficient to install cuda 11.6, but I found that driver 470 is able to install CUDA 11.x on Docs. However, the problem persists when using cuda 11.6.\nThe problem is that my PATH has been modified previously. I once added a line in my .bashrc: export LD_LIBRARY_PATH=/home/z/anaconda3/envs/GNT/lib\nThis caused the libcublas points to mismatched library. Refer to the answer of eval\nI remove that line, and having the nvidia-driver 470 + ctk 11.6 on ubuntu 18.04, the env installation succeeds with just one line: conda env create --file environment.yml\nHe said by setting env virable LD_LIBRARY_PATH=\u0026lt;anaconda dir\u0026gt;/python3.7/site-packages/torch/lib/nvidia/cublas/lib/:$LD_LIBRARY_PATH So that \u0026ldquo;the dlopen will firstly look for .so files in that directory.\u0026rdquo; (I didn\u0026rsquo;t try that at present.)\nSubmodules compile error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /usr/local/cuda/bin/nvcc -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/TH -I/home/z/anaconda3/envs/3dgs/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/z/anaconda3/envs/3dgs/include/python3.9 -c cuda_rasterizer/backward.cu -o build/temp.linux-x86_64-cpython-39/cuda_rasterizer/backward.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options \u0026#39;-fPIC\u0026#39; -I/home/z/Downloads/gaussian-splatting/submodules/diff-gaussian-rasterization/third_party/glm/ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\u0026#34;_gcc\\\u0026#34; -DPYBIND11_STDLIB=\\\u0026#34;_libstdcpp\\\u0026#34; -DPYBIND11_BUILD_ABI=\\\u0026#34;_cxxabi1011\\\u0026#34; -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++14 cuda_rasterizer/backward.cu:15:10: fatal error: cooperative_groups/reduce.h: No such file or directory #include \u0026lt;cooperative_groups/reduce.h\u0026gt; ^~~------------------------- compilation terminated. ERROR: Could not build wheels for diff-gaussian-rasterization, which is required to install pyproject.toml-based projects And compiling simple_knn.h will encounter:\n1 2 simple_knn.cu:17:10: fatal error: cub/cub.cuh: No such file or directory #include \u0026lt;cub/cub.cuh\u0026gt; Possible method: Add cub to CmakeLists.txt. Nvidia-forum\nnvcc mismatch and maybe 10.2 doesn\u0026rsquo;t have that function yet.\nUbuntu 20.04 Install new cudatookit on Alien PC (Ubuntu 20.04) CUDA Toolkit 12.3 Downloads | NVIDIA Developer\n1 2 3 4 5 6 7 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2004-12-3-local_12.3.0-545.23.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-12-3-local_12.3.0-545.23.06-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2004-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda-toolkit-12-3 Install driver:\n1 2 sudo apt-get install -y cuda-drivers # reboot is required But nvcc -V still returns 10.2.\nI found there are multiple \u0026ldquo;cuda\u0026rdquo; on my Alien PC. Nv-Install Guide Linux\nUnder dir \u0026ldquo;/usr/local/\u0026rdquo;, there are \u0026ldquo;cuda/\u0026rdquo;, \u0026ldquo;cuda11/\u0026rdquo;, \u0026ldquo;cuda11.6/\u0026rdquo;, \u0026ldquo;cuda12/\u0026rdquo;, \u0026ldquo;cuda12.3/\u0026rdquo;.\nOnly one could be used. SO. And the default version may can be checked by: update-alternatives --display cuda Docs-Nv\nAdd 11.6 to PATH:\n1 2 3 export PATH=/usr/local/cuda-11.6/bin${PATH:+:${PATH}} export CUDADIR=/usr/local/cuda-11.6 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.6/lib64 Then nvcc -V change to 11.6.\nSuccess to create env on Alien PC with just the one line: conda env create --file environment.yml\nFor Lambda Server, although there is \u0026ldquo;cuda-11.2\u0026rdquo; folder in \u0026ldquo;/usr/local/\u0026rdquo;, but \u0026ldquo;bin/\u0026rdquo; doesn\u0026rsquo;t exist inside, so it can\u0026rsquo;t be added into PATH.\nTherefore, the full cudatoolkit 11.6 and compatible driver (as above) are required, instead of the subset cudatoolkit for runtime installed by conda.\nUbuntu 22.04 (2023-11-03)\nCUDA Toolkit 11.6 doesn\u0026rsquo;t have version for Ubuntu 22.04 CUDA Toolkit 11.8 corresponds PyTorch which is higher than 2.0.0 1 2 3 4 5 6 7 # nvcc -V # 11.8, Ubuntu 22.04, nvidia driver 545, 3090Ti conda create -n 3dgs python=3.9 conda activate 3dgs conda install pytorch==2.0.0 torchvision==0.15.0 pytorch-cuda=11.8 -c pytorch -c nvidia pip install plyfile tqdm ninja pip install submodules/diff-gaussian-rasterization/ pip install submodules/simple-knn/ SIBR viewer install fails (Ubuntu 22.04) #151\nTrain NeRF Synthetic\n1 python train.py -s ../nerf/data/nerf_synthetic/lego -r 8 (2024-04-15)\nTest DTU dataset:\nInstall Colmap.\nTo get the CUDA support on Linux, a local compilation is required. Docs\nSIBR viewer SIBR Core - Git Lab\nUbuntu 20.04 On Ubuntu 20.04, 1050Ti:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Get into submodule: cd SIBR_viewers # For Ubuntu 20.04, otherwise opencv version (4.2) mismatch 4.5: git checkout fossa_compatibility # Dependencies sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev # Project setup cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release # add -G Ninja to build faster # Don\u0026#39;t add \u0026#39;-j24\u0026#39;, otherwise crash? cmake --build build --target install Some details:\nInstall cmake by compiling source code:\nDownload tar.gz (cmake-.tar.gz), such as cmake-3.29.1.tar.gz, and extract: tar -zxvf cmake-{version number}.tar.gz\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Dependecy for OpenSSL sudo apt-get install libssl-dev cd cmake-{version number} # Check required dependencies: ./bootstrap # Build package: make # Install sudo make install cmake --version Ref How to Install CMake on Ubuntu 22.04 or 20.04 - LinuxCapable\nOpenCV issue on Ubuntu 20.04 resolved by switching branch to \u0026ldquo;fossa_compatibility\u0026rdquo;. Installing SIBR - cannot find correct version of OpenCV #10\nsm_30 error\nWith cudatoolkit 11.6 added to PATH, an error occurs:\n1 2 3 4 #$ ptxas -arch=sm_30 -m64 \u0026#34;tmp/CMakeCUDACompilerId.ptx\u0026#34; -o \u0026#34;tmp/CMakeCUDACompilerId.sm_30.cubin\u0026#34; ptxas fatal : Value \u0026#39;sm_30\u0026#39; is not defined for option \u0026#39;gpu-name\u0026#39; Check if old-version cuda exists: apt-cache policy nvidia-cuda-toolkit. 【已解决】 Compilation error ptxas fatal :\n1 2 3 4 5 6 7 nvidia-cuda-toolkit: Installed: 10.1.243-3 Candidate: 10.1.243-3 Version table: *** 10.1.243-3 500 500 http://ca.archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages 100 /var/lib/dpkg/status Remove it: sudo apt remove nvidia-cuda-toolkit\nThen, re-build works.\nRun local viewer\n1 ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/c777580e-9 Error (Real-time viewer requires CC \u0026gt; 7.0):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 (gaussian_splatting) yi@yi-Alienware-Aurora-R8:~/Downloads/gaussian-splatting$ ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/c777580e-9/ --no_interop [SIBR] -- INFOS --: Initialization of GLFW [SIBR] -- INFOS --: OpenGL Version: 4.6.0 NVIDIA 545.23.06[major: 4, minor: 6] Number of input Images to read: 300 Number of Cameras set up: 300 [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego. [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego.ply. [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/lego.obj. LOADSFM: Try to open /home/yi/Downloads/nerf/data/nerf_synthetic/legopoints3D.bin [SIBR] -- INFOS --: Error: can\u0026#39;t load mesh \u0026#39;/home/yi/Downloads/nerf/data/nerf_synthetic/legopoints3D.bin. [SIBR] !! WARNING !!: FILE /home/yi/Downloads/gaussian-splatting/SIBR_viewers/src/core/scene/ProxyMesh.cpp LINE 29, FUNC loadFromData proxy model not found at /home/yi/Downloads/nerf/data/nerf_synthetic/lego [SIBR] ## ERROR ##: FILE /home/yi/Downloads/gaussian-splatting/SIBR_viewers/src/projects/gaussianviewer/renderer/GaussianView.cpp LINE 339, FUNC GaussianView Sorry, need at least compute capability 7.0+!terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): See log for message errors Aborted (core dumped) Even the 1080 Ti\u0026rsquo;s compute capacity is only 6.1 ? Your GPU Compute Capability Ubuntu 22.04 (2023-11-04)\n1 2 3 4 5 6 # Dependencies sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev # Project setup cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release -G Ninja cmake --build build -j24 --target install Train: python train.py -s ../Dataset/nerf_synthetic/lego\nReal-time viewer: sudo ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m output/4ee28b3c-9\nRun Network viewer\n1 2 3 4 5 6 7 8 (3dgs) z@homepc:~/Downloads/gaussian-splatting/SIBR_viewers$ ./install/bin/SIBR_remoteGaussian_app [SIBR] -- INFOS --: Initialization of GLFW [SIBR] ## ERROR ##: FILE /home/z/Downloads/gaussian-splatting/SIBR_viewers/src/core/graphics/Window.cpp LINE 30, FUNC glfwErrorCallback X11: The DISPLAY environment variable is missing terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): See log for message errors Aborted (core dumped) Ubuntu 18.04 (2024-04-05)\nThe current cmake on server is 3.10, which is too old.\nUsing the pre-built cmake cmake-3.29.1-linux-x86_64.tar.gz will lead to error.\n1 2 3 4 5 6 wget https://github.com/Kitware/CMake/releases/download/v3.29.1/cmake-3.29.1-linux-x86_64.tar.gz tar -zxvf cmake-3.29.1-linux-x86_64.tar.gz # Build SIBR cd ./Downloads/gaussian-splatting/SIBR_viewers /home/z/Downloads/cmake-3.29.1-linux-x86_64/bin/cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release Error 1 2 3 4 5 6 7 8 CMake Error at /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Call Stack (most recent call first): /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) /home/z/Downloads/cmake-3.29.1-linux-x86_64/share/cmake-3.29/Modules/FindGLEW.cmake:242 (find_package_handle_standard_args) cmake/linux/dependencies.cmake:69 (FIND_PACKAGE) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Using local cmake to compile remote folder, which is mounted via sshfs doesn\u0026rsquo;t work neither:\n1 2 3 4 (base) yi@yi:/mnt/Server/Downloads/gaussian-splatting/SIBR_viewers$ cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release CMake Error: The current CMakeCache.txt directory /mnt/Server/Downloads/gaussian-splatting/SIBR_viewers/CMakeCache.txt is different than the directory /home/z/Downloads/gaussian-splatting/SIBR_viewers where CMakeCache.txt was created. This may result in binaries being created in the wrong place. If you are not sure, reedit the CMakeCache.txt Clone the CMake repo and build it. git clone https://github.com/Kitware/CMake.git\nInstall lib for ssl: sudo apt-get install libssl-dev to avoid SSL error:\n1 2 3 4 -- Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the system variable OPENSSL_ROOT_DIR (missing: OPENSSL_CRYPTO_LIBRARY OPENSSL_INCLUDE_DIR) CMake Error at Utilities/cmcurl/CMakeLists.txt:644 (message): Could not find OpenSSL. Install an OpenSSL development package or configure CMake with -DCMAKE_USE_OPENSSL=OFF to build without OpenSSL. 1 2 3 cd CMake/ mkdir build \u0026amp;\u0026amp; cd build ../bootstrap \u0026amp;\u0026amp; make The using cmake to compile project:\n1 2 cd ~/Downloads/gaussian-splatting/SIBR_viewers ../../CMake/build/bin/cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release But this still encounters the same error as above.\n1 2 CMake Error at /home/z/Downloads/CMake/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Copy the compiled executable files onto system.\n1 2 cd sudo make install Cmake 3.29 gets installed (copied) into:\n1 2 3 4 5 6 7 8 9 10 11 /usr/local/doc/cmake-3.29/cmsys/Copyright.txt ... ... ... -- Installing: /usr/local/share/cmake-3.29/Modules/ ... -- Installing: /usr/local/share/cmake-3.29/Templates ... -- Installing: /usr/local/share/vim/vimfiles/indent ... -- Installing: /usr/local/share/emacs/site-lisp/cmake-mode.el -- Installing: /usr/local/share/aclocal/cmake.m4 -- Installing: /usr/local/share/bash-completion/completions/ Relaunch a terminal. Check the version:\n1 2 3 4 (base) z@lambda-server:~/Downloads/gaussian-splatting$ cmake --version cmake version 3.29.20240405-gc2949db CMake suite maintained and supported by Kitware (kitware.com/cmake). Build SIBR:\n1 2 3 4 5 git checkout fossa_compatibility git checkout master cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release The error persists!! as below: (Note: The following error corresponds to the master branch.)\n1 2 3 4 5 6 7 8 CMake Error at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find GLEW (missing: GLEW_INCLUDE_DIRS GLEW_LIBRARIES) Call Stack (most recent call first): /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) /usr/local/share/cmake-3.29/Modules/FindGLEW.cmake:245 (find_package_handle_standard_args) cmake/linux/dependencies.cmake:69 (FIND_PACKAGE) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Install dependencies:\n1 sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev # libembree-dev It prompts that E: Unable to locate package ibembree-dev.\nThen, the building will prompts error about libboost:\n1 2 3 4 5 6 7 8 9 10 CMake Error at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find Boost: Found unsuitable version \u0026#34;1.65.1\u0026#34;, but required is at least \u0026#34;1.71.0\u0026#34; (found /usr/include, found components: system chrono filesystem date_time) Call Stack (most recent call first): /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:598 (_FPHSA_FAILURE_MESSAGE) /usr/local/share/cmake-3.29/Modules/FindBoost.cmake:2394 (find_package_handle_standard_args) cmake/dependencies.cmake:173 (find_package) cmake/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) The header and libraries of libboost are in:\n1 2 3 -- Found Boost: /usr/include (found version \u0026#34;1.65.1\u0026#34;) -- Boost_INCLUDE_DIRS: /usr/include -- Boost_LIBRARY_DIRS: /usr/lib/x86_64-linux-gnu These information is return by adding command into CMakeLists.txt (guided by chatGPT):\n1 2 3 find_package(Boost REQUIRED) message(STATUS \u0026#34;Boost_INCLUDE_DIRS: ${Boost_INCLUDE_DIRS}\u0026#34;) message(STATUS \u0026#34;Boost_LIBRARY_DIRS: ${Boost_LIBRARY_DIRS}\u0026#34;) I don\u0026rsquo;t know how to update libboost.\nI found the CMakeLists.txt varies between different branch.\nThe branch of SIBR used by 3DGS is gaussian_code_release_union (4ae964a267):\n1 2 3 git checkout gaussian_code_release_union cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release Warnings about embree, and errors for OpenCV4.5:\nError 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Warnings CMake Warning at cmake/linux/dependencies.cmake:127 (find_package): By not providing \u0026#34;Findembree.cmake\u0026#34; in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \u0026#34;embree\u0026#34;, but CMake did not find one. Could not find a package configuration file provided by \u0026#34;embree\u0026#34; (requested version 3.0) with any of the following names: embreeConfig.cmake embree-config.cmake Add the installation prefix of \u0026#34;embree\u0026#34; to CMAKE_PREFIX_PATH or set \u0026#34;embree_DIR\u0026#34; to a directory containing one of the above files. If \u0026#34;embree\u0026#34; provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) # Errors: There is no provided OpenCV library for your compiler, relying on find_package to find it CMake Error at cmake/linux/dependencies.cmake:248 (find_package): Could not find a configuration file for package \u0026#34;OpenCV\u0026#34; that is compatible with requested version \u0026#34;4.5\u0026#34;. The following configuration files were considered but not accepted: /usr/share/OpenCV/OpenCVConfig.cmake, version: 3.2.0 Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Then I change to branch fossa_compatibility:\n1 2 3 git checkout fossa_compatibility cd SIBR_viewers cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release The error of OpenCV4 occurs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CMake Warning (dev) at /usr/local/share/cmake-3.29/Modules/FindPackageHandleStandardArgs.cmake:438 (message): The package name passed to `find_package_handle_standard_args` (EMBREE) does not match the name of the calling package (embree). This can lead to problems in calling code that expects `find_package` result variables (e.g., `_FOUND`) to follow a certain pattern. Call Stack (most recent call first): cmake/linux/Modules/Findembree.cmake:87 (FIND_PACKAGE_HANDLE_STANDARD_ARGS) cmake/linux/dependencies.cmake:127 (find_package) cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) This warning is for project developers. Use -Wno-dev to suppress it. -- EMBREE wasn\u0026#39;t found correctly. Set EMBREE_DIR to the root SDK installation directory. (missing: EMBREE_INCLUDE_DIR EMBREE_LIBRARIES) There is no provided OpenCV library for your compiler, relying on find_package to find it CMake Error at cmake/linux/dependencies.cmake:248 (find_package): Could not find a configuration file for package \u0026#34;OpenCV\u0026#34; that is compatible with requested version \u0026#34;4\u0026#34;. The following configuration files were considered but not accepted: /usr/share/OpenCV/OpenCVConfig.cmake, version: 3.2.0 Call Stack (most recent call first): cmake/linux/include_once.cmake:20 (include) src/CMakeLists.txt:46 (include_once) Authors said the SIBR has no support for Ubuntu 18. Related issues with searching \u0026ldquo;18.04\u0026rdquo;\n","date":"2023-05-14T13:41:00Z","image":"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.bfDD9_L28UK7AsADDzB8vwHaEo%26pid%3DApi\u0026f=1\u0026ipt=3b34b7952e1e03305e5c085185ede1ad09e8edc71e9c1161a33c17b6b9f2b3ac\u0026ipo=images","permalink":"https://zichen34.github.io/writenotes/model/splat/b-note-3dgs-read/","title":"read: NVS - 3D Gaussian Splatting"},{"content":"torch.utils.data.IterableDataset pytorch forum 2020-02-26; Docs\nConcatDataset Docs\nExample 1: Pytorch DataLoader multiple data source - SO;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import os import torch.utils.data as data class SingeJsonDataset(data.Dataset): # implement a single json dataset here... ... list_of_datasets = [] for j in os.path.listdir(root_dir): if not j.endswith(\u0026#39;.json\u0026#39;): continue # skip non-json files list_of_datasets.append(SingeJsonDataset(json_file=j, root_dir=root_dir, transform=None)) # once all single json datasets are created you can concat them into a single one: multiple_json_dataset = data.ConcatDataset(list_of_datasets) Example 2: PyTorch forum - Praateek\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class LazyTextDataset(Dataset): def __init__(self, filename): self._filename = filename self._total_data = int(subprocess.check_output(\u0026#34;wc -l \u0026#34; + filename, shell=True).split()[0]) - 1 def __getitem__(self, idx): line = linecache.getline(self._filename, idx + 1) csv_line = csv.reader([line]) return next(csv_line) def __len__(self): return self._total_data path = /where_csv_files_are_dumped/ files = list(map(lambda x : path + x, (filter(lambda x : x.endswith(\u0026#34;csv\u0026#34;), os.listdir(path))))) datasets = list(map(lambda x : LazyTextDataset(x), files)) dataset = ConcatDataset(datasets) Comments of Thomans Ahle:\nThe problem with ConcatDataset is that it doesn\u0026rsquo;t work with multiprocessing. It calls len(ds) on each dataset in it\u0026rsquo;s initializer, so you end up loading every dataset in the main process.\nnp.load(path, mmap_mode=\u0026lsquo;r\u0026rsquo;) Load multiple .npy files (size \u0026gt; 10GB) in pytorch - SO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np import torch from bisect import bisect import os, psutil # used to monitor memory usage class BigDataset(torch.utils.data.Dataset): def __init__(self, data_paths, target_paths): self.data_memmaps = [np.load(path, mmap_mode=\u0026#39;r\u0026#39;) for path in data_paths] self.target_memmaps = [np.load(path, mmap_mode=\u0026#39;r\u0026#39;) for path in target_paths] self.start_indices = [0] * len(data_paths) self.data_count = 0 for index, memmap in enumerate(self.data_memmaps): self.start_indices[index] = self.data_count self.data_count += memmap.shape[0] def __len__(self): return self.data_count def __getitem__(self, index): memmap_index = bisect(self.start_indices, index) - 1 index_in_memmap = index - self.start_indices[memmap_index] data = self.data_memmaps[memmap_index][index_in_memmap] target = self.target_memmaps[memmap_index][index_in_memmap] return index, torch.from_numpy(data), torch.from_numpy(target) # Test Code if __name__ == \u0026#34;__main__\u0026#34;: data_paths = [f\u0026#39;data/d{index}.npy\u0026#39; for index in range(10)] target_paths = [f\u0026#39;data/s{index}.npy\u0026#39; for index in range(10)] process = psutil.Process(os.getpid()) memory_before = process.memory_info().rss dataset = BigDataset(data_paths, target_paths) used_memory = process.memory_info().rss - memory_before print(\u0026#34;Used memory:\u0026#34;, used_memory, \u0026#34;bytes\u0026#34;) dataset_size = len(dataset) print(\u0026#34;Dataset size:\u0026#34;, dataset_size) print(\u0026#34;Samples:\u0026#34;) for sample_index in [0, dataset_size//2, dataset_size-1]: print(dataset[sample_index]) ","date":"2023-04-21T09:57:00Z","permalink":"https://zichen34.github.io/writenotes/lib/load_multi_files/","title":"memo: Datasets load multiple data files"},{"content":"Callbacks I asked chat-GPT: \u0026ldquo;How to save the intermediate tensor in a tensorflow model?\u0026rdquo;.\ntf.keras.callbacks.LambdaCallback class will call a custom function after each epoch or each batch or the begining/ending of traininng. Docs.\nCallbacks are useful to get a view on internal states and statistics of the model during training. Docs: Writing your own callbacks\nExample of callbacks in model.fit() refer to: 7;\nHowever, the NeRF model constitutes low-level operations without implementing the method model.fit(). Then the callback functions need to be called manually at the appropriate points in the training loop:\nAsking Chat-GPT: \u0026ldquo;Please do not use model.fit(). Make the model and callback in low-level options. Please given an example of using callbacks and using TensorFlow 1.15.\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras.callbacks import LambdaCallback tf.enable_eager_execution() # Define the model architecture model = tf.keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation=\u0026#39;relu\u0026#39;), Dense(10) ]) # Define the loss function and optimizer loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = tf.keras.optimizers.Adam() # Define the learning rate decay schedule def lr_decay(epoch, lr): if epoch % 10 == 0 and epoch \u0026gt; 0: return lr * 0.1 else: return lr # Define the callback to update the learning rate lr_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: optimizer.lr.assign(lr_decay(epoch, optimizer.lr))) # Define a function to save the intermediate tensor def save_intermediate_tensor(batch, logs): intermediate_tensor = model.layers[2].output tf.summary.scalar(\u0026#39;intermediate_tensor\u0026#39;, intermediate_tensor, step=epoch) save_callback = LambdaCallback(on_batch_end=save_intermediate_tensor) # Load the MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # Normalize the pixel values x_train = x_train[:100] / 255.0 x_test = x_test[:100] / 255.0 y_test = y_test[:100] # Compile the model for model.evaluate() model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\u0026#39;accuracy\u0026#39;]) # Train the model for epoch in range(10): for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train, y_train)): x_batch_train = x_batch_train.reshape((-1, 28, 28)) y_batch_train = y_batch_train.reshape((-1,)) with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) # Call the save callback at the end of each batch save_callback.on_batch_end(batch,{}) # Call the learning rate callback at the end of each epoch lr_callback.on_epoch_end(epoch, {\u0026#39;lr\u0026#39;: optimizer.lr.numpy()}) # Evaluate the model on the test set test_loss, test_acc = model.evaluate(x_test, y_test) print(\u0026#39;Epoch {}, Test Loss: {:.4f}, Test Accuracy: {:.4f}\u0026#39;.format(epoch+1, test_loss, test_acc)) Error: couldn\u0026rsquo;t find summary writer\nNotFoundError: Resource localhost/logdir:./logdir/exp1/N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:FlushSummaryWriter]\nSolution: Restart the python kernel. tf issue#47100 tf.io.write_file() How to save the value of a tensor in Tensorflow found by 1\n1 2 one_string = tf.strings.as_string(tensor) tf.io.write_file(filename, one_string) Problem: the rank of tensor has to be 0? tf TFRecordWriter How do you save a Tensorflow dataset to a file? found by 1\nThis is for making (x,y) dataset. io_ops._save() Is there a way to save an intermediate output in Tensorflow to a file? found by 2\nMay have been deprecated. Create a new model Obtaining output of an Intermediate layer in TensorFlow/Keras found by 2\nGet values of KerasTensor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import tensorflow as tf def init_nerf_model(): ... print(output.numpy()) # AttributeError: \u0026#39;KerasTensor\u0026#39; object has no attribute \u0026#39;numpy\u0026#39; model = tf.keras.Model(inputs=inputs, outputs=outputs) return model model= init_nerf_model(input_ch=63, input_ch_views=27, use_viewdirs=True) # model.layers # Check memory address of each layer temp_mode = tf.keras.Model(model.input, model.layers[11].output) temp_mode.summary() # print description of each layer Oiginal NeRF model summary:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Model: \u0026#34;model\u0026#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 90)] 0 [] tf.split (TFOpLambda) [(None, 63), 0 [\u0026#39;input_1[0][0]\u0026#39;] (None, 27)] dense (Dense) (None, 256) 16384 [\u0026#39;tf.split[0][0]\u0026#39;] dense_1 (Dense) (None, 256) 65792 [\u0026#39;dense[0][0]\u0026#39;] dense_2 (Dense) (None, 256) 65792 [\u0026#39;dense_1[0][0]\u0026#39;] dense_3 (Dense) (None, 256) 65792 [\u0026#39;dense_2[0][0]\u0026#39;] dense_4 (Dense) (None, 256) 65792 [\u0026#39;dense_3[0][0]\u0026#39;] tf.concat (TFOpLambda) (None, 319) 0 [\u0026#39;tf.split[0][0]\u0026#39;, \u0026#39;dense_4[0][0]\u0026#39;] dense_5 (Dense) (None, 256) 81920 [\u0026#39;tf.concat[0][0]\u0026#39;] dense_6 (Dense) (None, 256) 65792 [\u0026#39;dense_5[0][0]\u0026#39;] dense_7 (Dense) (None, 256) 65792 [\u0026#39;dense_6[0][0]\u0026#39;] dense_9 (Dense) (None, 256) 65792 [\u0026#39;dense_7[0][0]\u0026#39;] tf.concat_1 (TFOpLambda) (None, 283) 0 [\u0026#39;dense_9[0][0]\u0026#39;, \u0026#39;tf.split[0][1]\u0026#39;] dense_10 (Dense) (None, 128) 36352 [\u0026#39;tf.concat_1[0][0]\u0026#39;] dense_11 (Dense) (None, 3) 387 [\u0026#39;dense_10[0][0]\u0026#39;] dense_8 (Dense) (None, 1) 257 [\u0026#39;dense_7[0][0]\u0026#39;] tf.concat_2 (TFOpLambda) (None, 4) 0 [\u0026#39;dense_11[0][0]\u0026#39;, \u0026#39;dense_8[0][0]\u0026#39;] ================================================================================================== Total params: 595,844 Trainable params: 595,844 Non-trainable params: 0 Only keep the MLP part:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Model: \u0026#34;model_1\u0026#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 90)] 0 [] tf.split (TFOpLambda) [(None, 63), 0 [\u0026#39;input_1[0][0]\u0026#39;] (None, 27)] dense (Dense) (None, 256) 16384 [\u0026#39;tf.split[0][0]\u0026#39;] dense_1 (Dense) (None, 256) 65792 [\u0026#39;dense[0][0]\u0026#39;] dense_2 (Dense) (None, 256) 65792 [\u0026#39;dense_1[0][0]\u0026#39;] dense_3 (Dense) (None, 256) 65792 [\u0026#39;dense_2[0][0]\u0026#39;] dense_4 (Dense) (None, 256) 65792 [\u0026#39;dense_3[0][0]\u0026#39;] tf.concat (TFOpLambda) (None, 319) 0 [\u0026#39;tf.split[0][0]\u0026#39;, \u0026#39;dense_4[0][0]\u0026#39;] dense_5 (Dense) (None, 256) 81920 [\u0026#39;tf.concat[0][0]\u0026#39;] dense_6 (Dense) (None, 256) 65792 [\u0026#39;dense_5[0][0]\u0026#39;] dense_7 (Dense) (None, 256) 65792 [\u0026#39;dense_6[0][0]\u0026#39;] dense_9 (Dense) (None, 256) 65792 [\u0026#39;dense_7[0][0]\u0026#39;] ================================================================================================== Total params: 558,848 Trainable params: 558,848 Non-trainable params: 0 Ref DDG search: save tensorflow tensor to file DDG search: save intermediate tensor from tensorflow model A Guide to TensorFlow Callbacks ","date":"2023-04-17T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf_save_tensors/","title":"memo: TF | Save Tensors"},{"content":"Via conda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Check the version of the CUDA: $ conda list cudatookit # Name Version Build Channel cudatoolkit 10.1.243 h8cb64d8_10 conda-forge $ conda list cudnn: # Name Version Build Channel cudnn 7.6.5.32 hc0a50b0_1 conda-forge # install/update CUDA and CUDNN through conda: $ conda install -c anaconda cudatoolkit $ conda install -c anaconda cudnn # show the highest supported CUDA version? $ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 | # $ nvcc --version Refer to 1; 3\nAnother way to check the version of cuDNN 4:\n1 cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2 Via PyTorch 1 2 3 import torch print(torch.version.cuda) \u0026gt;\u0026gt;\u0026gt; 10.2 Via TensorFlow 1 2 3 4 5 import tensorflow as tf sys_details = tf.sysconfig.get_build_info() cuda_version = sys_details[\u0026#34;cuda_version\u0026#34;] print(cuda_version) Not quiet right. Refer to How to check CUDA version in TensorFlow - gcptutorials\nCompatible combinations for TF Refer to Docs; 2;\nCompatible CUDA for cards Determine the driver version first and then determine the cuda version. Genearlly, cuda is backward compatible with the driver. Nvidia-page\nRef get the CUDA and CUDNN version on windows with Anaconda installed searched by DDG: \"What is the version of CUDA and cuDNN\" Which TensorFlow and CUDA version combinations are compatible? How to get the CUDA version? [NV] How to check CUDA and cuDNN version | by totokk | Medium ","date":"2023-04-09T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/check_cuda_version/","title":"memo: Check Versions of CUDA and cuDNN"},{"content":"tf 1.x enable_eager_execution() Invoke the function tf.enable_eager_execution() at program startup, and then you can use the .numpy() method. ¹\n1 2 3 import tensorflow as tf tf.compat.v1.enable_eager_execution() pred_id = tf.multinomial(tf.exp(preds), num_samples=1)[0][0].numpy() (eager execution is enabled by default in TensorFlow 2.0.)\ntf.get_static_value(x) Answer from ¹. Link to docs\n1 tf.get_static_value( tensor, partial=False ) Create sess Without enabling the eager execution, a tensor can be evaluated in a session. The session can be created as a runtime context by with tf.Session() as sess:, or using InteractiveSession() in place ⁴.\nThe value of a Tensor can only be obtained after calling .eval(), because Tensor (in TF) is of lazy evaluation. ²\n1 2 3 4 5 6 7 8 9 10 import tensorflow as tf a = tf.constant([1, 1.5, 2.5], dtype=tf.float32) b = tf.constant([1, -2, 3], dtype=tf.float32) c = a * b with tf.Session() as sess: result = c.eval() print(result) Use InteractiveSession() Initialize an InteractiveSession as the default session. The following snippet is from 2.\n1 2 3 4 5 6 7 8 9 import tensorflow as tf a = tf.constant([1, 1]) # SparseTensor，只存储值及其对应的index b = tf.constant([2, 2]) c = tf.add(a, b) # Tensor，完整的矩阵 sess = tf.InteractiveSession() # v1.15 print(\u0026#34;a[0]=%s, a[1]=%s\u0026#34; % (a[0].eval(), a[1].eval())) print(\u0026#34;c = %s\u0026#34; % c.eval()) sess.close() Session.run() 通过 Session.run() 获取变量的值 6, 4\n1 2 3 4 5 6 x = tf.placeholder(tf.float32) y = tf.placeholder(tf.float32) bias = tf.Variable(1.0) y_pred = x**2 + bias loss = (y - y_pred)**2 print(\u0026#39;Loss(x,y) = %.3f\u0026#39; % session.run(loss, {x:3.0, y:9.0})) # 程序开头已定义sess 报错：\n1 2 3 4 5 6 FailedPreconditionError: 2 root error(s) found. (0) Failed precondition: Attempting to use uninitialized value Variable_1 [[{{node Variable_1/read}}]] [[pow_3/_33]] (1) Failed precondition: Attempting to use uninitialized value Variable_1 [[{{node Variable_1/read}}]] 原因： bias is a tf.Variable which needs initialization 5, like:\n1 2 3 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(session.run(loss, {x:3.0, y:9.0}) tf.keras.backend.get_value(x) Answer form DesiKeki⁴. Link to docs\n1 2 3 4 print(product) \u0026gt;\u0026gt;tf.Tensor([[12.]], shape=(1, 1), dtype=float32) print(tf.keras.backend.get_value(product)) \u0026gt;\u0026gt;[[12.]] tf.print() Python print() cannot be run in a tf.Graph, but tf.print() can be. tf_guide\n1 2 3 4 5 @tf.function def double(a): print(\u0026#34;Tracing with\u0026#34;, a) # only be executed when (re)tracing tf.print(\u0026#34;Executing with\u0026#34;, a) # executed every time the graph is called return a + a doubt: tf.print 用法？ \u0026ldquo;可以在计算一个变量的同时，指定打印一组变量\u0026rdquo;(不懂) tensorflow杂记 2016\nUse tf.print (instead of deprecated tf.Print). It returns None when executing eagerly. how to use tf.print (not tf.Print) in high-level Estimator api\nAttributeError: 'Tensor' object has no attribute '_datatype_enum' when executing: tf.print(inputs, output_stream=sys.stderr)\n(Deprecated) tf.Print() The pythonic print(feat_tensor) will only print the description of tensors once when the input function graph is constructed.\ntf.Print() is a graph node that splices (打结) the \u0026ldquo;print call\u0026rdquo; into the graph by taking the tensor you wanna print as input, and output a \u0026ldquo;same\u0026rdquo; variable that will be passed to the later node in the graph.\n1 2 3 4 5 6 7 def input_fn(dataset): def _fn(): feat_tensor = tf.constant(dataset.data) feat_tesnor = tf.Print(feat_tensor, data=[feat_tensor, tensor2, tensor3], # feat_tensor will be printed \u0026amp; returned, tensor2,tensor3 are printed btw. message=\u0026#34;Inputs are: \u0026#34;) feat = {feat_name: feat_tensor} Using tf.Print() in TensorFlow -Yufeng Guo\n(2023-04-16)\nLambdaCallback doesn\u0026rsquo;t work sumNotes\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras.callbacks import LambdaCallback tf.enable_eager_execution() # Define the model architecture model = tf.keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation=\u0026#39;relu\u0026#39;), Dense(10) ]) # Define the loss function and optimizer loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = tf.keras.optimizers.Adam() # Define a function to save the intermediate tensor def save_intermediate_tensor(epoch,): intermediate_tensor = model.layers[0].output # Replace this with the layer whose output you want to save print(intermediate_tensor) # only show the description once # tf.print(intermediate_tensor) # AttributeError: \u0026#39;Tensor\u0026#39; object has no attribute \u0026#39;_datatype_enum\u0026#39; # np.savez(f\u0026#39;vec{epoch}\u0026#39;, intermediate_tensor.numpy()) # AttributeError: \u0026#39;Tensor\u0026#39; object has no attribute \u0026#39;numpy\u0026#39; (with egar execution enabled) # tf.io.write_file(f\u0026#39;vec{epoch}\u0026#39;, tf.strings.as_string(intermediate_tensor)) # rank has to be 0 save_callback = LambdaCallback(on_epoch_end=save_intermediate_tensor) # Load the MNIST dataset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # Normalize the pixel values x_train = x_train[:100] / 255.0 # Compile the model model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\u0026#39;accuracy\u0026#39;]) # Train the model for epoch in range(10): for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train, y_train)): x_batch_train = x_batch_train.reshape((-1, 28, 28)) y_batch_train = y_batch_train.reshape((-1,)) with tf.GradientTape() as tape: logits = model(x_batch_train) loss_value = loss_fn(y_batch_train, logits) grads = tape.gradient(loss_value, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) save_callback.on_epoch_end(epoch) DDG search: \u0026lsquo;Tensor\u0026rsquo; object has no attribute \u0026rsquo;numpy\u0026rsquo;\nTF 2.0 \u0026lsquo;Tensor\u0026rsquo; object has no attribute \u0026rsquo;numpy\u0026rsquo; while using .numpy() although eager execution enabled by default#27519\nTF 2.x @tf.function Create a dataflow graph for a Python function. Then its outputs can be printed. Docs\n1 2 3 4 5 6 @tf.function def foo(): a = tf.random.uniform([1], maxval=5, seed=1) b = tf.random.uniform([1], maxval=5, seed=1) return a+b print(foo()) # 2*a Ref AttributeError: 'Tensor' object has no attribute 'numpy' - SO Tensorflow 的 Tensor 和 OpKernel 分析-高鹏 在TensorFlow中怎么打印Tensor对象的值 How to print the value of a Tensor object in TensorFlow? - SO FailedPreconditionError: Attempting to use uninitialized in Tensorflow - SO Tensorflow之调试(Debug)及打印变量 - Shiyu_Huang -博客园 ","date":"2023-04-08T10:48:00-05:00","permalink":"https://zichen34.github.io/writenotes/lib/tf_print_tensor/","title":"memo: TF | Print Tensors"},{"content":"Source video: 【俗说矩阵】逆矩阵的内涵原来这么丰富！快来看看吧！\nInverse of diagonal matrix A diagonal matrix with the given values: 2,3,4 as follows.\n$$ 𝐀 = \\text{diag\\{2, 3, 4\\}} = \\begin{bmatrix} 2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 4 \\end{bmatrix} $$\nThe augmented matrix is constructed and perform elementary row operations as:\n$$ 𝐀|𝐈 = \\begin{bmatrix} 2 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 4 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\overset{1/2r1,1/3r2, 1/4r3}{⇢} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 1/2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; | 0 \u0026amp; 1/3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1/4 \\end{bmatrix} $$\nFrom the results, the inverse matrix 𝐀⁻¹ of a diagonal matrix is also a diagonal matrix. And its elements on the main diagonal are the reciprocal of elements on the original diagonal, where the element cannot be 0.\n$$ 𝐀⁻¹ = \\begin{bmatrix} 1/2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1/3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1/4 \\end{bmatrix} = \\text{diag\\{1/2, 1/3, 1/4\\}} $$\nInverse of a scalar matrix If all elements on the diagonal are the same, the matrix is a schalar matrix. 𝚲 = diag{λ, \u0026hellip;, λ}, λ≠0.\nIts inverse matrix is a scalar matrix where the elements on the diagonal are all 1/λ, 𝚲⁻¹ = diag{λ⁻¹, \u0026hellip;, λ⁻¹}, λ≠0.\nIf λ = 1, the scalar matrix is identity matrix 𝐈. So the inverse of a identity matrix is itself: 𝐈⁻¹ = 𝐈.\nInverse of the elementary matrix The elementary matrix is definitely invertible. So their inverse matrices must exist.\nInverse of a row-switching matrix For a row-switching matrix (permutation matrix) switching the first 2 rows of the identity matrix:\n$$ 𝐅₁ = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nWrite the augmented matrix: $$ 𝐅₁|𝐈 = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \u0026amp;| 1 \u0026amp; 0 \u0026amp;0\\\\ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp;| 0 \u0026amp; 1 \u0026amp;0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp;| 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} $$\nThe inverse matrix can be obtained by switching the first two rows and letting the left part become a identity matrix. Then the inverse matrix is the right part:\n$$ 𝐅₁⁻¹ = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp;0\\\\ 1 \u0026amp; 0 \u0026amp;0\\\\ 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} $$\nIt indicates that the inverse of a row-switching matrix is itself. (Just like the identity matrix, its inverse is itself.)\nInverse of a row-multiplying matrix For a row-multiplying matrix scaling the second rows by 2 times:\n$$ 𝐅₂ = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nTherefore, its inverse is obtained by multiplying 1/2 onto the 2nd row of the identity matrix.\n$$ 𝐅₂⁻¹ = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1/2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThat means the inverse of a row-multiplying matrix will still be a row-multiplying matrix. But the scale factor becomes the reciprocal.\nInverse of a row-addition matrix For a row-addition matrix: the second row is added by 2-times first row. $$ 𝐅₃ = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 2 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nIts inverse is obtained by lettingthe 2nd row in the identity matrix add the -2-times of the first row:\n$$ 𝐅₃⁻¹ = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ -2 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThus, its inverse is just changing the scale factor to its opposite.\nIn summary, the inverse matrices of elementry matrices are the same type.\nThe inverse of a row-switching matrix is itself. The inverse of a row-multiplication matrix is changing the scale factor to its reciprocal. The inverse of a row-addition matrix is changing the scale factor to its opposite. Inverse of second-order matrix The number of rows and columns of a second-order matrix are both 2.\n","date":"2023-04-03T16:17:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/13_%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E6%89%A9%E5%B1%95%E5%BA%94%E7%94%A8/","title":"watch: LA - 高山 13 | Application of Inverse Matrix"},{"content":"Source video: 【俗说矩阵】初等变换求逆矩阵原来是这个原理！数学老师看后大赞！\n𝐀 can perform 9 elementary row operations to reach the identity matrix 𝐈.\n$$ 𝐀 = [^{_{1\\ 3\\ 5}} _{^{2\\ 4\\ 6} _{1\\ 4\\ 9}}] ➔ [^{_{1\\ 3\\ 5}} _{^{1\\ 2\\ 3} _{1\\ 4\\ 9}}] ➔ [^{_{1\\ 2\\ 3}} _{^{1\\ 3\\ 5} _{1\\ 4\\ 9}}] ➔ [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{1\\ 4\\ 9}}] ➔ [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 2\\ 6}}] ➔ [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 0\\ 2}}] ➔ [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 0\\ 1}}] ➔ [^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] ➔ [^{_{1\\ 2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] ➔ [^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] = 𝐈 $$\n𝐀 is multiplied with 9 elementary matrix on the left: 𝐈 = 𝐅₉𝐅₈\u0026hellip;𝐅₁𝐀\nThe product of these 9 elementary matrix is called 𝐀⁻¹, the inverse of 𝐀. So 𝐀⁻¹𝐀 = 𝐈, and 𝐀⁻¹ = 𝐅₉𝐅₈\u0026hellip;𝐅₁.\n𝐀⁻¹ will not change if it\u0026rsquo;s multiplied with 𝐈: 𝐀⁻¹ = 𝐅₉𝐅₈\u0026hellip;𝐅₁𝐈\nThis equation indicates that an identity matrix 𝐈 ($[^{_{1\\ 0\\ 0}}_{^{0\\ 1\\ 0}_{0\\ 0\\ 1}}]$) can perform exactly the same 9 elementary row operations to reach the inverse matrix 𝐀⁻¹ = $[^{_{-3\\ 7/4\\ 1/2}} _{^{ 3\\ -1\\ -1}_{-1\\ 1/4\\ 1/2}}]$.\n$$ \\begin{array}{c} 𝐀 = \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 2 \u0026amp; 4 \u0026amp; 6 \\\\ 1 \u0026amp; 4 \u0026amp; 9 \\end{bmatrix} \\rm \\overset{\\text{9 elementary row}}{operations→} 𝐈 = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\\\ 𝐈 = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\rm \\overset{\\text{9 elementary row}}{operations→} 𝐀⁻¹ = \\begin{bmatrix} -3 \u0026amp; 7/4 \u0026amp; 1/2 \\\\ 3 \u0026amp; -1 \u0026amp; -1 \\\\ -1 \u0026amp; 1/4 \u0026amp; 1/2 \\end{bmatrix} \\end{array} $$\nIf an matrix 𝐀 can perform multiple elementary row operations to become an identity matrix 𝐈, then 𝐈 can perform the same 9 elementary row operations simultaneously to reach the inverse matrix 𝐀⁻¹.\nIn another word, the transformation from 𝐈 to 𝐀⁻¹ replicates the 9 elementary row opertations from 𝐀 to 𝐈.\nAugmented matrix performs elementary row operations To keep the sequence of operations the same, the augmented matrix 𝐀|𝐈 is leveraged.\n$$ 𝐀|𝐈 = \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 2 \u0026amp; 4 \u0026amp; 6 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 4 \u0026amp; 9 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThis augmented matrix 𝐀|𝐈 can perform 9 elementary matrix to make the left part of the vertical line become a identity matrix. Such that the right part of the vertical line becomes the inverse matrix 𝐀⁻¹.\n$$ 𝐀|𝐈 = 𝐈|𝐀⁻¹ $$\nThis method is applicable to the matrix that its elements are given.\nExample Given a matrix 𝐀 = $[^{_{1\\ 1\\ 2}} _{^{1\\ 2\\ 3} _{2\\ 4\\ 5}}]$, solve the inverse 𝐀⁻¹.\nConstruct the augmented matrix:\n$$ 𝐀|𝐈 = \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nPerform elementary row operations on this augmented matrix to make the left part become 𝐈\n$$ \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 0 \u0026amp; 1 \u0026amp; 0 \\\\ 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; | 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} →^{\\text{row echelon}}_{form}: \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; | 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; | -1 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} →^{\\text{clear top-}}_{\\text{right part}}: \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 2 \u0026amp; -3 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; | -1 \u0026amp; -1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} $$\nRow echelon form: Clear the values on the lower left side of the main diagonal from top to bottom Clear the values on the top right side of the main diagonal from bottom to top The right-hand side of the vertical line individually is the inverse matrix 𝐀⁻¹.\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; | 2 \u0026amp; -3 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; | -1 \u0026amp; -1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; | 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} = 𝐈|𝐀⁻¹ $$\nSo,\n$$ 𝐀⁻¹ = \\begin{bmatrix} 2 \u0026amp; -3 \u0026amp; 1 \\\\ -1 \u0026amp; -1 \u0026amp; 1 \\\\ 0 \u0026amp; 2 \u0026amp; -1 \\end{bmatrix} $$\n2x2 (2024-02-12)\nThe inverse of $[^{a\\ b}_{d \\ c}]$ is:\n$$ \\underbrace{ \\begin{bmatrix} 1 \u0026amp; -\\frac{b}{a} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{d} \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{a}{ac-bd} \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \\\\ -1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} \\frac{d}{a} \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} }_{A⁻¹} \\begin{bmatrix} a \u0026amp; b \\\\ d \u0026amp; c \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} $$\nThe combinatio nof 5 matrices is the inverse matrix:\n$$ A⁻¹ = \\begin{bmatrix} \\frac{c}{ac-bd} \u0026amp; -\\frac{b}{ac-bd} \\\\ -\\frac{d}{ac-bd} \u0026amp; \\frac{a}{ac-bd} \\end{bmatrix} = \\frac{1}{ac-bd} \\begin{bmatrix} c \u0026amp; -b \\\\ -d \u0026amp; a \\end{bmatrix} $$\nHence, the 1/determinant is a part of the inverse matrix. Matlab A is invertible:\n1 2 3 4 5 6 7 8 A = [1, 1, 2; 1, 2, 3; 2, 4, 5] inv(A) ans = 2 -3 1 -1 -1 1 0 2 -1 B is not invertible:\n1 2 3 4 5 6 7 8 9 10 B = [1, 1, 1; 2, 2, 2; 3, 3, 3] inv(B) Warning: Matrix is singular to working precision. ans = Inf Inf Inf Inf Inf Inf Inf Inf Inf C is not a square matrix:\n1 2 3 4 5 C = [1, 2, 3; 4, 5, 6] inv(C) Error using inv Matrix must be square. ","date":"2023-04-03T12:31:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/12_%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E6%B3%95/","title":"watch: LA - 高山 12 | How to find the inverse matrix?"},{"content":"Matrix multiplication doesn\u0026rsquo;t have commutative property (交换律). But some square matrices satisfy 𝐀𝐁=𝐁𝐀\nMatrix multiplication has Associative property (结合律)\n(𝐀𝐁)𝐂 = 𝐀(𝐁𝐂)\nCompund linear mapping\nTranspose and multiplication (𝐀𝐁)ᵀ ≠ 𝐀ᵀ𝐁ᵀ\n(𝐀𝐁)ᵀ = 𝐁ᵀ𝐀ᵀ\nSquare matrix multiplication The number of rows = the number of columns\n𝐀 = $[^{1\\ 2} _{1\\ 4}]$ ; 𝐁 = $[^{4\\ -2} _{-1\\ 1}]$\nThere is 𝐀𝐁 = 𝐁𝐀.\nSo these two matrices 𝐀, 𝐁 are commutative 1. 𝐀,𝐁 must be the square matrices with the same size, and the result also has the identical-size square matrix.\nPower of square matrix 𝐀ᵏ = 𝐀⋅𝐀ᵏ⁻¹ = 𝐀ᵏ⁻¹⋅𝐀, where 𝐀 and 𝐀ᵏ⁻¹ are commutative because of the commutative property of matrix multiplication.\nTherefore, the two positive integar powers of matrix 𝐀 are always commutative.\nPower of the matrix only applicable to square matrices. Otherwise, the mismatched dimensions prevent performing matrices multiplication.\n𝐀¹=𝐀 ; 𝐀⁰ = 𝐈\nThus, the range of power k is the set of natural numbers k∈ ℕ\nPower of diagonal matrix $$ 𝐀 = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 3 \\end{bmatrix} = diag\\{1, 2, 3\\} $$\nEach elements on the main diagonal is multiplied with itself many times.\n2\nScalar matrix $$ 𝐀 = \\begin{bmatrix} λ \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; λ \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; λ \\end{bmatrix} = diag\\{λ, λ, λ\\} $$\nIf λ = 1, then 𝐀 = 𝐈. So 𝐈ᵏ=𝐈, and this is also applicable to zero matrix 𝟎ᵏ= 𝟎 (k≠0)\nRef When are two matrices A and B: AB = BA? - StackExchange 11. Powers of matrices - Massachusetts Institute of Technology ","date":"2023-04-02T14:55:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/08_%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E7%9A%84%E6%80%A7%E8%B4%A8/","title":"watch: LA - 高山 08 | Properties of matrix multiplication"},{"content":"Review Zero matrix 𝐀 = [^{4\\ 5\\ 6} _{5\\ 6\\ 7}]; 𝟎 = [^{_{0\\ 0}} _{^{0\\ 0} _{0\\ 0}}]₃ₓ₂\n𝐀𝟎 = [^{0\\ 0} _{0\\ 0}] = 𝟎₂ₓ₂\nNon-homogeneous system has no zero solution 𝐀𝐁 ≠ 0 means A, B both cannot be zero.\nDiagonal matrix and identical matrix 𝐁 is a diagonal matrix.\n𝐀𝐁 is scaling each column of 𝐀 by the value of element on the diagonal times.\nI = []\nVectors multiplication Rank relation 𝐀 = [^{1\\ 2} _{0\\ 3}]; 𝐁 = [^{2\\ 3} _{0\\ 4}]\n𝐀𝐁 = [^{2\\ 11} _{0\\ 12}]\nr(𝐀) = 2; r(𝐁) = 2; r(𝐀𝐁) = 2\nIf 𝐁 = [^{1\\ 0} _{0\\ 0}], r(𝐁) = 1, then 𝐀𝐁 = [^{1\\ 0}_{0\\ 0}]; r(𝐀𝐁) = 1.\nFurther letting 𝐀 = [^{1\\ 2} _{0\\ 0}], r(𝐀) = 1, then 𝐀𝐁 = [^{1\\ 0}_{0\\ 0}]; r(𝐀𝐁) = 1.\nIn addition, if 𝐀 = [^{1\\ 2} _{0\\ 0}], 𝐁 = [^{0\\ -2} _{0\\ 1}], then 𝐀𝐁 = [^{0\\ 0}_{0\\ 0}], r(𝐀𝐁) = 0\nConclusion: the rank of the product matrix is not greater than the rank of any multiplier.\nr(𝐀𝐁) ≤ min{ r(𝐀), r(𝐁)}\n","date":"2023-04-02T14:29:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/07_%E7%89%B9%E6%AE%8A%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E6%B3%95/","title":"watch: LA - 高山 07 | Extension for matrix multiplication"},{"content":"Source video: 【俗说矩阵】初识矩阵的秩，从最简单的例子讲起！\nFind the rank Rank of a matrix 𝐀 equals to the number of the non-zero rows of a matrix in row echelon form (after performing elementary row operations).\nDefinition of the rank determinant\nProperties The matrices appearing in the transforming process are able to reach the same row echelon form, so they have the same rank. In another word, the elementary row operations don\u0026rsquo;t change the rank of matrices. The rank of a matrix is no bigger than the number of rows or columns. Thus, the solution of a linear equation system can be represented as its rank.\nSolution judged by rank Homogeneous linear equation system:\nOnly zero solution: r(𝐀) = n, the rank of the coefficient matrix equals to the number of unknowns Exist non-zero solution: r(𝐀) \u0026lt; n, Non-homogeneous linear equation system:\nSingle unique solution: r(𝐀|𝐛) = r(𝐀) = n, the rank of the augmented matrix = the rank of the coefficient matrix = the number of the unknowns; Inifinitely many solutions: r(𝐀|𝐛) = r(𝐀) \u0026lt; n; No solution: r(𝐀|𝐛) ≠ r(𝐀) r(𝐀) = 0 𝐀 = $[^{_{0\\ 0\\ 0}} _{^{0\\ 0\\ 0} _{0\\ 0\\ 0}}]$\nAll of its elements are 0. There is no non-zero rows. So r(𝐀) = 0.\nThis is the so-called zero matrix noted as 𝐀 = 𝟎\nr(𝐀) = 0 and 𝐀=𝟎 are equivalent, because if any one of elements is not 0, then there is a non-zero row resulting the r(𝐀) ≠ 0.\nr(𝐀) = 1 𝐀 = $[^{_{1\\ 2\\ 3}} _{^{2\\ 4\\ 6}_{3\\ 6\\ 9}}]$ ➔ $[^{_{1\\ 2\\ 3}} _{^{0\\ 0\\ 0}_{0\\ 0\\ 0}}]$\nThe rows in the matrix of r(𝐀) = 1 are proportional to each other. In fact, the columns are also proportional. Their ratio factor can be 0, and also there must be at least 1 element which is not zero in the matrix. Otherwise, it\u0026rsquo;s the zero matrix. 𝐚= [1 2 3] and 𝐛 = $[^{_1} _{^2_3}]$ are 1x1 matrix with rank = 1.\nVectors are the matrix with only 1 row or 1 column.\nRow vector with at least 1 non-zero value is of rank=1.\nColumn vector has multiple rows with a single value. If it\u0026rsquo;s not a zero vector, the later rows can be reduced to 0 by adding the row1 multiplying with different factors. 𝐛 = $[^{_1} _{^2_3}]$ ➔ $[^{_1} _{^1_1}]$\nFor all the non-zero vector, no matter row vector or column vector, their rank = 1.\nVector 𝐚 ≠ 0 and r(𝐚) = 1 are equivalent.\n","date":"2023-04-02T12:13:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/04_%E5%88%9D%E8%AF%86%E7%9F%A9%E9%98%B5%E7%9A%84%E7%A7%A9/","title":"watch: LA - 高山 04 | Rank of matrix"},{"content":"The 1-17 tips come from Faster Deep Learning Training with PyTorch – a 2021 Guide - LORENZ KUHN\n1. Consider using another learning rate schedue torch.optim.lr_scheduler.CyclicLR torch.optim.lr_scheduler.OneCycleLR 2. Use multiple workers and pinned memory in DataLoader torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=4, pin_memory=pin_memory) DataLoader-Docs\nnum_workers Rule of thumb: set the num_workers to four times the number of available GPUs. Note that increasing num_workers will increase RAM consumption.\npin_memory When using a GPU it’s better to set pin_memory=True, this instructs DataLoader to use pinned (page-locked) memory and enables faster and asynchronous memory copy from the host to the GPU. Tutorial-Szymon Migacz\npin_memory avoid one implicit CPU-to-CPU (\u0026ldquo;Pageable Memory\u0026rdquo; to \u0026ldquo;Pinned Memory\u0026rdquo;) copy when perform a.cuda() operation. As the illustration shows in Nvidia Blog I fogot where I got this inspiration: \u0026ldquo;点对点复制\u0026rdquo;.\nWith pinned memory tensors, the copy process a.cuda(non_blocking=True) is asynchronous with respect to host (CPU) SO. If the code is structured as:\na.cuda(non_bloking=True) # copy from cpu to gpu Perform some CPU operations Perform GPU operations using a. The step 1 and step 2 can proceed parallelly. Hence, the maximum time can be saved is the duration of step 2.\n3. Max out the batch size Other hyperparameters, like learning rate, have to be adjusted. Rule of thumb: double the learning rate as double the batch size May cause worse generalization performance. 4. Use Automatic Mixed Precision (AMP) The optimizations of some operations use semi-precision (FP16) rather than single-precision (FP32) 5. Consider using another optimizer AdamW outperform Adam resulting from weight decay (rather than L2-regularization). 6. Turn on cudNN benchmarking Tutorial-Szymon Migacz\n7. Avoid unnecessary CPU-GPU synchronizations (Tutorial-Szymon Migacz):\ntensor.cpu() or tensor.cuda(), tensor.to() tensor.item() or tensor.numpy() print(cuda_tensor) cuda_tensor.nonzero() retrieves the indices of all non-zero elements; Avoid python control based on cuda tensors, e.g., if (cuda_tensor != 0).all() The good practice should let the CPU run ahead of the accelerator as much as possible to make sure that the accelerator work queue contains may operations.\nRef ","date":"2023-03-23T14:11:00-05:00","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_train_faster/","title":"memo: PyTorch | Tricks for Faster Training"},{"content":"Authors: Wandong Zhang, Jonathan Wu, Yimin Yang Neurocomputing (2020-07-18)\nAbstract Wide hierarchical subnetwork-based neural network (Wi-HSNN) Iterative training by adding subnetnetwork nodes into modle one-by-one batch-by-batch training instead of processing the entire dataset (one-batch) Place365 has 1.8 million samples 1 Introduction 3 Proposed Wi-HSNN Loss function of SLFN is MSE: min J= ½ ‖𝐓 - g(𝐗, 𝐰₁, 𝐛)⋅𝐰₂‖²\nThe output weights can be initialized from the Target data (letting 𝐇 denote g(𝐗, 𝐰₁, 𝐛)):\n𝐰₂ = ( 𝐇ᵀ𝐇 + I/C )⁻¹ 𝐇ᵀ 𝐓\nIn this paper, the input data 𝐗 is first transformed to 𝐅ₑₙ, which then passes a SNN (SLFN, subnetwork node) and becomes g(𝐅ₑₙ⋅𝐚ₑₓ + bₑₓ). Next, the input 𝐗 is fed into multiple SNN sequentially. And their outputs are accumulated with the weight 𝐚ₜ.\nTherefore, if there are L SNNs, the loss function for this problem is:\nmin J = ½ ‖𝐓 - ∑ᵢ₌₁ᴸ g(𝐅ₑₙⁱ⋅𝐚ₑₓⁱ + bₑₓⁱ) ⋅ 𝐚ₜᴸ‖²\n3.2 Training the Wi-HSNN Feedforward with randomly initialized (𝐚ₑₙ¹, bₑₙ¹) and (𝐚ₑₓ¹, bₑₓ¹) and calculate the optimal output weights based on pseudo-inverse:\n𝐅ₑₙ¹ = g(𝐗⋅𝐚ₑₙ¹ + bₑₙ¹) 𝐅ₜ¹ = 𝐅ₑₓ¹ = g(𝐅ₑₙ¹ ⋅𝐚ₑₓ¹ + bₑₓ¹) 𝐚ₜ¹ = (𝐅ₜᵀ𝐅ₜ + I/C)⁻¹ 𝐅ₜᵀ 𝐓\nObtain the feedback error (\u0026ldquo;feature H\u0026rdquo;) matrix 𝐏ₑₓ¹ and 𝐏ₑₙ¹ for solving the weights 𝐚ₑₓ (=𝐰₂), 𝐚ₑₙ (=𝐰₁) of next iteration.\n𝐏ₑₓ¹ = g⁻¹ (𝐞¹ ⋅ (I/C + (𝐚ₜ¹)ᵀ⋅𝐚ₜ¹)⁻¹ ⋅ (𝐚ₜ¹)ᵀ ) 𝐏ₑₙ¹ = g⁻¹ (𝐏ₑₓ¹ ⋅ (I/C + (𝐚ₑₓ¹)ᵀ ⋅ 𝐚ₑₓ¹ )⁻¹ ⋅ (𝐚ₑₓ¹)ᵀ )\nCalculate the 𝐚ₑₓ, 𝐚ₑₙ for next SNN:\n𝐚ₑₙⁱ = (𝐗ᵀ𝐗 + I/C )⁻¹ 𝐗ᵀ 𝐏ₑₓⁱ⁻¹ (Entrance layer weights) 𝐚ₑₓⁱ = ( (𝐅ₑₙⁱ)ᵀ𝐅ₑₙⁱ + I/C )⁻¹ (𝐅ₑₙⁱ)ᵀ 𝐏ₑₙⁱ⁻¹ (Exit layer weights)\nSummarize outputs 𝐅ₜⁱ from all exisiting SNN, and update output weight 𝐚ₜⁱ.\n𝐅ₜⁱ = ∑ₖ₌₁ⁱ 𝐅ₑₓᵏ 𝐚ₜⁱ = ( (𝐅ₜⁱ)ᵀ𝐅ₜⁱ + I/C )⁻¹ (𝐅ₜⁱ)ᵀ 𝐓\nObtain the feedback error 𝐏ₑₓⁱ and 𝐏ₑₙⁱ:\n𝐏ₑₓⁱ = g⁻¹ (𝐞ⁱ ⋅ (I/C + (𝐚ₜⁱ)ᵀ⋅𝐚ₜⁱ)⁻¹ ⋅ (𝐚ₜⁱ)ᵀ ) 𝐏ₑₙⁱ = g⁻¹ (𝐏ₑₓⁱ ⋅ (I/C + (𝐚ₑₓⁱ)ᵀ ⋅ 𝐚ₑₓⁱ )⁻¹ ⋅ (𝐚ₑₓⁱ)ᵀ )\nRepeat step 3 to step 5 L-2 times. 𝐅ₜᴸ is the final encoding. And 𝐘 = 𝐅ₜᴸ 𝐚ₜᴸ is the final classification prediction.\n3.4 Batch-by-batch scheme with parallelism strategy The entire feature set 𝐅ᴺᕽᵈ (i.e., 𝐇) and the target set 𝐓 are split into p subsets:\n𝐇 = $[^{𝐇(𝐱₁)}_{^{\u0026hellip;}_{𝐇(𝐱ₚ)}}]$, 𝐓 = $[^{𝐓(𝐱₁)}_{^{\u0026hellip;}_{𝐓(𝐱ₚ)}}]$\nThe desired weights matrix 𝐰₂ (i.e., \u0026ldquo;𝛃\u0026rdquo;) represented weith pseudo-inverse matrix becomes\n𝐰₂ = (𝐇ᵀ𝐇 + I/C)⁻¹𝐅ᵀ⋅𝐓\n𝐰₂ = ([𝐇₁ᵀ,\u0026hellip;, 𝐇ₚᵀ] $[^{_{𝐇(𝐱₁)}} _{^{\u0026hellip;}_{𝐇(𝐱ₚ)}}] + ^I_{^-_C}$)⁻¹ [𝐇₁ᵀ,\u0026hellip;, 𝐇ₚᵀ] $[ ^{_{𝐓(𝐱₁)}}_{^{\u0026hellip;}_{𝐓(𝐱ₚ)}} ]$\n𝐰₂ = ([𝐇(𝐱₁)ᵀ𝐇(𝐱₁) + \u0026hellip; + 𝐇(𝐱ₚ)ᵀ𝐇(𝐱ₚ)] + I/C)⁻¹ ⋅ [𝐇(𝐱₁)ᵀ𝐓(𝐱₁) + \u0026hellip; + 𝐇(𝐱ₚ)ᵀ𝐓(𝐱ₚ) ] 𝐰₂ = (∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐇(𝐱ᵢ) + I/C)⁻¹ ⋅ ∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐓(𝐱ᵢ)\nFirst, calculate (∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐇(𝐱ᵢ) + I/C)⁻¹.\nThe matrices are accumulated batch by batch in the code. After the 1st iteration, K=(𝐇₁ᵀ𝐇₁ + I/C)⁻¹ has obtained and returned.\nWhen next batch 𝐇₂ is retrieved:\nK_new = (𝐇₂ᵀ𝐇₂ + 𝐇₁ᵀ𝐇₁ + I/C)⁻¹ = (𝐇₂ᵀ𝐇₂ + K⁻¹)⁻¹ # analogy to (UBV + A)⁻¹, where U=𝐇₂ᵀ, V=𝐇₂, B=I, A=K⁻¹ Woodbury: \u0026ldquo;A⁻¹ - A⁻¹⋅U⋅(I+BV⋅A⁻¹⋅U)⁻¹ BV A⁻¹ \u0026quot; 1 = K - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ K = (I - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ ) ⋅ K Let Kₚ = (I - K⋅𝐇₂ᵀ⋅(I+𝐇₂⋅K⋅𝐇₂ᵀ)⁻¹ 𝐇₂ ). So K_new = Kₚ ⋅ K\nThen, for the second item ∑ᵢ₌₁ᵖ 𝐇(𝐱ᵢ)ᵀ𝐓(𝐱ᵢ),\nIn the first batch, 𝛃=K⋅𝐇₁ᵀ⋅𝐓₁ is obtained and returned.\nWhen the second batch coming, there should be (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂)\n𝛃_new = K_new ⋅ (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂) = K_new ⋅ (𝐇₁ᵀ⋅𝐓₁ + 𝐇₂ᵀ⋅𝐓₂) = Kₚ ⋅ K ⋅ 𝐇₁ᵀ⋅𝐓₁ + K_new ⋅ 𝐇₂ᵀ⋅𝐓₂ = Kₚ ⋅ 𝛃 + K_new ⋅ 𝐇₂ᵀ⋅𝐓₂\n矩阵之和的逆 (DDG search: \u0026ldquo;矩阵之和的逆\u0026rdquo;) 两个矩阵相加后求逆 - ~海棠依旧~ - CSDN\n关于两个矩阵之和逆阵的讨论 - docin\n(Google search: \u0026ldquo;两矩阵和的逆\u0026rdquo;) 兩矩陣和的逆矩陣 - 線代啟示錄 - 周志成(阳明交大)\n矩阵之和的逆 不等于 逆矩阵的和 (A+B)⁻¹ ≠ A⁻¹ + B⁻¹\n∵ (A+B)(A⁻¹ + B⁻¹) = E + BA⁻¹ + AB⁻¹ + E ≠ E\n最简单的例子：取 A=B=E，(A+B)(A⁻¹ + B⁻¹) = E + BA⁻¹ + AB⁻¹ + E = 4E ≠ E 矩阵和的逆矩阵 逆矩阵的和 相等吗 -百度知道\nRef 兩矩陣和的逆矩陣 - 線代啟示錄 - 周志成(阳明交大) ","date":"2023-03-15T20:13:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-batch-train/","title":"read: Wi-HSNN for dimension reduction"},{"content":"Inverse of Sigmoid 1 2 def inverse_sigmoid(x): return torch.log(x/(1-x)) Inverse of Softmax Determining Existence Only monotonic function has its inverse function. ¹\nThe softmax function is not invertible? Any constant C will work. Or C=(1−log(x⋅y⋅z))/3) Code Implement a layer to invert the softmax by tensorflow: ²\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import tensorflow as tf def inv_softmax(x, C): return tf.math.log(x) + C import math input = tf.keras.layers.Input(shape=(1,10)) x = tf.keras.layers.Lambda(lambda x : inv_softmax(x, math.log(10.)),name=\u0026#39;inv_softmax\u0026#39;)(input) model = tf.keras.Model(inputs=input, outputs=x) a = tf.zeros([1, 1, 10]) a = tf.nn.softmax(a) a = model(a) print(a.numpy()) Ref 反函数的定义及求法 - 半个冯博士的文章 - 知乎; (Searched by DDG: \"求和函数的反函数\") How to create a layer to invert a softmax (TensforFlow,python)? - StackOverflow Can we have an explicit representation of inverse of a softmax function? -StackExchange (searched by DDG: \"softmax function inverse\") Invert the softmax function - StackExchange What is an intuitive interpretation for the softmax transformation?\n一文详解Softmax函数 - 触摸壹缕阳光的文章 - 知乎 (searched by DDG: \"sotmax\") (2023-02-23)\nInverse of Sum Is the summation operation invertible?\n(DDG search: \u0026ldquo;inverse summation\u0026rdquo; or \u0026ldquo;invert a summation\u0026rdquo;)\n","date":"2023-02-19T11:00:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/inverse_softmax/","title":"memo: Calc | Inverse Function of Activations"},{"content":"Authors: Wandong Zhang et. al. Publish date: 2020-03-30 (Finished in 2019)\nIEEE Trans. Industrial Informatics | G.Drive | G.Scholar\nTry to summary (2023-02-26):\nDifferent features are concatenated and then fed into a \u0026ldquo;I-ELM with subnetwork nodes\u0026rdquo;. What is optimized it the combination weights, but the feature vectors themselves are not changed. It is the weights (IW, 𝛃) are refined. Sepecifically, the new R-SNN node is improved by adding a part of unlearned wights accquired from the residual error of the last node. That is the weights are accumulated on the newest node. So the ultimate R-SNN node contains all the previous training outcomes. What we kept is only the last R-SNN node, i.e., a SLFN. Does that require the performance of the final R-SNN is the best among the former nodes. (In code) The update process of a SLFN is as follows:\nflowchart LR subgraph node1[SLFN1] h1((h1)) \u0026 h2((h2)) \u0026 he((he)) \u0026 hd((hd)) end In[\"X\\n data\\n matrix\"] --\u003e IW1 --\u003e h1 \u0026 h2 \u0026 he \u0026 hd node1 --- beta1(\"𝛃1\") --\u003e Yout1 --\u003e Error1 Target --\u003e|\"pinv\"| beta1 beta1 \u0026 Error1 --\u003e|\"inv: 𝛃⋅P=𝐞₁\"| P[\"P\\n ( The H\\n yielding\\n Error1) \"] P \u0026 In --\u003e IW'[\"residual\\n IW\"] IW' \u0026 IW1 --\u003esum((+)) --\u003e IW2 subgraph node2[SLFN2] h21((h1)) \u0026 h22((h2)) \u0026 h2e((he)) \u0026 h2d((hd)) end IW2 --\u003e h21 \u0026 h22 \u0026 h2e \u0026 h2d node2 --- beta2(\"𝛃2\") Target --\u003e |\"pinv\"| beta2 beta2 --\u003e Yout2 Abstract A supervised multi-layer subnetwork-based feature refinement and classification model for representation learning. Expand the width for a generalized hidden layer rather than stack more layers to go deeper One-shot solution for finding the meaningful latent space to recognize the objects rather than searching separate spaces to find a generalized feature space. Multimodal fusion fusing various feature sources into a superstate encoding instead of a unimodal feature coding in the traditional feature representation methods. Ⅰ. Introduction (Task \u0026amp; Application \u0026amp; List of ralated research field \u0026amp; Problem \u0026amp; Existing solutions brif)\nTask: high-dimensional data processing and learning Problem definition: selecting the optimal feature descriptors 2 branch of solutions: hand-crafted descriptors and deep-learning-based features. (Criticize the former feature extraction solutions and introduce proposed method:)\nFeatures derived from approaches of those 2 categories are too inflexible to contribute a robust model. This method \u0026ldquo;encodes and refines these? raw features from multiple sources to improve the classification performance\u0026rdquo;. For example: 4 extracted features (from AlexNet, ResNet, HMP, and SPF) are concatenated into 1 vector taken as the input to a \u0026ldquo;3-layer\u0026rdquo; model, where only a single \u0026ldquo;genearlized\u0026rdquo; hidden layer (latent space) bridges the raw feature space (transformation ax+b) and the final target space (residual error). (Recap deep learning models and mention the theory base of this work)\nDeep networks often get \u0026ldquo;trapped in local minimum and are sensitive to the learning rate\u0026rdquo; because their training fundation is BP. Regression-based feature learning. Least-squares representation learning methods. (Problems to be solved) Drawbacks of regression-based approaches:\n\u0026ldquo;block\u0026rdquo; models? don\u0026rsquo;t perform one-shot training philosophy based on the relation between raw data and the target. A model trained by some \u0026ldquo;designed\u0026rdquo; process has a inferious generalizatio n capacity than the model derived from one-shot training strategy (least-squares). Drawbacks of multilayer neural networks \u0026amp; solution Deeper layer-stacked models suffer from overfitting with limited training samples. Network-in-network structure enhances the network\u0026rsquo;s generalization capacity for learning feature. ELM with subnetwork nodes. Contributions: Subnetwork neural nodes (SNN) realized multilayer representation learning. Unlike the ensembled network, the SNN is trained based on the error term. Feature space transformation and the classification are solved together by searching iteratively the optimal encoding space (hidden layer). Concatenation of multiple features result more discriminative representations for samples. Ⅱ. Literature review A. Conventional Feature Coding \u0026quot; Supervised method of learning representaiton evaluates the importance of a specific feature through the correlation between features and categories.\u0026quot;\nConventional feature coding of images depends on prior knowledge of the problem. Thus, the features are not complete representations.\nThis paper enhances the feature by fusing (discriminative) hand-crafted features and (class-specific) CNN-based features.\nB. Least-Squares Encoding Methods The least-squares approximation methods, such as random forest and alternating minimization, have been exhaustively investigated in single-layer neural networks.\nRelated works: Moore-Penrose inverse; Universal approximation capacity of I-ELM, ELM autoe-ncoder14, Features combined with subnetwork nodes 18\nEach SNN is applied as a local feature descriptor. Hence, the subspace features can be extracted? from the original data independently, and the useful features are generated via the combination of these features.\nⅢ. Proposed Method A. Algorithmic Summary Two steps:\nPreprocessing: concatenate various feature vectors into a single \u0026ldquo;supervector\u0026rdquo;. Train the width-growth model: Terminology: layer name marker params in out input Entrance (feature) layer 𝑓 𝐖ᵢᶠ, 𝐛ᵢᶠ random vct linear combination 𝐇 hidden Refinement layer/subspace 𝑟 𝐖ᵢʳ, 𝐛ᵢʳ (𝐚,b) 𝐇 partial feature Ψ output Least square learning layr 𝑣 𝐖ᵢᵛ (𝛃) Ψ sum up all partial features: 𝚪 residual error 𝐞 (An entrance layer and a refinement layer both are \u0026ldquo;SNN\u0026rdquo;, and their combination is a \u0026ldquo;R-SNN\u0026rdquo;)\nInitialization: For the 1st R-SNN, 𝐖₁ᶠ, 𝐖₁ʳ are random generating a false feature Ψ. Then the first least-square method (pseudoinverse) is performed to calculate 𝐖₁ᵛ based on target 𝐘 and Ψ.\nIteratively add the R-SNN (2≤ i≤ L) (refinement subspace) into the hidden layer (optimal feature space)\nflowchart TB subgraph In[input feature] x1((1)) \u0026 x2((2)) \u0026 xe((\"⋮\")) \u0026 xn((n)) end EnW(\"Entrance layer\\n 𝐖ᵢᶠ, 𝐛ᵢᶠ\\n random\") subgraph H[\"entrance feature 𝐇\"] h1((1)) \u0026 h2((2)) \u0026 he((\"⋮\")) \u0026 hD((D)) end RefineW(\"Refinement layer\\n 𝐖ᵢʳ, 𝐛ᵢʳ\") subgraph Psi[partial feature Ψ] Ψ1((1)) \u0026 Ψ2((2)) \u0026 Ψe((\"⋮\")) \u0026 Ψd((d)) end OW(\"Output layer\\n 𝐖ᵢᵛ\") subgraph Out[\"Output vector\"] o1((1)) \u0026 o2((2)) \u0026 oe((\"⋮\")) \u0026 om((m)) end x1 \u0026 x2 \u0026 xe \u0026 xn --\u003e EnW --\u003e h1 \u0026 h2 \u0026 he \u0026 hD --\u003e RefineW --\u003e Ψ1 \u0026 Ψ2 \u0026 Ψe \u0026 Ψd --\u003e OW --\u003e o1 \u0026 o2 \u0026 oe \u0026 om Out --\u003e|\"- 𝐞ᵢ₋₁\"| erri[\"𝐞ᵢ\"] erri \u0026 OW -.-\u003e|pinv| newΨ(\"𝐏 \\n yielding\\n 𝐞ᵢ\") subgraph H1[\"entrace feature 𝐇ᵢ₊₁\"] h11((1)) \u0026 h12((2)) \u0026 h1e((\"⋮\")) \u0026 h1D((D)) end In --\u003e EnW1(\"Entrance layer\\n 𝐖ᵢ₊₁ᶠ, 𝐛ᵢ₊₁ᶠ\\n random\") --\u003e h11 \u0026 h12 \u0026 h1e \u0026 h1D H1 --\u003e RefineW1(\"Refinement layer\\n 𝐖ᵢ₊₁ʳ, 𝐛ᵢ₊₁ʳ\") %%-.-|solved by P| newΨ newΨ -.-\u003e RefineW1 subgraph Psi1[partial feature Ψ] Ψ11((1)) \u0026 Ψ12((2)) \u0026 Ψ1e((\"⋮\")) \u0026 Ψ1d((d)) end RefineW1 --\u003e Ψ11 \u0026 Ψ12 \u0026 Ψ1e \u0026 Ψ1d --\u003e OW1(\"Output layer\\n 𝐖ᵢ₊₁ᵛ\") %%OW1 -.-|solved by| erri erri -.-\u003e OW1 subgraph Out1[\"Output vector\"] o11((1)) \u0026 o12((2)) \u0026 o1e((\"⋮\")) \u0026 o1m((m)) end OW1 --\u003e o11 \u0026 o12 \u0026 o1e \u0026 o1m Out1 --\u003e|\"- 𝐞ᵢ\"| erri+1[\"𝐞ᵢ₊₁\"] --\u003e newP B. Model Definition SLFN solves the regression problem can be expressed as:\nMLNN has nested transformation:\nProposed method is a generlized SLFN:\nminimize J = ½ ‖𝐘-f(𝐇ᵢᶠ, 𝐖ᵢʳ, 𝐛ᵢʳ)⋅𝐖_Lᵛ‖²,\nf(𝐇ᵢᶠ, 𝐖ᵢʳ, 𝐛ᵢʳ) = ∑ᵢ₌₁ᴸ g(𝐇ᵢᶠ ⋅ 𝐖ᵢʳ + 𝐛ᵢʳ): sum all R-SNN 𝐇ᵢᶠ = g(𝐖ᵢᶠ, 𝐛ᵢᶠ, 𝐗) 𝐘 ∈ ℝᴺᕽᵐ: expected output, target feature 𝐗 ∈ ℝᴺᕽⁿ: input matrix L : number of R-SNN node g : activateion function 3 differences from other least-squares-based MLNNs\nSNN combines each dimension of the feature vector serving as local feature descriptor. While the R-SNN is the basic unit to refine feature vectors.\nOptimal feature is the aggregation of R-SNN added one by one. R-SNN is densly connected to input vector and output layer containing twice linear projection. Different R-SNNs are independent because they learn from different error.\nThe latent space is the aggregation of all R-SNN nodes subspace. So the parameters training has no block-wise communication between different spaces. That means the feature refinement and classification are doen together.\nC. Proposed Width-Growth Model Input weights and bias 𝐖ᵢᶠ, 𝐛ᵢᶠ: randomly initialized; Entrance feature: 𝐇ᵢᶠ = g(𝐗𝐖ᵢᶠ+ 𝐛ᵢᶠ); Refined partial feature: Ψᵢ=g(𝐇ᵢᶠ𝐖ᵢʳ+ 𝐛ᵢʳ), where 𝐛ᵢʳ is random; Output weights: 𝐖ᵢᵛ=(𝐈/C + 𝚪ᵀ𝚪)⁻¹𝚪ᵀ⋅𝐘, where C is hyperparameter for regularization, and (𝐈/C + 𝚪ᵀ𝚪)⁻¹𝚪ᵀ is the pseudoinverse of output vector 𝚪 (label?) Error: 𝐞ᵢ = 𝐘 - 𝐖ᵢᵛ\n𝐏 is the desired matrix generating 𝐞ᵢ by: 𝐏ᵢ⋅𝐖ᵢᵛ=𝐞ᵢ, so 𝐏ᵢ = 𝐞ᵢ⋅(I/C + (𝐖ᵢᵛ)ᵀ𝐖ᵢᵛ)⁻¹(𝐖ᵢᵛ)ᵀ\nRefinement layer weights of next R-SNN: 𝐖ᵢ₊₁ʳ = (I/C +𝐇ᵢᵀ𝐇ᵢ)⁻¹𝐇ᵢᵀ ⋅ g⁻¹(𝐏ᵢ), because g(𝐇ᵢ₊₁⋅𝐖ᵢ₊₁ʳ+ 𝐛ᵢ₊₁ʳ) = 𝐏ᵢ. Next partial feature: Ψᵢ₊₁ = g(𝐇ᵢ₊₁⋅𝐖ᵢ₊₁ʳ+ 𝐛ᵢ₊₁ʳ)\nAccumulate the partial feature to the optimal feature: 𝚪ᵢ₊₁ = 𝚪ᵢ + Ψᵢ₊₁ Update error: 𝐞ᵢ₊₁ = 𝐞ᵢ-𝐖ᵢᵛ𝚪ᵢ\nRepeat steps 4-6 L-2 times, and the final feature 𝚪$_L$ is the generalized feature correponding to the best output parameter 𝐖 $_Lᵛ$ for classification.\nRef L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, “Representational learning with extreme learning machine for big data,” IEEE Intell. Syst.,Dec. 2013. Y. Yang and Q. J. Wu, “Features combined from hundreds of midlayers: Hierarchical networks with subnetwork nodes,” IEEE Trans. Neural Netw. Learn. Syst., Nov. 2019. J. Tang, C. Deng, and G.-B. Huang, “Extreme learning machine for multilayer perceptron,” IEEE Trans. Neural Netw. Learn. Syst., Apr. 2016. ","date":"2023-02-15T12:40:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-snn-refine-weights/","title":"read: Width-Growth Model with Subnetwork Nodes"},{"content":"Authors: Yimin Yang; Yaonan Wang; Xiaofang Yuan\nCode | TNNLS (2012-06-20)\nTry to summary: (2023-02-15) B-ELM is a variant of I-ELM by dividing the hidden nodes into 2 types: odd and even, which differ in wehther the input parameters (𝐚,b) is randomly generated or calculated by twice inverse operations.\nSpecifically, the first node is solved from the target 𝐭. Then for subsequent nodes, the even node is solved based on the residual error of its previous odd node.\nThe output weights of the odd nodes are calculated as: 𝐇₂ₙ₋₁ʳ + e₂ₙ₋₂ (or 𝐭) ➔ 𝛃₂ₙ₋₁ (➔e₂ₙ₋₁) The input weights 𝐚 and bias b of even nodes are solved based on the residual error: e₂ₙ₋₁ ➔ 𝐇₂ₙᵉ ➔ 𝐚,b (➔ ^𝐇₂ₙᵉ ➔ 𝛃₂ₙ ➔ e₂ₙ) Note: the superscript r and e stand for \u0026ldquo;random\u0026rdquo; and \u0026ldquo;error\u0026rdquo; marking the source of H. Abstract This algorithm tends to reduce network output error to 0\nby solving the input weights 𝐚 and bias b based on the network residual error. (In other words, the residual error is represented by a, b of subsequent nodes. Or the error is absorbed by others parameters besides 𝛃.)\nⅠ. Introduction For ELM with a fixed structure, the best number of hidden nodes need to ffound by trial-and-error, because the residual error is not always decreasing when there are more hidden nodes in an ELM.\nFor incremental ELM, the hidden node is added one by one, so the residual error keeps decreasing. But the model training has to do multiple iterations, i.e., calculating inverse matrix is needed after adding each node.\nCompared with other incremental ELM, this method is\nFaster and with fewer hidden nodes showing the relationship between the network output residual error 𝐞 and output weights 𝛃, which is named \u0026ldquo;error-output weights ellipse\u0026rdquo; The hidden layer (input weights) with determined parameters instead of random numbers would make the error reduce, or improve the accuracy. Ⅱ. Preliminaries and Notation A. Notations and Definitions ⟨u,v⟩ = ∫ₓu(𝐱)v‾(𝐱)d𝐱 is the Frobenius inner product of two matrices u,v, where the overline denotes the complex conjugate.\nB. I-ELM Lemma 1(proved by Huang⁸): indicated that (For the incremental ELM,) the target function can be approximated with more nodes added into the network by reducing the residual error to 0, as the 𝛃 of each new node is calculated based on the error of the network last status eₙ₋₁ as:\n𝛃 = $\\frac{⟨eₙ₋₁, 𝐇ₙʳ⟩}{‖𝐇ₙʳ‖²}$\nThe numerator is the inner product measuring the distance from the nth (random) hidden node 𝐇ₙʳ to be added and the error of the network with n-1 hidden nodes.\nSo the output of the newly added hidden nodes 𝐇ₙʳ are getting smaller and smaller, because they are learning something from the residual error.\nⅢ. Proposed Bidirectional ELM Method A. Structure of the Proposed Bidirectional ELM Method Two types of node, the node with odd index {2n-1} has random 𝐚,b, while the 𝐚,b of the node with even index {2n} are calculated based on the residual error of the network with an odd number of nodes at the last moment.\nTheir output weights both are calculated based on Lemma 1. The 𝐇 of the even node is from residual error, not from the random a,b.\nThe odd node aims to approximate the target through 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁, where 𝐇₂ₙ₋₁ is yield based on random a,b;\nThe odd node 2n-1 approximates the previous residual error with random generated a,b;\nBut the even node approximates the residual error 𝐞₂ₙ₋₁ through 𝐇₂ₙᵉβ₂ₙ with calculated a,b, where 𝐇₂ₙᵉ is yield with the weights a,b solved based on the residual error 𝐞₂ₙ₋₁ from the target (the job hasn\u0026rsquo;t done by all the previous nodes)\nBi-direction means the approximation is learned from both the target and the error, where the odd node (β₂ₙ₋₁) solved by the target, while the even node (β₂ₙ) calculated by the error. So a pair of odd node and even node is a complete step toward to the target.\nBi-directional means H₂ₙᵉ is calculated first from eq.(6); Then it is calculated again using the ^a,^b, which are solved based on H₂ₙᵉ, to get the updated ^H₂ₙᵉ, which is used to calculate the output weight for the next random odd node based on the Lemma 1.\nflowchart LR subgraph in[inputs] x1 \u0026 xe[\"⋮\"] \u0026 xn end rand((\"random\\n 𝐚,b\")) calculated((\"solved\\n 𝐚,b\")) subgraph hid[hidden] H1 \u0026 he[\"⋮\"] \u0026 h2n-1 \u0026 h2n end x1 \u0026 xe \u0026 xn --\u003e rand --\u003e h2n-1[\"𝐇₂ₙ₋₁ʳ\"] ---|Lemma 1| β2n-1((\"β₂ₙ₋₁\"))--\u003e e2n-1[\"e₂ₙ₋₁\"] x1 \u0026 xe \u0026 xn --\u003e calculated --\u003e h2n[\"𝐇₂ₙᵉ\"] ---|Lemma 1| β2n((\"β₂ₙ\"))--\u003e e2n[\"e₂ₙ\"] h2n -.- |\"⬅ inv of\\n β₂ₙ₋₁\"| e2n-1 calculated -.-|\"⬅ inv of\\n 𝐱\"| h2n The dot lines represent the inverse calculation. β₂ₙ₋₁ is derived from the network residual error of the last status.\nBlock diagram:\nflowchart TB init[Initialization: \\n Given training set,\\n expect accracy η and \\n let #hidden nodes L=0] --\u003e incre[L = L+1] --\u003e OdEv{\"Is L \\n odd or even?\"} OdEv --\u003e|L=2n+1| I-ELM --\u003e calcE[Calculate \\n residual error E] OdEv --\u003e|L=2n| Theorem2 --\u003e update[\"Update ^H_L = ^𝐚_L 𝐱 + ^b)\\n and calculate the output weight \\n β_L based on eq.(7)\"] --\u003e calcE subgraph Theorem2 direction TB calcH[\"Calculate output matrix \\n H_L basd on eq.(6)\"] --\u003e calcab[\"Calculate hidden-node parameters \\n (^𝐚_L,^b_L) based on eq.(14)\"] end subgraph I-ELM direction TB rand[\"Randomly assign hidden-node\\n parameters (𝐚_L,b_L) \\n and obtain\\n output matrix H_L\"] --\u003e |Lemma 1| outW[\"Calculate the output weight \\n βL according to eq.(8)\"] end calcE --\u003e thres{\"‖E‖\u003cη\"} --\u003e |Yes| END %%thres ----\u003e|No| incre %% mess up the chart incre o---o|No| thres B. Bidirectional ELM method Theorem 1 states the target function 𝑓 is approximated by the existing network 𝑓₂ₙ₋₂ plus the last two nodes: 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁ and 𝐇₂ₙᵉβ₂ₙ, when n→∞. On the other hand, the residual error the network is 0:\n$lim_{n→∞}‖𝑓-(𝑓₂ₙ₋₂ + 𝐇₂ₙ₋₁ʳβ₂ₙ₋₁ + 𝐇₂ₙᵉβ₂ₙ)‖ = 0$\nwhere the sequence of the 𝐇₂ₙᵉ (the output of the even node calculated from the feedback error) is determined by the inverse of last output weight β₂ₙ₋₁:\n$𝐇₂ₙᵉ = e₂ₙ₋₁ ⋅(β₂ₙ₋₁)⁻¹$ (6)\nThat means $𝐇₂ₙᵉ ⋅ β₂ₙ₋₁ = e₂ₙ₋₁$, this even node is approaching the last residual error based on the known output weight β₂ₙ₋₁.\nThen its output weight can be calculated based on the Lemma 1 as:\n$β₂ₙ = \\frac{⟨e₂ₙ₋₁, 𝐇₂ₙᵉ⟩}{‖𝐇₂ₙᵉ‖²}$ (7)\nOnce this even node is determined, the corresponding residual error e₂ₙcan be generated, and also the output weight of the next odd node (with 𝐇₂ₙ₊₁ʳ from random weights) can be calculated based on Lemma 1:\n$β₂ₙ₊₁ = \\frac{⟨e₂ₙ, 𝐇₂ₙ₊₁ʳ⟩}{‖𝐇₂ₙ₊₁ʳ‖²}$ (8)\nTheorem 2 further states the updated ^𝐇₂ₙᵉ calculated with the optimal ^a, ^b solved based on 𝐇₂ₙᵉ by the least-square (pseudo inverse) can also let the residual error converge to 0.\nRemark 1 clarifies the differences between this method and I-ELM, that is the input weights and bias of even nodes are calculated, not randomly generated. And the output weights are set similarly based on Lemma 1.\nBased on the eq. 6, the Δ = ‖e₂ₙ₋₁‖² + ‖e₂ₙ‖² can be reformalized to an ellipse curve.\nCode Training needs to store parameters for each node. And testing needs to query each node sequentially.\nReference G. B. Huang, L. Chen, and C. K. Siew, “Universal approximation using incremental constructive feedforward networks with random hidden nodes,” IEEE Trans. Neural Netw., Jul.2006. ","date":"2023-02-10T19:29:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-bidirec/","title":"read: B-ELM"},{"content":"Source video: 深度学习-啃花书0103伪逆矩阵最小二乘 (2021-07-22) - 科研小灰灰\nSolve linear regression For a n-dimensional linear regression problem,\nThere are N input samples: x₁,x₂,\u0026hellip;,$x_N$, where each sample is a n-dimension vector xᵢ∈ ℝⁿ Their corresponding target outputs are: y₁,y₂,\u0026hellip;,$y_N$, where each output is a scalar yᵢ∈ ℝ Hence, these data can be represented as a linear system:\n$$ \\begin{cases} x₁₁a₁ + x₁₂a₂ + \u0026hellip; + x₁ₙaₙ = y₁ \\\\ x₂₁a₁ + x₂₂a₂ + \u0026hellip; + x₂ₙaₙ = y₁ \\\\ \\vdots \\\\ x_{N1}a₁ + x_{N2}a₂ + \u0026hellip; + x_{Nn}aₙ = y_N \\\\ \\end{cases} $$\nFurther, this system can be represented with matrix and vector:\n$$ \\begin{bmatrix} x₁₁ \u0026amp; x₁₂ \u0026amp; … \u0026amp; x₁ₙ \\\\ x₁₁ \u0026amp; x₁₂ \u0026amp; … \u0026amp; x₁ₙ \\\\ ⋮ \u0026amp; ⋮ \u0026amp; ⋱ \u0026amp; ⋮ \\\\ x_{N1} \u0026amp; x_{N2} \u0026amp; … \u0026amp; x_{Nn} \\end{bmatrix} \\begin{bmatrix} a₁ \\\\ a₂ \\\\ ⋮ \\\\ aₙ \\end{bmatrix} = \\begin{bmatrix} y₁ \\\\ y₂ \\\\ ⋮ \\\\ y_N \\end{bmatrix} $$\nThe objective of the linear regression is to solve the weights vector: $[ ^{^{a₁}_{a₂}} _{^{\\ ⁞}_{aₙ}} ]$ from the linear equation: $𝐗_{N×n} 𝐚_{n×1} = 𝐘_{N×1}$.\nIf N=n (the coefficient matrix is a square matrix), and the data matrix $𝐗_{N×n}$ is a invertible matrix, then there will be 𝐚=𝐗⁻¹𝐘, such that the weights vector is determined directly.\nBut in general, the number of samples N is not equal to the number of features n (N ≠ n), that is 𝐗 is not invertible and 𝐚 cannot be represented as 𝐗⁻¹𝐘.\nShift from 𝐗 to 𝐗ᵀ𝐗 Therefore, when the 𝐚 cannot be reached directly, the solution should be as close to the optimal as possible.\nThat means the objective is to minimize the distance of two vectors:\nJ=‖𝐗𝐚-𝐘‖² (without constraints)\nAnd the optimal solution is obtained when\n∂J/∂𝐚 = 𝐗ᵀ(𝐗𝐚-𝐘) = 0 ⇒ 𝐗ᵀ𝐗𝐚 = 𝐗ᵀ𝐘.\nNow, the previous 𝐗 is shifted to here 𝐗ᵀ𝐗 ∈ ℝⁿᕁⁿ, which is a square matrix. And if 𝐗ᵀ𝐗 is invertible, then the optimal 𝐚 can be calculated in one-shot.\nIs 𝐗ᵀ𝐗 invertible? An invertible matrix has to satisfy 2 conditions: it\u0026rsquo;s a square matrix and its rank equals to the number of variables n (#columns).\nAccording to this video, there are two cases:\nIf N \u0026gt; n, for example N=5, n=3, then (𝐗ᵀ𝐗)₃ₓ₃ is inverible generally. So\n𝐚=(𝐗ᵀ𝐗)⁻¹𝐗ᵀ𝐘,\nwhere the coefficient in front of 𝐘, (𝐗ᵀ𝐗)⁻¹𝐗ᵀ, is called the pseudo-inverse matrix. And 𝐚 = (𝐗ᵀ𝐗)⁻¹𝐗ᵀ𝐘 is called the least-square solution. (最小二乘解)\n(Because 𝐗 has no inverse matrix, so we find its \u0026ldquo;pseudo\u0026rdquo; inverse matrix. Or if 𝐗 is invertible, 𝐗⁻¹=(𝐗ᵀ𝐗)⁻¹𝐗ᵀ, they\u0026rsquo;re equivalent, but the latter suits more general scenarios.). If N \u0026lt; n, for example N=3, n=5, then (𝐗ᵀ𝐗)₅ₓ₅ is not invertible, because: rank(𝐗ᵀ𝐗) ≤ rank(𝐗₃ₓ₅) ≤ N=3 \u0026lt; n=5.\nIn this case, 𝐚 cannot be calculated as (𝐗ᵀ𝐗)⁻¹𝐗ᵀ𝐘.\nThe problem can be understood that there are too many parameters (n is too high). When the parameters are much more than samples, there will be overfitting. And one of the solutions is regularization.\nEffect of the regularization term Since the reason why the optimal solution of the loss function J cannot be solved in one-shot is that the there are too many parameters, a regularization is added to the loss funciton as follows:\n$$J = ‖𝐗𝐚-𝐘‖² + λ‖𝐚‖², λ\u0026gt;0$$\nThen the derivative becomes:\n∂J/∂𝐚 = 𝐗ᵀ𝐗𝐚 - 𝐗ᵀ𝐘 + λ𝐚 = 0.\nBy moving items, the equation becomes:\n(𝐗ᵀ𝐗 + λ𝐈)𝐚 = 𝐗ᵀ𝐘,\nwhere the (𝐗ᵀ𝐗 + λ𝐈) is invertible. The proof is as follows.\nProof Since 𝐗ᵀ𝐗 is a symmetric matrix, it can be diagonalized. Thus, 𝐗ᵀ𝐗 can be written as:\n$$ 𝐗ᵀ𝐗 = 𝐏⁻¹ \\begin{bmatrix} λ₁ \u0026amp; \u0026amp; \\\\ \u0026amp; ⋱ \u0026amp; \\\\ \u0026amp; \u0026amp; λₙ \\end{bmatrix} 𝐏 $$\nThe determinant of 𝐗ᵀ𝐗 is: |𝐗ᵀ𝐗| = |𝐏⁻¹| ⋅ |$^{^{λ₁}_{\\quad ⋱}} _{\\qquad λₙ}$| ⋅ |𝐏| = λ₁ ⋅ λ₂ … ⋅ λₙ\nAnd λ₁, λ₂ …, λₙ are the eigen values for the 𝐗ᵀ𝐗\nThen the invertibility can be judged from this determinant:\nIf |𝐗ᵀ𝐗| = 0, then 𝐗ᵀ𝐗 is not invertible, because there are some zero lines in the matrix (after elementary row operations), that means r(𝐗ᵀ𝐗) \u0026lt; n. But if |𝐗ᵀ𝐗| \u0026gt; 0, the matrix 𝐗ᵀ𝐗 is invertible, because it has full rank, which is equal to the number of lines of rows. ² Analyze the two cases without and with adding the regularization term:\nOnly the 𝐗ᵀ𝐗:\nLet this matrix be enclosed by a non-zero real row vector 𝛂ᵀ and its column vector 𝛂 to constuct a quadratic form: 𝛂ᵀ(𝐗ᵀ𝐗)𝛂, which is used to characterize the definiteness of 𝐗ᵀ𝐗. ³ Based on the combination law of the matrix multiplication, it can be written as: (𝛂ᵀ𝐗ᵀ)(𝐗𝛂) = (𝐗𝛂)ᵀ(𝐗𝛂), which is the norm of the vector 𝐗𝛂. Because the norm is ≥ 0 definitely, the 𝛂ᵀ(𝐗ᵀ𝐗)𝛂 ≥ 0 (indicating 𝐗ᵀ𝐗 is positive semi-definite matrix). Based on the properties of quadratic form, eigen values λᵢ of 𝐗ᵀ𝐗 are all ≥ 0 Then the above determinant is |𝐗ᵀ𝐗|= λ₁ ⋅ λ₂ … ⋅ λₙ ≥ 0, so when |𝐗ᵀ𝐗| = 0, the rank of the matrix 𝐗ᵀ𝐗 is not full (≠ n), so the matrix 𝐗ᵀ𝐗 is not invertible. For 𝐗ᵀ𝐗+λ𝐈 (where λ is a hyper-parameter), it can be considered as a matrix:\nA quadratic form is constructed as: 𝛂ᵀ(𝐗ᵀ𝐗+λ𝐈)𝛂 = (𝛂ᵀ𝐗ᵀ)(𝐗𝛂) + λ𝛂ᵀ𝛂. The second item is always \u0026gt;0 because 𝛂 is not a zero vector, so its norm 𝛂ᵀ𝛂 \u0026gt; 0. Therefore, the matrix (𝐗ᵀ𝐗+λ𝐈) is a positive definite matrix. The eigen values λᵢ of (𝐗ᵀ𝐗+λ𝐈) are all \u0026gt; 0. The determinant of 𝐗ᵀ𝐗+λ𝐈 is the product of its eigen values, i.e., the determinant |𝐗ᵀ𝐗+λ𝐈|\u0026gt;0. So the matrix 𝐗ᵀ𝐗+λ𝐈 has full rank, and 𝐗ᵀ𝐗+λ𝐈 is invertible.\nTherefore, the optimal solution can be solved as 𝐚 = (𝐗ᵀ𝐗 + λ𝐈)⁻¹ 𝐗ᵀ𝐘. This is called the \u0026ldquo;least square - least norm solution\u0026rdquo;. (最小二乘-最小范数解)\nL2 regularization is originally added to make the 𝐗ᵀ𝐗 invertible.\n(bilibili search: \u0026ldquo;伪逆矩阵\u0026rdquo;)\ntodo: 01 2、最小二乘与pca（新） - 深度之眼官方账号-bilibili\ntodo: 【熟肉】线性代数的本质 - 06 - 逆矩阵、列空间与零空间 - 3B1B -bili\ntodo: 【26 深入理解逆矩阵】- cf98982002 -bili\nUse GPU to solve inverse faster? (DDG search: \u0026ldquo;矩阵求逆 gpu\u0026rdquo;)\nThe acceleration ratio of GPU to CPU is more than 16 times. ⁴.\nHow to solve inverse matrix? 4 ways ⁵:\nGaussian Eliminate LU decomposition, commenly used by computer because it can be performed parallelly. SVD decomposition QR decomposition Ref 【俗说矩阵】行列式等于0意味着什么？你一定要了解哦~ - 晓之车高山老师 - bilibili Definite matrix -wiki GPU矩阵计算是否会更快？（基于Pytorch） - 半个冯博士的文章 - 知乎 求逆矩阵的4种方法? - 疯狂绅士的回答 - 知乎 ","date":"2023-02-04T12:23:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/pseudo-inverse/","title":"memo: Calc | Pseudo-Inverse"},{"content":"Source video: 【俗说矩阵】矩阵乘法不神秘，揭开面纱很容易！\nGiven two linear systems:\n$$ \\begin{array}{cc} \\begin{cases} x₁ + 2x₂ = y₁ \\\\ 3x₁ + 4x₂ = y₂ \\end{cases} \u0026amp; \\begin{cases} 5y₁ + 6y₂ = 23 \\\\ 7y₁ + 8y₂ = 31 \\end{cases} \\end{array} $$\nUnlike the previous example, the constant terms of the left system is not fixed, but variables y₁, y₂. And the variables in the right system are y₁, y₂.\nNow, How to solve x₁, x₂ based on these 2 systems (compound equation system)?\nThe first step is to eliminate the intermediate variables y₁, y₂. One of methods is substituting the y₁, y₂ into the left system, so as to get a compound matrix:\n$$ \\begin{cases} 5(x₁ + 2x₂) + 6(3x₁ + 4x₂) = 23 \\\\ 7(x₁ + 2x₂) + 8(3x₁ + 4x₂) = 31 \\end{cases} ⇒ \\begin{cases} 23x₁ + 34x₂ = 23 \\\\ 31x₁ + 46x₂ = 31 \\end{cases} $$\nThe above three system of equations can be represented with matrices and vectors:\n$$ \\begin{array}{ccc} \\begin{array}{c} \\{^{x₁ + 2x₂ = y₁} _{3x₁ + 4x₂ = y₂} \\\\ ⇓\\\\ [^{1\\ 2}_{3\\ 4}] [^{x₁}_{x₂}] = [^{y₁}_{y₂}] \\\\ ⇓\\\\ f([^{x₁}_{x₂}]) = [^{y₁}_{y₂}] \\end{array} \u0026amp; \\begin{array}{c} \\{^{5y₁ + 6y₂ = 23} _{7y₁ + 8y₂ = 31} \\\\ ⇓\\\\ [^{5\\ 6}_{7\\ 8}] [^{y₁}_{y₂}] = [^{23}_{31}] \\\\ ⇓\\\\ g([^{y₁}_{y₂}]) = [^{23}_{31}] \\end{array} \u0026amp; \\begin{array}{c} \\{^{23x₁ + 34x₂ = 23} _{31x₁ + 46x₂ = 31}\\\\ ⇓\\\\ [^{23\\ 34}_{31\\ 46}] [^{x₁}_{x₂}] = [^{23}_{31}] \\\\ ⇓\\\\ g(f([^{x₁}_{x₂}])) = g∘f([^{x₁}_{x₂}]) = [^{23}_{31}] \\end{array} \\end{array} $$\nIn fact,the coefficient matrix maps the unknowns vector to the outcome vector. And the above 3rd system is a compound mapping from $[^{x₁}_{x₂}]$ to $[^{23}_{31}]$\nTherefore, each coefficient matrix is a mapping function: $[^{1\\ 2}_{3\\ 4}] ⇔ f$; $[^{5\\ 6}_{7\\ 8}]⇔ g$; and $[^{23\\ 34}_{31\\ 46}] ⇔ g∘f$\nCompound mapping matrix The relation between the compound mapping and its component mapping matrix.\nThe compound mapping matrix comes from \u0026ldquo;merging\u0026rdquo; two system through multiplication and add as follow:\n$$g∘f ⇔ [^{23\\ 34}_{31\\ 46}] = [^{5×1+6×3 \\quad 5×2+6×4} _{7×1+8×3 \\quad 7×2+8×4} ] = [^{5\\ 6}_{7\\ 8}]∘[^{1\\ 2}_{3\\ 4}] $$\nTherefore, this kind of compound operation rules between 2 matrices can be defined as matrix multiplication.\nFor the sake of convenience, the ring operator is omitted, then the matrix multiplication is written as: $[^{5\\ 6}_{7\\ 8}] [^{1\\ 2}_{3\\ 4}]$. Obviously, the result of 2-matrices multiplication is still a matrix.\nLaw of matrix multiplication $$ \\begin{array}{c} _{} \\\\ ^{r_{L1}} _{r_{L2}} [^{5\\ 6} _{7\\ 8} ] \\end{array} \\begin{array}{c} _{c_{L1}}\\ _{c_{L2}} \\\\ [^{1\\ 2} _{3\\ 4} ] \\end{array} \\begin{array}{c} _{}\\\\ = \\end{array} \\begin{array}{c} _{} \\\\ [^{5×1+6×3 \\quad 5×2+6×4} _{7×1+8×3 \\quad 7×2+8×4} ] \\end{array} $$\nr denotes row; c denotes column; L means left matrix; R means the right matrix. And use $[^{a₁₁\\ a₁₂} _{a₂₁\\ a₂₂} ]$ to represent the 4 elements in the outcome matrix.\na₁₁ is the sum of the products of the corresponding elements in the first row in the left matrix and the first column in the right matrix, i.e., the inner product of two vectors: $a₁₁ = r_{L1}⋅c_{R1} = (5,6)⋅(1,3)$\nSimilarly,\n$a₁₂ = r_{L1}⋅c_{R2} = (5,6)⋅(2,4)$; $a₂₁ = r_{L2}⋅c_{R1} = (7,8)⋅(1,3)$; $a₂₂ = r_{L2}⋅c_{R2} = (7,8)⋅(2,4)$; Rule: Take a row vector from the left matrix, take a column vector from the right matrix, and calculate the inner product of them, then the element position is determined based on the index of row and column.\n04:18/08:58\n","date":"2023-02-03T11:00:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/06_%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/","title":"watch: LA - 高山 06 | Matrix multiplication"},{"content":"Source video: 【俗说矩阵】用矩阵求非齐次线性方程组轻轻松松，几步就能搞定！\n3 cases of solutions There are 3 cases of solutions for a non-homogeneous linear system:\nA single unique solution Infinitely many solutions No solution Augmented matrix When performing Gaussian elimination, the equations results will also change. Hence, the coefficient matrix and the \u0026ldquo;results\u0026rdquo; column(s) are composed together forming the augmented matrix. Such that the \u0026ldquo;two matrices\u0026rdquo; will perform the same elementary row operations.\n$$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 5 \\\\ x₁ + 6x₂ + 7x₃ = 9 \\\\ x₁ + 10x₂ + 6x₃ = 8 \\\\ \\end{cases} $$\n$$ \\begin{array}{cc} A = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 6 \u0026amp; 7 \\\\ 1 \u0026amp; 10 \u0026amp; 6 \\end{bmatrix} \u0026amp; A|b = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 1 \u0026amp; 6 \u0026amp; 7 \u0026amp; | 9 \\\\ 1 \u0026amp; 10 \u0026amp; 6 \u0026amp; | 8 \\end{bmatrix} \\end{array} $$\nThe augmented matrix can be leveraged to determine the case of the solution.\nSingle unique solution Performing elementary row operations on the augmented matrix:\nEliminate the unknown x₁ in eq. 2 and eq. 3 by performing -r1 + r2 and -r1+r3: $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 0 \u0026amp; 4 \u0026amp; 4 \u0026amp; | 4 \\\\ 0 \u0026amp; 8 \u0026amp; 3 \u0026amp; | 3 \\end{bmatrix} $$\nCancel the common factor 4 for the 2nd line by multiplying the 2nd line with 1/4. $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; | 1 \\\\ 0 \u0026amp; 8 \u0026amp; 3 \u0026amp; | 3 \\end{bmatrix} $$\nEliminate the unknown x₂ in eq. 3 by performing -8r2 + r3 : $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; | 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; | 1 \\\\ 0 \u0026amp; 0 \u0026amp; -5 \u0026amp; | -5 \\end{bmatrix} $$\nAfter that, the augmented matrix becomes row echelon form.\nNext, the unknowns in the system can be solved from bottom to top by substituting x₃ into eq. 2, and then substituting x₂, x₃ to eq. 1. $$ \\begin{cases} x₁+\u0026amp; 2x₂ +\u0026amp; 3x₃ = 5 \\\\ \u0026amp; x₂ +\u0026amp; x₃ = 1 \\\\ \u0026amp; \u0026amp; x₃ = 1 \\\\ \\end{cases} $$\nTherefore, the unique solution for this non-homogeneous linear system is: 𝐱 = $[^{_{x₁}}_{^{x₂}_{x₃}} ] = [^{_{2}}_{^{0}_{1}} ]$\nAlso, by looking at the coefficient matrix and the augmented matrix after elementary row operations, the number of their non-zeros lines are both 3, which equals to the number of unknowns.\nThus, the conclusion is that if the augmented matrix has the same number of non-zeros lines of the coefficient matrix, where they both are in row echelon form, but also the number of non-zeros lines equals to the number of unknowns, then this non-homogeneous linear system has a single unique solution.\nInfinitely many solutions $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 5 \\\\ x₁ + 3x₂ + 4x₃ = 6 \\\\ x₁ + 4x₂ + 5x₃ = 7 \\\\ \\end{cases} $$ Performing elementary row operations on the augmented matrix as follows:\nEliminate x₁ of row2 and row3 by -r1+r2 and -r1+r3 : $[^{_{1\\ 2\\ 3\\ |5}} _{^{1\\ 3\\ 4\\ |6}_{1\\ 4\\ 5\\ |7}}]$ ➔ $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 2\\ 2\\ |2}}]$\nCancel the commen factor of row3 by multiplying with 1/2 : $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 2\\ 2\\ |2}}]$ ➔ $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 1\\ 1\\ |1}}]$\nEliminate the unknowns x₂, x₃ in row3 by performing -r2+r3: $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 1\\ 1\\ |1}}]$ ➔ $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 0\\ 0\\ |0}}]$\nThen the equations become: $$ \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ = 5 \\\\ \u0026amp; x₂ +\u0026amp; x₃ = 1 \\\\ \u0026amp; \u0026amp; 0 = 0 \\\\ \\end{cases} $$\nBy representing x₁, x₂ with x₃, the solution vector becomes: 𝐱 = $[^{_{3-x₃}}_{^{1-x₃}_{x₃}} ]$\nThat means x₃ can be any value, and x₁ and x₂ can be any value too. Therefore, this non-homogeneous linear system has infinitely many solutions.\nHere, the number of non-zero lines in coefficient matrix and augmented matrix is less than the number of unknowns: r(𝐀|𝐛) = r(𝐀) = 2 \u0026lt; 3\nSo the conclusion is if the augmented matrix and the coefficient matrix in row echelon form have the same number of non-zero rows, which is less than #unknowns, then the non-homogeneous linear system has infinitely many solutions.\nNo solutions $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 5 \\\\ x₁ + 3x₂ + 4x₃ = 6 \\\\ x₁ + 4x₂ + 5x₃ = 9 \\\\ \\end{cases} $$\n$[^{_{1\\ 2\\ 3\\ |5}} _{^{1\\ 3\\ 4\\ |6}_{1\\ 4\\ 5\\ |9}}]$ $^{-r1+r2}_{-r1+r3}$ ➔ $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 2\\ 2\\ |4}}]$ ½×r3 ➔ $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 1\\ 1\\ |2}}]$ -r2+r3 ➔ $[^{_{1\\ 2\\ 3\\ |5}} _{^{0\\ 1\\ 1\\ |1}_{0\\ 0\\ 0\\ |1}}]$\nThe corresponding equations are: $$ \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ = 5 \\\\ \u0026amp; x₂ +\u0026amp; 4x₃ = 6 \\\\ \u0026amp; \u0026amp; 0 = 1 \\\\ \\end{cases} $$\nThe 3rd equation is a fault. That means no matther what values the unknowns are, this linear equation system cannot be satisfied. Hence, there is no solution.\nIn this situation, the number of non-zero rows in the coefficient matrix is not equal to the augmented matrix: r(𝐀|𝐛) = 3 ≠ r(𝐀) = 2.\nThe conclusion is if the number of non-zero lines in the augmented matrix mismatch with the coefficient matrix in row echelon form, then the non-homogeneous linear equation system has no solution.\nGeneral solution of infinitely many solution $$ \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ = 5 \\\\ \u0026amp; x₂ +\u0026amp; x₃ = 1 \\\\ \u0026amp; \u0026amp; 0 = 0 \\\\ \\end{cases} $$\nIts general solution being represented with x₃ is 𝐱 = $[^{_{3-x₃}}_{^{1-x₃}_{x₃}} ]$\nTo structure the solution, the constant terms are separated out and x₃ is replaced by k.\n𝐱 = $[^{_{3-x₃}}_{^{1-x₃}_{x₃}} ]$ = x₃$[^{_{-1}}_{^{-1}_{1}} ] + [^{_{3}}_{^{1}_{0}} ]$ = k$[^{_{-1}}_{^{-1}_{1}} ] + [^{_{3}}_{^{1}_{0}} ]$\nThis general solution consistitute two components: random number times a column vector k$[^{_{-1}}_{^{-1}_{1}} ]$ and a constant vector.\nIf the $k [^{_{-1}}_{^{-1}_{1}} ]$ is substituted into equation system individually, the results of all equations are 0s.\n$$ \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ = 0 \\\\ \u0026amp; x₂ +\u0026amp; x₃ = 0 \\\\ \u0026amp; \u0026amp; 0 = 0 \\\\ \\end{cases} $$\nSo this term is the general solution for the homogeneous linear equations system.\nAnd the constant vector can just make the system satisfied. Hence, it\u0026rsquo;s the specific solution of the non-homogeneous linear equation system.\nTherefore, the general solution for a non-homogeneous linear equation system is made up by two parts: the general solution of the corresponding homogeneous linear equation system and the specific solution of the non-homogeneous linear equation system.\nThe method to find the general solution of the corresponding homogeneous linear equation system has been introduced in the last video.\nFind the specific solution Based on the augmented matrix in row echelon form, the leading variables are x₁ and x₂, while x3 is the free variable.\nTheoratically, any one of the solution satifying the non-homogeneous linear equation system is a specific solution. So, the free varaible can be assigned with any value. And in practice, free variables are all set to 0 for the purpose of solving convinently.\nIf Letting x3 = 0, then x2=1, x1 =3. Thus, the specific vector 𝛈 = $[^{_{3}} _{^{1}}_{0}]$\nSteps of solving Write the augmented matrix 𝐀|𝐛 based on the equation system and determine the number of unknowns n.\n$$ \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ +\u0026amp; x₄ \u0026amp;= 3 \\\\ x₁ -\u0026amp; 4x₂ -\u0026amp; x₃ -\u0026amp; x₄ \u0026amp;= 1 \\\\ 2x₁ +\u0026amp; x₂ +\u0026amp; 4x₃ +\u0026amp; x₄ \u0026amp;= 5 \\\\ x₁ -\u0026amp; x₂ +\u0026amp; x₃ \u0026amp; \u0026amp;= 2 \\\\ \\end{cases} $$\n𝐀|𝐛 = $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 1 \u0026amp; | 3 \\\\ 1 \u0026amp; -4 \u0026amp; -1 \u0026amp; -1 \u0026amp; | 1 \\\\ 2 \u0026amp; 1 \u0026amp; 4 \u0026amp; 1 \u0026amp; | 5 \\\\ 1 \u0026amp; -1 \u0026amp; 1 \u0026amp; 0 \u0026amp; | 2 \\end{bmatrix} $$ , n = 4\nTransform the augmented matrix to row echelon form by performing elementray row operations. $[^{^{1\\ 2\\ 3\\ 1\\ | 3}_{1\\ -4\\ -1\\ -1\\ |1}} _{^{2\\ 1\\ 4\\ 1\\ | 5}_{1\\ -1\\ 1\\ 0\\ | 2}}]$ $^{_{-(-r1+r2)}} _{^{-(-2r1+r3)} _{-(-r1+r4)}}$ ➔ $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 6\\ 4\\ 2\\ |2}} _{^{0\\ 3\\ 2\\ 1\\ | 1}_{0\\ 3\\ 2\\ 1\\ | 1}}]$ ½r2 ➔ $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 3\\ 2\\ 1\\ |1}} _{^{0\\ 3\\ 2\\ 1\\ | 1}_{0\\ 3\\ 2\\ 1\\ | 1}}]$ $^{-(-r2+r3)} _{-(-r2+r4)}$ ➔ $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 3\\ 2\\ 1\\ |1}} _{^{0\\ 0\\ 0\\ 0\\ | 0}_{0\\ 0\\ 0\\ 0\\ | 0}}]$\nCheck the number of non-zero rows r(𝐀|𝐛) of the obtained simplified augmented matrix and r(𝐀) for the coefficient matrix. r(𝐀|𝐛) = r(𝐀) = 2 \u0026lt; 3\nDetermine the case of the situations:\nr(𝐀|𝐛) = r(𝐀) = n, single unique solution r(𝐀|𝐛) = r(𝐀) \u0026lt; n, inifinitely many solutions r(𝐀|𝐛) ≠ r(𝐀), no solution Because r(𝐀|𝐛) = r(𝐀) = 2 \u0026lt; 4, this non-homogeneous equation system has infinitely many solutions.\nFor the case of infinitely many solutions, determine the leading variables and free variables first, and solve the fundamental system of solution, then make up the general solution for the non-homogeneous equation system: General solution of the corresponding homogeneous equation system\nSpecific solution of the non-homogeneous equation system The leading variables are x1 and x2, while x3 and x4 are free variables.\nSolve the general solution of the homogeneous equation system by letting the result of all equations be 0. $$ \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ +\u0026amp; x₄ \u0026amp;= 0 \\\\ \u0026amp; 3x₂ +\u0026amp; 2x₃ +\u0026amp; x₄ \u0026amp;= 0 \\\\ 0 \u0026amp;= 0 \\\\ 0 \u0026amp;= 0 \\\\ \\end{cases} $$\nTransform the coefficient matrix to row echelon form $[^{^{1\\ 2\\ 3\\ 1\\ | 3} _{0\\ 3\\ 2\\ 1\\ |1}} _{^{0\\ 0\\ 0\\ 0\\ | 0}_{0\\ 0\\ 0\\ 0\\ | 0}}]$ Determine the leading variables and free variables. By assigning values to free variables orthogonally, the general solution for homogeneous equation system can be constructed. • Take x3,x4 as 0,1, then x2=-1/3, x1=-1/3; • Take x3,x4 as 1,0, then x2=-2/3, x1=-5/3; • The fundamental system of solution constitute $[^{^1_1}_{^0_{-3}}]$ and $[^{^5_2}_{^{-3}_0}]$ • General solution for homogeneous equation system: 𝐱\u0026rsquo; = k₁ξ₁ +k₂ξ₂ = k₁$[^{^1_1}_{^0_{-3}}]$ + k₂ $[^{^5_2}_{^{-3}_0}]$ Find the specific solution of the non-homogeneous equation system by letting all free variables be 0 and solving leading variables from bottom to top: Take x3,x4 as 0, then x2=1/3, x1=7/3. So the specific solution is 𝛈=$[^{^{7/3}_{1/3}}_{^0_0}]$\nAdd the above two parts together to get the final general solution of the non-homogeneous equation system. 𝐱 = 𝐱\u0026rsquo; + 𝛈 = k₁$[^{^1_1}_{^0_{-3}}]$ + k₂ $[^{^5_2}_{^{-3}_0}]$ + $[^{^{7/3}_{1/3}}_{^0_0}]$\n","date":"2023-02-03T10:39:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/03_%E8%A7%A3%E9%9D%9E%E9%BD%90%E6%AC%A1%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/","title":"watch: LA - 高山 03 | Solve Non-homogeneous Linear System"},{"content":"Source video：【俗说矩阵】 用矩阵解齐次线性方程组这要这么几步就搞定了！真的好简单 - 晓之车高山老师 - bilibili\nThere are 2 cases of solutions for homogeneous linear system:\nHave and only have the zero solution Exist non-zero solution besides the zero solution How to judge the case of the solution? Given a homogeneous linear system in 3 variables:\n$$ \\begin{cases} x₁ + \u0026amp; 2x₂ + \u0026amp;3x₃ = 0 \\\\ \u0026amp; 4x₂ + \u0026amp; 5x₃ = 0 \\\\ \u0026amp; \u0026amp; x₃ = 0 \\end{cases} $$\nSubstitute the bottom variable to the topper equations to solve other variables. And the solved result is a zero vector: $𝐱 = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$\nThe coefficient matrix is in row echelon form resulting from a Gaussian elimination.\n$$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3\\\\ 0 \u0026amp; 4 \u0026amp; 5\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nIn the above matrix, the first elements next to the right side of the steps line are all non-zero elements, while on the left-hand side of the steps, the elements are all 0.\nAll three rows are not all zeros. That means the number of rows that are not 0 is 3, which is the same as the number of the unknowns. Hence, only if the number of non-zero rows in the echelon-form coefficient matrix is equal to the number of unknowns, the homogeneous linear system has only the zero solution. case1: only zero solution Given the following homogeneous linear system: $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 0 \\\\ x₁ + 4x₂ + 5x₃ = 0 \\\\ x₁ + 10x₂ + x₃ = 0 \\end{cases} $$\nIts coefficient matrix ($[^{_{1\\ 2\\ 3}}_{^{1\\ 6\\ 8}_{1\\ 10\\ 6}}]$) is not in row echelon form. But this linear system can be simplized through the (row) Gaussian elimination, i.e., the elementary row operations applied on its coefficients matrix.\nWith the row echelon form as the objective, the elementary row operations are operated on the coefficients matrix.\nUse row-1 to cancel the first element (1) in the row2 and row3 by multiplying the 1st row with -1 and adding it to the 2nd and 3rd rows. $[^{_{1\\ 2\\ 3}}_{^{1\\ 6\\ 8}_{1\\ 10\\ 6}}]$ ➔ $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 3}}]$\nUse row-2 to cancel the second element (8) in the 3rd row by multiplying the 2nd row with -2 and adding it to the 3rd rows. $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 3}}]$ ➔ $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ -7}}]$\nMultiply the 3rd row with -1/7. $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ -7}}] ➔ [^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ 1}}]$\nConclusion: Perform the elementary row operations on the coefficient matrix to transform it as in the row echelon form. Then if the number of non-zero rows is equal to the number of unknowns, the homogeneous linear system only has zero solution.\ncase2: infinitely many solution A homogeneous linear system is as follow: $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 0 \\\\ x₁ + 6x₂ + 8x₃ = 0 \\\\ x₁ + 10x₂ + 14x₃ = 0 \\end{cases} $$\nEliminate the elements from bottom to top and from left to righ:\nUse row-1 to cancel the first element of the 2nd and 3rd rows by \u0026ldquo;-r1+r2\u0026rdquo; and \u0026ldquo;-r1+r3\u0026rdquo;. $[^{_{1\\ 2\\ 3}}_{^{1\\ 6\\ 8}_{1\\ 10\\ 13}}]$ ➔ $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 10}}]$\nUse row-2 to cancel the second element of the 3rd row by \u0026ldquo;-2r2+r3\u0026rdquo;. $[^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 8\\ 10}}] ➔ [^{_{1\\ 2\\ 3}}_{^{0\\ 4\\ 5}_{0\\ 0\\ 0}}]$\nThe bottom row of this echelon-form matrix is all 0. Hence, there are only 2 independent equations essiensially: $$ \\begin{cases} x₁ + \u0026amp; 2x₂ +\u0026amp; 3x₃ = 0 \\\\ \u0026amp; 6x₂ +\u0026amp; 8x₃ = 0 \\\\ \u0026amp; \u0026amp; 0 = 0 \\end{cases} $$\nIf the number of non-zero rows (2) is less than the number of unknowns (3), then there exists non-zero solutions besides the zero solution.\nAnd we can give a general solution for this system.\nWhat is general solution? Suppose the homogeneous linear system is: $$ \\begin{cases} x₁+x₂=0\\\\ 2x₁+2x₂=0 \\end{cases} $$\nAs long as the 2 variables have opposite signs: $\\rm \\{^{x₁=0}_{x₂=0}\\ or\\ \\{^{x₁=1}_{x₂=-1}\\ or\\ \\{^{x₁=2}_{x₂=-2}\\ or \u0026hellip; $,\nthe system is satisfied.\nSo the solutions has a general form: $\\{^{x₁=k}_{x₂=-k}$\nWrite the solution as a column vector: $𝐱 = [^{x₁}_{x₂} ] = k [^{\\ 1}_{-1}]$\nHow to find the general solution? How to solve the general solution when there are infinitely many solutions?\n$$ \\begin{cases} x₁ + \u0026amp; 2x₂ +\u0026amp; 3x₃ = 0 \\\\ \u0026amp; 6x₂ +\u0026amp; 8x₃ = 0 \\\\ \u0026amp; \u0026amp; 0 = 0 \\end{cases} $$\nSimilarly, solve the variables from bottom to top. Fix the x₃, then x₁ and x₂ can be represented with x₃. Thus the solution is written as: $\\begin{bmatrix} -1/2⋅x₃\\\\ -5/4⋅x₃\\\\ x₃ \\end{bmatrix} = x₃⋅\\begin{bmatrix}-1/2\\\\ -5/4\\\\ 1 \\end{bmatrix}$\nHere, any x₃ can be the solution of this system. That purely numerical column vector constitutes the fundamental system of solutions(基础解系) for this homogeneous linear system.\nThe fundamental system can be scaled by any factor. For example, let x₃=-4k, it becomes a scaled fundamental system. Then the solution $k \\begin{bmatrix}2\\\\ 5\\\\ -4 \\end{bmatrix}$ containing only integars is the general solution. 2 free variables Given a homogeneous linear system: $$ \\begin{cases} x₁ + 2x₂ + 3x₃ = 0 \\\\ 2x₁ + 4x₂ + 6x₃ = 0 \\\\ 3x₁ + 6x₂ + 9x₃ = 0 \\end{cases} $$\nPerform the elementary row operations for its coefficient matrix: $[^{_{1\\ 2\\ 3}} _{^{2\\ 4\\ 6}_{3\\ 6\\ 9}} ]$ by \u0026ldquo;-2r1+r2\u0026rdquo; and \u0026ldquo;-3r1+r3\u0026rdquo;. The echelon-form matrix is $[^{_{1\\ 2\\ 3}} _{^{0\\ 0\\ 0}_{0\\ 0\\ 0}} ]$, where the 2nd and 3rd rows are all 0. That means only the 1st equation is valid. $\\{^{x₁ + 2x₂ + 3x₃ = 0}_{^{0=0}_{0=0}}$\nTherefore, only if the x₂ and x₃ both are set, then x₁ can be determined.\nIn other words, x₁ has to be represented by x₂ and x3 collectively: $\\begin{bmatrix} -2x₂-3x₃\\\\ x₂\\\\ x₃\\end{bmatrix}$\nTo clarify the representation, it can be broken down to separate different variables:\n$$ [^{_{-2x₂-3x₃}} _{^{x₂}_{x₃}} ] = [^{_{-2x₂}} _{^{x₂}_{0}} ] + [^{_{-3x₃}} _{^{0}_{x₃}} ] = x₂[^{_{-2}} _{^{1}_{0}} ] + x₃[^{_{-3}} _{^{0}_{1}} ] $$\nThen the general solution is a linear combination of two numerical column vectors with x₂,x₃ as their coefficients. And the x₁ in these 2 vector (-2 and -3) are the solution when x₂,x₃ are the corresponding values.\nThat means when x₂,x₃ are assigned (orthogonally) repeatedly with (1,0) and (0,1), two x₁ can be determined and then 2 numerical column vectors are generated, which constitute the fundamental system of solutions for this linear system.\nBy substituting x₂,x₃ with constants k₁,k₂, the genearl solution is the linear combination of all the numerical column vector in the fundamental system: $$ k₁\\begin{bmatrix} -2\\\\ 1\\\\ 0\\end{bmatrix} + k₂\\begin{bmatrix} -3\\\\ 0\\\\ 0\\end{bmatrix} $$\nComparing the above two general solutions: $k [^2_{^5_{-4}} ]$ and $k₁[^{_{-2}} _{^{1}_{0}} ] + k₂[^{_{-3}} _{^{0}_{1}} ]$, there is only 1 vector in the fundamental system of solutions for the former system, and there are 2 vectors in the fundamental system of solutions for the latter system. Obviously, this is related to the number of non-zero rows in the matrix in the row-echelon form.\nLeading variables \u0026amp; free variables Commonly, for a matrix in the row-echelon form, the first non-zero element next to the steps line corresponds to a leading variable. And also in the simplified system of equations, the leading variables are embodied as the coefficient of the first variable is not zero. The variables exposed at the start of each line are leading variables; And the other variables are called free variables.\n$$ \\begin{array}{cc} \\begin{array}{c} \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ = 0\\\\ \u0026amp; 4x₂ +\u0026amp; 5x₃=0\\\\ \u0026amp; \u0026amp; 0=0 \\end{cases} \\\\ ⇓\\\\ [^{1\\ 2\\ 3} _{^{0\\ 4\\ 5}_{0\\ 0\\ 0}} ] \\end{array} \u0026amp; \\begin{array}{c} \\begin{cases} x₁ +\u0026amp; 2x₂ +\u0026amp; 3x₃ = 0\\\\ \u0026amp; \u0026amp; 0=0\\\\ \u0026amp; \u0026amp; 0=0 \\end{cases} \\\\ ⇓\\\\ [^{1\\ 2\\ 3} _{^{0\\ 0\\ 0}_{0\\ 0\\ 0}} ] \\end{array} \\end{array} $$\nIn the above left system, x₁ and x₂ are leading variables. While in the right system, only x₁ is the leading variable.\nleading variables are variables whose column contains the leading entry of some row; And free variables are all the other variables. MST10030 notes wiki\nA few related conclusions:\nThe number of leading variables is equal to the number of non-zero rows in the matrix in the row echelon form, which is written as r(𝐀)\nIf there are total n unknowns, then the number of the free variables is t = n - r(𝐀).\nAll free variables need to be assigned orthogonally.\nIf there is only 1 free variable, then just assign it with 1 and calculate the other values (leading variables) in the numerical column vector constituting the fundamental system of solutions. If there are 2 free variables, they can be assigned with (1,0) and (0,1) twice separately\u0026hellip;. i.e., the number of free variables is the number of orthogonal assignmens required separately. The number of free variables is the number of vectors constituting the fundamental system of solutions.\nTo calculate the fundamental system of solutions, the free variables have to be assigned first, and then solve the other leading variables from bottom to top.\nComplete solution steps The steps of solving homogeneous linear system based on the coefficient matrix.\n$$ \\begin{cases} x₁ + 2x₂ + 3x₃ + x₄ = 0 \\\\ x₁ - 4x₂ - x₃ - x₄ = 0 \\\\ 2x₁ + x₂ + 4x₃ + x₄ = 0 \\\\ x₁ - x₂ + x₃ = 0 \\end{cases} $$\nWrite down the coefficient matrix 𝐀 and the number of unknowns n=4 based on the given equations system. 𝐀 = $[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}_{1\\ -4\\ -1\\ -1}} _{^{2\\ \\ 1\\ \\ 4\\ 1}_{1\\ -1\\ 1\\ 0}} ]$\nPerform the elementary on the coefficient matrix 𝐀 = $[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}_{1\\ -4\\ -1\\ -1}} _{^{2\\ \\ 1\\ \\ 4\\ 1}_{1\\ -1\\ 1\\ 0}} ]$ to reach the matrix in the row-echelon form.\nUse row-1 to cancel the first element in 2nd, 3rd, and 4th rows by \u0026ldquo;-r1+r2\u0026rdquo;, \u0026ldquo;-2r1+r3\u0026rdquo;, \u0026ldquo;-r1+r4\u0026rdquo;:\n𝐀 = $[^{^{1\\ \\ 2\\ \\ 3\\ \\ 1}_{0\\ -6\\ -4\\ -2}} _{^{0\\ -3\\ -2\\ -1}_{0\\ -3\\ -2\\ -1}} ]$ Multiply 2nd, 3rd, and 4th rows with some numbers to make common factor (\u0026quot;-0.5r2\u0026quot;, \u0026ldquo;-r3\u0026rdquo;, \u0026ldquo;-r4\u0026rdquo;)\n𝐀 = $[^{^{1\\ 2\\ 3\\ 1}_{0\\ 3\\ 2\\ 1}} _{^{0\\ 3\\ 2\\ 1}_{0\\ 3\\ 2\\ 1}} ]$ Use row-2 to cancel the 3rd and 4th rows by \u0026ldquo;-r2+r3\u0026rdquo;, \u0026ldquo;-r2+r4\u0026rdquo;: 𝐀 = $[^{^{1\\ 2\\ 3\\ 1}_{0\\ 3\\ 2\\ 1}} _{^{0\\ 0\\ 0\\ 0}_{0\\ 0\\ 0\\ 0}} ]$ Check the number of non-zero rows in this echelon-form matrix, r(𝐀)=2\nIf r(𝐀) = n, then this system only has zero solution.\nOtherwise, if r(𝐀) \u0026lt; n, this system exists non-zero solution. Find out the leading varibles based on the position of the down edges of the steps line: x₁, x₂. So the free variables are x₃ and x₄. Then x₃,x₄ are assigned with (1,0) and (0,1) repeatedly to get the 2 (=n-r(𝐀)) numerical column vector constituting the fundamental system of solutions.\nWhen $\\{^{x₃=1}_{x₄=0}$, the leading variables are $\\{^{x₁=-5/3}_{x₂=-2/3}$. Thus, one of the vectors in the fundamental system of solutions is $ξ₁ = [^{^{5}_{2}} _{^{-3}_{0}} ]$ When $\\{^{x₃=0}_{x₄=1}$, the leading variables are $\\{^{x₁=-1/3}_{x₂=-1/3}$. Thus, one of the vectors in the fundamental system of solutions is $ξ₂ = [^{^{1}_{1}} _{^{0}_{-3}} ]$ Combine linearly those two numerical column vectors by constants to get the general solution:\n𝐱 = k₁ξ₁ + k₂ξ₂ = $k₁[^{^{5}_{2}} _{^{-3}_{0}} ] + k₂[^{^{1}_{1}} _{^{0}_{-3}}]$\n","date":"2023-02-01T20:21:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/02_%E8%A7%A3%E9%BD%90%E6%AC%A1%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/","title":"watch: LA - 高山 02 | Solve Homogeneous Linear System"},{"content":"Source video: 【俗说矩阵】初等矩阵可一点也不初等，它居然有这么重要的意义！\n3 elementary row operations 换行：交换两行的位置 数乘：给某一行乘以非零常数k 倍加：将某一行的 k 倍加到另一行上 By means of these 3 kind of elementary row operations, a matrix can be transformed to row echelon form, which is useful in analysing and solving the system of linear equations.\nIdentical matrix $$ I = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nThe elements on the main diagonal are all 1 and the rest of elements in the martrix are all 0.\nIf multiplying a matrix with a identical matrix, the matrix doesn\u0026rsquo;t change. So the effect of I is similar to the 1 in numbers.\nA identical matrix can perform elementary row operations.\nElementary matrix 对单位矩阵 I 实施一次初等行变换。实施2次就不是了。wikipedia\n$$ I = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} ➔ \\begin{cases} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \u0026amp; \\text{Row switch: r1 ⟷ r2} \\\\ \\\\ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \u0026amp; \\text{Row multiplication: 3r1 ➔ r1} \\\\ \\\\ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 3 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \u0026amp; \\text{Row addition: 3r1+r2 ➔ r2} \\end{cases} $$\n初等矩阵只有3种：置换阵，数乘阵，倍加阵\nElementary matrices connect the matrix multiplication and elementary row operations.\nRow-switching transformations Row-multiplying transformations Row-addition transformations 对矩阵实施一次初等行变换 左乘一个初等矩阵，就是把对初等矩阵的初等行变换，施加到矩阵上。\n","date":"2023-02-01T12:41:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/10_%E5%88%9D%E7%AD%89%E7%9F%A9%E9%98%B5/","title":"watch: LA - 高山 10 | Elementary Matrix"},{"content":"Original Video：【俗说矩阵】矩阵原来这么简单！从二元一次方程组开始教你~ - 晓之车高山老师 - bilibili\nLinear equations system Linear equation in 2 variables (二元一次方程): An equation has 2 unknown variables with an exponent of 1.\nTwo linear equations in 2 variables can make up a system of linear equations (线性方程组). $$ \\begin{cases} x + y = 3 \\\\ 2x + 3y = 8 \\end{cases} $$\nGaussian elimination Two kinds of Gaussian elimination to solve this linear system:\nReformalize the 1st equation and substitute it into the 2nd one, such that it comes down to a linear equation in 1 variable. (代入消元法) $\\begin{cases}y=3-x\\\\ 2x+3y=8 \\end{cases}$ ➔ 2x+3(3-x)=8 ➔ $\\{^{x=1}_{y=2}$\nMultiply the equation with a certain number and add the 2 equations together to cancel a variable, such that only 1 linear equation is left. (加减消元法) $\\begin{cases}-2x-2y=-6\\\\ 2x+3y=8 \\end{cases}$ ➔ y=2 ➔ $\\{^{x=1}_{y=2}$\nGeometric interpretation A linear equation in 2 variables is a line on a plane. The solution of the linear system is the intersection of these lines.\nTwo kinds of linear system:\nHomogeneous system: all of the constant terms are zeroes. $\\begin{cases} x+y = 0\\\\ 2x+3y = 0 \\end{cases}$\nNonhomogeneous system: The constant term is not all zero. $\\begin{cases} x+y = 3\\\\ 2x+3y = 8 \\end{cases}$\nSolution of Homogeneous system has 2 cases The lines are all passing through the origin.\nIf the slopes are different ($\\{^{x+y=0}_{2x+3y=0}$ ➔ x=y=0), the only intersection of them is the origin. This means the homogenous system has only zero solution. If the slopes of them are identical ($\\{^{x+y=0}_{2x+2y=0}$ ➔ x=-y), the lines are overlapping. The solution set is a line: any point on the line is a solution of this system. That means there are infinitely many non-zero solutions besides the zero solution. There exists non-zero solution in this system. Solution of Non-homogeneous system has 3 cases Lines have distinct slopes ($\\{^{x+y=3}_{2x+3y=8}$), so there is only one intersection corresponding to the single unique solution.\nLines are overlapping ($\\{^{x+y=3}_{2x+2y=6}$). Any point on the line is a solution of this system. The system has infinitely many solutions.\nLines are parallel without any intersection ($\\{^{x+y=3}_{2x+2y=4}$), this system has no solution. (Its solution must not be zero.)\nTherefore, the assertion \u0026ldquo;The number of unknowns is the number of equations needed.\u0026rdquo; works only for the situation of \u0026ldquo;only zero solution\u0026rdquo; in a homogeneous system and \u0026ldquo;single unique solution\u0026rdquo; in a non-homogeneous system.\nIn the scenario with more variables, or the numbers of unknowns and equations are not equal, even if the gaussian elimination can be applied, the correct steps or the direction of elimination are hard to determine.\nHow to solve a linear system? A general homogeneous system is a combination of m linear equations in n unknowns.\n$$ \\begin{cases} a₁₁x₁ + a₁₂x₂ + \u0026hellip; + a₁ₙxₙ = 0 \\\\ a₂₁x₁ + a₂₂x₂ + \u0026hellip; + a₂ₙxₙ = 0 \\\\ \\dots \\\\ aₘ₁x₁ + aₘ₂x₂ + \u0026hellip; +aₘₙxₙ = 0 \\end{cases} $$\nwhere\nx₁, x₂, \u0026hellip;,xₙ are the n unkowns,\nThe coefficient aₘₙ, the first subscript represents the ordinal number of the equation, and the second subscript stands for the unknown that this coefficient multiplies with.\nFor example a₃₂ is the coefficient in the 3rd equation for the unknown x₃.\nCoefficients of each row cannot be all 0 and all of the parameters of an unknown cannot be all 0, so these m equations and n unknowns are effective.\nHomogeneous Linear system with 3 variables $$ \\begin{cases} x₁ + 3x₂ + 5x₃ = 0 \\\\ 2x₁ + 4x₂ + 6x₃ = 0 \\\\ 2x₁ + 5x₂ + 8x₃ = 0 \\end{cases} $$\nEach row is a inner product of two 3-dimensional vectors:\nx₁ + 3x₂ + 5x₃ = 0 ➔ (1 3 5)(x₁ x₂ x₃) = 0 2x₁ + 4x₂ + 6x₃ = 0 ➔ (2 4 6)(x₁ x₂ x₃) = 0 2x₁ + 5x₂ + 8x₃ = 0 ➔ (2 5 8)(x₁ x₂ x₃) = 0\nThere are 2 types of vector:\nRow vector (1 2 3) Column vector $[^1_{^2_3}]$ or (1 2 3)ᵀ Stack the above 3 expressions: $\\begin{bmatrix}1 \u0026amp; 3 \u0026amp; 5\\\\ 2 \u0026amp; 4 \u0026amp; 6\\\\ 2 \u0026amp; 5 \u0026amp; 8 \\end{bmatrix} ⋅ (x₁\\ x₂\\ x₃) = \\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix}$\nThe left array is matrix. This linear system can be regarded as a function: $f( (x₁\\ x₂\\ x₃) ) = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$\nwhich maps 3 variables to 3 zeros, i.e., maps a unknown (row) vector to a zero (column) vector 𝟎.\nTo normalize the notations, the unknown vector is written as a column vector 𝒙, such that the linear system can be represented by matrix:\n$$ \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5\\\\ 2 \u0026amp; 4 \u0026amp; 6\\\\ 2 \u0026amp; 5 \u0026amp; 8 \\end{bmatrix} \\begin{bmatrix} x₁\\\\ x₂\\\\ x₃ \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix} $$\nTherefore, the expression represents the mapping linearly from a column vector to another column vector.\n$f(\\begin{bmatrix} x₁\\\\ x₂\\\\ x₃ \\end{bmatrix}) = \\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$\nHere, the matrix is arranging all the coefficients into an array based on their positions. Hence, it\u0026rsquo;s called coefficient matrix 𝑨.\nThen this linear system can be expressed consiscely as: 𝑨𝒙=𝟎.\nFrom the view of function, the function $f(\\begin{bmatrix} x₁ \\\\ x₂ \\\\ x₃ \\end{bmatrix})=\\begin{bmatrix} 0\\\\ 0\\\\ 0 \\end{bmatrix}$ can be written as f(𝒙)=𝟎.\nThe coefficient matrix 𝑨 plays the role of function f, that means the coefficient matrix maps the unknown vector to zero vector.\nNon-homogeneous linear system A non-homogeneous linear system contains m linear equations with n unknowns, where their constant termsare not all 0.\n$$ \\begin{cases} a₁₁x₁ + a₁₂x₂ + \u0026hellip; + a₁ₙxₙ = b₁ \\\\ a₂₁x₁ + a₂₂x₂ + \u0026hellip; + a₂ₙxₙ = b₂ \\\\ \\dots \\\\ aₘ₁x₁ + aₘ₂x₂ + \u0026hellip; + aₘₙxₙ = bₙ \\\\ \\end{cases} $$\nThe notation is the same as homogeneous system and b₁, b₂,\u0026hellip;, bₙ are not all 0.\nFor instance, the following non-homogeneous linear system can be represented as matrix:\n$$ \\begin{cases} x₁ + 3x₂ + 5x₃ = 2 \\\\ 2x₁ + 4x₂ + 6x₃ = 4 \\\\ 2x₁ + 5x₂ + 8x₃ = 4 \\\\ \\end{cases} \\\\ ⇓ \\\\ \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5\\\\ 2 \u0026amp; 4 \u0026amp; 6\\\\ 2 \u0026amp; 5 \u0026amp; 8 \\end{bmatrix} \\begin{bmatrix} x₁\\\\ x₂\\\\ x₃ \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 4\\\\ 4 \\end{bmatrix} \\\\ ⇓ \\\\ 𝑨𝒙=𝐛 $$\nHere, the coefficient matrix 𝑨 maps the column vector to a known non-zero column vector: $$f(\\begin{bmatrix} x₁\\\\ x₂\\\\ x₃ \\end{bmatrix})=\\begin{bmatrix} 2\\\\ 4\\\\ 4 \\end{bmatrix} ➔ f(𝒙)=𝐛$$\nElementary row operations are equivalent to Gaussian elimination.\n","date":"2023-02-01T12:19:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/01_%E5%88%9D%E8%AF%86%E7%9F%A9%E9%98%B5/","title":"watch: LA - 高山 01 | Walk into the Matrix"},{"content":"Source video: 【俗说矩阵】逆矩阵原来要这么学！数学老师直呼内行！\nRow echelon form ➔ Identity matrix 𝐊₆ includes 6 elementary row operations that transform the matrix 𝐀 to row echelon form.\n$$ 𝐀₆ = 𝐊₆𝐀 \\\\ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 0 \u0026amp; 1 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} 0 \u0026amp; 1/2 \u0026amp; 0 \\\\ 1 \u0026amp; -1/2 \u0026amp; 0 \\\\ -1 \u0026amp; 1/4 \u0026amp; 1/2 \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 2 \u0026amp; 4 \u0026amp; 6 \\\\ 1 \u0026amp; 4 \u0026amp; 9 \\end{bmatrix} $$\n𝐀₆ can further perform elementary row operations to make the elements at the top right of the main diagonal 0, i.e., reaching an identity matrix.\n𝐀₆: $[^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 2} _{0\\ 0\\ 1}}]$ $\\overset{-2×r3+r2:\\ [^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ -2} _{0\\ 0\\ 1}}]}{{⇢}}$ 𝐀₇: $[^{_{1\\ 2\\ 3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]$ $\\overset{-3×r3+r1:\\ [^{_{1\\ 0\\ -3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]}{{⇢}}$ 𝐀₈: $[^{_{1\\ 2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]$ $\\overset{-2×r2+r1:\\ [^{_{1\\ -2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]}{{⇢}}$ 𝐀₉: $[^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}]$\nwhere,\n$$ [^{_{1\\ -2\\ 0}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] [^{_{1\\ 0\\ -3}} _{^{0\\ 1\\ 0} _{0\\ 0\\ 1}}] [^{_{1\\ 0\\ 0}} _{^{0\\ 1\\ -2} _{0\\ 0\\ 1}}] 𝐊₆𝐀 = 𝐊₉𝐀 = 𝐈 \\\\ 𝐊₉ = [^{_{-3\\ 7/4\\ 1⁄2}} _{^{3\\ -1\\ -1}_{-1\\ ¼\\ 1⁄2}}] $$\nThe above example indicates that there exist a matrix that can modify the matrix 𝐀 to 𝐈.\nLet 𝐀⁻¹ denote the 𝐊₉, and 𝐀⁻¹ is called the inverse matrix of 𝐀.\nSuch that there is 𝐀⁻¹𝐀 = 𝐈. Since the matrix multiplication doesn\u0026rsquo;t has commutative property, what does 𝐀𝐀⁻¹ equal?\n𝐀𝐀⁻¹ = [^{_{1\\ 3\\ 5} _{^{2\\ 4\\ 6}_{1\\ 4\\ 9}}}] [^{_{-3\\ 7/4\\ 1⁄2}} _{^{3\\ -1\\ -1}_{-1\\ ¼\\ 1⁄2}}] = 𝐈\nIn fact, 𝐀 and 𝐀⁻¹ are a pair of commutative matrices: 𝐀⁻¹𝐀 = 𝐀𝐀⁻¹ = 𝐈. Also, 𝐀 and 𝐀⁻¹ are the inverse matrix of each other: (𝐀⁻¹)⁻¹ = 𝐀\nAnalogy to reciprocal Given two numbers a, b, if ab = ba = 1, then b = a⁻¹, a≠0.\nAnd the identity matrix 𝐈 has the same effect as the number 1.\nTherefore, given two matrices 𝐀, 𝐁, if 𝐀𝐁 = 𝐁𝐀 = 𝐈, then 𝐁 = 𝐀⁻¹\nNo all matrix has its inverse matrix, which is like the number 0 doesn\u0026rsquo;t has its reciprocal.\nSo what kind of matrix has an inverse? And is the inverse unique? How to solve the inverse matrix?\nInvertible matrix An invertible matrix can perform multiple elementary row operations to become an identity matrix.\nIdentity matrix is a square matrix (#row = #colmuns) and its rank = #rows (= #cols). And the elementary row operations do not change the size and rank note4.\nTherefore, an invertible matrix must be an square matrix and its rank = #rows.\nBased on the commutative property that 𝐀⁻¹ and 𝐀 are the inverse matrices of each other, the 𝐀⁻¹ must also be a square matrix with the same rank of 𝐈.\n可逆的2个条件：1-方阵， 2-秩=列(行)数\nSince elementary row operation is that 𝐀 is multiplied by elementary matrix on the left. And the product of the series of elementary row operations that transformed 𝐀 to 𝐈 is represented as 𝐀⁻¹.\nTherefore, an invertible matrix and its inverse are both able to represented as the multiplication of multiple elementary matrix.\nSince the product of a series of elementary matrix must be invertible, a special case is that there is only a single elementary matrix. So, any of the elementary matrix is invertible.\nInverse is unique Let 𝐀𝐁 = 𝐁𝐀 = 𝐈 and 𝐀𝐂 = 𝐂𝐀 = 𝐈, then 𝐁 = 𝐁𝐈 = 𝐁(𝐀𝐂) = (𝐁𝐀)𝐂 = 𝐈𝐂 = 𝐂\n","date":"2023-02-01T11:00:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/11_%E9%80%86%E7%9F%A9%E9%98%B5%E6%A6%82%E5%BF%B5/","title":"watch: LA - 高山 11 | Concept of the Inverse Matrix"},{"content":" Wikipedia-ELM Controversy: RBF (1980s) raised the similar idea of ELM. Dispute about the originality of ELM: Origins of ELM Portal of ELM python toolbox: hpelm Facts ELM is ⁽¹⁾\na type of single hidden layer feedforward neural network (SLFN).\nThe parameters (𝐰,b) between input layer and hidden layer are set randomly. Thus, for N input n-dimensional samples and L hidden nodes, the output of the hidden layer is $𝐇 = 𝐗_{N×n} 𝐖_{n×L}+𝐛_{n×L}$\nOnly the number of hidden nodes needs to be predefined manually without other hyper-parameters.\nThe output weights are initialized randomly and solved based on the pseudo inverse matrix in one-shot.\nFor a n-dimensional sample 𝐱ⱼ and its target 𝐭ⱼ=[tᵢ₁, tᵢ₂, \u0026hellip;, tᵢₘ]ᵀ∈ ℝᵐ, the output of ELM with L hidden nodes is 𝐨ⱼ = ∑ᵢ₌₁ᴸ 𝛃ᵢ g(𝐰ᵢᵀ⋅𝐱ⱼ + bᵢ), where g(⋅) is activation function; 𝛃ᵢ is the weights of the ith ouput unit: 𝛃ᵢ=[βᵢ₁, βᵢ₂, \u0026hellip;, βᵢₙ]ᵀ; 𝐰ᵢ is input weight: 𝐰ᵢ=[wᵢ₁, wᵢ₂, \u0026hellip;, wᵢₙ]ᵀ; 𝐱ⱼ is a n-dimensional input: 𝐱ⱼ=[xᵢ₁, xᵢ₂, \u0026hellip;, xᵢₙ]ᵀ∈ ℝⁿ; bᵢ is the bias of the ith hidden unit; 𝐨ⱼ is a m-dimensional vector: 𝐨ⱼ=[oᵢ₁, oᵢ₂, \u0026hellip;, oᵢₘ]ᵀ∈ ℝᵐ; The ideal parameters (𝐰,b,𝛃) should satisfy: ∑ᵢ₌₁ᴸ 𝛃ᵢ g(𝐰ᵢᵀ⋅𝐱ⱼ + bᵢ) = 𝐭ⱼ For total N samples, this mapping can be reforomalized with matrices: $𝐇_{N×L} \\pmb\\beta_{L×m} = 𝐓_{N×m}$, where\n𝐇 is the output of the hidden layer for N samples: $$𝐇(𝐰₁,\u0026hellip;,𝐰_L, b₁,\u0026hellip;,b_L, 𝐱₁,\u0026hellip;𝐱_L) = \\\\ \\begin{bmatrix} g(𝐰₁⋅𝐱₁+b₁) \u0026amp; \\dots \u0026amp; g(𝐰_L⋅𝐱₁+b_L)\\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ g(𝐰₁⋅𝐱_N+b₁) \u0026amp; \\dots \u0026amp; g(𝐰_L⋅𝐱_N+b_L) \\end{bmatrix}_{N×L}$$\n𝛃 is the output weights matrix: [ 𝛃₁ᵀ ; \u0026hellip; ; 𝛃$_Lᵀ ]_{L×m}$\nTarget data: 𝐓 = $\\begin{bmatrix}𝐓₁ᵀ\\\\ \\vdots \\\\𝐓_Nᵀ\\end{bmatrix}_{N×m}$\nGenerally, $𝐇_{N×m}$ is not a square matrix (not invertible). Hence, 𝛃=𝐇⁻¹𝐓 cannot be applied. However, the optimal 𝛃 can be approached by minimizing the traning error iteratively: ∑ⱼ₌₁ᴺ‖𝐨ⱼ-𝐭ⱼ‖.\nBest estimation: $\\^𝐰ᵢ, \\^bᵢ$, ^𝛃ᵢ satisfy: ‖𝐇(^𝐰ᵢ, ^bᵢ)⋅^𝛃ᵢ- 𝐓‖ = min_{𝐰ᵢ, bᵢ, 𝛃ᵢ} ‖𝐇(𝐰ᵢ, bᵢ)⋅𝛃ᵢ- 𝐓‖, where i=1,\u0026hellip;,L\nLoss function: J = ∑ⱼ₌₁ᴺ (∑ᵢ₌₁ᴸ 𝛃ᵢ⋅g(𝐰ᵢ⋅𝐱ⱼ + bᵢ) - 𝐭ⱼ)²\nSolve 𝛃 based on the ∂J/∂𝛃=0, such that the optimal parameter is: ^𝛃 = $𝐇^† 𝐓$ = (𝐇ᵀ𝐇)⁻¹𝐇ᵀ 𝐓, where $𝐇^†$ is the Moore-Penrose inverse (Pseudo-inverse) of 𝐇. It can be proved that the norm of ^𝛃 is the smallest and unique solution (for a set of random (𝐰ᵢ, bᵢ)).\nMoore-Penrose inverse Also called pseudoinverse or generalized inverse ⁽²⁾.\n(bilibili search: \u0026ldquo;伪逆矩阵\u0026rdquo;) 深度学习-啃花书0103伪逆矩阵最小二乘\n(DDG search: \u0026ldquo;伪逆矩阵\u0026rdquo;)\n伪逆矩阵的意义及求法？ - 知乎\nnumpy.linalg.pinv() pinv(𝐗) = (𝐗ᵀ 𝐗)⁻¹ 𝐗ᵀ pinv(𝐗) 𝐗 = 𝐈 python之numpy之伪逆numpy.linalg.pinv - 千行百行 - CSDN Example Code This matlab code ⁽¹⁾ trains and tests a ELM on the NIR spectra dataset (regression) and the Iris dataset (classification).\nNote that each column is a sample, and each row is an attribute/feature. Notations:\nQ: number of samples R: input features S: output features $P_{R×Q}$: input pattern matrix $T_{S×Q}$: target data matrix N: number of hidden nodes TF: transfer function $IW_{N×R}$: input weights matrix $B_{N×Q}$: bias matrix $LW_{N×S}$: transposed output weights matrix Train (calculate the LW):\n$tempH_{N×Q} = IW_{N×R}⋅P_{R×Q} + B_{N×Q}$ $H_{N×Q} = TF(tempH)$ $LW_{S×N} = T_{S×Q}$⋅ pinv(H), based on: 𝛃$_{S×N} 𝐇_{N×Q} = 𝐓_{S×Q}$ Test:\n$tempH_{N×Q} = IW_{N×R}⋅P_{R×Q} + B_{N×Q}$ $H_{N×Q} = TF(tempH)$ $Y_{S×Q} = LW_{S×N}⋅H_{N×Q}$ Example code (py) Build an Extreme Learning Machine in Python | by Glenn Paul Gara \u0026hellip; searched by DDG: \u0026ldquo;incremental elm python\u0026rdquo;\nI-ELM incremental just means adding neurons?\ngithub\nOS-ELM On-line elm\nDeep incremental RVFL Deep incremental random vector functional-link network: A non-iterative constructive sketch via greedy feature learning\nReference 极限学习机(Extreme Learning Machine, ELM)原理详解和MATLAB实现 - 奔跑的Yancy - CSDN Moore-Penorse - Wikipedia (Back to Top)\n","date":"2023-01-31T15:49:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/c-sum-elm/","title":"sum: ELM"},{"content":"下一代国际华人青年学子面对面 第6期 2022年10月20日 周四\nOverview Research work Suggestions for graduate student (1st year) Research Contribution:\nNon-iterative learning strategy for training neural networks including single-layer networks, multi-layer networks, autoencoders, hierarchical networks, and deep networks. All the related publications in this category are my first-author papers\nThe proposed methods for pattern recognition related applications: Image Recognition, Video Recognition, Hybrid System Approximation, Robotics System Identification, EEG-brain Signal Processing. Most the related publications in this category are co-author papers with my HQPs.\nMy research works In the past 10 years, the works about Artificial neural networks：\nTheoretical Contributions to ANN Machine Learning based Applications Single-layer network with non-iterative learning Data Analysis and Robotics System Identification (Ph.D.) Hierarchical NN with Subnetwork Neurons Image Recognitions (Post Doctoral Fellow) Deep Networks without iterative learning Pattern Recognition (since 2018) Ⅰ. Single-layer network with non-iterative learning Starting with a small idea The labtorary mainly studies robots, control, mechanics. After 2008 Chinese Winter Storms, they got funding for creating Powerline De-icing robots.\nThe supervisor (Yaonan Wang): \u0026ldquo;Can you find a Neural Network for Identifying Robotics Dynamic Systems?\u0026rdquo; (in 2009 winter)\nLater, I found the following paper: \u0026ldquo;Universal approximation using incremental constructive feedforward networks with random hidden nodes\u0026rdquo;, By Huang, Guang-Bin et.al (Cannot be found on IEEE) version on elm portal\nWhat is the Neural Network? Single hidden layer feedforward NN:\nflowchart BT subgraph in[Input Layer] x1((1)) \u0026 xe((\"...\")) \u0026 xn((n)) end subgraph hid[Hidden Layer] h1((\"𝛂₁,♭₁,𝛃₁\")) \u0026 h2((\"𝛂₂,♭₂,𝛃₂\")) \u0026 he((\"......\")) \u0026 hL((\"𝛂L,♭L,𝛃L\")) end subgraph out[Output Layer] y1((1)) \u0026 ye((\"...\")) \u0026 ym((m)) end x1 \u0026 xn --\u003e h1 \u0026 h2 \u0026 hL --\u003e y1 \u0026 ym Output of additive hidden neurons: G(𝐚ᵢ, bᵢ, 𝐱) = g(𝐚ᵢ⋅𝐱+bᵢ) Output of RBF hidden nodes: G(𝐚ᵢ, bᵢ, 𝐱) = g‖𝐱-𝐚ᵢ‖ The output function of SLFNs is: fₗ(𝐱) = ∑ₗ₌₁ᴸ 𝛃ᵢ⋅G(𝐚ᵢ, bᵢ, 𝐱) Network training Advantage: Approximate unknown system through learnable parameters.\nMathematical Model:\nApproximation capability: Any continuous target function f(x) can be approximated by Single-layer feedforward network with appropriate parameters (𝛂,♭,𝛃). In other words, given any small positive value e, for SLFN with enough number of hidden nodes, we have: ‖f(𝐱)-fₗ(𝐱)‖ \u0026lt; e\nIn real applications, target function f(𝐱) is usually unknown. One wishes that unknown f could be approximated by the trained network fₗ(𝐱).\nWhat is Extreme Learning Machine? Feed forward random network without using BP to train, such that it has a good real-time performance. And it fits the real-time robot task exactly.\nB-ELM (2011) \u0026ldquo;Bidirectional ELM for regression problem and its learning effectiveness\u0026rdquo;, IEEE Trans. NNLS., 2012 paper\n(23-02-10) This the inception of his subnetwork series work, and I was supposed to read this brief firt. paperNote\nMotivation\nOriginal ELM has 3 kinds of parameters: 𝐚 is called the \u0026ldquo;input weights\u0026rdquo;, b is the bias, 𝛃 is the \u0026ldquo;output weights\u0026rdquo;, which are consistent with earlier feedfoward network, though current single-layer feedfoward network has removed the 𝛃.\nThe 1st layer in ELM is generated randomly and the 2nd layer is constructed based on Moore-Penrose inverse without iteration.\nflowchart LR in((X)) --\u003e lyr1[\"Layer1:\\n 𝐚ⁿ, b\\n Random\\n Generated\"] --\u003e weightSum1((\"X⋅𝐚ⁿ + b\")) --\u003e act[Activation\\n Function\\n g] --\u003e z((\"g(X⋅𝐚ⁿ+b)\")) --\u003e lyr2[\"Layer2:\\n 𝛃\"] --\u003e weightSum2((\"O =\\n 𝛃⋅g(X⋅𝐚ⁿ+b)\\n =T\")) -.-\u003e|\"𝛃 =\\n g(X⋅𝐚ⁿ+b)⁻¹T\"| lyr2 classDef lyrs fill:#ff9 class lyr1,act,lyr2 lyrs; Bidirectional ELM\nIn original ELM, the 𝐚ⁿ,b are random numbers, but they can be yield if pulling the error back further, that is doing twice more inverse computation. Therefore, in order to calculate the 𝐚ⁿ,b, there are 3 times inverse computation: for output weights 𝛃, activation function g(⋅) and activation z (X⋅𝐚ⁿ+b) respectively.\n%%{ init: { 'flowchart': { 'curve': 'bump' } } }%% flowchart LR in((X)) --\u003e lyr1[\"Layer1:\\n 𝐚ⁿ, b\\n Random\\n Generated\"] --\u003e weightSum1((\"X⋅𝐚ⁿ + b\")) --\u003e act[Activation\\n Function\\n g] --\u003e z((\"g(X⋅𝐚ⁿ+b)\")) --\u003e lyr2[\"Layer2:\\n 𝛃\"] --\u003e out((\"O =\\n 𝛃⋅g(X⋅𝐚ⁿ+b)\\n =T\")) -.-\u003e|\"𝛃 =\\n g(X⋅𝐚ⁿ+b)⁻¹T\"| lyr2 classDef lyrs fill:#ff9 class lyr1,act,lyr2 lyrs; out -.-\u003e |\"X⋅𝐚ⁿ+b =\\n g⁻¹ T 𝛃⁻¹\"|weightSum1 out -.-\u003e |\"𝐚ⁿ =\\n X⁻¹ (g⁻¹ T 𝛃⁻¹ -b ),\\n b = mse(O-T)\"|lyr1 𝛃 = [g(X⋅𝐚ⁿ+b)]⁻¹T X⋅𝐚ⁿ+b = g⁻¹ T 𝛃⁻¹ 𝐚ⁿ = X⁻¹ (g⁻¹ T 𝛃⁻¹ -b ), and b = mse(O-T) The error is surprisingly small even with few hidden nodes. Compared with the original ELM, the required neurons in this method are reduced by 100-400 times, and the testing error reduced 1%-3%, and also the training time reduced 26-250 times over 10 datasets.\nMajor differences\nRandomized Networks Bidirectional ELM Classifier ELM; Echo State Netowrk;\nRandom Forest;\nVector Functional-link Network Only works for regression task Performance Similar performance;\nFaster speed;\nLess required neurons Learning strategy (Semi-)Randomized input weights;\nNon-iterative training;\nSingle-layer network Non-iterative training;\nSingle-layer network;\nCalculated weights in a network Ⅱ. Hierarchical NN with Subnetwork Neurons Single-Layer Network with Subnetwork Neurons In 2014, deep learning is becoming popular. How to extend the B-ELM as a multi-layer network? \u0026ldquo;Extreme Learning Machine With Subnetwork Hidden Nodes for Regression and Classification\u0026rdquo;. paper; paperNote\nMotivation\nflowchart TB base[Bidirectional ELM] --\u003e theory[Theoretical Contributions on ANN] \u0026 app[Industrial Applications] theory --\u003e multilyr[1. Two-layer Neural Networks?\\n 2. Hierarchical Neural Networks?] app --\u003e tasks[1. Feature Extraction\\n 2. Dimension Reduction\\n 3. Image Recognition] Pull the residual error back to multiple B-ELMs sequentially:\nflowchart LR in((X)) --\u003e lyr11 \u0026 lyr21 \u0026 lyrN1 subgraph net1[B-ELM 1] lyr11[\"𝐚¹,b¹\"] ==\u003e|\"X⋅𝐚¹+b¹\"| act1[g] ==\u003e|\"g(X⋅𝐚¹+b¹)\"|lyr12[\"𝛃¹\"] ==\u003e|\"𝛃¹⋅g(X⋅𝐚¹+b¹)\"| out((\"O ➔ T\\n E=T\")) -.-\u003e lyr12 -.-\u003e act1 -.-\u003e lyr11 lyr11 --\u003e act1 --\u003e lyr12 end lyr12 --\u003e out1((\"O¹➔T\\n E=T-O¹\")) -.-\u003e lyr22 subgraph net2[B-ELM 2] lyr22[\"𝛃²\"] -.-\u003e act2[g] -.-\u003e lyr21[\"𝐚²,b²\"] lyr21 --\u003e|\"X⋅𝐚² + b²\"|act2 --\u003e|\"g(X⋅𝐚²+b²)\"|lyr22 end lyr22--\u003e out2((\"O²+O¹➔T\\n E=T-(O²+O¹)\")) out2 -.-\u003e|\"Solve\\n multiple\\n B-ELMs\"| outn-1((\"E=T-∑ᵢ₌₁ᴺ⁻¹ Oⁱ\"))-.-\u003elyrN2 %%out2 -.-\u003e lyrn2 %%subgraph nets[multiple B-ELMs] %%lyrn2[\"𝛃ⁿ\"] -.-\u003e actn[g] -.-\u003e lyrn1[\"𝐚ⁿ,bⁿ\"] %%lyrn1 --\u003e|\"X⋅𝐚ⁿ + bⁿ\"|actn --\u003e|\"g(X⋅𝐚ⁿ+bⁿ)\"|lyrn2 %%end %%lyrn2 --\u003e outn-1((\"E=T-∑ᵢ₌₁ᴺ⁻¹ Oⁱ\"))-.-\u003elyrN2 subgraph netN[\"B-ELM N\"] lyrN2[\"𝛃ᴺ\"] -.-\u003e actN[g] -.-\u003e lyrN1[\"𝐚ᴺ,bᴺ\"] lyrN1 --\u003e|\"X⋅𝐚ᴺ + bᴺ\"|actN --\u003e|\"g(X⋅𝐚ᴺ+bᴺ)\"|lyrN2 end lyrN2--\u003e outN((\"∑ᵢ₌₁ᴺ Oⁱ➔T\")) classDef lyrs fill:#ff9 class lyr11,act1,lyr12,lyr21,act2,lyr22,lyrN1,actN,lyrN2 lyrs; linkStyle 9,10,11,15,16,17,22,23,24 stroke:#0af,stroke-width:3px %%classDef node font-size:20px; Dotted links are computation with inverse. Cyan links is the second feedforward using the updated parameters to give a trustworthy result O¹. The objective is to approach the target T, so there is a residual error E=T-O¹. Then another B-ELM (with same structure) is used to reduce the error continuously. And this time, the prediction is O²+O¹, which is the approximation of T. Here, the residual error is E=T-(O²+O¹)\nRepeatedly pulling the residual error to a new B-ELM N times is equivalent to N SLFNs. But B-ELM is fast without iteration and less computation with a few hidden nodes in each SLFN.\nBased on original SLFN structure, each node contains a SLFN.\nTwo-Layer Network with Subnetwork Neurons (2015) How to extend the Single-layer network with subnetwork nodes system to a two-layer network system?\nA general two-layer system was built in paper: \u0026ldquo;Multilayer Extreme Learning Machine with Subnetwork Hidden Neurons for Representation Learning\u0026rdquo; paper; paperNote\nThough it only contains two \u0026ldquo;general\u0026rdquo; layers, this system includes hundreds of networks, and it\u0026rsquo;s fast due to the modest quantity and no iteration.\nCompared with ELM and B-ELM, it got better performance over 35 datasets:\nClassification (vs ELM) Regression (vs B-ELM) Required Neurons Reduced 2-20 times Reduced 13-20% Metrics Accuracy increase 1-17% Testing error reduced 1-8% Training Speed faster 25-200times faster 5-20% Two-layer network system can perform image compression or reconstruciton, etc. This method is better than Deep Belief Network on small datasets. But it\u0026rsquo;s inferior than deep learning with transfer learning technics on huge datasets.\nHierarchical Network with Subnetwork Neurons \u0026ldquo;Features combined from hundreds of mid-layers: Hierarchical networks with subnetwork nodes\u0026rdquo; IEEE Trans. NNLS, 2019. paper\nFrom a single-layer network with subnetwork neurons to the multi-layer network, and then to a neural network system, these 3 papers cost 5 years or so.\nCompared with deep learning network, it\u0026rsquo;s extremely fast and performs well on small datasets, like Scene15, Caltech101, Caltech256. But for large datasets, deep learning is the winner.\n\u0026ldquo;Somewhat regretfully, I turned to deep learning a bit late. But been hesitant to do research along this approach.\u0026rdquo;\nMajor differences between ours and Deep Networks\nSGD based methods in DCNN Moore-Penrose inverse matrix based methods Hyper params lr; momentum; bs; L2 regulariation; epochs L2 regularization (non-sensitive) Performance higher performance in Computer Vision tasks (with huge datasets);\nGPU-based computation resource;\nMore parameters;\nMore required training time Faster learning speed/less tuning;\nPromising performance in Tabular Datasets;\nLess over-fitting problem. Ⅲ. Deep Networks without iterative learning Since 2018: How to combine the non-iterative method (M-P inverse matrix) with deep convolutional network to gain advantages? This took 2-3 years.\nThis is the age of Deep Learning.\nInteresting 20 years of cycles\nRosenblatt\u0026rsquo;s Perceptron proposed in mid of 1950s, sent to \u0026ldquo;winter\u0026rdquo; in 1970s. Back-Propagation and Hopfield Network Proposed in 1970s, reaching research peak in mid of 1990s. Support vector machines proposed in 1995, reaching research peak early this century. There are exceptional cases:\nMost basic deep learning algorithms proposed in 1960s-1980s, becoming popular only since 2012 (for example, LeNet proposed in 1989). ImageNet pushed deep learning, because only when the huge network structure of deep learning meets the matched huge dataset, it can achive good performance.\nThe success of deep learning enlist three factors: 1. NN structure and algorithm; 2. Big data; 3. GPU availability.\nHundreds of layers result in tedious training time. \u0026ldquo;The study intensity is infinitely small and the study duration is infinitely large.\u0026rdquo;\n\u0026ldquo;The improvement space of deep neural network is limited. So can we introduce non-iterative learning strategies for training deep networks\u0026rdquo;\nTraining speed is more important for scientific research than accuracy. And also it\u0026rsquo;s necessary to reduce the dependence on GPUs and the involvement of undeterministic hyper parameters (lr,bs,\u0026hellip;)\nRetraining DCNN with the non-iterative strategy (2019): \u0026ldquo;Recomputation of the dense layers for the performance improvement of DCNN.\u0026rdquo; IEEE TPAMI, 2020. link\nMotivation\ngraph LR base[\"1. Two-layer Neural Networks\\n 2. Hierarchical Neural Networks\"] --\u003e explore[\"1. Deep learning withe non-iterative method\"] In a DCNN, the first few layers are convolutional layers, maxpooling, then there\u0026rsquo;re 3 or 1 dense layer.\nIf I cannot train all of the hundreds layers in my non-iterative method, can I train only certain layers that are easy trained with my method, rather the SGD?\nOnly the fully-connected layers are trained by non-iterative method (inverse matrix), and the rest of layers are trained by gradient descent (SGD, SGDM, Adam).\nOn some medium-size datasets(CIFAR10, CIFAR100, SUN397), this approach brought a moderate improvement because there are only 3 dense layer out of a 50/100-layer network (most of layers are trained with SGD), but speeds up the training.\nOne layer can be trained within 1-2 seconds.\n","date":"2023-01-27T10:19:00Z","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/d-vid-%E6%9D%A8%E6%98%93%E6%97%BB221020/","title":"watch: SNN - 杨易旻 | WeChat Live"},{"content":"IEEE Cybernetics(2015-11-02) | Google Drive | G.Scholar\nThis is the first paper of ELM with subnetwork nodes by Yimin Yang. The second paper in the series is MltLyr ELM with subnetwork nodes The outline of Yang\u0026rsquo;s research works: Yang-WeChatLive-20221020; Abstract Learning effectiveness and speed of SLFN are bottleneck. ELM is fast. Grow subnetwork nodes by pulling back residual network error to the hidden layer. Better generalization performance with fewr hidden nodes. Ⅰ. Introduction Bring out the subject: FNN (universial approximator) ➔ SLFN\nWhat is an SLFN? Input layer + hidden layer + output layer\nMath description: For N arbitary distinct samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, where 𝐱ᵢ∈ 𝐑ⁿ and 𝐭ᵢ∈ 𝐑ᵐ, the network output is:\n$𝐟_L(𝐱)$ = ∑ᵢ₌₁ᴸ 𝛃ᵢh(𝐚ᵢ⋅𝐱ⱼ + bᵢ) = ∑ᵢ₌₁ᴸ 𝐇ᵢ⋅𝛃ᵢ, j=1,\u0026hellip;,N (1)\nSLFN output is the weighted sum of 𝐿 hidden nodes (perceptrons) with the factor 𝛃. The ith perceptron receives the weighted sum of 𝑁 inputs through its parameters (𝐚ᵢ, bᵢ), 𝐚ᵢ∈ 𝐑ⁿ, b∈ 𝐑, and performs activation function h. Its contribution ratio to the all output nodes is 𝛃ᵢ. ELM traits: NN (all params are adjustable) ➔ partial random networks ➔ ELM is a full-random learning method, where the input weights and bias (𝐚, b) are generated randomly and independent of training data. (Will the Glorot normalization has no effect?)\nELM advantages: An unification of FNNs and SVM/LS-SVM\nELM application: CV, da,\u0026hellip;, online learning\nProblems:\nThe choice of the regularization parameter C which affects the generalization performance of ELM mainly relies on trial-and-error method.\nHow many neurons should be used in ELM. Although Huang suggested to use more than 1000 hidden nodes, whether the number of hidden nodes can be further reduced without affecting learning effectiveness for large-size/high dimension data set. Several improved ELM methods, like B-ELM pulls the network residual error back to the hidden layer but it only works for regression task, and other methods bring a higher computation complexity when compared to standard ELM.\nSolution: Growing subnetwork hidden nodes to the exisiting network by pulling back the network residual error to hidden layers. A hidden node itself can be formed by several hidden nodes.\nContributions:\nFaster than BP, SVM and other ELMs and compatible to regreesion and classification problems. The regularized parameter C do not affect the generalization performance of this method. This method with m hidden nodes (the desired output dimensionality) can achieve better training accuracy than the original ELM with a large number of hidden nodes. Ⅱ. Definitions and Basic-ELM A. Notations and Definitions 𝐑 : set of real numbers\n{(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ : N arbitrary distinct samples,\n𝐱ᵢ = [xᵢ₁,xᵢ₂,\u0026hellip;,xᵢₙ]ᵀ : n-dim input data, 𝐱ᵢ∈ 𝐑ⁿ is a column vector;\n𝐭ᵢ : m-dim desired output data, 𝐭ᵢ∈ 𝐑ᵐ\n𝐱 : input data matrix, 𝐱=[𝐱₁,\u0026hellip;𝐱_N], 𝐱ᵢ∈ 𝐑ⁿᕁᴺ\n𝐭 : desired output data matrix, 𝐭=[𝐭₁,\u0026hellip;𝐭_N], 𝐭∈ 𝐑ᵐᕁᴺ\n(^𝐚ₙ,^bₙ) : parameters of the 𝑛th subnetwork hidden node, ^𝐚ₙ∈ 𝐑ⁿᕁᵐ, ^bₙ∈ 𝐑 (suppose the number of hidden nodes equals to the output dimension m, thus the mapping is from n to m.)\n^𝐚ₙ = [𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ], n×m weights matrix (for a n-dimension sample 𝐱ᵢ), 𝐚ₙₘ∈ 𝐑ⁿ\n𝐞ₙ : residual error of current network output 𝑓ₙ with 𝑛 hidden nodes (for N samples), i.e., 𝐞ₙ=𝐭-𝑓ₙ, 𝐞ₙ∈ 𝐑ᵐᕁᴺ.\n𝐇 : output matrix of the hidden layer (of SLFN) for tarining set {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐇 = [h(𝐱₁),\u0026hellip;,h(𝐱_N)]ᵀ, 𝐇∈ 𝐑ᴺᕁᵐ, 𝐇 = g(𝐱ᵀ𝐚+𝐛)???\nh(𝐱) : activation function. ELM feature mapping (or Huang\u0026rsquo;s transform)\n𝐇ᵢ : the 𝑖th hidden node output w.r.t. inputs, i.e., the 𝑖th column of 𝐇\n𝐈 : unit matrix\nsum(𝐞) : the sum of all elements of the matrix 𝐞\nB. Basic-ELM ELM is proposed for single-hidden-layer feedforward networks (SLFNs).\nThe output function of ELM with L hidden nodes for SLFNs is:\n$𝑓_L(𝐱)$ = ∑ᵢ₌₁ᴸ βᵢ⋅h(𝐚ᵢ⋅𝐱ⱼ + bᵢ) = ∑ᵢ₌₁ᴸ 𝐇ᵢ⋅𝛃ᵢ, j=1,\u0026hellip;,N.\nwhere h(⋅) denotes an activation function, (𝐚ᵢ, bᵢ), 𝐚ᵢ∈ 𝐑ⁿ, bᵢ∈ 𝐑, denotes the ith hidden node parameters, and 𝛃ᵢ is the ith output weight between the ith hidden node and the output nodes.\nBased on Bartlett\u0026rsquo;s theory, ELM theory aims to reach not only the smallest training error, but also the smallest norm of output weights (Least square-least norm solution, where the regularization makes an invertible matrix, such that a special solution can be determined.):\nMinimize: ‖𝛃‖² + C⋅‖𝐇𝛃 - 𝐭‖²\n\u0026ldquo;then the generalization performance depends on the size of weights rather than the number of nodes.\u0026rdquo;\nLemma 1 (proved by Huang):\nGiven an SLFN with nonconstant piecewise continuous hidden nodes 𝐇(𝐱, 𝐚, b), then for any continuous target function 𝑓 and any function sequence 𝐇ₙʳ(𝐱) = 𝐇(𝐱, 𝐚ₙ, bₙ) randomly generated based on any continuous sampling distribution,\nlim$_{n➝∞}$ ‖𝑓 - (𝑓ₙ₋₁ + 𝐇ₙʳ⋅𝛃ₙ)‖ = 0\nholds with probabitliy 1 if: 𝛃ₙ = ⟨𝐞ₙ₋₁, 𝐇ₙʳ⟩ / ‖𝐇ₙʳ‖² (the weight of the 𝑛th node)\nwhere\n\u0026ldquo;⟨ , ⟩\u0026rdquo; stands for \u0026ldquo;dot product\u0026rdquo; (Frobenius inner product) of two matrices and is a scalar. n is the number of hidden nodes in the hidden layer. 𝐞ₙ₋₁ is the residual error of the last iteration, i.e., when there were n-1 hidden nodes. 𝐇ₙʳ is the output matrix of the current hidden layer (activated but havn\u0026rsquo;t scaled by 𝛃). Intuitively, as the residual error reduces, the weight of the newer node gets smaller.\nⅢ. Proposed ELM Method With Subnetwork Hidden Nodes A. Structure of the Proposed Method Motivations:\nSelecting an appropriate number of neurons can resort to optimization algorithms. The generalization performance depends on the size of the weights rather than the number of weights. Inspiration:\n\u0026ldquo;A hidden node itself can be a subnetwork formed by several nodes. And these subnetwork hidden nodes and output weights itself should be the smallest norm, and also aim to reach the smallest training error.\u0026rdquo;\nObjectives:\nGiven N training samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐱ᵢ∈ 𝐑ⁿ, 𝐭ᵢ∈ 𝐑ᵐ, generated from the same continuous system, if activation function h is invertible, the objectives are:\n‖𝐞ₙ₋₁‖ ≥ ‖𝐞ₙ₋₁ - h(^𝐚ₙ,𝐱)‖ ≥ ‖𝐞ₙ₋₁ - 𝐇ₙ‖ ≥ ‖𝐞ₙ₋₁ - 𝐇ₙ⋅𝛃ₙ‖ (residual error is decreasing.) ‖h(^𝐚ₙ,𝐱) - 𝐞ₙ₋₁‖ = min_{𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ} ‖h(𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ) - 𝐞ₙ₋₁‖ (minimize weights inside nodes) ‖𝐇ₙ⋅^𝛃ₙ - 𝐞ₙ₋₁‖ = min_{𝛃} ‖𝐇ₙ⋅𝛃 - 𝐞ₙ₋₁‖ (minimize the weights outside nodes) where\n^𝐚ₙ and ^𝛃ₙ are the optimal (the ultimate status) parameters with the smallest norm among all the least squares solutions. 𝐇ₙ = h(^𝐚ₙ, ^bₙ, 𝐱) is the output of the nth hidden node with the optimal parameters. If activation function h is invertible, subnetwork hidden nodes in SLFN can be calculated by pulling back network residual error to hidden layers.\nFor example, with sine function as the activation function, training a subnetwork hidden node (^𝐚) is equivalent to finding a least-square solution ^𝐚ₙ (letting the derivative of MSE=0) with the least norm for the linear system:\n[𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ]⋅𝐱 = arcsin(𝐞ₙ₋₁), 𝐞ₙ₋₁∈ (0,1],\nsuch that the optimal ^𝐚ₙ satifies:\n‖sin(^𝐚ₙ, 𝐱) - 𝐞ₙ₋₁‖ = min_{𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ} ‖sin(𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ, 𝐱) - 𝐞ₙ₋₁‖,\nThat means the output of the nth \u0026ldquo;subnetwork hidden node\u0026rdquo; ^𝐚ₙ is approaching the residual error 𝐞ₙ₋₁ of the last status.\nThe input weights ^𝐚ₙ of a node for this model is a matrix (instead of a vector), beacuse each \u0026ldquo;subnetwork (general) hidden node\u0026rdquo; contains a standard SLFN (several hidden nodes) internally.\nDifferences with standard ELM\nELM with subnetwork standard ELM hidden node m neurons: $𝐚_f∈ 𝐑ⁿᕁᵐ, 𝐛_f∈ 𝐑ᵐ$ single neuron:\n𝐚∈ 𝐑ⁿ, b∈ 𝐑 construct calculated generated randomly # hidden nodes L x m (m ⟂ L, m = #output dim) L B. Proposed Method Lemma 2: Given a bounded nonconstant piecewise continuous activation function h, there is: lim$_{(𝐚,b)→(𝐚₀,b₀)}$ ‖h(𝐚⋅𝐱+b) - h(𝐚₀⋅𝐱+b₀)‖ = 0 (连续性)\nTheorem 1: Given N arbitrary distinct samples {(𝐱ᵢ,𝐭ᵢ)}ᵢ₌₁ᴺ, 𝐱ᵢ∈ 𝐑ⁿ, 𝐭ᵢ∈ 𝐑ᵐ, a sigmoid or sine activation function h, and then for any continuous desired outputs 𝐭, the limit of error converges to 0:\nlim$_{n➝∞}$ ‖ 𝐭-{ u⁻¹(h(^𝐚⋅𝐱+b))} ‖ = 0\nProof:\nProve the sequence ‖𝐞ₙ‖ is decreasing with 0 as the lower bound and it converges.\nFor the 𝑛th subnetwork hidden node containing m hidden nodes, the linear mapping is:\n𝛌ₙ = [𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ]⋅𝐱, 𝛌ₙ∈ 𝐑ᵐ\nThen 𝛌ₙ passes through the activation function. Because the target is error, which should become 0 at the end, the error at present is the output of activation function:\n𝐞ₙ₋₁ = h(𝛌ₙ) ∈ 𝐑ᵐ\nThe inverse function of h is h⁻¹, and its input value should range from (0,1]. Therefore, if trying to solve 𝛌ₙ from 𝐞ₙ₋₁, every element in 𝐞ₙ₋₁ should be scaled to the range of (0,1] by the normalized function u(⋅). Then, 𝛌ₙ can be calculated through:\n𝛌ₙ = h⁻¹(u(𝐞ₙ₋₁))\nFurther, the input weights of this subnetwork hidden node can be solved:\n$$\\^𝐚ₙ = [𝐚ₙ₁,\u0026hellip;,𝐚ₙₘ] = h⁻¹(u(𝐞ₙ₋₁))⋅𝐱⁻¹$$\nFor different activation functions, there will be:\n$$\\rm \\{^{\\^𝐚ₙ = arcsin(u(𝐞ₙ₋₁))⋅𝐱⁻¹,\\quad sine} _{\\^𝐚ₙ = -log( (1/u(𝐞ₙ₋₁)) - 1)⋅𝐱⁻¹,\\quad sigmoid}$$\n(This work is a continuation on ELM, that is once the 𝛃 calculated based on target 𝐭 and 𝐇⁻¹, the residual error is also fixed, so it serves as the target for 𝐚,b. Still, applying least-square, the optimial a can be calculated based on \u0026ldquo;target\u0026rdquo; e and 𝐱⁻¹.)\n^b is the mean of hidden nodes.\nThe error can be reduced by adding the bias\nDo feedforward using the calculated ^𝐚ₙ and ^bₙ, so this time the output of the hidden layer is:\n$$\\^𝐇ₙᵉ = u⁻¹(h(\\^𝐚ₙ⋅𝐱+\\^bₙ))$$\nBecause 𝐞ₙ₋₁ is the last output of the activation fuction, 𝐞ₙ₋₁ and $\\^𝐇ₙᵉ$ are the same things. So they can subtract from each other. Then the residual error for this time is:\nΔ = ‖𝐞ₙ₋₁‖² - ‖𝐞ₙ₋₁ - ^𝐇ₙᵉ‖² = ‖𝐞ₙ₋₁‖² - (‖𝐞ₙ₋₁‖² - 2‖𝐞ₙ₋₁‖‖^𝐇ₙᵉ‖ + ‖^𝐇ₙᵉ‖²) = 2‖𝐞ₙ₋₁‖‖^𝐇ₙᵉ‖ - ‖^𝐇ₙᵉ‖² = 2⟨𝐞ₙ₋₁, ^𝐇ₙᵉ⟩ - ‖^𝐇ₙᵉ‖² = ‖^𝐇ₙᵉ‖² ( 2⟨𝐞ₙ₋₁, ^𝐇ₙᵉ⟩/‖^𝐇ₙᵉ‖² - 1 )\nΔ is ≥ 0\nProve the limit converges to 0 when n tends to infinity.\nThe target value is approximated while the error is decreased.\nThe final estimation is the summation of the ouput of d subnetwork hidden nodes\nThe VC dimension is lower than standard ELM, i.e., the dimension of feature space m≪ L, so the generalization ability of this method is better.\n(Back to top)\n","date":"2023-01-20T11:46:00-05:00","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-subnet/","title":"read: ELM with Subnetwork Nodes"},{"content":"Authors: Yimin Yang, and Q. M. Jonathan Wu IEEE Cybernetics; Publish Date: 2015-10-09.\nThis is the 2nd paper in his series, and the first paper is this.\n(感觉Intro写得不错，逻辑性强，信息量大；但后面method部分好多typo)\nAbstract Representation learning of multilayer ELM with subnetwork nodes outperform conventional feature learning methods.\nI. Introduction model performance ➔ data representaiton/features ➔ processing pipelines design and data transformations ➔ data representation ➔ effective learning\nFeature reduction and extraction techniques can be conducted in a supervised, unsupervised or semi-supervised manner.\nELMs learn representations of data to extract useful information when building classifiers or predictors.\nELMs provide a unified learning framework for \u0026ldquo;generalized\u0026rdquo; single-hidden layer feedforward NNs (SLFNs).\nIn ELM methods, the hidden layer parameters of NN need not be tuned during training, but generated randomly. ML-ELM is adding (multiple) general hidden nodes (subnetwork nodes) to existing single-hidden-layer ELM networks.\nA versatile platform with faster speed and better generalization performance on feature extraction. Its generalization performance is not sensitive to the parameters of the networks in the learning process. ML-ELM has universal approximation capability and representation learning. II. Preliminaries and Basic-ELM A. Notations 𝐑 : set of real numbers {(𝐱ᵢ,𝐲ᵢ)ᵢ₌₁ᴹ} (𝐱ᵢ∈ 𝐑ⁿ,𝐲ᵢ∈ 𝐑ᵐ) : M arbitrary distinct samples, 𝐱 : an input data matrix 𝐱∈ 𝐑ⁿᕁᴹ 𝐲 : desired output data matrix 𝐲∈ 𝐑ᵐᕁᴹ 𝛂ᵢ : weight vector connecting the 𝑖th hidden nodes and the input nodes ♭ᵢ : bias of the 𝑖th hidden nodes βᵢ : output weight between the 𝑖th hidden node and the output node 𝐞 : residual error of current network output, i.e., 𝐞=𝐲-𝐟 𝐈 : unit matrix sum(𝐞) : the sum of all elements of the matrix 𝐞 g : sigmoid or sine activation function (TABLE 1)\n(𝛂,♭) : a hidden node (in basic ELM) (𝐚,𝑏) : a general hidden node (or subnetwork node) ^𝐚ʲ_f : input weight of the jth general hidden node in feature mapping layer. ^𝐚ʲ_f∈ 𝐑ᵈᕁⁿ ^bʲ_f : bias of the 𝑗th general hidden node in feature mapping layer ^bʲ_f∈ 𝐑 (𝛂ᵢʲ_f,♭ʲ_f) : the 𝑖th general hidden node in the 𝑗th general hidden node. (^𝐚ₕ,^𝑏ₕ) : hidden nodes in ELM learning layer and ^𝐚ₕ∈ 𝐑 ᵐᕁᵈ uⱼ : normalized function in the 𝑗th general node, uⱼ(⋅):𝐑 ➔ (0,1], uⱼ⁻¹ represent its reverse function 𝐇ʲ_f : feature data generated by 𝑗general nodes in a feature mapping layer, i.e., 𝐇ʲ_f = ∑ᵢ₌₁ʲ uᵢ⁻¹ ⋅ g(𝐱, ^𝐚ⁱ_f, ^bⁱ_f), 𝐇ʲ_f∈ 𝐑ᵈᕁᴹ 𝐇ʲⁱ_f : feature data generated by the 𝑖th feature mapping layer M : number of training samples n : input data dimension m : output data dimension d : feature data dimension 𝐞_L : the residual error of current two-layer network (L general nodes in the first layer and (𝐚ₕ,𝑏ₕ) in the second layer) 𝐞ʲ_L : the residual error of current two-layer network (L general nodes in the first layer and 𝑗general nodes in the second layer) L : the numbers of general hidden nodes B. Basic-ELM The output function of ELM for SLFNs fed with input matrix 𝐱 is: fₙ(𝐱)=∑ᵢ₌₁ⁿ βᵢ g(𝐱, 𝛂ᵢ, ♭ᵢ).\n\u0026ldquo;ELM theory aims to reach the smallest training error but also the smallest norm of output weights\u0026rdquo; (regularization term?), so the objective is to minimize: ‖βᵢ‖ₚᶣ¹ + C⋅‖∑ᵢ₌₁ⁿ βᵢ g(𝐱, 𝛂ᵢ, ♭ᵢ) - 𝐲‖ᶣ²_q, i=1,\u0026hellip;,n. (ᶣ signifies μ)\nwhere μ₁\u0026gt;0, μ₂\u0026gt;0, p,q = 0, ½, 1, 2, \u0026hellip;, +∞, C is a positive value, g(𝐱, 𝛂, ♭) is referred to as ELM feature mapping (linear projection+activation) or Huang\u0026rsquo;s transform.\n(Convergence proved by Huang et al.)\nLemma 1: Given M aribitrary distinct samples {(𝐱, 𝐲)}, 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ sampled from a continuous system, an activation function g, then for any continous target function 𝐲 and any function sequence g(𝐱, 𝛂ₙʳ, ♭ₙʳ) randomly generated based on any continuous sampling distribution, lim_{n➝∞} ‖𝐲-（fₙ₋₁ + g(𝐱, 𝛂ₙʳ, ♭ₙʳ)）‖=0 holds with probabiltiy one if βₙ = ⟨𝐞ₙ₋₁, g(𝐱, 𝛂ₙʳ, ♭ₙʳ)⟩ / ‖g(𝐱, 𝛂ₙʳ, ♭ₙʳ)‖²,\nwhere (𝛂ₙʳ, ♭ₙʳ) represesnts the 𝑛th random hidden node, and 𝐞ₙ₋₁ = 𝐲-fₙ₋₁\nIII. Proposed Method A. ELM With Subnetwork Nodes A hidden node can be a subnetwork formed by several hidden nodes. Hence, a single mapping layer can contain multiple networks.\nComparision of the feature mapping layer:\nflowchart LR subgraph A[basic ELM] direction BT x1[\"x₁\"]--\u003e h1((\"𝛂₁,♭₁, β₁\")) \u0026 he1((...)) \u0026 hL((\"𝛂L,♭L, βL\")) xe[x...]--\u003e h1((\"𝛂₁,♭₁, β₁\")) \u0026 he1((...)) \u0026 hL((\"𝛂L,♭L, βL\")) xn[\"xₙ\"]--\u003e h1((\"𝛂₁,♭₁, β₁\")) \u0026 he1((...)) \u0026 hL((\"𝛂L,♭L, βL\")) h1 --\u003e y1[\"y₁\"] \u0026 ye[...] \u0026 ym[\"yₘ\"] he1--\u003e y1[\"y₁\"] \u0026 ye[...] \u0026 ym[\"yₘ\"] hL --\u003e y1[\"y₁\"] \u0026 ye[...] \u0026 ym[\"yₘ\"] subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 he1 \u0026 hL end end subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 he1 \u0026 hL end subgraph B[ELM with subnetwork nodes] direction BT x1_[\"x₁\"] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL xe_[x...] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL xn_[\"xₙ\"] --\u003e ghn1 \u0026 ghne((...)) \u0026 ghnL ghn1--\u003e y1_[\"y₁\"] \u0026 ye_[...] \u0026 ym_[\"yₘ\"] ghne--\u003e y1_[\"y₁\"] \u0026 ye_[...] \u0026 ym_[\"yₘ\"] ghnL--\u003e y1_[\"y₁\"] \u0026 ye_[...] \u0026 ym_[\"yₘ\"] end subgraph ghn1[\"^𝛂¹_f, ^♭¹_f, with weight u₁⁻¹\"] direction TB n11((\"𝛂¹_f1,\\n ♭¹_f1\")) \u0026 n1e((...)) \u0026 n1m((\"𝛂¹_fm,\\n ♭¹_fm\")) end subgraph ghne[\"general hidden nodes\"] direction TB ne1((1)) \u0026 nee((...)) \u0026 nem((m)) end subgraph ghnL[\"^𝛂ᴸ_f, ^♭_f, with weight u\\_L⁻¹\"] direction TB nL1((\"𝛂ᴸ_f1,\\n ♭ᴸ_f1\")) \u0026 nLe((...)) \u0026 nLm((\"𝛂ᴸ_fm,\\n ♭ᴸ_fm\")) end Three differences between ELM feature mapping layer and this feature mapping layer.\nDifference Standard ELM ELM with subnetwork nodes hidden node single hidden node generated\none by one general hidden node having subnetwork # hidden node Independent to the output dim 𝑚 In a subnetwork, it equals to the output dim relation A special case of the subnetwork case B. Proposed Method for Representation Learning 1) Optimal Projecting Parameters and Optimal Feature Data flowchart LR x1[\"x₁\"]--\u003e h1((\"^𝛂¹_f,^♭¹_f, β¹\")) \u0026 h2((\"^𝛂²_f,^♭²_f, β²\")) \u0026 he1((\"⋮\")) \u0026 hL((\"^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ\")) xe[x...]--\u003e h1((\"^𝛂¹_f,^♭¹_f, β¹\")) \u0026 h2((\"^𝛂²_f,^♭²_f, β²\")) \u0026 he1((\"⋮\")) \u0026 hL((\"^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ\")) xn[\"xₙ\"]--\u003e h1((\"^𝛂¹_f,^♭¹_f, β¹\")) \u0026 h2((\"^𝛂²_f,^♭²_f, β²\")) \u0026 he1((\"⋮\")) \u0026 hL((\"^𝛂ᴸ_f,^♭ᴸ_f, ^βᴸ\")) h1 \u0026 h2 \u0026 he1 \u0026 hL --\u003e feat[\"d-dimension\\n Feature data\"] feat --\u003e n1 \u0026 n2 \u0026 ne \u0026 nm --\u003e y1_[\"y₁\"] \u0026 ye_[\"⋮\"] \u0026 ym_[\"yₘ\"] subgraph A1[\"ELM feature mapping layer\"] h1 \u0026 h2 \u0026 he1 \u0026 hL end subgraph elm[\"ELM-learning layer\"] n1((\"𝛂ₕ₁,^♭ₕ\")) \u0026 n2((\"𝛂ₕ₂,^♭ₕ\")) \u0026 ne((\"⋮\")) \u0026 nm((\"𝛂ₕₘ,^♭ₕ\")) end Objective of representation learning: Represent the input features meaningfully in several different representations as follows.\nRepresen-\ntation feat dim (𝑑) vs\nin-dim (𝑛) feature target output Dimension Reduction 𝑑 \u0026lt; 𝑛 H_f ∈ 𝐑ᵈᕁᴹ 𝐲=label (Supervise)\nor 𝐲=𝐱 (Unsp~) Expanded Dimension 𝑑 \u0026gt; 𝑛 H_f ∈ 𝐑ᵈᕁᴹ 𝐲=label (Supervise)\nor 𝐲=𝐱 (Unsp~) The feature data is 𝐇_f(𝐱ₖ, ^𝐚_f, ^b_f), where the weights of feature mapping layer ^𝐚ʲ_f, j=1,\u0026hellip;,L belongs to 𝐑ᵈᕁⁿ.\nDefinition 1: Given a nonlinear piecewise continous activation function g, we call {(^𝐚ʲ_f, ^bʲ_f)ⱼ₌₁ᴸ} (^𝐚ʲ_f ∈ 𝐑ᵈᕁⁿ) the 𝐿 optimal general hidden nodes and 𝐇⃰ ⃰_f= ∑ᵢ₌₁ᴸ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) the optimal feature data if it satisfies: ‖𝐞_L‖ ≤ min_{𝐇⃰ᴸ_f∈ 𝐑ᵈᕁᴹ} ( min_{𝐚ₕ∈ 𝐑 ᵐᕁᵈ} ‖𝐲-uₕ⁻¹ g(𝐇⃰ᴸ_f, 𝐚ₕ, bₕ)‖ ) (4)\nwhere 𝐞_L = ‖𝐲-uₕ⁻¹ g(𝐇⃰ᴸ ⃰_f, ^𝐚ₕ, ^bₕ)‖ and sequence ‖𝐞_L‖ is decreasing and bounded below by zero.\nRemark 1: If the optimal projecting parameters are obtained in the feature mapping layer {(^𝐚ʲ_f, ^bʲ_f)ⱼ₌₁ᴸ} (where ^𝐚_f ∈ 𝐑ᵈᕁⁿ), the original n-dimension data points 𝐱 will be converted to d-dimension data points: 𝐇⃰_f= ∑ⱼ₌₁ᴸ g(𝐱ₖ, ^𝐚ʲ_f, ^bʲ_f), which satisfy the inequality (4).\nThus the purpose is to find optimal projecting parameters that make the inequality (4) true for all data points.\n2) Learning Steps Based on the inverse of the activation function.\nGiven M arbitrary distinct training samples {(𝐱ₖ,𝐲ₖ)ₖ₌₁ᴹ}, 𝐱ₖ∈ 𝐑ⁿ, 𝐲ₖ∈ 𝐑ᵐ, which are sampled from a continuous system.\nSet j=1 to initialize a general node of the feature mapping layer randomly as: 𝐇⃰ʲ_f = g(^𝐚ʲ_f⋅𝐱 + ^bʲ_f), (^𝐚ʲ_f)ᵀ⋅^𝐚ʲ_f=𝐈, (^bʲ_f)ᵀ⋅^bʲ_f=1,\nwhere ^𝐚ʲ∈ 𝐑ᵈᕁⁿ, ^bʲ_f∈ 𝐑 is the orthogonal random weight and bias of feature mapping layer. 𝐇⃰ʲ_f is current feature data.\nGiven a sigmoid or sine activation function g, for any continous desired outputs 𝐲, the parameters in the (general) ELM learning layer are obtained as:\n^𝐚ₕ = g⁻¹(uₙ(𝐲)) ⋅ (𝐇⃰ʲ_f)⁻¹, ^𝐚ʲₕ∈ 𝐑ᵈᕁᵐ, ^bₕ = √mse(^𝐚ₕʲ ⋅ 𝐇⃰ʲ_f - g⁻¹(uₙ(𝐲)) ), ^bʲₙ∈ 𝐑, $g⁻¹(⋅) = \\{^{arcsin(⋅) \\quad if\\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \\quad if\\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _ where 𝐇⃰⁻¹ = 𝐇⃰ᵀ( (C/𝐈) + 𝐇⃰ 𝐇⃰ᵀ)⁻¹; C is a positive value; uₙ is a normalized function uₙ(𝐲): 𝐑➔(0,1]; g⁻¹ and uₙ⁻¹ represent their reverse function.\nUpdate the output error 𝐞ⱼ as 𝐞ⱼ = 𝐲 - uₙ⁻¹ g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ) So the error feedback data is 𝐏ⱼ = g⁻¹(uₙ(𝐞ⱼ))⋅(^𝐚ₕ)⁻¹\nSet j=j+1, add a new general node (^𝐚ʲ_f, ^bʲ_f) in the feature mapping layer by\n^𝐚ʲ_f = g⁻¹( uⱼ(𝐏ⱼ₋₁) ) ⋅ 𝐱⁻¹, ^𝐚ʲ_f∈ 𝐑ⁿᕁᵈ ^bʲ_f = √mse(^𝐚ʲ_f ⋅ 𝐱 - 𝐏ⱼ₋₁), ^bʲ∈ 𝐑 and update the feature data 𝐇⃰ʲ_f = ∑ᵢ₌₁ʲ uₗ⁻¹ g(𝐱, ^𝐚ˡ_f, ^bˡ_f)\nRepeat step 2-4 𝐿-1 times. (Finally, 𝐿 nodes are added into feature mapping layer.) The set of parameters {^𝐚ʲ_f,^bʲ_f}ⱼ₌₁ᴸ are the optimal projecting parameters and the feature data 𝐇⃰ᴸ_f = ∑ⱼ₌₁ᴸ uⱼ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) = 𝐇⃰ ⃰_f are the optimal feature data.\nC. Proof of the Proposed Method (Proof of Convergence)\nGiven M arbitrary distinct samples {(𝐱ₖ,𝐲ₖ)}ₖ₌₁ᴹ (𝐱ₖ∈ 𝐑ⁿ, 𝐲ₖ∈ 𝐑ᵐ)\nLemma 2: Given a bounded nonconstant piecewise continuous activation function g, we have lim_{(𝛂,♭)→(^𝛂,^♭)} ‖g(𝐱,𝛂,♭) - g(𝐱,^𝛂,^♭)‖ = 0 where the (^𝛂,^♭) is one of the least-squares solutions of a general linear system 𝛂⋅𝐱+♭.\nRemark 2:\nLemma 2 shows that SLFN training problem can be considered as finding optimal hidden parameters which satisfy: g(^𝛂₁,^♭₁) + \u0026hellip; + g(^𝛂_L,^♭_L) → 𝐲. 𝛂 (alpha) stands for basic ELM hidden node.\nThus training an SLFN is equivalent to finding a least-square general input weight ^𝐚ₕ of the (linear+activation) system g(^𝐚ₕ⋅𝐱) = 𝐲.\nIf activation function g is invertible, the input weights matrix can be obtained by pulling back the residual error to the hidden layer.\nFor example, if g is a sine function,\nThe output of the hidden layer matrix is 𝐲=sin(𝐚ₕ ⋅ 𝐱). Thus, 𝐚ₕ⋅𝐱 = arcsin(𝐲), 𝐲∈ (0,1]. The smallest norm least-squares solution of the linear system sin(𝐚ₕ⋅𝐱)=𝐲 is: ^𝐚ₕ = arcsin(𝐲)⋅𝐱⁻¹, where 𝐱⁻¹ is the Moore-Penrose generalized inverse of matrix 𝐱. 𝐱⁻¹ = 𝐱ᵀ( (C/𝐈) + 𝐱𝐱ᵀ)⁻¹ Theorem 1: Given M arbitrary distinct samples {(𝐱ᵢ,𝐲ᵢ)ᵢ₌₁ᴹ}, (𝐱ᵢ∈ 𝐑ⁿ, 𝐲ᵢ∈ 𝐑ᵐ) and a sigmoid or sine activation function g, for any continuous desired outputs 𝐲, we have:\nthe optimal weights ^𝐚ₕ = argmin_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹(g(𝐱,𝐚ₕ)) - 𝐲‖ least square error ‖g(𝐱,^𝐚ₕ,^bₕ) - 𝐲‖ ≤ min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹(g(𝐚ₕ⋅𝐱)) - 𝐲‖ if the parameters are obtained by (similar to Algorithm step-2):\n^𝐚ₕ = g⁻¹( u(𝐲))⋅𝐱⁻¹, ^𝐚ₕ ∈ 𝐑ᵐᕁⁿ ^bₕ = √mse(^𝐚ₕ⋅𝐱 - g⁻¹(u(𝐲))), ^bₕ∈ 𝐑 $g⁻¹(⋅) = \\{^{arcsin(⋅) \\quad if\\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \\quad if\\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _ Proof:\nLet 𝛌=𝐚ₕ⋅𝐱, and 𝛌 satisfy g(𝛌) = 𝐲. Normalizing 𝐲 to (0,1] by u(𝐲) to let 𝛌∈ 𝐑. Thus, for a sine hidden node, 𝛌 = g⁻¹(u(𝐲)) = arcsin(u(𝐲)). While for a sigmoid hidden node, 𝛌 = g⁻¹(u(𝐲)) = -log(1/u(𝐲) - 1).\n^𝐚ₕ is the solution for the linear system (g(𝐚ₕ⋅𝐱)=𝐲). For sine activation: ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ = arcsin(u(𝐲))⋅𝐱⁻¹. For sigmoid activation: ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ = -log(1/u(𝐲) - 1)⋅𝐱⁻¹\nOne of the least-squares solutions of a general linear system 𝐚ₕ⋅𝐱=𝛌 is ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹, which means the smallest error can be reached by this solution: ‖^𝐚ₕ⋅𝐱 -𝛌ₙ‖ = min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖𝐚ₕ⋅𝐱 - g⁻¹( u(𝐲) )‖ (18)\nThe special solution ^𝐚ₕ = g⁻¹( u(𝐲) )⋅𝐱⁻¹ has the smallest norm among all the least-squares solutions of 𝐚ₕ⋅𝐱 = 𝛌. The error can be further reduced by adding bias bₙ: ^bₕ = √mse(^𝐚ₕ⋅𝐱 - h⁻¹( u(𝐲) ))\nBased on eq. (18) and Lemma2, optimization by minimizing the L2-loss can be reformulated as: min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹( g(𝐚ₕ⋅𝐱) ) - u⁻¹( g(𝛌))‖ = ‖u⁻¹( g(^𝐚ₕ⋅𝐱) ) - u⁻¹( g(𝛌))‖ ≥ ‖u⁻¹( g(^𝐚ₕ⋅𝐱 + ^bₕ) ) - 𝐲‖ (20)\nBased on eq. (18) and eq. (20), the optimal weights is proved as: ^aₕ = arg min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖g(𝐱,𝐚ₕ) - 𝐲‖ And it satisfy: ‖g(𝐱,^𝐚ₕ,^bₕ) - 𝐲‖ ≤ min_{𝐚ₕ∈ 𝐑ᵐᕁⁿ} ‖u⁻¹( g(^𝐚ₕ⋅𝐱) ) - 𝐲 ‖\nBased on Lemma 2 and Theorem 1, Theorem 2 is given:\nTheorem 2: Given M arbitrary distinct samples (𝐱, 𝐲), 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ, a sigmoid or sine activation function g, and the initial orthogonal random weights ^𝐚¹_f and bias ^b¹_f. For any continuous desired output 𝐲, the optimal feature data is: 𝐇⃰ᴸ⃰ _f(𝐱, (^𝐚¹_f, \u0026hellip;, ^𝐚ᴸ_f), (^b¹_f,\u0026hellip;,^bᴸ_f)) = ∑ⱼ₌₁ᴸ uⱼ⁻¹ g(^𝐚ʲ_f ⋅ 𝐱 + ^bʲ_f) which satisfy: ‖𝐞_L‖ ≤ min_{^𝐚ʲ_f∈ 𝐑ⁿᕁᵈ} ( min_{𝐚ₕ∈ 𝐑 ᵐᕁᵈ} ‖𝐲-uₙ⁻¹ g(𝐇⃰ᴸ_f, 𝐚ₕ, bₕ)‖) (21)\nand ‖𝐞_L‖ is decreasing and bounded below by zero if these parameters are obtained by:\n𝐇⃰ʲ_f = ∑ᵢ₌₁ʲ uᵢ⁻¹ g(𝐱, ^𝐚ⁱ_f, ^bⁱ_f), ^𝐚ₕ = g⁻¹(uₙ(𝐲)) ⋅ (𝐇⃰ʲ_f)⁻¹, ^𝐚ₕ∈ 𝐑 ᵐᕁᵈ, ^bₕ = √mse(^𝐚ₕ⋅𝐇⃰ʲ_f - g⁻¹( u(𝐲) )), ^bₕ∈ 𝐑 g⁻¹(⋅) = \\{^{arcsin(⋅) \\quad if\\ g(⋅)=sin(⋅)}_{-log(1/(⋅)-1) \\quad if\\ g(⋅) = 1/(1+e⁻⁽˙⁾)}$, _ 𝐞ⱼ = 𝐲 - uₙ⁻¹( g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ), 𝐏ⱼ = g⁻¹(uₙ(𝐞ⱼ))⋅(^𝐚ₕ)⁻¹ ), ^𝐚ʲ_f = g⁻¹(uⱼ(𝐏ⱼ₋₁)) ⋅ 𝐱⁻¹, ^𝐚ʲ∈ 𝐑ⁿᕁᵈ ^bʲ_f = √mse(^𝐚ʲ_f ⋅ 𝐱 - 𝐏ⱼ₋₁), ^bʲ_f∈ 𝐑 Proof:\nBase on Theorem 1, the validity of (21) is obvious. So here, we just prove that the error ‖𝐞_L‖ is decreasing and bounded below by zero.\nLet Δ = ‖eⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g(𝐇⃰ʲ_f, ^𝐚ₕ, ^bₕ)‖² (last error-current output), and take the newest item apart: = ‖eⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g( (∑ᵢ₌₁ʲ⁻¹ uᵢ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) + uᵢ⁻¹g(𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ) ‖² (24)\nLet ^Tʲ = uₙ⁻¹g(uⱼ⁻¹g(𝐱, ^𝐚ʲ_f, ^bʲ_f), ^𝐚ₕ, ^bₕ). Because activation function is sigmoid or sine function, eq. (24) can be simplified as: Δ ≥ ‖𝐞ⱼ₋₁‖² - ‖𝐲 - uₙ⁻¹g( (∑ᵢ₌₁ʲ⁻¹ uᵢ⁻¹ g(𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ) - ^Tʲ‖² = ‖𝐞ⱼ₋₁‖² - ‖𝐞ⱼ₋₁ - ^Tʲ‖² (unfold) = ‖𝐞ⱼ₋₁‖² - (‖𝐞ⱼ₋₁‖² - 2\u0026lt;eⱼ₋₁, ‖^Tʲ‖\u0026gt; + ‖^Tʲ‖²) (\u0026quot;\u0026lt;\u0026gt;\u0026quot; is dot product of 2 matrices: Frobenius inner product)\n= 2\u0026lt;𝐞ⱼ₋₁, ‖^Tʲ‖\u0026gt; - ‖^Tʲ‖² = ‖^Tʲ‖² ( 2\u0026lt;𝐞ⱼ₋₁, ‖^Tʲ‖\u0026gt; / ‖^Tʲ‖² - 1 ) (25)\nWe set ^Tʲ = uₙ⁻¹g(uⱼ⁻¹g( 𝐱, ^𝐚ʲ_f, ^bʲ_f) ), ^𝐚ₕ, ^bₕ ) = 𝐞ⱼ₋₁ ± σ. (σ is variance, and 𝐞ⱼ₋₁ is the expectation) So 𝐞ⱼ₋₁ = ^Tʲ ± σ. Then \u0026lt;𝐞ⱼ₋₁, ‖^Tʲ‖\u0026gt; = \u0026lt;^Tʲ± σ, ‖^Tʲ‖\u0026gt; = \u0026lt;‖^Tʲ‖² ± \u0026lt;‖^Tʲ‖,σ\u0026gt; \u0026gt;\nHence, eq. (25) can be reformulated: Δ ≥ ‖^Tʲ‖² ( 2\u0026lt; ‖^Tʲ‖² ± \u0026lt;‖^Tʲ‖,σ\u0026gt; \u0026gt; / ‖^Tʲ‖² - 1 ) = ‖^Tʲ‖² ( 1 ± 2‖σ⋅(^Tʲ)ᵀ‖/‖^Tʲ‖²) (Wandong thinks there should be a 2.) ≥\nIn addition, based on Theorem 1 and eq. (7), there will be:\n‖^Tʲ - 𝐞ⱼ₋₁‖ ≤ min_{^𝐚ʲ_f∈ 𝐑ᵈᕁⁿ} ‖uₙ⁻¹g(uⱼ⁻¹g( 𝐱, ^𝐚ʲ_f, ^bʲ_f), ^𝐚ₕ, ^bₕ) -𝐞ⱼ₋₁‖ ‖σ‖ ≤ ‖^Tʲ‖ Thus Δ ≥ 0 can be proved as: Δ ≥ ‖^Tʲ‖² (1 ± ‖σ‖ / ‖^Tʲ‖) ≥ 0 (28)\nEq. (28) means ‖𝐞ⱼ₋₁‖ ≥ ‖𝐞ⱼ‖ and ‖𝐞‖ is decreasing and bounded below by zero.\nBased on Theorem 2, Theorem 3 is given:\nTheorem 3: Given M arbitrary distinct samples (𝐱, 𝐲), 𝐱∈ 𝐑ⁿᕁᴹ, 𝐲∈ 𝐑ᵐᕁᴹ, a sigmoid or sine activation function g, and optimal feature data 𝐇⃰ᴸ_f obtained by Algorithm 1, then lim_{j➝+∞} ‖𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - \u0026hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖ = 0 holds with probability one if :\n𝐚ⱼ = g⁻¹( u(𝐲) ) ⋅ (𝐇⃰ᴸ_f)⁻¹, ^𝐚ⱼ∈ 𝐑ᵐᕁⁿ bⱼ = √mse(^𝐚ⱼ⋅(𝐇⃰ᴸ_f) - g⁻¹(u(𝐲))), ^bⱼ∈ 𝐑 βⱼ = ⟨𝐞ⱼ₋₁, g(𝐇⃰ᴸ_f, 𝐚ⱼ, bⱼ)⟩ / ‖g(𝐇⃰ᴸ_f, 𝐚ⱼ, bⱼ)‖², βⱼ∈ 𝐑 Proof:\nFirst prove that the sequence ‖𝐞ⱼᴸ‖ is decreasing and bounded below by zero. Then prove that the lim_{j➝+∞} ‖𝐞ⱼᴸ‖ = 0\nBased on Theorem 1 and Lemma 1, the network output error satisfies: ‖𝐞ⱼᴸ‖ = ‖𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - \u0026hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖ ≤ ‖𝐲 -u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁)‖ = ‖𝐞₁ᴸ‖\nBased on Theorem 2, there will be: ‖𝐞ⱼᴸ‖ ≤ ‖𝐞ⱼᴸ⁻¹‖ ≤ \u0026hellip; ≤ ‖𝐞ⱼ¹‖\nThus, ‖𝐞ⱼᴸ‖ ≤ ‖𝐞ⱼᴸ⁻¹‖ ≤ \u0026hellip; ≤ ‖𝐞ⱼ¹‖ ≤ \u0026hellip; ≤ ‖𝐞₁¹‖ and ‖𝐞ⱼᴸ‖ is decreasing and bounded below by 0.\nBased on Lemma 1, when all hidden nodes randomly generated based on any continuous sampling distribution, lim_{n➝∞} ‖f - (fₙ₋₁ + βₙ⋅g(𝐱, 𝛂ₙʳ, ♭ₙʳ) )‖ = 0 holds with probability one if βₙ = ⟨𝐞ₙ₋₁, g(𝐇⃰ᴸⱼ, 𝛂ₙʳ, ♭ₙʳ)⟩ / ‖g(𝐇⃰ᴸⱼ, 𝛂ₙʳ, ♭ₙʳ)‖².\nIn addition, ELM theories have shown that almost any nonlinear piecewise continuous random hidden node can be use in ELM, and the resultant networks have universal approximation capbilities. According to the definition of general hidden neurons, (a general hidden node contains m (basic) hidden node), a general hidden node (𝐚,b) = (𝛂ʳ₁, \u0026hellip;, 𝛂ʳₘ, bʳ₁, \u0026hellip;, bʳₘ), . Thus its output is g(𝐇⃰ᴸ_f, 𝐚ⱼʳ, ♭ⱼʳ) ≡ ∑ᵢ₌₁ᵐ g(𝐇⃰ᴸ_f, 𝛂ʳᵢ, bʳᵢ).\nTherefore, lim_{j➝∞} ‖ 𝐲 - β₁⋅u₁⁻¹g(𝐇⃰ᴸ_f, 𝐚₁, 𝑏₁) - \u0026hellip; - βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼ, 𝑏ⱼ)‖\n= lim_{n➝∞} ‖f- (fₙ₋₁ + βⱼ⋅uⱼ⁻¹g(𝐇⃰ᴸ_f, 𝐚ⱼʳ, 𝑏ⱼʳ)) ‖ = 0\nD. Proposed Method With Multinetwork Structures %%{ init: { 'flowchart': { 'curve': 'basis' } } }%% flowchart LR subgraph in[\"input feature\"] x1((x1)) \u0026 xe((\"⋮\")) \u0026 xn((xn)) end subgraph net1[\"Layer 1\"] l11((a,b)) \u0026 l1e((\"⋮\")) \u0026 l1L((a,b)) end subgraph net2[\"Layer 2\"] l21((a,b)) \u0026 l2e((\"⋮\")) \u0026 l2L((a,b)) end subgraph netC[\"Layer C\"] lC1((a,b)) \u0026 lCe((\"⋮\")) \u0026 lCL((a,b)) end x1 \u0026 xn --\u003e l11 \u0026 l1L l11 \u0026 l1L --\u003e l21 \u0026 l2L l21 \u0026 l2L -.-\u003e lC1 \u0026 lCL subgraph out[\"output y\"] direction LR y1((1)) \u0026 ye((\"⋮\")) \u0026 ym((m)) end subgraph belm1[\"Basic ELM 1\"] direction LR b11((\"aₕ,bₕ\")) \u0026 b1e((\"⋮\")) \u0026 b1m((aₕ,bₕ)) end subgraph belm2[\"Basic ELM 2\"] direction LR b21((aₕ,bₕ)) \u0026 b2e((\"⋮\")) \u0026 b2m((aₕ,bₕ)) end net1 --\u003e feat1[\"Feature\\n data\\n 𝐇¹f\"] --\u003e belm1 --\u003e out feat1 --\u003e net2 --\u003e feat2[\"Feature\\n data\\n 𝐇²f\"] --\u003e belm2 --\u003e out netC --\u003e featC[\"Feature\\n data\\n 𝐇ᶜf\"] subgraph MultiLayer ELM in \u0026 net1 \u0026 net2 \u0026 netC end %% inverse for initialization weights linkStyle 12,13,14,16,17,18 stroke:#f0f Pink links will do inverse to calculate the weights for corresponding layers.\n","date":"2023-01-18T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/b-note-elm-mltlyr/","title":"read: Multilayer Subnetwork Nodes"},{"content":"When generating random numbers, tensorflow calls:\nDDG search: \u0026ldquo;_pywrap_tensorflow.TFE_Py_FastPathExecute()\u0026rdquo;\nSimilar function: \u0026ldquo;Tile\u0026rdquo; Question about how TensorFlow API link with C++ code - reddit. The TF API name Tile is used to map it to C++ class or function name by a table.\nSimilar function \u0026ldquo;MatMul\u0026rdquo; Where can I find exactly how Tensorflow does matrix multiplication? - reddit:\n1 _result = _pywrap_tensorflow.TFE_Py_FastPathExecute( _ctx._context_handle, _ctx._eager_context.device_name, \u0026#34;MatMul\u0026#34;, name, _ctx._post_execution_callbacks, a, b, \u0026#34;transpose_a\u0026#34;, transpose_a, \u0026#34;transpose_b\u0026#34;, transpose_b) Run \u0026rsquo;nm \u0026ndash;demangle\u0026rsquo; on _pywrap_tensorflow_internal.so grep for MatMul, and get: tensorflow::SparseMatMulOp file: \u0026ldquo;tensorflow/tensorflow/core/kernels/sparse_matmul_op.cc\u0026rdquo; code \u0026ldquo;看python到C++调用关系\u0026rdquo; tensorflow二次开发 - 沉思语录20190227. Take matmul as an example：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 me@Server:~$ cd /mnt/Server/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core/python me@Server:~/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core/python$ grep -rni \u0026#34;tf_export.*matmul\u0026#34; # 这个函数需要用 tf_export 导出 ops/math_ops.py:2565:@tf_export(\u0026#34;linalg.matmul\u0026#34;, \u0026#34;matmul\u0026#34;) ops/math_ops.py:2859:tf_export(v1=[\u0026#34;sparse_matmul\u0026#34;])(sparse_matmul) ops/gen_nn_ops.py:10155:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBias\u0026#34;)(QuantizedMatMulWithBias) ops/gen_nn_ops.py:10306:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBiasAndRelu\u0026#34;)(QuantizedMatMulWithBiasAndRelu) ops/gen_nn_ops.py:10471:tf_export(\u0026#34;raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize\u0026#34;)(QuantizedMatMulWithBiasAndReluAndRequantize) ops/gen_sparse_ops.py:3078:tf_export(\u0026#34;raw_ops.SparseTensorDenseMatMul\u0026#34;)(SparseTensorDenseMatMul) ops/gen_linalg_ops.py:2531:tf_export(\u0026#34;raw_ops.TridiagonalMatMul\u0026#34;)(TridiagonalMatMul) ops/linalg/linalg_impl.py:552:@tf_export(\u0026#39;linalg.tridiagonal_matmul\u0026#39;) ops/sparse_ops.py:2188:@tf_export(\u0026#34;sparse.sparse_dense_matmul\u0026#34;, ops/gen_math_ops.py:1618:tf_export(\u0026#34;raw_ops.BatchMatMul\u0026#34;)(BatchMatMul) ops/gen_math_ops.py:1726:tf_export(\u0026#34;raw_ops.BatchMatMulV2\u0026#34;)(BatchMatMulV2) ops/gen_math_ops.py:6150:tf_export(\u0026#34;raw_ops.MatMul\u0026#34;)(MatMul) ops/gen_math_ops.py:7610:tf_export(\u0026#34;raw_ops.QuantizedMatMul\u0026#34;)(QuantizedMatMul) ops/gen_math_ops.py:10010:tf_export(\u0026#34;raw_ops.SparseMatMul\u0026#34;)(SparseMatMul) Read the usage description at math_ops.py:2565. It calls gen_math_ops.batch_mat_mul or gen_math_ops.mat_mul.\nGo to tensorflow.python.ops/gen_math_ops.py (This file maybe generated when compiling.)\nThe function batch_mat_mul calls:\n1 2 3 4 _result = _pywrap_tensorflow.TFE_Py_FastPathExecute( _ctx._context_handle, _ctx._thread_local_data.device_name, \u0026#34;BatchMatMul\u0026#34;, name, _ctx.post_execution_callbacks, x, y, \u0026#34;adj_x\u0026#34;, adj_x, \u0026#34;adj_y\u0026#34;, adj_y) So the Op function in C++ should be \u0026ldquo;BatchMatMul\u0026rdquo;.\nSeach all the place registering this Op by searching the definition of op in the source code/repo:\n1 2 3 4 5 6 7 8 # Cannot find anything in the python package installed by conda # yi@PC:/mnt/Server/anaconda3/pkgs/tensorflow-base-1.15.0-gpu_py37h9dcbed7_0$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; # yi@PC:/mnt/Server/anaconda3/envs/nerf/lib/python3.7/site-packages/tensorflow_core$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; yi@PC:~/Downloads/tensorflow_1.15$ grep -rni \u0026#34;REGISTER_OP(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; tensorflow/core/ops/math_ops.cc:946:REGISTER_OP(\u0026#34;MatMul\u0026#34;) tensorflow/compiler/mlir/tfr/resources/decomposition_lib.mlir:83:// REGISTER_OP(\u0026#34;MatMul\u0026#34;) tensorflow/c/experimental/ops/README.md:15:since `REGISTER_OP(\u0026#34;MatMul\u0026#34;)` appears in ***core/math_ops.cc***, the \u0026#34;MatMul\u0026#34; Search the kernel implementation of this Op:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 yi@PC:~/Downloads/tensorflow_1.15$ grep -rni \u0026#34;Name(\\\u0026#34;MatMul\\\u0026#34;)\u0026#34; tensorflow/core/transforms/remapper/tests/contraction.mlir:38: %MatMul, %ctl_1 = MatMul(%Placeholder, %Const) device(\u0026#34;/device:CPU:0\u0026#34;) name(\u0026#34;MatMul\u0026#34;) {T = f32, transpose_a = false, transpose_b = false} : (tensor\u0026lt;*xf32\u0026gt;, tensor\u0026lt;*xf32\u0026gt;) -\u0026gt; (tensor\u0026lt;*xf32\u0026gt;) tensorflow/core/transforms/remapper/tests/onednn_contraction.mlir:76: %MatMul, %ctl_1 = MatMul(%Placeholder, %Const) device(\u0026#34;/device:CPU:0\u0026#34;) name(\u0026#34;MatMul\u0026#34;) {T = f32, transpose_a = false, transpose_b = false} : (tensor\u0026lt;*xf32\u0026gt;, tensor\u0026lt;*xf32\u0026gt;) -\u0026gt; (tensor\u0026lt;*xf32\u0026gt;) tensorflow/core/grappler/utils/pattern_utils_test.cc:42: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), input, weight); tensorflow/core/grappler/optimizers/remapper_test.cc:1225: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/remapper_test.cc:1433: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/remapper_test.cc:1610: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/mkl_remapper_test.cc:466: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), input, filter); tensorflow/core/grappler/optimizers/mkl_remapper_test.cc:667: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), lhs, rhs); tensorflow/core/grappler/optimizers/constant_folding_test.cc:2909: Output matmul = ops::MatMul(scope.WithOpName(\u0026#34;matmul\u0026#34;), a, b); tensorflow/core/grappler/optimizers/arithmetic_optimizer_test.cc:1155: auto matmul_op = s.WithOpName(\u0026#34;matmul\u0026#34;); tensorflow/core/grappler/optimizers/arithmetic_optimizer_test.cc:1227: Output matmul = ops::BatchMatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), trans_a, trans_b); tensorflow/core/grappler/costs/analytical_cost_estimator_test.cc:79: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), flat, w2); tensorflow/core/kernels/matmul_op_test.cc:107: root.WithOpName(\u0026#34;matmul\u0026#34;), tensorflow/core/kernels/matmul_op_test.cc:126: root.WithOpName(\u0026#34;matmul\u0026#34;), tensorflow/core/kernels/mkl/mkl_fused_ops_test.cc:931: Output next_op = ops::MatMul(root.WithOpName(\u0026#34;matmul\u0026#34;), input_op, tensorflow/core/kernels/matmul_op_impl.h:881: Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_CPU).TypeConstraint\u0026lt;TYPE\u0026gt;(\u0026#34;T\u0026#34;), \\ tensorflow/core/kernels/matmul_op_impl.h:892: Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_GPU).TypeConstraint\u0026lt;TYPE\u0026gt;(\u0026#34;T\u0026#34;), \\ tensorflow/core/framework/op_kernel_test.cc:1062:REGISTER_KERNEL_BUILDER(Name(\u0026#34;MatMul\u0026#34;).Device(DEVICE_CPU), DummyKernel); tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:427: auto matmul = ops::MatMul(s.WithOpName(\u0026#34;matmul\u0026#34;), feed, const_1); tensorflow/compiler/tf2xla/kernels/matmul_op.cc:102:REGISTER_XLA_OP(Name(\u0026#34;MatMul\u0026#34;).TypeConstraint(\u0026#34;T\u0026#34;, kMatmulTypes), MatMulOp); DDG search: \u0026ldquo;How to read tensorflow c++ source code\u0026rdquo;\n","date":"2023-01-16T18:19:49-05:00","permalink":"https://zichen34.github.io/writenotes/lib/tf_randomuniform/","title":"memo: TF1.15 RandomUniform in C++"},{"content":"Normalize Dataset (2023-11-23)\nNormalizing a dataset means making the variance to 1.\n方差是一个标量（偏离期望的平方的期望）: the average of squared difference between each value in a dataset from the mean value.\nSquare ignores the sign of deviations, and only focuses on magnitudes.\nGiven 2 datasets with different scales: 1,2,6 and 10,20,60.\n1 2 -*--*--▲------* V.S. -*--*--▲------* 1 2 3 6 10 20 30 60 Their variance are 4.66 and 46.6. After normalization, their variance will both become 1. (Plain number, without any practical meaning.)\nTransform the variance to 1: subtract mean and divide by std for each sample.\n$$ Var = \\frac{∑(\\frac{x-μ}{σ} - 0)²}{n} = \\frac{∑\\frac{(x-μ)²}{σ²}}{n} = \\frac{1}{σ²} σ² = 1 $$\nThe consequences of normalization have 2 aspects:\nBecause all the squared deviations (x-μ)² are scaled by their average: σ², the measuring unit is eliminated (消除量纲). Hence, different attributes (dimensions) can be compared equally.\nOn the other hand, the sum of the scaled squared deviation becomes $n$. Thus, the new variance is 1.\n$$ Var = \\frac{∑\\frac{(x-μ)²}{σ²}}{n} = \\frac{∑\\frac{(x-μ)²}{ \\frac{∑(x-μ)²}{n} }}{n} = \\frac{n ∑\\frac{(x-μ)²}{∑(x-μ)²} }{n} = \\frac{n* 1}{n} =1 $$\nAnalogy: $\\frac{a}{a+b+c} + \\frac{b}{a+b+c} + \\frac{c}{a+b+c}=1$ Variance can be scaled to any value. Taking 1 is for interpretability and simplifying calculations (?) (2023-01-14)\nBatchNorm BatchNorm: 前向过程中每层的输入的分布一直在变化，不满足独立同分布，导致内部协变量偏移Internal Covariate Shift问题， 所以对每层的激活值在输入下一层之前，把这一个batch的分布调整为0均值，方差为1的标准正态分布， 即固定每个隐层节点的激活输入分布，让输入的激活值落在激活函数梯度较大的区域，加快收敛与避免梯度消失。 Batch Normalization导读-张俊林-知乎\n(2024-02-21) 不是“正态分布”，标准化不会改变数据的分布类型，数据原来是什么分布，做完 normalization 还是什么分布。 (mean=0, sigma=1) 只是一个 标准，之后经过一些运算，分布的参数发生变化了，还可以回到这个标准。 todo: L11.2 How BatchNorm Works\ntodo: L11.4 Why BatchNorm Works\nNormalization layer 对数据减均值，除以标准差。以下的 N 是一个 batch 中的样本个数(batch size, B)， 图片batch：(N,C,H,W)，序列batch: (N, embed_dim, seq_len)。 1d,2d,3d方法的区别在于 input 的维度。 Learnable parameters 是γ 和 β 用于对 μ和σ 做 affine 变换。 running mean 和 running var 在训练时不断使用在前向时得到的 x 的mean 和 var 做加权更新，权重为动量 momentum\nBatchNorm 让1个batch的，每个通道的均值为0，方差为1（整个batch的μ也=0）； 指定通道数量input.shape[1]: bn=nn.BatchNorm1d(num_features=C)\nLayerNorm 让每个样本的全部通道（最后几维，一个单词的特征向量，一幅图片的feature map）的均值为0，方差为1，layer指的是 fc net 的一层；\n指定最后几维: ln=nn.LayerNorm(normalized_shape=[C,H,W])）\nInstanceNorm 让每个样本的每个通道的均值为0，方差为1，同一batch内的样本没有联系，用于风格迁移；\n指定通道数量: in=nn.InstanceNorm1d(num_features=C)\nGroupNorm 介于 LayerNorm 和 Instance Norm之间，通道分组=1 就是LN，通道分组=C 就是IN；\n指定通道分组: gn=nn.GroupNorm(num_groups=2, num_channels=C) lecture 7b 神经网络的训练(归一化，迁移学习）-ranchlai-bili; github repo\nWu\u0026He ECCV2018 Weight Norm\n(2023-08-03)\nLayerNorm Customize LayerNorm, as referring to ConvNeXt-meta\u0026rsquo;s nn.LayerNorm, can only normalized the last few channels.\nF.layer_norm can be used to normlize one of the middle dimensions, as in ConvNeXt-torchvision\nLoop of ChatGPT\u0026rsquo;s Wrong Solutions:\nQ: In the following code, why is the grad of self.weight None after loss.backward()?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class LayerNorm(nn.Module): r\u0026#34;\u0026#34;\u0026#34; Input data: (batch_size, C, H, W) Use F.layer_norm to do normalization for the dim of channels. \u0026#34;\u0026#34;\u0026#34; def __init__(self, eps=1e-6): super().__init__() self.normalized_shape = (4, ) # C = 3, placeholder self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) self.eps = eps def forward(self, x): r\u0026#34;\u0026#34;\u0026#34; Since the `normalized_shape` (#channels) of x is unknown when initializing the model, implement normalization for the channels dimension here. \u0026#34;\u0026#34;\u0026#34; if self.normalized_shape != (x.shape[1],): self.weight = nn.Parameter(torch.ones(x.shape[1])) self.bias = nn.Parameter(torch.zeros(x.shape[1])) mean = torch.mean(x, dim=1, keepdim=True) var = torch.var(x, dim=1, correction=0, keepdim=True) # biased estimation x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x x = torch.ones(1,3,224,224) myLN = LayerNorm() myLN.weight.register_hook(lambda grad: print(grad.shape)) normalized_x = myLN(x) target_x = torch.rand(1,3,224,224) loss = nn.MSELoss()(normalized_x, target_x) loss.backward() Answer:\nThe redefined variables self.weight, self.bias inside the forward method are not part of the computation graph, so their gradients won\u0026rsquo;t be computed during the backward pass.\nAnd the new created parameter tensors have no connection to the original self.weight and self.bias (created during initialization), which are not used in the forward pass. so their grad attribute remain None.\nFinally, he suggested me don\u0026rsquo;t redefine in the forward.\nQ: But I don\u0026rsquo;t have information about the shape of self.weight to initialize it inside the init method.\nAnswer:\nInitialize them as None in the __init__ method and then initialize them with the correct shape during the first forward pass.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class LayerNorm(nn.Module): def __init__(self, eps=1e-6): super().__init__() self.eps = eps self.normalized_shape = None # Placeholder for the shape of self.weight self.weight = None # Placeholder for the weight parameter self.bias = None # Placeholder for the bias parameter def forward(self, x): if self.normalized_shape is None: # Initialize self.normalized_shape and create weight and bias with the correct shape self.normalized_shape = (x.shape[1],) self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) print(self.bias.shape) mean = torch.mean(x, dim=(2, 3), keepdim=True) var = torch.var(x, dim=(2, 3), correction=0, keepdim=True) x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x This code has an ValueError: optimizer got an empty parameter list\nAnwser:\nApologies for the oversight. The error is happening because the LayerNorm class does not define any learnable parameters (weights and biases) in its __init__ method. As a result, when you try to pass myLN.parameters() to the optimizer, it doesn\u0026rsquo;t find any parameters to optimize, leading to the \u0026ldquo;optimizer got an empty parameter list\u0026rdquo; error.\nIn order to use an optimizer like Adam, you need to define at least one learnable parameter within your model that will be optimized during training.\nSince you don\u0026rsquo;t have information about the shape of the input during initialization, you can create a placeholder for self.weight and self.bias in the __init__ method, and then initialize them with the correct shape during the first forward pass. To do this, we can use the register_buffer method, which allows us to create tensors that are not optimized during training but still part of the model.\nHere\u0026rsquo;s the updated code with the use of register_buffer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class LayerNorm(nn.Module): def __init__(self, eps=1e-6): super().__init__() self.eps = eps self.register_buffer(\u0026#39;normalized_shape\u0026#39;, torch.tensor([])) self.register_buffer(\u0026#39;weight\u0026#39;, torch.tensor([])) self.register_buffer(\u0026#39;bias\u0026#39;, torch.tensor([])) def forward(self, x): if self.normalized_shape.numel() == 0: # Initialize self.normalized_shape and create weight and bias with the correct shape self.normalized_shape = torch.tensor([x.shape[1]]) self.weight = nn.Parameter(torch.ones(self.normalized_shape)) self.bias = nn.Parameter(torch.zeros(self.normalized_shape)) print(self.bias.shape) mean = torch.mean(x, dim=(2, 3), keepdim=True) var = torch.var(x, dim=(2, 3), correction=0, keepdim=True) x = (x - mean) / torch.sqrt(var + self.eps) x = self.weight[:, None, None] * x + self.bias[:, None, None] print(x.shape) return x Now the model will contain learnable parameters that can be optimized using an optimizer like Adam. The grad attribute of self.weight should be available and updated after loss.backward().\nAdditionally, you can also use torch.nn.init functions to initialize the weight and bias tensors in a more standard way. For example, you can use torch.nn.init.ones_ and torch.nn.init.zeros_ to initialize the self.weight and self.bias tensors, respectively.\nregister_buffer doesn\u0026rsquo;t add parameters to optimizer to optimize neither.\nIt may be necessary to know the number of channels.\nJust sum up the attribute out_chnnels of each layer, e.g., alexnet.features[0].out_channels. PyTorch Forum\nBut Swin transformer doesn\u0026rsquo;t have Conv2d.\nMaybe I can only hard code the numbers of channels.\nIf you add the norm layer following the convolution layer, the normalized_shape is the out_channels of the convolution layer.\nBut here, I want to normalize the stacked extracted feature maps. So I need to know the total channels of the incoming feature maps.\nIf the #channels is known, I can use F.layer_norm() directly.\n(2023-10-17)\nAbsorb BN Normalization is performed to keep each \u0026ldquo;stage\u0026rdquo; (layer) in a network maintain the same distribution, such that the training could be more efficient due to\nLeaving out modeling distributions shift of each layer\u0026rsquo;s input data.\nStabilizing optimization by suppressing \u0026ldquo;outliers\u0026rdquo; (high value) in the output feature map of each layer to avoid big variations in the final prediction.\nRefer to: BN、BN同步、吸收BN - EveK的文章 - 知乎\nSpecifically, input data has (mean=0, std=1), but after a conv layer, the outcome feature maps may don\u0026rsquo;t persist (mean=0, std=1).\nIdeally, featue maps\u0026rsquo;s distribution should be transformed to (mean=0, std=1) to align with the input data. And that transformation requires its whitening matrix , which transforms co-variance matrix of the feature maps an identity matrix, meaning each dimension is unrelated. CSDN\nHowever, solving the whitening matrix for a high-dimension tensor is time consuming.\nTherefore, the \u0026ldquo;whitening\u0026rdquo; is performed only for each channel.\nOn the other hand, it\u0026rsquo;s difficult to get the exact distributions at once, because the data is trained batch-by-batch.\nTherefore, the distribution to be corrected pertains only to the small batch of data. (Or involving previous data by using moving average of history mean and std)\nAnd only 2 parameters (mean, std) of their distribution are considered to keep the distributions consistent.\nOnce a feature map $M_{(N, C, H, W)}$ spit out from a conv layer, BatchNorm normalizes the data at the same channel for all N samples in the batch:\n\\begin{algorithm} \\begin{algorithmic} \\FOR {i=0 \\TO M.size(1)} \\PROCEDURE{BN}{ M, meanʰ, stdʰ, scale, bias} \\STATE mean = M[:, i, :, :].sum() / N \\STATE std = ( (M[:, i, :, :] - mean)² / N ).sqrt() \\STATE M⁰¹ = (M[:, i, :, :] - mean) / std \\STATE M' = scale * M⁰¹ + bias \\ENDPROCEDURE \\ENDFOR \\end{algorithmic} \\end{algorithm} meanʰ and stdʰ are from history.\nThe scale factor γ and bias β are for the situation where the variation is very small. Then the differences can be magnified through scaling to avoid representation capacity degradation.\nAnd γ,β are required to be learnable to automatically find the appropriate feature levels. Otherwise, bias will grow to infinity as explained in paper.\nCombine BN into wights and bias of the last layer\n$$ M\u0026rsquo; = γ \\frac{M - mean}{std} + β = \\frac{γ⋅M}{std} \\left( β - \\frac{γ ⋅ mean}{std} \\right) $$\nIf M is calculated as $M = w⋅x + b$, by substituting it, the equation becomes:\n$$ M\u0026rsquo;= \\frac{γ⋅w}{std} x + \\left( β - \\frac{γ⋅mean}{std} + \\frac{γ⋅b}{std} \\right) $$\nThus, w becomes $\\frac{γ⋅w}{std}$ and b becomes $( β - \\frac{γ⋅mean}{std} + \\frac{γ⋅b}{std} )$.\nGiven a conv layer with $C_i$ input channels and $C_o$ output channels, BN for this layer has $C_i × C_o$ parameters, where $C_i$ is for the history channels, $C_o$ is for target channels.\n","date":"2023-01-14T12:54:12-05:00","permalink":"https://zichen34.github.io/writenotes/calc/dl_normalization/","title":"memo: DL | Normalization"},{"content":"The maximum gradient of the sigmoid activation function is 0.25, which may cause partial derivative of loss with respect to the earlier weight w very small after passing throught multipler layers. And scaling the weights down can mitigate the gradient decrease.\nBased on the chain rule, the derivative of the weight in the fisrt layer (l=1) is ∂loss/∂w¹ = ∂loss/∂o ⋅ ∂o/∂a² ⋅ ∂a²/∂a¹ ⋅ ∂a¹/∂w¹, where a = g(z), g is an activation function.\nIf the weight w¹ is small, the activation z¹ is small (around zero), so ∂a¹/∂z¹ corresponds to the highest derivative. And also if w² is small, ∂a²/∂a¹ = ∂a²/∂z²⋅ ∂z²/∂a¹, where ∂a²/∂z² will be a big derivative. Hence, ∂loss/∂w¹ can maintain a high derivative.\nSo it\u0026rsquo;s importance to initialize the weights centered at zero with small variance for getting the maximum gradient. L11.5 Weight Initialization \u0026ndash; Why Do We Care?\nActivation z is a sum of wᵢxᵢ, so it may be exploding or vanishing quickly if the W doesn\u0026rsquo;t have constriant. Weight Initialization in a Deep Network (C2W1L11) - Andrew Ng\nXavier (Glorot) initialization L11.6 Xavier Glorot and Kaiming He Initialization - Sebastian Raschka\nStep 1: Initialize weights from Gaussian or uniform distribution Step 2: Scale the weights proportional to the number of input features to the layer In particular, the weights of layer l is defined as: 𝐖 ⁽ˡ⁾ ≔ 𝐖 ⁽ˡ⁾⋅ √(1/m⁽ˡ⁻¹⁾), where m is the number of input units of the previous layer (𝑙-1) to the next layer (𝑙).\n𝐖 is initialized from Gaussion (or uniform) distribution: Wᵢⱼ⁽ˡ⁾~N(μ=0, σ²=0.01)\nRationale behind this scaling factor:\nHe (Kaiming) initialization Usage Three different commonly used initialization techniques. Here are what their variants need to be set to and which activation functions they work best with.\nInitialization Activation function Variance (σ²) Mean Glorot Linear; Tanh; Sigmoid; Softmax σ² = 1/(½⋅(fanᵢₙ+fanₒᵤₜ)) 0 He ReLu; Variants of ReLU σ² = 2/fanᵢₙ 0 LeCun SELU σ² = 1/fanᵢₙ 0 Weight Initialization for Deep Feedforward Neural Networks\n","date":"2023-01-14T12:24:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/dl_weight_initialization/","title":"memo: DL | Weight initialization"},{"content":"视频封面：一个统一视角下的概率论+统计学+信息论\n原视频：1. 从头开始，把概率、统计、信息论中零散的知识统一起来-王木头学科学\n概率论最基础的问题：用数学的方式描述\u0026quot;不确定性\u0026quot;（或\u0026quot;可能性\u0026quot;）\n把所有事件及其发生的可能性写到一个表 f 里，每次通过查表就能知道可能性是多少: f(S) = K。\nidx S K 1 事件a 数值1 2 事件b 数值2 3 事件c 数值3 \u0026hellip; \u0026hellip;.. \u0026hellip;.. 这里的“可能性” 满足一些限制：\n数值要满足事件可能性的相对关系。比如，如果事件 a 的可能性 \u0026gt; 事件 b 的可能性，则 数值1 \u0026gt; 数值2 数值需要满足事件的包含关系。比如，事件 c = {事件a，事件b}（a,b中任意一个发生），则 数值3 = 数值1 + 数值2 注意，这里并未要求所有数值归一化，所以还不是概率值 这种方式只是把“可能性”做了数学符号化，并不是“数学化”。除了要保证定义出来的这个体系自洽之外，还要尽可能简约。 比如上面的事件 c 并不需要单独定义，它的数值可以从事件 a 和 b 推导出来。 所以表格中并不需要列举所有的事件，只需要包含不可再分的原子事件及其可能性。\nV 2.0\n表1：\nidx S K 1 原子事件a 数值1 2 原子事件b 数值2 3 原子事件c 数值3 4 原子事件d 数值4 \u0026hellip; \u0026hellip;.. \u0026hellip;.. 它可以定义出：f(S) = K。只留下原子事件后，就可以确定“可能性”的最大值为所有原子事件“合”在一起组成的事件发生的可能性（只要有1个原子事件发生，这件事就算发生）： max(∑K) = ∑_{s∈all} f(s)\n有了最大可能性之后，可以定义归一化的数值 K = K/(∑_{s∈all} f(s)) ∈ [0,1]\n从上面的原子事件可推导出以下表2:\nP(S) ∑K {1,2} 数值1+数值2 {1,2,3} 数值1+数值2+数值3 {3,4} 数值3+数值4 {2,4} 数值2+数值4 \u0026hellip;\u0026hellip;. \u0026hellip;.. P(S) 表示集合 S 的幂集。\n但是原子事件对于不同的问题，不好确定。对于离散问题（掷骰子），原子事件就是点数。 但对于连续的变量（温度），原子事件可以取一个小区间。因为可以无限细分，当区间趋近于无穷小时，它发生的可能性就趋于0。 如果所有原子事件的可能性都是0，就无法从表 1 推导出表 2。\n可以从表 2 建立数学体系。\nV 3.0\n对于连续的情况，原子事件就是一个个点，如表 3，它们对应的数值并不代表发生的可能性，而是有其他意义，因为我们要用表 4 建立定义，所以表 3 的数值是由表 4 推导出来的。\n表 3：\nℝ S K 0.0 点a 数值1 \u0026hellip; 点b 数值2 \u0026hellip; 点c 数值3 \u0026hellip; 点d 数值4 \u0026hellip; \u0026hellip;. \u0026hellip;.. 表 4：\nP(ℝ) K 0.0 [1,2] f[数值1,数值2] \u0026hellip; [1,3] f[数值1,数值3] \u0026hellip; [3,4] f[数值3,数值4] \u0026hellip; [1,2]∪[3,4] f[数值1,数值2]+f[数值3,数值4] \u0026hellip; \u0026hellip;. \u0026hellip;.. ","date":"2023-01-08T16:09:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/22_%E6%A6%82%E7%8E%87%E8%AE%BA1-%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%81%94/","title":"watch: DL - 王木头 22 | Overview of Probability theory, Statistics, Information theory"},{"content":"变分自编码器（三）：这样做为什么能成？\n采样一次就够 先推断学习 z 的后验分布 p(z|x) 的参数，再从分布中采样一个 z，用它计算 x 的后验分布 p(x|z) 的参数，再算从x的后验分布中采样得到 x‘的概率，\n在 VAE 的损失函数：𝓛 = 𝔼_pᐢ(x) [ KL( p(z|x)||q(z) ) + 𝔼_p(z|x) [ -ln q(x|z) ] ]中，\nKL散度的计算是使用神经网络拟合出的z的后验分布（正态分布）的期望和方差， 而第 2 项只采了一个样本 x 做近似，所以这一项变为：-ln q(x|z), z~p(z|x)\n因为 KL 散度也可写成期望：E_p(z|x) [ ln (p(z|x)/q(z)) ]，所以它也可以只采样一个点来近似。所以损失函数就可写为：\n𝓛 = 𝔼_pᐢ(x) [ ln p(z|x) - ln q(z) - ln q(x|z) ], z~p(z|x) (5)\n苏神说，以上的损失函数也能收敛到相似的结果。\n似然不能只用一个采样点估计 极大似然的公式可以写成期望： q(x|z) = arg max_q(x|z) ∫ pᐢ(x) ln (∫ q(x|z) q(z) dz) dx ，数值计算，要乘x的概率\n= arg max_q(x|z) 𝔼_pᐢ(x) [ ln ( ∫ q(x|z) q(z) dz) ] ，采样近似求积分\n= arg max_q(x|z) 𝔼_pᐢ(x) [ ln (1/K ∑_ₖ₌₁ᴷ q(x|zₖ) ], z1,z2,\u0026hellip;,2ₖ～q(z)\n对这个期望近似：先从先验 q(z) 中采 k 个 z，算积分（求和），再采 1 个 x，求它的概率的对数就行了。\nshuhuai 说是因为 log 的方差大，所以采样太少会失效。 苏神说，因为z 和 x 是一一对应的，如果没有采到 zₖ，那它对应的 xₖ 也就采不出来，概率就算不出来，因为采样是随机的，不能保证每次采的 k 个 z，包含了本 batch 中所有 x 对应的 z，所以容易失效。\nVAE采一个点确实够了 根据对数据集的了解，数据集X本身带有很强的约束，真正独立的维度很少，所以数据集可以被投影到低维空间的一个隐变量上。 这和普通的自编码器一样，也就是 z 与 x 一一对应，也就意味这 p(z|x) 和 q(x|z) 的方差为0。 在引入标准正态形式的先验分布 q(z) 后，粗略地看，只是对隐变量空间做了平移和缩放，所以方差也可以不大。\n因为 x 的后验分布的方差很小，每次采的结果都一样，都是均值 μ(z)。 因为 z 与 x 是一一对应的，所以 z 的后验分布的方差也很小，所以每次从中采的 z 都相同。 所以采样一次，与采样多次没什么差别，期望都是一样的。\n后验之妙 直接从先验 q(z) 中采样不可行，但在后验分布 p(z|x) 中采样一个点就够了， 因为自编码器里的方差为0，引入 z 的先验（标准正态分布），方差也不会太大。\n耿直的IWAE 重要性加权自编码器 Importance Weighted AutoEncoders arxiv\n对 p(x) 做了等价变换：乘一个p(z|x),除一个 p(z|x)\np(x) = ∫ q(x|z) q(z) dz = ∫ p(z|x) ⋅ [q(x|z) ⋅ q(z)] / p(z|x) dz = 𝔼_p(z|x) [ q(x|z) ⋅ q(z) / p(z|x) ]\n这样，从 q(z) 中采样就变成了从 p(z|x) 中采样，此前已论述了后验分布 p(z|x) 方差较小，所以采样几个点就够了：\n∫ q(x|z) q(z) dz = 1/k ∑ₖ₌₁ᴷ [ q(x|zₖ)q(zₖ)/p(zₖ|x) ], z1,z2,..,zₖ～p(z|x)\n代入似然函数：\nq(x|z) = arg max_q(x|z) 𝔼_pᐢ(x) [ ln ( ∫ q(x|z) q(z) dz) ] = arg max_q(x|z) 𝔼_pᐢ(x) [ ln ( 1/k ∑ₖ₌₁ᴷ [ q(x|zₖ)q(zₖ)/p(zₖ|x) ] ) ]\n= arg min_{q(x|z),p(z|x)} 𝔼_pᐢ(x) [ -ln ( 1/k ∑ₖ₌₁ᴷ [ q(x|zₖ)q(zₖ)/p(zₖ|x) ] ) ], z1,z2,..,zₖ～p(z|x) ，加个负号，求极小值。\n当 k=1 时，与 (5) 式一样，从这个角度看，IWAE 是VAE的升级版。\n其实，等价变换可以使用 z 的任意分布（只要能采出z就行），\n“选择 p(z|x) 只是因为它有聚焦性，便于采样。而当 k 足够大时，p(z|x) 的具体形式就不重要了”\nIWAE 中削弱了推断模型 p(z|x) 的作用，不去近似后验分布\n","date":"2023-01-01T22:48:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/d-note-vae_3-su/","title":"read: Blog - 苏剑林 | VAE-3"},{"content":"Use the built-in dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torchvision import datasets, transforms # Root directory for the dataset data_root = \u0026#39;data\u0026#39; # Spatial size of training images, images are resized to this size. image_size = 64 celeba_data = datasets.CelebA( data_root, download=True, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) ] ) ) This will download and extract the zip file.\nSOURCE CODE FOR TORCHVISION.DATASETS.CELEBA\nExtract the zip file 1 2 3 4 5 6 7 8 9 import zipfile data_root = \u0026#39;data/celeba\u0026#39; # Add shortcut of dataset to your google drive zip_path = \u0026#39;/content/drive/MyDrive/CelebA/Img/img_align_celeba.zip\u0026#39; with zipfile.ZipFile(zip_path, \u0026#39;r\u0026#39;) as ziphandler: ziphandler.extractall(\u0026#39;data\u0026#39;) How do I load the CelebA dataset on Google Colab, using torch vision, without running out of memory?\nZipFile - GfG\nDownload with gdown Install the package: pip install gdown. Copy the URL in the address bar. 1 2 3 4 import gdown url = \u0026#34;https://drive.google.com/u/0/uc?id=1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ\u0026amp;export=download\u0026#34; output = \u0026#34;celeba.zip\u0026#34; gdown.download(url, output) Then unzip it and its subfolder:\n1 2 3 unzip celeba.zip cd celeba unzip img_align_celeba.zip (Python) Use the gdown package to download files from Google Drive\n(2024-02-21)\ngdown 4.7.1 cannot download large dataset dtu.zip 554 MB with the following error reported:\n1 2 3 4 5 6 7 8 9 (base) zi@lambda-server:~/Downloads$ gdown 135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ Access denied with the following error: Cannot retrieve the public link of the file. You may need to change the permission to \u0026#39;Anyone with the link\u0026#39;, or have had many accesses. You may still be able to access the file from the browser: https://drive.google.com/uc?id=135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_ Update gdown to 5.1.0 to avoid it: issue\n1 pip install --upgrade gdown ","date":"2022-12-31T23:36:00Z","permalink":"https://zichen34.github.io/writenotes/lib/load_celeba_colab/","title":"memo: load CelebA on Colab"},{"content":"笔记 for 变分自编码器（二）：从贝叶斯观点出发\n数值计算 vs 采样计算 数值计算是先给个数列，对里面的每个数求概率p(x⁽ⁱ⁾)，再加权求和 ∑ᵢ₌₀ⁿ x⁽ⁱ⁾p(x⁽ⁱ⁾) (x⁽ⁱ⁾-x⁽ⁱ⁻¹⁾)； 采样计算是先从分布中采样，求采样点概率的平均，所以不需要再乘样本点出现的概率：E[x]≈1/n⋅∑ᵢ₌₀ⁿ x⁽ⁱ⁾, x⁽ⁱ⁾∼p(x)。\n推导VAE 的损失函数 苏神从逼近联合概率 p(x,z) 出发，而不是从逼近z的后验 p(z|x) 出发。 也许是因为沿着 EM 的思路走，就需要对后验 p(z|x) 求近似，所以很多人聚焦于推导 p(z|x)。\n因为想求样本集合 x的分布，\n但是难以直接描述复杂分布，所以通过引入隐变量把x的分布变成条件分布的叠加，而后可以对隐变量的分布和条件分布做适当简化（比如都假设为正态分布），并且可以用深度学习模型近似求（隐变量）条件分布的参数，即“深度概率图模型”。\n假设 x 是由 z 生成的，所以求 p(x) 可以通过把联合分布 p(x,z) 中的 p(z) 积掉求得： p(x) = ∫p(x,z) dz = ∫ p(x|z)p(z) dz\n目标是用一个 q(x,z) 逼近 p(x,z)，又因为 p(z) 是先验（已知），所以当 q(x,z)≈p(x,z) 时，生成模型 p(x|z) 也就学到了，“一举两得”。\n通过最小化 KL 散度逼近：KL( p(x,z) || q(x,z) ) = ∫∫ p(x,z) [ ln (p(x,z)/q(x,z)) ] dz dx，是一个二重积分\n把 p(x,z) 写成 pᐢ(x)⋅p(z|x)，也就是推断过程，由x的先验推出z：\nKL( p(x,z) || q(x,z) ) = ∫∫ pᐢ(x)⋅p(z|x) [ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) ] dz dx = ∫ pᐢ(x) [ ∫ p(z|x) ⋅ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) dz ] dx = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) dz]\n可以蒙特卡罗采样近似求这个期望，也就是把每个样本 x⁽ⁱ⁾ 代入上面中括号里的函数（代入概率密度公式可算出概率值），把函数值求均值。\n这个期望可以进一步简化，把 ln 拆开： ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) = ln pᐢ(x) + ln (p(z|x)/q(x,z))\n𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( pᐢ(x)⋅p(z|x) / q(x,z) ) dz ] = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln pᐢ(x) dz] + 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln (p(z|x)/q(x,z)) dz]\n上面第 1 个期望： 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln pᐢ(x) dz] = 𝔼_pᐢ(x) [ ln pᐢ(x)⋅∫ p(z|x) dz ] = 𝔼_pᐢ(x) [ ln pᐢ(x) ]\n这里的 pᐢ(x) 是根据样本 x⁽⁰⁾, x⁽¹⁾,\u0026hellip;, x⁽ⁿ⁾ 确定的关于 x 的先验分布，是已知的确定的，所以这一项是一个常数。 所以 KL 散度 = 常数 + 一个期望：\nKL( p(x,z) || q(x,z) ) = 常数 + 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln (p(z|x)/q(x,z)) dz]\n所以最小化KL散度，对应目标函数 𝓛 就是第2个期望：\n𝓛 = KL( p(x,z) || q(x,z) ) - 常数，则𝓛 的下界就是\u0026quot;-常数\u0026quot;: -𝔼_pᐢ(x) [ ln pᐢ(x) ]， 其中 pᐢ(x) 不一定是概率，在连续情况时，pᐢ(x) 是概率密度函数，它可以大于1 也可以小于1，所以下界不一定是非负的，即 loss 可能是负数。\n再把 𝓛 里的 ln 和 q(x,z) 展开：\n𝓛 = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( p(z|x) / q(x,z) ) dz] = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( p(z|x) / (q(x|z)q(z)) dz ] = 𝔼_pᐢ(x) [ ∫ p(z|x) ⋅ ln ( p(z|x)/q(z) ) dz - ∫ p(z|x) ⋅ ln q(x|z) dz]\n把里面的积分写成期望：\n𝓛 = 𝔼_pᐢ(x) [ KL( p(z|x)||q(z) ) + 𝔼_p(z|x) [ -ln q(x|z) ] ]\n括号里的就是 VAE 的损失函数：KL散度（正则化项）+ x的后验按照 z 的后验求期望 shuhuai008-30VAE\n不能把括号里面的两项分开看或分开最小化。 如果只令 KL( p(z|x)||q(z) )=0，即每个后验都是标准正态分布，与x无关，导致生成的 x 不准，概率 q(x|z) 会很小，-ln q(x|z) 就会很大。 而如果 -ln q(x|z) 很小，即x后验概率 q(x|z) 大，后验分布 p(z|x) 的峰肯定集中在 x 附近，即 p(z|x) 的方差小，与标准正态的差距大，KL( p(z|x)||q(z) ) 不会小。 所以这两部分 loss 是相互拮抗的，𝓛 要以整体来看。 也就是要推断过程与生成过程相互博弈。\n算法设计 损失函数中未知的分布包括：z 的先验 q(z)，z 的后验 p(z|x)，x 的后验 q(x|z) （x 的先验pᐢ(x) 是已知的）\n为了便于采样，假设 z 的先验分布为标准多元正态分布：q(z)=N(0,I)\n用神经网络拟合 z 的后验 p(z|x) 和 x 的后验 q(x|z)。\n计算 z 的后验是推断过程，对应 EM 的 E步：近似求得 p(z|x)；计算 x 的后验是生成过程，对应 EM 的 M步：把z的近似后验代入似然函数，求极大似然时，对应的模型参数。 （EM中用于逼近 p(z|x) 的神经网络的参数是 ϕ；用于逼近 q(x|z) 的神经网络（也可直接求导）的参数是 θ）\n后验分布近似 假设 z 的后验是（各分量独立的）一般正态分布，所以需要神经网络逼近它的期望和方差。期望和方差都由 x 决定，即是 x 的函数 μ_ϕ(x), Σ_ϕ(x)\n然后 KL 散度就可以写出来了: 1/2(-logσ² + μ² + σ² -1)，已在VAE第一篇推导过。变分自编码器（一）：原来是这么一回事\n生成模型近似 对于生成模型部分 q(x|z) 的假设，原作者在论文《Auto-Encoding Variational Bayes》中，给出了两种方案：二项分布或正态分布。“既要满足概率的定义（归一化），又要容易算，没多少选择”\n二项分布只有一个参数：\u0026ldquo;抛硬币向上的概率 ρ\u0026rdquo;。所以对于一个 D 维的样本 x，x 的每一维都是个二值的，所以一个输入样本 x 在 z 成立的情况下，发生的概率就是： q(x|z) = ∏ₖ₌₁ᴰ (ρₖ(z))ˣᵏ (1-ρₖ(z))¹⁻ˣᵏ\n此时的 -ln q(x|z) = ∑ₖ₌₁ᴰ [ -xₖ ln ρₖ(z) - (1-xₖ) ln(1-ρₖ(z)) ]\n也就是说神经网络的输出 ρ(z) 需要是在 0～1 之间（比如用 sigmoid 激活），然后用交叉熵作为损失函数。\n如果假设 q(x|z) 是正态分布，用神经网络估计它的期望 μ_θ(z) 和方差 Σ_θ(z)，于是： -ln q(x|z) = ½ || (x-μ_θ(z)) / Σ_θ(z) ||² + D/2 ln 2π + ½∑ₖ₌₁ᴰ lnΣ_θ(z)。\n很多时候，训练时方差会固定为一个较小的常数（每次采样都会采到μ），所以神经网络只需估计μ，也就是把 μ 当作生成的 x\u0026rsquo;，则上式重构误差可简化为： -ln q(x|z) = ½ || (x-μ_θ(z)) / Σ_θ(z) ||²\nx \u0026ndash;\u0026gt; z \u0026ndash;\u0026gt; x'\n综上，对于二值数据，假设 q(x|z) 是伯努利分布（二项分布），可以对 decoder （第2个神经网络）的输出用 sigmoid 激活，并用交叉熵作为损失函数； 而对于一般数据，假设 q(x|z) 是正态分布，则使用 mse 作为损失函数。\n从后验中采样 z 的技巧 损失函数的第 2 项是：𝔼_p(z|x) [ -ln q(x|z) ]，根据蒙特卡罗的思想，这个期望用均值近似：先采样 z，用 z 计算 x 的后验分布 q(x|z)，再从中采样 x 计算它出现概率的对数：-ln q(x|z)，再求均值：\n𝔼_p(z|x) [ -ln q(x|z) ] = -1/n ∑ᵢ₌₁ᴺ ln q(x|zᵢ), zᵢ～p(z|x)\n假设了 p(z|x) 是正态分布，它的参数 μ_ϕ, Σ_ϕ 已由神经网络算出，再使用重参数化技巧就能采样出 z 。\n但是要采样多少个合适呢？因为每个 z 是专属于 1 个 x，所以只从 p(z|x) 中采一个 z 来计算 x 的分布 q(x|z)，再计算 -ln q(x|z)，就是loss值。\n最终：𝓛 = 𝔼_pᐢ(x) [ KL( p(z|x)||q(z) ) -ln q(x|z) ] , z～p(z|x)\n因为每次 epoch 的隐变量都是随机生成的，因此当 epoch 数足够多时，可以保证采样的充分行。苏神试过采样多个的情形，感觉生成的样本没有明显变化。\n","date":"2022-12-30T20:07:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/d-note-vae_2-su/","title":"read: Blog - 苏剑林 | VAE-2"},{"content":"(2023-10-11)\n一维高斯 数据 的空间分布像是一个中间重两头轻的铁棒 (线)； 二维高斯数据的分布像一个 椭圆 (面)； 服从三维高斯分布的数据的空间分布像一个 椭球 (体)。\nSource video: 机器学习-白板推导系列(二)-数学基础\n1. 高斯分布1-极大似然估计 P1\n高斯分布在（统计）机器学习中非常重要。比如线性高斯模型是一整套体系，卡尔曼滤波就是一种线性高斯模型。 隐变量与隐变量，隐变量与观测变量之间服从高斯分布。\n1 2 3 z1 -\u0026gt; z2 -\u0026gt; ... -\u0026gt; zN ↓ ↓ ↓ ↓ x1 -\u0026gt; x2 -\u0026gt; ... -\u0026gt; xN 比如从 z⁽ᵗ⁾ ➔ z⁽ᵗ⁺¹⁾ 的转移服从 线性高斯：z⁽ᵗ⁺¹⁾=Az⁽ᵗ⁾+B+ε，先做线性变换再加上 ε 高斯噪声。\n再比如 P-PCA（概率PCA）中，原数据是 p 维的 x∈ℝᵖ 要降到 q 维空间 z∈ℝ^q， 假设 x 与 z 之间的变换为：x=Wz+μ+ε，ε～N(0,σ²⋅I)，0均值的各向同性的（即 Σ 是对角矩阵，并且对角线上的值都是σ²）高斯分布（标准正态分布）。 如果对角线上的值不相同，就变成了因子分析\n而且线性高斯模型有很多特性：比如一个高维的随机变量 x∈ℝᵖ 服从高斯分布 x～N(μ,Σ)，将它分成两个小组 x₁∈ℝᵐ, x₂∈ℝⁿ, m+n=p。 则 x₁ 也服从高斯分布，条件概率 x₂|x₁ 也服从高斯分布。\n参数估计 给定数据 𝐗：N个样本，每个样本 x 是 p 维的 x∈ℝᵖ，𝐗 就是一个 N x p 的矩阵：\n𝐗 = (x₁, x₂,\u0026hellip;, xₙ)ᵀ = $[^{^{ x₁ᵀ}_{x₂ᵀ}} _{^{⋱}_{ xₙᵀ}}]$ₙₓₚ\n$$ \\pmb X = (x_1, x_2, \u0026hellip;,x_N)^T \\\\ = \\begin{pmatrix} x_{11} \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{1p} \\\\ x_{21} \u0026amp; x_{22} \u0026amp; \\cdots \u0026amp; x_{2p} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ x_{p1} \u0026amp; x_{p2} \u0026amp; \\cdots \u0026amp; x_{pp} \\end{pmatrix}^T \\\\ = \\begin{pmatrix} x_{11} \u0026amp; x_{21} \u0026amp; \\cdots \u0026amp; x_{p1} \\\\ x_{12} \u0026amp; x_{22} \u0026amp; \\cdots \u0026amp; x_{p2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ x_{1p} \u0026amp; x_{2p} \u0026amp; \\cdots \u0026amp; x_{pp} \\end{pmatrix} $$\n$x_i \\in \\R^p$ : p维实数向量空间（p维欧氏空间），列向量 假设样本 xᵢ 之间是独立同分布，都是从一个高斯分布中抽出来的: xᵢ～N(μ,Σ)。\n令参数 θ = (μ,Σ) （Σ是协方差矩阵：对角线上是σ²，μ 是位置参数，σ是尺度参数）\nθ 的极大似然估计 (MLE)：θₘₗₑ = arg max_θ P(X|θ)\n为简化计算：令 p=1 (一维)，则 θ = (μ,Σ=σ²)，一维高斯分布的概率密度函数: p(x) = 1/(√(2π)⋅σ) ⋅ exp(-(x-μ)²/2σ²)\n$$ p(x) = \\frac{1}{\\sqrt{2π}⋅σ} exp(-\\frac{(x-μ)²}{2σ²}) $$\n高维（p维）的高斯分布的概率密度函数 (PDF)：p(x)=1/(2πᵖᐟ²⋅|Σ|¹ᐟ²) ⋅ exp(-½(x-μ)ᵀ⋅Σ⁻¹⋅(x-μ))\n$$ p(𝐱) = \\frac{1}{(2π)^{p/2} |Σ|^½} exp(-\\frac{(𝐱-\\bm μ)ᵀ(𝐱-\\bm μ)}{2Σ}) $$\nlog P(X|θ) = log ∏ᵢ₌₁ᴺ p(xᵢ|θ) ，独立同分布，联合概率写成连乘 = ∑ᵢ₌₁ᴺ log p(xᵢ|θ) ，log把连乘变连加\n= ∑ᵢ₌₁ᴺ log 1/(√(2π)⋅σ) ⋅ exp(-(xᵢ-μ)²/2σ²)，代入高斯分布\n= ∑ᵢ₌₁ᴺ [ log 1/√(2π) + log 1/σ -(xᵢ-μ)²/2σ² ]\n先求最佳的 μ: 当对数似然最大时，μ 等于多少？\nμₘₗₑ = arg max_μ P(X|θ) = arg max_μ ∑ᵢ₌₁ᴺ [ log 1/√(2π) + log 1/σ -(xᵢ-μ)²/2σ² ] = arg max_μ ∑ᵢ₌₁ᴺ [ -(xᵢ-μ)²/2σ² ] ，只保留与μ相关的项 = arg min_μ ∑ᵢ₌₁ᴺ (xᵢ-μ)² ，σ² 一定是正的 对 μ 求偏导，令其等于0：\n∂ ∑ᵢ₌₁ᴺ (xᵢ-μ)² / ∂μ = ∑ᵢ₌₁ᴺ -2(xᵢ-μ) = 0 ∑ᵢ₌₁ᴺ (xᵢ-μ) = 0 ∑ᵢ₌₁ᴺ xᵢ - ∑ᵢ₌₁ᴺ μ = 0 ∑ᵢ₌₁ᴺ xᵢ = Nμ ，μ 与 i 无关 μₘₗₑ = 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ ，最优的 μ 就是样本的均值。 μₘₗₑ 是无偏的，因为 μₘₗₑ 的期望：\nE[μₘₗₑ] = 1/N ⋅ ∑ᵢ₌₁ᴺ [μₘₗₑ] = 1/N ⋅ ∑ᵢ₌₁ᴺ [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ ] = 1/N ⋅ ∑ᵢ₌₁ᴺ [μ] ，因为样本iid,服从高斯分布 = 1/N ⋅ Nμ = μ (真实的 μ)\nμₘₗₑ 的期望是无偏的，但每次的 μₘₗₑ 并不是真实的 μ，每次估出来的可能比μ大或者小，这个 μ 无法通过一次估计得到，只能是随着样本量的增加，把每次试验的结果求平均才会无限近似真实的 μ。 （但是样本不可能有无限个，所以实际上真实均值是无法得到的）\n求最优的 σ² 用类似的过程:\nσ²ₘₗₑ = arg max_σ² P(X|θ) = arg max_σ² ∑ᵢ₌₁ᴺ [ log 1/√(2π) + log 1/σ -(xᵢ-μ)²/2σ² ] = arg max_σ² ∑ᵢ₌₁ᴺ [ -log σ -(xᵢ-μ)²/2σ² ] ，只保留与σ²相关的项 目标函数对 σ 求偏导：\n∂ ∑ᵢ₌₁ᴺ [ -log σ -(xᵢ-μ)²/2σ² ] / ∂σ = ∑ᵢ₌₁ᴺ [-1/σ - (xᵢ-μ)²/2 ⋅ -2σ⁻³ = 0 ，两边同乘σ³ ∑ᵢ₌₁ᴺ [-σ² + (xᵢ-μ)² = 0 ，把∑ 带进去 ∑ᵢ₌₁ᴺ σ² = ∑ᵢ₌₁ᴺ (xᵢ-μ)² ，σ² 与 i 无关 N σ² = ∑ᵢ₌₁ᴺ (xᵢ-μ)² σ²ₘₗₑ = 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ-μₘₗₑ)²\n最优的 σ² 也是对样本方差求期望。 这个 σ²ₘₗₑ 是有偏估计，因为 σ²ₘₗₑ 的期望不等于 σ²：\n先对 σ²ₘₗₑ 做简化:\nσ²ₘₗₑ = 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ - μₘₗₑ)² = 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ² - 2xᵢμₘₗₑ + μₘₗₑ²) ， 展开括号的平方\n= 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - 1/N ⋅ ∑ᵢ₌₁ᴺ 2xᵢ⋅μₘₗₑ + 1/N ⋅ ∑ᵢ₌₁ᴺ μₘₗₑ²) ，∑带进去 = 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - 2μₘₗₑ² + μₘₗₑ² ，第2项里有个样本均值，第3项与i无关 = 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - μₘₗₑ²\n对 σ²ₘₗₑ 求期望：\nE[ σ²ₘₗₑ ] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ - μₘₗₑ)² ] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² - μₘₗₑ² ] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² -μ² - (μₘₗₑ²-μ²) ] ，添加μ² 横等变换 = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² -μ²] - E [μₘₗₑ²-μ²] ，拆成2个期望 先看第 1 项： 把 μ² 写到 ∑ 里面： E [ 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ² -μ²] = E [ 1/N ⋅ ∑ᵢ₌₁ᴺ (xᵢ² -μ²)] ，μ²与i无关 = 1/N ⋅ ∑ᵢ₌₁ᴺ [E(xᵢ² -μ²)] ，里面是一个期望，把外面的期望展开 = 1/N ⋅ ∑ᵢ₌₁ᴺ [ E(xᵢ²) - E(μ²) ] ，μ²是常数 = 1/N ⋅ ∑ᵢ₌₁ᴺ [ E(xᵢ²) - μ² ] ，因为 μ 是随机变量 xᵢ 的期望：E(xᵢ)=μ = 1/N ⋅ ∑ᵢ₌₁ᴺ [ E(xᵢ²) - E(xᵢ)² ] ，这是xᵢ 的方差（定义）\n= 1/N ⋅ ∑ᵢ₌₁ᴺ σ² = σ²\n也就是说，第 1 项是 σ²，对于第 2 项： E [μₘₗₑ²-μ²] = E (μₘₗₑ²) - E(μ²) = E (μₘₗₑ²) - μ² = E (μₘₗₑ²) - E²(μₘₗₑ) = Var(μₘₗₑ) ，这是 μₘₗₑ 的方差 = Var( 1/N ⋅ ∑ᵢ₌₁ᴺ xᵢ ) ，把1/N 提出来，会变成平方 = 1/N² ⋅ ∑ᵢ₌₁ᴺ Var(xᵢ) = 1/N² ⋅ ∑ᵢ₌₁ᴺ σ² = 1/N ⋅ σ²\n所以 σ²ₘₗₑ 的期望等于上面两项相减： E[ σ²ₘₗₑ ] = σ² - 1/N ⋅ σ² = (N-1)/N ⋅ σ²\nσ² 的无偏估计是 σ² = 1/(N-1) ⋅ ∑ᵢ₌₁ᴺ (xᵢ-μₘₗₑ)²\n2. 高斯分布2-极大似然估计（无偏估计VS有偏估计） P2\n一个估计量 T(θ) 的期望 E(T(θ) 等于它的本身最初的值，这个参数的估计是无偏的，比如 E(μ^) = μ; E(σ^) = σ。如果不相等就是有偏的。\n用MLE估计出来的方差 σ²ₘₗₑ 的期望小于模型的真实方差。 因为计算方差时，计算的是样本到样本均值 的距离，而不是到真实均值的距离，因为真实均值要做无数次试验才能得到（除非μ=样本均值）。 如果用样本均值代替真实均值μ 的话：Var(x)= E [(x-μ)²] ➔ Var(x)= E [( x-x̄ )]²，除非 μ=样本均值 x⁻，Var(x) = E( x-x̄ )² \u0026lt; E(x-μ)²。 证明如下样本方差与总体方差 - 小时候挺菜 -博客园：\n(1/n)⋅∑ᵢ₌₁ᴺ(xᵢ-x⁻)² = (1/n)⋅∑ᵢ₌₁ᴺ [(xᵢ-μ)+ (μ-x⁻)]² = (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)² + (2/n)⋅∑ᵢ₌₁ᴺ(xᵢ-μ)⋅(μ-x⁻) + (1/n)⋅∑ᵢ₌₁ᴺ(μ-x⁻)² = (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)² + 2(x⁻-μ)(μ-x⁻) + (μ-x⁻)² = (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)² - (μ-x⁻)²\n如果 μ ≠ x⁻，则 (1/n)⋅∑ᵢ₌₁ᴺ(xᵢ-x⁻)² \u0026lt; (1/n)⋅∑ᵢ₌₁ᴺ (xᵢ-μ)²\nMLE 是点估计，会造成偏差\n3. 高斯分布3-从概率密度函数角度观察 P3\n多维高斯分布的概率密度函数: 对于一个 p 维的随机向量 𝐱∈ℝᵖ 服从多维的高斯分布，其概率密度函数为：\n𝐱～N(𝛍,Σ)= 1/(2πᵖᐟ²⋅|Σ|¹ᐟ²) ⋅ exp(-½(𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍))， 其中μ 是期望, Σ是方差矩阵，exp里面-1/2后面是线代中的二次型\n对于一个样本（随机向量）p个维度: 𝐱 = (x1,\\ x2,\\ \u0026hellip;,\\ xp)\n𝛍 也是 p 维的向量：𝛍 = (μ1,\\ μ,\\ \u0026hellip;,\\ μp)\nΣ 就是 p×p 维的矩阵：\nΣ =(σ11, σ12, \u0026hellip;, σ1p σ21, σ22, \u0026hellip;, σ2p ⋮ ⋮ ⋮ ⋮ σp1, σp2, \u0026hellip;, σpp)\n通常，这个矩阵Σ是半正定的，而且是沿对角线对称的，比如 σ12 = σ21。 在本节中假设 Σ 是正定的，以便叙述。\n在 PDF 中，𝐱 是自变量，𝛍,Σ 是参数。式中与 𝐱 相关的只有 (𝐱-𝛍)ᵀ，其他部分认为是系数，所以集中看一下 exp 中的部分：\n(𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) 是一个标量，这个函数可以看作 马氏距离，两个向量：𝐱 和 𝛍 之间的距离\n马氏距离 Mahalanobis distance is used to measure multivariate distances between a point and a normal distribution with covariance 𝚺. janakiev-blog\n假设有两个二维的向量：𝐳1=(z11,\\\\ z12)，𝐳2=(z21,\\\\ z22)\n根据定义求两向量之间的距离： (𝐳1-𝐳2)ᵀ ⋅ Σ⁻¹ ⋅ (𝐳1-𝐳2) = (z11-z21, z12-z22)ᵀ ⋅ Σ⁻¹ ⋅ (z11-z21, \\\\ z12-z22)\n如果方差矩阵 Σ 是单位矩阵：Σ=I，马氏距离就是欧氏距离\n(z11-z21, z12-z22)ᵀ ⋅ I ⋅ (z11-z21, \\\\ z12-z22) = (z11-z21)² + (z12-z22)²\n方差矩阵 因为假设了 Σ 是正定的（每个特征值λᵢ都是大于0的，不能等于0），而且是对称的，所以对 Σ 做一个特征值分解：\nΣ = UΛUᵀ，其中 U 是正交矩阵：UUᵀ=UᵀU=I，U=(𝐮₁,𝐮₂,\u0026hellip;, 𝐮ₚ)，每个小 𝐮 是列向量，所以U是p×p的矩阵；Λ 是对角的：Λ=diag(λᵢ), i=1,\u0026hellip;p；\n把矩阵形式展开：\nΣ = UΛUᵀ = (𝐮₁,𝐮₂,\u0026hellip;, 𝐮ₚ) ⋅\n(λ₁, 0, 0, \u0026hellip; 0 0, λ₂, 0, \u0026hellip; 0 ⋮⋮⋮⋮ 0, 0, 0, \u0026hellip; λₚ) ⋅ (𝐮₁ᵀ,\\\\ 𝐮₂ᵀ,\\\\ \u0026hellip;,\\\\ 𝐮ₚᵀ) = (𝐮₁λ₁, 𝐮₂λ₂, \u0026hellip;, 𝐮ₚλₚ) ⋅ (𝐮₁ᵀ,\\\\ 𝐮₂ᵀ,\\\\ \u0026hellip;, 𝐮ₚᵀ) = ∑ᵢ₌₁ᵖ 𝐮ᵢλᵢ𝐮ᵢᵀ\nΣ⁻¹ = (UΛUᵀ)⁻¹ = (Uᵀ)⁻¹ Λ⁻¹ U⁻¹ = U Λ⁻¹ U⁻¹ ，（正交矩阵的转置等于它的逆），其中特征值矩阵 Λ⁻¹ = diag(1/ λᵢ), ᵢ=1,\u0026hellip;,p = ∑ᵢ₌₁ᵖ 𝐮ᵢ (1/λᵢ) 𝐮ᵢᵀ\n把 Σ⁻¹ 代入马氏距离：\n(𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) = (𝐱-𝛍)ᵀ ⋅ ∑ᵢ₌₁ᵖ [ 𝐮ᵢ (1/λᵢ) 𝐮ᵢᵀ ] ⋅ (𝐱-𝛍) ，把(𝐱-𝛍) 放到 Σ 里面 = ∑ᵢ₌₁ᵖ [ (𝐱-𝛍)ᵀ ⋅ 𝐮ᵢ (1/λᵢ) 𝐮ᵢᵀ ⋅ (𝐱-𝛍) ]\n定义一个向量 𝐲 = (y₁, y₂, \u0026hellip;, yₚ)，其中每一维是一个标量：yᵢ = (𝐱-𝛍)ᵀ ⋅ 𝐮ᵢ 所以上面的马氏距离可以写成： (𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) = ∑ᵢ₌₁ᵖ [ yᵢ (1/λᵢ) yᵢᵀ ] = ∑ᵢ₌₁ᵖ [ yᵢ² (1/λᵢ) ] ， yᵢ 是标量，就是平方\n假设 p=2，马氏距离= y₁²/λᵢ + y₂²/λ₂，而且令 马氏距离= 1: y₁²/λ₁ + y₂²/λ₂ = 1，就得到一个椭圆曲线\nx1 和 x2 是原来的坐标轴，𝐮₁, 𝐮₂ 是新的基向量。把 xᵢ 变换成 yᵢ 就是向量 𝐱 减去均值之后（去中心化），然后投影到向量 𝐮ᵢ 上得到的坐标。\n𝐱 和 𝛍 之间的马氏距离，在𝐮₁, 𝐮₂ 的坐标系下是一个椭圆曲线，不同的样本x对应曲线上不同的点。 如果 λ₁ \u0026gt; λ₂，则半长轴=√λ₁；半短轴=√λ₂\n𝐱 和 𝛍 之间的马氏距离可以是任意值，即椭圆方程不等于1，取不同的值 r：\ny₁²/λ₁ + y₂²/λ₂ = r\n因为马氏距离是概率密度中的一项，马氏距离不同对应的概率就不同，r-\u0026gt; p(r)\n当 r 取不同值的时候，就是一圈一圈的椭圆。对于一个两维的随机变量 𝐱=(x1,x2)，它的概率值y是第3维，所以在这个三维空间中，如果固定 y 值（即r值），就是对曲面水平横切了一刀，得到一条等高线，是在特征向量 𝐮 下的椭圆\n如果 Σ 分解出来的所有的特征值 λᵢ 都相同，曲线就变成圆了，在各个轴上的投影都相等。\n4. 高斯分布4-局限性 P4\n在高维高斯分布的概率密度函数中，只有 exp 里面的“二次型”与样本 x 相关，而且是𝐱 和 𝛍 之间的马氏距离， 对正定矩阵 Σ 做特征值分解，可以发现“二次型”对应于以特征向量 𝐮 为基底的坐标系下的标准椭圆曲线，\n用 Δ 表示二次型，则概率密度函数：p(x)=1/(2πᵖᐟ²⋅|Σ|¹ᐟ²) ⋅ exp(-Δ/2)\n在概率密度函数中，只有 x 是自变量，𝛍,Σ 都是模型的参数。 给定一个概率值 0 \u0026lt;= p(x) \u0026lt;= 1，比如 0.5，它会对应一个 Δ= r1，对应到椭圆方程的右侧，再把 r1 乘到左边就变成了标准的椭圆方程。 当 p=0.5, 则 Δ= r2，会得到另一个椭圆曲线。 如果把所有概率取一遍，就对应一个曲面。椭圆对应一条条等高线\n(1) 方差矩阵参数太多 Σₚₓₚ 有 p² 个参数，但又因为它是对称的，实际的参数个数= (p²-p)/2 + p = (p²+p)/2 = p(p+1)/2 = O(p²)\n时间复杂度是维数的平方，在高维问题中，p 会很大，参数太多，计算太复杂。\n可以简化方差矩阵：假设 Σ 是对角矩阵，只有对角线上有值 (λ₁ 0 0 \u0026hellip; 0 \\ 0 λ₂ 0 \u0026hellip; 0 \\ \u0026hellip; \\ 0 0 0 \u0026hellip; λₚ)\n因为它是对角矩阵（已经相互独立），就不需要对它做特征值分解了，Σ就用自己，没有引入新基向量 U=(𝐮₁,𝐮₂,\u0026hellip;, 𝐮ₚ)，U就是单位矩阵，所以基向量还是 (𝐱₁,𝐱₂,\u0026hellip;, 𝐱ₚ)，仍向𝐱投影 journeyc 的评论\nΔ = (𝐱-𝛍)ᵀ⋅Σ⁻¹⋅(𝐱-𝛍) = ∑ᵢ₌₁ᵖ (𝐱ᵢ-𝛍ᵢ)² (1/λᵢ)\n在各个轴上的投影为 yᵢ = (𝐱-𝛍)ᵀ ⋅ 𝐱ᵢ\n所以这里的椭圆就是在 (𝐱₁,𝐱₂,\u0026hellip;, 𝐱ₚ) 下的，长短轴与各x轴平行，而没有旋转\n如果 ∑ 是对角阵，并且 λ₁ = λ₂ = \u0026hellip; = λₚ，曲线就变成“正的”圆形，称为各向同性的高斯分布\n通过简化方差矩阵，变成各向同性的，解决参数过多的问题。 比如在因子分析中，假设隐变量 z 是对角矩阵。概率PCA （P-PCA) 就是因子分析的特殊情况，假设 z 是各向同性的概率分布\n(2) 一个高斯表达力有限 仅用一个高斯分布，对模型的描述可能不准确。GMM 是多个高斯分布的混合。\n5. 高斯分布5-已知联合概率求边缘概率及条件概率 P5\n已知一个多维高斯分布，求它的边缘概率分布，和条件概率分布\n把 p 维的随机向量 𝐱 看作两组的联合：𝐱=(𝐱ₐ,\\ 𝐱b)，其中 xₐ∈ℝᵐ, xb∈ℝⁿ, m+n=p。 把 p 维的期望 𝛍 也分成两组: 𝛍 = (𝛍ₐ,\\ 𝛍b)； 把方差矩阵 ∑ 拆成 4 块：∑ = (∑ₐₐ , ∑ₐb \\ ∑bₐ , ∑bb)，这是对称矩阵，所以∑ₐb = ∑bₐ\n然后把随机向量 𝐱 看作是 (𝐱ₐ,𝐱b) 的联合概率分布，求边缘概率分布 P(𝐱ₐ)，以及条件概率分布 P(𝐱b|𝐱ₐ)。根据对称性，P(𝐱b) 和 P(𝐱ₐ|𝐱b) 也可求得\n求解方法：配方法（PRML中），这节会采用一种比配方法稍微简单一点的方法\n首先引入一个定理： 已知一个随机变量 x 服从高斯分布 x～N(μ,Σ)，有 y=Ax+B，则 y 也服从高斯分布： y～N(Aμ+B, AΣAᵀ)。 （x 是p维向量，y 是 q 维向量，则 A 是qxp 的矩阵，Σ是pxp的，则 AΣAᵀ 是qxq的）\n不严谨的解释：\nE[y] = E[Ax+B] = AE +B = Aμ+B\nVar[y] = Var [ Ax+B ] = Var [Ax] + Var [B] , B是常数方差=0 = A Var Aᵀ = AΣAᵀ\n比如对一维随机变量 x～N(μ, σ²)，y=ax+b，则 Var[y] = Var[ax+b] = a²Var = a²⋅σ²。直观上看，一维是a²，多维应该是AAᵀ\n求边缘概率 P(𝐱ₐ) 的分布 构造一个矩阵： 𝐱ₐ = (Iₘ 0) (𝐱ₐ \\ 𝐱b)\n其中 (Iₘ 0) 对应 A，(𝐱ₐ \\ 𝐱b) 就是 x，B=0\n根据上述定理:\nE[𝐱ₐ] = E[A𝐱+B] = A𝛍+B = (Iₘ 0) (𝛍ₐ,\\ 𝛍b) = 𝛍ₐ\nVar[𝐱ₐ] = AΣAᵀ = (Iₘ 0) (∑ₐₐ , ∑ₐb \\ ∑bₐ , ∑bb) (Iₘ \\ 0) = (Iₘ ∑ₐₐ, Iₘ ∑ₐb) (Iₘ \\ 0) = ∑ₐₐ\n所以 𝐱ₐ～N(𝛍ₐ, ∑ₐₐ)\n求条件概率 P(𝐱b|𝐱ₐ) 的分布 （可用配方法）\n构造3个变量：\n先定义一个 x_{b⋅a} 的变量：x_{b⋅a} = x_b - ∑_bₐ ∑ₐₐ⁻¹ xₐ 。 如果 x_{b⋅a} 的分布知道了 x_{b⋅a}～N(𝛍^, ∑^)，那么 x_b = x_{b⋅a} + ∑_bₐ ∑ₐₐ⁻¹ xₐ，x_b 与 xₐ 之间的关系就找到了。 rationalizable 的评论：“如果能找到一个线性变换 Z = Xa+C⋅Xb，使得Z与Xb不相关 Cov(Z, Xb)=0，那么 Var(Xa|Xb) = Var(Z|Xb) = Var(Z)，E(Xa|Xb)=E(Z)-C⋅Xb，就都可以计算出来了。”\n与此对应，定义 𝛍_{b⋅a} = 𝛍_b - ∑_bₐ ∑ₐₐ⁻¹ 𝛍ₐ。\n因为 ∑ 是分块矩阵，把∑_{bb⋅a} 定义为∑_{aa} 的舒尔补Schur complement: ∑_{bb⋅a} =∑_{bb} - ∑_{ba} ∑_{aa}⁻¹ ∑_{ab}\n(1) 先求 x_{b⋅a} 的分布: x_{b⋅a} = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) (xₐ,\\\\ x_b) = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) 𝐱\n所以 (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) 就是 A\nE[x_{b⋅a}] = A𝛍+B = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ)⋅(𝛍ₐ,\\\\ 𝛍b) = 𝛍b - ∑_bₐ ∑ₐₐ⁻¹⋅𝛍ₐ = 𝛍_{b⋅a}\nVar[x_{b⋅a}] = AΣAᵀ = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) ⋅ (∑ₐₐ , ∑ₐb \\\\ ∑bₐ , ∑bb) ⋅ ((-∑_bₐ ∑ₐₐ⁻¹)ᵀ,\\\\ Iₙ) = (- ∑_bₐ ∑ₐₐ⁻¹, Iₙ) ⋅ (∑ₐₐ , ∑ₐb \\\\ ∑bₐ , ∑bb) ⋅ (-∑ₐₐ⁻¹ ∑_bₐᵀ, \\\\ I) ，其中 ∑ₐₐ⁻¹ 是对称的，转置没变 = (- ∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐₐ + ∑bₐ, -∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐb + ∑bb) ⋅ (-∑ₐₐ⁻¹ ∑_bₐᵀ, \\\\ I)\n在第 1 项中，∑ₐₐ 是可逆，∑ₐₐ⁻¹ ⋅ ∑ₐₐ = I，然后 -∑_bₐ + ∑bₐ = 0：\n= (0, -∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐb + ∑bb) ⋅ (-∑ₐₐ⁻¹ ∑_bₐᵀ, \\\\ I)\n因为两个向量变量的协方差 Cov(X,Y) 与 Cov(Y,X) 互为转置矩阵，但它自身并不是对称矩阵，因此转置并不是它自己: ∑_bₐᵀ ≠ ∑_bₐ\n= ∑bb - ∑_bₐ ∑ₐₐ⁻¹ ⋅ ∑ₐb ，就是定义的 ∑_{bb⋅a}\n（弹幕：应该是根据舒尔补反向构造的 x_{b⋅a} 和 𝛍_{b⋅a}。\u0026ldquo;构造型证明\u0026rdquo;?）\n所以得出结论： x_{b⋅a} ～ N(𝛍_{b⋅a}, ∑_{bb⋅a})\n(2) 证明：x_{b⋅a} 与 xₐ 相互独立 专栏\n若 x 为服从高斯分布的随机变量 x～N(μ,Σ)，则相互独立的两变量就不相关： M⋅x ⟂ N⋅x ⇔ M∑N =0。 其中 M，N 均为矩阵，M⋅x，N⋅x 也服从高斯分布。\n证：因为 x～N(μ,Σ)，所以 M⋅x～N(Mμ, MΣMᵀ)，N⋅x～N(Nμ, NΣNᵀ)，计算二者的协方差矩阵：\nCov(M⋅x，N⋅x) = E[(M⋅x-M⋅μ) (N⋅x-N⋅μ)ᵀ] = E[ M ⋅ (x-μ)⋅(x-μ)ᵀ ⋅ N] = M⋅E[ (x-μ) (x-μ)ᵀ ]⋅N = MΣNᵀ\n因为 M⋅x ⟂ N⋅x 且均为高斯分布，所以 Cov(M⋅x，N⋅x) = MΣNᵀ = 0\n在之前的推导中，引入了 x_{b⋅a} = x_b - ∑_bₐ⋅∑ₐₐ⁻¹ ⋅ xₐ 可以改写为：\nx_{b⋅a} = (-∑_bₐ⋅∑ₐₐ⁻¹, I) (xₐ,\\\\ x_b)，这里 (-∑_bₐ⋅∑ₐₐ⁻¹, I) 对应 M，(xₐ,\\\\ x_b) 是 x\nxₐ = (I, 0) (xₐ,\\\\ x_b)，这里(I, 0)对应 N，(xₐ,\\\\ x_b) 是 x\nx_{b⋅a} 就是 M⋅x，xₐ 就是N⋅x， 所以 MΣNᵀ = (-∑_bₐ⋅∑ₐₐ⁻¹, I) (∑ₐₐ , ∑ₐb \\\\ ∑bₐ , ∑bb) (I,\\\\ 0)，其中∑是x的方差矩阵 = (0, ∑bb-∑_bₐ⋅∑ₐₐ⁻¹) (I,\\\\ 0) = 0\n所以 x_{b⋅a} 与 xₐ 相互独立可推出 x_{b⋅a} 与 xₐ 不相关： x_{b⋅a} ⟂ xₐ ⇒ x_{b⋅a} | xₐ = x_{b⋅a}\n注意：\n一般情况下两个随机变量之间独立一定不相关，不相关不一定独立（也就是独立的概念更“苛刻”一点，不相关的概率稍微“弱”一点） 如果两个随机变量均服从高斯分布，那么“不相关”等价于“独立” 随机变量独立是由分布函数定义的，而不相关只是用一阶矩（即数学期望）定义的。分布函数是比矩更高的概念，分布函数能决定矩，而矩未必能决定分布函数。 独立和互斥是什么关系？独立和不相关是什么关系？ - 武辰的文章 -知乎\n(3) 再求 xb|xₐ 的分布 知道了 x_{b⋅ₐ} 的分布后，可知 x_b：\nx_b = x_{b⋅a} + ∑_{ba} ∑ₐₐ⁻¹ xₐ\n因为 x_b 与 xₐ 是相互独立的服从高斯分布的随机变量，所以：\nx_b|xₐ = x_{b⋅a}|xₐ - ∑_{ba} ∑ₐₐ⁻¹ xₐ | xₐ = x_b = x_{b⋅a} - ∑_{ba} ∑ₐₐ⁻¹ xₐ\n还是同样套用Ax+B，把 x_{b⋅a} 看作 x, A=I, B=∑_bₐ ⋅ ∑ₐₐ⁻¹ ⋅ xₐ\nE [x_b|xₐ] = 𝛍_{b⋅a} + ∑_bₐ⋅∑ₐₐ⁻¹⋅xₐ\nVar [x_b|xa] = A⋅Var(x_{b⋅a})⋅Aᵀ = ∑_{bb⋅a}\n所以 xb|xₐ ～N ( 𝛍_{b⋅a} + ∑_bₐ⋅∑ₐₐ⁻¹⋅xₐ, ∑_{bb⋅a} )\n用同样的方法，可得 x_b～N(𝛍_b, ∑_{bb})。\nP(xₐ|x_b) 就是把 P(xb|xₐ) 中的 a,b 对换。\n6. 高斯分布6-已知边缘和条件概率求联合概率分布 P6\n已知（边缘概率分布）p(x) = N(x | μ,Λ⁻¹) 和（条件概率分布）p(y|x) = N(y | Ax+b, L⁻¹) （其中 Λ 是精度矩阵=协方差矩阵 ∑ 的逆，Λ⁻¹=∑）， 并且假设 y 的期望与 x 之间有线性关系：μ_y = Ax+b，但二者的方差之间没关系 求 p(y)，p(x|y)。（类似于贝叶斯定理：p(x|y) = p(y|x)p(x) / p(y)）\n这个问题经常在线性高斯模型中出现，比如在卡尔曼滤波中，隐状态（高斯分布）与观测变量之间有线性关系： z⁽ᵗ⁺¹⁾ = A z⁽ᵗ⁾ + B + ε，ε是噪声，ε～N(0,Q)，ε与z 相互独立， x⁽ᵗ⁾= Cz⁽ᵗ⁾+D+δ，δ也是高斯噪声 δ～N(0,R)\n还比如在概率pca中，把 p 维的 x 降到 q 维的 z 空间，满足线性关系 x = Wz+b+ε，ε 是服从各向同性的高斯分布 ε～N(0, σ²I)，z 的先验可以是标准高斯分布 z～N(0,I)，ε 与 z 相互独立。也是一种线性高斯模型。\n（在PRML 中仍使用配方法求解，而且比上一节的推导更复杂）\n(1) 求 p(y) 依据两个前提条件，可以把 y 定义为： y = Ax + b + ε，令 ε 是一个高斯噪声 ε～N(0, L⁻¹)，x、y、ε 都是随机变量，ε和x相互独立，A和b 都是系数。 （y 是在 x 给定的情况下发生的，固定了x则它的方差为0，所以方差L⁻¹中不含 x 的方差）\nE[y] = E[Ax + b + ε] = E [ Ax+b ] + E [ε] ，按照上一节引入的定理\n= Aμ+b ，ε的期望是0\nVar[y] = Var[Ax + b + ε] = Var[ Ax+b ] + Var [E] = A⋅Λ⁻¹⋅Aᵀ + L⁻¹\n所以就得到了 y 的分布： y～N(Aμ+b, A⋅Λ⁻¹⋅Aᵀ + L⁻¹)\n(2) 求 p(z) 构造变量 z 是 x 和 y 的联合：z = (x,\\ y)。 （“任意个有限维的高斯分布的联合分布均是高斯分布”，是说两个随机变量合起来的分布还是高斯，并不是GMM（对似然加权）正态分布随机变量的和还是正态分布吗？ - 江城雨-知乎。幽冥若炎的评论：不需要两个多维随机变量相互独立？x与y独立，因为x与ε独立）\n根据上一节的结论，边缘概率分布就相当于仅仅考虑一部分维度，所以z服从的分布的期望和方差就是 x 的μ,∑ 与 y 的μ,∑ 拼起来：\nE[z] = [μ,\\\\ Aμ+b]\nVar[z] = [Λ⁻¹, unknown,\\\\ unknown, A⋅Λ⁻¹⋅Aᵀ + L⁻¹]\n也就是： z = (x,\\ y) ～ N ( [μ,\\\\ Aμ+b], [Λ⁻¹, unknown,\\\\ unknown, A⋅Λ⁻¹⋅Aᵀ + L⁻¹] )\n因为方差矩阵本身应该是对称的，所以两个 unknown 是一样的，记为 Δ，它的最后结果里面不应该出现x 和 y。\nΔ = Cov(x,y) = E[ (x-E[x]) ⋅ (y-E[y])ᵀ ] = E [ (x-μ) ⋅ (y-(Aμ+b)ᵀ) ，把 y 的表达式代入 = E [ (x-μ) ⋅ (Ax + b + ε -Aμ -b)ᵀ) = E [ (x-μ) ⋅ (Ax - Aμ + ε)ᵀ) ，两项里有共同的(x-μ) = E [ (x-μ) ⋅ (Ax - Aμ)ᵀ + (x-μ) ⋅ εᵀ) = E [ (x-μ) ⋅ (Ax - Aμ)ᵀ ] + E [ (x-μ) ⋅ εᵀ ]\n因为 x 与 ε 独立：x⟂ε，所以 x-μ 与 ε 独立，所以第2个期望可拆开： E [ (x-μ) ⋅ εᵀ ] = E [ (x-μ) ] ⋅ E[ εᵀ ]，又因为 ε 的期望等于0，所以整项都=0，所以就剩第1项\nΔ = E [ (x-μ) ⋅ (Ax - Aμ)ᵀ ] = E [ (x-μ) ⋅ (x - μ)ᵀ ⋅ Aᵀ ] ，A不是随机变量 = E [ (x-μ) ⋅ (x - μ)ᵀ ] ⋅ Aᵀ，第1项是x的方差 = Var[x] ⋅ Aᵀ = Λ⁻¹ ⋅ Aᵀ\n所以 z 的方差矩阵： Var(z) = [Λ⁻¹, Λ⁻¹ ⋅ Aᵀ,\\\\ Λ⁻¹ ⋅ Aᵀ, A⋅Λ⁻¹⋅Aᵀ + L⁻¹]\n(3) 求 p(x|y) 根据上一节的结论：已知 x=(xₐ,\\ x_b)，则有 P(x_b | xₐ) = N(𝛍_{b⋅a} + ∑_bₐ⋅∑ₐₐ⁻¹⋅xₐ, ∑_{bb⋅a})。\nz 对应 x, xₐ,x_b 对应 x,y，z 的期望和方差已知，代入即可。\n7. 不等式1-Jensen不等式 P7\n比如在 EM 算法推导时，会用到此不等式\n假设 f(x) 是凸 convex function 则该函数的期望大于等于期望的函数： E(f(x)) ≥ f(E(x))\n(1) 证明 （以下是一个构造性证明）\n有一个凸函数 f(x)，在 x 轴上取 x 的期望值 E[x]，它对应的函数值是 f(E[x])，过这个函数值做这个凸函数的切线，将这条切线定义为 l(x) = ax+b。\nf(E[x]) = l(E[x]) = aE(x)+b\n因为 f(x) 是凸函数，所以对于任意的 x 都有 f(x)≥l(x)，对此式两边同时求期望： E[ f(x) ] ≥ E [ l(x) ] = E[ax+b] = E[ax] + E[b] = aE[x] +b = f(E[x])\n也就是 E[ f(x) ] ≥ f(E[x])\n死神之名111 的评论： “第七节：很不幸，这个推导是错误的，你先说了f=l，你其实只说明了线性函数的jensen不等式成立，真正严格证明你需要做两个积分差，再利用凸性，二阶导＞0，综合一下就是当前结果。”\n(2) 直观理解 通常的表述：在 x 轴上取两个点 a 和 b，然后在两点之间任意选取一个 c 点，选取时通常先在 [0,1] 上去一个 t 值，然后做“线性插值”：c=ta+(1-t)b。\nf(a) 和 f(b) 连线为函数 g，则 c 的函数值为 g(c)，可以看到 g(c) ≥ f(c)\n设线段ac 的长度是 t，线段 cb 的长度是 1-t（实际应该是ta 和(1-t)b），所以两梯形的斜边之比也是 t:1-t，\n然后分别过 g(c) 和 f(a) 做水平线，形成相似三角形，则 f(b)-g(c) 与 g(c)-f(a) 之比是 1-t : t。\n所以 g(c) = t ⋅ f(a) + (1-t) ⋅ f(b)\n代入 g(c) ≥ f(c) 就是： t ⋅ f(a) + (1-t) ⋅ f(b) ≥ f(t⋅a+(1-t)⋅b)\n","date":"2022-12-28T00:31:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/02_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","title":"watch: ML - 白板 02 | Mathematical Basis"},{"content":"Difference between plt and fig,ax object-oriented\nInverse the X-axis GeeksforGeeks\n1 2 3 import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.invert_xaxis() Add caption below the x-label SO\n1 2 3 fig, ax = plt.subplots() txt = \u0026#39;near and far are distance\u0026#39; fig.text(x=.5, y=.001, s=txt, ha=\u0026#39;center\u0026#39;) 1 2 3 fig, ax = plt.subplots() ax.set_xlabel(\u0026#39;\u0026#39;\u0026#39;z_cam near and far are distance\u0026#39;\u0026#39;\u0026#39;) 微分柱形图 SO\nSet xticks xticks is the (fixed) number on the x-axis, while xticklabels can be customized strings.\nFor plt (Matplotlib.pyplot.xticks()): Matplotlib Set_xticks - Detailed Tutorial - Python Guides\nMatplotlib.axes.Axes.set_xticklabels() Matplotlib Set_xticklabels -Python Guides\n1 2 3 fig, ax = plt.subplots() ax.set_xticks([0, np.pi, 2*np.pi, 3*np.pi]) ax.set_xticklabels(labels=[\u0026#39;0\u0026#39;, r\u0026#39;$\\pi$\u0026#39;, r\u0026#39;2$\\pi$\u0026#39;, r\u0026#39;3$\\pi$\u0026#39;], fontdict=None, fontsize=6, fontstyle=\u0026#39;italic\u0026#39;, color=\u0026#39;red\u0026#39;, verticalalignment=\u0026#39;top\u0026#39;, horizontalalignment=\u0026#39;left\u0026#39;, rotation=90, minor=True, Change the number and text of the showing Ticks GfG\nFor plt:\n1 2 plt.xticks(ticks=[1,2,3,4], labels=None, **kwargs) plt.yticks(ticks=[7,13,24,22], labels=None,) Use locator_param() to change the tightness and number of ticks.\nplt.locator_params(axis='both', nbins=4)\nUse xlim() to restrict the diplayed area, rather the whoel plot.\n1 2 plt.xlim(0,3) plt.locator_params(axis=\u0026#39;x\u0026#39;, nbins=3) Use Matplotlib.ticker.MaxNLocator class\nSet Number of Ticks in Matplotlib | Delft Stack\n1 2 3 4 5 6 7 8 from matplotlib.ticker import MaxNLocator fig, ax = plt.subplots(1,1) ax.yaxis.set_major_locator(MaxNLocator(5)) # hide specific ticks for i, tick in enumerate(ax.xaxis.get_ticklabels()): if i%2 !=0: tick.set_visible(False) Set y-axis view limits Matplotlib.axes.Axes.set_ylim() in Python - GeeksforGeeks\n1 2 3 4 5 6 7 fig, ax = plt.subplots(facecolor =\u0026#39;# A0F0CC\u0026#39;) x, y = 4*(np.random.rand(2, 100) - .5) ax.plot(x, y, \u0026#39;g\u0026#39;) ax.set_ylim(-3, 3) # 游标 cursor = Cursor(ax, useblit = True, color =\u0026#39;red\u0026#39;, linewidth = 2) Set the 1st x as 1 (not 0) Python MatplotLib plot x-axis with first x-axis value labeled as 1 (instead of 0)\nAdd extra ticks and labels Add extra ticks which must be number, not work for string SO\n1 2 3 fig, ax= plt.subplots() extraticks = [2.1, 3, 7.6] ax.set_xticks(list(ax.get_xticks()) + extraticks) Use ax.set_xticklabels() for string Create label list - SO\n1 2 3 4 5 near, far = 1, 15 ax.set_xticks(list(ax.get_xticks()) + [-near,-far]) # Their labels also have to be appended at the end. xticklabels= [\u0026#39;-16\u0026#39;, \u0026#39;-14\u0026#39;, \u0026#39;-12\u0026#39;, \u0026#39;-10\u0026#39;, \u0026#39;-8\u0026#39;, \u0026#39;-6\u0026#39;, \u0026#39;-4\u0026#39;, \u0026#39;-2\u0026#39;,\u0026#39;0\u0026#39;, \u0026#39;-near\u0026#39;, \u0026#39;-far\u0026#39;] ax.set_xticklabels(xticklabels, rotation=90) Example: Mark the yticks for the highest and lowest points:\n1 2 3 4 5 6 7 rmse = np.load(\u0026#34;./error_list.npz\u0026#34;)[\u0026#39;Train_RMSE\u0026#39;] idx = np.arange(len(rmse)) fig, ax = plt.subplots() ax.plot(idx, rmse) extraticks = [rmse.min(), rmse.max()] ax.set_yticks(list(ax.get_yticks()) + extraticks) Set xticks as integer (Not sure) ax.xaxis.set_major_locator(MaxNLocator(integer=True))\nPython：使用f-string保留小数点位数 csdn\n用decimal 模块：w3cschool\n1 2 3 from decimal import Decimal a = 12.345 Decimal(a).quantize(Decimal(\u0026#34;0.00\u0026#34;)) # 使用默认的进位方式（同round）“0.00”表示保留小数点后两位 Margin to leave more space around the figure to show the ticks of the boundary Geeksforgeeks\nOr prevent the markers get clipped by the axes GfG plt xticks\n1 2 fig, ax = plt.subplots() ax.margins(0.5) Hide ticks GfG Legend gfg Docs\n1 2 ax.plot([1, 2, 3], label=\u0026#39;Inline label\u0026#39;) ax.legend(loc=\u0026#39;best\u0026#39;, bbox_to_anchor=(0.5, 0., 0.5, 0.5)) Put it below the subplots: ax.legend(loc=\u0026quot;upper center\u0026quot;, bbox_to_anchor=(0.5, -0.13),fancybox=False, shadow=False, ncol=5, fontsize=6)\nHow to add legend below subplots in matplotlib?\nDraw a horizontal line plt : gfg\n1 plt.axhline(y=0.5, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;) ax: SO\n1 ax.hlines(y=0.2, xmin=0.01, xmax=20, linewidth=2, color=\u0026#39;r\u0026#39;) Tight layout 1 plt.tight_layout() Or: plt.rcParams[\u0026quot;savefig.bbox\u0026quot;] = 'tight'\nSave as png gfg\n1 2 3 4 ax.set_facecolor(\u0026#39;pink\u0026#39;) # inner backgraound plt.savefig(\u0026#39;depth_buffer.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, dpi =100, facecolor=\u0026#34;white\u0026#34;) # bkg color plt.savefig(\u0026#39;output.jpg\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, dpi =40) #is smaller Draw markers on axes plt.plot(x,y, zorder=10, clip_on=False)\nplotting markers on top of axes\nArrow of axis matplotlib axis arrow tip\nChange fonts Plot in Ubuntu has ugly fonts (which is like \u0026lsquo;sans-serif\u0026rsquo;).\nplt.rcParams[\u0026quot;font.family\u0026quot;] = \u0026quot;cursive\u0026quot;, This will change to your computer\u0026rsquo;s default monospace font. How to change fonts in matplotlib (python)?\n散点图 plt.scatter(x,y, c='r', s=4), s can control marker size, docs\nmarker 是反着画的\nmatplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None, **kwargs) x,y组成了散点的坐标；s为散点的面积；c为散点的颜色（默认为蓝色\u0026rsquo;b\u0026rsquo;）；marker为散点的标记；alpha为散点的透明度（0与1之间的数，0为完全透明，1为完全不透明）;linewidths为散点边缘的线宽；如果marker为None，则使用verts的值构建散点标记；edgecolors为散点边缘颜色。 csdn\n拉长坐标间距 Python设置matplotlib.plot的坐标轴刻度间隔以及刻度范围\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import matplotlib.pyplot as plt from matplotlib.pyplot import MultipleLocator # 用于设置刻度间隔 x_vals = list(range(11)) y_vals = [x**2 for x in x_vals] plt.plot(x_vals, y_vals, c=\u0026#39;green\u0026#39;) plt.title(\u0026#39;Squares\u0026#39;, fontsize=24) plt.tick_params(axis=\u0026#39;both\u0026#39;,which=\u0026#39;major\u0026#39;,labelsize=14) plt.xlabel(\u0026#39;Numbers\u0026#39;,fontsize=14) plt.ylabel(\u0026#39;Squares\u0026#39;,fontsize=14) x_major_locator=MultipleLocator(1) #把x轴的刻度间隔设置为1，并存在变量里 y_major_locator=MultipleLocator(10) #把y轴的刻度间隔设置为10，并存在变量里 ax=plt.gca() #ax为两条坐标轴的实例 ax.xaxis.set_major_locator(x_major_locator) #把x轴的主刻度设置为1的倍数 ax.yaxis.set_major_locator(y_major_locator) #把y轴的主刻度设置为10的倍数 plt.xlim(-0.5,11) #把x轴的刻度范围设置为-0.5到11，因为0.5不满一个刻度间隔，所以数字不会显示出来，但是能看到一点空白 plt.ylim(-5,110) #把y轴的刻度范围设置为-5到110，同理，-5不会标出来，但是能看到一点空白 plt.show() 设置图片尺寸大小 如何指定matplotlib输出图片的尺寸？ - pythonic生物人的回答 - 知乎 https://www.zhihu.com/question/37221233/answer/2250419008\nplt.rcParams['figure.figsize'] = (12.0, 8.0)\nfig, ax = plt.subplots(figsize=(15,0.5),dpi=300) pool\n设置图片分辨率 plt.figure(figsize=(a, b), dpi=dpi)\nmatplotlib设置分辨率\n网格 plt.grid(visible=True,linestyle=\u0026quot;--\u0026quot;, color='gray', linewidth='1',)\n绘制CDF (221211) Repeat drawing How to change a matplotlib figure in a different cell in Jupyter Notebook?\n1 2 fig.gca().scatter(x3, y3, color=\u0026#39;g\u0026#39;, linewidth=5) fig (2022-12-13)\nGet the arguments name use lib inspect with two .f_back: callers_local_vars = inspect.currentframe().f_back.f_back.f_locals.items(). Getting the name of a variable as a string\nGet current color How to get color of most recent plotted line in Python\u0026rsquo;s plt plt.gca().lines[-1].get_color()\nuse ax Get default line colour cycle\n1 2 line = ax.plot(x,y) ax.plot(x, y+.3, color = line.get_color()) Combine two figures matplotlib: combine different figures and put them in a single subplot sharing a common legend Multiple subplots GfG\n1 2 3 4 5 6 7 fig, ax = plt.subplots(3, 3) # draw graph for i in ax: for j in i: j.plot(np.random.randint(0, 5, 5), np.random.randint(0, 5, 5)) plt.show() title of subplots: ax[0][0].set_title(\u0026quot;xxx\u0026quot;)\nhide x ticks for top subplots and y ticks for right plots: How to set a single, main title above all the subplots with Pyplot?\n1 2 plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False) plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False) Set xticks for subplots: How to set xticks in subplots\n1 2 3 4 5 6 7 8 9 10 11 12 fig, axes = plt.subplots(nrows=3, ncols=4) # Set the ticks and ticklabels for all axes plt.setp(axes, xticks=[0.1, 0.5, 0.9], xticklabels=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], yticks=[1, 2, 3]) # Use the pyplot interface to change just one subplot... plt.sca(axes[1, 1]) plt.xticks(range(3), [\u0026#39;A\u0026#39;, \u0026#39;Big\u0026#39;, \u0026#39;Cat\u0026#39;], color=\u0026#39;red\u0026#39;) fig.tight_layout() plt.show() Set the fontsize of numbers of xticks: plt.setp(ax.get_xticklabels(), fontsize=12, fontweight=\u0026quot;normal\u0026quot;, horizontalalignment=\u0026quot;left\u0026quot;, rotation=90) set_xticks() needs argument for \u0026lsquo;fontsize\u0026rsquo; #12318 set_xticklabels-Docs\nScale y-axis Docs\n1 2 3 4 x = np.arange(len(Train_RMSE)) fig, ax = plt.subplots() plt.yscale(\u0026#39;log\u0026#39;,) ax.plot(x, Train_RMSE) Composite 2 imgs (2023-12-20) Setting alpha:\n1 2 3 4 5 6 7 8 9 10 11 12 13 import cv2 import matplotlib.pyplot as plt img1 = cv2.imread(\u0026#39;rot1.png\u0026#39;) pts1 = [(24, 124), (49, 124), (98, 124), (104, 124), (120, 124), (18, 146), (37, 146), (65, 146), (102, 146), (133, 146)] for pt in pts1: cv2.circle(img1, pt, 0, (0, 0, 255)) plt.figure(figsize=(20,10)) plt.imshow(img1[:,:, ::-1]) img2 = cv2.imread(\u0026#39;rot2.png\u0026#39;) plt.imshow(img2[:,:,::-1], alpha=0.6) An example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 import matplotlib.pyplot as plt import numpy as np from matplotlib.ticker import MultipleLocator epoch = np.array( [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]) train_loss = np.array( [0.27195, 0.07782, 0.05188, 0.03321, 0.03054, 0.02322, 0.01962, 0.01537, 0.01347, 0.01266, 0.00895, 0.01107, 0.00755, 0.00633, 0.00915, 0.00872, 0.00431, 0.00733, 0.00584, 0.00477, 0.00502, 0.00384, 0.00282, 0.00506, 0.00307, 0.00325, 0.00588, 0.00277, 0.00445, 0.00318, 0.00181, 0.00532, 0.00218, 0.00379, 0.00190, 0.00201, 0.00242, 0.00164, 0.00215, 0.00176, 0.00374, 0.00149, 0.00145, 0.00170, 0.00481, 0.00136, 0.00184, 0.00165, 0.00162, 0.00265, 0.00285, 0.00172, 0.00241, 0.00139, 0.00181, 0.00161, 0.00135, 0.00264, 0.00165, 0.00158]) valid_loss = np.array( [0.04518, 0.02028, 0.01517, 0.01853, 0.00928, 0.01604, 0.01256, 0.01068, 0.01229, 0.01066, 0.01251, 0.00756, 0.00836, 0.00777, 0.00623, 0.00942, 0.00761, 0.00691, 0.00626, 0.00607, 0.00708, 0.00703, 0.01061, 0.00758, 0.00807, 0.00541, 0.00521, 0.00570, 0.00844, 0.00610, 0.00735, 0.00448, 0.00466, 0.00734, 0.00633, 0.00511, 0.00737, 0.00380, 0.00410, 0.00604, 0.00685, 0.00546, 0.00618, 0.00411, 0.00543, 0.00572, 0.00631, 0.00821, 0.00578, 0.00525, 0.00391, 0.00529, 0.00621, 0.00606, 0.00979, 0.00515, 0.00555, 0.00712, 0.00535, 0.00439]) # 解决曲线图里面中文显示乱码问题 # plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;Hiragino Sans GB\u0026#39;] # 用来正常显示中文标签 # # plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False plt.figure(figsize=(8,6)) # 增加曲线说明 A, = plt.plot(epoch, train_loss, lw=1, label=\u0026#39;train_loss\u0026#39;) B, = plt.plot(epoch, valid_loss, lw=1, label=\u0026#39;valid_loss\u0026#39;) font1 = {\u0026#39;family\u0026#39;: \u0026#39;Times New Roman\u0026#39;, \u0026#39;weight\u0026#39;: \u0026#39;normal\u0026#39;, \u0026#39;size\u0026#39;: 18, } # x轴label plt.xlabel(\u0026#34;epoch\u0026#34;, labelpad=2, fontsize=18,) # y轴label plt.ylabel(\u0026#34;loss\u0026#34;, labelpad=4, fontsize=18) # 设置标题 # plt.title(\u0026#34;change in loss\u0026#34;, x=0.5, y=-0.2, pad=0.3, fontsize=18) plt.title(\u0026#34;change in loss\u0026#34;, fontsize=18) fig=plt.gcf() fig.set_facecolor(\u0026#39;white\u0026#39;) # ------限制显示x,y轴最小-最大值范围（刻度不一定是多少） plt.xlim(0, 60) plt.ylim(0, 0.3) # ------设置x,y轴刻度 x_major_locator = MultipleLocator(10) y_major_locator = MultipleLocator(0.02) ax = plt.gca() # ax为两条坐标轴的实例 ax.xaxis.set_major_locator(x_major_locator) # ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) # 把x轴的主刻度设置为1的倍数 ax.yaxis.set_major_locator(y_major_locator) # 把y轴的主刻度设置为10的倍数 # ------显示网格 # plt.grid() plt.grid(True, linestyle=\u0026#34;--\u0026#34;, color=\u0026#39;gray\u0026#39;, linewidth=\u0026#39;1\u0026#39;, axis=\u0026#39;both\u0026#39;) plt.legend(handles=[A, B], prop=font1) plt.tick_params(labelsize=18) labels = ax.get_xticklabels() + ax.get_yticklabels() [label.set_fontname(\u0026#39;Time New Roman\u0026#39;) for label in labels] font2 = {\u0026#39;family\u0026#39;: \u0026#39;Time New Roman\u0026#39;, \u0026#39;weight\u0026#39;: \u0026#39;normal\u0026#39;, \u0026#39;size\u0026#39;: 18} plt.show() plt.savefig(\u0026#39;saved_figure.jpg\u0026#39;) 3D interactive plot (2024-03-23)\nMake 3D interactive Matplotlib plot in Jupyter Notebook - GfG\n3D Scatter plot:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # for creating a responsive plot %matplotlib widget # importing required libraries from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt # creating random dataset xs = [14, 24, 43, 47, 54, 66, 74, 89, 12, 44, 1, 2, 3, 4, 5, 9, 8, 7, 6, 5] ys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 3, 5, 2, 4, 1, 8, 7, 0, 5] zs = [9, 6, 3, 5, 2, 4, 1, 8, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0] # creating figure fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # creating the plot plot_geeks = ax.scatter(xs, ys, zs, color=\u0026#39;green\u0026#39;) # setting title and labels ax.set_title(\u0026#34;3D plot\u0026#34;) ax.set_xlabel(\u0026#39;x-axis\u0026#39;) ax.set_ylabel(\u0026#39;y-axis\u0026#39;) ax.set_zlabel(\u0026#39;z-axis\u0026#39;) # displaying the plot plt.show() 3D Bar plot:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %matplotlib widget from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt import numpy as np # creating random dataset xs = [2, 3, 4, 5, 1, 6, 2, 1, 7, 2] ys = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] zs = np.zeros(10) dx = np.ones(10) dy = np.ones(10) dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # creating figure figg = plt.figure() ax = figg.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # creating the plot plot_geeks = ax.bar3d(xs, ys, zs, dx, dy, dz, color=\u0026#39;green\u0026#39;) # setting title and labels ax.set_title(\u0026#34;3D bar plot\u0026#34;) ax.set_xlabel(\u0026#39;x-axis\u0026#39;) ax.set_ylabel(\u0026#39;y-axis\u0026#39;) ax.set_zlabel(\u0026#39;z-axis\u0026#39;) plt.show() Camera Poses (2024-03-23)\nGenerate by ChatGPT with prompt: \u0026ldquo;How to plot multiple camera poses in a single figure?\u0026rdquo;\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 %matplotlib widget import numpy as np import matplotlib.pyplot as plt def plot_camera_poses(poses, axis_length=0.1): # Create a new figure fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # Plot each camera pose for pose in poses: # Extract camera position and orientation cam_position = pose[\u0026#39;position\u0026#39;] cam_orientation = pose[\u0026#39;rotation\u0026#39;] # Plot camera position ax.scatter(cam_position[0], cam_position[1], cam_position[2], c=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;) # Plot camera axes axes_endpoints = cam_position + axis_length * cam_orientation ax.plot3D([cam_position[0], axes_endpoints[0,0]], [cam_position[1], axes_endpoints[0,1]], [cam_position[2], axes_endpoints[0,2]], \u0026#39;r\u0026#39;) ax.plot3D([cam_position[0], axes_endpoints[1,0]], [cam_position[1], axes_endpoints[1,1]], [cam_position[2], axes_endpoints[1,2]], \u0026#39;g\u0026#39;) ax.plot3D([cam_position[0], axes_endpoints[2,0]], [cam_position[1], axes_endpoints[2,1]], [cam_position[2], axes_endpoints[2,2]], \u0026#39;b\u0026#39;) # Set plot limits and labels ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) # Show plot plt.show() # Example camera poses (positions and rotation matrices) poses = [ { \u0026#39;position\u0026#39;: np.array([0, 0, 0]), \u0026#39;rotation\u0026#39;: np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) }, { \u0026#39;position\u0026#39;: np.array([1, 1, 1]), \u0026#39;rotation\u0026#39;: np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]]) } ] # Plot camera poses plot_camera_poses(poses) 3D arrow (2024-03-23)\nPutting arrowheads on vectors in a 3d plot - SO\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyArrowPatch from mpl_toolkits.mplot3d import proj3d class Arrow3D(FancyArrowPatch): def __init__(self, xs, ys, zs, *args, **kwargs): super().__init__((0,0), (0,0), *args, **kwargs) self._verts3d = xs, ys, zs def do_3d_projection(self, renderer=None): xs3d, ys3d, zs3d = self._verts3d xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M) self.set_positions((xs[0],ys[0]),(xs[1],ys[1])) return np.min(zs) arrow_prop_dict = dict(mutation_scale=10, arrowstyle=\u0026#39;-|\u0026gt;\u0026#39;, color=\u0026#39;k\u0026#39;, shrinkA=0, shrinkB=0) fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) a = Arrow3D([0, 1], [0, 0], [0, 0], **arrow_prop_dict) ax.add_artist(a) a = Arrow3D([0, 0], [0, 1], [0, 0], **arrow_prop_dict) ax.add_artist(a) a = Arrow3D([0, 0], [0, 0], [0, 1], **arrow_prop_dict) ax.add_artist(a) Convex polygon on 2D (2024-03-26)\nHow to fill an area within a polygon in Python using matplotlib? - SO\nExample from Scipy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Convex hull of a random set of points: from scipy.spatial import ConvexHull, convex_hull_plot_2d import numpy as np rng = np.random.default_rng() points = rng.random((30, 2)) # 30 random points in 2-D hull = ConvexHull(points) # Plot it: import matplotlib.pyplot as plt plt.plot(points[:,0], points[:,1], \u0026#39;o\u0026#39;) for simplex in hull.simplices: plt.plot(points[simplex, 0], points[simplex, 1], \u0026#39;k-\u0026#39;) plt.plot(points[hull.vertices,0], points[hull.vertices,1], \u0026#39;r--\u0026#39;, lw=2) plt.plot(points[hull.vertices[0],0], points[hull.vertices[0],1], \u0026#39;ro\u0026#39;) plt.show() Fill 3D Polygon (2024-03-26)\nax.fill is used for 2D, and doesn\u0026rsquo;t work for 3D. ax.add_collection3d cannot follow the counter-clockwise order of convex hull to fill the polygon. For example, when drawing a rectangle, it will fill 2 triangles:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) verts = np.array([[ [0,0,0], [1,0,0], [0,1,0], [1,1,0] ]]) # (1,4,3) rec = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, edgecolors=\u0026#39;k\u0026#39;) ax.add_collection3d(rec) The Poly3DCollection code is generated by ChatGPT with prompt: \u0026ldquo;How to fill a 3D triangle with matlabplot\u0026rdquo; So, I draw 2 triangles to form a quadrilateral:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) verts = np.array([[ [0,0,0], [1,0,0], [0,1,0],]]) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) verts = np.array([[ [1,0,0], [0,1,0], [1,1,0] ]]) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;,) ax.add_collection3d(tri) Plotting 3D Polygons - SO\nProject 3D point onto plane (2024-03-26)\nHow to project a point onto a plane in 3D? - SO\nThe distance from 3D point to the plane: dot product for 3D point and plane normal vector.\n3D point minus the distance, resulting in the projection.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 %matplotlib widget import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d.art3d import Poly3DCollection R = np.array([[ 0.97026268, 0.00747991, 0.24193878], # cam x-axis [-0.01474287, 0.99949291, 0.02822342], # cam y-axis [-0.24160499, -0.030951 , 0.96988095]]) #cam z-axis fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) # fill rectangle verts = np.array([[ [0,0,0], R[0]*2, R[1]*2, ]]) # (1,4,3) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, ) ax.add_collection3d(tri) verts = np.array([[ R[0]*2, R[1]*2, (R[0]+R[1])*2 ]]) # (1,4,3) tri = Poly3DCollection(verts, alpha=0.5, facecolors=\u0026#39;cyan\u0026#39;, ) ax.add_collection3d(tri) p = np.array([1,1,1]) ax.scatter(*p, c=\u0026#39;r\u0026#39;, marker=\u0026#39;o\u0026#39;) plane_normal = R[2] len2plane = R[2]@ p.T projection = p-len2plane * plane_normal ax.scatter(*projection, marker=\u0026#39;*\u0026#39;) ax.plot3D([p[0], projection[0]], [p[1], projection[1]], [p[2], projection[2]], \u0026#39;gray\u0026#39;) ","date":"2022-12-26T17:05:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_draw/","title":"memo: Python | Plotting"},{"content":"args \u0026amp; kwargs (2022-08-05)\n*args：表示接受任意个数的 Positional arguments，然后存放入一个元组中；传参时 位置 要对应，例如\n1 2 3 4 5 def fun (*args) print(args) fun(\u0026#39;fruit\u0026#39;, \u0026#39;animal\u0026#39;, \u0026#39;human\u0026#39;) \u0026gt;\u0026gt;\u0026gt; \u0026#39;fruit\u0026#39;，\u0026#39;animal\u0026#39;，\u0026#39;human\u0026#39; **kwargs：表示接受任意长的 Keywords arguments，然后存放入一个字典中；传参时要对应 key，例如\n1 2 3 4 5 def fun(**kwargs): for key, value in kwargs.items(): print(\u0026#34;%s:%s\u0026#34; % (key,value) fun(a=1,b=2,c=3)会输出 a=1 b=2 c=3 Positional args and keywords args\n1 2 3 4 5 6 def foo(a, b, **kwargs): var1 = a var2 = b args = kwargs var3 = args.[\u0026#39;key1\u0026#39;] 注解 annotation bilibili\n类型不匹配警告\n1 2 3 4 from typing import * def func(a: int, b: List[int]): # 第一个参数是int，第二个参数是list，其中每个参数是int ... 1 2 3 4 5 6 def func(a: List[List[int]], # 二维int数组 b: Dict, c: Set[int], d: Tuple[int] ): ... 1 2 3 4 5 6 7 8 9 class TreeNode: def __init__(self): self.val = None self.left = None self.right = None def dfs(node: TreeNode): # 类型为类名，或\u0026#34;TreeNode\u0026#34; node.val = 1 ... 1 2 3 4 5 def func(x: float) -\u0026gt; str: # 返回值类型 return str(x) def func2(func1: Callable[[float], str]): ... 1 a: int = 15 # 变量注解 1 print(func.__annotations__) # 查看函数的注解类型 Class Variable \u0026amp; Instance Variable 1 2 3 4 5 class MyClass(): class_var = 1 # Class Variable def __init__(self): self.var = 123 # Instance Variable All instances of the class have access to class_var, as well as the class itself. If class_var is mutable type, like list, class_var will influence on all instances. Reference: Python Class Attributes: An Overly Thorough Guide\n@classmethod classmethod can only be called through the class name instead through an object instantiated from the class, because it gets passed the cls as the argument.\nSimilarly, self means a method belongs to an instance, and can only be called via instance. classmethod can access the class variables.\nclassmethod usually is used for instantiating an object with a set of specific arguments rather through __init__().\n1 2 3 4 5 6 7 8 9 10 11 12 13 class PositionalEncoding(torch.nn.Module): def __init__(self, ..): # instance method ... @classmethod def from_conf(cls, conf, d_in=3): # conf: \u0026#39;default.conf\u0026#39;.model.code{} # PyHocon construction return cls( conf.get_int(\u0026#34;num_freqs\u0026#34;, 6), d_in, conf.get_float(\u0026#34;freq_factor\u0026#34;, np.pi), conf.get_bool(\u0026#34;include_input\u0026#34;, True), ) Code snippest from pixel-nerf\nRef: An essential guide to Python class methods and when to use them\n@staticmethod @staticmethod in Python - AskPython\nNo need to pass an instance (self) or class (cls) to it as the first argument. So it can be called both by the class name and an instance. Can be overridden by children class. __name__ 当一个 py 文件被直接执行时，__name__ 被设置为 '__main__'，如果是被 impot，则它的 __name__ 被设置为它的文件名(不包括后缀)，并且被导入时会执行顶层的代码（不包括 function, class）\nfreeCodeCamp-Goran Aviani\n__file__ (2024-02-28) Ref: GfG\nThe file path of the function\u0026rsquo;s definition\n1 2 3 # Hello.py def HelloWorld(): print(\u0026#34;This is Hello.py\u0026#34;) Call that module in another file:\n1 2 3 4 5 6 7 8 9 10 11 # GFK.py import Hello import os Hello.HelloWorld() print(Hello.__file__) print(f\u0026#39;GFK.py\\\u0026#39;s __file__: {__file__}\u0026#39;) print(os.path.join(os.path.dirname(os.path.abspath(__file__)), \u0026#34;third_party/glm/\u0026#34;)) Output of python GFK.py\n1 2 3 4 This is Hello.py /home/yi/Downloads/ipynb_test/test_file_/Hello.py GFK.py\u0026#39;s __file__: GFK.py /home/yi/Downloads/ipynb_test/test_file_/third_party/glm/ Built-in functions Python Docs\ndir(object) returns a list of the names of attributes and methods of any kind of object, e.g., module, class, instance. No values are stored.\nvars(object) returns a dict: object.__dict__ containing changeable attributes.\nvars() acts like locals(): a dict of local variables for read. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class person: name = \u0026#34;jack\u0026#34; dir(person) # A list, len = 27 person.__dir__ # A method vars(person) # same as person.__dict__, len=5 # instance student = person() setattr(student, \u0026#39;age\u0026#39;, 14) vars(student) # out: {\u0026#39;age\u0026#39;: 14} dir(student) # a list of attr and methods of the object set(student.__dir__()) - set(dir(person)) # out: {\u0026#39;age\u0026#39;} set(student.__dir__()) == set(dir(student)) # True set(dir(student)) == set(person.__dir__(student)) # True set(dir(student)) - set(person.__dir__(person)) # {\u0026#39;__weakref__\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;name\u0026#39;} set(person.__dir__(person)) - set(dir(student)) # {\u0026#39;__abstractmethods__\u0026#39;, \u0026#39;__base__\u0026#39;, \u0026#39;__bases__\u0026#39;, ... [::-1] flip image 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import matplotlib.pyplot as plt from PIL import Image # pillow import cv2 Image.open(\u0026#39;1.jpg\u0026#39;) # RGB cv2.imread(\u0026#39;1.jpg\u0026#39;) # nd array, BGR plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)) plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[:,:, ::-1]) # 最后的通道取反 plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[::-1,:, ::-1]) # 上下翻转 plt.imshow(cv2.imread(\u0026#39;1.jpg\u0026#39;)[::-1, ::-1, ::-1]) # 左右也翻转 表达式内赋值 PEP 572, Python 3.8+， 类似 n := xxx\n比如，有一个字符串数组，想去掉每个字符串里的空格和空字符串\n1 2 3 arr = [\u0026#39;a\u0026#39;, \u0026#39;bb \u0026#39;, \u0026#39; \u0026#39;] arr = [x.strip() for x in arr] # 删除字符串前面和后面的空格 arr = [x for x in arr if x] # 空字符串会当作false 两行写一行: arr = [ y for x in arr if (y := x.strip)) ]\n团队开发最好还是用易懂的写法。 1_00_00什么意思？x:=y？Python里面的几个有趣特性\nAssignment expression - RealPython Index value from dict Get value by giving index from a dictionary\nAccessing dictionary value by index in python\nPython Dictionaries are Ordered from python 3.7 6, so the index can be used like:\nvalue_at_index = list(dic.values())[index]\nPython Dictionaries are Ordered now, but how?…and why? Make one-hot matrix Convert a vector to one-hot matrix\nConvert array of indices to one-hot encoded array in NumPy - StackOvf\n1 2 3 values = [1, 0, 3] n_values = np.max(values) + 1 np.eye(n_values)[values] if/else in a list comprehension SO\nFlatten a list of list Python - Flatten a list of lists to a single list (Dead link now)\nConcatenate list of tensor: torch.stack() first, then reshape Redirect print content (2023-08-08)\nRedirect the standard output (print stream) to a file using the sys module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sys # Save the current stdout stream to a variable original_stdout = sys.stdout # Write to file with open(\u0026#39;output.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: sys.stdout = f print(\u0026#34;The content given to sys.stdout \\ will be redirected to file, \\ instead of being printed out\u0026#34;) # Restore the original stdout stream sys.stdout = original_stdout argparse (2023-08-28)\naction=\u0026quot;store_true\u0026quot;, this option is True when this argument appears in the command\nnargs=+, this argument at least be assigned with 1 value. Similarly, nargs=3, the argument must be assigned with 3 values.\nArgparse: how to handle variable number of arguments (nargs=\u0026rsquo;*\u0026rsquo;) - SO\nExample:\n1 parser.add_argument(\u0026#39;--img_wh\u0026#39;, nargs=\u0026#34;+\u0026#34;, type=int, default=[1152, 864]) The command line: python eval.py --img_wh 640 512. The args in vscode \u0026ldquo;launch.json\u0026rdquo;: \u0026quot;args\u0026quot;: [\u0026quot;--img_wh\u0026quot;, \u0026quot;1152\u0026quot;, \u0026quot;864\u0026quot;,]\nUpdate Dict attributes (packge: mmcv)\nmmcv Docs\nmmsegment配置参数说明（五）- CSDN\n1 2 3 4 5 6 7 from mmcv import DictAction parser = argparse.ArgumentParser(description=\u0026#39;Train a recognizer\u0026#39;) parser.add_argument(\u0026#39;--cfg-options\u0026#39;, nargs=\u0026#39;+\u0026#39;, action=DictAction, default={}, help=\u0026#39;override some settings in the used config, the key-value pair \u0026#39; \u0026#39;in xxx=yyy format will be merged into config file. For example, \u0026#39; \u0026#34;\u0026#39;--cfg-options model.backbone.depth=18 model.backbone.with_cp=True\u0026#39;\u0026#34;) Example in launch.json of adapt-image-models:\n1 2 3 4 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34;, ] Set choices:\n1 2 3 4 5 parser.add_argument( \u0026#39;--launcher\u0026#39;, choices=[\u0026#39;none\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;slurm\u0026#39;, \u0026#39;mpi\u0026#39;], default=\u0026#39;none\u0026#39;, help=\u0026#39;job launcher\u0026#39;) Accept a dict (2023-08-31)\nAccepting a dictionary as an argument with argparse and python (duplicate) -SO\nargparse don\u0026rsquo;t have a type of dict, there\u0026rsquo;re 2 workarounds:\nSet type=str, then use json.loads() to parse:\nSet type=json.loads:\n1 2 3 4 5 import json parser.add_argument(\u0026#39;-d\u0026#39;, \u0026#39;--my-dict\u0026#39;, type=json.loads) args = parse.parse_args() mydict = args.my_dict # Will return a dictionary The DictAction of mmcv will only parse the top-level as a dict (key-value), but if the value passed is a dict too, it\u0026rsquo;ll be parsed as a string.\n1 2 3 4 5 6 7 8 9 10 \u0026#34;args\u0026#34;: [ \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.num_frames=3\u0026#34;, // \u0026#34;train_pipeline[1].clip_len=3\u0026#34;, // Cannot identify [1] // \u0026#34;dict(train_pipeline={\u0026#39;1\u0026#39;: dict(clip_len=3)})\u0026#34;, // Error: has to 2 params, but got 1. // \u0026#34;train_pipeline= {\\\u0026#34;1\\\u0026#34;: {\\\u0026#34;clip_len\\\u0026#34;: \\\u0026#34;3\\\u0026#34;}}\u0026#34; // The value will be parsed as a string, which will override the whole previous dict. ] Access a dict myDict.update({'aKey': \u0026quot;newVal\u0026quot;}) or myDict.update(aKey='newVal')\nmyDict.get('aKey', retVal)\nmyDict.setdefault('aKey', 'defVal'): if 'aKey' doesn\u0026rsquo;t exist, it will be inserted with the default value defVal.\nmyDict.copy() or newDict = dir(myDict) are shallow copy: when change the no-primitive data (such as list, nested data), the orignal dict will also change, because it only duplicate the top-level data, while the low-level data is referenced to the original data. Copy a Python Dictionary: A Complete Guide • datagy\nmyDict.pop('aKey') will return and remove an item.\nDelete items from dictionary while iterating Have to iterate the dict twice, the 1st round collects the keys to be deleted, and the 2nd round del those items.\nlogging Real Python\n1 2 3 4 5 6 7 import logging logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logger=logging.getLogger(__name__) logger.setLevel(logging.INFO) logger.info(\u0026#34;This is an info message\u0026#34;) Output: INFO:__main__:This is an info message\nlogging.info doesn\u0026rsquo;t show up on console tqdm (2023-10-20)\nDDG with searching \u0026ldquo;avoid writting tqdm bar into the log file\u0026rdquo;\nUse tqdm to iterate the outer loop and print in the inner loop. How can I make the tqdm progress bar be printed less frequently in the log file?\n1 2 3 for outer in tqdm(range(0, 1e5, 1e4)): for inner in range(1e4): print(outer+inner) Output tqdm to a null device, and print/log the status bar after desired steps. python - Making tqdm write to log files - Stack Overflow\n1 2 3 4 tqdm_bar = tqdm(range(20), file=open(os.devnull, \u0026#39;w\u0026#39;)) for i in tqdm_bar: if tqdm_bar.n % 5 ==0: print(str(tqdm_bar)) Redirecting console logging to tqdm.write(), leaving other logging handlers (e.g. log files) unaffected. tqdm.contrib.logging - Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 import logging from tqdm import trange from tqdm.contrib.logging import logging_redirect_tqdm LOG = logging.getLogger(__name__) if __name__ == \u0026#39;__main__\u0026#39;: logging.basicConfig(level=logging.INFO) with logging_redirect_tqdm(): for i in trange(9): if i == 4: LOG.info(\u0026#34;console logging redirected to `tqdm.write()`\u0026#34;) # logging restored Discussion: Pass progress bar to logger · Issue #313 · tqdm/tqdm tqdm output to stderr, which is not in sync with stdout. So only redict the stdout (print) to file: python test.py \u0026gt; output.log\nHow to \u0026ldquo;flush\u0026rdquo; tqdm progress bar explicitly?\nFound in Google\nMonitor log file in real-time: tail -f output.log. reddit\nDisable tqdm bar\nPresetting argument. Inspired by the comment of this answer of Why is tqdm printing to a newline instead of updating the same line?; Searched by DDG with searching \u0026ldquo;tqdm leave\u0026rdquo;\n1 2 3 4 5 from functools import partial from tqdm import tqdm tqdm = partial(tqdm, disable=True) for i in tqdm(iterable): Disable tqdm bar by environment variable after version 4.66.0:\n1 2 # pip install tqdm --upgrade eport TQDM_DISABLE=1 Silence tqdm\u0026rsquo;s output while running tests or running the code via cron; Searched by DDG with searching \u0026ldquo;tqdm disable\u0026rdquo;\nEnv variable overrides supported by tqdm==4.66.0\nleave=False means the bar will disappear instead of leaving it left after finishing. How to remove progressbar in tqdm once the iteration is complete\nUse Progress bar as status bar showing current entry, by removing all field but the description field and specifying position.\n1 2 3 4 5 6 7 8 outer = tqdm.tqdm(total=len(files), desc=\u0026#34;Files\u0026#34;, position=0) cur_file = tqdm.tqdm(total=0, position=1, bar_format=\u0026#34;{desc}\u0026#34;) for name in files: video = imageio.mimread(name, memtest=False) cur_file.set_description_str(f\u0026#34;Current file: {name}\u0026#34;) outer.update(1) Progress bar and status logging in python with tqdm\nRegistration mechanism A registry is a centralized \u0026ldquo;object\u0026rdquo; for convenience of interacting between differen components.\n1 2 3 4 5 from ..builder import BACKBONES # registry @BACKBONES.register_module() # add a component class ViT_CLIP(nn.Module): # backbone model def __init__(): isinstance() Check if an object is one of types\ndataset is expected to be a list or a tuple:\n1 dataset = dataset if isinstance(dataset, (list, tuple)) else [dataset] Convert str to classname Use eval\n1 eval(\u0026#34;cls_name\u0026#34;) Convert string to Python class object?\nUse getattr\n1 2 optim_type = \u0026#39;AdamW\u0026#39; self.optim = getattr(torch.optim, optim_type)(optim_params, **optim_kwargs) Code from MatchNeRF.\nEven and odd compatible 1 eval_batch_size = (self.eval_batch_size - 1) // sb + 1 Code from pixelNeRF\u0026rsquo;s \u0026ldquo;nerf.py\u0026rdquo;.\nSave argparse to file (2023-09-25)\nSaving python argparse file - SO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import json import time from pathlib import Path timestamp = time.strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;, time.localtime()) log_file = Path(cfg.work_dir) / f\u0026#39;{timestamp}.log\u0026#39; with open(f\u0026#34;log_file\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump({\u0026#34;CommandLine_Args\u0026#34;: args.__dict__, \u0026#34;ConfigFile_Settings\u0026#34;: conf }, f, indent=2 ) # --- Read --- parser = ArgumentParser() args = parser.parse_args() with open(\u0026#39;commandline_args.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: args.__dict__ = json.load(f) json has better readabiity, interoperability, security, but slower than pickle. Pickle or json?\njson.dumps convert any python object to JSON formatted string, while json.dump() write a serialized object to file. PYnative\nWrite JSON File (2024-03-21)\njson.dump() convert a python object to a json string. GfG\n1 2 3 4 json_data = {\u0026#34;camera_angle_x\u0026#34;: fovx, \u0026#34;frames\u0026#34;: frams_ls} with open(f\u0026#34;{dir_json}/transforms_train.json\u0026#34;, \u0026#39;w\u0026#39;) as file: file.write(json.dumps(json_data, indent=4)) Write a json string into a file using pathlib to avoid dangling with open(): SO\n1 2 3 json_data = {\u0026#34;camera_angle_x\u0026#34;: fovx, \u0026#34;frames\u0026#34;: frams_ls} pathlib.Path(f\u0026#34;{dir_json}/transforms_train.json\u0026#34;).write_text(json.dumps(json_data, indent=4)) But, in this way, I don\u0026rsquo;t know how to realize 'w' that always creats a new file. Working With JSON Data in Python - Real Python Move a dict to GPU (2023-09-27)\nGiven a dict containing list, tuple, and torch.Tensor,\nRecursive\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def move_to_device(X, device): if isinstance(X, dict): for k, v in X.items(): X[k] = move_to_device(v, device) elif isinstance(X, list): for i, e in enumerate(X): X[i] = move_to_device(e, device) elif isinstance(X, tuple) and hasattr(X, \u0026#34;_fields\u0026#34;): # collections.namedtuple dd = X._asdict() dd = move_to_device(dd, device) return type(X)(**dd) elif isinstance(X, torch.Tensor): return X.to(device=device) return X Code from MatchNeRF.\nMake pairs from 2 list Two for in one line:\n1 index_lists = [(a, b) for a in range(n_views - 1) for b in range(a + 1, n_views)] # [(0,1), (0,2), (1,2)] Code from MatchNeRF.\nSlice() for a dim (2023-09-29)\n1 2 3 a = torch.arange(5) x = slice(0, -1, 1) # specify start index, end index, step size a[x] # [0,1,2,3] Convert class to dict (2023-10-05)\nIn python, how do I cast a class object to a dict - SO\ncollections.namedtuple vs typing.NamedTuple- RealPython\ntyping.NamedTuple - Docs\nWhat are \u0026ldquo;named tuples\u0026rdquo; in Python? - SO\n1 2 3 4 5 6 7 8 9 10 11 from typing import NamedTuple class myNT(NamedTuple): r\u0026#34;\u0026#34;\u0026#34;a doc string\u0026#34;\u0026#34;\u0026#34; foo: int bar: str baz: list aTupleObj = myNT(1, \u0026#39;bar\u0026#39;, []) aTupleObj._asdict() NamedTuple (2024-04-09)\nNamedTuple is not mutable, so the following error will occur when trying to re-assign its atttibutes: AttributeError: can\u0026rsquo;t set attribute in python - SO\n1 2 3 4 5 6 7 from typing import NamedTuple class BasicPointCloud(NamedTuple): points : np.array colors : np.array normals : np.array pcd.points = new_points Format digits Fix length filling with zeros: How do I format a number with a variable number of digits in Python? - SO\n'123'.zfill(7): 0000123\nf-string round a float: Fixed digits after decimal with f-strings\n1 2 a = 10.1234 f\u0026#39;{a:.2f}\u0026#39; (2024-01-22)\nFiles Open multiple files: Source code\n1 2 with open(scene_info.ply_path, \u0026#39;rb\u0026#39;) as src_file, open(os.path.join(self.model_path, \u0026#34;input.ply\u0026#34;) , \u0026#39;wb\u0026#39;) as dest_file: dest_file.write(src_file.read()) Pass by Assignment Python is Pass by Reference - RealPython\n(2024-02-08)\nIn Python, passing arguments to a function is assigning the incoming objects to new variable name (identifier). This can be verified through \u0026ldquo;reference counter\u0026rdquo; and namespace, which is a dictionary, including key-value pairs representing a binding between target variable name and object.\n1 x = 2 # object 2\u0026#39;s refcount+1, namespace add: \u0026#39;x\u0026#39;:\u0026#39;2\u0026#39; When an object (value) is assigned to a variable name (identifier), the reference counter of the object is incremented. If the variable name is reassigned to another value, the pre-bounded object\u0026rsquo;s reference counter is decremented.\nEach function has its own namespace, which can be checked by print(locals())\nIf an attribute of the object supports modification, and the object is passed to a function argument, assigning values to the object\u0026rsquo;s attribute is in-place modification. Ref\nMisc:\nUse id() to get the address of a variable\nWhen Python modifies a variable through a function, the variable is not modified in place, but reassigned with the returned value.\nPassing by reference in Python can be replicated with: object\u0026rsquo;s attributes, dictionary (mapping type), and list-like data (subscriptable and mutable)\nExample refers to 3DGS:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class GaussianModel: def __init__(self,): self._xyz = torch.empty(0) class Scene: def __init__(self, gaussian: GaussianModel, exTen, color, ls): self.gaussian = gaussian # a binding created in locals() # The attribute _xyz supports assignment as tensor is mutable # object\u0026#39;s attribute is modified in place: self.gaussian._xyz = torch.ones(2,3) self.myTen = exTen # exTen\u0026#39;s refcount+1, self.myTen = torch.ones(1,4) # update to new binding self.color = color # a binding between self.color and color self.color.view(2,2) # new binding self.ls = ls self.ls[0] = 88 # affect the original list gaussian = GaussianModel() exTensor = torch.empty(0) color=torch.arange(4) myObj = Scene(gaussian, exTensor, color,ls:=[1,2]) print(gaussian._xyz) # changed to ones print(exTensor) # remain empty print(color) # unchanged print(ls) # changed to [88,2] Mutalbe - RealPython\nperplexity\nre Find numbers (2024-03-17)\nHow to extract numbers from a string in Python? - SO\n1 2 3 4 5 6 import re regx_pattern = r\u0026#39;\\d+\\.\\d+|\\d+\u0026#39; txt = \u0026#34;The price is $12.99 and the quantity is 5 21321 dasdsa 123213.\u0026#34; matches = re.findall(regx_pattern, txt) print(matches) Output: ['12.99', '5', '21321', '123213']\nimport (2024-04-09)\nImport file in 2 levels up.\nValueError: attempted relative import beyond top-level package - SO\n1 2 3 4 5 6 7 8 9 10 Project _3DGS scene __init__.py utils # for _3DGS # there is no __init__.py (Don\u0026#39;t know if it matters) utils # for current project __init__.py # empty is ok file2.py I want to use a method (write_j) implemented in file2.py inside scene/__init__.py. Although, the above SO post stated that all folder and subfolders should have an __init__.py, I tried the following syntax works:\n1 from utils.fuse_neighbors import fuse_neighbors Previously, the 3 dots: from ...utils.fuse_neighbors import fuse_neighbors, will lead to an error:\n1 ValueError: attempted relative import beyond top-level package ","date":"2022-12-25T23:37:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_misc/","title":"memo: Python | Misc"},{"content":"总结之前讲过的各种模型，引出之后要讲的生成模型\n1. 定义 P1 - 【机器学习】白板推导系列(三十) ～ 生成模型综述(Generative Model Introduction)\n生成模型除了生成数据还能做什么任务？\nGAN 用来生成数据。 GMM 用来做聚类任务的。 这2个都是无监督学习（无标签）。 逻辑回归虽然与概率相关，但它不是生成模型。P(Y=1 | X) = ?, P(Y=0 | X) = ? 只是对条件概率建模，而不关注样本数据X本身的分布，\n所以（概率）生成模型关注的是样本的概率分布本身，而与它要解决的任务没有必然的联系。 既可以解决监督学习的任务（对 P(X,Y) 建模），也可以解决无监督学习的任务，对于隐变量模型，可以构造 P(X,Z) 并建模；或者不用隐变量，比如自回归模型，直接对 P(X) 建模（把P(X) 拆成各个维度之积）。\n所以关注样本的概率分布的模型就是生成模型\n2. 模型分类 P2\n按照解决的任务：监督 vs. 非监督 （自监督未介绍） 对各种模型分类\n任务：分类，回归，标记，降维，聚类，特征学习，密度估计，生成数据\n把机器学习模型分为“概率”与“非概率”一般没太大必要，但是生成模型一定是与概率相关的，所以这里以概率为分类标准\n监督学习任务\n概率模型（建模与概率相关）\n判别模型\n对”条件概率“的分布建模：逻辑回归(LR)，最大熵(MEMM)，条件随机场(CRF)，（输出为概率分布的参数的）神经网络NN\n生成模型\n简单的神经网络只能是判别模型，不是生成模型。但是NN里分步式表示可以和概率图模型结合，就变成了（深度）生成模型 具体见下文 非概率模型（建模时未考虑概率分布），若要解决分类问题，则大概率是判别模型\n感知机PLA，（硬间隔）支持向量机SVM，KNN，（输出为类别的）神经网络NN，树模型 非监督学习任务\n概率模型：必然是生成模型，因为非监督里没有标签Y无法判别，只能描述样本X的概率分布。概率图模型中的大部分是生成模型\n非概率模型：PCA降维（SVD分解），潜语义分析LSA (pLSA, LDA)，K-means，（不带标签的NN）自编码器Auto-Encoder\nPCA 从概率角度看，它就是P-PCA的一种，就是因子分析Factor Analysis LSA 的概率模式是 pLSA, 再改造就是 LDA K-means 从概率角度看，是特殊的GMM Auto-Encoder 的概率模式就是 VAE 各种生成模型 生成模型分成监督，非监督不重要\nNaive Bayes\n朴素贝叶斯是最简单的生成模型，直接描述 x，假设简单：因为它是判别模型，所以它假设是在给定 y 的情况下，样本 x∈ℝᵖ 可以表示成各维度之积\nP(x|y) = ∏ᵢ₌₁ᵖ p(xᵢ|y)\nMixture model\nGMM 认为 x 由 z 生成，在给定 z 的情况下，x服从高斯分布。\nTime-series model\n从时间序列角度，从有限到无限，比如 HMM，卡尔曼滤波，粒子滤波\nNon-parametric\n在参数空间上，从有限到无限：高斯过程 Gaussian process/ Dirichlet process，是非参贝叶斯模型， 它的参数不再是固定的，未知的常数了。高斯分布是参数模型：通过learning把（mu,sigma）学习出来，\nMixed Membership Model\n也是混合模型，比GMM那中复杂些，变量多。LDA 隐含狄利克雷分布，用来做文档聚类\nFactorial Modeel\n因子模型：因子分析FA，概率PCA P-PCA, ICA, 稀疏编码sparse coding\n以上 6 种都是结构化的概率图模型，这些模型每一类都有固定的套路，思想比较底层，专家设计的处理特定问题的算法。与下面的“深度”生成模型对应，它们则是“浅层”生成模型。\n以下是与深度学习神经网络相结合的模型:\nEnergy-based Model\nBoltzmann Machine玻尔兹曼机，包括 sigmoid network，deep belif network， 都是无向图模型\nVAE\n自编码器与概率图结合，用变分的手段处理\nGAN\nAuto regressive model\n自回归网络\nFlow-based model\n3. 模型表示\u0026amp;推断\u0026amp;学习 P3\n从模型表示，推断，和学习的角度去认识一个生成模型\n模型表示 “形神”兼备\n形\n节点可以是离散的，有可以是连续的； 边可以是有向的，也可以是无向的。如果所有的边是有向的，则为有向图模型 以上写出来的11 种除了 玻尔兹曼机，其他都是有向图模型 含有隐变量节点就是隐变量模型，若不使用隐变量就是“fully observed model” 概率图的结构：从层次来看，shallow （前6种） 或者 deep（后5种）； 或者从连接数量来看，就对应 sparse 和 dense 两类， 比如玻尔兹曼机的层间的连接没有缺失,它是稠密的, 而HMM一个隐变量只有2-3条边 神（概率分布本身）\n对于样本的概率密度函数来讲，它既可以是参数化模型，即它的参数是固定的，未知的常量，也可以是非参数化模型，即非参贝叶斯。\n参数角度 parametric vs. Non-parametric models 显性与隐性密度函数 Implicit Density vs. Explicit Density。显性是直接对P(X) 建模，若是隐变量模型，就对P(X,Z)建模，若是fully observed model，就直接对P(X) 分解。 隐性不直接对 P(X) 建模，因为它的任务不是先估计出概率密度函数，再从中生成样本。只需要确保样本是从 P(X) 中生成的即可。上述的只有 GAN 是Implicit 的，其余10种都是显式的 推断 推断是否 tractable， (intractable)\n学习 Likelihood-based model （极大似然估计) vs. Likelihood-free model (GAN 不关心 P(X)，也就不管样本的似然是多少，它有自己的判别器和目标函数）\n4. 模型分类 P4\n主要关注：无监督的，有向图的，深层结构的，参数化的，模型\nLikelihood-based model\n概率密度函数是显式的 推断 Tractable\nFully-observed model 它的概率/似然可直接求出，比如自回归 Change of variable model 变量替换，比如 Flow-based 模型不直接求解复杂的P(X)， 而把x与一个服从简单分布的变量z，用一个连续可逆的复杂函数联系起来，以引入非线性转换，x=g(z)，然后去学习 g(z) ， 因为 z=g⁻¹(x)，所以 Pₓ(X) = Pz(g⁻¹(x) | ∂g⁻¹(x)/∂x) 推断 Intractable 就用近似推断Approximate inference\n基于变分推断，比如VAE 基于马尔科夫链，比如 Energy-based model Likelihood-free model\n概率密度函数是隐式的，不直接关心P(X)， GAN 直接用generator 生成样本，然后用 判别器 去评价样本好坏。它是直接生成样本，而不是先估计PDF，再从PDF中采样生成样本。 不直接生成样本，可以用MC 采样，比如“生成随机网络” GSN，有点类似“去噪自编码器” 5. 概率图 vs. 神经网络 P5\n他俩不是非此即彼的关系，不是互斥的，是独立的，两者都有发生的可能性\n神经网络里有“计算图”\n概率图是概率分布的表示，而(前馈)神经网络是函数逼近器，只不过函数可能比较复杂，如果不给神经网络加修正，它与概率没关系。\nx \u0026ndash;\u0026gt; NN \u0026ndash;\u0026gt; y，y可以是离散的类别，也可以是连续的数值\nNN 的作用只有一个：逼近函数。输入样本，得到目标函数的值，用此值对 NN 中的参数（权重、偏置）求梯度，然后做梯度下降\n概率图模型可笼统的分为：有向图模型（贝叶斯网络），无向图模型（比如Boltzmann machine)\n玻尔兹曼机既属于无向图模型，也属于神经网络，代表“广义连结主义”。\n神经网络也可分为：确定性神经网络（CNN，RNN），随机性神经网络（比如 Boltzmann machine，sigmoid belief network)\n所以在讨论概率图与神经网络的区别时，不考虑 Boltzmann machine， 只比较有向图模型（贝叶斯网络）与确定性神经网络\n从“表示”，“推断”，“学习” 3个角度对比:\n模型表示\n贝叶斯网络：结构化的，浅层的，稀疏的（高维问题有各种条件独立性假设），节点有概率意义，具有可解释性，每个节点在建模时被赋予意义，比如LDA和HMM\n神经网络：深层的，稠密的（无条件独立性假设），节点仅用于计算σ(∑wᵢxᵢ)，没有任何概率意义/物理意义。它的解释性未知也不重要，神经网络每层的意义在建模时并未赋予。\n推断\n贝叶斯网络：精确推断，近似推断，MC采样推断，变分推断，估计后验分布 神经网络：它的推断很容易（前向）但没有意义，参数的分布不重要，只关注输出 学习\n贝叶斯网络：Likelihood-based: EM 神经网络：梯度下降（反向传播：一种高效的求导方法，就是链式求导法则+动态规划（递归+Cache）） 用法\n概率图描述了模型，适合高级别的推理任务 high-level reasoning 神经网络（的计算图）只用来计算，适合低级别简单的推理 low-level reasoning：弱推理，只是分类图像，而没有像人类一样理解；还适合表示学习：声音、图像识别（现在的语言模型是两个的综合） 6. 重参数化技巧（随机后向传播） P6\n最基础的神经网络就是一个函数逼近器。用样本 (X,Y) 去逼近函数 y=f(x;θ)。基于 y 构建目标函数，用BP求神经网络的权重和偏置的梯度，通过随机梯度下降修正\n用神经网络逼近概率分布（概率图），结合到一起就叫随机后向传播 Stochastic Backpropagation 或者叫重参数化技巧 Reparameterization Trick\n假设 y 是一个概率分布，它的概率密度函数是 P(y)。\n假定 P(y) = N(μ,σ²)，当对它采样时，先对中间变量 z 采样，再由 z 得到 y，其中 z 服从标准正态分布 z～N(0,1)，那么 y 与 z 的关系就是 y = μ+σ⋅z。\n因为对标准正态分布是很容易采样的，先采一个 z⁽ᶦ⁾～N(0,1)，那么 y⁽ᶦ⁾=μ+σ⋅z⁽ᶦ⁾\n给定 z，则 y 也固定，把 μ,σ 看作是未知但确定的参数，所以 y 与 z 之间就是一个线性变换。\n可以将 y 看作一个函数 y = f(μ,σ,z)，其中 z 是随机变量，除它之外都是确定性变换，可以用神经网络去逼近这个线性函数\nz \u0026ndash;\u0026gt; NN \u0026ndash;\u0026gt; y，NN 逼近 f；因为 z 是个随机变量，所以 y 也是个随机变量\n以上假设了 P(y) 是正态分布，所以 y 与 z 之间是线性关系，所以神经网络的参数是 μ, σ，令 θ={μ,σ²}。\n可以构造关于 y 的目标函数: J(y) ，因为 y 是关于μ, σ 的函数，所以求梯度∇_θ J(y)时：\n∂J(y)/∂θ = ∂J(y) / ∂y ⋅ ∂y/∂θ\n如果 目标变量 是个条件概率分布: P(y|x) = N(x; μ,σ²)，然后 z 还是服从一个标准正态分布 z～N(0,1)，那么 y 与 z 的关系是：y = μ(x) + σ(x)⋅z 其中 x 是输入，所以 μ,σ 都是 x 的函数。\n仍然可用神经网络去逼近 y 与 z 之间的函数，神经网络的参数是θ：\n1 2 3 x ——\u0026gt; NN ——\u0026gt; y ▲ z ————┘ 更变量之间的关系画得更详细： μ, σ 都从神经网络中出来，它们就是 θ 的函数，则 y = μ_θ(x) + σ_θ(x)⋅z\n1 2 3 4 x ─\u0026gt; NN-θ ──\u0026gt; μ_θ(x) ──── + ──\u0026gt; y └───\u0026gt; σ_θ(x) ─┐ ▲ ▼ │ z ──\u0026gt; × ──┘ 也可以用两个NN 分别逼近 μ, σ : μ(x) = f(x;θ) σ(x) = g(x;θ)\n然后构造关于 y 的目标函数 J_θ(y) = ∑ᵢ₌₁ᴺ ||y-yⁱ||²，然后对 θ 求梯度：\n∂J(y)/∂θ = ∂J(y) / ∂y ⋅ ∂y/∂μ ⋅ ∂μ/∂θ + ∂J(y) / ∂y ⋅ ∂y/∂σ ⋅ ∂σ/∂θ\n所以无论想求一个普通的概率分布，还是要求一个条件概率分布，都可以用神经网络逼近那个概率分布\n在本节的例子中，都是假设 P(y) 是高斯分布，即它是连续的，可微的，而且要求 y 本身是一个连续的随机变量，因为需要 y 对 μ,σ 求偏导。 如果 y 是离散随机变量，就不能用这个方法。\n最后，可以把上面两种情况，合并起来描述： P(y|w)，如果只求 P(y)，则w 就是参数 θ；如果要求 P(y|x)，那 w 就代表 x 和 θ，w={x;θ}，x 无所谓，它是条件概率中的条件，是输入。\n神经网络的参数 w，用神经网络去逼近概率分布 P(y|w)\n","date":"2022-12-25T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/30-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/","title":"watch: ML - 白板 30 | Review of Generative models"},{"content":"1 背景 P1\n频率角度： 解一个优化问题，比如：\n线性回归模型，数据拟合，用数据估计直线的W：f(W) = WᵀX: 策略(loss func): 最小化所有样本点的误差之和(最小二乘估计，无约束的优化问题): L(w) = Σᵢᴺ(wᵀxᵢ-yᵢ)²，Dataset: {(xᵢ,yᵢ)}ᵢᴺ，xᵢ∈ ℝᵖ，yᵢ∈ ℝ。 最优 w^ = argmin L(w)。\n解法：\n解析解：损失函数对 w 求导=0，则W^ = (XᵀX)⁻¹XᵀY, where X is train set 数值解：（随机）梯度下降， SVM 模型，分类问题，估计符号函数：f(w) = sign(wᵀx+b)；\n策略(loss func): 类间大，类内小（有约束的凸优化问题）： L(w) = min 1/2 wᵀw, s.t. yᵢ(wᵀxᵢ+b)≥1, i=1,\u0026hellip;,N\n解法：\n调用 QP 套件 拉格朗日对偶 EM 模型，迭代优化模型参数θ，使似然值取得最大：θ^ = argmax log P(X|θ)\n策略：迭代公式： $θ⁽ᵗ⁺¹⁾ = argmax_θ ∫_Z log P(X,Z|θ) P(Z|X,θ⁽ᵗ⁾) dZ$ （输入数据和隐变量的完全数据分布按照Z的后验分布求期望（加权和，求积分）） 贝叶斯角度： 解一个积分问题\n贝叶斯定理：P(θ|X) = P(X|θ) P(θ) / P(X)，后验分布 = 似然值⋅先验分布/参数空间的积分∫P(X|θ)P(θ)dθ\n贝叶斯推断Inference：依据贝叶斯定理求后验分布P(θ|X)。（然后可求分布的期望，方差）\n贝叶斯决策Decision：借助后验分布P(θ|X)，用已有样本 X 预测新样本 x^ 发生的概率： $P(\\^x|X) = ∫ P(\\^x,θ|X) dθ = ∫_θ P(\\^x|θ,X) P(θ|X) dθ = E_{θ|X} [P(\\^x|θ)]$，即对似然P(x^|θ)按照θ的后验分布求期望\n如何求后验分布P(θ|X)：\n精确推断，问题简单（参数空间/隐变量的维度不高）可以直接计算出分母的积分\n近似推断，参数空间维度高，无法直接求出分母积分\n确定性近似\n变分推断 随机近似\nMCMC 马尔科夫链蒙特卡洛方法 MH, Metropolis-Hastings Gibbs, 吉布斯采样 2 公式推导 P2\nCreate on 2022-12-16\n变分推断的目的：找到一个分布 q(Z) 去逼近无法得到解析解(intractable)的后验分布P(Z|X,θ)\nz: latent variable z + parameters θ，把参数也看作随机变量 X: observed data 样本数据 大Z: 样本的隐变量 (X,Z): Complete data P(X,Z) = P(Z|X)P(X)，联合概率 P(X) = P(X,Z)/P(Z|X)，“似然” 省略了θ，重点不在θ 似然取对数，两概率相除变成相减（类似推导也出现在EM，不过下面省略了θ，因为包含在了Z中）：\nlog P(X) = log P(X,Z) - log P(Z|X)\n引入 Z 的分布 q(Z)：\nlog P(X) = log (P(X,Z)/q(Z)) - log (P(Z|X)/q(Z))\n两边同时按照大 Z 的概率密度函数 q(Z) 求似然的期望:\n$$ ∫_Z q(Z)⋅logP(X) dZ = \\\\ ∫_Z q(Z)⋅log (\\frac{P(X,Z)}{q(Z)}) dZ \\\\ - ∫_Z q(Z)⋅log (\\frac{P(Z|X)}{q(Z)}) dZ \\\\ log P(X) = ELBO + KL(q(Z)||P(Z|X)) $$\n因为 logP(X) 与 Z 无关，所以等号左边没变，而等号右边变成了下界 ELBO + KL(Z的分布||Z的后验)。 ELBO 是 q(Z) 的函数，记为 L(q(Z))，是q(Z)的一个变分。\n当X固定，logP(X) 是固定的，又因为 KL 散度≥0，所以 L(q(Z)) 最大为 logP(X)。 希望 KL 散度最小，让 q(Z) 与 P(Z|X) 最接近，也就是让 L(q(Z) 最大\n数学表达： $\\rm \\^q(Z) = arg max_{q(Z)} L(q(Z)) = arg min KL(q(Z)||P(Z|X))$\n求解：\nz 包含隐变量和参数，所以 q(z) 是一个很大的联合概率，假设 q(z) 可以划分成 M 个相互独立的组（统计物理中的平均场理论）： q(z) = ∏ᵢ₌₁ᴹ qᵢ(zᵢ)\n每次只求 1 组 qⱼ，同时固定其余的组{1,2,\u0026hellip;,j-1,j+1,\u0026hellip;M}，逐个求完后，把M组的 q 连乘起来就是整体的 q(z)\nL(q(Z)) = ∫z q(Z)⋅log P(X,Z) dZ - ∫z q(Z)⋅log q(Z) dZ ，把 q(Z) 代入 L = E1 + E2\n对于E1： ∫z q(Z)⋅log P(X,Z) dZ = ∫z log P(X,Z)⋅∏ᵢ₌₁ᴹ qᵢ(Zᵢ) dz₁,z₂,\u0026hellip;, zₘ\n. . . .\n3. 再回首 P3\n符号修正\n. . . .\n4. 随机梯度变分推断-SGVI-1 P4\n基于平均场理论的变分推断无法解决复杂隐变量 z 的情况，比如 z 是一个神经网络，平均场就失效了，z 可以是任意复杂度的。\nstateDiagram-v2 隐变量z --\u003e 观测变量x: Generative model,\\n 条件P(x|z),\\n Decoder 观测变量x --\u003e 隐变量z: Inference model,\\n 后验P(z|x),\\n Encoder 基于平均场理论的变分推断（classical VI）也是一种坐标上升法Coordinate Ascend：一次迭代需要逐个更新 Z 的每个维度。 与此相对的，可以用（随机）梯度上升法解决最大化问题，做变分推断就叫做SGVI / SGVB\n先看下关于 q(z) 的参数的梯度能否求出？\n基于梯度的参数更新公式：θ⁽ᵗ⁺¹⁾ = θ⁽ᵗ⁾ + λ⁽ᵗ⁾⋅∇θ⁽ᵗ⁾。 变分推断以最小化两分布（Z 的假设分布与 Z 的后验分布）的KL散度，或者最大化下界ELBO（ L(q(Z)) ）为目标函数： q^ = arg min_q KL( q(Z) || P(Z|X,θ) ) = arg max_q L(q)， 要更新 q，就需要求出 L 对 q 的梯度：∂L/∂q。\nq(z) 或 q(z|x) 是隐变量 z 的概率分布，x 是观测变量，可以简化掉。 假设 q(z) 是指数族分布，它就有一个参数形式，因此求 q(z) 就是求它的参数。 假设 z 的概率分布 q(z) 是以 ϕ 为参数，如果能求出最好的 ϕ，就相当于得到了 q(z)，所以目标变为去求 ϕ：\nL(ϕ) = ELBO = E_qᵩ(z) [ log (P(x⁽ⁱ⁾,z|θ) / qᵩ(z)) ] = E_qᵩ(z) [ log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z) ] = ∫_z qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) dz - log qᵩ(z)) dz 样本似然：log P(x⁽ⁱ⁾|θ) = L(ϕ) + KL(q||P) ≥ L(ϕ)\n目标函数：ϕ^ = arg maxᵩ L(ϕ)\n把期望写成对随机变量 z 的积分:\n∇ᵩL(ϕ) = ∇ᵩ∫_z qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) dz - log qᵩ(z)) dz ：求偏导 ∇ᵩ 与积分 ∫_z 可交换 = ∫_z [∇ᵩqᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z))] + [qᵩ(z) ⋅ ∇ᵩ(log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z))] dz ：对两项之积求导 = ∫_z ① dz + ∫_z ② dz ：拆为两项分析 在第 ② 项中的 log P(x⁽ⁱ⁾,z|θ) 与 ϕ 无关，对它求导=0，所以 ② 对应的积分为：\n∫z qᵩ(z) ⋅ ∇ᵩ(-log qᵩ(z)) dz = -∫z qᵩ(z) ⋅ 1/qᵩ(z) ⋅ ∇ᵩqᵩ(z) dz = -∫z ∇ᵩqᵩ(z) dz = -∇ᵩ∫z qᵩ(z) dz = - ∇ᵩ1 = 0 所以第2项就约去了，L对 ϕ 求梯度就等于第1项：\n∇ᵩL(ϕ) = ∫z ∇ᵩqᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) dz\n这个式子没办法直接写成期望的形式，如果可以的话，就可以利用蒙特卡罗采样，把未知分布的期望近似出来。\n技巧：利用 ∇ᵩlog qᵩ(z) = 1/qᵩ(z)⋅∇ᵩqᵩ(z) 做等价变换：∇ᵩqᵩ(z) = qᵩ(z) ⋅ ∇ᵩlog qᵩ(z)，然后就可以写成一个期望：\n∇ᵩL(ϕ) = ∫z qᵩ(z) ⋅ ∇ᵩlog qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) dz = E_qᵩ(z) [ ∇ᵩlog qᵩ(z) ⋅ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ]\n所以 L 对 ϕ 的梯度 ∇ᵩL(ϕ) 就等于那一坨关于随机变量 z 的函数按照 qᵩ(z) 求期望。可以用蒙特卡罗采样对其估计：\n假定第 l 个样本 z⁽ˡ⁾~ qᵩ(z), l=1,2,..,L，从分布 qᵩ(z) 中采样 L 个样本， 则 L 个样本的平均值就是近似期望： ≈ 1/L ⋅ ∑ₗ₌₁ᴸ [ ∇ᵩlog qᵩ(z⁽ˡ⁾) ⋅ (log P(x⁽ⁱ⁾,z⁽ˡ⁾|θ) - log qᵩ(z⁽ˡ⁾)) ]\n每一步计算这个期望就是梯度，然后可以用梯度上升法\n5. 随机梯度变分推断-SGVI-2 Source video: P5\n上面使用 MC 采样是有问题的：\n先对 z 采样，算出 qᵩ(z)，当 qᵩ(z) 很小时（靠近0），对应的 log 值（log qᵩ(z⁽ˡ⁾)）变化剧烈，所以梯度 ∇ᵩlog qᵩ(z⁽ˡ⁾)就非常大， 造成整个统计量（所求期望的量:∇ᵩlog qᵩ(Z) ⋅ (log P(X⁽ⁱ⁾,Z|θ) - log qᵩ(Z))）的方差会非常大， 意味着需要更多的样本才能得到比较好的近似，或者如果方差非常大，可能就无法采样，这种直接用MC采样的方法就行不通。\n而且即便用蒙特卡罗采样得到了近似的期望（方差较大，不够精确），它等于下界 L(ϕ) 的梯度，再用梯度上升求分布 qᵩ(Z) 的参数 ϕ^，这样一环扣一环，不精确的梯度再叠加上梯度上升时引入的误差，误差（方差）会越来越大，所以在实际中不可行。\n如何降低统计量的方差？Variance Reduction\n重参数化技巧 Reparameterization trick 最初，下界 L(ϕ) 的梯度 ∇ᵩL(ϕ) = ∇ᵩE_qᵩ(Z) [ log P(X⁽ⁱ⁾,Z|θ) - log qᵩ(Z) ]， 在这个期望中，被统计量（似然-Z后验）和“权重”（Z的分布qᵩ(Z)）都与 ϕ 有关，只能像上面那样先展开，比较复杂。\n如果假定 Z 的分布（概率密度函数）qᵩ(Z) 是已经确定的分布 p(ε)，与 ϕ 无关，然后梯度号就可以直接写到中括号里面，先对似然值求梯度，再按这个确定分布（常量）p(ε)求期望： E_p(ε) [ ∇ᵩ(log P(X⁽ⁱ⁾,Z|θ) - log qᵩ(Z)) ]\n换句话说，z 是分布 qᵩ(Z|X) 中的采样 z～qᵩ(Z|X)，z 是一个随机变量，考虑把 z 和 ϕ 之间的关系解耦，也就是把 z 的随机成分单拎出来。\n重参数化技巧：假定 z 与 ε 和 x 之间有函数关系：z = gᵩ(ε,x⁽ⁱ⁾)，而且 ε 服从一个给定的（简单的）分布 ε~P(ε)。 这样 z 是关于随机变量 ε 的函数，z 仍然是一个随机变量，但它的随机性转移到了 ε 上（当ε和x定了，z就定了）:\nZ～qᵩ(Z|X) ↓ “随机性通过函数关系 g 转移” ε～p(ε)\n因为 ∫z qᵩ(z|x⁽ⁱ⁾)⋅dz = 1，∫ p(ε)⋅dε =1，而且 z 是 ε 的一个（线性）变换， 因此“定性地”认为：|qᵩ(z|x⁽ⁱ⁾)⋅dz| = |p(ε)⋅dε|\n把 ∇ᵩL(ϕ) 中的 z 代换为变换 gᵩ(ε,x⁽ⁱ⁾):\n∇ᵩL(ϕ) = ∇ᵩE_qᵩ(z) [ log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z) ] ，写成对Z积分的形式\n= ∇ᵩ ∫z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ⋅ qᵩ(z|x⁽ⁱ⁾)⋅dz ，代换 = ∇ᵩ ∫z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ⋅ p(ε)⋅dε ，写成期望 = ∇ᵩ E_p(ε) [ log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z) ] ，p(ε)与ϕ无关,∇ᵩ写里面\n= E_p(ε) [∇ᵩ (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z)) ] ，先对z求∇，再对ϕ求∇ z 是 ϕ 的函数: z=gᵩ(ε,x⁽ⁱ⁾) = E_p(ε) [∇_z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z|x⁽ⁱ⁾)) ⋅∇ᵩz] ，链式法则 = E_p(ε) [∇_z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z|x⁽ⁱ⁾)) ⋅ ∇ᵩgᵩ(ε,x⁽ⁱ⁾)]\n其中 p(ε) 与 ϕ 无关，两个 log 是 z 的函数（θ是上一时刻的），z是ε的函数，然后是 g 对 ε 的梯度。 此时再用蒙特卡罗采样对 ε 采样：ε⁽ˡ⁾～p(ε), l=1,2,..,L，把 ε 带入 中括号里的那个函数算函数值，再求平均值就是近似的期望：\n∇ᵩL(ϕ) ≈ 1/L ∑ₗ₌₁ᴸ ∇z (log P(x⁽ⁱ⁾,z|θ) - log qᵩ(z|x⁽ⁱ⁾)) ⋅ ∇ᵩgᵩ(ε⁽ˡ⁾,x⁽ⁱ⁾)]， 其中 z=gᵩ(ε⁽ˡ⁾,x⁽ⁱ⁾)\n然就可以把这个近似梯度带入梯度上升公式：ϕ⁽ᵗ⁺¹⁾ = ϕ⁽ᵗ⁾ + λ⁽ᵗ⁾⋅∇ᵩL(ϕ)， 每一步求个近似期望，得到梯度，再做梯度上升\n","date":"2022-12-20T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/12-%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/","title":"watch: ML - 白板 12 | Variational Inference"},{"content":"Gaussian Mixture Model 高斯混合模型\n1. 模型介绍 Source video: P1\n根据中心极限定理，假设数据服从高斯分布是合理的\n一维数据:\n横轴是数据点，服从两个高斯分布的叠加，纵轴是概率密度函数值(PDF)，点越密代表该区域出现样本的概率越大。\n可以认为这些样本点服从一个高斯分布，但并不合理，用两个高斯比较精确，红色曲线是2个分布的叠加（混合）。\n(1) 从几何角度来看 GMM 的概率密度函数是多个高斯分布的加权平均，并不是直接相加，否则概率密度函数的积分可能大于1。权重是取到各种高斯分布的概率\n每个高斯有自己的均值 μ 和方差 Σ （如果是多维的，就是协方差矩阵），它们的参数是要学习出来的。\nGMM 中一个样本 x 的概率密度函数是K个高斯分布的加权平均： p(x) = ∑ₖ₌₁ᴷ αₖ N(x|μₖ,Σₖ), where ∑ₖ₌₁ᴷ αₖ = 1，α 是权重\n(2) 从混合模型（或概率生成）角度来看 GMM 用离散的随机变量 z 代表样本 x 分别属于 K 个高斯分布 概率。N 个样本是重复 N 次生成过程：先从K个高斯分布中选一个，再在这个高斯分布中采样生成一个样本。\n两维数据的 PDF 是3维的（曲面）:\n小x 是观测变量observed variable，两维的: x=(x1,x2) 引入隐变量latent variable 小z，一个 z 表明了一个样本 x 可能属于哪一个高斯分布，所以 z 是一个离散型的随机变量， 一个样本 x 属于各个高斯分布的概率分别为： z C₁ C₂ \u0026hellip; Cₖ 概率密度 p₁ p₂ \u0026hellip; pₖ z 代表不同的类别categories（不同的高斯分布），其中概率密度求和等于1：∑ₖ₌₁ᴷ pₖ = 1\n一个样本既可属于C1，也可属于C2\u0026hellip;, 只不过概率不同，所以用一个随机变量表达。 用隐变量 z 代表一个样本 x 所属的高斯分布比较方便: x ～ z，z是一个离散分布，可以是 C₁，也可以是 C₂,C₃,\u0026hellip;Cₖ,\nGMM 是一个生成模型（混合模型一般都是生成模型），每个样本都是按照生成过程逐一生成的： 假设有一个骰子有 K 个面，而且重量分布不均匀，所以各面的概率不同（越重越大）\n掷一次得到了第 k 面(概率是pₖ)，对应于第 k 个高斯分布 在这第 k 个高斯分布中去采样，就生成了一个样本 GMM 是最简单的生成模型，它的概率图：\nN 表示生成 N 个样本，小 x 是观测变量，用阴影来表示，小 z 是随机变量，用空心的圈表示，它服从的分布是以 p 为参数: p=(p1,p2,\u0026hellip;,pₖ) 是一个向量，所以用实心点表示参数 p；而 X 服从高斯分布，其参数为 (μ,Σ)\n2. 极大似然 Source video: P2\n从混合模型角度，写出 GMM 的概率密度函数 p(x): 既然引入了隐变量 𝐳，也是一个随机变量，把它的概率积掉就行了，又因为 𝐳 是离散型的随机变量，所以是把各个可能的取值求和。\n一个样本 x 的概率密度函数：\nP(x) = ∑_z P(x,z) = ∑_ₖ₌₁ᴷ P(x,z=Cₖ)，分别从 K 个高斯分布中采出 x 的概率: P(x,zₖ) 累加 = ∑ₖ₌₁ᴷ P(z=Cₖ) P(x|z=Cₖ)，联合概率拆成两项积 = ∑ₖ₌₁ᴷ pₖ ⋅ N(x | μₖ,Σₖ)，第k个高斯分布出现的概率 乘以 在第k个高斯分布中采出x的概率\n所以混合模型的概率密度函数与加权平均是一样的，只不过“权重” aₖ 变成了这里的概率值 pₖ\n做 N 次随机试验，每次做试验时(扔骰子)，(假设)隐变量 𝐳 是不同的（即K个正态分布出现的概率不同）， 每次生成样本时，依据 𝐳 从 K 个高斯分布中选一个分布，再从被选中的高斯分布中采一个 x，所以每个 x 和一个 𝐳 对应: (xᵢ,𝐳ᵢ)。 要求 xᵢ 的似然，就把这次试验中的 𝐳ᵢ 从联合概率P(xᵢ,𝐳ᵢ)中积掉。\nN个样本就是 N 次试验同时发生，发生的概率（似然）就是多次试验的连乘 P(X,Z)=p(x₁,z₁)p(x₂,z₂)\u0026hellip;p(xN,zN))，所以求 P(X)=∫z P(X,Z) dZ 就是多重积分: 先对 z1 积，再对 z2 积，\u0026hellip;.\nx observed variable, 观测随机变量 z latent variable, 隐变量，服从离散分布: K个类别（高斯分布）C₁,C₂,\u0026hellip;,Cₖ 分别对应概率 p₁,p₂,\u0026hellip;,pK； X observed data, N个观测数据 x1,x2,\u0026hellip;xN, 样本之间相互独立 Z \u0026ldquo;latent data\u0026rdquo;, z1,z2,\u0026hellip;zN, N 个样本 x 对应的生成它的隐变量 (X,Z): complete data, “完整数据” (x1,z1),(x2,z2),\u0026hellip;(xN,zN)，每个样本 x 对应一个隐变量 为了叙述方便，用 θ 代表参数 = {p₁,p₂,\u0026hellip;,pK, μ₁,Σ₁, μ₂,Σ₂,\u0026hellip;, μK,ΣK}，隐变量中 K 个高斯分布出现的概率 + K 个高斯的均值和方差 用极大似然估计参数 θ: θ^ = arg max_θ log P(X)\nGMM 是没有解析解的，所以直接用 MLE 是做不出来的，只能用数值方法得到近似解。而且 GMM （混合模型）中含有隐变量，所以用 EM 算法（迭代）会更有效率。\nθ^ = arg max_θ log P(X) = arg max_θ log ∏ᵢ₌₁ᴺ P(xᵢ) ，N个样本iid = arg max_θ ∑ᵢ₌₁ᴺ log P(xᵢ) ，连乘变连加 = arg max_θ ∑ᵢ₌₁ᴺ log ( ∑ₖ₌₁ᴷ pₖ ⋅ N(xᵢ | μₖ,Σₖ) )，将P(xᵢ)代入\n要求极大值对应的参数，可以对上式中的参数（pk,μₖ,Σₖ）求偏导=0，但是因为 log 里面是多项连加，而且高斯分布可能是高维的（表达式很复杂），无法求出解析解\n对于单个高斯分布，它的μ,σ² 都可以直接用 MLE 求出来。因为单一个正态分布的表达式简单，log可以把两项之积拆开，也可以把exp去掉，简化之后，对似然求导，可得到解析解。\n3. EM求解-E-step Source video: P3\n用 EM 算法通过 MLE 求出 GMM 的参数 θ。\nEM 是要最大化似然的下界，即似然的期望：θ⁽ᵗ⁺¹⁾ = arg max_θ ∫_Z q(Z) ⋅ log P(X,Z|θ) dZ， 这个积分的本意是对（每个样本的）似然值 log P(x,z|θ) 按照隐变量 z 的分布 q(z) 求期望。 因为 z 的分布是离散的，包括 K 个（高斯分布出现的）概率值，所以 logP(x,z|θ) 和 q(z) 也都是离散的，二者相乘的积分就是加权和 ∑ q(z)⋅log P(x,z|θ)，就是一个样本的期望。\n因为大 Z 是多个小 z 的联合，是多个”事件“同时发生的联合概率，又因为样本之间独立同分布，大Z的概率就是单个 z 的概率连乘：q(Z)=q(z₁)⋅q(z₂)\u0026hellip;q(zN)， 所以对大 Z 的积分就是多重积分，即对每个小 z 积分: ∫z₁∫z₂\u0026hellip;∫zN； 而只做有限 N 次试验的话，就是把 N 次实验的加权和连乘起来：\nE = ∫_Z q(Z) ⋅ log P(X,Z|θ) dZ ，X和Z都是大写, N个样本集合. (|也可写成;: θ是随机变量vs固定值 ¹)\n= ∫_z₁ ∫_z₂\u0026hellip;∫_zN q(Z) ⋅ log P(X,Z|θ) dz₁ dz₂ dzN，把q(Z)写开\n= ∑_z₁ ∑_z₂\u0026hellip;∑_z$_N$ ∏ᵢ₌₁ᴺ q(zᵢ) ⋅ log ∏ᵢ₌₁ᴺ P(xᵢ,zᵢ|θ)，zᵢ都是离散分布\n= ∑_z₁ ∑_z₂\u0026hellip;∑_z$_N$ ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ log ∏ᵢ₌₁ᴺ P(xᵢ,zᵢ|θ) ，当 q(zᵢ) 等于Z的后验分布时，KL散度=0，\u0026ldquo;积分\u0026rdquo;（似然的期望）取到最大。 = ∑_z₁ ∑_z₂\u0026hellip;∑_z$_N$ [ ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ ∑ᵢ₌₁ᴺ log P(xᵢ,zᵢ|θ) ]，log连乘变连加\n把这个期望 E 记为 Q(θ,θ⁽ᵗ⁾)，是关于 θ 的一个函数（log 中的 θ 是变量，而θ⁽ᵗ⁾是常数）。\n先把 log 的连加展开：\nQ(θ,θ⁽ᵗ⁾) = ∑_z₁ ∑_z₂\u0026hellip;∑_z$_N$ ∏ᵢ₌₁ᴺP(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ [ log P(x₁,z₁|θ) + log P(x₂,z₂|θ) + \u0026hellip; + log P($x_N,z_N$|θ)]\n多重\u0026quot;积分\u0026quot;，先对 z₁ 积分：先只取出第 1 项 logP(x₁,z₁|θ)，并且只把 z₁ 从联合概率中取出来：\n∑_z₁ ∑_z₂\u0026hellip;∑_z$_N$ [ log P(x₁,z₁|θ) ⋅ ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ]\n= ∑_z₁ ∑_z₂\u0026hellip;∑_zN [ log P(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾) ⋅ ∏ᵢ₌₂ᴺP(zᵢ|xᵢ,θ⁽ᵗ⁾) ] ，把只与 z₁ 相关的项分出来 = ∑_z₁ logP(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾) ⋅ ∑_z₂\u0026hellip;∑_zN [ ∏ᵢ₌₂ᴺP(zᵢ|xᵢ,θ⁽ᵗ⁾) ]\n对于后半部分的从 2 到 N 的联合概率：\n∑_z₂\u0026hellip;∑_zN [ ∏ᵢ₌₂ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ] = ∑_z₂\u0026hellip;∑_zN [ P(z₂|x₂,θ⁽ᵗ⁾) ⋅ P(z₃|x₃,θ⁽ᵗ⁾) \u0026hellip; ⋅ P(zN|xN,θ⁽ᵗ⁾)] 因为每一个累加号都只与一个 z 相关，也就是把每个 z 对应的离散概率分布（p₂,\u0026hellip;,pK）求和，并且概率密度函数积分等于 1: = ∑_z₂ P(z₂|x₂,θ⁽ᵗ⁾) ∑_z₃ P(z₃|x₃,θ⁽ᵗ⁾) \u0026hellip; ∑_zN P(zN|xN,θ⁽ᵗ⁾) = 1⋅1⋅\u0026hellip;⋅1 = 1\n所以对 z₁ 积分的结果，除了只与 z1 相关的项: ∑_z₁ logP(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾)，其余都是1。同理之后对 z2, z3,\u0026hellip;zN积分时，也只保留与自己相关的项\nQ(θ,θ⁽ᵗ⁾) = ∑_z₁∑_z₂\u0026hellip;∑_zN ∏ᵢ₌₁ᴺ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ⋅ [logP(x1,z1|θ) + logP(x2,z2|θ) +\u0026hellip; + logP(x_N,zN|θ)]\n= ∑_z₁ log P(x₁,z₁|θ) ⋅ P(z₁|x₁,θ⁽ᵗ⁾) + ∑_z₂ log P(x₂,z₂|θ) ⋅ P(z₂|x₂,θ⁽ᵗ⁾) + \u0026hellip; + ∑_zN log P(xN,zN|θ) ⋅ P(zN|xN,θ⁽ᵗ⁾) = ∑ᵢ₌₁ᴺ ∑_zᵢ log P(xᵢ,zᵢ|θ) ⋅ P(zᵢ|xᵢ,θ⁽ᵗ⁾) ，这里xᵢ,zᵢ都是小写，单个样本\n综上，似然P(X)的期望等于 N 个样本的似然期望的和。\n其中，单个样本的联合概率（或者说似然）: P(x,z|θ) = P(z|θ) P(x|z,θ) = p_z ⋅ N(x | μ_z,Σ_z)，这里下标 z 表示该样本的隐变量。 因为 P(x) = ∑ₖ₌₁ᴷ pₖ ⋅ N(x | μₖ,Σₖ)， 所以单个样本的后验概率：P(z|x,θ) = P(x,z)/P(x) = p_z ⋅ N(x | μ_z,Σ_z) / ∑ₖ₌₁ᴷ pₖ ⋅ N(x | μₖ,Σₖ)\n把似然和后验代入 Q:\nQ(θ,θ⁽ᵗ⁾) = ∑ᵢ₌₁ᴺ ∑_zᵢ log (p_zᵢ ⋅ N(xᵢ | μ_zᵢ,Σ_zᵢ)) ⋅ [ p_zᵢ ⋅ N(xᵢ | μ_zᵢ,Σ_zᵢ) / ∑ₖ₌₁ᴷ pₖ ⋅ N(xᵢ | μₖ,Σₖ) ] ，zᵢ是1⋯K中的任意一个\n以上就是 EM 中的 E-step：把（N个样本的）似然的期望表示出来。M-step 要关于参数 θ 求 Q 的最大值。\n4. EM求解-M-step Source video: P4\n上面得到了 Q 的表达式，对每个样本的似然按照后验求期望，然后N个样本累加。 M-step 要解一个最优化问题。\n后验展开是：p_zᵢ ⋅ N(xᵢ | μ_zᵢ⁽ᵗ⁾,Σ_zᵢ⁽ᵗ⁾) / ∑ₖ₌₁ᴷ pₖ⁽ᵗ⁾ ⋅ N(xᵢ | μₖ⁽ᵗ⁾,Σₖ⁽ᵗ⁾)\n在后续推导中，仍然把一个样本z 的后验简记为： P(zᵢ|xᵢ,θ⁽ᵗ⁾)，因为 θ⁽ᵗ⁾是上一时刻的参数：θ⁽ᵗ⁾={p₁⁽ᵗ⁾,p₂⁽ᵗ⁾,\u0026hellip;,pK⁽ᵗ⁾, μ₁⁽ᵗ⁾,Σ₁⁽ᵗ⁾, μ₂⁽ᵗ⁾,Σ₂⁽ᵗ⁾,\u0026hellip;, μ_K⁽ᵗ⁾,Σ_K⁽ᵗ⁾}，是常数，所以就简单写。 而似然 logP(xᵢ,zᵢ|θ) 中的θ 是变量，所以似然的期望的累加就表示为：\nQ(θ,θ⁽ᵗ⁾) = ∑ᵢ₌₁ᴺ ∑_zᵢ log (p_zᵢ ⋅ N(xᵢ | μ_zᵢ, Σ_zᵢ)) ⋅ P(zᵢ|xᵢ, θ⁽ᵗ⁾)\n交换两个累加符号顺序：\nQ(θ,θ⁽ᵗ⁾) = ∑_zᵢ ∑ᵢ₌₁ᴺ log (p_zᵢ ⋅ N(xᵢ | μ_zᵢ, Σ_zᵢ)) ⋅ P(zᵢ|xᵢ, θ⁽ᵗ⁾)\nzᵢ 是一个样本上，K个高斯分布可能的概率，z 在最外面，可以把 zᵢ 替换成小 k（从1到K），∑_zᵢ 替换为 ∑ₖ₌₁ᴷ：\nQ(θ,θ⁽ᵗ⁾) = ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ log (pₖ ⋅ N(xᵢ | μₖ, Σₖ)) ⋅ P(zᵢ=Cₖ|xᵢ, θ⁽ᵗ⁾) = ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ [ log pₖ + log N(xᵢ | μₖ, Σₖ) ] ⋅ P(zᵢ=Cₖ|xᵢ, θ⁽ᵗ⁾)\n目标函数就为： θ⁽ᵗ⁺¹⁾ = arg max_θ Q(θ,θ⁽ᵗ⁾)\nθ⁽ᵗ⁺¹⁾ 包括 pₖ⁽ᵗ⁺¹⁾={p₁⁽ᵗ⁺¹⁾,p₂⁽ᵗ⁺¹⁾,\u0026hellip;,p_K⁽ᵗ⁺¹⁾}, μₖ⁽ᵗ⁺¹⁾={μ₁⁽ᵗ⁺¹⁾,μ₂⁽ᵗ⁺¹⁾,\u0026hellip;,μ_K⁽ᵗ⁺¹⁾}, Σₖ⁽ᵗ⁺¹⁾={Σ₁⁽ᵗ⁺¹⁾,Σ₂⁽ᵗ⁺¹⁾,Σ_K⁽ᵗ⁺¹⁾}，\n下面只介绍 pₖ 的求法：\n在 Q(θ,θ⁽ᵗ⁾) 中，只有 log pₖ 与 pₖ 相关：\np⁽ᵗ⁺¹⁾ = arg max_pₖ ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ log pₖ ⋅ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾), s.t. ∑ₖ₌₁ᴷpₖ=1\n求这个带约束的最优化问题，用拉格朗日乘子法求解:\n写出拉格朗日函数：\nL(pₖ,λ) = ∑ₖ₌₁ᴷ ∑ᵢ₌₁ᴺ log pₖ ⋅ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + λ(∑ₖ₌₁ᴷpₖ - 1)\n对 pₖ 求偏导时，后面的z的后验分布是常量，求导后是系数。pₖ 的 k 是从1到K的任一数，只对 pₖ 求导，所以带有 p₁,p₂,\u0026hellip;, pₖ₋₁ 的项都是0，即最外层的累加号没了。\n∂L/∂pₖ = ∑ᵢ₌₁ᴺ 1/pₖ ⋅ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + λ ≜ 0 ，令其等于0\n两边同时乘以 pₖ:\n∑ᵢ₌₁ᴺ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + pₖ ⋅ λ = 0\n为了利用约束条件，把 k=1,2,\u0026hellip;,K 的式子都加起来：\n∑ᵢ₌₁ᴺ ∑ₖ₌₁ᴷ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + ∑ₖ₌₁ᴷpₖ ⋅ λ = 0\n其中 ∑ₖ₌₁ᴷ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) 是对隐变量 zᵢ 的概率密度函数“积分”，为1，所以\n∑ᵢ₌₁ᴺ 1 + λ = 0 λ = -N\n把 λ=-N 代入上式：\n∑ᵢ₌₁ᴺ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾) + pₖ ⋅ (-N) = 0 pₖ⁽ᵗ⁺¹⁾ = 1/N ⋅ ∑ᵢ₌₁ᴺ P(zᵢ=Cₖ|xᵢ,θ⁽ᵗ⁾)\n上式是通项，把 k=1,2,..K，代入就可得到每个 p。\n求 μₖ 和 Σₖ 的方法类似，只关心 log N(xᵢ | μₖ,Σₖ)，log 可以把高斯分布的概率密度函数简化（两项积拆成两项和，exp也可去掉），而且是无约束，直接（关于矩阵）求导，令其=0就行\nRef 概率论：p(x|theta)和p(x;theta)的区别 - -柚子皮- - CSDN ","date":"2022-12-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/11-gmm/","title":"watch: ML - 白板 11 | GMM"},{"content":"Source videos: 【机器学习】白板推导系列(三十二) ～ 变分自编码器(VAE)】\n1. 模型表示 P1\nVariation 来自概率图模型； Auto Encoder 来自神经网络\nVAE 也是一种 LVM (Latent Variable Model)，最简单的隐变量模型是 GMM，它的概率图模型表示为：\nflowchart TB latent((z)) --\u003e observed((x)) 小 x 是观测变量，小 z 是假想的服从某种分布的隐变量，先对 z 的分布 P(z) 中采样，再在 z 固定的情况下，从分布 P(x|z) 中采样 x。\nGMM K 个高斯分布的混合，样本数据 x 可能来自于这 K 个高斯分布的任一个，只是概率不同。 而 VAE 是无限个高斯分布的混合。\nGMM 假设隐变量 z 是服从一维的离散型概率分布：z～Categorical dist，分布列：\nz 1 2 \u0026hellip; K p p₁ p₂ \u0026hellip; pₖ z 代表不同的类别categories（不同的高斯分布），其中概率密度求和等于1：∑ₖ₌₁ᴷ pₖ = 1。x 在 z 选定的情况下，服从高斯分布：x|zₖ～N(x|μₖ,Σₖ)\nGMM 顶多可以被用来做聚类任务，但无法解决复杂的任务，比如目标检测，因为它的 z 太简单了，只有 1 维还是离散型的变量，只能对 1 个属性划分 K 类别，\n比如有一群人作为样本，想学习出一个z 用来表示一个样本。 GMM 假设了 z 只是一个 1 维K类的变量，就只能把这群人分成 K 类 （比如按“职业”属性分成：工人，农民，知识份子\u0026hellip;），所以只能表达出一个人是来自于哪类。 也就是说，GMM 对样本的表达非常肤浅，因为一个人有多个属性，必须用用多个维度表达： 比如 “性别”z₁={男,女}，“肤色”z₂={黄,白,黑}，“年龄”z₃={ℤ}，“身高”z₄连续的，这么多维度 GMM 无法表达出来。\n隐变量 z 应该是高维的，连续的随机变量，假设 z 服从高斯分布: z～N(μ=0,Σ=I)，满足高维连续。P(z) 是先验，它并不重要，只是辅助建模，我们最终关心的是 inference 过程：给一个 x 返回它的 z，也就是后验 P(z|x)\n如果样本 x 本身是连续的，可以假设它的条件概率也服从高斯分布：x|z～N(μ(z),Σ(z))。若是离散的，则仍然用 categorical distribution。\n条件概率 x|z 的均值和方差都是 z 的函数，也就是先给定了 z，然后求出 μ(z) 和 Σ(z)，相当于得到了 x。所以实际上要学 μ,Σ 与 z 之间函数关系。 可以用神经网络逼近出来这个函数，所以 μ,Σ 是神经网络（参数θ）的函数，用 μ_θ, Σ_θ 表示。\n而不直接通过算 Likelihood 求 x|z 的概率分布，是因为 z 的维度太高，不好把 z 积掉：P(x) = ∫_z P(x,z) dz =∫_z P(z) P(x|z) dz。 因为假设了 z 的维度很高，高到积分算不出来，则 P(x) 是 intractable 的， 又因为后验分布 P(z|x) = P(x|z)P(z) / P(x)，所以z的后验也算不出来。也就无法从 x 到 z 做inference。 只能用重参数化技巧和神经网络逼近后验分布。\n推断\u0026amp;学习 P2\nflowchart TB latent((z)) -- \"Pᶿ(x|z) \\n Decode \\n Generation\" --\u003e observed((x)) observed((x)) -. \"Pᶿ(z|x) or qᶲ(z|x)\\n Encode \\n Inference\" .-\u003e latent((z)) z 是隐变量假定服从高斯分布 P(z)=N(0,I)，x 是观测变量假定服从高斯分布 P_θ (x|z)=N(μ_θ (z), Σ_θ (z))。 假如参数 θ 已经训练好了，生成样本时，先从 z 的分布 P(z) 中采样一个 z⁽ⁱ⁾，然后就能从 P_θ( x|z⁽ⁱ⁾) 采样出一个 x⁽ⁱ⁾。\n因为后验分布 P(z|x) 无法通过贝叶斯公式算出，所以用 q_ϕ(z|x) 不断逼近后验分布P_θ(z|x)。\n用 EM 求解 GMM 时，是最大化似然，似然可以分成下界 ELBO 和 KL( q_ϕ(z|x) || P_θ(z|x))。\n最原始的 EM 的 E-step 要写出n个样本似然p(x)的（最大的）期望：当 q(z|x)=P(z|x) 时, KL=0, 似然的期望就是 ELBO。\nM-step 就是解最大化问题：θ = arg max_θ ELBO = arg max_θ E_P(z|x,θ⁽ᵗ⁺¹⁾) [ log P(x,z|θ⁽ᵗ⁾) ]. E-M 即“最大的期望 与 期望的最大” VAE 不能用基础的 EM 解决，因为需要 q(z|x) 能取到 P(z|x)，但是这里的后验 P(z|x) 是 intractable 的，所以只能让 KL 散度足够小，找出最好的 q，也就是找出它最好的参数ϕ，让 q_ϕ(z|x) 足够接近 P_θ(z|x)。\n在EM中，通过引入隐变量 z 和 Z 的分布 q(Z)，对数似然 logP(X) 被拆分成 ELBO + KL散度(q(Z)||P(Z|X))。 对应到这里的情况，KL 散度中的 q(Z) 应该是后验 qᵩ(z|x)，也就是对似然 P(x|z) 引入的是 qᵩ(z|x)，同样 ELBO 中也是按 q(z|x) 求期望。 ELBO 把里面的 log 展开，即可写成联合概率P(z,x) + 熵H[q(z|x)]；再把联合概率拆开，其中 z 的先验 P(z) 可与后验的熵 H[q(z|x)] 结合成一个KL散度：z的后验与z的先验要靠近。\n$\u0026lt;\\^θ,\\^ϕ\u0026gt;$ = arg min KL( q_ϕ(z|x) || P_θ(z|x) ) = arg max ELBO ，等价于最大化似然P(x|z)的下界 = arg max E_qᵩ(z|x) [ log (P_θ (z,x)/qᵩ(z|x)) ] ，以 z 的后验加权 = arg max E_qᵩ(z|x) [ log P_θ (z,x) - log qᵩ(z|x)) ] = arg max E_qᵩ(z|x) [ log P_θ(z,x) ] - E_qᵩ(z|x) [ log q(z|x) ] ，第2项是 q(z|x) 的熵 = arg max E_qᵩ(z|x) [ log P_θ(z,x) ] + H[qᵩ(z|x)] ，log里面的联合概率拆开 = arg max E_qᵩ(z|x) [ log P_θ(x|z) ] + E_qᵩ(z|x) [ log P(z) ] + H[qᵩ] ，P(z) 是先验分布不带参数\n= arg max E_qᵩ(z|x) [ log P_θ(x|z) ] + E_qᵩ(z|x) [ log P(z)-log q(z|x)] = arg max E_qᵩ(z|x) [ log (P_θ(x|z) ] + E_qᵩ(z|x) [ log (P(z) / q(z|x))] = arg max E_qᵩ(z|x) [ log P_θ(x|z) ] + ∫_z qᵩ(z|x)⋅log ( P(z) / q(z|x)) dz = arg max E_qᵩ(z|x) [ log P_θ(x|z) ] - KL( qᵩ(z|x) || P(z) )\n目标函数是一个期望减 KL 散度，期望要最大，而KL散度要最小。\n变分推断用梯度上升求最大化问题，首先求目标函数对 θ,ϕ 的梯度，然后更新\n采用 SGVI / SGVB / SVI / Armotized Inference （重参数化技巧+神经网络）求后验，解决 Inference 问题。\nSGVI 假设 z|x 服从高斯分布 z|x～N(μ_ϕ(x),Σ_ϕ(x))，并且与随机高斯噪声 ε～N(0,I) 之间有函数关系 z= μ_ϕ(x) + Σ_ϕ¹ᐟ²(x)⋅ε，协方差矩阵的指数是½，因为 Σ 里面是σ²。\nflowchart LR x(\"Input x\") --\u003e net(\"NN-ϕ\") --\u003e a(\"μ(x)\") --\u003e o((\"+\")) --\u003e z(\"latent z\") net --\u003e b(\"Σ(x)\") --\u003e m((\"×\")) --\u003e o ε --\u003e m 对于目标函数中的 E_qᵩ(z|x) [ log P_θ(x|z) ]，中括号里面是给定 z 生成 x，而期望的权重 q(z|x) 是给定 x 时，z 的后验概率。 整个训练过程就是，先给了样本 x 得到了后验 q_ϕ(z|x)，从中采样得一个 z⁽ⁱ⁾，然后用它算似然 logP(x|z⁽ⁱ⁾)，是一个环路。 所以在训练过程中，log 里的 z 不是先验 P(z)，而是后验 P(z|x)。 在训练好之后，得到了θ，生成样本时，就直接从 P(z) 中采样，再代入 P_θ(x|z) 得到 x。\nflowchart LR x --\u003e Decoder --\u003e z --\u003e Encoder --\u003e x' 目标函数后面的 KL 散度相当于 正则化项。在训练时，要让 q_ϕ(z|x) 尽量与先验 P(z) 靠近，避免坍缩到一个点上，否则第 1 项似然的期望很可能就过拟合了。 或者说让 q_ϕ 的熵 H[q_ϕ] 倾向于大。 熵意味着信息量，信息量大意味着有广泛的可能性，分布更平均，高斯分布越扁方差越大熵越大，钟形曲线越瘦高，说明只在期望那一个点上可能性最大，基本上是确定的，熵就很小，\nsumNote\n","date":"2022-12-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/32-vae/","title":"watch: ML - 白板 32 | VAE"},{"content":"先验概率 标签的直观分布\n后验概率： 在某一事件先成立的条件下，标签的分布。\n假设已知玩不玩英雄联盟这件事情 (𝐘) 上的概率分布（先验）为：\nP(Y=玩)=0.6；P(Y=不玩)=0.4\n另外，已知性别 (𝐗) 分布（似然/类条件概率）：玩LOL人群中:80%是男生,20%女生；不玩LOL的人中有:20%男生,80%女生，也就是：\nP(X=男性|Y=玩lol)=0.8，P(X=小姐姐|Y=玩lol)=0.2 P(X=男性|Y=不玩lol)=0.2，P(X=小姐姐|Y=不玩lol)=0.8\n求：一个男生他玩LOL的概率（后验，它是在先观察到性别X事件发生后得到的）\n根据贝叶斯定理 P(Y|X)=(P(X|Y)⋅P(Y))/P(X), P(Y=玩 | X=男性) = P(X=男性|Y=玩)⋅P(Y=玩) / (P(X=男性|Y=玩)⋅P(Y=玩) + P(X=男性|Y=不玩)⋅P(Y=不玩))\n知乎用户V6oo4r 的评论： 先验概率是以全事件为背景下，A事件发生的概率: P(A|Ω). 后验概率是以新事件B为背景下，A事件发生的概率: P(A|B).\n全事件一般是统计获得的，所以成为先验概率，没有做实验前的概率。\n新事件一般是实验，如试验B，此时的事件背景从全事件变成了B，该事件B可能对A的概率有影响，那么需要对A现在的概率进行一个修正，从P(A|Ω) 变成了 P(A|B)，所以成 P(A|B) 为后验概率，也就是试验（事件B发生）之后的概率。 P(A|B)= P(B|A)⋅P(A|Ω)/P(B|Ω)\n例子来源：如何理解先验概率与后验概率-昌硕-知乎专栏\n机器学习-白板推导系列(一)-开篇\n频率派 参数θ可能不仅是一个，也可能是一组数\n贝叶斯派 参数 θ 是随机变量，服从一种概率分布（先验分布）\n贝叶斯定理把参数的先验分布和后验分布通过似然值联系起来，参数的后验分布= 似然x先验分布/在参数空间对样本数据积分。\n参数估计的方法：MAP 最大后验估计\n","date":"2022-12-16T13:02:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/01_%E9%A2%91%E7%8E%87%E6%B4%BE-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE/","title":"watch: ML - 白板 01 | Frequentist vs Bayesian"},{"content":"贝叶斯解释“L1和L2正则化”，本质上是最大后验估计。如何深入理解贝叶斯公式？\n(贝叶斯公式中的概率可以是随机事件的概率，也可以是概率分布wiki)\n似然函数 L(θ|X)：X发生之后，关于θ的函数。 似然值：参数 θ 发生后，X 发生的概率，即 L(θ|X) = P(X|θ)。\n频率派认为训练数据是来自真实分布的采样，似然值（X发生概率）最大的 θ 就是最接近真实的，但并不能说这组参数就是最有可能的分布。 贝叶斯派想直接求参数 θ 的概率，认为真实θ分布发生的概率应该最大。\nMLE 在两个地方做了近似：寻找使 X 发生概率最大的 θ 并不等于 θ 发生的概率最大；θ 是一个分布，找到的是它的众数。 MAP 只有一个近似：寻找概率最大的 θ 分布是严谨的，只剩\u0026quot;θ 是一个分布，找到的是它的众数\u0026quot;。\n(221216): 概率模型的参数θ出现的概率是概率密度函数，但在概率密度函数上，任一个点（一个θ）的概率是0，因为它对应的面积为0，所以只有谈论\u0026quot;一段区间\u0026quot;出现的概率才有意义，比如θ落在0.2~0.3之间的概率为60%。\nMAP 等于 MLE 加上正则化 θ 在训练集 X 上发生的概率 P(θ|X) 可用贝叶斯公式展开：\nP(θ|X) = (P(X|θ)/P(X)) ⋅ P(θ)\n模型参数 θ 的概率 = X 在参数为 θ 时发生的概率 与 X 在所有可能的 θ 下发生的概率之和 的比值 × θ 的起始概率。 其中 P(X) =∫ P(X|θ)P(θ) dθ 。\nP(θ|X) 和 P(θ) 都是 θ 的概率，不过前者是有条件的（是X发生之后的，是更新后的），所以叫做“后验分布”（X是试验，即“试验之后”）；后者是在X发生之前，（不看X）心中已有的知识，是先验分布。\nP(X|θ) 和 P(X) 都是 X 的概率，不过前者是在参数为 θ 时，X 发生的概率，后者是各种参数得到 X 的概率之和，它们的比值是先验分布 P(θ) 的置信度。\n从先验分布P(θ)出发，算出实验结果 X 在参数为 θ 时发生的概率，然后除以 X 在各种θ时发生的概率之和，做为系数，对先验分布做修正。\nP(X) 是对所有可能的 θ 做积分（也称边缘概率），比如对于抛硬币实验，就要对正-反概率：从 (正:0, 反:1) 到 (正:1, 反:0) 之间的所有情况，求出X发生的概率再积分（是个常数）。无法直接求，可以用蒙特卡洛方法近似。 不过，想求概率最大时的 θ，并不必求出概率最大是多少。因为 P(X) 与 θ 无关，所以 P(θ|X) ∝ P(X|θ) P(θ)，只需求似然度乘以先验分布乘积最大时，对应的 θ 就行。\nMLE 认定：θ = arg max_θ P(X|θ)；\nMAP 认定：θ = arg max_θ (P(X|θ)⋅P(θ))\n(MLE和MAP的求解推导 极大似然估计/最大后验估计—通过抛硬币例子理解)\n代入神经网络中的符号：\nMLE: W = arg max_W P(X,Y|W) ∝ arg max_W log P(X,Y|W)，这就是最大似然估计损失函数（“是猫”的概率）\nMAP: W = arg max_W P(W|X,Y) ∝ arg max_W (log P(X,Y|W) + log P(W))，损失函数相比 MLE 多了先验分布 P(W)\n理论上先验分布可以任意选择，只要使用大量数据迭代无限次后，后验分布的最大概率对应的就是真实θ，但如果数据量有限，不同的先验分布会收敛到不同的θ。\n如果采用正态分布 wᵢ~N(0,σ²) 作为先验分布 P(W)，则其对应 L2 正则化项：λ||𝐖||₂+C：\n$$ log P(W) = log ∏_i \\frac{1}{σ\\sqrt{2π}} e^{-\\frac{(w_i-0)²}{2σ²}} = -\\frac{1}{2σ²}∑_i w_i² + C $$\n如果采用拉普拉斯分布 wᵢ~Laplace(0,b) 作为先验分布 P(W)，则其对应 L1 正则化项：λ||𝐖||₁+C：\n$$ log P(W) = log ∏_i \\frac{1}{2b} e^{-\\frac{|w_i-0|}{b}} = -\\frac{1}{b}∑_i |w_i| + C $$\n正态分布和拉普拉斯分布都是限定了高维空间中的向量 𝐖 的位置，L2正则化约束了向量模长（坐标平方和再开方）服从正态分布，L1正则化约束了两向量距离（坐标之差）服从拉普拉斯分布\n如果先验分布 P(W) 是平均分布，概率是常数，与W无关，MAP 就退化成了 MLE\nMAP 比 MLE 多了一个先验分布，先验分布就是正则化项。\n贝叶斯公式描述的是：用新的实验结果对先验分布做修正，先验分布就是优化的起点，不同的先验分布，对应的优化起点不同.\n最大后验估计像是一个损失函数的集合，选择不同的先验分布，相当于选择了不同的损失函数，包括MLE, L1正则化，L2正则化。\nMAP 比 MLE 更正确 有一个人，女性，27岁，985硕士毕业，单身，在上海生活，平时喜欢表达，幽默，有些理想主义，关心少数人群，经常在网上发表犀利言论，问这个人更有可能是一个脱口秀演员呢？ 还是更有可能是一个女性主义的脱口秀演员？\n直觉选择后者，但其实前者概率更大，因为“脱口秀演员”包含的范围更大。\n人的直觉运用了最大似然估计，选择“女性主义的脱口秀演员” 更可能出现描述的那些特质。 如果运用最大后验估计，按照概率乘法即可计算出哪个情况概率较大。\n给定特征集合 D={女性，27岁，985硕士毕业，\u0026hellip;, 发表犀利言论}， 结论集合 T = {t₁=p₁, t₂=p₁^p₂}，其中 p₁ 是脱口秀演员，p₂ 是女性主义脱口秀演员\n最大似然估计： T = arg max_T P(D|T)\n当 T = t₂ 时：P(D|T=t₂) = P(D|p₁^p₂) = P(p₁^p₂|D) P(D) / P(p₁^p₂) = P(p₁,p₂,D)/P(p₁,p₂) = P(p₂|p₁,D) P(D|p₁) P(p₁) / (P(p₁|p₂)P(p₁)) = P(p₂|p₁,D) P(D|p₁) / P(p₁|p₂) = (P(p₂|p₁,D)/P(p₁|p₂)) P(D|T=t₁)\nP(D|T=t₂) 与 P(D|T=t₁) 之间相差一个大于1的系数，所以当 T=t₂ 时似然值更大\n最大后验估计： T = arg max_T P(T|D)\n当 T = t₂ 时：P(T=t₂|D) = P(p₁^p₂|D) = P(D|p₁^p₂) P(p₁^p₂) / P(D) = P(p₁,p₂,D) / P(D) = P(p₂|p₁,D) P(p₁|D) P(D)/ P(D) = P(p₂|p₁,D) P(p₁|D) = P(p₂|p₁,D) P(T=t₁|D)\nP(T=t₂|D) 与 P(T=t₁|D) 之间相差一个小于1的系数，所以当 T=t₁ 时后验概率更大\n所以用贝叶斯方式思考更理性\n用贝叶斯理解概率 概率的两种理解：频率派和贝叶斯派\n频率派认为概率是某件事多次发生的频率，抛硬币可以多次重复，但不适合解释神经网络，(多分类)神经网络蕴含的模型与人脑中的模型的差异是损失函数，被看做为一个概率（“是猫的概率”）。这个概率是是softmax 对各类别上的 activation z 做归一化后得到的，是样本之间的差异，不好用频率解释。\n贝叶斯公式中用的都是概率密度函数，先验概率 P(θ) 理解成在没有数据可供参考时，对神经网络参数的相信程度，相信程度可以是正态分布；后验概率是已知数据X之后，经过修正的对网络参数的相信程度。θ分布修正后，似然度就发生变化，在配分函数中的占比就变化，从而继续修正分布，当某个分布的似然度在配分函数中占比最大时，它就是最优的 θ 分布，其后验概率最大，它发生的可能性最大，对它的相信程度最高。\n分母 P(X) 是边缘概率，也叫配分函数，将其按条件概率展开：\n$$ P(θ_i | X) = \\frac{P(X|θ_i) P(θ_i)}{∑ P(X|θ) P(θ)} = \\frac{P(X|θ_i) P(θ_i)}{\\int_θ P(X|θ) P(θ) dθ}\n= \\frac{P(X|θ_i) P(θ_i)}{P(X|θ_i) P(θ_i) + ∑_{k≠i} P(X|θ_k) P(θₖ) dθ} $$\n分子是具体的一个 θ，分母是把所有可能的 θ 全部取一遍并相加，如果 θ 是连续的，就是积分的过程。\n置信度是 [0,1] 之间的数，但不好求，似然值用梯度下降求，配分函数用蒙特卡洛方法求\n用贝叶斯理解梯度下降 找最优参数的过程拆解成一个序列： P(θ|X) = P(s₁ s₂ s₃ \u0026hellip; sₙ|X)； 给定数据集 X， 先得到 s₁，下降一次得到 s₂，再得到 s₃，下降n次到达 sₙ\n用贝叶斯公式展开：\nP(θ|X) = P(s₁ s₂ s₃ \u0026hellip; sₙ|X) = P(X|s₁ s₂ s₃ \u0026hellip; sₙ) P(s₁ s₂ s₃ \u0026hellip; sₙ) / P(X) = P(s₁ s₂ s₃ \u0026hellip; sₙ, X)/P(X) = P(sₙ | s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X) P(s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X)/P(X)\n= P(sₙ | s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X) P(sₙ₋₁| s₁ s₂ s₃ \u0026hellip; sₙ₋₂, X) \u0026hellip;. P(s₃|s₁,s₂,X) P(s₂|s₁,X) P(s₁|X) P(X)/P(X)\n= P(sₙ | s₁ s₂ s₃ \u0026hellip; sₙ₋₁, X) P(sₙ₋₁| s₁ s₂ s₃ \u0026hellip; sₙ₋₂, X) \u0026hellip;. P(s₃|s₁,s₂,X) P(s₂|s₁,X) P(s₁|X)\n从右向左求。\n假设中间的每一步并不依赖前面的所有结果，只依赖于它前面一步的结果:\nP(θ|X) = P(sₙ | sₙ₋₁, X) P(sₙ₋₁| sₙ₋₂, X) \u0026hellip;. P(s₃|s₂,X) P(s₂|s₁,X) P(s₁|X)\n要求 P(θ|X) 的最大值，计算量还是太大，不是考虑整体乘积达到最大，只考虑局部每一小节，下一步等于上一步的最大值:\n$$s_{i+1} = arg max_{s_{i+1}} P(s_{i+1} | s_i, X)$$\n从第一项到最后一项依次都取最大，最后的结果不一定是整体最大值，但也能得到一个近似的结果。上式就是最大后验估计， 损失函数是：P(sᵢ₊₁ | sᵢ, X)， 则梯度下降为： sᵢ₊₁ = sᵢ + η ⋅ ∇ P(sᵢ₊₁ | sᵢ, X)\n","date":"2022-12-16T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/13_l1l2%E6%AD%A3%E5%88%99%E5%8C%963-%E8%B4%9D%E5%8F%B6%E6%96%AF/","title":"watch: DL - 王木头 13 | L1L2 Reg (3), Bayesian Probability"},{"content":"1. 算法收敛性证明 Source video: P1\nEM \u0026ldquo;期望最大\u0026rdquo; 用于解决具有隐变量的混合模型的参数估计，解决极大似然问题。 对于简单模型的参数估计问题，解析解可以直接求导得到，得不到解析解的用梯度下降或EM求数值解。\nX: random variable, observed data X={x₁,x₂,..xₙ} 各样本独立同分布iid θ: all parameters log P(X|θ): log-likelihood, 对数似然 用极大似然估计： θ_MLE = arg max_θ P(X|θ) ➔ arg max log P(X|θ)\n对于含有隐变量的混合模型，直接求解析解非常困难，比如在 GMM （高斯混合模型）中，就很难写\nEM迭代公式 Z: Latent variable 隐变量也是随机变量，不同值有不同的出现概率 (X,Z): Complete data 完整数据 θ⁽ᵗ⁺¹⁾: t+1 时刻的参数 P(Z|X,θ): Z出现的后验概率 P(X,Z): X 和 Z 同时发生的联合概率。 log P(X,Z|θ): 对数联合概率，对数完全数据 θ⁽ᵗ⁺¹⁾ = arg max_θ ∫_Z log P(X,Z | θ) ⋅ P(Z|X,θ⁽ᵗ⁾) dZ，（很像最大似然估计 log P(X|θ)，只是这里的数据除了X还有隐变量Z，不用梯度下降而用迭代更新参数）\n可以看作是按照 Z 的后验分布 (Z|X,θ⁽ᵗ⁾) 求对数联合概率的期望 （期望的定义-wiki）：\n第一步 Expectation：求对数完全数据 log P(X,Z|θ) 以后验 P(Z|X,θ⁽ᵗ⁾) 为概率密度函数的期望:\n$E_{Z|X,θ⁽ᵗ⁾}[log P(X,Z|θ)]$\n第二步 Maximization：找到期望最大时对应的参数作为下一时刻的参数: $\\rm θ⁽ᵗ⁺¹⁾ = arg\\ max_θ E_{P(Z|X,θ⁽ᵗ⁾)} [log P(X,Z|θ)]$\n收敛性证明 （非严格）只证明一步，从 θ⁽ᵗ⁾ ➔ θ⁽ᵗ⁺¹⁾，似然会增大：log P(X|θ⁽ᵗ⁾) ≤ log P(X|θ⁽ᵗ⁺¹⁾)，事件 X 在新一时刻的参数 θ⁽ᵗ⁺¹⁾ 所代表的概率模型下，发生的可能性变大了，不断迭代最后可取得最大 log-likelihood 对应的参数。\n完全数据的似然：P(X,Z|θ) = P(Z|X,θ) P(X|θ)，θ是起始值可以任意取\n取对数：log P(X|θ) = log P(X,Z|θ) - log P(Z|X,θ)\n两边关于 Z 的后验分布求积分（求对数似然的期望）：\n左边：\n∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(X|θ) dZ = log P(X|θ) ∫z P(Z|X,θ⁽ᵗ⁾) dZ （似然与Z无关提到积分外面）\n= log P(X|θ) （对Z的概率积分为1）\n右边：\n∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(X,Z|θ) dz - ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(Z|X,θ) dz = Q(θ, θ⁽ᵗ⁾) - H(θ,θ⁽ᵗ⁾)，分别分析两项\nQ 存在于EM迭代公式中，根据EM 的定义：θ⁽ᵗ⁺¹⁾ 对应最大的Q，所以： Q(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) ≥ Q(θ, θ⁽ᵗ⁾) 是成立的。 上式的θ是初始值，若令θ = θ⁽ᵗ⁾，则 Q(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) ≥ Q(θ⁽ᵗ⁾, θ⁽ᵗ⁾) 得证\n因为 H 前面有个负号，所以要证 H(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) ≤ H(θ⁽ᵗ⁾, θ⁽ᵗ⁾)\n后 - 前时刻的 H：\nH(θ⁽ᵗ⁺¹⁾, θ⁽ᵗ⁾) - H(θ⁽ᵗ⁾, θ⁽ᵗ⁾) = ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(Z|X,θ⁽ᵗ⁺¹⁾) dz - ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(Z|X,θ⁽ᵗ⁾) dz = ∫z P(Z|X,θ⁽ᵗ⁾)⋅log (P(Z|X,θ⁽ᵗ⁺¹⁾ / P(Z|X,θ⁽ᵗ⁾)) dz （合并） = - KL( P(Z|X,θ⁽ᵗ⁾) || P(Z|X,θ⁽ᵗ⁺¹⁾)) （相对熵≥0） ≤ 0\n(另一种证法) 用Jensen不等式，而不使用相对熵。\n因为log是 concave （凹）函数：在曲线上任意取两个点，连成的直线小于log函数。concave函数的性质：a,b两点之间的一点c（两端点的线性组合）对应到直线函数上的值一定小于对应到log函数上的值\n上面的合并为 1个log 之后的式子，可以看作是先求Z的后验的对数，再求\u0026quot;对数值\u0026quot;的期望，一定小于先求（自变量\u0026quot;Z的后验\u0026quot;的）期望再求对数：（E[log x] ≤ log E[x]）\n∫z P(Z|X,θ⁽ᵗ⁾)⋅log (P(Z|X,θ⁽ᵗ⁺¹⁾ / P(Z|X,θ⁽ᵗ⁾)) dz ≤ log ∫z P(Z|X,θ⁽ᵗ⁾)⋅(P(Z|X,θ⁽ᵗ⁺¹⁾ / P(Z|X,θ⁽ᵗ⁾))) = log ∫z (P(Z|X,θ⁽ᵗ⁺¹⁾) dz = log 1 = 0\n2. 迭代公式的导出 Source video: P2\nEM 迭代公式来自于将对数似然 logP(X|θ) 分解为 ELBO + KL散度。\n引入变量Z，则 P(X) 就变成了联合概率：P(X,Z|θ) = P(Z|X,θ)⋅P(X|θ)， 所以对数似然就变为：\nlogP(X|θ) = log P(X,Z|θ) - log P(Z|X,θ)\n引入 Z 的概率分布 q(Z): logP(X|θ) = log P(X,Z|θ) - log q(Z) - log P(Z|X,θ) + log q(Z) = log (P(X,Z|θ) / q(Z)) - log (P(Z|X,θ) / q(Z))，q(Z)≠0\n技巧：对等式两边，按照分布 q(Z) 求似然的期望（积分）：\n左边：\n∫z q(Z)⋅log P(X|θ) dZ = log P(X|θ)⋅∫z q(Z) dZ = log P(X|θ)，q(Z)是概率密度函数积分为1，因此左边求完期望无变化，所以求似然 logP(X|θ) 就变成了求q(Z)。\n右边：\n∫z q(Z)⋅log (P(X,Z|θ)/q(Z)) dz - ∫z q(Z)⋅log (P(Z|X,θ)/q(Z)) dz = ELBO + KL(q(Z) || P(Z|X,θ)) 其中第1项：Evidence Lower Bound (ELBO,下界)，第2项是Z的概率密度函数q(Z)与Z的后验概率P(Z|X,θ)的相对熵。\n因为 KL散度恒≥0，所以样本似然 log P(X|θ) ≥ ELBO。只有当 Z 的概率分布 q(Z) 与 Z 的后验分布 P(Z|X,θ) 相等时，KL散度等于0，似然=ELBO。\nELBO 是 log (P(Z,X|θ)/q(Z)) 按照 q(Z) 求期望（加权和;求积分）。当对数似然=ELBO，即达到最大时，q(Z)=P(Z|X,θ⁽ᵗ⁾)，也就是先用上一时刻的θ⁽ᵗ⁾ 求出q(Z)，然后ELBO里就只有log里的θ是变量，滑动调整θ使ELBO最大，取对应的θ作为θ⁽ᵗ⁺¹⁾。\nEM 想让 ELBO 达到最大，先（通过KL散度=0）找到样本对数似然 log P(X|θ) 的最大值对应的参数θ，作为新的θ，然后再算它对应的期望并得到ELBO， 取到最大值时的形式，即 ELBO， 然后取ELBO最大时对应的参数θ； 不断提高下界从而让对数似然 log P(X|θ) 也逐渐变大，最终ELBO等于logP(X|θ)，KL散度=0，Z的概率分布与它的后验分布相等， 最优θ^ 是 ELBO 取最大时的θ\nθ^ = arg max_θ ELBO = arg max_θ ∫z q(Z) ⋅ log (P(X,Z|θ) / q(Z)) dz ，代换q(Z)\n= arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ log (P(X,Z|θ) / P(Z|X,θ⁽ᵗ⁾)) dZ\n这个式子与 EM 的迭代公式相比，log 里多了一个分母：P(Z|X,θ⁽ᵗ⁾)，展开log：\nθ^ = arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ [log P(X,Z|θ) - log P(Z|X,θ⁽ᵗ⁾)] dZ = arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ log P(X,Z|θ) dZ - P(Z|X,θ⁽ᵗ⁾) ⋅ log P(Z|X,θ⁽ᵗ⁾)] dZ\n其中第2项与θ无关，因为 θ⁽ᵗ⁾ 是上一时刻的参数，是个常数，不是变量，而 log 中的 θ 是变量，会变到ELBO 取最大时对应的参数。所以就得到了迭代公式：\nθ^ = arg max_θ ∫_Z P(Z|X,θ⁽ᵗ⁾) ⋅ log P(X,Z|θ) dz\n3. 公式导出之ELBO+Jensen\u0026rsquo;s Inequality Source video: P3\nEM 迭代公式也可来自于将对数似然 logP(X|θ)分解为 ELBO + Jensen\u0026rsquo;s Inequality\nJensen\u0026rsquo;s Inequality 结论：对于一个凹concave 函数 f(x)，在定义域x上取两点：a,b 连线小于a,b之间的函数值。 任意在 a,b 之间取一点 c = t⋅a+(1-t)b, where t∈[0,1]，f(c)=f(t⋅a+(1-t)b) ≥ t⋅f(a) + (1-t)f(b)。\n比如当 t=½ 时，f(a/2+b/2) ≥ f(a)/2 + f(b)/2，两边都是期望（平均数,加权和），简记为:先求期望再求函数值 大于等于 先求函数值再求期望，f(E) ≥ E[f]。当 f(x) 是常函数时，等号成立。\nlog P(X|θ) = log ∫zP(X,Z|θ) dz，在似然中引入隐变量 Z，然后求X的边缘概率，即对Z求积分（“把最终结果中不需要的事件合并成其事件的全概率而消失” marginal probability-wiki）\n= log ∫z ( P(X,Z|θ) / q(Z)) * q(Z) dz，引入 Z 的分布 q(Z) = log E_q(z) [P(X,Z|θ)/q(Z)]，把积分看作求期望 ≥ E_q(z) [ log(P(X,Z|θ)/q(Z)) ]，Jensen不等式等号在 P(X,Z|θ) / q(Z)=C 成立, log C 是常函数\n这个期望 E_q(z) [ log(P(X,Z|θ)/q(Z)) ] 就是对数似然的下界，就是ELBO。\nq(Z) = P(X,Z|θ) / C ∫z q(Z) dZ = 1 = ∫z P(X,Z|θ) / C dZ = 1/C ∫z P(X,Z|θ) dZ = 1/C P(X|θ) （求边缘概率）\nC = P(X|θ) 把 C 代换：q(Z) = P(X,Z|θ) / P(X|θ) = P(Z|X,θ)\n所以当 Jensen 不等式取等号时，Z的分布 q(Z) 就是 Z 的后验分布P(Z|X,θ)。\n所以 EM 第一步先按照上一时刻的参数 θ⁽ᵗ⁾ 和数据 X 求出 Z 的后验分布 P(Z|X,θ⁽ᵗ⁾)， 再将对数似然 log(P(X,Z|θ)/P(Z|X,θ⁽ᵗ⁾)) 按照 Z 的后验分布求期望，得到对数似然的下界ELBO， 这个下界是关于 θ 的函数: ∫z P(Z|X,θ⁽ᵗ⁾)⋅log P(X,Z|θ) dZ，所以可找到这个下界最大时对应的θ，作为θ⁽ᵗ⁺¹⁾。不断迭代提高下界（期望），从而提高对数似然\n4. 再回首 Source video: P4\nEM 是解决优化问题的迭代算法（和梯度下降是一个level），而HMM，GMM 是模型\n从之前狭义的（理想的）EM 推广到广义的（一般的）EM 狭义的EM 是广义EM 的一个特例 EM 的变种 EM 主要用于概率生成模型，数据（随机变量）包括观测数据 X 及其对应的隐变量 Z，所以(X,Z) 叫做完全数据complete data，θ 是概率模型的参数。Z 是建模时引入的，Z生成了X，我们只能观测到X。 比如GMM 中，假设 z 是一个离散的分布，比如 K 个类别 z=1,2,\u0026hellip;,K，每个类别都有一定的概率：\n隐变量 z 1 2 \u0026hellip; K 概率密度 p₁ p₂ \u0026hellip; pₖ 然后在 z 给定的情况下，x 满足高斯分布：P(x|z) ~ Gaussian，因此对完整数据 P(X,Z) 建模\n还有 HMM 也可以从生成的角度来解释。比如 N 个隐变量: z₁, z₂, \u0026hellip;, zₙ 是马尔科夫链的结构:\n$$ s1 ➔ s2 ➔ \u0026hellip; ➔ sₙ \\\\ ↧ \\quad\\ ↧ \\quad \u0026hellip; \\quad ↧ \\\\ x1 \\quad x2 \\quad \u0026hellip; \\quad xₙ $$\n可以把 z₁, z₂, \u0026hellip;, zₙ 看成是统一的变量 Z，把 x₁, x₂, \u0026hellip;, xₙ 看成是一个X\n观测到了 X，假设它的概率模型的参数是θ，就可以用 EM 来估计参数 θ。\n使用MLE 来估计参数：θ^ = arg max P(X|θ) = arg max ∏ᵢ₌₀ᶰP(xᵢ|θ) ➔ arg max log P(X|θ)\n不能直接求解这个最大化问题的原因是，不知道P(X)，因为样本X是非常复杂的，所以“会引入自己的归纳偏置，假定它是服从某个模型的。”\n生成模型就是假设每个样本 x⁽ⁱ⁾ 都有一个隐变量 z⁽ⁱ⁾，x⁽ⁱ⁾ 是由 z⁽ⁱ⁾ 生成的，所以P(X) 就变成了联合分布 P(X,Z)（即把X分解处理），然后把 Z 积分掉就可以了: P(X) = ∫z P(X,Z) dZ\nnote-苏剑林-VAE\n5. 广义EM Source video: P5\nEM 用于解决参数估计问题，优化函数使用 MLE ，找到让对数似然 log P(X|θ) 达到最大的参数 θ。 样本X 的分布 P(X) 未知，所以假设每个 x 是由隐变量 z 生成的。比如 GMM 假设 z 服从离散的概率分布，HMM 假设 z 满足马尔科夫链。然后 P(X) 就等于联合概率分布P(X,Z) 比上Z的后验分布P(Z|X)，也就是未知X分布经过生成模型的假设，把问题具体化了。\n因为 P(X|θ) = P(Z,X|θ) / P(Z|X,θ)，所以（复杂的未知的）对数似然目标函数可分解为: 下界ELBO+Z的分布q(Z)与Z的后验分布P(Z|X,θ)的KL散度: log P(X|θ) = ELBO + KL(q||P)\nELBO 是一个期望，它和q(Z) 和 θ 有关，所以将其记为 L(q,θ); KL\u0026gt;=0, 当q=P时，KL=0，所以：log P(X|θ) ≥ L(q,θ)，而且最优参数 θ^ 是在 q^= P(Z|X,θ) 时取到。\n但是 q^ 并不一定能取到 P(Z|X,θ)，因为这个后验可能是 intractable，是算不出来的。这由生成模型的复杂度决定， 如果生成模型比较简单，比如GMM的 z 和 HMM 的 z，他们是离散的，结构化的，是tractable，是可以（用EM）计算出来的\n但是对于绝大多数的 z 是无法求出他的后验 P(z|x)。比如 VAE 的 z 是高维的，无法把它从 P(x,z) 中积掉，就得不到 P(x)，也就无法（用贝叶斯公式）得到后验 P(z|x)，所以最优的 q^(Z) 取不到，所以就需要变分（近似）推断：重参数化技巧+神经网络梯度下降用 q_ϕ(z|x) 逼近 P_θ(z|x)。\n数学表达： 当 θ 固定 的时候，对数似然 log P(X|θ) 就是固定的，然后当 q(Z) 越接近 Z 的后验分布 P(Z|X,θ)，KL散度就越小，同时 ELBO 就越大。所以求最优的分布 q(Z) 就变成一个优化问题： q^(Z) = arg min_q KL(q(Z) || P(Z|X,θ)) = arg max_q L(q,θ)\n当 q^固定 的时候，再做极大似然找 θ，也就是做“狭义”的EM，最优的 θ^= arg maxᶱ L(q^, θ)\n广义EM（两个最大化问题）:\nE-step: q⁽ᵗ⁺¹⁾ = arg max_q L(q,θ⁽ᵗ⁾)，固定θ求最优的q^ M-step: θ⁽ᵗ⁺¹⁾ = arg maxᶱ L(q⁽ᵗ⁺¹⁾, θ)，固定q^求最优的θ 对 ELBO 做变形（展开log）：\nL(q,θ) = E_q(z) [log (P(Z,X|θ)/q(Z))] = E_q(z) [ log P(Z,X|θ) - log q(Z) ]\n= E_q(z) [log P(Z,X|θ)] - E_q(z) [log q(Z)] = E_q(z) [log P(Z,X|θ)] + H[q(Z)]\n其中第2项 H[q(Z)] 是分布 q(Z) 的熵: ∫ q(Z)⋅log (1/q(z)) dz ， 所以 ELBO = 完全数据似然按照 q(Z) 求期望+ q(Z) 的熵\n之前的EM 是广义EM 的一个特殊情况。对于 E-step, 狭义EM 默认 q 直接就取到了后验 P(Z|X,θ⁽ᵗ⁾)，因为假定了后验能够求出。对于 M-step, 狭义EM 认为 q^ 已经找到了，那么广义EM 的M-step 中的熵就是确定值，要优化的只有似然。\n6. EM 的变种 Source video: P6\n（标准的，一般指的）广义的EM：对似然下界(ELBO联合概率按照z的后验求期望) L(q,θ) 求两次最大化，先固定θ⁽ᵗ⁾求q⁽ᵗ⁺¹⁾，然后固定q⁽ᵗ⁺¹⁾ 求 θ⁽ᵗ⁺¹⁾\n因为两步都是求 Maximum，所以 EM 也称 MM (Maximation Maximation)\n对于两个参数，先固定一个求另一个，再反过来，这种算法是坐标上升法，比如SMO。如果参数是多维的，固定其中某一个/两个，然后去求其他的。求参数的顺序没关系。\n坐标上升法 与 梯度上升法并列\n损失函数的等高线如下图，梯度上升法的参数路径是沿着梯度的，而坐标上升法类似曼哈顿距离，\n如果 E-step 中最优的 q^(Z)，也就是Z 的后验P(Z|X,θ) 无法求得，就可以用变分推断求近似最优，比如基于平均场理论的变分法近似后验分布，再做 M-step，称这个组合为VBEM 变分贝叶斯EM。 如果用蒙特卡罗采样去求近似后验分布，叫作MCEM，蒙特卡罗EM。\nVI（变分推断） 和 VB （变分贝叶斯）指的是同一个东西\n","date":"2022-12-16T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/10-em%E7%AE%97%E6%B3%95/","title":"watch: ML - 白板 10 | EM Algorithm"},{"content":"2连咸饼 ➔ P(袋子) 从同一袋子里连续取出了2个咸饼干，问最有可能是以下5个袋子中的哪个？\n饼干比例: 100%甜 75%甜25%咸 50%甜50%咸 25%甜75%咸 100%咸 MLE 0 0.0625 0.25 0.5625 1 设从每个袋子中取出咸饼干的概率是p，则MLE 只需对每个袋子求p²，则第5个袋子的似然最大。而MAP还要考虑各袋子出现的概率g，所以MAP函数为 p²×g，可得第4个袋子的后验最大\n先验g (袋子出现的概率) 0.1 0.2 0.4 0.2 0.1 袋中咸饼干比例 θ 0 0.25 0.5 0.75 1 事件 X:二连咸(MLE) 0 0.0625 0.25 0.5625 1 MAP 0 0.0125 0.1 0.1125 0.1 后验 0 3.85% 30.8% 34.6% 30.8% 因为这里的样本空间由两个随机变量组成：事件X和袋子概率g，所以P(X)就是在所有θ可能的情况下X发生的概率和(0.325)。因为P(X)是个常数，所以做 MAP 只看分子就行。\n男生 ➔ P(打lol) 一个男生，求他打lol的概率\n先验g 60% 40% 男女比例 θ 80%男20%女 20%男80%女 事件X:1男 (MLE) 0.8 0.2 事件X:1男 (MAP) 0.48 0.08 后验 85.7% 14.3% 9正1反 ➔ P(硬币) 抛10次硬币9正1反，求这枚硬币朝上的概率? 【机器学习我到底在学什么】哲学角度聊聊贝叶斯派和频率派，数学角度看看极大似然估计和最大后验估计\n频率派会说是0.9，因为这样对应\u0026quot;抛出9次朝上\u0026quot;的概率最大； 贝叶斯派依据贝叶斯公式，带入先验概率和实验结果，认为向上概率落在 0.5~0.9 之间的概率最大: 比如参数θ\u0026rsquo;=0.5 的先验概率是 P(θ\u0026rsquo;)=0.8, 把θ\u0026rsquo;=0.5 带入二项分布可算出事件9正1反发生的概率：P(X|θ\u0026rsquo;)=C₉¹⁰⋅(0.5)⁹⋅(0.5)¹=0.00976. 所以贝叶斯公式的分子=0.8×0.00976=0.00781。 而分母 P(X) 应为 θ 从0取到1时，各种0对应的事件X:9正1反发生的似然之和，但从0到1 是不可数的，无法得到解析解，可以做数值模拟。下面的例子只假设样本空间只有 3 中参数θ 的取值：0.5, 0.9, 0.4。\n先验 g 80% 1% 19% 参数 θ 50%上50%下 90%上10%下 40%上60%下 事件 X C₉¹⁰⋅(0.5)⁹⋅(0.5)¹ C₉¹⁰(0.9)⁹(0.1)¹ C₉¹⁰(0.4)⁹(0.9)¹ MLE 0.00976 0.387 0.00236 MAP 0.00781 0.00387 0.000448 后验 64.4% 31.9% 3.7% 上表中 MLE = P(X|θ), MAP = P(X|θ)P(θ), 而“事件X发生，参数取到θ的概率”: P(θ|X) = MAP/P(X)，其中 P(X) 是事件X在样本空间中发生的概率，应为当参数取各种 θ 时，事件X 发生的条件概率 之和，也就是各种θ 对应的 MAP 求和。\n感觉不太恰当 甲地下雨了，求乙地下雨概率 条件概率 例题\n特征 ➔ P(嫁) 数据挖掘实验-Naive Bayes\n","date":"2022-12-15T23:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/map_examples/","title":"MAP examples"},{"content":"Code | Arxiv (2010)\nMain steps (pseudo-code) of GRF\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 train() ├ load_data() ├ create_model() │ └─ render_kwargs_train{network_query_fn, model, ...} │ (train cycle) └ for i in range(start, 1000000): │ (pick args.N_rand rays) ├─ batch_rays:ro+rd, target_s:rgb, attention_images:(NS,H,W,3), attention_poses:(NS,3,4) ├─ render(batch_rays, attention_images, attention_poses, intrinsic, ..., unet_model, **render_kwargs_train) │ ├─ rays:ro+rd+n+f+viewdirs # (args.N_rand,8) │ ├─ img_feats = unet_model( Eembedd(attention_imgs+cam_center+rotation)) # (NS,H,W,33+128) │ └─ batchify_rays(rays, chunk, attention_poses, intrinsic, img_feats **kwargs) │ │ │ (for loop: args.chunk) │ │ ├─ render_rays(chunk-rays, **kwargs) │ │ │ ├─ pts: sample along rays, (args.chunk,args.N_samples,3) │ │ │ ├─ raw,attn_cache = network_query_fn(pts, viewdirs, network_fn, attention_poses, intrinsic, training, img_feats) │ │ │ │ └─ run_network(pts, viewdirs, network_fn, embed_fn, ...) │ │ │ │ ├─ emb_coord: PosEnc(xyz; dirs)=90 │ │ │ │ └─ batchify(nerf_attention_model, args.netchunk, ndc2world(), gather_indices()) │ │ │ │ └─ ret(emb_coord, attention_poses, intrinsic, img_feats, pts) │ │ │ │ │ (for loop: args.netchunk) │ │ │ │ ├─ NS_feats=gather_indices(world_xyz, attention_poses, intrinsic, img_feats) # PosEnc(rgb)+img_feats+pixloc=163 │ │ │ │ └─ nerf_attention_model(netchunk_emb_coord, NS_feats, netchunk_pts) │ │ │ │ ├─ pts_feats = slot_att(NS_feats+PosEnc(xyz)) #get (n_pts,256) │ │ │ │ └─ nerf_model(pts_feats + netchunk_emb_coord) │ │ │ │ └─ return: raw, pts_feats #(n_pts,4),(n_pts,256+90) │ │ │ ├─ rgb_map_0, disp_map_0, acc_map_0 = raw2outputs(raw,z_vals,rays_d) │ │ │ ├─ raw_fine,_= network_query_fn(pts, viewdirs, network_fine, ...) │ │ │ └─ rgb_map, disp_map, acc_map = raw2outputs(raw_fine,z_vals,rays_d) │ │ └─ all_ret{} │ └─ return: rgb, disp, acc, extracs ├─ loss = img2mse(rgb, target_s) ├─ img2psnr(loss) ","date":"2022-12-15T13:49:00Z","image":"https://github.com/alextrevithick/GRF/raw/main/imgs/process.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-grf/","title":"read: GRF"},{"content":"1 Peak signal-to-noise Ratio 两幅大小为m×n的单色图，一幅是原图I，另一幅是近似图像K。 两者之间的平均平方误差Mean Squared Error是：\n$$ MSE = \\frac{1}{mn} \\sum_{i=1}^m \\sum_{j=1}^n\\ [I(i,j) - K(i,j)]^2 $$\n则峰值信噪比定义为： PSNR = 10 log₁₀ (MAXᵢ² / MSE)\nMAXᵢ 是原图中可能的最大像素值，当一个像素用8 bits表示时，MAXᵢ=255\n为什么是这样子？\n常用于衡量有损压缩编解码器的重建质量。 wikipedia\nMSE越小，PSNR越大。当两图没有误差时，PSNR趋于无穷\n2 Structural SIMilarity Index 用于预测数字图像的可感知质量 perceived quality SSIM ² is used to measure the distortion degree of an image, or measure the similarity between two images from 3 aspects: Luminance, Contrast, and Structure.\nRatio of luminance l(X,Y) = 2μₓμᵧ+C₁/(μₓ²+μᵧ²+C₁), where μₓ is the mean of the intensity of all pixels in image X: μₓ = 1/N ∑ᵢ₌₁ᴺ Xᵢ. And C₁ prevents the divisor from being 0.\nBased on AM-GM inequality: (√x - √y)²≥0, if and only if μₓ=μᵧ, l(X,Y)=1.\nRatio of contrast c(X,Y) = 2σₓσᵧ+C₂/(σₓ²+σᵧ²+C₂), where σₓ is the (non-bias) standard deviation of the intensity of all pixels in the image X: σₓ = (1/(N-1) ∑ᵢ₌₁ᴺ (Xᵢ-μₓ)²)¹ᐟ². If and only if σ₁=σ₂, c(X,Y)=0.\nStructure is reflected by correlation coefficient: s(X,Y) = σₓᵧ+C₃/(σₓσᵧ+C₃), where σₓᵧ is the covaraiance of the intensity of 2 images: σₓᵧ= 1/(N-1) ∑ᵢ₌₁ᴺ (Xᵢ-μₓ)(Yᵢ-μᵧ).\nThe intent of the paper may be to measure the information aside from Luminance and Contrast ⁵, so each pixel is reduced by the mean and devided by stddev: (Xᵢ-μₓ)/σₓ and (Yᵢ-μᵧ)/σᵧ, i.e., normalization, and then compute the distance between corresponding pixels in 2 images by inner product.\nThus, S(X,Y) is 1/(N-1) ∑ᵢ₌₁ᴺ [(Xᵢ-μₓ)/σₓ ⋅ (Yᵢ-μᵧ)/σᵧ ] = σₓᵧ/σₓσᵧ\nThe final expression is the product of the above 3 features with specified power (weights) α β γ:\nS(X,Y) = l(X,Y)ᵅ ⋅ c(X,Y)ᵝ ⋅ s(X,Y)ᵞ ∈ [-1,1]\nwhere l(X,Y), c(X,Y), s(X,Y) ∈ [-1,1], and since brightness ≥0, the actural range of l and c are (0,1]. If and only if the images X and Y are the same, the 3 items are equal to 1 at the same time.\nLet α, β, γ=1, the SSIM(X,Y) = $\\frac{ 2(μₓμᵧ+C₁) (2σₓᵧ + C₂) }{(μₓ²+μᵧ²+C₁) (σₓ²+σᵧ²+C₂)}$, where C₁ = (K₁L)², C₂ = (K₂L)², C₃=C₂/2, and L is the maximum for a pixel (L=2^b). Based on the rule of thumb, K₁ = 0.01, K₂=0.03.\nIn practice, SSIM is not performed on the entire image, but calculating the mean and stddev in a local sliding window (kernel, filter of 11x11 with stddev=1.5, sum=1), which represents a circular-symmetric Gaussian Weighting Function.\nHence, the local mean and std-dev are confined within the kernel:\nμₓ = ∑ᵢ wᵢ Xᵢ σₓ = ∑ᵢ wᵢ (Xᵢ-μₓ)²)¹ᐟ² σₓᵧ= ∑ᵢ wᵢ (Xᵢ-μₓ)(Yᵢ-μᵧ) where wᵢ is the parameters of the Gaussian kernel smoothing the image, such that fine-details are smeared and compare mainly the general features.\nAt the end, the average of all local SSIM is M-SSIM(X,Y) = 1/M . ∑ⱼ₌₁ᴹ SSIM(Xᵢ, Yⱼ)\nUsed as loss Since the SSIM measures the similarity between 2 images, it can be used in the supervised training. Thus, SSIM dissimilarity, SSIMD = (1-SSIM)/2 ∈ (0,] is a kind of loss function.\n有损压缩Lossy compression两种基本机制： wikipedia\n有损变换编解码：对图像/声音采样， 预测编解码： 3 LPIPS The motivation of Learned Perceptual Image Patch Similarity (LPIPS) ³ is that: the conclusion for the similarity between 2 images from deep neural networks are aligned with humans perception. But the structure-based metrics usually give opposite judgement on the too-smoothed images ⁵. (2023-02-18)\nLPIPS compares the intermediate convolutional feature vectors at different levels.\nlpips.LPIPS(net=\u0026quot;alex\u0026quot;) 把两幅图输入神经网络 (VGG, Alexnet) 进行多层级特征间的对比。 每层输出的 feature mapᴴˣᵂˣᶜ 激活后归一化，再相减，将各层差异按像素加权求和，并(除以像素个数)做spatial 平均，再把各层差异加起来 ⁴\nRef Image quality assessment: from error visibility to structural similarity The Unreasonable Effectiveness of Deep Features as a Perceptual Metric LPIPS图像相似性度量标准_Alocus_的博客-CSDN博客_lpips度量 NeRF常用评价指标都是什么意思？PSNR、SSIM、LPIPS详解 - 意の茗 - bilibili ","date":"2022-12-08T12:59:00Z","permalink":"https://zichen34.github.io/writenotes/vis/metrics/","title":"memo: Vis | Visual Metrics"},{"content":"Code | Arxiv (2103) | ProjPage\n(2023-08-16) Re-read\nExperiments Train model only on (MVSNet) DTU dataset, where the objects are partitioned the same as PixelNeRF (imgsize: 300x400).\nDatasets (Download):\nStage Data Contents Resolution N_views Size Train DTU 88 scenes 512x640 19G Test DTU 16 scenes 3/20 Test LLFF Test Blender Feature map: 32 channels Cost volume: 128 planes MLP: 6 layers Coarse-to-fine: One field and fine-tune Ray pts: 128 Device: 2080Ti Batch size: 1024 rays Optimizer: Adam, lr=5e-4 Old Notes Old notes on 2022-12-06 Abstract\n3 nearby input views plane-swept cost volumes geometry-aware scene reasoning generalize across scenes 1 Introduction\nTopic: Novel view synthesis\nRecent progress: neural rendering\nFormer solutino and drawbacks:\nNeRF and its following works can produce photo-realistic novel view synthesis results. But they need a long-time per-scene optimization Own Solution:\nGoal: use \u0026hellip; to \u0026hellip; 1 sentences introduce the name and functionality, properties. Analysis: generalizability -\u0026gt; avoid tedious per-scene optimization and regress directly novel viewpoints quantitative outcomes Tech stack\nMVSNet -\u0026gt; generalizable net of 3D reconstruction Cost volume is built by warping 2D img features of src views onto sweeping planes Regress geometry and appearance from a cost volume (per-voxel neural features) 3D cnn aggregates the cost volumes to a neural scene encoding volume (2023-08-16)\nPlay Environment inplace-abn needs to be installed from source, as issue#36. But I cannot install it:\n1 2 3 RuntimeError: The detected CUDA version (10.2) mismatches the version that was used to compile PyTorch (11.3). Please make sure to use the same CUDA versions. My cuda version is 11.3 as shown by:\n1 2 import torch print(torch.version.cuda) Then I specified the version the same as the pl-tutorial of AIkui:\n1 2 3 4 5 6 - pip - pip: - torch==1.11.0 - torchvision==0.12.0 - pytorch_lightning==1.6.0 - inplace_abn With that, inplace_abn has installed.\nBut the API has change since v1.5 (as issue) and resulted in: Exception has occurred: TypeError __init__() got an unexpected keyword argument 'distributed_backend'\nTry this setting:\n1 2 3 4 - torch==1.10.1 - torchvision==0.11.2 - pytorch_lightning==1.3.5 - inplace_abn Cannot import inplace_abn:\n1 2 Exception has occurred: ImportError /home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/inplace_abn/_backend.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c1015SmallVectorBaseIjE8grow_podEPvmm I checked my cuda version, which is 10.2:\n1 2 3 4 5 (mvsnerf) zichen@lambda-server:~/Downloads/mvsnerf-comments$ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Wed_Oct_23_19:24:38_PDT_2019 Cuda compilation tools, release 10.2, V10.2.89 According to Previous PyTorch Versions | PyTorch, I install the versions compiled with cuda10.2. Then print(torch.version.cuda) returned 10.2 instead.\n1 2 3 4 5 6 7 8 - pip - pip: - torch==1.10.1+cu102 - torchvision==0.11.2+cu102 - torchaudio==0.10.1 - -f https://download.pytorch.org/whl/cu102/torch_stable.html - pytorch_lightning==1.3.5 - inplace_abn Still cannot import inplace_abn with the same ImportError as above:\nThen Install from source, and inplace_abn can be installed:\nAnother Error:\n1 2 3 4 Exception has occurred: ImportError cannot import name \u0026#39;get_num_classes\u0026#39; from \u0026#39;torchmetrics.utilities.data\u0026#39; (/home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/torchmetrics/utilities/data.py) File \u0026#34;/home/zichen/Downloads/mvsnerf-comments/train_mvs_nerf_pl.py\u0026#34;, line 17, in \u0026lt;module\u0026gt; from pytorch_lightning.callbacks import ModelCheckpoint Change to specific versions, as summary:\n1 2 pip install torchmetrics==0.5.0 pip install setuptools==59.5.0 Numpy version incompatible issue#77\n1 ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part. File \u0026ldquo;/home/zichen/Downloads/mvsnerf-comments/data/dtu.py\u0026rdquo;, line 98, in build_proj_mats self.proj_mats, self.intrinsics = np.stack(proj_mats), np.stack(intrinsics)\n","date":"2022-12-06T14:19:00Z","image":"https://ar5iv.labs.arxiv.org/html/2103.15595/assets/x2.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-mvsnerf/","title":"read: MVSNeRF"},{"content":"Homography warps two images to a common plane, or maps a pixel from a camera plane to another camera plane.\nA pixel can be mapped to another camera because the 2 pixels are the projections of a common 3D point.\nHomography warps the images captured from different angles, observing a common plane at a certain depth, to a unified viewpoint for stitching or other applications.\nA homography corresponds to only one depth. If the image doesn\u0026rsquo;t only capture a plane at a certain depth, the part with a deviated depth will be distorted in the warped image. Thus, solving homographies requires specifying depth.\nHomography relation is built based on the pose transformation between two cameras, and a depth embodied by a co-observed point or a co-observed plane. Or it can be sovled from 4 pairs of matched pixels.\nSpecifically, two cameras are connected via the 2 camera-space coordinates conversion for a depth-known co-observed 3D point.\nThe conversion can be represented by two camera\u0026rsquo;s poses in the world space, or the direct transformation from one camera to the other.\nThe depth of the co-observed 3D point can be given by specifying either the z-coordinate directly or a plane (𝐧,d) indirectly.\n(2023-12-19) Two scenarios for homography: stitching images based on a co-observed plane, and transferring viewpoint for a plane surface. They both rely on the observed plane.\nTwo Cameras The homography matrix is solved based on matching points in a pair of images.\n𝐲 ₁ c a W m o 𝐏 1 r ₁ l d s 𝐇 p a 𝐐 c e 𝐏 c ₂ a m 2 𝐲 ₂ P 𝐱 2 r o t p j o l e a c n t e s $$ 𝐲₁ = 𝐏₁ ⋅ 𝐐 \\\\ 𝐲₂ = 𝐏₂ ⋅ 𝐐 \\\\ 𝐲₂ = 𝐏₂ ⋅ 𝐏₁⁻¹ * 𝐲₁ $$\n𝐏 is the projecting operation that converts a world coordinates of a point 𝐱 to pixel coordinates 𝐲.\n$𝐏 = 𝐊[𝐑|𝐭]$ and $𝐏⁻¹ = [𝐑|𝐭]⁻¹𝐊⁻¹$\n𝐏₂ ⋅ 𝐏₁⁻¹ is the homography matrix.\nBesides 𝐊[𝐑|𝐭], the z-coordinate of covisible point 𝐐 (or the \u0026ldquo;distance from camera center to plane\u0026rdquo; d) is necessary.\np l a n e 𝐧 C c a e m n d e t : : : r e a r 𝐐 ( x , y , z ) Since the inverse intrisics $𝐊⁻¹ ⋅(u,v,1)ᵀ$ solely can\u0026rsquo;t determine a unique 3D point as z has been canceled when $(u,v,1)ᵀ=\\frac{1}{z}⋅𝐊⁻¹ ⋅ (x,y,z)ᵀ$, the z must be specified at first. 5\n$$ (x,y,z)ᵀ = 𝐊⁻¹ ⋅z ⋅(u,v,1)ᵀ $$\nIf assuming camera\u0026rsquo;s z-axis is perpendicular to the observed plane, then z is indeed $d$. Thus, the z-retained coordinates of the pixel 𝐲₁ is (ud,vd,d), such that $𝐊⁻¹ ⋅d⋅(u,v,1)ᵀ$ is a unique 3D point in the camera space.\nRecall the NeRF\u0026rsquo;s sampling points (get_rays_np): meshgrid as pixel coordinates → divided by focal (with z=1) to be in camera space → @c2w to be in world space → z vals sampling\nThus $𝐊⁻¹ ⋅(u,v,1)ᵀ$ only determines the viewdir, not a 3D point.\nThen, roll back it to world space and project to the other camera to get the pixel 𝐲₂.\n$$ 𝐲₂ = 𝐊₂[^{𝐑₂ \\quad 𝐭₂}_{0 \\quad 1}] [^{𝐑₁ \\quad 𝐭₁}_{0 \\quad 1}]⁻¹𝐊₁⁻¹ ⋅d⋅𝐲₁ $$\nOverview: Plane1 → camera1 → world space → camera 2 → plane 2\nHowever, if either 𝐐\u0026rsquo;s $z$ (or the distance $d$), or poses of two cameras is unknown, the 3x3 homography matrix can be solved from 4 pairs of matching pixels.\n(2023-12-04)\nBasis Changes Refer to Planar homographies - Department of Computer Science, University of Toronto\nIn a \u0026ldquo;meta space\u0026rdquo; holding two cameras, for example the world space, a ray emitted from the world origin passed through 2 world points 𝐦, 𝐧 locating on 2 planes separately.\nc W o M a o r e m r i t e l g a r d i a n P c l a 𝐚 a m ₁ n e 𝐚 e r 𝐦 ₂ a o f 1 𝐚 ₀ 𝐛 𝐧 ₁ P c l a a m n e 𝐛 e r ₀ a o 𝐛 f 2 ₂ R a y $\\vec{a₀}$ and $\\vec{b₀}$ are the normal vector of two planes.\n$\\vec{a₁}$, $\\vec{a₂}$ are orthogonal, so they form a basis of the plane, same as $\\vec{b₁}$, $\\vec{b₂}$.\nThus, the world point 𝐦 on the plane can be represented by its planar basis as 𝐩 = (p₁,p₂,1) :\n$$ 𝐦 = 𝐚₁ p₁ + 𝐚₂ p₂ + 𝐚₀ = \\begin{pmatrix}𝐚₁ \u0026amp; 𝐚₂ \u0026amp; 𝐚₀\\end{pmatrix} \\begin{pmatrix} p₁ \\\\ p₂ \\\\ 1 \\end{pmatrix} = 𝐀 \\begin{pmatrix} p₁ \\\\ p₂ \\\\ 1 \\end{pmatrix} $$\nSpecifically, 𝐚₀, 𝐚₁, 𝐚₂ make up the camera space.\nSince each column in A is the camera (source) space basis axis seen from the world (original) space, 𝐀 contains c2w, and $𝐀 = [𝐑|𝐭]ₘ⁻¹𝐊ₘ⁻¹$\nSimilarly, the point 𝐧 represented by the basis of its plane is 𝐪 = (q₁,q₂,1):\n$$𝐧 = \\begin{pmatrix}𝐛₁ \u0026amp; 𝐛₂ \u0026amp; 𝐛₀\\end{pmatrix} \\begin{pmatrix} q₁ \\\\ q₂ \\\\ 1 \\end{pmatrix} = 𝐁 \\begin{pmatrix} q₁ \\\\ q₂ \\\\ 1 \\end{pmatrix} $$\nwhere 𝐁 can be interpreted as $𝐁= [𝐑|𝐭]ₙ⁻¹𝐊ₙ⁻¹$ Because the two world points are on the common line, and they satisfy the perspective projection centered at the world origin, so the only difference between them is a scaling factor.\n$$𝐦 = α 𝐧$$\nα is a scalar that depends on 𝐧. And eventually, on 𝐪. So α is a function of 𝐪, i.e., α(𝐪). Substituting their coordinates 𝐩 and 𝐪:\n$$ \\begin{aligned} 𝐀𝐩 = α(𝐪) ⋅𝐁⋅𝐪 \\\\ 𝐩 = α(𝐪) ⋅𝐀⁻¹⋅𝐁⋅𝐪 \\end{aligned} $$\nA is invertible because its 3 rows (base vectors) are independent and nonzero as the plane doesn\u0026rsquo;t cross zero. If 𝐦 and 𝐧 are overlapped, i.e., the same point, which is just observed from different angles, the scale factor α = 1.\n(2023-12-05)\nTransfer Camera Source article: Pinhole Camera: Homography - 拾人牙慧 - 痞客邦\nHomography transfers a pixel on cam1 to another camera\u0026rsquo;s film.\nA homography can be solved if the rotation and translation from one camera to the other and the co-observed plane (𝐧,d) is defined under the original camera space.\nIn other words, the camera-1\u0026rsquo;s space is regraded as the world space. Thus, the extrinsics of cam1 is [𝐈|0], and cam2\u0026rsquo;s extrinsics is [𝐑|𝐭].\n𝐐 c 𝐩 a m 𝐑 1 , 𝐭 , ( 𝐧 c , a d 𝐪 m ) 2 𝐐 ' Note: In this setting, the co-observed 3D point 𝐐\u0026rsquo;s coordinates aren\u0026rsquo;t given, so the plane {𝐧,d} is necessary to derive the crucial z-coordinate of 𝐐.\nSupp: A plane in a 3D space can be defined by its normal vector 𝐧 (a,b,c) and a point 𝐐 (x,y,z) located on it as: $𝐧𝐐 = 0$ (12.5: Equations of Lines and Planes in Space).\n$$ 𝐧⋅𝐐 = (a,b,c) (x-x₀, y-y₀, z-z₀) \\\\ = ax-ax₀+by-by₀+cz-cz₀ = 0 $$\nwhere (x₀,y₀,z₀) is the \u0026ldquo;origin\u0026rdquo;. If the plane is displaced from the origin along its normal vector by distance $d$, the plane is $𝐧𝐐-d=0$. (Plane Equation - Song Ho)\n$$ 𝐧⋅𝐐 = (a,b,c) (x-da-x₀, y-db-y₀, z-dc-z₀) \\\\ = ax-ax₀+by-by₀+cz-cz₀ -d = 0 $$\nThus, 𝐐 can be determined by 𝐧 and d.\nThe normal vector 𝐧 uses coordinates in the camera-1, as it\u0026rsquo;s used to locate the point 𝐐 observed by camera-1.\n𝐂 d ₁ : : : : 𝐑 , 𝐭 𝐩 𝐊 ⁻ 𝐧 ¹ 𝐐 𝐊 𝐐 C ' o p m l 𝐪 m a o n 𝐂 n e ₂ Within the camera-1 space, the distance from the camera center 𝐂₁ to the plane can be expressed as the inner product of $𝐐$ (or $𝐂₁𝐐$) and $𝐧$\n$$ d = 𝐧ᵀ⋅𝐐 = 1⋅|𝐐|⋅cosθ $$\nIf 𝐧 points towards the side with the camera-1, then θ is larger than 90°, so d is negative. Thus, a minus sign will be added.\nIn MVSNet, all preset depth planes are parallel to the reference camera, so 𝐧 represented in the reference camera space is $(0,0,-1)ᵀ$, and $𝐧 𝐑_{ref}$ (i.e., fronto_direction) is the opposite z-axis of $𝐑_{ref}$ (extrinsics) in the world space.\nIf $d$ and $𝐧$ are known, the coordinate of 𝐐 under camera-1 space can be written as:\nIf $d$ and 𝐧 are known, although there is $𝐐 = \\frac{d}{𝐧ᵀ}$, the 𝐐=(Qx,Qy,Qz)ᵀ can\u0026rsquo;t be determined uniquely based on it alone. Because there are 3 unkonwns with only 1 constraint, there is infinitely many solutions.\nHowever, the following equation can be used:\n$$ 1 = - \\frac{𝐧ᵀ⋅𝐐}{d} $$\nAssume the conversion between two cameras is: $$𝐐\u0026rsquo; = 𝐑𝐐 + 𝐭$$\n𝐑 and 𝐭 transform the coordinates in camera-1 to coordinates in camera-2, i.e., the camera-1 is transformed to camera-2.\n𝐐\u0026rsquo; is the coordinates of the covisible point in the camera-2\u0026rsquo;s space. With it, the target pixel on camera-2 is: $𝐊\u0026rsquo;𝐐\u0026rsquo;$.\nBy substituting the above equation of 1, 𝐐\u0026rsquo; can be expressed with plane\u0026rsquo;s parameters:\n$$ 𝐐\u0026rsquo; = 𝐑𝐐 + 𝐭⋅1 = (𝐑 -𝐭 \\frac{𝐧ᵀ}{d})𝐐 $$\nEssentially, the z-coordinate of 𝐐 is determined by the plane uniquely.\nThus, 𝐐 can be given by specifying z directly or a plane (𝐧,d) equivalently.\nAnd 𝐐\u0026rsquo; can be represented by two cameras\u0026rsquo; poses or the transformation from one camera to the other.\nSuppose two cameras have different intrinsics 𝐊 and 𝐊\u0026rsquo;, so their projection pixels are 𝐩 and 𝐪 respectively:\n$$ 𝐩 = 𝐊𝐐 \\quad \\text{and} \\quad 𝐪 = 𝐊\u0026rsquo; 𝐐\u0026rsquo; $$\nSubstituting 𝐐\u0026rsquo;, the 𝐪 will be derived from 𝐩:\n$$ 𝐪 = 𝐊\u0026rsquo; (𝐑-𝐭 \\frac{𝐧ᵀ}{d})𝐐 = 𝐊\u0026rsquo; (𝐑-𝐭 \\frac{𝐧ᵀ}{d}) 𝐊⁻¹𝐩 $$\nHence, the homography from 𝐩 to 𝐪 is: $$ 𝐇 = 𝐊\u0026rsquo; (𝐑-𝐭 \\frac{𝐧ᵀ}{d}) 𝐊⁻¹ $$\n(2023-12-06)\n$𝐊⁻¹ ⋅(u,v,1)ᵀ$ can\u0026rsquo;t restore a 3D point uniquely because z has been absorbed into u,v. (i.e., z is unknown in $\\rm 𝐐 = 𝐊⁻¹⋅ z⋅ (u,v,1)ᵀ$) Instead, it represents a line of points with various z, corresponding to the epipolar line on the other camera film.5\nTherefore, the distance from a camera to the plane is used to specify the point on a certain plane, which will be transformed first to the other camera space and then projected perspectively onto camera film.\n$$ \\begin{aligned} 𝐐 \u0026amp;= -\\frac{d}{𝐧ᵀ} \u0026amp; \\text{unique point} \\\\ 𝐐\u0026rsquo; \u0026amp;= 𝐑 𝐐 + 𝐭 \u0026amp; \\text{Another camera space} \\\\ 𝐪 \u0026amp;= 𝐊 𝐐\u0026rsquo; = 𝐊(𝐑 𝐐 +𝐭) \u0026amp; \\text{perspective project} \\\\ \u0026amp;= 𝐊 (𝐑 -𝐭 \\frac{𝐧ᵀ}{d}) 𝐐 \\\\ \u0026amp;= 𝐊 (𝐑 -𝐭 \\frac{𝐧ᵀ}{d}) 𝐊^{-1} 𝐩 \\end{aligned} $$\nIn this way, the projection on a camera film is mapped to another camera\u0026rsquo;s film.\nNote: the pixel coordinates on the two films are not the same.\n(2023-12-07)\nGeneralize H Source article: 5. Multi-View Stereo中的平面扫描(plane sweep) - ewrfcas的文章 - 知乎\nThe generalized homography describes the pixel mapping from the reference plane 𝐂₁ to the plane of an arbitrary camera 𝐂ᵢ.\nW o o r 𝐑 𝐭 r i ₁ ₁ l g ⋅ d i n 𝐇 ᵢ 𝐑 ₁ ᵢ R , e 𝐭 f ᵢ 𝐩 1 S c o a 𝐐 u m 𝐪 r e 𝐐 c r ' e a i To apply the above derived expression 𝐇, the camera Ref-1 should always be the \u0026ldquo;world space\u0026rdquo;, and the target plane is various source camera-i.\nThus, the coordinates 𝐐\u0026rsquo; in a source camera-i space is transformed from 𝐐 in the Ref-1 camera space as:\n$$ 𝐐\u0026rsquo; =\\begin{bmatrix} 𝐑ᵢ \u0026amp; 𝐭ᵢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 𝐑₁ \u0026amp; 𝐭₁ \\\\ 0 \u0026amp; 1 \\end{bmatrix}^{-1} 𝐐 $$\nThe matrices product can be simplified as:\n$$ \\begin{bmatrix} 𝐑ᵢ \u0026amp; 𝐭ᵢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 𝐑₁ \u0026amp; 𝐭₁ \\\\ 0 \u0026amp; 1 \\end{bmatrix}^{-1} = \\begin{bmatrix} 𝐑ᵢ \u0026amp; 𝐭ᵢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\frac{1}{𝐑₁}\\begin{bmatrix} 1 \u0026amp; -𝐭₁ \\\\ 0 \u0026amp; 𝐑₁ \\end{bmatrix} = \\\\ \\begin{bmatrix} 𝐑ᵢ \u0026amp; 𝐭ᵢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix}𝐑₁⁻¹ \u0026amp; -𝐑₁⁻¹𝐭₁ \\\\ 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} 𝐑ᵢ𝐑₁⁻¹ \u0026amp; -𝐑ᵢ𝐑₁⁻¹𝐭₁+𝐭ᵢ \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\\\ $$\nTherefore, the generalized relation between 𝐐\u0026rsquo; and 𝐐 is:\n$$ 𝐐\u0026rsquo; = \\tilde{𝐑}𝐐+ \\tilde{𝐭} = 𝐑ᵢ𝐑₁⁻¹ 𝐐 + (-𝐑ᵢ𝐑₁⁻¹𝐭₁+𝐭ᵢ)$$\nSubstituting $\\tilde{𝐑}$ and $\\tilde{𝐭}$ into the planar homography:\n$$ 𝐪 =𝐊ᵢ (𝐑ᵢ𝐑₁⁻¹ - \\frac{(-𝐑ᵢ𝐑₁⁻¹𝐭₁+𝐭ᵢ) 𝐧ᵀ}{d} ) 𝐊₁⁻¹𝐩 $$\nRearrange it to align with the MVSNet paper:\n$$ 𝐪 =𝐊ᵢ 𝐑ᵢ(𝐑₁⁻¹ - \\frac{(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ)𝐧ᵀ}{d}) 𝐊₁⁻¹𝐩 \\\\ = 𝐊ᵢ 𝐑ᵢ ( 𝐈 - \\frac{(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ) 𝐧ᵀ 𝐑₁}{d})𝐑₁⁻¹ 𝐊₁⁻¹𝐩 $$\nThe eq. (1) in MVSNet was wrong: $𝐇_i(d) = 𝐊ᵢ 𝐑ᵢ ( 𝐈 - \\frac{(𝐭₁-𝐭ᵢ) 𝐧ᵀ}{d})𝐑₁ᵀ 𝐊₁ᵀ$, (issue #77)\n$𝐊₁⁻¹ \\neq 𝐊₁ᵀ$ since 𝐊 may not be an orthogonal matrix. 𝐑ᵢ⁻¹ and 𝐑₁ can\u0026rsquo;t be cancled. Corresponding to MVSNet-TF code, c_left is $-𝐑₁⁻¹𝐭₁$; c_right is $-𝐑ᵢ⁻¹𝐭ᵢ$; c_relative is $-(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ)$ fronto_direction is $-𝐧ᵀ 𝐑₁$, which is R_left[2,:]. Therefore, temp_vec = tf.matmul(c_relative, fronto_direction) is $(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ) 𝐧ᵀ 𝐑₁$.\nWarp An Image (2023-12-19) Homography warps a non-rectangle to a rectangle.\n(2023-02-08)\nHomography matrix warps an image to another plane. In the application of image stitching, multiple images are warped to a unified plane.\nThe images have to be observing the common plane.\nIt works for stitching planar panoramas because the scene is far from the camera.\n(2023-12-17) In other words, co-planar points remain co-planar after a homography transformation (matrix). For example, a point on a plane that in image-1 remains located on the plane viewed by image-2.\nTwo pairs of matched points will perform the same transformation: $$\\begin{cases}p_1 = H p_2 \\\\ q_1 = H q_2 \\end{cases}$$\nPut differently, \u0026ldquo;a plane remains a plane in another view\u0026rdquo;, while the portion that is not on the plane will be warped.\nR,T can be solved from H and K.\nDemo: Warp the image to simulate the scene seen from another displaced camera (Aerial View). Refer to: OTC4 Homography說明以及小實驗-AI葵\nSelect 10 points to form a plane, which remains a plane in another view after transformion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import cv2 import numpy as np import matplotlib.pyplot as plt %matplotlib inline img1 = cv2.imread(\u0026#34;rot1.png\u0026#34;) pts1 = [(24, 124), (49, 124), (98, 124), (104, 124), (120, 124), (18, 146), (37, 146), (65, 146), (102, 146), (133, 146)] for pt in pts1: cv2.circle(img1, pt, 0, (0, 0, 255)) plt.figure(figsize=(20,10)) plt.imshow(img1[:,:, ::-1]) img2 = cv2.imread(\u0026#34;rot2.png\u0026#34;) pts2 = [(45, 116), (64, 120), (110, 127), (118, 128), (135, 131), (31, 135), (46, 137), (69, 143), (105, 150), (140, 158)] src_points = np.array(pts1, dtype=np.float32).reshape(-1,1,2) dst_points = np.array(pts2, dtype=np.float32).reshape(-1,1,2) Homography, mask = cv2.findHomography(src_points, dst_points, 0, 0.0) # (3,3) # warp img1 to the view of img2 warped1 = cv2.warpPerspective(img1, Homography, (img1.shape[1], img1.shape[0])) plt.figure(figsize=(20,10)) plt.imshow(warped1[:,:,::-1]) # reverse plt.imshow(img2[:,:,::-1], alpha=0.6) # overlap Bird\u0026rsquo;s-Eye View:\n1 2 3 4 5 src_trapezoid = np.float32([[1, 128], [47, 91], [120, 91], [157, 128]]) dst_rectangle = np.float32([[64, 104], [64, 64], [104, 64], [104, 104]]) homoMatrix = cv2.getPerspectiveTransform(src_trapezoid, dst_rectangle) warped = cv2.warpPerspective(img, homoMatrix,(img.shape[1], img.shape[0])) The image got transformed by the homography to deform the trapezoidal road lane to a rectangle displaying on the image by changing the camera pose to looking down:\nBEV is a homography, but only make sense for the ground plane. 单应性Homograph估计：从传统算法到深度学习 - 白裳的文章 - 知乎 Multi-Depth Planes (2023-12-17)\nGiven camera parameters of a reference view and a source view in the world space respectively, and multiple depth planes, solve the homographies that transforming the source image into the reference camera space:\nR e 𝐊 𝐑 𝐭 f ᵣ ᵣ ᵣ v i 𝐊 𝐑 𝐭 e ₛ ₛ ₛ w S i 𝐳 r m n 𝐧 c g o r m 𝐇 a l G i d v ₁ e n d e p d t ₂ h p l a n d e ₃ s : d ₄ The formula used in MVSNet-tf code is: $𝐊ᵢ𝐑ᵢ ( 𝐈 - \\frac{-(-𝐑₁⁻¹𝐭₁+𝐑ᵢ⁻¹𝐭ᵢ) (-𝐧ᵀ 𝐑₁)}{d})𝐑₁⁻¹𝐊₁⁻¹$ because the $-𝐧ᵀ 𝐑₁$ is the z-axis of 𝐑₁ in the world space.\nA source images is warped to the reference camera space and watching a common depth plane. Therefore, the points on the depth plane will have the same x coordinates as the points in the reference image.\nWarp by Sampling (2023-12-18)\nSample the source image to be warped at the locations that is determined by the mapping of homography inversely from the target view to the source view. Refer to MVSNet-pytorch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch from torchvision import transforms h, w = ref.shape[:2] vu = torch.cartesian_prod(torch.arange(h), torch.arange(w)) uv = torch.flip(vu, [1]) # (hw,2), As x varies, y is fixed uv1 = torch.cat([uv, torch.ones(len(uv), 1)], dim=-1) # (hw,3) src_KRT = torch.eye(4) src_KRT[:3] = torch.from_numpy(K_s @ RT_s[:3, :4]) ref_KRT = torch.eye(4) ref_KRT[:3] = torch.from_numpy(K_r @ RT_r[:3, :4]) proj = src_KRT @ torch.inverse(ref_KRT) rot, trans = torch.split(proj, [3,1], dim=-1) rot_uv1 = rot[:3] @ uv1.t() # (3, hw) d = 425.0 + 1.06*2.5 * torch.arange(192).view(1,-1,1) #(1, 192, 1) rot_uvd = rot_uv1.unsqueeze(1).expand(-1, 192,-1) * d #(3, 192, hw) pix_proj = rot_uvd + trans[:3].unsqueeze(1).expand(-1, 192, -1) # (3, 192, hw) u_src = 2*(pix_proj[0] / pix_proj[2])/(w-1)-1 v_src = 2*(pix_proj[1] / pix_proj[2])/(h-1)-1 uv_src = torch.stack([u_src, v_src], dim=-1) # (192, hw, 2) uv_src_d1 = uv_src[-1] # plane at the furthest depth sampled = torch.nn.functional.grid_sample( transforms.ToTensor()(src).unsqueeze(0), # (1,3,512,640) uv_src_d1.view(1, h, w, 2), mode=\u0026#39;nearest\u0026#39;) sampled_scaled = torch.tensor(sampled[0] * 255, dtype=int) # (3, 512, 640) fig, ax = plt.subplots(1,3, figsize=(30,15)) ax[0].imshow(src[:,:,::-1]) # (512, 640, 3) ax[0].set_title(\u0026#34;Source view\u0026#34;) ax[1].imshow(sampled_scaled.permute(1,2,0).numpy()[:,:,::-1]) ax[1].set_title(\u0026#34;Warped source\u0026#34;) ax[2].imshow(ref[:,:,::-1]) ax[2].set_title(\u0026#34;Reference view\u0026#34;) The result is almost the same as the one yielded by opencv:\n4 Co-planar Points (2022-12-03)\nHomography matrix builds the relatioship between 2 images to make the pixels corresponding to the same 3D points overlap ³ based on the projection from the common 3D point to different image planes.\nIn following example, two cameras look at a flat plane. And they both observe the points located on that flat plane.\nThen, the matching pixels on the two camera planes can be connected by the chain: img1 ➔ plane0 ➔ img2.\nWhen a 3D world point is projected to 2D camera plane, the coordinates transformation consists of two matrix: extrinsic matrix [R|T] transforming the coordinate system, and the intrinsic matrix [K] projecting the 3D point to a 2D pixel, that is:\n$$ \\begin{bmatrix} u\\\\ v\\\\ 1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f/dₓ \u0026amp; 0 \u0026amp; cₓ \u0026amp; 0 \\\\ 0 \u0026amp; f/d_y \u0026amp; c_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp;1 \\end{bmatrix} \\begin{bmatrix} r₁₁ \u0026amp; r₁₂ \u0026amp; r₁₃ \u0026amp; t₁ \\\\ r₂₁ \u0026amp; r₂₂ \u0026amp; r₂₃ \u0026amp; t₂ \\\\ r₃₁ \u0026amp; r₃₂ \u0026amp; r₃₃ \u0026amp; t₃ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ W \\end{bmatrix} $$\nwhere the W is the homogeneous coordinate for accommodating the translation vector $[^{^{t₁}_{t₂}} _{^{t₃}_{1}}]$ in the extrinsic matrix.\nOmit z because all points on the plane have z=0.\nGiven $[^{_{x₁}}_{^{y₁}_{w₁}}]$ = H₁₀ $[^{_{x₀}}_{^{y₀}_{w₀}}]$, then there is $$\\rm [^{_{x₂}}_{^{y₂}_{w₂}}] = H₂₀ H₁₀⁻¹[^{_{x₁}}_{^{y₁}_{w₁}}]$$, where H₂₀ H₁₀⁻¹ is the homograhpy matrix H₃ₓ₃. It represents the mapping between the pair of points: (x₁,y₁,w₁) and (x₂,y₂,w₂).\n$$ \\begin{bmatrix} x₂\\\\ y₂\\\\ w₂ \\end{bmatrix} = \\begin{bmatrix} H₁₁ \u0026amp; H₁₂ \u0026amp; H₁₃ \\\\ H₂₁ \u0026amp; H₂₂ \u0026amp; H₂₃ \\\\ H₃₁ \u0026amp; H₃₂ \u0026amp; H₃₃ \\\\ \\end{bmatrix} \\begin{bmatrix} x₁\\\\ y₁\\\\ w₁ \\end{bmatrix} $$\nExpand the above equation, there are 2 equations (with considering x₂\u0026rsquo;=x₂/w₂, y₂\u0026rsquo;=y₂/w₂, such that w = 1):\n$$\\begin{cases} x₂\u0026rsquo; = \\frac{x₂}{w₂} = \\frac{H₁₁x₁ + H₁₂y₁ + H₁₃w₁}{H₃₁x₁ + H₃₂y₁ + H₃₃w₁} \\\\ y₂\u0026rsquo; = \\frac{y₂}{w₂} = \\frac{H₂₁x₁ + H₂₂y₁ + H₂₃w₁}{H₃₁x₁ + H₃₂y₁ + H₃₃w₁}\\end{cases} \\\\ ⇓ \\\\ \\rm\\{^{x₂\u0026rsquo;H₃₁x₁ + x₂\u0026rsquo;H₃₂y₁ + x₂\u0026rsquo;H₃₃w₁ - H₁₁x₁ - H₁₂y₁ - H₁₃w₁ = 0} _{y₂\u0026rsquo;H₃₁x₁ + y₂\u0026rsquo;H₃₂y₁ + y₂\u0026rsquo;H₃₃w₁ - H₂₁x₁ - H₂₂y₁ - H₂₃w₁ = 0} $$\nThe last element H₃₃ is determined when other elements are set, so the degree of freedom for this matrix is 8.\nTherefore, the homography matrix can be solved from 4 pairs of matching points: (x₁,y₁)-(x₂,y₂); (x₃,y₃)-(x₄,y₄); (x₅,y₅)-(x₆,y₆); (x₇,y₇)-(x₈,y₈).\nAfter reformalizing the equation system to a matrix form, the sigular value decomposition can be applied.\n$$\\rm{ [^{^{^{-x₁\\; -y₁\\; -w₁\\; 0\\quad 0\\quad 0\\quad x₂\u0026rsquo;x₁\\quad x₂\u0026rsquo;y₁\\quad x₂\u0026rsquo;w₁} _{0\\quad 0\\quad 0\\quad -x₁\\; -y₁\\; -w₁\\; y₂\u0026rsquo;x₁\\quad y₂\u0026rsquo;y₁\\quad y₂\u0026rsquo;w₁}} _{^{-x₃\\; -y₃\\; -w₃\\; 0\\quad 0\\quad 0\\quad x₄\u0026rsquo;x₃\\quad x₄\u0026rsquo;y₃\\quad x₄\u0026rsquo;w₃} _{0\\quad 0\\quad 0\\quad -x₃\\; -y₃\\; -w₃\\; y₄\u0026rsquo;x₃\\quad y₄\u0026rsquo;y₃\\quad y₄\u0026rsquo;w₃}}} _{^{^{pair3-1}_{pair3-2}} _{^{pair4-1}_{pair4-2}}}] \\ [^{^{^{H₁₁}_{H₁₂}} _{^{H₁₃}_{H₂₁}}} _{^{^{H₂₂}_{H₂₃}} _{^{H₃₁}_{^{H₃₂}_{H₃₃}}}}] = 0 } $$\nSolution This is a homogeneous linear system 𝐀𝐡=0, so its solution has 2 cases:\nIf the number of non-zero rows of the coefficients matrix 𝐀 after Gaussian elimination is less than the number of unknowns of 𝐡, i.e., (rank(𝐀) \u0026lt; #h), there is only the zero solution;\nOr infinitely many non-zero solution, but only the solution satisfying constraints of magnitude is selected.\nIf 𝐀 is invertible (square matrix \u0026amp; rank(𝐀) = #col), then 𝐡=𝐀⁻¹⋅0 = 0 ???\nOtherwise, if 𝐀 is not invertible, there would be inifinetly many non-zero solutions. So the optimal solution should be approached by minimizing the error $$J = ½‖𝐀𝐡-0‖²$$ Therefore, the least-square solution is when the derivative\n$$ ∂J/∂𝐡 = 0 \\\\ ⇒ 𝐀ᵀ(𝐀𝐡-0) = 0\\\\ ⇒ 𝐀ᵀ𝐀𝐡-𝐀ᵀ⋅0 = 0 $$\nIf 𝐀ᵀ𝐀 is invertible, the optimal 𝐡=(𝐀ᵀ𝐀)⁻¹𝐀ᵀ⋅0 = 0 ???, where the (𝐀ᵀ𝐀)⁻¹𝐀ᵀ is the pseudo-inverse matrix.\nUse all the matching pints to compute a robust H, even though 4 pairs are enough to solve the 8 unkowns (An element is 1 after scaling the matrix by it). Constrained Least Squares. Solve for 𝐡: A𝐡=0, such that ‖𝐡‖²=1. Define least squares problem: min_𝐡 ‖A𝐡‖², such that ‖𝐡‖²=1. We know that: ‖A𝐡‖² = (A𝐡)ᵀ(A𝐡) = 𝐡ᵀAᵀA𝐡, and ‖𝐡‖²= 𝐡ᵀ𝐡 =1. So the problem is: minₕ(𝐡ᵀAᵀA𝐡), such that 𝐡ᵀ𝐡 =1.\nDefine Loss function L(𝐡,λ): L(𝐡,λ)= 𝐡ᵀAᵀA𝐡 - λ(𝐡ᵀ𝐡 -1). And take derivatives of L(𝐡,λ) w.r.t. 𝐡: 2AᵀA𝐡 - 2λ𝐡 =0. It\u0026rsquo;s the classical eigenvalue problem: AᵀA𝐡 = λ𝐡.\nSVD. Find the eigenvalue and eigenvectors of the matrix AᵀA. 𝐡 is he eighenvector with the smallest eigenvalue λ.\nFirst Principles of CV-Dr.Shree;\nMisc (2023-12-07) Epipolar geometry requires translation between cameras, whereas homography doesn\u0026rsquo;t. homography - Carleton University Ref OpenCV4 - Docs - Basic concepts of the homography explained with code Homography in computer vision explained - Behnam Asadi 3D投影变换（含透视投影Perspective Projection）-子燕若水 -CSDN ","date":"2022-12-03T17:30:00-05:00","permalink":"https://zichen34.github.io/writenotes/vis/planar_homography/","title":"memo: Vis | Planar Homography"},{"content":"原视频：“L1和L2正则化”直观理解(之一)，从拉格朗日乘数法角度进行理解 - 王木头学科学 -bilibili\n(2023-01-30) 加上正则化项，就变为求拉格朗日函数的最小值，即求\u0026quot;损失函数+权重模长\u0026quot;整体的最小值。 λ 是个比值，λ不同对应的W的位置不同，即λ确定了W的位置。然后 𝐖 自己不断调整，使 J(𝐖) 与 λ‖𝐖‖ 二者的梯度等大反向(抵消)，从而使 $∇_𝐖 L(𝐖,λ)=0$。λ不同，则最优W就不同。_\n机器学习的两个核心议题：优化与正则化。优化找到最优参数；正则化减少模型的过拟合：对模型权重𝐖（不处理b）进行L1和L2正则化\n从3个角度理解 L1 和 L2 正则化：\n拉格朗日乘数法 权重衰减 贝叶斯概率 正则化 与“正则表达式”没有关系，正则表达式是编程中用来处理字符串的技术。 正则化常常指模型权重的L1和L2范数。Dropout 也是一种正则化，在训练时，随机的让一些神经元失效。\n花书：凡是可以减少泛化误差（过拟合），而不是减少训练误差的方法，都可以称作正则化方法。\nL1, L2 正则化项是加在损失函数上，约束模型参数𝐖的 L1, L2 范数的项。L1范数使W变得稀疏，L2范数使W变得小。\n拟合时一般希望保留更多特征，高次的函数表达能力更强，所以模型倾向于学出高次项的系数，使训练误差更小，但不能很好地预测新数据，没有找到基础规律，出现过拟合 3。\n如果特征（各属性）已知，可以把 polynomials 的高次项系数置为0；否则，需采用与\u0026quot;特征的定义\u0026quot;无关的约束，可以选择模长最小的，误差(mse)也最小的参数。 Problem is now well-posed for any degree。Even very high polynomials, simple function tend to be learned with regularization2 所以对细节特征的系数（θ₃²+θ₄²）增大惩罚力度，bias them not to be large （min ∑ᵢᵐ(y^(xᵢ)-yᵢ)² + 1000θ₃²+1000θ₄²）1\n范数：向量长度。网络权重 𝐖 是高维空间中的一个向量（也可认为W是高维空间一个点，范数即为这个点到原点的距离）。 若使用欧几里得距离（坐标差求平方和再开根号）度量它的模长，就是 L2 范数: ‖𝐖‖₂=√(|w₁|²+|w₂|²+\u0026hellip;+|wᵢ|²)； 若用曼哈顿距离计算（坐标差的绝对值求和），就是 L1 范数: ‖𝐖‖₁=|w₁|+|w₂|+\u0026hellip;+|wᵢ|。\n当 Lp 范数的 p 取值大于等于 1 时，有相同距离的点构成的集合是一个凸集； 当 p 取 0-1 之间，是非凸集。 当p小于等于1时，会带来稀疏性，所以 the L1 norm is the only norm which both induces some sparsity in the solution and remains convex for easy optimization. 如果问题对应的函数是一个凸函数，它的取值范围（可行域）是一个凸集，这就是一个凸优化问题，容易求解。L1和L2正则化在某种程度上，就是在利用 L1 和 L2 范数的凸集特性。\n出现过拟合的原因之一：W太大会把噪声放大 神经网络的输出层做了三件事：\n接收上一层的激活值 a⁽ˡ⁻¹⁾，乘以权重 Wˡᵀ，加上偏移bᵀ，得到线性叠加后的 zˡ： zˡ= Wˡᵀ⋅a⁽ˡ⁻¹⁾+bᵀ。\n再做（激活引入非线性/限制取值0-1，然后）softmax 把线性结果 zˡ 变成概率分布: aˡ = softmax (zˡ)。\n输出层的损失函数：J(aˡ) = MLE (aˡ)。\n极大似然 MLE 与 CrossEntropy 等价，前者最大化重复采样训练样本的概率，后者最小化网络给出的类别概率乘以真实概率对应的信息量。 （softmax 搭配MLE，等价于最大熵。）\n调整 𝐖,𝐛 使损失函数J(𝐖,𝐛)最小，但当损失值最小时，对应的 𝐖,𝐛 不是唯一的。比如:\n把前面所有隐藏层的权重和偏置都放大2倍（用ReLu激活），但同时输出层的权重和偏置缩小 1/2⁽ˡ⁻¹⁾倍，得到的线性结果不变，则损失值不变，所以最终优化得到的参数与初始值相关。\n如果输入放大 2 倍的同时输出层权重缩小 2 倍，线性结果不变，损失值也不变。所以最终优化得到的 𝐖,𝐛 与输入数据相关。\n训练神经网络时的目的是找到损失函数 J(W,b) 的最小值，但相同的损失值可能对应多组绝对值（模长）不同的(W,b)。\n绝对值不同的 𝐖,𝐛 可能在训练集上的损失值相同，但在测试集、unseen输入、\u0026ldquo;带噪声的输入\u0026quot;上的表现有差别，不同参数的泛化能力不同。 绝对值大的参数会把噪声放大，从而影响预测结果。\n正则化就是人为的设定参数𝐖 的取值范围（可行域），让W不能超出该范围，从而在可行域范围内，求损失函数的最小值。带条件的优化问题可以用拉格朗日乘数法求解。 我们只需要规定 𝐖 的范围（多项式各未知数的系数），因为 W 直接决定了模型曲线是什么样子，而 b 与最终拟合的曲线的形状（过拟合与否）无关，b只影响平移，因此只要𝐖 被约束好了，b在训练时会自动调整好，所以不需额外做约束。\nw 的可行域范围 在求损失函数最小值时，权重 𝐖 的模长不要超过C：\n若采用曼哈顿距离度量模长，即计算向量的L1范数，则优化问题为：\nmin J(𝐖 , 𝐛, 𝐗), s.t. ‖𝐖‖₁-C≤0\n若采用欧几里得距离计算模长（向量的L2范数），则优化问题为：\nmin J(𝐖 , 𝐛, 𝐗), s.t. ‖𝐖‖₂-C≤0\n把优化问题写成拉格朗日函数 = 目标函数 + 约束条件乘以拉格朗日乘子 λ： L(𝐖, λ) = J(𝐖) + λ(‖𝐖‖₁ - C), 然后求解：$\\rm min_𝐖 \\ max_λ L(𝐖,λ), s.t. λ≥0$ （不考虑b,X）\n上图坐标轴是 𝐖 的各个维度，椭圆是损失函数J(𝐖)的等高线，中心是损失函数的最小值对应的 𝐖，绿色框代表可行域范围（𝐖 的模长限制），蓝色点是在满足约束条件时，损失函数能取到的最小值，对应的𝐖 ；或者说在可行域范围内找损失函数梯度+𝐖可行域梯度=0的点。\n“当目标函数是一个凸函数或是一个凹函数，并且对应的约束条件是一个凸集，那么整个问题（目标函数+λ⋅约束条件）就是一个凸优化问题”。 约束条件采用L1或L2范数时，可行域是一个凸集。对应于凸集的约束条件并不会改变原来问题的性质， 即如果原来的问题本身是一个凸问题，加上这个约束条件后，仍然是凸问题，如果原来的问题是非凸问题，加上这个约束条件也不会让这个问题变得更糟糕。\n(2022-11-07) 损失函数与正则化项都画在二维平面上，是因为研究的只是“两个对象”：输入X和输出Y之间的关系？\n不，因为是要限制w的模长，二维平面只表示了二维w。加了正则化项，相当于修改了初始时的w，缩短它到那条虚线的距离，但不考虑它对应的损失值变大还是变小了，虚线总是沿着损失函数的梯度方向，而且连接着损失值最小的那个w，不过那条虚线上的w是等比的，等比例缩放的 w 能收敛到的最值是相同的\n(2023-01-30) 正则化项 λ‖𝐖‖ 的作用是持久的。在 λ‖𝐖‖ 的基础上，𝐖 做调整以使损失值最小，也就是使 J(𝐖) 与 λ(‖𝐖‖₁ - C) 两项的梯度抵消（等大反向）。\u0026ldquo;在原始最小二乘的结果上做了缩放\u0026rdquo;4。\nL1, L2 正则化项 把拉格朗日函数展开：\nL(𝐖,λ) = J(𝐖) + λ(‖𝐖‖₂-C) = J(𝐖) + λ‖𝐖‖₂ - λC\n但是常见的 “损失函数+ L2正则化项” 的表达式是 拉格朗日函数+λC：\nL\u0026rsquo;(𝐖,λ) = L(𝐖,λ) + λC = J(𝐖) + λ‖𝐖‖₂\n在求L\u0026rsquo; 和 L 这两个拉格朗日函数最小值时，虽然最小值可能不一样，但对应的 𝐖 是一样的 （因为只要求解 $∇_𝐖 L\u0026rsquo;(𝐖,λ) =∇_𝐖 L(𝐖,λ)=0$）：\n$arg_𝐖 (min_𝐖\\ max_λ L\u0026rsquo;(𝐖,λ), s.t.\\ λ≥0) = arg_𝐖 (min_𝐖\\ max_λ L(𝐖,λ), s.t.\\ λ≥0)$\n如何直观理解 L\u0026rsquo; 呢？ 或者说：为什么\u0026quot;常用的L2正则化表达式\u0026quot;比\u0026quot;损失函数加上 𝐖 约束条件写成的拉格朗日函数\u0026rdquo;，少一个 λC 呢？\nC 决定了 𝐖 的模长，不指定 C 是因为 λ 可以控制模长的范围。拉格朗日乘子λ的作用是调节\u0026quot;约束条件λ‖𝐖‖的梯度\u0026quot;的大小，使之与\u0026quot;目标函数J(𝐖)的梯度\u0026quot;等大反向抵消（求导=0就是求梯度=0的点）。 当λ=0时，相当于没有约束；而当λ=inf时，W会趋于0，失去拟合能力了。λ 可以用cross validation 来选择\n红色是损失函数的等高线，绿色是约束条件的等高线，箭头表示梯度\nλ 的绝对值等于\u0026quot;损失函数J(𝐖)的梯度大小\u0026quot;除以\u0026quot;约束条件对应函数λ‖𝐖‖的梯度大小\u0026quot;，所以空间中各点处的λ就能算出来，反过来不同的 λ 对应的点的位置不同。只有在恰当的位置J(W)与λ‖𝐖‖的梯度之和才等于0。 ~~不同的 λ 对应的梯度=0的点的位置不同，损失函数J(𝐖)最小值不同，但对应的（拉格朗日函数的最优）W是相同的??? ~~\nλ和C之中只有一个超参数：在拉格朗日函数中，C是超参数，人为指定模长范围C后，λ 也就跟着确定了；而在 L\u0026rsquo; 中，λ 是超参数，人为指定后，就能唯一确定梯度=0的点的位置。\nL2正则化（给定λ后）确定的最值点（最优𝐖）基本不会落在坐标轴上（可行域是圆形）， 而L1正则化找到的满足约束条件与目标函数的梯度之和=0的点（可行域范围与损失函数等高线相切的点）容易取在坐标轴上（可行域有尖角且落在坐标轴上，容易与损失函数相切）， 当最优 𝐖 落在某一坐标轴上时，只有那一维不是0，其他维度的坐标都是0。\n比如依据两个特征做判断时，最优 𝐖 不落在坐标轴上，则两个特征都有一定的决定作用， 而通过调整 λ 使最优W落在坐标轴上时，则只会关注该轴特征的有无，所以使用 L1 可以使决策变得简单，使 W 向量变得稀疏（只有某个“重要”维度(对结果贡献大)有值，其他维都是0），把特征之间的关系去耦合了，把模型复杂度降低了，从而减少过拟合。 但是L1 的解不太稳定，训练时各批次数据的损失函数不同，椭圆会变化，切点可能从一个轴换到另一个轴上，对应的W变化大。3\nL1 和 L2 都是对 W 进行约束，但效果不同，可以把二者结合起来一起用。L2正则化只是限制模长，L1 正则化还带来了稀疏性\n正则化带来的损失值误差？不重要啊 评论：上图绘制的椭圆对应的损失函数J(𝐖)的接收的输入是整个实数域，而在神经网络中，损失函数接收的是网络的输出值，网络不同的权重可能对应相同的输出值，损失值也就相同\n因为不同的初始参数 𝐖 最终收敛到的损失函数最小值可能是相等的，比如等比缩放参数时，min J(𝐖 ,𝐛) = min J(a𝐖 ,a𝐛)， (𝐖 ,𝐛) 与 (a𝐖 ,a𝐛) 共线（虚线），\n加约束条件λ‖𝐖‖与不加约束条件(λ=0)，通过最小化拉格朗日函数L(𝐖,λ)收敛到的 W 不同（W由λ决定），但是如果两个向量W是共线的，它们对应的 L(W,λ) 能够收敛到的最小值（where L(W,λ)的梯度=0）是相同的，只不过λ取的不好，还没走到最小值，还没平衡好 J(W) 和 ||W||之间的权重比例。\n同一虚线上的 W 最终能够收敛到的拉格朗日函数值是相同的。虚线上的点是本可能收敛到的最值\n如果不加约束条件，初始的较大的 W 本可以收敛到J(W)椭圆中心（损失最小值），但是因为指定了约束条件λ，模长被限制了，被拉向原点了，找到的最优W与椭圆中心离得很远，损失值J(W)变大了，看起来就带来了误差。 但是（最小化拉格朗日函数过程中的）误差并不是到椭圆中心的距离，而是到“虚线”的距离，虚线上的 W 已经调整到能使 J(W) 与 ||W||的梯度共线，因为 λ 是人为设置的，所以W还要继续调整，最终虚线上的W使 L(W,λ)的梯度=0。\n在虚线上的不同 W 对应的拉格朗日函数值L(W,λ)的梯度是相同的（在虚线上的点，方向共线已满足，只是λ不同）， 虽然可行域越小，对损失函数J(W)的最小值偏离的误差就越大，因为W初始值决定了最终能收敛到哪个J(W)，但关注的不是它J(W)。\nW调整到最后：J(W)的梯度方向与||W||梯度方向共线，并且对于超参数缩放因子λ，有L(W,λ)的梯度=0。\n所以真正关注的到最优解的偏差，是到虚线的距离，超参数λ选的好与坏，带来的偏差不大，所以能用正则化就用。\n- 例子：只有两个分量的W 与对应的损失值 绘制的图像为：\n同心椭圆是损失函数 J 的等高线，即每一圈椭圆上，损失值是相同的，中间的灰色图形的最外沿的一圈代表 W 的范数。正方形代表 L1 范数的图像，圆形代表 L2 范数的图像\n看下这篇：机器学习之正则化（Regularization）- Acjx -博客园\nDDG搜索：entropy regularization 熵正则化\n(2023-01-30) DDG search: \u0026ldquo;正则化项\u0026rdquo;\n正则化的作用 原文链接：正则化及正则化项的理解 - guuuuu - CSDN\n防止过拟合：𝐖 越小，拟合的函数曲线越简单光滑，越不容易过拟合；\n正则化项代表先验信息：试验之前的对‖𝐖‖的认知，λ 是对先验信息的相信程度。\n频率派直接对参数𝐖进行分析（而贝叶斯派是对参数出现的概率P(𝐖)进行分析）； 参数 𝐖 直接出现在损失函数中，所以频率派对损失函数做修正：加上了先验部分知识，即正则化项。\n对于模型：y = θ₀ + θ₁x₁ + \u0026hellip; + θⱼxⱼ + \u0026hellip; + θₙxₙ，解最优化问题：\n$arg\\ min_{θ₀,θ₁,\u0026hellip;,θₙ}\\ J(θ) = 1/2m ⋅(∑ᵢ₌₁ᵐ(h_θ(xⁱ)-yⁱ)² + λ∑ⱼ₌₁ⁿ(θⱼ-\\^θⱼ)²)$， 其中 ^θⱼ 为先验解。 λ 不同大小的选择，体现了这个先验解 ^θⱼ 的可信程度。如果 λ 是一个很小的整数，那正则化项将不起什么作用，说明给的先验解有很大的不确定性，在一定程度上是不可信的；\n如果 λ 很大，则正则化项占支配地位，最后的解将靠近于 ^θⱼ。\n频率派 贝叶斯派 找最优𝐖 直接对参数 𝐖 本身下手 对参数出现的概率 P(𝐖) 下手 思路 最优 𝐖 使训练误差最小 最优 𝐖 出现的后验概率 P(𝐖 优化公式 损失函数 贝叶斯公式 先验 正则化项（对𝐖 的先验认知） 自带先验概率（P(𝐖)） 修正 加上一部分先验信息 对似然值做 \u0026ldquo;P(𝐖)/配分函数\u0026rdquo; 的缩放 超参数 λ; L1 or L2范数 先验概率的分布: Laplace, Gaussian 有助于处理条件数（condition number）不好的情况下，矩阵求逆困难的问题。\n概念：如果方阵 A 是非奇异的（A的行列式不等于0，正定矩阵一定是非奇异的），那么 A 的 condition number 定义为：𝜅(A) = ‖A‖ ‖A⁻¹‖ 可以看出：如果 A 是奇异的，那么 A 的条件数为无穷大。条件数越小，所获得的解越可靠，模型鲁棒性越好，抗干扰能力越强。 例如对于模型 AX=b，A 的条件数越小（A的行列式远不接近于0），那么 A，b 的稍微的变化对解 X 的影响越小，对 X 的求解对样本集（A，b）中引入的干扰的抵抗能力越强，即所求解 X 越可靠。\nDDG search: \u0026ldquo;正则化项 pytorch\u0026rdquo;\nPyTorch 12.正则化-科技猛兽-知乎 1 2 3 # optimizer w and w/o regularization optim_normal = torch.optim.SGD(net_normal.parameters(), lr=lr_init, momentum=0.9) optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-2) DDG search: \u0026ldquo;pytorch optimizer weight decay\u0026rdquo; SGD-pytorch docs\n(2023-02-26) DDG serach: \u0026ldquo;正则化项 可以完全避免过拟合吗\u0026rdquo;\n在机器学习中，L2正则化为什么能够缓过拟合？ - 知乎\nRef [知识梳理-03] Regularization 正则化-凩子白-bilibili Linear regression (6): Regularization (UCI cs273a) - Alexander Ihler 什么是 L1 L2 正规化 正则化 Regularization (深度学习 deep learning) ","date":"2022-11-07T11:52:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/11_l1l2%E6%AD%A3%E5%88%99%E5%8C%961-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/","title":"watch: DL - 王木头 11 | L1L2 Reg (1), Larange Multiplier"},{"content":"(2022-5-25) 损失函数是输入样本batch的函数，不同batch的误差函数不同，如果在一个batch上某 w 的导数为零，在下一个batch上该 w 的导数不为零，就可以继续修正，而不会停滞在鞍点。误差函数横轴是 w，纵轴是 error。0\n3 loss functions 损失函数是为了: 衡量两个概率模型间的差别， 三种思路：最小二乘法(MSE)，极大似然估计(MLE)，交叉熵(CE) ¹\nMSE 对于单分类问题（是or不是），也就是抛硬币（反面的概率已经蕴含在正面的概率之中了），那么：\n最小二乘就是：概率(sigmoid输出0~1)减标签。为了依据误差修正w，误差取平方使其可导。如果损失函数中代入的是预测的正(反)概率，那损失函数是个二次曲线 L = (prob - label)²，横坐标是概率，纵坐标是loss，当prob=target, 预测出来的值与离散的观察值最接近。\n对于输出是多维的，在各个维度上都是二次曲线，多元最小二乘（多元线性回归）: J(w) = (Xw-Y)ᵀ(Xw-Y)。解∇J(w) = 0，就是求 pseudoinverse matrix ⁸。\n如果这样能一次求出 w，为什么还要梯度下降呢？ 因为有时 X 不可逆（可以加正则化项解决 note）， 而且有的激活函数 g 也不可逆（通过手动做归一化解决，例如 B-ELM 中使用了 minmax 函数）。\n当MSE 用于回归问题，loss=∑ᵢ(yᵢ-wxᵢ-b)²是凸函数，直接求导等于零，即可求出解析解；但是用于分类问题，输出需要经过sigmoid/softmax变成概率，loss=∑ᵢ(yᵢ-1/(1+e⁻ʷˣⁱ))²是非凸的，不能直接求解析解，而且不宜优化 ³。\nMLE 极大似然估计：各个可能的假设模型产生训练样本标签的分布的概率是多少，目标就是找到概率最大时对应的模型（加个负号取最小）；∏ᵢ pᵢˣ (1-pᵢ)¹⁻ˣ\nCE 交叉熵：网络模型要与人脑中的模型足够接近，某一事件在网络模型中发生对应的信息量要接近在人脑中发生对应的信息量，多个事件要以他们在人脑中发生的概率加权。∑ᵢ humanᵢ(-log₂ netᵢ)\n最小二乘可以用于回归，即网络输出可以是任意的数值；而极大似然估计和交叉熵都是基于概率的，网络的输出是概率，位于0-1之间，所以采用MLE或CE损失函数时，输出层神经元的激活函数需要用sigmoid，把输出压缩到0-1之间; 而隐藏层都可以用ReLu。 多类别问题输出用 softmax 激活，得到各类别的概率分布。\n交叉熵认为各类别相互独立，每一维是一个二分类器，单个样本的概率（似然）是：P₁ʸ¹ ⋅ P₂ʸ² ⋅ \u0026hellip; ⋅ Pₖʸᵏ， 所以需要用 softmax 做一下归一化\nMSE 与 CE 区别 (Google search: \u0026ldquo;为什么不用mse做损失函数\u0026rdquo;)\nMSE 不适合分类问题² 工程角度：如果用MSE做分类，对 softmax 的输出使用 MSE，即正确类的概率越接近 1 越好，其他类的概率越小越好: minimize Loss = (prob_true-1)² + ∑(prob_other)²。 但是在 Loss 的梯度表达式中存在 prob_true 这个因子，可能在训练初期 prob_true 很小，梯度趋于0，无法更新。 而在用 CE 做Loss时，它的梯度中不含有单独的 prob_true 这一项（被消掉了），就不易发生梯度消失² 。\n(2022-11-06) 分类问题常使用 softmax，所以适合使用CE；而回归问题不常使用softmax，所以适合使用 MSE。\n理论角度：二者假设不同，MSE假设观察到的 y\u0026rsquo;=真实y+高斯噪声，所以通过极大似然法求解一组参数使得对应的高斯噪声最小的情况。所以MSE求解出来的值会更偏向于各个离散的观察值。而CE的假设应该是多分类情况下，拟合不同类别的概率分布。\u0026ldquo;多分类问题的分布符合多项式分布，CE是多项式分布的最大似然⁵\u0026rdquo;\n交叉熵不适用回归问题⁴ 在(多)分类问题中，交叉熵的损失函数只和分类正确的预测结果有关系，而MSE的损失函数还和错误的分类有关系，因此该\u0026quot;分类\u0026quot;函数除了让正确的分类尽量变大，还会让错误的分类变得平均，但实际在分类问题中，MSE 的这个调整是没有必要的 ⁶。\n把 多分类问题 中的 \u0026ldquo;类别\u0026rdquo; 对应到 多元回归问题 中的 \u0026ldquo;特征\u0026rdquo;。对于一个连续的输出量，应是由各个特征共同作用的，分别有不同的贡献，而不能只看重某一个特征，所以CE不适合回归问题。但也可以用 ⁷。\n损失函数的性质 (2023-02-17)\n可微分性 可导性 单调性 凸性 可分离性 可表示性 借助 pytorch 可视化损失函数的导数9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch import matplotlib.pyplot as plt def abs_func(x): return x.abs() x = torch.linspace(-2,2,100) x.requires_grad_(True) y = abs_func(x) plt.plot(x.detach().numpy(), y.detach().numpy()) plt.show() y_prime = torch.autograd.grad(y.sum(), x, create_graph=True)[0] plt.plot(x.detach().numpy(), y_prime.detach().numpy()) plt.show() Ref 【機器學習2021】類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)-李宏毅 “损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法” 机器学习面试之MSE与CE的区别？- 简书 交叉熵损失(Cross-entropy)和平方损失(MSE)究竟有何区别？ 为什么均方差（MSE）不适合分类问题？交叉熵（cross-entropy）不适合回归问题？ 简单的交叉熵，你真的懂了吗？ - 蔡杰的文章 - 知乎 分类模型中交叉熵比MSE更合适 - 郝曌骏- github 分类必然交叉熵，回归无脑MSE？未必 - 冷比特er的文章 - 知乎 机器学习基础学习-多元线性回归问题（数学解实现）- csdn 6.2 损失函数性质 - 梗直哥丶(质量不高，只看代码) ","date":"2022-11-06T22:23:00Z","permalink":"https://zichen34.github.io/writenotes/calc/dl_loss_functions/","title":"memo: DL | Loss Functions"},{"content":" iterable 是存储多个值的对象，并可以逐个返回它的值 generator defined by yield itertools.cycle(iterable) vs while True: 前者生成的 iterator 可以无限次重复遍历一个有限的 iterable，而while True 只会遍历一遍 iterable。 so\n1 2 3 4 5 6 7 8 9 10 def myGenerator():\t# define a iterable yield 1 yield 2 def loop_iterator(iterable=[]): for val in itertools.cycle(iterable): # turn to iterator yield val\t# this func is a generator gen = loop_iterator(iterable=myGenerator())\t# create a generator next(gen) Contrast:\n1 2 3 4 5 6 def loop_while(iterable): while True: for val in iterable: yield val\t# define a generator gen = loop_while(myGenerator()) next(gen) yield 关键字用于 generator function 中，返回一个值，但不终止函数执行，等待下一次调用再继续 GfG-yield。 Generator function 创建一个 generator object，也就是iterable，它可以用 __next__() 方法或 for 循环遍历。 定义 generator function 只需要 yield 关键字，比较方便，而定义 iterator 需要定义 __next__() 和 __iter__() 方法 GfG-gener\niterator has next iterator 是实现了__iter__() 和 __next__() 方法的对象，而 list, tuple, dict, set 都是 iterable 对象，它们都可以调用自己的 iter() 方法变成一个 iterator。 for 循环遍历 iterable 时其实创建了一个iterator，每次循环执行__next__()方法 w3school\n1 2 3 myList = [1,2,3] myIterator = iter(myList) # 或 ls.__iter__() print(next(myIterator)) # traverse the next value ","date":"2022-10-24T10:57:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_iter/","title":"memo: Python | iter"},{"content":"2022-8-21: Experiments were conducted on the code of PixeNerf. Trying to get the identical loss curves every time.\n在 train.py 中设置np.random.seed(0)和torch.manual_seed(0)，使每次训练时的图片和像素，以及验证时的object和视图是一样的；\n在 trainer.py 中设置 worker_fn\n在 nerf.py 中设置torch.manual_seed(2201)，每次取一样的随机数，loss曲线有的地方还是有0.1的差异。\ntrain_set 通过ColorJitterDataset 做了颜色增强，data.util.py中加np.random.seed(0)，然后E 0 的第1个batch 的psnr就相同了(10.55053)。\n设置 models.py 中的运算为 deterministic，pytorch=1.6.0 （Docs中）只有以下两个设置，但好像没效果。\n1 2 torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False pytorch=1.8 才能使用 torch.use_deterministic_algorithms()，于是:\n卸载conda uninstall pytorch，重装了1.12.1，报错:\npyparsing.exceptions.ParseException: Expected '}', found '='。\n又重装了 1.10.2 之后: 在from torch.utils.tensorboard import SummaryWriter处报错: AttributeError: module 'distutils' has no attribute 'version'\n降级:\n1 2 pip uninstall setuptools pip install setuptools==59.5.0 然后那个参数解析pyparsing还是报错，不知道怎么解决。 pytorch forum\n把环境删了，修改 environment.yml 中的版本：pytorch==1.11.0, torchvision==0.12.0 (版本号相差1)，重新创建环境：conda env create -f envxx.yml。然后在 torch.matmul() 处报错:\n1 2 3 4 5 6 7 RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA \u0026gt;= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility 在程序开头设置环境变量：export CUBLAS_WORKSPACE_CONFIG=:4096:8。\n又在 loss.backward() 中 autograd 时报错：\n1 2 3 4 5 6 RuntimeError: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set \u0026#39;torch.use_deterministic_algorithms(True)\u0026#39;. You can turn off determinism just for this operation, or you can use the \u0026#39;warn_only=True\u0026#39; option, if that\u0026#39;s acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.` 加上参数：torch.use_deterministic_algorithms(True, warn_only=False) 可以运行；再加上torch.backends.cudnn.benchmark = False，除batch 1外，两次实验结果仍不完全一致，而且性能下降很多，好像是pytorch版本导致的，放弃。又重装回原环境。 torch-reproducibility-doc; It\u0026rsquo;s introduced in 1.8\npython train/train.py --name dtu_origin --conf conf/exp/dtu.conf --datadir data/DTU_Dataset/rs_dtu_4 --nviews 3 --gpu_id='0 2' --epochs 400_000\n","date":"2022-10-24T10:57:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_reproducibility/","title":"memo: PyTorch | Reproducibility"},{"content":"Pytorch-Transformer Model Parallelism using Transformers and PyTorch\nLoading the data\nInstantiate a model\nCreate torch Dataset and DataLoader\n1 class myDataset(torch.utils.data.Dataset): Split the data into train and val sets:\n1 2 from sklearn.model_selection import train_test_split df_train, df_val = train_test_split(imdb_df, test_size=0.3, random_state=2021 create DataLoader for train set and val set:\nMake a wrapper for the model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MultiGPUClassifier(torch.nn.Module): def __init__(self, roberta_model): super(MultiGPUClassifier, self).__init__() # Embedding layer --\u0026gt; cuda:0 self.embedding = roberta_model.roberta.embeddings.to(\u0026#39;cuda:0\u0026#39;) # Encoder Layer --\u0026gt; cuda:1 self.encoder = roberta_model.roberta.encoder.to(\u0026#39;cuda:1\u0026#39;) # Classifier --\u0026gt; cuda:1 self.classifier = roberta_model.classifier.to(\u0026#39;cuda:1\u0026#39;) def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None): # Pass the input_ids to cuda:0 since embedding layer in cuda:0 emb_out = self.embedding(input_ids.to(\u0026#39;cuda:0\u0026#39;)) # Move the outputs of embedding layer to cuda:1 as input to encoder layer enc_out = self.encoder(emb_out.to(\u0026#39;cuda:1\u0026#39;)) classifier_out = self.classifier(enc_out[0]) return classifier_out # Initialize the model multi_gpu_roberta = MultiGPUClassifier(roberta_model) Upon constructing the model, the memory usage can be seen using nvidia-smi.\nCreate optimizer and loss function for the model:\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import get_linear_schedule_with_warmup, AdamW EPOCHS = 2 LR = 1e-5 optimizer = AdamW(multi_gpu_roberta.parameters(), lr=LR) total_steps = len(train_data_loader)*EPOCHS scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) loss_fn = torch.nn.CrossEntropyLoss().to(\u0026#39;cuda:1\u0026#39;) # match with the roberta.classifier layer Create a helper function for training the model and returning accuracy and losses:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def train_model (model, data_loader, loss_fn, optimizer, scheduler, n_examples): model = model.train() # losses = [] correct_predictions = 0 for d in data_loader: # take a batch input_ids = d[\u0026#39;input_ids\u0026#39;] attention_mask = d[\u0026#39;attention_mask\u0026#39;] # Reshaping attention mask for adapting the forward pass reshaped_attention_mask = attention_mask.reshape(d[\u0026#39;attention_mask\u0026#39;].shape[0],1,1,d[\u0026#39;attention_mask\u0026#39;].shape[1]) targets = d[\u0026#39;labels\u0026#39;] outputs = model(input_ids = input_ids, attention_mask = reshaped_attention_mask) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets.to(\u0026#39;cuda:1\u0026#39;)) # move targets to cuda:1 to calculate loss correct_prediction += torch.sum(preds == targets.to(\u0026#39;cuda:1\u0026#39;)) losses.append(loss.item()) loss.backward() # Clip the gradients of the model to prevent exploding gradients using clip_grad_norm torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() # gradient descent scheduler.step() # lr decay optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) Create a helper function for evaluating the model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def eval_model(model, data_loader, loss_fn, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for d in data_loader: input_ids = d[\u0026#39;input_ids\u0026#39;] attention_mask = d[\u0026#39;attention_mask\u0026#39;] reshaped_attention_mask = attention_mask.reshaped(d[\u0026#39;attention_mask\u0026#39;].shape[0],1,1,d[\u0026#39;attention_mask\u0026#39;].shape[1]) targets = d[\u0026#39;labels\u0026#39;] outputs = model(input_ids = input_ids, attention_mask=reshaped_attention_mask) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets.to(\u0026#39;cuda:1\u0026#39;)) correct_predictions += torch.sum(preds == targets.to(\u0026#39;cuda:1\u0026#39;)) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) Create the training loop and only store the best one:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from collections import defaultdict history = defaultdict(list) best_accuracy = 0 %%time for epoch in range(EPOCHS): print(f\u0026#39;Epoch {epoch+1}/{EPOCHS}) print(\u0026#39;-\u0026#39; * 10) train_acc, train_loss = train_model(multi_gpu_roberta, train_data_loader, loss_fn, optimizer, scheduler, len(df_train)) print(f\u0026#39;Train Loss:{train_loss}; Train Accuracy: {train_acc}\u0026#39;) val_acc, val_loss = eval_model(multi_gpu_roberta, val_data_loader, loss_fn, len(df_val)) print(f\u0026#39;Val Loss: {val_loss}; Val Accuracy: {val_acc}\u0026#39;) history[\u0026#39;train_acc\u0026#39;].append(train_acc) history[\u0026#39;train_loss\u0026#39;].append(train_loss) history[\u0026#39;val_acc\u0026#39;].append(val_acc) history[\u0026#39;val_loss\u0026#39;].append(val_loss) if val_acc \u0026gt; best_accuracy: torch.save(multi_gpu_roberta.state_dict(), \u0026#39;multi_gpu_roberta_best_model_state.bin\u0026#39;) best_accuracy = val_acc Visualizing model performance\nCombining DDP with Model Parallelism DDP tutoria\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class ToyMpModel(nn.Module): # model parallel, multi-gpu model def __init__(self, dev0, dev1): # use 2 gpu super(ToyMpModel, self).__init__() self.dev0 = dev0 self.dev1 = dev1 self.net1 = torch.nn.Linear(10,10).to(dev0) # move 0th layer to dev0 self.relu = torch.nn.ReLU() self.net2 = torch.nn.Linear(10,5).to(dev1) # move 1st layer to dev1 def forward(self, x): x = x.to(self.dev0) # move input to the dev0 same as 0th layer x = self.relu(self.net1(x)) x = x.to(self.dev1) # move output of 0th layer to dev1 return self.net2(x) DDP wraps a multi-GPU model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def demo_model_parallel(rank, world_size): # rank indicates the index of this process, world_size is the total numer of gpu will be used. print(f\u0026#34;Running DDP with model parallel example on rank {rank}.\u0026#34;) setup(rank, world_size) # set env vars, and initialize process group # set up multi-GPU model and devices for this process dev0 = (rank * 2) % world_size dev1 = (rank * 2 + 1) % world_size mp_model = ToyMpModel(dev0, dev1) ddp_mp_model = DDP(mp_model) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001) optimizer.zero_grad() # outputs will be on dev1 outputs = ddp_mp_model(torch.randn(20,10)) labels = torch.randn(20, 5).to(dev1) loss_fn(outputs, labels).backward() optimizer.step() cleanup() if __name__ == \u0026#34;__main__\u0026#34;: n_gpus = torch.cuda.device_count() assert n_gpus \u0026gt;=2, f\u0026#34;Requires at least 2 GPUs to run, but got {n_gpus}\u0026#34; world_size = n_gpus run_demo(demo_model_parallel, world_size) Apply Model Parallel to Existing Modules SINGLE-MACHINE MODEL PARALLEL BEST PRACTICES ResNet50\nnn.Sequential\nGNT model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class GNTModel(object): ├ __init__(self,args) │ ├─ self.net_coarse = GNT().to(device) │ │ └─ __init__() │ │ ├─ self.rgbfeat_fc = nn.Sequential(Liner, ReLU, Linear) │ │ ├─ for i in range(args.trans_depth): │ │ │ ├─ self.view_trans.append( Transformer2D() ) # ModuleList() │ │ │ │ ├─ self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate) │ │ │ │ │ ├─ self.fc1 = nn.Linear(dim, ff_hid_dim) │ │ │ │ │ ├─ self.fc2 = nn.Linear(ff_hid_dim, dim) │ │ │ │ │ ├─ self.dp = nn.Dropout(ff_dp_rate) │ │ │ │ │ └─ self.activ = nn.ReLU() │ │ │ │ │ │ │ │ │ └─ self.attn = Attention2D(dim,attn_dp_rate) │ │ │ │ ├─ self.q_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.k_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.v_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.pos_fc = nn.Sequential(Linear, ReLU, Linear) │ │ │ │ ├─ self.attn_fc = nn.Sequential(Linear, ReLU, Linear) │ │ │ │ ├─ self.out_fc = nn.Linear(dim, dim) │ │ │ │ └─ self.dp = nn.Dropout(dp_rate) │ │ │ │ │ │ │ ├─ self.ray_trans.append( Transformer() ) # nn.ModuleList() │ │ │ │ ├─ self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate) │ │ │ │ └─ self.attn = Attention(dim, n_heads, attn_dp_rate, attn_mode, pos_dim) │ │ │ │ ├─ if attn_mode in [\u0026#34;qk\u0026#34;,\u0026#34;gate\u0026#34;]: │ │ │ │ ├─ if attn_mode in [\u0026#34;pos\u0026#34;, \u0026#34;gate\u0026#34;]: │ │ │ │ ├─ if attn_mode == \u0026#34;gate\u0026#34;: │ │ │ │ ├─ self.v_fc = nn.Linear(dim, dim, bias=False) │ │ │ │ ├─ self.out_fc = nn.Linear(dim, dim) │ │ │ │ ├─ self.dp = nn.Dropout(dp_rate) │ │ │ │ ├─ self.n_heads = n_heads │ │ │ │ └─ self.attn_mode = attn_mode │ │ │ │ │ │ │ └─ if i % 2 == 0: self.q_fc.append( nn.Sequential()) │ │ │ └─ else: self.q_fc.append(nn.Identity()) # nn.ModuleList() │ │ │ │ │ ├─ self.pos_enc = Embedder() # 21x3=63 funcs │ │ └─ self.view_enc = Embedder() # 21x3=63 funcs │ │ │ ├─ self.net_fine = GNT().to(device) │ ├─ self.feature_net = ResUNet(coarse_out_ch, fine_out_ch, signle_net).to(device) │ │ ├─ self.conv1 = nn.Conv2d() │ │ ├─ self.bn1 │ │ ├─ self.relu │ │ ├─ self.layer1 = self._make_layer(block,planes,blocks,stride,dilate) │ │ │ ├─ norm_layer │ │ │ ├─ previous_dilation │ │ │ ├─ if dilate: self.dilation *= stride │ │ │ ├─ if stride !=1 or inplanes differs from outchanl * expansion: │ │ │ │ └─ downsample = nn.Sequential(conv1x1(), norm_layer()) │ │ │ │ │ │ │ ├─ layers.append(BasicBlock(self.inplanes, planes, stride,downsample,)) # list │ │ │ │ ├─ norm_layer │ │ │ │ ├─ self.conv1 │ │ │ │ ├─ self.bn1 │ │ │ │ ├─ self.relu │ │ │ │ ├─ self.conv2 │ │ │ │ ├─ self.bn2 │ │ │ │ ├─ self.downsample │ │ │ │ └─ self.stride │ │ │ │ │ │ │ ├─ self.inplanes = planes * block.expansion │ │ │ └─ for _ in range(1,blocks): │ │ │ └─ layers.append(BasicBlock(self.inplanes,planes,)) │ │ │ ├─ norm_layer │ │ │ ├─ self.conv1 │ │ │ ├─ self.bn1 │ │ │ ├─ self.relu │ │ │ ├─ self.conv2 │ │ │ ├─ self.bn2 │ │ │ ├─ self.downsample │ │ │ └─ self.stride │ │ │ │ │ ├─ self.layer2 │ │ ├─ self.layer3 │ │ ├─ self.upconv3 │ │ ├─ self.iconv3 │ │ ├─ self.upconv2 │ │ ├─ self.iconv2 │ │ └─ self.out_conv │ │ │ ├─ learnable_params │ ├─ self.optimizer = torch.optim.Adam() │ ├─ self.scheduler │ ├─ self.start_step │ └─ if args.distributed: │ ├─ self.net_coarse = torch.nn.parallel.DDP │ ├─ self.feature_net = torch.nn.parallel.DDP │ └─ self.net_fine = torch.nn.parallel.DDP │ ├ switch_to_eval(self): │ └─ render_kwargs_train{network_query_fn, model, ...} ├ switch_to_train(self): ├ save_model(self, filename): ├ load_model(self, filename): └ load_from_ckpt(self, out_folder): │ () ├─ h In a new file: model_parallel.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class GNTModel(object): ├ __init__(self, args, load_opt=True, load_schedular=True): │ ├─ device = [\u0026#39;cuda:0\u0026#39;,\u0026#39;cuda:1\u0026#39;,\u0026#39;cuda:2\u0026#39;,\u0026#39;cuda:3\u0026#39;] │ ├─ self.net_coarse = GNT(..., device) # transformer_network_parallel.py │ ├─ __init__(self, args, in_feat_ch=...) │ ├─ super(GNT, self).__init__() │ ├─ self.rgbfeat_fc = nn.Sequential().to(device[0]) │ ├─ │ │ │ │ │ │ │ │ │ │ │ │ In train.py:\n1 from gnt.model_parallel import GNTModel Pytorch-DDP-RPC Invited Talk: PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li ytb\n(DDG search: tensorflow model split distributed parallel)\n","date":"2022-10-21T20:35:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_model_parallel/","title":"memo: PyTorch | Model Parallel"},{"content":"Elliot Waite Source video: PyTorch Autograd Explained - In-depth Tutorial-Elliot Waite\nTwo tensors are created with the values of 2 and 3 and assigned to variables a and b. Then the product of a and b is assigned to variable c.\nIn the following diagram, each struct is a tensor containing several attributes.\ndata holds the data of the tensor grad holds the calculated gradient value grad_fn, gradient function points to a node in the backwards graph is_leaf marks is this tensor a leaf of a graph requires_grad is False by default for all tensors that are being input (like a,b) into or output (like c) from an operation. Such that no backwards graph will be created. However, if the attribute requires_grad of a is set to True and a is passed into any operation (Mul), the output tensor (c) will also have requires_grad=True and be apart of the backwards graph, because c is no longer a leaf. And c\u0026rsquo;s grad_fn points to MulBackward, which calculates the gardient of its operation w.r.t. input tensor a: ∂c/∂a = ∂(a∗b)/∂a = b\nThe blue contents are the backwards graph behind the tensors and their operations.\nWhen the Mul function is called, the ctx context variable will save the values to be used in the backwards pass: MulBackward operation.\nSpecifically, the input tensor a is stored by the method ctx.save_for_backward(...) and referenced by the property ctx.saved_tensors in the backward pass.\nThe MulBackward has another attribute next_functions is a list of tuples, each one associated with an input tensor that were passed to the Mul operation.\n(AccumulatedGrad, 0) corresponds to the input tensor a meaning the gradient of a will be calculated continuously by AccumulatedGrad operation. (None, 0) is associated with input tensor b, no further calculation is needed for its gradient. The AccumulateGrad operation is used to sum the gradients (from multiple operations) for the input tensor a.\nWhen executing c.backward(), the backward pass of gradients starts. The initial gradient is 1.0 and then passed into MulBackward, where it times b getting 3.0. Then by looking at the next_functions, it needs to get into AccumulatedGrad to obtain the gradient w.r.t. tensor a. Finally, the attribute grad of a comes from AccumulateGrad.\nSimple example Two input tensors are both requires_grad=True. They\u0026rsquo;re multiplied together to get c. If here executing c.backward(), the initial gradient 1.0 will start the backward pass from its grad_fn. Then tensor d is created to multiply with c to get e. If executing e.backward() here, to calculate the gradient of e w.r.t. the leaft nodes on the graph, the backward pass is as follows: The initial gradient 1.0 is first passed into e.grad_fn, i.e., MulBackward, where it multiplies with gradient of the operation w.r.t. the leaf nodes: ∂(c∗d)/∂d (=c=6.0). (Since c is not a leaf, ∂e/∂c doesn\u0026rsquo;t need to compute.)\nThen by looking at property next_functions, the gradients go continuously into MulBackward and AccumulateGrad separately.\nIn MulBackward, to get ∂e/∂a, the incoming gradient ∂e/∂c multiplies with gradient of the Mul operation w.r.t. a: ∂(a∗b)/∂a, so the gradient of leaf a is ∂e/∂a = ∂e/∂c × ∂(a∗b)/∂a = 4×3=12. Also to get ∂e/∂b, the incoming gradient ∂e/∂c multiplies with ∂(a∗b)/∂b, ∂e/∂b = ∂e/∂c × ∂(a∗b)/∂b = 4×2 = 8 While ∂e/∂c gets into AccumulateGrad, no other operations needed to calculate the gradients w.r.t. d, the grad of leaf node d is 6.0. Avoid In-place operation When the MulBackward operation retrieve input tensors through ctx.saved_tensors, it is necessary to ensure that the correct values are referenced. Therefore, each tensor must maintains its attribute _version, which will be increamented (+1) when performing in-place operation (e.g., c+=1) each time.\nThus, if calling e.backward() after c+=1, the ctx.saved_tensors will get an error, beacuse the attribute _version of the input tensor c is not matched with the previously saved one. In this way, the input tensors to be used are ensured haven\u0026rsquo;t changed in the time since the operation was performed in the forward pass.\nHowever, the Add function doesn\u0026rsquo;t affect the graidents, so it doesn\u0026rsquo;t need to save any its input tensors for the backward pass. Hence, the ctx will not store its input tensors. And the initial gradient will directly looking at the next_functions after getting into AddBackward node. In this case, doing c+=1 before e.backward(), no errors will occur because the input tensor c is not retrieved by ctx.saved_tensors. Unbind operation A tensor is created with 1-D list holding 3 values and assigned to variable a. Then by executing b,c,d = a.unbind(), tensor a is split along the 1st dimension and tensors b, c, d are created. This operation will generate the graph as follows: All of the grad_fn of b, c, d point to the same UnbindBackward function.\nIf b, c, d are multiplied together to get e, there will be two Mul operation in the forward pass and two MulBackward is the backward pass. The property next_functions of those 2 MulBackward functions both have tuple (UnbindBackward, i ), but their indecies are different, where i is 0, 1, 2 corresponding to the 3 outputs from the Unbind function.\n(UnbindBackward, 0) means the current gradient is associated with the first input tensor b of the (1st) Mul operation, which is also the first output from the Unbind function. (UnbindBackward, 1) is saying this is the gradient for the second output fromt he Unbind function. (UnbindBackward, 2) indicates this gradient is for the third output of the Unbind function. The index value is used to inform the UnbindBackward which output tensor the gradient is calculated for. Such that the UnbindBackward can output a list of gradients.\nThe gradient of the leat node can be calculated by calling e.backward(). The backward pass is started off with the initial gradient of 1. Complicated example The following scalar tensor can be replaced with any vector or matrix or any n-dimension array.\nThe tensors are created with the same value of 2 and they both don\u0026rsquo;t require grad. a and b are multiplied together to get c, which doesn\u0026rsquo;t require grad too. Then by executing c.requires_grad = True, the tensor c will be a port of the backwards graph. So that any future operations done using c as an input will start to build the backwards graph.\nThe full forward graph is build as:\nAnother tensor d is created with requires_grad=False. Multiply d with c to get e. Another leaf f is constructed with requires_grad=False (not on the graph). Multiply f with e to get g Another tensor h is created with requires_grad=True (a leaf on the graph). Devide g by h to get i; Add i and h together to get j. (h is leaf and fed into 2 operations, so its AccumulteGrad has two inputs from DivBackward and AddBackward. Multiply j and i together to get k. (i is passed into both Add and Mul, so its (grad_fn) DivBackward has 2 streams from AddBackward and MulBackward. Unlike h has its 2 backward streams converge at AccumulateGrad, the 2 backward streams of i instead converge at the DivBackward that corresponds to the operation Div generating the i. (Yellow means the leaf that doesn\u0026rsquo;t on the graph. Green means the leaf on the graph. Brown means the non-leaf)\nAt the end, by executing k.backward(), the backward pass will start with a gradient of 1.0, which will times the gradient of each operation w.r.t. local input tensors sequentially from bottom to top. If an operation\u0026rsquo;s input is a leaf, the next_functions will point to the AccumulateGrad for this leaf node. Once all gradients streams are accumulated, the sum is put into the attribute grad of the left node. Finally, ∂k/∂leaf will obtained. retain_grad() By default, the gradients will only calculated for the leaf nodes, and the grad of the intermediate nodes are kept None. But an intermediate tensor can retain their gradient by calling its retain_grad() method. For example, i.retain_grad(), which will set up a hook \u0026ldquo;that gets called in the backward pass that will basically tell the DivBackward function that any gradients passed into it should be saved on the grad attribute of tensor i. detach() m = k.detach() will create a new tensor sharing the same underlying data as k, but m will no longer require gradients. m will be a leaft node but not on the graph because its grad_fn is None without pointing any node on the graph.\nUsually, the backwards graph is expected to get garbage collected after finishing the training loop. Once the k.backward() is executed, some values have no references (get freed) in the graph. Specifically, the references to the saved_tensors. But the actual graph still exists in memory. If the output tensor k is needed to kept for longer than the training loop, k can be detached from the graph.\nSimilar functions:\nk.numpy() : ndarray k.item() : python int or python float k.tolist() : original tensor that holds multiple values will be converted to python list (old summary on 2022-10-19)\nThose tensors whose attribute requires_grad=True will be nodes on a backwards graph. Any output tensor yielded from any operation is not leaf node.\ngrad_fn \u0026amp; grad The grad_fn of the \u0026rsquo;non-leaf\u0026rsquo; (intermediate) node points to a node (like MulBackward) which will multiply the incoming gradient by the gradient of this operation w.r.t. its inputs (leaf nodes with requires_grad=True), or pass the grad of the \u0026rsquo;non-leaf\u0026rsquo; node up to next gradient-computing node for other leaf nodes before. This\u0026rsquo;s like the divergence of ∂L/∂wᶦ and ∂L/∂wᶦ⁻¹.\nIf an \u0026ldquo;non-leaf\u0026rdquo; node is passed into multiple operations, its gradient equals the sum of the gradients coming from each operation. Its gradient will be fed into the \u0026ldquo;gradient-computing\u0026rdquo; node as the incoming gradient of the operation that generates it.\nWhile if a leaf node is used by multiple functions, its gradient equals to the sum of the gradients comeing from the \u0026ldquo;gradient-computeing\u0026rdquo; nodes of every operation. Hence, the grad of the leaf node is accumulated by AccumulateGrad function.\nctx ctx (context) stores the operators doing the operation for computing the gradient in backward graph. The gradients at different time are different, so the version of variables needs to be recorded. The in-place operation will increment the version of the variable, and then calling the backward method will cause an error due the mismatch of the _version value. However, if an operation doesn\u0026rsquo;t bring gradients (like Add), its operators are not necessary to store. The incoming gradient will not change when passing through its corresponding backwards function (AddBackward). In this case, modifying its operators before calling .backward() is okay, becaue they are not involved with gradient. Gradient for a one-dim tensor is a list of partial derivative. The second value in the tuple is the index among the mutliple outputs of the operation, indicating to who the gradient belongs\noutput don\u0026rsquo;t have grad by default The \u0026ldquo;non-leaf\u0026rdquo; node will not store grad by default, unless set its .retain_grad(), from which a hook can be set up to make its grad_fn to save any gradient passed to it into grad of this node.\ndetach() m=k.detach() will create a leaf node m sharing the same data as k, but will no longer require gradient. Its grad_fn=None meaning it doesn\u0026rsquo;t have a reference to the backwards graph. This operation is used to store a output value separately, but free the backwards graph after a training loop (forward+backward). Alternatives for detaching from the graph:\nk.numpy(), k.item() convert one-element tensor to python scalar. k.tolist(). (2022-11-01)\ngradient is the upstream gradients until this calling tensor (from the intermediate node to the network ouput). Since the explicit expression of output is unknown, the derivative $\\rm\\frac{ d(output) }{ d(input) }$ cannot be calculated directly. But the derivative $\\rm\\frac{ d(output) }{ d(intermediate) }$ and $\\rm\\frac{ d(intermediate) }{ d(input) }$ are all known. Therefore, by passing the d(output)/d(intermediate), say 0.05, to gradient, the d(output)/d(input) is the returned value of intermeidate.backward(gradient=0.05,). The gradient argument in PyTorch backward - devpranjal Blog (2023-08-02)\nFreeze params module.requires_grad_(False) will change all its parameters. PyTorch Forum\nIf I explicitly add the learnable params one-by-one, like this tutorial (if p.requires_grad), only the ordinal of the layers in the state dict ckpt['optimizer]['state'] changed, while the total number of states remains the number of layers to be performing gradient descent.\n1 2 3 4 5 6 7 # Test in GNT with multiple encoders: self.optimizer.add_param_group({\u0026#34;params\u0026#34;: self.multiEncoders.parameters()}) # Saved `ckpt[\u0026#39;optimizer\u0026#39;][\u0026#39;state\u0026#39;]` has a max index: 1390, but total 262 layers. params = [p for p in self.multiEncoders.parameters() if p.requires_grad] self.optimizer.add_param_group({\u0026#34;params\u0026#34;: params}) #`ckpt[\u0026#39;optimizer\u0026#39;][\u0026#39;state\u0026#39;]` has a max index: 281, but total 262 layers too. with torch.no_grad(): will stop backward-propagation for a block of operations wrapped in its region (in a context). This is equivalent to module.requires_grad_(False). Demo-SO\nInference mode is similar with no_grad, but even faster. module.eval() = module.train(False) affects the settings of nn.Dropout and nn.BatchNorm2d. And it has nothing to do with grad. Docs - Autograd\nIn pixelNeRF, self.net.eval() and self.net.train() will jump to SwinTransformer2D_Adapter.train(mode=True) requires_grad_ vs requires_grad requires_grad_ can do for non-leaf node, while requires_grad will have error. PyTorch Forum\nTwice nn.Parameter (2024-04-11)\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from torch import nn from torchviz import make_dot x = torch.rand(2,3) w = torch.rand(3,2).requires_grad_(True) for i in range(2): f = nn.Parameter(w @ x) # shape (3,3) loss = f.sum() loss.backward() # dot = make_dot(loss, params={\u0026#39;w\u0026#39;: w, \u0026#39;x\u0026#39;: x}) # dot.render(\u0026#39;computational_graph\u0026#39;, format=\u0026#39;png\u0026#39;) As shown above, once \u0026ldquo;re-convert\u0026rdquo; the w@x as the nn.Parameter, the forward graph doesn\u0026rsquo;t include w and the operation Matrixmultiplication. The graph is starting from f instead.\nTherefore, even though w is a leaf node and require_grad is True, it won\u0026rsquo;t obtain .grad: w.grad is None.\nIn contrast, do not re-set the nn.Parameter won\u0026rsquo;t change the leaf tensors.\n1 2 3 4 5 6 7 import torch x = torch.rand(2,3) w = torch.rand(3,2).requires_grad_(True) for i in range(2): f = w @ x loss = f.sum() loss.backward() In this way, the leaf tensors are prepared outside the for loop (specifally forward()). And the leaf tensors (i.e., w) will be added onto the graph correctly during each iteration rebuilding.\nGraph in For Loop (2024-04-11)\nThe computational graph (of PyTorch 1.x) is build in each iteration, as the previous graph has been destroyed after executing .backward()\nTherefore, the tensors to be optimized, i.e., the leaf node of the graph, have to be present within the for loop as the starting point of the graph.\nIn the following code, w needs to be optimized.\n1 2 3 4 5 6 7 8 9 import torch x = torch.rand(2,3) w = torch.rand(2,3).requires_grad_(True) optimizer = torch.optim.Adam([w], lr=0.0001) for i in range(2): f = w * x loss = f.sum() loss.backward() optimizer.step() The forward graph can be plotted as:\nThe endpoint (green box) is the tensor loss The w has performed Multiplication and Sum to get loss The graph will be rebuilt in each iteration, with the w and x as the starting points. And the graph will be freed after loss.backward() optimizer.step() updates leaf tensors that require grad. Docs Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch from torchviz import make_dot x = torch.rand(2,3) w = torch.rand(2,3).requires_grad_(True) optimizer = torch.optim.Adam([w], lr=0.0001) for i in range(2): f = w * x loss = f.sum() loss.backward() optimizer.step() if i == 0: dot = make_dot(loss, params={\u0026#39;w\u0026#39;: w, \u0026#39;x\u0026#39;: x}) dot.render(\u0026#39;computational_graph\u0026#39;, format=\u0026#39;png\u0026#39;) However, if moving the operation f=w*x outside the for loop, the graph only contain an operation: f.sum()\n1 2 3 4 5 6 7 import torch x = torch.rand(2) # leaf w = torch.rand(2).requires_grad_(True) # leaf f = w * x # non-leaf tensor won\u0026#39;t have .grad for i in range(5): loss = f.sum() loss.backward() From the second run, the rebuilt graph only includes \u0026ldquo;Sum\u0026rdquo;, and no longer includes w (out of this graph). In other words, the w doesn\u0026rsquo;t belong to this newly rebuilt graph.\nTherefore, the .backward() method cannot be completed for w.\nw ∗ x = f r f e o b w u a i r f l d o d ( r ) t l h o e o p g r l a o p s h s e a c h i t e r In the second iteration, the graph doesn\u0026rsquo;t include w. So, when the gradient backpropagates, the gradient cannot reach w.\nOn the other hand, the w still points to the previous graph, without updating.\nError:\n1 2 3 4 5 6 7 File \u0026#34;/home/yi/Downloads/CppCudaExt_PT_Tut_AIkui/test_debug.py\u0026#34;, line 8, in \u0026lt;module\u0026gt; loss.backward() RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. ","date":"2022-10-19T21:17:00-05:00","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_autograd/","title":"memo: PyTorch | Autograd"},{"content":"手写数字数据集有10个类别，\n如果对每一类别按照二分类问题（是/不是）计算概率的话，每种类别的概率互相独立，与真实情况中，各结果之间互相抑制的事实矛盾。\n样本的分类结果满足一个概率分布，这就要求属于各类的概率都要大于0，而且概率之和为1。\n二分类问题只需要计算一个概率（另一个是互补），所以十分类问题只需要计算9个概率，但是第10个分类的计算方式与前9个不统一，就导致需要构造一些额外的计算图处理特殊情况，无法最大化地实现并行计算，所以希望所有类别的概率运算处理都是一样的。\n所以除了最后一层，前面的层还是用sigmoid，最后一层用softmax激活函数，满足：\n$$ \\begin{cases} P(y=i) ≥ 0 \\ \\sumᵢ₌₀^9 P(y=i) =1 \\end{cases} $$\n假设 $Z^l \\in \\mathbb R^K$ 是最后一个线性层 $l$ 的输出，共有K个类别，则经过 Softmax函数，线性层的输出变成概率分布：\n$$ P(y=i) = \\frac{e^{Z_i}}{\\sum_{j=0}^{K-1} e^{Z_j}},\\ i\\in{0,\\cdots, K-1 } $$\n分子使用指数运算从而恒大于零，分母是各输出之和，实现归一化。\n对于二分类问题（样本标签Y=1,0）交叉熵：$-(1\\cdot log \\hat{Y} + 0\\cdot log(1-\\hat{Y}))$。\n对于三分类问题（样本标签Y=1,0,0），交叉熵：$-(1 \\cdot log \\hat{Y}₁+ 0 + 0)$\n不管有多少类，只有1项是非零的。零项对训练没有意义，所以损失函数直接写为：$Loss(\\hat{Y}, Y) = -Y log \\hat{Y}$\n例如最后一个线性层的输出为：\n$$ [^{_{0.2}} _{^{0.1} _{-0.1}}] \\overset{\\rm Exponent}{\\longrightarrow} [^{_{1.22}} _{^{1.11} _{0.90}}] \\overset{\\rm Divide\\ sum}{\\longrightarrow} [^{_{0.38}} _{^{0.34} _{0.28}}] \\overset{-Y log \\hat{Y}}{\\longrightarrow} Loss $$\n对预测值先求对数，再数乘以样本 label (-Y)，被称为Negative Log Likelihood Loss (NLLLoss)，用numpy实现此计算过程：\n1 2 3 4 5 6 import numpy as np y = np.array([1,0,0]) #样本标签 z = np.array([0.2, 0.1, -0.1]) #线性层的输出 y_pred = np.exp(z) / np.exp(z).sum() #预测值归一化 loss = (- y* np.log(y_pred)).sum() #取对数乘以-Y，就是NLLLoss print(loss) 如果把softmax函数也算到损失函数中，在pytorch中叫做交叉熵损失：Torch.nn.CrossEntropyLoss()。这样的话，神经网络的最后一个线性层不要做激活，直接传给交叉熵损失：\n1 2 3 4 5 6 import torch y = torch.LongTensor([0]) #长整型 (第0个类别) z = torch.Tensor([[0.2, 0.1, -0.1]]) #线性层输出 criterion = torch.nn.CrossEntropyLoss() #定义损失函数 loss = criterion(z,y) #计算损失 print(loss) Mini-Batch: batch_size=3。\n1 2 3 4 5 6 7 8 9 10 11 12 import torch criterion = torch.nn.CrossEntropyLoss() Y = torch.LongTensor([2,0,1]) #三条样本，分别属于第2类，第0类，第1类,用于索引真实类别对应的预测值 Y_pred1 = torch.Tensor( [0.1, 0.2, 0.9], #(2)classified [1.1, 0.1, 0.2], #(0)classified [0.2, 2.1, 0.1]) #(1)classified Y_pred2 = torch.Tensor( [0.8, 0.2, 0.3], #(0)misclassified [0.2, 0.3, 0.5], #(2)misclassified [0.2, 0.2, 0.5]) #(2)misclassified loss1 = criterion(Y_pred1, Y) #损失较小 0.4966 loss2 = criterion(Y_pred1, Y) # 1.2389 print(\u0026#34;Batch Loss1=\u0026#34;, loss1.data,\u0026#34;\\nBatch Loss2=\u0026#34;,loss2.data) MNIST Dataset 图像是28×28的矩阵。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## 引入包 import torch from torchvision import transforms #处理图像 from torchvision import datasets from torch.utils.data import DataLoader import torch.nn.functional as F #激活 import torch.optim as optim ## 准备数据 batch_size = 64 transform = transforms.Compose([ #把一系列对象组成一个pipeline transforms.ToTensor(), #把整数像素值0-255转变为图像张量：值0-1，维度：CxWxH (1x28x28)，方便卷积 transforms.Normalize((0.1307,), (0.3081,)) ]) #归一化，减去均值,除以标准差, 使所有的像素值满足0-1分布 train_dataset = datasets.MNIST(root=\u0026#39;../dataset/mnist/\u0026#39;, train=True, download=True, transform=transform) #读取数据时就做转变 train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size) test_dataset = dataset.MNIST(root=\u0026#39;../dataset/mnist/\u0026#39;, train=False, download=True, transform=transform) test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) #不打乱，每次测试顺序一样，方便对比结果 ## 设计模型 class Net(torch.nn.Module): def __init__(self): self.l1 = torch.nn.Linear(784, 512) #线性层把784维变成512维 self.l2 = torch.nn.Linear(512, 256) #将到256 self.l3 = torch.nn.Linear(256, 128) #将到128 self.l4 = torch.nn.Linear(128, 64) #将到64 self.l5 = torch.nn.Linear(64, 10) #将到10，输出(N,10)的矩阵 def forward(self, x): #向前计算输出 x = x.view(-1, 784) #改变张量的形状，把一张图像变成一个二阶的张量（矩阵）784列，-1表示自动计算行数N x = F.relu(self.l1(x)) #输入l1，对输出做激活 x = F.relu(self.l2(x)) x = F.relu(self.l3(x)) x = F.relu(self.l4(x)) return self.l5(x) #最后一个线性层不激活 model = Net() ## 构造损失和优化器 criterion = torch.nn.CrossEntropyLoss() #经过softmax，求对数，乘以-Y optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) #模型较大，用冲量 ## 训练和测试 def train(epoch): #一轮训练的运算 running_loss = 0.0 for batch_idx, data in enumerate(train_loader, 0): #取出训练样本 inputs, target = data #取出样本和标签 optimizer.zero_grad() #梯度清零 outputs = model(inputs) loss = criterion(outputs, target) #前馈:计算输出和损失 loss.backward() #反馈 optimizer.step() #更新一步权重 running_loss += loss.item() #累计损失 if batch_idx %300 == 299 #每300批（因为从0开始数）输出一次loss print(\u0026#39;[%d, %5d] loss: %.3f\u0026#39; % (epoch+1, batch_idx + 1, running_loss/300)) running_loss = 0.0 def test(): correct = 0 total = 0 with torch.no_grad(): #不需要反向传播，就不需要计算梯度 for data in test_loader: images, labels = data #取出测试样本及其标签 outputs = model(image) #计算预测值 Nx10 的矩阵 _, predicted = torch.max(outputs.data, dim=1) #找出每一行中最大值的下标, 即所属类别，和它的值。dim=1表示沿着行方向寻找（0是列方向） total += labels.size(0) #测试样本总数N correct += (predicted == labels).sum().item() #正确分类的个数 print(\u0026#34;Accuracy on test set: %d %%\u0026#34; % (100*correct /total)) if __name__ == \u0026#39;__main__\u0026#39;: for epoch in range(10): #训练10轮 train(epoch) if epoch %10 ==9: test() ","date":"2022-10-18T22:11:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/9_%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/","title":"watch: PyTorch - 刘二 09 | Multi-classification Task"},{"content":"The first paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\nSaliency in images unique features 1\nSaliency map A scalar quantity representing the conspicuity to guide the selection of attended locations, based on the spatial distribution of saliency. 2\nSteps:\nProcess input image: colors, intensity and orientations Color: the input image is convert to red-green-blue-yellow color space. Intensity: the input image is converted to grayscale. Orientation feature: the input image are processed by Gabor filters with respect to 4 angles Create Gaussian pyramids for each of the 3 processed images Saliency map is the mean of 3 Gaussian pyramids Improvements in saliency Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet 3 从 CNN feature map 中预测 saliency\nSaliency Detection by Multi-Context Deep Learning 4\nAnother Saliency Map ? To interpret how a deep learning model achieve classification. One way is to see the amount of contribution to the predicted class score for each pixel .\nSince pixels are combined together with different weights (𝐖ᵀ𝐈+𝐛) to form ultimately another representation to perform classification, the weight of the pixel indicates its importance to the class label.5\nThe gradient of the objective function (correct label score Sc=𝐖ᵀ𝐈+𝐛) with respect to a pixel: w=∂Sc / ∂I\ncode: Ref What Are Saliency Maps In Deep Learning?\rA Model of Saliency-Based Visual Attention for Rapid Scene Analysis - Laurent Itti, Christof Koch and Ernst Niebur 98 pami Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet\rSaliency Detection by Multi-Context Deep Learning Saliency Map for Visualizing Deep Learning Model Using PyTorch -medium-Irfan\r","date":"2022-10-17T14:29:00Z","permalink":"https://zichen34.github.io/writenotes/vis/saliency_map/","title":"memo: Vis | Saliency map"},{"content":"Source video: “随机梯度下降、牛顿法、动量法、Nesterov、AdaGrad、RMSprop、Adam”，打包理解对梯度下降法的优化\n梯度下降法在实际应用中的优化：\n减小每次梯度下降的计算量：随机（分批次）梯度下降 减少迭代次数，即优化下降路径 随机梯度下降 对于一个凸问题，时间复杂度与误差量级的关系：\n对于一个强凸问题，能收敛得更快\n正常情况下，标准梯度下降法（1个batch）应该比随机梯度下降法快，但可以证明，不会快过 1/k\n一个 batch 上的损失函数，可能与整个数据集上的损失函数不同，各处对应的梯度也不同， 所以每次迭代时的梯度方向不一定是“全体数据的误差函数”上的最优，每一步的行进可能会偏离下降最快的最优路径，从而导致需要更多次的迭代，才能到达极值点。\n如下图，从 A 点 到 A\u0026rsquo; 点的最优路径是橙色线，如果分两步，先走到 B ，B 再沿着它的梯度方向走，就走偏了。\n减小步长，可以让下降路径更贴近最优下降路径，但是计算量太大。\n牛顿法 牛顿法是用来拟合曲线的，在梯度下降中，就是拟合损失函数表面上的最优下降路径对应的曲线。\n对于一个只有一维变量的问题，纵轴是各变量取值对应的误差，蓝色曲线即是损失函数。 要到达损失函数的最小值处，根据梯度下降法，先求出损失函数在当前点的梯度（各个方向分量，按向量加法相加），这里只有一个变量（一个方向），就是求损失函数的切线。 然后变量沿着梯度（切线）方向移动一点，看看此时的误差值。\n抛物线比直线更贴近损失函数，从而使下降路径与损失函数更贴合，而不是折线。 因为整个数据集上的损失函数未知，每下降一步，就在当前点的邻域范围内做泰勒展开，用一段高次函数对损失函数做近似代替； 又因为是找下降的方向，所以要保留到二次项，这样就能 拟合出在损失函数（表面）上的下降路径。 牛顿法的每一步是确定的：抛物线的顶点对应的横坐标就是这一步要走到的位置，所以牛顿法里没有步长。 下降的方向就不是梯度的方向了?\n如上图，灰色的直线是到极值点的最优路径，但是未知。 牛顿法希望每一步都走在损失函数上，即拟合出损失函数（表面）上的最优下降路径。\n泰勒展开保留二阶导，用二次函数近似表达损失函数上的每一点：\nf(x) = J(a₀) + J\u0026rsquo;(a₀)(x-a₀) + 1/2 J\u0026quot;(a₀)(x-a₀)²\n求 f(x) 的极值，就是求顶点所在位置，令 f\u0026rsquo;(x) = 0:\n$$ f\u0026rsquo;(x) = 0 + J\u0026rsquo;(a₀) + J\u0026quot;(a₀)(x-a₀) = 0 \\\\ x = a₀ - J\u0026rsquo;(a₀) / J\u0026quot;(a₀) $$\n然后让变量走到顶点的位置，对应于权重更新：W = W - J\u0026rsquo;(a₀) / J\u0026quot;(a₀)，没有学习率η。 用一阶导数与二阶导数的比值对（一元）变量W做更新。path-int\n直观理解：按固定步长做梯度下降法是橙色线，步长无穷小时，下降路径是灰色线，牛顿法的下降路径是绿色线，比梯度下降法更贴近最优路径的灰色线。 所以牛顿法是在拟合最优路径对应的曲线。\n对高维函数求二阶偏导，需要算 Hessian 矩阵。对于高维的损失函数 J(W)，参数更新公式为： 𝐖 = 𝐖 - 𝛁𝐉²(𝐖)⁻¹ ⋅ 𝛁𝐉(𝐖)，其中 𝛁𝐉²(𝐖) 就是 Hessian 方阵 𝐇(𝐖)\n$$ 𝐇(𝐖) = \\begin{bmatrix} \\frac{∂}{∂W₁}\\frac{∂f}{∂W₁} \u0026amp; \\frac{∂}{∂W₂}\\frac{∂f}{∂W₁} \u0026amp; \u0026hellip; \u0026amp; \\frac{∂}{∂Wₙ}\\frac{∂f}{∂W₁}\\\\ \\frac{∂}{∂W₁}\\frac{∂f}{∂W₂} \u0026amp; \\frac{∂}{∂W₂}\\frac{∂f}{∂W₂} \u0026amp; \u0026hellip; \u0026amp; \\frac{∂}{∂Wₙ}\\frac{∂f}{∂W₂}\\\\ \u0026hellip; \\\\ \\frac{∂}{∂W₁}\\frac{∂f}{∂Wₙ} \u0026amp; \\frac{∂}{∂W₂}\\frac{∂f}{∂Wₙ} \u0026amp; \u0026hellip; \u0026amp; \\frac{∂}{∂Wₙ}\\frac{∂f}{∂Wₙ}\\\\ \\end{bmatrix} $$\n列向量更新公式：𝐖ₙₓ₁ = 𝐖ₙₓ₁ - 𝐇(𝐖)⁻¹ₙₓₙ ⋅ 𝛁𝐉(𝐖)ₙₓ₁\n虽然迭代次数比梯度下降法少，但并不实用，缺点:path-int\n计算量太大，每一步都要计算n维向量的一阶差分 O(n)，Hessian矩阵 O(n²)，及其逆 不能保证目标函数值在迭代过程中一直下降，可能会先升高，再下降； 不能保证收敛：因为牛顿法使用二阶泰勒展开近似，需要初始点在极小点附近，每一步都满足近似条件（Hessian矩阵是正定的），效果才会比较好。 如果离得很远，获得的结果可能非常奇怪。一般应用其他方法先搜索到极小点附近，再用牛顿法（或拟牛顿法）来继续更高精度的搜索。 如果目标函数 f(x) 只是一阶可微，二阶不可微（Hessian矩阵不存在），牛顿法就不适用了。 如果二阶可微，理论上牛顿法收敛速度比梯度法要快。牛顿法收敛阶数至少是2，梯度法收敛阶数最差情况下是1。 如果是一个多元凸函数，但是不是处处可导，Taylor近似展开不能适应，牛顿法不可应用。 若 H 不可逆，需要用 Levenberg-Marquadt 修正：加常量阵 λ𝐈，即给对角线加上足够大的值，使所有的 eigenvalue 都大于0，意味着在任何方向上的一阶导数都大于0. path-int 动量法 牛顿法同时考虑损失函数的所有维度，找出最优下降路径。\n梯度是一个多维的向量，可以把各个维度拆开（向量的分解），单独分析每个方向上的变化\n加权是要把维度分开考虑\n考虑下降两步，抵消相反方向的梯度 惯性 mv，力作用的效果？把速度抵消，减少震荡\n前几步梯度大，直接加的话，占主导，而且不准，用一个系数控制前后两个部分的权重就用 beta 和 1-beta （exponentially weighted moving average），越是先前发生的状态，乘以的（1-beta）次数越多，占比越来越小，对当前的值影响越小\nNesterov 等高线坐标系下的点是权重W，Nesterov 把上一时刻的权重按照上一时刻的梯度方向走了一步，没有与当前时刻的梯度加权，把权重点移动到的新位置，这个位置是比如果采用加权时的权重超调一些的，把新位置的梯度带回去，也就是考虑了下一步位置的情况\n当前梯度方向d1，历史梯度方向d2，未来梯度方向d3，共同决定向哪个方向走。未来就是先按d2走一步梯度下降，走到一个点，计算那个点的梯度。不是按d1和d2的加权方向走，那样就是提前做了决策，还把决策的结果拿过来用了，应该用历史数据来决策\nAdaGrad 当前梯度除以累计历史梯度内积再开根号，梯度的各方向分量数量级不一致，所以除以累计历史梯度模长，梯度值大的那个方向，除以一个数（可能与该方向数值在同一数量级）就没那么大了，不至于在那个方向上走太多，而且分母越加越大，梯度越往后越小。 稀疏数据集：只要关心维度/特征的有无就能把类别分开，而不需关心特征是否明显（猴子和人）；非稀疏数据：需要关注同一特征上的差别，在同一特征上的数值，才能分类（长尾猫和短尾猫） 稀疏数据不同样本没有共同的特征，不同特征的绝对值差异大，数值相减后差值大，梯度大\n维度越多，越不需要区分在同一特征上的数值差异，只需关心特征的有无，单位球内的点分散到各个维度上了，不再挤在同一维度上了，各个轴上的长度就不用那么长了，所以体积就变小了\n王木头解释成学习率的衰减不太合适\n梯度按泰勒公式展开，牛顿法用了二次项，动量法修正0次项基础值，AdaGrad修正1次项（梯度变化量）\n","date":"2022-10-02T12:07:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/08_%E6%94%B9%E8%BF%9B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/","title":"watch: DL - 王木头 08 | Advanced Gradient Descent"},{"content":"10“拉格朗日对偶问题”如何直观理解？“KKT条件” “Slater条件” “凸优化”打包理解\n10.1 拉格朗日乘数法 一种寻找多元函数在其变量受到一个或多个约束条件时的极值的方法（自变量的取值范围有限制）\n将一个有n个变量与k个约束条件的最优化问题转换为一个解有 n+k 个变量的方程组的解的问题。\n对每个约束条件用拉格朗日乘子(待定系数) λᵢ 加权，加到目标函数后面，就是拉格朗日函数。 拉格朗日函数与目标函数的最值是一样的，所以求目标函数的最值就转化为求拉格朗日函数的最值。\n$$ \\begin{aligned} 目标函数求最小值：\u0026amp; min\\ f_0(𝐱),\\quad 𝐱 ∈ ℝⁿ \\\\ m个约束条件：\u0026amp; s.t. \\quad f_i(𝐱) ≤ 0,\\ 其中 i=1,2,3\u0026hellip;m \\\\ 拉格朗日函数：\u0026amp; L(𝐱,\\pmb{λ}) = f_0(𝐱) + \\sum λ_i f_i(𝐱) \\end{aligned} $$\n目标函数被加上了约束条件，变量只能在规定的范围内取值，求导得到的极值点可能不在规定范围内，而在规定范围内可能也没有极值点。拉格朗日乘数法把带约束问题转化为无约束问题。\n每个约束条件用拉格朗日乘子加权\n10.2 用梯度理解Lagrange Multiplier 一个约束条件的情况：\n求 $f(x,y)$ 的最小值，并且有一个约束条件 $y=g(x)$。\n该问题的拉格朗日函数为：$L(x,y) = f(x,y) + λ(y-g(x))$；[约束条件=0 表示一条线]\n对拉格朗日函数求梯度，梯度等于0的点就是极值对应的点；\n把 $∇ L(x,y) = 0$，沿x,y两个方向展开，调整λ使两个方向上的偏导都为0，也就是梯度在两个方向上的分量都为0：\n$$ \\begin{cases} \\frac{∂f(x,y)}{∂x} + λ\\frac{∂(y-g(x))}{∂x} = 0 \\\\ \\frac{∂f(x,y)}{∂y} + λ\\frac{∂(y-g(x))}{∂y} = 0 \\end{cases} $$\n1 2 在图1中，坐标系是x-y，同心圆是目标函数 $f(x,y)$ 的等高线，圆心点对应的函数值最小，越往外值越大。红线是(x,y)约束条件。\n图2显示了两个函数的梯度，只有在相切的位置，目标函数的梯度方向与约束条件的梯度方向才是共线的，再通过拉格朗日乘子 λ 调整向量长短，使两个梯度相加才可能为零。除了相切点位置，都无法实现两梯度之和为零。\n多个约束条件：\n$$ \\begin{aligned} 目标函数：\u0026amp; min \\ f(𝐱), \\quad 𝐱∈ ℝⁿ \u0026amp; \\text{(𝐱 是个n维向量)}\\\\ m个约束条件：\u0026amp; s.t. \\quad g_i(𝐱) = \\pmb{a_i^T} ⋅ 𝐱 + b_i ≤ 0, \u0026amp;\\text{(m个超平面围成的区域)}\\\\ \u0026amp; 其中i=1,2,3\u0026hellip;m, \\pmb{a_i} ∈ ℝⁿ, \\ b_i ∈ ℝⁿ \\\\ 拉格朗日函数：\u0026amp; L(𝐱, \\pmb λ) = f(𝐱) + \\sum λ_i g_i(𝐱) \\end{aligned} $$\n假设有5个一维的约束条件，就是5条直线，并且假设它们围成了一个五边形，还要假设五边形的内部是5个约束条件同时满足的区域，如下图1。\n(对于二维平面上的一条直线:$y=ax+b$，想知道 $y≤0$ 表示的是哪块区域，可以根据(x,0)这个点判断。 $y=ax+b ≤0 \\Rightarrow x≤\\frac{-b}{a}$， 所以在$\\frac{-b}{a}$左侧就是规定的区域。 如果a是负数，符号就会改变，从而多条直线能围出一个封闭的限制区域。 )\n1 2 对拉格朗日函数求梯度，令其等于零：\n$$ \\begin{aligned} \\pmb ∇ L(𝐱,\\pmb λ) = 0 \\Downarrow -\\pmb ∇ f(\\pmb λ) = ∑ λ_i ⋅ \\pmb∇ g_i(𝐱) \\end{aligned} $$\n目标函数梯度的反方向应等于所有约束条件的梯度加权和。\n由图2可知，真正起贡献的只有两个约束条件在，只需求两个约束条件的梯度和，五边形内部是 ≤ 0，则外部是\u0026gt;0，所以梯度方向指向外面，两个梯度通过 λ 调节成与目标函数梯度等大反向.\n两约束条件的交点 x\u0026rsquo; 既满足这两个约束条件，在这点的梯度=0：\n$$ \\begin{aligned} \u0026amp; g_α(x\u0026rsquo;) = g_β(x\u0026rsquo;) = 0 \\ \u0026amp; λ_α ∇ g_α(x\u0026rsquo;) + λ_β ∇ g_β(x\u0026rsquo;) = -∇ f(x) ⇒ λ_α, λ_β≠0 \\end{aligned} $$\n而该点 x\u0026rsquo; 也满足其他约束条件 gᵢ(x\u0026rsquo;)\u0026lt;0，但它们的 λᵢ 都等于0，不起作用: λᵢ∇ gᵢ(x)=0 ⇒ λᵢ=0 (i≠α,β) λ 不能小于 0，否则会把梯度方向取反，会参与梯度叠加。\n所有的 λᵢ 都≥0，如果λᵢ=0, 它对应的约束条件 gᵢ(x) 是松弛的；如果λᵢ\u0026gt;0, 它对应的约束条件 gᵢ(x) 是紧致的。约束条件像皮筋，把最优解从最值点拽到了极值点。 当目标函数的最小值落在可行域范围内，则所有的约束条件都是松弛的。\n拉格朗日乘数法本质还是求导=0，梯度等于0的点是极值点，但不一定是最值点\nKKT 条件中的互补松弛\n","date":"2022-10-01T14:14:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/10_%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/","title":"watch: DL - 王木头 10 | Method of Lagrange Multipliers"},{"content":"Source video: 5-“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”\n信息量 事件发生 概率 的负对数 -log₂ p\n若$f(p)$ 被定义为信息量，要让体系自洽则需满足：\n$$ \\begin{aligned} f(p) \u0026amp; \\coloneqq 信息量 \\\\ f(p_1 ⋅ p_2) \u0026amp; = f(p_1) + f(p_2) \\end{aligned} $$\n对于阿根廷从八强打到冠军这件事，可以拆成两件事：阿根廷进入决赛+阿根廷赢了决赛。\n这两种描述的信息量是一样的：$f$(阿根廷夺冠) = $f$(阿根廷进入决赛) + $f$(阿根廷赢了决赛) $\\Rightarrow f(\\frac{1}{8}) = f(\\frac{1}{4}) + f(\\frac{1}{2})$\n另外还要满足事件间的概率关系：P(阿根廷夺冠) = P(阿根廷进入决赛) ⋅ P(阿根廷赢了决赛)\n所以选择 log 函数，可以满足自洽；从直观来看，发生概率越小，所含信息量越大， 而 log 函数是递增的，所以系数取 -1；而底数可以选 e，也可选 2。\n$$ f(x) \\coloneqq -log_2 x $$\n底数选2，相当于用抛硬币事件来衡量信息量。某事件的发生概率是 1/8，相当于抛3个硬币全部朝上的概率。并且以 2 为底计算出的信息量的单位是比特。类似地，输入 16 比特的数据就是把 16 个 0/1 确定下来\n熵 系统中各事件信息量的 期望\n$$ \\begin{aligned} \u0026amp;H(P) \\coloneqq E(P_f) \\\\ \u0026amp;= ∑_{i=1}^m p_i ⋅ f(p_i) = ∑_{i=1}^m p_i(-log_2p_i) \\\\ \u0026amp;= -∑_{i=1}^m p_i ⋅ log_2 p_i \\end{aligned} $$\n衡量整个系统中的所有事件的不确定性\nKL散度 (相对熵) 系统Q相对于系统P差多少：事件 i 在两系统中的 信息量之差，按照 i 在系统 P 中的概率加权求和。\n$$ D_{KL}(P\\|Q) = ∑_{i=1}^m p_i ⋅ (-log_2 q_i) - ∑_{i=1}^m p_i ⋅ (-log_2 p_i) $$\n无法直接对比两个不同种类模型之间的差异（无法公度），而且人脑中的概率模型不清楚，无法求熵，需要 相对熵\nQ系统P系统的概率分布\n对于某事件 $i$ 在系统 Q 中的信息量 $f_Q(q_i)$ 减去它对应到在系统 P 中的信息量 $f_P(p_i)$，再按照在系统 P 中的概率 $p$ 求期望：\n$$ \\begin{aligned} D_{KL} (P \\| Q) \u0026amp; \\coloneqq ∑_{i=1}^m p_i ⋅ \\left(f_Q(q_i) - f_P(p_i)\\right) \\\\ \u0026amp;= ∑_{i=1}^m p_i ⋅ \\left((-log_2 q_i) - (-log_2 p_i)\\right) \\\\ \u0026amp;= \\underbrace{∑_{i=1}^m p_i ⋅ (-log_2 q_i)}_{交叉熵H(P,Q)}- \\underbrace{∑ᵢ₌₁ᵐ pᵢ⋅ (-log₂pᵢ)}_{P的熵} \\end{aligned} $$\n如果事件 i 在两系统中的信息量相等，差值为0，说明两个系统完全相等。\n根据吉不斯不等式：\n如果有两个概率系统，$∑_{i=1}^n p_i = ∑_{i=1}^n q_i = 1$，且 $p_i, q_i \\in (0,1]$，则有：\n$$ - ∑_{i=1}^n p_i log_{p_i} ≤ -∑_{i=1}^n p_i log_{q_i} $$\n当且仅当 $p_i = q_i\\ ∀ i$ 时，等号成立。\n也就是说 KL 散度恒大于等于0（距离）。\n交叉熵 $$ H(P,Q) = ∑_{i=1}^m p_i ⋅ (- \\operatorname{log}_2 q_i) $$\n事件 i 在概率系统 Q 中的信息量，按照 i 在系统 P 中的概率加权求和\n为了使 KL 散度最小，即概率系统 Q 最接近系统P，就需要交叉熵最小（最接近系统P的熵）\n对于一张图片，人脑系统（目标系统 P）只有 2 个事件：是猫和不是猫 (\u0026ldquo;是猫\u0026quot;事件发生的概率为 x，则\u0026quot;非猫\u0026quot;事件发生的概率为 1-x)， 神经网络（系统Q）的结果（y）是像猫的概率，所以交叉熵为：\n$$ \\begin{aligned} H(P,Q) \u0026amp;= ∑_{i=1}^2 p_i ⋅ (-log_2 q_i) \\\\ \u0026amp;= x ⋅ log(y) + (1-x) ⋅ log(1-y) \\\\ \u0026amp;= 1 ⋅ log(y) + 0 ⋅ log(1-y) \\end{aligned} $$\n","date":"2022-09-30T13:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/05_%E4%BA%A4%E5%8F%89%E7%86%B5/","title":"watch: DL - 王木头 05 | Info Quantity \u0026 Cross Entropy"},{"content":"4-“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”\n损失函数 定量衡量两个概率模型的差异 三种方法： 最小二乘法 极大似然 交叉熵 最小二乘法 直接比较判断结果。\nmin $\\sum_{i=1}^n (x_i - y_i)^2$\n人脑中的概率模型无法准确说出，而神经网络的概率模型蕴藏在参数里面，没有统一的表达？只能从结果入手。\n将每次人的判断结果 $x_i$ (1/0) 与 神经网络的判断结果 $y_i$ (45%:1, 55%:0) 的误差 $|x_i -y_i|$ 求和取最小，从而保证在结果上看是最接近的。\n$$ min \\sum_{i=1}^n |x_i - y_i| $$\n因绝对值在定义域上不是全程可导的，求平方不影响 x 和 y 的大小关系，而且全程可导，加1/2是为了求导方便。\n用最小二乘法作为损失函数，使用梯度下降法很麻烦\u0026hellip;\n极大似然 对于已发生的现实事件，有很多概率模型都能导致这个情况发生，取似然值最大的那个概率模型作为最“真实的”概率模型。\n由于噪声的存在，现实世界中的概率模型偏移了理想世界中的模型，由于理想世界与现实世界之间有次元壁，无法直接知道理想世界中真实的概率模型，所以只能从现实世界反推，估计出一个概率模型，它使该现实事件发生的可能性最大。\n比如掷10次硬币的结果是7次正面，3次反面。有3种概率模型：\n概率统计模型 θ 正面 反面 1 0.1 0.9 2 0.7 0.3 3 0.8 0.2 这三种概率模型都可以掷出7次正面，3次反面。不过第2种概率模型掷出7正3反的概率（似然值）最大，所以认为第2种是最接近“真实的”概率模型。\n$$ P(C₁, C₂, C₃, \u0026hellip;, C_10 | \\theta) = \\prod_{i=1}^{10} P(C_i | \\theta) $$\n似然值：用可能导致现实事件发生的概率模型，计算出来的这种情况发生的概率值.\n用已经标注好的 n 张图片（硬币正反）去训练神经网络，神经网络的概率模型 (𝐖, 𝐛) 产生出这n张图片的概率就是模型的似然值：\n$$ \\begin{aligned} \u0026amp;P(x_1, x_2, x_3, \u0026hellip;, x_n | \\mathbf {W,b}) \\\\ \u0026amp;= \\prod_{i=1}^n P(x_i | \\mathbf{W,b}) \u0026amp; \\text{$x_i \\in {0,1}$,表示\u0026quot;是猫\u0026quot;,\u0026ldquo;不是猫\u0026rdquo;}\\\\ \u0026amp;= \\prod_{i=1}^n P(x_i | y_i) \u0026amp; \\text{𝐖,𝐛决定了模型（给出的\u0026quot;是猫的概率\u0026quot;$y_i$}） \\\\ \u0026amp;= \\prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-{x_i}} \u0026amp;\\text{采样服从0-1分布: $P=\\begin{cases} y_i, \u0026amp; x=1是猫 \\\\ 1-y_i, \u0026amp; x=0不是猫\\end{cases}$} \\end{aligned} $$\n比如有7张是猫，3张不是猫，并假设神经网络在 𝐖,𝐛 的参数下，判断是猫的概率是 45%，不是猫的概率是 55%。那么此模型的似然值=$(0.45)^7 (0.55)^3$\n连乘变连加：\n$$ \\begin{aligned} \u0026amp;log \\left( \\prod_{i=1}^n y_i^{x_i} (1-y_i)^{1-x_i} \\right) \\\\ \u0026amp;= \\sum_{i=1}^n log(y_i^{x_i} (1-y_i)^{1-x_i}) \\\\ \u0026amp;= \\sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i)) \\end{aligned} $$\n当似然值达到最大的时候，就是最接近人脑的模型。\n极大似然估计：$max \\sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i))$\n习惯求极小：$min - \\sum_{i=1}^n (x_i ⋅ log y_i + (1-x_i) ⋅ log(1-y_i))$\nx 是训练数据（从真实分布中采样得到的），有自己的分布（目标分布），希望模型输出的标签与输入的标签分布一致，所以 \u0026ldquo;让学习到的分布产生输入数据的概率最大 ∑log P(输入数据 | learned分布)\u0026rdquo; 与 \u0026ldquo;最小化两分布差异 ∑ P(目标分布) log(P(learned分布))” 等价，也就是极大似然与交叉熵等价。\n","date":"2022-09-30T13:26:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/04_%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","title":"watch: DL - 王木头 04 | Loss Functions"},{"content":"类似分布式系统中的概念 1\ngroup: 当前process group (world)，一个job是一个组。一个组里面有多个主机（node） world_size: 参与job（整个网络中）的进程个数 (or gpu个数 or 数据集被切成world_size份） rank: 当前主机的编号，group内各进程的标志符是从 0 到 world_size 的连续整数 local_rank: 为当前主机内的一个进程分配GPU（每台主机可以开启多个进程（执行同一份代码）） DP vs DDP Comparison between Dataparallel and DistributedDataParallel 8\nDataParallel is single-process, multi-thread, and only works on single machine (with multiple card). While DistributedDataParallel is multi-process and works for both single- and multi- machine training. DDP works with model parallel torch.distributed 使用流程 Refer to 2\n创建进程组:\n1 torch.distributed.init_process_group(backend=\u0026#39;nccl\u0026#39;, init_method=\u0026#39;env://\u0026#39;) 如果需要 group 内集体通信，用new_group 创建子分组\n创建DDP对象：\n1 2 ddp_model = torch.nn.parallel.DistributedDataParallel( net, device_ids=[args.local_rank], output_device=args.local_rank) net位于local_rank指定的gpu上\n为数据集创建sampler:\n1 train_sampler = DistributedSampler(train_set, num_replicas=world_size, rank=rank) 确保每个进程的 dataloader 只会 load 到整个数据集的一个特定子集，而不重复 3\n在每个主机上用命令 torch.distributed.launch 启动进程（如果已开启的进程未达到world_size，则所有进程会一直等待），开始训练\n恢复训练时，给第一主机的命令加上--resume\n销毁进程组：destory_process_group()\ninit_method 指定了各进程(主机)向 rank=0 的进程发送信息的(url)地址。\nTCP 方式需要指定 rank 0 进程的ip地址 (init_method='tcp://10.1.1.20:23456')，并且需要手动指定各进程的rank。\n第2种是使用一个在同组内各进程共享的文件交换信息, url应以file://开头+文件地址，例如 init_method=file:///mnt/nfs/sharedfile，不会自动删除。\n第3种（默认）从环境变量中读取配置：MASTER_PORT, MASTER_ADDR, WORLD_SIZE, RANK。 init_method=env:// 7\n使用tcp初始化，使用3台主机，执行3次命令:\nMulti-node multi-gpu All you need to do is 4\nCreate a process group by RANK and WORLD_SIZE (auto set from the command arguments nproc_per_node,nnodes,and node_rank of torchrun) Wrap the model by torch.nn.parallel.DistributedDataParallel() move the model to gpu through LOCAL_RANK Wrap the dataset by DistributedSampler() resnet_ddp.py 5:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def train(): # read hyper params from command ... parser.add_argument(\u0026#34;--local_rank\u0026#34;, type=int, help=\u0026#34;Local rank. Necessary for using the torch.distributed.launch utility.\u0026#34;) # specify gpu index # initialize process group torch.distributed.init_process_group(backend=\u0026#34;nccl\u0026#34;) # construct model model = torchvision.models.resnet18(pretrained=False) # Wrap the model on the GPU assigned to the current process device = torch.device(f\u0026#34;cuda:{local_rank}\u0026#34;) model = model.to(device) ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) # only save (and restore) the model on the gpu whose local_rank=0 if resume == True: map_location = {\u0026#34;cuda:0\u0026#34;: \u0026#34;cuda:{}\u0026#34;.format(local_rank)} ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location)) # prepare dataset train_set = torchvision.datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True,transform=transform) # a process will only use its own subset train_sampler = DistributedSampler(dataset=train_set) tarin_loader = DataLoader(dataset=train_set, batch_size=128, sampler=train_sampler, num_workers=8) # loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5) # training cycle for epoch in range(1000): # check accuracy of the model on local_rank=0 # train mode ddp_model.train() # iter all train_set for data in train_loader: inputs, labels = data[0].to(device), data[1].to(device) outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.zero_grad() optimizer.step() if __name__ == \u0026#34;__main__\u0026#34;: train() The training script resnet_ddp.py will run on two nodes, and each of nodes has 8 gpus and each gpu would launch one process.\nIn the terminal of the first node, excute the following command.\n1 2 3 4 python -m torch.distributed.launch \\ --nproc_per_node=8 --nnodes=2 --node_rank=0 \\ --master_addr=\u0026#34;192.168.0.1\u0026#34; \\ --master_port=1234 resnet_ddp.py Excute the same command but with different node_rank on the second node:\n1 2 3 4 python -m torch.distributed.launch \\ --nproc_per_node=8 --nnodes=2 --node_rank=1 \\ --master_addr=\u0026#34;192.168.0.1\u0026#34; \\ --master_port=1234 resnet_ddp.py Single-node multi-worker Refer to 6\n1 2 3 4 torchrun --standalone --nnodes=1 \\ --nproc_per_node=$NUM_TRAINERS \\ YOUR_TRAINING_SCRIPT.py \\ (--arg1 ... train script args ...) (2023-08-27)\nExample of launch.json For running AIM\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Python: Current File\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;torch.distributed.launch\u0026#34;, \u0026#34;console\u0026#34;: \u0026#34;internalConsole\u0026#34;, \u0026#34;justMyCode\u0026#34;: true, \u0026#34;env\u0026#34;: {\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;: \u0026#34;4\u0026#34;}, \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;1\u0026#34;, // GPUs \u0026#34;--master_port\u0026#34;, \u0026#34;29500\u0026#34;, \u0026#34;tools/train.py\u0026#34;, \u0026#34;configs/recognition/vit/vitclip_base_diving48.py\u0026#34;, \u0026#34;--launcher\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;--test-last\u0026#34;, \u0026#34;--validate\u0026#34;, \u0026#34;--cfg-options\u0026#34;, \u0026#34;model.backbone.pretrained=openaiclip\u0026#34;, \u0026#34;work_dir=work_dirs_vit/diving48/debug\u0026#34; ] } ] } Pass env to args Settings below in \u0026ldquo;launch.json\u0026rdquo; don\u0026rsquo;t work:\n1 2 3 4 5 6 7 8 \u0026#34;env\u0026#34;: { \u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;GPUS\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;PORT\u0026#34;: \u0026#34;29500\u0026#34; }, \u0026#34;args\u0026#34;: [ \u0026#34;--nproc_per_node\u0026#34;, \u0026#34;${env:GPUS}\u0026#34;, \u0026#34;--master_port\u0026#34;, \u0026#34;${env:PORT}\u0026#34;, May refer to\nLaunch.json: how to reference an environment variable Resolve Environment Variables in Args defined in launch.json for Debugging #91053 Ref PyTorch 多进程分布式训练实战-murphypei-githubio Pytorch 分布式训练 - 会飞的闲鱼的文章 - 知乎 Pytorch多机多卡分布式训练 - 谜一样的男子的文章 - 知乎 Multi node PyTorch Distributed Training Guide For People In A Hurry-Lambda PyTorch Distributed Training - Lei Mao TORCHRUN (ELASTIC LAUNCH) DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED GETTING STARTED WITH DISTRIBUTED DATA PARALLEL 【pytorch distributed】nccl 集合通信（collective communication）\n","date":"2022-09-22T19:49:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_distributed/","title":"memo: PyTorch | Data Parallel"},{"content":"Basics 长伪代码跨栏 Placing a single algorithm in two columns\n1 2 \\usepackage[linesnumbered,ruled,vlined]{algorithm2e} \\usepackage{multicol} 把算法内容 用 \\begin{multicols}[2] ... \\end{multicols} 包起来，而不是在 \\begin{algorithm} ... \\end{algorithm}外面。\n文档页面旋转90度 Rotate a page 180 degrees in LaTeX\nFor the whole document:\n1 \\documentclass[landscape]{article} Table每行自动标号 Reference table row in LaTeX\n使用 counter\n1 2 3 4 5 6 7 8 9 10 11 12 13 \\usepackage{array} \\newcounter{rowcntr}[table] \\renewcommand{\\therowcntr}{\\arabic{rowcntr}} % A new columntype to apply automatic stepping \\newcolumntype{N}{\u0026gt;{\\refstepcounter{rowcntr}\\therowcntr}w{c}{0.1em}} % Reset the rowcntr counter at each new tabular % \\AtBeginEnvironment{tabular}{\\setcounter{rowcntr}{0}} ....... \\begin{tabular}{|N|c|c|c|} \\end{tabular} How does the \\newcolumntype command work?\nw{c}{0.1em} 是控制对齐和列宽\n长表格自动换页 latex long table automatic move to next page\n\\usepackage{longtable}\nMulti-page tables using \\longtable\n里面不能用 tabular 环境，不过据说它支持大部分tabular 特性\n1 2 3 4 \\begin{longtable}{|c|c|c|c|} Title \u0026amp; Nerf’s Problems \u0026amp; Solutions \u0026amp; Flaw \\endhead % 让表头出现在每页上 \\end{longtable} 表格内换行 cell 内部再用个 tabular 环境:\n\\begin{tabular}[t]{@{}p{\\linewidth}@{}}PlenOctrees for Real-time Rendering of Neural Radiance Fields \\\\ ICCV 2021 Oral \\end{tabular}\n\\newline ICCV 2021\n交叉引用替换为文字 LaTeX/Hyperlinks\n用法：\n1 2 3 \\usepackage{hyperref} ... \\hyperref[label_name]{\u0026#39;\u0026#39;link text\u0026#39;\u0026#39;} \\ref{label_name} 只是数字\nAdvanced LaTeX Cross-references post\n\\pageref{key} command, which prints the number of the page where the \\label{key} was inserted.\n画程序流程图 Code using algorithm2e package\n插入代码块 matlab Highlighting MATLAB Code in LaTeX with mcode\n画坐标系 Coordinate system in LaTeX with TikZ\nPgfplots package overleaf\nOverfull \\hbox with pgfplots graph\n画夹角 How to draw a simple angle, two intersecting lines Tikz\n画圆柱体 generate simple cylindrical shape with text in latex (tikz)\nLearn How to Draw a Cylinder Shape in TikZ\n同步缩放 TikZ 与其中的 node 在 LaTeX 中同步缩放 TikZ 与其中的 node\n缩放表格 Is there a way to slightly shrink a table, including font size, to fit within the column boundaries?\n多张图片并排 subfigure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \\usepackage{subfigure}%并排子图 共享标题 有子标题 \\begin{document} \\begin{figure}[H] \\centering \\subfigure[一次函数]{ \\label{fig:subfig:onefunction} \\includegraphics[scale=0.3]{figureDemo2}} \\hspace{0.5in} % 两图片之间的距离 \\subfigure[二次函数]{ \\label{fig:subfig:twofunction} \\includegraphics[scale=0.3]{figureDemo3}} \\caption{2个图片并排演示} \\label{fig:twopicture} \\end{figure} \\end{document} minipage\n1 2 3 4 \\begin{figure}[htbp] \\centering \\end{figure} 矩阵每行加 label 1 \\usepackage{blkarray} overleaf DIP assign 3\nSuitable for R markdown Label rows and columns of a matrix in R Markdown with Latex and HTML rendering\nbasic usage (2021-01-15)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 \\documentclass{article}\t%文档类型：文章 \\documentclass{book}\t%书 \\documentclass{beamer}\t%幻灯片格式 \\documentclass[UTF8]{ctexart}\t%ctexart支持中英文混拍，指定文档编码类型 \\documentclas[UTF8]{ctexbook}\t%对书籍排版 \\title{文章标题} \\author{文章作者} \\date{文档的修改日期} \\date{\\today}\t%自动生成当天日期 \\begin{document}\t%下面是正文，上面是前言，\\begin和\\end之间是环境/作用域，位于同一个环境中的内容将会共享相同的文字格式 \\maketitle\t%在当前位置生成文档的标题（前言区设置的信息） \\textbf{加粗文字}\t%bold font \\textit{斜体}\t%italic \\underline{加下划线} %两个回车是换段，一个回车是一个空格 \\part{书籍的第一部分} \\chapter{书籍的第一章} \\section{这是地一个章节} 你好！ \\subsection{子章节} 二级章节下的内容 \\subsubsection{这是一个三级章节} 三级章节下的内容 \\section{这是第二个章节} 第二章节下的内容 %插入图片 \\usepackage{graphicx}\t%引用包，包含了若干绘制图片的指令 \\begin{figure}\t%把图片嵌套到figure环境中，可以指定标题 \\centering\t%将图片居中显示 \\includegraphics[width=0.5\\textwidth]{图片名字可省略后缀}\t%在当前位置添加一张图片,图片宽度等于0.5倍的当前文本区域的宽度 \\caption{这是图片的标题} \\end{figure} %列表 \\begin{itemize}\t%无序列表的创建：列表中的每一个元素都需要以\\item开头 \\item 列表项1 \\item 列表项2 \\item 列表项3 \\end{itemize} \\begin{enumrate}\t%有序列表:前面带编号 \\item 列表项1 \\item 列表项2 \\item 列表项3 \\end{enumerate} %数学公式 质能方程：$E=mc^2$ \\begin{equation} E=mc^2 \\end{equation} \\[ E=mc^2 \\] \\over 是几分之几 \\[ d={k \\varphi(n)+1} \\over e \\] codecogs 可以测试公式 %表格 \\begin{table}\t%table环境设置标题 \\caption{表格的标题} \\center\t%表格居中显示 \\begin{tabular}{|c| c| c|}\t%有三列，每列都居中（centering），| 代表竖直边框。 % {l c c} 则表示第一列左对齐(left) % {p{2cm} c c}\t设置列宽2cm \\hline\t#水平边框 单元格1 \u0026amp; 单元格2 \u0026amp; 单元格3 \\\\ \\hline\\hline\t%双横线 单元格4 \u0026amp; 单元格5 \u0026amp; 单元格6 \\\\ \\hline 单元格7 \u0026amp; 单元格8 \u0026amp; 单元格9 \\\\ \\hline \\end{tabular} \\end{table} Paper tips (2024-03-21)\nguanyingc/latex_paper_writing_tips\nPlots Tikz Examples (2024-04-11)\nExample of Petar Veličković X\nInkscape (2024-04-23)\nHow I draw figures for my mathematical lecture notes using Inkscape - Gilles Castel Take Notes (2024-04-23)\nHow I\u0026rsquo;m able to take notes in mathematics lectures using LaTeX and Vim (Found Gilles Castel as he was followed by cxzhou35 on Github.)\nHow I manage my LaTeX lecture notes\n","date":"2022-09-12T17:40:00Z","permalink":"https://zichen34.github.io/writenotes/lang/latex_misc/","title":"memo: Latex"},{"content":"Arxiv\nNotes UNet 常用于图像 Segmentation。\nCNN的目标是提取特征（减少冗余信息），经过多个pooling层（以及strides\u0026gt;1），最后的 feature map 的尺寸（分辨率）是最小的， 但图像分割任务需要为图片的每个像素判断 label，所以需要把 CNN 最后输出的 feature map 恢复至原来的尺寸， U-Net 通过逐级 upsampling (插值) 得到了与cnn对称的feature maps。 然后就可以把 CNN 中间过程产生的分辨率较高的 feature maps 与对应的 upsampled feature maps 结合起来，从而输出更精确的 segmentation map。 U-Net architecture Ronneberger 2015 左侧是 CNN \u0026ldquo;收缩\u0026quot;路径（encoder），右侧是\u0026quot;扩展\u0026quot;路径（decoder）。\n左侧 CNN 的每一级做两次 conv3x3 (unpadded)，然后ReLU激活并通过 2x2 max-pooling 做下采样， 每次下采样会把 通道数量加倍。\n右侧每次先对 feature map 做 2 倍上采样和两次 con3x3 把通道数减一半。\n灰色箭头是 concatenation（Resnet 的 skipconnect 是直接相加，不是拼接），把来自cnn的feature map 的边缘裁剪一下，拼接到右侧的feature map上。最后的feature map 做conv1x1 把64通道变换到所需的类别个数\n上采样不会增加(恢复)空间信息\nskip connection的原理是什么？为什么U-net中要用到skip connection?-akkaze-郑安坤的回答\n(2023-07-10)\nExample Tensorflow: CV2020 - 16 - Object Segmentation.ipynb\nPyTorch: U-Net: Training Image Segmentation Models in PyTorch\nSegmentation needs to give a label for each pixel, so the output should have the same size as the input image.\nThe hidden feature vector has lost spatial information along contracting. And up-sampling (interpolation) doesn\u0026rsquo;t restore the location of features, but just kind of \u0026ldquo;copy\u0026rdquo; the features to around pixel.\nThe feature on each pixel is concatenated with the feature before contracting which still locates in its original position. Then the convolution later on will \u0026ldquo;fuse\u0026rdquo; one pixel\u0026rsquo;s location \u0026ldquo;characteristic\u0026rdquo; into its feature vector.\nThe output feature map is an expansion of the compact hidden featuer map, but conditioned with spatial location.\nThis way the trained model can classify a pixel based on spatial position and surrouding RGB feature.\nU-Net Explained: Understanding its Image Segmentation Architecture - medium\n","date":"2022-09-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/misc/b-note-unet/","title":"read: UNet"},{"content":"Note GL_PROJECTION matrix: camera space point(xe,ye,-ze) -\u0026gt; perspective projection to near plane -\u0026gt; homo:(xp,yp,1) -\u0026gt; scale -\u0026gt; homo:(xn,yn,zn,1) -\u0026gt; (x_clip,y_clip,z_clip, w_clip) , where \u0026lsquo;xn,yn,zn\u0026rsquo; are variables raning from -1 to 1.\nRefer to OpenGL Projection Matrix - songho\nNormalized Device Coordinates (NDC) is used to determine whether a 3D point can appear on the computer monitor, which is a cube with length 1. The transformation from eye coordinates to NDC is mapping the truncated pyramid frustum to a cube.\nPerspective Projection - songho\nAccording to the perspective projection, the projection point of a world point (xₑ,yₑ,zₑ) on the near plane is\n$$ \\begin{cases} x_p = \\frac{n}{-z_e} x_e \\\\ y_p = \\frac{n}{-z_e} y_e \\end{cases} $$\n(Camera coordinates is right-hand system looking in the -z direction, while NDC is looking in the z direction under left-hand coordinates)\nIn order to use a matrix to represent NDC transformation, the homogeneous coordinates are used to enable division, so the transformation can be represented as:\n$$ \\begin{pmatrix} x_{clip} \\\\ y_{clip} \\\\ z_{clip} \\\\ w_{clip} \\end{pmatrix} = M_{projection} \\cdot \\begin{pmatrix} x_e \\\\ y_e \\\\ z_e \\\\ w_e \\end{pmatrix} $$\n(2024-02-15) wₑ is the homogeneous coordinate for storing the original depth value of the camera point after the multiplication with the projection matrix, where the 4-th row is [0 0 -1 0], so wₑ = 1. And the depth will be divided at the very end step: the perspective division, so as to make the intermediate processes linear operations.\nComparing merely projecting a 3D point onto plane with a w2c, the projection matrix specifies specific behavior for the z-axis of the ND space (not the source camera space any more).\nTherefore, the NDC is:\n$$ \\begin{pmatrix} x_{ndc} \\\\ y_{ndc} \\\\ z_{ndc} \\end{pmatrix} = \\begin{pmatrix} x_{clip} / w_{clip} \\\\ y_{clip}/w_{clip} \\\\ z_{clip}/w_{clip} \\end{pmatrix} $$\nBecause $w_{clip}$ is the denominator, it should equal to -zₑ; Hence, the forth row of matrix should be $[0\\ 0\\ -1\\ 0]$\nMapping [l, r] and [b, t] to [-1, 1] with linear realtionship: Two points (l,-1),(r,1) can be used to determine the line:\n$$ x_{NDC} = \\frac{1-(-1)}{r-l} \\cdot x_p + β $$\nand then substitute (r,1) for $(x_p,x_{NDC})$ to solve β = -(r+l)/(r-l).\nTherefore, $x_{NDC} = \\frac{2}{r-l}x_p - \\frac{r+l}{r-l}$.\nSimilarly, $y_{NDC} = \\frac{2}{t-b} y_p- \\frac{t+b}{t-b}$\nSubstitute xp, yp with the form of xₑ, yₑ:\n$$ x_{NDC} = (\\frac{2n}{r-l} \\cdot x_e + \\frac{r+l}{r-l} \\cdot z_e) / -z_e \\\\ y_{NDC} = (\\frac{2n}{t-b} \\cdot y_e + \\frac{t+b}{t-b} \\cdot z_e) / -z_e $$\nTherefore, the first two row elements of the matrix can be determined.\nSuppose the third row is $[0\\ 0\\ A\\ B]$ (z value is independent to x and y), so:\n$$ z_{NDC} = \\frac{A z_e + B w_e}{-z_e} $$\nSubstitute the correspondence between (-n, -f) and (-1, 1) into the above equation:\n$$ \\begin{cases} \\frac{-A n + B}{n} = -1 \\newline \\frac{-A f + B}{f} = 1 \\end{cases} $$\nTherefore, A = -(f+n)/(f-n), and B = -2fn / (f-n)\nFinally, the matrix $M_{projection}$ is\n$$ \\begin{pmatrix} \\frac{2n}{r-l} \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{-(f+n)}{f-n} \u0026amp; \\frac{-2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} $$\nCode (2023-10-02)\n3D world coords multiplied with Inverse intrinsics matrix, Scale the [near, far] to [0,1] Code credits MatchNeRF\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_coord_ref_ndc(extr_ref, intr_ref, pts_3D, inv_scale, near_far=None, lindisp=False): \u0026#39;\u0026#39;\u0026#39; Warp the provided position to the reference coordinate, and normalize to NDC coordinate. pts_3D [batch, N_rays N_sample 3] \u0026#39;\u0026#39;\u0026#39; bs, N_rays, N_samples, N_dim = pts_3D.shape # (bs=1, n_rays=1024, n_pts=128, n_dim=3) pts_3D = pts_3D.reshape(bs, -1, N_dim) # (bs, n_rays*n_pts, n_dim) near, far = torch.split(near_far, [1, 1], dim=-1) # (1,2) -\u0026gt; both are (1,1) # wrap to ref view if extr_ref is not None: # 3D pts in world space -\u0026gt; camera space of a src view pts_ref_world = world2cam(pts_3D, extr_ref) if intr_ref is not None: # using projection # pts in camera space -\u0026gt; image plane coords with z point_samples_pixel = pts_ref_world @ intr_ref.transpose(-1, -2) # normalize to 0~1 point_samples_pixel[..., :2] = (point_samples_pixel[..., :2] / point_samples_pixel[..., -1:] + 0.0) / inv_scale.reshape(bs, 1, 2) if not lindisp: point_samples_pixel[..., 2] = (point_samples_pixel[..., 2] - near) / (far - near) # normalize to 0~1 else: point_samples_pixel[..., 2] = (1.0/point_samples_pixel[..., 2]-1.0/near)/(1.0/far - 1.0/near) else: # using bounding box near, far = near.view(bs, 1, 3), far.view(bs, 1, 3) point_samples_pixel = (pts_ref_world - near) / (far - near) # normalize to 0~1 point_samples_pixel = point_samples_pixel.view(bs, N_rays, N_samples, 3) # (bs, n_rays*n_pts, 3) -\u0026gt; (bs, n_rays, n_pts, 3) return point_samples_pixel near-far = [0,1] (2024-01-16)\nProjection Matrix 详解 - 贰芍的文章 - 知乎\n","date":"2022-09-02T11:39:00Z","permalink":"https://zichen34.github.io/writenotes/vis/ndc/","title":"memo: Vis | NDC"},{"content":"Stereographic Projection Source video: Visualizing quaternions (4d numbers) with stereographic projection\nLeveraging projection to reduce the dimensionality by 1. Such that quaternion in 4D space can be learned through the projection counterparts in 3D space.\n把 (i-实轴) 单位圆 做球极投影 stereographic projection 到 y 轴上:\n从 x 轴的 -1 点出发过圆上每个点做射线，与 y 轴的交点即是圆上点的投影，\n在这条投影线上，靠近 0 的点密集，越往外越稀疏，-i 和 i 是真实的圆上的点（代表着 x 为 0 的圆上点），(-i,i) 之间是右半圆的投影；\n左乘 i 就是圆逆时针旋转 90 度，就是 1 ➔ i ➔ -1 ➔ -i 做一次循环移位。\n把三维 (i-j-实轴) 单位球投影到水平面上:\n从实轴的 -1 点出发连接球面上各点，与 i-j 平面交点即为投影点，\n赤道是真实的球面上的点（代表着实部为 0 的三维复数），赤道之内是北半球的投影，之外是南半球的投影， 赤道两侧是对称的，极点附近的投影点密集；\n左乘 i 就是球在 i 轴方向顺时针旋转 90°，赤道和零度经线共同在 1-i 平面内完成了一次 1 ➔ i ➔ -1 ➔ -i 循环移位， 乘以 j 就是球在 j 轴方向上顺时针，在 1-j 平面内完成一次 1➔ j ➔ -1 ➔ -j 循环移位\n把四维 (i-j-k-实轴) 单位超球投影，\n从实轴的 -1 点出发连接超球面上各点，与某个平面的交点形成一个个球面，每个球面就是每个四元数的投影，\n在 i-j-k 空间的三维单位球是真实的超球面上的点（代表着所有实部为 0 的四元数）， 单位球内是实部为 0-1 之间的四元数的投影，负数实部的四元数被投影到了单位球外，实部=-1 的四元数在无穷远处(我们看不到)，\n我们能看到的投影点都有相同的模长（？不懂），四元数旋转时，就是三个维度共同移位，\n(2023-11-07) 如果绘制出来的三维球代表 i,j,k 三个虚轴，那实轴就画不出来了，只能先固定它再分析。 （把以上两种情况合起来:）i 是圆的球极投影，\u0026lsquo;j+k\u0026rsquo; 是球的球极投影， i 的移位顺序是 1,i,-1,-i，\u0026lsquo;jk\u0026rsquo; 的移位顺序是 j,k,-j,-k。 i 与 jk 满足右手定则：拇指指向 i，则四指卷曲方向就是 jk 旋转方向（拇指指向 j，k ➔ i ➔ -k ➔ -i 循环移位）\n四元数的可视化-3B1B + Krasjet评论\n(2023-11-07)\n3D Rotation Because projection has been performed, the real axis is collapsed.\nSo when \u0026ldquo;rotating\u0026rdquo;，the real axis is fixed and only imaginary axes rotate.\nTherefore, a 4-D quaternion can be used to describe the rotation in 3D space through its mutable 3 imaginary axes.\nCompared with Euler angles that define the hierarchical order of three angles, quaternion depicts the evolution of a single 4D vector. Convert a quaternion to a 3x3 rotation matrix\n","date":"2022-08-31T22:00:00Z","image":"https://img.youtube.com/vi/d4EgbgTm0Bg/maxresdefault.jpg","permalink":"https://zichen34.github.io/writenotes/calc/quaternion/","title":"memo: Calc | Quaternion"},{"content":"Define multi layers by for loop 1 2 3 4 5 6 for i in range (n_layers): setattr(self, f\u0026#34;layer{i}\u0026#34;, nn.Linear(2, n_hidden_units), nn.ReLU(True)) # 取出多层： for i in range(n_layers): layer = getattr(self, f\u0026#34;layer{i}\u0026#34;) Access all weights of a model 1 for name, param in model.named_parameters: Refer:\npytorch教程之nn.Module类详解——使用Module类来自定义模型- CSDN Initialize weights of nn.Linear model.apply(fn) will apply function fn to every children submodule. Therefore, let fn be init_weights()\n1 2 3 4 5 6 7 8 9 10 @torch.no_grad() # this func won\u0026#39;t create graph def init_weights(m): print(m) if type(m) == nn.Linear: torch.nn.init.ones_(m.weight) m.bias.data.fill_(0.01) print(m.weight) model = Net() model.apply(init_weights) Refer:\nHow to initialize weights in PyTorch? - StackOverflow init_weights Example code ModuleList usage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026#39;\u0026#39;\u0026#39; ModuleList is like a iterator \u0026#39;\u0026#39;\u0026#39; class LinearNet(nn.Module): def __init__(self, in_features, out_features, num_layers, layer_size): super(LinearNet, self).__init__() self.linears = nn.ModuleList([nn.Linear(in_features, layer_size)]) # 1st layer self.linears.extend([nn.Linear(layer_size, layer_size) for i in range(1, num_layers-1)]) self.linears.append(nn.Linear(layer_size, out_features)) def forward(self, x): for idx, layer in enumerate(self.linears): x = layer(x) # x = self.linears[idx // 2](x) + layer(x) # pass x through each layer return x Refer:\nWhen should I use nn.ModuleList and when should I use nn.Sequential? torch.Tensor 包含单一数据类型元素的多维矩阵 有10种张量类型，torch.Tensor是默认张量类型torch.FloatTensor的别名 Note 张量变异方法都带有下划线后缀,它们直接原地修改原张量的属性，而不派生新张量。例如torch.FloatTensor.abs_()直接计算并修改原张量，而torch.FloatTensor.abs()在新张量中计算结果。\nGet value of a tensor a = torch.tensor([3])\na.data ⮕ tensor([3]) a.item() ⮕ 3 (\u0026ldquo;Convert one-element tensor to python scalar.\u0026rdquo;) a.numpy() ⮕ array([3]) b = (a/2).requires_grad_(True)\nb.detach().cpu().numpy() ⮕ array([1.5], dtype=float32) (2023-07-27)\nMemory allocated for a tensor 1 2 a = torch.arange(4) # torch.int64 Num_bytes = a.element_size() * a.nelement() # 8 * 4 How to know the memory allocated for a tensor on gpu?\nMove specified axis Move the specified axis to the second axis\nQuestion for ChatGPT:\nIf I was given a variable channel_dim, and it can be 3, that means the tensor is (B, H, W, channel_dim), so how can I transform the tensor to make the channel_dim the 2nd axis?\nI mean the channel_dim is a variable, it can be any axis.\nAnswer:\nApologies for the confusion. If the channel_dim is a variable and can be any axis, you can use the following code snippet to move the specified axis to the second axis of the tensor:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch # Assuming you have a tensor with size (B, H, W, C) and channel_dim can be any axis tensor = torch.randn(B, H, W, C) channel_dim = 2 # Replace this with the desired axis you want to move to the second position # Permute the tensor to move the specified axis (channel_dim) to the second axis # Generate the permutation order: (0, channel_dim, 1, 2, ..., channel_dim-1, channel_dim+1, ..., C-1) perm_order = list(range(len(tensor.shape))) perm_order.remove(channel_dim) perm_order = [0, channel_dim] + perm_order[1:] tensor_permuted = tensor.permute(*perm_order) # The resulting tensor will have size (B, C, H, W) if channel_dim was initially the third axis (2) # The resulting tensor will have size (B, W, C, H) if channel_dim was initially the second axis (1), and so on. print(tensor_permuted.size()) (2023-08-01)\nDynamically change Conv2d layer Question of chatGPT:\nI\u0026rsquo;m writing pytorch. I want to use nn.Conv2d, but the in_channels is the result generated in the forward method, how could I make the parameter of Conv2d optimized during training?\nMake the in_channels a instance variable, like self.in_chnls = in_channels.\nThen in the __init__(self, in_channels) method, a Conv2d layer can be construct: self.conv = nn.Conv2d(self.in_chnls, out_chnls, ...)\nThen update it in the forward before calling self.conv layer: self.in_chnls = x.shape[1]\n(DDG search: \u0026ldquo;use nn.Conv2d with dynamically determined in_channels during the forward pass\u0026rdquo;)\nSimilar question: Dynamically set Conv2d based on input channels - PyTorch Forum\nHow to create a custom layer that accepts the input during forward pass - PyTorch forum\nMake other class (nn.Module), in which the prefix model is called. But the in_channels is required when initializing Conv2d(), so the parameters of it can be defined. \u0026ldquo;Can we not define the filter size at runtime?\u0026rdquo; Why does nn.Conv2d require in_channels? - PyTorch Forum\nnn.LazyConv2d(out_channels, ...) doesn\u0026rsquo;t need in_channels. Docs torch.nn.modules.lazy.LazyModuleMixin Get device of a module 1 dev = next(model.parameters()).device How to get the device type of a pytorch module conveniently?\nModule.apply(fn) Docs Example - AIM Apply the function fn to each submodule and the Module itself. This can be used to initialize weights, like the code in AIM:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class ViT_CLIP(nn.Module): def __init__(self,): self.__init__() def init_weights(self,): # A class method def _init_weights(m): # Pass a submodule if isinstance(m, nn.Linear): trunc_normal_(m.weight, std=.02) if isinstance(m, nn.Linear) and m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.LayerNorm): nn.init.constant_(m.bias, 0) nn.init.constant_(m.weight, 1.0) # Call _init_weights self.apply(_init_weights) Customize Autograd Op (2024-01-23)\nThe .apply() method of a customized operation that subclasses autograd.Function requires forward and backward to be static methods. Docs\nExample - 3DGS\ntorch.roll Circular shift (\u0026ldquo;循环移位\u0026rdquo;) along some dimensions. Docs\nExample - AIM - swin2d Move 1 step along 1 dimension:\n1 2 x = torch.tensor([[1,2,3],[4,5,6]]) torch.roll(x, shifts=1, dims=0) It will shift 1 element in the dimension 0, i.e., [4,5,6] to the next position, so x will become: [[4,5,6],[1,2,3]]\nMove steps (2, 1) along 2 dimensions separately:\n1 torch.roll(x, shifts=(2,1), dims=(0,1)) The dimension 0 will shift twice, and the dimension 1 will shift once. So x becomes: [[3,1,2],[6,4,5]]\ntorch.max torch.max(x, dim) compares each atom element on the equal position according to the dimension dim\n1 2 3 4 5 6 7 8 a = torch.randint(8, (2,3,3) tensor([[[0, 3, 2], [6, 1, 5], [2, 7, 0]], [[4, 7, 4], [6, 1, 0], [0, 1, 6]]]) torch.max(a, 0) will compare: 0-4, 3-7,, 2-4; 6-6, 1-1, 5-0; 2-0, 7-1, 0-6, so the result is [[4,7,4] [6, 1, 5] [2, 7, 6].\ntorch.max(a,1) will compare: 0-6-2, 2-1-7, 2-5-0; 4-6-0, 7-1-1, 4-0-6; then the result is [[6,7,5], [6,7,6]]\ntorch.diff The back one minus the front one.\n1 2 3 4 5 6 a = torch.arange(12, (2,2,3)) tensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) torch.diff(a): the last dim subtract [ [ [1-0, 2-1], [4-3, 5-4]]; [7-6, 8-7], [10-9, 11-10] ] = [ [ [1,1], [1,1] ]; [1,1], [1,1] ] ]\ntorch.diff(a, n=2): do again for 1st-time result: [ [ [0], [0]; [[0],[0]]]\ntorch.diff(a, append=a), the append tensor needs to have the same dimensions of input.\nCount #params From Match-NeRF:\n1 2 3 4 5 for name, param in self.model.named_parameters(): log.info(f\u0026#39;{name}: {param.requires_grad}\u0026#39;) num_param = sum(p.numel() for p in self.model.parameters() if p.requires_grad) num_total_param = sum(p.numel() for p in self.model.parameters()) log.info(\u0026#39;Number of total parameters: {}, tunable parameters: {}\u0026#39;.format(num_total_param, num_param)) (2023-12-21) From MVSNet:\n1 sum([p.data.nelement() for p in model.parameters()]) param_group (2024-04-11)\nPer-parameter options: an iterable of parameter groups. Docs\nHave practiced in MatchNeRF exp1 before: different modules are trained with separate LRs. Example in 3DGS\n","date":"2022-08-26T20:25:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_misc/","title":"memo: PyTorch | Misc"},{"content":"(2022-12-28)\n白板推导: VAE Source video：【机器学习】白板推导系列(三十二) ～ 变分自编码器(VAE)】\nVAE 和 GMM 一样也是生成模型：隐变量 z 生成观测变量 x，从而学习样本数据 x 本身的分布。不过 VAE 的 z 不是1维的，而是多维的。\n用 EM 解 GMM 的最优参数 θ 时，假设 z 的后验分布 P(z|x;θ⁽ᵗ⁾) 能够取到，而且因为 z 是离散的，P(x) = ∑ₖ₌₁ᴷP(x,z=Cₖ;θ) 就能算出来， 所以后验 P(z|x)=P(x|z)P(z)/P(x) 也能算出来，其中分子两项是假设的分布。E步把目标函数: \u0026ldquo;最大的期望\u0026rdquo; $E_{P(z|x)} [ log P(x,z|θ) ]$ 写出来，M步对其求导找出（期望最大时的）最佳θ⁽ᵗ⁺¹⁾。\n而 VAE 的 z 是高维连续的，P(x) = $∫_z P(x,z) dz$ 积不出来，后验P(z|x) 就没法用贝叶斯公式导出，但可以用随机梯度下降变分推断(SGVI)近似后验。\n“推断”是从 x 到 z 的过程：用样本 x 修正 z 的先验 P(z) 得到 z 的后验 P(z|x)；而“生成”是从 z 的后验分布 P(z|x) 中采样出 z，再变换成样本的分布 P(x)。\nflowchart LR; observed((x)) --\u003e|Inference| latent((z)); latent --\u003e|Generation| observed 用 q(z|x;ϕ) 逼近后验 P(z|x;θ) 时，要最小化 $KL(q_{ϕ(z|x)} || P_{θ(z|x)})$，在 θ 固定，则似然 P(X) 也固定的情况下，等价于最大化 ELBO： arg max $E_{qᵩ(z|x)} [ log (P(z,x;θ)/qᵩ(z|x)) ]) ]$。类似广义 EM 的 E步，在 θ 固定的情况下，求后验的近似。\n通过把联合概率拆开，这个 ELBO 也可写成: $E_{qᵩ(z|x)} [ log P(x|z;θ) ] - KL(q(z|x;ϕ) ||P(z))$。 当 z 的先验分布 P(z) 是标准正态时，那么KL散度就是希望 qᵩ(z|x) 的方差保持为I，不要太小，分布坍缩到一点就是过拟合：x与z一一对应，就变成 AE了，只能对训练样本 x 推出正确的 z。\n求生成模型的最优参数 θ 同样是要使似然期望最大：arg max ELBO（x后验与KL散度同时优化），（没法直接求导）可采用梯度上升法，所以 VAE 是把 EM 的两步合起来了，既逼近后验 p(z|x) 的参数 ϕ，又逼近生成模型 p(x|z) 的参数θ。\n在计算 ELBO 对 ϕ 的梯度时，∇ᵩL(ϕ) 可以写成一个期望，直接对 z 采样求均值可能由于方差太大而失效（而且采样操作不可导，就无法对z求梯度）， 所以先对一个高斯噪声 ε 采样，再根据变换：z=μ_ϕ(x) + Σ_ϕ¹ᐟ²(x)⋅ε 得到 z (重参数化)。然后求均值(=梯度)，带入梯度上升公式，更新ϕ。\n训练NN时，输入一个 x，神经网络输出后验分布（编码）p(z|x) 以及采样出一个z。用这个z 通过另一个网络逼近 x 的后验分布（生成模型）p(x|z)， 也就是在学到的 z 成立的情况下，从 p(x|z) 中采到 x 的概率，目标函数就是希望这个概率越大越好，可以假设 x服从二项或正态，把参数代入公式即得概率 或者说，分布 p(x|z) 的\u0026quot;众数\u0026quot; x\u0026rsquo;要与输入 x 的距离越小越好，当方差很小时，众数就是期望，即每次采样都会采到期望 μ(z)，所以以 μ(z) 与 x 的距离作为目标函数。 一个x是多维的，它有自己（维度之间）的分布。\nflowchart LR subgraph Encoder x(\"Input x\") --\u003e net(\"NN-ϕ\") --\u003e a(\"μ(x)\") \u0026 b(\"Σ(x)\") end subgraph Decoder a --\u003e o((\"+\")) --\u003e z(\"sampled z \\n from N(μ(x),Σ(x))\") --\u003e net2(\"NN-θ\\n(μ(z))\") --\u003er(\"reconstructed\\n x'\") b --\u003e m((\"×\")) --\u003e o ε(\"ε～N(0,𝐈)\") --\u003e m end 完整笔记：shuhuai008-32； 参考：隐变量模型到EM到变分自编码器 - 我要给主播生猴子 -知乎\n(2022-12-29)\n苏剑林: VAE（一） Source article：变分自编码器（一）：原来是这么一回事-苏剑林\n通过隐变量 z 求样本 x 的分布：p(x) = ∫_z p(x|z) p(z) dz。 先学习 z 的后验分布 p(z|x)（ 编码 ），再由它 生成 $\\^x$，$\\^x$应与x一样。\nflowchart LR sample(x) --\u003e|\"N(μᶿ,Σᶿ)\"| latent(\"p(z|x)\") --\u003e|\"sampling z \\n P(x)=Σp(x|z)⋅p(z)？\"| recon(x^) sample -.-|=| recon 注意区分先验 p(z) 与后验 p(z|x)。因为假设了后验是正态分布的形式，所以是对 μᶿ,Σᶿ 做重参数化。而先验只出现在正则化项中，不参与后验的训练。\n一个样本点x⁽ⁱ⁾对应一个（独立的、多元的）后验分布 p(z|x⁽ⁱ⁾)，这样从中采出的 z 就一定对应这个样本点。所以每个样本有自己的正态分布。\nz 是后验分布的一个采样，采样就会有偏差（方差），导致重构误差不为0。如果不加正则化，为了减小误差，Σ会不断减小到0，退化成AE。 从这个角度看 vae 是 AE 加上噪声，并约束噪声的强度（方差）尽量为1.\nvae 希望所有的后验分布（”一般正态“）都与标准正态相似：μ=0，Σ=I，采样z时就保证了生成能力。因为各分量独立，所以是d维一元N的加和： KL( N(μ,Σ)|| N(0,I) ) = ½ Σ [(-logσ² + μ² + σ² -1) ]\n重参数化技巧把服从N(μ,Σ)的随机变量 z 的概率拆成一个服从标准正态的变量ε和一个参数变换μ+ε×σ，从而实现虽然采样操作不可导，但它不参与梯度的反向传播。\n条件VAE：样本属于不同的类别-期望不同 cvae代码：用2个线性层分别拟合μ和Σ，用重参数化技巧采样z，x与x\u0026rsquo;之间的重构损失用了交叉熵 model：\nflowchart LR x(x\\n original_dim) --\u003e|\"Dense\\n(intermediate_dim)\"| h --\u003e|\"Dense\\n(latent_dim)\"| μ(\"z_mean\") \u0026 Σ(\"z_log_var\"); y(y\\n num_classes) --\u003e|\"Dense\\n(latent_dim)\"| yh μ \u0026 Σ --\u003e rp{sampling} --\u003e|reparame\\n terization| z --\u003e|\"Dense\\n (intermediate_dim)\"| h_decoded --\u003e|\"Dense\\n(original_dim)\"| x_decoded_mean (2022-12-30)\n苏剑林: VAE（二） 原文：变分自编码器（二）：从贝叶斯观点出发\n期望的数值计算与采样计算不同：数值计算是先给一个数列 x（其中$x⁰ \u0026lt; x¹ \u0026lt; x²\u0026lt;\u0026hellip;xⁿ$），然后对里面的每个数 x⁽ⁱ⁾ 按它的概率加权求和：E[x]=∫ xp(x) dx。 但如果 x⁽ⁱ⁾ 是分布 p(x) 中的采样，概率大的会被多采几次，样本集合 x 中就包含了概率信息，不用再乘 p(x⁽ⁱ⁾)了：E[x]≈1/n⋅∑ᵢ₌₀ⁿ x⁽ⁱ⁾, x⁽ⁱ⁾∼p(x)\n推导目标函数时，先构造了 p(x,z)=p(z|x)⋅p^(x)，再构造 q(x,z)=q(x|z)q(z)，这两个构造毫无关系，希望它俩互相靠近，而不是为了逼近z的后验p(z|x)。notes; (vae三-josh00的评论)\n在生成阶段，若假设 p(x|z) 服从二项分布，则重构误差就是交叉熵；若假设 p(x|z) 服从正态分布，则重构误差就是MSE\n训练时，生成阶段只从 z 的后验分布中采样一个，因为 z 是专属于一个x。P(x)➔ μ_ϕ(x), Σ_ϕ¹ᐟ²(x) ➔ z ➔ μ_θ(z) ➔ x'\n(2022-12-31)\n苏剑林: VAE（三） 原文：变分自编码器（三）：这样做为什么能成？\nvae生成时，只采一个z：因为x与z一一对应（自编码器）方差为0，vae引入了先验q(z)=N(0,I)，方差也不会太大，也就是每次采样，结果都一样。如果直接做最大似然p(x|z)，就需要从z的先验p(z)中采多个样本先估计出每个x的似然，再求似然的期望最大化。但如过没采到 zₖ，它对应的 xₖ的似然就是0，ln0是-∞，导致训练失败。\nVAE 的重建生成相当于在AE上加了噪声（方差），所以可以生成与原始样本不同的数据。\n%%{ init: { 'flowchart': { 'curve': 'linear' } } }%% flowchart TB x[\"x\\n(样本)\"] --\u003e nn[\"隐变量的分布 μ(x), Σ(x)\"] --\u003e z --\u003e nn2[\"数据的分布 p(x|z)\\n 方差很小\"] --\u003e recon[x'\\n重建样本] nn --\u003e |\"通过采样，从 “多个” 到 “一个”，\n从 “无限” 到 “唯一”\"| recon IWAE 对p(x,z)=∫p(x|z)p(z)dz 做等价变换，从而可从后验p(z|x)中采样z。\n(2023-06-04)\nPCA 与 VAE They\u0026rsquo;re both learning the distribution of data.\nDDG search: \u0026ldquo;PCA 与 VAE 对比\u0026rdquo;\nUnderstanding Variational Autoencoders (VAEs) - Joseph Rocca\nVariational Autoencoders explained — with PyTorch Implementation - Sanna Persson\n1 import torch Instead of mapping the input into a fixed vector, we want to map it into a distribution. From Autoencoder to Beta-VAE - Lil\u0026rsquo;Log\n","date":"2022-08-26T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/c-sum-vae/","title":"sum: VAE"},{"content":"Code | Arxiv\n一句中文：构建 point-alinged feature field, 然后用 attention 把特征融合成图像，特征的attn score 反映了深度\nInsights:\nNeRF is in a backward optimization fashion. The color is mapped to points along with the optimization. So the radience field is recovered backward. While Generalizable NeRFs assign feature onto points in the feed-forward process.\nProjecting the points onto feature maps exerts the inductive bias of epipolar constraints for injecting geometry prior.\nIt\u0026rsquo;s inferior to NLF\nSamples on a single ray cannot recover refraction and scattering, in which the ray will bend. So GNT managed this by its view transformer?\nOcclusion-aware is realized by giveing the most visible reference view the most weight. Depth-aware is endowed by the importance of each point to the pixel color. \u0026ldquo;importance=density\u0026rdquo;\nGNT doesn\u0026rsquo;t care the quality of 3D geometry reconstruction.\nOnly net_coarse is used and trained with 4096 rays, 192 N_samples in one iteration. They didn\u0026rsquo;t split the N_rand into chunks in the train.py, but they did when rendering a full imagein evaluation period. So I need to distribute the 4 blocks onto 4 crads for accomodating the 4096 rays in one batch, if I want to reproduce their expriment. In my previous training, the number of points fed into MLP is N_rand x N_samples = 2048x64 = 131072.\nAbstract Generalizable NeRF Transformer (GNT) two transformer-based stages view transformer: multi-view geometry, coordinate-aligned features, epipolar lines ray transformer: ray marching rendering, decode point features reconstruct single scene without rendering formula attention map 1 Introduction Topoic: novel view synthesis by NeRF (coordinate-based model + differentiable volumetric rendering)\nProblems: one network overfits one scene with long optimization.\nFormer solutions \u0026amp; drawbacks: Ibrnet, pixelNerf, NLF proved the coordinates are not necessary, but the novel view can be interpolated from other views\u0026rsquo; image features.\nTask:\nContributions \u0026amp; Reason\ncooridnate network and volumetric renderer are composed into a transformer architecture. use multi-view image features to infer coordinate-aligned features. Later these features are decoded to novel view directly without volume rendering. Results statement\n有泛化能力意思是，训练好的模型可以直接用输入图像重建 unseen 场景的新视图？\n2 Related Work Advances in Transformer\nNeural Radiance Fiels: NeRF, Mip-NeRF; surface representation, dynamic scenes, reflection modeling; Generalization nerf: PixelNeRF, IBRNet, MVSNeRF, NeuRay; accelerate nerf.\nTransformer Meets Radiance Fields: IBRNet, NerFormer, Generalizable neural radiance field, NLF, Vision transformer for nerf-based view synthesis; SRT\n3 Preliminaries 4 Method: Make Attention All NeRF needs 4.1 Epipolar Geometry Constrained Scene Representation Multi-View Feature Aggregation.\nVanilla NeRF use MLP to parameterize scene in a backward optimization fashion? (先渲染出图片，依据图片误差再返回去更新辐射场）\nIn contrast, generalizable NeRFs construct the radiance field in a feed-forward scheme\n用图像特征优化辐射场，训练好后，就可以对辐射场应用volume rendering来渲染新视图；而本文是从辐射场映射回图像特征，从而生成新视图\n(2023-12-16) NeRF 是从待渲染的像素出发，向空间发出射线，去查询颜色。而 Forward 是把空间点投影到像素上。\nRepresent the scene as a feature field, where each point in the space has a part of image feature.\nUse attention to fuse all pixel on ResUNet feature maps of source views is memory prohibitive\nOnly fuse the pixels locating in the paring epipolar lines of \u0026ldquo;neighboring views\u0026rdquo; （和 PixelNeRF 一样，不过人家没提对极几何这个词，就是把光线上的点投影到不同视图上）\nMemory-Efficient Cross-View Attention\nOnly use one read-out token in the query sequence to iteratively fuse features from neighbor views\nThe similarity is not computed by dot multiplication, but subtraction, so the attention score is calculated for every channel of the features.\nThe attention scores matrix and \u0026lsquo;V\u0026rsquo; are added by the relative directions between source views and the target view.\nDiscussion on Occlusion Awareness.\n4.2 Attention Driven Volumetric Rendering Different illumination effects and material scenarios need to apply specific handcrafted rendering formula. Data-driven renderer decode the image features into images realizing various phenomena in one way.\nRay Feature Aggregation:\nAnalogy the token features as the color in the volum rendering fomula. Therefore, do attention for coordinate-aligned features to aggregate the final color (rgb) for the novel pixel/ray. mean pooling to compress the feature patch into a pixel use dot-product attention to fully mix features of other points getting comprehensive contextual information. 关于 auto-regressive rendering 的延伸讨论？ Discussion on Depth Cuing\ndepth is the average of marching distance with attention score NLF 有相同的架构，区别在哪？ 5 Experiments single scene; generalization to unseen scenes\n5.1 Implementation Details Source and Target view sampling 在 Blender 数据集上的 PSNR 无明显提升\n(2023-08-16)\nCode train.py\n\\begin{algorithm} \\begin{algorithmic} \\PROCEDURE{train}{args} \\STATE data: [ rgb$_{(1,H,W,3)}$, $\\newline\\qquad$ camera$_{(1,34)}$, $\\newline\\qquad$ rgb\\_path$_{(str)}$,$\\newline\\qquad$ src\\_rgbs$_{(8,H,W,3)}$, $\\newline\\qquad$ src\\_camera$_{(8,34)}$, $\\newline\\qquad$ depth\\_range$_{(1,2)}$ ] \\STATE $\\newline$ \\STATE ray\\_sampler = RaySamplerSingleImage(data) \\STATE ray\\_batch = ray\\_sampler.random\\_sample(N\\_rand) \\STATE featmaps = model.feature\\_net(ray\\_batch[\"src\\_rgbs\"]) \\STATE ret = render\\_rays(ray\\_batch, model, projector, featmaps) \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} llff_test.py __init__() of a training dataset:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def LLFFDataset(Dataset): def __init__(self, args, mode, scenes=(), random_crop=True): scenes = os.listdir(folder_path) # Stuff all scenes into lists for i, sceneName in enumerate(scenes): rgb, poses, bds, render_poses, i_test, imgnames = load_llff_data(sceneName) near, far = np.min(bds), np.max(bds) intrinsics, c2w = batch_parse_llff_poses(poses) self.train_intrinsics.append(intrinsics[i_train]) self.train_poses.append(c2w[i_train]) self.train_rgb_files.append(imgnames[i_train]) self.render_rgb_files.extend(imgnames[i_train]) self.render_intrinsics.extend(intrinsics[i_train]) self.render_poses.extend(c2w[i_train]) self.render_depth_range.extend([[near,far]]*len(i_train)) self.render_train_set_ids.extend([i]*len(i_train)) ","date":"2022-08-23T00:00:00Z","image":"https://vita-group.github.io/GNT/assets/overview.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-gnt/","title":"read: GNT"},{"content":" torch.view() 只能处理 contiguous tensor，但 torch.reshape() 不限。\n我在改变形状时：latent = latent.view(SB, NS, self.latent_size, B).permute(0,3,1,2).view(-1, NS, self.latent_size)，\n报错： RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\ncontiguous means \u0026ldquo;Two adjacent cells in a row of tensor are neighbors on the memory as well\u0026rdquo;. What is the difference between contiguous and non-contiguous arrays?\n(2023-12-23) \u0026ldquo;数组存储顺序与按行展开的顺序一致\u0026rdquo;。 【代码精读】开山之作MVSNet PyTorch版本超详细分析 - doubleZ的文章 - 知乎\nview vs reshape: What\u0026rsquo;s the difference between reshape and view in pytorch?\nFind whether a tensor is copied or not after reshape: Check if the reshaped tensor will be affected by the original tensor.\nStride (2023-09-28) contiguous means the relative sequence (underlying 1D memory arrangement when they were first created) of elements in a tensor hasn\u0026rsquo;t been changed, although the shape can be mutated. (The data on memory blocks never reshape.)\nE.g., the layout of a = torch.arange(4) means 0,1,2,3 are next to each other in this order. a.stride() is (1,).\nb = a.reshape(2,2) $[^{[0,1]}_{[2,3]}]$ is still contiguous. Because only the data\u0026rsquo;s dimensional interpretation changed, and the read order of 0,1,2,3 didn\u0026rsquo;t change. b.stride() is (2,1).\ntorch.flip(b, [0]) $[^{[2,3]}_{[0,1]}]$ is contiguous, because they\u0026rsquo;re next to each other in terms of the tensor\u0026rsquo;s .stride() which remains (2,1).\nThe stride is responsible to the initial underlying layout, if the stride gets changed, the tensor won\u0026rsquo;t be contiguous anymore.\nThe operation transpose and permute will change stride, which isn\u0026rsquo;t row-contiguous any longer.\nso b.transpose(0,1) or b.permute(1,0): $[^{[0,2]}_{[1,3]}]$ is not contiguous any more.\n.contiguous() will change the stride to match the current shape. What does .contiguous() do in PyTorch?\n(2023-12-22) .contiguous() will copy the data to a new memory strip, which can be checked via .storage(). The new and the original tensors won\u0026rsquo;t affect each other. Pytorch - Contiguous vs Non-Contiguous Tensor / View — Understanding view(), reshape(), transpose() - Kathryn\n.permute() made .stride() decoupled with the tensor\u0026rsquo;s varying shapes mutating from the original shape.\nThat means when the tensor reverts to the original size, the .stride() won\u0026rsquo;t return to the structure matched with the tensor\u0026rsquo;s original shape.\n1 2 3 4 5 6 7 8 9 a = torch.arange(8).reshape(2,4) # contiguous b = a.transpose(0,1) # (4,2) not contiguous b.view(2,4) # Can\u0026#39;t work # because the stride don\u0026#39;t match the target shape b.contiguous().view(2,4) # Or use b.reshape(2,4) # will copy data if .view() don\u0026#39;t work. Therefore, .contiguous() is necessary before view() after transpose() or permute().\nRead-Write Head (2023-10-13) stride 是为了从一条内存上索引（\u0026ldquo;拼凑\u0026rdquo;） 取出 张量的某一个维度，读写头的步幅。 How does NumPy\u0026rsquo;s transpose() method permute the axes of an array?\n计算一个 tensor 的 stride：给定一个 contiguous 的tensor： x.shape = (2,2,4), 则 x.stride() = (8,4,1), 所以 一个维度上的 stride 等于它后面的维数累乘：8=2×4, 4=4×1, 1=1.\n【pytorch Tensor shape 变化 view 与 reshape（contiguous 的理解）】\ntranspose 转置某2个维度，即交换那两个维度上的 stride，不同的维度在内存上走不同的距离。\n.T 会倒序全部维度上的 stride, e.g., (2,2,4) -\u0026gt; (4,2,2)\n(2023-10-23) Examples:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026gt;\u0026gt;\u0026gt; a = torch.arange(6).reshape(2,3,1) # stride: (3,1,1) tensor([[[0], [1], [2]], [[3], [4], [5]]]) \u0026gt;\u0026gt;\u0026gt; b = a.transpose(0,1) # stride: (1,3,1), not contiguous tensor([[[0], [3]], [[1], [4]], [[2], [5]]]) \u0026gt;\u0026gt;\u0026gt; c = a.transpose(1,2) # stride: (3,1,1), contiguous tensor([[[0, 1, 2]], [[3, 4, 5]]]) \u0026gt;\u0026gt;\u0026gt; d = a.transpose(0,2) # stride: (1,1,3), not contiguous tensor([[[0, 3], [1, 4], [2, 5]]]) \u0026gt;\u0026gt; e = a.T # stride: (1,1,3) .transpose swaps strides, .T reverses strides, and permute reorders strides, while .view changes stride to match the shape while keeping elements \u0026ldquo;互相接壤的\u0026rdquo;。\ncontiguous \u0026ldquo;互相接壤的\u0026rdquo;： 矩阵的一行中相邻的 2 个元素，在内存上也相邻。 其中，矩阵一行的末尾与下一行的开头相邻。\n(2023-10-24) contiguous means that no matter how the shape changes, the movement of the read/write head acts like indexing a flattened tensor.\nRectangle\u0026rsquo;s Shape (2023-12-10) Changing the shape of a tensor is like stretching an area-fixed rectangle, although h and w are changed, the relative order of internal elements is not changed.\nImagine dragging the bottom right corner to change the height and width of the rectangle. Alternatively, one can imagine the data as sand grains being enclosed by a size-changble frame. And once the shape changed, the rows are filled up first:\nm e m o r y : R ⋅ ⋅ ⋅ W 3 ⋅ ⋅ ⋅ 1 ↑ h x e ⋅ ⋅ ⋅ 2 a 6 d ⋅ ⋅ ⋅ 3 ⋅ ⋅ ⋅ 4 ⋅ ⋅ ⋅ ⋯ D f r i H c a l , h g l a W n h i g e n e r t e o ⋅ ⋅ ⋅ ⋅ 2 ⋅ ⋅ x ⋅ ⋅ 9 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ 1 8 ⋅ ⋅ The reading order is unchanged, keeping the sequence from 1 to 18:\n1 7 1 3 2 8 1 4 3 9 1 5 4 1 1 0 6 5 1 1 1 7 6 1 1 2 8 1 1 0 2 1 1 3 1 2 4 1 3 5 1 4 6 1 5 7 1 6 8 1 7 9 1 8 (我忘了是不是有这么一种玩具：有一个长方形，框住了一些“棋子”。 当你拖动长方形的一个角的时候，因为它面积是固定的，所以里面的棋子会重新排列。对应到 .view() 就是每行先填满。)\nSince the view doesn\u0026rsquo;t changes the stride of the read/write header, the target shape requires to match the current stride. torch view - Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 xy= torch.cartesian_prod(torch.arange(3), torch.arange(2)) # tensor([0, 0], # [0, 1], # [1, 0], # [1, 1], # [2, 0], # [2, 1]]) # memory: 0 0 0 1 1 0 1 1 2 0 2 1 # shape: (2,6), stride: (2,1) print(xy.view(2,6)) # contiguous, stride: (4,1) # tensor([[0, 0, 0, 1, 1, 0], # [1, 1, 2, 0, 2, 1]]) # 一条龙串下来 xy.t().is_contiguous() # False, stride: (1,2) # tensor([[0, 0, 1, 1, 2, 2], # [0, 1, 0, 1, 0, 1]]) view is filling an empty box from the innermost dimension to outermost by consuming the 1D memory data.\n(2023-12-22) The tensors created by torch.meshgrid() are not contiguous.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 y, x = torch.meshgrid(torch.arange(3), torch.arange(2)) # y: vertical coordinate # tensor([[0, 0], # [1, 1], # [2, 2]]) # Memory is: 0 1 2 # Not contiguous. stride on each dim: (1,0) y.storage() # 0 # 1 # 2 # [torch.LongStorage of size 3] # x: horizontal coordinate # tensor([[0, 1], # [0, 1], # [0, 1]]) # Memory is 0 1 # Not contiguous. stride on each dim: (0,1) x.storage() # 0 # 1 # [torch.LongStorage of size 2] y.contiguous().stride() # stride: (2,1) When reading y and x, the read-write head has to go back or repeat some bytes, instead iterates the 1D memory sequence once, so they\u0026rsquo;re not contiguous.\nrepeat will copy the data, while expand won\u0026rsquo;t, with a stride of 0 on the singleton dimension. (singleton means the size of that dimension is 1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 y.unsqueeze(2).repeat(1,1,2) # shape: (3,2,2), contiguous # tensor([[[0, 0], # [0, 0]], # # [[1, 1], # [1, 1]], # # [[2, 2], # [2, 2]]]) # stride: (4,2,1) y.unsqueeze(2).expand(-1,-1,2) # shape: (3,2,2), Not contiguous # tensor([[[0, 0], # [0, 0]], # # [[1, 1], # [1, 1]], # # [[2, 2], # [2, 2]]]) # stride: (1,0,0) ","date":"2022-08-21T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_contiguity/","title":"memo: PyTorch | Contiguous \u0026 Stride"},{"content":"Background 全连接：线性层串行\n输入图像 in pytorch：(C, H, W) ToTensor doc\nCNN = Feature Extraction + Classification\nCCD 光敏电阻 + 透镜系统， 一个电阻只检测一个光锥区域的光线，根据电阻与光强的函数关系，得到灰度图\ninput channels: rgb\nConv Layer 每次取一个块：(C * kernel_h * kernel_w) 通过 Conv layer 得到 (C\u0026rsquo; * kernel_h\u0026rsquo; * kernel_w\u0026rsquo;)。\n做 Conv layer 时，input 的所有 channel 都做卷积操作（各通道使用的卷积核不同），然后(按不同权重)相加。 这两步做 n 次，那么 output的 channels 就是 n。所以后续的每一层 channel 都包含了input 的全部通道的信息。 CNN 的权重存在于卷积核中。\n(2023-10-22) 各输入通道 乘以的 kernel 不同，然后各通道直接 相加:\n1 2 3 4 5 6 7 8 9 10 a = torch.arange(18.).reshape(1, 2,3,3) # (bs, C, h,w) n = 1 # convolution repeat times conv_lyr = torch.nn.Conv2d(a.size(1), n, kernel_size=3, bias=False) print(conv_lyr.weight) # shape (1,2,3,3) conv_lyr.weight = torch.nn.Parameter( torch.stack([torch.ones(3,3), torch.zeros(3,3)]).unsqueeze(0)) print(conv_lyr.weight) out_a = conv_lyr(a) # (1,1,1,1), tensor([[[[36.]]]]) # i.e. 0+1+2+3+4+...+8 + 0+0+..0 = 36. (2023-10-22) Conv2d 与 FC 做的数学运算 相同：\np p p p i i i i F x x x x C e e e e l l l l L 1 2 3 4 a y c e h r n R G B R G B R G B R G B ' l - - - - - - - - - - - - s s d w i w ₂ w w m ₁ ₃ ₄ 1 d d d d i i i i m m m m 1 1 1 1 o o o o f f f f o o o o u u u u t t t t 1 2 3 4 1 s t A S O u c p u m h o t n r o l t = f i C o o a o f n l n l v 4 o f ⁺ w l p e a i o i y x x u g e e t h r l p t ' s u ⁺ e s t d w w ' c ₁ ₃ s i h n n 1 p l w w s u ₂ ₄ t t 1 c c h h n n l l s 如果是 1x1 的卷积 (stride=1)，即没有邻居像素相加，Conv 就会和 FC 等价: 把每个像素投影到另一个维度的空间。\nchnl1 × w\u0026rsquo; + chnl2 × w\u0026rsquo;\u0026rsquo; + chnl3 × w\u0026rsquo;\u0026rsquo;\u0026rsquo; = out-chnl1\ndim1 × w\u0026rsquo; + dim2 × w\u0026rsquo;\u0026rsquo; + dim3 × w\u0026rsquo;\u0026rsquo;\u0026rsquo; = out-dim1\n(2023-12-01) I already forgot the original meaning of the left picture because I didn\u0026rsquo;t write descriptions.\nFor a FC layer given by 4 samples of 3 dimensions, each dimension multiplied by a factor wₓ becomes a portion of the out dimension. This corresponds to the operations in Conv2d layer: every channel is multiplied by a kernel and then sum up all weighted channels to form one of out channels.\nThe difference is that in FC the 4 samples aren\u0026rsquo;t merged, but Conv2d merged 4 pixels to 1 pixel. Thus, the number of pixels in FC is consistent, whereas conv layer reduces pixels.\nEach channel in feature maps always shares a common 2-D kernel. A Conv2d layer stands for a 4-D kernel as each channel uses different kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch in_channels, out_channels = 5, 10 width, height = 100, 100 kernel_size = 3 batch_size = 1 input = torch.randn(batch_size, in_channels, width, height) # (1,5,100,100) conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size) output = conv_layer(input) # (1,10,98,98) print(conv_layer.weight.shape) # (10,5,3,3) Padding 保持输出图像尺寸不变。3x3核pad 1圈，5x5 pad 2圈\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) conv_layer = torch.nn.Conv2d(1,1, kernel_size=3, padding=1, bias=False) kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3) # (out_chnls, in_chnls, h, w) conv_layer.weight.data = kernel.data output = conv_layer(input) print(output) stride 卷积核移动步长，（减少步数）用于缩小feature map 的大小\nMax Pooling 下采样, 每 h x w 区块里面取最大值，kerenl_size=2 尺寸缩小为原来的一半。该操作无参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch input = [3, 4, 6, 5, 7, 2, 4, 6, 8, 2, 1, 6, 7, 8, 4, 9, 7, 4, 6, 2, 3, 7, 5, 4, 1] input = torch.Tensor(input).view(1, 1, 5, 5) maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2) output = maxpooling_layer(input) print(output) \u0026gt;\u0026gt;\u0026gt; tensor([[[[4., 8.], \u0026gt;\u0026gt;\u0026gt; [9., 8.]]]]) 定义网络：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5) self.pooling = torch.nn.MaxPool2d(kernel_size=2) self.fc = torch.nn.Linear(320, 10) def forward(self, x): batch_size = x.size(0) x = F.relu(self.pooling(self.conv1(x))) x = F.relu(self.pooling(self.conv2(x))) x = x.review(batch_size, -1) # or flatten x = self.fc(x) return x # 使用交叉熵损失，所以不做激活 model = Net() device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_avaliable() else \u0026#34;cpu\u0026#34;) model.to(device) ","date":"2022-08-14T16:52:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/10_cnn%E5%9F%BA%E7%A1%80/","title":"watch: PyTorch - 刘二 10 | CNN Basics"},{"content":"【北京大学】Tensorflow2.0 - bilibili\n创建环境 1 2 3 4 conda create -n tf2.1 python=3.7 conda install -c conda-forge cudatoolkit=10.1 cudnn=7.6 pip install --upgrade pip pip install tensorflow==2.1 导入失败，报错：\n1 2 3 4 5 TypeError: Descriptors cannot not be created directly. If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc \u0026gt;= 3.19.0. If you cannot immediately regenerate your protos, some other possible workarounds are: 1. Downgrade the protobuf package to 3.20.x or lower. 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower). 查看protobuf 版本：pip show protobuf\n1 2 pip uninstall protobuf pip install protobuf==3.19.0 StackOverflow ans\n","date":"2022-08-14T15:15:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/0/","title":"watch: TF2 - PKU 00 | Environment Setup"},{"content":"编码器中是 self-attention，是自编码，q、k、v同源，计算出各单词(query)在整个句子(values、keys) 中的份量； 解码器中是 encoder-decoder attention，q、k(v)不同源，query是解码器已生成内容的词向量，values、keys 来自编码器； decoder 输出一个词向量，还要经过 linear 和 softmax 才能变成单词。\n17 Transformer 的解码器（Decoders）——我要生成一个又一个单词- 爱钓鱼的程序猿\n","date":"2022-08-06T17:20:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/17/","title":"watch: Transf - Nick 17 | Decoder"},{"content":"Reference:\nPL Viedo tutorial of AI葵 youtube\nkwea123/pytorch-lightning-tutorial\nPyTorch Lightning official Tutorial\nPL example mnist source code\npytorch-lighting 优点：\n方便在各种设备(cpu/gpu/tpu/多节点)上运行，只需关心算法实现 控制随机数，重现实验结果，固定划分各batch 工程文件夹目录：\nrequirements.txt : 所需依赖包 train.py : main opt.py : hyperparameter models 文件夹： 包含不同的模型 networks1.py datasets 文件夹： 包含不同数据集的 dataloader losses.py: 各种loss func .gitignore: ckpts/, logs/, MNIST/ requirements.txt\n1 2 3 torch==1.11.0 torchvision==0.12.0 pytorch_lightning==1.6.0 Versioning Policy opt.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import argparse def get_opts(): parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--root_dir\u0026#39;, type=str, default=\u0026#39;./data/\u0026#39;, help=\u0026#39;root directory of dataset\u0026#39;) parser.add_argument(\u0026#39;--hidden_dim\u0026#39;, type=int, default=128, help=\u0026#39;number of hidden dimensions\u0026#39;) parser.add_argument(\u0026#39;--val_len\u0026#39;, type=int, default=5000, help=\u0026#39;number of validation samples split from train set\u0026#39;) parser.add_argument(\u0026#39;--batch_size\u0026#39;, type=int, default=128, help=\u0026#39;number of training samples in one batch\u0026#39;) parser.add_argument(\u0026#39;--lr\u0026#39;, type=float, default=1e-4, help=\u0026#39;learning rate\u0026#39;) parser.add_argument(\u0026#39;--num_epochs\u0026#39;, type=int, default=1, help=\u0026#39;number of epochs\u0026#39;) parser.add_argument(\u0026#39;--num_gpus\u0026#39;, type=int, default=1, help=\u0026#39;number of gpus to be used\u0026#39;) parser.add_argument(\u0026#39;--expname\u0026#39;, type=str, default=\u0026#39;test\u0026#39;, help=\u0026#39;experiment name\u0026#39;) # return parser.parse_args() args, unknown = parser.parse_known_args() return args networks1.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch from torch import nn class myLinearModel(nn.Module): def __init__(self, hidden_dim): super().__init__() self.net = nn.Sequential( nn.Linear(28*28, hidden_dim), nn.ReLU(True), nn.Linear(hidden_dim, 10) ) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; x: (B, 1, 28, 28) channel=1 \u0026#39;\u0026#39;\u0026#39; x = x.flatten(start_dim=1) # (B, 28*28) return self.net(x) train.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 import torch from torch import nn from torch.nn import functional as F from torchvision import datasets, transforms from torch.utils.data import DataLoader, random_split from torch.optim.lr_scheduler import CosineAnnealingLR # from opt import get_opts # from models.networks1 import myLinearModel from pytorch_lightning import LightningModule, Trainer, seed_everything from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar from pytorch_lightning.loggers import TensorBoardLogger seed_everything(1234, workers=True) def get_learning_rate(optimizer): # for recording logs for param_group in optimizer.param_groups: return param_group[\u0026#39;lr\u0026#39;] class MNISTSystem(LightningModule): # LightningModule puts all parts together # Design model def __init__(self, hparams): \u0026#39;\u0026#39;\u0026#39; hparams: all hyper parameters \u0026#39;\u0026#39;\u0026#39; super().__init__() # self.hparams = hparams self.save_hyperparameters(hparams) # store exp conditions for reproduction and loss visualization # network components self.net = myLinearModel(self.hparams.hidden_dim) def forward(self, x): return self.net(x) # Prepare data def prepare_data(self): \u0026#39;\u0026#39;\u0026#39; Download train set and test set (execute only once) \u0026#39;\u0026#39;\u0026#39; datasets.MNIST(self.hparams.root_dir, train=True, download=True) datasets.MNIST(self.hparams.root_dir, train=False, download=True) def setup(self, stage=None): \u0026#39;\u0026#39;\u0026#39; Preprocessing Split the train and validation set (called by every device) \u0026#39;\u0026#39;\u0026#39; dataset = datasets.MNIST(self.hparams.root_dir, train=True, download=False, transform=transforms.ToTensor()) # load data (B, channel, H,W) train_length = len(dataset) self.train_set, self.val_set = random_split(dataset, \\ [train_length - self.hparams.val_len, self.hparams.val_len]) def train_dataloader(self): \u0026#39;\u0026#39;\u0026#39; Split the train set into multiple batches for one epoch \u0026#39;\u0026#39;\u0026#39; return DataLoader(self.train_set, shuffle=True, # every epoch is different num_workers=4, # cpu threads batch_size=self.hparams.batch_size, pin_memory=True) def val_dataloader(self): return DataLoader(self.val_set, shuffle=False, # comparing acc between epochs num_workers=4, batch_size=self.hparams.batch_size, pin_memory=True) # Construct optimizer and loss func def configure_optimizers(self): self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.hparams.lr) schedular = CosineAnnealingLR(self.optimizer, T_max=self.hparams.num_epochs, \\ eta_min=self.hparams.lr/1e2) return [self.optimizer], [schedular] # different models use different optimizers,schedulars # Training cycle def training_step(self, batch, batch_idx): \u0026#39;\u0026#39;\u0026#39; batch: come from iterable train_dataloader batch_idx: index of the current batch \u0026#39;\u0026#39;\u0026#39; imgs, labels = batch # images\u0026#39; pixels, labels logits = self(imgs) # call forward loss = F.cross_entropy(logits, labels) # including softmax # tensorboard self.log(\u0026#39;train/loss\u0026#39;, loss) self.log(\u0026#39;lr\u0026#39;,get_learning_rate(self.optimizer)) return loss def validation_step(self, batch, batch_idx): \u0026#39;\u0026#39;\u0026#39; Compute acc for every batch \u0026#39;\u0026#39;\u0026#39; imgs, labels = batch logits = self(imgs) loss = F.cross_entropy(logits, labels) acc = torch.sum(torch.eq(torch.argmax(logits, -1),labels).to(torch.float32))/len(labels) log = {\u0026#39;val_loss\u0026#39;:loss, \u0026#39;acc\u0026#39;: acc} return log def validation_epoch_end(self, batch_outputs) -\u0026gt; None: \u0026#39;\u0026#39;\u0026#39; Compute average loss/acc among all batches for one epoch \u0026#39;\u0026#39;\u0026#39; mean_loss = torch.stack([x[\u0026#39;val_loss\u0026#39;] for x in batch_outputs]).mean() mean_acc = torch.stack([x[\u0026#39;acc\u0026#39;] for x in batch_outputs]).mean() self.log(\u0026#39;val/loss\u0026#39;, mean_loss, prog_bar=True) # record loss of every step self.log(\u0026#39;val/acc\u0026#39;, mean_acc, prog_bar=True) # show on the progress bar if __name__ == \u0026#39;__main__\u0026#39;: hparams = get_opts() mnistsystem = MNISTSystem(hparams) # construct training system # save weights to files ckpt_cb = ModelCheckpoint(dirpath=f\u0026#39;ckpts/{hparams.expname}\u0026#39;, filename=\u0026#39;{epoch:d}\u0026#39;, # epoch=0,... monitor=\u0026#39;val/acc\u0026#39;, mode=\u0026#39;max\u0026#39;, save_top_k=5) # only store 5 max acc models\u0026#39; weights (-1 all) # progress bar pbar = TQDMProgressBar(refresh_rate=1) callbacks = [ckpt_cb, pbar] # tensorboard events logger = TensorBoardLogger(save_dir=\u0026#39;logs\u0026#39;, name=hparams.expname, default_hp_metric=False) # trainer = Trainer(max_epochs=hparams.num_epochs, callbacks=callbacks, logger = logger, enable_model_summary = True, # print model structure accelerator=\u0026#39;auto\u0026#39;, # devices type devices = hparams.num_gpus, num_sanity_val_steps = 1, # run once val before training to verfiy if it\u0026#39;s normal benchmark=True, # cudnn accelerate need each batch has same size profiler=\u0026#34;simple\u0026#34; if hparams.num_gpus==1 else None, # count time for every operation ) trainer.fit(mnistsystem) ","date":"2022-08-06T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pl_template-aikui/","title":"memo: PL | Template - AIkui"},{"content":"工具类：\nDataset 用于构造数据集，可以用索引取出数据\nDataLoader: 取出一个 mini-batch，做训练\n\u0026ldquo;Batch\u0026rdquo; 是把全部数据输入神经网络，计算预测值，这样可以充分发挥向量计算的并行性，速度很快，但是可能到达鞍点，无法继续训练。可以利用数据的噪声避免停留在鞍点上，从而达到全局最优，所以需要1个样本，1个样本地计算，叫作随机梯度下降SGD，性能较好但是训练时间太长。所以采用 \u0026ldquo;mini-batch\u0026rdquo; 来平衡速度和性能。（通常把mini-batch叫做batch）\nEpoch: 训练轮数，所有的样本都做过一次前馈和反馈\nBatch-size: 每次训练使用的样本量，经过一次前馈、反馈和更新\nIteration：迭代次数，分了几个batch\nDataLoader: 对支持索引，支持获取其长度的数据集进行加载，生成一个 iterable loader, 对数据分组，每次迭代给出一个batch的X和y，并自动转换成Tensor。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torch from torch.utils.data import Dataset #抽象类,不能实例化，只能被它的子类继承 from torch.utils.data import DataLoader #加载, 划分数据 class DiabetesDataset(Dataset): #自定义的类继承自Dataset def __init__(self): pass # def __getitem__(self, index): #使对象支持下标操作，根据索引返回数据 pass def __len__(self): #返回数据集的条数 pass dataset = DiabetesDataset() #实例化数据集，支持索引数据和获取长度 train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) #实例化加载器, 洗牌使每次epoch的数据都不一样，使用2个并行进程读取数据 加载数据有两种方式，如果数据量小，直接把所有的数据加载进内存，之后使用getitem读取；当数据集很大时，需要将其拆分成小文件，在__init__方法中初始化列表，把小文件名放入列表中；有时标签也很大，比如对图片的每个像素预测语义信息，y的维度与x相同，所以也需要拆分，文件名放入列表。在需要使用某数据的时候，用__getitem__方法读取。\npytorch 0.4中出现一个问题：由于windows 和 linux的多进程库不同，spawn与fork不同。Windows下需要把迭代的loader封装起来，不能直接“顶格”写\n1 2 3 4 5 6 train_loader = DataLoder(...) if __name__ == \u0026#39;__main__\u0026#39;: for epoch in range(100): for i, data in enumerate (train_loader, 0): ... 加载糖尿病数据集:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import torch from torch.utils.data import Dataset, DataLoader class DiabetesDataset(Dataset): def __init__(self, filepath): #需要文件名 xy = np.loadtxt(filepath, delimiter=\u0026#39;,\u0026#39;, dtype=np.float32) self.len = xy.shape[0] #样本条数：矩阵的第一个维度 self.x_data = torch.from_numpy(xy[:, :-1]) #样本, 直接放在内存中 self.y_data = torch.from_numpy(xy[:, [-1]]) #标签取最后一列 def __getitem__(self, index): return self.x_data[index], self.y_data[index] #返回一个元组 def __len__(): return self.len data = DiabetesDataset(\u0026#39;diabetes.csv.gz\u0026#39;) train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) for epoch in range(100): for i, data in enumerate(train_loader, 0): #迭代加载器，enumerate获得第几次迭代，x和y的元组放入data inputs, labels = data #取出样本和标签 y_pred = model(inputs) #计算预测值 loss = criterion(y_pred, labels) #计算损失 optimizer.zero_grad() #梯度清零 loss.backward() #反向传播 optimizer.step() #更新梯度 ","date":"2022-08-05T19:25:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/8_%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/","title":"watch: PyTorch - 刘二 08 | Load Datasets"},{"content":"注意力核心作用：特征融合。在输入的序列内做特征融合。\nbilibili\ncnblog\n从海量数据中找出重要的部分，设计一种运算让机器自动甄别数据，对于深度学习领域，为模型添加一个模块，只关心输入输出即可\n如何在深度学习模型上做注意力?\n一张图片的信息分布是不均匀的，人类会给蕴含更多、更重要信息的区域更多的注意力。\n参与“注意力”的两个主体：人脑（query），图片（value=key），人脑知道哪些地方重要，它会着重看图片中的那些地方，所以应该给那些区域更高的权重，也就是计算人脑中的“重要度分布”模版与图片的相似度。\nquery 与 key (=value) 可以做点乘，计算余弦相似度。 一个 query 与 keys 的各个部分做内积得到多个标量，就是各个部分与 query 的相似度，即注意力分数\n对这些分数做一次 softmax()，得到各个key 与 query的相似度的概率分布，就是各个部分的权重，有些部分权重趋于0，则它们被舍弃了，只把重点提取出来，把各个部分加权求和，就得到\u0026quot;被注意过后\u0026quot;的图片了，比原始图片多了每部分的重要度。对这个新图片再做卷积，重要的部分会被保留下来。\n(如果一张图片以像素级别做注意力，计算量太大，所以可以先用卷积提取一下特征，把尺寸降下来，如果卷积太多次了，与原始图片差异太大了，可以用跳连接补充原始信息)\n一般 keys = values，也可以不相等，但二者之间必有某种联系，这样才能依据 query 与 keys 的相似度找出 values 中重要的和不重要的部分。\n每一个 query 都与 keys 点积，则会产生 num_query x num_key 的注意力分数矩阵\n内积得到各个 key 的注意力分数之后，除以根号下方差dk (标准差)，做一个缩放是为了平滑各分数间的差距，如果差额太大，做了e指数，高分数概率会很大，而低分数的概率会很低，然后乘以 value 就是变成很小的值，导致在反向传播时，这些小数值没有足够的梯度（梯度消失），更新不了它对应的权重。或者说如果不除以标准差，各key注意力分数的分布的方差可能很大，大的很大，小的很小，做了softmax后，小数更小。所以除以标准差，让方差为1，削弱差距。(为什么不让方差再小呢？)\n","date":"2022-08-05T17:37:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/9_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","title":"watch: Transf - Nick 09 | Attention Mechanism"},{"content":"Code | Arxiv | ProjPage\n一句话：用图像特征训练NeRF，（所以有泛化能力,不局限于单场景，不像nerf只做优化）\n如图1，光线是随机从像素空间选择的（为了随机选一些(N个)空间点），空间点投影到不同视图的 feature maps (512 chnls) 上，得到 (N∗Nviews个) feature volume，送入 mlp 变换成 r,g,b,density，再做 volume rendering\nAbstract NeRF conditioned by few images costly independent optimizing use convolution to learn scene prior no explicit 3D supervision single image novel view synthesis task 1 Introduction Topic: Synthesis novel views for a scene from sparse views\nProblem\nFormer solution \u0026amp; drawbacks:\nDifferentiable neural rendering: represent the sence as a neural network which generates rendered images NeRF: encode the 3D position and view dirs to volume density and color. It needs too many images. Task: predicting NeRFs from one or several images.\nContributions: Utilize pixel-aligned spatial image features to learn scene priors (specifically) Input image → convolutional feature grid → 𝐱,𝐝,feature(residual) → NeRF (pool-based multi-view features fusion) → σ,rgb\nResults statement\nno 3D supervision: 3D shape or object masks not trained in a canonical coordinate system, but in each camera coordinate system \u0026ldquo;convolution preserves the spatial alignment between the image and the output 3D representation\u0026rdquo; flexibility on number of input views in test period (Experiments) ShapeNet; DTU\n2 Related Work Novel View Synthesis Learning-based 3D reconstruction Viewer-centric 3D reconstruction 3 Background NeRF (job) (input-output) x,d -\u0026gt; density, color (training manner) volume rendering (loss func) Limitation: only use geometric consistency, resulting in individual optimization for every view 4 Image-conditioned NeRF (improvement) input spatial image features into network. (Specifically) Two components: convolutional encoder + nerf mlp. Spatial query position is drawn from camera space.\n(paragraph order) From simple case to general case\n4.1 Single-Image pixelNeRF (main idea) The inputs are all from view space. (steps) Input image → feature volume W=E(I) → assign image feature to sample points by projecting the 3D position to 2D location → NeRF network (pipeline) (the role of query view direction) 4.2 Incorporating Multiple Views (main idea) extract information from multi views to resolve geometric ambiguities and able to accept multiple images in test time. No relation with world space\nWorld space can base on any view. (notion explain)\nProject the world query point into each input view space. The fisrt layer processes each view independently and the final layer aggregates all views.\n(model pipeline)\nEncode each input img into feature volume and retrieve the corresponding feature at the sample point\u0026rsquo;s projecting location. The features companied with (xyz,viewdirs) are input to the first layer and all the intermediate vectors are fused by average pooling before entering the final layers, which compress the vector into density and color. 5 Experiments Datasets:\nShapeNet (category-specific and category-agnostic view synthesis); ShapNet scenes with unseen categories and multiple objects; DTU rescaled to 1/4: 400x300 Baselines: SRN, DVR\nMetrics: PSNR, SSIM, LPIPS\nImplementation Details:\nImage encoder: ResNet34; Features: non-pooling, upsampled using bilinear interpolation, concatenated feature maps of every image. Combine the points\u0026rsquo; position and view direction in a ResNet manner (residual) 5.1 ShapeNet Benchmarks ShapeNet for category-specific and category-agnostic\nCategory-specific View Synthesis Benchmark\none-shot and two-shot view synthesis\nA model is trained for reconstructing one category and the used images contains 50 random views per object instance. Only 2 are encoded and fed into network.\nAblations. The benefit of local features\nCategory-agnostic Object Prior\nTrain a single model to the 13 largest categories of ShapeNet pick one view randomly from 24 fixed elevation view of each object instance. 5.2 Pushing the Boundaries of ShapeNet (tasks) less controlled capture scenarios in ShapeNet:\nunseen object categories, multiple-object scene, simulation-to-real transfer on car images. Generalization to novel categories: apply the model on unseen categories\nMultiple-object scenes: The geometric features need can be applied in any view direction (360°)\n5.3 Scene Prior on Real Images Code (2023-07-20)\ncalc_losses():\n\\begin{algorithm} \\caption{Main steps of clac$\\_$losses()} \\begin{algorithmic} \\PROCEDURE{clac-losses}{} \\STATE Sample 4 objects from 88 training scanned objects (folder) as a super batch \\STATE Each object samples randomly 1 or 3 from its total of 49 NV as target images for training \\STATE Sample a batch of rays (N$\\_$rand) \\STATE Extract latent feature maps `self.latent` \\STATE Sample z$\\_$coarse points on the rays \\STATE Transform points onto NS camera space \\STATE Perspective project xyz to 2D location uv on feature maps \\STATE Indexing latent vectors from feature maps by `F.grid$\\_$sample()` \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} DTU dataset (2023-08-17)\nget_split_dataset() (in \u0026ldquo;data/init.py\u0026rdquo;) will construct different Dataset objects for different datasets.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 flags[\u0026#34;list_prefix\u0026#34;] = \u0026#34;new_\u0026#34; if training: flags[\u0026#34;max_imgs\u0026#34;] = 49 flags[\u0026#34;sub_format\u0026#34;] = \u0026#34;dtu\u0026#34; flags[\u0026#34;scale_focal\u0026#34;] = False flags[\u0026#34;z_near\u0026#34;] = 0.1 flags[\u0026#34;z_far\u0026#34;] = 5.0 # Apply color jitter during train train_aug = ColorJitterDataset train_aug_flags = {\u0026#34;extra_inherit_attrs\u0026#34;: [\u0026#34;sub_format\u0026#34;]} train_set = DVRDataset(path=datadir, stage=\u0026#34;train\u0026#34;, **flags, **kwargs) if train_aug is not None: train_set = train_aug(train_set, **train_aug_flags) ","date":"2022-08-04T00:00:00Z","image":"https://ar5iv.labs.arxiv.org/html/2012.02190/assets/x2.png","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-pixelnerf/","title":"read: PixelNeRF"},{"content":"Self-Attention(自注意力机制) - bilibili\nvalue 是一个 d 维的向量； 现分析 n 个 value:\nself-attention 的目的是为了使每个 value 带上所有 value 的信息，也就是所有 value 加权相加，相似部分的权重大些，无关部分的权重小些。\n权重即是各 value 与 value 之间的相似度，只需每个 value 都与所有 value 做内积就可得到，内积大说明两个value夹角小，含义相似；如果各value之间正交，比如one-hot编码，那么只有自己和自己乘结果是1，其余都是0。\n用矩阵运算表示就是“自己乘以自己的转置” (n,d)⋅(n,d)ᵀ→(n,n)，得到 nxn 的注意力分数方阵\n把这套机制放到Attention运算中，queries 和 keys 都等于 values。\n但是这些权重是不变的，因为每次 querys 都等于 keys，对协方差矩阵做完softmax 得到的分布总是一样的？ doubt: 实验：两个矩阵分别自己和自己做矩阵乘法，然后做softmax，观察两个结果\n如果仅凭 values 自己调整，要达到任务所需的向量表示，可能速度太慢，所以分别给 q,k,v 加了一层线性变换，反向传播时也调整这个线性层的权重，让 value 的新词向量更快的移动到准确的位置。\u0026ldquo;从而提升模型的拟合能力1\u0026quot;。\nSelf-attention 中每一个 input 都与所有 input 做内积，没有考虑到 input 的顺序，所以原始文本的顺序信息丢失了，所以需要位置编码 1\n例子：\n两个单词 thinking 与 machines，分别乘上 Wq, Wk, Wv 得到线性变换后的 queries (q1,q2), keys (k1,k2), values (v1,v2)\ns11 = q1 × k1, s12 = q1 × k2; 然后 s11, s12 做scale，再做softmax得到两个权值 s11\u0026rsquo; (0.88), s12\u0026rsquo; (0.12), 则 z1 = s11\u0026rsquo; × v1 + s12\u0026rsquo; × v2 就是 thinking 的新的向量表示。\n对于 thinking，初始的词向量（one-hot, Elmo）为 x1，这个x1 与其他词的向量正交，无关，不包含其他单词的任何信息，对 thinking machines 这两个词做完 self-attention 之后，thinking 新的词向量带上了 machines 的信息，带上了多少呢？带上了 machines 与 thinking 相似的部分。或者说：这个新词向量蕴含了 thinking machines 这句话对于 thinking 而言哪个词更重要的信息。\n新的词向量包含了整个句子所有单词的信息，重要的单词占比多一点。\nAttention 与 Self-attention 区别： QKV相乘就是注意力，只是一种运算，但并没有规定 QKV 是怎么来的。通过一个查询变量 Q 去找到 V 里面比较重要的东西：QK相乘求相似度 S，然后 SV 相乘得到 V 的新的向量表示（对于词向量，则它包含了句法/语义特征）。Q 可以是任何一个东西，V也是任何一个东西，K 往往是和V 同源的\n自注意力要求 QKV 同源，QKV 是对同一个 X 乘上了不同的权重矩阵，做了不同的线性变换，使它们在空间中岔开了，X 就是一个词向量，但它表达的信息可能没那么准确，通过反向传播调整它们的权重矩阵，把它们移动了合适的位置上，这个位置的词向量可以为我的任务准确地分配各部分的重要性。\n交叉注意力机制：Q 和 V 不同源，但 K 和 V 同源\nSelf-Attention 代码实现pytorch 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from math import sqrt import torch import torch.nn class SelfAttentionLayer(nn.Module): def __init__(self, input_dim, dim_k, dim_v): \u0026#39;\u0026#39;\u0026#39; Inputs: input_dim: dim of input feature vector dim_k: dim of key is same as query, because they do dot product dim_v: dim of value, suitable for task \u0026#39;\u0026#39;\u0026#39; super(SelfAttentionLayer, self).__init__() self.q = nn.Linear(input_dim, dim_k) self.k = nn.Linear(input_dim, dim_k) self.v = nn.Linear(input_dim, dim_v) self._norm_fact = 1/sqrt(dim_k) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; Input: x: (batch_size, seq_len, input_dim) \u0026#39;\u0026#39;\u0026#39; Q = self.q(x) # (batch_size, seq_len, dim_k) K = self.k(x) # (batch_size, seq_len, dim_k) V = self.v(x) # (batch_size, seq_len, dim_v) atten = nn.Softmax(dim=-1)(torch.bmm(Q,K.permute(0,2,1))) * self._norm_fact # (bs,seq_len,seq_len) output = torch.bmm(atten, V) # Q * K.T() * V, (bs, seq_len, dim_v) return output if __name__ == \u0026#39;__main__\u0026#39;: X = torch.randn(4,3,2) self_atten = SelfAttentionLayer(input_dim=2, dim_k=4, dim_v=5) res = self_atten(X) print(res.size()) Multi-head self-attention:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SelfAttentionMultiHead(nn.Module): def __init__(self, input_dim, dim_k, dim_v, num_heads): super(SelfAttentionMultiHead, self).__init__() assert dim_k % num_heads == 0 # dim_k is divided by num_heads assert dim_v % num_heads == 0 self.q = nn.Linear(input_dim, dim_k) self.k = nn.Linear(input_dim, dim_k) self.v = nn.Linear(input_dim, dim_v) self.num_heads = num_heads self.dim_k = dim_k self.dim_v = dim_v self._norm_fact = 1/sqrt(dim_k) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; Input: x: (batch_size, seq_len, input_dim) \u0026#39;\u0026#39;\u0026#39; # 过线性层之后，拆分成（num_heads, bs, seq_len, dim_k÷num_heads） Q = self.q(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.num_heads) K = self.k(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k // self.num_heads) V = self.v(x).reshape(-1, x.shape[0], x.shape[1], self.dim_v // self.num_heads) print(x.shape) print(Q.size()) # Q * K.T() atten = nn.Softmax(dim=-1)(torch.matmul(Q, K.permute(0,1,3,2))) # (bs, seq_len, seq_len) # Q * k.T() * V output = torch.matmul(atten, V).reshape(x.shape[0], x.shape[1], -1) # (bs, seq_len, dim_v) return output Self-Attention - TF 2 1 2 3 4 5 6 7 8 9 10 11 12 def single_head_attention(z): # z: [None, n, dm] Q = tf.keras.layers.Dense(units = dk)(z) # Q: [None, n, dk] K = tf.keras.layers.Dense(units = dk)(z) V = tf.keras.layers.Dense(units = dv)(z) score = tf.matmul(Q, K, transpose_b = True)/ tf.sqrt(dk * 1.0) # score : [None, n, n] W = tf.nn.softmax(score, axis = -1) # W: [None, n, n] H = tf.matmul(W, V) # H: [None, n, dv] return H Multi-head self-attention 3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, dk, dv, num_heads): \u0026#39;\u0026#39;\u0026#39; Inputs: dk: dim of keys after linear layer num_heads: the dk is divided into num_heads parts \u0026#39;\u0026#39;\u0026#39; super(MultiHeadAttention, self).__init__() assert dk % num_heads == 0 assert dv % num_heads == 0 self.dk = dk self.dv = dv self.num_heads = num_heads self.wq = tf.keras.layers.Dense(dk) self.wk = tf.keras.layers.Dense(dk) self.wv = tf.keras.layers.Dense(dv) def call(self, x): \u0026#39;\u0026#39;\u0026#39; x: a batch of sequences that need to do self-attention (batch_size, seq_len, d_input) \u0026#39;\u0026#39;\u0026#39; seq_len = tf.shape(x)[1] q = self.wq(x) # (batch_size, seq_len, dk) k = self.wk(x) # (batch_size, seq_len, dk) v = self.wv(x) # (batch_size, seq_len, dv) # Split the last dimension and transpose to (bs, num_heads, seq_len_q, dk) Q = tf.transpose(tf.reshape(q, (-1, seq_len, self.num_heads, self.dk//self.num_heads)), perm=[0,2,1,3]) K = tf.transpose(tf.reshape(k, (-1, seq_len, self.num_heads, self.dk//self.num_heads)), perm=[0,2,1,3]) V = tf.transpose(tf.reshape(v, (-1, seq_len, self.num_heads, self.dv//self.num_heads)), perm=[0,2,1,3]) # Attention score = tf.matmul(Q, K, transpose_b=True)/ tf.sqrt(self.dk // self.num_heads * 1.0) # scale by the \u0026#34;dk\u0026#34; of one head attention_weights = tf.keras.activations.softmax(score) scaled_attention = tf.matmul(attention_weights, V) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, d_subspace) concat_attention = tf.reshape(scaled_attention, (-1, seq_len, self.dv)) # (batch_size, seq_len_q, dv) return concat_attention, attention_weights Refer:\n超详细图解Self-Attention - 伟大是熬出来的的文章 - 知乎 Transformer I - zaidalyafeai/AttentioNN - github TF tutorial transformer ","date":"2022-08-02T23:37:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/10_%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","title":"watch: Transf - Nick 10 | Self-attention Mechanism"},{"content":"bilibili\n每个单词都要与前后所有单词加权相加，每个单词包含了句子的全员信息，只考虑了相似性，而丢失了 Input sequence 信息 （也就是改变单词顺序，不会改变各单词的词向量）。所以需要先加上位置信息\n为每个 input 做一次位置编码（各 input 的全局顺序）\n\u0026hellip;.\n","date":"2022-07-29T17:28:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/14_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/","title":"watch: Transf - Nick 14 | Positional Encoding"},{"content":"bilibili\n多头是为了用不同的权重初始化线性变换\nSelf-attention 输出的新词向量比输入的词向量有更多的句法特征和语义特征\nMulti-Head self-attention 输出的新词向量 比 self-attention 得到的词向量包含更多的\n头的个数用 h 表示，一般 h=8 。并不是直接用输入 X 做Attention 计算，而是先把 X 分成 8 块（分别位于8个子空间），分别计算得到 8 个新词向量，然后把它们拼起来，再做一次线性变换（改变维度与 X 匹配）得到最终词向量\n机器学习的本质：非线性变换 y = activation(wx+b)，把一个向量从一个位置变换到另一个位置上\n多头的作用：把一个向量拆分成8个子向量，8个向量同时优化，可以更快的收敛到合适的位置。分太多也不好。\n词向量一般 512 维，分 8 块；若是视频向量 5120，可以分 80 块\n参数量更大，8套参数(Wq,Wk,Wv)\n","date":"2022-07-29T17:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/13_%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B/","title":"watch: Transf - Nick 13 | Multi-head Self-Attention Mechanism"},{"content":"import torch 就是导入pytorch。Python中的torch就是pytorch，所以这里不是import pytorch，而是import torch 也合理。\ntorch.Tensor 包含单一数据类型元素的多维矩阵 有10种张量类型，torch.Tensor是默认张量类型torch.FloatTensor的别名 Note 张量变异方法都带有下划线后缀,它们直接原地修改原张量的属性，而不派生新张量。例如torch.FloatTensor.abs_()直接计算并修改原张量，而torch.FloatTensor.abs()在新张量中计算结果。\nTensors Operation on Tensors 张量有100多种运算，这些运算在GPU上运行比在CPU上快。可使用张量的.to方法转移到GPU上，对大张量的移动需要花费很多时间和内存。\n张量的索引和切片与Numpy很像。\ntensor[...,-1] 和 tensor[:,-1] 都表示取张量的最后一列\nCreation Ops torch.tensor(data) 是一个构造器construtor 复制data, 构造一个张量 当data是一个张量x时，这种方法等效于x.clone().detach()：创建新的leaf 张量，并不在当前计算图中 如果仅希望改变requires_grad标志，使用requires_grad_()和detach()方法来避免复制数据。如果data是ndarry，使用torch.as_tensor()创建张量，不复制数据。 torch.tensor.requires_grad_() 把该张量的属性requires_grad 置为True 有的张量是从DataLoader中来的，需要做一些预处理，再开始让autograd开始记录这个张量上的操作 在原地修改，不需创建新变量，没有复制 torch.tensor.detach() 创建一个新张量，与原张量指向同一块内存，但不允许修改二者的size/stride/storage，否则报错 新张量从当前计算图中分离，不需计算梯度 没有复制 torch.as_tensor() 把data转换为tensor 与 torch.tensor 不同，这种创建方式尽量避免复制数据(指向同一块内存) 如果data是ndarry（或tensor），并且它的dtype和device都与目标输出对应一致，那么就不会复制数据，而是新张量和data共同指向那块内存，改变张量，原data也会改变。 当data是list, tuple, scalar 或其他array_like的数据，或者dtype不一致，或者device不一样，都会复制数据创建新tensor。 torch.sparse COO tensors 一种存储形式: tensor is stored as 2 tensor: indices and values indices are coordinate in tensor Reduce memory consumption Strided tensor stores each elements, while COO tensor only record non-zero numbers. torch.as_strided() 创建一个窗口view，对底层连续的(一维)数据重新排列(\u0026ldquo;裁剪\u0026rdquo;) 输入一个张量，指定输出张量的size和每个维度跳跃的步长stride。 对于 1 2 x = torch.randn(3,5,5) #三维张量，一共125个数据 t = torch.as_strided(x, (3,3,3), (5,3,1)) #输出一个(3,3,3)的张量，最低维度的起点从第一个数开始，跳跃步长为1，倒数第2维度的每个起点,跳跃步长为3，最高维度的每个起点间隔步长为5。 根据size和stride可以判断内存上是否连续。对pytorch中Tensor的剖析 torch.from_numpy(ndarray) 新张量与ndarry共享同一块内存，但此张量不能修改size Reverse a dim, e.g. from bottom to top, or from right to left.\n1 xy = torch.flip(xy, [1]) # to match the code ","date":"2022-07-28T15:30:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_torch/","title":"memo: PyTorch | Tensor Ops"},{"content":"卷积就是特征提取器，就是CBAPD: convolution, batchnorm, activateion, pooling, dropout\ntf中定义卷积层，指定卷积核的个数，kernel核长\n1x1卷积核是为了减少计算量，减少特征图（通道）的个数\n","date":"2022-07-28T11:30:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/5_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"watch: TF2 - PKU 05 | CNN"},{"content":"统计语言模型：用概率估计下一个单词是哪个\nn 元语言模型：只分析 n 个单词\n神经网络语言模型：用词向量表示每个单词，词向量越紧凑越精炼越好（相对于one-hot编码）\n","date":"2022-07-26T22:41:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/4_%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","title":"watch: Transf - Nick 04 | 统计语言模型"},{"content":"4 网络八股扩展 4.1 自制数据集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 import tensorflow as tf from PIL import Image import numpy as np import os train_dir = \u0026#39;./mnist_image_label/mnist_train_jpg_60000/\u0026#39; train_txt = \u0026#39;./mnist_image_label/mnist_train_jpg_60000.txt\u0026#39; x_train_savepath = \u0026#39;./mnist_image_label/mnist_x_train.npy\u0026#39; y_train_savepath = \u0026#39;./mnist_image_label/mnist_y_train.npy\u0026#39; test_dir = \u0026#39;./mnist_image_label/mnist_test_jpg_10000/\u0026#39; test_txt = \u0026#39;./mnist_image_label/mnist_test_jpg.txt\u0026#39; x_test_savepath = \u0026#39;./mnist_image_label/mnist_x_test.npy\u0026#39; y_test_savepath = \u0026#39;./mnist_image_label/mnist_y_test.npy\u0026#39; def generateDS(dir, labels_path): f = open(labels_path, \u0026#39;r\u0026#39;)\t# 存有 文件名 及其 标签 contents = f.readlines()\t# 读取所有行 f.close()\t# 关闭 x, y_ = [], []\t# 每张图对应的灰度值数据和标签 for content in contents:\t# 逐行读出 value = content.split()\t# 以空格分开 img_path = dir + value[0]\t# 图片名 img = Image.open(img_path) img = np.array(img.convert(\u0026#39;L\u0026#39;)) # 8位灰度图像 img = img/255. x.append(img)\t# 灰度值np.array 放入列表 y_.append(value[1])\t# 标签 放入列表 print(\u0026#39;loading: \u0026#39;+content) x = np.array(x)\t# 列表变 np.array y_ = np.array(y_) y_ = y_.astype(np.int64)\t# 标签是64位整型 return x, y_ if os.path.exists(x_train_savepath) and os.path.exists(y_train_savepath) and os.path.exists(x_test_savepath) and os.path.exists(y_test_savepath): print(\u0026#39;---Loading dataset---\u0026#39;) x_train_save = np.load(x_train_savepath)\t# 读 y_train = np.load(y_train_savepath) x_test_save = np.load(x_test_savepath) y_test = np.load(y_test_savepath) x_train = np.reshape(x_train_save, (len(x_train_save),28,28))\t# 变形 x_test = np.reshape(x_test_save, (len(x_test_save),28,28)) else:\t#不存在，要制作数据集 print(\u0026#39;---Generating dataset---\u0026#39;) x_train, y_train = generate(train_dir, train_txt) x_test, y_test = generate(test_dir, test_txt) print(\u0026#39;---Saving dataset\u0026#39;)\t# 保存为以后使用 x_train_save = np.reshape(x_train, (len(x_train), -1)) x_test_save = np.reshape(x_test, (len(x_test), -1)) np.save(x_train_savepath, x_train_save) np.save(y_train_savepath, y_train) np.save(x_test_savepath, x_test_save) np.save(y_test_savepath, y_test) 4.2 数据增强 用于扩展数据集，对图像的增强就是对图像的简单形变，用来应对因拍照角度不同引起的图片变形\n1 2 3 4 5 6 7 8 9 image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator( rescale = 所有数据将乘以该数值， rotation_range = 随机旋转角度数范围， width_shift_range = 随机宽度偏移量， height_shift_range = 随机高度偏移量， horizontal_flip = 是否随机水平翻转， zoom_range = 随机缩放的范围[1-n, 1+n]) image_gen_train.fit(x_train) 其中 x_train 需要是四维，需要变形：x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)，把 (60000, 28, 28) -\u0026gt; (60000, 28, 28, 1)\n例如：\n1 2 3 4 5 6 7 8 9 image_gen_train = ImageDataGenerator( rescale = 1. / 1., # 若为图像，分母为255时，可归至0~1 rotation_range = 45, # 随机45度旋转 width_shift_range=.15, # 宽度偏移 height_shift_range=.15, # 高度偏移 horizontal_flip=False, # 水平翻转 zoom_range = 0.5 # 将图像随机缩放阈量50% ) image_gen_train.fit(x_train) 模型训练也要改，把 model.fit(x_train, y_train, batch_size=32, ...) 改为 model.fit(image_gen_train.flow(x_train, y_train, batch_size=32), ...)\n4.3 断点续训, 存取模型 读取模型：load_weights(文件路径)\n1 2 3 4 checkpoint_save_path = \u0026#39;./checkpoint/mnist.ckpt\u0026#39; # 定义文件路径 if os.path.exists(checkpoint_save_path + \u0026#39;.index\u0026#39;): # 存在索引表说明已经保存过参数了 print(\u0026#39;---- Loading the model ----\u0026#39;) model.load_weights(checkpoint_save_path) 保存模型：\n1 2 3 4 5 6 7 8 9 cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_save_path, # 保存路径 save_weights_only = True, # 是否只保留 weights save_best_only = True, # 是否只保留最优结果 # 在训练过程中保存，记录到history中 history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) 4.4 提取可训练参数 返回模型中可训练的参数 model.trainable_variables\n设置print输出格式：np.set_printoptions(threshold=超过多少省略显示) np.inf 表示无限大\n1 2 3 4 5 6 7 8 9 10 print(model.trainable_variables) # 把参数直接打印出来 file = open(\u0026#39;./weights.txt\u0026#39;, \u0026#39;w\u0026#39;) # 把参数存入文件 for v in model.trainable_variables: file.write(str(v.name) + \u0026#39;\\n\u0026#39;) file.write(str(v.shape) + \u0026#39;\\n\u0026#39;) file.write(str(v.numpy()) + \u0026#39;\\n\u0026#39;) file.close() 4.5 acc/locc 可视化 用于查看训练效果。history中记录了训练集的loss和sparse_categorical_accuracy；测试集上的val_loss 和 val_sparse_categorical_accuracy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 acc = history.history[\u0026#39;sparse_categorical_accuracy\u0026#39;] val_acc = history.history[\u0026#39;val_sparse_categorical_accuracy\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] plt.subplot(1,2,1) plt.plot(acc, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(val_acc, label=\u0026#39;Validation Accuracy\u0026#39;) plt.title(\u0026#39;Training and Validation Accuracy\u0026#39;) plt.legend() plt.subplot(1,2,2) plt.plot(loss, label=\u0026#39;Training Loss\u0026#39;) plt.plot(val_loss, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training and Validation Loss\u0026#39;) plt.legend() plt.show() 4.6 调用模型 返回前向传播计算结果：predict(输入特征，batch_size=整数)\n三步：复现模型，加载参数，预测\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from PIL import Image import numpy as np import tensorflow as tf model_save_path = \u0026#39;./checkpoint/mnist.ckpt\u0026#39; model = tf.keras.models.Sequential([ # 复现网络 tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)]) model.load_weights(model_save_path) # 加载网络 preNum = int(input(\u0026#34;Input the number of test pictures: \u0026#34;)) # 接收用户输入 for i in range(preNum): image_path = input(\u0026#34;the path of test picture: \u0026#34;) # 输入 1.png img = Image.open(image_path) # 读取图片 # 预处理输入图片的格式 与 训练数据一致 img = Img.resize((28, 28), Image.ANTIALIAS) # 变为 28x28 与训练图片尺寸相同 img_arr = np.array(img.convert(\u0026#39;L\u0026#39;)) # 变为灰度图 img_arr = 255 - img_arr # 黑白反转 # 预处理或者变为只有黑白像素的高对比度图片, 2种方法二选一 for i in range(28): for j in range(28): if img_arr[i][j] \u0026lt; 200: # 小于200 全变白. 可滤去背景噪声，当阈值选择合适识别效果更好 img_arr[i][j] = 255 else: img_arr[i][j] = 0 img_arr = img_arr / 255.0 # 归一化 x_predict = img_arr[tf.newaxis, ...] # (28, 28) -\u0026gt; (1, 28, 28) result = model.predict(x_predict) # 输入网络 pred = tf.argmax(result, axis=1) # 返回最大概率值的索引 print(\u0026#39;\\n\u0026#39;) tf.print(pred) ","date":"2022-07-24T16:41:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/4_%E7%BD%91%E7%BB%9C%E5%85%AB%E8%82%A1%E6%89%A9%E5%B1%95/","title":"watch: TF2 - PKU 04 | NN Framework Extension"},{"content":"3 keras搭建神经网络 3.1 神经网络搭建八股 import train data, test data model = tf.keras.models.Sequential model.compile 设置优化器，损失函数，评价指标 model.fit 设置训练过程，输入输出 epoch，batch model.summary 打印网络结构和参数统计 model = tf.keras.models.Sequential([网络结构]) # 描述各层网络\n网络结构举例：\n拉直层：tf.keras.layers.Flatten() 把输入特征变成一维数组\n全连接层：tf.keras.layers.Dense(神经元个数，activation=\u0026quot;激活函数\u0026quot;，kernel_regularizer=哪种正则化) 。激活函数可选：\u0026lsquo;relu\u0026rsquo;, \u0026lsquo;softmax\u0026rsquo;, \u0026lsquo;sigmoid\u0026rsquo;, \u0026rsquo;tanh\u0026rsquo;；kernel_regularizer可选：tf.keras.regularizers.l1()、tf.keras.regularizers.l2()\n卷积层：tf.keras.layers.Conv2D(filters=卷积核个数，kernel_size=卷积核尺寸，strides=卷积步长，padding=\u0026quot;valid\u0026quot; or \u0026quot;same\u0026quot;)\nLSTM层：tf.keras.layers.LSTM()\nmodel.compile(optimizer=优化器，loss=损失函数，metrics=[\u0026quot;准确率\u0026quot;])\noptimizer可选：\n\u0026lsquo;sgd\u0026rsquo; 或 tf.keras.optimizers.SGD(lr=学习率，momentum=动量参数) \u0026lsquo;adagrad\u0026rsquo; 或 tf.keras.optimizers.Adagrad(lr=学习率) \u0026lsquo;adadelta\u0026rsquo; 或 tf.keras.optimizers.Adadelta(lr=学习率) \u0026lsquo;adam\u0026rsquo; 或 tf.keras.optimizers.Adam(lr=学习率，beta_1=0.9, beta_2=0.999) loss 可选：\n\u0026lsquo;mse\u0026rsquo; or tf.keras.losses.MeanSquaredError() \u0026lsquo;sparse_categorical_crossentropy\u0026rsquo; or tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) , from_logits 为True表示没经过softmax，是网络的直接结果 Metrics 可选：\n\u0026lsquo;accuracy\u0026rsquo;：y_ 和 y 都是数值，类别是数字，如 y_=[1], y=[1] \u0026lsquo;categorical_accuracy\u0026rsquo;，target是独热码，pred是概率分布，如 y_=[0,1,0], y=[0.256, 0.695, 0.048] \u0026lsquo;sparse_categorical_accuracy\u0026rsquo;: target是数值，pred是独热码 (概率分布)，如 y_ = [1], y=[0.256, 0.695, 0.048] model.fit(训练集的输入特征，训练集的标签，batch_size= , epoches= , validation_data=(测试集的输入特征，测试集的标签)【或用 validation_split=从训练集划分多少比例给测试集】，validation_freq=多少次epoch测试一次)\n3.2 Iris 代码用keras重写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf from sklearn import datasets import numpy as np x_train = datasets.load_iris().data y_train = datasets.load_iris().target np.random.seed(116) np.random.shuffle(x_train) np.random.seed(116) np.random.shuffle(y_train) tf.random.set_seed(116) # 设计模型 model = tf.keras.models.Sequential([ tf.keras.layers.Dense(3, activation=\u0026#39;softmax\u0026#39;, kernel_regularize=tf.keras.regularizers.l2()) ]) # 配置训练方法 model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.1), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) # 训练过程 model.fit(x_train, y_train, batch_size=32, epoches=500, validataion_split=0.2, validation_freq=20) # 训练集的20%做测试集 model.summary() Sequential 搭建上层输出就是下层输入的顺序网络结构，但无法写出带有跳连接的非顺序网络结构，\n继承Model类 自定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from tensorflow.keras.layers import Dense from tensorflow.keras import Model class MyModel(Model): def __init__(self): super(MyModel, self).__init__() #定义网络结构块 self.d1 = Dense(3, activation=\u0026#39;sigmoid\u0026#39;) def call(self, x): #调用网络结构块，实现前向传播 y = self.d1(x) return y model = MyModel() MNIST 数据集 总共7万张图片，6万的训练集，1万作为测试集\n使用Sequential\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import tensorflow as tf from matplotlib import pyplot as plt # 读取MNIST数据集 mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train/255.0, x_test/255.0\t# 输入数值变小更适合神经网络 model = tf.keras.models.Sequential([ # 将输入样本拉直为一维向量，784个像素点的灰度值 tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) model.fit(x_train, y_train, batch_size=32, epochs=5, validation=(x_test, y_test), validation_freq=1) model.summary() 使用 Model类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten from tensorflow.keras import Model mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class MyModel(Model): def __init__(self): super(MyModel, self).__init__() self.flatten = Flatten() self.d1 = Dense(128, activation=\u0026#39;relu\u0026#39;) self.d2 = Dense(10, activation=\u0026#39;softmax\u0026#39;) def call(self, x): x = self.flatten(x) x = self.d1(x) y = self.d2(x) return y model = MyModel() model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\u0026#39;sparse_categorical_accuracy\u0026#39;]) model.fit(x_train, y_train, batch_size=32, epochs=5, validation=(x_test, y_test), validation_freq=1) model.summary() FASHION 数据集\n1 2 fashion = tf.keras.datasets.fashion_mnist (x_train, y_train), (x_test,y_test) = fashion.load_data() ","date":"2022-07-24T14:01:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/3_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%85%AB%E8%82%A1/","title":"watch: TF2 - PKU 03 | NN Building Steps"},{"content":"2 神经网络优化 2.1 常用函数 在每个元素上执行条件语句，为真则返回A，为假则返回B：tf.where(条件语句，真返回A，假返回B)\n1 2 3 4 a = tf.constant([1, 2, 3, 1, 1]) b = tf.constant([0, 1, 3, 4, 5]) c = tf.where(tf.greater(a,b), a, b)\t# 若a\u0026gt;b, 返回a对应位置的元素，否则返回b对应位置的元素 print(c)\t# tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32) 返回一个 [0,1) 之间的随机数：np.random.RandomState.rand(维度) 维度为空时返回标量\n1 2 3 4 5 rdm = np.random.RandomState(seed=1)\t# 设置随机种子每次生成的随机数相同 a = rdm.rand() b = rdm.rand(2,3) print(a) # 0.4170220047 print(b) # [[7.20324493e-01 1.14374817e-04 3.02332573e-01] [1.46755891e-01 9.23385948e-02 1.86260211e-01]] 将两个数组按垂直方向叠加 np.vstack(数组1，数组2)\n1 2 3 4 a = np.array([1, 2, 3])\t# shape=(3,) b = np.array([4, 5, 6]) c = np.vstack((a,b)) print(c)\t# [[1 2 3] [4 5 6]] shape=(2,3) 生成网格坐标\nnp.mgrid[起始值：结束值：步长，起始值：结束值：步长，...] 生成若干维度的等差数组，不包括结束值\nx.ravel() 将多维数组x变为一维数组，“把变量拉直”\nnp.c_[数组1，数组2，...] 数组对应位置元素配对\n1 2 3 4 5 6 7 import numpy as np x, y = np.mgrid[1:3:1, 2:4:0.5] # x坐标+1递增，y坐标+0.5递增 grid = np.c_[x.ravel(), y.ravel()] # 两个(8,) array配对 print(x) # [[1. 1. 1. 1.] [2. 2. 2. 2.]] shape=(2,4) print(y) # [[2. 2.5 3. 3.5] [2. 2.5 3. 3.5]] print(grid) # [[1. 2.] [1. 2.5] [1. 3.] [1. 3.5] [2. 2.] [2. 2.5] [2. 3.] [2. 3.5]] shape=(8,2) 神经网络复杂度\n多用网络层数和网络参数的个数表示:\n空间复杂度：层数=隐藏层的层数+1个输出层；总参数=总w+总b\n时间复杂度：乘加运算次数（多少次wx+b）\n2.2 指数衰减学习率 先用较大的学习率，快速得到较优解，然后逐步减小学习率，使模型在训练后期稳定\n指数衰减学习率 = 初始学习率 x 学习率衰减率 ^ (当前epoch(或batch) / 多少epoch(或batch)衰减一次)\n1 2 3 4 5 6 7 8 9 10 11 12 13 epoch = 40 LR_BASE = 0.2 LR_DECAY = 0.99 LR_STEP = 1 for epoch in range(epoch): lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP) with tf.GradientTape() as tape: loss = tf.square(w+1) grads = tape.gradient(loss, w) w.assign_sub(lr * grads) print(f\u0026#34;After {epoch} epoch, w is {w.numpy()}, loss is {loss}, lr is {lr}\u0026#34;) 2.3 激活函数 sigmoid 函数 作用：把无限变有限，把无穷归一，引入非线性，全域可导（导数在0-0.25之间）；缺点：应用链式法则容易造成梯度消失；输入特征最好使均值为0的小数，但经过sigmoid后变为正数，导致收敛慢；其中有幂运算，计算时间长\ntf.math.tanh(x) 激活后的输出的均值是0，导数在0-1之间，仍易造成梯度消失，两次幂运算，计算时间 更长\ntf.nn.relu(x) f(x)=max(x,0) 优点：导数不是0就是1，在正区间内解决了梯度消失问题；只需判断输入是否大于0，计算速度快；收敛速度远快于sigmoid 和 tanh。缺点：输出不是以0为均值，收敛慢；Dead Relu问题，当输入特征是负数时，激活函数输出为0，反向传播时，梯度为0，导致相应的参数无法更新。可以改进随机初始化，避免过多的负数特征输入Relu函数；可以通过减小学习率，减小参数分布的巨大变化，避免训练中产生过多的负数特征\ntf.nn.leaky_relu(x) f(x) = max(ax, x) 是为了解决Relu在负区间的导数为0 引起神经元死亡问题而设计的，它在负区间引入了固定的斜率a。理论上来讲，Leaky Relu 有 Relu 的所有优点，外加不会有 Dead Relu问题，但是在实际操作中，并没有完全证明 Leaky Relu 总好于Relu\n对于初学者的建议：\n首选relu激活函数 学习率设置较小值 输入特征标准化，即让输入特征满足以0为均值，以1为标准差的正态分布 初始参数中心化，即让随机生成的参数满足以0为均值，根号下当前层输入特征个数分之2 为标准差的正态分布。 2.4 损失函数 预测值y 与 已知答案 y_ 的差距\nNN优化目标：loss最小，主流三种计算方法：mse，交叉熵，自定义\nloss_mse = tf.reduce_mean(tf.square(y_ - y))\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import tensorflow as tf import numpy as np SEED = 23455 rdm = np.random.RandomState(seed=SEED) x = rdm.rand(32, 2)\t# 真实y = x1+x2 y_ = [[x1+x2 + (rdm.rand()/10.0 - 0.05)] for (x1, x2) in x] # 加上噪声[0,1)/10 = [0,0.1); [-0.05, 0.05) x = tf.cast(x, dtype=tf.float32) w1 = tf.Variable(tf.random.normal([2,1], stddev=1, seed=1)) # 初始化网络参数 2x1 epoch = 15000 lr = 0.002 for epoch in range(epoch): with tf.GradientTape() as tape: y = tf.matmul(x, w1)\t# 前向 loss_mse = tf.reduce_mean(tf.square(y_ - y)) grads = tape.gradient(loss_mse, w1)\t# 损失函数对参数求偏导 w1.assign_sub(lr * grads)\t# 更新参数 if epoch % 500 == 0:\t# 每迭代500轮，打印参数 print(f\u0026#34;After {epoch} training setps, w1 is {w1.numpy()}\u0026#34;) print(f\u0026#34;final w1 is {w1.numpy()}\u0026#34;) # [[1.0009] [0.9977]] 。。。。\n2.6 优化器 参数更新：下一时刻的参数 等于 当前时刻的参数 减去 η （梯度下降）。η等于学习率乘上（一阶动量除以根号下二阶动量）。\nSGD中无动量，一阶动量等于loss对参数的偏导数，二阶动量=1，所以 η 是沿梯度方向改变的步长。\n单层网络 w*x+b 应用 SGD：\n1 2 w1.assign_sub(lr * grads[0]) b1.assign_sub(lr * grads[1]) SGDM 在SGD基础上增加了一阶动量，一阶动量等于上一时刻的一阶动量与当前时刻的梯度共同作用：mₜ = β⋅mₜ₋₁ + (1-β)⋅gₜ ， β是接近1的系数（经验值0.9），所以上一时刻的动量占主导；二阶动量=1。 一阶动量是“指数滑动平均值”？过去一段时间的平均值？\n1 2 3 4 5 6 7 8 m_w, m_b = 0, 0 beta = 0.9 # sgd-momentum m_w = beta * m_w + (1-beta) * grads[0]\t# grads是loss对各参数的偏导数 m_b = beta * m_b + (1-beta) * grads[1] w1.assign_sub(lr * m_w) b1.assign_sub(lr * m_b) Adagrad 在SGD 基础上增加了二阶动量，可以对模型中的每个参数分配自适应学习率了。一阶动量与SGD设置相同，等于loss的梯度，二阶动量是从开始时刻到现在，梯度平方的累计和 $V_t = \\sum_{τ=1}^t g_τ^2$\n1 2 3 4 5 6 7 v_w, v_b = 0, 0\t# 二阶动量零时刻初值为0 # 在 for 循环里，每batch更新一次： v_w += tf.square(grads[0])\t# 梯度平方的累计和 v_b += tf.square(grads[1]) w1.assign_sub(lr * grads[0] / tf.sqrt(v_w)) b1.assign_sub(lr * grads[1] / tf.sqrt(v_b)) RMSProp 在SGD基础上增加了二阶动量，一阶动量仍等于当前时刻loss的梯度，二阶动量 $V_t = β⋅V_{t-1} + (1-β)⋅g_t^2$\n1 2 3 4 5 6 7 8 v_w, v_b = 0, 0\t# 二阶动量零时刻初值为0 beta = 0.9 # 在 for 循环里，每batch更新一次： v_w += beta*v_w + (1-beta) * tf.square(grads[0])\t# 梯度平方 与 上一时刻的二阶梯度的加权和 v_b += beta*v_b + (1-beta) * tf.square(grads[1]) w1.assign_sub(lr * grads[0] / tf.sqrt(v_w)) b1.assign_sub(lr * grads[1] / tf.sqrt(v_b)) Adam 同时结合了SGDM的一阶动量和RMSProp的二阶动量，并分别增加修正系数：mₜ = β₁⋅mₜ₋₁ + (1-β₁)⋅gₜ ，$\\hat{m_t} = \\frac{m_t}{1-β_1^t}$ ，Vₜ = β₂⋅Vₜ₋₁ + (1-β₂)⋅gₜ²，$\\hat{V_t}=\\frac{V_t}{1-β_2^t}$ , t 是从开始到现在经历的所有batch数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 m_w, m_b = 0, 0 v_w, v_b = 0, 0 beta1, beta2 = 0.9, 0.999 delta_w, delta_b = 0, 0 global_step = 0 m_w = beta1 * m_w + (1 - beta1) * grads[0]\t# 一阶动量=上一时刻与此刻梯度加权和 m_b = beta1 * m_b + (1 - beta1) * grads[1] v_w = beta2 * v_w + (1-beta2) * tf.square(grads[0])\t# 二阶动量=上一时刻与此刻梯度平方的加权和 v_b = beta2 * v_b + (1-beta2) * tf.square(grads[1]) # 修正 m_w_correction = m_w / (1-tf.pow(beta1, int(global_step))) m_b_correction = m_b / (1-tf.pow(beta1, int(global_step))) v_w_correction = v_w / (1-tf.pow(beta2, int(global_step))) v_b_correction = v_b / (1-tf.pow(beta2, int(global_step))) w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction)) b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction)) 不同优化器训练速度不同\n","date":"2022-07-24T13:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/2_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/","title":"watch: TF2 - PKU 02 | NN Optimization Methods"},{"content":"创建张量 tf.constant(张量内容, dtype=数据类型(可选))\n1 2 3 4 5 import tensorflow as tf a = tf.constant([1,5], dtype=tf.int64) # 创建一阶张量, 2个元素 print(a) # tf.Tensor([1,5], shape=(2,), dtype=int64). tf1.x不会显示元素 print(a.dtype) # \u0026lt;dtype: \u0026#39;int64\u0026#39;\u0026gt; print(a.shape) # (2,) 将numpy的数据类型转换为 Tensor 数据类型:\ntf.convert_to_tensor(数据名，dtype=数据类型（可选）)\n1 2 3 4 5 6 import tensorflow as tf import numpy as np a = np.arange(0, 5) b = tf.convert_to_tensor(a, dtype=tf.int64) print(a) # [0 1 2 3 4] print(b) # tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64) 生成全0，全1，全指定值的张量：\n1 2 3 4 5 tf.zeros([2,3]) #tf.Tensor([[0. 0. 0.] [0. 0. 0 .]],shape=(2,3), dtype=float32) tf.ones(4) #tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32) tf.fill([2,2], 9) # tf.Tensor([[9 9] [9 9]], shape=(2,2),dtype=int32) 生成正态分布的随机数（默认均值为0，标准差为1），常用于初始化参数:\ntf.random.normal(维度，means=均值，stddev=标准差)\n生成截断式正态分布的随机数，分布更集中在均值附近，随机数取值在正负2个标准差之内 (mu-2sigma, mu+2sigma)，如果落在外面则重新生成：\ntf.random.truncated_normal(维度， mean=均值，stddev=标准差)\n1 2 d = tf.random.normal([2,2], mean=0.5, stddev=1) e = tf.random.truncated_normal([2,2], mean=0.5, stddev=1) 生成均匀分布随机数 [min, max) 左闭右开：\ntf.random.uniform(维度，minval=最小值，maxval=最大值)\n常用函数 强制 tensor 转换为指定类型 tf.cast(张量名，dtype=数据类型) 计算张量维度上元素的最小值 tf.reduce_min(张量名) 找到张量中的最大元素：tf.reduce_max(张量名) 1 2 3 4 5 6 7 x1 = tf.constant([1., 2., 3.], dtype=tf.float64) print(x1) # tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64) x2 = tf.cast(x1, tf.int32) print(x2) # tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64) print(tf.reduce_min(x2), tf.reduce_max(x2)) # tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(1, shape=(), dtype=int32) axis指定操作方向，对于二维张量，axis=0表示对第0维操作；若不指定axis，则所有元素参与计算 1 2 3 4 5 x = tf.constant([[1,2,3], [2,2,3]]) print(tf.reduce_mean(x)) # 所有元素的平均值 tf.Tensor(2, shape=(), dtype=int32) print(tf.reduce_mean(x, axis=0)) # tf.Tensor([1 2 3], shape=(3,) dtype=int32) print(tf.reduce_sum(x, axis=1)) # 对第1维求和 tf.Tensor([6 7], shape=(2,) dtype=int32) tf.Variable() 将变量标记为“可训练”，被标记的变量会在反向传播中记录梯度信息。在神经网络训练中，常用该函数标记待训练参数。 w = tf.Variable(tf.random.normal([2,2], mean=0, stddev=1)), 把生成的随机数标记为可训练\n常用数学运算：tf.add, tf.subtract, tf.multiply, tf.divide, tf.square, tf.pow, tf.sqrt, tf.matmul。 1 2 3 4 tf.add(张量1，张量2) tf.subtract(张量1，张量2) tf.multiply(张量1，张量2) tf.divide(张量1，张量2) 只有维度相同的张量才可以做四则运算\n1 2 3 4 5 6 a = tf.ones([1,3]) b = tf.fill([1,3], 3.] print(tf.add(a,b)) print(tf.subtract(a,b)) # tf.Tensor([[-2. -2. -2.]], shape=(1,3), dtype=float32) print(tf.multiply(a,b)) print(tf.divide(b,a)) # tf.Tensor([[3. 3. 3.]], shape=(1,3), dtype=float32) 两矩阵相乘：\n1 2 3 a = tf.ones([3,2]) b = tf.fill([2,3], 3.) print(tf.matmul(a,b)) # tf.Tensor([[6. 6. 6.] [6. 6. 6.] [6. 6. 6.], shape=(3,3), dtype=float32) 把特征和标签配对 tf.data.Dataset.from_tensor_slices((输入特征，标签))，Numpy和Tensor格式都适用 1 2 3 4 5 6 features = tf.constant([12, 23, 10, 17]) # 一个数是一个样本 labels = tf.constant([0, 1, 1, 0]) dataset = tf.data.Dataset.from_tensor_slices((features, labels)) print(dataset) for element in dataset: print(element) 运行结果：\n1 2 3 4 5 \u0026lt;TensorSliceDataset shapes: ((),()), types: (tf.int32, tf.int32)\u0026gt; （特征，标签）对 (\u0026lt;tf.Tensor: id=9, shape=(), dtype=int32, numpy=12\u0026gt;, \u0026lt;tf.Tensor: id=10, shape=(), dtype=int32, numpy=0\u0026gt;) (\u0026lt;tf.Tensor: id=11, shape=(), dtype=int32, numpy=23\u0026gt;, \u0026lt;tf.Tensor: id=12, shape=(), dtype=int32, numpy=1\u0026gt;) (\u0026lt;tf.Tensor: id=13, shape=(), dtype=int32, numpy=10\u0026gt;, \u0026lt;tf.Tensor: id=14, shape=(), dtype=int32, numpy=1\u0026gt;) (\u0026lt;tf.Tensor: id=15, shape=(), dtype=int32, numpy=17\u0026gt;, \u0026lt;tf.Tensor: id=16, shape=(), dtype=int32, numpy=0\u0026gt;) 实现某函数对指定参数的求导运算，用 with 结构记录计算过程：\n1 2 3 with tf.GradientTape() as tape: 若干个计算过程 grad = tape.gradient(函数，对谁求导) 例如：\n1 2 3 4 5 with tf.GradientTape() as tape: w = tf.Variable(tf.constant(3.0))\t# 初值为3，可以求导 loss = tf.pow(w, 2) grad = tape.gradient(loss, w)\t# loss 对 w 求导，2w=6 print(grad)\t# tf.Tensor(6.0, shape=(), dtype=float32) 在遍历时返回索引号 enumerate(iterable)\n1 2 3 seq = [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;] for count, value in enumerate(seq): print(count, value) 分类问题中，用独热码表示标签，tf.one_hot(待转换数据, depth=几分类) 将标签列表转换为 one-hot 形式的数据\n1 2 3 4 classes = 3 labels = tf.constant([1,0,2])\t# 输入的元素值最小为0，最大为2 output = tf.one_hot(labels, depth=classes) print(output)\t# [[0. 1. 0.] [1. 0. 0.] [0. 0. 1.]],shape=(3,3), dtype=float32) 先做大小排序？ 使网络输出符合概率分布 tf.nn.softmax(x)\n1 2 3 y = tf.constant([1.01, 2.01, -0.66]) y_prob = tf.nn.softmax(y) print(y_prob)\t# tf.Tensor([0.25598174 0.69583046 0.0481878], shape=(3,), dtype=float32) 参数自更新 (自减) assign_sub()，参数要用tf.Variable定义为”可训练“\n1 2 3 w = tf.Variable(4) w.assign_sub(1)\t# w -=1, 即 w=w-1 print(w)\t# \u0026lt;tf.Variable \u0026#39;Variable:0\u0026#39; shape=() dtype=int32,numpy=3\u0026gt; 返回指定维度的最大值的索引 tf.argmax(张量名，axis=操作轴)\n1 2 3 test = np.array([[1, 2, 3] [2,3,4] [5,4,3] [8,7,2]])\t# shape=(4,3) print(tf.argmax(test, axis=0))\t# tf.Tensor([3 3 1], shape=(3,), dtype=int64) print(tf.argmax(test, axis=1))\t# tf.Tensor([2 2 0 0 ],shape=(4,),dtype=int64) 鸢尾花分类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # 1. Prepare data # read from sklearn.datasets import datasets x_data = datasets.load_iris().data\t# features y_data = datasets.load_iris().target\t# labels # mess up np.random.seed(116) np.random.shuffle(x_data) np.random.seed(116)\t# 使用相同的seed，特征与标签保持对应 np.random.shuffle(y_data) tf.random.set_seed(116) # separate 永不相见的训练集和测试集 x_train = x_data[:-30]\t# 前120 y_train = y_data[:-30] x_test = x_data[-30:] y_test = y_data[-30:] # pair， 每次输入一个batch train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) # 定义网络中所有可训练参数 w1 = tf.Variable(tf.random.truncated_normal([4,3], stddev=0.1, seed=1)) # 一层网络指的使那个密密麻麻交织的网络啊，重点不在节点，而在网！ b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1)) lr=0.1 train_loss_results = []\t# 存储各epoch的loss test_acc = []\t# 存储各epoch后在测试集上的准确率 epoch = 500 loss_all = 0\t# 各batch的loss求和 # 嵌套循环迭代，with结构更新参数，显示当前loss for epoch in range(epoch):\t# 数据集级别迭代 for step, (x_train, y_train) in enumerate(train_db): # batch 级别迭代 with tf.GradientTape() as tape:\t# 记录梯度信息 y = tf.matmul(x_train, w1) + b1\t# 线性 y = tf.nn.softmax(y)\t# 变成概率 y_ = tf.one_hot(y_train, depth=3)\t# 标签变为独热码，方便分类 loss = tf.reduce_mean(tf.square(y_ - y))\t# 均方误差损失 loss_all += loss.numpy()\t# 各batch的loss累加 grads = tape.gradients(loss, [w1, b1])\t# 1个batch的loss 对 w1,b1 求偏导 w1.assign_sub(lr*grads[0])\t# 参数自更新 b1.assign_sub(lr*grads[1]) print(\u0026#34;Epoch {}, loss: {}\u0026#34;.format(epoch, loss_all/4))\t# 4个batch的loss平均一下 train_loss_results.append(loss_all / 4) loss_all = 0 # 每个epoch之后，在test集上的表现 total_correct, total_number = 0,0 for x_test, y_test in test_db: y = tf.matmul(x_test,w1) + b1\t# y y = tf.nn.softmax(y) pred = tf.argmax(y, axis=1)\t# 提取类别 pred = tf.cast(pred, dtype=y_test.dtype)\t# 转换到与y_test相同的数据类型 correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32) # 把bool结果转换为int correct = tf.reduce_sum(correct)\t# 各batch的正确数加起来 total_correct += int(correct)\t# 当前正确率 total_number += x_test.shape[0]\t# 当前已测试过的样本数目 acc = total_correct / total_number\t# 一个epoch后，在测试集上的准确率 test_acc.append(acc) print(\u0026#34;Test_acc:\u0026#34;, acc) print(\u0026#34;----------------\u0026#34;) # 绘制loss曲线 plt.title(\u0026#39;Loss Function Curve\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.plot(train_loss_results, label=\u0026#34;$Loss$\u0026#34;) plt.legend() plt.show() plt.title(\u0026#34;ACC Curve\u0026#34;) plt.xlabel(\u0026#34;Epoch\u0026#34;) plt.ylabel(\u0026#39;Acc\u0026#39;) plt.plot(test_acc, label=\u0026#34;$Accuracy$\u0026#34;) plt.legend() plt.show() ","date":"2022-07-24T12:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/tf2-%E5%8C%97%E5%A4%A7/1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B/","title":"watch: TF2 - PKU 01 | NN Computation Process"},{"content":"F.grid_sample F.grid_sample(tensor,p,mode) 把 tensor 插补成与网格 p 有相同大小的 tensor。 网格p指定了在 input 上的采样点坐标，在采样点附近插值形成一个新像素，各 channel 上采样点相同。 grid 中的 (x,y) 已缩放到 [-1,1]，在函数内变换到 [0,W],[0,H] 索引像素 PyTorch中grid_sample的使用方法-csdn\nmode = \u0026lsquo;bilinear\u0026rsquo; 是三线性插值: 双线性插值（bilinear）的3D形式(卿卿小徐的评论); c++源码\n4D input 对应一个 batch 的图片 (B,C,H,W)。\nalign_corners = True 认为input以像素为单位，各像素由其中心点代表，从而做双线性插值的时候，就用像素的四个角点； (2023-10-08: 角点？邻点？)\nFalse 是把 input 当作各像素角点的集合，此时 input 的边界 (W-1,H-1) 小于原来的图片边界 (W,H)，则grid采样点的坐标可能超出 input边界（比如采样点落在图片边缘，而双线性插值需要用周围 4 个点），所以需要在input外围padding，再与各neighbor点做插值。\ntorch.nn.functional.grid_sample() 函数实例-csdn\nAs shown below, black points are input datapoints. In the left figure, the datapoints fit the pixels corners of the input image to be scaled, whereas in the right figure, the datapoints forms an independent image to perform interpolation.\na = l i T g r n u _ e c o r n e r s a = l i F g a n l _ s c e o r n e r s A numerical example of difference: Docs - nn.Upsample\n(2024-03-09) In the left figure, the corners of the grid formed by pixels and sampling points are aligned. Whereas, the right figure isn\u0026rsquo;t.\n像素中心点的位置与一个像素的大小有关，所以对于相同维度的输入，采样点坐标可能不同。 所以使用像素角点做为基准 (align_corners = False)，与像素尺寸无关，采样点位置是相对的。 Docs\n线性插值是一维长度的加权平均，双线性插值是二维面积（两个方向）的加权平均：每个顶点的权重是其对角位置上的矩阵面积占比；三线性插值是三维体积的加权和。\n【三维重建和NeRF常用的三线性插值，原理讲解+代码实现】-意の茗\n双线性插值：在x方向和y方向上做线性回归并预测在目标点上的数值。所插值是周围4个neighbor的加权和，权重是两方向上neighbor到目标点距离与两点间距之比的乘积\ngrid_sample()函数及双线性采样 - 180天后再改名的文章 - 知乎\n(2023-12-18) F.grid_sample() cannot sample images?\n1 2 3 import cv2 src = cv2.imread(\u0026#34;dtu/Rectified/scan11_train/rect_011_3_r5000.png\u0026#34;) # ndarray, (h,w,3) F.grid_sample(torch.from_numpy(src).unsqueeze(0), uv_src.view(1, 192*h, w, 2)) And it will lead to: RuntimeError: grid_sampler_2d_cpu not implemented for Byte\nTherefore, the image requires to be normalized to [0,1]:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import PIL import torch.nn.functional as F from torchvision import transforms from matplotlib import pyplot as plt h, w = 128, 160 y,x = torch.meshgrid(torch.arange(h), torch.arange(w)) normalized_y, normalized_x = y/h, x/w normalized_xy = torch.stack([normalized_x, normalized_y], dim=-1) src = PIL.Image.open(\u0026#34;dtu/Rectified/scan11_train/rect_009_6_r5000.png\u0026#34;) src = transforms.ToTensor()(src) samp= F.grid_sample(src.unsqueeze(0), normalized_xy.unsqueeze(0)) # revert to an image samp_scaled = torch.tensor(samp[0] * 255, dtype=int).permute(1,2,0) plt.imshow(samp_scaled) Ex in GNT train_imgs is a 4D tesnor.\nfeatmaps is a 5D tensor (N, Chanl, D, H, W,), the first dimension also vary determined by the indexing tensor\n1 2 3 4 5 6 7 8 9 10 11 12 # compute the projection of the query points to each reference image pixel_locations, mask_in_front = self.compute_projections(xyz, train_cameras) # pixel coords: (n_views, n_rays, n_samples, 2), (n_views, n_rays, n_samples) normalized_pixel_locations = self.normalize( # pixel coords range: ([0,h],[0,w])-\u0026gt; [-1,1] for F.grid_sample pixel_locations, h, w ) # [n_views, n_rays, n_samples, 2] # rgb sampling rgbs_sampled = F.grid_sample(input=train_imgs, grid=normalized_pixel_locations, align_corners=True) # (n_views, 3, n_rays, n_samples) rgb_sampled = rgbs_sampled.permute(2, 3, 0, 1) # [n_rays, n_samples, n_views, 3] # deep feature sampling # sample n_view feature maps for each 3D point. All chanls on the location in a feature map will be taken. feat_sampled = F.grid_sample(featmaps, normalized_pixel_locations, align_corners=True) # (n_views, out_chnls, H, W)-\u0026gt;(n_views, out_chnls, n_rays,n_samples) F.interpolate F.interpolate(input, size, [scale_factor,] mode, ) 把 input 缩放到 size，或者各维度缩放 factor 倍。 插值算法为mode。可以处理3D,4D,5D input。4D input的维度：(B, chnls, H, W) Docs\n(2023-10-10)\nnn.Upsample This \u0026ldquo;layer\u0026rdquo; has no learnable parameters:\n1 2 3 4 upsampler = torch.nn.Upsample(size=(3,4), mode=\u0026#34;bilinear\u0026#34;) a = torch.arange(4).view(1,1, 2,2).float() dict(upsampler.named_parameters()).items() # out: dict_items([]) So it may be equal to torch.nn.functional.interpolate, which can be used as a layer in a model: Which function is better for upsampling: upsampling or interpolate?\ntorch.nn.functional layers require passing the learnable parameters from outside, since they don\u0026rsquo;t contain nn.Parameter like nn.Module does. Are torch.nn.Functional layers learnable?\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.weight = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) # Corresponding to a nn.module: self.linear = nn.Linear(in_features, out_features) def forward(self, input): return F.linear(input, weight, bias) # return self.linear(input) (2023-10-22)\nDownsample Conv layer can perform evenly downsampling, referring to Context Cluster Reducing points by fusing neighbors covered by kernels:\n1 2 proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=padding) Pooling can also be used for placing centers, referring to Context Cluster\n1 centers_proposal = nn.AdaptiveAvgPool2d((proposal_w, proposal_h)) F.interpolate\nTrilinear Interp (2024-02-28)\nPerform 2-point interpolation in 3 directions sequentially:\nEach point has 3 weights: u, v, w (or (1-u), (1-v), (1-w))\n","date":"2022-06-23T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch_sample_interpolate/","title":"memo: PyTorch | Sample \u0026 Interpolation"},{"content":"最浅显易懂的 KMP 算法讲解-奇乐编程学院-bilibili\nKMP的next数组中保存了: 主串中最长的已匹配的字符。线性时间复杂度：主串指针不回退，子串指针回退到已经匹配的部分的后面（跳过已匹配的部分)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def kmp_search(string, patt): # string主串，patt子串 next = build_next(patt): i = 0 # 主串中的指针 j = 0 # 子串中的指针 while i \u0026lt; len(string): if string[i] == patt[j]: # 字符匹配，指针后移 i += 1 j += 1 elif j \u0026gt; 0: # 字符失配，根据 next 数组跳过子串前面的一些字符 j = next[j - 1] else: # 子串第一个字符就失配 i += 1 if j == len(patt): # 匹配成功 return i - j next 数组统计了各字符（包括它在内）之前已重复部分的长度。构造时有两个指针，前方指针指向前缀长度的后面一个字（前缀长度就是该指针之前的(不包括指向的)字符个数）；后方的指针指向当前字符（后缀）。后方指针一直后移，而前方指针要回退查看后缀是否存在与前缀中。 若两指针指向的字符相同，则后方指针对应的next数组元素为: 前缀长度数值+1，然后两个指针都后移一位； 如果它们指向的字符不相同，就看看后缀是否是前缀的一个子集（前缀中是否包含以后缀为结尾的短串，所以就要回到它前面的另一次重复看是否与后缀相接），则后方指针不动，前方指针回退到它左侧字符第一次出现时的下一个字符上（也就是左侧字符对应的前缀长度的下一个字符上，也就是跳过比较左侧字符对应的前缀长度个字符），因为再往前就是与此左侧字符的前缀重复的部分，比较这两个指针指向的字符，相同的话，也就是能连上这个后缀，则后方指针指向的next数组元素就是前方指针的前缀长度+1，然后两指针前移；不同的话，前方指针再回退，再与后缀比较，移动到第一个字符(前缀长度=0)还是与后缀不同的话就置0，后方指针前移，前方指针停在第一字符上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def build_next(patt): \u0026#34;\u0026#34;\u0026#34; 构造next数组 \u0026#34;\u0026#34;\u0026#34; next = [0] # next数组 初值元素一个0 prefix_len = 0 # 当前前缀长度 i = 1 # 后方指针 while i \u0026lt; len(patt): # 一次遍历 if patt[prefix_len] == patt[i]: # 前方指针字符 = 后方指针指向字符 prefix_len += 1 # 前缀长度+1 next.append(prefix_len) i += 1 # 后方指针后移 else: if prefix_len == 0: # 前方指针回到了第一个字符 next.append(0) # next[i]=0 i += 1 else: prefix_len = next[prefix_len - 1] # 前方指针回退到左侧字符的前缀长度的下一个字上，也就是前缀长度变成左侧字符的前缀长度 return next ","date":"2022-06-19T22:00:00Z","permalink":"https://zichen34.github.io/writenotes/algo/kmp/","title":"KMP algorithm"},{"content":"相机旋转 (2022-05-22)\n坐标系旋转是 point 旋转的逆（点逆时针旋转θ 等效于坐标系顺时针旋转θ）\n点P在初始坐标系下的坐标为 Pₒ，在目标坐标系下的坐标为 Pₜ， 从 Pₒ 到 Pₜ 中间是 target 坐标系在 origin 坐标系下的表示（方向向量）[𝐫ₓ 𝐫ᵧ]ᵀ:\n$$ \\begin{bmatrix} xₜ \\\\ yₜ \\end{bmatrix} = \\begin{matrix} rₓ: \\\\ r_y: \\end{matrix} \\begin{bmatrix} a₁ \u0026amp; b₁ \\\\ a₂ \u0026amp; b₂ \\end{bmatrix} \\begin{bmatrix} xₒ \\\\ yₒ \\end{bmatrix} $$\n横着看：行向量 (a₁,b₁) 是 target 系的 x 轴在 origin 系下的方向向量， (a₂,b₂) 是 target 系的 y 轴在 origin 系下的方向向量。 例如，下图中 origin 系是 world，target 系是 camera:\nOriginal 坐标$[^x_y]$做线性组合，变换到了 Target 坐标系$[^{x_c}_{y_c}]$。 竖着看：列向量 (a₁,a₂) 是 origin 系的 x 轴在 target 系下的方向向量， (b₁,b₂) 是 origin 系的 y 轴在 target 系下的方向向量。\n综上，旋转矩阵 R (in w2c) 横着看就是 camera (target) 系在 world 系下的表示，竖着看就是 world (original) 系在 camera 系下的表示。\n(2023-11-05) 所以通过转置就可以把 R in w2c 切换成 R in c2w。 但是要把 w2c 变成 c2w，需要对 [R|t] 整体求逆。 欧几里得变换 [R T]：旋转矩阵 R 加平移向量 T，把点在 origin 系的坐标变成在 target 系下的坐标，或者说把 origin 系变换成 target 系。\nFor example, w2c as below.\n先旋转后平移：点在 origin (world) 系下的坐标先经过 target (camera) 系在 origin 系下的方向向量 R 的线性组合，即 投影（做内积）到了一个与 target 系坐标轴都平行的新坐标系（虚线）下， 再加上一段平移向量 T，从而使新坐标是以 target 系的原点开始，所以 T 就是 origin 系的原点在 target 系视角下的坐标。\nThis process is expressed by [R|t]. Conversely, [R|t] denotes rotation first then translation.\n(2024-01-09) 𝐭 is coordinate measured in the target space, because 𝐭 is simply added onto the target coordiantes without \u0026ldquo;recombination\u0026rdquo; of the elementary vectors in a basis. Therefore, 𝐭 is the original center seen from the target space.\n或者先平移后旋转：origin (world) 系下的坐标先减去 target (camera) 系原点在 origin 系下的坐标 C， 变到了一个新坐标系下（其原点与 target 系原点重合）， 再旋转到与 target 系各轴重合；所以 C 就是 camera 光心在 world 系下的坐标。\nIn this case, R and T can\u0026rsquo;t be written as an augmented matrix, but separate matrices. If the given point\u0026rsquo;s coords are world coords, then apply [R|t]. While if point is already in camera space, only apply [R]. 可借助 vanishing points (待定系数)来求旋转矩阵（This point is at infinite but finite in image.） Camera Projection Matrix-UofMinnesota\n透视投影 (2022-05-08)\n透视投影（内参矩阵K）把 camera space 下的坐标投影到焦平面上，X 除以 Z 乘以 f（即以 f/z 为系数对 x,y 做一个缩放）。 如果焦距(焦平面距离) f 是常数，那就直接是与 z 成反比。\n(2024-01-31) 所以 \u0026ldquo;z\u0026rdquo; 代表的是 \u0026ldquo;Zoom\u0026rdquo; 缩放：近大远小. Ray Marching for Dummies! - Ytb - The Art of Code （这里坐标都是绝对值，不考虑坐标系的选取）\nB a c k p l a y n e f - / 0 p - i - n Z - h - o Y l e F r o n t 因为景物是 倒置 的，所以像素坐标系的 y 轴是向下的？ 若采用齐次坐标（用矩阵表达除法）, 对 [X,Y,Z] 做透视投影得到的是 [fX, fY, Z]，则 [u, v, 1] = [fX/Z, fY/Z, 1]。 再以像素尺寸 dx,dy 缩放并加上(+)光心坐标 cx,cy，把原点从光心移到左上角（像素系的v轴是朝下的，所以还需要加负号？），就变到了像素坐标系下：\n$$ \\begin{bmatrix} \\frac{f}{dx} \u0026amp; 0 \u0026amp; cₓ \\\\ 0 \u0026amp; \\frac{f}{dy} \u0026amp; c_y \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} = \\begin{bmatrix} fₓX + cₓZ \\\\ f_yY + c_yZ \\\\ Z \\end{bmatrix} = \\begin{bmatrix} \\frac{fₓX}{Z}+cₓ \\\\ \\frac{f_yY}{Z}+c_y \\\\ 1 \\end{bmatrix} $$\n投影变换 投影变换 GL_PROJECTION 是把 相机空间 下的点 (xₑ,yₑ,zₑ) 变换到 屏幕空间 的 clip 坐标 (xc,yc,zc,wc)：\n先透视投影 (1/Z缩放) 到相机的 near plane (焦距为-n)。\n透视除法需要除以 -zₑ，所以齐次坐标的 wₑ = -zₑ 再把 x,y 的取值范围：[top,bottom],[left,right] 线性变换到[-1,1]。\n令相机空间下的 [near, far] 的 NDC 坐标等于 [-1, 1]\n因为是从三维到三维，要想用矩阵表达透视缩放和平移，就需要使用齐次坐标。\nclip 坐标是 NDC 的齐次形式，所以这个矩阵（Projection Matrix） 完成了 frustum culling 和 NDC 变换。\n$$ \\begin{bmatrix} \\frac{2}{r-l}⋅n \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2}{t-b}⋅n \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -\\frac{f+n}{f-n} \u0026amp; -\\frac{2fn}{f-n} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} xₑ \\\\ yₑ \\\\ zₑ \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{r-l}⋅nxₑ + \\frac{r+l}{r-l}⋅zₑ \\\\ \\frac{2}{t-b}⋅nyₑ + \\frac{t+b}{t-b}⋅zₑ \\\\ -\\frac{f+n}{f-n}⋅zₑ -\\frac{2fn}{f-n} \\\\ -zₑ \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{r-l}⋅n\\frac{xₑ}{-zₑ} + \\frac{r+l}{r-l} \\\\ \\frac{2}{t-b}⋅n\\frac{yₑ}{-zₑ} + \\frac{t+b}{t-b} \\\\ \\frac{f+n}{f-n} +\\frac{2fn}{(f-n)zₑ} \\ 1 \\end{bmatrix} $$\nviewing\u0026amp;project-utexas\nNDC 空间 屏幕显示的世界深度范围是 near,far 两个焦平面之间的区域。 openGL 中两焦平面间的 z interval 被映射到 [-1,1]，即 Normalized Device Coordinates， 变换到 NDC 后就可以根据z_ndc把超出范围外的物体裁剪掉。\n屏幕窗口显示的是近焦平面。可以用 fov 控制近焦平面的边长，可以把屏幕的 t,b,l,r 映射到近焦平面的边长 [-1,1]，这样屏幕显示的空间就是一个立方体。 探秘三维透视投影-齐次坐标的妙用 -奇乐bili\n(2023-11-30) NDC = Perspective projection to the near plane with depths kept + Scaling.\n相机变换矩阵 推导相机变换矩阵-csdn-潘宏\n(不同基底间的) 坐标转换公式: 𝐯=𝐐 𝐯\u0026rsquo;=𝐑 𝐯\u0026rsquo;\u0026rsquo; ⇒ 𝐯\u0026rsquo;\u0026rsquo;= 𝐑⁻¹𝐐 𝐯\u0026rsquo;，其中𝐐,𝐑 是不同的正交矩阵，代表坐标系，因为正交矩阵的逆等于转置，所以可以写为：𝐯\u0026rsquo;\u0026rsquo;= 𝐑ᵀ𝐐 𝐯'\nUVN相机模型用向量定义相机朝向：N 是相机观察方向的反方向，U 由辅助向量up与N叉乘确定，辅助向量用于让相机产生偏转（不歪头一般取(0,1,0)）；V=N×U，V 落在 up 与 N 形成的平面上。\n例如nerf的函数viewmatrix() 用于构建平均相机位姿 poses_avg 的UVN相机坐标系 [X|Y|Z]（世界系只有一个，而相机系有多个，取平均相机系作为\u0026rsquo;新世界系\u0026rsquo;）。\nView transformation: 把物体坐标从世界系变换到相机系下，也就是做一次相机运动的逆变换。变换过程：初始时相机系与世界系重合，(在世界系下)相机做旋转、再平移接近物体，然后相机与物体一起做逆平移、逆旋转，相机又回到初始位置，物体就变到了相机系下。\n逆平移易求(取反)，逆旋转不易求(求逆的顺序)；但是做完逆平移后，相机系与世界系的原点重合了，只是基底不同，利用坐标转换公式就可以求出在相机系下的坐标 𝐯\u0026rsquo;\u0026rsquo;= 𝐑ᵀ𝐐 𝐯\u0026rsquo;，其中𝐑是UVN系统，𝐐 是世界系(对角阵)，𝐯\u0026rsquo;是逆平移后的向量𝐓⁻¹𝐯，故最终的坐标变换矩阵(w2c外参矩阵Extrinsic matrix)：𝐑ᵀ𝐐 𝐓⁻¹\ndoubt: 旋转矩阵求逆 DDG\n3个矩阵 外参矩阵把点的 world space 坐标 Xw 变换到相机系下：Xc=R⋅Xw+T；\n内参矩阵把点的 camera space 坐标 Xc 变换到焦平面(原点在图片中央) （加上缩放因子fx,fy和光心坐标(cx,cy)可以变换到像素坐标系u,v， 原点在图片左上角,v轴朝下）上：P=K⋅Xc；\n相机投影矩阵 Camera projection matrix：把世界点直接变换到图像平面上（内参矩阵K₃ₓ₃ ∗ 外参矩阵[R T]₃ₓ₄ = P₃ₓ₄）。\nRef:\n11.1 Camera matrix-CMU SLAM入门之视觉里程计(2)：相机模型（内参数，外参数） Camera Calibration and 3D Reconstruction-openCV UVN 模型 从外参矩阵Extrinsic matrix₄ₓ₄ 提取出相机的位置和朝向: (最后一行是齐次坐标）最后一列是世界系中心在相机系中的位置， 左上3x3是相机在世界系下旋转运动R的转置（列向量是世界系，行向量是相机系）。如果再知道相机的观察方向，借助一个辅助向量up，就能确定UVN系统。 StackExchange\ndoubt: UVN 相机模型 Google Search\n(2024-04-02)\nEstablishing w2c from camera position and looking-at vector. Placing a Camera: the LookAt Function - Scratchapixel The \u0026ldquo;forward\u0026rdquo; direction is defined as From - To, because He marked the \u0026ldquo;out\u0026rdquo; of the screen as \u0026ldquo;forward\u0026rdquo;\nAfter determining the \u0026ldquo;forward\u0026rdquo; vector, specify a temporary \u0026ldquo;up\u0026rdquo; vector (usually (0,1,0)), which is not necessary to be perpendicular to the \u0026ldquo;forward\u0026rdquo;, to produce the \u0026ldquo;right\u0026rdquo; vector, accroding to \u0026ldquo;forward\u0026quot;× temporary \u0026ldquo;up\u0026rdquo;.\nOnce the \u0026ldquo;right\u0026rdquo; vector is obtained, re-construct the accurate \u0026ldquo;up\u0026rdquo; vector by \u0026ldquo;forward\u0026rdquo; cross \u0026ldquo;right\u0026rdquo;. Note: He define the Backward as \u0026ldquo;forward\u0026rdquo;.\nThe c2w in this post is row-major format. So, he writes each row is a direction.\nz = -1 is the camera-space coordinate of the ray direction vector.\nThe limitation of the looking-at method is the case that the \u0026ldquo;forward\u0026rdquo; direction is aligned with the temporary \u0026ldquo;up\u0026rdquo; (0,1,0), as the cross product of 2 parallel vectors is zero $\\vec{0}$. 10.4: The Cross Product - Mathematics LibreTexts\nA solution is using quaternion.\n(2024-04-04)\nNeRF builds RUB matrix (i.e., averaged c2w)：average up × back (z-axis) = right (x-axis) Code Cam Coords System (2024-03-21)\n确定了相机坐标系与世界坐标系的相对关系，才能正确地把一个 3D 点的世界坐标变成相机坐标系下的坐标。\n假定世界坐标系是右手坐标系，相机坐标系可能不与世界系重合。\n相机坐标系有 3 个轴：左（右），上（下），前（后），方向不同则：点在该方向上的坐标相差 1 个负号。而且 3 个轴的排列次序也不统一。\nNeRF代码解读-相机参数与坐标系变换 - 陈冠英 - 知乎\nOpen3D\u0026rsquo;s camera coord. sys. is RDF. camera coordinate system of visualization #1347 确定了相机在 世界 坐标系中的朝向和位置: Up vector, viewing direction and position， 才可以操纵相机 (Camera Manipulation)：Changing roll, yaw, pitch, dollying (Slides - Hong Qin). An interactive example in LearnWebGL。 Songho also explain camera manipulation OpenGL Camera.\n所以坐标变换的顺序是：世界系下的坐标 ➡ 相机系下的坐标 ➡ 相机做 6 DoF 运动（等价于物体的相机系坐标做 inverse 运动）➡ 在相机运动完成后，再把 3D 点在相机系下的坐标投影到相机平面。\n对于矩阵 w2c，前 3 列的每一列是世界坐标系的每个 axis 在相机坐标系下的坐标。\n(2024-03-25)\nw2c 是把一个 点 的世界坐标转换成相机坐标。一个 3D 点的坐标等于 一个数组乘以坐标系。 所以 w2c 等于 rotation matrix 乘以世界坐标系：\n$$ \\begin{bmatrix} c_{x_1} \u0026amp; c_{y_1} \u0026amp; c_{z_1} \\\\ c_{x_2} \u0026amp; c_{y_2} \u0026amp; c_{z_2} \\\\ c_{x_3} \u0026amp; c_{y_3} \u0026amp; c_{z_3} \\end{bmatrix} = \\begin{bmatrix} r₁₁ \u0026amp; r₁₂ \u0026amp; r₁₃ \\\\ r₂₁ \u0026amp; r₂₂ \u0026amp; r₂₃ \\\\ r₃₁ \u0026amp; r₃₂ \u0026amp; r₃₃ \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\n因为 rotation matrix 的特性，它的一行就是目标坐标系的一个轴在 源系下的坐标。它的一列就是源系的一个轴在目标系下的坐标。\nEach row in rotation matrix for w2c is an axis of target camera coordinate system. This can be verified by the example below:\nPlotting script: Test_rotation_matrix.ipynb\n同样，对于矩阵 c2w，前 3 列的每 一列 是相机坐标系的每个 axis 在世界坐标系下的坐标。 所以要 调换 相机坐标系在世界坐标系下的朝向，对 c2w 的 rot 的 某一列 乘上一个负号即可。\n相机坐标系的定义影响的是 3D 点在相机坐标系下的坐标，改变相机朝向，最终体现在 3D 点在相机系下的坐标的正负。 具体来说，对于 w2c 中的旋转矩阵，要 调换 相机系的一个轴的方向，应对 rotation matrix 中对应的 一行 添加负号。\n因为 NeRF 使用的是 c2w，它的旋转矩阵的 每一列 是一个相机系的轴，而且次序是 DRB 所以代码中交换第 0 列(Down) 和第 1 列 (Right)，然后对 y 方向乘上 -1 变成 Up。最终变到了 OpenGL 的 RUB。 Code\n(2024-03-24)\n\u0026ldquo;The view matrix transforms all world coordinates to camera-space coordinates.\u0026rdquo; \u0026ndash; LearnOpenGL - Camera\nTherefore, the view matrix (extrinsics) transforms the X,Y,Z axes of the world coordinates system to X,Y,Z axes of the camera coordinates system.\nLet the world X-Y-Z axes be the 3 unit column vectors, as shown in the below right matrix, they\u0026rsquo;re transformed to camera axes by a w2c:\n$$ \\begin{bmatrix} r₁₁ \u0026amp; r₁₂ \u0026amp; r₁₃ \u0026amp; t₁ \\\\ r₂₁ \u0026amp; r₂₂ \u0026amp; r₂₃ \u0026amp; t₂ \\\\ r₃₁ \u0026amp; r₃₂ \u0026amp; r₃₃ \u0026amp; t₃ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\end{bmatrix} $$\nThe extrinsics matrix transforms a world coordinates into camera-space coordinates. Next, the point will be applied with the projection matrix (scaling axes to preserve points whose $z_{clip}$ is larger than its $x_{clip},\\ y_{clip},\\ z_{clip}$, and performing intrinsics) for frustum clipping. The projection matrix requires a 4D homogeneous coordinates: $[x_{cam},y_{cam},z_{cam},1]^T$. So, the above extrinsic matrix has a 4-th row, that results in an additional $1$, which is reserved for storing the depth z value, for the final perspective division. After multiplied with the extrinsics (w2c), the X,Y,Z axes of the world system are still 3 columns, but values become their coordinates under the camera coordinate system. And the 3 rows in the R of w2c are the camera coordinate system.\nHowever, the camera coordinate system has many different matrix formats in different 3D applications. For example, OpenGL (Blender) uses RUB order.\nUsually, the world space is RUB as well. So, transforming world-space coordinates to OpenGL camera-space coordiantes doesn\u0026rsquo;t need to reverse axes.\nWhereas, OpenCV uses RDF camera coord. sys. Thus, the sign of y and z coordinates require flips in the camera space.\nOne of the differences between OpenCV and OpenGL is that in OpenGL, the z-axis has near and far boundary. Refer to Amy Tabb.\nThat post was found with searching \u0026ldquo;Converting camera poses from OpenCV to OpenGL can be easy\u0026rdquo; (DDG), that is a medium blog, which is found when searching \u0026ldquo;camera coordinates right front up\u0026rdquo; (DDG)\n(2024-03-25)\nExample with a 3D point p:\nZ Y O D o w n p F o r w a X r , d R i g h t The above figure shows the right-hand world coordinate system (X-Y-Z) and a OpenCV camera coordinate system (Right-Down-Forward).\nThose 2 coordinate systems have a common origin $O$. And the camera has no rotation.\nThe coordinates of a point p under the world space is $(2, 2, 1)$. However, the coordinates in the camera space is $(2, -2, -1)$.\nThis shows that when converting the world-space coordinate to OpenCV camera-space coordinates, there is a \u0026ldquo;sign matrix\u0026rdquo;:\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} $$\nWhen the camera shifts from the world origin by rotation and translation, i.e., the extrinsics matrix, which transforms the axes of world system. So, the result coordinates is measured in a transformed world coordinate system.\nThus, the \u0026ldquo;sign matrix\u0026rdquo; is required to convert the \u0026ldquo;transformed world\u0026rdquo; system to OpenCV (or other) camera coordinate system.\n$$ \\begin{bmatrix} x_{cam} \\\\ y_{cam} \\\\ z_{cam} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} r₁₁ \u0026amp; r₁₂ \u0026amp; r₁₃ \u0026amp; t₁ \\\\ r₂₁ \u0026amp; r₂₂ \u0026amp; r₂₃ \u0026amp; t₂ \\\\ r₃₁ \u0026amp; r₃₂ \u0026amp; r₃₃ \u0026amp; t₃ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x_{world} \\\\ y_{world} \\\\ z_{world} \\\\ 1 \\end{bmatrix} $$\nTherefore, the matrix transforming world coordinates to OpenCV camera coordiantes is:\n$$ \\begin{bmatrix} r₁₁ \u0026amp; r₁₂ \u0026amp; r₁₃ \u0026amp; t₁ \\\\ -r₂₁ \u0026amp; -r₂₂ \u0026amp; -r₂₃ \u0026amp; -t₂ \\\\ -r₃₁ \u0026amp; -r₃₂ \u0026amp; -r₃₃ \u0026amp; -t₃ \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nSince w2c transforms world axes to camera-space coordinates, the coordinates performed by w2c must be a world coordinates of a point, instead of a camera coordinates.\nSo, the \u0026ldquo;sign matrix\u0026rdquo; must be applied after w2c. Otherwise, the world coordinate becomes a camera coordinate immediately, which doesn\u0026rsquo;t match w2c. Specifically, the below order is incorrect:\n$$ w2c \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} x_{world} \\\\ y_{world} \\\\ z_{world} \\\\ 1 \\end{bmatrix} $$\nIn other words, the \u0026ldquo;sign matrix\u0026rdquo; should be applied on a camera-space coordinates.\nIn NeRF, the provided matrix is c2w, where each column of the rot is a camera axis, and the columns order is DRB. So, the first 2 columns need to switch, thus, becoming RDB. Then, to align the camera coordinate system of Blender: RUB, the second row (the U axis) needs to be multiplied with -1. Code\n(2024-03-26)\nNote: the rotation matrix in c2w and w2c are different. For the rot in c2w, each column is an axis of camera, so reversing the direction of a camera axis requires multiplying a column with -1.\nWhereas, for the rot in w2c, each row is an axis of a camera. Thus, to reverse a camera axis, a row needs to be negated.\n(2024-03-26)\nTest reversing a row and a column in a rotation matrix for w2c:\nOriginal Rot in w2c Flip 0th row Flip 0th column $$\\begin{bmatrix} 0.970 \u0026amp; 0.00747 \u0026amp; 0.241 \\\\ -0.0147 \u0026amp; 0.999 \u0026amp; 0.028 \\\\ -0.241 \u0026amp; -0.0309 \u0026amp; 0.969 \\end{bmatrix}$$ $$\\begin{bmatrix} -0.970 \u0026amp; -0.00747 \u0026amp; -0.241 \\\\ -0.0147 \u0026amp; 0.999 \u0026amp; 0.0282 \\\\ -0.241 \u0026amp; -0.0309 \u0026amp; 0.969 \\end{bmatrix}$$ $$\\begin{bmatrix} -0.970 \u0026amp; 0.00747 \u0026amp; 0.241 \\\\ 0.0147 \u0026amp; 0.999 \u0026amp; 0.0282 \\\\ 0.241 \u0026amp; -0.0309 \u0026amp; 0.969 \\end{bmatrix}$$ $p_{cam}=[2.18994, 0.99823, 0.45571]$ $p_{cam}=[-2.18994, 0.99823, 0.45571]$ $p_{cam}=[-1.69110, 1.05720, 1.42213]$ A row of the rotation matrix in w2c is the coordinates of a camera axis in the world space.\nA column is the coordinates of a world axis in the camera space.\nThe original rotation matrix transforms the world axes to a tilted coordinate system.\nFlip the 0th row of the rotation matrix: only the X axis entirely turns to the oppsite direction.\nFlip the 0th row of the rotation matrix: the x component of all the X-Y-Z axes are affected. Apparently, this is not desired result. When flipping a single axis, the other axes should be unchanged.\nFigure plotting script: Test_reverse_cam_axis.ipynb\nIdentify Cam Axes 只有旋转矩阵 R，但不知道相机在世界坐标系中的朝向（也不知道各轴的次序）。\n@will 在 23-11-10 8:25 AM 说：画出来看看正不正对场景。 @什么办 在 22-11-21 8:50 PM 展示过他用 plt 画的相机位姿。 (2024-03-27)\nThe pose1 does not face towards the object (MVSNet_testing/dtu/scan1/cams/00000000_cam.txt):\nR for w2c rect_001_0 (full) mod signs I have dragged the figure to make the z axis upside down. The camera position is $[-191.02, 3.28832, 22.5401 ]$ And I feel the pose should be $[3, 191, 22]$\n(2024-03-30) Camera position was wrong, but it\u0026rsquo;s not due to signs. The 4-th column in w2c is not the camera position in world.\nDragging z-axis to upside down is equivalent to negating the z coordinate and switching the x and y of points coordinates. Specifically, given a point (x,y,z), dragging z to flip equals: x=y; y=x; z=-z\nDrag manually Flip z and switch x,y Code for modifying axes for scan23 1 2 3 4 5 6 7 8 9 10 11 12 13 14 %matplotlib widget import numpy as np import matplotlib.pyplot as plt import open3d as o3d pcd = o3d.io.read_point_cloud(\u0026#34;/mnt/data2_z/SampleSet/MVS Data/Points/stl/stl023_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vs = np.asarray(pcd.points) samples = vs[np.random.choice(vs.shape[0],1000)] x = samples[:,1] y = samples[:,0] z = -samples[:,2] fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(x, y, z, color=\u0026#39;black\u0026#39;) (204-03-28)\nI know DTU matches the setup of OpenCV because the function cv2.decomposeProjectionMatrix is used. But, I\u0026rsquo;m still confused about the scene visualization with matplotlib. (2024-03-30)\nThe 4-th column of the w2c is not the camera center position in world space!! The camera position in world should be the 4-th column of c2w, i.e., $-R_{w2c}^T t_{w2c}$.\nAnd the t returned by decomposeProjectionMatrix is the camera position as well.\nAfter correcting the camera center position, the camera geometry is correct:\nPlotting script: gist\nIdentifying the axes directions should be independent of camera geometry.\nOnly drawing one camera may not easily indicate if it\u0026rsquo;s facing the scene.\n(2024-03-31)\nOpen3D can set the window (visualizer) to be the specified camera pose.\nChange set_front to different row in the rotation mat:\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([0, 0, 0]) view_ctl = vis.get_view_control() vis.add_geometry(pcd) view_ctl.set_front(w2c[0][:3]) vis.run() vis.destroy_window() set_front(w2c[2][:3]) set_front(w2c[0][:3]) Set the extrinsic to simulate a camera pose: Determining the Proper Camera Position #2338\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import open3d as o3d import numpy as np pcd = o3d.io.read_point_cloud(\u0026#34;/home/yi/Downloads/DTU_SampleSet/MVS Data/Points/stl/stl001_total.ply\u0026#34;, format=\u0026#39;ply\u0026#39;) vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() vis.get_render_option().background_color = np.asarray([0, 0, 0]) vis.add_geometry(pcd) view_ctl = vis.get_view_control() w2c = np.array([[0.970263, 0.00747983, 0.241939, -191.02], [-0.0147429, 0.999493, 0.0282234, 3.28832], [-0.241605, -0.030951, 0.969881, 22.5401], [0.0, 0.0, 0.0, 1.0] ]) cam = view_ctl.convert_to_pinhole_camera_parameters() cam.extrinsic = w2c view_ctl.convert_from_pinhole_camera_parameters(cam, True) current_param = view_ctl.convert_to_pinhole_camera_parameters() print(current_param.extrinsic) vis.run() vis.destroy_window() The argument allow_arbitrary=True is required (using 0.18.0), reminded by: How to you position camera and look at certain location in Open3D? #1483\nThis argument is added to free the limitation on pinhole camera models. ConvertFromPinholeCameraParameters() failed #834\nSimilar issues:\nconvert_from_pinhole_camera_parameters does not work #1343\nconvert_from_pinhole_camera_parameters allow_arbitrary=True modifies intrinsic matrix #5816\n\u0026ldquo;view matrix\u0026rdquo; means w2c. While \u0026ldquo;world transformation matrix\u0026rdquo; is c2w. 3D GEP\nEach column in c2w is a camera axis coordinates in world space. So, 3 columns represent directions, such as RDF, and the 4-th colmun is the camera center in world space.\nHe gave a code demo to show the matrix format in column-major memory accessing.\nOpenCV2OpenGL (2024-03-29)\nJust negate the 2nd and 3rd rows in the rotation matrix that transforms word coords to cam coords. Such that, RDF camera system becomes RUB. (And note OpenGL reads matrix by columns.) OpenCV to OpenGL coordinate system transform - SO\nThis process can be done with a \u0026ldquo;sign matrix\u0026rdquo;: $[[1,0,0,0],\\ [0,-1,0,0],\\ [0,0,-1,0],\\ [0,0,0,1]]$.\nThis \u0026ldquo;sign matrix\u0026rdquo; also appears in PixelNeRF to process DTU. Yu called it as \u0026ldquo;similarity transform\u0026rdquo;. (issue#2)\nSame as the function T_opencv_to_opengl() in camtools\n(2024-04-02)\nMapping a coordinate system to another has two transformations: rotation+translation [R|t] and sign matrix. Converting camera poses from OpenCV to OpenGL can be easy - readmedium (Found when searching \u0026ldquo;opencv to opengl transformation\u0026rdquo; DDG)\nSpecifically, change the source basis first, and then flip axes of the source basis (world) to target basis (camera).\nz y W o r l d x R | t R y o t a t e d z w o x r l d f s l i i g p n z ᶜ C a m e r x a ᶜ y ᶜ I realized that the terminologies: w2c and c2w are siutable for the transition between world and the OpenGL camera coordinate syste, beacuse their axes are aligned, i.e., both RUB.\nOtherwise, for example, between world and OpenCV camera system, the world system is not directly becoming the target camera system after rotation and translation [R|t].\nThat post also takes the column-major memory access into account.\nDTU dataset Original (2024-02-21)\nDTU Homepage Each object scan is taken from 49 fixed camera positions.\nFor the SampleSet, the images dimensions are 1600x1200:\n1 2 yi@yi:~/Downloads/DTU_SampleSet$ identify MVS\\ Data/Rectified/scan1/rect_001_0_r5000.png MVS Data/Rectified/scan1/rect_001_0_r5000.png PNG 1600x1200 1600x1200+0+0 8-bit sRGB 2.85068MiB 0.000u 0:00.000 The camera projection matrix 𝐏₃ₓ₄ from world to image, i.e. K@[R|t] (Casmvsnet and SO):\n1 2 3 4 yi@yi:~/Downloads/DTU_SampleSet$ cat MVS\\ Data/Calibration/cal18/pos_001.txt 2607.429996 -3.844898 1498.178098 -533936.661373 -192.076910 2862.552532 681.798177 23434.686572 -0.241605 -0.030951 0.969881 22.540121 (2024-03-27) The P is estimated by Matlab, and Matlab regards camera coordinates system as RDF (mentioned in Multi-cameras on same coordinate system - Dima found by Perplexity), which is the same as OpenCV camera model.\nThe P can be decomposed to K,R,t by decomposeProjectionMatrix(): 1 2 3 4 5 P_orig=np.array( [[2607.429996, -3.844898, 1498.178098, -533936.661373], [-192.076910, 2862.552532, 681.798177, 23434.686572], [-0.241605, -0.030951, 0.969881, 22.540121]]) K,R,t = cv2.decomposeProjectionMatrix(P_orig)[:3] The original intrinsic matrix K (performed K/K[2][2]) is:\n1 2 3 array([[ 2.89233051e+03, -2.48063349e-04, 8.23205273e+02], [ 0.00000000e+00, 2.88317528e+03, 6.19070918e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]) It\u0026rsquo;s aligned with the intrinsics in mvs_training (not the cams in train/ folder):\nmvs_training/dtu/Cameras/00000000_cam.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 z@homepc:~/Downloads/Datasets_life/mvs_training/dtu/Cameras$ cat 00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 2892.33 0 823.205 0 2883.18 619.071 0 0 1 425 2.5 The table lists focal length and image resolution correspondence:\nScale resolusion cropped f_x 1 1200x1600 2892.3 1/2 600x800 512x640 1446.1 1/4 300x400 723.08 1/8 150x200 361.5 (2024-03-27)\nIn Dima\u0026rsquo;s answer, he described RDF as the world space. That means the extrinsics has been applied by the \u0026ldquo;sign matrix\u0026rdquo;, which changes the world axes to camera axes. So, the R decomposed from P essentially corresponds to the RDF coordinates system.\nIn other words, the camera coordinate system is used as the world coord. sys.\nWhereas, the world system during visualization is usually RUB (Y-axis is Up), like OpenGL. So, the object is upside down when plotting the point cloud with matplotlib.\nAnd the ccs in Open3D also is RDF (relative to world space RUB), so its initial w2c has reverse the y-axis and z-axis of the world space:\n1 2 3 4 [[ 1. 0. 0. -0.] [-0. -1. -0. 0.] [-0. -0. -1. 0.] [ 0. 0. 0. 1.]] Code: Print current cam pose 1 2 3 4 5 6 7 8 import open3d as o3d vis = o3d.visualization.VisualizerWithKeyCallback() vis.create_window() view_ctl = vis.get_view_control() current_param = view_ctl.convert_to_pinhole_camera_parameters() print(current_param.extrinsic) vis.run() vis.destroy_window() MVSNet (2024-02-22)\nTraining set: dtu_training.rar (19G) (\u0026ldquo;mvs_training/dtu/\u0026rdquo;)\nAs mentioned in the section 4.1 of the MVSNet paper, the training images are 1/2 * (1200,1600) = (600,800), which then cropped to (512,640).\nIn addition, because the camera is looking at feature maps, the focal lengths should be scaled with the ratio of the size of feature map to the input image size.\nAs feature map size (128,160) is 1/4 input image (512,640) mentioned in paper section 3.1, the focal_x should be: 2892.33 * 1/2 * 1/4 = 361.541. Issue\nNote: The already calculated trianing camera params are placed in \u0026ldquo;mvs_training/dtu/Cameras/train\u0026rdquo;. Code While the cameras displayed outside the \u0026ldquo;train/\u0026rdquo; are params corresponding to the original DTU images (1200,1600).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 z@lambda:~/Downloads/mvs_training/dtu$ identify Rectified/scan1_train/rect_001_0_r5000.png Rectified/scan1_train/rect_001_0_r5000.png PNG 640x512 640x512+0+0 8-bit sRGB 626KB 0.000u 0:00.000 z@lambda:~/Downloads/mvs_training/dtu$ cat Cameras/train/00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 361.54125 0.0 82.900625 0.0 360.3975 66.383875 0.0 0.0 1.0 425.0 2.5 Testing set (dtu.zip) has the full-size images:\nTesting images are\u0026rsquo;t downsized twice or cropped, so the focal lengths only times 1/4. Code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 z@lambda:~/Downloads/data2/MVSNet_testing/dtu$ identify scan1/images/00000000.jpg scan1/images/00000000.jpg JPEG 1600x1200 1600x1200+0+0 8-bit sRGB 705KB 0.000u 0:00.000 z@lambda:~/Downloads/data2/MVSNet_testing/dtu$ cat scan1/cams/00000000_cam.txt extrinsic 0.970263 0.00747983 0.241939 -191.02 -0.0147429 0.999493 0.0282234 3.28832 -0.241605 -0.030951 0.969881 22.5401 0.0 0.0 0.0 1.0 intrinsic 2892.33 0 823.205 0 2883.18 619.071 0 0 1 425 2.5 The factor adaptive_scaling is used for the requirement that image size must be evenly divisible by 32 (e.g., 864x1152) and reducing images for limited VRAM. So, this step will also change resolution, focals, and principle points: Code\nMVSNet-PyTorch Training cam: 1/8 focal of the original DTU Code\nTesting cam: 1/4 focal of the original DTU Code\nPixelNeRF (2023-08-17)\n\u0026ldquo;rs_dtu_4\u0026rdquo; follows the DVR format. Each object has 6 matrices. Take the object 0 as an example:\n1 2 3 4 5 6 [\u0026#39;scale_mat_0\u0026#39;, \u0026#39;scale_mat_inv_0\u0026#39;, \u0026#39;world_mat_0\u0026#39;, # Projection Matrix, 4x4 \u0026#39;world_mat_inv_0\u0026#39;, # Inverse Projection matrix, 4x4 \u0026#39;camera_mat_0\u0026#39;, # ??? \u0026#39;camera_mat_inv_0\u0026#39;] Use cv2.decomposeProjectionMatrix(P) to solve 𝐊,𝐑,𝐭 from 𝐏₃ₓ₄. Code in PixelNeRF:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 P = all_cam[\u0026#34;world_mat_0\u0026#34;] P = P[:3] # (3,4), projection: Intrinsics * Extrinsics * 3Dpoint K, R, t = cv2.decomposeProjectionMatrix(P)[:3] K = K / K[2, 2] # Not the camera_mat, # Xc = extrinsics*Xw extrinsics = np.eye(4, dtype=np.float32) extrinsics[:3,:3] = R # The 4th column is the rotated transVec extrinsics[:3,3] = -(R @ (t[:3]/t[3]))[:,0] print(extrinsics) # c2w equals inverse extrinsics c2w = np.eye(4, dtype=np.float32) c2w[:3, :3] = R.transpose() # The 4th column is the normalized t decomposed by cv2. c2w[:3, 3] = (t[:3] / t[3])[:, 0] c2w == np.linalg.inv(extrinsics) The K (focal) in pixelNeRF is about twice as large as the intrinsics of \u0026ldquo;dtu_training\u0026rdquo;, because the image size of pixelNeRF (300x400) is twice as small as MVSNet (or MVSNeRF) in each dimension, where (512x640) is cropped from (600x800). It\u0026rsquo;s like when you observe the scene from far away, the image captured gets smaller.\nSince the projection matrix computed from K@(R|Rt) is different, the decomposed intrinsics will be different.\n(2024-02-22) The above statement may be wrong. Take the 1st camera as an example:\n1 2 3 4 cams = np.load(\u0026#34;pixel-nerf/data/DTU_Dataset/rs_dtu_4/DTU/scan1/cameras.npz\u0026#34;) P = cams[\u0026#39;world_mat_0\u0026#39;][:3] K, R, t = cv2.decomposeProjectionMatrix(P)[:3] K / K[2, 2] The K equals 1/4 the intrinsics of the original DTU dataset:\n1 2 3 [[ 7.23082629e+02, -6.20158374e-05, 2.05801318e+02], [ 0.00000000e+00, 7.20793819e+02, 1.54767729e+02], [ 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]] Because camera is used to project 3D points ont feature maps, the focals should be scaled based on the ratio of the feat map to the original image (1600,1200).\nc2w (3x4) is not the Inverse Projection Matrix (4x4).\nInverse Projection Matrix = np.linalg.inv(Projection Matrix)\nProjection matrix converts a 3D world coords to 2D pixel coords;\nExtrinsics ( w2c = [R|t] = (R|Rt) ) converts 3D world coords to 3D camera coords.\nInverse Extrinsics ( c2w ) converts 3D camera coords to 3D world coords.\nGive Extrinsics and Intrinsics (of dataset \u0026ldquo;dtu_training\u0026rdquo; from MVSNet), the Projection matrix can be restored as implemented in MVSNeRF def build_proj_mats()\nThe translation vector also needs rotation. OpenCV Decompose projection matrix\nProjection matrix = Intrinsics@Extrinsics = K@[R|t] = K@(R|Rt) = (KR|KRt)\nDecomposed t needs normalization, and to be negated sometimes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 K = np.array([[631, 0, 384], [ 0, 631, 288], [ 0, 0, 1]]) R = np.array([[-0.30164902, 0.68282439, -0.66540117], [-0.63417301, 0.37743435, 0.67480953], [ 0.71192167, 0.6255351 , 0.3191761 ]]) t = np.array([ 3.75082481, -1.18089565, 1.06138781]) P = np.eye(4) P[:3, :3] = K @ R P[:3, 3] = K @ R @ t K1, R1, t1 = cv2.decomposeProjectionMatrix(P[:3, :])[:3] t == -(t1[:3]/t1[3]) The original t can be obtained directly from projection matrix: np.linalg.inv(P[:3,:3]) @ P[:3,3], i.e., use the inverse rotation to rotate the transVec back.\n(2024-03-29)\nThe t returned by cv2.decomposeProjectionMatrix is the position of a camera in the world space. (Docs) So, it\u0026rsquo;s actually the translation vector in c2w: $t_{c2w}$.\nBecause t (denoted as $t_{cv2}$ for later) is the camera center, its corresponding camera-space coordinates is 0. Thus, this is the relationship:\n$$t_{cv2} = R_{c2w} 0 + t_{c2w} \\\\ t_{cv2} = t_{c2w}$$\nTo get the $t_{w2c}$, i.e., the 4-th column in the w2c (extrinsics), the conversion formula is $t_{w2c} = - R_{w2c} t_{c2w}$.\nThis relationship can be derived from the transformation between camera-space coordinate X and world coordinate P:\n$$ P = R_{c2w} X + t_{c2w} \\\\ X = \\underbrace{R_{c2w}^T}_{R_{w2c}} P \\underbrace{- R_{c2w}^T t_{c2w}}_{t_{w2c}} $$\nConsidering the above example, t is not the 4-th column in extrinsics, but the -R @ (t/t[3])[:3] is.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;\u0026gt;\u0026gt; t array([[-0.99198397], [ 0.00603084], [-0.12611273], [-0.00519817]]) \u0026gt;\u0026gt;\u0026gt; t/t[3] array([[190.83346195], [ -1.16018638], [ 24.26100588], [ 1. ]]) \u0026gt;\u0026gt;\u0026gt; -R @ (t/t[3])[:3] array([[-191.01958721], [ 3.28830259], [ 22.54011993]]) Therefore, in PixelNeRF directly used t as the 4-th column of the c2w (named as pose)\nI was reminded by:\nOne question about cv2.decomposeProjectionMatrix #10\nQuestions on how to use PyTorch3D\nHow to find camera position and rotation from a 4x4 matrix? - SE (surfaced by \u0026ldquo;given camera extrinsics, how to determine right, up, front\u0026rdquo; DDG)\n$$0=RC+T \\\\ C=−R^T T$$\nIn the folllwing posts, they all mentioned the 4-th column in Extrinsics is not the camera center, but $-R_{c2w}^T C$, where C is the camera center in world space:\nDissecting the Camera Matrix, Part 2: The Extrinsic Matrix - ksimek\nHe derived w2c from c2w, ie. w2c = (c2w)⁻¹, and provided an interactive demo for visualizing camera intrinsic, extrinsic.\nCamera Pose \u0026amp; Pose Estimation - MiaoDX refered by Camera extrinsic matrix from camera location and rotation\nHow to plot the camera and image positions from camera calibration data? - SO\nThe OP cited wikipedia $C = -R^{-1} T = -R^T T$\nPlot Camera Trajectory - SO Understanding COLMAP\u0026rsquo;s Camera Poses and Depth Data #1476\ninv([R|t]) = [R'|-R'*t] Camera position in world coordinate from cv::solvePnP - SO\n","date":"2022-05-22T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/camera_matrices/","title":"memo: Vis | Camera Matrices"},{"content":"(2022-07-20)\nHyper-params N_rand 或 batch_rays 是一个 epoch/iteration 要训练的光线, no_batching是每epoch只从一张图中选像素训练，False则打乱所有图像的像素（都与test阶段渲染全图无关）。\nllff 使用了batching，每一个 epoch 使用的(4096)光线来自所有训练图像；而blender 不batching，每epoch只从一张图中取光线，所以N_rand 也较小(1024)，有人说是因为 blender 包含360度的图片，可能会采到完全对立的两条光线（一个从正面看，一个从反面看），可能影响训练（影响density分布？）。NeRF源码解析-什度学习\nN_samples决定每条光线上的采样点数量和NeRF的渲染质量;\nchunk是model一次计算的rays, netchunk 是实际每次输入model的3D points量（模型的train和test都与它们有关）。 chunk 和 netchunk 是两层循环。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Select N_rand rays for training. # Or input all rays in an image (H,W) for rendering def render(rays): for each ray-chunk in rays: ret = render_rays(ray-chunk) def render_rays(): # Sampling 3D points and do PE for each point-netchunk in all points of this ray-chunk: raw.append(model(pts-netchunk)) return tf.concat(raw) # Collect ret for each ray-chunk ret_list.append[ret] # All rays are composed: all_ret = tf.concat(ret_list) # for N_rand or (H,W) rays chunk（光线）和netchunk（点）可依显存大小调整。当N_rand相同，而 netchunk 不同，曲线几乎重叠(~0.03)，只影响训练速度。\n虽预设chunk=1024x32，但N_rand (batch_rays)预设只有1024，所以实际输入model的光线是1024，则输入coarse model 的数据点有1024x64个（真正的\u0026rsquo;batch_size\u0026rsquo;），当通过attention layer时，要创建一个 (65536,128,128) 张量要用1.07G （不，tf先创建计算图，PE函数占了8个G，其他的显存应该是返回值(tensor)占的）\nndc 在render()中默认为True，只有当数据集不是llff类型，或指定不使用ndc（--no_ndc）时，参数字典会添加一项ndc=False。\nlindisp = False 则是对深度（世界空间:[near,far]或者ndc空间:[0,1]）线性采样；为True 则对 1/depth 线性采样\ntestskip 不影响llff, 也不影响 train 集（逐张训练），跑 val 集和 test 集会跳过图片。\nfactor是 llff 数据集在 load_llff_data() 时的 shrink 倍数， half_res影响 blender(和deepVoxel)数据集； render_factor是test阶段(render_path())的渲染分辨率\nrandom_seed 在test阶段不起作用(perturb=False,raw_noise_std=0)。 在train 时，(use_batching) 会打乱所有像素，(no_batching)会随机选图片和像素, 在density上加随机noise，在光线采样时加入随机perturb。\nperturb 和 raw_noise_std 在 test (--render_only, render_path()) 渲染阶段都是0。\nOOM 3 similar variables:\nN_rand is the #rays used for 1 \u0026ldquo;epoch\u0026rdquo; to train\nchunk is the #rays to do render_rays() in a for loop, where points will be sampled to do positional embedding (fine stage has double pts to do PE.),\nThe embedded_fn() in coarse stage takes 2G meomery; The embedded_dirs() in coarse stage takes 2G; And the fine stage takes another 4G. NeRF model takes 357 MB.\nDict all_ret contains all the returend values from model. In the 12th iteration of render_rays() after coarse stage, VRAM becomes 10763 MB from 8465 MB and then OOM, where I returned 3 additional tensors: pts, viewdirs, z_vals. So the return value seems to occupy memory as well.\nBut if keeping the original settings where fewer tensors are returned, it becomes 10763 MB when finishing the ray-chunk for loop (32768 ray x 23 chunks) in batchify_rays().\nMemory change: 485 MB ⮕ 2277 MB ⮕ 4369 MB ⮕ 8465 MB\nnetchunk is the #pts fed into model.\nretraw is one of the reasons:\nWith setting --render_only and --factor=4, if retraw=True, OOM occurs at the line all_ret = {k: tf.concat(all_ret[k], 0) for k in all_ret} in batchify_ray(). 1 2 rgb, disp, acc, extras = render( H, W, focal, chunk=chunk, c2w=c2w[:3, :4], retraw=True, **render_kwargs) This shows that the returned values indeed take some memory. (2022-12-26)\nInverse transform sampling sample_pdf(z_vals_mid, color_weights[...,1:-1],N_importance,) code\nPDF is the proportion of the color-weight of a point to the sum of all weights on a ray. Each point has a ratio indicating how much of its color contributes to the pixel color. (They exclueded the start and end pts, and may only focus on the distribution of the middle part.)\nCDF tells how much the color has been rendered up to the current point. The input of CDF is the color-weight of a point, and its output is the cumulative weight from ro to that point. The weights will be accumulated to 1. So coarse net learns the weights CDF roughly. And fine sampling is based on the CDF increment.\n(When testing) N_importance points u are selected evenly from [0,1] and they will fall into buckets spaced apart by CDFs of coarse points. The percentage of the position of u between the coarse point below and above it determines the distance marching from its corresponding left-neighbor z_vals_mid. The marching distance of a fine pts z is proportional to the increment of cdf.\nWhen the CDF rises faster, there are more samples because the steep slope makes many us fall together.\nIn the above figure, the vertical coordiantes are the uniformly sampled u. Horizontal x-ticks are coarse samples on a ray. Green markers | are midpoints of coarse samples\u0026rsquo; z. Red points are fine samples. Red points are all start from a midpoint and march a distance which is proportional to the cdf increment of its corresponding u.\ntf.searchsorted(seq, values, side) returns the bin-index for each value. seq defines a series of bins, and values are assigned to bin-indices based on the edges listed in seq. side indicates whether the index of a bin is marked by its left or right edge. For example: edges = [-1, 3.3, 9.1, 10.0]; values = [0.0, 4.1, 12.0], return array([1,2,4]) more\n1 2 3 0 1 2 3 4 ————|———●—————|————●———————————|————|————●———— -1 0 3.3 4.1 9.1 10 12 (2022-06-15)\nPE has no π (pi in positional embedding? #12)\n世界坐标系下的 z（通过 projectmatrix 的第3行）从 (-near,-∞) 的范围缩放到了ndc坐标的[-1,1]，x,y并没有参与z的缩放，x,y是依据屏幕大小缩放的，如果 scene 不能被一个屏幕装下（一幅image没有把scene拍全），屏幕外的点的坐标是在[-1,1]范围外的：比如 scene 的范围是[-1.5, 1.5]，然后在embed时乘上π，则最低频时的定义域为 [-1.5π, 1.5π]，因为周期性，多个x可能对应同一个y，导致重复嵌入，所以再缩放一个π，定义域变为[-1.5,1.5]，在这段区间上是单调的，不同的x被编码后的值是不同的。 nerf代码分享-爱睡觉的人人-bili\n(2022-06-15)\nUse t to sample points 使用中间量 t 是为了统一 LLFF 场景和 blender 场景对光线采样的代码（llff的光线长度z∈[0,1]，blender的光线长度z∈[near,far]）。\nblender场景边界有限可以直接对光线长度采样，但是 llff 场景光线无限长（直接采点效果不好？），需要先将场景的世界系变换为NDC系（只对llff场景有效），左乘 projection matrix 使世界系的z方向边界坐标[-near,-far]变为[-1,1]。\n则世界系中的rays表达式: 𝐫=𝐨+t𝐝 变成了 𝐫=𝐨\u0026rsquo;+t\u0026rsquo;𝐝\u0026rsquo; (起点𝐨\u0026rsquo;和方向𝐝\u0026rsquo;都是near,far的函数)； 令𝐨\u0026rsquo;=𝐨，则(世界)光线长度t∈[0,∞] 变成(NDC)光线长度 t\u0026rsquo;∈[0,1] 附录公式15。 后面就可以通过对 t\u0026rsquo;∈[0,1] 采样，来对世界系中的各条光线采样（不是对世界空间的z轴采样，而是在各条光线上采样）。\n在对世界光线做 NDC 变换前，把各条光线的起点 𝐨 转移到其与近平面 (z_world=-near) 的交点处，这样变换到NDC后，光线长度t\u0026rsquo;=0 就对应着 -near，只会对各条光线落在z_world∈[-near,-far]的部分采样。在从世界系变到NDC中光线(𝐨\u0026rsquo;和𝐝\u0026rsquo;)时，认为near平面在z_world=1处(函数ndc_rays()的参数), far平面在 z_world=∞ 处 (公式23,24)。 (作者在issue34解释说:) 因为NDC的z轴是按 1/depth 线性变化的，所以世界系-far=∞没关系（变到ndc中就是far=1），只会稍微降低采样效率。\n确定好ndc光线后，把near,far重新赋值为光线长度的取值范围（llff 的NDC中光线长度的取值范围只能是[0,1]，而blender场景的光线长度范围就是世界系的[near,far]），用于在各光线上采样，再根据光线公式𝐨\u0026rsquo;+z⋅𝐝\u0026rsquo;得到采样点的3D坐标。（near,far不是depth深度范围，而是射线长度。） nerf code\nnear,far本来只是长度，在nerf中，世界系(avg_camera)与相机系z轴重合，scene落在-z方向，所以近远边界的 世界坐标 为-near,-far，而ndc的z轴方向是相机观测方向，近远平面的ndc坐标=-1,1。\n(2023-05-25)\nt or z sampling When sampling points:\n1 pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3] 这里rays_d要是没有归一化，直接就是 K逆*(u,v,1)，那这里z_vals 就是到成像平面距离；如果归一化了，那就是到光心距离 \u0026ndash; will. (QQ chat 2023-05-25T04:09:00) 基于数据集给定的深度直接映射三维点，远端会膨胀，投影到z轴上几何就正常了，所以NeRF出来的深度是视点到三维点的距离 \u0026ndash; 哈哈 (2022-06-15)\nRays in NeRF 对光线的处理：(camera ➔ world (avg camera) ➔ ndc)\n构造（世界系中的）光线：rays=get_rays_np(c2w) -\u0026gt; (ro+rd, 3)\n像素系(u,v) ➔ 成像平面系(u-cx, v-cy) ➔ unproj2相机系(x=u-cx/f, y=-(v-cy)/f, z=-1) ➔ ∗c2w（\u0026ldquo;世界系\u0026rdquo;），这里取平均相机系作为“世界系”。只有把光线变换到一个共同的\u0026quot;世界系\u0026quot;下才能用不同视图对同一 3D point 做优化。 传入的c2w=poses[:,:3,:4] 是从 load_llff_data()返回，其中函数recenter_poses()/spherify_poses()调整原poses，让它们能够把点变换到\u0026quot;新世界系\u0026quot;下。（NeRF代码中的poses全是c2w变换） avgc_2w=poses_avg(poses)，平均相机系➔世界系 poses=np.linalg.inv(avgc_2w)@poses，相机系➔世界系➔平均相机系(\u0026lsquo;新世界系\u0026rsquo;) 渲染（世界系中的）光线：rgb=render(batch_rays)\nro, rd = ndc_rays(near, ro, rd)，光线的世界坐标变NDC all_ret = batchify_rays(rays,chunk)；在render_rays()中，从NDC的 [near,far] 采样z 不直接使用 colmap 生成的c2w，因为 colmap (sfm) 中的世界系是由输入序列的第一帧确定的，但第一帧的方向是随机的，不一定正对scene，场景就重建不全，后面帧的 poses (列向量)都是相机在世界系中的表示 爱睡觉人人16:00。\n从像素平面构建出的 rays 是沿着相机系的-z轴的（观测方向，即景物在相机z轴的背面），也就是NDC的Z轴方向(ndc假设)；把光线变到平均相机系(世界系)下后，再变换到NDC，则scene就落在NDC的正半z轴上（之后都使用ndc坐标）。 因为 NDC 假设相机是沿 -z 方向观察，所以取各相机z轴反方向的平均，作为NDC的z轴方向。 因为光线先被变换到世界系下，再把世界系变为NDC（而OpenGL是把相机系变换到ndc），所以世界系的z轴与NDC的z轴一样看向scene。 nerf-issue#34 LLFF data preprocessing\n求平均相机系(在世界系下的表示) recenter_poses： 相机光心的坐标直接对第4列取平均得到；旋转矩阵R分为3个列向量：各相机z轴取平均，再使用右手定则确定x,y。 把4个列向量x,y,z轴和光心 concat 起来就是平均相机系在世界系中的表示，就是c2w。c2w 求逆得 w2c。 原poses 把相机系中点变换到世界系下，再由w2c变换到平均相机系(‘新世界系’)下。\n(矩阵的列名是源系, 行名是目标系, 变换顺序为从\u0026rsquo;列\u0026rsquo;到\u0026rsquo;行\u0026rsquo;, 每列的意义是\u0026rsquo;源\u0026rsquo; seen from \u0026lsquo;目标系\u0026rsquo;)\n对poses $\\begin{bmatrix} R \u0026amp; t \\\\ 0 \u0026amp; 1 \\end{bmatrix}$ 求逆时，R转置，t 会有一点变化，最后一行还是0001。\n(2024-03-29)\n对 c2w 求逆后，w2c 中的 t 等于 $-R_{c2w}^T t_{c2w}$. For example, given a 3D point p, suppose its world-space coordinates is P. And its camera-space coordinates is X. So, there is:\n$$ \\begin{bmatrix} R_{c2w} \u0026amp; t_{c2w} \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} P \\\\ 1 \\end{bmatrix} \\\\ R_{c2w} X + t_{c2w} = P \\\\ X = R_{c2w}^{-1} (P - t_{c2w}) \\\\ X = \\underbrace{R_{c2w}^T}_{R_{w2c}} P \\underbrace{- R_{c2w}^T t_{c2w}}_{t_{w2c}} $$\ndoubt: rays_d = dir@c2w.T 是怎么推导的? (c2w ∗ dir)ᵀ = dirᵀ ∗ c2wᵀ nerf-pl (2022-05-22)\ndoubt: C2W 是 K[R|T] 的逆，colmap直接求出了？对\nc2w 中的 [R|T] 为： $$ \\begin{array}{ccc} \\qquad \\rm Xcam \\quad\\quad Ycam \\quad\\quad Zcam \\quad cam\\ center\\\\ \\begin{bmatrix} 0.989 \u0026amp; -0.0224 \u0026amp; -0.142 \u0026amp; -3.67 \\\\ -0.0272 \u0026amp; -0.999 \u0026amp; -0.0318 \u0026amp; -1.603 \\\\ -0.141 \u0026amp; -0.0354 \u0026amp; -0.989 \u0026amp; -0.276 \\end{bmatrix} \\end{array} $$\n构造 world 系中的光线即确定 ro 和 rd：ro就是 cam center 在world中的坐标，就等于c2w第4列 ro=c2w[:3,-1]，rd 是用像素(u,v)对应的相机系坐标(u,v,1)变换到world系下的坐标(xw,yw,zw)表示 rd=c2w[:3,:3]@pixels_cam_coord bds.min 在load数据时，缩放场景下界bds.min()和平移变量T（至1./bd_factor，R不需缩放）是为了以防场景太大，变换到 NDC 时超出 near 平面；虽然这里缩放后的边界不等于 near，但之后在ndc_rays()中会被缩放回 -near (-1)。issue#34 (2022-06-03)\nget_rays() get_rays()从像素平面构建ray的方向向量dir： 首先把像素坐标 (u,v) 反投影到相机系 (RUB) 下: (xₚ=(u-cx)/f, yₚ=-(v-cy)/f, zₚ=-1)，v轴与y轴反向，取这些点深度值=-1（如下图）。 再乘以 c2w 变换到 world 坐标系下。从而一条光线在世界系中表示为：𝐫=𝐨+t𝐝）dir 与球坐标 φθ 没有关系。 nerf-issue#24: the direction in get_rays； 爱睡觉人人04:07\nColmap/OpenCV中的相机坐标系是 [right | down | forwards]， 经过LLFF存储poses为 [down|right|backwards]， NeRF 在_load_data()读取后立即变换为 (OpenGL中的相机系) [right | up | backwards]。\nposes_bounds.npy文件中是 camera-to-world！(而不是外参矩阵!) 每一行17个值， 前15个值是3x5的pose matrix，最后2个值是near,far场景边界。 一个 pose matrix 由5个列向量组成：cam down-axis, cam right-axis, cam back-axis, cam center, hwf(cam intrinsics)， 前3列是世界系下的相机系。\n_load_data() 返回的 poses 是 reshape 后的(3,5,20), 索引 poses[:,1:2] 对应到matrix中的第2列。 所以在 #250行 重新调整poses的\u0026quot;行\u0026quot;顺序，使R的列顺序(相机系)为：[right,up,back]。又在#251行把poses变回(20,3,5) Using your own poses without running COLMAP-LLFF\n在后续工作中，把nerf使用的poses（经load_llff_data()处理，作用为cam-to-avg_cam），又变回OpenCV下的相机系（比如IBRNet,GNT）。 如下图(from Zirui Wang-Twitter)\n(2022-11-14) The direction of the constructed world rays rays_d should not be normalized to 1 when used for sampling points (where in nerf-pl is wrong confess). Original nerf didn\u0026rsquo;t normalize in get_rays(). Because the points are sampled based on z, so they locate on parallel planes. If the directions are normalized, the plane will become a sphere.\nBut the viewdirs is normalized when it used as the model input (See run_nerf.py line#306). viewdir is the normalized rd.\ndist dists are intervals between [near,far] and used for transforming the density σ (activation) to opacity α of each interval on the ray: α=(1-e⁻ʳᵉˡᵘ⁽⁶ ᕁ ᵈⁱˢᵗ⁾), because the opacity is proportional to distance (\u0026ldquo;thickness\u0026rdquo;). The dists can be equal when no perturb, so if density is high like around the surface, the opacity is close to 1. And the color coefficient: w=(1-α) ∏ₜ₌₁ᵗ⁻¹ αₜ is monotonically decreasing, so the further a point is, the smaller its color coeff will be until 0.\nThe derivative of color weight at the surface point t∗ is not zero: $dw(t^∗)/dt \u0026lt; 0$ (see Neus proof). Hence, the minia is not on the surface. Errors (2023-05-13)\nmogrify error A new environment is created on Ubuntu 22.04. The package imagemagick is installed in the env nerf by checking conda list. But when I debug the code in vscode, the shell /bin/sh cannot found the command mogrify:\n1 2 3 4 Fixing random seed 2201 Minifying 64 ./data/nerf_llff_data/fern mogrify -resize 1.5625% -format png *.JPG /bin/sh: 1: mogrify: not found Testing in a terminal, if the env is (nerf), executing the command: /bin/sh -c mogrify is okay and it will prompt the optional arguments.\nBut if the env is (base), executing /bin/sh -c mogrify, the same error occurs: /bin/sh: 1: mogrify: not found.\nI found a Chinese post for the same problem. He installed imagemagick again: sudo apt install imagemagick 【NeRF】在yenchenlin/nerf-pytorch上运行新的数据集\nFor this CalledProcessError \u0026quot;/bin/sh: 1: MY_COMMAND: not found\u0026quot; problem, some people suggest to create a link to /usr/bin, e.g., linux.org and ms-community\nmogrify doesn\u0026rsquo;t exist in /opt or /usr as the command find /opt /usr -name magick didn\u0026rsquo;t return anything. mogrify command not found (homebrew)\nmogrify only exits in:\n1 2 3 (nerf) w@homepc:~$ find ~/anaconda3 -name mogrify /home/w/anaconda3/pkgs/imagemagick-7.1.1_5-pl5321h211c493_1/bin/mogrify /home/w/anaconda3/envs/nerf/bin/mogrify Therefore, maybe the debugger of vscode will only search in the /usr/bin (without searching the newly created virtrual envs) ? But the interperter of VScode is indeed shown as Python 3.7.12 ('nerf') ~/anaconda3/envs/nerf/bin/python. I don\u0026rsquo;t know \u0026hellip;\nSolutions:\nCreat a symbolic link sudo ln -s ~/anaconda3/envs/nerf/bin/mogrify /usr/bin/mogrify, then that error disappeared. apt install imagemagick works too because it creates a binary in /usr/bin, which will be found by \u0026ldquo;/bin/sh\u0026rdquo;. (Didn\u0026rsquo;t try) Add the ~/anaconda3/envs/nerf/bin into PATH by adding the line: export PATH=$PATH:/home/zichen/anaconda3/envs/nerf/bin in ~/.bashrc imread_v2() error The version of imageio in this env is 2.28.1. And it\u0026rsquo;s 2.19.0 in my lab server. When reading images:\n1 2 3 def imread(f): if f.endswith(\u0026#39;png\u0026#39;): return imageio.imread(f, ignoregamma=True) 1 read() got an unexpected keyword argument \u0026#39;ignoregamma\u0026#39; Maybe I can remove that argument. But I chose to downgraded the package: conda install imageio=2.19.0.\n(2023-05-14)\nBlas GEMM launch failed Matrices cannot be multiplied at the 1st layer, but the input data and the coarse model both are correct.\n1 2 def ret(inputs): # inputs:(N_rays*N_samples, 90); ret: (n_inputs, 4) return tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0) Error message:\n1 tensorflow.python.eager.core._NotOkStatusException: InternalError: Blas GEMM launch failed : a.shape=(65536, 63), b.shape=(63, 256), m=65536, n=256, k=63 [Op:MatMul] Reboot doesn\u0026rsquo;t work. (Verified: nothing to with imagemagick.)\nnvidia-smi doesn\u0026rsquo;t show another process (notebooks, pycharm) using GPU besides Xorg and gnome-shell.\nNvidia suggested 30 series card to use CUDA 11.2 or newer. Error Internal: Blas GEMM launch failed 30系显卡不兼容?-知乎; Problems Training on RTX3080 - DeepSpeech - Mozilla Discourse\nSolution is using tf1.15 maintained by Nvidia Dr.Donald-2020-12-09 ,referenced by this post Solved: Error with Tensorflow \u0026amp; GPU - Dataiku community And this package requires Python 3.8, but nerf is using 3.7, so there is an error: error: subprocess-exited-with-error, when installing pip install --user nvidia-tensorflow[horovod] issue#15 However, if I directly downgrade conda install python=3.8, there will be too many conflict error. Also modifying environment.yml and creating based on that doesn\u0026rsquo;t work neither.\nBase on the answer on SO, I create a new env:\n1 2 3 4 5 6 conda create -n nerf-nvtf python=3.8 conda activate nerf-nvtf pip install --user nvidia-pyindex # conda install -c conda-forge openmpi # for multi-GPU # export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/anaconda3/envs/nerf/lib/ pip install --user nvidia-tensorflow[horovod] Then I tried import tensorflow works. But once I installed other packages:\n1 conda install numpy matplotlib imageio imageio-ffmpeg configargparse imagemagick Then import tensorflow cannot find the module.\nIs this problem caused by the following modificaitons?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 The following packages will be REMOVED: libgomp-11.2.0-h1234567_1 The following packages will be UPDATED: libgcc-ng anaconda/pkgs/main::libgcc-ng-11.2.0-~ --\u0026gt; anaconda/cloud/conda-forge::libgcc-ng-12.2.0-h65d4601_19 libstdcxx-ng anaconda/pkgs/main::libstdcxx-ng-11.2~ --\u0026gt; anaconda/cloud/conda-forge::libstdcxx-ng-12.2.0-h46fd767_19 zlib anaconda/pkgs/main::zlib-1.2.13-h5eee~ --\u0026gt; anaconda/cloud/conda-forge::zlib-1.2.13-h166bdaf_4 The following packages will be SUPERSEDED by a higher-priority channel: _libgcc_mutex anaconda/pkgs/main::_libgcc_mutex-0.1~ --\u0026gt; anaconda/cloud/conda-forge::_libgcc_mutex-0.1-conda_forge _openmp_mutex anaconda/pkgs/main::_openmp_mutex-5.1~ --\u0026gt; anaconda/cloud/conda-forge::_openmp_mutex-4.5-2_kmp_llvm python anaconda/pkgs/main::python-3.8.16-h7a~ --\u0026gt; anaconda/cloud/conda-forge::python-3.8.16-0_73_pypy I tried to remove those packages, but there\u0026rsquo;s a long waitting.\nSo I create a new environment:\n1 2 3 4 5 conda create -n nerf-nvtf-1.15 python=3.8 conda activate nerf-nvtf-1.15 pip install --user nvidia-pyindex pip install --user nvidia-tensorflow[horovod] pip install \u0026#39;imageio==2.19.0\u0026#39; configargparse Then the code can run normally.\nLego ply (2024-04-03)\nThe points3d.ply opened in Meshlab is a cube.\n","date":"2022-05-17T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-nerf_code_notes/","title":"read: NeRF | Code"},{"content":"(2022-05-08)\nCondense math expression Homogeneous coord 可以把 常数项 和 除法 引入矩阵运算，是为了把平移（加常数）和透视投影（除深度）写到一个矩阵中。 矩阵乘法就是先乘再加，当齐次坐标为1，用于添加常数项； 当齐次坐标不为1，可以作为系数被除掉，得到归一的xyz。\nRefer to 探秘三维透视投影-齐次坐标的妙用 -奇乐编程学院 bilibili (2023-12-20) 使用齐次坐标，则非线性的透视投影可以写成线性的运算。\nA linear operation T() satisfies additivity $T(a+b) = T(a) + T(b)$ and homogeneity $T(ca) = c T(a)$, where c is a scalar.\nHowever, perspective projection 𝐏 for representing a 3D scene on a 2D plane requires x, y divided by depth: x/d, y/d. Intuitively, use scaling to indicate depth. Perspective projection will alter the shape, such as parallel lines are no longer parallel:\nParallel lines aren\u0026rsquo;t parallel after projection.\nSimilarly, the 2D projection of a 3D ellipsoid may not an ellipse with both ends of equal size, but a tapered oval, resembling the outline of an egg.\nEllipse vs oval\nPerspective projection of a 3D Gaussian does not result in a 2D Gaussian. \u0026ndash; Mathematical Supplement for the gsplat Library\nThus, 𝐏 is not linear.\nBut if we bypass the division by considering x, y are already divided by d, forming 2D plane coordiantes (x/d, y/d), and then multiplying by d again, the 2D coordinates (x/d, y/d) need to append an additional dimension, i.e., (x/d, y/d, 1), to record the multiplier d.\nSo, the 2D plane coordinates (x/d, y/d, 1) becomes (x,y,d) after multiplying with d.\n$$(u,v) = (u,v,1) = (x/d, y/d, 1) = (x, y, d)$$\nNote: Appending 1 is not trying to revert a 2D pixel to 3D space, but used to represent the one more operation for the 2D coordinates. So it still represents a 2D pixel after appending 1.\nThat means, during the intermediate computation, there is no need to calculate (x/d, v/d) to obtain (u,v) , but use (x, y, d) to refer a 2D plane coordiantes.\nIn summary, the division is skipped, and 𝐏 becomes a linear operation represented with a 3x3 matrix.\nWhen moving in 3D space, the 3D coordiantes (x,y,z) needs an extra dimension as (x,y,z,1) to combine rotation and translation together. And [R|t] is a linear operation.\n(2024-02-15) The projection matrix also applies to a 4D camera point (x,y,z,1), where the homogeneous coordinate 1 will store the z, after the camera point got multiplied by the projection matrix, where the 4-th row is [0 0 -1 0].\nAs the resulting clip coordinates are not the final transformation yet in the pipeline, the original depth $z$ in camera space requires to be recorded for the perspective division that is supposed to be the final step.\nAfter frustum clipping and the clip coordinates perform perspective division, ND coordinates are obtained and utilized in the ND space for image formation.\n(2024-01-01) Summarize again\nIn the perspective projection, homogeneous coordinates use 3D coordinates to represent a 2D pixel for temporarily storing the depth value, which will be divided at the very end to keep the intermediate computations as linear operations.\nMoreover, in the view transformation, homogeneous coordinates uses 4D coordinates to represent a 3D point for holding the translation vector.\n(2022-08-16)\nDistinguish point and vector 齐次坐标用于区分 向量 和 点。\u0026lsquo;向量\u0026rsquo;只需基向量的线性组合，而\u0026rsquo;点\u0026rsquo;需要加上原点，把\u0026rsquo;点\u0026rsquo;表示为从原点出发的向量。 给定一组基向量𝐱,𝐲,𝐳，则一个向量𝐯 = a𝐱+b𝐲+c𝐳；而一个点 𝐩 = 𝐨+a𝐱+b𝐲+c𝐳，其中𝐨是原点。所以(a,b,c,0)是向量𝐯的坐标，而(a,b,c,1) 是点𝐩的坐标。\n平移变换需要使用齐次坐标，是因为只有‘点’需要平移，要想表示点就得用齐次坐标。而向量没有位置的概念，只有大小和方向\nRefer to 深入探索透视投影变换 - popy007 -CSDN (2024-02-17) Tutorial 3 : Matrices - opengl-tutorial.org\n(x,y,z,w=1) is a position, while (x,y,z,w=0) is a direction.\n(2023-02-13)\nShow 3D world on a plane When the homogeneous coordinate w=1 is appended behind the Cartesian coordinates (u,v), the result (u,v,w=1) becomes the 3D point (x,y,depth) because u=x/depth, v=y/depth.\n2D world on plane\n3D world on plane\nFor example, the railroad tracks are parallel on the 2D ground plane. But when they\u0026rsquo;re observed in a (higher-dimension 3D) projective space (human eyes, camera, convex lens), the parallel lines would converge. Otherwise, if our eyes are plane mirror, we will never find the world is 3D.\nThis effect can be interpreted as that the coordinates (x,y) scale down as 3D points get further away. Hence, drawing a railroad onto canvas should follow th relationship: (x/depth, y/depth), where x,y are constants and the depth increases.\n2D plane can only represent 2 directions, so if we want to display 3D world on a 2D plane, the additional dimension (depth) has to be engaged implicitly.\nTherefore, the meaning of pixel (u,v) on plane is (x/depth, y/depth), which corresponds to the 3D point (x,y,depth), such that the picture mimics the scene looked at by human: x,y are inversely proportional to depth (Big near, small far: perspective).\nThe homogeneous coordinate w=1 is used to accommodate the depth: (u = x/depth, v = y/depth, w=1) ⇒ (x, y, depth).\nThe w is not specified arbitrarily. If the given 2D coordinates are (u,v), then the w should =1, waiting for the depth split from u,v.\nTherefore, an extra dimension is supplimented to adapt the observation from higher-dimension space.\nThen, the homogenous coordinates of a pixel (u,v) on the plane is (u,v,w). When analyzing it in 2D space, its coordinates are (u/w, v/w).\nFor example, two pixels represented by homogeneous coordinates are (1,2,1) and (1,2,0)\nThat is, the projection pixel (u,v) of a 3D point, when the point goes far away, the coordinates (u,v) are not constant but inversely changing with depth. This effect can be represented by an extra dimension to reflect the depth change.\n(2023-02-12)\nCompensate for Cartesian coord The homogeneous coordiante w is supplemented to adapting Cartesian coordiantes to represent projective space Homogeneous Coordinates - songho. (Cannot convince me)\nThe parallel lines should never intersect at infinity in Cartesian space (plane), but they have to converge in projective space (human eye/camera).\nTo use 2D planes to represent perspective, the homogeneous coordinate w is appended behind the Cartesian coordinates (x,y) of each point to adapt to the projective observation.\nThus, each point in projective space has 3 coordinates (x,y,w). Then, the 2D coordinates of each point are obtained by normalizing the 3rd dimension: (x/w, y/w, 1), such that two parallel lines would converge. In other words, it\u0026rsquo;s easier to analyze multiple points by scaling their w to 1.\nHomogeneous coordinate w is the auxiliary for the Cartesian space. Thus, the effect of depth can be represented on a plane (like projection).\nIf the point (1,2) from Cartesian space is combined with different w to make up the homogeneous coordinates (1,2,w), the corresponding 2D coordinates (1/w, 2/w) will form a line.\nIf the 3 coordinates change propotionally, like (1,2,3), (2,4,6), \u0026hellip; (n,2n,3n), these homogeneous coordinates corresponds to a common 2D coordinates (1/3, 2/3) on the plane. This means the homogeneous coordinates are scale invariant. Or inversely, a pixel on the plane corresponds to a line in the 3D space (homogeneous coordinates system).\nw is an attribute for each point in perspective space, where every point has 3 coordinates (x,y,w), while the points in Euclidean space don\u0026rsquo;t have this property.\nWhat we human perceived on our retina or captured on the camera plane are the projection: (x/w, y/w, 1).\nBecause each 3D point has differnt w, their projections are located on different position on the image plane. Thus,\nTherefore, given an image, the homogeneous coordinates for each pixel are (u/w, v/w, 1). If the w is known, then the homogeneous coordinates can be wrriten as (u, v, w).\nThe point in 3D space has the coordinates: (x,y,w) is divided by the w. homogeneous coordinates (x,y,w)\nA picture showing projective effect actually is a stack of different planes with different depth.\n(x/w, y/w), where the x,y are already divided by the dpeth w, so if we want to get the Cartisan coordiantes back, the w has to be separated: (x, y, w), then the first 2 number are 2D Cartesian coordinates.\nThat means the homogeneous coordiantes of 2D point (x,y) is just appending a w at the end, like (x,y,w).\nThe points with propotional homogeneous coordinates corresponds to the same 2D Cartesian point. For example, (1,2,3) and (2,4,6) With the extra dimenstion, the coordinates for a 2D pixel\nHomogeneous coordinates convert the non-homogeneous linear system to a homogeneous system.\nIf the homogeneous coordinate w added behind Cartesian coordinates is to represent the depth (x/w,y/w,1)ᵀ, then the homogeneous coordinate w=1 added behind a 3D points is to accommodate the translation (x/w, y/w, z/w, 1)ᵀ. Ref 深入探索透视投影变换(续) - popy007 -CSDN Homogeneous coordinates - Wikipedia (Back to top)\n","date":"2022-05-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/homogeneous_coord/","title":"memo: Vis | Homogeneous Coordinates"},{"content":"P15\nIrradiance The power per (perpendicular/projected) unit area incident on a surface point. $E(x)≡\\frac{dΦ}{dA} cos\\theta$ 单位面积上的能量(垂直与表面的分量) 光线在单位球面的能量与距离呈平方反比， ","date":"2022-05-02T09:24:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/15-light_transport_global_illumination/","title":"watch: CG - 闫令琪 15 | Ray-Training-3"},{"content":"P14\nSpatial Partitioning 对空间做划分 tree 八叉树Oct-Tree 把一个3维的包围盒切成8份(2^3) 高维空间就是2^n 叉树， KD-Tree 每次划分只在1个维度上划分(二叉树) n个维度依次循环被划分，保证空间上是均匀的 问题：包围盒是否包围三角面需要对包围盒与三角面求交，不好写; 一个物体与多个叶子节点相交，所以每个叶子节点中都有存储这个物体（冗余？），最好是一个物体只存在于一个格子中（基于物体划分）。 BSP-Tree 每次划分方向并不与坐标轴平行 计算量大 KD-Tree Preprocessing 对场景建立KD-Tree Traversing a KD-Tree 从最大的包围盒开始，判断光线是否与之有交点，如果有交点，分别对它的两个子节点判断是否与光线有交点，如果没交点就不判断其子节点。走到叶子节点，就对包围盒中的所有物体求交。 Object Partitions \u0026amp; Bounding Volume Hierarchy (BVH) 把三角形分成两组，再分别重新求它们的包围盒，知道一个盒子中最多含有5个三角形就停止划分\n一个物体只会存在于一个包围盒中,(各盒子会有重叠)\nHow to subdivide a node?\nChoose a dimension to split Heurstic #1: Always choose the longest axis in node (沿最长轴划分) Heurstic #2: Split node at location of median object (以中间物体的位置划分，两侧物体数量相等，平衡意味着最大深度小，搜索次数少) Data Structure for BVHs\nInternal nodes store: Bounding box and Pointers to its child nodes Leaf nodes store: Bounding box and Objects Nodes represent subset of primitives(基础元素) in scene. All objects in subtree 遍历方式与KD-Tree 相同：\n1 2 3 4 5 6 7 8 9 10 11 Intersect(Ray ray, BVH node) { if (ray misses node.bbox) return; if (node is a leaf node) test intersection with all objs; return closest intersection; hit1 = Intersect(ray, node.child1); hit2 = Intersect(ray, node.child2); return the closer of hit1, hit2; Radiometry Termiology Radiant energy: 电磁辐射的能量 Q Radiant flux: 单位时间的能量 Φ=dQ/dt [Watt]（功率：单位时间射出多少光子） Radiant intensity: 从光源发出的光 Irradiance: 表面接收的光 Radiance: 沿光线传播的光 Radiant intensity The radiant (luminous) intensity I is the power Φ per unit solid angle ω emitted by a point light source. $I(w) ≡ \\frac{dΦ}{dω}$ [candela]; $Φ = \\int_S² I dw = 4πI$ 某一方向上的能量 ","date":"2022-05-01T18:28:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/14-ray_tracing2/","title":"watch: CG - 闫令琪 14 | Ray-Tracing-2"},{"content":"P13\n判断点是否在多边形内部：以该点为起点沿任意方向做一条射线，与多边形的交点个数是奇数（相切情况认为是两个相同的实根？）可以推广至三维。\n夹板\n","date":"2022-05-01T14:24:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/13/","title":"watch: CG - 闫令琪 13"},{"content":"Multi-Layer Perceptron MLP consists of many layers of perceptrons. MLP 由多层感知机构成。 Perceptrons 只能解决线性二分类问题，是因为它采用的阶跃激活函数： $e(t)=\\{^{0,t\u0026lt;0}_{1,t\u0026gt;0}$，只能确定多维输入空间中的一个超平面_。\n比如在二维空间中，它就无法解决异或问题。要产生非线性（斜率不固定）的决策边界可以换用非线性的激活函数，sigmoid function, tanh 或者 ReLu等等。它们使得感知机可以表达曲线，也就是即使输入超过阈值一些，输出没有对应的变化，依然为0。此时输入输出不再是线性关系，而不像阶跃函数，超过阈值就输出1。引入非线性之后，多层感知机才可以逼近任意非线性的决策边界。否则，MLP就相当于多条直线加权相加，最终得到的还是一条直线4，只能给出0-1分类。(这里的多条直线并不是联立求方程组，可以限定一块区域，而是被感知机加权，最后还是一条直线: wn(\u0026hellip;(w2(w1+b1)+b2+\u0026hellip;+bn) ）。Example with cubic function.6\n另一方面非线性激活函数可以帮助自动调整权重。多层感知机会带来大量需要调节的权重 w 1，手动调节是不可能实现的。阶跃函数的纵轴是0或1，横轴是wx（激活值），w的微小变化会导致激活值发生变化，如果输出层采用阶跃函数，output可能会跳变（在零点处不可微），就无法直接求导找到输出与输入之间的关系2。为了量化微小变化Δw带给输出的偏差，需要把台阶铺开，也就是变成sigmoid函数。从此目标就是调整 w 使MLP输出与目标值之间的误差e不断降低。而w的变化方向应该是使误差下降最快的方向，也就是梯度方向，就需要e对w求导数（e 先对 sigmoid 求导，sigmoid 再对 w求导），再乘以步长就是 w 每次需要修正的量。\n同时，网络中其余的感知机也都要使用sigmoid函数，才能求得网络中所有w的导数。 因为误差e是前一层感知机输出Z的函数，比如采用最小二乘法，则 e = (σ(Z)-target)²，根据链式法则，需要对左侧一层的激活函数求导才能求得之前 w 的导数。如果它们仍采用阶跃激活函数，导数在非零处为零，但在零点处不可导3，就无法修正之前的w，也就无法完成误差的反向传播。\nMSE 对 σ 求导：((σ(z)-target)²)\u0026rsquo; = 2σ(z)， σ 对 z 求导：σ(z)\u0026rsquo; = σ(z)(1-σ(z))， z 对 w 求导：x（来自上一层的输出，或是网络Input）， 所以输出层的 w 的导数为：2σ(z)∗σ(z)(1-σ(z))∗x（简化为 δ∗x）\n要求左侧一层的 w₋₁ 的导数，根据 chain rule: 这层的z对 x (也就是 σ₋₁(z₋₁)) 的导数: w， x 对 z₋₁ 的导数：σ(z₋₁)(1-σ(z₋₁))， z₋₁ 对 w₋₁ 的导数：x₋₁， 所以e对左侧一层的 w₋₁ 的导数为：δ∗w∗σ(z₋₁)(1-σ(z₋₁))∗x₋₁（简化为 δ₋₁∗x₋₁）\n再往左的话，即 δ₋₂∗x₋₂ , \u0026hellip;\n对于输出层有多个神经元，误差是多个神经元输出之和，所以e求导的时候要兵分多路，e分别对每一路求导数，然后加起来。4\n(2022-12-25) 比如下图中，有两个输出神经元 y₁^, y₂^，求 e 对 w₁ 的梯度∂e/∂w₁，就是两个输出神经元分别对 w₁ 求梯度，再加起来。\n刘二那个反向传播的图，分两路：一路对x求导，一路对w求导。\n每个局部计算的导数，是在forward 构建计算图时就算好的，函数的梯度存储在各input节点上，而不是存在output节点上。\nsigmoid function Perceptron accepts several input signals and give one output, including 0 and 1. Each input times its corresponding weight and is added up. If the weighted summation is larger than threshold, the output is 1. Otherwise, output is 0.\n深度学习一定要有偏置项Bias吗？外加一段简单的神经网络反向传播BP算法手写推导～ 一般要有 bias，因为线性函数 ax+b，b让模型在竖轴上移动，它在线性模型中是需要的。但在深度学习神经网络中，并不是每个地方都要设置 bias。 在清华大学的 cpm 项目中，认为在一些地方不设置 bias，使得模型训练稳定性会更强，计算速度以及显存消耗有优势\n参考 1. Lecture 10 - Neural Networks -Caltech (Learning from data)-ytb 2. NNDL Sigmoid Neurons 3. PyTorch: Introduction to Neural Network — Feedforward / MLP 4. 神经网络激活函数的作用和原理？有没有形象解释？ - 颜沁睿的回答 - 知乎 5. How Does Backpropagation Work? - kasperfred Why Neural Networks can learn (almost) anything - Emergent Garden - ytb ","date":"2022-02-14T23:10:00Z","permalink":"https://zichen34.github.io/writenotes/calc/dl_mlp_nonlinearity/","title":"memo: MLP Nonlinearity"},{"content":"一级标题 二级标题 正文\nbold text （设置快捷健 ,b\nitalic text (斜体快捷键 ,i)\nbold italic\nwasted text (删除线 ,s)\n3个` 括起来是代码块(1左边那个键)\n加上python会显示对应的高亮\n1 print(\u0026#34;hello\u0026#34;) [Named Link](http://www.baidu.com/\"Named link title\u0026quot;)\nhttp://www.baidu.com/\nhttp://example.com/\nheading-1 (转到一级标题处）\nFirst Header second Header Content Cell Content Cell content Cell content cell 一行代码 用` `括起来\ncode\nBullet List Item1 (*号和文字之间有空格） Nested bullet Sub_nested bullet etc Sub_nested sub_nested Bullet List item 2 1.A number list 1. A nested numbered list 2, which is numbered 2.which is numberd\n[] An uncompleted task ( - 和 [ 之间有空格） A complete task [] A subtask 引用块\n子引用块\n\u0026mdash; 水平线\nTitle 1 折叠内容Content 1 Content 1 Content 1 Content 1 Content 1\nF\n多行公式独立编号：(来源：Markdown下LaTeX公式、编号、对齐) $$ \\begin{eqnarray*} x^n+y^n \u0026amp;=\u0026amp; z^n \\tag{1.4} \\ x+y \u0026amp;=\u0026amp; z \\tag{1.5} \\end{eqnarray*} $$\n单个公式换行:\n单个公式很长的时候需要换行，但仅允许生成一个编号时，可以用split标签包围公式代码，在需要转行的地方使用\\\\，每行需要使用1个\u0026amp;来标识对齐的位置，结束后可使用\\tag{...}标签编号。 $$ \\begin{split}a \u0026amp;= b \\c \u0026amp;= d \\e \u0026amp;= f \\end{split}\\tag{1.3} $$\n字母头上 ^ 1 $\\^θ$ 用 vim 插件 MarkdownPreview 在浏览器中可以正确渲染这个公式， 但是 Hugo 的KaTex 需要多打一个back slash：\\\\^θ 才能正确渲染，但这样做 vim 插件里就渲染不对了。\n单行写分段函数 For katex, there is one more \\\n1 \\\\{^{x=0}\\_{x=1} 交换图表 MathJax基础（10）：Commutative diagrams\n$$ $\\require{AMScd}$ \\begin{CD} A @\u0026gt;a\u0026raquo; B\\ @V b V V= @VV c V\\ C @\u0026raquo;d\u0026gt; D \\end{CD} $$\nmermaid style %% node style classDef lyrs fill:#ff9 class lyr11,act1,lyr12,lyr21,act2,lyr22 lyrs; %% link linkStyle 9,10,11,12,16,17,18,19 stroke:#0af,stroke-width:3px %% fontsize classDef node font-size:20px; Anchor Uppercase -\u0026gt; lowercase; Spaces -\u0026gt; -\nGithub Markdown anchor only linking to top of the page\nImages side by side 1 2 3 4 5 \u0026lt;table\u0026gt;\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/DOG.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px;\u0026#34;\u0026gt;是因为边界没有极值？所以只用中间层\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/DOG_levels.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px\u0026#34;\u0026gt;图2.DOG金字塔的实际显示效果\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;./pictures/SIFT_Features.png\u0026#34;\u0026gt;\u0026lt;center style=\u0026#34;font-size:14px\u0026#34;\u0026gt;图3.DOG图像归一化结果\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/table\u0026gt; Common Unicode U+2016\t‖\tDOUBLE VERTICAL LINE\nWhite square: □ U+25A1 White Square Unicode Character - Compart\nThere is no superscript comma in unicode. Superscripting any Letter/Number for comma\nleft ceil, left floor, right ceil, right floor:\n⌈x₀⌉ |x₁| ⌊x₂⌋\ncircle:\n⦿\nClipboard of symbl.cc\nεφμf𝐱⋱⋮⋯⌊⌋σ√↓↑↓▼⬇λ˙⋅□↑⇔γβ∈α∑×‖⑊ ⊠∂Λθ∭∞°Σ➔©→✅⬯⬭τδ⊗∫𝚺𝛍𝐔𝐈𝐮∈𝐱𝐌𝐖𝚫ℤω℮∏‖⊙⊛ηρ𝐝𝐜𝐕𝒢𝐭𝐉𝐦𝛈≈❙❘⇤↳⋮⋆⭑✦★•𝑳𝐿𝐲𝐏𝐩𝐪𝐚𝐛𝐦𝐧𝐊𝐑𝐭𝐀𝐁𝐐𝐂𝐇𝐅𝐕|⏐▭✚▱𝐘𝐗𝐳𝐃𝐄⬯˚∘Δ😊≤▱■⌷⍨⍩⍤⟨〈⟩⬮↘∇𝛁 ▦\n表格列宽 公式 箭头指示 DDG search: \u0026ldquo;latex math equation block where a symbol is pointed by an upward arrow?\u0026rdquo;\nArrow to an equal symbol in a equation to justify it - SE\n$$ a+b\\underset{\\substack{\\color{red}\\uparrow \\\\ % or: \\rotatebox{90}{$\\longrightarrow$} \\mathclap{\\textup{\\tiny commutative}} \\\\ \\mathclap{\\textup{\\tiny property}}}}{=}b+a $$\n符号加方框 $$ \\boxed{x} $$\n","date":"2022-02-12T20:33:00Z","permalink":"https://zichen34.github.io/writenotes/lang/markdown_misc/","title":"memo: Markdown"},{"content":"Arxiv\nSummary (2022-01-08)\nThe input and output are the same as a volumetric model, but the loss function is based on the renderer error. A volumetric model maps something to the attributes of every volumetric grid. The output of a model is determined by the loss function. In NeRF, the loss is the rendering error between ground truth RGB image and the rendered image that is generated according to density (alpha channel) and RGB color at each point on the path of ray 1. Therefore, the output of the network should be a filed of density and color. Overview diagram\nOptimize Tricks Positional encoding: mapping the inputs to a higher dimensional space so that MLP could converge to a high-frequency function.\n(2023-10-27) PE is Fourier transform and then MLP re-composes Fourier coefficicent to the final value.\nwill. - QQ group:\n傅里叶级数是对三角函数项的线性加权；神经网络的线性层是对输入的线性加权。 density 网络就是用傅里叶级数拟合了 sigma 在三维空间中的分布 用优化器优化了傅里叶级数，然后通过体渲染约束体密度的性质。\n赤子泛舟：\n我理解也是等效做了一次傅里叶变换和反变换，mlp其实在拟合频域特征。 虽然声称没用卷积，但其实整个过程相当于做了个大的卷积。\n群除我佬：就是一个更吊的3d输入。\nHierarchical volume sampling: allocating more samples to where it is expected to have a significant contribution on the final rendering.\nviewdir is represented by Cartesian (x,y,z), rather than spherical coordinates (φ,θ,r)\nσ is output of linear layer, so it lies in (-∞,+∞); but volume density is [0~∞), so σ(x) is activated by ReLU. Then it converts to alpha (the probability of terminating) by $(1-e^{-σ⋅δ})$, that is when σ=0, alpha=0 meaning no volume no block, while if σ→∞, the alpha→1 meaning this volume block light entirely. Therefore, the transmittance is (1-alpha)=$e^{-σδ}$, so the decay coefficient (cumulative transimitance) of a RGB color is $Π^{i-1}e^{-σδ}$\nMath derivation 2.3 Integration along Ray Segmetns\nSecond sampling is based on \u0026ldquo;color decay coeff (weights)\u0026rdquo;, rather than volume density. The bigger color weights means more volume here. σ(x) is proportional to alpha and weights\nCode ImportError: No module named \u0026lsquo;_pywrap_tensorflow_internal\u0026rsquo;\nCreate environment: conda env create -f environment.yml, and this error occured when executing import tensorflow.\nSolution: I didn\u0026rsquo;t creat the envrionment from the yml file but directly specify packages and version. conda create -n nerf python=3.7 cudatoolkit=10.0 tensorflow-gpu=1.15 -c conda-forge\nReference 1. Ray tracing volume densities ","date":"2022-01-08T00:00:00Z","image":"https://github.com/bmild/nerf/raw/master/imgs/pipeline.jpg","permalink":"https://zichen34.github.io/writenotes/model/nerfs/b-note-nerf_bmild/","title":"read: NeRF"},{"content":"title: \u0026ldquo;3D Content Creation and Stylization with AI\u0026rdquo;\nSource link: GAMES Webinar 215-视觉专题：智能三维内容生成\u0026ndash;重建与创造-Kangxue Yin\n3D内容创作： 游戏，电影特效，动画，VR/AR，专业化程度高（建模软件）\n动画电影创作流程:\n剧本 原画 Layout: 物体相机摆放，位置关系 特效技术研发 2D 建模为3D 纹理 绑定骨骼，皮肤，相对运动关系 人物运动（运动捕捉） VFX 特效模拟（爆炸） 打光 渲染(物理仿真) 梦工厂pipeline：https://www.youtube.com/watch?v=ru0tQRJ4qKs\n3D 内容成本高：2009 阿凡达（3D） $ 2.37亿 ，而 2010年的2D电影 让子弹飞 只有 $ 0.18亿\n元宇宙需要大量3D 内容\n降低3D内容制作成本：让普通玩家参与创作（房子，汽车\u0026hellip;）NFT，未经训练，细节不足，风格不够多样，用人工智能辅助。\nResearch works Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape\n输入 voxel 模型，网络合成细节。\n3D represention 选择\nDiscrete Repre\nImplicit Fields:\n生成网络细节不足，转换为mesh有问题\ndifferentiable iso-surfacing 把隐式方程转化为一个mesh，然后用 mesh 和 ground truth 之间的差（loss）来优化网络，降低 iso-surfacing 离散化带来的误差，其中使用的不是 Marching cube 而是 Marching Tetrahedra\n3DStyleNet: Creating\n3D 物体的风格迁移\n","date":"2022-01-07T22:22:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/215-%E8%A7%86%E8%A7%89%E4%B8%93%E9%A2%98%E6%99%BA%E8%83%BD%E4%B8%89%E7%BB%B4%E5%86%85%E5%AE%B9%E7%94%9F%E6%88%90--%E9%87%8D%E5%BB%BA%E4%B8%8E%E5%88%9B%E9%80%A0-kangxue-yin/","title":"watch: 215-智能三维内容生成"},{"content":"6-如何理解“梯度下降法”？什么是“反向传播”？通过一个视频，一步一步全部搞明白\n正向传播 输入数据沿着神经网络正向传递 输入数据与各个感知机的w和b点乘，将结果代入激活函数，给出判断结果 反向传播 把判断结果的偏差传递给各个w和b，根据参数对偏差贡献的大小，成比例的调整 未训练好的神经网络的$\\mathbf w,b$不准确，导致判断结果有偏差。如果某 $\\mathbf w/b$ 对最终的判断结果有重大影响，则该参数对于偏差也是有重大影响的。所以在减小误差的过程中，应优先调整d对偏差有重大影响的参数。 脑补过程：\n神经网络输出层的输出值是 $a^{[3]}$:\n$$ \\mathbf a^{[3]} = \\sigma(\\mathbf w^{[3]} \\cdot \\mathbf a^{[2]} + b^{[3]}) $$\n其中 $\\sigma$ 是激活函数，$\\mathbf w^{[3]}$输出层权重，$\\mathbf a^{[2]}$上一层的输出值，$b^{[3]}$是输出层偏置系数。\n可以计算损失函数（交叉熵），得到偏差J：\n$$ J = \\frac{1}{n} \\left( -∑_{i=1}^n (y^{(i)}) ⋅ log_2 a^{[3](i)} + (1-y^{(i)} ⋅ log_2(1-a^{[3](i)}) )\\right) $$\n其中 $y_i$ 是理想系统中输出y的概率；$log₂ a^{[3](i)}$是判断结果的信息量。\n偏差直接来自输出层的感知机:\n偏差来自三部分：当前层的 w 和 b，以及上一层的输出 $a^{[2]}$。其中 w 和 b 可以根据占比直接调整，而 $a^{[2]}$ 的偏差来自于上一层，按照贡献大小分配偏差：\n然后前一层的感知机占有的偏差又可分成3部分，调整 $w, b$ 和 上一层的输出.\n第一隐藏层的感知机的偏差与第二隐藏层所有感知机相关：\n由此，整个神经网络中的每个 $w$ 和 b 都能分配到偏差占比。\n以上利用的是数值的加法分配偏差，还可使用向量的加法来分配偏差，不过首先要确定向量的方向。\n梯度的反方向就是要找的向量方向：数值减小最快的方向。 梯度可以在i方向和j方向上分解，对于点(x,y)沿变化率最大的方向变化就是在i和j方向上同步变化。\n对(输出层)损失函数 J 求梯度：\n$$ \\begin{aligned} \u0026amp;\\nabla J (\\mathbf w^{[3]}, a^{[2]}, b^{[3]}) \\\\ \u0026amp; = (\\alpha, \\beta, \\gamma) \u0026amp; \\text{三个分量的系数，简略表示} \\\\ \u0026amp; = \\alpha \\cdot \\mathbf i + \\beta \\cdot \\mathbf j + \\gamma \\cdot \\mathbf k \\end{aligned} $$\n输出层的输出沿梯度方向变化$\\eta$步长，向目标值靠近：\n$$ \\begin{aligned} \\mathbf w^{[3]}{(\\rm target)} \u0026amp;= \\mathbf w^{[3]} - \\eta \\cdot \\alpha \\ b^{[3]}{(\\rm target)} \u0026amp;= b^{[3]} - \\eta \\cdot \\gamma \\ \\ a^{[2]}{(\\rm target)} \u0026amp;= a^{[2]} - \\eta \\cdot \\beta \u0026amp; 需要反向传递分配到第2隐藏层上 \\ \\cancel{\\eta \\cdot}\\ \\beta \u0026amp;= a^{[2]}{(\\rm target)} - a^{[2]}_{(\\rm now)} \\end{aligned} $$\n感知机输出值a的中间传递不考虑 $\\eta$，偏差最终分配在输入层的w和b上，其中含有$\\eta$。从而可以看出隐藏层与输出层类似，也是目标与输出之间的差值。因此对于第2隐藏层的\u0026quot;损失函数\u0026quot;： $J^{[2]} = a^{[2]}{(\\rm target)} - a^{[2]}{(\\rm now)}$，$J^{[2]} (\\mathbf w^{[3]}, a^{[2]}, b^{[3]})$ 开启下一轮：求梯度,求损失函数\n各层的运算形式相同（损失函数），就可以迭代分配偏差。J沿梯度方向变化就可以最快的减小偏差，因此使用了向量的加法做偏差分配。\n梯度 求偏导就是求（固定另一个维度）曲线的切线，两个偏导组合就是两条切线组合，两条切线确定了切面。所以对曲面求偏导，就是在求切面 把两个切线合成一个向量，就是梯度 $\\alpha, \\beta,\\gamma$ 的具体表示（求偏导）：\n$$ \\begin{aligned} \\alpha \u0026amp;= \\frac{\\partial J}{\\partial \\mathbf w^{[3]}} \\ \\gamma \u0026amp;= \\frac{\\partial J}{\\partial b^{[3]}} \\ \\beta \u0026amp;= \\frac{\\partial J}{\\partial \\mathbf a^{[2]}} \\end{aligned} $$\n分配偏差：\n$$ \\begin{aligned} \\mathbf w^{[3]} \u0026amp;= \\mathbf w^{[3]} - \\eta \\cdot \\frac{\\partial J}{\\partial \\mathbf w^{[3]}} \\ b^{[3]} \u0026amp;= b^{[3]} - \\eta \\cdot \\frac{\\partial J}{\\partial b^{[3]}} \\ \\frac{\\partial J}{\\partial \\mathbf a^{[2]}} \u0026amp;= a^{[2]}{(\\rm target)} - a^{[2]}{(\\rm now)} \u0026amp; \\text{“新损失函数”} \\end{aligned} $$\n令$J^{[2]}=a^{[2]}{(\\rm target)} - a^{[2]}{(\\rm now)}$ 作为下一轮的损失函数，对损失函数$J^{[2]}(\\mathbf w^{[2]}, \\mathbf a^{[1]}, b^{[2]}$)分配：\n$$ \\begin{aligned} \\mathbf w^{[2]} \u0026amp;= \\mathbf w^{[2]} - \\eta \\cdot \\frac{\\partial J^{[2]}}{\\partial \\mathbf w^{[2]}} \\ b^{[2]} \u0026amp;= b^{[2]} - \\eta \\cdot \\frac{\\partial J^{[2]}}{\\partial b^{[2]}} \\\n\\frac{1}{n} \\sum_{i=1}{n} \\frac{\\partial J^{[2]}}{\\partial \\mathbf a^{[1]}} \u0026amp;= a^{[1]}{(\\rm target)} - a^{[1]}{(\\rm now)} \u0026amp; \\text{“$a^{[1]}$的偏差是4个$a^{[2]}$的偏差之平均”} \\end{aligned} $$\n令 𝑱⁽¹⁾=a₍ₜₐᵣᵨₑₜ₎⁽¹⁾ - a₍ₙₒᵥ₎⁽¹⁾ 作为下一轮的损失函数，对损失函数$J^{[1]}(\\mathbf w^{[1]}, \\mathbf a^{[0]}, b^{[1]}$)分配，其中 $a^{[0]}$ 是输入，无法修改，只调整 w 和 b。\n对于第 $l$ 层的第 $i$ 个感知机，接受上一层所有神经元的输出 $a^{[l-1]}$，与它的 $\\mathbf w_i^{[l]}$ 和 $b_i^{[l]}$ 做线性运算得到 $z_{i}^{[l]}$，把 z 送入激活函数得到这个感知机的输出：$\\sigma(z_i^{[l]})=a_i^{[l]}$\n$$ \\begin{aligned} z_i^{[l]} \u0026amp;= \\mathbf w_i^{[l]} \\cdot \\mathbf a^{[l-1]} + b_i^{[l]} \\ z_i^{[l]} \u0026amp;= \\begin{bmatrix} \\mathbf w_{i,1}^{[l]} \\ \\vdots \\\\mathbf w_{i,n}^{[l]} \\end{bmatrix}^T \\cdot \\begin{bmatrix} a_{1}^{[l-1]} \\ \\vdots \\ a_{n}^{[l-1]} \\end{bmatrix} + b_i^{[l]} \\end{aligned} $$\n对一层感知机进行表述:\n没有下标 $i$ 表示整层，第$l$层各感知机的输出：\n$$ \\begin{aligned} \\mathbf z^{l} \u0026amp;= \\mathbf w^{l} \\cdot \\mathbf a^{l-1} + b^{[l]} \\ \\mathbf z^{l} \u0026amp;= \\begin{bmatrix} z_1^{[l]} \\ \\vdots \\z_i^{[l]} \\end{bmatrix} = \\begin{bmatrix} \\mathbf w_{1,1}^{[l]} \u0026amp; \\cdot \u0026amp; \\mathbf w_{1,j}^{[l]} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\cdots \\ \\mathbf w_{i,1}^{[l]} \u0026amp; \\cdot \u0026amp; \\mathbf w_{i,j}^{[l]} \\ \\end{bmatrix} \\cdot \\begin{bmatrix} a_1^{[l-1]} \\ \\vdots \\ a_j^{[l-1]} \\end{bmatrix} + \\begin{bmatrix} b_1^{[l]} \\ \\vdots \\ b_i^{[l]} \\end{bmatrix} \\\na^{[l]} \u0026amp;=\\sigma\\left(z^{[l]}\\right)=\\sigma\\left(\\left[\\begin{array}{c} z_{1}^{[l]} \\ \\vdots \\ z_{i}^{[l]} \\end{array}\\right]\\right)=\\left[\\begin{array}{c} \\sigma\\left(z_{1}^{[l]}\\right) \\ \\vdots \\ \\sigma\\left(z_{i}^{[l]}\\right) \\end{array}\\right]=\\left[\\begin{array}{c} a_{1}^{[l]} \\ \\vdots \\ a_{i}^{[l]} \\end{array}\\right]\n\\end{aligned} $$\n输出层的损失函数：$J(y,a^{[l]})$，y是label，(k)是第几个样本：\n每个样本$\\mathbf x$都有 j 个属性，对应第0层的输出 $a^{[0]}$：\n$$ a^{[l]}=\\sigma\\left(z^{[l]}\\right)=\\sigma\\left(\\left[\\begin{array}{c} z_{1}^{[l]} \\ \\vdots \\ z_{i}^{[l]} \\end{array}\\right]\\right)=\\left[\\begin{array}{c} \\sigma\\left(z_{1}^{[l]}\\right) \\ \\vdots \\ \\sigma\\left(z_{i}^{[l]}\\right) \\end{array}\\right]=\\left[\\begin{array}{c} a_{1}^{[l]} \\ \\vdots \\ a_{i}^{[l]} \\end{array}\\right] $$\n多分类问题，有多个输出 $aᵢ^{l}$，此时的损失函数是把所有输出节点都考虑进来。不考虑常量: 样本x和标签y，损失函数的输出层感知机的函数：\n反向传播：对J求梯度，给各变量分配偏差(偏导)，走$eta$步长\n把对输出层各感知机的偏导看作是第 $l+1$ 层，第 l+1 层只对 第l层的一个感知机有作用，$J_1^{[l+1]}$ 是 $\\mathbf w_1^{[l]},\\ a^{l-1},\\ b_1^{l}$ 的函数：\n最后$\\nabla J$的各项都从输出层 $a$ 开始求导（链式求导）：\n$$ \\nabla J_{1}^{[l+1]} = \\left( \\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial W_{1}^{[l]}}, \\quad \\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial a^{[l-1]}}, \\quad \\sqrt{\\frac{\\partial J}{\\partial a_{1}^{[l]}} \\frac{\\partial a_{1}^{[l]}}{\\partial \\sigma}} \\frac{\\partial \\sigma}{\\partial z_{1}^{[l]}} \\frac{\\partial z_{1}^{[l]}}{\\partial b_{1}^{[l]}} \\right) $$\n其中 $\\mathbf w$ 是向量，对其求偏导要对它的每个分量求偏导：\n$$ \\begin{array}{l} \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1,1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1,2}^{[l]}}, \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1, j}^{[l]}}\\right) \\ \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{1}^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{2}^{[l-1]}}, \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a_{j}^{[l-1]}}\\right) \\end{array} $$\n每层的 $\\mathbf w$ 和 $b$ 求偏导之后可直接修改：\n$$ \\begin{array}{c} W_{1}^{[l]}=W_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}} \\quad b_{1}^{[l]}=b_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}} \\ \\vdots \\ W_{i}^{[l]}=W_{i}^{[l]}-\\eta \\cdot \\frac{\\partial J_{i}^{[l+1]}}{\\partial W_{i}^{[l]}} \\quad b_{i}^{[l]}=b_{1}^{[l]}-\\eta \\cdot \\frac{\\partial J_{i}^{[l+1]}}{\\partial b_{i}^{[l]}} \\end{array} $$\n对 $a^{l-1}$ 求偏导得到的是第 $a^{[l-1]}$ 层的变化量，作为损失函数：\n$$ J_{1}^{[l]}=\\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}} \\quad \\cdots \\quad J_{i}^{[l]}=\\frac{\\partial Jᵢ^{[l+1]}}{\\partial a^{[l-1]}} $$\n3个窗口反向移动，做下一轮:\n以第2隐藏层为中心，进行分析：\n把 a 展开（对a的各分量求偏导）：\n$$ \\begin{array}{c} \\nabla J_{1}^{[l+1]}=\\left(\\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\left.\\partial J_{1}^{[l+1]}\\right]}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\vdots \\ \\nabla J_{k}^{[l+1]}=\\left(\\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial a^{[l-1]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\end{array} $$\n第l层的每个感知机的偏差由第l+1 层的所有偏差值赋予：\n然后第l层各参数的变化量：\n$$ \\left(\\Delta W_{1}^{[l]}, \\Delta a^{[l-1]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, \\Delta a^{[l-1]}, \\Delta b_{i}^{[l]}\\right) $$\n$\\delta a$ 作为下一轮的损失函数：\n$$ \\left(\\Delta W_{1}^{[l]}, J_{1}^{[l]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, J_{i}^{[l]} \\quad, \\Delta b_{i}^{[l]}\\right) $$\n第2次迭代：\n$a^{[0]}$ 是常量输入，求导为零，此时的$J^{l-1}$只与 $\\mathbf w$ 和 $b$ 有关:\n$$ \\begin{array}{c} \\nabla J_{1}^{[l+1]}= \\left( \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{1}^{[l]}},\\ \\ldots, \\frac{\\partial J_{1}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{1}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\vdots \\\n\\nabla J_{k}^{[l+1]}= \\left( \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{1}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{1}^{[l]}}, \\ldots, \\frac{\\partial J_{k}^{[l+1]}}{\\partial W_{i}^{[l]}}, \\frac{\\partial J_{k}^{[l+1]}}{\\partial b_{i}^{[l]}}\\right) \\ \\ \\left( \\Delta W_{1}^{[l]}, \\Delta b_{1}^{[l]}, \\ldots, \\Delta W_{i}^{[l]}, \\Delta b_{i}^{[l]}\\right) \\end{array} $$\n","date":"2022-01-04T23:29:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/06_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/","title":"watch: DL - 王木头 06 | Gradient Descent"},{"content":"3 线性判别分析 (Fisher判别分析) - 模型定义 Source Video-P3\nData:\n$$ \\begin{aligned} \\text{Samples:}\\ \u0026amp; \\mathbf X = (\\mathbf x_1, \\mathbf x_2, \\cdots, \\mathbf x_N)^T_{N\\times p} = \\begin{pmatrix} \\mathbf x_1^T \\ \\mathbf x_2^T \\ \\vdots \\ \\mathbf x_N^T \\end{pmatrix}_{N \\times p} \\\\\n\\text{Samples:} \\ \u0026amp; \\mathbf Y = \\begin{pmatrix} y_1 \\ y_2 \\ \\vdots \\ y_N \\end{pmatrix}_{N \\times 1} \\\\\n\\text{Abbreviations:}\\ \u0026amp; \\left{ (\\mathbf x_i, y_i) \\right}_{i=1}^N, \\ \\mathbf x_i \\in \\R^p, \\ \\underset{(\\text{2class: } C_1,C_2)}{y_i \\in {+1, -1}}\\\\\n\\text{2 Group:}\\ \u0026amp;\\mathbf x_{C_1} = { \\mathbf x_i | y_i=+1} ; \\quad \\mathbf x_{C_2} = { \\mathbf x_i | y_i=-1} \\\\\n\\text{Number:}\\ \u0026amp; |\\mathbf x_{C_1}| = N_1, \\ |\\mathbf x_{C_2}|=N_2, \\ N_1 + N_2 = N \\end{aligned} $$\nFisher 类内小，类间大\n把p维降到1维，投影到某一个轴上再做分类。\n在投影方向($\\mathbf w$)上，类内点的坐标方差足够小，类间距离要大\n投影方向就是分类超平面（$\\mathbf w^T \\mathbf x$）的法向量。\n记号\n限定 $| \\mathbf w | =1$\n样本点$\\mathbf x_i$ 在投影轴$\\mathbf w$上的投影长度: $z_i = \\mathbf w^T \\mathbf x_i$\n$$ \\begin{aligned}\n均值: \\overline{z} =\u0026amp; \\frac{1}{N} \\sum_{i=1}^N z_i = \\frac{1}{N} \\sum_{i=1}^N \\mathbf w^T \\mathbf x_i \\\n协方差矩阵: S_{z} =\u0026amp; \\frac{1}{N} \\sum_{i=1}^N (z-\\overline z)(z-\\overline z)^T \\\n=\u0026amp; \\frac{1}{N} \\sum_{i=1}^N (\\mathbf w^T \\mathbf x_i-\\overline z)(\\mathbf w^T \\mathbf x_i-\\overline z)^T \\\n类C_1:\\ \\overline{z_1} =\u0026amp; \\frac{1}{N_1} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\\nS_{z_1} =\u0026amp;\\frac{1}{N_1} \\sum_{i=1}^{N_1} (\\mathbf w^T \\mathbf x_i-\\overline{z_1})(\\mathbf w^T \\mathbf x_i-\\overline{z_1})^T \\\\ 类C_2:\\ \\overline{z_2} =\u0026amp; \\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\\nS_{z_2} =\u0026amp; \\frac{1}{N_2} \\sum_{i=1}^{N_2} (\\mathbf w^T \\mathbf x_i-\\overline{z_2})(\\mathbf w^T \\mathbf x_i-\\overline{z_2})^T \\\\ 类内距离: \u0026amp; S_{z_1} + S_{z_2} \\ 类间距离: \u0026amp; (\\overline{z_1} - \\overline{z_2})^2 \\\n目标函数: \u0026amp; J(\\mathbf w) = \\frac{(\\overline{z_1} - \\overline{z_2})^2}{S_{z_1} + S_{z_2}} \\\n分子：\u0026amp; \\left[ \\frac{1}{N_1} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i-\\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf w^T \\mathbf x_i \\right]^2 \\\n=\u0026amp; \\left[ \\mathbf w^T \\left( \\frac{1}{N_1} \\sum_{i=1}^{N_1}\\mathbf x_i-\\frac{1}{N_2} \\sum_{i=1}^{N_1} \\mathbf x_i \\right) \\right]^2\\\\ =\u0026amp; \\left[ \\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) \\right]^2\\\\ =\u0026amp; \\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}})^T \\mathbf w^T \\\\ S_{z_1}=\u0026amp; \\frac{1}{N_1} \\sum_{i=1}^{N_1} (\\mathbf w^T \\mathbf x_i - \\frac{1}{N_1} \\sum_{j=1}^{N_1} \\mathbf w^T \\mathbf x_j)(\\mathbf w^T \\mathbf x_i - \\frac{1}{N_1} \\sum_{j=1}^{N_1} \\mathbf w^T \\mathbf x_j)^T \\\n=\u0026amp; \\frac{1}{N_1}\\sum_{i=1}^{N_1} \\mathbf w^T (\\mathbf x_i - \\overline{\\mathbf x_{C_1}}) (\\mathbf x_i - \\overline{\\mathbf x_{C_1}})^T \\mathbf w\\\\ =\u0026amp; \\mathbf w^T \\left[ \\frac{1}{N}\\sum_{i=1}^{N_1} (\\mathbf x_i - \\overline{\\mathbf x_{C_1}}) (\\mathbf x_i - \\overline{\\mathbf x_{C_1}})^T \\right] \\mathbf w \\\\ =\u0026amp; \\mathbf w^T S_{C_1} \\mathbf w \\\\ 分母：\u0026amp; \\mathbf w^T S_{C_1} \\mathbf w + \\mathbf w^T S_{C_2} \\mathbf w\\ =\u0026amp; \\mathbf w^T (S_{C_1} + S_{C_2}) \\mathbf w \\\nJ(\\mathbf w) =\u0026amp; \\frac{\\mathbf w^T (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}}) (\\overline{\\mathbf x_{C_1}}-\\overline{\\mathbf x_{C_2}})^T \\mathbf w^T} {\\mathbf w^T (S_{C_1} + S_{C_2}) \\mathbf w} \\ =\u0026amp; \\frac{\\mathbf w^T S_b \\mathbf w}{\\mathbf w^T S_{w} \\mathbf w} \\quad \\text{($S_b$: 类间方差; $S_w$: 类内方差)} \\ =\u0026amp; \\mathbf w^T S_b \\mathbf w \\cdot (\\mathbf w^T S_{w} \\mathbf w)^{-1}\n\\end{aligned} $$\n求: $\\hat{\\mathbf w} = \\underset{\\mathbf w}{\\operatorname{arg\\ max}}\\ J(\\mathbf w)$\n4 线性判别分析 (Fisher判别分析) - 模型求解 Video-P4\n$$ \\begin{aligned} \\frac{\\partial J(\\mathbf w)}{\\partial \\mathbf w} \u0026amp;= 0 \\\n2 S_b \\mathbf w (\\mathbf w^T S_w \\mathbf w)^{-1} - \\mathbf w^T S_b \\mathbf w (\\mathbf w^T S_w \\mathbf w)^{-2} \\cdot 2S_w \\mathbf w \u0026amp;= 0 \\\nS_b \\mathbf w (\\mathbf w^T S_w \\mathbf w) - \\mathbf w^T S_b \\mathbf w S_w \\mathbf w \u0026amp;= 0 \\\n\\underbrace{\\mathbf w^T}{1\\times p} \\underset{\\in \\R} {\\underbrace{S_b}{p\\times p} \\underbrace{\\mathbf w}{p\\times 1}} S_w \\mathbf w \u0026amp;= S_b \\mathbf w \\underset{\\in \\R} {(\\underbrace{\\mathbf w^T}{1\\times p} \\underbrace{S_w}{p\\times p} \\underbrace{\\mathbf w}{p\\times 1})} \\\nS_w \\mathbf w \u0026amp;= \\frac{\\mathbf w^T S_w \\mathbf w}{\\mathbf w^T S_b \\mathbf w} S_b \\mathbf w \\\n\\mathbf w \u0026amp;= \\frac{\\mathbf w^T S_w \\mathbf w}{\\mathbf w^T S_b \\mathbf w} S_w^{-1} \\cdot S_b \\cdot \\mathbf w \\\n只关心方向：\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot S_b \\cdot \\mathbf w \\\n\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}}) \\underbrace{ (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})^T \\mathbf w }_{1\\times 1 \\in \\R} \\\n\\mathbf w \u0026amp; \\propto S_w^{-1} \\cdot (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})\n\\end{aligned} $$\n如果$S_w^-1$ 是对角矩阵，而且是各向同性，则 $S_w^{-1} \\propto 单位矩阵I$，所以 $\\mathbf w \\propto (\\overline{\\mathbf x_{C_1}} - \\overline{\\mathbf x_{C_2}})$\n5 逻辑回归 (Logistic Regression) Video-P5\n硬输出：0 和 1；或者 +-1\n软输出：概率\n9 朴素贝叶斯分类器 (Naive Bayes Classifer) Video-P9\n核心思想：朴素贝叶斯假设，又叫条件独立性假设，最简单的概率有向图模型\n在给定类别的情况下，属性（维度）之间是相互独立的\n随机变量 $y$ 是随机变量，对应 p 维自变量\n从概率图角度来看，在给定 y 的情况下，从 $x_1$ 到 $x_2$ 的路径被 y 阻断了，所以 $x_1$ 和 $x_2$ 独立。\n概率表达式：\n$$ P(\\mathbf x | y) = \\prod_{j=1}^p P(x_i | y) $$\n假设的动机：为了简化运算， 对于 $\\mathbf x = (x_1, x_2, \\cdots x_p)^T$，忽略了 $x_i$ 与 $x_j$ 之间的关系，如果p非常大，导致计算困难。\n$$ P(y|x) = \\frac{P(x,y)}{P(x)} = \\frac{P(y)\\cdot P(x|y)}{P(x)} \\propto P(y) \\cdot P(x | y) $$\n分类数据：${(x_i, y_i)}_{i=1}^N, \\ x_i \\in \\R^p, \\ y_i \\in {0,1}$，对于一个给定的 x，对它分类：\n$$ \\begin{aligned} \\hat y =\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ \\underset{后验}{\\underline{P{(y|x)}}} \\\n=\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ \\frac{P(x,y)}{P(x)}\\\\ =\u0026amp; \\underset{y}{\\operatorname{arg\\ max}}\\ P(y) \\cdot P(x|y) \\end{aligned} $$\n","date":"2021-12-24T13:58:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/04_%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/","title":"watch: ML - 白板 04 | Linear Classification"},{"content":"Convolution 对像素重新计数，并计算新的“像素值”的过程\n卷积核从左上角开始，每次向左或向下滑动，并与其重叠的部分做内积（对应项相乘再求和）\n提取特征\n不做填充（valid padding），卷积后的输出尺寸为 $\\lfloor\\frac{n-k}{s}\\rfloor+1$\n图像尺寸：n×n\n卷积核尺寸：k×k\n步长：s\n卷积核从左上角开始，每次向左滑动一列，最后停靠在右边缘，这时卷积核左侧的像素数再加上1（当前次），就是输出的尺寸 n-k+1。\n比如下图一行有5个像素，k=2，卷积核前面有3个再加上最后1个: 3+1 =4。\n如果步长s=2，不能正好滑到最后，可以丢掉多余的部分或者填充像素。2\n如果步长s=3，计算式应为：$\\frac{n-k}{s}+1 = \\frac{5-2}{3}+1 =2$\n如果步长s=4，计算式应为：$\\lfloor\\frac{n-k}{s}\\rfloor+1 = \\lfloor\\frac{5-2}{4}\\rfloor+1 =1$\n对于 same padding, 输出尺寸：$\\lfloor \\frac{(n+2\\times p-k)}{s} \\rfloor+1$\n就是先对原始图像补充 p 圈像素，再做卷积。\nPadding 在图像外围填充一圈或几圈像素，像素值通常为0 保证输出与输入的尺寸一致。1 常见两种padding： valid padding: 不填充，只使用原始图像 same padding: 填充边缘，使卷积结果与输入尺寸一致。\n为了使输出尺寸仍等于n，即：$\\frac{n-k+2*p}{s}+1 = n$，解得：$p=\\frac{(n-1)*s+k-n}{2}$；如果s=1，则 $p=\\frac{k-1}{2}$。 Stride 卷积核滑动的步长 s stride=1，则卷积核每次向左滑动一列或者向下滑动一行 压缩信息：成比例缩小输出的尺寸，stride=2，则输出为输入的1/2。1 Pooling 保留特征，并减少计算量 max-pooling: 近视眼，只能看到最大的;\naverage-pooling\n(2023-12-12)\nF.avg_pool3d Number of channels doesn\u0026rsquo;t change, and D, H, W shrink. Docs\n1 2 inp = torch.ones(1, 3, 7, 9, 13) F.avg_pool3d(inp, (4, 2, 3), stride=4, padding=1) # (1,3,2,3,4) C h D n = l 4 1 ⋮ ⋱ ⋱ ⋱ ⋮ ⋮ ⋮ ' C h D n = l 4 2 ⋮ ⋱ ⋱ ⋱ ⋮ ⋮ ⋮ ' C h D n = l 4 3 ⋮ ⋱ ⋱ ⋱ ⋮ ⋮ ⋮ MVSNet uses AvgPool3d to compute the sum of every 4 depth-probability planes:\n1 2 3 4 5 avg4 = F.avg_pool3d( F.pad(prob_volume.unsqueeze(1),# (bs,C=1,D=192,H=128,W=160) pad=(0, 0, 0, 0, 1, 2)), # (bs,C=1,D=195,H=128,W=160) (4, 1, 1), stride=1, padding=0)# (bs,C=1,D=192,H=128,W=160) prob_volume_sum4 = 4 * avg4 Deconvolution Complexity of CNN 3\nConvTranspose2d() 1 torch.nn.ConvTranspose2d() Deconvolution visualization\n(2023-07-19)\ntorchvision.models.resnet34 ResNet - PyTorch | Source code\nlayers: [3,4,6,3] means that layer1 has 3 BasicBlock (resnet50 is Bottleneck) convolution blocks, and layer2 has 4 blocks, and layer3 has 6 blocks, and layer4 has 3 blocks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def _forward_impl(self, img) # img.shape: (3, 300, 400) x = self.conv1(img) # nn.Conv2d(), channels↑ and size↓, (1, 64, 150, 200) x = self.bn1(x) # batch norm, shape doesn\u0026#39;t change, (1, 64, 150, 200) x = self.relu(x) # act, shape keeps the same, (1, 64, 150, 200) x = self.maxpool(x) # max-pooling, size↓, (1, 64, 75, 100) # 3 identical \u0026#39;BasicBlock\u0026#39; x = self.layer1(x) # stride in the 1st block is 1, so shape doesn\u0026#39;t change, (1, 64, 75, 100) # 4 identical \u0026#39;BasicBlock\u0026#39; x = self.layer2(x) # stride in the 1st block is 2, so size↓, (1, 128, 38, 50) # 6 identical \u0026#39;BasicBlock\u0026#39; x = self.layer3(x) # stride in the 1st block is 2, so size↓, (1, 256, 19, 25) # 3 identical \u0026#39;BasicBlock x = self.layer4(x) # stride in the 1st block is 2, so size↓, (1, 512, 10, 13) x = self.avgpool(x) # target output is (1,1), so (1, 512, 1, 1) x = torch.flatten(x,1) x = self.fc(x) # 512 -\u0026gt; num_classes (2023-09-12)\nF.pad Padding an image along width, or height, or depth directions. Docs\nThe order of dimensions should be arranged according to Width, Height, Depth, e.g., padding the last 3 dimensions: F.pad(x, (padding_left, padding_right, padding_top, padding_bottom, padding_front, padding_back) )\nSo the order of (l,r,t,b,f,b) is reverse against an image tensor: (Depth, H, W)\nnn.Conv3d Input: (B, Ch_in, D, H, W); Output: (B, Ch_out, D_out, H_out, W_out)\nFor example, a tensor with shape of (2, 3, 4, 224, 224) is 2 video clips with 3 frames and each frame is a 4-channel image with size 224x224.\nAfter convolution with a kernel of size (2, 4, 4), it can be transformed to (B=2, Ch_out=128, D=2, H=56, W=56)\nc o O a n u l v t l o ( l 2 c 4 v ( , h e 2 4 1 c , , = h 2 4 4 n , ) s l f 4 u s r ) o m a o f m f e c c h e h 1 a c 2 h s t e p f r a m e 3 Iterate each channel for D frames to convolve with a unique 3D kernel. Once every channel has multiplied by a kernel, all the 4 weighted channels are summed directly to form one of output channels.\nDepthwise Convolution date: 2023-07-25\nSeparate a convolution into two steps:\nShrink the size of the feature maps using 1-channel plane-wise kernel (Depthwise Conv);\nExpand the number of channels using 1x1 kernel (Pointwise Conv).\nFLOPs reduced, but the IO access increased resulting in slower inference. Depth-wise Convolution - 沈景兵的文章 - 知乎\nExpanding channels process costs the equal amount of FLOPs in normal convolution and pointwise convolution. For example, when expanding 3 channels to 256 channels, each pixel performs multiplication 256 times.\nHowever, the depthwise convolution doesn\u0026rsquo;t multiply a kernel by each channel and sum them together, but only multiply a kernel by only one channel. A Basic Introduction to Separable Convolutions - Medium\nFewer parameters: 3x3x253 kernels are replaced with 1x1x256 kernels for every pixel on the resultant feature map.\nReference CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride) - G-kdom的文章 - 知乎 (accessed Dec. 22, 2021). 卷积神经网络参数计算及卷积层输出尺寸计算 - 凡夫俗子的文章 - 知乎 卷积神经网络的复杂度分析 - Michael Yuan的文章 - 知乎 ","date":"2021-12-23T17:29:00Z","permalink":"https://zichen34.github.io/writenotes/calc/dl_conv_layers/","title":"memo: DL | Convolution Layers"},{"content":"从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变\n卷积 $\\int_{-\\infin}^{+\\infin} f(\\tau) g(x-\\tau) d\\tau$\n一个人一边进食一边消化，进食函数 f 显示了各时刻的进食量：\n消化函数 g 表示肚子里剩余食物的比例随时间的变化，它与吃多少无关：\n比如要求一个人在下午2点，肚子里还剩多少食物，就是之前吃的每一顿饭经过了“独立”的消化后剩余的部分再求和：\n一般情况，求t时刻的胃中还剩多少食物：x时刻吃的食物f(x)，经过了(t-x)时间，还剩下的比例是$g(t-x)$，乘以总量就是x时刻吃的食物到t时刻还剩多少$f(x)\\cdot g(t-x)$，对t之前每一时刻吃的东西到了t时刻还剩多少，加起来：$\\int_0^t f(x) g(t-x) dx$\n其中两函数的自变量相加后，x就被消掉了，这是卷积的一个标志。\nx 与 (t-x) 在图像上的对应：\n一个系统，输入(f)不稳定，输出(g)稳定，就可以用卷积来求这个系统的存量\n另一种理解：之前发生的事件对当前事件的影响，每件事的影响力随时间变化是g，所以导致 t 时刻事件的发生是之前各时刻事件的影响在t时刻的和。\n如果影响是随距离变化的函数，那么在某位置的事件就是之前各位置在此位置产生的影响之和。\n始皇既没，余威震于殊俗。 \u0026ndash; 贾谊《过秦论》\n图像的卷积操作 周围像素点对当前像素点的影响\n3x3卷积核规定周围一圈像素对当前像素点的影响，5x5就是用了周围2圈。平滑卷积操作就是把当前像素值替换为与周围像素的平均值。\n卷积核与图片的数学运算：$f(x,y)\\star g(m,n) = \\sum f(x,y) \\cdot g(m-x, n-y)$。（x与m-x相加只剩m，y与n-y相加只剩n，两个维度上都是卷积）\n如果只考虑点(x,y) 周围相邻1个像素对它的影响，并且每个相邻像素对当前像素的影响由卷积核g规定。比如要求像素f(x-1,y-1)对f(x,y)的影响，就是它本身乘以对应的比例g。 类比吃饭的例子，f(x,y)是t时刻，f(x-1,y-1)是x时刻，x时刻吃的食物到了t时刻还剩下百分之$g(t-x)$，所以 $f(x-1,y-1)$ 对应的比例为 $g(x-(x-1), y-(y-1)) = g(1,1)$，图片像素位置f与卷积核中g的位置并不一一对应，而是要旋转180度。\ng函数不等于卷积核：\n图像的卷积操作省略了g函数的旋转，直接与卷积核相乘再相加\n卷积层 卷积核是局部特征的模板 不考虑某位置就将其卷积核对应位置设置成零，而要重点考虑某位置，就设置得高一些 保留局部特征，得到feature map 垂直边界滤波器和水平边界滤波器 卷积神经网络可视化-github\n","date":"2021-12-23T07:46:00-05:00","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/01_%E5%8D%B7%E7%A7%AF/","title":"watch: DL - 王木头 01 | Convolution"},{"content":"什么是“感知机”，它的缺陷为什么让“神经网络”陷入低潮\n感知机 分类工具\n只有两个输出\n为线性二分类问题(定义域元素无穷多，值域只有两个值)提供了模板答案\n一个线性函数再加一个激活函数\n线性函数是对标准模型的描述，激活函数是判断输入数据是否符合标准模型\n感知机缺陷 异或运算无法实现 异或没办法线性可分：无法只用一条线把0和1区分开，必须要画一个圈才能将0和1区分开 增加感知机数量：通过中间层，合并两个相同的状态 核方法升维 3-“神经网络”是什么？如何直观理解它的能力极限？它是如何无限逼近真理的？\n神经网络 多层感知机\n输入层，隐藏层，输出层\n全连接网络：每个节点都和下一层节点全部相连 前馈神经网络：数据的传递方向是单向向前传播 普遍逼近定理：只要神经网络有隐藏层，它就可以任意逼近一个连续函数\n不像感知机那样，用两侧的数据（是否）把分界线夹逼出来\n对于多个类别，因为无法明确的定义每一个类别(猫的定义是什么？)，所以不同类别之间的界限不是黑白分明，没有明确的是非，只能说最有可能是哪一类。激活函数采用sigmoid函数，而不是感知机的0-1阶跃函数，把是非问题转化为好坏问题。\n人和神经网络都用自己的认知(标准)去判断是非，神经网络把自己的结果与人的结果进行比对，调整自己的模型，使两个模型之间的差异(损失函数)最小\n","date":"2021-12-22T14:39:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/02-3/","title":"watch: DL - 王木头 02 | Perceptron"},{"content":"1. 最小二乘法及其几何意义 Source video: P1\n2. 最小二乘法-概率视角-高斯噪声-MLE 3. 正则化-岭回归 4. 正则化-岭回归-概率角度-高斯噪声高斯先验-MAP ","date":"2021-12-21T23:02:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/03-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"watch: ML - 白板 03 | Linear Regression"},{"content":"1-背景\n抑制过拟合 增加样本数据 正则化：增加约束限制参数空间 降维 维度灾难 数学角度： 比如每增加一个二值属性，要想完全cover样本空间，所需样本数会以2的指数增长\n几何意义： 在高维空间中，立方体的内切球的体积趋近于零，也就是说把立方体的四个角削掉，只剩下内切球，基本就一点不剩了知乎:机器学习中的维度灾难，四个角所占比例不高，却拥有几乎全部的体积。 所以如果在高维空间中取一超立方体，其中存在样本的概率很低，因为样本只存在于四个角中，这就是数据的稀疏性，并且分布不均匀。很难做分类。\n维度 超立方体体积 超内切球体积 2 1 π (0.5)² 3 1 4/3 π (0.5)³ D 1 K(0.5)ᴰ; 当 D→∞, V(超球体)→0 几何意义2: 两个同心圆的半径相差 $\\varepsilon \\ (0\u0026lt;\\varepsilon\u0026lt;1)$，内圆的半径为 $1-\\varepsilon$，外超球体的体积为：$V_外=K \\cdot 1^D = K$；环形带的体积：$V_{环形带} = V_外-V_内 = K - K(1-\\varepsilon)^D$。\n两体积之比：$\\frac{V_环}{V_外} = \\frac{K- K(1-\\varepsilon)^D}{K} = 1-(1-\\varepsilon)^D$。 不论$\\varepsilon$取多小，当维度趋于无穷，$\\underset{D\\rightarrow \\infin}{lim} (1-\\varepsilon)^D = 0$，也就是比值为1，环形带(壳)体积等于外球的体积 球内的样本只存在与球壳上\n维度灾难会导致过拟合\n需要降维\n降维 避免过拟合，减小泛化误差 直接降维/特征选择: 只保留重要的维度; LASSO带来系数的系数性，使某些属性对应的系数等于0。 线性降维: PCA, MDS 非线性降维: 流形（ISOmap, LLE） 2-样本均值\u0026amp;样本方差矩阵\nData:\n$$ \\mathbf X_{p\\times 1} = (\\mathbf x_1, \\mathbf x_2, \\cdots, \\mathbf x_N)^T_{N\\times p} = \\begin{pmatrix} \\mathbf x_1^T \\ \\mathbf x_2^T \\ \\vdots \\ \\mathbf x_N^T \\end{pmatrix}_{N \\times p},\\quad\n\\mathbf x_i \\in \\R^p,\\ i=1, 2, \\cdots, N $$\nSample Mean:\n$$ \\begin{aligned} \\bar{\\mathbf X} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\mathbf x_i \\ \u0026amp; = \\frac{1}{N} (\\mathbf x_1, \\mathbf x_2,\\cdots, \\mathbf x_N) \\begin{pmatrix} 1 \\ 1 \\ \\vdots \\ 1 \\end{pmatrix}_{N\\times 1} \\ \u0026amp; = \\frac{1}{N} \\ \\mathbf X^T \\ \\mathbf 1_N\n\\end{aligned} $$\nSample Covariance:\n$$ S = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X})^2 \\quad (一维样本) $$\n$$ \\begin{aligned} S_{p\\times p} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X}) (\\mathbf x_i - \\bar{\\mathbf X})^T \\quad (p维样本) \\\n\u0026amp; = \\frac{1}{N} (\\mathbf x_1 - \\bar{\\mathbf X} \\quad \\mathbf x_2 - \\bar{\\mathbf X}\\ \\cdots \\ \\mathbf x_N - \\bar{\\mathbf X}) \\begin{pmatrix} (\\mathbf x_1 - \\bar{\\mathbf X})^T \\ (\\mathbf x_2 - \\bar{\\mathbf X})^T \\ \\vdots \\ (\\mathbf x_N - \\bar{\\mathbf X})^T \\end{pmatrix} \\\n\u0026amp; = \\frac{1}{N} \\left[(\\mathbf x_1 \\ \\mathbf x_2 \\cdots \\mathbf x_N) - \\mathbf{\\bar{X}} \\ (1 \\ 1 \\cdots 1)\\right] (\\mathbf x_i - \\bar{\\mathbf X})^T \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T_{p\\times N} - \\frac{1}{N} \\mathbf{X}^T \\mathbf 1_N \\ \\mathbf 1_N^T \\ ]\\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) ]\\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\quad \\text{($I_N$是NxN方阵)} \\\n\u0026amp; = \\frac{1}{N} [ \\ \\mathbf X^T \\underline{(I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T)} ] \\cdot [ \\underline{(I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T)^T} \\mathbf X] \\\n\u0026amp; = \\frac{1}{N} \\mathbf X^T H \\cdot H^T \\mathbf X \\ \u0026amp; = \\frac{1}{N} \\mathbf X^T H \\mathbf X\n\\end{aligned} $$\nH 是中心矩阵，把数据的均值移动到原点(中心化).\n$$ \\begin{aligned} H \u0026amp;= (\\mathbf I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) \\ H^T \u0026amp;= (\\mathbf I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) =H \u0026amp; (对称性)\\ H^2 \u0026amp;= H \\cdot H \u0026amp; (幂等性)\\ \u0026amp;= (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) (I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T) \\ \u0026amp;= I_N - \\frac{2}{N} \\mathbf 1_N \\mathbf 1_N^T + \\frac{1}{N^2} 1_N \\mathbf 1_N^T 1_N \\mathbf 1_N^T \\\n\u0026amp;= I_N - \\frac{2}{N} \\begin{pmatrix} 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\end{pmatrix} + \\frac{1}{N^2} \\begin{pmatrix} N \u0026amp; N \u0026amp; \\cdots \u0026amp; N \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ N \u0026amp; N \u0026amp; \\cdots \u0026amp; N \\\\ \\end{pmatrix} \\\\ \u0026amp;= I_N - \\frac{1}{N} \\mathbf 1_N \\mathbf 1_N^T \\\\ \u0026amp;= H \\\\ H^n \u0026amp;= H \\end{aligned} $$\n3 PCA-最大投影方差\n经典PCA 一个中心，两个基本点\n核心：将一组可能线性相关的变量，通过正交变换，变换成一组线性无关的变量/基/投影方向（对原始特征空间的重构）\n基本点：最大投影方差；最小重构距离（两种角度,效果相同）\n最大投影方差 最能体现原来样本的分布 Steps: 中心化：把样本均值移动到原点 ($\\mathbf x_i - \\bar{\\mathbf X}$)\n样本点在 $\\mathbf u_1$ 方向上的投影，也是在$\\mathbf u_1$方向上的坐标：\n$$ (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 $$\n其中 $| \\mathbf u_1| = 1$ (或$\\mathbf u_1^T \\mathbf u_1 = 1$)，所以内积等于投影。 两个向量的内积写成一个向量的转置乘以另一个向量，$\\mathbf a_{p\\times 1} \\cdot \\mathbf b_{p \\times 1} = \\mathbf a^T_{1\\times p} \\ \\mathbf b_{p \\times 1} = n_{1\\times 1}$\n投影方差：因为均值已经为0，投影直接平方\n$$ \\begin{aligned} J \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\left( (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 \\right)^2 \\\n\u0026amp;= \\sum_{i=1}^N \\ \\frac{1}{N} \\ \\mathbf u_1^T (\\mathbf x_i - \\bar{\\mathbf X}) \\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1 \\\n\u0026amp;= \\mathbf u_1^T \\left(\\frac{1}{N} \\ \\sum_{i=1}^N (\\mathbf x_i - \\bar{\\mathbf X}) \\ (\\mathbf x_i - \\bar{\\mathbf X})^T \\right) \\mathbf u_1 \\\n\u0026amp;= \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 \\end{aligned} $$\n找到使 J 最大的方向 $\\mathbf u_1$\n$$ \\begin{cases} \\hat \\mathbf u_1 = \\operatorname{arg\\ max}\\ \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 \\ s.t. \\quad \\mathbf u_1^T \\mathbf u_1 = 1 \\end{cases} $$\n带约束的优化问题，用拉格朗日乘子法，写出拉格朗日函数：\n$$ L(\\mathbf u_1, \\lambda) = \\mathbf u_1^T \\cdot S \\cdot \\mathbf u_1 + \\lambda (1 - \\mathbf u_1^T \\mathbf u_1) $$\n求导：\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\mathbf u_1} = 2 S \\cdot \\mathbf u_1 \u0026amp;- \\lambda \\cdot 2 \\mathbf u_1 = 0 \\ S \\underbrace{\\mathbf u_1}{\\text{Eigen vector}} \u0026amp;= \\underbrace{\\lambda}{\\text{Eigen value}} \\mathbf u_1 \\end{aligned} $$\n4-PCA-最小重构代价\n最小代价重构 从重构空间恢复到原始空间，代价最小 Steps: 向量$\\mathbf x_i$在新的特征空间中的表示：\n$$ \\begin{aligned} \\mathbf x_i \u0026amp;= ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_1)\\cdot \\mathbf u_1 + ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_2)\\cdot \\mathbf u_2 + \\cdots + ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_p)\\cdot \\mathbf u_p \\ \u0026amp;= \\sum_{k=1}^p ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k \\end{aligned} $$\n降维：根据特征值，取前q个最大的特征向量(方向)。\n$$ \\hat{\\mathbf x}i = \\sum{k=1}^q ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k $$\n重构代价: $| \\mathbf x_i - \\hat{\\mathbf x}_i |^2$\nN个样本的重构代价最小：\n$$ \\begin{aligned} J \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N | \\mathbf x_i - \\hat{\\mathbf x}i |^2 \\ \u0026amp;= \\frac{1}{N} \\sum{i=1}^N | \\sum_{k=q+1}^p ((\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k) \\cdot \\mathbf u_k |^2 \\\n\u0026amp;= \\frac{1}{N} \\sum_{i=1}^n \\sum_{k=q+1}^p \\left( (\\mathbf x_i - \\bar{\\mathbf x})^t \\mathbf u_k \\right)^2 \\quad \\text{(向量的模等于坐标的平方和)} \\\n\u0026amp;= \\sum_{k=q+1}^p \\underline{ \\sum_{i=1}^n \\frac{1}{N} \\left( (\\mathbf x_i - \\bar{\\mathbf X})^T \\mathbf u_k \\right)^2 } \\\n\u0026amp;= \\sum_{k=q+1}^p \\mathbf u_k^T \\cdot S \\cdot \\mathbf u_k \\qquad\\ \\rm s.t.\\ \\mathbf u_k^T \\mathbf u_k = 1 \\\n\\end{aligned} $$\n最优化问题：\n$$ \\begin{cases} \\mathbf u_k = \\operatorname{arg\\ min} \\sum_{k=q+1}^p \\mathbf u_k^T \\cdot S \\cdot \\mathbf u_k \\ s.t. \\quad \\mathbf u_k^T \\mathbf u_k = 1 \\end{cases} $$\n因为各特征向量互不相关，所以可以一个一个解，也就是求剩余的每个特征向量的最小重构代价对应的特征值$\\lambda$\n$$ \\begin{cases} \\operatorname{arg\\ min} \\mathbf u_{q+1} \\cdot S \\cdot \\mathbf u_{q+1}\\ s.t. \\quad \\mathbf u_{q+1}^T \\ \\mathbf u_{q+1} = 1 \\end{cases} $$\n$$ J = \\sum_{i=q+1}^p \\lambda_i $$\n当J最小时，对应的就是最小的几个特征值\n5-SVD角度看PCA和PCoA\nPCA 找最大的投影方向(特征向量)，就是主成分\n求解主成分：对方差矩阵做特征值分解：$S = G K G^T$（因为S是对称矩阵，所以它的奇异值分解就是特征值分解）， 其中特征向量是正交的: $G^T G = I$；K是对角矩阵，元素都是特征值，其排列满足： $k_1 \u0026gt; k_2 \u0026gt; \\cdots \u0026gt; k_p$。降到q维，就取前 q 个值，作为G的q个列向量。\n$$ K= \\begin{pmatrix} k_1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; k_2 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; k_p \\end{pmatrix} $$\n探索一下：\n对中心化之后的原始数据做SVD奇异值分解：\n$$ H X = U \\Sigma V^T \\rightarrow SVD: \\begin{cases} U^T U = I \u0026amp; \\text{(列正交)} \\ V^T V = V V^T = I \u0026amp; \\text{(正交)} \\ \\Sigma \u0026amp; \\text{(对角)} \\end{cases} $$\n然后代入协方差矩阵（推导省略常数$\\frac{1}{N}$）：\n$$ \\begin{aligned} S_{p\\times p} \u0026amp;= X^T H X \\ \u0026amp;= X^T H^T H X \u0026amp; \\text{(等幂性)} \\ \u0026amp;= V \\Sigma \\underline{U^T \\cdot U} \\Sigma V^T \\ \u0026amp;= V \\Sigma I \\Sigma V^T \u0026amp; \\text{(U列正交)}\\ \u0026amp;= V \\Sigma^2 V^T \u0026amp; \\text{($\\Sigma$对角阵)} \\end{aligned} $$\n就相当于对 S 做了奇异值分解了，对应于上面 S 的特征值分解：\n$$ 特征向量G = V, 特征值K = \\Sigma^2 $$\n所以，不用直接对 S做特征值分解，直接对数据做完中心化之后，做奇异值分解，就可以得到特征向量V。\n定义一个矩阵 T（S反过来，对数据内积分解）:\n$$ \\begin{aligned} T_{N\\times N} \u0026amp;= H X X^T H^T \\ \u0026amp;= U \\Sigma V^T \\cdot V \\Sigma U^T \\ \u0026amp;= U \\Sigma^2 U^T \\end{aligned} $$\nT 和 S 有相同的特征值(eigen value): $\\Sigma^2$。\nPCA：先对 S 做特征值分解，找到了主成分（特征向量/投影方向）；然后样本点 $HX$ 乘以方向向量$V$（投影），得到各方向上的坐标。 坐标矩阵：$HX \\cdot V = U \\Sigma \\underline{V^T \\cdot V} = U \\Sigma$\n而对T做特征分解，可以直接得到坐标，这叫主坐标分析（PCoA）\n对T两边左乘$U\\Sigma$（做一个缩放）： $$ \\begin{aligned} T \u0026amp;= U \\Sigma^2 U^T \\ T U \\Sigma \u0026amp;= U \\Sigma^2 \\underline{U^T U} \\Sigma \\ \u0026amp;= U \\Sigma^3 \\ T \\underbrace{U \\Sigma}{特征向量} \u0026amp;= U \\Sigma \\underbrace{\\Sigma^2}{特征值} \\end{aligned} $$\n也就是说，对T做SVD奇异值分解后，直接得到的特征向量就是坐标。\n如果数据的维度太高，$S_{p\\times p}$ 不好计算，可以对$T_{N\\times N}$分解。\n6-Probablistic PCA\n","date":"2021-12-16T13:33:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/05_%E9%99%8D%E7%BB%B4/","title":"watch: ML - 白板 05 | Dimensionality Reduction"},{"content":"Video 7 2021-10-04\n[toc]\nDimensionality Reduction Avoids the curse of Dimensionality October 6, 2021\nCurse of Dimensionality When dimensionality increases, data becomes increasingly sparse Concepts become less meaningful: density and distance Subspace combinations grow very fast Dimentionality Reduction Eliminate irrelevant features and reduces noise\n$X$ is a set of $N$ features: $X={X_1, X_2, \\cdots X_N}$，a reduced set $X\u0026rsquo;$ is a transformation of $X$ and consists of $d$ features so that $d\u0026lt;N$:\n$$ X\u0026rsquo; = T(X) = { X_1\u0026rsquo;,\\ X_2\u0026rsquo;,\\ \\cdots,\\ X_d\u0026rsquo;} \\ T: \\R^N \\rightarrow \\R^d,\\ d\u0026lt;N $$\nAvoids the curse of dimensionality. Reduces time and space required for computations.\u0026gt;\nTwo ways:\nFeature Extraction: transformation to a lower dimension Wavelet transforms and PCA Feature Selection: transformation is limited to only selection from original features Filters, Wrappers, Embedded. 3 Features Relevant feature is neither irrelevant nor redundant to the target concept. Irrelevant feature is useless information for the learning task, and may causes greater computational cost and overfitting Redundant feature is duplication of information that has contained in other features. Feature Selection Assume a binary classification model, X → Model → Y ∈ {0, 1}, where X consists of N different features, e.g., age, weight, temperature, blood pressure, etc. X = {X1, X2, X3 , . . . , XN } N could be small, or relatively large value, e.g., an image of size of 300 × 300.\nClass Separation Criterion Evaluation of data separation result based on selected features.\n$$ \\begin{aligned} 变换 \\quad \u0026amp; T: \\R^N → \\R^d, \\quad d\u0026lt;N \u0026amp; \\text{(把数据从N维降到d维)} \\ 分离结果\\quad \u0026amp; X\u0026rsquo;= T(X) \\ 分离评价\\quad \u0026amp; J(X\u0026rsquo;,Y) = 1-ε^* (X\u0026rsquo;,Y) \u0026amp; \\text{(Y: class distribution; ε是X\u0026rsquo;与Y的误差)} \\end{aligned} $$\n根据最大化 J 或者最小化ε来设计h，实现更好的样本分离\nFeature Selection Find the optimal subset of d features $S^$ from N features, according to $S^ = \\underset{|S|=d}{\\operatorname{arg,max}}J(S)$\nOptimal number of features bring lowest error of model\nFS包括两部分： 搜索策略 Search Strategy 和 目标函数 Objective Function\nSearch Strategy: 若 d 固定会有$C_d^N$次评价；若 d 也需优化，就有$2^N$次评价比较，为了在所有的特征组合之中搜索，需要一个搜索策略来引导。 Objective Function: 量化特征子集的好坏，以供search strategy 判断 Importance:\nReduce computational complexity, run-time, required storage. FS can get meaningful and explainable rules from model based on raw data with real meaning. Building better generalizable model. FS can be applied for non-numerical features, which cannot be transformed easily. 类分离评价标准：$J(X^S,Y) = J(S)$越大越好，由Bayes error, classifier error或Mahalanobis distance定义。误差越小，J越大。\nSearch Strategies Search sequence of N features Find the subset of d features from all possible combination of features quickly Three major categories： Sequential algorithms: Add or remove features sequentially\nShortcoming: a tendency to become trapped in local minima\nSequential Forward Selection Sequential Backward Selection Plus-L Minus-R Selection Bidirectional Search Sequential Floating Selection Exponential algorithms: Used to evaluate a number of subsets that grows exponentially with the dimensionality of the search space.\nExhaustive Search Branch and Bound Approximate Monotonicity with a Branch and Bound Beam Search Randomized algorithms: Incorporate randomness into search procedure to escape from local minima\nRandom Generation plus Sequential Selection Objective Function Evaluation of subset goodness 三大类： Filter Methods: statistical analysis without using predictive model statistical dependence, information gain, Chi square, log likelihood ratio, interclass distance or information-theoretic measures Wrapper Methods: pre-determined predictive models or classifiers Hybrid Methods: complement of wrapper and filter approaches Filters Approaches select d features greedy which are used for training a predictive model $h_M$ with M samples\n$$ X^N \\overset{FS}{\\longrightarrow} X^d \\overset{h_m}{\\longrightarrow}Y $$\nWhy is it?\nEvaluation is independent of the predictive models or classifiers. Objective function evaluate the information content and statistical measures of feature subsets Role: evaluate each feature individually or a batch of features\nMajor steps:\nEvaluating and ranking features Choosing the features with the highest ranks to induce models Advantages:\nFast Execution: non-iterative computation is faster than training session of predictive models Generality: evaluate intrinsic properties of the data, rather than their interactions with a particular predictive model. So the final subset is general for any subsequent predictive models Disadvantages:\nTendency to select large subsets: more features will make the monotonic objective functions larger Independence: ignore the performance on predictive models Wrapper Approches a predictive model $h_M$ is trained to find the best subset $S^*$\n一个预测模型被包在了“选择系统”里面\nMaximize the separation criterion J or minimize the estimiated error$\\epsilon$\n不采取穷尽搜索，而是搜索较少的特征空间，找到 sub-optimal 解。Evaluation 标准与 predictive models 和 classifiers 相关。通过在测试数据上验证准确性来评选特征子集。\nAdvantages:\nAccuracy: 因为更关注特定 predictive model 与数据之间的交互，所以要比 filter approaches 在预测结果上更准确。 Generalization ability: 因为通常采取 cross-validation，所以可以避免过拟合 Disadvantages:\nSlow execution: 训练predictive model 花时间 Lack of generality: 仅使用一个特定的predictive model Best Individual Features 最佳个体特征：一个特征的单独分类效果好于其他特征单独作用的效果\n$$ J_{i*} = \\underset{i \\in N}{\\operatorname{arg\\ max}} J(X_i, Y) $$\n并不能保证组合起来的效果还是最好的\nSequential Forward Search 顺序前向搜素：第一个特征选最佳单体特征BIF，然后每次都取当前的最佳组合，选够d个特征，或者 J 出现下降，就是输出\n最简单的贪心搜索算法 (the simplest greedy search algorithm)\n适合挑选小容量的特征子集\nFeature freeze: Once a feature added to the selection list can not be deleted. 所以不能保证最终结果是最好的\nSequential Backward Search 顺序后向搜索：从特征全集开始，顺序移除那个使$J(X_i,Y)$下降最小的特征， 适合挑选大容量的特征子集 Limitation: 无法评价已经被移除特征的作用 General SFS and General SBS 广义顺序前向搜索：允许每次选中多个（r）特征，进行评估，添加或删除\n减少判断次数\nGSFS:\nPLUS-L TAKE-R 加 L 减 R。In Plus-L Take away-R\nIf L\u0026gt;R: 从空集开始，先按SFS加入L个特征，再按SBS移除R个特征。\nIf L\u0026lt;R: 从全集开始，先按SBS移除R个特征，再按SFS加入L个特征\n作用：attemps to compensate for the weaknesses of SFS and SBS with some backtracking capabilities (回溯能力)\nLimitation: Lack of theory to help predict the optimal values of L and R.\n例：从10个特征中选3个，L=3，R=2\n3\u0026gt;2，所以从空集开始，先加3个特征，再移除2个特征\nSequential Floating Selection 顺序浮动选择：每次迭代时，可以调整 L 和 R\n在PLTS的基础上，不固定L 和 R。\n优化L和R\nTwo floating methods:\nSequential Floating Forward Selection (SFFS) 顺序浮动前向选择 Starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases.\nSequential Floating Backward Selection (SFBS) 顺序浮动后向选择 Starts from the full set. SFBS performs forward steps as long as the objective function increases\n","date":"2021-12-14T14:47:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/aml_feature-selection/","title":"watch: AML | Feature Selection"},{"content":"Video 17 Validation 2021-12-08\nOutline:\nThe validation set Model selection Cross validation Review of Lec12 (期末不涉及12)\nRegularization: add a overfit or complexity penalty term，与模型复杂度有关，使用这个\u0026quot;惩罚项\u0026quot;估计out-of-sample error\n两种正则化方法：\nconstrained regularization: select some type of hypotheses unconstrained regularization: 不是最小化$E_{out}$，而是最小化 $E_{\\rm augment}(\\mathbf w) = E_{in}(\\mathbf w) + \\underbrace{\\frac{\\lambda}{N} \\mathbf w^T \\bf w}_{\\text{penalty term}}$ 选择一个 regularizer 去估计penalty项: $E_{\\rm augment}(\\mathbf w) = E_{in}(\\mathbf w) + \\frac{\\lambda}{N} \\Omega(h)$\n其中 $\\Omega(h)$ 是regularizer，$\\lambda$ 是正则化参数(regularization parameter)\n$\\Omega(h)$: 启发式地选择 heuristic，通常使用weight decay，找到一个 smooth, simple $h$\n$\\lambda$ 决定了正则化被引入的程度。如果选了正确的$\\lambda$，可以很好的估计未知目标函数。Validation 也要找到一个合适的$\\lambda$\nValidation vs Regularization 在learning 过程中，$E_{out}(h)$ 未知（因为目标函数未知），但是它等于 $E_{in}(h)+$ overfit penalty，Ein 是已知的（预测值与训练样本真实值的误差），还需要知道overfit penalty。 所以为了计算 Eout 有两种方法：Regularization 是先估计出 overfit penalty。而Validation 是直接估计 Eout。 $$ \\begin{aligned} \\rm Regularization: E_{out}(h) = E_{in}(h) + \\underbrace{\\text{overfit penalty}}{\\mathclap{\\text{regularization estimates this quantity}}} \\ \\ \\rm Validation: \\underbrace{E{out}(h)}{\\mathclap{\\text{validation estimates this quantity}}} = E{in}(h) + \\text{overfit penalty} \\end{aligned} $$\nAnalyzing the estimate Out-of-sample point 是没有在训练阶段中使用的点，\n在一个out-of-sample 点 $(\\mathbf X,y)$ 上的误差是 $\\mathbf e(h(\\mathbf x),y)$。根据要解决问题的不同，误差函数有不同的形式：\n$$ \\begin{aligned} \\text{回归, Squared error:} \u0026amp; (h(\\mathbf x)-y)^2 \\ \\text{分类, Binary error:} \u0026amp; [![ h(\\mathbf x)\\neq y]!] \\end{aligned} $$\n$h$ 在 out-of-sample分布 上的误差的期望是$E_{out}(h)$： $\\mathbb E[\\mathbf e(h(\\mathbf x),y)] = E_{out}(h)$\n$h$ 在 out-of-sample分布 上的误差的方差是$\\sigma^2$： $\\operatorname{var}[\\mathbf e(h(\\mathbf x),y)] = \\sigma^2$\n从1个点到1组点： 从training set 中独立地选出K个点组成一个验证集(validation set) $(\\mathbf x_1,y_1), \\cdots, (\\mathbf x_K, y_K)$，验证集上的误差是 $E_{\\text{val}}(h) = \\frac{1}{K} \\sum_{k=1}^K \\mathbf e(h(\\mathbf x_k), y_k)$\n不同验证集误差的期望：$\\mathbb E[E_{\\text{val}}(h)] = \\frac{1}{K} \\sum_{k=1}^K \\mathbb E[\\mathbf e(h(\\mathbf x_k), y_k)] = E_{out}(h)$ (期望放里面，就是$E_{out}$)\n不同验证集误差的方差：$\\operatorname{var} [E_{\\text{val}}(h)] = \\frac{1}{K^2} \\sum_{k=1}^K \\operatorname{var}[\\mathbf e(h(\\mathbf x_k), y_k)] = \\frac{\\sigma^2}{K}$ (因为各点互相独立，所以协方差矩阵除了对角线其他位置都是零)\n验证集的误差等于Eout 加一个 $\\frac{1}{\\sqrt{K}}$ 阶（标准差）的偏置项：\n$$ E_{\\text{val}}(h) = E_{\\text{out}}(h) \\pm O(\\frac{1}{\\sqrt{K}}) $$\n如果增加验证集样本数量 K，偏置项变小，验证集误差就越接近Eout。\n对于数据集 $\\mathcal D = (\\mathbf x_1, y_1), \\cdots, (\\mathbf x_N, y_N)$\n选K个点作为验证集：$\\mathcal D_{\\rm val}$\n剩下 N-K 个点是训练集：$\\mathcal D_{\\rm train}$\n对于偏置项：$O(\\frac{1}{\\sqrt{K}})$，小K让Eval 与 Eout 差的远，而大K让Ein 与 Eout 差得远。所以K需要tradeoff\n以前通常用全部的数据集来训练，得到g，现在只用了一部分数据 (reduced dataset) 来训练，得到$g^-$，所以它的 Ein和Eout 都比g大。然后计算 $g^-$ 在验证集上的误差 $E_{val}(g^-)$，作为Eout 的近似，如果K很大，近似效果会差。经验法则：$K= \\frac{N}{5}$\nValidation set 不是 test set\n$E_{val}(g^-)$ 也不是 $E_{out}$。测试集与训练无关 (unbiased)，而验证集会在训练阶段帮助我们选择超参数，从而影响了学习过程 (optimistic bias)。\n比如，有两个假设 $h_1$ 和 $h_2$，其实它们真正的Eout都是0.5：$E_{out}(h_1) = E_{out}(h_2) = 0.5$ ，但是未知。它们分别在验证集上的误差为 $\\mathbf e_1,\\ \\mathbf e_2$，然后我们会选择留下误差小的那个：$\\mathbf e = min(\\mathbf{e_1,e_2})$, 它的 Eout $\\mathbb E(\\mathbf e)$ 要小于真实值0.5，因为它用的训练数据少于全部数据集，所以validataion 给出的误差是偏向“乐观的”\nModel selection 比如要解决一个分类问题，有M个假设空间：$\\mathcal H_1,\\cdots, \\mathcal H_M$ （比如svm的核可以为linear, polynomial, rbf，选哪种好呢？）。\n根据 (有缩减的reduced) 训练集，从每个假设空间选出“最佳假设”(finalists model 决赛选手)。然后分别在验证集上计算Eval。根据这 M 个Eval，选出最佳 $E_{val}$ 和最佳假设空间 $\\mathcal H_{m^}$。然后再使用整个数据集在最佳假设空间中找出最佳假设 $g_{m^}$\n使用$\\mathcal D_{\\rm val}$ 和 $E_{\\rm val}(g_{m^}^-)$ 选择的最佳假设空间 $\\mathcal H_{m^}$ 是 $E_{out}(g_{m^*}^-)$ 的一个 biased estimate，因为没有使用全部的数据集，所以叫biased。\n不同容量的验证集与预期偏差的关系如下图：\n验证集中数据 K 越多，用于训练的样本越少，Eout越差，但是同时 $O(\\frac{1}{\\sqrt{K}})$ 减小，$E_{\\rm val}$ 会越接近 $E_{\\rm out}$。\nHow much bias 对于 M 个假设空间：$\\mathcal H_1, \\cdots ,\\mathcal H_M$，从中选出了 M 个 finalists model $H_{\\rm val} = { g_1^-, g_2^-,\\cdots, g_M^- }$，然后用验证集 $\\mathcal D_{\\rm val}$ 去“训练”它们，也就是再找出它们中的最佳 minus 假设 $g_{m^\\star}^-$（$E_{\\rm val}$最小）。\n对于一个\u0026quot;训练\u0026quot;过程，对于假设 $g_{m^\\star}^-$ 有Hoeffding不等式成立：\n$$ E_{out} (g_{m^\\star}^-) \\leq E_{val}(g_{m^\\star}^-) + O \\left( \\sqrt{\\frac{ln M}{K}} \\right) $$\n如果有无穷多个假设集（无穷多个正则化参数，$\\lambda$ 是连续值），所以 $O \\left( \\sqrt{\\frac{ln M}{K}} \\right)$ 就变得不再有效\n为了约束 M，就像之前那样，引入 VC 维。比如，我们不关心正则化参数 $\\lambda$ 能取多少值，而是关心我们有几个参数（自由度），我们只有1个参数 $\\lambda$，所以VC维是1。\nData contamination 在训练阶段用了多少数据样本 $E_{in}，E_{out}(E_{test})，E_{\\rm val}$ Contamination: Optimistic (deceptive) bias in estimating Eout Training set: totally contaminated Validation set: slightly contaminated (起到了“测试”的效果，但也被用于训练了) Test set: totally \u0026lsquo;clean\u0026rsquo; (完全用于测试) Cross validation 把train set 分成n折，每次取n-1折做训练，计算在剩下那折上的准确率，n个准确率求平均就是该组超参数的表现。\n不使用test set，却可以估计在test set上的表现。\n目的是选最佳的超参数；不能根据在train set上的准确率判断好坏。\n选用不同超参数时，CV准确率的变化趋势与在test set上的变化趋势近似一致。\nK 进退两难: $g^-$是用 reduced训练集找出的最佳，K越小，用于训练的数据越多，越接近真实的Eout，而根据Hoeffding不等式，$E_{\\rm val}(g^-)$需要很大的K，才能近似$E_{out}(g^-)$\n$$ E_{\\rm out}(g) \\underset{\\mathclap{\\substack{\\ \\text{小K才近似}}}}{\\approx} E_{\\rm out}(g^-) \\underset{\\mathclap{\\substack{\\ \\text{大K才近似}}}}{\\approx} E_{\\rm val}(g^-) $$\n$E_{out}$ 是最终目标，但是只知道验证误差 $E_{\\rm val}(g^-)$\nhave K both small and large\n两种交叉验证方法：\nLeave One Out\nK=1，每次迭代选1个样本做验证，剩下N-1个样本做训练。去除第n个样本的训练集$\\mathcal D_n:$\n$$ \\mathcal D_n = (\\mathbf x_1,y_1),\\cdots,(\\mathbf x_{n-1},y_{n-1}),\\sout{(\\mathbf x_n, y_n)},(\\mathbf x_{n+1},y_{n+1}),\\cdots,(\\mathbf x_N, y_N) $$\n从 $\\mathcal D_n$ 中学到的假设是 $g_n^-$，验证误差 $\\mathbf e_n = E_{\\rm val}(g_n^-) = \\mathbf e(g_n^- (\\mathbf x_n),y_n)$\n对每个留出的样本点，计算验证误差，然后取平均，就是交叉验证误差 (cross validation error): $$ E_{CV} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf e_n $$ 对于3个点，每次取出一个做验证集，剩下两个做训练集，线性回归问题，对于两个样本，误差最小的Linear假设，就是过两点的一条直线。\n对于 Constant 假设：\n对比 $E_{CV}$，constant 模型的交叉验证误差较小，所以最终选择constant模型\nN个样本的数据集要迭代 N 次，每次在 N-1 个样本上训练，如果有1千个样本就要迭代1千次，计算复杂度太高。\nLeave More Out\n把数据集划分成多份，划分成10份的话：$K = \\frac{N}{10}$，只需迭代10 ($\\frac{N}{K}$)次，每次在N-K个点上训练。\nCross validation in action 数字分类任务，把2个特征（symmetry和Average intensity）非线性变换到20维空间，最高幂次为5的多项式\n$$ \\left(1, x_{1}, x_{2}\\right) \\rightarrow\\left(1, x_{1}, x_{2}, x_{1}^{2}, x_{1} x_{2}, x_{2}^{2}, x_{1}^{3}, x_{1}^{2} x_{2}, \\ldots, x_{1}^{5}, x_{1}^{4} x_{2}, x_{1}^{3} x_{2}^{2}, x_{1}^{2} x_{2}^{3}, x_{1} x_{2}^{4}, x_2^{5}\\right) $$\n使用特征数量越多，模型越复杂，$E_{in}$ 越小（迭代了很多次），$E_{out}$先减小后增大，出现Overfitting，而$E_{CV}$的趋势与$E_{out}$相同，因为$E_{out}$未知，$E_{CV}$是 $E_{out}$ 的近似，所以可以根据 $E_{CV}$ 来决定该选用几个特征。Ecv 的最小值出现在5 和7，所以可以选用6个特征的模型。\n没用validation时，直接使用20个特征的模型很复杂，而且过拟合（噪音），Ein为零；使用validation后，决定只用6个特征，模型相对简单，Eout较小。\n例题 Given three two-dimensional data examples $x_1 = (-1,1)，x_2=(0,2)$, and $x_3=(1,1)$, perform the leave-one-out cross validation for a linear fit using these data examples. What is $E_{CV}$?\n$$ E_{CV} = \\frac{1}{N} \\sum_{n=1}^N \\varepsilon_n $$\nwhere $\\varepsilon_n = (y_n - g(x_n))^2$\nNote: The line passing through two-dimensional data points $(x_1, y_1)$ and $(x_2,y_2)$ can be obtained as follows: $y-y_1 = \\frac{y_2 - y_1}{x_2-x_1} \\times (x-x_1)$\nGA answer:\nKeep $x_1$ as for the validation, while $x_2, x_3$ as for training:\n$g:\\ y-2 = \\frac{1-2}{1-0}(x-0) \\Rightarrow y=-x+2$\n$\\varepsilon_1 = (1-g(-1))^2 = (1-3)^2 = 4$\nKeep $x_2$ as for the validation:\n$g:\\ y-1 = \\frac{1-1}{1+1}(x+1) \\Rightarrow y=1$\n$\\varepsilon_2 = (2-g(0))^2 = (2-1)^2 = 1$\nKeep $x_3$ as for the validation:\n$g:\\ y-1 = \\frac{2-1}{0+1}(x+1) \\Rightarrow y=x+2$\n$\\varepsilon_3 = (1-g(1))^2 = (1-3)^2 = 4$\n$E_{CV} = \\frac{1}{3}(4+1+4) = 3$\n","date":"2021-12-14T01:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec13_validation/","title":"watch: AML 13 | Validation"},{"content":"Video 13 Neural Network 2021-11-10\nOutline:\nStochastic gradient descent Neural network model Backpropagation algorithm Gradient Descent 沿着误差函数 $\\mathbf e$ 的负梯度方向，一步一步最小化 in-sample error。\nEin 是（线性/非线性）模型的权重 $\\mathbf w$ 的函数：\n$$ E_{in}(\\mathbf w) = \\frac{1}{N} \\sum_{n=1}^N \\mathbf e(h(\\mathbf x_n), y_n) $$\n$\\mathbf e$ 是误差函数，计算假设值与样本真实值之间的误差。复杂的误差函数越难优化。有时不能向 linear regression 那样 one-shot 求出最佳w。可以用梯度下降，一步一步地使误差下降。移动的方向是负梯度方向$-\\nabla$，每次移动的大小与 $-\\nabla E_{in}$ (Gradient error) 成比例（$\\eta$是学习率）：\n$$ \\Delta \\mathbf w = - \\eta \\nabla E_{in}(\\mathbf w) $$\n这里$\\nabla$ Ein 是基于所有的样本点($\\mathbf x_n, y_n$)，叫做\u0026quot;batch GD\u0026quot;，也就是使用所有点做了一次梯度下降。\nStochastic gradient descent 一次随机选一个点做梯度下降。\nPick one ($\\mathbf x_n, y_n$) at a time. Apply GD to $\\mathbf e(h(\\mathbf x_n),y_n)$\nN 次梯度下降的“平均方向”(Average direction)还是等于$-\\nabla E_{in}$:\n$$ \\mathbb E_n [-\\nabla \\mathbf e(h(\\mathbf x_n), y_n)] = \\frac{1}{N} \\sum_{n=1}^N -\\nabla \\mathbf e(h(\\mathbf x_n),y_{n}) = -\\nabla E_{in} $$\n随机梯度下降是梯度下降的 randomized version\nSGD 的好处：\n简化计算 (Cheaper computation): 每次只看一个样本点 随机化 (Randomization): 避免陷入局部最小或鞍点，无法继续优化。如果使用“batch GD”，那么初始位置最关键，因为只走一步，所以容易陷入附近的局部最优。 简单 (Simple) Rule of thumb (经验法则): $\\eta = 0.1$ works\n例子：电影评分\nUser $u_i$ 的喜好有K个属性，Movie $v_j$ 的也有对应的K个属性。根据这个用户他之前评价过的电影 $r_{ij}$ (rating)，调整用户的各属性权重，最小化误差。\n$$ \\mathbf e_{ij} = \\left( \\underbrace{ r_{ij}}{\\text{actual}} - \\underbrace{\\sum{k=1}^K u_{ik}v_{jk}}_{\\text{predict}} \\right)^2 $$\n反过来，把用户属性输入模型就可以估计某电影的评分\n\u0026hellip;\n2D perceptron 的break point=4，也就是感知机无法解决异或问题。\n\u0026hellip;\nNeural Network model 对于从神经元 $i$ 出发，指向第 $l$ 层的神经元 $j$ 的权重 $w_{ij}^{(l)}$\n$$ \\begin{cases} 1 \\leq l \\leq L \u0026amp; \\text{隐藏层/输出层序号, 输入层是0} \\ 0 \\leq i \\leq d^{(l-1)} \u0026amp; \\text{w出发的神经元: 0代表从bias出发}\\ 1 \\leq j \\leq d^{(l)} \u0026amp; \\text{w指向的神经元: 至少有一个,最多有$d^{l}$}\\ \\end{cases} $$\n第 $l$ 层的某神经元，接受了来自上一层所有神经元的输入（内积）：\n$$ x_j^{(l)} = \\theta(s_j^{(l)}) = \\theta \\left( \\sum_{i=0}^{d^{(l-1)}} w_{ij}^{(l)} x_i^{(l-1)} \\right) $$\n从bias term 开始加到第 $d^{(l-1)}$ 个，把信号 $s_j^{(i)}$ 传入 $\\theta$ 非线性激活函数\n一个样本 $\\mathbf x$ 有$d^{(0)}$ 个维度，所以输入层对应有：$x_1^{(0)} \\cdots x_{d^{(0)}}^{(0)}$，经过一层一层传递，直到最终输出一个值：$x_1^{(L)} = h(\\mathbf x)$\nBackpropagation algorithm 应用随机梯度下降，调节神经网络的权重，使误差函数最小\n网络全部的权重 $\\mathbf w = {w_{ij}^{(l)}}$ 决定了一个假设 $h(\\mathbf x)$ (输入到输出的映射)\n对于一个样本 $(\\mathbf x_n,\\ y_n)$ 上的误差：$\\mathbf e(h(\\mathbf X_n),\\ y_n) = \\mathbf e(\\mathbf w)$，使用SGD，调整权重，减小误差\n误差函数对每个权重求梯度：\n$$ \\nabla \\mathbf e(\\mathbf w): \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}, \\quad \\text{for all } i,j,l $$\n计算误差函数 $\\mathbf e$ 对各权重 w 的梯度 $\\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}$\n误差 $\\mathbf e$ 是实际输出 $\\theta(s)$ 减去真实值 $y$，所以误差函数首先是 $s_{j}^{(l)}$ 的函数，第 $l$ 层的第 $j$ 个神经元的输入信号$s_{j}^{(l)}$ 是来自上一层所有神经元的输出贡献之和，所以 $s_{j}^{(l)}$ 是 $w_{ij}^{(l)}$ 的函数，根据链式求导法则：\n$$ \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}} = \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_{j}^{(l)}} \\times \\frac{\\partial s_{j}^{(l)}}{\\partial w_{ij}^{(l)}} $$\n其中：$\\frac{\\partial s_{j}^{(l)}}{\\partial w_{ij}^{(l)}} = x_i^{(l-1)}$，也就是前一层的神经元的输出。 把误差对输入信号的导数称为：$\\delta_j^{(l)} = \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_{j}^{(l)}}$。\n所以（某神经元上的）误差对各权重 w 的梯度等于：$\\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial w_{ij}^{(l)}}= \\delta_j^{(l)} x_i^{(l-1)}$。\n计算$\\delta$:\n从最后一层（输出层$l=L,\\ j=1$）的 $\\delta_1^{(L)}$ 开始计算，输出神经元的输入信号是 $s_1^{(L)}$:\n$$ \\begin{aligned} \\delta_1^{(L)} \u0026amp;= \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_1^{(L)}}\\ \\mathbf e(\\mathbf w) \u0026amp;= \\left ( x_1^{(L)}- y_n \\right)^2 \u0026amp; \\text{预测值-实际值} \\ x_1^{(L)} \u0026amp;= \\theta(s_1^{(L)}) \u0026amp; \\text{神经元的输出是$\\theta$的输出} \\ \\theta\u0026rsquo;(s) \u0026amp;= 1 - \\theta^2(s) \u0026amp; \\text{for the tanh} \\end{aligned} $$\n之前层 ($l-1$层) 神经元上的误差对其输入信号的导数：\n因为第 $l-1$ 层的某个神经元会对第 $l$ 层的全部神经元都有贡献，所以它的误差来自第 $l$ 层的全部神经元 $\\delta_j^{(l)}$，所以需要求和。根据链式法则，误差$\\mathbf e$ 从上一层 ($l$层) 过来，所以首先是 $s^{(l)}$ 的函数，然后$s^{(l)}$ 是第 $(l-1)$ 层神经元 $x^{(l-1)}$ 的函数，最后 $x^{(l-1)}$ 才是它输入信号 $s^{(l-1)}$ 的函数：\n$$ \\begin{aligned} \\delta_i^{(l-1)} \u0026amp;= \\frac{\\partial \\mathbf e(\\mathbf w)}{\\partial s_i^{(l-1)}} \\ \u0026amp;=\\sum_{j=1}^{d^{(l)}} \\frac{\\partial \\mathbf{e}(\\mathbf{w})}{\\partial s_{j}^{(l)}} \\times \\frac{\\partial s_{j}^{(l)}}{\\partial x_{i}^{(l-1)}} \\times \\frac{\\partial x_{i}^{(l-1)}}{\\partial s_{i}^{(l-1)}} \u0026amp; \\text{第$l$层所有神经元误差求和}\\\n\u0026amp;=\\sum_{j=1}^{d^{(l)}} \\delta_{j}^{(l)} \\times w_{i j}^{(l)} \\times \\theta^{\\prime}\\left(s_{i}^{(l-1)}\\right) \\\n\\delta_{i}^{(l-1)} \u0026amp;=\\left(1-\\left(x_{i}^{(l-1)}\\right)^{2}\\right) \\sum_{j=1}^{d^{(l)}} w_{i j}^{(l)} \\delta_j^{(l)} \u0026amp; \\text{$\\theta$与j无关,求导放前面; $x_i^{l-1}$也就是$\\theta(s)$} \\end{aligned} $$\n所以最后一层之前层的神经元的 $\\delta$ 等于 1 减去这个神经元输出的平方，再乘上从它出发的各权重与下一层的$\\delta$ 的内积之和。最后一层的$\\delta^{(L)}$算出来了，才能算倒数第2层的$\\delta^{(L-1)}$，从而可以反向地一层一层求出误差$\\mathbf e$ 对各个权重 w 的梯度。\nBackpropagation algorithm:\nInitialize all weights $w_{ij}^{(l)}$ at random for t=0,1,2, \u0026hellip;, do //循环 Pick $n \\in { 1,2,\\cdots,N }$ //从N个样本中挑一个 Forward: Compute all $x_j^{(l)}$ //计算每个神经元的输出，从而得出预测值 Backward: Compute all $\\delta_j^{l}$ //计算每个神经元的误差(贡献) Update the weights: $w_{ij}^{(i)} \\leftarrow w_{ij}^{l} - \\eta x_i^{(l-1)} \\delta_j^{(l)}$ //迭代直到收敛 Iterate to the next step until it is time to stop Return the final weights $w_{ij}^{l}$ Final remark: hidden layers\n隐藏层是在“模仿”非线性变换：把高维样本（线性不可分）变换到新的维度空间，叫做“learned nonlinear transform”。隐藏层的每个神经元是 \u0026ldquo;learned feature\u0026rdquo;。\n神经元数量越多，自由度越多（有效参数越多），VC维越高，模型复杂度越高，需要更多的样本，才能保证可以从 $E_{in}$ 泛化到Eout.\nExample： Back Propagation（梯度反向传播）实例讲解\n令 $x_1=1, x_2=0.5$ ，然后我们令 $w_1, w_2, w_3, w_4$ 的真实值分别是 1,2,3,4 ，令 $w_5, w_6$ 的真实值是 $0.5, 0.6$ 。这样我们可以算出 $y$ 的真实目标值是 $t=4$ 。\n那么为了模拟一个Back Propagation的过程，我们假设我们只知道 $x_1=1, x_2=0.5$ ，以及对应的目标 $t=4$ 。我们不知道 $w_1,w_2,w_3,w_4,w_5,w_6$ 的真实值，现在我们需要随机为他们初始化值，假设我们的随机化结果是 $w_1=0.5, w_2=1.5, w_3=2.3, w_4=3, w_5=1, w_6=1$。\nForward: 计算 $h_1, h_2, y$ 的预测值和误差项 E，其中 $E=\\frac{1}{2}(t-y)^2$\n$$ \\begin{aligned} h_1 \u0026amp;= w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0.5 \\cdot 1 + 1.5 \\cdot 0.5 = 1.25 \\ h_2 \u0026amp;= w_3 \\cdot x_1 + w_4 \\cdot x_2 = 2.3 \\cdot 1 + 3 \\cdot 0.5 = 3.8 \\ y \u0026amp;= w_5 \\cdot h_1 + w_6 \\cdot h_2 = 1 \\cdot 1.25 + 1 \\cdot 3.8 = 5.05 \\ E \u0026amp;= \\frac{1}{2} (y-t)^2 = 0.55125 \\end{aligned} $$\nBackward\nupdata $w_5$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_5} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_5} = (y-t)\\cdot h_1 = 1.05 \\cdot 1.25 =1.3125 \\\n\\frac{\\partial E}{\\partial y} \u0026amp;= (t-y)\\cdot -1 = y-t \\ \\frac{\\partial y}{\\partial w_5} \u0026amp;= \\frac{\\partial (w_5 h_1 + w_6 h_2)}{\\partial w_5} = h_1 \\ w_5^+ \u0026amp;= w_5 - \\eta \\cdot \\frac{\\partial E}{\\partial w_5} = 1-0.1\\cdot 1.3125 = 0.86875 \\end{aligned} $$\nupdata $w_6$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_6} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_6} = (y-t)\\cdot h_2 = 1.05 \\cdot 3.8 = 3.99 \\ w_6^+ \u0026amp;= w_6 -\\eta \\cdot \\frac{\\partial E}{\\partial w_6} = 1-0.1\\cdot 3.99 = 0.601 \\end{aligned} $$\n下面我们再来看 $w_1, w_2, w_3, w_4$ ，由于这四个参数在同一层，所以求梯度的方法是相同的\nupdata $w_1$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_1} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_1} = (y-t) \\cdot w_5 \\cdot x_1 = 1.05 \\cdot 1 \\cdot 1 = 1.05 \\\nw_1^+ \u0026amp;= w_1 - \\eta \\cdot \\frac{\\partial E}{\\partial w_1} = 0.5 - 0.1 \\cdot 1.05 = 0 .395 \\end{aligned} $$\nupdata $w_2$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_2} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_2} = (y-t) \\cdot w_5 \\cdot x_2 = 1.05 \\cdot 1 \\cdot 0.5 = 0.525 \\\nw_2^+ \u0026amp;= w_2 - \\eta \\cdot \\frac{\\partial E}{\\partial w_2} = 1.5 - 0.1 \\cdot 0.525 = 1.4475 \\end{aligned} $$\nupdata $w_3$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_3} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_3} = (y-t) \\cdot w_6 \\cdot x_1 = 1.05 \\cdot 1 \\cdot 1 = 1.05 \\\nw_3^+ \u0026amp;= w_3 - \\eta \\cdot \\frac{\\partial E}{\\partial w_3} = 2.3 - 0.1 \\cdot 1.05 = 2.195 \\end{aligned} $$\nupdata $w_4$:\n$$ \\begin{aligned} \\frac{\\partial E}{\\partial w_4} \u0026amp;= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_4} = (y-t) \\cdot w_6 \\cdot x_2 = 1.05 \\cdot 1 \\cdot 0.5 = 0.525 \\\nw_4^+ \u0026amp;= w_4 - \\eta \\cdot \\frac{\\partial E}{\\partial w_4} = 3 - 0.1 \\cdot 0.525 = 2.9475 \\end{aligned} $$\nForward:\n$$ \\begin{aligned} h_1 \u0026amp;= w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0.395 \\cdot 1 + 1.4475 \\cdot 0.5 = 1.11875 \\ h_2 \u0026amp;= w_3 \\cdot x_1 + w_4 \\cdot x_2 = 2.195 \\cdot 1 + 2.9475 \\cdot 0.5 = 3.66875 \\ y \u0026amp;= w_5 \\cdot h_1 + w_6 \\cdot h_2 = 0.97191 + 2.204918 = 3.17683 \\ E \u0026amp;= \\frac{1}{2} (y-t)^2 = 0.338802 \\end{aligned} $$\n","date":"2021-12-14T00:53:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec10_neural_networks/","title":"watch: AML 10 | Neural Networks"},{"content":"Video 12 Bias-Variance Tradeoff 11-01-2021\nOutline:\nBias and Variance Learning Curves Review of Lec 7\n$d_{VC}(\\mathcal H)$: the most number of points $\\mathcal H$ can shatter $d_{VC}$是有限值，让 g 近似 $f$ 成为可能。VC维仅由假设集决定。 为了降低到某一概率，$d_{VC}$越大，所需样本点数N越多。$N\\geq 10 d_{VC}$ $E_{out} \\leq E_{in}+\\Omega$ Generalization bound $\\Omega(N,\\mathcal H, \\delta))$: Bias and Variance Approximation-generalization tradeoff 近似与泛化的权衡\n小的Eout 意味着g在 out-of-sample 上也是f的一个好的近似（样本外误差也很小）。 越复杂的假设集 $\\mathcal H$（M越大），有更好的机会近似 $f$（更可能包含最佳假设g） 越简单的假设集 $\\mathcal H$，有更好的机会在out-of-sample上泛化。 最理想情况：假设集中只包含一个正在寻找的“未知的目标函数” $\\mathcal H={f}$，g 也就是f。 Quantifying the tradeoff 之前的 VC analysis 是一种评估方法：$E_{out} \\leq E_{in}+\\Omega$ 与之相似，Bias-variance analysis 是另一种评估方法：把 Eout 分解成两项： 假设集$\\mathcal H$ 能有多近似 $f$ （Bias） 能在多大程度上确定$\\mathcal H$中的好的假设 （Variance） 这里分析的目标函数是实值的 real-valued, 并且使用平方误差 squared error Start with $E_{out}$ Eout 是假设集中的最佳假设 $g$ 与 未知目标函数 $f$ 在输入空间 $\\mathcal X$ 的各个点上的差距的期望；g 是样本集 $\\mathcal D$ 的函数，样本集不同，选出来的最佳假设 g 也不同：\n$$ E_{out}(g^{(D)}) = \\mathbb E_{\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] $$\n因为每次从输入空间抽出的样本集 $\\mathcal D$ 不一样，g就不一样，Eout 也就不一样，所以把各个Eout求个期望，作为最终的Eout：\n$$ \\begin{aligned} \\mathbb E_{\\mathcal D} \\left[ E_{out} \\left( g^{(\\mathcal D)} \\right) \\right] = \\mathbb E_{\\mathcal D} \\left[ \\mathbb E_{\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \\\n= \\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \u0026amp; \\text{(交换位置)} \\ \\end{aligned} $$\n只关注其中 \u0026ldquo;1个点上的平均误差期望\u0026rdquo;：$\\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$\n定义平均假设 (average hypothesis)：$\\bar{g}(\\mathbf x) = \\mathbb E_{\\mathcal D} \\left[ g^{(\\mathcal D)}(\\mathbf x) \\right]$ (the best thing you can do)，从各不同训练集上得出的最佳假设的平均值。\n比如有 K 个训练集：$\\mathcal D_1, \\mathcal D_2, \\cdots, \\mathcal D_K$，那么平均假设就是：$\\bar{g}(\\mathbf x) \\approx \\frac{1}{K} \\sum_{k=1}^K g^{\\mathcal D_k}(\\mathbf x)$\n把平均假设代入\u0026quot;1个点上的平均误差期望\u0026quot;：\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\ \u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) -\\bar{g}(\\mathbf x) + \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\quad \\text{(减一个加一个)} \\\n\u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 + \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 + 2 \\left( g^{(D)}(\\mathbf x)-\\bar{g}(\\mathbf x) \\right) \\left( \\bar{g}(\\mathbf x) -f(\\mathbf x) \\right) \\right] \\quad \\text{(代入括号)} \\\n\u0026amp; = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] + \\underbrace{\\mathbb E_{\\mathcal D} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]}{与D无关,期望还是自己} + 2 \\left( \\underbrace{ \\mathbb E{\\mathcal D} \\left[ g^{(D)}(\\mathbf x) \\right] -\\bar{g}(\\mathbf x) }_{相等, =0} \\right) \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right) \\\n\u0026amp; = \\underbrace{ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] }{\\rm var(\\mathbf x)} + \\underbrace{ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 }{\\rm bias(\\mathbf x)} \\end{aligned} $$\n$\\bar{g}(\\mathbf x)$ 是\u0026quot;最佳假设\u0026quot;，它与目标未知函数的差是常数 bias (与D无关)，而 $g^{\\mathcal D}(\\mathbf x)$ 随训练集不同，会上下波动，与\u0026quot;平均值\u0026quot;的差的平方，再取平均就是方差。 各个最佳假设与目标未知函数的平方误差的期望，被拆成了两部分：各最佳假设与平均假设的方差，加上平均假设与目标未知函数的平方误差。\n所以 Eout 等于：\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D} \\left[ E_{out} (g^{(\\mathcal D)}) \\right] \\ \u0026amp; = \\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] \\right] \\ \u0026amp; = \\mathbb E_{\\mathbf x} \\left[ \\rm bias(\\mathbf x) + var(\\mathbf x) \\right] \\ \u0026amp; = \\rm bias + var \\end{aligned} $$\nThe trade off between bias and var bias = $\\mathbb E_{\\mathbf x} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$\nvariance = $\\mathbb E_{\\mathbf x} \\left[ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] \\right]$\n如果假设集中只有一个假设 h，它与 $f$ 的距离就是 bias（方差为0）。如果是一个复杂的假设集，其中包含很多假设，更有可能囊括了 f，在它附近的都是“最佳假设”（红色），因为平均假设$\\bar g(\\mathbf x)$就在 f 附近所以bias较小，而方差比较大（很多小值加起来也会大）。\nExample: sine target 近似正弦曲线，未知目标函数 $f(x) = sin(\\pi x)$，把 f 从输入空间从 [-1,1] 扩展到实数域 $f:[-1,1] \\rightarrow \\mathbb R$。只有两个样本点 N=2。\n有两个假设集，自由度不同：\n$$ \\begin{aligned} \u0026amp; \\mathcal H_0 : h(x) = b \u0026amp;{\\text{各假设只有一个参数b (常数)}} \\ \u0026amp; \\mathcal H_1 : h(x) = ax+b \u0026amp;{\\text{各假设有两个参数a,b (直线)}} \\end{aligned} $$\nApproximation: 上帝视角可以看出两个假设集中的最佳假设(bias最小)分别应该为：\n黄色区域是 bias (或者说就是 Eout, 因为单个假设的方差为0)。所以在近似[-1,1]区间上的正弦函数时，$\\mathcal H_1$ 比 $\\mathcal H_0$ 的 bias 更小。\nLearning: 两个样本点的位置是随机的\n如果一开始两个样本点位置如图：\n为了使 bias 最小，$\\mathcal H_0$中的“最佳假设”应该在两样本点中间，$\\mathcal H_1$ 中的“最佳假设”应该穿过两个样本点。\n两个样本点（训练集）每次从输入空间中取的都不一样，对应的“最佳假设”也有很多种可能：\n对于假设集 $\\mathcal H_0$，最佳假设的分布如图：\n对各个“最佳假设”取平均，平均假设位于”平衡位置“，灰色区域是 variation.\n对于假设集 $\\mathcal H_1$，最佳假设（过两样本点的直线）的分布如图：\n平均假设是红色直线，灰色区域是variation。\n对比两个假设集：\n只有一个参数的，最简单的（常数）假设集 $\\mathcal H_0$ 的(平均假设) bias大，方差小。而比较复杂的（直线）假设集 $\\mathcal H_1$ 的 bias 小，方差大。\n最终，$\\mathcal H_0$ 的 Eout= 0.50+0.25 = 0.75，$\\mathcal H_1$ 的 Eout=0.21+1.69 = 1.9。根据 Eout，简单的假设集 $\\mathcal H_0$ 好于复杂的假设集 $\\mathcal H_1$。因为我们是从 Ein \u0026ldquo;泛化\u0026rdquo; 到 out，如果Eout 太大，Generalization bound 太大，Ein 与 Eout 相差太大，二者不follow，就不能通过Ein 学习到 Eout.\n复杂的（自由度多的）假设集有很好近似能力，但是学习能力很差，因为方差太大，不一定能学到最佳假设。\nLesson learned: 模型的复杂度应该匹配 数据（样本决定了最终找出的假设），而不应该匹配目标函数的复杂度。比如只有15个样本，假设已知目标函数是10阶的。你可以选1阶，2阶的模型，它们对应的参数有2个，3个，按照经验法则 $N\u0026gt;10 d_{VC}$，它们至少需要20个，30个样本。但现在只有15个，如果你觉得1阶（直线）不太可能的话，可以用2阶（二次函数）。如果用10阶模型，就出现过拟合了。\nLearning Curves Ein 与 Eout 的曲线。\n对于任意有N个(训练)样本数据集 $\\mathcal D$。\nExpected Eout = $\\mathbb E_{\\mathcal D} \\left[ E_{out} \\left( g^{(\\mathcal D)} \\right) \\right]$ (不同测试集上的最佳假设 g 的平均)\nExpected Ein = $\\mathbb E_{\\mathcal D} \\left[ E_{in} \\left( g^{(\\mathcal D)} \\right) \\right]$ (不同(训练)数据集上的最佳假设 g 的平均)\n它们随 N 如何变化？(How do they vary with N?)\n对于Simple Model（的假设集），随着样本数不断增加，Eout越来越小，越来越近似 f，而 Ein越来越大，因为每个样本都有误差，样本越多加起来越大。N越大，Ein与Eout越接近，Generalized bound越小，收敛于 “平均假设” (黑色水平线)。\n对于Complex Model，“平均假设”更靠近 f，bias较小，所以黑色水平线它更低，同样随着N增大，Eout与Ein 不断趋近于 “平均假设”。但是当N很小的时候，Eout很大（方差很大）。Eout 与Ein 差距很大，复杂模型相较于简单模型的 Generalization bound 更大。复杂模型的 Ein 在样本很少的时候是零，因为在样本数小于VC维或者假设集的effective参数自由度时，模型可以把所有点全部分开 (shatter all the points)。在超过VC维之后，Ein开始增加。\nVC vs Bias-variance 在 VC 分析中，$E_{out} = E_{in} + \\text{Generalization error}$\n在 Bias-variance 中，不再关注$E_{in}$，因为 $E_{out}=\\text{var + bias} = \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] +\\mathbb E_{\\mathcal D} \\left[ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right]$，Eout 等于“粗黑线”(average hypothesis, bias)加上variance。bias取决与假设集而与N无关，所以bias是直线。\n随着N增大，它们都趋近于平均假设。所以都需要tradeoff：简单的模型近似能力差，但它 Generalization error小；复杂的模型近似能力强，但它需要更多的样本，才能减小 Generalization bound。对于复杂的模型，样本数越少，Eout越大。样本少的时候，简单模型的Eout 可能比复杂模型的 Eout 还要小。所以样本少选简单模型，样本多选复杂模型。\nLinear Regression case Noisy target $y = \\mathbf w^{*T} \\mathbf x$ + noise （用线性模型, 从有噪声的样本中，学习一个线性目标函数）\nData set $\\mathcal D = { (\\mathbf x_1, y_1), \\cdots, (\\mathbf x_N, y_N) }$\nLinear regression solution: $\\mathbf w = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf y$\nIn-sample error vector = $\\bf X w - y$\n\u0026ldquo;Out-of-sample\u0026rdquo; error vector = $\\bf X w - y\u0026rsquo;$ （使用相同的x, noise不同，得到测试集）\n有了上面非常特殊的情况，才能得到下面的公式：\n$\\sigma^2$ 是 energy of the noise。一个 zero-mean noise的energy 与variance 成正比，\n","date":"2021-12-13T15:53:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec8_bias_variance/","title":"watch: AML 08 | Bias-Variance Tradeoff"},{"content":"Video 14 Overfitting 2021-11-17\nOutline:\nWhat is overfitting? The role of noise Deterministic noise Dealing with overfitting An Illustration of Overfitting on a Simple Example\n蓝色曲线是 unknowen target function。从这未知函数中生成5个样本点，因为有噪声，所以它们偏离了曲线。 根据这5个点去近似未知函数。因为噪声的存在，在用4阶的多项式（有5个参数）拟合这生成的5个点时，出现了过拟合：$E_{in}=0;\\ E_{out} \\gg 0$。（如果没有噪声，有可能使用相同阶数的模型就能完美拟合。为了拟合带噪声的数据，就使用了超过样本所表达信息的高阶模型，造成Eout 很大）\nWhat is Overfitting? 随着VC 维的增加，$E_{in}$ 不断下降，$E_{out}$反而上升。\n拟合数据的程度超过了样本数据应有(支持)的程度 (Fitting the data more than is warranted)\n随着 VC 维的增加，假设集$\\mathcal H$中的模型复杂度增加，对未知函数的近似能力增强，虽然generalization error在增加，但一开始，$E_{in}$ 和 $E_{out}$ 都在下降。超过某一点后，$E_{in}$继续减小，$E_{out}$反而上升，Generalization error 变更大:\n实验两种情况：一个10阶的函数有噪声采样15个样本，一个50阶的函数无噪声采样15个样本：\n分别用2阶和10阶的多项式去拟合：\n从2阶模型转到10阶模型，穿过了更多的样本点，Ein更小了，但Eout变大了，说明出现了过拟合。简单模型甚至好于无噪声的复杂目标函数。noise 的阶数可能很高，所以简单模型的Ein 比较差。但是没有噪声的高阶目标函数样本也存在某种“噪声”：determinstic noise.\n假设集$\\mathcal H$ should match to the quantity and quality of the data, than the complexity of unknown target function.\n两个学习器：O (overfitting) 和 R (restricted)\n\u0026hellip;\u0026hellip;.\n结果:\n左图，固定目比函数的复杂度为20th 阶多项式，随着增加噪声能量，样本的数量越来越不足以表达出目标函数的复杂度，这时希望不断减小Ein，非要拟合出超出数据表达范围的部分来减小Eout 会事与愿违，适得其反，南辕北辙，导致错误的形式，Eout反而越来越大，所以过拟合越来越严重。增加样本数量，可以减小过拟合。\n右图，无噪声的高阶目标函数样本，固定噪声能量 $\\sigma^2=0.1$。随着增加目标函数的复杂度，过拟合也越来越严重。所以这里存在另一种不同于左图随机噪声的“确定性噪音”。\nDeterministic noise 是假设集$\\mathcal H$ 无法捕捉到的 f 的部分。(The part of f that H cannot capture)\n因为模型复杂度有限，所以假设集中的最佳假设无法拟合目标函数的某些部分，$h^\\star$ 与目标函数 f 之间的面积就是确定性噪音: $f(\\mathbf x) - h^\\star(\\mathbf x)$。假设集太简单，无法表达出目标函数的复杂度，它无法理解的部分会给它带来困扰，当它尝试拟合这些能力范围之外的东西，就会导致错误的形式。那些无法理解的东西对它来说就是噪音。当使用更复杂的假设集，确定性噪声就会减小，因为它可以捕捉到更多\n确定性噪声与随机噪声的主要区别:\n确定性噪音取决于假设集$\\mathcal H$，而随机噪声对所有的假设集都一样，Nothing can capture it, therefore it\u0026rsquo;s noise. 对于一个给定的样本$\\mathbf x$，确定性噪声的量是固定的，就是 $f(\\mathbf x) - h^\\star(\\mathbf x)$。而随机噪声，两个相同的x，产生的噪声大小是随机的。 但是它们两者对机器学习造成的影响是相同的，因为数据集是给定的（一次使用），假设集一旦确定它的确定性噪声也固定了。So in a given learning situation, they behave the same. Impact on overfitting 确定性噪声造成的过拟合在10阶以上的target complexity 才出现，因为这里的假设集是10阶的，超出10阶的部分它才无法近似。 噪声造成过拟合是因为有限的样本，让你误以为你可以完美拟合。但其实随机噪声是捕捉不到的，而且当目标函数复杂度高于假设集的时候，确定性噪声也是捕捉（学）不到的。你以为你学到了，但其实造成了过拟合。 Noise and bias-variance $$ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] = \\underbrace{ \\mathbb E_{\\mathcal D} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar{g}(\\mathbf x) \\right)^2 \\right] }{\\rm var(\\mathbf x)} + \\underbrace{ \\left( \\bar{g}(\\mathbf x) - f(\\mathbf x) \\right)^2 }{\\rm bias(\\mathbf x)} $$\nEout 被分成了两部分，其中的 f 是没有噪声的，如果给样本数据加入噪声，上式会如何变化？\n给定actual output：$y = f(\\mathbf x) + \\varepsilon(\\mathbf x)$，假设噪声期望是0：$\\mathbb E[\\varepsilon(\\mathbf x)] = 0$\n$$ \\begin{aligned} \u0026amp; \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - y \\right)^2 \\right] \u0026amp; \\text{$\\varepsilon$影响了y，也要对ε求期望}\\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - f(\\mathbf x) -\\varepsilon(\\mathbf x) \\right)^2 \\right] \u0026amp; \\text{加一个减一个} \\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) +\\bar g(\\mathbf x) - f(\\mathbf x) -\\varepsilon(\\mathbf x) \\right)^2 \\right] \\\n\u0026amp;= \\mathbb E_{\\mathcal D,\\varepsilon} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) \\right)^2\n\\left( \\bar g(\\mathbf x) - f(\\mathbf x) \\right)^2 \\left( \\varepsilon(\\mathbf x) \\right)^2 + \\text{cross term} \\right] \\end{aligned} $$ 求期望之后 cross term 都变成0了，因为它们包含ε，ε 的期望为0。前面两项只与数据集有关，而与训练样本的噪声ε无关，所以原来（没加噪声）等于0的项现在还等于0。Eout 只剩三项：\n$$ E_{out} = \\underbrace{ \\mathbb E_{\\mathcal D,\\mathbf x} \\left[ \\left( g^{(\\mathcal D)}(\\mathbf x) - \\bar g(\\mathbf x) \\right)^2 \\right] }_{var}\n\\underbrace{ \\mathbb E_{\\mathbf x} \\left[ \\left( \\bar g(\\mathbf x) - f(\\mathbf x) \\right)^2 \\right] }_{\\substack{bias \\↑ \\ \\text{deterministic noise}}} \\underbrace{ \\mathbb E_{ε, \\mathbf x} \\left[ \\left( \\varepsilon(\\mathbf x) \\right)^2 \\right] }_{\\substack{\\sigma^2 \\ ↑ \\ \\text{stochastic noise}}} $$ 第三项是 target 与 actual output之间的差距，就是随机噪声，第二项是\u0026quot;平均假设\u0026quot;（最佳假设the best thing you can do）与target 之间的差距，它无法拟合的部分就是确定性噪声。\n两个噪声是平等的，因为增加样本数量，方差缩小，但两个噪声不可避免的（假设集给定，数据集给定，整体的近似就确定了）。当假设尝试拟合噪声时，就产生了方差（和过拟合）。\nTwo cures Regularization: putting the brakes 踩刹车（增加一点限制; 提前停止）\nValidation: Checking the bottom line 找到底线（找到比Ein 更好的反映拟合质量的误差$E_{CV}$）\n","date":"2021-12-13T13:55:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec11_overfitting/","title":"watch: AML 11 | Overfitting"},{"content":"Video 10 Training and testing 10-20-2021\nOutline\nFrom training to testing Illustrative examples Break point Puzzle Train multiple model / hypotheses: $$ \\mathbb P[|E_{in} - E_{out}| \u0026gt; \\varepsilon] \\leq \\underbrace{2 \\ M \\ e^{-2\\varepsilon^2 N}}_{\\text{union bound}} $$\nTest one model / hypothesis: (Hoeffding Inequality) $$ \\mathbb P[|E_{in} - E_{out}| \u0026gt; \\varepsilon] \\leq 2 e^{-2\\varepsilon^2 N} $$\nM: 假设集中所有可能假设的个数, 可以是无穷，所以概率可以是无穷，就不能保证 $E_{out}\\approx E_{in}$\nLearning is to find the best hypothesis g which make the probability of \u0026quot;\u0026quot; ($\\mathcal B$ad event) minimum.\n\u0026ldquo;Learning\u0026rdquo; 是找到假设集中的最佳假设 g（让 $\\mathcal B$ad event: $|E_{in}(h_m) - E_{out}(h_m)|\u0026gt;\\varepsilon$ 发生的概率最小），g 可以是假设1，或者假设2，\u0026hellip;，或者假设M，就对应概率相加，就是union bound： $$ \\mathbb P [\\rm \\mathcal B_1 or \\mathcal B_2 or \\cdots \\mathcal B_M] \\leq \\underbrace{ \\mathbb P [\\mathcal B_1] + \\mathbb P [\\mathcal B_2] + \\cdots +\\mathbb P [\\mathcal B_M]}_{\\text{no overlaps: M terms}} $$\n“等于” 发生在它们互不重叠的情况，如果各假设间有重叠，它们的和就变小。\n事实上，Bad Events are very overlapping.\n白色与灰色分界线是 unknown target function。 蓝线和绿线是两个 hypothesis。 假设 h 与真实界线所差的面积（红色阴影）是$E_{out}$，黄色面积是 $\\Delta E_{out}$。$E_{in}$未在图中体现，需要对整个面积空间采样，样本点落在 $E_{out}$ 中的个数是$E_{in}$，样本点落在黄色区域内的个数是$\\Delta E_{in}$。 $h_1$ 与 $h_2$ 有很大的重叠，$|E_{in}(h_1) - E_{out}(h_1)| \\approx |E_{in}(h_2)-E_{out}(h_2)|$\n改变h，$E_{out}$ 有显著变化，但$E_{in}$不会有显著变化，因为训练集样本点的个数很有限。所以很多假设的分类效果是一样的。\nReplace M with the number of dichotomies (把样本点成功二分类的斜线的条数).\nA hypothesis $\\rightarrow$ A dichotomy A dichotomy is a mini hypothesis\n对比:\nInput space: A hypothesis, 全输入空间都是输入 $h: \\mathcal X \\rightarrow{-1, +1}$ A dichotomy, 只有 N 个训练点 $h: {\\mathbf{x_1,x_2,\\cdots,x_N}}\\rightarrow {-1, +1}$ Number of hypotheses $|\\mathcal H|$ = M: 可以是无穷个 $|\\mathcal H(\\mathbf{x_1,x_2,\\cdots,x_N}})|$ = $m_{\\mathcal H}(N)$ 最多 $2^N$ 个 （N个$\\pm 1$组合) 用 growth function $m_{\\mathcal H}(N)$ 替代 M\n$m_{\\mathcal H}(N)$ 必须是polynomial in N，这样e的负指数可以起作用抵消 $m_{\\mathcal H}(N)$，让union bound变得小。\nGrowth function $m_{\\mathcal H} (N)$ 由 N 个样本点组成任意的，能成功被假设集 $\\mathcal H$ 二分的 dichotomies 最多的个数 (不限位置): $m_{\\mathcal H}(N) = \\underset{x_1,\\cdots,x_N \\in \\mathcal X}{max}\\ |\\mathcal H(x_1, \\cdots, x_N)|$. （\u0026ldquo;most dichotomies on any N points\u0026rdquo;） $$ m_{\\mathcal H}(N) \\leq 2^N $$\n有些 config 的 dichotomies 达不到 $2^N$ 种：\n对于 2D perceptron hypothesis / dichotomies：\n3个点，最多能分8种（若限定点的位置可能更少）\n4个点，最多能分14种（有2种情况 (异或)，2D perceptron分不开）\n(2行2列摆放最多）按照4个o，3个o (1个x)，2个o，1个o，0个o 排列组合。\n如果4点共线，只能分8种。\n如果3点共线，余1个单独在一行，只能分12种。\n对于 Positive rays hypothesis：\n位置摆放是固定的：共线。对于N个点，正射线只能处理 N+1 种配置，也就是圆圈全在右侧，可以是0个圈，1个圈，2个圈，\u0026hellip;，N个圈。\n对于 Positive Intervals hypothesis\n正区间只能处理中间有一段是+1，两侧是-1的配置。需要用两个边界 (threshold) 来确定一个区间：一共有N个点，形成N+1个空，在这N+1个空里选两个，就形成了一个区间，所以是 $C_{N+1}^2$，再加上两个边界在同一个空的情况。\n对于 Convex sets hypothesis\n凸集可以把任意颜色配置的N个点完全分开，凸集里面是+1，外面是-1。所以它的growth function 是 $2^N$，凸集模型没有break point。\nBreak point k 无法用假设集 $\\mathcal H$ 完全把两类点分开的点的个数 k。也就是开始出现 $m_{\\mathcal H}(k) \u0026lt; 2^k$ 这种情况时的点数。\n例如没有任何一种 4 个点的配置，能用2D perceptron 将其完全分开，所以 2D perceptron 的 break point 是 4：$m_{\\text{2D percep}}(4) = 14 \u0026lt; 2^4$。大于4个的点集也不能被完全分开。\n如果没有 break point, growth function is $2^N$.\n如果存在 break point, growth function is a polynomial function in N : $\\sum_{i=0}^{k-1} C_N^i$, whose maximum power is $N^{k-1}$.\n","date":"2021-12-12T23:23:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec5_traning_vs_testing/","title":"watch: AML 05 | Training vs Testing"},{"content":"Video 11 Theory of Generalization 10-25-2021\nOutline\nProof that the growth function is polynomial Proof that $m_{\\mathcal H}(N)$ can replace M VC dimension Examples of polynomial $m_{\\mathcal H}(N)$ Proof see the book\nKey quantity $B(N,k)$: Maximum number of dichotomies on N points, with break point $k$.\nTheorem: $B(N,k) \\leq \\sum_{i=0}^{k-1} \\left( \\begin{aligned} N\\ i \\end{aligned} \\right)$\nFor a given $\\mathcal H$, the break point $k$ is fixed:\n$$ m_{\\mathcal H}(N) \\leq \\underbrace{ \\sum_{i=0}^{k-1} \\left( \\begin{aligned} N\\ i \\end{aligned} \\right) }_{\\text{maximum power is} N^{k-1}} $$\nPositive ray (break point k=2):\n$$ m_{\\mathcal H}(N) = C_{N+1}^1 \\leq (=) \\sum_{i=0}^{k-1} C_N^i = N+1 $$\nPositive intervals (break point k=3):\n$$ m_{\\mathcal H}(N) = C_{N+1}^2 + 1 \\leq (=) \\sum_{i=0}^{k-1} C_{N}^i = \\frac{1}{2}N^2 + \\frac{1}{2}N + 1 $$\n2D perceptrons (break point k=4): $$ m_{\\mathcal H}(N)=? \\leq \\sum_{i=0}^{k-1} C_{N}^i = \\frac{1}{6}N^3 + \\frac{5}{6}N + 1 $$\n如果存在 break point, growth function is polynomial function in N : $\\sum_{i=0}^{k-1} C_N^i$, whose maximum power is $N^{k-1}$ ($d_{VC}$).\n如果没有break point, growth function is $2^N$，就不能保证它是关于N的多项式（最高次幂是 $d_{VC}$，能被e的负幂次抵消），也就不能保证 Bad events 的概率上界足够小，那样就不能通过 Ein 学到Eout。\nProof that $m_{\\mathcal H}(N)$ can replace $M$ Replace $M$ with $m_{\\mathcal H}(N)$ to narrow the upper bound. If M hypotheses are not overlapping, the probability upper bound is very lose.\n$$ \\begin{aligned} \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 2\\ M\\ e^{-2\\varepsilon^2 N} \\quad \\text{(Union bound)} \\ \u0026amp; \\Downarrow \\text{(Not quite)} \\ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 2\\ m_{\\mathcal H}(N)\\ e^{-2\\varepsilon^2 N} \\ \\end{aligned} $$\n长方形中的每个点代表一个样本数据集。 The \u0026ldquo;flower\u0026rdquo; represents \u0026ldquo;bad events\u0026rdquo; probability (Ein 和 Eout 相差超过$\\varepsilon$ 的概率).\nFig (a) 表示的是一个假设的“验证”。While Learning needs to pick the best hypothesis from M hypotheses based on \u0026ldquo;bad events\u0026rdquo; probability.\nIn Fig (b), every hypothesis is not overlapping each other. So their bad events probabilities are taking the whole space, resulting the upper bound maybe very large. 所以不可能通过 $E_{in}$ 学到 $E_{out}$，因为大部分情况下，坏事件发生，它们相差超过了$\\varepsilon$。\nIn Fig （c）, many hypotheses overlap together in training data. VC bound 比较小，Ein与Eout 相差超过 $\\varepsilon$ 的概率不大。\nWhat to do about $E_{out}$?\nTake another set of samples, called $E_{in}\u0026rsquo;(h)$. $E_{in}$ 与 $E_{in}\u0026rsquo;$ 独立同分布，所以 Ein and Ein\u0026rsquo; are both following Eout, Ein and Ein\u0026rsquo; are following each other.\n$$ \\begin{aligned} \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 2\\ m_{\\mathcal H}(N)\\ e^{-2\\varepsilon^2 N} \\ \u0026amp; \\Downarrow \\text{(but rather)} \\ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026amp; \u0026gt; \\varepsilon] \\leq 4\\ m_{\\mathcal H}(2N)\\ e^{-\\frac{1}{8} \\varepsilon^2 N} \\quad \\text{(VC bound)}\\ \\end{aligned} $$\n(Lec7)\nThe Vapnik-Chervonenkis Inequality: $$ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026gt; \\varepsilon] \\leq 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} $$\nVapnik-Chervonenkis Inequality 对任何有break point的 hypothesis 都成立。\n相较于直接用$m_{\\mathcal H}(N)$替换M，VC维其实 make the bound worser, larger: $2 m_{\\mathcal H}(N) e^{-2\\varepsilon^2 N} \u0026lt; 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N}$\nVC dimension VC维 $d_{VC}$ 是某假设集 $\\mathcal H$ 最多能完全（二分）分开的点的个数。\n$$ d_{VC}(\\mathcal H) = k-1 $$\n完全分开意味着，假设集$\\mathcal H$能分割开由N个点组成的任意的颜色配置：$m_{\\mathcal H}(N) = 2^N$\n有了 $d_{VC}$ 之后：\n$N \\leq d_{VC}(\\mathcal H) \\Rightarrow \\mathcal H$ can shatter N points\n$k \\geq d_{VC}(\\mathcal H) \\Rightarrow k$ is a break point for $\\mathcal H$\n$m_{\\mathcal H}(N)$ 的最高维度是 $d_{VC}$\nGrowth function 用 break point k 表示：$m_{\\mathcal H}(N) \\leq \\sum_{i=0}^{k-1} \\begin{pmatrix} N \\ i \\end{pmatrix}$\nGrowth function 用 VC dimension $d_{VC}$ 表示：$m_{\\mathcal H}(N) \\leq \\underbrace{ \\sum_{i=0}^{d_{VC}} \\begin{pmatrix} N \\ i \\end{pmatrix} }{\\text{minimum power is} N^{d{VC}}}$\n对于不同类型的 hypothesis set:\n$\\mathcal H$ is positive rays: $d_{VC}=1$ $\\mathcal H$ is 2D perceptrons: $d_{VC}=3$ $\\mathcal H$ is convex sets: $d_{VC}=\\infin$ VC dimension and learning 因为 $d_{VC}(\\mathcal H)$ 是有限的, 就可以从假设集 $\\mathcal H$ 中学习到足够接近未知目标函数 $f$ 的假设g。（保证了Ein与Eout相差很多的概率不大） $d_{VC}$ 独立于 learing algorithm, input distribution, target function. 不管它们是什么样，$d_{VC}$只由假设集决定。 VC dimension of perceptrons $d_{VC} = d+1$, d is the working dimension of perceptrons (证明见书) 例如：2D perceptron 位于 2D plane，所以$d_{VC}=3$; 而 3D perceptron位于3D space，所以$d_{VC}=4$ $d_{VC} \\ (d+1)$ 是权重参数的个数 ($w_0, w_1, \\cdots, w_d$)。1 代表bias项，d代表每个样本点的维度数。 Interpreting the VC dimension VC维是自由度 (Degrees of freedom)\n不同的weights就是不同的 hypothesis。Every dimension of $d_{VC}$ is a tunable parameter, and parameters create degrees of freedom.\nNumber of parameters analog degrees of freedom; $d_{VC}$ equivalent \u0026ldquo;binary\u0026rdquo; degrees of freedom.\nThe usual suspects\npositive ray: $d_{VC}=1$，一个参数对应一个边界 positive interval: $d_{VC}=2$，两个参数对应两个边界，确定一个hypothesis Not just parameters\nParameters 在实际情况中可能不是自由度。 每个 perceptron 有 2 个 parameters ($w_0, w_1$ 对应 $b$ 和$x$). So 4 perceptron have 8 parameters those have an impact on a hypothesis, but the $d_{VC}$ maybe not 8. $d_{VC}$ measures the effective number of parameters Number of data points needed 经验法则： $N \\geq 10 d_{VC}$\nVC维的两个量化参数: $\\varepsilon, \\delta$:\n$$ \\mathbb P [ |E_{in}(g) - E_{out}(g)| \u0026gt; \\varepsilon] \\leq \\underbrace{ 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} }_\\delta $$\n$\\delta$是 N 的函数：$N^{d_{VC}} e^{-N}$ （$m_{\\mathcal H}(2N)$是N的多项式）\n横坐标是 N，纵坐标是概率上界 $\\delta$，每条曲线代表不同的VC维:\n$d_{VC}$越大，概率会先上升得越高，之后负指数take the control，突然急剧减小。因为这是概率，所以只考虑小于1 ($10^0$ 以下)的部分，概率越小越好。\n在同一概率下，增加VC维，N的值也呈线性增加。更大的VC维（要调节更多的参数）需要更多的样本点。为了达到很小的 Bad events概率，就需要更多的样本点。\nRule of thumb:(经验) $N \\geq 10 d_{VC}$\nRearranging things For VC inequality，把 Bad events 发生的概率称为 $\\delta$\n$$ \\mathbb P [ |E_{out}(g) - E_{in}(g)| \u0026gt; \\varepsilon] \\leq \\underbrace{ 4 m_{\\mathcal H} (2N) e^{-\\frac{1}{8} \\varepsilon^2 N} }_\\delta $$\n从 $\\delta$ 中推出 ε :\n$$ \\varepsilon = \\underbrace{ \\sqrt{\\frac{8}{N} ln \\frac{4 m_{\\mathcal H} (2N)}{\\delta}} }_\\Omega $$\n$\\Omega$是$N, \\mathcal H, \\delta$ 的函数。\nGood events 表示为：$|E_{out} - E_{in}|\\leq \\Omega(N,\\mathcal H, \\delta)$，它的概率是 $P(good) \\geq 1-\\delta$。\nGeneralization bound 通常 Eout 比 Ein 大，所以可以去掉绝对值，那么 Good events 就是: $E_{out}-E_{in} \\leq \\Omega(N,\\mathcal H, \\delta)$，称为 Generalization error。\n然后移项得到 $E_{out}$:\n$$ E_{out} \\leq \\underbrace{ E_{in} + \\underbrace{\\Omega(N,\\mathcal H, \\delta)}{\\text{Generalization error}} }{\\text{Generalization bound}} $$\n$E_{in}$ 从训练样本中得知，再根据以上关系，就可得知Eout。\n","date":"2021-12-12T20:01:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec6_7_generalization_dvc/","title":"watch: AML 06 | Genearalization-dVC"},{"content":"Video 6 Linear Models 9-29-2021\nOutline\nInput representation Linear Classificatin Linear Regression Nonlinear Transformation Review of Lecture 2\nLearning is feasible in a probabilistic sense.\nRed marble frequence $\\nu$ in the bin is unknow → $E_{out}$ is unknown → $E_{in}$: red marble frequence in the sample $\\mu$ → 最佳假设 g 使 $E_{in}$ 和 $E_{out}$ 最接近，这时Bad events 的概率为: Ein 和 Eout 相差超过 $\\epsilon$ 的概率 $|E_{in}(g) - E_{out}(g)| \u0026gt; \\epsilon$\n因为 g 肯定是假设集 H 中 M 个假设的其中之一，每个都有可能:\n$$ |E_{in}(h_2) - E_{out}(h_2)| \u0026gt; \\epsilon\\ \\textbf{ or } |E_{in}(h_2) - E_{out}(h_2)| \u0026gt; \\epsilon\\ \\textbf{ or }\\ \\cdots |E_{in}(h_M) - E_{out}(h_M)| \u0026gt; \\epsilon $$\n也就是 M 个假设对应的Bad events发生概率之和（各假设之间无overlapping，也就是最坏的情况，但通常各假设间有相关性），把M加到Hoeffding Inequality右侧，称为Union Bound：\n$$ P[ |E_{in}(g)-E_{out}(g)|\u0026gt; \\epsilon ] \\leq 2M e^{-2 \\epsilon^2 N} $$\n如果样本数量N足够大，Bad event 的概率就会变小.\nLinear model Weights are linear related.\nDigits pictures:\nraw input: $\\mathbf{x} = (x_0, x_1, \\cdots, x_{256})$ (x0是bias)\nlinear model: $(w_0, w_1, \\cdots, w_{256})$ (inputs constribution,w0=1)\nFeatures: Extracted useful information （257维太高,线性模型不太行）\nIntensity and symmetry $\\mathbf x=(x_0, x_1, x_2)$， $x_0=$bias\nlinear model: $(w_0, w_1, w_2)$, $w_0=1$\nPLA is not so smart. It focus on one misclassified point and update the weight, then the in-sample error maybe better or worse. There are a lot of flactration during iterations. 错过了最小误差，就回不去了。\n$E_{out}$ 是验证集上的误差，error of out-of-sample follows $E_{in}$\nPocket: Keep the best weights (in pocket). Replace it when finding better in the future.\nLinear Classification Outputs of a linear model are binary\n$$ h(\\mathbf x) = \\operatorname{sign} \\left( \\sum_{i=0}^d w_i x_i \\right) : { +1,-1} $$\n+1 or -1 (Approve or Deny)\nLinear Regression Outputs of a linear model are real-valued\n$$ h(\\mathbf x) = \\sum_{i=0}^d w_i x_i = \\mathbf{w}^T \\mathbf{x} \\quad \\text{(w0 for bias x0)} $$\n不再传给sign函数，判断$\\pm 1$分类\ndata set: $\\rm (\\pmb x_1, y_1), (\\pmb x_2, y_2),\\cdots (\\pmb x_N, y_N)$\n用线性回归去复制 data set，然后预测未来x的y。\n用假设 $h(\\mathbf x) = \\mathbf w^T \\mathbf x$ 近似未知目标函数 $f(\\mathbf x)$:\n$$ \\left(h(\\mathbf x) - f(\\mathbf x) \\right)^2 $$\n(Square error will make solving linear regression problem easily one-shot.)\nIn-sample error:\n一个特征是点到直线距离，两个特征是点到超平面距离。\n$$ E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^N (h(\\mathbf x_n) - y_n)^2 $$\nIn-sample error 是权重 $\\mathbf w$ 的函数 （$\\mathbf x$和y都是固定的训练样本,只有$\\mathbf w$是变量），线性回归的目标是找到使 In-sample error 最小的$\\mathbf w$:\n$$ \\begin{aligned} E_{in}(\\mathbf w) \u0026amp;= \\frac{1}{N} \\sum_{n=1}^N (\\mathbf w^T \\mathbf x_n - y_n)^2 \\\\ \u0026amp;= \\frac{1}{N} | \\mathbf{Xw} - \\mathbf y |^2 \\end{aligned} $$\n（把求和变成矩阵，方便求导找最值）其中：\n$$ \\mathbf X= \\begin{bmatrix} \\mathbf x_1^T \\\\ \\mathbf x_2^T \\\\ \\vdots \\\\ \\mathbf x_N^T \\end{bmatrix}, \\mathbf y= \\begin{bmatrix} y_1^T \\\\ y_2^T \\\\ \\vdots \\\\ y_N^T \\end{bmatrix} $$\n求 $E_{in}$ 的最小值：对 $\\mathbf w$ 求导，并令其等于0:\n$$ \\begin{aligned} E_{in}\u0026rsquo;(\\mathbf w) \u0026amp;= 0 \\\\ \\frac{2}{N} \\mathbf X^T (\\mathbf {Xw} -y) \u0026amp;= 0 \\\\ \\mathbf X^T \\mathbf {Xw} \u0026amp;= \\mathbf X^T y \\\\ \\mathbf w \u0026amp;= \\mathbf X^{\\dagger} y \u0026amp; \\text{where } \\mathbf X^\\dagger = (\\bf X^T X)^{-1} X^T \\end{aligned} $$\nPerceptron is more similar to the learning process that you\u0026rsquo;re just trying to learn something from one iteration to the other iteration. Here it\u0026rsquo;s not iterative. Linear regression is a kind of one-shot learner that learns one iteration.\n$\\mathbf X^\\dagger$ is the pseudo-inverse of $\\mathbf X$:\n$$ \\underbrace{ \\begin{pmatrix} \\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{10} \u0026amp; \\cdots x_{N0} \\ x_{01} \u0026amp; x_{11} \u0026amp; \\cdots x_{N1} \\ \\vdots \\ x_{0d} \u0026amp; x_{1d} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }_{(d+1)\\times N}\n\\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{01} \u0026amp; \\cdots x_{0d} \\ x_{10} \u0026amp; x_{11} \u0026amp; \\cdots x_{1d} \\ \\vdots \\ x_{N0} \u0026amp; x_{N1} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }_{N\\times (d+1)}\n\\end{pmatrix}^{-1}\n\\underbrace{ \\begin{bmatrix} x_{00} \u0026amp; x_{10} \u0026amp; \\cdots x_{N0} \\ x_{01} \u0026amp; x_{11} \u0026amp; \\cdots x_{N1} \\ \\vdots \\ x_{0d} \u0026amp; x_{1d} \u0026amp; \\cdots x_{Nd} \\end{bmatrix} }{(d+1)\\times N} }{(d+1)\\times N} $$\n$\\mathbf w = \\mathbf X^\\dagger y = \\underbrace{[w_0\\ w_1\\ \\cdots w_{d}]}_{(d+1)\\times 1}$\nLinear regression algorithm 构建 the data matrix $\\mathbf X$ and the vector y from the data set $(\\mathbf X_1, y_1), \\cdots, (\\mathbf X_N, y_N)$\n$$ \\mathbf X = \\begin{bmatrix} \\cdots \\mathbf x_1^T \\cdots \\ \\cdots \\mathbf x_2^T \\cdots \\ \\vdots \\ \\cdots \\mathbf x_N^T \\cdots \\ \\end{bmatrix} , y= \\begin{bmatrix} y_1^T \\ y_2^T \\ \\vdots \\ y_N^T \\end{bmatrix} $$\n计算伪逆矩阵 $\\mathbf X^\\dagger = (\\bf X^T X)^{-1} X^T$\n返回 $\\mathbf w = \\mathbf X^\\dagger y$\nLinear regression for classification 利用线性回归一次性解出 $\\mathbf w$，将其作为perception的初值，再迭代。 Linear regression 学习一个实值函数 $y=f(x)$ 二分类函数的 $\\pm 1$ 也是实数 使用linear regression “训练”（学习）到最佳$\\mathbf w$（使$E_{in}$(平方误差)最小）$\\mathbf w^T \\mathbf x_n \\approx y_n = \\pm 1$ 将这个$\\mathbf w$ 作为perceptron的初始值开始训练，随机初始化w可能迭代很多次也不会收敛。 Linear regression boundary 线性回归 one-shot 解出的w 对应一条直线. 回归是为了使整体的 $E_{in}$ (点到超平面的距离) 最小，当两类数据分布不均匀时，超平面会偏移“分类边界”。in-smaple Error (平方误差)不是Classification error。再用 perception 优化分类结果。 Video 9\nNonlinear transformation Use $\\Phi$ to transform the non-linear input space $\\mathcal X$ to a linear space $\\mathcal Z$ (where there is linear relation between $\\mathbf w$s)\nAny point $\\mathbf x \\overset{\\Phi}{\\rightarrow} \\mathbf z$ preserves the linearity, so that points are linearly separable.\n$g(\\mathbf x) = \\tilde g(\\Phi(\\mathbf x)) = \\rm sign(\\tilde \\mathbf{w}^T \\Phi(\\mathbf x))$\nTransformation:\n$$ \\begin{aligned} \\mathbf x = (x_0, x_1, \\cdots, x_d)\\ \u0026amp;\\overset{\\Phi}{\\rightarrow} \\mathbf z = (z_0, z_1, \\cdots, z_{\\tilde d}) \u0026amp; \\text{维度可以不同,$x_0$是bias} \\\n\\mathbf{x_1, x_2, \\cdots, x_N}\\ \u0026amp;\\overset{\\Phi}{\\rightarrow} \\mathbf{z_1, z_2, \\cdots, z_N} \u0026amp; \\text{n个点都做变换}\\\ny_1, y_2, \\cdots, y_N \\ \u0026amp;\\overset{\\Phi}{\\rightarrow} y_1, y_2, \\cdots, y_N \u0026amp; \\text{标签不变} \\\n\\text{No weights in } \\mathcal X \u0026amp; \\qquad \\widetilde \\mathbf w =(w_0, w_1,\\cdots, w_{\\tilde d}) \u0026amp; \\text{z空间中建立线性模型} \\\ng(\\mathbf x) \u0026amp;= \\rm sign (\\tilde \\mathbf w^T \\Phi(\\mathbf x)) \\end{aligned} $$\n","date":"2021-12-12T20:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec3_linear_model/","title":"watch: AML 03 | Linear Model"},{"content":"Video 16 Classification 2021-12-06\nsupervised learning practical diagram\nK-means 对三螺线数据训练，分类结果不好，因为它是用于聚类的\nFuzzy C Means 是另一种改进的聚类算法，分类效果也不好\nDecision Tree 分类器对三螺线的训练样本分类效果很好\nK nearest neighbor 是非常强大的 lazy learner\nMultilayer neural network 强大\nPerformance evaluation 分类器预测类别标签有多准确？选择哪个分类器（模型）更合适？ Remember: The data have to be used both for training and testing. More training data → better generalization. More test data → better estimation for the classification error probability. Do not evaluate performance on training data → the conclusion would be optimistically biased. (否则会偏向训练集) Methods for estimating a classifier\u0026rsquo;s accuracy: Holdout method (reserve 2/3 for training and 1/3 for testing) random subsampling (iterative holdout) Cross-validation (partition the data into k folders Stratified oversampling and undersampling (保持类别比例) Bootstrap (sampling with replacement) Comparing classifiers: Confidence intervals Cost-benefit analysis and ROC Curves Once evaluation is finished, all the available data can be used to train the final classifier. (知道了最佳参数后，再用全部的数据训练最佳假设) Hold out method Given data is randomly partitioned into two independent sets 比如：2/3 作为Training set 去构建模型，1/3作为Test set 去估计准确率 Random sampling: It is a variation of holdout method.\nRepeat the method k times, accuracy is estimated as average of obtained accuracies. Confusion matrix Represents the number of correct and incorrect predictions made by the classification model in comparison with the real outcomes (actual class).\nTP or True positive: # of tuples in class positive that were labeled by the classifier as class positive. FN or False negative: # of tuples in class positive that were labeled by the classifier as class negative FP or False positive: # of tuples in class negative that were labeled by the classifier as class positive. TN or True negative # of tuples in class negative that were labeled by the classifier as class negative. Evaluation measures\nMeasure Formula Accuracy, recognition rate (TP+TN)/all Error rate, misclassification rate (FP+FN)/all Sensitivity, true positive rate, recall TP/(TP+FN) Specificity, true negative rate TN/(TN+FP) Precision TP/(TP+FP) F, F1,F-score, Harmonic mean of precision and recall $\\frac{2 \\times \\rm Precision \\times recall}{\\rm Precision + recall}$ $F_\\beta$ where $\\beta$ is a none negative real number $\\frac{(1+\\beta)^2 \\times \\rm Precision \\times recall}{\\beta^2 \\times \\rm Precision +recall}$ Accuracy/recognition rate: the proportion of the total number of predictions that were correct. Error rate: 1- accuracy Precision: what % of tuples that the classifier labeled as positive are actually positive (查准率) Recall: what % of positive tuples did the classifier label as positive? (查全率) 当数据几乎是均匀分布时：准确性可以成为一个很好的评估指标\n比如100个人，99个没患癌，1个是癌症患者，但是模型结果是100个人都是健康，准确率99%，但它并不是可靠的模型。\nImbalanced data: There is an important class which is rare. e.g. cancerous patient Classifier may ignore the small class! Accuracy is not a good measurement as it does not consider FN rate that is so important in imbalanced data. In this case, classifier evaluation measures such sensitivity (or recall), specificity, precision, F-measure are better suited. Evaluation 也可以关注其他的指标：\nSpeed, Robustness, Scalability, Interpretability\nReceiving Operating Characteristic (ROC) Represent a relation between sensitivity and specificity for a given classifier.\nThe area under the curve is the measure of the accuracy of the classifier. The perfect accuracy is equal to one. The closer to red line, the less accurate model 如果模型的准确率显著低于红线，是不能接受的(有错)。ROC 曲线上升越快，越接近1，越好 (Learner 1最好)\nIt can be used for visual comparison of classification models.\nROC space: Two dimensional: FP rate on X axis → FPR=FP/(TN+FP) TP rate on Y axis → TPR=TP/(TP+FN) （灵敏度） FPR=1-SPC （= 1- 特异度） Model Selection Criteria Model selection criteria is always based on a compromise between the complexity of the model and its prediction accuracy on the training data\nGiven a dataset, basically we are looking for the simplest model that attains highest accuracy.\nModel 1 Model 2 Model 3 Complexity ✓✓ ✓ x (overfit) Training error xx ✓ ✓✓ Overall - ✓ - Ensemble system - Strategies \u0026amp; components 合奏系统\n每次抽取不同的样本(子集)，训练多个模型，然后聚合（aggregation）起来，误差可能更小\n也可以训练不同种类的分类器：感知机，DT,kNN,SVM\u0026hellip;\nEnsemble 系统有两Key Component: 分类算法（注意训练集样本的多样性）和融合方法（简单：多数票）\nEnsemble 适合用于很大容量数据，也可以用于很小容量数据。\nLarge volume data:\nSmall size data:\n数据很少，用复杂的模型可能导致过拟合，所以第一次使用比较弱的感知机，有3个点分错了，增加它们的权重，使它们更可能被抽取到作为下一次的训练样本。第二次分类后，再强调分错的2个蓝点。第三次分类，就只有1个红点分错了。Ensemble 使得模型不复杂，更准确\n其他优势：处理复杂的决策边界，非线性情况，实时\u0026hellip;\n","date":"2021-12-12T19:58:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/aml_classification/","title":"watch: AML | Classification"},{"content":"Outline\nExample of machine Learning Components of Learning Perceptron Types of learning Puzzle Machine Learning Essence A pattern exists We cannot pin it down mathematically we have data on it. Perceptron $h(\\mathbf x) = \\operatorname{sign} \\left( \\sum_{i=0}^d w_i x_i \\right) = \\operatorname{sign} (\\mathbf w^T \\mathbf x)$ PLA steps: Given the training set: $(\\mathbf x_1, \\mathbf y_1),(\\mathbf x_2, \\mathbf y_2), \\cdots, (\\mathbf x_N, \\mathbf y_N)$ pick a misclassified point: $sign (\\mathbf w^T \\mathbf x_n) \\neq y$ update the weight vector: $\\mathbf w \\leftarrow \\mathbf w + y_n \\mathbf x_n$ Types of learning Supervised learning: input \u0026ldquo;correct output\u0026rdquo;. Unsupervised learning: no \u0026ldquo;correct output\u0026rdquo; input. Reinforcement learning: introduce the grade of output. 这门课在证明一件事：通过fit数据，就可以”learn“到未知目标函数。\n","date":"2021-12-12T19:30:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec1_learning_problem/","title":"watch: AML 01 | Learning Problem"},{"content":"Video 15 Clustering 2021-11-22\nOutline:\nClustering K-means K nearest neighbor Cluster Cluster is a set of data objects that similar to one another within the same group dissimilar to the objects in other groups High quality clusters: High intra-class similarity; Low inter-class similarity Cluster analysis an unsupervised learning (unlabeled) 给定一组数据对象 找到数据对象之间的相似性 把相似的数据对象归到 clusters 典型应用:\nAs a stand-alone tool to get insight into data distribution As a preprocessing step for other algorithms (classifier, regressor) 聚类方法的质量的相关因素：\nthe similarity measure used by the method its implementation its ability to discover some or all of the hidden patterns 聚类分析的考虑因素\nPartitioning criteria Single level vs. hierarchical partitioning (often, multi-level hierarchical partitioning is desirable) Separation of clusters Exclusive (e.g., one customer belongs to only one region) vs. non-exclusive (e.g., one document may belong to more than one class) 是否专属于1类 Similarity measure Distance-based (e.g., Euclidian, road network, vector) vs. connectivity-based (e.g., density or contiguity) Clustering space Full space (often when low dimensional) vs. subspaces (often in high-dimensional clustering) 聚类的挑战和要求\nQuality Ability to deal with different type of attributes (不同属性) Discovery of clusters with arbitrary shape (任意形状) Ability to deal with noisy data (噪声) Interpretability and usability Constraint based clustering Scalability Constraint based clustering High dimensionality Incremental clustering and insensitivity to input order Similarity measure 对于两个样本点的第i个维度: $x_i$和$y_i$，两者的相似性可以用一个距离函数表达：$\\rm d(x_i, y_i)$ Similarity measure are usually different based on type of data: interval-scaled, boolean, categorical, ordinal ratio, and vector variables. 距离种类： Euclidean: $\\sqrt{\\sum_{i=1}^{k}\\left(x_{i}-y_{i}\\right)^{2}}$ 两点所有属性间的距离 Manhattan: $\\sum_{i=1}^{k}\\left|x_{i}-y_{i}\\right|$ Minkowski: $\\left(\\sum_{i=1}^{k}\\left(\\left|x_{i}-y_i\\right|\\right)^{2}\\right)^{1 / q}$ Major approaches Partitioning approaches (分区): They create various partitions and then evaluate them by some criterion e.g., minimizing the sum of square errors typical methods: k-means, k-medoids, CLARANS\nHierarchical approaches (分层): They create a hierarchical decomposition of the set of data (or objects) using some criterion typical methods: Diana, Agnes, BIRCH, CHAMELEON\nDensity-based approaches: They are based on connectivity and density functions typical methods: DBSACN, OPTICS, DenClue\nGrid-based approaches: They are based on a multiple-level granularity structure typical methods: STING, WaveCluster, CLIQUE\nModel-based approaches: A model is hypothesized for each of the clusters and then aim to find the best fit of that model to each other typical methods: EM, SOM, COBWEB\nFrequent pattern-based: They are based on the analysis of frequent patterns typical methods: p-Cluster\netc\u0026hellip;\nPartitioning method K-means 可视化：Visualizing K-Means Clustering\nK nearest neighbors supervised, efficient, clustering, classification and regression learning algorithm ","date":"2021-12-11T14:44:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/aml_clustering/","title":"watch: AML | Clustering"},{"content":"Video 9 - Error and noise 10-18-2021\nOutline\nError measures Noisy targets Preamble to the theory Review of Lec 3\nLinear Models\nUsing “signal” to classify and regress\nsignal:\n$$ \\sum_{i=0}^d w_i x_i = \\mathbf{w^T x} $$\nLinear Classification: $h(\\mathbf x) = \\rm sign(\\mathbf{w^T x})$ (把信号传入threshold, PLA, Pocket)\nLinear Regression: $h(\\mathbf x) = \\mathbf{w^T x}$ (不把信号传入threshold, one-shot learning)\n$\\mathbf w = \\mathbf{(x^T x)^{-1} x^T} y$\nError measures Quantify the dissimilarity between the output of hypothesis $h$ and the output of the unknown target function $f$.\nAlmost all error measures are pointwise\nCompute $h$ and $f$ on individual points $\\mathbf x$ using a pointwise error $e(h(\\mathbf x), f(\\mathbf x))$:\nBinary error: $e(h(\\mathbf x), f(\\mathbf x))= [![ h(\\mathbf x) \\neq f(\\mathbf x) ]!]$ （不相等error=1, 相等error=0） (Classification)\nSquared error: $e(h(\\mathbf x), f(\\mathbf x)) = (h(\\mathbf x) - f(\\mathbf x))^2$ (真实距离) (Regression)\nIn-sample error: $h(x)$ 与 $f(x)$ 在各样本点上的差异\n$$ E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^N e(h(\\mathbf x_n), f(\\mathbf x_n)) $$\nOut-of-sample error: $h(x)$ 与 $f(x)$ 在空间所有点上的偏差的期望\n$$ E_{out}(h) = \\mathbb E_x [e(h(\\mathbf x), f(\\mathbf x))] $$\nHow to choose the error measure\nFalse accept and False reject\nconfusion matrix (混淆矩阵):\n$$ \\begin{array}{c|lcr} \u0026amp; \\qquad f (\\text{unknown}) \u0026amp; \\ h\u0026amp; +1 \u0026amp; -1 \\ \\hline +1 \u0026amp; \\text{no error} \u0026amp; \\text{false accept} \\ -1 \u0026amp; \\text{false reject} \u0026amp; \\text{no error} \\ \\end{array} $$\nThe error measure is pretty much related to the kind of application with different penalty.\nNoisy targets 确定的目标分布 $f(\\mathbf x) = \\mathbb E(y|\\mathbf x)$ + 噪声 $y-f(\\mathbf x)$\n有时相同的输入对应不同的标签，所以潜在关系不是一个\u0026quot;函数\u0026quot; $y=f(\\mathbf x)$，而是一个分布 $P(y|\\mathbf x)$\n$\\mathbf x$ 按照某种未知的分布 $P(\\mathbf x)$ 从空间$\\mathcal X$ 中抽取出来。标签 $y$ 服从分布 $P(y|\\mathbf x)$。所以输入 $(\\mathbf x,y)$ 是由联合分布 $P(\\mathbf x) P(y|\\mathbf x) = P(\\mathbf x,y)$ 产生。\nDetermistic target 是当 P(y|x)=0 的特殊的noisy target, 那时噪声=0，也就是 $y=f(\\mathbf x)$\nPreamble to the theory\nLearning is feasible in a probabilitstic sence: $E_{out}(g) \\approx E_{in}(g)$ We need $g\\approx f$, which means $E_{out}(g) \\approx 0$ $E_{out}(g) \\approx E_{in}(g)$ (Hoeffding Inequality) $E_{in}(g) \\approx 0$ (PLA, Pocket, Linear classification/regression) ","date":"2021-12-05T19:19:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec4_error_and_noise/","title":"watch: AML 04 | Error and Noise"},{"content":"Selecting dimensionality reduction with Pipeline and GridSearchCV (在sklearn网站搜索\u0026quot;gridsearchcv\u0026quot;发现的)\nPCA降维实例(GridSearchCV求最优参)\n若不进行降维，速度慢，准确度低(非常低不能接受)\n","date":"2021-11-22T03:24:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E9%99%8D%E7%BB%B4gridsearch/","title":"DR-GridSearch"},{"content":"Pytorch 保存模型与加载模型 保存和加载模型\n保存模型与加载\n使用的三个核心功能：\ntorch.save 将序列化对象保存到磁盘。此函数调用pickle模块把（模型、tensor、字典等）对象序列化。 torch.load 函数调用pickle的unpickling功能把文件反序列化到内存。 torch.nn.Module.load_state_dict 调用反序列化函数state_dict加载模型的参数字典。 1. 状态字典 state_dict 模型的state_dict包括：各层网络的可学习参数（权重和偏置），优化器的state_dict包括优化器的状态和超参数。 模型torch.nn.Module的可学习参数，用model.parameters()访问 2. 两种方式： 只保存网络中的参数（速度快，占空间少，推荐）\n1 2 3 4 5 6 7 #保存参数 torch.save(net1.state_dict(), \u0026#39;net_params.pt\u0026#39;) #或\u0026#39;.pth\u0026#39;,或\u0026#39;pkl\u0026#39; #加载 model = ModelClass(*args, **kwargs) #先实例化一个模型对象 model = model.load_state_dict(torch.load(PATH)) #把文件反序列化成字典对象，把参数传给模型 model.eval() #设置dropuout 和 batch normalization层为评估模式，否则可能导致模型推断结果不一致。 保存整个网络的结构和参数与加载：\n1 2 3 4 5 6 #保存 torch.save(net1, \u0026#39;net.pkl\u0026#39;) #加载 newmodel = torch.load(PATH) #不需重构模型，直接load newmodel.eval() 3. 保存和加载Checkpoint 用于推理/继续训练 保存训练状态：\n1 2 3 4 5 6 torch.save({\u0026#39;epoch\u0026#39;: epoch+1, #保存当前的迭代次数 \u0026#39;model_state_dict\u0026#39;: model.state_dict(), #保存模型参数 \u0026#39;optimizer_state_dict\u0026#39;: optimizer.state_dict(), #保存优化器参数 \u0026#39;loss\u0026#39;: loss, #其余一些想保持的参数都可以添加进来 ..., }, PATH) #后缀可以用 \u0026#39;.pth.tar\u0026#39;或 \u0026#39;.pth\u0026#39; 加载：\n1 2 3 4 5 6 7 8 9 10 11 12 model = ModelClass(*args, **kwargs) optimizer = OptimizerClass(*args, **kwargs) #** checkpoint = torch.load(PATH) model.load_state_dict(checkpoint[\u0026#39;model_state_dict\u0026#39;]) optimizer.load_state_dict(checkpoint[\u0026#39;optimizer_state_dict\u0026#39;]) epoch = checkpoint[\u0026#39;epoch\u0026#39;] loss = checkpoint[\u0026#39;loss\u0026#39;] model.eval() # - or - model.train() 。。。。。部分内容需公众号验证\n4. 在一个文件中保存多个模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 保存 torch.save({ \u0026#39;modelA_state_dict\u0026#39;: modelA.state_dict(), \u0026#39;modelB_state_dict\u0026#39;: modelB.state_dict(), \u0026#39;optimizerA_state_dict\u0026#39;: optimizerA.state_dict(), \u0026#39;optimizerB_state_dict\u0026#39;: optimizerB.state_dict(), ... }, PATH) # 加载 modelA = TheModelAClass(*args, **kwargs) modelB = TheModelBClass(*args, **kwargs) optimizerA = TheOptimizerAClass(*args, **kwargs) optimizerB = TheOptimizerBClass(*args, **kwargs) checkpoint = torch.load(PATH) modelA.load_state_dict(checkpoint[\u0026#39;modelA_state_dict\u0026#39;]) modelB.load_state_dict(checkpoint[\u0026#39;modelB_state_dict\u0026#39;]) optimizerA.load_state_dict(checkpoint[\u0026#39;optimizerA_state_dict\u0026#39;]) optimizerB.load_state_dict(checkpoint[\u0026#39;optimizerB_state_dict\u0026#39;]) modelA.eval() modelB.eval() # - or - modelA.train() modelB.train() 当保存一个模型由多个torch.nn.Modules组成时，例如GAN(对抗生成网络)、sequence-to-sequence (序列到序列模型), 或者是多个模 型融合, 可以采用与保存常规检查点相同的方法。换句话说，保存每个模型的 state_dict 的字典和相对应的优化器。如前所述，可以通 过简单地将它们附加到字典的方式来保存任何其他项目，这样有助于恢复训练。\nPyTorch 中常见的保存 checkpoint 是使用 .tar 文件扩展名。\n要加载项目，首先需要初始化模型和优化器，然后使用torch.load()来加载本地字典。这里，你可以非常容易的通过简单查询字典来访问你所保存的项目。\n请记住在运行推理之前，务必调用model.eval()去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。 如果你想要恢复训练，请调用model.train()以确保这些层处于训练模式。\n5. 使用在不同模型参数下的热启动模式 1 2 3 4 5 保存 torch.save(modelA.state_dict(), PATH) 加载 modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False) 在迁移学习或训练新的复杂模型时，部分加载模型或加载部分模型是常见的情况。利用训练好的参数，有助于热启动训练过程，并希望帮助你的模型比从头开始训练能够更快地收敛。\n无论是从缺少某些键的 state_dict 加载还是从键的数目多于加载模型的 state_dict , 都可以通过在load_state_dict()函数中将strict参数设置为 False 来忽略非匹配键的函数。\n如果要将参数从一个层加载到另一个层，但是某些键不匹配，主要修改正在加载的 state_dict 中的参数键的名称以匹配要在加载到模型中的键即可。\n6.通过设备保存/加载模型 6.1 保存到 CPU、加载到 CPU 1 2 3 4 5 6 保存 torch.save(model.state_dict(), PATH) 加载 device = torch.device(\u0026#39;cpu\u0026#39;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=device)) 当从CPU上加载模型在GPU上训练时, 将torch.device(\u0026lsquo;cpu\u0026rsquo;)传递给torch.load()函数中的map_location参数.在这种情况下，使用 map_location参数将张量下的存储器动态的重新映射到CPU设备。\n6.2 保存到 GPU、加载到 GPU 1 2 3 4 5 6 7 8 保存 torch.save(model.state_dict(), PATH) 加载 device = torch.device(\u0026#34;cuda\u0026#34;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH)) model.to(device) # 确保在你提供给模型的任何输入张量上调用input = input.to(device) 当在GPU上训练并把模型保存在GPU，只需要使用model.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))，将初始化的 model 转换为 CUDA 优化模型。另外，请 务必在所有模型输入上使用.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))函数来为模型准备数据。请注意，调用my_tensor.to(device)会在GPU上返回my_tensor的副本。 因此，请记住手动覆盖张量：my_tensor= my_tensor.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))。\n6.3 保存到 CPU，加载到 GPU 1 2 3 4 5 6 7 8 保存 torch.save(model.state_dict(), PATH) 加载 device = torch.device(\u0026#34;cuda\u0026#34;) model = TheModelClass(*args, **kwargs) model.load_state_dict(torch.load(PATH, map_location=\u0026#34;cuda:0\u0026#34;)) # Choose whatever GPU device number you want model.to(device) # 确保在你提供给模型的任何输入张量上调用input = input.to(device) 在CPU上训练好并保存的模型加载到GPU时，将torch.load()函数中的map_location参数设置为cuda:device_id。这会将模型加载到 指定的GPU设备。接下来，请务必调用model.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))将模型的参数张量转换为 CUDA 张量。最后，确保在所有模型输入上使用 .to(torch.device(\u0026lsquo;cuda\u0026rsquo;))函数来为CUDA优化模型。请注意，调用my_tensor.to(device)会在GPU上返回my_tensor的新副本。它不会覆盖my_tensor。 因此， 请手动覆盖张量my_tensor = my_tensor.to(torch.device(\u0026lsquo;cuda\u0026rsquo;))。\n6.4 保存 torch.nn.DataParallel 模型 1 2 3 4 保存 torch.save(model.module.state_dict(), PATH) 加载 # 加载任何你想要的设备 torch.nn.DataParallel是一个模型封装，支持并行GPU使用。要普通保存 DataParallel 模型, 请保存model.module.state_dict()。 这样，你就可以非常灵活地以任何方式加载模型到你想要的设备中。\n","date":"2021-11-22T02:56:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E4%BF%9D%E5%AD%98%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","title":"memo: PyTorch | Save Model"},{"content":"F1 score: F1 = 2* (precision * recall )/(precision+recall)\nsklearn.metrics.f1_score\nsklearn中 F1-micro 与 F1-macro区别和计算原理\n","date":"2021-11-22T02:16:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E8%AF%84%E4%BB%B7metrics/","title":"Metrics-sklearn"},{"content":"决策树调参\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import numpy as np import matplotlib.pyplot as plt ## 加载iris数据集 from sklearn.datasets import load_iris iris = load_iris() X,y = iris.data[:,:2], iris.target #只取前2个特征 ## 分割测试集与训练集 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify = y, random_state= 42) ## 适配决策树，并计算准确率 from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score dtc = DecisionTreeClassifier() dtc.fit(X_train, y_train) y_pred = dtc.predict(X_test) accuracy_score(y_test, y_pred) ## 画出决策树示意图 from sklearn.tree import export_graphviz from io import StringIO from IPython.display import Image import pydot def create_png(clf): dot_iris = StringIO() export_graphviz(clf.out_file = dot_iris, feature_name = iris.feature_names[:2], filled = True) graphs = pydot.graph_from_dot_data(dot_iris.getvalue()) return graphs[0].create_png() Image(create_png(dtc)) ## 用GridSearchCV搜索最佳参数 from sklearn.model_selection import GridSearchCV dtc = DecisionTreeClassifier() grid = {\u0026#39;criterion\u0026#39;: [\u0026#39;gini\u0026#39;,\u0026#39;entropy\u0026#39;], \u0026#39;max_depth\u0026#39;: [3,5,7,9,20] } gs = GridSearchCV(dtc, param_grid=grid, cv=5) gs.fit(X_train, y_train) ## 查看网格搜索得到的模型的准确率 accuracy_score(y_test, gs.predict(X_test)) ## 查看搜索结果 gs.cv_results_ gs.best_estimator_ ## 查看网格搜索得到的决策树示意图 Image(create_png(gs.best_estimator_)) ## 探索最大深度对决策树性能的影响 grid=[\u0026#39;max_depth\u0026#39;:range(3,50)] gs2 = GridSearchCV(dtc, param_grid=grid, cv=5) gs2.fit(X_train, y_train) gs2.cv_results_[\u0026#39;mean_test_score\u0026#39;] plt.plot(range(3,50), gs2.cv_results_[\u0026#39;mean_test_score\u0026#39;]) 随机森林-【机器学习】【sklearn】网格搜索GridSearchCV 1. 加载数据 from sklearn.datasets import load_wine\nwine = load_wine() X = wine.data y = wine.target\nfrom sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier()\n2. 网格搜索找出最优参数 param_grid = {\u0026ldquo;n_estimator\u0026rdquo;:np.arange(10,201,10), \u0026ldquo;max_features\u0026rdquo;:np.arange(0.1, 1, 0.1), \u0026ldquo;max_depth\u0026rdquo;: np.arange(3,13), \u0026ldquo;bootstrap\u0026rdquo;: [True, False] } #定义字典，设置参数的可取值\nfrom sklearn.model_selection import GridSearchCV\nmyGrid = GridSearchCV(rfc, param_grid=param_grid, cv=5) #构造网格搜索，内置k折交叉验证\nmyGrid.fit(X,y) #训练\nprint( myGrid.best_params_, #最优参数组合 myGrid.best_score_, myGrid.best_estimator_, #最优模型 myGrid.best_index_ )\n输出最优参数：\n1 2 3 4 5 6 7 8 9 10 11 from sklearn import metrics best_parameters = dict() best_parameters = grid_search.best_estimator_.get_params() for param_name in sorted(parameters.keys()): print \u0026#34;\\t%s: %r\u0026#34; % (param_name, best_parameters[param_name]) pipeline.set_params(clf__alpha = 1e-05, tfidf__use_idf = True, vect__max_df = 0.5, vect__max_features = None) pipeline.fit(X_train, y_train) pred = pipeline.predict(X_test) 3. 使用最优模型做分类 \u0026hellip;.\n","date":"2021-11-21T21:44:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9gridsearchcv/","title":"GridSearchCV-sklearn"},{"content":"每天一点sklearn之KFold(9.8)\nKFod把数据集划分成K份，返回一个索引生成器，可以用循环遍历它。\nclass sklearn.model_selection.KFold(n_splits='warm', shuffle=False, random_state=None)\nn_splits: K，把数据集划分成k份 shuffle: 打乱顺序再划分 random_state: 相当于随机种子，一般都要和shuffle搭配使用，只有当shuffle=True的时候，才有意义，每次打乱的结果是一样的 1 2 3 4 5 6 from sklearn.model_selection import KFold kf1 = KFold(n_splits=3, shuffle=True) #把数据集划分成3份 for train_index, test_index in kf1.split(xtrain[:20]): print(\u0026#39;In KFold,test_index is:{}\u0026#39;.format(test_index)) #第一份做验证集，剩下两份做训练集 print(\u0026#39;In KFold,train_index is:{}\u0026#39;.format(train_index)) 分层抽样，需要传入label：\n1 2 3 4 5 6 7 8 9 from sklearn.model_selection import StratifiedKFold skf = StratifiedKFold(n_splits=3,shuffle=True, random_state=1) for train_index, test_index in skf.split(xtrain[:20],ytrain[:20]): print(\u0026#39;In StratifiedFold,test_index is:{}\u0026#39;.format(test_index)) #第一份做验证集，剩下两份做训练集 print(ytest[test_index].value_counts()) #各类个数1:1 print(\u0026#39;In StratifiedFold,train_index is:{}\u0026#39;.format(train_index)) print(ytrain[test_index].value_counts()) ","date":"2021-11-21T15:28:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/kfold%E5%87%BD%E6%95%B0/","title":"KFold函数-sklearn"},{"content":"RNN Cell 一个线性单元 把输入x的维度（inputSize）变换到 hiddenSize 处理带有时间序列的数据 x2 与 h1 共同决定 h2 t时刻的输入 $x_t \\in \\mathbb R^3$ 经过 RNN cell 转换，变成了一个隐状态变量 $h_t \\in \\mathbb R^5$， 因此RNN cell是一个线性层（矩阵运算）映射到另外一个维度空间，不过这个线性层是共享的。\n$h_t$ 会参与下次 RNN cell 计算 $h_{t+1}$，把之前的信息合并。 在计算 $h_1$ 时，需要输入先验 $h_0$，如果没有先验，就输入与 $h_1, h_2\u0026hellip;$ 同维度的零向量。\n遍历数据集 $x_t \\in \\mathbb R^{\\rm input_size}$，做线性变换\n$$ W^{\\rm hidden\\ size \\times input\\ size}_{hi} xₜ + b_{hi} $$\n变换到一个 hidden_size×1 的向量；\n上一层的隐变量 $h_{t-1}$ 也进行线性变换 $W_{hh} h_{t-1} + b_{hh}$ 得到一个 hidden_size×1的向量， 它与 $x_t$ 的线性变换输出相加，得到的向量仍为 hidden_size × 1， 再做激活 tanh，算出隐向量 $h_t \\in \\mathbb R^{\\rm hidden_size}$\n两个线性运算可以合一起：\n$$ \\begin{aligned} \u0026amp; W_{hh} h_{t-1} + W_{hi} x_{t} \\\\ \u0026amp; = [W_{hh} \\ \\ W_{hi}]^{\\rm h\\ size \\times (h\\ size + i\\ size)} \\begin{bmatrix} h_{t-1} \\\\ x_t \\end{bmatrix}^{\\rm (h\\ size+i\\ size) \\times 1} \\\\ \u0026amp; = h_t^{\\rm h\\ size \\times 1} \\end{aligned} $$\n$h_t = \\rm tanh(W_{hi} x_t + b_{hi} + W_{hh}h_{t-1}+b_{hh})$\n在Pytorch中，可以自己构造 RNN cell，并处理序列的循环；也可以直接使用RNN。\n自己创建RNN Cell: cell = torch.nn.RNNCell (input_size=i_size, hidden_size = h_size)\n用 cell 计算下一时刻的隐变量：h_1 = cell (x_1, h_0)。\n这两个输入的 shape：x(batch_size, input_size)，h(batch_size, hidden_size)， h_1的维度与h_0一样。batch是一批的样本条数。\n数据集可以表达成一个张量：dataset.shape = (seqLen, batch_size, input_size)\n使用RNNCell：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import torch batch_size = 1 seq_len = 3 #有3个样本:x1,x2,x3 input_size = 4 #x都是4x1的向量 hidden_size =2 #h都是2×1的向量 cell = torch.nn.RNNCell( input_size=input_size, hidden_size=hidden_size ) dataset = torch.randn(seq_len, batch_size, input_size) #初始化h0，全零 hidden = torch.zeros(batch_size, hidden_size) for idx, input in enumerate(dataset): hidden = cell(input, hidden) RNN class 实例化RNN: cell = torch.nn.RNN(input_size = input_size, hidden_size=hidden_size, num_layer=num_layers)\nRNN的输出有两个张量，调用：out, hidden = cell(inputs, hidden)。\ninputs 是整个输入序列，输入的 hidden 是 $h_0$，out 是所有的隐层输出 $h_1 \\cdots h_N$，输出的 hidden 是最后一个 cell 的输出$h_N$。\ninputs 的形状是 (seqLen, batch, input_size)，hidden 的形状是 (numLayers, batch, hidden_size)，numLayers 是RNN的层数，每层最终都会输出一个隐变量。 output的形状为(seqLen, batch, hidden_size)。输出hidden与输入hidden的形状一样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch batch_size = 1 seq_len = 3 input_size = 4 hidden_size = 2 num_layers = 1 cell = torch.nn.RNN(input_size = input_size, hidden_size= hidden_size, num_layers = num_layers) inputs = torch.randn(seq_len, batch_size, input_size) hidden = torch.zeros(num_layers, batch_size, hidden_size) out, hidden = cell(inputs, hidden) Example: Seq ➔ Seq 例子： 序列 ➔ 序列，把“hello\u0026quot;转换到“ohlol”\n把输入“hello”变成由数字构成的向量：\n字符级别，为出现过的字符构造一个词典； 词级别，为出现过的单词构造词典。 这里为每个元素分配索引，用索引代替字母。再把每个索引变成一个向量（独热码），向量的长度与词典的条数一样\n$$ \\begin{aligned} \\begin{bmatrix} h \\\\ e \\\\ l \\\\ l \\\\ o \\end{bmatrix} \\underset{\\longrightarrow}{词典有4条索引} \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 2 \\\\ 3 \\end{bmatrix} \\rightarrow \\begin{matrix} [0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0] \\\\ [1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0] \\\\ [0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0] \\\\ [0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0] \\\\ [0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1] \\end{matrix} \\end{aligned} $$\n依次把 5 个独热向量输入 RNN，向量长度（input_size） = 4， output 是 5 个 RNN Cell的输出，每个输出对应于 4 个字母（h, e, l, o）中的一个，因此是一个多分类问题。\n所以设置每个 hidden 是一个长度为 4 的向量，这 4 个线性输出分别对应（h,e,l,o）。 把这个向量传入 softmax就变成了一个概率分布，表示 4 个字母分别可能的概率。\n再与真实标签的独热向量计算损失，即可用反向传播+梯度下降优化网络参数。\n自己设计RNN Cell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 import torch ## Step-1 准备数据 input_size = 4 # 每个样本的独热向量的长度为4 hidden_size = 4 # 隐变量的长度为4，对应4个输出类别 batch_size = 1 # number of tokens idx2char = [\u0026#39;e\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;] # 方便打印结果 x_data = [1, 0, 2, 2, 3] # 样本 \u0026#34;hello\u0026#34;（索引） y_data = [3, 1, 2, 3, 2] # 输出 \u0026#34;ohlol\u0026#34; 的标签(真实类别), o是第4类，h是第2类 one_hot_lookup =[[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]] # 把样本索引变成独热向量，其形状为 (seqSize, input_size) = (5, 4) x_one_hot = torch.Tensor([one_hot_lookup[x] for x in x_data]) # 全部样本分成几个 batch，-1表示 batch 的个数 num_batch 自动计算 inputs = x_one_hot.view(-1, batch_size, input_size) # (5,1,4) # label vector (seqSize, 1)。因为 batch_size 是 1，所以第 1 维度是 1 labels = torch.LongTensor(y_data).view(-1,1) # (5,1) ## Step-2 设计模型 class Model(torch.nn.Module): #继承自Module def __init__(self, input_size, hidden_size, batch_size): super(Model, self).__init__() self.batch_size = batch_size self.input_size = input_size self.hidden_size = hidden_size self.rnncell = torch.nn.RNNCell( input_size=self.input_size, hidden_size=self.hidden_size ) # 实例化 RNN Cell def forward(self, input, hidden): # Module 里面的 __call__函数调用了forward方法 # 用 input 和 hidden state 计算下一个 hidden state hidden = self.rnncell(input, hidden) # input: (batchSize, inputSize) return hidden # (batchSize, hiddenSize) def init_hidden(self): #生成默认的h0,全零矩阵 return torch.zeros(self.batch_size, self.hidden_size) # 实例化模型对象,batch_size 只有在构造默认 h0 时被用到 net = Model(input_size, hidden_size, batch_size) ## Step-3 损失和优化器 criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.1) #基于SGD的改进优化器 ## Step-4 训练循环 n_iters = 15 for epoch in range(n_iters): loss = 0 optimizer.zero_grad() #梯度归零 hidden = net.init_hidden() #生成 h0 print(\u0026#39;Predicted string: \u0026#39;, end=\u0026#39;\u0026#39;) # 遍历每个 batch，每个 batch 里有 batch_size 个 token。 # inputs:（num_batch, batchSize, inputSize） for input, label in zip(inputs, labels): # input: (batchSize, inputSize), label: (batchSize,) # zip 是沿着矩阵的第 0 维度拼接，也就是沿着 num_batch 方向拼接，所以一个矩阵对应一个label。 # 计算下一时刻的 hidden，维度为 (batchSize, hiddenSize) hidden = net(input, hidden) # 累加各 batch 的损失（需要构造计算图,不可用item()） loss += criterion(hidden, label) # 按维度 1 找 hidden 里的最大值的下标,就属于那类 _, idx = hidden.max(dim=1) # 看一下输出了什么字符，虽然字符对了，但它的概率不是1，所以还有损失。 print(idx2char[idx.item()], end=\u0026#39;\u0026#39;) loss.backward() # 计算梯度 optimizer.step() # 更新一步 print(f\u0026#39;, Epoch [{epoch+1}/{n_iters}], loss ={loss.item():.4f}\u0026#39;) #打印损失 调用pytorch的RNN类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 import torch ## 准备数据 input_size = 4 #共有helo 4种字符，需要用4元素的独热向量来表示每种字符 hidden_size = 4 #输出分别属于4类的概率 num_layers = 1 batch_size = 1 #每个batch是1个字符 seq_len = 5 #全部样本分成5个seq/batch：\u0026#34;h\u0026#34;;\u0026#34;e\u0026#34;;\u0026#34;l\u0026#34;;\u0026#34;l\u0026#34;;\u0026#34;o\u0026#34; idx2char = [\u0026#39;e\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;] x_data = [1,0,2,2,3] y_data = [3,1,2,3,2] #各样本的真实类别，用于索引真实类别对应的预测值 one_hot_lookup =[[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]] x_one_hot = [one_hot_lookup[x] for x in x_data] #把样本变成独热向量，其形状为seq×input_size inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size) labels = torch.LongTensor(y_data) #维度(seqSize×batchSize,1)：torch.Size([5])，所有样本的标签(类别) ## 设计模型 class Model(torch.nn.Module): def __init__(self, input_size, hidden_size, batch_size, num_layers=1): super(Model, self).__init__() self.num_layers = num_layers self.batch_size = batch_size #用于创建默认h0 self.input_size = input_size self.hidden_size = hidden_size self.rnn = torch.nn.RNN(input_size= self.input_size, hidden_size = self.hidden_size, num_layers=num_layers) def forward(self, input): #Module调用此方法，自动迭代数据集，给出最终输出 hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size) #如果在外面定义，这就不用写 out, _ = self.rnn(input, hidden) return out.view(-1, self.hidden_size) #输出变成两维的（seqLen×batchSize, hiddenSize），交叉熵损失只能接受二维的Tensor和一维的labels：一条样本的向量对应一个label net = Model(input_size, hidden_size, batch_size, num_layers) ## 损失和优化器 criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.05) #lr 0.1比较好 ## 训练循环 for epoch in range(15): optimizer.zero_grad() outputs = net(inputs) #inputs的维度：SeqLen*BatchSize*InputSize，outputs的维度：SeqLen*BatchSize*HiddenSize loss = criterion(outputs, labels) #labels维度是SeqLen×BatchSize×1，也就是（5,1） loss.backward() optimizer.step() _, idx = outputs.max(dim=1) idx = idx.data.numpy() print(\u0026#39;Predicted:\u0026#39;, \u0026#39;\u0026#39;.join([idx2char[x] for x in idx]), end=\u0026#39;\u0026#39;) #把4个字符拼成一个单词 print(\u0026#39;,Epoch [%d/15] loss =%.3f\u0026#39; % (epoch+1, loss.item())) #打印损失 独热向量在编码词/字符时：\n独热向量维度太高，字符级别需128维，单词级别维度太高 向量太稀疏 是硬编码的 希望吧 单词/字符 联系到一个低维、稠密、从数据中学习到的向量，流行又强大的方法是嵌入层Embedding。把高维稀疏样本映射到低维稠密的空间(数据降维)。\n独热向量$x_n$通过嵌入层 Embed 变成稠密的表示，经过RNN线性变换后，再用一个线性层，让最后的隐变量输出与分类的数量一致\n4维转5维，构建一个查找表：\n查找表做转置，再乘以输入的独热向量，就可取出“对应行”\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 num_class = 4 #类别 input_size = 4 #输入样本是4维的 hidden_size = 8 #隐变量输出是8维 embedding_size = 10 #把输入从4维嵌入到10维空间 num_layers = 2 #2层RNN batch_size = 1 seq_len = 5 idx2char = [\u0026#39;e\u0026#39;,\u0026#39;h\u0026#39;,\u0026#39;l\u0026#39;,\u0026#39;o\u0026#39;] x_data = [[1,0,2,2,3]] y_data = [3,1,2,3,2] inputs = torch.LongTensor(x_data) labels = torch.LongTensor(y_data) class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.emb = torch.nn.Embedding(input_size, embedding_size) #嵌入层查找表的大小：inputSize×embeddingSize，对输入数据做维度转换 self.rnn = torch.nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #RNN输入:(batchSize, seqLen, embeddingSize)，RNN输出:(batchSize, seqLen, hiddenSize) self.fc = torch.nn.Linear(hidden_size, num_class) #维度变换: 从hidden_size到类别数量 def forward(self, x): hidden = torch.zeros(num_layers, x.size(0), hidden_size) x = self.emb(x) #(batch,seqLen, embeddingSize) 把长整形的张量转变成嵌入层稠密的向量 x, _ = self.rnn(x, hidden) x = self.fc(x) return x.view(-1,num_class) #维度等于类别数 net = Model() #后面都一样 criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=0.05) for epoch in range(15): optimizer.zero_grad() outputs = net(inputs) #inputs的维度：SeqLen*BatchSize*InputSize，outputs的维度：SeqLen*BatchSize*HiddenSize loss = criterion(outputs, labels) #labels维度是SeqLen×BatchSize×1，也就是（5,1） loss.backward() optimizer.step() _, idx = outputs.max(dim=1) #取出值最大的维度的索引，也就是所属的第几类。 idx = idx.data.numpy() #索引从0开始 print(\u0026#39;Predicted:\u0026#39;, \u0026#39;\u0026#39;.join([idx2char[x] for x in idx]), end=\u0026#39;\u0026#39;) #把4个字符拼成一个单词 print(\u0026#39;,Epoch [%d/15] loss =%.3f\u0026#39; % (epoch+1, loss.item())) #打印损失 用RNN对MNIST图片分类 感觉里面的流程挺规范的。\n初始化h0 为一个对角矩阵； 用了3层RNN，只用10个epoch就达到96%的accuracy了。 大概原理 把MNIST中每张图片看成一个序列，这个序列含有28个$x$（对应每一行像素），x的维度是28。每行像素都变换到一个h，这个h会参与计算下一行的h，最终得到第28行的h，将此h输入线性全连接层，变换到10维，每一维对应着0-9的每一类。然后这个10维的向量通过softmax，就是属于每类的概率了。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 import torch import torch.nn as nn import torch.nn.functionall as F import torch.optim as optim import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt # 1. 加载数据集 MNIST batch_size = 128 train_dataset = torchvision.datasets.MNIST(root=\u0026#39;./\u0026#39;, train=True, transform=transforms.ToTensor(), download=True) #加载训练集 test_dataset = torchvision.datasets.MNIST(root=\u0026#39;./\u0026#39;, train=False, transform=transforms.ToTensor()) #加载测试集 train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) #构造数据加载器，每次迭代从数据集中取出batchSize个样本，每个epoch都不同 test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # 查看一下数据 def imshow(img): npimg = img.numpy() plt.imshow(np.transpose(npimg,(1,2,0))) dataiter = iter(train_loader) #取出一份128张图片 images, labels = dataiter.next() imshow(torchvision.utils.make_grid(images,nrow=15)) #一排15张图 # 2. 定义模型 N_STEPS = 28 # 1张图片是1个序列，所以1个序列包含28个样本 N_INPUTS = 28 # 输入数据的维度(1行有28个像素) N_NEURONS = 150 # RNN中间的特征的大小(hiddenSize) N_OUTPUT = 10 # 输出数据的维度(分类的个数) N_EPHOCS = 10 # epoch的大小(训练10轮) N_LAYERS = 3 # 3层RNN class ImageRNN(nn.Module): #继承自nn.Module def __init__(self, batch_size, n_inputs, n_neurons, n_outputs, n_layers): super(ImageRNN, self).__init__() self.batch_size = batch_size #每次输入batchSize张图片 self.n_inputs = n_inputs # 输入的维度 self.n_outputs = n_outputs # 分类的大小 self.n_neurons = n_neurons # RNN中输出的维度 self.n_layers = n_layers # RNN中的层数 self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons, num_layers=self.n_layers) #实例化RNN对象，inputSize,hiddenSize self.FC = nn.Linear(self.n_neurons, self.n_outputs) def init_hidden(self): #生成h0是一个对角矩阵 # (num_layers, batch_size, n_neurons) # initialize hidden weights with zero values # 这个是net的memory, 初始化memory为0 return (torch.zeros(self.n_layers, self.batch_size, self.n_neurons).to(device)) def forward(self,x): #计算RNN Cell的输出 # transforms x to dimensions : n_step × batch_size × n_inputs x = x.permute(1,0,2) # 需要把n_step放在第一个维度 self.batch_size = x.size(1) # 每次需要重新计算batch_size, 因为可能会出现不够一个batch的情况 self.hidden = self.init_hidden() # 初始化hidden state rnn_out, self.hidden = self.basic_rnn(x,self.hidden) # 前向传播 out = self.FC(rnn_out[-1]) # 求出每一类的概率 return out.view(-1,self.n_outputs) # 最终输出大小 : batch_size × n_output(10)。然后接softmax找出所属类别，计算损失。 # 测试一下 device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) #方便之后使用gpu运算 model = ImageRNN(batch_size, N_INPUTS, N_NEURONS, N_OUTPUT, N_LAYERS).to(device) #构造RNN模型 model.basic_rnn.weight_hh_l0.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) # 初始化模型的weight为对角矩阵 model.basic_rnn.weight_hh_l1.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) model.basic_rnn.weight_hh_l2.data = torch.eye(n=N_NEURONS, m=N_NEURONS, out=None).to(device) dataiter = iter(train_loader) # 取1份数据 images, labels = dataiter.next() model.hidden = model.init_hidden() #初始化h0 logits = model(images.view(-1,28,28).to(device)) #输入模型 print(logits[0:2]) #打印前2张图的hidden输出:10维向量 # 3. 定义损失函数和优化器 criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(),lr=0.001) def get_accuracy(logit, target, batch_size): #用来最后计算模型的准确率 corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum() accuracy = 100.0 * corrects/batch_size return accuracy.item() # 4. 训练 for epoch in range(N_EPHOCS): train_running_loss = 0.0 train_acc = 0.0 model.train() # trainging round for i, data in enumerate(train_loader): #每次取出一个batch optimizer.zero_grad() #梯度清零 # reset hidden states model.hidden = model.init_hidden() #初始化h0 # get inputs inputs, labels = data #输入一个batch的样本和标签 inputs = inputs.view(-1,28,28).to(device) labels = labels.to(device) # forward+backward+optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_running_loss = train_running_loss + loss.detach().item() train_acc = train_acc + get_accuracy(outputs, labels, batch_size) model.eval() print(\u0026#39;Epoch : {:0\u0026gt;2d} | Loss : {:\u0026lt;6.4f} | Train Accuracy : {:\u0026lt;6.2f}%\u0026#39;.format(epoch, train_running_loss/i, train_acc/i)) # 5. 评价模型：计算测试集准确率 est_acc = 0.0 for i,data in enumerate(test_loader,0): inputs, labels = data labels = labels.to(device) inputs = inputs.view(-1,28,28).to(device) outputs = model(inputs) thisBatchAcc = get_accuracy(outputs, labels, batch_size) print(\u0026#34;Batch:{:0\u0026gt;2d}, Accuracy : {:\u0026lt;6.4f}%\u0026#34;.format(i,thisBatchAcc)) test_acc = test_acc + thisBatchAcc print(\u0026#39;============平均准确率===========\u0026#39;) print(\u0026#39;Test Accuracy : {:\u0026lt;6.4f}%\u0026#39;.format(test_acc/i)) #96% # 6. 定义hook, 查看模型中间过程 k折交叉验证 《动手学深度学习》的pytorch版\n返回第i折训练和验证数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def get_k_fold_data(k,i,X,y): #我不想把标签分出来,Dataset对象可以分 assert k\u0026gt;1 fold_size = X.shape[0] // k #先做除法，再向下取整(不大于) X_train, y_train = None, None for j in range(k): idx = slice(j*fold_size, (j+1)*fold_size) #切片函数slice(start, end, step) X_part, y_part = X[idx,:], y[idx] if j == i: X_valid, y_valid = X_part, y_part elif X_train is None: X_train, y_train = X_part, y_part else: X_train = torch.cat((X_train, X_part), dim=0) #增加行数 y_train = torch.cat((y_train, y_part), dim=0) return X_train, y_train, X_valid, y_valid 训练k次\n1 2 3 4 5 def k_fold(k, X_train, num_epochs, learning_rate, weight_decay, batch_size): train_ls_sum, valid_ls_sum = 0, 0 for i in range(k): data = get_k_fold_data(k, i, X_train) .... ","date":"2021-11-21T10:26:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/12_rnn%E5%9F%BA%E7%A1%80/","title":"watch: PyTorch - 刘二 12 | RNN Basics"},{"content":"用pytorch 提供的工具构建线性模型\nforward 前馈：求一组样本的损失 backward 反向：求损失关于各w的梯度 update 梯度下降更新w 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import torch ## 1. 准备数据 x_data = torch.Tensor([[1.0],[2.0],[3.0]]) #3条数据作为一个batch，把3个样本一次性求出一个损失，定义成矩阵以便利用numpy的广播机制 y_data = torch.Tensor([[2.0],[4.0],[6.0]]) #y与x一样也是3x1的矩阵 ## 2. 设计模型: 计算 y_pred = x*w +b class LinearModel(torch.nn.Module): #把模型定义为一个类，所有的神经网络模型都继承自nn.Module（包含了很多训练方法(i.e.反向传播)） def __init__(self): #必须实现构造函数,初始化对象 super(LinearModel, self).__init__() #super调用父类的构造函数 self.linear = torch.nn.Linear(1,1) #构造线性单元对象：输入样本和输出样本的特征（维度）都是1列，样本条数是一个batch（3条） def forward(self, x): #必须实现计算预测值方法 y_pred = self.linear(x) #linear也是callable的对象，加上括号()会调用nn.Linear的 call 方法，其中包含forward方法 wx+b return y_pred model = LinearModel() #实例化模型对象 ## 3. 构造损失函数和优化器 criterion = torch.nn.MSELoss(size_average = False) #损失函数MSE, 得到标量损失值，这个过程会构建计算图，所以也应该继承自nn.Moudle optimizer = torch.optim.SGD(model.parameter(), lr=0.01) #实例化优化器对象，它不会构建计算图, 传入需要优化的参数，learning rate固定 ## 4. 训练周期 for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred, y_data)#前馈：计算预测值和损失 print(epoch, loss) #loss是对象，打印时会自动调用__str__();不会产生计算图 optimizer.zero_grad() #所有参数的梯度归零 loss.backward() #反向传播 optimizer.step() #进行一次更新，根据所有参数的梯度和步长做更新 ## 输出w和b print(\u0026#39;w=\u0026#39;, model.linear.weight.item()) print(\u0026#39;b=\u0026#39;, model.linear.bias.item()) ## 测试模型 x_test = torch.Tensor([[4.0]]) y_test = model(x_test) print(\u0026#39;y_pred =\u0026#39;, y_test.data) 输入数据的格式 用矩阵一次性计算出一个batch的y_pred 或 loss\nnumpy的广播机制：做运算的两个数组维度不同，把小矩阵(重复)扩充到与大矩阵相同的大小\n运算：\n$$ \\begin{aligned} \\begin{bmatrix} y_{pred}^{(1)} \\\\ y_{pred}^{(2)} \\\\ y_{pred}^{(3)} \\end{bmatrix}_{3\\times 1} = w \\cdot \\begin{bmatrix} x^{(1)} \\\\ x^{(2)} \\\\ x^{(3)} \\end{bmatrix}_{3\\times 1} + b \\end{aligned} $$\n其中 w 和 b 会触发广播机制，变成 $[w\\ w\\ w]^T_{3\\times 1},\\ [b\\ b\\ b]^T_{3\\times 1}$\n计算损失：\n$$ \\begin{bmatrix} loss_1 \\\\ loss_2 \\\\ loss_3 \\end{bmatrix} = \\begin{pmatrix} \\begin{bmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ \\hat{y_3} \\end{bmatrix} - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} \\end{pmatrix}^2 $$\nloss 需要是一个标量，所以需求和：$loss = (\\frac{1}{N}) \\sum_{i}^N loss_i$。向量没法backward\nnn.Linear类 class torch.nn.Linear(in_features, out_features, bias=True) Docs-Linear\n对数据数据应用一个线性变换：$y_{1\\times n} = w^T_{1\\times 3} X_{3\\times n}+b$，n条样本，x有3个特征，y有一个特征\n传入参数:\nin_feature: 每个输入样本的维度（列数,特征） out_feature: 输出样本的列数 bias: 是否需要b，默认为true 线性单元包括两个成员Tensors(weight, bias)可以完成w*x+b的运算（同样继承自module，可以反向传播）\nmagic method __call__() callable 对象是一个可以被调用执行的对象 如果为一个类编写了__call__()方法，那么在该类的对象后面加括号，就会调用执行 __call__() 方法 1 2 3 4 5 class Foobar: def __init__(self): pass def __call__(self, *args, **kwargs): #实现此方法，让对象可调用 *args 代表没有固定数量和变量名的输入参数，是一个元组；**kwargs表示带变量名的输入参数，是一个字典：\n1 2 def fun(*args, **kwargs): pass func(1,2,3, x=3, y=5)，*args为(1,2,3)，**kwargs为{'x':3, 'y':5}，就可以遍历各个参数了\nnn.MSELoss类 class torch.nn.MSELoss(size_average=True, reduce=True)\n继承自nn.Module\n$\\hat{𝐲} - 𝐲$，减完之后求平方，求和\n参数：\nsize_average=True 是否最后的损失要求均值，$\\frac{1}{N}$没什么用，求导与w无关 reduce=True 是否降维 SGD类 class torch.optim.SGD(params.lr=\u0026lt;object, object\u0026gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)\nparams 是需要优化的参数。使用model.parameters()，这个成员函数会检查model中的所有成员，有哪些参数需要用梯度下降更新，加入需要训练的参数集合中。搜索model中的linear成员时，会调用linear.params()，linear有2个参数，\n","date":"2021-11-20T16:03:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/5_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92pytorch/","title":"watch: PyTorch - 刘二 05 | Linear Regression"},{"content":"每次迭代，loss的波动很大的解决办法\n每次迭代，loss的波动很大，有如下几方面：\n学习率选取较大； 在loss不再下降的时候降低学习率； 每个epoch训练之前，对数据进行重新打乱，如果你的 batch 的内容和顺序都是固定的，可能造成模型 overfit 这个顺序； 各个 batch 的 loss 有不同是正常的，但如果波动太大，可能说明你的各个 batch 不是 homogeneous 的（即内容差别太大），不能代表整体数据。可以试试加大 batch size。 总结就是： 当loss不下降时，降低学习率，一般降低到原来的0.1，在每个epoch开始之前，均打乱一次数据，适当增大batch_size的大小。\n————————————————\nPytorch使用shuffle打乱数据\n","date":"2021-11-20T11:43:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/%E6%AF%8F%E6%AC%A1%E8%BF%AD%E4%BB%A3loss%E6%B3%A2%E5%8A%A8%E5%BE%88%E5%A4%A7/","title":"loss波动大"},{"content":"RBF核函數參數自動挑選法Python實作\n$x,z$ 是原空间两个点的坐标，$\\gamma$ 是参数：\n$$ \\kappa (x,z,\\gamma) = \\operatorname{exp} (-\\gamma | x-z |^2) $$\nrbf核的特殊性质：\n在特征空间中，自己和自己的内积为1（范数），所以$\\phi(x)$ 是在一个超球面上\n两向量夹角等于内积：\n$$ cos(\\theta) = \\frac{\\kappa(x,z,\\gamma)}{\\sqrt{\\kappa(x,x,\\gamma)} \\sqrt{\\kappa(z,z,\\gamma)}} = \\frac{\\kappa(x,z,\\gamma)}{1} = \\kappa(x,z,\\gamma) $$\n当 $x \\neq z$ 时：\n$\\gamma \\rightarrow 0 \\ \\Rightarrow cos(\\theta)=\\kappa(x,z,\\gamma) \\rightarrow 1 \\ \\Rightarrow \\theta \\rightarrow 0^\\circ \\ \\Rightarrow \\phi(x)与\\phi(z)越相似$ $\\gamma \\rightarrow \\infin \\ \\Rightarrow cos(\\theta)=\\kappa(x,z,\\gamma) \\rightarrow 1 \\ \\Rightarrow \\theta \\rightarrow 90^\\circ \\ \\Rightarrow \\phi(x)与\\phi(z)越不相似$ 调参 varying the parameter 在原空间中的距离越大向量，当$\\gamma$变大时，角度拉大的速度越快。 当$\\gamma$ 很小的时候，各向量夹角很小，点很集中，不好分类； 当$\\gamma$很大时，各向量互相垂直，每个点都变一类；所以要取不大不小的$\\gamma$，同类向量夹角小，不同类向量的夹角已经拉到很开。\n目标： 调整gamma，同类的内积越接近1越好，不同类内积越接近0越好\ncode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # 生成样本数据及其标签 from sklearn import datasets Data,labels = datasets.make_circles(n_samples=400, noise=0.1, factor=0.1) #Data是400x2的矩阵，labels是1x400的列表 # 画三维散点图 import plotly.express as px fig = px.scatter(x=Data[:,0], y=Data[:,1], color=labels) fig.update_layout(yaxis=dict(scalenchor=\u0026#39;x\u0026#39;)) #y轴的刻度与x轴一样 from sklearn.metrics.pairwise import rbf_kernel import numpy as np def AutoRBF(gv, Data, labels): #计算gv对应的指标J # 计算Kernel matrix K = rbf_kernel(Data, gamma=gv) #400x400 # 组内：对角线上的block n=[] #统计各类样本的个数 w=0 #同组内积矩阵和累计 for i in range(0, max(labels)+1): #遍历每一类 idx = labels==i #把labels列表中等于某类的元素变成:True/False，形成列表idx #Within the Kernel matrix，取出同类矩阵 KW = K[:,idx] #先取列号等于idx列表中元素为True的index的列 KW = KW[idx,:] #再取行号=idx列表中True元素的index的行 n.append(sum(idx)) #统计各类元素个数，这里是[200,200] w = w + np.sum(KW) #第一类内积矩阵求和加上第二类内积矩阵求和 # 组间（不同类样本内积的block） b = np.sum(K) - w #全部元素之和减去同类内积矩阵之和w就是不同类内积矩阵之和b nw = sum(np.power(n,2)) #每类200个，共200x200 + 200x200=4万个值 nb = sum(n)**2 - nw w=w/nw #取平均 b=b/nb #取平均 J=(1-w)+b #w越接近1越好，b越接近0越好 return J # 第一种方法找最佳gamma：grid grid = np.linspace(0,50,1000) #0-50切成1000份 J=[] #存储各gv对应的J值 for gv in grid: J.append(AutoRBF(gv,Data,labels)) px.line(x=grid, y=J) #把J画成曲线 # 第二种方法：用 minimize 函数找最低点（比grid快很多） from scipy.optimize import minimize gv0=1/Data.shape[1] #初始值：1/维度数 sol = minimize(AutoRBF,gv0,args=(Data,labels)) #solution是使目标函数AutoRBF最小的gv（初值为gv0），args放函数的其余参数 bestgv = sol.x #最低点的x坐标，最好的gv # 用kpca降维后，能不能分得好 from sklearn.decomposition import KernelPCA myKPCA = KernelPCA(n_components=2, kernel=\u0026#39;rbf\u0026#39;, gamma=bestgv) myKPCA.fit(Data) #训练 reduced_Data = myKPCA.transform(Data) #投影到低维 fig = px.scatter(x=reduced_Data[:,0], y=reduced_Data[:,1],color=labels) fig.update_layout(yaxis=dict(scaleanchor=\u0026#39;x\u0026#39;)) ","date":"2021-11-19T17:14:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/rbf%E6%A0%B8%E5%8F%82%E6%95%B0%E8%87%AA%E5%8A%A8%E6%8C%91%E9%80%89/","title":"RBF-Kernel-Param-Select(Python)"},{"content":"最基本的数据类型:Tensor，存储所有的数值，标量，向量，矩阵，高阶tensor\ntensor有两个成员： data（权重数值本身w）， grad（也是tensor，损失值（标量）对权重的导数 ∂loss/∂w）\n构建计算图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import torch # 准备数据集 x_data = [1.0, 2.0, 3.0] y_data = [2.0, 4.0, 6.0] w = torch.Tensor([1.0]) # 创建权重初始值，tensor变量中只有一个值 w.requires_grad = True # 该变量需要计算梯度，默认的tensor不需要计算梯度 # 设计模型 def forward(x): return x* w # w是个tensor，乘法*被重载了: tensor 与tensor之间的数乘，x被自动转换成tensor，所以乘法结果也是tensor，并且也会需要计算梯度。 # 计算损失 def loss(x,y): y_pred = forward(x) # 计算预测值 return (y_pred - y) ** 2 # 每调用一次loss函数，计算图被动态地构建出来 # 训练过程 print(\u0026#34;predict (before training)\u0026#34;, 4, forward(4).item()) for epoch in range(100): #训练100轮 for x,y in zip(x_data, y_data): # 对应组合拼起来 # 前馈：计算每个样本的损失(随机梯度下降)，是一个(标量)tensor，含有1个值， # 如果是一个向量没法backward l = loss(x,y) # 反向传播：调用张量 l 的成员函数，把图上的所有梯度求出来，存到 w 中，然后计算图就被释放了， # 下一次loss计算会创建新的计算图（因为每次计算图可能不一样） l.backward() print(\u0026#39;\\t grad:\u0026#39;, x,y, w.grad.item()) #item把grad变成int/float,直接拿出它的数值(防止产生计算图) w.data = w.data - 0.01 * w.grad.data #更新数值，成员grad也是一个tensor，tensor的乘法会建立计算图，所以要取其data再做乘法，就不会建立计算图，只是修改w的数值（并不是说以后还要对这个运算求梯度） w.grad.data.zero_() #权重的梯度(导数)的数值清零，否则各次由.backward()计算出的梯度值会累加（有的时候需要梯度累加） print(\u0026#34;progress\u0026#34;,epoch, 1.item()) #打印每轮训练的loss print(\u0026#34;predict (after training)\u0026#34;, 4, forward(4).item()) 如果要在程序中对损失 l 求和取平均，注意要取出数值（使用int/float运算）： sum + = l.item()，否则因为 l 是tensor，一直加，计算图一直延长，导致内存泄漏。 （这就是为什么要避免 in-place operation? 1)\nRef AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD ","date":"2021-11-17T21:47:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/4_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","title":"watch: PyTorch - 刘二 04 | Backpropagation"},{"content":"用于做分类\n线性回归： $y\\in \\mathbb R$是连续的\n$$ Affine Model(Linear unit): \\hat{y} = x*w + b \\ Loss function: loss = (\\hat{y} - y)^2 = (x\\cdot w - y)^2 $$\n分类：y的取值是一个离散值集合：MINIST 0-9\n计算属于每一类的概率，取最大值做判别\n","date":"2021-11-15T10:39:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/6_logistic%E5%9B%9E%E5%BD%92/","title":"watch: PyTorch - 刘二 06 | Logistic Regression"},{"content":"Source video: 《PyTorch深度学习实践》完结合集-02\n刘洪普 Blog\n监督学习过程：\n准备数据集 模型设计 训练 inferring 数据集：\nx(hours) y(grades) 1 2 2 4 3 6 4 ? 1-3 training, 4 testing 预测\n监督学习：输出值已知\n数据分成 Training set 和 Testing set 两部分(不能偷看测试集的label)\nTraining set 接近 数据的真实联合分布 D(x,y)，根据大数定律需要大量数据\nTraining Set 中分出一部分做开发集，验证模型性能\n模型设计：\n模型：y = f(x)\n先用线性模型是否有效，再换其他的模型。\n线性模型：$\\hat{y}=f(x) = x*w +b$\n预测值 $\\hat{y} = x * w$ (先不考虑b)\n找到最优的权重，先随机数，计算与数据集的误差（平方和最小），\n损失函数,针对一个样本：$loss = (\\hat(y)-y)^2 = (x*w-y)^2$，必须是一个标量，才能让他变得更小，不断优化。\nx(Hours) y(grades) y_predict(w=3) Loss(w=3) 1 2 3 1 2 4 6 4 3 6 9 9 mean = 14/3 选取w，让平均损失降到最低\nCost function, 对于整个training set, 平均平方误差(Mean Square Error): $cost = \\frac{1}{N} \\sum_{n=1}{N}(\\hat{y}_n - y_n)^2$\nx(Hours) Loss(w=0) Loss(w=1) Loss(w=2) Loss(w=3) Loss(w=4) 1 4 3 0 1 4 2 16 6 0 4 16 3 36 9 0 9 36 MSE 18.7 4.7 0 4.7 18.7 不保证能找到0，在真实最小值附近穷举。经过测试最优w存在于0-4之间，对之间所有可能取值（对实数域采样）都计算一下损失，绘制曲线，找最低点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np import matplotlib.pyplot as plt x_data = [1.0, 2.0, 3.0] #数据用两个列表表示 y_data = [2.0, 4.0, 6.0] # 定义模型 def forward(x): #前馈 return x*w def loss(x,y): y_pred = forward(x) return (y_pred - y) * (y_pred - y) w_list = [] mse_list = [] for w in np.arange(0.0, 4.1, 0.1): #间隔0.1 print(\u0026#39;w=\u0026#39;, w) l_sum = 0 for x_val, y_val in zip(x_data, y_data): #拼成training pair y_pred_val = forward(x_val) loss_val = loss(x_val, y_val) #每个样本的loss l_sum += loss_val #cost function是loss function 的平均 print(\u0026#39;\\t\u0026#39;,x_val, y_val, y_pred_val, loss_val) print(\u0026#39;MSE=\u0026#39;, l_sum/3) w_list.append(w) mse_list.append(l_sum/3) plt.plot(w_list, mse_list) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.xlabel(\u0026#39;w\u0026#39;) plt.show() 用训练的轮数 epoch 做横坐标，检查超参数，判别是否收敛.\n打印日志输出，实时画图，Visdom\n要存盘，避免崩溃，计算白费\n模型：$\\hat{y} = x*w +b$ 有两个参数，损失函数是曲面,找最低点\n3d图绘制使用 np.meshgrid()\n","date":"2021-11-14T23:32:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/2_%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","title":"watch: PyTorch - 刘二 02 | Linear Model"},{"content":"训练模型的最常用算法\nwhat is best model for the data?\nlinear model, 随机猜测一个斜率（权重）,找让误差最小的最优权重。\n通常损失函数有多个参数，穷举某一区间中所有点，搜索最优参数，计算量太大（1个参数搜索100个点，2个参数就是100^2）\n分治：先分大块(4x4)搜索，确定一个小区域，再在小区域中分（4x4）搜索。但是如果cost function 很粗糙，。\n寻找使cost funciton 最小的权重，是一个优化问题：\n$$ Mean Square Error: cost(w) = \\frac{1}{N} \\sum_{n=1}^N (\\hat{y_n}- y_n)^2 \\ w^* = \\underset{w}{argmin} cost(w) $$\n梯度下降：\n初始权重，确定滚动方向，到达最低点\n$w= w- \\alpha \\frac{\\partial cost}{\\partial w}$ (a 是学习率）\n梯度下降只能找到局部最优。实际上，深度神经网络的损失函数并没有很多局部最优，但是存在鞍点，它的梯度为零，到达了按点没办法继续迭代。\n随机\n","date":"2021-11-14T22:36:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/3_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","title":"watch: PyTorch - 刘二 03 | Gradient Descent"},{"content":"Automatic RBF Kernel Parameter Selection Method-李政轩\nKernel Method 使用kernel mapping $\\phi$ 把数据送到 feature space。但通常 $\\phi$ 未知，所以使用 kernel function $\\kappa$ 做变换。\n最常用 RBF kernel:\n$$ \\kappa (x,z,\\sigma) = \\operatorname{exp} \\left( -\\frac{| x-z |^2}{2\\sigma^2} \\right) $$\n表示内积 $\\phi(x)^T \\phi(z)$\n性质较好：\n在特征空间中，每个样本的范数是 +1； 把样本投影到球面上（而且是在正的卦限） $$ $$\n","date":"2021-11-11T16:05:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/automatic_rbf_kernel_para_select/","title":"RBF-Kernel-Param-Select"},{"content":"P1-背景介绍\nKernel Method 把低维空间的非线性问题，转化到高维空间的线性问题求解 Kernel trick 使用核函数减少计算量，避免计算高维特征空间的内积（计算角度） Kernel Function 把输入空间𝓧 映射（任意形式$ϕ(x)$）到高维特征空间𝓩\n$$ K(x,x\u0026rsquo;)=\\phi(x)^T \\phi(x\u0026rsquo;) $$\n为什么是：\n非线性带来高维转换:\n线性分类最完美的情况:二分类问题严格线性可分，即存在一个或多个线性超平面可以把两类正确分开，不同的初值最终收敛的结果不同。对于非线性输入无法收敛。\n对于不同的输入数据，采用不同的算法：\n线性可分 线性不可分 存在一点点非线性 严格非线性 PLA Pocket Algorithm 多层感知机(隐藏层数≥1,逼近任一连续函数);\n非线性转换(Cover Therom: 高维比低维更易线性可分) Hard-margin SVM Soft-margin SVM Kernel SVM(先做非线性转换，再做SVM) 对偶表示带来内积:\nSVM的思想是最大间隔分类，是一个（凸）优化问题。然后根据拉格朗日对偶性，转化为求解原问题的对偶问题:\n$$ \\begin{array}{c} \\begin{array}{cc} 原问题\\ \\begin{cases} \\underset{\\mathbf w,b}{\\operatorname{min}}\\ \\frac{1}{2} \\mathbf w^T \\mathbf w \\ s.t. \\quad y_i(\\mathbf w^T x_i + b) \\geq 1 \\end{cases} \\end{array}\n\\Longrightarrow \\begin{array}{cc} 对偶问题\\ \\begin{cases} \\underset{\\lambda}{\\operatorname{min}}\\ \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j \\mathbf x_i^T \\mathbf x_j - \\sum_{i=1}^N \\lambda_i \\ s.t. \\quad \\lambda_i \\geq 0, \\quad \\forall i=1,\u0026hellip;,N \\ \\ \\qquad \\sum_{i=1}^N \\lambda_i y_i =0 \\end{cases} \\end{array}\n\\end{array}\n$$\nDual Problem 包含内积: $\\mathbf x_i^T \\mathbf x_j$，即需要求解任意两个数据之间的内积\n而对于非线性可分问题，做了非线性转换之后，内积变为：$\\phi(x_i)^T \\phi(x_j)$，但是对于高维空间的 $\\phi(x)$，由于维度太高很难求。 所以希望找到一个函数直接求内积: $K(\\mathbf{x,x\u0026rsquo;})$，而避免求单个特征的 $\\phi(x)$\n数学表示：\n$$ \\forall \\mathbf{x,x\u0026rsquo;} \\in \\mathcal X,\\quad \\exist \\phi: \\mathcal X \\rightarrow \\mathcal Z s.t. \\quad K(\\mathbf{x,x\u0026rsquo;}) = \\phi(\\mathbf x)^T \\phi(\\mathbf x\u0026rsquo;) = \u0026lt;\\phi(\\mathbf x) \\phi(\\mathbf x\u0026rsquo;)\u0026gt; 则称：K(\\mathbf{x,x\u0026rsquo;}) 是一个核函数 $$\n将(输入空间的)样本代入核函数即可算出(高维空间的)内积，减少了计算量\n","date":"2021-11-10T13:36:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/07_%E6%A0%B8%E6%96%B9%E6%B3%95/","title":"watch: ML - 白板 07 | Kernel Method"},{"content":"向量投影到哪个方向，就用该方向的方向向量的转置乘以向量\n把$[-3,4]^T$ 投影到 $[0,1]^T$ 方向上，就是：$[0, 1] \\begin{bmatrix} -3 \\ 4 \\end{bmatrix} = 4$\n把 $[-4, 1]^T$ 投影到 $[1,2]^T$ 方向上，先对投影向量的模长归一: $[\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}]^T$， 然后 $[\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}] \\begin{bmatrix} -4 \\ 1 \\end{bmatrix} =\\frac{-2}{\\sqrt{5}}$\n三维空间，$\\mathbf x$ 在$(\\mathbf x_1, \\mathbf x_2)$ 的投影变换到以 $(\\mathbf e_1, \\mathbf e_2)$ 为基（相互垂直）的坐标系下。向量没变，坐标变了。$\\mathbf x$ 分别在$\\mathbf{e_1,e_2}$ 方向上投影，得到新坐标：\n$$ \\begin{aligned} \\mathbf e_1^T \\mathbf x \u0026amp;= \\left[\\frac{1}{\\sqrt{2}}\\ \\frac{1}{\\sqrt{2}}\\ 0 \\right] \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\\n\\mathbf e_2^T \\mathbf x \u0026amp;= \\left[\\frac{1}{\\sqrt{2}}\\ -\\frac{1}{\\sqrt{2}}\\ 0\\right] \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\end{aligned}\n\\Longrightarrow\n\\underbrace{ \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; 0 \\\\ \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{2}} \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} 1 \\ 0 \\2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} }_{三维到二维，降维}\n\\Rightarrow\n\\begin{bmatrix} \\mathbf e_1^T \\ \\mathbf e_2^T \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} $$\n投影矩阵\n$$ A \\coloneqq [\\mathbf e_1 \\ \\mathbf e_2] = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{2}} \\ 0 \u0026amp; 0 \\end{bmatrix} $$\n所以坐标变换，相当于左乘投影矩阵的转置 $A^T$\n$$ \\begin{bmatrix} \\mathbf e_1^T \\ \\mathbf e_2^T \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} = A^T \\begin{bmatrix} 1 \\ 0 \\ 2 \\end{bmatrix} $$\n主成分之间是相互独立的，互相垂直\n如上图，选 $\\mathbf e_1$ 为拉得最开的方向，是为了方便找 $\\mathbf e_2$ 方向上的最大最小值 ","date":"2021-11-04T09:55:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E6%9D%8E%E6%94%BF%E8%BD%A9/pca/","title":"PCA"},{"content":"Maximum Variance Unfolding\n它找到一个好的核函数，使 \u0026ldquo;所有转换后的数据点之间的距离之和 \u0026ldquo;在 \u0026ldquo;邻域图中的距离 \u0026ldquo;不变的约束下达到最大。直观地说，当一个流形被适当地展开时，各点的方差是最大的。因此，目标函数旨在展开变换空间中的数据点（由核函数诱导），同时，约束条件保证在展开时满足局部属性。\n给定 n 个点 $X={x_1, \\cdots, x_n}$，MVU 首先构建一个 neighborhood graph G，其中每个点 $x_i$ 都和离它最近的 k 个点有连接。\n找一个图形 $Y={y1, \\cdots,y_n}$，其满足：$| y_i - y_j |^2 = |x_i - x_j |^2,\\ for \\forall (i,j) \\in G$，然后对Y中各点间的距离之和取最大：\n$$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} \\sum_{ij} | y_i -y_j |^2 \\\n\u0026amp; \\text{subject to}\\ |y_i - y_j |^2 = | x_i - x_j |^2 ,\\ \\text{for } \\forall (i,j) \\in G \\end{aligned} $$\n可以通过引入一个核函数($k: X \\times X \\rightarrow R$)找到一个合理而可行的Y的近似。\n因此需要找到一个好的核函数，在变换后的空间中，所有变换后的数据之间的距离之和是最大，满足3个限制条件：\n核函数有效; 变换后数据点的均值为零;(便于写目标函数) $|\\phi(x_i) - \\phi(x_j) |^2 = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G$ $$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} \\sum_{ij} |\\phi(x_i) - \\phi(x_j) |^2 \\\n\u0026amp; \\text{subject to} \\begin{cases} K \\text{ is positive semidefinite} \u0026amp; \\text{有效}\\ | \\sum_i \\phi(x_i) |^2 = 0 \u0026amp; \\text{零均值}\\ |\\phi(x_i) - \\phi(x_j) |^2 = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G \\end{cases} \\end{aligned} $$\n首先改写约束条件2：\n$$ | \\sum_i \\phi(x_i) |^2 = \\left( \\sum_i \\phi(x_i) \\right)^T \\left( \\sum_j \\phi(x_j) \\right) = \\sum_{ij} \\phi(x_i)^T \\phi(x_j) = \\sum_{ij} k_{ij} = 0 $$\n约束条件3的左边部分：\n$$ |\\phi(x_i) - \\phi(x_j) |^2 = \\left( \\phi(x_i) - \\phi(x_j) \\right)^T \\left( \\phi(x_i) - \\phi(x_j) \\right) = k_{ii} + k_{jj} - 2k_{ij} $$\n改写目标函数：\n$$ \\sum_{ij} | \\phi(x_i) - \\phi(x_j) |^2 = \\sum_{ij} (k_{ii} + k_{jj} - 2 k_{ij} ) = \\sum_{ij} k_{ii} + \\sum_{ij} k_{jj} -2\\sum_{ij} k_{ij} = 2n \\sum_i k_{ii} - 2 \\cdot 0 = 2n \\cdot tr(K) $$\n最终，优化问题是一个半正定规划问题(SDP)：\n$$ \\begin{aligned} \u0026amp; \\operatorname{Maximize} tr(K) \\\n\u0026amp; \\text{subject to} \\begin{cases} K \\text{ is positive semidefinite} \u0026amp; \\text{有效}\\ \\sum_{ij} k_{ij} = 0 \u0026amp; \\text{零均值} \\ k_{ii} + k_{jj} - 2k_{ij} = | x_i - x_j |^2 ,\\ for \\forall (i,j) \\in G \\end{cases} \\end{aligned} $$\nSDP 的解K 是用于输入Kernel PCA的核矩阵。\nstat946f10-uwaterloo\nMVU is a variation of Kernel PCA\n","date":"2021-11-03T12:37:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/projecttips/mvu_notes/","title":"MVU notes"},{"content":"3 component of using Learning Pattern exists Pin down mathematically have data (most important) Learning set up Unkown target function Dataset: Learing algorithm picks $g\\appriox f from hypothesis set H Perceptron Learning Algorithm Feasibility of learning. In order to establish that learning setup and modifier the data, and in order to answer if that learning is feasible. We said that we gonna restart with specific __ and we went an example of bin. I just quickly review that.\nWe suppose to have a bin. In bin there are red marbles and green marbles as we see. And probility that if you pick a red marble is called $\\mu$, and a probability that you pick a\nBIN Model Bin with red and blue marbles; Pick a sample of N marble independently\n$\\mu$: probability to pick a red marbles from the bin (blue: 1-$\\mu$) $\\nu$: fraction of red marbles in the sample In a large sample (large N), ν is probably close to μ (within tolerance ε)\nHoeffding\u0026rsquo;s Inequality:\n$$ \\begin{aligned} P[Bad] \u0026amp;= P[|ν-μ|\u0026gt;ε]≤ 2e^{-2ε^2N}, \u0026amp; \\text{for any ε\u0026gt;0} \\ P[Good] \u0026amp;= P[|ν-μ|≤ε]\u0026gt; 1-2e^{-2ε^2N},\u0026amp; \\text{for any ε\u0026gt;0} \\end{aligned} $$\nN is large, ε is large, the P[Bad] becomes small, ν and μ are very close to each other.\nN=1000, ε=0.05, the probability of ν-ε ≤ μ ≤ ν+ε is 0.986 N=1000, ε=0.1, the probability of ν-ε ≤ μ ≤ ν+ε is 0.999 μ∈[ν-ε, μ+ε], error bar is ±ε learn from ν and reach outside the data (μ). There still is a probability of getting wrong sample, but not often.\nμ≈ν is probably approximately correct (PAC-learning)\n\u0026ldquo;probably\u0026rdquo; : probabilty 2exp(-2 ε^2 N) \u0026ldquo;approximately\u0026rdquo; : tolerance ε\n","date":"2021-10-22T16:03:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/lec2_learning-feasible/","title":"watch: AML 02 | Learning Feasibility"},{"content":"SVM SVM本质上是一个判别模型，解决二分类问题，与概率无关 超平面: 𝐰ᵀ𝐱+b=0 分类模型：f(𝐰) = sign(𝐰ᵀ𝐱+b) SVM有3宝，间隔对偶核技巧 三大分类算法： Hard-margin SVM （硬间隔） Soft-margin SVM Kernel SVM 1 硬间隔SVM-模型定义（最大间隔分类器） Video-P1\nHard-margin SVM 最大间隔分类器（max margin()）\n几何意义：对于平面上点的分类问题， 如果分界线两侧的点到线的距离很小，会对噪声很敏感，如果有一点噪声可能就被分到另一侧去了，泛化误差大。所以最好的分界线满足对所有点的距离都足够大。\n从无限条可以正确分类的直线（超平面）中，选择最好的\n数学表述：\n使间隔margin函数最大，实现对N个p维的样本点 ${(x_i, y_i)}_{i=1}^{N}, \\quad x_i \\in \\R^p, y\\in{-1,1}$正确分类\n$$ \\begin{aligned} \u0026amp; \\rm max , margin(\\mathbf w, b) \\\n\u0026amp; s.t. \\begin{cases} \\mathbf w^T x_i + b \u0026gt; 0, \u0026amp; y_i = 1 \\ \\mathbf w^T x_i + b \u0026lt; 0, \u0026amp; y_i = -1 \\end{cases}\n\\Rightarrow y_i(\\mathbf w^Tx_i +b )\u0026gt;0, \u0026amp; \\text{for $\\forall$ i=1,\u0026hellip;,N}（同号） \\end{aligned} $$\n每个点到直线的距离: $\\rm distance(w,b,x_i) = \\frac{1}{| w |} |w^T x_i +b|$\n$$ \\begin{aligned} \\rm margin(\\mathbf w,b) \u0026amp; = \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, distance(\\mathbf w,b,x_i) \\ \u0026amp; = \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, \\frac{1}{| \\mathbf w |} |\\mathbf w^T x_i +b|\n\\end{aligned} $$\n因为限制条件 $y_i(\\mathbf w^T x_i + b)\u0026gt;0$，而且$y_i={-1,1}$，所以可以替换上式中的绝对值。\n所以最大间隔分类器可写为：\n$$ \\rm \\underset{\\mathbf w,b}{max} \\frac{1}{| \\mathbf w|}; \\underset{x_i, i=1,\u0026hellip;,N}{min} ; |\\mathbf w^T x_i +b| $$\n(因为min只与$x_i$有关，而$\\frac{1}{| \\mathbf w|}$与$x$无关，所以移到了前面)\n对于限制条件 $y_i(\\mathbf w^T x_i + b)\u0026gt;0$，说明存在最小值：\n$$ \\exist , r\u0026gt;0, \\quad s.t.; \\rm \\underset{\\mathbf w,b,x_i,i=1,\u0026hellip;,N}{min}, y_i (\\mathbf w^T x_i +b) = r $$\n所以可以继续简化：\n$$ \\rm \\underset{\\mathbf w,b}{max} \\frac{1}{| \\mathbf w|} r $$\n因为超平面可以同比例缩放（$\\mathbf{w^T x} + b = 2\\mathbf{w^T x} + 2b$），所以可以设置最小值r为1，相当于把超平面缩放到1，系数乘在前面的$\\frac{1}{| \\mathbf w|}$里\n所以问题最终转化为一个凸优化问题：\n$$ \\begin{cases} \\rm \\underset{w,b}{max} \\frac{1}{| \\mathbf w |} \\ s.t. ; \\operatorname{min} y_i (\\mathbf w^T x_i + b) = 1 \\end{cases}\n\\Rightarrow\n\\begin{cases} \\rm \\underset{w,b}{min} \\frac{1}{2} \\mathbf{w^T w} \u0026amp; \\text{(目标函数是二次)}\\ s.t. ; y_i (\\mathbf w^T x_i + b) ≥ 1, ; i=1,\u0026hellip;,N \u0026amp; \\text{(N个线性约束)} \\end{cases} $$\n求解QP问题 (凸二次规划Quadratic programming问题)\n在维度不高，样本个数不多的情况下，比较好求解。 但是对于维度很高，样本很多，或者对数据$x$做$\\phi(x)$的特征转换到了新的特征空间$Z$中，而Z的维度比原数据的维度高很多，就没办法直接求解，计算量太大。 借助拉格朗日乘子，引出它的对偶问题，求解相对容易的对偶问题。\n把目标函数写成拉格朗日函数 $L$，把带约束问题化成无约束问题：拉格朗日函数只有对 λ 的约束，没有对 $𝐰$ 和 $b$ 的约束\n$$ L(\\mathbf w,b,λ)= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} ; \\underbrace{λ_i}{≥0} ; \\underbrace{(1-y_i(𝐰^T x_i + b))}{≤0} $$\n问题转化为：\n$$ \\begin{cases} \\rm \\underset{\\mathbf w,b}{min}\\ \\underset{λ}{max}\\ L(\\mathbf w,b,λ)\\ s.t. ; λ_i ≥0 \\end{cases} $$\n原问题的对偶问题：先对𝐰,b求L的最小值，再对 λ 求 L 的最大值\n$$ \\begin{cases} \\rm \\underset{λ}{max}\\ \\underset{\\mathbf w,b}{min}\\ L(\\mathbf w,b,λ)\\ s.t. ; λ_i ≥0 \\end{cases} $$\n从直观上看，从最大值里面选的最小值一定是大于从最小值里面的最大值（省略证明）,也就是弱对偶关系：\n$$ \\rm min, max, L ≥ max , min, L $$\n我们还想要强对偶关系，即 $\\rm min, max, L = max , min, L$。对于凸二次优化问题，天生满足强对偶关系（证明略），原问题和它的对偶问题是同解的。所以直接求解对偶问题即可。\n先固定 $λ$，对 $𝐰,b$ 求L的最小值，在此对偶问题中没有对𝐰,b的约束条件，所以是一个无约束的优化问题，直接求导：\n先对b求导：\n$$ \\begin{aligned} \\frac{∂L}{∂b} \u0026amp;= \\frac{∂}{∂b} \\left[ \\cancel{\\sum_{i=1}^{N} λ_i} - \\sum_{i=1}^{N} λ_i y_i(\\cancel{\\mathbf w^T x_i} + b)\\right] \\ \u0026amp;= \\frac{∂}{∂b} \\left[ -\\sum_{i=1}^{N} λ_i y_i b \\right] \\ \u0026amp;= -\\sum_{i=1}^{N} λ_i y_i =0 \\end{aligned} $$\n把这个结果带入$L$\n$$ \\begin{aligned} L(\\mathbf w,b,λ) \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} ; λ_i ; (1-y_i(\\mathbf w^T x_i + b))\\ \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} \\ λ_i - \\sum_{i=1}^{N} ; λ_i y_i(\\mathbf w^T x_i + b)\\ \u0026amp;= \\frac{1}{2}\\mathbf{w^Tw} + \\sum_{i=1}^{N} \\ λ_i - \\sum_{i=1}^{N} ; λ_i y_i\\mathbf w^T x_i\n\\cancel{\\sum_{i=1}^{N} ; λ_i y_i b} \\ \\end{aligned} $$\n再对$𝐰$求导，定义导数为等于0\n$$ \\begin{aligned} \u0026amp; \\frac{∂L}{∂𝐰} = \\frac{1}{2} 2𝐰 - \\sum_{i=1}^N λ_i y_i x_i ≔ 0 \\ \u0026amp; \\Rightarrow 𝐰^* = \\sum_{i=1}^{N} λ_i y_i x_i \\end{aligned} $$\n把这个最优$𝐰^*$的表达式再带入$L$，就是L的最小值：\n$$ \\begin{aligned} L(\\mathbf w,b,λ) \u0026amp; = \\frac{1}{2} \\underbrace{ \\left(\\sum_{i=1}^{N} λ_i y_i 𝐱_i \\right)^T}{\\sum{i=1}^N λ_i y_i 𝐱_i^T } \\sum_{j=1}^N λ_j y_j 𝐱_j\n\\sum_{i=1}^N \\ λ_i y_i \\left(\\sum_{j=1}^{N} λ_j y_j 𝐱_j \\right)^T 𝐱_i \\sum_{i=1}^N λ_i \\ \u0026amp; = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N λ_i λ_j y_i y_j \\underline{𝐱_i^T 𝐱_j} \\sum_{i=1}^N λ_i y_i \\sum_{j=1}^N λ_j y_j \\underline{𝐱_j^T 𝐱_i} \\sum_{i=1}^N λ_i \\ \u0026amp; = -\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N λ_i λ_j y_i y_j 𝐱_i^T 𝐱_j \\sum_{i=1}^N λ_i \\ \\end{aligned} $$ 所以对偶问题又化为：\n$$ \\begin{cases} \\underset{λ}{\\operatorname{min}} ; \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N λ_i λ_j y_i y_j 𝐱_i^T 𝐱_j\n\\sum_{i=1}^N λ_i \\ s.t. \\ λ_i ≥0; \\ \\sum_{i=1}^N λ_i y_i =0 \\end{cases} $$ 固定 $\\mathbf w,b$, 对于向量$λ$求$L$的最小值\n此时拉格朗日函数只与 $\\lambda$ 有关\n求出 $\\lambda$，根据 $\\mathbf w = \\sum_{i=1}^{m} \\lambda_i y^{(i)} x^{(i)}$ 就可求出 $\\mathbf w$，再根据 $b^* = -\\frac{\\rm max\\ x_{i:y^{(i)}=-1} \\mathbf w^{*T} x^{(i)} + min_{i:y^{(i)}} \\mathbf w^{*T} x^{(i)}}{2}$，就可求出 $b$，最终得到分离超平面和分类决策函数。\nKKT条件 原问题与对偶问题具有强对偶关系的充要条件就是满足KKT条件\n$$ \\begin{cases} \\frac{∂L}{∂𝐰} =0,\\frac{∂L}{∂b} =0,\\frac{∂L}{∂λ} =0 \u0026amp;\\text{拉格朗日函数对w，对b，对λ 求偏导都等于0}\\ λ_i (1-y_i(𝐰^T x_i + b)) = 0 \u0026amp; \\text{松弛互补条件slackness complementary} \\ λ_i ≥ 0 \\ 1-y_i(𝐰^T x_i + b)≤0 \\end{cases} $$\n由这些条件就可以求出最优解的$w^$和$b^$\n由 ∂L/∂𝐰 可求出𝐰∗=∑ᵢ₌₁ᴺ λᵢ yᵢ xᵢ\n4 软间隔SVM-模型定义 Video-P4\nSoft-margin SVM 允许一点点错误 loss\n$$ min \\frac{1}{2} w^T w + loss $$\nloss函数:\n违反约束条件的点的个数：$loss = \\sum_{i=1}^N I \\left{ \\underbrace{y_i (w^T x_i + b)\u0026lt;1}_{关于w不连续} \\right}$ （I是指示函数）\n不连续性： 令$z=y(\\mathbf w^T \\mathbf x+b)$，则 $loss_{0/1} = \\begin{cases} 0, \u0026amp;\\text{z\u0026lt;1} \\\\ 0, \u0026amp;otherwise \\end{cases}$\n在z=1处有跳跃，不连续导致求导有问题。\n用距离\n超平面 能把 n 维欧式空间分成两部分的 n-1 维子空间。\nn 维空间 $\\R^n$ 的超平面是由方程：$𝐰^T 𝐱 + b = 0$ 定义的子集。（𝐰 和 𝐱 都是n维向量）\n法向量与超平面内任一向量垂直。假设在三维空间中，水平面内的一个向量 $𝐱-𝐱\u0026rsquo;$ 与法向量 $\\mathbf w$ 垂直，如下图(源自如何理解超平面？)：\n满足：\n$$ \\begin{aligned} (𝐱-𝐱\u0026rsquo;)\\mathbf w \u0026amp;= 0 \\ (x_1 - x_1\u0026rsquo;, x_2 - x_2\u0026rsquo;, x_3 - x_3\u0026rsquo;) \\cdot (w_1, w_2, w_3) \u0026amp;= 0 \\ x_1 w_1 + x_2 w_2 + x_3 w_3 \u0026amp;= w_1 x_1\u0026rsquo; + w_2 x_2\u0026rsquo; + w_3 x_3\u0026rsquo; \\ \\mathbf w^T \\mathbf x \u0026amp;= \\mathbf w^T \\mathbf x' \\end{aligned} $$\n由于 $\\mathbf w^T \\mathbf x\u0026rsquo;$ 是常数项，$-\\mathbf w^T \\mathbf x\u0026rsquo; ≔ b$， 所以超平面的公式可写为：\n$$ \\mathbf w^T \\mathbf x + b = 0 $$\n点到超平面距离 点到超平面的函数距离，除以法向量的范数\n求平面外一点 $\\mathbf x$ 到平面的距离 d。\n根据三角函数：$cos \\theta = \\frac{d}{| \\mathbf x-\\mathbf x\u0026rsquo; |}$ (空间中一点向超平面作垂线，$\\theta$只能是锐角，不必担心正负)\n$\\mathbf x-\\mathbf x\u0026rsquo;$ 与 $\\mathbf w$ 的内积为： $|(\\mathbf x-\\mathbf x\u0026rsquo;) \\cdot \\mathbf w | = |\\mathbf x-\\mathbf x\u0026rsquo; | \\cdot | \\mathbf w | \\cdot cos \\theta$ （因为法向量可能反向，所以给等式左边加上绝对值）\n联立可得：\n$$ d = \\dfrac{|(𝐱 - 𝐱\u0026rsquo;) 𝐰 |}{| 𝐰 |} = \\dfrac{|𝐰𝐱 - 𝐰𝐱\u0026rsquo;|}{| 𝐰 |} $$\n因为 $𝐱\u0026rsquo;$在超平面内，$𝐰𝐱\u0026rsquo; = -b$，于是最后得到的任意点到超平面的距离公式：\n$$ d = \\frac{|𝐰𝐱+b|}{| 𝐰 |} $$\n几何距离与函数距离 几何距离：点到直线（超平面）距离 函数距离：$Δy$，直线（超平面）上的点y=0，所以不在直线上的点到直线的函数距离就是点的y值。 ","date":"2021-10-13T18:59:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/06_svm/","title":"watch: ML - 白板 06 | SVM"},{"content":"P4\n3D Transformations 3D Scale $$ \\mathbf S(s_x, s_y, s_z) = \\begin{pmatrix} s_x \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_y \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_z \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} $$\n3D Translation $$ \\mathbf T(t_x,t_y,t_z) = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; t_x \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; t_y \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; t_z \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} $$\nRotaion around axises 绕x轴旋转：\nx方向坐标不变，所以旋转矩阵各列向量的x分量都不贡献，所以第一行是1 0 0；\n为了保证逆时针旋转是正向旋转，其余两轴的顺序如图：\ny-z平面旋转，用待定系数法或展开三角函数，可得旋转关系为： $\\begin{cases} x\u0026rsquo;=x \\\\ y\u0026rsquo;=ycosθ-zsinθ \\\\ z\u0026rsquo;=ysinθ+zcosθ \\end{cases}$，\n则旋转矩阵为 $\\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ z\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix} 1\u0026amp;0\u0026amp;0\\\\ 0 \u0026amp; cosθ \u0026amp; -sinθ \\\\0 \u0026amp; sinθ \u0026amp; cosθ \\end{pmatrix} \\begin{pmatrix} x\\\\y\\\\z\\end{pmatrix}$；\n写成齐次坐标，不考虑平移，所以仿射变换矩阵为：\n$$ R_x(α)= \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; cosθ \u0026amp; -sinθ \u0026amp; 0 \\\\ 0 \u0026amp; sinθ \u0026amp; cosθ \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{pmatrix} $$\n绕y轴旋转：\ny方向不变，所以矩阵第2行为：0 1 0；\nx-z平面旋转，用待定系数法，可得旋转关系： $\\begin{pmatrix}z\u0026rsquo;\\\\x\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix}cosθ\u0026amp; -sinθ\\\\sinθ \u0026amp; cosθ\\end{pmatrix} \\begin{pmatrix} z\\\\x \\end{pmatrix}$ (或者展开三角函数，可得旋转关系为： $\\begin{cases} x\u0026rsquo;=zsinθ+xcosθ \\\\ z\u0026rsquo;=zcosθ-xsinθ\\end{cases}$) 写成旋转矩阵为：\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ z\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix} cosθ \u0026amp; 0 \u0026amp; sinθ \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ -sinθ \u0026amp; 0 \u0026amp; cosθ \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} $$\n三个轴循环对称：$x\\boxed{yzx}y\\boxed{zxy}z\\boxed{xyz}$，即给定任意两个可得后面一个，所以Y是由$Z×X$的到的 $\\begin{pmatrix}Z\\\\ X\\\\ Y\\end{pmatrix}$，这与通常书写 矩阵顺序$\\begin{pmatrix}X\\\\ Y\\\\ Z\\end{pmatrix}$不同，导致系数易位，$-sinθ$不在左上角，看起来与绕其他两轴的旋转矩阵不一致。\n绕z轴旋转：\nz方向不变，所以矩阵第3行为：0 0 1；\nx-y平面旋转，有旋转关系 $\\begin{cases}x\u0026rsquo;=rcos(α+θ)\\\\y\u0026rsquo;=rcos(α+θ)\\end{cases}$ 以及 $\\begin{cases}x=rcosα\\\\y=rsinα\\end{cases}$ 可得：$\\begin{cases}x\u0026rsquo;=xcosθ-ysinθ\\\\y\u0026rsquo;=ycosθ+xsinθ\\end{cases}$ 写成旋转矩阵为：\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\z\u0026rsquo; \\end{pmatrix} = \\begin{pmatrix} cosθ \u0026amp; -sinθ \u0026amp; 0 \\\\ sinθ \u0026amp; cosθ \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\z \\end{pmatrix} $$\n任意三维旋转 渲染 给三维物体拍照片 摆好模型 Model transformation 摆好照相机 View transformation 投影到相片 Projection transformation View Transformation 通过旋转和平移，把世界坐标系中的相机调整到标准姿态\n相机的姿态包括：位置、朝向和旋转\n标准姿态：\n光心在原点； 朝着-z方向看； 向上是y轴 从标准姿态变换到当前姿态，再取逆，就是视图变换\n正交投影 平行光 把空间中的长方体映射到一个边长为2的立方体（左右下上远近的边界都是1） 中心平移到原点: $\\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; -\\frac{r+l}{2} \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; -\\frac{t+b}{2} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -\\frac{n+f}{2} \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}$ 缩放到2: $\\begin{pmatrix}\\frac{2}{r-l} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2}{t-b} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{2}{n-f} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\end{pmatrix}$ 透视投影 视锥 近大远小 把棱台挤压成长方体，长方体再做正交投影 远平面缩放到与近平面相同大小\n根据相似，确定不同深度的缩放系数\n$$ \\begin{cases} y\u0026rsquo;=\\frac{n}{z}y \\\\ x\u0026rsquo;=\\frac{n}{z}x \\\\ z\u0026rsquo;=unknow \\end{cases} $$\n远平面缩放之后各点坐标：\n$$ \\begin{pmatrix} \\frac{nx}{z} \\\\ \\frac{ny}{z} \\\\ unknown \\\\ 1 \\end{pmatrix} \\overset{乘以深度z}{==} \\begin{pmatrix} nx \\\\ ny \\\\ unkown \\\\ z \\end{pmatrix} $$\n确定$M_{persp→ortho}$\n$$ M_{persp→ortho}^{(4×4)} \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}= \\begin{pmatrix} nx \\\\ ny \\\\ unknown \\\\ z \\end{pmatrix} $$\n$$ M_{persp→ortho}^{(4×4)} = \\begin{pmatrix} n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; n \u0026amp; 0 \u0026amp; 0 \\\\ ? \u0026amp; ? \u0026amp; ? \u0026amp; ? \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ \\end{pmatrix} $$\n近平面上的点作用之后不变：\n$$ \\begin{pmatrix} n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; n \u0026amp; 0 \u0026amp; 0 \\\\ ? \u0026amp; ? \u0026amp; ? \u0026amp; ? \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ n \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} x \\\\ y \\\\ n \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} nx \\\\ ny \\\\ n^2 \\\\ n \\end{pmatrix} $$\n因为$n^2$与x,y无关，所以第三行为：(0 0 A B), $An+B=n^2$\n远平面的中心点也没变，满足：\n$$ \\begin{pmatrix} n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; n \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; A \u0026amp; B \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ f \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ f \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ f^2 \\\\ f \\end{pmatrix} $$\n所以$Af+B=f^2$\n联立可解得A,B:\n$$ \\begin{cases} An+B=n^2 \\\\ Af+B=f^2 \\end{cases} ⇒ \\begin{cases} A=n+f \\\\ B=-nf \\end{cases} $$\n透视投影矩阵：$M_{persp} = M_{ortho} M_{persp→ortho}$\n","date":"2021-09-21T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/04-transform-cont/","title":"watch: CG - 闫令琪 04 | Transformation Cont."},{"content":"P5\n光栅化Rasterize 把物体画在屏幕上 把$[-1,1]^3$标准立方体(canonical cube)投影到屏幕上 viewing frustum 摄像机的视野\n图4 视体 三维世界中屏幕上可见的区域\n作用：确定哪些物体会被屏幕显示\n视体可用近平面的宽高比($\\frac{width}{height}$)和垂直可视角度定义\n屏幕 二维数组 每个元素是一个像素 ","date":"2021-09-20T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/05-rasterization-1triangles/","title":"watch: CG - 闫令琪 05 | Rasterization-1(Triangles)"},{"content":"P3 Transformation\n旋转矩阵 乘以一个向量时，改变向量方向的矩阵\n表示了不同维度坐标的线性变换\n作用：把向量(默认绕原点、逆时针)的旋转用矩阵乘法表示\n对于一个二维向量，旋转关系用矩阵乘法形式为：\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y' \\end{pmatrix} = M_{2×2} \\begin{pmatrix} x \\\\ y \\end{pmatrix} $$\n用待定系数法可以确定各元素\n对于(a,0)点：\n$$ \\begin{pmatrix} acosθ \\\\ asinθ \\end{pmatrix} = \\begin{pmatrix} A \u0026amp; B \\\\ C \u0026amp; D \\end{pmatrix}\n\\begin{pmatrix} a \\\\ 0 \\end{pmatrix} $$\n解得：$A=cosθ, C=sinθ$； 同理代入(0,b)点，可解得：$B=-sinθ, D=cosθ$，所以旋转矩阵为：\n$$ \\begin{pmatrix} cosθ \u0026amp; -sinθ \\\\ sinθ \u0026amp; cosθ \\end{pmatrix} $$\n旋转矩阵的逆 等于它的转置（$R_θ^{-1}=R_θ^T$）\n因为旋转矩阵是一个正交矩阵\n逆操作就是顺时针旋转相同的角度，也就是正向旋转$-θ$，代入得：\n$$ \\begin{pmatrix} cosθ \u0026amp; sinθ \\\\ -sinθ \u0026amp; cosθ \\end{pmatrix} $$\n即为旋转矩阵的转置\n齐次坐标 向量最后加个0，点最后加个1\n3D vector: $(x,y,z,0)^T$ 3D point: $(x,y,z,1)^T$\n作用：平移变换也可写成一个矩阵\n对于平移关系：\n$$ \\begin{cases} x\u0026rsquo; = x + t_x \\\\ y\u0026rsquo; = y + t_y \\end{cases} $$\nx,y方向都没有旋转，所以旋转矩阵为：\n$$ \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} $$\n写成齐次坐标：\n$$ \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \\end{pmatrix} $$\n只有平移，附加到后面：\n$$ \\begin{pmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; t_x \\\\ 0 \u0026amp; 1 \u0026amp; t_y \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ 0 \\end{pmatrix} $$\n同一点的表示不唯一：\n$$ \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} kx \\\\ ky \\\\ kz \\\\ k \\end{pmatrix} $$\n仿射变换 旋转变换和平移变换拼成一个矩阵 ","date":"2021-09-19T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/03-transformation/","title":"watch: CG - 闫令琪 03 | Transformation"},{"content":"画竖直和水平的分割线 1 2 3 plt.vlines(x, ymin, ymax) plt.hlines(y, xmin, xmax) 例：plt.vlines(0, 0, 0.5, colors = \u0026quot;r\u0026quot;, linestyles = \u0026quot;dashed\u0026quot;)\n纵坐标刻度变文字 参考Matplotlib：设置坐标轴范围，刻度，位置，自定义刻度名称，添加数据标签\n1 2 3 # 把纵坐标-2变为文字“really bad\u0026#34; plt.yticks([-2, -1.8, -1, 1.22, 3],[r\u0026#39;$really\\ bad$\u0026#39;, r\u0026#39;$bad$\u0026#39;, r\u0026#39;$normal$\u0026#39;, r\u0026#39;$good$\u0026#39;, r\u0026#39;$really\\ good$\u0026#39;]) 坐标显示范围 1 2 3 #设置坐标轴范围 plt.xlim((-5, 5)) plt.ylim((-2, 2)) 坐标轴设置 1 2 3 4 5 6 7 8 9 10 11 ax = plt.gca() # 设置上边和右边无边框 ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) # 设置x坐标刻度数字或名称的位置 ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) # 设置边框位置 ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;, 0)) 带箭头的x-y坐标系 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import matplotlib.pyplot as plt import numpy as np import math # 引入axisartist工具 import mpl_toolkits.axisartist as axisartist # 创建画布 # fig = plt.figure(figsize=(8,8)) fig = plt.figure(dpi = 100) # 使用axisartist.Subplot 方法创建一个绘图区对象 ax ax = axisartist.Subplot(fig, 111) # 将绘图区对象添加到画布中 fig.add_axes(ax) # 设置绘图区原来所有坐标轴隐藏 ax.axis[:].set_visible(False) # 添加新的坐标轴 ax.axis[\u0026#34;x\u0026#34;] = ax.new_floating_axis(0,0) #第一个0代表水平直线，第二个0代表直线经过0点 ax.axis[\u0026#34;y\u0026#34;] = ax.new_floating_axis(1,0) #1代表竖直直线，0代表直线经过0点 # 给 x轴,y轴 加上箭头 ax.axis[\u0026#34;x\u0026#34;].set_axisline_style(\u0026#34;-\u0026gt;\u0026#34;,size =1.0)#空心箭头 ax.axis[\u0026#34;y\u0026#34;].set_axisline_style(\u0026#34;-|\u0026gt;\u0026#34;,size=1.0)#实心箭头 # 设置刻度显示方向 ax.axis[\u0026#34;x\u0026#34;].set_axis_direction(\u0026#34;top\u0026#34;) ax.axis[\u0026#34;y\u0026#34;].set_axis_direction(\u0026#34;right\u0026#34;) # 绘制曲线： 两点连线 1 plt.plot([0,6],[0,0.5]) 箭头 1 plt.arrow(0,0,6,0.5,head_width=0.1,head_length=0.1,overhang=1,ec=\u0026#34;deepskyblue\u0026#34;,linestyle=\u0026#34;:\u0026#34;) 用注释画箭头 1 2 3 ax=plt.gca() ax.annotate(\u0026#34;隐逝波\u0026#34;, xy=(0,0), xytext=(6,0.5), arrowprops=dict(arrowstyle=\u0026#34;\u0026lt;-\u0026#34;, color=\u0026#34;deepskyblue\u0026#34;, linestyle=\u0026#34;:\u0026#34;) ) 隐藏坐标轴 1 plt.xticks([]), plt.yticks([]) # 隐藏x和y轴 坐标轴标签 函数原型及参数 matplotlib.pyplot.xlabel(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)\nxlabel：类型为字符串，即标签的文本。 labelpad：类型为浮点数，默认值为None，即标签与坐标轴的距离。 loc：取值范围为{‘left’, ‘center’, ‘right’}，默认值为rcParams[“xaxis.labellocation”]（‘center’），即标签的位置。 **kwargs：Text 对象关键字属性，用于控制文本的外观属性，如字体、文本颜色等。 ","date":"2021-06-30T10:57:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/python_matplot/","title":"memo: Python | Matplotlib"},{"content":"Books 频率派 统计机器学习\n《统计学习方法》李航\n12章：1绪论，12总结，中间10个常用算法：感K朴决逻 支提E隐条\n《机器学习》“西瓜书” 周志华\n很多学习方法，全面但不深入，基本推导和原理，\nThe Elements of Statistical Learning（ESL）\n贝叶斯派 概率图模型\nPattern Recognition And Machine Learning (PRML)\n12章算法：回分神核析 图混近采连 顺组\nMachine Learning-A Probabilistic Perspective (MLAPP)\n百科全书，包罗万象\n深度学习 《DeepLearning》“圣经” 张志华译\nVideos shuhuai007 github\n《机器学习基石》——台大 林轩田\n基本理论：VC Theroy，正则化，基础线性模型\n《机器学习技法》——台大 林轩田\n各种模型：SVM(好)，决策树，随机森林，神经网络，深度学习（前向网络）\n《机器学习导论》——张志华\n频率派角度，推导较多\n《统计机器学习》——张志华\n一些统计理论，贝叶斯角度：如概率不等式，偏数学，推导较多\n《斯坦福课堂 CS 229》——吴恩达\n大量数学推导，好像有2017年的新版添加deepLearning内容\n概率模型一系列视频——2015 徐亦达\n深度较深，EM，MCMC，HMM，滤波算法。在github上有notes：概率模型，DeepLearning\n《ML》(机器学习)——2017 台大 李宏毅\nCNN，RNN，LSTM\n《MLDS》——2018 台大 李宏毅\n深度学习里的优化，正则化，实践方法，NLP模型\n李宏毅2020机器学习深度学习（附完整课件和源码）[b站]\n","date":"2021-06-10T06:52:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/","title":"watch: ML - 白板 00 | Learning Materials"},{"content":"入门：C/C++中的段错误（Segmentation fault）【转】\nGDB调试 1 2 gcc -g test.c -o test #把调试信息加入二进制代码中 gdb test #启动gdb调试 一、暂停/恢复程序运行 1. 启动gdb 1 2 3 gdb \u0026lt;program\u0026gt;\t#当前目录下编译好的二进制代码 gdb \u0026lt;program\u0026gt; core\t#同时调试程序和core文件（core dump后产生的文件） gdb \u0026lt;program\u0026gt; \u0026lt;PID\u0026gt;\t#如果是服务程序，可以指定进程id，gdb会自动attach上去调试它，program应在PATH环境变量中搜得到 2. 设置断点 用break命令设置断点：\nbreak \u0026lt;function\u0026gt; ：在进入指定函数时停住\n1 2 break class::function break function(type, type) break \u0026lt;linenum\u0026gt; ：在指定行号停住\nbreak +offset / -offset ： 在当前行的前 / 后的offset行停住\nbreak filename:linenum ：在源文件filename的linenum行停住\nbreak filename::function ：在源文件filename的function函数入口停住\nbreak *address ：在程序运行的内存地址处停住\nbreak ：没有加参数时，表示在下一条指令处停住\nbreak \u0026hellip; if \u0026lt;condition\u0026gt; ：\u0026hellip;可以是上述参数，condition表示条件，在条件成立时停住 （条件断点）\n1 break if i=100\t#当i=100时，停住程序 查看断点，可使用 info 命令：\ninfo breakpoints [n] （ n 表示断点号） info break [n] 3. 设置观察点 watchpoint 一般用来观察某个表达式（变量也是一种表达式）的值是否有变化了，如果有变化，马上停住程序。我们有下面的几种方法来设置观察点:\nwatch \u0026lt;expr\u0026gt; ：为表达式（变量）expr 设置一个观察点。 rwatch \u0026lt;expr\u0026gt; ：当表达式（变量）expr 被读时，停住程序。 awatch \u0026lt;expr\u0026gt; ：当表达式（变量）的值被读或写时，停住程序 info watchpoints ：列出当前设置的所有观察点 4. 设置捕捉点 catchpoint 用来捕捉程序运行时的一些事件。如:载入共享库(动态链接库)或是 C++ 的异常。设置捕捉点的格式为:\ncatch \u0026lt;event\u0026gt; ：当event发生时，停住程序\n1 2 catch throw\t#一个C++抛出的异常 catch catch\t#一个C++捕捉到的异常 tcatch \u0026lt;event\u0026gt; ：只设置一次捕捉点，当程序停住后，点被自动删除\n5. 维护断点 GDB 中的断点也就是上述的三类。在 GDB 中,如果你觉得已定义好的停止点没有用了,你可以使用 delete 、 clear 、 disable 、 enable 这几个命令来进行维护。\nclear ：清除所有的已定义的停止点 clear \u0026lt;function\u0026gt; 或 clear \u0026lt;filename:function\u0026gt; ：清除所有设置在函数中的停止点 clear \u0026lt;linenum\u0026gt; 或 clear \u0026lt;filename:linenum\u0026gt; ：清除所有设置在指定行上的停止点 delete [breakpoints] [range\u0026hellip;] ：删除指定的断点（breakpoints是断点号，若不指定断点号，表示删除所有的断点。range表示断点号的范围（如：3-7）。其简写命令为 d disable [breakpoints] [range\u0026hellip;] ：断点不会被删除，当再次需要时，enable即可。如果不指定断点，会disable所有断点 enable [breakpoints] [range\u0026hellip;] enable [breakpoints] once range\u0026hellip; ：使能指定的断点一次，当程序停住，该断点立刻被GDB禁用disable enable [breakpoints] delete range\u0026hellip; ：使能指定的断点一次，当程序停止后，该断点立刻被删除 6. 维护停止条件 一般来说,为断点设置一个条件,我们使用 if 关键词,后面跟其断点条件。并且,条件设置好后,我们可以用 condition 命令来修改断点的条件。 (只有 break 和 watch 命令支持 if, catch 目前暂不支持 if )\nconditon \u0026lt;bnum\u0026gt; \u0026lt;expression\u0026gt; ：修改断点号为 bnum 的停止条件为 expression condition \u0026lt;bnum\u0026gt; ：清除断点号 bnum 的停止条件 ignore \u0026lt;bnum\u0026gt; \u0026lt;count\u0026gt; ：指定程序运行时，忽略断点号为 bnum 的停止条件 count 次 7. 为断点设定运行命令 可以使用 GDB 提供的 command 命令来设置停止点的运行命令。 也就是说,当运行的程序在被停止住时,我们可以让其自动运行一些别的命令,这很有利行自动化调试。对基于 GDB 的自动化调试是一个强大的支持。\ncommands [bnum] \u0026hellip; command-list \u0026hellip; end\n为断点号 bnum 写一个命令列表，当程序被该断点停住时，gdb 会依次运行命令列表中的命令。例如：\n1 2 3 4 5 break foo if x\u0026gt;0\t#断点设置在函数foo中，断点条件是x\u0026gt;0 commands printf \u0026#34;x is %d/n\u0026#34;,x\t#打印 x 的值 continue\t#然后继续运行程序 end 如果要清除断点上的命令序列,那么只要简单的执行一下 commands 命令,并直接在输个 end 就行了。\n8. 断点菜单 在 C++ 中，可能会重复出现同一个名字的函数若干次（函数重载），在这种情况下, break \u0026lt;function\u0026gt; 不能告诉 GDB 要停在哪个函数的入口。 当然，你可以使用 break \u0026lt;function(type)\u0026gt; 也就是把函数的参数类型告诉 GDB，以指定一个函数。 否则的话，GDB 会给你列出一个断点菜单供你选择你所需要的断点。你只要输入你菜单列表中的编号就可以了。如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (gdb) b String::after [0] cancel [1] all [2] file:String.cc; line number:867 [3] file:String.cc; line number:860 [4] file:String.cc; line number:875 [5] file:String.cc; line number:853 [6] file:String.cc; line number:846 [7] file:String.cc; line number:735 \u0026gt; 2 4 6 Breakpoint 1 at 0xb26c: file String.cc, line 867. Breakpoint 2 at 0xb344: file String.cc, line 875. Breakpoint 3 at 0xafcc: file String.cc, line 846. Multiple breakpoints were set. Use the \u0026#34;delete\u0026#34; command to delete unwanted breakpoints. (gdb) 可见，GDB 列出了所有 after 的重载函数,你可以选一下列表编号就行了。 0 表示放弃设置断点, 1 表示所有函数都设置断点。\n9. 恢复程序运行和单步调试 当程序被停住了，可以用continue命令恢复程序的运行直到程序结束，或到下一个断点处。也可以使用step 或者 next 单步执行\ncontinue [ignore-count] 或者 c [ignore-count] 或者 fg [ignore-count] ：恢复程序运行直到 Basic:\n1 2 3 gcc -ggdb3 hello.c #gdb3会生成更丰富的调试信息，可以和gdb更好的完成内联功能 gdb a.out (gdb) start #主要的临时断点 Output 1 2 3 4 5 Temporary breakpoint 1 at 0x1169: file hello.c, line 3. Starting program: /home/jack/backup/opencvTest/a.out Temporary breakpoint 1, main () at hello.c:3 3 { 显示代码片段：\n1 (gdb) list\t#显示源代码 Output 1 2 3 4 5 6 7 8 9 10 1 #include\u0026lt;stdio.h\u0026gt; 2 int main(void) 3 { 4 int i = 0; 5 printf(\u0026#34;Hello, world\\n\u0026#34;); 6 printf(\u0026#34;i is %d\\n\u0026#34;, i); 7 i++; 8 printf(\u0026#34;i is now %d\\n\u0026#34;, i); 9 return 0; 10 } 无法显示中文 GDB Ref:\nCppCon 2016: Greg Law “GDB - A Lot More Than You Knew\u0026quot;\n\u0026lsquo;Become a GDB Power User\u0026rsquo; - Greg Law [ ACCU 2016 ] (2024-01-19) 两个视频差不多又有区别，第一个画质好内容少一些，第二个画质差但内容多一些 冬眠贝尔熊 的评论\n1. More than you knew 按下 Ctrl+x+a，从1979进入八十年代！类似图形界面的TUI，再按一遍回到命令行\nctrl + l : 刷新屏幕\nctrl + p / ctrl+n : prev / next command\nctrl + x + 2 : 第2个窗口，cycle though\n或者输入：tui enable，或者 layout src，或者 layout asm（显示汇编代码），进入 TUI\n2. GDB has Python Full Pyton interpreter with access to standard modules (unless your gdb installaion is messed up!)\nThe gdb python module gives most access to gdb\n1 2 3 (gdb) python gdb.execute()\t#执行gdb命令 (gdb) python gdb.parse_and_eval()\t#to get data from inferior (gdb) python help(\u0026#39;gdb\u0026#39;)\t#to see online help Python Pretty Printers\n1 2 3 4 5 class MyPrinter(object) def __init__(self,val): self.val = val; def to_string(self): return (self.val[\u0026#39;member\u0026#39;]) 1 2 3 4 import gdb.printing pp = gdb.printing.RegexpCollectionPrettyPrinter(\u0026#39;mystruct\u0026#39;) pp.add_printer(\u0026#39;mystruct\u0026#39;, \u0026#39;^mystruct$\u0026#39;, MyPrinter) gdb.printing.register_pretty_printer( gdb.current_objfile(),pp) 3. In-built pretty printers for STL GDB will ( try to ) pretty-print most STL container classes ( std : : vector , std : string , etc ) , e.g.\n1 2 3 4 5 6 10\tvec.push_back(5); (gdb) next 12\treturn 0; (gdb) print vec $6 = std::vector of length 3, capacity 4 = {3, 4, 5} (gdb) Note that this relies on Python pretty printers installed on the target system\nCompiling / linking with a different version of libstdc + + ( e . g . executable built on a different host than the one beingused to debug ) , then pretty printing might give strange results.\nThere are many ( list with info pretty-printers ) , includingstd : string , std : bitset , std : list , std : multimap , std : queue , std : set , std : shared _ ptr ,std : stack , std : tuple , std : unique _ ptr , std : vector , std : weak _ ptr , and iterators.\n4. .gdbinit 5. GDB is built on ptrace and signals GDB是建立在ptrace之上的。当一个正在被跟踪的程序收到一个信号，它会暂停并且tracer会通过waitpid 注意到，所以当 the inferior收到信号，它就会停止然后gbd获得控制。通常gdb会回到prompt，但是具体会做什么取决于信号和设置。\n1 (gdb) info signals 有两个信号很特别：\nSIGINT 当在按下 Crtl+c 会产生 SIGTRAP 当 the inferior 遇到断点或者单步调试时会产生。(改变代码，0xCC操作码生成陷阱) 1 2 3 4 5 6 (gdb) handle SIGINT stop pirnt pass Signal\tStop\tPrint\tPass to program\tDescription SIGINT\tYes\tYes\tYes\tInterrupt (gdb) handle SIGINT stop print nopass Signal\tStop\tPrint\tPass to program\tDescription SIGINT\tYes\tYes\tNo\tInterrupt 6. Breakpoints \u0026amp; watchpionts 1 2 3 4 5 watch foo #stop when foo is modified watch -l foo\t#watch location rwatch foo\t#stop when foo is read watch foo thread 3\t#stop when thread 3 modifies foo watch foo if foo \u0026gt; 10\t#stop when foo is\u0026gt;10 foo 是一个局部变量\n7. thread apply 1 2 3 thread apply 1-4 print $sp thread apply all backtrace thread apply all backtrace full 8. Dynamic Printf 不需改变代码，用printf的话需要考虑加在哪里，然后重新编译运行，查看输出，没查到结果，又要重复一遍\n9. Calling inferior functions call foo will call foo in your inferior\n10. Catchpoints 像断点\n11. Remote debugging 通过serial/sockets 调试远程服务器。\ngdbserver localhost:2000 ./a.out\n12. Multiprocess Debugging Modern Source video: 【GDB调试教程】如何设置条件断点？如何动态修改变量？Python和C++混合怎么调试？如何附加到进程？- 双笙子佯谬 - bilibili\n(2024-01-19)\n调试 Python 程序\nDocs: DebuggingWithGdb\n用 gdb 调试 python，进入之后把要调试的 test.py 文件作为参数传入. vid\n1 2 3 4 5 6 7 gdb test.py # This trivial way won\u0026#39;t work. gdb python # debug python r test.py # i.e., shell cmd: `python test.py` # the appended arguments will be put into argv # Or, with `-ex` gdb python -ex \u0026#39;r test.py\u0026#39; 这样，GDB 就可以捕获 python 的异常。\nTODO: Debugging Python C extensions with GDB - Redhat Developers\nTemporary breakpoint only break once.\n1 tb func ","date":"2021-04-08T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/gdb_cpp/","title":"memo: C++ | Debug with GDB"},{"content":"(Feature figure from g++ → make → cmake - 二圈妹的文章 - 知乎)\n《CMake实践》\n三、cmake工程 建立工程目录 t1\n1 2 3 mkdir t1 cd t1 touch main.c CMakeLists.txt 编辑源文件 main.c\n1 2 3 4 5 6 # include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;Hello World from t1 Main!\\n\u0026#34;); return 0; } 编辑CMakeLists.txt (原书最后一行有误)\n1 2 3 4 5 PROJECT(HELLO)\t#定义工程名称 SET(SRC_LIST main.c)\t#定义变量SRC_LIST, 值为main.c源文件 MESSAGE(STATUS \u0026#34;This is BINARY dir\u0026#34; ${HELLO_BINARY_DIR})\t#向终端输出用户定义的信息 MESSAGE(STATUS \u0026#34;This is SOURCE dir\u0026#34; ${HELLO_SOURCE_DIR}) ADD_EXECUTABLE(hello ${SRC_LIST})\t#编译源文件生成可执行文件hello PROJECT(projectname [CXX] [C] [JAVA])\n定义工程名称projectname，可指定工程支持的语言(支持的语言列表可缺省)，默认支持所有语言。这条projecct指令隐式地定义了2个cmake变量：\n1 2 \u0026lt;projectname\u0026gt;_BINARY_DIR\t#此例为：HELLO_BINARY_DIR 二进制文件目录 \u0026lt;projectname\u0026gt;_SOURCE_DIR\t#源文件目录 因为采用的是内部编译，2个变量目前指的都是工程所在路径 /backup/cmake/t1\n同时 cmake 系统也帮我们预定义了2变量：\n1 2 PROJECT_BINARY_DIR PROJECT_SOURCE_DIR 他们的值分别跟 HELLO_BINARY_DIR 与 HELLO_SOURCE_DIR 一致。 为了统一起见,建议以后直接使用 PROJECT_BINARY_DIR，PROJECT_SOURCE_DIR，即使修改了工程名称,也不会影响这两个变量。 如果使用了**\u0026lt;projectname\u0026gt;_SOURCE_DIR**，修改工程名称后,需要同时修改这些变量。 SET (VAR [VALUE] [CACHE TYPE DOCSTRING [FORCE]])\nset 指令可以用来显式的定义变量。\n1 2 SET(SRC_LIST main.c) SET(SRC_LIST main.c;t1.c;t2.c)\t#源文件列表也可以是多个文件 MESSAG ([SEND_ERROR | STATUS | FATAL_ERROR] \u0026ldquo;message to display\u0026rdquo;\u0026hellip;)\n向终端输出用户定义的信息，包含了三种类型：\nSEND_ERROR 产生错误，生成过程被跳过 STATUS 输出前缀为 - 的信息 FATAL_ERROR 立即终止所有 cmake 过程 ADD_EXECUTABLE(hello ${SRC_LIST})\n定义了这个工程会生成一个文件名为 hello 的可执行文件，相关的源文件是 SRC_LIST中定义的源文件列表。用 ${ } 来引用变量\n开始构建\n在工程目录下：\n1 cmake .\t#构建当前目录，生成了Makefile 根据Makefile编译源代码，连接，生成目标文件、可执行文件(hello)\n1 2 3 make #或者 make VERBOSE=1\t#可以看到make构建的详细过程,以便排查错误 运行可执行文件(hello)\n1 ./hello cmake基本语法 变量使用 ${ } 方式取值，但是在IF控制语句中是直接使用变量名\n指令(参数1 参数2 \u0026hellip;)\n参数用括号括住，参数之间用空格或分号隔开：\n1 2 ADD_EXECUTABLE(hello main.c func.c) ADD_EXECUTABLE(hello main.c;func.c) 指令是大小写无关的，参数和变量是大小写敏感的。（推荐全部大写指令）\n工程名HELLO 和 可执行文件名hello 是没有任何关系的\n如果文件名中有空格，使用双引号括住：\n1 SET(SRC_LIST \u0026#34;func.c\u0026#34;) 清理工程：\n清除上次的make命令所产生的目标(object)文件（后缀为“.o”的文件）及可执行文件。\n1 make clean make disclean 对cmake无效，所以需要用外部构建(out-of-source)，来使工程目录整洁。\n安装\n将编译成功的可执行文件安装到系统目录中，一般为/usr/local/bin 目录中\n1 make install 生成发行版软件包\n将可执行文件及相关文件打包成一个tar.gz压缩的文件用来作为发布软件的软件包。\n1 make dist 它会在当前目录下生成一个名字类似“PACKAGE-VERSION.tar.gz”的文件。PACKAGE和VERSION，是我们在configure.in中定义的AM_INIT_AUTOMAKE(PACKAGE, VERSION)。\n检查发行软件包\n生成发布软件包并对其进行测试检查，以确定发布包的正确性。\n1 make distcheck 这个操作将自动把压缩包文件解开，然后执行configure命令，并且执行make，来确认编译不出现错误，最后提示你软件包已经准备好，可以发布了。\n外部构建： 编译会生成一些无法自动删除的中间文件，所以在工程目录下建立build目录，用于存放中间文件，然后 cmake .. 对上层目录编译，在build目录中生成了make需要的Makefile和其他的中间文件。运行make编译，就会在build目录下获得目标文件 hello.o\nPROJECT_SOURCE_DIR 仍指代工程目录，即 /backup/cmake/t1 PROJECT_BINARY_DIR 则指代编译目录，即 /backup/cmake/t1/build 四、规范的工程 规范要求 为工程添加一个子目录 src， 用来放置工程源代码 添加一个子目录 doc，用来放置工程的文档 hello.txt； 在工程目录添加文本文件 COPYRIGHT，README； 在工程目录添加一个 runhello.sh 脚本，用来调用可执行文件 hello 将构建后的目标(object)文件放入构建目录的 bin 子目录 最终安装这些文件：将可执行文件 hello 与 runhello.sh 安装至 /usr/bin, 将doc 目录的内容以及 COPYRIGHT/README 安装到 /usr/share/doc/cmake/t2, 编译 准备工作\n在 /backup/cmake 目录下建立 /t2 目录，将 /t1 工程的 main.c 和 CMakeLists.txt 拷贝到 /t2 目录下。\n构建工程\n添加子目录 src：\n1 2 mkdir src mv main.c src\t#源代码放入 src 目录 进入 /t2/src，编写 CMakeLists.txt （需要为任何子目录建立一个 CMakeLists.txt）\n1 ADD_EXECUTABLE(hello main.c)\t#把mainc.c源码编译成一个名为hello的可执行文件 修改 /t2 工程目录下的CMakeLists.txt ，指定源代码文件夹和编译输出(包括中间结果：每个子文件夹下都有CMakeLists.txt，都会产生编译结果)文件夹\n1 2 PROJECT(HELLO) ADD_SUBDIRECTORY(src bin)\t#指定源码目录/t2/src，指定make编译结果放入/build/bin， ADD_SUBDIRECTORY(source_dir [binary_dir] [EXCLUDE_FORM_ALL])\n此指令用于向当前工程添加存放源文件的子目录(source_dir)，并可以指定编译输出存放的位置(binary_dir)。[EXCLUDE_FROM_ALL] 参数的含义是将这个目录从编译过程中排除，比如，工程的example，可能就需要工程编译完成后，再进入example 目录单独进行构建。\n如果不指定 bin 目录，编译结果（包括中间结果）都将存放在 build/src 目录下，指定 bin 目录后，相当于在编译时将 /build/src 重命名为 /bin。\n换个地方保存可执行文件和库文件\n不论是 SUBDIRS 还是 ADD_SUBDIRECTORY 指令(不论是否指定编译输出目录),我们都可以通过 SET 指令重新定义 EXECUTABLE_OUTPUT_PATH和 LIBRARY_OUTPUT_PATH 变量，来指定最终的目标二进制的位置（即最终生成的可执行文件 hello 或者最终的共享库,不包含编译生成的中间文件)\n1 2 SET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\t#即为/build/bin SET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)\t#即为/build/lib 在ADD_EXECUTABLE或ADD_LIBRARY后面，写这条指令。\n建立build，进入build目录，进行外部编译\n1 2 3 cd build cmake .. make 构建完成后，你会发现生成的可执行文件 hello 位于 build/bin 目录中。\nINSTALL指令 有两种安装方法，一种是从代码编译后直接 make install 安装，一种是打包时的指定目录安装。可以通过：\n1 make install\t#将 hello 直接安装到 /usr/bin 目录 或者：\n1 make install DESTDIR=/tmp/test\t#安装在/tmp/test/usr/bin 目录，打包时这个方式经常被使用。 稍微复杂一点的是还需要定义 PREFIX，一般autotools工程，会运行这样的指令：\n./configure -prefix=/usr 或者 **./configure --prefix=/usr/local**来指定 PREFIX。\n对于cmake来说，使用：\nINSTALL 指令：\n用于定义安装规则，安装的内容可以包括二进制、动态库、静态库以及文件、目录、脚本等：\n1 2 3 4 5 6 INSTALL(TARGETS \u0026lt;target\u0026gt;... [...])\t#安装二进制 INSTALL({FILES | PROGRAMS} \u0026lt;file\u0026gt;... [...]) INSTALL(DIRECTORY \u0026lt;dir\u0026gt;... [...]) INSTALL(SCRIPT \u0026lt;file\u0026gt; [...]) INSTALL(CODE \u0026lt;code\u0026gt; [...]) INSTALL(EXPORT \u0026lt;export-name\u0026gt; [...]) 有时候，也会用到一个非常有用的变量**CMAKE_INSTALL_PREFIX**，用于指定cmake install时的相对地址前缀。用法如：\n1 cmake -DCMAKE_INSTALL_PREFIX=/usr .. CMAKE_INSTALL_PREFIX变量类似与configure 脚本的 -prefix。\nINSTALL 指令的各种安装类型：\n1. 目标文件的安装： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INSTALL(TARGETS targets ...\t#各种目标文件 [EXPORT \u0026lt;export-name\u0026gt;] [[ARCHIVE|LIBRARY|RUNTIME|OBJECTS|FRAMEWORK|BUNDLE| PRIVATE_HEADER|PUBLIC_HEADER|RESOURCE] #目标文件类型 [DESTINATION \u0026lt;dir\u0026gt;]\t#指定各文件的安装目录\u0026lt;dir\u0026gt; [PERMISSIONS permissions...]\t#文件的权限 [CONFIGURATIONS [Debug|Release|...]]\t#指定安装规则适用的构建配置列表(DEBUG或RELEASE等) [COMPONENT \u0026lt;component\u0026gt;] [NAMELINK_COMPONENT \u0026lt;component\u0026gt;] [OPTIONAL] #如果要安装的文件不存在，则指定不是错误。 [EXCLUDE_FROM_ALL]\t#指定该文件从完整安装中排除，仅作为特定于组件的安装的一部分进行安装； [NAMELINK_ONLY|NAMELINK_SKIP] ] [...] [INCLUDES DESTINATION [\u0026lt;dir\u0026gt; ...]] ) 参数中的TARGET可以是很多种目标文件，最常见的是通过ADD_EXECUTABLE或者ADD_LIBRARY定义的目标文件，即可执行二进制、动态库、静态库：以下是默认的安装路径\n目标文件 内容 安装目录变量 默认安装文件夹 ARCHIVE 静态库 ${CMAKE_INSTALL_LIBDIR} lib LIBRARY 动态库 ${CMAKE_INSTALL_LIBDIR} lib RUNTIME 可执行二进制文件 ${CMAKE_INSTALL_BINDIR} bin PUBLIC_HEADER 与库关联的PUBLIC头文件 ${CMAKE_INSTALL_INCLUDEDIR} include PRIVATE_HEADER 与库关联的PRIVATE头文件 ${CMAKE_INSTALL_INCLUDEDIR} include 为了符合一般的默认安装路径，如果设置了DESTINATION参数，推荐配置在安装目录变量下的文件夹。\n例如：\n1 2 3 4 5 INSTALL(TARGETS myrun mylib mystaticlib RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} ) 上面的例子会将：可执行二进制myrun安装到${CMAKE_INSTALL_BINDIR}目录，动态库libmylib.so安装到${CMAKE_INSTALL_LIBDIR}目录，静态库libmystaticlib.a安装到${CMAKE_INSTALL_LIBDIR}目录。\nINSTALL命令的其他一些参数的含义：\nDESTINATION：指定磁盘上要安装文件的目录； PERMISSIONS：指定安装文件的权限。有效权限是OWNER_READ，OWNER_WRITE，OWNER_EXECUTE，GROUP_READ，GROUP_WRITE，GROUP_EXECUTE，WORLD_READ，WORLD_WRITE，WORLD_EXECUTE，SETUID和SETGID；（11种权限） CONFIGURATIONS：指定安装规则适用的构建配置列表(DEBUG或RELEASE等)； EXCLUDE_FROM_ALL：指定该文件从完整安装中排除，仅作为特定于组件的安装的一部分进行安装； OPTIONAL：如果要安装的文件不存在，则指定不是错误。 注意一下CONFIGURATIONS参数，此选项指定的值仅适用于此选项之后列出的选项：例如，要为调试和发布配置设置单独的安装路径，请执行以下操作：\n1 2 3 4 5 6 INSTALL(TARGETS target CONFIGURATIONS Debug RUNTIME DESTINATION Debug/bin) INSTALL(TARGETS target CONFIGURATIONS Release RUNTIME DESTINATION Release/bin) 也就是说，DEBUG和RELEASE版本的DESTINATION安装路径不同，那么DESTINATION必须在CONFIGUATIONS后面。\n2. 普通文件的安装 1 2 3 4 5 6 INSTALL(\u0026lt;FILES|PROGRAMS\u0026gt; files... TYPE \u0026lt;type\u0026gt; | DESTINATION \u0026lt;dir\u0026gt; [PERMISSIONS permissions...] [CONFIGURATIONS [Debug|Release|...]] [COMPONENT \u0026lt;component\u0026gt;] [RENAME \u0026lt;name\u0026gt;] [OPTIONAL] [EXCLUDE_FROM_ALL]) FILES|PROGRAMS若为相对路径给出的文件名，将相对于当前源目录进行解释。其中，FILES为普通的文本文件，PROGRAMS指的是非目标文件的可执行程序(如脚本文件)。\n如果未提供PERMISSIONS参数，默认情况下，普通的文本文件将具有OWNER_WRITE，OWNER_READ，GROUP_READ和WORLD_READ权限，即644权限；而非目标文件的可执行程序将具有OWNER_EXECUTE, GROUP_EXECUTE,和WORLD_EXECUTE，即755权限。\n其中，不同的TYPE，cmake也提供了默认的安装路径，如下表：\nTYPE类型 安装目录变量 默认安装文件夹 BIN ${CMAKE_INSTALL_BINDIR} bin SBIN ${CMAKE_INSTALL_SBINDIR} sbin LIB ${CMAKE_INSTALL_LIBDIR} lib INCLUDE ${CMAKE_INSTALL_INCLUDEDIR} include SYSCONF ${CMAKE_INSTALL_SYSCONFDIR} etc SHAREDSTATE ${CMAKE_INSTALL_SHARESTATEDIR} com LOCALSTATE ${CMAKE_INSTALL_LOCALSTATEDIR} var RUNSTATE ${CMAKE_INSTALL_RUNSTATEDIR} /run DATA ${CMAKE_INSTALL_DATADIR} INFO ${CMAKE_INSTALL_INFODIR} /info LOCALE ${CMAKE_INSTALL_LOCALEDIR} /locale MAN ${CMAKE_INSTALL_MANDIR} /man DOC ${CMAKE_INSTALL_DOCDIR} /doc 请注意，某些类型的内置默认值使用DATAROOT目录作为前缀，以CMAKE_INSTALL_DATAROOTDIR变量值为内容。\n该命令的其他一些参数的含义：\nDESTINATION：指定磁盘上要安装文件的目录； PERMISSIONS：指定安装文件的权限。有效权限是OWNER_READ，OWNER_WRITE，OWNER_EXECUTE，GROUP_READ，GROUP_WRITE，GROUP_EXECUTE，WORLD_READ，WORLD_WRITE，WORLD_EXECUTE，SETUID和SETGID； CONFIGURATIONS：指定安装规则适用的构建配置列表(DEBUG或RELEASE等)； EXCLUDE_FROM_ALL：指定该文件从完整安装中排除，仅作为特定于组件的安装的一部分进行安装； OPTIONAL：如果要安装的文件不存在，则指定不是错误； RENAME：指定已安装文件的名称，该名称可能与原始文件不同。仅当命令安装了单个文件时，才允许重命名。 3. 目录的安装 1 2 3 4 5 6 7 8 9 10 INSTALL(DIRECTORY dirs... TYPE \u0026lt;type\u0026gt; | DESTINATION \u0026lt;dir\u0026gt; [FILE_PERMISSIONS permissions...] [DIRECTORY_PERMISSIONS permissions...] [USE_SOURCE_PERMISSIONS] [OPTIONAL] [MESSAGE_NEVER] [CONFIGURATIONS [Debug|Release|...]] [COMPONENT \u0026lt;component\u0026gt;] [EXCLUDE_FROM_ALL] [FILES_MATCHING] [[PATTERN \u0026lt;pattern\u0026gt; | REGEX \u0026lt;regex\u0026gt;] [EXCLUDE] [PERMISSIONS permissions...]] [...]) 该命令将一个或多个目录的内容安装到指定的目的地，目录结构被逐个复制到目标位置。每个目录名称的最后一个组成部分都附加到目标目录中，但是可以使用后跟斜杠来避免这种情况，因为它将最后一个组成部分留空。这是什么意思呢？\n比如，DIRECTORY后面如果是abc意味着abc这个目录会安装在目标路径下，abc/意味着abc这个目录的内容会被安装在目标路径下，而abc目录本身却不会被安装。即，如果目录名不以\u0026quot;/\u0026ldquo;结尾，那么这个目录将被安装为目标路径下的abc，如果目录名以/结尾，代表将这个目录中的内容安装到目标路径，但不包括这个目录本身。\nFILE_PERMISSIONS和DIRECTORY_PERMISSIONS选项指定对目标中文件和目录的权限。如果指定了USE_SOURCE_PERMISSIONS而未指定FILE_PERMISSIONS，则将从源目录结构中复制文件权限。如果未指定权限，则将为文件提供在命令的FILES形式中指定的默认权限(644权限)，而目录将被赋予在命令的PROGRAMS形式中指定的默认权限(755权限)。\n可以使用PATTERN或REGEX选项以精细的粒度控制目录的安装，可以指定一个通配模式或正则表达式以匹配输入目录中遇到的目录或文件。PATTERN仅匹配完整的文件名，而REGEX将匹配文件名的任何部分，但它可以使用/和$模拟PATTERN行为。\n某些跟随PATTERN或REGEX表达式后的参数，仅应用于满足表达式的文件或目录。如：EXCLUDE选项将跳过匹配的文件或目录。PERMISSIONS选项将覆盖匹配文件或目录的权限设置。\n例如：\n1 2 3 4 5 6 INSTALL(DIRECTORY icons scripts/ DESTINATION share/myproj PATTERN \u0026#34;CVS\u0026#34; EXCLUDE PATTERN \u0026#34;scripts/*\u0026#34; PERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ) 这条命令的执行结果是：将icons目录安装到share/myproj，将scripts/中的内容安装到share/myproj，两个目录均不包含目录名为CVS的子目录，对于scripts/*的文件指定权限为OWNER_EXECUTE，OWNER_WRITE，OWNER_READ，GROUP_EXECUTE，GROUP_READ。\n4. 安装时脚本的运行 有时候需要在install的过程中打印一些语句，或者执行一些cmake指令：\n1 2 INSTALL([[SCRIPT \u0026lt;file\u0026gt;] [CODE \u0026lt;code\u0026gt;]] [COMPONENT \u0026lt;component\u0026gt;] [EXCLUDE_FROM_ALL] [...]) SCRIPT参数将在安装过程中调用给定的CMake脚本文件(即.cmake脚本文件)，如果脚本文件名是相对路径，则将相对于当前源目录进行解释。CODE参数将在安装过程中调用给定的CMake代码。将代码指定为双引号字符串内的单个参数。\n例如：\n1 INSTALL(CODE \u0026#34;MESSAGE(\\\u0026#34;Sample install message.\\\u0026#34;)\u0026#34;) 这条命令将会在install的过程中执行cmake代码，打印语句。\n安装 就是把文件复制到到指定目录下\n添加 doc 目录及文件：\n1 2 3 cd /backup/cmake/t2 mkdir doc\t#存储工程文档 touch doc/hello.txt **添加脚本：**在工程目录添加 runhello.sh，内容为：\n1 2 cd /home/jack/backup/cmake/t2/build/bin ./hello\t#调用可执行文件 **添加文件：**工程目录中的 COPYRIGHT 和 README\n1 touch COPYRIGHT README 修改 CMakeLists.txt ，使之可以支持各种文件的安装\n安装文档，修改工程目录下的 CMakeLists.txt\n1 2 3 4 5 # 安装 COPYRIGHT/README 到 /\u0026lt;prefix\u0026gt;/share/doc/cmake/t2 INSTALL(FILES COPYRIGHT README DESTINATION share/doc/cmake/t2) # 安装 runhello.sh 到 /\u0026lt;prefix\u0026gt;/bin INSTALL(PROGRAMS runhello.sh DESTINATION bin) 安装 doc 中的 hello.txt , 两种方式：\n通过 doc 目录建立 CMakeLists.txt 并将 doc 目录通过 ADD_SUBDIRECTORY 加入工程来完成。\n直接在工程目录通过 INSTALL(DIRECTORY 来完成)：\n因为 hello.txt 要安装到 /\u0026lt;prefix\u0026gt;/share/doc/cmake/t2,所以我们不能直接安装整个 doc 目录，这里采用的方式是安装 doc 目录中的内容，也就是使用 \u0026quot; doc/ \u0026ldquo;。 在工程目录下的CMakeLists.txt 中添加：\n1 INSTALL(DIRECTORY doc/ DESTINATION share/doc/cmake/t2) 编译并安装\n进入build 目录进行外部编译，注意使用 CMAKE_INSTALL_PREFIX 参数，这里将它安装到了 /tmp/t2 目录:\n1 2 3 cmake -DCMAKE_INSTALL_PREFIX=/tmp/t2/usr .. make make install cd 进入 /tmp/t2 目录看以下安装结果：\n1 2 3 4 5 6 7 8 9 10 11 ./usr ./usr/share ./usr/share/doc ./usr/share/doc/cmake ./usr/share/doc/cmake/t2 ./usr/share/doc/cmake/t2/hello.txt ./usr/share/doc/cmake/t2/README ./usr/share/doc/cmake/t2/COPYRIGHT ./usr/bin ./usr/bin/hello ./usr/bin/runhello.sh 如果要直接安装到系统，可以使用如下指令：\n1 cmake -DCMAKE_INSTALL_PREFIX=/usr .. 如果没有额外定义，CMAKE_INSTALL_PREFIX 的默认定义是 /usr/local\n五、静态库与动态库构建 动态库、静态库与可执行文件的区别：\n动态链接库（Dynamic Link Library，缩写为DLL）是在程序运行时动态调用的，可以被其它应用程序共享的程序模块，其中封装了一些可以被共享的例程和资源。动态链接库文件的扩展名一般是dll，也有可能是drv、sys和fon，它和可执行文件（exe）非常类似，区别在于DLL中虽然包含了可执行代码却不能单独执行，而应由Windows应用程序直接或间接调用。\nLib称为静态链接库(static link library)，是在编译的链接期间使用的，他里面其实就是源文件的函数实现。Lib只是Dll的附带品，是DLL导出的函数列表文件而已。\nDll其实和Exe是几乎完全一样的，唯一的不一样就是Exe的入口函数式WinMain函数（console程序是main函数），而Dll是DllMain函数，其他完全是一样的。所以有人也戏称Dll是不能自己运行的Exe。\n静态链接是指把要调用的函数或者过程链接到可执行文件中，成为可执行文件的一部分(拷贝函数) 动态链接所调用的函数代码并没有被拷贝到应用程序的可执行文件中去，而是仅仅在其中加入了所调用函数的描述信息（往往是一些重定位信息）。仅当应用程序被装入内存开始运行时，在操作系统的管理下，才在应用程序与相应的DLL之间建立链接关系。当要执行所调用DLL中的函数时，根据链接产生的重定位信息，操作系统才转去执行DLL中相应的函数代码。 可执行文件和动态库之间的区别：可执行文件中有main函数，动态库中没有main函数，可执行文件可以被程序执行，动态库需要依赖程序调用者。\n本节任务：\n建立一个静态库和动态库，提供 HelloFunc 函数供其他程序编程使用，HelloFunc 向终端输出 Hello World 字符串。 安装头文件与共享库。 1. 准备工作 在 /back/cmake 目录下建立 t3 目录，用于存放本节工程\n1 2 3 4 5 6 cd /backup/cmake/t3 touch CMakeLists.txt\t#建立工程文件 mkdir lib\t#建立库文件夹 cd lib touch hello.c hello.h\t#在lib目录下建立两个源文件 touch CMakeLists.txt 2. 建立共享库 编辑工程目录下的 CMakeLists.txt 文件内容为：\n1 2 PROJECT(HELLOLIB)\t#定义工程名 ADD_SUBDIRECTORY(lib)\t#指定 库 的文件夹 编辑 lib文件夹下的 hello.h 内容如下：( #if、#ifdef、#ifndef 区别 )\n1 2 3 4 5 #ifndef HELLO_H\t//如果当前的宏未被定义，HELLO_H是宏名 #define HELLO_H\t//定义宏 #include \u0026lt;stdio.h\u0026gt;\t//引入头文件 void HelloFunc();\t//声明函数 #endif 编辑 lib文件夹下的 hello.c 内容为：\n1 2 3 4 5 #include \u0026#34;hello.h\u0026#34;\t//\u0026#34;\u0026#34;括住表示: 预处理程序先到当前目录下寻找文件，再到预定义的缺省路径(通常由INCLUDE环境变量指定)下寻找文件 void HelloFunc() { printf(\u0026#34;Hello world\\n\u0026#34;); } 编辑 /lib/CMakeLists.txt :\n1 2 SET(LIBHELLO_SRC hello.c)\t#定义变量 ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})\t#将指定的源文件编译成库 采用外部编译，在工程目录下建立一个 build 目录，\n1 2 3 4 mkdir build cd build cmake .. make 这时，在 /build/lib 目录下得到一个 libhello.so，这就是我们期望的共享库。\n如果要指定 libhello.so 生成的位置，可以通过在主工程文件 CMakeLists.txt 中修改 ADD_SUBDIRECTORY(lib) 指令来指定一个编译输出位置或者在 lib/CMakeLists.txt 中添加：\n1 SET(LIBRARY_OUTPUT_PATH \u0026lt;路径\u0026gt;)\t#指定新的位置 ADD_LIBRARY 主要作用就是将指定的源文件生成链接文件，然后添加到工程中去。\n1 2 3 4 ADD_LIBRARY(libname #生成的库文件的名字 [SHARED|STATIC|MODULE]\t#库文件类型 [EXCLUDE_FROM_ALL]\t#指定这个库不会被默认构建,除非有其他的组件依赖或者手动构建 source1 source2 ... sourceN)\t#各个源文件 SHARED 库：会被动态链接（动态链接库），在运行时会被加载。 STATIC 库：是目标文件的归档文件，在链接其它目标的时候使用。 MODULE 库：是一种不会被链接到其它目标中的插件，但是可能会在运行时使用dlopen-系列的函数。 3. 添加静态库 同样使用上面的指令，我们在支持动态库的基础上再为工程添加一个静态库，按照一般的习 惯，静态库名字跟动态库名字应该是一致的，只不过后缀是.a 罢了。\n添加静态库的指令：\n1 ADD_LIBRARY(hello STATIC ${LIBHELLO_SRC}) 然后再在 build 目录进行外部编译，我们会发现，静态库根本没有被构建，仍然只生成了 一个动态库。因为 hello 作为一个 target 是不能重名的，所以，静态库构建指令无效。\n如果我们把上面的 hello 修改为 hello_static:\n1 ADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) 就可以构建一个 libhello_static.a 的静态库了。\n但是这种结果显示不是我们想要的,我们需要的是名字相同的静态库和动态库,因为 target 名 称是唯一的,所以,我们肯定不能通过 ADD_LIBRARY 指令来实现了。这时候我们需要用到 另外一个指令:\n1 2 3 4 SET_TARGET_PROPERTITES(target1 target2 ... PROPERTIES prop1 value1 prop2 value2 ...) 这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。\n在本例中,我们需要作的是向 lib/CMakeLists.txt 中添加一条:\n1 SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \u0026#34;hello\u0026#34;) 这样，我们就可以同时得到 libhello.so 和 libhello.a 两个库了。\n与SET_TARGET_PROPERTIES对应的指令是：\n1 GET_TARGET_PROPERTY(VAR target property)\t#得到属性值 具体用法如下：我们向 lib/CMakeLists.txt 中添加：\n1 2 GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME) MESSAGE(STATUS \u0026#34;This is the hello_static OUTPUT_NAME:\u0026#34;${OUTPUT_VALUE}) 如果没有这个属性定义，则返回 NOTFOUND。\n4. 动态库版本号 按照规则，动态库是应该包含一个版本号的，我们可以看一下系统的动态库，一般情况是：\n1 2 3 libhello.so.1.2 libhello.so -\u0026gt;libhello.so.1 libhello.so.1-\u0026gt;libhello.so.1.2 为了实现动态库版本号,我们仍然需要使用 SET_TARGET_PROPERTIES 指令。 具体使用方法如下：\n1 SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1) VERSION ：指代动态库版本， SOVERSION：指代API 版本 将上述指令加入 lib/CMakeLists.txt 中,重新构建看看结果。 在 build/lib 目录会生成: libhello.so.1.2 libhello.so.1-\u0026gt;libhello.so.1.2 libhello.so -\u0026gt;libhello.so.1\n5. 安装共享库和头文件 以上面的例子，我们需要将 libhello.a， libhello.so.x 以及 hello.h 安装到系统目录，才能真正让其他人开发使用，在本例中我们将 hello 的共享库安装到 \u0026lt;prefix\u0026gt;/lib 目录，将 hello.h 安装到**\u0026lt;prefix\u0026gt;/include/hello** 目录。\n利用上一节了解到的 INSTALL 指令，我们向 lib/CMakeLists.txt 中添加如下指令:\n1 2 3 4 5 INSTALL(TARGETS hello hello_static LIBRARY DESTINATION lib ARCHIVE DESTINATION lib)\t#注意,静态库要使用 ARCHIVE 关键字 INSTALL(FILES hello.h DESTINATION include/hello) 终端输入：\n1 2 3 cmake -DCMAKE_INSTALL_PREFIX=/usr .. make make install\t#Permission denied 我们就可以将头文件和共享库安装到系统目录/usr/lib 和/usr/include/hello 中了。\n完整代码： lib/CMakeLists.txt\n1 2 3 4 5 6 7 8 9 10 11 12 SET(LIBHELLO_SRC hello.c) ADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})\t#将源码编译成动态库 ADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC})\t#编译成静态库 SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \u0026#34;hello\u0026#34;) #重命名 GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME) #读取名字输出显示 MESSAGE(STATUS \u0026#34;This is the hello_static OUTPUT_NAME:\u0026#34;${OUTPUT_VALUE}) SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1) #版本号 INSTALL(TARGETS hello hello_static\t#安装动态库和静态库 LIBRARY DESTINATION lib ARCHIVE DESTINATION lib)\t#注意,静态库要使用 ARCHIVE 关键字 INSTALL(FILES hello.h\t#安装头文件 DESTINATION include/hello) 六、如何使用外部共享库和头文件 上一节我们已经完成了 libhello 动态库的构建以及安装，本节我们的任务很简单： 编写一个程序使用我们上一节构建的共享库。\n1. 准备工作 建立工程文件夹：在/backup/cmake 目录建立 t4 目录,本节所有资源将存储在 t4 目录。\n1 2 3 4 5 cd t4 touch CMakeLists.txt mkdir src cd src touch main.c CMakeList.txt 2. 编写源文件 建立 src 目录，编写源文件 main.c ：\n1 2 3 4 5 #include \u0026lt;hello.h\u0026gt; int main() { HelloFunc(); } 编辑工程目录下的 CMakeLists.txt：\n1 2 PROJECT(NEWHELLO) ADD_SUBDIRECTORY(src) 编辑 /src/CMakeLists.txt：\n1 ADD_EXECUTABLE(main main.c) 如果直接编译会出错：/backup/cmake/t4/src/main.c:1:19: error: hello.h: 没有那个文件或目录\n3. 引入头文件搜索路径 hello.h 位于 /usr/include/hello 目录中，并没有位于系统标准的头文件路径，为了让我们的工程能够找到 hello.h 头文件，我们需要引入一个新的指令：INCLUDE_DIRECTORIES，其完整的语法是：\n1 2 3 INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...) 这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的后面，你可以通过两种方式来进行控制搜索路径添加的方式:\nCMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以 将添加的头文件搜索路径放在已有路径的前面。 通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。 现在我们在 src/CMakeLists.txt 中添加一个头文件搜索路径，方式很简单，加入:\n1 INCLUDE_DIRECTORIES(/usr/include/hello) 进入 build 目录，重新进行构建，这是找不到 hello.h 的错误已经消失，但是出现了一个新的错误： main.c:(.text+0x12): undefined reference to HelloFunc\u0026rsquo;`\n这是因为我们并没有 link 到共享库 libhello 上。\n4. 为 target 链接共享库 我们现在需要完成的任务是将目标文件(target)链接到 libhello，这里我们需要引入两个新的指令**LINK_DIRECTORIES** 和 TARGET_LINK_LIBRARIES\n1 LINK_DIRECTORIES(directory1 directory2 ...) 这个指令非常简单，添加非标准的共享库搜索路径，比如，在工程内部同时存在共享库和可执行二进制,在编译时就需要指定一下这些共享库的路径。这个例子中我们没有用到这个指令。\n1 2 3 TARGET_LINK_LIBRARIES(target library1 \u0026lt;debug | optimized\u0026gt; library2 ...) 这个指令可以用来为 target 添加需要链接的共享库，本例中是一个可执行文件，但是同样可以用于为自己编写的共享库添加共享库链接。\n为了解决我们前面遇到的 HelloFunc 未定义错误,我们需要作的是向 src/CMakeLists.txt 中添加如下指令：\n1 2 3 TARGET_LINK_LIBRARIES(main hello)\t#hello 是共享库 libhello.so # 也可以写成 TARGET_LINK_LIBRARIES(main libhello.so) 进入 build 目录重新进行构建，就得到了一个链接到 libhello 的可执行程序 main，位于 build/src 目录，运行 main 的结果是输出：Hello world\n让我们来检查一下 main 的链接情况:\n1 ldd src/main\t#可以用來尋找此執行檔鏈接了哪一些函式庫 1 2 3 4 linux-gate.so.1 =\u0026gt; (0xb7ee7000) libhello.so.1 =\u0026gt; /usr/lib/libhello.so.1 (0xb7ece000) libc.so.6 =\u0026gt; /lib/libc.so.6 (0xb7d77000) /lib/ld-linux.so.2 (0xb7ee8000) 可以清楚的看到 main 确实链接了共享库 libhello，而且链接的是动态库 libhello.so.1\n那如何链接到静态库呢? 方法很简单: 将 TARGET_LINK_LIBRRARIES 指令修改为:\n1 TARGET_LINK_LIBRARIES(main libhello.a) 重新构建后再来看一下 main 的链接情况：\n1 ldd src/main 1 2 3 linux-gate.so.1 =\u0026gt; (0xb7fa8000) libc.so.6 =\u0026gt; /lib/libc.so.6 (0xb7e3a000) /lib/ld-linux.so.2 (0xb7fa9000) 说明,main 确实链接到了静态库 libhello.a\n特殊的环境变量: CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 务必注意,这两个是环境变量而不是 cmake 变量。 使用方法是要在 bash 中用 export 或者在 csh 中使用 set 命令设置或者 CMAKE_INCLUDE_PATH=/home/include cmake ..等方式。\n这两个变量主要是用来解决以前 autotools 工程中 --extra-include-dir 等参数的支持的。也就是,如果头文件没有存放在常规路径 (/usr/include, /usr/local/include 等)，则可以通过这些变量就行弥补。\n我们以本例中的 hello.h 为例，它存放在**/usr/include/hello** 目录,所以直接查找肯定是找不到的。前面我们直接使用了**绝对路径****INCLUDE_DIRECTORIES(/usr/include/hello)**告诉工程这个头文件目录。\n为了将程序更智能一点,我们可以使用 **CMAKE_INCLUDE_PATH**来进行，使用 bash 的方法如下：\n1 export CMAKE_INCLUDE_PATH=/usr/include/hello 然后在头文件中将 **INCLUDE_DIRECTORIES(/usr/include/hello)**替换为：\n1 2 3 4 FIND_PATH(myHeader hello.h) IF(myHeader) INCLUDE_DIRECTORIES(${myHeader}) ENDIF(myHeader) 上述的一些指令我们在后面会介绍。这里简单说明一下，FIND_PATH 用来在指定路径中搜索文件名，比如: FIND_PATH(myHeader NAMES hello.h PATHS /usr/include /usr/include/hello)\n这里我们没有指定路径，但是，cmake 仍然可以帮我们找到 hello.h 存放的路径，就是因为我们设置了环境变量 CMAKE_INCLUDE_PATH。\n如果你不使用 FIND_PATH，**CMAKE_INCLUDE_PATH**变量的设置是没有作用的，你不能指望它会直接为编译器命令添加参数 -I\u0026lt;CMAKE_INCLUDE_PATH\u0026gt;。\n以此为例，CMAKE_LIBRARY_PATH 可以用在 FIND_LIBRARY 中。\n同样,因为这些变量直接为 FIND_指令所使用,所以所有使用 FIND_指令的 cmake 模块都会受益。\n","date":"2021-01-06T00:00:00Z","image":"https://picx.zhimg.com/v2-42db546bd0f2ffdd462636ddd87ecdc3_1440w.jpg?source=172ae18b","permalink":"https://zichen34.github.io/writenotes/lang/cmake_practice/","title":"memo: CMake实践"},{"content":"Frequency folding\nThe sampled signal\u0026rsquo;s frequency spectrum is mirrored by the sample frequency. 1-Steve\nRef Shannon Nyquist Sampling Theorem ","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aliasing/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/img/","title":""},{"content":"推荐几个不错的CUDA入门教程（非广告） - godweiyang的文章 - 知乎\nCUDA Python\u0026ndash;系列课程（主讲：何琨）_哔哩哔哩_bilibili\n灰常专注 (2023-06-28 00:09): cupy，numba 和纯C+Cuda的，就一个普通矩阵乘法运行效率差别大吗\n扫地的小何尚 (2023-06-28 00:27): 大，通用的库的底线很高，但是上线不如自己写高，毕竟自己定制的应该是最快的 Source video: CUDA编程模型系列一(核心函数) ","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/cuda/rsc/","title":""},{"content":"生成式人工智能研究焦点：揭秘基于扩散的模型 - CSDN - 扫地的小和尚\n","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/c-post-rsc/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/shapes/img/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/splat/img/","title":"image"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ala-nathan/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/aml/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%8E%8B%E6%9C%A8%E5%A4%B4/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/ml-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/%E4%BF%97%E8%AF%B4%E7%9F%A9%E9%98%B5-%E9%AB%98%E5%B1%B1%E8%80%81%E5%B8%88/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lib/pytorch-%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/diffusion/imgs/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/imagen/vae/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/mvs/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/nerfs/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/subnetwork/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/model/transformers/transf-nickchen/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/games-101_cg/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/vis/img/","title":"images"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/lang/python/img/","title":"img"},{"content":"Source Video: Reflection laws proof using Huygen\u0026rsquo;s principle | Wave optics | Physics | Khan Academy\n","date":"0001-01-01T00:00:00Z","permalink":"https://zichen34.github.io/writenotes/calc/waveoptics/khan_waveoptics/","title":"watch: WaveOptics - Khan Academy India"}]