<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>sympo on Zichen Wang</title>
        <link>https://zichen34.github.io/categories/sympo/</link>
        <description>Recent content in sympo on Zichen Wang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 06 Mar 2024 17:25:00 +0000</lastBuildDate><atom:link href="https://zichen34.github.io/categories/sympo/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>sympo: Point Cloud Processing</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/c-symp-pc_pro/</link>
        <pubDate>Wed, 06 Mar 2024 17:25:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/c-symp-pc_pro/</guid>
        <description>&lt;h2 id=&#34;neural-depth&#34;&gt;Neural Depth&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GPU-Accelerated 3D Point Cloud Processing with Hierarchical GMM&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/somanshu25/GPU-Accelerated-Point-Cloud-Registration-Using-Hierarchical-GMM&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Video: &lt;a class=&#34;link&#34; href=&#34;https://developer.nvidia.com/gtc/2019/video/s9623&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GTC Silicon Valley-2019: GPU-Accelerated 3D Point Cloud Processing with Hierarchical Gaussian Mixtures&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Found by DDG (image) with searching the paper: &amp;ldquo;GPU-Accelerated LOD Generation for Point Clouds&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visualization&#34;&gt;Visualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GPU-Accelerated LOD Generation for Point Clouds&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/m-schuetz/CudaLOD&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://snosixtyboo.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Bernhard Kerbl (3DGS)&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://papers.cool/arxiv/2302.14801&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CoolPapers&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CudaLOD&lt;/li&gt;
&lt;li&gt;Multi-level details for point cloud&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;refinement&#34;&gt;Refinement&lt;/h2&gt;
&lt;p&gt;(2024-03-15)&lt;/p&gt;
&lt;p&gt;Journey when searching: &amp;ldquo;point cloud refinement research&amp;rdquo; &lt;a class=&#34;link&#34; href=&#34;https://duckduckgo.com/?q=point&amp;#43;cloud&amp;#43;refinement&amp;#43;research&amp;amp;ia=web&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DDG&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Real-Time Point Cloud Refinement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.researchgate.net/publication/237009007_Real-Time_Point_Cloud_Refinement&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RSGate&lt;/a&gt;
(2024-03-16)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upsampiling point cloud by inserting new point set&lt;/li&gt;
&lt;li&gt;Splatting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A voxel-based multiview point cloud refinement method via factor graph optimization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/WuHao-WHU/EVPA?tab=readme-ov-file&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;No Code&lt;/a&gt;
| Paper not accessible&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s compared with ICP, ICP-Normal, GICP, LUM, BALM&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;Search the title of above paper 1 &lt;a class=&#34;link&#34; href=&#34;https://duckduckgo.com/?q=Real-Time&amp;#43;Point&amp;#43;Cloud&amp;#43;Refinement&amp;amp;ia=web&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DDG&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PR-GCN: A Deep Graph Convolutional Network With Point Refinement for 6D Pose Estimation&lt;/strong&gt; (ICCV 2021)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.semanticscholar.org/paper/PR-GCN%3A-A-Deep-Graph-Convolutional-Network-with-for-Zhou-Wang/a442e264eaa8be82b74b902f10fb83c73cd1a482&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Semantic&lt;/a&gt;
&lt;figure&gt;&lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2108.09916&#34;&gt;&lt;img src=&#34;https://ar5iv.labs.arxiv.org/html/2108.09916/assets/x2.png&#34;/&gt;&lt;/a&gt;
   &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;(2024-03-16)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KNN and Graph Convolution Network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;edge-aware&#34;&gt;Edge-aware&lt;/h3&gt;
&lt;p&gt;The paper-1 is cited by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Ec-net: an edge-aware point set consolidation network&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/yulequan/EC-Net&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>sympo: Point Cloud from Depth Maps</title>
        <link>https://zichen34.github.io/writenotes/model/shapes/c-symp-dmapest/</link>
        <pubDate>Wed, 21 Feb 2024 12:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/shapes/c-symp-dmapest/</guid>
        <description>&lt;hr&gt;
&lt;h2 id=&#34;depth-completion&#34;&gt;Depth Completion&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://paperswithcode.com/task/depth-completion&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Depth Completion - Paper with code&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Guided Convolutional Network for Depth Completion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1908.01238&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://github.com/kakaxi314/GuideNet&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://papers.cool/arxiv/1908.01238&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CoolPapers&lt;/a&gt;
| Jie Tang, Ping Tan&lt;/p&gt;
&lt;p&gt;(2024-02-21)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic convolution kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;depth-super-resolution&#34;&gt;Depth Super-Resolution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/zhyever/PatchFusion&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://zhyever.github.io/patchfusion/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ProjPage&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://zhyever.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Zhenyu Li&lt;/a&gt;, Peter Wonka&lt;/p&gt;
&lt;p&gt;(2024-02-23)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Joint Implicit Image Function for Guided Depth Super-Resolution&lt;/strong&gt;
~ ACM MM 2021&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ashawkey/jiif&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2107.08717&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://me.kiui.moe/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jiaxiang Tang&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(2024-02-22)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>sympo: MVS | Depth Maps Fusion</title>
        <link>https://zichen34.github.io/writenotes/model/mvs/c-symp-dmapfuse/</link>
        <pubDate>Tue, 20 Feb 2024 23:54:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/mvs/c-symp-dmapfuse/</guid>
        <description>&lt;p&gt;Conduct 3D reconstruction through depth map fusion.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Solution categories&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Explicitly optimization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Depp learning-based methods&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Journey when searching &amp;ldquo;Depthmap fusion with depth and normal consistency check&amp;rdquo;
&lt;a class=&#34;link&#34; href=&#34;https://duckduckgo.com/?q=Depthmap&amp;#43;fusion&amp;#43;with&amp;#43;depth&amp;#43;and&amp;#43;normal&amp;#43;consistency&amp;#43;check&amp;amp;ia=web&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DDG&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Depthmap fusion with depth and normal consistency check&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Massively Parallel Multiview Stereopsis by Surface Normal Diffusion (ICCV 2015)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://prs.igp.ethz.ch/content/dam/ethz/special-interest/baug/igp/photogrammetry-remote-sensing-dam/documents/pdf/galliani-lasinger-iccv15.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Paper&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.ca/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Massively&amp;#43;Parallel&amp;#43;Multiview&amp;#43;Stereopsis&amp;#43;by&amp;#43;Surface&amp;#43;Normal&amp;#43;Diffusion&amp;#43;&amp;amp;btnG=&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GScholar&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code for Gipuma &lt;a class=&#34;link&#34; href=&#34;https://github.com/kysucix/gipuma&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;kysucix/gipuma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code for Section 4.3 Fusion: &lt;a class=&#34;link&#34; href=&#34;https://github.com/kysucix/fusibile&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;kysucix/fusibile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(2024-03-16)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;4.3 Fusion&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Obtain depth map for each view&lt;/li&gt;
&lt;li&gt;Iterate each depth map to be treated as the reference image and perform reprojection.&lt;/li&gt;
&lt;li&gt;Check if the disparity between the reference and source depth maps is lower than a threshold,
so as to determine valid 3D points.&lt;/li&gt;
&lt;li&gt;Check if the estimated surface normal direction consistent among all views.&lt;/li&gt;
&lt;li&gt;Check the if reprojected depth on other view is consistent with the reference view.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accuracy and completeness is a trade-off.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:9067666&#34;&gt;&lt;img src=&#34;https://d3i71xaburhd42.cloudfront.net/cfca076f6a686ebdb942f39fc8110196960af15e/250px/7-Table1-1.png&#34;/&gt;&lt;/a&gt;
    &lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RoutedFusion: Learning Real-time Depth Map Fusion&lt;/strong&gt; ~ CVPR 2020&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2001.04388&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(2024-03-15)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D depth routing network + 3D depth fusion network&lt;/li&gt;
&lt;li&gt;Less training data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Review of Depth and Normal Fusion Algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5855899/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;pmc&lt;/a&gt;
| Sensors MDPI&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Depth (and Normal) Map Fusion Algorithm - GitHub&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/morsingher/depth_fusion&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-view stereo via depth map fusion: A coordinate decent optimization method&lt;/strong&gt; (CoD-Fusion)&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/pii/S0925231215016070&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neurocomputing 2015&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://zhaoxinli.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Zhaoxin Li&lt;/a&gt;, HIT&lt;/p&gt;
&lt;img src=&#34;https://ars.els-cdn.com/content/image/1-s2.0-S0925231215016070-gr1_lrg.jpg&#34;&gt;
&lt;p&gt;(2024-03-15)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Goal of depth map fusion is recovering the surface.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use coordinate descent to decompose the energy functional (泛函) for each voxel.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3D total variance TV is the loss function. So the Objective function = TV + regularizer L.
Naming refers to &lt;a class=&#34;link&#34; href=&#34;https://people.csail.mit.edu/romer/papers/SchFunRos_ECML07.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Fast Optimization Methods for L1 Regularization - ECML07&lt;/a&gt;
and &lt;a class=&#34;link&#34; href=&#34;https://openlearninglibrary.mit.edu/assets/courseware/v1/920d2b98ba33e5f1ef544c1e54d9a69d/asset-v1:MITx&amp;#43;6.036&amp;#43;1T2019&amp;#43;type@asset&amp;#43;block/notes_chapter_Logistic_regression.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MIT courseware&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scene Representation: TSDF for surface reconstruction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Depth maps are generated based on binocular stereo matching using library &lt;code&gt;libelas&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leverage TSDFs to fuse depth maps&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Eq (1) is an expection of all-view TSDFs on a voxel.
It fails for nosiy depth maps due to lack of spatial regularizer.&lt;/p&gt;
&lt;p&gt;$$ min \ \int_B ∑_{i∈ Φ(𝐱)} w_i (u - f_i)² d𝐱
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integral over &lt;strong&gt;all voxels&lt;/strong&gt; in the bounding box $B$&lt;/li&gt;
&lt;li&gt;$fᵢ$ is a &lt;em&gt;TSDF set&lt;/em&gt; generated from the i-th depth map.&lt;/li&gt;
&lt;li&gt;Each TSDF corresponds a weight $wᵢ$  representing its reliability.&lt;/li&gt;
&lt;li&gt;$Φ$ is the &lt;strong&gt;filtered&lt;/strong&gt; TSDF set for a voxel $𝐱=B(i,j,k)$.
And the original set is $F$ that contains TSDFs on 𝐱 from all depth maps.&lt;/li&gt;
&lt;li&gt;u is the resultant TSDF. It will finally be the &lt;strong&gt;weighted mean&lt;/strong&gt; of depth values on all dpeth maps.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
