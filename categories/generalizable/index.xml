<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Generalizable on Zichen Wang</title>
        <link>https://zichen34.github.io/categories/generalizable/</link>
        <description>Recent content in Generalizable on Zichen Wang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 14 Jul 2023 14:59:00 +0000</lastBuildDate><atom:link href="https://zichen34.github.io/categories/generalizable/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>read: Match-NeRF</title>
        <link>https://zichen34.github.io/writenotes/model/nerfs/b-note-matchnerf/</link>
        <pubDate>Fri, 14 Jul 2023 14:59:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nerfs/b-note-matchnerf/</guid>
        <description>&lt;img src="https://i.imgur.com/rQN8Eg5.png" alt="Featured image of post read: Match-NeRF" /&gt;&lt;p&gt;&lt;em&gt;Explicit Correspondence Matching for Generalizable Neural Radiance Fields&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/donydchen/matchnerf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt; |
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2304.12294&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;
&lt;h3 id=&#34;idea&#34;&gt;Idea&lt;/h3&gt;
&lt;p&gt;Take the difference between reference image features of a 3D point as &lt;strong&gt;geometry prior&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(2023-10-25) Method is like a simplified GPNR: feature differences between views measued by cosine similarity (dot product).
However, GPNR is more ultimate, while MatchNeRF only computes differences for each two views and differences are taken averaged as the input feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(2023-12-09) Image features &lt;strong&gt;alone&lt;/strong&gt; cannot constrain geometry of the &lt;strong&gt;unseen scenes&lt;/strong&gt; well.
In contrast, MVSNet &lt;strong&gt;preset depths&lt;/strong&gt; explicitly.&lt;/p&gt;
&lt;p&gt;And MVSNet used the &lt;strong&gt;variance of features&lt;/strong&gt; to measure the depth misalignment &lt;strong&gt;among all&lt;/strong&gt; feature maps,
whereas MatchNeRF used differences of &lt;strong&gt;each 2&lt;/strong&gt; feature maps.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;CNN extract image feature (1/8 reso), which will be upsampled to (1/4 reso), for each reference view.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select &lt;strong&gt;a pair&lt;/strong&gt; of reference views and Mix their feature maps by cross-attention (&lt;a class=&#34;link&#34; href=&#34;https://github.com/haofeixu/gmflow&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GMFlow&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Project a 3D point onto this pair of interacted feature maps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Measuring the difference of &lt;strong&gt;feature&lt;/strong&gt; vectors by cosine similarity (dot prodcut)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The feature difference indicates whether the 3D point is at surface, so that it provides a geometry prior.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dot product of two feature vectors is a scalar, which will lost much information.
So they divided the total channel into many groups to do &amp;ldquo;group dot product&amp;rdquo;.
And concatenate the dot product of each group as a vector ùê≥.&lt;/p&gt;
&lt;p&gt;Also, for the 1/4 feature map, there is a &amp;ldquo;dot-products&amp;rdquo; vector $\^ùê≥$ for a pair of reference views.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given ùëÅ reference views, there are ùëÅ(ùëÅ-1)/2 pairs of reference views, corresponding to ùëÅ(ùëÅ-1)/2 &amp;ldquo;difference&amp;rdquo; vectors,
which will merge together by taking their element-wise average as a single ùê≥&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This &amp;ldquo;feature difference&amp;rdquo; vector ùê≥ (geometry prior) is fed along with the 3D point&amp;rsquo;s position and viewdir into decoder (MLP and ray-transformer),
which regresses the color and volume density.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments&lt;/h3&gt;
&lt;p&gt;Settings are following &lt;a class=&#34;link&#34; href=&#34;https://github.com/apchenstu/mvsnerf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MVSNeRF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Datasets:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Stage&lt;/th&gt;
&lt;th&gt;Data&lt;/th&gt;
&lt;th&gt;Contents&lt;/th&gt;
&lt;th&gt;Resolution&lt;/th&gt;
&lt;th&gt;N_views&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Train&lt;/td&gt;
&lt;td&gt;DTU&lt;/td&gt;
&lt;td&gt;88 scenes&lt;/td&gt;
&lt;td&gt;512x640&lt;/td&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;DTU&lt;/td&gt;
&lt;td&gt;16 scenes&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;NeRF real&lt;/td&gt;
&lt;td&gt;8 scenes&lt;/td&gt;
&lt;td&gt;640x960&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;Blender&lt;/td&gt;
&lt;td&gt;8 scenes&lt;/td&gt;
&lt;td&gt;800x800&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Device: 16G-V100&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;play&#34;&gt;Play&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;(2023-08-24)&lt;/p&gt;
&lt;h3 id=&#34;compare-with-gnt&#34;&gt;Compare with GNT&lt;/h3&gt;
&lt;p&gt;The architectures of Match-NeRF and GNT are similar.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(2023-12-09) Overview: Souce images&amp;rsquo; features are extracted, mixed and regressed to rgbœÉ.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Match-NeRF is trained &lt;strong&gt;only on DTU&lt;/strong&gt; dataset, while GNT can be trained on &lt;strong&gt;multiple datasets&lt;/strong&gt; (&lt;code&gt;gnt_full&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GNT merges multiple source views via subtract attention,
while Match-NeRF fuses multi-view feature maps before getting into the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Match-NeRF mixes the &lt;strong&gt;entire feature maps&lt;/strong&gt; for &lt;strong&gt;each two&lt;/strong&gt; reference views,
and then project 3D points onto the fused feature maps to index feature vectors.&lt;/p&gt;
&lt;p&gt;However, GNT directly mixes &lt;strong&gt;point&amp;rsquo;s feature vectors&lt;/strong&gt; coming from each feature maps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Different training settings:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Hyper-params&lt;/th&gt;
&lt;th&gt;GNT&lt;/th&gt;
&lt;th&gt;MatchNeRF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;#rays for grad-descnt&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;#source views&lt;/td&gt;
&lt;td&gt;8~10&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1080Ti only supports &lt;code&gt;--nerf.rand_rays_train=512&lt;/code&gt; for MatchNeRF.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;opts.batch_size&lt;/code&gt; will be divided evenly to each gpu (&lt;code&gt;self.opts.batch_size // len(self.opts.gpu_ids)&lt;/code&gt;),
so bs (num of images)=1 &lt;strong&gt;cannot be split&lt;/strong&gt; to multiple GPUs.&lt;/p&gt;
&lt;p&gt;And if setting bs=2, each card still have to process 1024 rays selected from an image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Testing with 1 1080Ti:&lt;code&gt;python test.py --yaml=test --name=matchnerf_3v --nerf.rand_rays_test=10240&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;(2023-09-27)&lt;/p&gt;
&lt;h3 id=&#34;code-details&#34;&gt;Code Details&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GMFlow uses 6 transformer blocks consisting of &lt;code&gt;self_attn&lt;/code&gt; and
&lt;code&gt;cross_attn&lt;/code&gt; for fusing &lt;strong&gt;windows&lt;/strong&gt;,
where the 1st and odd blocks perform window shift.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MatchNeRF &lt;strong&gt;fully-finetuned&lt;/strong&gt; the pre-trained GMFlow.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Does Inner product of a pair of features come from GMFlow?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(2023-10-25) I guess it&amp;rsquo;s a &lt;strong&gt;simplified attention&lt;/strong&gt; only for pairs, instead of among all views.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(2023-12-09) &lt;strong&gt;Inferring geometry&lt;/strong&gt; from the difference in high-dimensional features may have been present even earlier than MVSNet.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Self-attn and cross-attn for two samples &lt;code&gt;data1&lt;/code&gt; and &lt;code&gt;data2&lt;/code&gt; can be done in a single transformer block of GMFlow by
concating 2 samples in the &lt;strong&gt;batch&lt;/strong&gt; dimension &lt;strong&gt;twice&lt;/strong&gt; in different order, i.e.,
&lt;code&gt;source&lt;/code&gt;=[&amp;lsquo;data1&amp;rsquo;,&amp;lsquo;data2&amp;rsquo;] and &lt;code&gt;target&lt;/code&gt;=[&amp;lsquo;data2&amp;rsquo;,&amp;lsquo;data1&amp;rsquo;].&lt;/p&gt;
&lt;p&gt;Such that self-attn is performed on &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;source&lt;/code&gt;.
And cross-attn is &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;.
If fused &lt;code&gt;source&lt;/code&gt; returned after a block, the order requires reverse again to form the new &lt;code&gt;target&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;viewdir didn&amp;rsquo;t perform positional embedding (same as PixelNeRF).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ray transformer (MHA) in decoder mixes 16-dim feature vectors. (Unexpectedly tiny)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Take the nearest views.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Coordinates of 3D points are projected onto the image plane of the source view 0 to do positional embedding.
&lt;a class=&#34;link&#34; href=&#34;https://github.com/donydchen/matchnerf/blob/08c0943454c57c84f60083722fd1e6594528bc0e/models/matchnerf.py#L120-L126&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The encoder is supposed to provide the &lt;strong&gt;overall&lt;/strong&gt; (geometry) prior, so they emphasized in the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;do not tune encoder for per-scene fine tuning&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>read: GRF</title>
        <link>https://zichen34.github.io/writenotes/model/nerfs/b-note-grf/</link>
        <pubDate>Thu, 15 Dec 2022 13:49:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nerfs/b-note-grf/</guid>
        <description>&lt;img src="https://github.com/alextrevithick/GRF/raw/main/imgs/process.png" alt="Featured image of post read: GRF" /&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/alextrevithick/GRF&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.04595&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv (2010)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Main steps (pseudo-code) of GRF&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;‚îú&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;‚îú&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;create_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;render_kwargs_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;network_query_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cycle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;‚îî&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pick&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N_rand&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ro&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rgb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;H&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;render&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intrinsic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;unet_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render_kwargs_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ro&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;viewdirs&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# (args.N_rand,8)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_feats&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;unet_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Eembedd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attention_imgs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cam_center&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rotation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# (NS,H,W,33+128)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batchify_rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intrinsic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_feats&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;render_rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;along&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rays&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;N_samples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attn_cache&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;network_query_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;viewdirs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;network_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intrinsic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;training&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_feats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;run_network&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;viewdirs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;network_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embed_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;     &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;emb_coord&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PosEnc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xyz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dirs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;90&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;     &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batchify&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nerf_attention_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;netchunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ndc2world&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gather_indices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;        &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ret&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;emb_coord&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intrinsic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_feats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;           &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;netchunk&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;           &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NS_feats&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gather_indices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;world_xyz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attention_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intrinsic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_feats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# PosEnc(rgb)+img_feats+pixloc=163&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;           &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nerf_attention_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;netchunk_emb_coord&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NS_feats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;netchunk_pts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;              &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pts_feats&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;slot_att&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NS_feats&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PosEnc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xyz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#get (n_pts,256)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;              &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nerf_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pts_feats&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;netchunk_emb_coord&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;                 &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pts_feats&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#(n_pts,4),(n_pts,256+90)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rgb_map_0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;disp_map_0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;acc_map_0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw2outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z_vals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rays_d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw_fine&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;network_query_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;viewdirs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;network_fine&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rgb_map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;disp_map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;acc_map&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw2outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_fine&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z_vals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rays_d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;all_ret&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îÇ&lt;/span&gt;  &lt;span class=&#34;err&#34;&gt;‚îî‚îÄ&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rgb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;disp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;acc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extracs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img2mse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rgb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;err&#34;&gt;‚îú‚îÄ&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img2psnr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>read: MVSNeRF</title>
        <link>https://zichen34.github.io/writenotes/model/nerfs/b-note-mvsnerf/</link>
        <pubDate>Tue, 06 Dec 2022 14:19:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nerfs/b-note-mvsnerf/</guid>
        <description>&lt;img src="https://ar5iv.labs.arxiv.org/html/2103.15595/assets/x2.png" alt="Featured image of post read: MVSNeRF" /&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/apchenstu/mvsnerf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.15595&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv (2103)&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://apchenstu.github.io/mvsnerf/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ProjPage&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;(2023-08-16) Re-read&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;Train model only on (&lt;a class=&#34;link&#34; href=&#34;https://github.com/YoYo000/MVSNet&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MVSNet&lt;/a&gt;) DTU dataset,
where the objects are partitioned the same as &lt;a class=&#34;link&#34; href=&#34;https://github.com/sxyu/pixel-nerf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PixelNeRF&lt;/a&gt; (imgsize: 300x400).&lt;/p&gt;
&lt;p&gt;Datasets (&lt;a class=&#34;link&#34; href=&#34;https://github.com/apchenstu/mvsnerf#dtu-dataset&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Download&lt;/a&gt;):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Stage&lt;/th&gt;
&lt;th&gt;Data&lt;/th&gt;
&lt;th&gt;Contents&lt;/th&gt;
&lt;th&gt;Resolution&lt;/th&gt;
&lt;th&gt;N_views&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Train&lt;/td&gt;
&lt;td&gt;DTU&lt;/td&gt;
&lt;td&gt;88 scenes&lt;/td&gt;
&lt;td&gt;512x640&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;19G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;DTU&lt;/td&gt;
&lt;td&gt;16 scenes&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;3/20&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;LLFF&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;Blender&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Feature map: 32 channels&lt;/li&gt;
&lt;li&gt;Cost volume: 128 planes&lt;/li&gt;
&lt;li&gt;MLP: 6 layers&lt;/li&gt;
&lt;li&gt;Coarse-to-fine: One field and fine-tune&lt;/li&gt;
&lt;li&gt;Ray pts: 128&lt;/li&gt;
&lt;li&gt;Device: 2080Ti&lt;/li&gt;
&lt;li&gt;Batch size: 1024 rays&lt;/li&gt;
&lt;li&gt;Optimizer: Adam, lr=5e-4&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;old-notes&#34;&gt;Old Notes&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;Old notes on 2022-12-06&lt;/summary&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 nearby input views&lt;/li&gt;
&lt;li&gt;plane-swept cost volumes&lt;/li&gt;
&lt;li&gt;geometry-aware scene reasoning&lt;/li&gt;
&lt;li&gt;generalize across scenes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;1 Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Topic: Novel view synthesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recent progress: neural rendering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Former solutino and drawbacks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NeRF and its following works can produce photo-realistic novel view synthesis results. But they need a long-time per-scene optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Own Solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Goal: use &amp;hellip; to &amp;hellip;&lt;/li&gt;
&lt;li&gt;1 sentences introduce the name and functionality, properties.&lt;/li&gt;
&lt;li&gt;Analysis:
&lt;ol&gt;
&lt;li&gt;generalizability -&amp;gt; avoid tedious per-scene optimization and regress directly novel viewpoints&lt;/li&gt;
&lt;li&gt;quantitative outcomes&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tech stack&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MVSNet -&amp;gt; generalizable net of 3D reconstruction
&lt;ul&gt;
&lt;li&gt;Cost volume is built by warping 2D img features of src views onto sweeping planes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Regress geometry and appearance from a cost volume (per-voxel neural features)
&lt;ul&gt;
&lt;li&gt;3D cnn aggregates the cost volumes to a neural scene encoding volume&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;p&gt;(2023-08-16)&lt;/p&gt;
&lt;h2 id=&#34;play&#34;&gt;Play&lt;/h2&gt;
&lt;h3 id=&#34;environment&#34;&gt;Environment&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;inplace-abn&lt;/code&gt; needs to be installed from source, as &lt;a class=&#34;link&#34; href=&#34;https://github.com/apchenstu/mvsnerf/issues/36#issuecomment-998701389&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;issue#36&lt;/a&gt;.
But I cannot install it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;RuntimeError: 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;The detected CUDA version &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;10.2&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; mismatches the version that was used to compile
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;PyTorch &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;11.3&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;. Please make sure to use the same CUDA versions.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;My cuda version is 11.3 as shown by:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;version&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then I specified the version the same as the &lt;a class=&#34;link&#34; href=&#34;https://github.com/kwea123/pytorch-lightning-tutorial/blob/master/requirements.txt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;pl-tutorial of AIkui&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;- &lt;span class=&#34;l&#34;&gt;pip&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;pip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;torch==1.11.0&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;torchvision==0.12.0&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;pytorch_lightning==1.6.0&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;inplace_abn&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;With that, &lt;code&gt;inplace_abn&lt;/code&gt; has installed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But&lt;/strong&gt; the API has change since v1.5 (as &lt;a class=&#34;link&#34; href=&#34;https://github.com/Lightning-AI/lightning/issues/12070&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;issue&lt;/a&gt;) and resulted in:
&lt;code&gt;Exception has occurred: TypeError __init__() got an unexpected keyword argument &#39;distributed_backend&#39;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try this setting:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;torch==1.10.1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;torchvision==0.11.2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;pytorch_lightning==1.3.5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;inplace_abn&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Cannot import &lt;code&gt;inplace_abn&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Exception has occurred: ImportError
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/inplace_abn/_backend.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c1015SmallVectorBaseIjE8grow_podEPvmm
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I checked my cuda version, which is 10.2:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;mvsnerf&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; zichen@lambda-server:~/Downloads/mvsnerf-comments$ nvcc --version
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nvcc: NVIDIA &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;R&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; Cuda compiler driver
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Copyright &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;c&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; 2005-2019 NVIDIA Corporation
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Built on Wed_Oct_23_19:24:38_PDT_2019
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Cuda compilation tools, release 10.2, V10.2.89
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;According to &lt;a class=&#34;link&#34; href=&#34;https://pytorch.org/get-started/previous-versions/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Previous PyTorch Versions | PyTorch&lt;/a&gt;,
I install the versions compiled with cuda10.2.
Then &lt;code&gt;print(torch.version.cuda)&lt;/code&gt; returned 10.2 instead.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;- &lt;span class=&#34;l&#34;&gt;pip&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;pip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;torch==1.10.1+cu102 &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;torchvision==0.11.2+cu102 &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;torchaudio==0.10.1 &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- -&lt;span class=&#34;l&#34;&gt;f https://download.pytorch.org/whl/cu102/torch_stable.html&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;pytorch_lightning==1.3.5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;inplace_abn&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Still cannot import inplace_abn with the same ImportError as above:&lt;/p&gt;
&lt;p&gt;Then Install from source, and inplace_abn can be installed:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another Error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Exception has occurred: ImportError
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cannot import name &lt;span class=&#34;s1&#34;&gt;&amp;#39;get_num_classes&amp;#39;&lt;/span&gt; from &lt;span class=&#34;s1&#34;&gt;&amp;#39;torchmetrics.utilities.data&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;/home/zichen/anaconda3/envs/mvsnerf/lib/python3.8/site-packages/torchmetrics/utilities/data.py&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  File &lt;span class=&#34;s2&#34;&gt;&amp;#34;/home/zichen/Downloads/mvsnerf-comments/train_mvs_nerf_pl.py&amp;#34;&lt;/span&gt;, line 17, in &amp;lt;module&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    from pytorch_lightning.callbacks import ModelCheckpoint
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Change to specific versions, as &lt;a class=&#34;link&#34; href=&#34;https://github.com/apchenstu/mvsnerf/issues/36#issuecomment-1638311240&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;summary&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;torchmetrics&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;0.5.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;setuptools&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;59.5.0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Numpy version incompatible &lt;a class=&#34;link&#34; href=&#34;https://github.com/apchenstu/mvsnerf/issues/77&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;issue#77&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; dimensions. The detected shape was &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;2,&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; + inhomogeneous part.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;File &amp;ldquo;/home/zichen/Downloads/mvsnerf-comments/data/dtu.py&amp;rdquo;, line 98, in build_proj_mats
self.proj_mats, self.intrinsics = np.stack(proj_mats), np.stack(intrinsics)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>read: GNT</title>
        <link>https://zichen34.github.io/writenotes/model/nerfs/b-note-gnt/</link>
        <pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nerfs/b-note-gnt/</guid>
        <description>&lt;img src="https://vita-group.github.io/GNT/assets/overview.png" alt="Featured image of post read: GNT" /&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://vita-group.github.io/GNT/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2207.13298&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;‰∏ÄÂè•‰∏≠ÊñáÔºöÊûÑÂª∫ point-alinged feature field, ÁÑ∂ÂêéÁî® attention ÊääÁâπÂæÅËûçÂêàÊàêÂõæÂÉèÔºåÁâπÂæÅÁöÑattn score ÂèçÊò†‰∫ÜÊ∑±Â∫¶&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;




  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/nerfs/img/GNT.jpg width=&gt;
  
  

&lt;/div&gt;
&lt;p&gt;Insights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;NeRF is in a backward optimization fashion. The color is mapped to points along with the optimization. So the radience field is recovered backward.
While Generalizable NeRFs assign feature onto points in the feed-forward process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Projecting the points onto feature maps exerts the inductive bias of epipolar constraints for injecting geometry prior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s inferior to NLF&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Samples on a single ray cannot recover refraction and scattering, in which the ray will bend. So GNT managed this by its view transformer?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Occlusion-aware is realized by giveing the most visible reference view the most weight. Depth-aware is endowed by the importance of each point to the pixel color. &amp;ldquo;importance=density&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GNT doesn&amp;rsquo;t care the quality of 3D geometry reconstruction.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Only net_coarse is used and trained with 4096 rays, 192 N_samples in one iteration. They didn&amp;rsquo;t split the N_rand into chunks in the train.py, but they did when rendering a full imagein evaluation period. So I need to distribute the 4 blocks onto 4 crads for accomodating the 4096 rays in one batch, if I want to reproduce their expriment.
In my previous training, the number of points fed into MLP is N_rand x N_samples = 2048x64 = 131072.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generalizable NeRF Transformer (GNT)&lt;/li&gt;
&lt;li&gt;two transformer-based stages&lt;/li&gt;
&lt;li&gt;view transformer: multi-view geometry, coordinate-aligned features, epipolar lines&lt;/li&gt;
&lt;li&gt;ray transformer: ray marching rendering, decode point features&lt;/li&gt;
&lt;li&gt;reconstruct single scene without rendering formula&lt;/li&gt;
&lt;li&gt;attention map&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Topoic: novel view synthesis by NeRF (coordinate-based model + differentiable volumetric rendering)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problems: one network overfits one scene with long optimization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Former solutions &amp;amp; drawbacks: Ibrnet, pixelNerf, NLF proved the coordinates are not necessary, but the novel view can be interpolated from other views&amp;rsquo; image features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Task:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contributions &amp;amp; Reason&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cooridnate network and volumetric renderer are composed into a transformer architecture.&lt;/li&gt;
&lt;li&gt;use multi-view image features to infer coordinate-aligned features. Later these features are decoded to novel view directly without volume rendering.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Results statement&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ÊúâÊ≥õÂåñËÉΩÂäõÊÑèÊÄùÊòØÔºåËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂèØ‰ª•Áõ¥Êé•Áî®ËæìÂÖ•ÂõæÂÉèÈáçÂª∫ unseen Âú∫ÊôØÁöÑÊñ∞ËßÜÂõæÔºü&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/nerfs/img/GNT_view_transformer.png width=&gt;
  
  


&lt;h2 id=&#34;2-related-work&#34;&gt;2 Related Work&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Advances in Transformer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Radiance Fiels: NeRF, Mip-NeRF; surface representation, dynamic scenes, reflection modeling; Generalization nerf: PixelNeRF, IBRNet, MVSNeRF, NeuRay; accelerate nerf.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transformer Meets Radiance Fields: IBRNet, NerFormer, Generalizable neural radiance field, NLF, Vision transformer for nerf-based view synthesis; SRT&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-preliminaries&#34;&gt;3 Preliminaries&lt;/h2&gt;
&lt;h2 id=&#34;4-method-make-attention-all-nerf-needs&#34;&gt;4 Method: Make Attention All NeRF needs&lt;/h2&gt;
&lt;h3 id=&#34;41-epipolar-geometry-constrained-scene-representation&#34;&gt;4.1 Epipolar Geometry Constrained Scene Representation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Multi-View Feature Aggregation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vanilla NeRF use MLP to parameterize scene in a &lt;strong&gt;backward optimization fashion&lt;/strong&gt;? (ÂÖàÊ∏≤ÊüìÂá∫ÂõæÁâáÔºå‰æùÊçÆÂõæÁâáËØØÂ∑ÆÂÜçËøîÂõûÂéªÊõ¥Êñ∞ËæêÂ∞ÑÂú∫Ôºâ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;In contrast, generalizable NeRFs construct the radiance field in a feed-forward scheme&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;del&gt;Áî®ÂõæÂÉèÁâπÂæÅ‰ºòÂåñËæêÂ∞ÑÂú∫ÔºåËÆ≠ÁªÉÂ•ΩÂêéÔºåÂ∞±ÂèØ‰ª•ÂØπËæêÂ∞ÑÂú∫Â∫îÁî®volume renderingÊù•Ê∏≤ÊüìÊñ∞ËßÜÂõæÔºõËÄåÊú¨ÊñáÊòØ‰ªéËæêÂ∞ÑÂú∫Êò†Â∞ÑÂõûÂõæÂÉèÁâπÂæÅÔºå‰ªéËÄåÁîüÊàêÊñ∞ËßÜÂõæ&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;(2023-12-16) NeRF ÊòØ‰ªéÂæÖÊ∏≤ÊüìÁöÑÂÉèÁ¥†Âá∫ÂèëÔºåÂêëÁ©∫Èó¥ÂèëÂá∫Â∞ÑÁ∫øÔºåÂéªÊü•ËØ¢È¢úËâ≤„ÄÇËÄå Forward ÊòØÊääÁ©∫Èó¥ÁÇπÊäïÂΩ±Âà∞ÂÉèÁ¥†‰∏ä„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Represent the scene as a feature field, where each point in the space has a part of image feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use attention to fuse all pixel on ResUNet feature maps of source views is memory prohibitive&lt;/p&gt;
&lt;p&gt;Only fuse the pixels locating in the paring epipolar lines of &amp;ldquo;neighboring views&amp;rdquo;
ÔºàÂíå PixelNeRF ‰∏ÄÊ†∑Ôºå‰∏çËøá‰∫∫ÂÆ∂Ê≤°ÊèêÂØπÊûÅÂá†‰ΩïËøô‰∏™ËØçÔºåÂ∞±ÊòØÊääÂÖâÁ∫ø‰∏äÁöÑÁÇπÊäïÂΩ±Âà∞‰∏çÂêåËßÜÂõæ‰∏äÔºâ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory-Efficient Cross-View Attention&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Only use one read-out token in the query sequence to iteratively fuse features from neighbor views&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The similarity is not computed by dot multiplication,
but &lt;strong&gt;subtraction&lt;/strong&gt;, so the attention score is calculated for every channel of the features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The attention scores matrix and &amp;lsquo;V&amp;rsquo; are added by the relative directions between source views and the target view.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;




  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/nerfs/img/GNT_memory_reduce.jpg width=&gt;
  
  

&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discussion on Occlusion Awareness.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;42-attention-driven-volumetric-rendering&#34;&gt;4.2 Attention Driven Volumetric Rendering&lt;/h3&gt;
&lt;p&gt;Different illumination effects and material scenarios need to apply specific handcrafted rendering formula.
Data-driven renderer decode the image features into images realizing various phenomena in one way.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Ray Feature Aggregation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analogy the token features as the color in the volum rendering fomula. Therefore, do attention for coordinate-aligned features to aggregate the final color (rgb) for the novel pixel/ray.&lt;/li&gt;
&lt;li&gt;mean pooling to compress the feature patch into a pixel&lt;/li&gt;
&lt;li&gt;use dot-product attention to fully mix features of other points getting comprehensive contextual information.&lt;/li&gt;
&lt;li&gt;ÂÖ≥‰∫é auto-regressive rendering ÁöÑÂª∂‰º∏ËÆ®ËÆ∫Ôºü&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discussion on Depth Cuing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;depth is the average of marching distance with attention score&lt;/li&gt;
&lt;li&gt;NLF ÊúâÁõ∏ÂêåÁöÑÊû∂ÊûÑÔºåÂå∫Âà´Âú®Âì™Ôºü&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;5-experiments&#34;&gt;5 Experiments&lt;/h2&gt;
&lt;p&gt;single scene; generalization to unseen scenes&lt;/p&gt;
&lt;h3 id=&#34;51-implementation-details&#34;&gt;5.1 Implementation Details&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Source and Target view sampling&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Âú® Blender Êï∞ÊçÆÈõÜ‰∏äÁöÑ PSNR Êó†ÊòéÊòæÊèêÂçá&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;(2023-08-16)&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;train.py&lt;/p&gt;
&lt;pre class=&#34;pseudocode&#34; data-line-number=true&gt;\begin{algorithm}
\begin{algorithmic}
\PROCEDURE{train}{args}
\STATE data: [ 
    rgb$_{(1,H,W,3)}$,  $\newline\qquad$
    camera$_{(1,34)}$,  $\newline\qquad$
    rgb\_path$_{(str)}$,$\newline\qquad$
    src\_rgbs$_{(8,H,W,3)}$, $\newline\qquad$
    src\_camera$_{(8,34)}$, $\newline\qquad$
    depth\_range$_{(1,2)}$
    ]
\STATE $\newline$

\STATE ray\_sampler = RaySamplerSingleImage(data)

\STATE ray\_batch = ray\_sampler.random\_sample(N\_rand)

\STATE featmaps = model.feature\_net(ray\_batch[&#34;src\_rgbs&#34;])

\STATE ret = render\_rays(ray\_batch, model, projector, featmaps)

\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
&lt;/pre&gt;

&lt;p&gt;llff_test.py &lt;code&gt;__init__()&lt;/code&gt; of a training dataset:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;LLFFDataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scenes&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;random_crop&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;scenes&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;folder_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# Stuff all scenes into lists&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sceneName&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scenes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;rgb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;render_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;imgnames&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;load_llff_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sceneName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;near&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;far&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;intrinsics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c2w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_parse_llff_poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;poses&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_intrinsics&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;intrinsics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_poses&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c2w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_rgb_files&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imgnames&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render_rgb_files&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imgnames&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render_intrinsics&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;intrinsics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render_poses&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c2w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render_depth_range&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;near&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;far&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render_train_set_ids&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>read: PixelNeRF</title>
        <link>https://zichen34.github.io/writenotes/model/nerfs/b-note-pixelnerf/</link>
        <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
        
        <guid>https://zichen34.github.io/writenotes/model/nerfs/b-note-pixelnerf/</guid>
        <description>&lt;img src="https://ar5iv.labs.arxiv.org/html/2012.02190/assets/x2.png" alt="Featured image of post read: PixelNeRF" /&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/sxyu/pixel-nerf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Code&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2012.02190&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Arxiv&lt;/a&gt;
| &lt;a class=&#34;link&#34; href=&#34;https://alexyu.net/pixelnerf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ProjPage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;‰∏ÄÂè•ËØùÔºöÁî®ÂõæÂÉèÁâπÂæÅËÆ≠ÁªÉNeRFÔºåÔºàÊâÄ‰ª•ÊúâÊ≥õÂåñËÉΩÂäõ,‰∏çÂ±ÄÈôê‰∫éÂçïÂú∫ÊôØÔºå‰∏çÂÉènerfÂè™ÂÅö‰ºòÂåñÔºâ&lt;/p&gt;





  
  
  
  
   
  
  
   
  
  &lt;img src= /writenotes/model/nerfs/img/pixelNerf_byHand.jpg width=&gt;
  
  


&lt;p&gt;Â¶ÇÂõæ1ÔºåÂÖâÁ∫øÊòØÈöèÊú∫‰ªéÂÉèÁ¥†Á©∫Èó¥ÈÄâÊã©ÁöÑÔºà‰∏∫‰∫ÜÈöèÊú∫ÈÄâ‰∏Ä‰∫õ(N‰∏™)Á©∫Èó¥ÁÇπÔºâÔºåÁ©∫Èó¥ÁÇπÊäïÂΩ±Âà∞‰∏çÂêåËßÜÂõæÁöÑ feature maps (512 chnls) ‰∏äÔºåÂæóÂà∞ (N‚àóNviews‰∏™) feature volumeÔºåÈÄÅÂÖ• mlp ÂèòÊç¢Êàê r,g,b,densityÔºåÂÜçÂÅö volume rendering&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NeRF conditioned by few images&lt;/li&gt;
&lt;li&gt;costly independent optimizing&lt;/li&gt;
&lt;li&gt;use convolution to learn scene prior&lt;/li&gt;
&lt;li&gt;no explicit 3D supervision&lt;/li&gt;
&lt;li&gt;single image novel view synthesis task&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Topic: Synthesis novel views for a scene from sparse views&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Former solution &amp;amp; drawbacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Differentiable neural rendering: represent the sence as a neural network which generates rendered images&lt;/li&gt;
&lt;li&gt;NeRF: encode the 3D position and view dirs to volume density and color. It needs too many images.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Task: predicting NeRFs from one or several images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contributions: Utilize pixel-aligned spatial image features to learn scene priors
(specifically) Input image ‚Üí convolutional feature grid ‚Üí ùê±,ùêù,feature(residual) ‚Üí NeRF (pool-based multi-view features fusion) ‚Üí œÉ,rgb&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Results statement&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;no 3D supervision: 3D shape or object masks&lt;/li&gt;
&lt;li&gt;not trained in a canonical coordinate system, but in each camera coordinate system&lt;/li&gt;
&lt;li&gt;&amp;ldquo;convolution preserves the spatial alignment between the image and the output 3D representation&amp;rdquo;&lt;/li&gt;
&lt;li&gt;flexibility on number of input views in test period&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Experiments) ShapeNet; DTU&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-related-work&#34;&gt;2 Related Work&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Novel View Synthesis&lt;/li&gt;
&lt;li&gt;Learning-based 3D reconstruction&lt;/li&gt;
&lt;li&gt;Viewer-centric 3D reconstruction&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-background-nerf&#34;&gt;3 Background NeRF&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(job) (input-output) x,d -&amp;gt; density, color&lt;/li&gt;
&lt;li&gt;(training manner) volume rendering&lt;/li&gt;
&lt;li&gt;(loss func)&lt;/li&gt;
&lt;li&gt;Limitation: only use geometric consistency, resulting in individual optimization for every view&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-image-conditioned-nerf&#34;&gt;4 Image-conditioned NeRF&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(improvement) input spatial image features into network. (Specifically) Two components: convolutional encoder + nerf mlp. Spatial query position is drawn from camera space.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(paragraph order) From simple case to general case&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;41-single-image-pixelnerf&#34;&gt;4.1 Single-Image pixelNeRF&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;(main idea) The inputs are all from view space.&lt;/li&gt;
&lt;li&gt;(steps) Input image ‚Üí feature volume W=E(I) ‚Üí assign image feature to sample points by projecting the 3D position to 2D location ‚Üí NeRF network&lt;/li&gt;
&lt;li&gt;(pipeline)&lt;/li&gt;
&lt;li&gt;(the role of query view direction)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;42-incorporating-multiple-views&#34;&gt;4.2 Incorporating Multiple Views&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(main idea) extract information from multi views to resolve geometric ambiguities and able to accept multiple images in test time. No relation with world space&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;World space can base on any view. (notion explain)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Project the world query point into each input view space. The fisrt layer processes each view independently and the final layer aggregates all views.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(model pipeline)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Encode each input img into feature volume and retrieve the corresponding feature at the sample point&amp;rsquo;s projecting location.&lt;/li&gt;
&lt;li&gt;The features companied with (xyz,viewdirs) are input to the first layer and all the intermediate vectors are fused by average pooling before entering the final layers, which compress the vector into density and color.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5-experiments&#34;&gt;5 Experiments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Datasets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ShapeNet (category-specific and category-agnostic view synthesis);&lt;/li&gt;
&lt;li&gt;ShapNet scenes with unseen categories and multiple objects;&lt;/li&gt;
&lt;li&gt;DTU rescaled to 1/4: 400x300&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Baselines: SRN, DVR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Metrics: PSNR, SSIM, LPIPS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implementation Details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image encoder: ResNet34;&lt;/li&gt;
&lt;li&gt;Features: non-pooling, upsampled using bilinear interpolation, concatenated feature maps of every image.&lt;/li&gt;
&lt;li&gt;Combine the points&amp;rsquo; position and view direction in a ResNet manner (residual)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;51-shapenet-benchmarks&#34;&gt;5.1 ShapeNet Benchmarks&lt;/h3&gt;
&lt;p&gt;ShapeNet for category-specific and category-agnostic&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Category-specific View Synthesis Benchmark&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;one-shot and two-shot view synthesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A model is trained for reconstructing one category and the used images contains 50 random views per object instance.
Only 2 are encoded and fed into network.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ablations. The benefit of local features&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Category-agnostic Object Prior&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train a single model to the 13 largest categories of ShapeNet&lt;/li&gt;
&lt;li&gt;pick one view randomly from 24 fixed elevation view of each object instance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;52-pushing-the-boundaries-of-shapenet&#34;&gt;5.2 Pushing the Boundaries of ShapeNet&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(tasks) less controlled capture scenarios in ShapeNet:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;unseen object categories,&lt;/li&gt;
&lt;li&gt;multiple-object scene,&lt;/li&gt;
&lt;li&gt;simulation-to-real transfer on car images.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to novel categories: apply the model on unseen categories&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple-object scenes:
The geometric features need can be applied in any view direction (360¬∞)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;53-scene-prior-on-real-images&#34;&gt;5.3 Scene Prior on Real Images&lt;/h3&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;(2023-07-20)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;calc_losses()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;pseudocode&#34; data-line-number=true&gt;\begin{algorithm}
\caption{Main steps of clac$\_$losses()}
\begin{algorithmic}
  \PROCEDURE{clac-losses}{}
  \STATE Sample 4 objects from 88 training scanned objects (folder) as a super batch
  \STATE Each object samples randomly 1 or 3 from its total of 49 NV as target images for training
  \STATE Sample a batch of rays (N$\_$rand)
  \STATE Extract latent feature maps `self.latent`
  \STATE Sample z$\_$coarse points on the rays
  \STATE Transform points onto NS camera space
  \STATE Perspective project xyz to 2D location uv on feature maps
  \STATE Indexing latent vectors from feature maps by `F.grid$\_$sample()`
  \ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
&lt;/pre&gt;

&lt;hr&gt;
&lt;h3 id=&#34;dtu-dataset&#34;&gt;DTU dataset&lt;/h3&gt;
&lt;p&gt;(2023-08-17)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;get_split_dataset()&lt;/code&gt; (in &amp;ldquo;data/&lt;strong&gt;init&lt;/strong&gt;.py&amp;rdquo;) will construct different Dataset objects for different datasets.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;list_prefix&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;new_&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;training&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;max_imgs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;49&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sub_format&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;dtu&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;scale_focal&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;z_near&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;z_far&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;5.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Apply color jitter during train&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train_aug&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ColorJitterDataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train_aug_flags&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;extra_inherit_attrs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sub_format&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train_set&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DVRDataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datadir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stage&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_aug&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;train_set&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_aug&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_aug_flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        
    </channel>
</rss>
