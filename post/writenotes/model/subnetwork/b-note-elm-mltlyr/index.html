<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Multilayer Extreme Learning Machine With Subnetwork Nodes for Representation Learning (Cybernet 2015)">
<title>Read: Optim - SLFN | Multilayer SNN</title>

<link rel='canonical' href='http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-elm-mltlyr/'>

<link rel="stylesheet" href="/scss/style.min.0e95b70faf4f470dc6ba88da538ece0f8cf22d03bab371e1b345205b81899732.css"><meta property='og:title' content="Read: Optim - SLFN | Multilayer SNN">
<meta property='og:description' content="Multilayer Extreme Learning Machine With Subnetwork Nodes for Representation Learning (Cybernet 2015)">
<meta property='og:url' content='http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-elm-mltlyr/'>
<meta property='og:site_name' content='Zichen Wang'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='SLFN' /><meta property='article:published_time' content='2023-01-18T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2023-01-18T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="Read: Optim - SLFN | Multilayer SNN">
<meta name="twitter:description" content="Multilayer Extreme Learning Machine With Subnetwork Nodes for Representation Learning (Cybernet 2015)">
    <link rel="shortcut icon" href="/favicon-32x32.png" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_b598b46054b3fa80.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Zichen Wang</a></h1>
            <h2 class="site-description"></h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/zichen34'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com/luckily1640'
                        target="_blank"
                        title="Twitter"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/post/writenotes/' >
                
                
                
                <span>WriteNotes</span>
            </a>
        </li>
        
        
        <li >
            <a href='/post/writenotes/model/acadmodel/aboutme/' >
                
                
                
                <span>About</span>
            </a>
        </li>
        
        
        <li >
            <a href='/page/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>Dark Mode</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#i-introduction">I. Introduction</a></li>
    <li><a href="#ii-preliminaries-and-basic-elm">II. Preliminaries and Basic-ELM</a>
      <ol>
        <li><a href="#a-notations">A. Notations</a></li>
        <li><a href="#b-basic-elm">B. Basic-ELM</a></li>
      </ol>
    </li>
    <li><a href="#iii-proposed-method">III. Proposed Method</a>
      <ol>
        <li><a href="#a-elm-with-subnetwork-nodes">A. ELM With Subnetwork Nodes</a></li>
        <li><a href="#b-proposed-method-for-representation-learning">B. Proposed Method for Representation Learning</a>
          <ol>
            <li><a href="#1-optimal-projecting-parameters-and-optimal-feature-data">1) Optimal Projecting Parameters and Optimal Feature Data</a></li>
            <li><a href="#2-learning-steps">2) Learning Steps</a></li>
          </ol>
        </li>
        <li><a href="#c-proof-of-the-proposed-method">C. Proof of the Proposed Method</a></li>
        <li><a href="#d-proposed-method-with-multinetwork-structures">D. Proposed Method With Multinetwork Structures</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/read/" style="background-color: #2a9d8f; color: #fff;">
                Read
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/post/writenotes/model/subnetwork/b-note-elm-mltlyr/">Read: Optim - SLFN | Multilayer SNN</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            Multilayer Extreme Learning Machine With Subnetwork Nodes for Representation Learning (Cybernet 2015)
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Jan 18, 2023</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    14 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
<section class="toc toc--inline">
    <h2 class="">Table of contents</h2>
    <div class="" >
        <nav id="TableOfContents">
  <ol>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#i-introduction">I. Introduction</a></li>
    <li><a href="#ii-preliminaries-and-basic-elm">II. Preliminaries and Basic-ELM</a>
      <ol>
        <li><a href="#a-notations">A. Notations</a></li>
        <li><a href="#b-basic-elm">B. Basic-ELM</a></li>
      </ol>
    </li>
    <li><a href="#iii-proposed-method">III. Proposed Method</a>
      <ol>
        <li><a href="#a-elm-with-subnetwork-nodes">A. ELM With Subnetwork Nodes</a></li>
        <li><a href="#b-proposed-method-for-representation-learning">B. Proposed Method for Representation Learning</a>
          <ol>
            <li><a href="#1-optimal-projecting-parameters-and-optimal-feature-data">1) Optimal Projecting Parameters and Optimal Feature Data</a></li>
            <li><a href="#2-learning-steps">2) Learning Steps</a></li>
          </ol>
        </li>
        <li><a href="#c-proof-of-the-proposed-method">C. Proof of the Proposed Method</a></li>
        <li><a href="#d-proposed-method-with-multinetwork-structures">D. Proposed Method With Multinetwork Structures</a></li>
      </ol>
    </li>
  </ol>
</nav>

    </div>
</section>



    
    
    <p>Authors: <a class="link" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=fFP4b9kAAAAJ&amp;citation_for_view=fFP4b9kAAAAJ:-mN3Mh-tlDkC"  target="_blank" rel="noopener"
    >Yimin Yang</a>, and Q. M. Jonathan Wu <br></p>
<p><a class="link" href="https://ieeexplore.ieee.org/abstract/document/7295596"  target="_blank" rel="noopener"
    >IEEE Cybernetics</a>; Publish Date: 2015-10-09.</p>
<p>This is the 2nd paper in his series, and the first paper is <a class="link" href="http://blog.zichen.uk/post/writenotes/model/subnetwork/b-note-elm-subnet/" >this</a>.</p>
<p>(æ„Ÿè§‰Introå†™å¾—ä¸é”™ï¼Œé€»è¾‘æ€§å¼ºï¼Œä¿¡æ¯é‡å¤§ï¼›ä½†åé¢methodéƒ¨åˆ†å¥½å¤štypo)</p>
<h2 id="abstract">Abstract
</h2><p>Representation learning of multilayer ELM with subnetwork nodes outperform conventional feature learning methods.</p>
<h2 id="i-introduction">I. Introduction
</h2><p>model performance â” data representaiton/features â” processing pipelines design and data transformations â” data representation â” effective learning</p>
<ol>
<li>
<p>Feature reduction and extraction techniques can be conducted in a supervised, unsupervised or semi-supervised manner.</p>
</li>
<li>
<p>ELMs learn representations of data to extract useful information when building classifiers or predictors.</p>
</li>
<li>
<p>ELMs provide a unified learning framework for &ldquo;generalized&rdquo; single-hidden layer feedforward NNs (SLFNs).</p>
<ul>
<li>In ELM methods, the hidden layer parameters of NN need not be tuned during training, but generated randomly.</li>
</ul>
</li>
<li>
<p>ML-ELM is adding (multiple) general hidden nodes (subnetwork nodes) to existing single-hidden-layer ELM networks.</p>
<ol>
<li>A versatile platform with faster speed and better generalization performance on feature extraction.</li>
<li>Its generalization performance is not sensitive to the parameters of the networks in the learning process.</li>
<li>ML-ELM has universal approximation capability and representation learning.</li>
</ol>
</li>
</ol>
<h2 id="ii-preliminaries-and-basic-elm">II. Preliminaries and Basic-ELM
</h2><h3 id="a-notations">A. Notations
</h3><ul>
<li>ğ‘ : set of real numbers</li>
<li>{(ğ±áµ¢,ğ²áµ¢)áµ¢â‚Œâ‚á´¹} (ğ±áµ¢âˆˆ ğ‘â¿,ğ²áµ¢âˆˆ ğ‘áµ) : M arbitrary distinct samples,</li>
<li>ğ± : an input data matrix ğ±âˆˆ ğ‘â¿á•á´¹</li>
<li>ğ² : desired output data matrix ğ²âˆˆ ğ‘áµá•á´¹</li>
<li>ğ›‚áµ¢ : weight vector connecting the ğ‘–th hidden nodes and the input nodes</li>
<li>â™­áµ¢ : bias of the ğ‘–th hidden nodes</li>
<li>Î²áµ¢ : output weight between the ğ‘–th hidden node and the output node</li>
<li>ğ : residual error of current network output, i.e., ğ=ğ²-ğŸ</li>
<li>ğˆ : unit matrix</li>
<li>sum(ğ) : the sum of all elements of the matrix ğ</li>
<li>g : sigmoid or sine activation function</li>
</ul>
<p>(TABLE 1)</p>
<ul>
<li>(ğ›‚,â™­) : a hidden node (in basic ELM)</li>
<li>(ğš,ğ‘) : a general hidden node (or subnetwork node)</li>
<li>^ğšÊ²_f : input weight of the jth general hidden node in feature mapping layer. ^ğšÊ²_fâˆˆ ğ‘áµˆá•â¿</li>
<li>^bÊ²_f : bias of the ğ‘—th general hidden node in feature mapping layer ^bÊ²_fâˆˆ ğ‘</li>
<li>(ğ›‚áµ¢Ê²_f,â™­Ê²_f) : the ğ‘–th general hidden node in the ğ‘—th general hidden node.</li>
<li>(^ğšâ‚•,^ğ‘â‚•) : hidden nodes in ELM learning layer and ^ğšâ‚•âˆˆ ğ‘ áµá•áµˆ</li>
<li>uâ±¼ : normalized function in the ğ‘—th general node, uâ±¼(â‹…):ğ‘ â” (0,1], uâ±¼â»Â¹ represent its reverse function</li>
<li>ğ‡Ê²_f : feature data generated by ğ‘—general nodes in a feature mapping layer,
i.e., ğ‡Ê²_f = âˆ‘áµ¢â‚Œâ‚Ê² uáµ¢â»Â¹ â‹… g(ğ±, ^ğšâ±_f, ^bâ±_f), ğ‡Ê²_fâˆˆ ğ‘áµˆá•á´¹</li>
<li>ğ‡Ê²â±_f : feature data generated by the ğ‘–th feature mapping layer</li>
<li>M : number of training samples</li>
<li>n : input data dimension</li>
<li>m : output data dimension</li>
<li>d : feature data dimension</li>
<li>ğ_L : the residual error of current two-layer network (L general nodes in the first layer and (ğšâ‚•,ğ‘â‚•) in the second layer)</li>
<li>ğÊ²_L : the residual error of current two-layer network (L general nodes in the first layer and ğ‘—general nodes in the second layer)</li>
<li>L : the numbers of general hidden nodes</li>
</ul>
<h3 id="b-basic-elm">B. Basic-ELM
</h3><p>The output function of ELM for SLFNs fed with input matrix ğ± is: <br>
fâ‚™(ğ±)=âˆ‘áµ¢â‚Œâ‚â¿ Î²áµ¢ g(ğ±, ğ›‚áµ¢, â™­áµ¢).</p>
<p>&ldquo;ELM theory aims to reach the smallest training error but also the smallest norm of output weights&rdquo; (regularization term?), so the objective is to minimize: <br>
â€–Î²áµ¢â€–â‚šá¶£Â¹ + Câ‹…â€–âˆ‘áµ¢â‚Œâ‚â¿ Î²áµ¢ g(ğ±, ğ›‚áµ¢, â™­áµ¢) - ğ²â€–á¶£Â²_q, i=1,&hellip;,n. â€ƒ (á¶£ signifies Î¼)</p>
<p>where Î¼â‚&gt;0, Î¼â‚‚&gt;0, p,q = 0, Â½, 1, 2, &hellip;, +âˆ, C is a positive value, g(ğ±, ğ›‚, â™­) is referred to as ELM feature mapping (linear projection+activation) or Huang&rsquo;s transform.</p>
<p>(Convergence proved by Huang et al.)</p>
<blockquote>
<p><strong>Lemma 1</strong>: Given M aribitrary distinct samples {(ğ±, ğ²)}, ğ±âˆˆ ğ‘â¿á•á´¹, ğ²âˆˆ ğ‘áµá•á´¹ sampled from a continuous system, an activation function g,
then for any continous target function ğ² and any function sequence g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³) randomly generated based on any continuous sampling distribution,
lim_{nââˆ} â€–ğ²-ï¼ˆfâ‚™â‚‹â‚ + g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³)ï¼‰â€–=0 holds with probabiltiy one if <br>
Î²â‚™ = âŸ¨ğâ‚™â‚‹â‚, g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³)âŸ© / â€–g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³)â€–Â²,</p></blockquote>
<p>where (ğ›‚â‚™Ê³, â™­â‚™Ê³) represesnts the ğ‘›th random hidden node, and ğâ‚™â‚‹â‚ = ğ²-fâ‚™â‚‹â‚</p>
<h2 id="iii-proposed-method">III. Proposed Method
</h2><h3 id="a-elm-with-subnetwork-nodes">A. ELM With Subnetwork Nodes
</h3><p>A hidden node can be a subnetwork formed by several hidden nodes. Hence, a single mapping layer can contain multiple networks.</p>
<p>Comparision of the feature mapping layer:</p>
<div class="mermaid">flowchart LR
subgraph A[basic ELM]
direction BT
x1["xâ‚"]--> h1(("ğ›‚â‚,â™­â‚, Î²â‚")) & he1((...)) & hL(("ğ›‚L,â™­L, Î²L")) 
xe[x...]--> h1(("ğ›‚â‚,â™­â‚, Î²â‚")) & he1((...)) & hL(("ğ›‚L,â™­L, Î²L")) 
xn["xâ‚™"]--> h1(("ğ›‚â‚,â™­â‚, Î²â‚")) & he1((...)) & hL(("ğ›‚L,â™­L, Î²L")) 

h1 --> y1["yâ‚"] & ye[...] & ym["yâ‚˜"]
he1--> y1["yâ‚"] & ye[...] & ym["yâ‚˜"]
hL --> y1["yâ‚"] & ye[...] & ym["yâ‚˜"]

subgraph A1["ELM feature mapping layer"]
h1 & he1 & hL
end
end

subgraph A1["ELM feature mapping layer"]
h1 & he1 & hL
end

subgraph B[ELM with subnetwork nodes]
direction BT
x1_["xâ‚"] --> ghn1 & ghne((...)) & ghnL
xe_[x...] --> ghn1 & ghne((...)) & ghnL
xn_["xâ‚™"] --> ghn1 & ghne((...)) & ghnL

ghn1--> y1_["yâ‚"] & ye_[...] & ym_["yâ‚˜"]
ghne--> y1_["yâ‚"] & ye_[...] & ym_["yâ‚˜"]
ghnL--> y1_["yâ‚"] & ye_[...] & ym_["yâ‚˜"]
end

subgraph ghn1["^ğ›‚Â¹_f, ^â™­Â¹_f, with weight uâ‚â»Â¹"]
direction TB
n11(("ğ›‚Â¹_f1,\n â™­Â¹_f1")) & n1e((...)) & n1m(("ğ›‚Â¹_fm,\n â™­Â¹_fm"))
end

subgraph ghne["general hidden nodes"]
direction TB
ne1((1)) & nee((...)) & nem((m))
end

subgraph ghnL["^ğ›‚á´¸_f, ^â™­_f, with weight u\_Lâ»Â¹"]
direction TB
nL1(("ğ›‚á´¸_f1,\n â™­á´¸_f1")) & nLe((...)) & nLm(("ğ›‚á´¸_fm,\n â™­á´¸_fm"))
end
</div>

<p>Three differences between ELM feature mapping layer and this feature mapping layer.</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th>Difference</th>
          <th>Standard ELM</th>
          <th>ELM with subnetwork nodes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>hidden node</td>
          <td>single hidden node generated<br> one by one</td>
          <td>general hidden node having subnetwork</td>
      </tr>
      <tr>
          <td># hidden node</td>
          <td>Independent to the output dim ğ‘š</td>
          <td>In a subnetwork, it equals to the output dim</td>
      </tr>
      <tr>
          <td>relation</td>
          <td>A special case of the subnetwork case</td>
          <td></td>
      </tr>
  </tbody>
</table></div>
<h3 id="b-proposed-method-for-representation-learning">B. Proposed Method for Representation Learning
</h3><h4 id="1-optimal-projecting-parameters-and-optimal-feature-data">1) Optimal Projecting Parameters and Optimal Feature Data
</h4><div class="mermaid">flowchart LR
x1["xâ‚"]--> h1(("^ğ›‚Â¹_f,^â™­Â¹_f, Î²Â¹")) & h2(("^ğ›‚Â²_f,^â™­Â²_f, Î²Â²")) & he1(("â‹®")) & hL(("^ğ›‚á´¸_f,^â™­á´¸_f, ^Î²á´¸")) 
xe[x...]--> h1(("^ğ›‚Â¹_f,^â™­Â¹_f, Î²Â¹")) & h2(("^ğ›‚Â²_f,^â™­Â²_f, Î²Â²")) & he1(("â‹®")) & hL(("^ğ›‚á´¸_f,^â™­á´¸_f, ^Î²á´¸")) 
xn["xâ‚™"]--> h1(("^ğ›‚Â¹_f,^â™­Â¹_f, Î²Â¹")) & h2(("^ğ›‚Â²_f,^â™­Â²_f, Î²Â²")) & he1(("â‹®")) & hL(("^ğ›‚á´¸_f,^â™­á´¸_f, ^Î²á´¸")) 

h1 & h2 & he1 & hL --> feat["d-dimension\n Feature data"]
feat --> n1 & n2 & ne & nm --> y1_["yâ‚"] & ye_["â‹®"] & ym_["yâ‚˜"]

subgraph A1["ELM feature mapping layer"]
h1 & h2 & he1 & hL
end

subgraph elm["ELM-learning layer"]
n1(("ğ›‚â‚•â‚,^â™­â‚•")) & n2(("ğ›‚â‚•â‚‚,^â™­â‚•")) & ne(("â‹®")) & nm(("ğ›‚â‚•â‚˜,^â™­â‚•"))
end
</div>

<p>Objective of representation learning: Represent the input features meaningfully in several different representations as follows.</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th>Represen-<br>tation</th>
          <th>feat dim (ğ‘‘) vs<br> in-dim (ğ‘›)</th>
          <th>feature</th>
          <th>target output</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Dimension Reduction</td>
          <td>ğ‘‘ &lt; ğ‘›</td>
          <td>H_f âˆˆ ğ‘áµˆá•á´¹</td>
          <td>ğ²=label (Supervise)<br>or ğ²=ğ± (Unsp~)</td>
      </tr>
      <tr>
          <td>Expanded Dimension</td>
          <td>ğ‘‘ &gt; ğ‘›</td>
          <td>H_f âˆˆ ğ‘áµˆá•á´¹</td>
          <td>ğ²=label (Supervise)<br>or ğ²=ğ± (Unsp~)</td>
      </tr>
  </tbody>
</table></div>
<p>The feature data is ğ‡_f(ğ±â‚–, ^ğš_f, ^b_f), where the weights of feature mapping layer ^ğšÊ²_f, j=1,&hellip;,L belongs to ğ‘áµˆá•â¿.</p>
<blockquote>
<p><strong>Definition 1</strong>: Given a nonlinear piecewise continous activation function g, we call <br>
{(^ğšÊ²_f, ^bÊ²_f)â±¼â‚Œâ‚á´¸} (^ğšÊ²_f âˆˆ ğ‘áµˆá•â¿) the ğ¿ optimal general hidden nodes <br>
and ğ‡âƒ° âƒ°_f= âˆ‘áµ¢â‚Œâ‚á´¸ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) the optimal feature data if it satisfies: <br>
â€–ğ_Lâ€– â‰¤ min_{ğ‡âƒ°á´¸_fâˆˆ ğ‘áµˆá•á´¹} ( min_{ğšâ‚•âˆˆ ğ‘ áµá•áµˆ} â€–ğ²-uâ‚•â»Â¹ g(ğ‡âƒ°á´¸_f, ğšâ‚•, bâ‚•)â€– )  â€‚ (4)</p></blockquote>
<p>where ğ_L = â€–ğ²-uâ‚•â»Â¹ g(ğ‡âƒ°á´¸ âƒ°_f, ^ğšâ‚•, ^bâ‚•)â€– and sequence â€–ğ_Lâ€– is decreasing and bounded below by zero.</p>
<blockquote>
<p><strong>Remark 1</strong>: If the optimal projecting parameters are obtained in the feature mapping layer
{(^ğšÊ²_f, ^bÊ²_f)â±¼â‚Œâ‚á´¸} (where ^ğš_f âˆˆ ğ‘áµˆá•â¿), <br>
the original n-dimension data points ğ± will be converted to d-dimension data points:
ğ‡âƒ°_f= âˆ‘â±¼â‚Œâ‚á´¸ g(ğ±â‚–, ^ğšÊ²_f, ^bÊ²_f), which satisfy the inequality (4).</p></blockquote>
<p>Thus the purpose is to find optimal projecting parameters that make the inequality (4) true for all data points.</p>
<h4 id="2-learning-steps">2) Learning Steps
</h4><p>Based on the inverse of the activation function.</p>
<p>Given M arbitrary distinct training samples {(ğ±â‚–,ğ²â‚–)â‚–â‚Œâ‚á´¹}, ğ±â‚–âˆˆ ğ‘â¿, ğ²â‚–âˆˆ ğ‘áµ, which are sampled from a continuous system.</p>
<ol>
<li>
<p>Set j=1 to initialize a <strong>general node</strong> of the feature mapping layer randomly as: <br>
ğ‡âƒ°Ê²_f = g(^ğšÊ²_fâ‹…ğ± + ^bÊ²_f), (^ğšÊ²_f)áµ€â‹…^ğšÊ²_f=ğˆ, (^bÊ²_f)áµ€â‹…^bÊ²_f=1,</p>
<p>where ^ğšÊ²âˆˆ ğ‘áµˆá•â¿, ^bÊ²_fâˆˆ ğ‘ is the orthogonal random weight and bias of feature mapping layer. ğ‡âƒ°Ê²_f is current feature data.</p>
</li>
<li>
<p>Given a sigmoid or sine activation function g, for any continous desired outputs ğ², the parameters in the (general) ELM learning layer are obtained as:</p>
<ul>
<li>^ğšâ‚• = gâ»Â¹(uâ‚™(ğ²)) â‹… (ğ‡âƒ°Ê²_f)â»Â¹, ^ğšÊ²â‚•âˆˆ ğ‘áµˆá•áµ,</li>
<li>^bâ‚• = âˆšmse(^ğšâ‚•Ê² â‹… ğ‡âƒ°Ê²_f - gâ»Â¹(uâ‚™(ğ²)) ), ^bÊ²â‚™âˆˆ ğ‘,</li>
<li>$gâ»Â¹(â‹…) = \\{^{arcsin(â‹…) \quad if\ g(â‹…)=sin(â‹…)}_{-log(1/(â‹…)-1) \quad if\ g(â‹…) = 1/(1+eâ»â½Ë™â¾)}$, _</li>
</ul>
<p>where ğ‡âƒ°â»Â¹ = ğ‡âƒ°áµ€( (C/ğˆ) + ğ‡âƒ° ğ‡âƒ°áµ€)â»Â¹; C is a positive value; uâ‚™ is a normalized function
uâ‚™(ğ²): ğ‘â”(0,1]; gâ»Â¹ and uâ‚™â»Â¹ represent their reverse function.</p>
</li>
<li>
<p>Update the output error ğâ±¼ as <br>
ğâ±¼ = ğ² - uâ‚™â»Â¹ g(ğ‡âƒ°Ê²_f, ^ğšâ‚•, ^bâ‚•)  <br>
So the error feedback data is ğâ±¼ = gâ»Â¹(uâ‚™(ğâ±¼))â‹…(^ğšâ‚•)â»Â¹</p>
</li>
<li>
<p>Set j=j+1, add a new general node (^ğšÊ²_f, ^bÊ²_f) in the feature mapping layer by</p>
<ul>
<li>^ğšÊ²_f = gâ»Â¹( uâ±¼(ğâ±¼â‚‹â‚) ) â‹… ğ±â»Â¹, ^ğšÊ²_fâˆˆ ğ‘â¿á•áµˆ</li>
<li>^bÊ²_f = âˆšmse(^ğšÊ²_f â‹… ğ± - ğâ±¼â‚‹â‚), ^bÊ²âˆˆ ğ‘</li>
</ul>
<p>and update the feature data ğ‡âƒ°Ê²_f = âˆ‘áµ¢â‚Œâ‚Ê² uâ‚—â»Â¹ g(ğ±, ^ğšË¡_f, ^bË¡_f)</p>
</li>
<li>
<p>Repeat step 2-4 ğ¿-1 times. (Finally, ğ¿ nodes are added into feature mapping layer.)
The set of parameters {^ğšÊ²_f,^bÊ²_f}â±¼â‚Œâ‚á´¸ are the optimal projecting parameters
and the feature data ğ‡âƒ°á´¸_f = âˆ‘â±¼â‚Œâ‚á´¸ uâ±¼â»Â¹ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) = ğ‡âƒ° âƒ°_f are the optimal feature data.</p>
</li>
</ol>
<h3 id="c-proof-of-the-proposed-method">C. Proof of the Proposed Method
</h3><p>(Proof of Convergence)</p>
<p>Given M arbitrary distinct samples {(ğ±â‚–,ğ²â‚–)}â‚–â‚Œâ‚á´¹ (ğ±â‚–âˆˆ ğ‘â¿, ğ²â‚–âˆˆ ğ‘áµ)</p>
<blockquote>
<p><strong>Lemma 2</strong>: Given a bounded nonconstant piecewise continuous activation function g, we have <br>
lim_{(ğ›‚,â™­)â†’(^ğ›‚,^â™­)} â€–g(ğ±,ğ›‚,â™­) - g(ğ±,^ğ›‚,^â™­)â€– = 0
where the (^ğ›‚,^â™­) is one of the least-squares solutions of a general linear system ğ›‚â‹…ğ±+â™­.</p></blockquote>
<p>Remark 2:</p>
<ol>
<li>
<p>Lemma 2 shows that SLFN training problem can be considered as finding optimal hidden parameters which satisfy:
g(^ğ›‚â‚,^â™­â‚) + &hellip; + g(^ğ›‚_L,^â™­_L) â†’ ğ². â€ƒ ğ›‚ (alpha) stands for basic ELM hidden node.</p>
</li>
<li>
<p>Thus training an SLFN is equivalent to finding a least-square general input weight ^ğšâ‚• of the (linear+activation) system g(^ğšâ‚•â‹…ğ±) = ğ².</p>
</li>
<li>
<p>If activation function g is invertible, the input weights matrix can be obtained by pulling back the residual error to the hidden layer.</p>
</li>
</ol>
<p>For example, if g is a sine function,</p>
<ul>
<li>The output of the hidden layer matrix is ğ²=sin(ğšâ‚• â‹… ğ±).</li>
<li>Thus, ğšâ‚•â‹…ğ± = arcsin(ğ²), ğ²âˆˆ (0,1].</li>
<li>The smallest norm least-squares solution of the linear system sin(ğšâ‚•â‹…ğ±)=ğ² is: <br>
^ğšâ‚• = arcsin(ğ²)â‹…ğ±â»Â¹,
where ğ±â»Â¹ is the Moore-Penrose generalized inverse of matrix ğ±. ğ±â»Â¹ = ğ±áµ€( (C/ğˆ) + ğ±ğ±áµ€)â»Â¹</li>
</ul>
<blockquote>
<p><strong>Theorem 1</strong>: Given M arbitrary distinct samples {(ğ±áµ¢,ğ²áµ¢)áµ¢â‚Œâ‚á´¹}, (ğ±áµ¢âˆˆ ğ‘â¿, ğ²áµ¢âˆˆ ğ‘áµ) and a sigmoid or sine activation function g, for any continuous desired outputs ğ², we have:<br>
the optimal weights ^ğšâ‚• = argmin_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹(g(ğ±,ğšâ‚•)) - ğ²â€– <br>
least square error â€–g(ğ±,^ğšâ‚•,^bâ‚•) - ğ²â€– â‰¤ min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹(g(ğšâ‚•â‹…ğ±)) - ğ²â€–  <br>
if the parameters are obtained by (similar to Algorithm step-2):</p>
<ul>
<li>^ğšâ‚• = gâ»Â¹( u(ğ²))â‹…ğ±â»Â¹, ^ğšâ‚• âˆˆ ğ‘áµá•â¿</li>
<li>^bâ‚• = âˆšmse(^ğšâ‚•â‹…ğ± - gâ»Â¹(u(ğ²))), ^bâ‚•âˆˆ ğ‘</li>
<li>$gâ»Â¹(â‹…) = \\{^{arcsin(â‹…) \quad if\ g(â‹…)=sin(â‹…)}_{-log(1/(â‹…)-1) \quad if\ g(â‹…) = 1/(1+eâ»â½Ë™â¾)}$, _</li>
</ul></blockquote>
<p>Proof:</p>
<ol>
<li>
<p>Let ğ›Œ=ğšâ‚•â‹…ğ±, and ğ›Œ satisfy g(ğ›Œ) = ğ². Normalizing ğ² to (0,1] by u(ğ²) to let ğ›Œâˆˆ ğ‘. <br>
Thus, for a sine hidden node, ğ›Œ = gâ»Â¹(u(ğ²)) = arcsin(u(ğ²)).
While for a sigmoid hidden node, ğ›Œ = gâ»Â¹(u(ğ²)) = -log(1/u(ğ²) - 1).</p>
</li>
<li>
<p>^ğšâ‚• is the solution for the linear system (g(ğšâ‚•â‹…ğ±)=ğ²). <br>
For sine activation: ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹ = arcsin(u(ğ²))â‹…ğ±â»Â¹.
For sigmoid activation: ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹ = -log(1/u(ğ²) - 1)â‹…ğ±â»Â¹</p>
</li>
<li>
<p>One of the least-squares solutions of a general linear system ğšâ‚•â‹…ğ±=ğ›Œ is ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹, which means the smallest error can be reached by this solution:
â€–^ğšâ‚•â‹…ğ± -ğ›Œâ‚™â€– = min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–ğšâ‚•â‹…ğ± - gâ»Â¹( u(ğ²) )â€–   â€ƒ (18)</p>
</li>
<li>
<p>The special solution ^ğšâ‚• = gâ»Â¹( u(ğ²) )â‹…ğ±â»Â¹ has the smallest norm among all the least-squares solutions of ğšâ‚•â‹…ğ± = ğ›Œ.
The error can be further reduced by adding bias bâ‚™:
^bâ‚• = âˆšmse(^ğšâ‚•â‹…ğ± - hâ»Â¹( u(ğ²) ))</p>
</li>
<li>
<p>Based on eq. (18) and Lemma2, optimization by minimizing the L2-loss can be reformulated as:
min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹( g(ğšâ‚•â‹…ğ±) ) - uâ»Â¹( g(ğ›Œ))â€–
= â€–uâ»Â¹( g(^ğšâ‚•â‹…ğ±) ) - uâ»Â¹( g(ğ›Œ))â€–
â‰¥ â€–uâ»Â¹( g(^ğšâ‚•â‹…ğ± + ^bâ‚•) ) - ğ²â€–    â€ƒ (20)</p>
</li>
<li>
<p>Based on eq. (18) and eq. (20), the optimal weights is proved as:
^aâ‚• = arg min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–g(ğ±,ğšâ‚•) - ğ²â€– <br>
And it satisfy: â€–g(ğ±,^ğšâ‚•,^bâ‚•) - ğ²â€– â‰¤ min_{ğšâ‚•âˆˆ ğ‘áµá•â¿} â€–uâ»Â¹( g(^ğšâ‚•â‹…ğ±) ) - ğ² â€–</p>
</li>
</ol>
<p>Based on Lemma 2 and Theorem 1, Theorem 2 is given:</p>
<blockquote>
<p><strong>Theorem 2</strong>: Given M arbitrary distinct samples (ğ±, ğ²), ğ±âˆˆ ğ‘â¿á•á´¹, ğ²âˆˆ ğ‘áµá•á´¹, a sigmoid or sine activation function g,
and the initial orthogonal random weights ^ğšÂ¹_f and bias ^bÂ¹_f.
For any continuous desired output ğ², the optimal feature data is: <br>
ğ‡âƒ°á´¸âƒ° _f(ğ±, (^ğšÂ¹_f, &hellip;, ^ğšá´¸_f), (^bÂ¹_f,&hellip;,^bá´¸_f))
= âˆ‘â±¼â‚Œâ‚á´¸ uâ±¼â»Â¹ g(^ğšÊ²_f â‹… ğ± + ^bÊ²_f)
which satisfy: <br>
â€–ğ_Lâ€– â‰¤ min_{^ğšÊ²_fâˆˆ ğ‘â¿á•áµˆ} ( min_{ğšâ‚•âˆˆ ğ‘ áµá•áµˆ} â€–ğ²-uâ‚™â»Â¹ g(ğ‡âƒ°á´¸_f, ğšâ‚•, bâ‚•)â€–) â€‚  (21)</p>
<p>and â€–ğ_Lâ€– is decreasing and bounded below by zero if these parameters are obtained by:</p>
<ul>
<li>ğ‡âƒ°Ê²_f = âˆ‘áµ¢â‚Œâ‚Ê² uáµ¢â»Â¹ g(ğ±, ^ğšâ±_f, ^bâ±_f),</li>
<li>^ğšâ‚• = gâ»Â¹(uâ‚™(ğ²)) â‹… (ğ‡âƒ°Ê²_f)â»Â¹, ^ğšâ‚•âˆˆ ğ‘ áµá•áµˆ,</li>
<li>^bâ‚• = âˆšmse(^ğšâ‚•â‹…ğ‡âƒ°Ê²_f - gâ»Â¹( u(ğ²) )), ^bâ‚•âˆˆ ğ‘</li>
<li>gâ»Â¹(â‹…) = \{^{arcsin(â‹…) \quad if\ g(â‹…)=sin(â‹…)}_{-log(1/(â‹…)-1) \quad if\ g(â‹…) = 1/(1+eâ»â½Ë™â¾)}$, _</li>
<li>ğâ±¼ = ğ² - uâ‚™â»Â¹( g(ğ‡âƒ°Ê²_f, ^ğšâ‚•, ^bâ‚•), ğâ±¼ = gâ»Â¹(uâ‚™(ğâ±¼))â‹…(^ğšâ‚•)â»Â¹ ),</li>
<li>^ğšÊ²_f = gâ»Â¹(uâ±¼(ğâ±¼â‚‹â‚)) â‹… ğ±â»Â¹, ^ğšÊ²âˆˆ ğ‘â¿á•áµˆ</li>
<li>^bÊ²_f = âˆšmse(^ğšÊ²_f â‹… ğ± - ğâ±¼â‚‹â‚), ^bÊ²_fâˆˆ ğ‘</li>
</ul></blockquote>
<p>Proof:</p>
<p>Base on Theorem 1, the validity of (21) is obvious. So here, we just prove that the error â€–ğ_Lâ€– is decreasing and bounded below by zero.</p>
<ol>
<li>
<p>Let Î” = â€–eâ±¼â‚‹â‚â€–Â² - â€–ğ² - uâ‚™â»Â¹g(ğ‡âƒ°Ê²_f, ^ğšâ‚•, ^bâ‚•)â€–Â² (last error-current output), and take the newest item apart: <br>
= â€–eâ±¼â‚‹â‚â€–Â² - â€–ğ² - uâ‚™â»Â¹g( (âˆ‘áµ¢â‚Œâ‚Ê²â»Â¹ uáµ¢â»Â¹ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) + uáµ¢â»Â¹g(ğ±, ^ğšÊ²_f, ^bÊ²_f) ), ^ğšâ‚•, ^bâ‚•) â€–Â²  â€ƒ   (24)</p>
</li>
<li>
<p>Let ^TÊ² = uâ‚™â»Â¹g(uâ±¼â»Â¹g(ğ±, ^ğšÊ²_f, ^bÊ²_f), ^ğšâ‚•, ^bâ‚•). Because activation function is sigmoid or sine function, eq. (24) can be simplified as: <br>
Î” â‰¥ â€–ğâ±¼â‚‹â‚â€–Â² - â€–ğ² - uâ‚™â»Â¹g( (âˆ‘áµ¢â‚Œâ‚Ê²â»Â¹ uáµ¢â»Â¹ g(ğ±, ^ğšÊ²_f, ^bÊ²_f) ), ^ğšâ‚•, ^bâ‚•) - ^TÊ²â€–Â² <br>
= â€–ğâ±¼â‚‹â‚â€–Â² - â€–ğâ±¼â‚‹â‚ - ^TÊ²â€–Â²  â€ƒ (unfold)   <br>
= â€–ğâ±¼â‚‹â‚â€–Â² - (â€–ğâ±¼â‚‹â‚â€–Â² - 2&lt;eâ±¼â‚‹â‚, â€–^TÊ²â€–&gt; + â€–^TÊ²â€–Â²) Â  (&quot;&lt;&gt;&quot; is dot product of 2 matrices: Frobenius inner product)<br>
= 2&lt;ğâ±¼â‚‹â‚, â€–^TÊ²â€–&gt; - â€–^TÊ²â€–Â²   <br>
= â€–^TÊ²â€–Â² ( 2&lt;ğâ±¼â‚‹â‚, â€–^TÊ²â€–&gt; / â€–^TÊ²â€–Â² - 1 )   â€ƒ (25)</p>
</li>
<li>
<p>We set ^TÊ² = uâ‚™â»Â¹g(uâ±¼â»Â¹g( ğ±, ^ğšÊ²_f, ^bÊ²_f) ), ^ğšâ‚•, ^bâ‚• ) = ğâ±¼â‚‹â‚ Â± Ïƒ. (Ïƒ is variance, and ğâ±¼â‚‹â‚ is the expectation) <br>
So ğâ±¼â‚‹â‚ = ^TÊ² Â± Ïƒ. <br>
Then &lt;ğâ±¼â‚‹â‚, â€–^TÊ²â€–&gt; = &lt;^TÊ²Â± Ïƒ, â€–^TÊ²â€–&gt; = &lt;â€–^TÊ²â€–Â² Â± &lt;â€–^TÊ²â€–,Ïƒ&gt; &gt;</p>
<p>Hence, eq. (25) can be reformulated: <br>
Î” â‰¥ â€–^TÊ²â€–Â² ( 2&lt; â€–^TÊ²â€–Â² Â± &lt;â€–^TÊ²â€–,Ïƒ&gt; &gt; / â€–^TÊ²â€–Â² - 1 ) <br>
= â€–^TÊ²â€–Â² ( 1 Â± 2â€–Ïƒâ‹…(^TÊ²)áµ€â€–/â€–^TÊ²â€–Â²)  (Wandong thinks there should be a 2.) <br>
â‰¥</p>
</li>
<li>
<p>In addition, based on Theorem 1 and eq. (7), there will be:</p>
<ul>
<li>â€–^TÊ² - ğâ±¼â‚‹â‚â€– â‰¤ min_{^ğšÊ²_fâˆˆ ğ‘áµˆá•â¿} â€–uâ‚™â»Â¹g(uâ±¼â»Â¹g( ğ±, ^ğšÊ²_f, ^bÊ²_f), ^ğšâ‚•, ^bâ‚•) -ğâ±¼â‚‹â‚â€–</li>
<li>â€–Ïƒâ€– â‰¤ â€–^TÊ²â€–</li>
</ul>
<p>Thus Î” â‰¥ 0 can be proved as:
Î” â‰¥ â€–^TÊ²â€–Â² (1 Â± â€–Ïƒâ€– / â€–^TÊ²â€–) â‰¥ 0  â€ƒ (28)</p>
<p>Eq. (28) means â€–ğâ±¼â‚‹â‚â€– â‰¥ â€–ğâ±¼â€– and â€–ğâ€– is decreasing and bounded below by zero.</p>
</li>
</ol>
<p>Based on Theorem 2, Theorem 3 is given:</p>
<blockquote>
<p><strong>Theorem 3</strong>: Given M arbitrary distinct samples (ğ±, ğ²), ğ±âˆˆ ğ‘â¿á•á´¹, ğ²âˆˆ ğ‘áµá•á´¹, a sigmoid or sine activation function g,
and optimal feature data ğ‡âƒ°á´¸_f obtained by Algorithm 1, <br>
then lim_{jâ+âˆ} â€–ğ² - Î²â‚â‹…uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚) - &hellip; - Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼, ğ‘â±¼)â€– = 0
holds with probability one if :</p>
<ul>
<li>ğšâ±¼ = gâ»Â¹( u(ğ²) ) â‹… (ğ‡âƒ°á´¸_f)â»Â¹, ^ğšâ±¼âˆˆ ğ‘áµá•â¿</li>
<li>bâ±¼ = âˆšmse(^ğšâ±¼â‹…(ğ‡âƒ°á´¸_f) - gâ»Â¹(u(ğ²))), ^bâ±¼âˆˆ ğ‘</li>
<li>Î²â±¼ = âŸ¨ğâ±¼â‚‹â‚, g(ğ‡âƒ°á´¸_f, ğšâ±¼, bâ±¼)âŸ© / â€–g(ğ‡âƒ°á´¸_f, ğšâ±¼, bâ±¼)â€–Â², Î²â±¼âˆˆ ğ‘</li>
</ul></blockquote>
<p>Proof:</p>
<p>First prove that the sequence â€–ğâ±¼á´¸â€– is decreasing and bounded below by zero.
Then prove that the lim_{jâ+âˆ} â€–ğâ±¼á´¸â€– = 0</p>
<ol>
<li>
<p>Based on Theorem 1 and Lemma 1, the network output error satisfies:
â€–ğâ±¼á´¸â€– = â€–ğ² - Î²â‚â‹…uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚) - &hellip; - Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼, ğ‘â±¼)â€–  <br>
â‰¤ â€–ğ² -uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚)â€–  <br>
= â€–ğâ‚á´¸â€–</p>
</li>
<li>
<p>Based on Theorem 2, there will be: <br>
â€–ğâ±¼á´¸â€– â‰¤ â€–ğâ±¼á´¸â»Â¹â€– â‰¤ &hellip; â‰¤ â€–ğâ±¼Â¹â€–</p>
</li>
<li>
<p>Thus, â€–ğâ±¼á´¸â€– â‰¤ â€–ğâ±¼á´¸â»Â¹â€– â‰¤ &hellip; â‰¤ â€–ğâ±¼Â¹â€– â‰¤ &hellip; â‰¤ â€–ğâ‚Â¹â€– and â€–ğâ±¼á´¸â€– is decreasing and bounded below by 0.</p>
</li>
<li>
<p>Based on Lemma 1, when all hidden nodes randomly generated based on any continuous sampling distribution,
lim_{nââˆ} â€–f - (fâ‚™â‚‹â‚ + Î²â‚™â‹…g(ğ±, ğ›‚â‚™Ê³, â™­â‚™Ê³) )â€– = 0 holds with probability one if
Î²â‚™ = âŸ¨ğâ‚™â‚‹â‚, g(ğ‡âƒ°á´¸â±¼, ğ›‚â‚™Ê³, â™­â‚™Ê³)âŸ© / â€–g(ğ‡âƒ°á´¸â±¼, ğ›‚â‚™Ê³, â™­â‚™Ê³)â€–Â².</p>
</li>
<li>
<p>In addition, ELM theories have shown that almost any nonlinear piecewise continuous random hidden node can be use in ELM, and the resultant networks have universal approximation capbilities. <br>
According to the definition of general hidden neurons, (a general hidden node contains m (basic) hidden node),
a general hidden node (ğš,b) = (ğ›‚Ê³â‚, &hellip;, ğ›‚Ê³â‚˜, bÊ³â‚, &hellip;, bÊ³â‚˜), . <br>
Thus its output is g(ğ‡âƒ°á´¸_f, ğšâ±¼Ê³, â™­â±¼Ê³) â‰¡ âˆ‘áµ¢â‚Œâ‚áµ g(ğ‡âƒ°á´¸_f, ğ›‚Ê³áµ¢, bÊ³áµ¢).</p>
<p>Therefore, lim_{jââˆ} â€– ğ² - Î²â‚â‹…uâ‚â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ‚, ğ‘â‚) - &hellip; - Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼, ğ‘â±¼)â€–<br>
= lim_{nââˆ} â€–f- (fâ‚™â‚‹â‚ + Î²â±¼â‹…uâ±¼â»Â¹g(ğ‡âƒ°á´¸_f, ğšâ±¼Ê³, ğ‘â±¼Ê³)) â€– = 0</p>
</li>
</ol>
<h3 id="d-proposed-method-with-multinetwork-structures">D. Proposed Method With Multinetwork Structures
</h3><div class="mermaid">%%{ init: { 'flowchart': { 'curve': 'basis' } } }%%
flowchart LR
subgraph in["input feature"]
x1((x1)) & xe(("â‹®")) & xn((xn))
end
subgraph net1["Layer 1"]
l11((a,b)) & l1e(("â‹®")) & l1L((a,b))
end
subgraph net2["Layer 2"]
l21((a,b)) & l2e(("â‹®")) & l2L((a,b))
end
subgraph netC["Layer C"]
lC1((a,b)) & lCe(("â‹®")) & lCL((a,b))
end
x1 & xn --> l11 & l1L
l11 & l1L --> l21 & l2L
l21 & l2L -.-> lC1 & lCL

subgraph out["output y"]
direction LR
y1((1)) & ye(("â‹®")) & ym((m))
end
subgraph belm1["Basic ELM 1"]
direction LR
b11(("aâ‚•,bâ‚•")) & b1e(("â‹®")) & b1m((aâ‚•,bâ‚•))
end
subgraph belm2["Basic ELM 2"]
direction LR
b21((aâ‚•,bâ‚•)) & b2e(("â‹®")) & b2m((aâ‚•,bâ‚•))
end

net1 --> feat1["Feature\n data\n ğ‡Â¹<sub>f</sub>"] --> belm1 --> out
feat1 --> net2 --> feat2["Feature\n data\n ğ‡Â²<sub>f</sub>"] --> belm2 --> out
netC --> featC["Feature\n data\n ğ‡á¶œ<sub>f</sub>"]

subgraph MultiLayer ELM
in & net1 & net2 & netC
end
%% inverse for initialization weights
linkStyle 12,13,14,16,17,18 stroke:#f0f
</div>

<p><font color="#f0f">Pink links</font> will do inverse to calculate the weights for corresponding layers.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/slfn/">SLFN</a>
        
    </section>


    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    

    

    
      <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
      <script>
        mermaid.initialize({ startOnLoad: true });
      </script>
    


    

    


    
    


    
    


    
    

    

    


</article>



    

    

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2025 Zichen Wang
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
